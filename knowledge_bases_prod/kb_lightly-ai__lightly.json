[
  "def load_description(path_dir=PATH_ROOT, filename='DOCS.md'):\n    \"\"\"Load long description from readme in the path_dir/ directory\n\n    \"\"\"\n    with open(os.path.join(path_dir, filename)) as f:\n        long_description = f.read()\n    return long_description",
  "def load_requirements(path_dir=PATH_ROOT, filename='base.txt', comment_char='#'):\n    \"\"\"From pytorch-lightning repo: https://github.com/PyTorchLightning/pytorch-lightning.\n       Load requirements from text file in the path_dir/requirements/ directory.\n\n    \"\"\"\n    with open(os.path.join(path_dir, 'requirements', filename), 'r') as file:\n        lines = [ln.strip() for ln in file.readlines()]\n    reqs = []\n    for ln in lines:\n        # filer all comments\n        if comment_char in ln:\n            ln = ln[:ln.index(comment_char)].strip()\n        # skip directly installed dependencies\n        if ln.startswith('http'):\n            continue\n        if ln:  # if requirement is not empty\n            reqs.append(ln)\n    return reqs",
  "def _get_config_path(config_path):\n    \"\"\"Find path to yaml config file\n\n    Args:\n        config_path: (str) Path to config.yaml file\n\n    Returns:\n        Path to config.yaml if specified else default config.yaml\n\n    Raises:\n        ValueError if the config_path is not None but doesn't exist\n\n    \"\"\"\n    if config_path is None:\n        dirname = os.path.dirname(cli.__file__)\n        config_path = os.path.join(dirname, 'config/config.yaml')\n    if not os.path.exists(config_path):\n        raise ValueError(\"Config path {} does not exist!\".format(config_path))\n\n    return config_path",
  "def _load_config_file(config_path):\n    \"\"\"Load a yaml config file\n\n    Args:\n        config_path: (str) Path to config.yaml file\n\n    Returns:\n        Dictionary with configs from config.yaml\n\n    \"\"\"\n    Loader = yaml.FullLoader\n    with open(config_path, 'r') as config_file:\n        cfg = yaml.load(config_file, Loader=Loader)\n\n    return cfg",
  "def _add_kwargs(cfg, kwargs):\n    \"\"\"Add keyword arguments to config\n\n    Args:\n        cfg: (dict) Dictionary of configs from config.yaml\n        kwargs: (dict) Dictionary of keyword arguments\n\n    Returns:\n        Union of cfg and kwargs\n\n    \"\"\"\n    for key, item in kwargs.items():\n        if isinstance(item, dict):\n            if key in cfg:\n                cfg[key] = _add_kwargs(cfg[key], item)\n            else:\n                cfg[key] = item\n        else:\n            cfg[key] = item\n    return cfg",
  "def train_model_and_embed_images(config_path: str = None, **kwargs):\n    \"\"\"Train a self-supervised model and use it to embed images.\n\n    Calls the same function as lightly-magic. All arguments passed to\n    lightly-magic can also be passed to this function (see below for an\n    example).\n\n    Args:\n        config_path:\n            Path to config.yaml. If None, the default configs will be used.\n        **kwargs:\n            Overwrite default configs py passing keyword arguments.\n\n    Returns:\n        Embeddings, labels, and filenames of the images.\n\n    Examples:\n        >>> import lightly\n        >>>\n        >>> # train a model and embed images with default configs\n        >>> embeddings, _, _ = lightly.train_model_and_embed_images(\n        >>>     input_dir='path/to/data')\n        >>>\n        >>> #\u00a0train a model and embed images with separate config file\n        >>> my_config_path = 'my/config/file.yaml'\n        >>> embeddings, _, _ = lightly.train_model_and_embed_images(\n        >>>     input_dir='path/to/data', config_path=my_config_path)\n        >>>\n        >>> # train a model and embed images with default settings + overwrites\n        >>> my_trainer = {max_epochs: 10}\n        >>> embeddings, _, _ = lightly.train_model_and_embed_images(\n        >>>     input_dir='path/to/data', trainer=my_trainer)\n        >>> #\u00a0the command above is equivalent to:\n        >>> #\u00a0lightly-magic input_dir='path/to/data' trainer.max_epochs=10\n\n    \"\"\"\n    config_path = _get_config_path(config_path)\n    config_args = _load_config_file(config_path)\n    config_args = _add_kwargs(config_args, kwargs)\n    return _lightly_cli(config_args, is_cli_call=False)",
  "def train_embedding_model(config_path: str = None, **kwargs):\n    \"\"\"Train a self-supervised model.\n\n    Calls the same function as lightly-train. All arguments passed to\n    lightly-train can also be passed to this function (see below for an\n    example).\n\n    Args:\n        config_path:\n            Path to config.yaml. If None, the default configs will be used.\n        **kwargs:\n            Overwrite default configs py passing keyword arguments.\n\n    Returns:\n        Path to checkpoint of the trained embedding model.\n\n    Examples:\n        >>> import lightly\n        >>>\n        >>> # train a model with default configs\n        >>> checkpoint_path = lightly.train_embedding_model(\n        >>>     input_dir='path/to/data')\n        >>>\n        >>> #\u00a0train a model with separate config file\n        >>> my_config_path = 'my/config/file.yaml'\n        >>> checkpoint_path = lightly.train_embedding_model(\n        >>>     input_dir='path/to/data', config_path=my_config_path)\n        >>>\n        >>> # train a model with default settings and overwrites: large batch\n        >>> # sizes are benefitial for self-supervised training and more \n        >>> #\u00a0workers speed up the dataloading process.\n        >>> my_loader = {\n        >>>     batch_size: 100,\n        >>>     num_workers: 8,\n        >>> }\n        >>> checkpoint_path = lightly.train_embedding_model(\n        >>>     input_dir='path/to/data', loader=my_loader)\n        >>> #\u00a0the command above is equivalent to:\n        >>> #\u00a0lightly-train input_dir='path/to/data' loader.batch_size=100 loader.num_workers=8\n    \"\"\"\n    config_path = _get_config_path(config_path)\n    config_args = _load_config_file(config_path)\n    config_args = _add_kwargs(config_args, kwargs)\n\n    return _train_cli(config_args, is_cli_call=False)",
  "def embed_images(checkpoint: str, config_path: str = None, **kwargs):\n    \"\"\"Embed images with a self-supervised model.\n\n    Calls the same function as lightly-embed. All arguments passed to\n    lightly-embed can also be passed to this function (see below for an\n    example).\n\n    Args:\n        checkpoint:\n            Path to the checkpoint file for the embedding model.\n        config_path:\n            Path to config.yaml. If None, the default configs will be used.\n        **kwargs:\n            Overwrite default configs py passing keyword arguments.\n\n    Returns:\n        Embeddings, labels, and filenames of the images.\n\n    Examples:\n        >>> import lightly\n        >>> my_checkpoint_path = 'path/to/checkpoint.ckpt'\n        >>>\n        >>> # embed images with default configs\n        >>> embeddings, _, _ = lightly.embed_images(\n        >>>     my_checkpoint_path, input_dir='path/to/data')\n        >>>\n        >>> #\u00a0embed images with separate config file\n        >>> my_config_path = 'my/config/file.yaml'\n        >>> embeddings, _, _ = lightly.embed_images(\n        >>>     my_checkpoint_path, input_dir='path/to/data', config_path=my_config_path)\n        >>>\n        >>> # embed images with default settings and overwrites: at inference,\n        >>> #\u00a0we can use larger input_sizes because it requires less memory.\n        >>> my_collate = {input_size: 256}\n        >>> embeddings, _, _ = lightly.embed_images(\n        >>>     my_checkpoint_path, input_dir='path/to/data', collate=my_collate)\n        >>> #\u00a0the command above is equivalent to:\n        >>> #\u00a0lightly-embed input_dir='path/to/data' collate.input_size=256\n\n    \"\"\"\n    config_path = _get_config_path(config_path)\n    config_args = _load_config_file(config_path)\n    config_args = _add_kwargs(config_args, kwargs)\n\n    config_args['checkpoint'] = checkpoint\n\n    return _embed_cli(config_args, is_cli_call=False)",
  "def _is_prefetch_generator_available():\n        return _prefetch_generator_available",
  "class SymNegCosineSimilarityLoss(torch.nn.Module):\n    \"\"\"Implementation of the Symmetrized Loss used in the SimSiam[0] paper.\n\n    [0] SimSiam, 2020, https://arxiv.org/abs/2011.10566\n    \n    Examples:\n\n        >>> # initialize loss function\n        >>> loss_fn = SymNegCosineSimilarityLoss()\n        >>>\n        >>> # generate two random transforms of images\n        >>> t0 = transforms(images)\n        >>> t1 = transforms(images)\n        >>>\n        >>> # feed through SimSiam model\n        >>> out0, out1 = model(t0, t1)\n        >>>\n        >>> # calculate loss\n        >>> loss = loss_fn(out0, out1)\n\n    \"\"\"\n\n    def _neg_cosine_simililarity(self, x, y):\n        v = - torch.nn.functional.cosine_similarity(x, y.detach(), dim=-1).mean()\n        return v\n\n    def forward(self, \n                out0: torch.Tensor, \n                out1: torch.Tensor):\n        \"\"\"Forward pass through Symmetric Loss.\n\n            Args:\n                out0:\n                    Output projections of the first set of transformed images.\n                    Expects the tuple to be of the form (z0, p0), where z0 is\n                    the output of the backbone and projection mlp, and p0 is the\n                    output of the prediction head.\n                out1:\n                    Output projections of the second set of transformed images.\n                    Expects the tuple to be of the form (z1, p1), where z1 is\n                    the output of the backbone and projection mlp, and p1 is the\n                    output of the prediction head.\n \n            Returns:\n                Contrastive Cross Entropy Loss value.\n\n            Raises:\n                ValueError if shape of output is not multiple of batch_size.\n        \"\"\"\n        z0, p0 = out0\n        z1, p1 = out1\n\n        loss = self._neg_cosine_simililarity(p0, z1) / 2 + \\\n               self._neg_cosine_simililarity(p1, z0) / 2\n\n        return loss",
  "def _neg_cosine_simililarity(self, x, y):\n        v = - torch.nn.functional.cosine_similarity(x, y.detach(), dim=-1).mean()\n        return v",
  "def forward(self, \n                out0: torch.Tensor, \n                out1: torch.Tensor):\n        \"\"\"Forward pass through Symmetric Loss.\n\n            Args:\n                out0:\n                    Output projections of the first set of transformed images.\n                    Expects the tuple to be of the form (z0, p0), where z0 is\n                    the output of the backbone and projection mlp, and p0 is the\n                    output of the prediction head.\n                out1:\n                    Output projections of the second set of transformed images.\n                    Expects the tuple to be of the form (z1, p1), where z1 is\n                    the output of the backbone and projection mlp, and p1 is the\n                    output of the prediction head.\n \n            Returns:\n                Contrastive Cross Entropy Loss value.\n\n            Raises:\n                ValueError if shape of output is not multiple of batch_size.\n        \"\"\"\n        z0, p0 = out0\n        z1, p1 = out1\n\n        loss = self._neg_cosine_simililarity(p0, z1) / 2 + \\\n               self._neg_cosine_simililarity(p1, z0) / 2\n\n        return loss",
  "class HypersphereLoss(torch.nn.Module):\n    \"\"\"\n\n    Implementation of the loss described in 'Understanding Contrastive Representation Learning through\n    Alignment and Uniformity on the Hypersphere.' [0]\n    \n    [0] Tongzhou Wang. et.al, 2020, ... https://arxiv.org/abs/2005.10242\n\n    Note\n    ----\n    In order for this loss to function as advertized, an l1-normalization to the hypersphere is required.\n    This loss function applies this l1-normalization internally in the loss-layer.\n    However, it is recommended that the same normalization is also applied in your architecture,\n    considering that this l1-loss is also intended to be applied during inference.\n    Perhaps there may be merit in leaving it out of the inferrence pathway, but this use has not been tested.\n\n    Moreover it is recommended that the layers preceeding this loss function are either a linear layer without activation,\n    a batch-normalization layer, or both. The directly upstream architecture can have a large influence\n    on the ability of this loss to achieve its stated aim of promoting uniformity on the hypersphere;\n    and if by contrast the last layer going into the embedding is a RELU or similar nonlinearity,\n    we may see that we will never get very close to achieving the goal of uniformity on the hypersphere,\n    but will confine ourselves to the subspace of positive activations.\n    Similar architectural considerations are relevant to most contrastive loss functions,\n    but we call it out here explicitly.\n\n        Examples:\n\n        >>> # initialize loss function\n        >>> loss_fn = HypersphereLoss()\n        >>>\n        >>> # generate two random transforms of images\n        >>> t0 = transforms(images)\n        >>> t1 = transforms(images)\n        >>>\n        >>> # feed through SimSiam model\n        >>> out0, out1 = model(t0, t1)\n        >>>\n        >>> # calculate loss\n        >>> loss = loss_fn(out0, out1)\n\n    \"\"\"\n\n    def __init__(self, t=1., lam=1., alpha=2.):\n        \"\"\"Parameters as described in [0]\n\n        Args:\n            t : float\n                Temperature parameter;\n                proportional to the inverse variance of the Gaussians used to measure uniformity\n            lam : float:\n                Weight balancing the alignment and uniformity loss terms\n            alpha : float\n                Power applied to the alignment term of the loss. At its default value of 2,\n                distances between positive samples are penalized in an l-2 sense.\n\n        \"\"\"\n        super(HypersphereLoss, self).__init__()\n        self.t = t\n        self.lam = lam\n        self.alpha = alpha\n\n    def forward(self, z_a: torch.Tensor, z_b: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n\n        Args:\n            x : torch.Tensor, [b, d], float\n            y : torch.Tensor, [b, d], float\n\n        Returns:\n            torch.Tensor, [], float\n                scalar loss value\n\n        \"\"\"\n        x = F.normalize(z_a)\n        y = F.normalize(z_b)\n\n        def lalign(x, y):\n            return (x - y).norm(dim=1).pow(self.alpha).mean()\n        def lunif(x):\n            sq_pdist = torch.pdist(x, p=2).pow(2)\n            return sq_pdist.mul(-self.t).exp().mean().log()\n        return lalign(x, y) + self.lam * (lunif(x) + lunif(y)) / 2",
  "def __init__(self, t=1., lam=1., alpha=2.):\n        \"\"\"Parameters as described in [0]\n\n        Args:\n            t : float\n                Temperature parameter;\n                proportional to the inverse variance of the Gaussians used to measure uniformity\n            lam : float:\n                Weight balancing the alignment and uniformity loss terms\n            alpha : float\n                Power applied to the alignment term of the loss. At its default value of 2,\n                distances between positive samples are penalized in an l-2 sense.\n\n        \"\"\"\n        super(HypersphereLoss, self).__init__()\n        self.t = t\n        self.lam = lam\n        self.alpha = alpha",
  "def forward(self, z_a: torch.Tensor, z_b: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n\n        Args:\n            x : torch.Tensor, [b, d], float\n            y : torch.Tensor, [b, d], float\n\n        Returns:\n            torch.Tensor, [], float\n                scalar loss value\n\n        \"\"\"\n        x = F.normalize(z_a)\n        y = F.normalize(z_b)\n\n        def lalign(x, y):\n            return (x - y).norm(dim=1).pow(self.alpha).mean()\n        def lunif(x):\n            sq_pdist = torch.pdist(x, p=2).pow(2)\n            return sq_pdist.mul(-self.t).exp().mean().log()\n        return lalign(x, y) + self.lam * (lunif(x) + lunif(y)) / 2",
  "def lalign(x, y):\n            return (x - y).norm(dim=1).pow(self.alpha).mean()",
  "def lunif(x):\n            sq_pdist = torch.pdist(x, p=2).pow(2)\n            return sq_pdist.mul(-self.t).exp().mean().log()",
  "class NTXentLoss(MemoryBankModule):\n    \"\"\"Implementation of the Contrastive Cross Entropy Loss.\n\n    This implementation follows the SimCLR[0] paper. If you enable the memory\n    bank by setting the `memory_bank_size` value > 0 the loss behaves like \n    the one described in the MoCo[1] paper.\n\n    [0] SimCLR, 2020, https://arxiv.org/abs/2002.05709\n    [1] MoCo, 2020, https://arxiv.org/abs/1911.05722\n    \n    Attributes:\n        temperature:\n            Scale logits by the inverse of the temperature.\n        use_cosine_similarity:\n            Whether to use cosine similarity over L2 distance.\n        memory_bank_size:\n            Number of negative samples to store in the memory bank. \n            Use 0 for SimCLR. For MoCo we typically use numbers like 4096 or 65536.\n\n    Raises:\n        ValueError if abs(temperature) < 1e-8 to prevent divide by zero.\n\n    Examples:\n\n        >>> # initialize loss function without memory bank\n        >>> loss_fn = NTXentLoss(memory_bank_size=0)\n        >>>\n        >>> # generate two random transforms of images\n        >>> t0 = transforms(images)\n        >>> t1 = transforms(images)\n        >>>\n        >>> # feed through SimCLR or MoCo model\n        >>> batch = torch.cat((t0, t1), dim=0)\n        >>> output = model(batch)\n        >>>\n        >>> # calculate loss\n        >>> loss = loss_fn(output)\n\n    \"\"\"\n\n    def __init__(self,\n                temperature: float = 0.5,\n                use_cosine_similarity: bool = True,\n                memory_bank_size: int = 0):\n        super(NTXentLoss, self).__init__(size=memory_bank_size)\n        self.temperature = temperature\n        self.similarity_function = self._get_similarity_function(\n                                        use_cosine_similarity)\n        self.cross_entropy = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n        self.correlated_mask = None\n        self.eps = 1e-8\n\n        if abs(self.temperature) < self.eps:\n            raise ValueError('Illegal temperature: abs({}) < 1e-8'\n                             .format(self.temperature))\n\n    def _get_similarity_function(self, use_cosine_similarity):\n        if use_cosine_similarity:\n            self._cosine_similarity = torch.nn.CosineSimilarity(dim=-1)\n            return self._cosine_simililarity\n        else:\n            return self._dot_simililarity\n\n    def _torch_get_correlated_mask(self, batch_size, device=None):\n        diag = torch.eye(2 * batch_size, device=device)\n        diag[batch_size:, :batch_size] += torch.eye(batch_size, device=device)\n        diag[:batch_size, batch_size:] += torch.eye(batch_size, device=device)\n        mask = (1 - diag).type(torch.bool)\n        return mask\n\n    def _get_correlated_mask(self, batch_size):\n        # TODO: deprecate\n        diag = np.eye(2 * batch_size)\n        l1 = np.eye((2 * batch_size), 2 * batch_size, k=-batch_size)\n        l2 = np.eye((2 * batch_size), 2 * batch_size, k=batch_size)\n        mask = torch.from_numpy((diag + l1 + l2))\n        mask = (1 - mask).type(torch.bool)\n        if torch.cuda.is_available():\n            mask.to(\"cuda\")\n        return mask\n\n    @staticmethod\n    def _dot_simililarity(x, y):\n        v = torch.tensordot(x.unsqueeze(1), y.T.unsqueeze(0), dims=2)\n        # x shape: (N, 1, C)\n        # y shape: (1, C, 2N)\n        # v shape: (N, 2N)\n        return v\n\n    def _cosine_simililarity(self, x, y):\n        # x shape: (N, 1, C)\n        # y shape: (1, 2N, C)\n        # v shape: (N, 2N)\n        v = self._cosine_similarity(x.unsqueeze(1), y.unsqueeze(0))\n        return v\n\n    def forward(self,\n                out0: torch.Tensor,\n                out1: torch.Tensor):\n        \"\"\"Forward pass through Contrastive Cross-Entropy Loss.\n\n        If used with a memory bank, the samples from the memory bank are used\n        as negative examples. Otherwise, within-batch samples are used as \n        negative samples.\n\n            Args:\n                out0:\n                    Output projections of the first set of transformed images.\n                out1:\n                    Output projections of the second set of transformed images.\n\n            Returns:\n                Contrastive Cross Entropy Loss value.\n\n        \"\"\"\n\n        device = out0.device\n        batch_size, _ = out0.shape\n\n        # normalize the output to length 1\n        out0 = torch.nn.functional.normalize(out0, dim=1)\n        out1 = torch.nn.functional.normalize(out1, dim=1)\n\n        # ask memory bank for negative samples and extend it with out1 if \n        # out1 requires a gradient, otherwise keep the same vectors in the \n        # memory bank (this allows for keeping the memory bank constant e.g.\n        # for evaluating the loss on the test set)\n        out1, negatives = \\\n            super(NTXentLoss, self).forward(out1, update=out0.requires_grad)\n\n        if negatives is not None:\n            negatives = negatives.to(device)\n            # use negatives from memory bank\n            l_pos = torch.einsum('nc,nc->n', [out0, out1]).unsqueeze(-1)\n            l_neg = torch.einsum('nc,ck->nk', [out0, negatives.clone().detach()])\n        else:\n            # use other samples from batch as negatives\n            output = torch.cat((out0, out1), axis=0)\n            similarity_matrix = self.similarity_function(output, output)\n\n            # filter out the scores from the positive samples\n            l_pos = torch.diag(similarity_matrix, batch_size)\n            r_pos = torch.diag(similarity_matrix, -batch_size)\n            l_pos = torch.cat([l_pos, r_pos]).view(2 * batch_size, 1)\n\n            if self.correlated_mask is None:\n                self.correlated_mask = \\\n                    self._torch_get_correlated_mask(batch_size, device=device)\n            if 2 * batch_size != self.correlated_mask.shape[0]:\n                self.correlated_mask = \\\n                    self._torch_get_correlated_mask(batch_size, device=device)\n\n            l_neg = similarity_matrix[\n                self.correlated_mask].view(2 * batch_size, -1)\n\n        logits = torch.cat([l_pos, l_neg], dim=1)\n        logits /= self.temperature\n\n        labels = torch.zeros(logits.shape[0]).long()\n        loss = self.cross_entropy(logits, labels.to(device))\n\n        return loss",
  "def __init__(self,\n                temperature: float = 0.5,\n                use_cosine_similarity: bool = True,\n                memory_bank_size: int = 0):\n        super(NTXentLoss, self).__init__(size=memory_bank_size)\n        self.temperature = temperature\n        self.similarity_function = self._get_similarity_function(\n                                        use_cosine_similarity)\n        self.cross_entropy = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n        self.correlated_mask = None\n        self.eps = 1e-8\n\n        if abs(self.temperature) < self.eps:\n            raise ValueError('Illegal temperature: abs({}) < 1e-8'\n                             .format(self.temperature))",
  "def _get_similarity_function(self, use_cosine_similarity):\n        if use_cosine_similarity:\n            self._cosine_similarity = torch.nn.CosineSimilarity(dim=-1)\n            return self._cosine_simililarity\n        else:\n            return self._dot_simililarity",
  "def _torch_get_correlated_mask(self, batch_size, device=None):\n        diag = torch.eye(2 * batch_size, device=device)\n        diag[batch_size:, :batch_size] += torch.eye(batch_size, device=device)\n        diag[:batch_size, batch_size:] += torch.eye(batch_size, device=device)\n        mask = (1 - diag).type(torch.bool)\n        return mask",
  "def _get_correlated_mask(self, batch_size):\n        # TODO: deprecate\n        diag = np.eye(2 * batch_size)\n        l1 = np.eye((2 * batch_size), 2 * batch_size, k=-batch_size)\n        l2 = np.eye((2 * batch_size), 2 * batch_size, k=batch_size)\n        mask = torch.from_numpy((diag + l1 + l2))\n        mask = (1 - mask).type(torch.bool)\n        if torch.cuda.is_available():\n            mask.to(\"cuda\")\n        return mask",
  "def _dot_simililarity(x, y):\n        v = torch.tensordot(x.unsqueeze(1), y.T.unsqueeze(0), dims=2)\n        # x shape: (N, 1, C)\n        # y shape: (1, C, 2N)\n        # v shape: (N, 2N)\n        return v",
  "def _cosine_simililarity(self, x, y):\n        # x shape: (N, 1, C)\n        # y shape: (1, 2N, C)\n        # v shape: (N, 2N)\n        v = self._cosine_similarity(x.unsqueeze(1), y.unsqueeze(0))\n        return v",
  "def forward(self,\n                out0: torch.Tensor,\n                out1: torch.Tensor):\n        \"\"\"Forward pass through Contrastive Cross-Entropy Loss.\n\n        If used with a memory bank, the samples from the memory bank are used\n        as negative examples. Otherwise, within-batch samples are used as \n        negative samples.\n\n            Args:\n                out0:\n                    Output projections of the first set of transformed images.\n                out1:\n                    Output projections of the second set of transformed images.\n\n            Returns:\n                Contrastive Cross Entropy Loss value.\n\n        \"\"\"\n\n        device = out0.device\n        batch_size, _ = out0.shape\n\n        # normalize the output to length 1\n        out0 = torch.nn.functional.normalize(out0, dim=1)\n        out1 = torch.nn.functional.normalize(out1, dim=1)\n\n        # ask memory bank for negative samples and extend it with out1 if \n        # out1 requires a gradient, otherwise keep the same vectors in the \n        # memory bank (this allows for keeping the memory bank constant e.g.\n        # for evaluating the loss on the test set)\n        out1, negatives = \\\n            super(NTXentLoss, self).forward(out1, update=out0.requires_grad)\n\n        if negatives is not None:\n            negatives = negatives.to(device)\n            # use negatives from memory bank\n            l_pos = torch.einsum('nc,nc->n', [out0, out1]).unsqueeze(-1)\n            l_neg = torch.einsum('nc,ck->nk', [out0, negatives.clone().detach()])\n        else:\n            # use other samples from batch as negatives\n            output = torch.cat((out0, out1), axis=0)\n            similarity_matrix = self.similarity_function(output, output)\n\n            # filter out the scores from the positive samples\n            l_pos = torch.diag(similarity_matrix, batch_size)\n            r_pos = torch.diag(similarity_matrix, -batch_size)\n            l_pos = torch.cat([l_pos, r_pos]).view(2 * batch_size, 1)\n\n            if self.correlated_mask is None:\n                self.correlated_mask = \\\n                    self._torch_get_correlated_mask(batch_size, device=device)\n            if 2 * batch_size != self.correlated_mask.shape[0]:\n                self.correlated_mask = \\\n                    self._torch_get_correlated_mask(batch_size, device=device)\n\n            l_neg = similarity_matrix[\n                self.correlated_mask].view(2 * batch_size, -1)\n\n        logits = torch.cat([l_pos, l_neg], dim=1)\n        logits /= self.temperature\n\n        labels = torch.zeros(logits.shape[0]).long()\n        loss = self.cross_entropy(logits, labels.to(device))\n\n        return loss",
  "class MemoryBankModule(torch.nn.Module):\n    \"\"\"Memory bank implementation\n\n    This is a parent class to all loss functions implemented by the lightly\n    Python package. This way, any loss can be used with a memory bank if \n    desired.\n\n    Attributes:\n        size:\n            Number of keys the memory bank can store. If set to 0,\n            memory bank is not used.\n\n    Examples:\n        >>> class MyLossFunction(MemoryBankModule):\n        >>>\n        >>>     def __init__(self, memory_bank_size: int = 2 ** 16):\n        >>>         super(MyLossFunction, self).__init__(memory_bank_size)\n        >>>\n        >>>     def forward(self, output: torch.Tensor,\n        >>>                 labels: torch.Tensor = None):\n        >>>\n        >>>         output, negatives = super(\n        >>>             MyLossFunction, self).forward(output)\n        >>>\n        >>>         if negatives is not None:\n        >>>             # evaluate loss with negative samples\n        >>>         else:\n        >>>             # evaluate loss without negative samples\n\n    \"\"\"\n\n    def __init__(self, size: int = 2 ** 16):\n\n        super(MemoryBankModule, self).__init__()\n\n        if size < 0:\n            msg = f'Illegal memory bank size {size}, must be non-negative.'\n            raise ValueError(msg)\n\n        self.size = size\n\n        self.bank = None\n        self.bank_ptr = None\n    \n    @torch.no_grad()\n    def _init_memory_bank(self, dim: int):\n        \"\"\"Initialize the memory bank if it's empty\n\n        Args:\n            dim:\n                The dimension of the which are stored in the bank.\n\n        \"\"\"\n        # create memory bank\n        # we could use register buffers like in the moco repo\n        # https://github.com/facebookresearch/moco but we don't\n        # want to pollute our checkpoints\n        self.bank = torch.randn(dim, self.size)\n        self.bank = torch.nn.functional.normalize(self.bank, dim=0)\n        self.bank_ptr = torch.LongTensor([0])\n\n    @torch.no_grad()\n    def _dequeue_and_enqueue(self, batch: torch.Tensor):\n        \"\"\"Dequeue the oldest batch and add the latest one\n\n        Args:\n            batch:\n                The latest batch of keys to add to the memory bank.\n\n        \"\"\"\n        batch_size = batch.shape[0]\n        ptr = int(self.bank_ptr)\n\n        if ptr + batch_size >= self.size:\n            self.bank[:, ptr:] = batch[:self.size - ptr].T.detach()\n            self.bank_ptr[0] = 0\n        else:\n            self.bank[:, ptr:ptr + batch_size] = batch.T.detach()\n            self.bank_ptr[0] = ptr + batch_size\n\n    def forward(self,\n                output: torch.Tensor,\n                labels: torch.Tensor = None,\n                update: bool = False):\n        \"\"\"Query memory bank for additional negative samples\n\n        Args:\n            output:\n                The output of the model.\n            labels:\n                Should always be None, will be ignored.\n\n        Returns:\n            The output if the memory bank is of size 0, otherwise the output\n            and the entries from the memory bank.\n\n        \"\"\"\n\n        # no memory bank, return the output\n        if self.size == 0:\n            return output, None\n\n        _, dim = output.shape\n\n        # initialize the memory bank if it is not already done\n        if self.bank is None:\n            self._init_memory_bank(dim)\n\n        # query and update memory bank\n        bank = self.bank.clone().detach()\n\n        # only update memory bank if we later do backward pass (gradient)\n        if update:\n            self._dequeue_and_enqueue(output)\n\n        return output, bank",
  "def __init__(self, size: int = 2 ** 16):\n\n        super(MemoryBankModule, self).__init__()\n\n        if size < 0:\n            msg = f'Illegal memory bank size {size}, must be non-negative.'\n            raise ValueError(msg)\n\n        self.size = size\n\n        self.bank = None\n        self.bank_ptr = None",
  "def _init_memory_bank(self, dim: int):\n        \"\"\"Initialize the memory bank if it's empty\n\n        Args:\n            dim:\n                The dimension of the which are stored in the bank.\n\n        \"\"\"\n        # create memory bank\n        # we could use register buffers like in the moco repo\n        # https://github.com/facebookresearch/moco but we don't\n        # want to pollute our checkpoints\n        self.bank = torch.randn(dim, self.size)\n        self.bank = torch.nn.functional.normalize(self.bank, dim=0)\n        self.bank_ptr = torch.LongTensor([0])",
  "def _dequeue_and_enqueue(self, batch: torch.Tensor):\n        \"\"\"Dequeue the oldest batch and add the latest one\n\n        Args:\n            batch:\n                The latest batch of keys to add to the memory bank.\n\n        \"\"\"\n        batch_size = batch.shape[0]\n        ptr = int(self.bank_ptr)\n\n        if ptr + batch_size >= self.size:\n            self.bank[:, ptr:] = batch[:self.size - ptr].T.detach()\n            self.bank_ptr[0] = 0\n        else:\n            self.bank[:, ptr:ptr + batch_size] = batch.T.detach()\n            self.bank_ptr[0] = ptr + batch_size",
  "def forward(self,\n                output: torch.Tensor,\n                labels: torch.Tensor = None,\n                update: bool = False):\n        \"\"\"Query memory bank for additional negative samples\n\n        Args:\n            output:\n                The output of the model.\n            labels:\n                Should always be None, will be ignored.\n\n        Returns:\n            The output if the memory bank is of size 0, otherwise the output\n            and the entries from the memory bank.\n\n        \"\"\"\n\n        # no memory bank, return the output\n        if self.size == 0:\n            return output, None\n\n        _, dim = output.shape\n\n        # initialize the memory bank if it is not already done\n        if self.bank is None:\n            self._init_memory_bank(dim)\n\n        # query and update memory bank\n        bank = self.bank.clone().detach()\n\n        # only update memory bank if we later do backward pass (gradient)\n        if update:\n            self._dequeue_and_enqueue(output)\n\n        return output, bank",
  "class BarlowTwinsLoss(torch.nn.Module):\n    \"\"\"Implementation of the Barlow Twins Loss from Barlow Twins[0] paper.\n    This code specifically implements the Figure Algorithm 1 from [0].\n    \n    [0] Zbontar,J. et.al, 2021, Barlow Twins... https://arxiv.org/abs/2103.03230\n\n        Examples:\n\n        >>> # initialize loss function\n        >>> loss_fn = BarlowTwinsLoss()\n        >>>\n        >>> # generate two random transforms of images\n        >>> t0 = transforms(images)\n        >>> t1 = transforms(images)\n        >>>\n        >>> # feed through SimSiam model\n        >>> out0, out1 = model(t0, t1)\n        >>>\n        >>> # calculate loss\n        >>> loss = loss_fn(out0, out1)\n\n    \"\"\"\n\n    def __init__(self, lambda_param=5e-3):\n        \"\"\"Lambda param configuration with default value like in [0]\n\n        Args:\n            lambda_param ([float], optional): parameter for importance of\n                redundancy reduction term.\n            Defaults to 5e-3 [0].\n        \"\"\"\n        super(BarlowTwinsLoss, self).__init__()\n        self.lambda_param = lambda_param\n\n    def forward(self, z_a: torch.Tensor, z_b: torch.Tensor):\n\n        device = z_a.device\n\n        # normalize repr. along the batch dimension\n        z_a_norm = (z_a - z_a.mean(0)) / z_a.std(0) # NxD\n        z_b_norm = (z_b - z_b.mean(0)) / z_b.std(0) # NxD\n\n        N = z_a.size(0)\n        D = z_a.size(1)\n\n        # cross-correlation matrix\n        c = torch.mm(z_a_norm.T, z_b_norm) / N # DxD\n        # loss\n        c_diff = (c - torch.eye(D, device=device)).pow(2) # DxD\n        # multiply off-diagonal elems of c_diff by lambda\n        c_diff[~torch.eye(D, dtype=bool)] *= self.lambda_param\n        loss = c_diff.sum()\n\n        return loss",
  "def __init__(self, lambda_param=5e-3):\n        \"\"\"Lambda param configuration with default value like in [0]\n\n        Args:\n            lambda_param ([float], optional): parameter for importance of\n                redundancy reduction term.\n            Defaults to 5e-3 [0].\n        \"\"\"\n        super(BarlowTwinsLoss, self).__init__()\n        self.lambda_param = lambda_param",
  "def forward(self, z_a: torch.Tensor, z_b: torch.Tensor):\n\n        device = z_a.device\n\n        # normalize repr. along the batch dimension\n        z_a_norm = (z_a - z_a.mean(0)) / z_a.std(0) # NxD\n        z_b_norm = (z_b - z_b.mean(0)) / z_b.std(0) # NxD\n\n        N = z_a.size(0)\n        D = z_a.size(1)\n\n        # cross-correlation matrix\n        c = torch.mm(z_a_norm.T, z_b_norm) / N # DxD\n        # loss\n        c_diff = (c - torch.eye(D, device=device)).pow(2) # DxD\n        # multiply off-diagonal elems of c_diff by lambda\n        c_diff[~torch.eye(D, dtype=bool)] *= self.lambda_param\n        loss = c_diff.sum()\n\n        return loss",
  "class CO2Regularizer(MemoryBankModule):\n    \"\"\"Implementation of the CO2 regularizer [0] for self-supervised learning.\n\n    [0] CO2, 2021, https://arxiv.org/abs/2010.02217\n\n    Attributes:\n        alpha:\n            Weight of the regularization term.\n        t_consistency:\n            Temperature used during softmax calculations.\n        memory_bank_size:\n            Number of negative samples to store in the memory bank.\n            Use 0 to use the second batch for negative samples.\n\n    Examples:\n        >>> # initialize loss function for MoCo\n        >>> loss_fn = NTXentLoss(memory_bank_size=4096)\n        >>>\n        >>> # initialize CO2 regularizer\n        >>> co2 = CO2Regularizer(alpha=1.0, memory_bank_size=4096)\n        >>>\n        >>> # generate two random trasnforms of images\n        >>> t0 = transforms(images)\n        >>> t1 = transforms(images)\n        >>>\n        >>> # feed through the MoCo model\n        >>> out0, out1 = model(t0, t1)\n        >>> \n        >>> # calculate loss and apply regularizer\n        >>> loss = loss_fn(out0, out1) + co2(out0, out1)\n\n    \"\"\"\n\n    def __init__(self,\n                alpha: float = 1,\n                t_consistency: float = 0.05,\n                memory_bank_size: int = 0):\n\n        super(CO2Regularizer, self).__init__(size=memory_bank_size)\n        # try-catch the KLDivLoss construction for backwards compatability\n        self.log_target = True\n        try:\n            self.kl_div = torch.nn.KLDivLoss(\n                reduction='batchmean',\n                log_target=True\n            )\n        except TypeError:\n            self.log_target = False\n            self.kl_div = torch.nn.KLDivLoss(\n                reduction='batchmean'\n            )\n\n        self.t_consistency = t_consistency\n        self.alpha = alpha\n\n    def _get_pseudo_labels(self,\n                           out0: torch.Tensor,\n                           out1: torch.Tensor,\n                           negatives: torch.Tensor = None):\n        \"\"\"Computes the soft pseudo labels across negative samples.\n\n        Args:\n            out0:\n                Output projections of the first set of transformed images (query).\n                Shape: bsz x n_ftrs\n            out1:\n                Output projections of the second set of transformed images (positive sample).\n                Shape: bsz x n_ftrs\n            negatives:\n                Negative samples to compare against. If this is None, the second\n                batch of images will be used as negative samples.\n                Shape: memory_bank_size x n_ftrs\n\n        Returns:\n            Log probability that a positive samples will classify each negative\n            sample as the positive sample.\n            Shape: bsz x (bsz - 1) or bsz x memory_bank_size\n\n        \"\"\"\n        batch_size, _ = out0.shape\n        if negatives is None:\n            # use second batch as negative samples\n            # l_pos has shape bsz x 1 and l_neg has shape bsz x bsz\n            l_pos = torch.einsum('nc,nc->n', [out0, out1]).unsqueeze(-1)\n            l_neg = torch.einsum('nc,ck->nk', [out0, out1.t()])\n            # remove elements on the diagonal\n            # l_neg has shape bsz x (bsz - 1)\n            l_neg = l_neg.masked_select(\n                ~torch.eye(batch_size, dtype=bool, device=l_neg.device)\n            ).view(batch_size, batch_size - 1)\n        else:\n            # use memory bank as negative samples\n            # l_pos has shape bsz x 1 and l_neg has shape bsz x memory_bank_size\n            negatives = negatives.to(out0.device)\n            l_pos = torch.einsum('nc,nc->n', [out0, out1]).unsqueeze(-1)\n            l_neg = torch.einsum('nc,ck->nk', [out0, negatives.clone().detach()])\n            \n        # concatenate such that positive samples are at index 0\n        logits = torch.cat([l_pos, l_neg], dim=1)\n        # divide by temperature\n        logits = logits / self.t_consistency\n\n        # the input to kl_div is expected to be log(p) \n        return torch.nn.functional.log_softmax(logits, dim=-1)\n\n\n    def forward(self,\n                out0: torch.Tensor,\n                out1: torch.Tensor):\n        \"\"\"Computes the CO2 regularization term for two model outputs.\n\n        Args:\n            out0:\n                Output projections of the first set of transformed images.\n            out1:\n                Output projections of the second set of transformed images.\n\n        Returns:\n            The regularization term multiplied by the weight factor alpha.\n\n        \"\"\"\n\n        # normalize the output to length 1\n        out0 = torch.nn.functional.normalize(out0, dim=1)\n        out1 = torch.nn.functional.normalize(out1, dim=1)\n\n        # ask memory bank for negative samples and extend it with out1 if \n        # out1 requires a gradient, otherwise keep the same vectors in the \n        # memory bank (this allows for keeping the memory bank constant e.g.\n        # for evaluating the loss on the test set)\n        # if the memory_bank size is 0, negatives will be None\n        out1, negatives = \\\n            super(CO2Regularizer, self).forward(out1, update=True)\n        \n        # get log probabilities\n        p = self._get_pseudo_labels(out0, out1, negatives)\n        q = self._get_pseudo_labels(out1, out0, negatives)\n        \n        # calculate symmetrized kullback leibler divergence\n        if self.log_target:\n            div = self.kl_div(p, q) + self.kl_div(q, p)\n        else:\n            # can't use log_target because of early torch version\n            div = self.kl_div(p, torch.exp(q)) + self.kl_div(q, torch.exp(p))\n\n        return self.alpha * 0.5 * div",
  "def __init__(self,\n                alpha: float = 1,\n                t_consistency: float = 0.05,\n                memory_bank_size: int = 0):\n\n        super(CO2Regularizer, self).__init__(size=memory_bank_size)\n        # try-catch the KLDivLoss construction for backwards compatability\n        self.log_target = True\n        try:\n            self.kl_div = torch.nn.KLDivLoss(\n                reduction='batchmean',\n                log_target=True\n            )\n        except TypeError:\n            self.log_target = False\n            self.kl_div = torch.nn.KLDivLoss(\n                reduction='batchmean'\n            )\n\n        self.t_consistency = t_consistency\n        self.alpha = alpha",
  "def _get_pseudo_labels(self,\n                           out0: torch.Tensor,\n                           out1: torch.Tensor,\n                           negatives: torch.Tensor = None):\n        \"\"\"Computes the soft pseudo labels across negative samples.\n\n        Args:\n            out0:\n                Output projections of the first set of transformed images (query).\n                Shape: bsz x n_ftrs\n            out1:\n                Output projections of the second set of transformed images (positive sample).\n                Shape: bsz x n_ftrs\n            negatives:\n                Negative samples to compare against. If this is None, the second\n                batch of images will be used as negative samples.\n                Shape: memory_bank_size x n_ftrs\n\n        Returns:\n            Log probability that a positive samples will classify each negative\n            sample as the positive sample.\n            Shape: bsz x (bsz - 1) or bsz x memory_bank_size\n\n        \"\"\"\n        batch_size, _ = out0.shape\n        if negatives is None:\n            # use second batch as negative samples\n            # l_pos has shape bsz x 1 and l_neg has shape bsz x bsz\n            l_pos = torch.einsum('nc,nc->n', [out0, out1]).unsqueeze(-1)\n            l_neg = torch.einsum('nc,ck->nk', [out0, out1.t()])\n            # remove elements on the diagonal\n            # l_neg has shape bsz x (bsz - 1)\n            l_neg = l_neg.masked_select(\n                ~torch.eye(batch_size, dtype=bool, device=l_neg.device)\n            ).view(batch_size, batch_size - 1)\n        else:\n            # use memory bank as negative samples\n            # l_pos has shape bsz x 1 and l_neg has shape bsz x memory_bank_size\n            negatives = negatives.to(out0.device)\n            l_pos = torch.einsum('nc,nc->n', [out0, out1]).unsqueeze(-1)\n            l_neg = torch.einsum('nc,ck->nk', [out0, negatives.clone().detach()])\n            \n        # concatenate such that positive samples are at index 0\n        logits = torch.cat([l_pos, l_neg], dim=1)\n        # divide by temperature\n        logits = logits / self.t_consistency\n\n        # the input to kl_div is expected to be log(p) \n        return torch.nn.functional.log_softmax(logits, dim=-1)",
  "def forward(self,\n                out0: torch.Tensor,\n                out1: torch.Tensor):\n        \"\"\"Computes the CO2 regularization term for two model outputs.\n\n        Args:\n            out0:\n                Output projections of the first set of transformed images.\n            out1:\n                Output projections of the second set of transformed images.\n\n        Returns:\n            The regularization term multiplied by the weight factor alpha.\n\n        \"\"\"\n\n        # normalize the output to length 1\n        out0 = torch.nn.functional.normalize(out0, dim=1)\n        out1 = torch.nn.functional.normalize(out1, dim=1)\n\n        # ask memory bank for negative samples and extend it with out1 if \n        # out1 requires a gradient, otherwise keep the same vectors in the \n        # memory bank (this allows for keeping the memory bank constant e.g.\n        # for evaluating the loss on the test set)\n        # if the memory_bank size is 0, negatives will be None\n        out1, negatives = \\\n            super(CO2Regularizer, self).forward(out1, update=True)\n        \n        # get log probabilities\n        p = self._get_pseudo_labels(out0, out1, negatives)\n        q = self._get_pseudo_labels(out1, out0, negatives)\n        \n        # calculate symmetrized kullback leibler divergence\n        if self.log_target:\n            div = self.kl_div(p, q) + self.kl_div(q, p)\n        else:\n            # can't use log_target because of early torch version\n            div = self.kl_div(p, torch.exp(q)) + self.kl_div(q, torch.exp(p))\n\n        return self.alpha * 0.5 * div",
  "class _DatasetsMixin:\n\n    def set_dataset_id_by_name(self, dataset_name: str):\n        \"\"\"Sets the dataset id given the name of the dataset\n\n        Args:\n            dataset_name:\n                The name of the dataset for which the dataset_id\n                should be set as attribute\n\n        Raises: ValueError\n\n        \"\"\"\n        current_datasets: List[DatasetData] \\\n            = self.datasets_api.get_datasets()\n\n        try:\n            dataset_with_specified_name = next(dataset for dataset in current_datasets if dataset.name == dataset_name)\n            self._dataset_id = dataset_with_specified_name.id\n        except StopIteration:\n            raise ValueError(f\"A dataset with the name {dataset_name} does not exist on the web platform. \"\n                             f\"Please create it first.\")\n\n    def create_dataset(self, dataset_name: str):\n        \"\"\"Creates a dataset on the webplatform\n\n        If a dataset with that name already exists, instead the dataset_id is set.\n        Args:\n            dataset_name:\n                The name of the dataset to be created.\n\n        \"\"\"\n        try:\n            self.set_dataset_id_by_name(dataset_name)\n        except ValueError:\n            self._create_dataset_without_check_existing(dataset_name=dataset_name)\n\n    def _create_dataset_without_check_existing(self, dataset_name: str):\n        \"\"\"Creates a dataset on the webplatform\n\n        No checking if a dataset with such a name already exists is performed.\n        Args:\n            dataset_name:\n                The name of the dataset to be created.\n\n        \"\"\"\n        body = DatasetCreateRequest(name=dataset_name)\n        response: CreateEntityResponse = self.datasets_api.create_dataset(body=body)\n        self._dataset_id = response.id\n\n    def create_new_dataset_with_unique_name(self, dataset_basename: str):\n        \"\"\"Creates a new dataset on the web platform\n\n        If a dataset with the specified name already exists,\n        a counter is added to the name to be able to still create it.\n        Args:\n            dataset_basename:\n                The name of the dataset to be created.\n\n        \"\"\"\n        current_datasets: List[DatasetData] \\\n            = self.datasets_api.get_datasets()\n        current_datasets_names = [dataset.name for dataset in current_datasets]\n\n        if dataset_basename not in current_datasets_names:\n            self._create_dataset_without_check_existing(dataset_name=dataset_basename)\n            return\n\n        counter = 1\n        dataset_name = f\"{dataset_basename}_{counter}\"\n        while dataset_name in current_datasets_names:\n            counter += 1\n            dataset_name = f\"{dataset_basename}_{counter}\"\n        self._create_dataset_without_check_existing(dataset_name=dataset_name)\n\n    def delete_dataset_by_id(self, dataset_id: str):\n        \"\"\"Deletes a dataset on the web platform\n\n        Args:\n            dataset_id:\n                The id of the dataset to be deleted.\n\n        \"\"\"\n        self.datasets_api.delete_dataset_by_id(dataset_id=dataset_id)\n        del self._dataset_id",
  "def set_dataset_id_by_name(self, dataset_name: str):\n        \"\"\"Sets the dataset id given the name of the dataset\n\n        Args:\n            dataset_name:\n                The name of the dataset for which the dataset_id\n                should be set as attribute\n\n        Raises: ValueError\n\n        \"\"\"\n        current_datasets: List[DatasetData] \\\n            = self.datasets_api.get_datasets()\n\n        try:\n            dataset_with_specified_name = next(dataset for dataset in current_datasets if dataset.name == dataset_name)\n            self._dataset_id = dataset_with_specified_name.id\n        except StopIteration:\n            raise ValueError(f\"A dataset with the name {dataset_name} does not exist on the web platform. \"\n                             f\"Please create it first.\")",
  "def create_dataset(self, dataset_name: str):\n        \"\"\"Creates a dataset on the webplatform\n\n        If a dataset with that name already exists, instead the dataset_id is set.\n        Args:\n            dataset_name:\n                The name of the dataset to be created.\n\n        \"\"\"\n        try:\n            self.set_dataset_id_by_name(dataset_name)\n        except ValueError:\n            self._create_dataset_without_check_existing(dataset_name=dataset_name)",
  "def _create_dataset_without_check_existing(self, dataset_name: str):\n        \"\"\"Creates a dataset on the webplatform\n\n        No checking if a dataset with such a name already exists is performed.\n        Args:\n            dataset_name:\n                The name of the dataset to be created.\n\n        \"\"\"\n        body = DatasetCreateRequest(name=dataset_name)\n        response: CreateEntityResponse = self.datasets_api.create_dataset(body=body)\n        self._dataset_id = response.id",
  "def create_new_dataset_with_unique_name(self, dataset_basename: str):\n        \"\"\"Creates a new dataset on the web platform\n\n        If a dataset with the specified name already exists,\n        a counter is added to the name to be able to still create it.\n        Args:\n            dataset_basename:\n                The name of the dataset to be created.\n\n        \"\"\"\n        current_datasets: List[DatasetData] \\\n            = self.datasets_api.get_datasets()\n        current_datasets_names = [dataset.name for dataset in current_datasets]\n\n        if dataset_basename not in current_datasets_names:\n            self._create_dataset_without_check_existing(dataset_name=dataset_basename)\n            return\n\n        counter = 1\n        dataset_name = f\"{dataset_basename}_{counter}\"\n        while dataset_name in current_datasets_names:\n            counter += 1\n            dataset_name = f\"{dataset_basename}_{counter}\"\n        self._create_dataset_without_check_existing(dataset_name=dataset_name)",
  "def delete_dataset_by_id(self, dataset_id: str):\n        \"\"\"Deletes a dataset on the web platform\n\n        Args:\n            dataset_id:\n                The id of the dataset to be deleted.\n\n        \"\"\"\n        self.datasets_api.delete_dataset_by_id(dataset_id=dataset_id)\n        del self._dataset_id",
  "class _UploadDatasetMixin:\n\n    def upload_dataset(self, input: Union[str, LightlyDataset], max_workers: int = 8,\n                       mode: str = 'thumbnails', verbose: bool = True):\n        \"\"\"Uploads a dataset to to the Lightly cloud solution.\n\n        Args:\n            input:\n                one of the following:\n                    - the path to the dataset, e.g. \"path/to/dataset\"\n                    - the dataset in form of a LightlyDataset\n            max_workers:\n                Maximum number of workers uploading images in parallel.\n            max_requests:\n                Maximum number of requests a single worker can do before he has\n                to wait for the others.\n            mode:\n                One of [full, thumbnails, metadata]. Whether to upload thumbnails,\n                full images, or metadata only.\n\n        Raises:\n            ValueError if dataset is too large or input has the wrong type\n            RuntimeError if the connection to the server failed.\n\n        \"\"\"\n        no_tags_on_server = len(self._get_all_tags())\n        if no_tags_on_server > 0:\n            warnings.warn(f\"Dataset with id {self.dataset_id} has already been completely uploaded to the platform. Skipping upload.\")\n            return\n\n        # Check input variable 'input'\n        if isinstance(input, str):\n            dataset = LightlyDataset(input_dir=input)\n        elif isinstance(input, LightlyDataset):\n            dataset = input\n        else:\n            raise ValueError(f\"input must either be a LightlyDataset or the path to the dataset as str, \"\n                             f\"but is of type {type(input)}\")\n\n        # check the allowed dataset size\n        max_dataset_size_str = self.quota_api.get_quota_maximum_dataset_size()\n        max_dataset_size = int(max_dataset_size_str)\n        if len(dataset) > max_dataset_size:\n            msg = f'Your dataset has {len(dataset)} samples which'\n            msg += f' is more than the allowed maximum of {max_dataset_size}'\n            raise ValueError(msg)\n\n        # handle the case where len(dataset) < max_workers\n        max_workers = min(len(dataset), max_workers)\n\n        # upload the samples\n        if verbose:\n            print(f'Uploading images (with {max_workers} workers).', flush=True)\n\n        pbar = tqdm.tqdm(unit='imgs', total=len(dataset))\n        tqdm_lock = tqdm.tqdm.get_lock()\n\n        # define lambda function for concurrent upload\n        def lambda_(i):\n            # load image\n            image, label, filename = dataset[i]\n            # try to upload image\n            try:\n                self._upload_single_image(\n                    image=image,\n                    label=label,\n                    filename=filename,\n                    mode=mode,\n                )\n                success = True\n            except Exception as e:\n                warnings.warn(f\"Upload of image {filename} failed with error {e}\")\n                success = False\n\n            # update the progress bar\n            tqdm_lock.acquire()  # lock\n            pbar.update(1)  # update\n            tqdm_lock.release()  # unlock\n            # return whether the upload was successful\n            return success\n\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            results = list(executor.map(\n                lambda_, [i for i in range(len(dataset))], chunksize=1))\n\n        if not all(results):\n            msg = 'Warning: Unsuccessful upload(s)! '\n            msg += 'This could cause problems when uploading embeddings.'\n            msg += 'Failed at image: {}'.format(results.index(False))\n            warnings.warn(msg)\n\n        # set image type of data and create initial tag\n        if mode == 'full':\n            img_type = 'full'\n        elif mode == 'thumbnails':\n            img_type = 'thumbnail'\n        else:\n            img_type = 'meta'\n\n        initial_tag_create_request = InitialTagCreateRequest(img_type=img_type, creator=TagCreator.USER_PIP)\n        self.tags_api.create_initial_tag_by_dataset_id(body=initial_tag_create_request, dataset_id=self.dataset_id)\n\n    def _upload_single_image(self, image, label, filename: str, mode):\n        \"\"\"Uploads a single image to the Lightly platform.\n\n        \"\"\"\n\n        # check whether the filename is too long\n        basename = filename\n        if not check_filename(basename):\n            msg = (f'Filename {basename} is longer than the allowed maximum of '\n                   'characters and will be skipped.')\n            warnings.warn(msg)\n            return False\n\n        # calculate metadata, and check if corrupted\n        metadata = check_image(image)\n\n        # generate thumbnail if necessary\n        thumbname = None\n        if not metadata['is_corrupted'] and mode in [\"thumbnails\", \"full\"]:\n            thumbname = '.'.join(basename.split('.')[:-1]) + '_thumb.webp'\n\n        body = SampleCreateRequest(file_name=basename, thumb_name=thumbname, meta_data=metadata)\n        sample_id = retry(\n            self.samples_api.create_sample_by_dataset_id,\n            body=body,\n            dataset_id=self.dataset_id\n        ).id\n\n        if not metadata['is_corrupted'] and mode in [\"thumbnails\", \"full\"]:\n            thumbnail = get_thumbnail_from_img(image)\n            image_to_upload = PIL_to_bytes(thumbnail, ext='webp', quality=90)\n\n            signed_url = retry(\n                self.samples_api.get_sample_image_write_url_by_id,\n                dataset_id=self.dataset_id,\n                sample_id=sample_id,\n                is_thumbnail=True\n            )\n\n            # try to upload thumbnail\n            retry(\n                self.upload_file_with_signed_url,\n                image_to_upload,\n                signed_url\n            )\n\n            thumbnail.close()\n\n        if not metadata['is_corrupted'] and mode == \"full\":\n            image_to_upload = PIL_to_bytes(image)\n\n            signed_url = retry(\n                self.samples_api.get_sample_image_write_url_by_id,\n                dataset_id=self.dataset_id,\n                sample_id=sample_id,\n                is_thumbnail=False\n            )\n\n            # try to upload thumbnail\n            retry(\n                self.upload_file_with_signed_url,\n                image_to_upload,\n                signed_url\n            )\n\n            image.close()",
  "def upload_dataset(self, input: Union[str, LightlyDataset], max_workers: int = 8,\n                       mode: str = 'thumbnails', verbose: bool = True):\n        \"\"\"Uploads a dataset to to the Lightly cloud solution.\n\n        Args:\n            input:\n                one of the following:\n                    - the path to the dataset, e.g. \"path/to/dataset\"\n                    - the dataset in form of a LightlyDataset\n            max_workers:\n                Maximum number of workers uploading images in parallel.\n            max_requests:\n                Maximum number of requests a single worker can do before he has\n                to wait for the others.\n            mode:\n                One of [full, thumbnails, metadata]. Whether to upload thumbnails,\n                full images, or metadata only.\n\n        Raises:\n            ValueError if dataset is too large or input has the wrong type\n            RuntimeError if the connection to the server failed.\n\n        \"\"\"\n        no_tags_on_server = len(self._get_all_tags())\n        if no_tags_on_server > 0:\n            warnings.warn(f\"Dataset with id {self.dataset_id} has already been completely uploaded to the platform. Skipping upload.\")\n            return\n\n        # Check input variable 'input'\n        if isinstance(input, str):\n            dataset = LightlyDataset(input_dir=input)\n        elif isinstance(input, LightlyDataset):\n            dataset = input\n        else:\n            raise ValueError(f\"input must either be a LightlyDataset or the path to the dataset as str, \"\n                             f\"but is of type {type(input)}\")\n\n        # check the allowed dataset size\n        max_dataset_size_str = self.quota_api.get_quota_maximum_dataset_size()\n        max_dataset_size = int(max_dataset_size_str)\n        if len(dataset) > max_dataset_size:\n            msg = f'Your dataset has {len(dataset)} samples which'\n            msg += f' is more than the allowed maximum of {max_dataset_size}'\n            raise ValueError(msg)\n\n        # handle the case where len(dataset) < max_workers\n        max_workers = min(len(dataset), max_workers)\n\n        # upload the samples\n        if verbose:\n            print(f'Uploading images (with {max_workers} workers).', flush=True)\n\n        pbar = tqdm.tqdm(unit='imgs', total=len(dataset))\n        tqdm_lock = tqdm.tqdm.get_lock()\n\n        # define lambda function for concurrent upload\n        def lambda_(i):\n            # load image\n            image, label, filename = dataset[i]\n            # try to upload image\n            try:\n                self._upload_single_image(\n                    image=image,\n                    label=label,\n                    filename=filename,\n                    mode=mode,\n                )\n                success = True\n            except Exception as e:\n                warnings.warn(f\"Upload of image {filename} failed with error {e}\")\n                success = False\n\n            # update the progress bar\n            tqdm_lock.acquire()  # lock\n            pbar.update(1)  # update\n            tqdm_lock.release()  # unlock\n            # return whether the upload was successful\n            return success\n\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            results = list(executor.map(\n                lambda_, [i for i in range(len(dataset))], chunksize=1))\n\n        if not all(results):\n            msg = 'Warning: Unsuccessful upload(s)! '\n            msg += 'This could cause problems when uploading embeddings.'\n            msg += 'Failed at image: {}'.format(results.index(False))\n            warnings.warn(msg)\n\n        # set image type of data and create initial tag\n        if mode == 'full':\n            img_type = 'full'\n        elif mode == 'thumbnails':\n            img_type = 'thumbnail'\n        else:\n            img_type = 'meta'\n\n        initial_tag_create_request = InitialTagCreateRequest(img_type=img_type, creator=TagCreator.USER_PIP)\n        self.tags_api.create_initial_tag_by_dataset_id(body=initial_tag_create_request, dataset_id=self.dataset_id)",
  "def _upload_single_image(self, image, label, filename: str, mode):\n        \"\"\"Uploads a single image to the Lightly platform.\n\n        \"\"\"\n\n        # check whether the filename is too long\n        basename = filename\n        if not check_filename(basename):\n            msg = (f'Filename {basename} is longer than the allowed maximum of '\n                   'characters and will be skipped.')\n            warnings.warn(msg)\n            return False\n\n        # calculate metadata, and check if corrupted\n        metadata = check_image(image)\n\n        # generate thumbnail if necessary\n        thumbname = None\n        if not metadata['is_corrupted'] and mode in [\"thumbnails\", \"full\"]:\n            thumbname = '.'.join(basename.split('.')[:-1]) + '_thumb.webp'\n\n        body = SampleCreateRequest(file_name=basename, thumb_name=thumbname, meta_data=metadata)\n        sample_id = retry(\n            self.samples_api.create_sample_by_dataset_id,\n            body=body,\n            dataset_id=self.dataset_id\n        ).id\n\n        if not metadata['is_corrupted'] and mode in [\"thumbnails\", \"full\"]:\n            thumbnail = get_thumbnail_from_img(image)\n            image_to_upload = PIL_to_bytes(thumbnail, ext='webp', quality=90)\n\n            signed_url = retry(\n                self.samples_api.get_sample_image_write_url_by_id,\n                dataset_id=self.dataset_id,\n                sample_id=sample_id,\n                is_thumbnail=True\n            )\n\n            # try to upload thumbnail\n            retry(\n                self.upload_file_with_signed_url,\n                image_to_upload,\n                signed_url\n            )\n\n            thumbnail.close()\n\n        if not metadata['is_corrupted'] and mode == \"full\":\n            image_to_upload = PIL_to_bytes(image)\n\n            signed_url = retry(\n                self.samples_api.get_sample_image_write_url_by_id,\n                dataset_id=self.dataset_id,\n                sample_id=sample_id,\n                is_thumbnail=False\n            )\n\n            # try to upload thumbnail\n            retry(\n                self.upload_file_with_signed_url,\n                image_to_upload,\n                signed_url\n            )\n\n            image.close()",
  "def lambda_(i):\n            # load image\n            image, label, filename = dataset[i]\n            # try to upload image\n            try:\n                self._upload_single_image(\n                    image=image,\n                    label=label,\n                    filename=filename,\n                    mode=mode,\n                )\n                success = True\n            except Exception as e:\n                warnings.warn(f\"Upload of image {filename} failed with error {e}\")\n                success = False\n\n            # update the progress bar\n            tqdm_lock.acquire()  # lock\n            pbar.update(1)  # update\n            tqdm_lock.release()  # unlock\n            # return whether the upload was successful\n            return success",
  "def retry(func, *args, **kwargs):\n    \"\"\"Repeats a function until it completes successfully or fails too often.\n\n    Args:\n        func:\n            The function call to repeat.\n        args:\n            The arguments which are passed to the function.\n        kwargs:\n            Key-word arguments which are passed to the function.\n\n    Returns:\n        What func returns.\n\n    Exceptions:\n        RuntimeError when number of retries has been exceeded.\n\n    \"\"\"\n\n    # config\n    backoff = 1. + random.random() * 0.1\n    max_backoff = 32\n    max_retries = 5\n\n    # try to make the request\n    for i in range(max_retries):\n        try:\n            # return on success\n            return func(*args, **kwargs)\n        except Exception:\n            # sleep on failure\n            time.sleep(backoff)\n            backoff = 2 * backoff if backoff < max_backoff else backoff\n        \n    # max retries exceeded\n    raise RuntimeError('The connection to the server timed out.')",
  "def getenv(key: str, default: str):\n    \"\"\"Return the value of the environment variable key if it exists,\n       or default if it doesn\u2019t.\n\n    \"\"\"\n    try:\n        return os.getenvb(key.encode(), default.encode()).decode()\n    except Exception:\n        pass\n    try:\n        return os.getenv(key, default)\n    except Exception:\n        pass\n    return default",
  "def PIL_to_bytes(img, ext: str = 'png', quality: int = None):\n    \"\"\"Return the PIL image as byte stream. Useful to send image via requests.\n\n    \"\"\"\n    bytes_io = io.BytesIO()\n    if quality is not None:\n        img.save(bytes_io, format=ext, quality=quality)\n    else:\n        subsampling = -1 if ext.lower() in ['jpg', 'jpeg'] else 0\n        img.save(bytes_io, format=ext, quality=100, subsampling=subsampling)\n    bytes_io.seek(0)\n    return bytes_io",
  "def image_mean(np_img: np.ndarray):\n    \"\"\"Return mean of each channel.\n\n    \"\"\"\n    return np_img.mean(axis=(0, 1))",
  "def image_std(np_img: np.ndarray):\n    \"\"\"Return standard deviation of each channel.\n\n    \"\"\"\n    return np_img.std(axis=(0, 1))",
  "def sum_of_values(np_img: np.ndarray):\n    \"\"\"Return the sum of the pixel values of each channel.\n\n    \"\"\"\n    return np_img.sum(axis=(0, 1))",
  "def sum_of_squares(np_img: np.ndarray):\n    \"\"\"Return the sum of the squared pixel values of each channel.\n\n    \"\"\"\n    return (np_img ** 2).sum(axis=(0, 1))",
  "def signal_to_noise_ratio(img, axis: int = None, ddof: int = 0):\n    \"\"\"Calculate the signal to noise ratio of the image.\n\n    \"\"\"\n    np_img = np.asanyarray(img)\n    mean = np_img.mean(axis=axis)\n    std = np_img.std(axis=axis, ddof=ddof)\n    return np.where(std == 0, 0, mean / std)",
  "def sharpness(img):\n    \"\"\"Calculate the sharpness of the image using a Laplacian Kernel.\n\n    \"\"\"\n    img_bw = img.convert('L')\n    filtered = img_bw.filter(\n        ImageFilter.Kernel(\n            (3, 3),\n            # Laplacian Kernel:\n            (-1, -1, -1, -1, 8, -1, -1, -1, -1),\n            1,\n            0,\n        )\n    )\n    return np.std(filtered)",
  "def size_in_bytes(img):\n    \"\"\"Return the size of the image in bytes.\n\n    \"\"\"\n    img_file = io.BytesIO()\n    img.save(img_file, format='png')\n    return img_file.tell()",
  "def shape(np_img: np.ndarray):\n    \"\"\"Shape of the image as np.ndarray.\n\n    \"\"\"\n    return np_img.shape",
  "def get_meta_from_img(img):\n    \"\"\"Calculates metadata from PIL image.\n\n        - Mean\n        - Standard Deviation\n        - Signal To Noise\n        - Sharpness\n        - Size in Bytes\n        - Shape\n        - Sum of Values\n        - Sum of Squares\n\n    Args:\n        PIL Image.\n\n    Returns:\n        A dictionary containing the metadata of the image.\n    \"\"\"\n\n    np_img = np.array(img) / 255.0\n    metadata = {\n        'mean': image_mean(np_img).tolist(),\n        'std': image_std(np_img).tolist(),\n        'snr': float(signal_to_noise_ratio(img)),\n        'sharpness': float(sharpness(img)),\n        'sizeInBytes': size_in_bytes(img),\n        'shape': list(shape(np_img)),\n        'sumOfValues': sum_of_values(np_img).tolist(),\n        'sumOfSquares': sum_of_squares(np_img).tolist(),\n    }\n    return metadata",
  "def get_thumbnail_from_img(img, size: int = 128):\n    \"\"\"Compute the thumbnail of the image.\n\n    Args:\n        img:\n            PIL Image.\n        size:\n            Size of the thumbnail.\n\n    Returns:\n        Thumbnail as PIL Image.\n\n    \"\"\"\n    thumbnail = img.copy()\n    thumbnail.thumbnail((size, size), Image.LANCZOS)\n    return thumbnail",
  "def resize_image(image, max_width: int, max_height: int):\n    \"\"\"Resize the image if it is too large for the web-app.\n\n    \"\"\"\n    width = image.width\n    height = image.height\n    new_image = image.copy()\n    new_image.format = image.format\n\n    width_factor = max_width / width\n    height_factor = max_height / height\n    factor = min(width_factor, height_factor)\n    new_image.resize(\n        (\n            np.floor(factor * width).astype(int),\n            np.floor(factor * height).astype(int)\n        )\n    )\n    return new_image",
  "def check_filename(basename):\n    \"\"\"Checks the length of the filename.\n\n    Args:\n        basename:\n            Basename of the file.\n\n    \"\"\"\n    return len(basename) <= MAXIMUM_FILENAME_LENGTH",
  "def check_image(image):\n    \"\"\"Checks whether an image is corrupted or not.\n\n    The function reports the metadata, and opens the file to check whether\n    it is corrupt or not.\n\n    Args:\n        image:\n            PIL image from which metadata will be computed.\n\n    Returns:\n        A dictionary of metadata of the image.\n    \"\"\"\n\n    # try to load the image to see whether it's corrupted or not\n    try:\n        image.load()\n        is_corrupted = False\n        corruption = ''\n    except IOError as e:\n        is_corrupted = True\n        corruption = e\n\n    # calculate metadata from image\n    if is_corrupted:\n        metadata = { 'corruption': corruption }\n    else:\n        metadata = get_meta_from_img(image)\n        metadata['corruption'] = ''\n\n    metadata['is_corrupted'] = is_corrupted\n    return metadata",
  "class ApiWorkflowClient(_UploadEmbeddingsMixin, _SamplingMixin, _UploadDatasetMixin, _DownloadDatasetMixin,\n                        _DatasetsMixin):\n    \"\"\"Provides a uniform interface to communicate with the api \n    \n    The APIWorkflowClient is used to communicaate with the Lightly API. The client\n    can run also more complex workflows which include multiple API calls at once.\n    \n    The client can be used in combination with the active learning agent. \n\n    Args:\n        token:\n            the token of the user, provided in webapp\n        dataset_id:\n            the id of the dataset, provided in webapp. \\\n            If it is not set, but used by a workflow, \\\n            the last modfied dataset is taken by default.\n        embedding_id:\n            the id of the embedding to use. If it is not set, \\\n            but used by a workflow, the newest embedding is taken by default\n    \"\"\"\n\n    def __init__(self, token: str, dataset_id: str = None, embedding_id: str = None):\n\n        self.check_version_compatibility()\n\n        configuration = Configuration()\n        configuration.host = getenv('LIGHTLY_SERVER_LOCATION', 'https://api.lightly.ai')\n        configuration.api_key = {'token': token}\n        api_client = ApiClient(configuration=configuration)\n        self.api_client = api_client\n\n        self.token = token\n        if dataset_id is not None:\n            self._dataset_id = dataset_id\n        if embedding_id is not None:\n            self.embedding_id = embedding_id\n\n        self.datasets_api = DatasetsApi(api_client=self.api_client)\n        self.samplings_api = SamplingsApi(api_client=self.api_client)\n        self.jobs_api = JobsApi(api_client=self.api_client)\n        self.tags_api = TagsApi(api_client=self.api_client)\n        self.embeddings_api = EmbeddingsApi(api_client=api_client)\n        self.mappings_api = MappingsApi(api_client=api_client)\n        self.scores_api = ScoresApi(api_client=api_client)\n        self.samples_api = SamplesApi(api_client=api_client)\n        self.quota_api = QuotaApi(api_client=api_client)\n\n    def check_version_compatibility(self):\n        minimum_version = get_minimum_compatible_version()\n        if version_compare(__version__, minimum_version) < 0:\n            raise ValueError(f\"Incompatible Version of lightly pip package. \"\n                             f\"Please upgrade to at least version {minimum_version} \"\n                             f\"to be able to access the api and webapp\")\n\n    @property\n    def dataset_id(self) -> str:\n        ''' Returns the dataset_id\n\n        If the dataset_id is set, it is returned.\n        If it is unset, then the dataset_id of the last modified dataset is taken.\n\n        '''\n        try:\n            return self._dataset_id\n        except AttributeError:\n            all_datasets: List[DatasetData] = self.datasets_api.get_datasets()\n            datasets_sorted = sorted(all_datasets, key=lambda dataset: dataset.last_modified_at)\n            last_modified_dataset = datasets_sorted[-1]\n            self._dataset_id = last_modified_dataset.id\n            warnings.warn(UserWarning(f\"Dataset has not been specified, \"\n                                      f\"taking the last modified dataset {last_modified_dataset.name} as default dataset.\"))\n            return self._dataset_id\n\n    def _get_all_tags(self) -> List[TagData]:\n        return self.tags_api.get_tags_by_dataset_id(self.dataset_id)\n\n    def _order_list_by_filenames(self, filenames_for_list: List[str], list_to_order: List[object]) -> List[object]:\n        \"\"\"Orders a list such that it is in the order of the filenames specified on the server.\n\n        Args:\n            filenames_for_list:\n                The filenames of samples in a specific order\n            list_to_order:\n                Some values belonging to the samples\n\n        Returns:\n            The list reordered. The same reorder applied on the filenames_for_list\n            would put them in the order of the filenames in self.filenames_on_server\n\n        \"\"\"\n        assert len(filenames_for_list) == len(list_to_order)\n        dict_by_filenames = dict(zip(filenames_for_list, list_to_order))\n        list_ordered = [dict_by_filenames[filename] for filename in self.filenames_on_server\n                        if filename in filenames_for_list]\n        return list_ordered\n\n    @property\n    def filenames_on_server(self):\n        if not hasattr(self, \"_filenames_on_server\"):\n            self._filenames_on_server = self.mappings_api. \\\n                get_sample_mappings_by_dataset_id(dataset_id=self.dataset_id, field=\"fileName\")\n        return self._filenames_on_server\n\n    def upload_file_with_signed_url(self, file: IOBase, signed_write_url: str,\n                                    max_backoff: int = 32, max_retries: int = 5) -> Response:\n        \"\"\"Uploads a file to a url via a put request.\n\n        Args:\n            file:\n                The file to upload.\n            signed_write_url:\n                The url to upload the file to. As no authorization is used,\n                the url must be a signed write url.\n            max_backoff:\n                Maximal backoff before retrying.\n            max_retries:\n                Maximum number of retries before timing out.\n\n        Returns:\n            The response of the put request, usually a 200 for the success case.\n\n        \"\"\"\n\n        response = requests.put(signed_write_url, data=file)\n\n        if response.status_code != 200:\n            msg = f'Failed PUT request to {signed_write_url} with status_code'\n            msg += f'{response.status__code}!'\n            raise RuntimeError(msg)\n\n        return response",
  "def __init__(self, token: str, dataset_id: str = None, embedding_id: str = None):\n\n        self.check_version_compatibility()\n\n        configuration = Configuration()\n        configuration.host = getenv('LIGHTLY_SERVER_LOCATION', 'https://api.lightly.ai')\n        configuration.api_key = {'token': token}\n        api_client = ApiClient(configuration=configuration)\n        self.api_client = api_client\n\n        self.token = token\n        if dataset_id is not None:\n            self._dataset_id = dataset_id\n        if embedding_id is not None:\n            self.embedding_id = embedding_id\n\n        self.datasets_api = DatasetsApi(api_client=self.api_client)\n        self.samplings_api = SamplingsApi(api_client=self.api_client)\n        self.jobs_api = JobsApi(api_client=self.api_client)\n        self.tags_api = TagsApi(api_client=self.api_client)\n        self.embeddings_api = EmbeddingsApi(api_client=api_client)\n        self.mappings_api = MappingsApi(api_client=api_client)\n        self.scores_api = ScoresApi(api_client=api_client)\n        self.samples_api = SamplesApi(api_client=api_client)\n        self.quota_api = QuotaApi(api_client=api_client)",
  "def check_version_compatibility(self):\n        minimum_version = get_minimum_compatible_version()\n        if version_compare(__version__, minimum_version) < 0:\n            raise ValueError(f\"Incompatible Version of lightly pip package. \"\n                             f\"Please upgrade to at least version {minimum_version} \"\n                             f\"to be able to access the api and webapp\")",
  "def dataset_id(self) -> str:\n        ''' Returns the dataset_id\n\n        If the dataset_id is set, it is returned.\n        If it is unset, then the dataset_id of the last modified dataset is taken.\n\n        '''\n        try:\n            return self._dataset_id\n        except AttributeError:\n            all_datasets: List[DatasetData] = self.datasets_api.get_datasets()\n            datasets_sorted = sorted(all_datasets, key=lambda dataset: dataset.last_modified_at)\n            last_modified_dataset = datasets_sorted[-1]\n            self._dataset_id = last_modified_dataset.id\n            warnings.warn(UserWarning(f\"Dataset has not been specified, \"\n                                      f\"taking the last modified dataset {last_modified_dataset.name} as default dataset.\"))\n            return self._dataset_id",
  "def _get_all_tags(self) -> List[TagData]:\n        return self.tags_api.get_tags_by_dataset_id(self.dataset_id)",
  "def _order_list_by_filenames(self, filenames_for_list: List[str], list_to_order: List[object]) -> List[object]:\n        \"\"\"Orders a list such that it is in the order of the filenames specified on the server.\n\n        Args:\n            filenames_for_list:\n                The filenames of samples in a specific order\n            list_to_order:\n                Some values belonging to the samples\n\n        Returns:\n            The list reordered. The same reorder applied on the filenames_for_list\n            would put them in the order of the filenames in self.filenames_on_server\n\n        \"\"\"\n        assert len(filenames_for_list) == len(list_to_order)\n        dict_by_filenames = dict(zip(filenames_for_list, list_to_order))\n        list_ordered = [dict_by_filenames[filename] for filename in self.filenames_on_server\n                        if filename in filenames_for_list]\n        return list_ordered",
  "def filenames_on_server(self):\n        if not hasattr(self, \"_filenames_on_server\"):\n            self._filenames_on_server = self.mappings_api. \\\n                get_sample_mappings_by_dataset_id(dataset_id=self.dataset_id, field=\"fileName\")\n        return self._filenames_on_server",
  "def upload_file_with_signed_url(self, file: IOBase, signed_write_url: str,\n                                    max_backoff: int = 32, max_retries: int = 5) -> Response:\n        \"\"\"Uploads a file to a url via a put request.\n\n        Args:\n            file:\n                The file to upload.\n            signed_write_url:\n                The url to upload the file to. As no authorization is used,\n                the url must be a signed write url.\n            max_backoff:\n                Maximal backoff before retrying.\n            max_retries:\n                Maximum number of retries before timing out.\n\n        Returns:\n            The response of the put request, usually a 200 for the success case.\n\n        \"\"\"\n\n        response = requests.put(signed_write_url, data=file)\n\n        if response.status_code != 200:\n            msg = f'Failed PUT request to {signed_write_url} with status_code'\n            msg += f'{response.status__code}!'\n            raise RuntimeError(msg)\n\n        return response",
  "def _hex_to_int(hexstring: str) -> int:\n    \"\"\"Converts a hex string representation of an integer to an integer.\n    \"\"\"\n    return int(hexstring, 16)",
  "def _bin_to_int(binstring: str) -> int:\n    \"\"\"Converts a binary string representation of an integer to an integer.\n    \"\"\"\n    return int(binstring, 2)",
  "def _int_to_hex(x: int) -> str:\n    \"\"\"Converts an integer to a hex string representation.\n    \"\"\"\n    return hex(x)",
  "def _int_to_bin(x: int) -> str:\n    \"\"\"Converts an integer to a binary string representation.\n    \"\"\"\n    return bin(x)",
  "def _get_nonzero_bits(x: int) -> List[int]:\n    \"\"\"Returns a list of indices of nonzero bits in x.\n    \"\"\"\n    offset = 0\n    nonzero_bit_indices = []\n    while x > 0:\n        # if the number is odd, there is a nonzero bit at offset\n        if x % 2 > 0:\n            nonzero_bit_indices.append(offset)\n        # increment the offset and divide the number x by two (rounding down)\n        offset += 1\n        x = x // 2\n    return nonzero_bit_indices",
  "def _invert(x: int) -> int:\n    \"\"\"Flips every bit of x as if x was an unsigned integer.\n    \"\"\"\n    # use XOR of x and 0xFFFFFF to get the inverse\n    # return x ^ (2 ** (x.bit_length()) - 1)\n    # TODO: the solution above can give wrong answers for the case where\n    # the tag representation starts with a zero, therefore it needs to know\n    # the exact number of samples in the dataset to do a correct inverse\n    raise NotImplementedError('This method is not implemented yet...')",
  "def _union(x: int, y: int) -> int:\n    \"\"\"Uses bitwise OR to get the union of the two masks.\n    \"\"\"\n    return x | y",
  "def _intersection(x: int, y: int) -> int:\n    \"\"\"Uses bitwise AND to get the intersection of the two masks.\n    \"\"\"\n    return x & y",
  "def _get_kth_bit(x: int, k: int) -> int:\n    \"\"\"Returns the kth bit in the mask from the right.\n    \"\"\"\n    mask = 1 << k\n    return x & mask",
  "def _set_kth_bit(x: int, k: int) -> int:\n    \"\"\"Sets the kth bit in the mask from the right.\n    \"\"\"\n    mask = 1 << k\n    return x | mask",
  "def _unset_kth_bit(x: int, k: int) -> int:\n    \"\"\"Clears the kth bit in the mask from the right.\n    \"\"\"\n    mask = ~(1 << k)\n    return x & mask",
  "class BitMask:\n    \"\"\"Utility class to represent and manipulate tags.\n    Attributes:\n        x:\n            An integer representation of the binary mask.\n    Examples:\n        >>> # the following are equivalent\n        >>> mask = BitMask(6)\n        >>> mask = BitMask.from_hex('0x6')\n        >>> mask = Bitmask.from_bin('0b0110')\n        >>> # for a dataset with 10 images, assume the following tag\n        >>> # 0001011001 where the 1st, 4th, 5th and 7th image are selected\n        >>> # this tag would be stored as 0x59.\n        >>> hexstring = 0x59                    # what you receive from the api\n        >>> mask = BitMask.from_hex(hexstring)  # create a bitmask from it\n        >>> indices = mask.to_indices()         # get list of indices which are one\n        >>> # indices is [0, 3, 4, 6]\n    \"\"\"\n\n    def __init__(self, x):\n        self.x = x\n\n    @classmethod\n    def from_hex(cls, hexstring: str):\n        \"\"\"Creates a bit mask object from a hexstring.\n        \"\"\"\n        return cls(_hex_to_int(hexstring))\n\n    @classmethod\n    def from_bin(cls, binstring: str):\n        \"\"\"Creates a BitMask from a binary string.\n        \"\"\"\n        return cls(_bin_to_int(binstring))\n\n    def to_hex(self):\n        \"\"\"Creates a BitMask from a hex string.\n        \"\"\"\n        return _int_to_hex(self.x)\n\n    def to_bin(self):\n        \"\"\"Returns a binary string representing the bit mask.\n        \"\"\"\n        return _int_to_bin(self.x)\n\n    def to_indices(self) -> List[int]:\n        \"\"\"Returns the list of indices bits which are set to 1 from the right.\n        Examples:\n            >>> mask = BitMask('0b0101')\n            >>> indices = mask.to_indices()\n            >>> # indices is [0, 2]\n        \"\"\"\n        return _get_nonzero_bits(self.x)\n\n    def invert(self):\n        \"\"\"Sets every 0 to 1 and every 1 to 0 in the bitstring.\n\n        \"\"\"\n        self.x = _invert(self.x)\n\n    def complement(self):\n        \"\"\"Same as invert but with the appropriate name.\n        \"\"\"\n        self.invert()\n\n    def union(self, other):\n        \"\"\"Calculates the union of two bit masks.\n        Examples:\n            >>> mask1 = BitMask.from_bin('0b0011')\n            >>> mask2 = BitMask.from_bin('0b1100')\n            >>> mask1.union(mask2)\n            >>> # mask1.binstring is '0b1111'\n        \"\"\"\n        self.x = _union(self.x, other.x)\n\n    def intersection(self, other):\n        \"\"\"Calculates the intersection of two bit masks.\n        Examples:\n            >>> mask1 = BitMask.from_bin('0b0011')\n            >>> mask2 = BitMask.from_bin('0b1100')\n            >>> mask1.intersection(mask2)\n            >>> # mask1.binstring is '0b0000'\n        \"\"\"\n        self.x = _intersection(self.x, other.x)\n\n    def get_kth_bit(self, k: int) -> bool:\n        \"\"\"Returns the boolean value of the kth bit from the right.\n        \"\"\"\n        return _get_kth_bit(self.x, k) > 0\n\n    def set_kth_bit(self, k: int):\n        \"\"\"Sets the kth bit from the right to '1'.\n        Examples:\n            >>> mask = BitMask('0b0000')\n            >>> mask.set_kth_bit(2)\n            >>> # mask.binstring is '0b0100'\n        \"\"\"\n        self.x = _set_kth_bit(self.x, k)\n\n    def unset_kth_bit(self, k: int):\n        \"\"\"Unsets the kth bit from the right to '0'.\n        Examples:\n            >>> mask = BitMask('0b1111')\n            >>> mask.unset_kth_bit(2)\n            >>> # mask.binstring is '0b1011'\n        \"\"\"\n        self.x = _unset_kth_bit(self.x, k)",
  "def __init__(self, x):\n        self.x = x",
  "def from_hex(cls, hexstring: str):\n        \"\"\"Creates a bit mask object from a hexstring.\n        \"\"\"\n        return cls(_hex_to_int(hexstring))",
  "def from_bin(cls, binstring: str):\n        \"\"\"Creates a BitMask from a binary string.\n        \"\"\"\n        return cls(_bin_to_int(binstring))",
  "def to_hex(self):\n        \"\"\"Creates a BitMask from a hex string.\n        \"\"\"\n        return _int_to_hex(self.x)",
  "def to_bin(self):\n        \"\"\"Returns a binary string representing the bit mask.\n        \"\"\"\n        return _int_to_bin(self.x)",
  "def to_indices(self) -> List[int]:\n        \"\"\"Returns the list of indices bits which are set to 1 from the right.\n        Examples:\n            >>> mask = BitMask('0b0101')\n            >>> indices = mask.to_indices()\n            >>> # indices is [0, 2]\n        \"\"\"\n        return _get_nonzero_bits(self.x)",
  "def invert(self):\n        \"\"\"Sets every 0 to 1 and every 1 to 0 in the bitstring.\n\n        \"\"\"\n        self.x = _invert(self.x)",
  "def complement(self):\n        \"\"\"Same as invert but with the appropriate name.\n        \"\"\"\n        self.invert()",
  "def union(self, other):\n        \"\"\"Calculates the union of two bit masks.\n        Examples:\n            >>> mask1 = BitMask.from_bin('0b0011')\n            >>> mask2 = BitMask.from_bin('0b1100')\n            >>> mask1.union(mask2)\n            >>> # mask1.binstring is '0b1111'\n        \"\"\"\n        self.x = _union(self.x, other.x)",
  "def intersection(self, other):\n        \"\"\"Calculates the intersection of two bit masks.\n        Examples:\n            >>> mask1 = BitMask.from_bin('0b0011')\n            >>> mask2 = BitMask.from_bin('0b1100')\n            >>> mask1.intersection(mask2)\n            >>> # mask1.binstring is '0b0000'\n        \"\"\"\n        self.x = _intersection(self.x, other.x)",
  "def get_kth_bit(self, k: int) -> bool:\n        \"\"\"Returns the boolean value of the kth bit from the right.\n        \"\"\"\n        return _get_kth_bit(self.x, k) > 0",
  "def set_kth_bit(self, k: int):\n        \"\"\"Sets the kth bit from the right to '1'.\n        Examples:\n            >>> mask = BitMask('0b0000')\n            >>> mask.set_kth_bit(2)\n            >>> # mask.binstring is '0b0100'\n        \"\"\"\n        self.x = _set_kth_bit(self.x, k)",
  "def unset_kth_bit(self, k: int):\n        \"\"\"Unsets the kth bit from the right to '0'.\n        Examples:\n            >>> mask = BitMask('0b1111')\n            >>> mask.unset_kth_bit(2)\n            >>> # mask.binstring is '0b1011'\n        \"\"\"\n        self.x = _unset_kth_bit(self.x, k)",
  "class _UploadEmbeddingsMixin:\n\n    def set_embedding_id_by_name(self, embedding_name: str = None):\n        embeddings: List[DatasetEmbeddingData] = \\\n            self.embeddings_api.get_embeddings_by_dataset_id(dataset_id=self.dataset_id)\n\n        if embedding_name is None:\n            self.embedding_id = embeddings[-1].id\n            return\n\n        try:\n            self.embedding_id = next(embedding.id for embedding in embeddings if embedding.name == embedding_name)\n        except StopIteration:\n            raise ValueError(f\"No embedding with name {embedding_name} found on the server.\")\n\n    def upload_embeddings(self, path_to_embeddings_csv: str, name: str):\n        \"\"\"Uploads embeddings to the server.\n\n        First checks that the specified embedding name is not on ther server. If it is, the upload is aborted.\n        Then creates a new csv with the embeddings in the order specified on the server. Next it uploads it to the server.\n        The received embedding_id is saved as a property of self.\n        Args:\n            path_to_embeddings_csv: the filepath to the .csv containing the embeddings, e.g. \"path/to/embeddings.csv\"\n            name: The name of the embedding. If an embedding with such a name already exists on the server,\n                the upload is aborted.\n\n        Returns:\n            None\n\n        \"\"\"\n        # get the names of the current embeddings on the server:\n        embeddings_on_server: List[DatasetEmbeddingData] = \\\n            self.embeddings_api.get_embeddings_by_dataset_id(dataset_id=self.dataset_id)\n        names_embeddings_on_server = [embedding.name for embedding in embeddings_on_server]\n        if name in names_embeddings_on_server:\n            print(f\"Aborting upload, embedding with name='{name}' already exists.\")\n            self.embedding_id = next(embedding for embedding in embeddings_on_server if embedding.name == name).id\n            return\n\n        # create a new csv with the filenames in the desired order\n        path_to_ordered_embeddings_csv = self._order_csv_by_filenames(\n            path_to_embeddings_csv=path_to_embeddings_csv)\n\n        # get the URL to upload the csv to\n        response: WriteCSVUrlData = \\\n            self.embeddings_api.get_embeddings_csv_write_url_by_id(self.dataset_id, name=name)\n        self.embedding_id = response.embedding_id\n        signed_write_url = response.signed_write_url\n\n        # upload the csv to the URL\n        with open(path_to_ordered_embeddings_csv, 'rb') as file_ordered_embeddings_csv:\n            self.upload_file_with_signed_url(file=file_ordered_embeddings_csv, signed_write_url=signed_write_url)\n\n    def _order_csv_by_filenames(self, path_to_embeddings_csv: str) -> str:\n        \"\"\"Orders the rows in a csv according to the order specified on the server and saves it as a new file.\n\n        Args:\n            path_to_embeddings_csv:\n                the path to the csv to order\n\n        Returns:\n            the filepath to the new csv\n\n        \"\"\"\n        with open(path_to_embeddings_csv, 'r') as f:\n            data = csv.reader(f)\n\n            rows = list(data)\n            header_row = rows[0]\n            rows_without_header = rows[1:]\n            index_filenames = header_row.index('filenames')\n            filenames = [row[index_filenames] for row in rows_without_header]\n\n            if len(filenames) != len(self.filenames_on_server):\n                raise ValueError(f\"There are {len(filenames)} rows in the embedding file, but \"\n                                 f\"{len(self.filenames_on_server)} filenames/samples on the server.\")\n            if set(filenames) != set(self.filenames_on_server):\n                raise ValueError(f\"The filenames in the embedding file and the filenames on the server do not align\")\n\n            rows_without_header_ordered = self._order_list_by_filenames(filenames, rows_without_header)\n\n            rows_to_write = [header_row]\n            rows_to_write += rows_without_header_ordered\n\n        path_to_ordered_embeddings_csv = path_to_embeddings_csv.replace('.csv', '_sorted.csv')\n        with open(path_to_ordered_embeddings_csv, 'w') as f:\n            writer = csv.writer(f)\n            writer.writerows(rows_to_write)\n\n        return path_to_ordered_embeddings_csv",
  "def set_embedding_id_by_name(self, embedding_name: str = None):\n        embeddings: List[DatasetEmbeddingData] = \\\n            self.embeddings_api.get_embeddings_by_dataset_id(dataset_id=self.dataset_id)\n\n        if embedding_name is None:\n            self.embedding_id = embeddings[-1].id\n            return\n\n        try:\n            self.embedding_id = next(embedding.id for embedding in embeddings if embedding.name == embedding_name)\n        except StopIteration:\n            raise ValueError(f\"No embedding with name {embedding_name} found on the server.\")",
  "def upload_embeddings(self, path_to_embeddings_csv: str, name: str):\n        \"\"\"Uploads embeddings to the server.\n\n        First checks that the specified embedding name is not on ther server. If it is, the upload is aborted.\n        Then creates a new csv with the embeddings in the order specified on the server. Next it uploads it to the server.\n        The received embedding_id is saved as a property of self.\n        Args:\n            path_to_embeddings_csv: the filepath to the .csv containing the embeddings, e.g. \"path/to/embeddings.csv\"\n            name: The name of the embedding. If an embedding with such a name already exists on the server,\n                the upload is aborted.\n\n        Returns:\n            None\n\n        \"\"\"\n        # get the names of the current embeddings on the server:\n        embeddings_on_server: List[DatasetEmbeddingData] = \\\n            self.embeddings_api.get_embeddings_by_dataset_id(dataset_id=self.dataset_id)\n        names_embeddings_on_server = [embedding.name for embedding in embeddings_on_server]\n        if name in names_embeddings_on_server:\n            print(f\"Aborting upload, embedding with name='{name}' already exists.\")\n            self.embedding_id = next(embedding for embedding in embeddings_on_server if embedding.name == name).id\n            return\n\n        # create a new csv with the filenames in the desired order\n        path_to_ordered_embeddings_csv = self._order_csv_by_filenames(\n            path_to_embeddings_csv=path_to_embeddings_csv)\n\n        # get the URL to upload the csv to\n        response: WriteCSVUrlData = \\\n            self.embeddings_api.get_embeddings_csv_write_url_by_id(self.dataset_id, name=name)\n        self.embedding_id = response.embedding_id\n        signed_write_url = response.signed_write_url\n\n        # upload the csv to the URL\n        with open(path_to_ordered_embeddings_csv, 'rb') as file_ordered_embeddings_csv:\n            self.upload_file_with_signed_url(file=file_ordered_embeddings_csv, signed_write_url=signed_write_url)",
  "def _order_csv_by_filenames(self, path_to_embeddings_csv: str) -> str:\n        \"\"\"Orders the rows in a csv according to the order specified on the server and saves it as a new file.\n\n        Args:\n            path_to_embeddings_csv:\n                the path to the csv to order\n\n        Returns:\n            the filepath to the new csv\n\n        \"\"\"\n        with open(path_to_embeddings_csv, 'r') as f:\n            data = csv.reader(f)\n\n            rows = list(data)\n            header_row = rows[0]\n            rows_without_header = rows[1:]\n            index_filenames = header_row.index('filenames')\n            filenames = [row[index_filenames] for row in rows_without_header]\n\n            if len(filenames) != len(self.filenames_on_server):\n                raise ValueError(f\"There are {len(filenames)} rows in the embedding file, but \"\n                                 f\"{len(self.filenames_on_server)} filenames/samples on the server.\")\n            if set(filenames) != set(self.filenames_on_server):\n                raise ValueError(f\"The filenames in the embedding file and the filenames on the server do not align\")\n\n            rows_without_header_ordered = self._order_list_by_filenames(filenames, rows_without_header)\n\n            rows_to_write = [header_row]\n            rows_to_write += rows_without_header_ordered\n\n        path_to_ordered_embeddings_csv = path_to_embeddings_csv.replace('.csv', '_sorted.csv')\n        with open(path_to_ordered_embeddings_csv, 'w') as f:\n            writer = csv.writer(f)\n            writer.writerows(rows_to_write)\n\n        return path_to_ordered_embeddings_csv",
  "def get_versioning_api() -> VersioningApi:\n    configuration = Configuration()\n    configuration.host = getenv('LIGHTLY_SERVER_LOCATION', 'https://api.lightly.ai')\n    api_client = ApiClient(configuration=configuration)\n    versioning_api = VersioningApi(api_client)\n    return versioning_api",
  "def get_latest_version(current_version: str) -> Tuple[None, str]:\n    try:\n        versioning_api = get_versioning_api()\n        version_number: str = versioning_api.get_latest_pip_version(current_version = current_version)\n        return version_number\n    except Exception as e:\n        return None",
  "def get_minimum_compatible_version():\n    versioning_api = get_versioning_api()\n    version_number: str = versioning_api.get_minimum_compatible_pip_version()\n    return version_number",
  "def version_compare(v0, v1):\n    v0 = [int(n) for n in v0.split('.')][::-1]\n    v1 = [int(n) for n in v1.split('.')][::-1]\n    assert len(v0) == 3\n    assert len(v1) == 3\n    pairs = list(zip(v0, v1))[::-1]\n    for x, y in pairs:\n        if x < y:\n            return -1\n        if x > y:\n            return 1\n    return 0",
  "def pretty_print_latest_version(latest_version, width=70):\n    warning = f\"There is a newer version of the package available. \" \\\n              f\"For compatability reasons, please upgrade your current version:\" \\\n              f\"pip install lightly=={latest_version}\"\n    warnings.warn(Warning(warning))",
  "def _make_dir_and_save_image(output_dir: str, filename: str, img: Image):\n    \"\"\"Saves the images and creates necessary subdirectories.\n\n    \"\"\"\n    path = os.path.join(output_dir, filename)\n\n    head = os.path.split(path)[0]\n    if not os.path.exists(head):\n        os.makedirs(head)\n\n    img.save(path)\n    img.close()",
  "def _get_image_from_read_url(read_url: str):\n    \"\"\"Makes a get request to the signed read url and returns the image.\n\n    \"\"\"\n    request = Request(read_url, method='GET')\n    with urlopen(request) as response:\n        blob = response.read()\n        img = Image.open(io.BytesIO(blob))\n    return img",
  "class _DownloadDatasetMixin:\n\n    def download_dataset(self,\n                         output_dir: str,\n                         tag_name: str = 'initial-tag',\n                         verbose: bool = True):\n        \"\"\"Downloads images from the web-app and stores them in output_dir.\n\n        Args:\n            output_dir:\n                Where to store the downloaded images.\n            tag_name:\n                Name of the tag which should be downloaded.\n            verbose:\n                Whether or not to show the progress bar.\n\n        Raises:\n            ValueError if the specified tag does not exist on the dataset.\n            RuntimeError if the connection to the server failed.\n\n        \"\"\"\n\n        # check if images are available\n        dataset = self.datasets_api.get_dataset_by_id(self.dataset_id)\n        if dataset.img_type != ImageType.FULL:\n            # only thumbnails or metadata available\n            raise ValueError(\n                f\"Dataset with id {self.dataset_id} has no downloadable images!\"\n            )\n\n        # check if tag exists\n        available_tags = self._get_all_tags()\n        try:\n            tag = next(tag for tag in available_tags if tag.name == tag_name)\n        except StopIteration:\n            raise ValueError(\n                f\"Dataset with id {self.dataset_id} has no tag {tag_name}!\"\n            )\n\n        # get sample ids\n        sample_ids = self.mappings_api.get_sample_mappings_by_dataset_id(\n            self.dataset_id,\n            field='_id'\n        )\n\n        indices = BitMask.from_hex(tag.bit_mask_data).to_indices()\n        sample_ids = [sample_ids[i] for i in indices]\n        filenames = [self.filenames_on_server[i] for i in indices]\n\n        if verbose:\n            print(f'Downloading {len(sample_ids)} images:', flush=True)\n            pbar = tqdm.tqdm(unit='imgs', total=len(sample_ids))\n\n        # download images\n        for sample_id, filename in zip(sample_ids, filenames):\n            read_url = self.samples_api.get_sample_image_read_url_by_id(\n                self.dataset_id, \n                sample_id,\n                type=\"full\",\n            )\n\n            img = _get_image_from_read_url(read_url)\n            _make_dir_and_save_image(output_dir, filename, img)\n\n            if verbose:\n                pbar.update(1)",
  "def download_dataset(self,\n                         output_dir: str,\n                         tag_name: str = 'initial-tag',\n                         verbose: bool = True):\n        \"\"\"Downloads images from the web-app and stores them in output_dir.\n\n        Args:\n            output_dir:\n                Where to store the downloaded images.\n            tag_name:\n                Name of the tag which should be downloaded.\n            verbose:\n                Whether or not to show the progress bar.\n\n        Raises:\n            ValueError if the specified tag does not exist on the dataset.\n            RuntimeError if the connection to the server failed.\n\n        \"\"\"\n\n        # check if images are available\n        dataset = self.datasets_api.get_dataset_by_id(self.dataset_id)\n        if dataset.img_type != ImageType.FULL:\n            # only thumbnails or metadata available\n            raise ValueError(\n                f\"Dataset with id {self.dataset_id} has no downloadable images!\"\n            )\n\n        # check if tag exists\n        available_tags = self._get_all_tags()\n        try:\n            tag = next(tag for tag in available_tags if tag.name == tag_name)\n        except StopIteration:\n            raise ValueError(\n                f\"Dataset with id {self.dataset_id} has no tag {tag_name}!\"\n            )\n\n        # get sample ids\n        sample_ids = self.mappings_api.get_sample_mappings_by_dataset_id(\n            self.dataset_id,\n            field='_id'\n        )\n\n        indices = BitMask.from_hex(tag.bit_mask_data).to_indices()\n        sample_ids = [sample_ids[i] for i in indices]\n        filenames = [self.filenames_on_server[i] for i in indices]\n\n        if verbose:\n            print(f'Downloading {len(sample_ids)} images:', flush=True)\n            pbar = tqdm.tqdm(unit='imgs', total=len(sample_ids))\n\n        # download images\n        for sample_id, filename in zip(sample_ids, filenames):\n            read_url = self.samples_api.get_sample_image_read_url_by_id(\n                self.dataset_id, \n                sample_id,\n                type=\"full\",\n            )\n\n            img = _get_image_from_read_url(read_url)\n            _make_dir_and_save_image(output_dir, filename, img)\n\n            if verbose:\n                pbar.update(1)",
  "class _SamplingMixin:\n\n    def sampling(self, sampler_config: SamplerConfig, al_scores: Dict[str, List[np.ndarray]] = None,\n                 preselected_tag_id: str = None, query_tag_id: str = None) -> TagData:\n        \"\"\"Performs a sampling given the arguments.\n\n        Args:\n            sampler_config:\n                The configuration of the sampler.\n            al_scores:\n                The active learning scores for the sampler.\n            preselected_tag_id:\n                The tag defining the already chosen samples (e.g. already labelled ones), default: None.\n            query_tag_id:\n                The tag defining where to sample from, default: None resolves to the initial-tag.\n\n        Returns:\n            The newly created tag of the sampling.\n\n        Raises:\n            ApiException, ValueError, RuntimeError\n\n        \"\"\"\n\n        # make sure the tag name does not exist yet\n        tags = self._get_all_tags()\n        if sampler_config.name in [tag.name for tag in tags]:\n            raise RuntimeError(f\"There already exists a tag with tag_name {sampler_config.name}.\")\n\n        # make sure we have an embedding id\n        try:\n            self.embedding_id\n        except AttributeError:\n            self.set_embedding_id_by_name()\n\n\n        # upload the active learning scores to the api\n        if al_scores is not None:\n            if preselected_tag_id is None:\n                raise ValueError\n            for score_type, score_values in al_scores.items():\n                if isinstance(score_values, np.ndarray):\n                    score_values = score_values.astype(np.float64)\n                body = ActiveLearningScoreCreateRequest(score_type=score_type, scores=list(score_values))\n                self.scores_api.create_or_update_active_learning_score_by_tag_id(\n                    body, dataset_id=self.dataset_id, tag_id=preselected_tag_id)\n\n        # trigger the sampling\n        payload = self._create_sampling_create_request(sampler_config, preselected_tag_id, query_tag_id)\n        payload.row_count = self._get_all_tags()[0].tot_size\n        response = self.samplings_api.trigger_sampling_by_id(payload, self.dataset_id, self.embedding_id)\n        job_id = response.job_id\n\n        # poll the job status till the job is not running anymore\n        exception_counter = 0  # TODO; remove after solving https://github.com/lightly-ai/lightly-core/issues/156\n        job_status_data = None\n\n        wait_time_till_next_poll = getattr(self, \"wait_time_till_next_poll\", 1)\n        while job_status_data is None \\\n            or job_status_data.status == JobState.RUNNING \\\n                or job_status_data.status == JobState.WAITING \\\n                    or job_status_data.status == JobState.UNKNOWN:\n            # sleep before polling again\n            time.sleep(wait_time_till_next_poll)\n            # try to read the sleep time until the next poll from the status data\n            try:\n                job_status_data: JobStatusData = self.jobs_api.get_job_status_by_id(job_id=job_id)\n                wait_time_till_next_poll = job_status_data.wait_time_till_next_poll\n            except Exception as err:\n                exception_counter += 1\n                if exception_counter == 20:\n                    print(f\"Sampling job with job_id {job_id} could not be started because of error: {err}\")\n                    raise err\n\n        if job_status_data.status == JobState.FAILED:\n            raise RuntimeError(f\"Sampling job with job_id {job_id} failed with error {job_status_data.error}\")\n\n        # get the new tag from the job status\n        new_tag_id = job_status_data.result.data\n        if new_tag_id is None:\n            raise RuntimeError(f\"TagId returned by job with job_id {job_id} is None.\")\n        new_tag_data = self.tags_api.get_tag_by_tag_id(self.dataset_id, tag_id=new_tag_id)\n\n        return new_tag_data\n\n    def _create_sampling_create_request(self, sampler_config: SamplerConfig, preselected_tag_id: str, query_tag_id: str\n                                        ) -> SamplingCreateRequest:\n        \"\"\"Creates a SamplingCreateRequest\n\n        First, it checks how many samples are already labeled by\n            getting the number of samples in the preselected_tag_id.\n        Then the stopping_condition.n_samples\n            is set to be the number of already labeled samples + the sampler_config.batch_size.\n        Last the SamplingCreateRequest is created with the necessary nested class instances.\n\n        \"\"\"\n\n        sampling_config = SamplingConfig(\n            stopping_condition=SamplingConfigStoppingCondition(\n                n_samples=sampler_config.n_samples,\n                min_distance=sampler_config.min_distance\n            )\n        )\n        sampling_create_request = SamplingCreateRequest(new_tag_name=sampler_config.name,\n                                                        method=sampler_config.method,\n                                                        config=sampling_config,\n                                                        preselected_tag_id=preselected_tag_id,\n                                                        query_tag_id=query_tag_id)\n        return sampling_create_request",
  "def sampling(self, sampler_config: SamplerConfig, al_scores: Dict[str, List[np.ndarray]] = None,\n                 preselected_tag_id: str = None, query_tag_id: str = None) -> TagData:\n        \"\"\"Performs a sampling given the arguments.\n\n        Args:\n            sampler_config:\n                The configuration of the sampler.\n            al_scores:\n                The active learning scores for the sampler.\n            preselected_tag_id:\n                The tag defining the already chosen samples (e.g. already labelled ones), default: None.\n            query_tag_id:\n                The tag defining where to sample from, default: None resolves to the initial-tag.\n\n        Returns:\n            The newly created tag of the sampling.\n\n        Raises:\n            ApiException, ValueError, RuntimeError\n\n        \"\"\"\n\n        # make sure the tag name does not exist yet\n        tags = self._get_all_tags()\n        if sampler_config.name in [tag.name for tag in tags]:\n            raise RuntimeError(f\"There already exists a tag with tag_name {sampler_config.name}.\")\n\n        # make sure we have an embedding id\n        try:\n            self.embedding_id\n        except AttributeError:\n            self.set_embedding_id_by_name()\n\n\n        # upload the active learning scores to the api\n        if al_scores is not None:\n            if preselected_tag_id is None:\n                raise ValueError\n            for score_type, score_values in al_scores.items():\n                if isinstance(score_values, np.ndarray):\n                    score_values = score_values.astype(np.float64)\n                body = ActiveLearningScoreCreateRequest(score_type=score_type, scores=list(score_values))\n                self.scores_api.create_or_update_active_learning_score_by_tag_id(\n                    body, dataset_id=self.dataset_id, tag_id=preselected_tag_id)\n\n        # trigger the sampling\n        payload = self._create_sampling_create_request(sampler_config, preselected_tag_id, query_tag_id)\n        payload.row_count = self._get_all_tags()[0].tot_size\n        response = self.samplings_api.trigger_sampling_by_id(payload, self.dataset_id, self.embedding_id)\n        job_id = response.job_id\n\n        # poll the job status till the job is not running anymore\n        exception_counter = 0  # TODO; remove after solving https://github.com/lightly-ai/lightly-core/issues/156\n        job_status_data = None\n\n        wait_time_till_next_poll = getattr(self, \"wait_time_till_next_poll\", 1)\n        while job_status_data is None \\\n            or job_status_data.status == JobState.RUNNING \\\n                or job_status_data.status == JobState.WAITING \\\n                    or job_status_data.status == JobState.UNKNOWN:\n            # sleep before polling again\n            time.sleep(wait_time_till_next_poll)\n            # try to read the sleep time until the next poll from the status data\n            try:\n                job_status_data: JobStatusData = self.jobs_api.get_job_status_by_id(job_id=job_id)\n                wait_time_till_next_poll = job_status_data.wait_time_till_next_poll\n            except Exception as err:\n                exception_counter += 1\n                if exception_counter == 20:\n                    print(f\"Sampling job with job_id {job_id} could not be started because of error: {err}\")\n                    raise err\n\n        if job_status_data.status == JobState.FAILED:\n            raise RuntimeError(f\"Sampling job with job_id {job_id} failed with error {job_status_data.error}\")\n\n        # get the new tag from the job status\n        new_tag_id = job_status_data.result.data\n        if new_tag_id is None:\n            raise RuntimeError(f\"TagId returned by job with job_id {job_id} is None.\")\n        new_tag_data = self.tags_api.get_tag_by_tag_id(self.dataset_id, tag_id=new_tag_id)\n\n        return new_tag_data",
  "def _create_sampling_create_request(self, sampler_config: SamplerConfig, preselected_tag_id: str, query_tag_id: str\n                                        ) -> SamplingCreateRequest:\n        \"\"\"Creates a SamplingCreateRequest\n\n        First, it checks how many samples are already labeled by\n            getting the number of samples in the preselected_tag_id.\n        Then the stopping_condition.n_samples\n            is set to be the number of already labeled samples + the sampler_config.batch_size.\n        Last the SamplingCreateRequest is created with the necessary nested class instances.\n\n        \"\"\"\n\n        sampling_config = SamplingConfig(\n            stopping_condition=SamplingConfigStoppingCondition(\n                n_samples=sampler_config.n_samples,\n                min_distance=sampler_config.min_distance\n            )\n        )\n        sampling_create_request = SamplingCreateRequest(new_tag_name=sampler_config.name,\n                                                        method=sampler_config.method,\n                                                        config=sampling_config,\n                                                        preselected_tag_id=preselected_tag_id,\n                                                        query_tag_id=query_tag_id)\n        return sampling_create_request",
  "def _prefix(*args, **kwargs):\n    \"\"\"Returns the prefix for the users routes.\n\n    All routes through users require authentication via jwt.\n\n    \"\"\"\n    server_location = getenv(\n        'LIGHTLY_SERVER_LOCATION',\n        'https://api.lightly.ai'\n    )\n    return server_location + '/users/docker'",
  "def get_soft_authorization(token: str):\n    \"\"\"Makes a call to the api to request authorization to run the container.\n\n    Args:\n        token:\n            User access token.\n\n    Returns:\n        The server's response and the response status.\n\n    \"\"\"\n    dst_url = _prefix() + '/soft_authorization'\n    payload = {\n        'token': token,\n    }\n\n    response = requests.get(dst_url, params=payload)\n    status = response.status_code\n    return response.json(), status",
  "def get_authorization(token: str,\n                      timestamp: str,\n                      task_description: dict):\n    \"\"\"Makes a call to the api to request authorization to run the container.\n\n    Args:\n        token:\n            User access token.\n        timestamp:\n            Access request timestamp.\n        task_description:\n            A dictionary describing the task.\n\n    Returns:\n        The body of the request, the server's response and the response status.\n\n    \"\"\"\n    dst_url = _prefix() + '/authorization'\n    payload = {\n        'token': token,\n    }\n    json = {\n        'token': token,\n        'timestamp': timestamp,\n        'task_description': task_description,\n    }\n\n    response = requests.post(dst_url, params=payload, json=json)\n    status = response.status_code\n    return json, response.json(), status",
  "def post_diagnostics(token: str,\n                     run_id: str,\n                     action: str,\n                     data: dict,\n                     timestamp: str):\n    \"\"\"Make a call to the api to add a diagnostics entry\n\n    Args:\n        token:\n            User access token.\n        run_id:\n            Unique id of the docker run.\n        action:\n            Description of the diagnostics.\n        data:\n            Relevant extra data.\n        timestamp:\n            Timestamp of the diagnostics.\n\n    Returns:\n        True if the post was successful, false otherwise.\n\n    \"\"\"\n    dst_url = _prefix()\n    payload = {\n        'token': token,\n        'runId': run_id,\n        'action': action,\n        'data': data,\n        'timestamp': timestamp,\n    }\n\n    try:\n        response = requests.post(\n            dst_url,\n            json=payload,\n        )\n    except Exception:\n        return False\n    else:\n        return (response.status_code == 200)",
  "class Scorer():\n    def __init__(self, model_output):\n        self.model_output = model_output\n\n    def _calculate_scores(self) -> Dict[str, np.ndarray]:\n        raise NotImplementedError",
  "def __init__(self, model_output):\n        self.model_output = model_output",
  "def _calculate_scores(self) -> Dict[str, np.ndarray]:\n        raise NotImplementedError",
  "def _object_frequency(model_output: List[ObjectDetectionOutput],\n                      frequency_penalty: float,\n                      min_score: float) -> np.ndarray:\n    \"\"\"Score which prefers samples with many and diverse objects.\n\n    Args:\n        model_output:\n            Predictions of the model of length N.\n        frequency_penalty:\n            Penalty applied on multiple objects of the same category. A value\n            of 0.25 would count the first object fully and every additional\n            object only as 0.25.\n        min_score:\n            The minimum score a single sample can have\n        \n    Returns:\n        Numpy array of length N with the computed scores\n\n    \"\"\"\n    n_objs = []\n    for output in model_output:\n        val = 0\n        objs = {}\n        for label in output.labels:\n            if label in objs:\n                objs[label] += frequency_penalty\n            else:\n                objs[label] = 1\n        for k, v in objs.items():\n            val += v\n        n_objs.append(val)\n\n    _min = min(n_objs)\n    _max = max(n_objs)\n    scores = [np.interp(x, (_min, _max), (min_score, 1.0)) for x in n_objs]\n    return np.asarray(scores)",
  "def _prediction_margin(model_output: List[ObjectDetectionOutput]):\n    \"\"\"Score which prefers samples with low max(class prob) * objectness.\n\n    Args:\n        model_output:\n            Predictions of the model of length N.\n\n    Returns:\n        Numpy array of length N with the computed scores.\n\n    \"\"\"\n    scores = []\n    for output in model_output:\n        if len(output.scores) > 0:\n            # prediction margin is 1 - max(class probs), therefore the mean margin\n            # is mean(1 - max(class probs)) which is 1 - mean(max(class probs))\n            score = 1. - np.mean(output.scores)\n        else:\n            # set the score to 0 if there was no bounding box detected\n            score = 0.\n        scores.append(score)\n    return np.asarray(scores)",
  "class ScorerObjectDetection(Scorer):\n    \"\"\"Class to compute active learning scores from the model_output of an object detection task.\n\n    Currently supports the following scorers:\n\n        `object-frequency`:\n            This scorer uses model predictions to focus more on images which\n            have many objects in them. Use this scorer if you want scenes\n            with lots of objects in them like we usually want in\n            computer vision tasks such as perception in autonomous driving.\n\n        `prediction-margin`:\n            This scorer uses the margin between 1.0 and the highest confidence\n            prediction. Use this scorer to select images where the model is\n            insecure.\n\n    Attributes:\n        model_output:\n            List of model outputs in an object detection setting.\n        config:\n            A dictionary containing additional parameters for the scorers.\n\n            `frequency_penalty` (float):\n                Used by the `object-frequency` scorer.\n                If objects of the same class are within the same sample we\n                multiply them with the penalty. 1.0 has no effect. 0.5 would\n                count the first object fully and the second object of the same\n                class only 50%. Lowering this value results in a more balanced\n                setting of the classes. 0.0 is max penalty. (default: 0.25)\n            `min_score` (float):\n                Used by the `object-frequency` scorer.\n                Specifies the minimum score per sample. All scores are\n                scaled to [`min_score`, 1.0] range. Lowering the number makes\n                the sampler focus more on samples with many objects.\n                (default: 0.9)\n\n    Examples:\n        >>> # typical model output\n        >>> predictions = [{\n        >>>     'boxes': [[0.1, 0.2, 0.3, 0.4]],\n        >>>     'object_probabilities': [0.1024],\n        >>>     'class_probabilities': [[0.5, 0.41, 0.09]]\n        >>> }]\n        >>>\n        >>> # generate detection outputs\n        >>> model_output = []\n        >>> for prediction in predictions:\n        >>>     # convert each box to a BoundingBox object\n        >>>     boxes = []\n        >>>     for box in prediction['boxes']:\n        >>>         x0, x1 = box[0], box[2]\n        >>>         y0, y1 = box[1], box[3]\n        >>>         boxes.append(BoundingBox(x0, y0, x1, y1))\n        >>>     # create detection outputs\n        >>>     output = ObjectDetectionOutput(\n        >>>         boxes,\n        >>>         prediction['object_probabilities'],\n        >>>         prediction['class_probabilities']\n        >>>     )\n        >>>     model_output.append(output)\n        >>>\n        >>> # create scorer from output\n        >>> scorer = ScorerObjectDetection(model_output)\n\n    \"\"\"\n\n    def __init__(self,\n                 model_output: List[ObjectDetectionOutput],\n                 config: Dict = None):\n        super(ScorerObjectDetection, self).__init__(model_output)\n        self.config = config\n        self._check_config()\n\n    def _check_config(self):\n        default_conf = {\n            'frequency_penalty': 0.25,\n            'min_score': 0.9\n        }\n\n        # Check if we have a config dictionary passed in constructor\n        if self.config is not None and isinstance(self.config, dict):\n            # check if constructor received keys which are wrong\n            for k in self.config.keys():\n                if k not in default_conf.keys():\n                    raise KeyError(\n                        f'Scorer config parameter {k} is not a valid key. '\n                        f'Use one of: {default_conf.keys()}'\n                    )\n\n            # for now all values in config should be between 0.0 and 1.0 and numbers\n            for k, v in self.config.items():\n                if not (isinstance(v, float) or isinstance(v, int)):\n                    raise ValueError(\n                        f'Scorer config values must be numbers. However, '\n                        f'{k} has a value of type {type(v)}.'\n                    )\n\n                if v < 0.0 or v > 1.0:\n                    raise ValueError(\n                        f'Scorer config parameter {k} value ({v}) out of range. '\n                        f'Should be between 0.0 and 1.0.'\n                    )\n\n                # use default config if not specified in config\n                for k, v in default_conf.items():\n                    self.config[k] = self.config.get(k, v)\n        else:\n            self.config = default_conf\n\n    def _calculate_scores(self) -> Dict[str, np.ndarray]:\n        scores = dict()\n        scores['object-frequency'] = self._get_object_frequency()\n        scores['prediction-margin'] = self._get_prediction_margin()\n        return scores\n\n    def _get_object_frequency(self):\n        scores = _object_frequency(\n            self.model_output,\n            self.config['frequency_penalty'],\n            self.config['min_score'])\n        return scores\n\n    def _get_prediction_margin(self):\n        scores = _prediction_margin(self.model_output)\n        return scores",
  "def __init__(self,\n                 model_output: List[ObjectDetectionOutput],\n                 config: Dict = None):\n        super(ScorerObjectDetection, self).__init__(model_output)\n        self.config = config\n        self._check_config()",
  "def _check_config(self):\n        default_conf = {\n            'frequency_penalty': 0.25,\n            'min_score': 0.9\n        }\n\n        # Check if we have a config dictionary passed in constructor\n        if self.config is not None and isinstance(self.config, dict):\n            # check if constructor received keys which are wrong\n            for k in self.config.keys():\n                if k not in default_conf.keys():\n                    raise KeyError(\n                        f'Scorer config parameter {k} is not a valid key. '\n                        f'Use one of: {default_conf.keys()}'\n                    )\n\n            # for now all values in config should be between 0.0 and 1.0 and numbers\n            for k, v in self.config.items():\n                if not (isinstance(v, float) or isinstance(v, int)):\n                    raise ValueError(\n                        f'Scorer config values must be numbers. However, '\n                        f'{k} has a value of type {type(v)}.'\n                    )\n\n                if v < 0.0 or v > 1.0:\n                    raise ValueError(\n                        f'Scorer config parameter {k} value ({v}) out of range. '\n                        f'Should be between 0.0 and 1.0.'\n                    )\n\n                # use default config if not specified in config\n                for k, v in default_conf.items():\n                    self.config[k] = self.config.get(k, v)\n        else:\n            self.config = default_conf",
  "def _calculate_scores(self) -> Dict[str, np.ndarray]:\n        scores = dict()\n        scores['object-frequency'] = self._get_object_frequency()\n        scores['prediction-margin'] = self._get_prediction_margin()\n        return scores",
  "def _get_object_frequency(self):\n        scores = _object_frequency(\n            self.model_output,\n            self.config['frequency_penalty'],\n            self.config['min_score'])\n        return scores",
  "def _get_prediction_margin(self):\n        scores = _prediction_margin(self.model_output)\n        return scores",
  "def _entropy(probs: np.ndarray, axis: int = 1) -> np.ndarray:\n    \"\"\"Computes the entropy of a probability matrix over one array\n\n    Args:\n        probs:\n            A probability matrix of shape (N, M)\n        axis:\n            The axis the compute the probability over, the output does not have this axis anymore\n\n    Exammple:\n        if probs.shape = (N, C) and axis = 1 then entropies.shape = (N, )\n\n    Returns:\n        The entropy of the prediction vectors, shape: probs.shape, but without the specified axis\n    \"\"\"\n    zeros = np.zeros_like(probs)\n    log_probs = np.log2(probs, out=zeros, where=probs > 0)\n    entropies = -1 * np.sum(probs * log_probs, axis=axis)\n    return entropies",
  "class ScorerClassification(Scorer):\n    \"\"\"Class to compute active learning scores from the model_output of a classification task.\n\n    Currently supports the following scorers:\n\n        `prediction-margin`:\n            This scorer uses the margin between 1.0 and the highest confidence\n            prediction. Use this scorer to select images where the model is\n            insecure.\n\n        `prediction-entropy`:\n            This scorer computes the entropy of the prediction. All\n            confidences are considered to compute the entropy of a sample.\n\n    Attributes:\n        model_output:\n            Predictions of shape N x C where N is the number of unlabeled samples\n            and C is the number of classes in the classification task. Must be\n            normalized such that the sum over each row is 1.\n            The order of the predictions must be the one specified by\n            ActiveLearningAgent.unlabeled_set.\n\n    Examples:\n        >>> # example with three unlabeled samples\n        >>> al_agent.unlabeled_set\n        >>> > ['img0.jpg', 'img1.jpg', 'img2.jpg']\n        >>> predictions = np.array(\n        >>>     [\n        >>>          [0.1, 0.9], # predictions for img0.jpg\n        >>>          [0.3, 0.7], # predictions for img1.jpg\n        >>>          [0.8, 0.2], # predictions for img2.jpg\n        >>>     ] \n        >>> )\n        >>> np.sum(predictions, axis=1)\n        >>> > array([1., 1., 1.])\n        >>> scorer = ScorerClassification(predictions)\n\n    \"\"\"\n    def __init__(self, model_output: np.ndarray):\n        super(ScorerClassification, self).__init__(model_output)\n\n    def _calculate_scores(self) -> Dict[str, np.ndarray]:\n        scores = dict()\n        scores[\"prediction-margin\"] = self._get_prediction_margin_score()\n        scores[\"prediction-entropy\"] = self._get_prediction_entropy_score()\n        return scores\n\n    def _get_prediction_margin_score(self):\n        uncertainties = np.array([1 - max(class_probabilities) for class_probabilities in self.model_output])\n        return uncertainties\n\n    def _get_prediction_entropy_score(self):\n        uncertainties = _entropy(self.model_output, axis=1)\n        return uncertainties",
  "def __init__(self, model_output: np.ndarray):\n        super(ScorerClassification, self).__init__(model_output)",
  "def _calculate_scores(self) -> Dict[str, np.ndarray]:\n        scores = dict()\n        scores[\"prediction-margin\"] = self._get_prediction_margin_score()\n        scores[\"prediction-entropy\"] = self._get_prediction_entropy_score()\n        return scores",
  "def _get_prediction_margin_score(self):\n        uncertainties = np.array([1 - max(class_probabilities) for class_probabilities in self.model_output])\n        return uncertainties",
  "def _get_prediction_entropy_score(self):\n        uncertainties = _entropy(self.model_output, axis=1)\n        return uncertainties",
  "class ActiveLearningAgent:\n    \"\"\"Interface for active learning queries.\n\n    Attributes:\n        api_workflow_client:\n            The client to connect to the api.\n        preselected_tag_id:\n            The id of the tag containing the already labeled samples, default: None == no labeled samples yet.\n        query_tag_id:\n            The id of the tag defining where to sample from, default: None resolves to initial_tag\n        labeled_set:\n            The filenames of the samples in the labeled set, List[str]\n        unlabeled_set:\n            The filenames of the samples in the unlabeled set, List[str]\n\n    Examples:\n        >>> # set the token and dataset id\n        >>> token = '123'\n        >>> dataset_id = 'XYZ'\n        >>>\n        >>> # create an active learning agent\n        >>> client = ApiWorkflowClient(token, dataset_id)\n        >>> agent = ActiveLearningAgent(client)\n        >>>\n        >>> # make an initial active learning query\n        >>> sampler_config = SamplerConfig(n_samples=100, name='initial-set')\n        >>> initial_set = agent.query(sampler_config)\n        >>> unlabeled_set = agent.unlabeled_set\n        >>>\n        >>> # train and evaluate a model on the initial set\n        >>> # make predictions on the unlabeled set (keep ordering of filenames)\n        >>>\n        >>> # create active learning scorer\n        >>> scorer = ScorerClassification(predictions)\n        >>>\n        >>> # make a second active learning query\n        >>> sampler_config = SamplerConfig(n_samples=200, name='second-set')\n        >>> second_set = agent.query(sampler_config, scorer)\n\n    \"\"\"\n\n    def __init__(self, api_workflow_client: ApiWorkflowClient, query_tag_name: str = None, preselected_tag_name: str = None):\n\n        self.api_workflow_client = api_workflow_client\n        if query_tag_name is not None or preselected_tag_name is not None:\n            tag_name_id_dict = dict([tag.name, tag.id] for tag in self.api_workflow_client._get_all_tags())\n            if preselected_tag_name is not None:\n                self.preselected_tag_id = tag_name_id_dict[preselected_tag_name]\n            if query_tag_name is not None:\n                self.query_tag_id = tag_name_id_dict[query_tag_name]\n\n        if not hasattr(self, \"preselected_tag_id\"):\n            self.preselected_tag_id = None\n        if not hasattr(self, \"query_tag_id\"):\n            self.query_tag_id = None\n        self._set_labeled_and_unlabeled_set()\n\n    def _set_labeled_and_unlabeled_set(self, preselected_tag_data: TagData = None):\n        \"\"\"Sets the labeled and unlabeled set based on the preselected and query tag id\n\n        It loads the bitmaks for the both tag_ids from the server and then\n        extracts the filenames from it given the mapping on the server.\n\n        Args:\n            preselected_tag_data:\n                optional param, then it must not be loaded from the API\n\n        \"\"\"\n        if self.preselected_tag_id is None:\n            self.labeled_set = []\n        else:\n            if preselected_tag_data is None:\n                preselected_tag_data = self.api_workflow_client.tags_api.get_tag_by_tag_id(\n                    self.api_workflow_client.dataset_id, tag_id=self.preselected_tag_id)\n            chosen_samples_ids = BitMask.from_hex(preselected_tag_data.bit_mask_data).to_indices()\n            self.labeled_set = [self.api_workflow_client.filenames_on_server[i] for i in chosen_samples_ids]\n\n        if not hasattr(self, \"unlabeled_set\"):\n            if self.query_tag_id is None:\n                self.unlabeled_set = self.api_workflow_client.filenames_on_server\n            else:\n                query_tag_data = self.api_workflow_client.tags_api.get_tag_by_tag_id(\n                    self.api_workflow_client.dataset_id, tag_id=self.query_tag_id)\n                chosen_samples_ids = BitMask.from_hex(query_tag_data.bit_mask_data).to_indices()\n                self.unlabeled_set = [self.api_workflow_client.filenames_on_server[i] for i in chosen_samples_ids]\n\n        filenames_labeled = set(self.labeled_set)\n        self.unlabeled_set = [f for f in self.unlabeled_set if f not in filenames_labeled]\n\n    def query(self, sampler_config: SamplerConfig, al_scorer: Scorer = None) -> List[str]:\n        \"\"\"Performs an active learning query.\n\n        As part of it, the self.labeled_set and self.unlabeled_set are updated\n        and can be used for the next step.\n\n        Args:\n            sampler_config:\n                The sampling configuration.\n            al_scorer:\n                An instance of a class inheriting from Scorer, e.g. a ClassificationScorer.\n\n        Returns:\n            The filenames of the samples in the new labeled_set.\n\n        \"\"\"\n        # check input\n        if sampler_config.n_samples < len(self.labeled_set):\n            warnings.warn(\"ActiveLearningAgent.query: The number of samples which should be sampled \"\n                           \"including the current labeled set \"\n                           \"(sampler_config.n_samples) \"\n                            \"is smaller than the number of samples in the current labeled set.\")\n            return self.labeled_set\n\n        # calculate scores\n        if al_scorer is not None:\n            no_unlabeled_samples = len(self.unlabeled_set)\n            no_samples_with_predictions = len(al_scorer.model_output)\n            if no_unlabeled_samples != no_samples_with_predictions:\n                raise ValueError(f\"The scorer must have exactly as much samples as in the unlabeled set,\"\n                                 f\"but there are {no_samples_with_predictions} predictions in the scorer,\"\n                                 f\"but {no_unlabeled_samples} in the unlabeled set.\")\n            scores_dict = al_scorer._calculate_scores()\n        else:\n            scores_dict = None\n\n        # perform the sampling\n        new_tag_data = self.api_workflow_client.sampling(\n            sampler_config=sampler_config,\n            al_scores=scores_dict,\n            preselected_tag_id=self.preselected_tag_id,\n            query_tag_id=self.query_tag_id)\n\n        # set the newly chosen tag as the new preselected_tag_id and update the sets\n        self.preselected_tag_id = new_tag_data.id\n        self._set_labeled_and_unlabeled_set(new_tag_data)\n\n        return self.labeled_set",
  "def __init__(self, api_workflow_client: ApiWorkflowClient, query_tag_name: str = None, preselected_tag_name: str = None):\n\n        self.api_workflow_client = api_workflow_client\n        if query_tag_name is not None or preselected_tag_name is not None:\n            tag_name_id_dict = dict([tag.name, tag.id] for tag in self.api_workflow_client._get_all_tags())\n            if preselected_tag_name is not None:\n                self.preselected_tag_id = tag_name_id_dict[preselected_tag_name]\n            if query_tag_name is not None:\n                self.query_tag_id = tag_name_id_dict[query_tag_name]\n\n        if not hasattr(self, \"preselected_tag_id\"):\n            self.preselected_tag_id = None\n        if not hasattr(self, \"query_tag_id\"):\n            self.query_tag_id = None\n        self._set_labeled_and_unlabeled_set()",
  "def _set_labeled_and_unlabeled_set(self, preselected_tag_data: TagData = None):\n        \"\"\"Sets the labeled and unlabeled set based on the preselected and query tag id\n\n        It loads the bitmaks for the both tag_ids from the server and then\n        extracts the filenames from it given the mapping on the server.\n\n        Args:\n            preselected_tag_data:\n                optional param, then it must not be loaded from the API\n\n        \"\"\"\n        if self.preselected_tag_id is None:\n            self.labeled_set = []\n        else:\n            if preselected_tag_data is None:\n                preselected_tag_data = self.api_workflow_client.tags_api.get_tag_by_tag_id(\n                    self.api_workflow_client.dataset_id, tag_id=self.preselected_tag_id)\n            chosen_samples_ids = BitMask.from_hex(preselected_tag_data.bit_mask_data).to_indices()\n            self.labeled_set = [self.api_workflow_client.filenames_on_server[i] for i in chosen_samples_ids]\n\n        if not hasattr(self, \"unlabeled_set\"):\n            if self.query_tag_id is None:\n                self.unlabeled_set = self.api_workflow_client.filenames_on_server\n            else:\n                query_tag_data = self.api_workflow_client.tags_api.get_tag_by_tag_id(\n                    self.api_workflow_client.dataset_id, tag_id=self.query_tag_id)\n                chosen_samples_ids = BitMask.from_hex(query_tag_data.bit_mask_data).to_indices()\n                self.unlabeled_set = [self.api_workflow_client.filenames_on_server[i] for i in chosen_samples_ids]\n\n        filenames_labeled = set(self.labeled_set)\n        self.unlabeled_set = [f for f in self.unlabeled_set if f not in filenames_labeled]",
  "def query(self, sampler_config: SamplerConfig, al_scorer: Scorer = None) -> List[str]:\n        \"\"\"Performs an active learning query.\n\n        As part of it, the self.labeled_set and self.unlabeled_set are updated\n        and can be used for the next step.\n\n        Args:\n            sampler_config:\n                The sampling configuration.\n            al_scorer:\n                An instance of a class inheriting from Scorer, e.g. a ClassificationScorer.\n\n        Returns:\n            The filenames of the samples in the new labeled_set.\n\n        \"\"\"\n        # check input\n        if sampler_config.n_samples < len(self.labeled_set):\n            warnings.warn(\"ActiveLearningAgent.query: The number of samples which should be sampled \"\n                           \"including the current labeled set \"\n                           \"(sampler_config.n_samples) \"\n                            \"is smaller than the number of samples in the current labeled set.\")\n            return self.labeled_set\n\n        # calculate scores\n        if al_scorer is not None:\n            no_unlabeled_samples = len(self.unlabeled_set)\n            no_samples_with_predictions = len(al_scorer.model_output)\n            if no_unlabeled_samples != no_samples_with_predictions:\n                raise ValueError(f\"The scorer must have exactly as much samples as in the unlabeled set,\"\n                                 f\"but there are {no_samples_with_predictions} predictions in the scorer,\"\n                                 f\"but {no_unlabeled_samples} in the unlabeled set.\")\n            scores_dict = al_scorer._calculate_scores()\n        else:\n            scores_dict = None\n\n        # perform the sampling\n        new_tag_data = self.api_workflow_client.sampling(\n            sampler_config=sampler_config,\n            al_scores=scores_dict,\n            preselected_tag_id=self.preselected_tag_id,\n            query_tag_id=self.query_tag_id)\n\n        # set the newly chosen tag as the new preselected_tag_id and update the sets\n        self.preselected_tag_id = new_tag_data.id\n        self._set_labeled_and_unlabeled_set(new_tag_data)\n\n        return self.labeled_set",
  "class SamplerConfig:\n    \"\"\"Configuration class for a sampler.\n\n    Attributes:\n        method:\n            The method to use for sampling, one of CORESET, RANDOM, CORAL, ACTIVE_LEARNING\n        n_samples:\n            The maximum number of samples to be chosen by the sampler\n            including the samples in the preselected tag. One of the stopping\n            conditions.\n        min_distance:\n            The minimum distance of samples in the chosen set, one of the\n            stopping conditions.\n        name:\n            The name of this sampling, defaults to a name consisting of all\n            other attributes and the datetime. A new tag will be created in the\n            web-app under this name.\n\n    Examples:\n        >>> # sample 100 images with CORESET sampling\n        >>> config = SamplerConfig(method=SamplingMethod.CORESET, n_samples=100)\n        >>>\n        >>> # give your sampling a name\n        >>> config = SamplerConfig(method=SamplingMethod.CORESET, n_samples=100, name='my-sampling')\n        >>>\n        >>> # use minimum distance between samples as stopping criterion\n        >>> config = SamplerConfig(method=SamplingMethod.CORESET, n_samples=-1, min_distance=0.1)\n\n    \"\"\"\n    def __init__(self, method: SamplingMethod = SamplingMethod.CORESET, n_samples: int = 32, min_distance: float = -1,\n                 name: str = None):\n\n        self.method = method\n        self.n_samples = n_samples\n        self.min_distance = min_distance\n        if name is None:\n            date_time = datetime.now().strftime(\"%m_%d_%Y__%H_%M_%S\")\n            name = f\"{self.method}_{self.n_samples}_{self.min_distance}_{date_time}\"\n        self.name = name",
  "def __init__(self, method: SamplingMethod = SamplingMethod.CORESET, n_samples: int = 32, min_distance: float = -1,\n                 name: str = None):\n\n        self.method = method\n        self.n_samples = n_samples\n        self.min_distance = min_distance\n        if name is None:\n            date_time = datetime.now().strftime(\"%m_%d_%Y__%H_%M_%S\")\n            name = f\"{self.method}_{self.n_samples}_{self.min_distance}_{date_time}\"\n        self.name = name",
  "class BoundingBox:\n    \"\"\"Class which unifies different bounding box formats.\n\n    Attributes:\n        x0:\n            x0 coordinate (normalized to [0, 1])\n        y0:\n            y0 coordinate (normalized to [0, 1])\n        x1:\n            x1 coordinate (normalized to [0, 1])\n        y1:\n            y1 coordinate (normalized to [0, 1])\n\n    Examples:\n    >>> # simple case, format (x0, y0, x1, y1)\n    >>> bbox = BoundingBox(0.1, 0.2, 0.3, 0.4)\n    >>>\n    >>> # same bounding box in x, y, w, h format\n    >>> bbox = BoundingBox.from_x_y_w_h(0.1, 0.2, 0.2, 0.2)\n    >>>\n    >>> # often the coordinates are not yet normalized by image size\n    >>> # for example, for a 100 x 100 image, the coordinates could be\n    >>> # (x0, y0, x1, y1) = (10, 20, 30, 40)\n    >>> W, H = 100, 100 # get image shape\n    >>> bbox = BoundingBox(10 / W, 20 / H, 30 / W, 40 / H)\n\n    \"\"\"\n\n    def __init__(self, x0: float, y0: float, x1: float, y1: float):\n\n        if x0 > 1 or x1 > 1 or y0 > 1 or y1 > 1 or \\\n            x0 < 0 or x1 < 0 or y0 < 0 or y1 < 0:\n            raise ValueError(\n                'Bounding Box Coordinates must be relative to '\n                'image width and height but are ({x0}, {y0}, {x1}, {y1}).'\n            )\n\n        if x0 > x1:\n            raise ValueError(\n                'x0 must be smaller than or equal to x1 for bounding box '\n                f'[{x0}, {y0}, {x1}, {y1}]'\n            )\n\n        if y0 > y1:\n            raise ValueError(\n                'y0 must be smaller than or equal to y1 for bounding box '\n                f'[{x0}, {y0}, {x1}, {y1}]'\n            )\n\n        self.x0 = x0\n        self.y0 = y0\n        self.x1 = x1\n        self.y1 = y1\n\n    @classmethod\n    def from_x_y_w_h(cls, x: float, y: float, w: float, h: float):\n        \"\"\"Helper to convert from bounding box format with width and height.\n\n        Examples:\n        >>> bbox = BoundingBox.from_x_y_w_h(0.1, 0.2, 0.2, 0.2)\n\n        \"\"\"\n        return cls(x, y, x + w, y + h)\n\n    @property\n    def width(self):\n        \"\"\"Returns the width of the bounding box relative to the image size.\n\n        \"\"\"\n        return self.x1 - self.x0\n\n    @property\n    def height(self):\n        \"\"\"Returns the height of the bounding box relative to the image size.\n\n        \"\"\"\n        return self.y1 - self.y0\n\n    @property\n    def area(self):\n        \"\"\"Returns the area of the bounding box relative to the area of the image.\n\n        \"\"\"\n        return self.width * self.height",
  "def __init__(self, x0: float, y0: float, x1: float, y1: float):\n\n        if x0 > 1 or x1 > 1 or y0 > 1 or y1 > 1 or \\\n            x0 < 0 or x1 < 0 or y0 < 0 or y1 < 0:\n            raise ValueError(\n                'Bounding Box Coordinates must be relative to '\n                'image width and height but are ({x0}, {y0}, {x1}, {y1}).'\n            )\n\n        if x0 > x1:\n            raise ValueError(\n                'x0 must be smaller than or equal to x1 for bounding box '\n                f'[{x0}, {y0}, {x1}, {y1}]'\n            )\n\n        if y0 > y1:\n            raise ValueError(\n                'y0 must be smaller than or equal to y1 for bounding box '\n                f'[{x0}, {y0}, {x1}, {y1}]'\n            )\n\n        self.x0 = x0\n        self.y0 = y0\n        self.x1 = x1\n        self.y1 = y1",
  "def from_x_y_w_h(cls, x: float, y: float, w: float, h: float):\n        \"\"\"Helper to convert from bounding box format with width and height.\n\n        Examples:\n        >>> bbox = BoundingBox.from_x_y_w_h(0.1, 0.2, 0.2, 0.2)\n\n        \"\"\"\n        return cls(x, y, x + w, y + h)",
  "def width(self):\n        \"\"\"Returns the width of the bounding box relative to the image size.\n\n        \"\"\"\n        return self.x1 - self.x0",
  "def height(self):\n        \"\"\"Returns the height of the bounding box relative to the image size.\n\n        \"\"\"\n        return self.y1 - self.y0",
  "def area(self):\n        \"\"\"Returns the area of the bounding box relative to the area of the image.\n\n        \"\"\"\n        return self.width * self.height",
  "class ObjectDetectionOutput:\n    \"\"\"Class which unifies different object detection output formats.\n\n    Attributes:\n        boxes:\n            List of BoundingBox objects with coordinates (x0, y0, x1, y1).\n        object_probabilities:\n            List of probabilities that the boxes are indeed objects.\n        class_probabilities:\n            List of probabilities for the different classes for each box.\n        scores:\n            List of confidence scores (i.e. max(class prob) * objectness).\n        labels:\n            List of labels (i.e. argmax(class prob)).\n\n    Examples:\n        >>> # typical model output\n        >>> prediction = {\n        >>>     'boxes': [[0.1, 0.2, 0.3, 0.4]],\n        >>>     'object_probabilities': [0.6],\n        >>>     'class_probabilities': [0.1, 0.5],\n        >>> }\n        >>>\n        >>> # convert bbox to objects\n        >>> boxes = [BoundingBox(0.1, 0.2, 0.3, 0.4)]\n        >>> object_probabilities = prediction['object_probabilities']\n        >>> class_probabilities = prediction['class_probabilities']\n        >>>\n        >>> # create detection output\n        >>> detection_output = ObjectDetectionOutput(\n        >>>     boxes,\n        >>>     object_probabilities,\n        >>>     class_probabilities,\n        >>> )\n\n    \"\"\"\n\n    def __init__(self,\n                 boxes: List[BoundingBox],\n                 object_probabilities: List[float],\n                 class_probabilities: List[List[float]]):\n\n        if len(boxes) != len(object_probabilities) or \\\n            len(object_probabilities) != len(class_probabilities):\n            raise ValueError('Boxes, object and class probabilities must be of '\n                             f'same length but are {len(boxes)}, '\n                             f'{len(object_probabilities)}, and '\n                             f'{len(class_probabilities)}')\n\n        scores = []\n        labels = []\n        for o, c in zip(object_probabilities, class_probabilities):\n            # calculate the score as the object probability times the maximum\n            # of the class probabilities\n            scores.append(o * max(c))\n            # the label is the argmax of the class probabilities\n            labels.append(c.index(max(c)))\n\n        self.boxes = boxes\n        self.scores = scores\n        self.labels = labels\n        self.object_probabilities = object_probabilities\n        self.class_probabilities = class_probabilities\n\n\n    @classmethod\n    def from_scores(cls,\n                    boxes: List[BoundingBox],\n                    scores: List[float],\n                    labels: List[int]):\n        \"\"\"Helper to convert from output format with scores.\n\n        Since this output format does not provide class probabilities, they\n        will be replaced by a one-hot vector indicating the label provided in\n        labels. The objectness will be set to the score for each bounding box.\n\n        Args:\n            boxes:\n                List of BoundingBox objects with coordinates (x0, y0, x1, y1).\n            scores:\n                List of confidence scores (i.e. max(class prob) * objectness).\n            labels:\n                List of labels.\n\n        Examples:\n            >>> # typical model output\n            >>> prediction = {\n            >>>     'boxes': [[0.1, 0.2, 0.3, 0.4]],\n            >>>     'scores': [0.1234],\n            >>>     'labels': [1]\n            >>> }\n            >>>\n            >>> # convert bbox to objects\n            >>> boxes = [BoundingBox(0.1, 0.2, 0.3, 0.4)]\n            >>> scores = prediction['scores']\n            >>> labels = prediction['labels']\n            >>>\n            >>> # create detection output\n            >>> detection_output = ObjectDetectionOutput.from_scores(\n            >>>     boxes, scores, labels)\n\n        \"\"\"\n\n        if any([score > 1 for score in scores]):\n            raise ValueError('Scores must be smaller than or equal to one!')\n\n        if any([score < 0 for score in scores]):\n            raise ValueError('Scores must be larger than or equal to zero!')\n\n        if not all([isinstance(label, int) for label in labels]):\n            raise ValueError('Labels must be list of integers.')\n\n        # create fake object probabilities\n        object_probabilities = [s for s in scores]\n\n        # create one-hot class probabilities\n        max_label = max(labels) if len(labels) > 0 else 0\n        class_probabilities = []\n        for label in labels:\n            c = [0.] * (max_label + 1)\n            c[label] = 1.\n            class_probabilities.append(c)\n\n        # create object detection output\n        output = cls(boxes, object_probabilities, class_probabilities)\n        output.scores = scores\n        output.labels = labels\n        return output",
  "def __init__(self,\n                 boxes: List[BoundingBox],\n                 object_probabilities: List[float],\n                 class_probabilities: List[List[float]]):\n\n        if len(boxes) != len(object_probabilities) or \\\n            len(object_probabilities) != len(class_probabilities):\n            raise ValueError('Boxes, object and class probabilities must be of '\n                             f'same length but are {len(boxes)}, '\n                             f'{len(object_probabilities)}, and '\n                             f'{len(class_probabilities)}')\n\n        scores = []\n        labels = []\n        for o, c in zip(object_probabilities, class_probabilities):\n            # calculate the score as the object probability times the maximum\n            # of the class probabilities\n            scores.append(o * max(c))\n            # the label is the argmax of the class probabilities\n            labels.append(c.index(max(c)))\n\n        self.boxes = boxes\n        self.scores = scores\n        self.labels = labels\n        self.object_probabilities = object_probabilities\n        self.class_probabilities = class_probabilities",
  "def from_scores(cls,\n                    boxes: List[BoundingBox],\n                    scores: List[float],\n                    labels: List[int]):\n        \"\"\"Helper to convert from output format with scores.\n\n        Since this output format does not provide class probabilities, they\n        will be replaced by a one-hot vector indicating the label provided in\n        labels. The objectness will be set to the score for each bounding box.\n\n        Args:\n            boxes:\n                List of BoundingBox objects with coordinates (x0, y0, x1, y1).\n            scores:\n                List of confidence scores (i.e. max(class prob) * objectness).\n            labels:\n                List of labels.\n\n        Examples:\n            >>> # typical model output\n            >>> prediction = {\n            >>>     'boxes': [[0.1, 0.2, 0.3, 0.4]],\n            >>>     'scores': [0.1234],\n            >>>     'labels': [1]\n            >>> }\n            >>>\n            >>> # convert bbox to objects\n            >>> boxes = [BoundingBox(0.1, 0.2, 0.3, 0.4)]\n            >>> scores = prediction['scores']\n            >>> labels = prediction['labels']\n            >>>\n            >>> # create detection output\n            >>> detection_output = ObjectDetectionOutput.from_scores(\n            >>>     boxes, scores, labels)\n\n        \"\"\"\n\n        if any([score > 1 for score in scores]):\n            raise ValueError('Scores must be smaller than or equal to one!')\n\n        if any([score < 0 for score in scores]):\n            raise ValueError('Scores must be larger than or equal to zero!')\n\n        if not all([isinstance(label, int) for label in labels]):\n            raise ValueError('Labels must be list of integers.')\n\n        # create fake object probabilities\n        object_probabilities = [s for s in scores]\n\n        # create one-hot class probabilities\n        max_label = max(labels) if len(labels) > 0 else 0\n        class_probabilities = []\n        for label in labels:\n            c = [0.] * (max_label + 1)\n            c[label] = 1.\n            class_probabilities.append(c)\n\n        # create object detection output\n        output = cls(boxes, object_probabilities, class_probabilities)\n        output.scores = scores\n        output.labels = labels\n        return output",
  "def _get_filename_by_index(dataset, index):\n    \"\"\"Default function which maps the index of an image to a filename.\n\n    \"\"\"\n    if isinstance(dataset, datasets.ImageFolder):\n        # filename is the path of the image relative to the dataset root\n        full_path = dataset.imgs[index][0]\n        return os.path.relpath(full_path, dataset.root)\n    elif isinstance(dataset, DatasetFolder):\n        # filename is the path of the image relative to the dataset root\n        full_path = dataset.samples[index][0]\n        return os.path.relpath(full_path, dataset.root)\n    elif isinstance(dataset, VideoDataset):\n        # filename is constructed by the video dataset\n        return dataset.get_filename(index)\n    else:\n        # dummy to prevent crashes\n        return str(index)",
  "def _ensure_dir(path):\n    \"\"\"Makes sure that the directory at path exists.\n\n    \"\"\"\n    dirname = os.path.dirname(path)\n    os.makedirs(dirname, exist_ok=True)",
  "def _copy_image(input_dir, output_dir, filename):\n    \"\"\"Copies an image from the input directory to the output directory.\n\n    \"\"\"\n    source = os.path.join(input_dir, filename)\n    target = os.path.join(output_dir, filename)\n    _ensure_dir(target)\n    shutil.copyfile(source, target)",
  "def _save_image(image, output_dir, filename, fmt):\n    \"\"\"Saves an image in the output directory.\n\n    \"\"\"\n    target = os.path.join(output_dir, filename)\n    _ensure_dir(target)\n    try:\n        # try to save the image with the specified format or\n        # derive the format from the filename (if format=None)\n        image.save(target, format=fmt)\n    except ValueError:\n        # could not determine format from filename\n        image.save(target, format='png')",
  "def _dump_image(dataset, output_dir, filename, index, fmt):\n    \"\"\"Saves a single image to the output directory.\n\n    Will copy the image from the input directory to the output directory\n    if possible. If not (e.g. for VideoDatasets), will load the image and\n    then save it to the output directory with the specified format.\n\n    \"\"\"\n\n    if isinstance(dataset, datasets.ImageFolder):\n        # can safely copy the image from the input to the output directory\n        _copy_image(dataset.root, output_dir, filename)\n    elif isinstance(dataset, DatasetFolder):\n        # can safely copy the image from the input to the output directory\n        _copy_image(dataset.root, output_dir, filename)\n    else:\n        # need to load the image and save it to the output directory\n        image, _ = dataset[index]\n        _save_image(image, output_dir, filename, fmt)",
  "class LightlyDataset:\n    \"\"\"Provides a uniform data interface for the embedding models.\n\n    Should be used for all models and functions in the lightly package.\n    Returns a tuple (sample, target, fname) when accessed using __getitem__.\n\n    The LightlyDataset supports different input sources. You can use it\n    on a folder of images. You can also use it on a folder with subfolders\n    with images (ImageNet style). If the input_dir has subfolders each subfolder\n    gets its own target label. You can also work with videos (requires pyav).\n    If there are multiple videos in the input_dir each video gets a different\n    target label assigned. If input_dir contains images and videos\n    only the videos are used.\n\n    Can also be used in combination with the `from_torch_dataset` method\n    to load a dataset offered by torchvision (e.g. cifar10).\n\n    Args:\n        input_dir:\n            Path to directory holding the images or videos to load.\n        transform:\n            Image transforms (as in torchvision).\n        index_to_filename:\n            Function which takes the dataset and index as input and returns\n            the filename of the file at the index. If None, uses default.\n\n    Examples:\n        >>> # load a dataset consisting of images from a local folder\n        >>> # mydata/\n        >>> # `- img1.png\n        >>> # `- img2.png\n        >>> # `- ...\n        >>> import lightly.data as data\n        >>> dataset = data.LightlyDataset(input_dir='path/to/mydata/')\n        >>> sample, target, fname = dataset[0]\n        >>>\n        >>> # also works with subfolders\n        >>> # mydata/\n        >>> # `- subfolder1\n        >>> #     `- img1.png\n        >>> # `- subfolder2\n        >>> # ...\n        >>>\n        >>> # also works with videos\n        >>> # mydata/\n        >>> # `- video1.mp4\n        >>> # `- video2.mp4\n        >>> # `- ...\n    \"\"\"\n\n    def __init__(self,\n                 input_dir: str,\n                 transform: transforms.Compose = None,\n                 index_to_filename: Callable[[datasets.VisionDataset, int], str] = None):\n\n        # can pass input_dir=None to create an \"empty\" dataset\n        self.input_dir = input_dir\n        if self.input_dir is not None:\n            self.dataset = _load_dataset(self.input_dir, transform)\n\n        # initialize function to get filename of image\n        self.index_to_filename = _get_filename_by_index\n        if index_to_filename is not None:\n            self.index_to_filename = index_to_filename\n\n    @classmethod\n    def from_torch_dataset(cls,\n                           dataset,\n                           transform=None,\n                           index_to_filename=None):\n        \"\"\"Builds a LightlyDataset from a PyTorch (or torchvision) dataset.\n\n        Args:\n            dataset:\n                PyTorch/torchvision dataset.\n            transform:\n                Image transforms (as in torchvision).\n            index_to_filename:\n                Function which takes the dataset and index as input and returns\n                the filename of the file at the index. If None, uses default.\n\n        Returns:\n            A LightlyDataset object.\n\n        Examples:\n            >>> # load cifar10 from torchvision\n            >>> import torchvision\n            >>> import lightly.data as data\n            >>> base = torchvision.datasets.CIFAR10(root='./')\n            >>> dataset = data.LightlyDataset.from_torch_dataset(base)\n\n        \"\"\"\n        # create an \"empty\" dataset object\n        dataset_obj = cls(\n            None,\n            transform=transform,\n            index_to_filename=index_to_filename\n        )\n\n        # populate it with the torch dataset\n        dataset_obj.dataset = dataset\n        return dataset_obj\n\n    def __getitem__(self, index: int):\n        \"\"\"Returns (sample, target, fname) of item at index.\n\n        Args:\n            index:\n                Index of the queried item.\n\n        Returns:\n            The image, target, and filename of the item at index.\n\n        \"\"\"\n        fname = self.index_to_filename(self.dataset, index)\n        sample, target = self.dataset.__getitem__(index)\n\n        return sample, target, fname\n\n    def __len__(self):\n        \"\"\"Returns the length of the dataset.\n\n        \"\"\"\n        return len(self.dataset)\n\n    def __add__(self, other):\n        \"\"\"Adds another item to the dataset.\n\n        \"\"\"\n        raise NotImplementedError()\n\n    def get_filenames(self) -> List[str]:\n        \"\"\"Returns all filenames in the dataset.\n\n        \"\"\"\n        list_of_filenames = []\n        for index in range(len(self)):\n            fname = self.index_to_filename(self.dataset, index)\n            list_of_filenames.append(fname)\n        return list_of_filenames\n\n    def dump(self,\n             output_dir: str,\n             filenames: Union[List[str], None] = None,\n             format: Union[str, None] = None):\n        \"\"\"Saves images in the dataset to the output directory.\n\n        Will copy the images from the input directory to the output directory\n        if possible. If not (e.g. for VideoDatasets), will load the images and\n        then save them to the output directory with the specified format.\n\n        Args:\n            output_dir:\n                Output directory where the image is stored.\n            filenames:\n                Filenames of the images to store. If None, stores all images.\n            format:\n                Image format. Can be any pillow image format (png, jpg, ...).\n                By default we try to use the same format as the input data. If\n                not possible (e.g. for videos) we dump the image \n                as a png image to prevent compression artifacts.\n\n        \"\"\"\n\n        if self.dataset.transform is not None:\n            raise RuntimeError('Cannot dump dataset which applies transforms!')\n\n        # create directory if it doesn't exist yet\n        os.makedirs(output_dir, exist_ok=True)\n\n        # dump all the files if no filenames were passed, otherwise dump only\n        # the ones referenced in the list\n        if filenames is None:\n            indices = [i for i in range(self.__len__())]\n            filenames = self.get_filenames()\n        else:\n            indices = []\n            all_filenames = self.get_filenames()\n            for i in range(len(filenames)):\n                if filenames[i] in all_filenames:\n                    indices.append(i)\n\n        # dump images\n        for i, filename in zip(indices, filenames):\n            _dump_image(self.dataset, output_dir, filename, i, fmt=format)\n\n    @property\n    def transform(self):\n        \"\"\"Getter for the transform of the dataset.\n\n        \"\"\"\n        return self.dataset.transform\n\n    @transform.setter\n    def transform(self, t):\n        \"\"\"Setter for the transform of the dataset.\n\n        \"\"\"\n        self.dataset.transform = t",
  "def __init__(self,\n                 input_dir: str,\n                 transform: transforms.Compose = None,\n                 index_to_filename: Callable[[datasets.VisionDataset, int], str] = None):\n\n        # can pass input_dir=None to create an \"empty\" dataset\n        self.input_dir = input_dir\n        if self.input_dir is not None:\n            self.dataset = _load_dataset(self.input_dir, transform)\n\n        # initialize function to get filename of image\n        self.index_to_filename = _get_filename_by_index\n        if index_to_filename is not None:\n            self.index_to_filename = index_to_filename",
  "def from_torch_dataset(cls,\n                           dataset,\n                           transform=None,\n                           index_to_filename=None):\n        \"\"\"Builds a LightlyDataset from a PyTorch (or torchvision) dataset.\n\n        Args:\n            dataset:\n                PyTorch/torchvision dataset.\n            transform:\n                Image transforms (as in torchvision).\n            index_to_filename:\n                Function which takes the dataset and index as input and returns\n                the filename of the file at the index. If None, uses default.\n\n        Returns:\n            A LightlyDataset object.\n\n        Examples:\n            >>> # load cifar10 from torchvision\n            >>> import torchvision\n            >>> import lightly.data as data\n            >>> base = torchvision.datasets.CIFAR10(root='./')\n            >>> dataset = data.LightlyDataset.from_torch_dataset(base)\n\n        \"\"\"\n        # create an \"empty\" dataset object\n        dataset_obj = cls(\n            None,\n            transform=transform,\n            index_to_filename=index_to_filename\n        )\n\n        # populate it with the torch dataset\n        dataset_obj.dataset = dataset\n        return dataset_obj",
  "def __getitem__(self, index: int):\n        \"\"\"Returns (sample, target, fname) of item at index.\n\n        Args:\n            index:\n                Index of the queried item.\n\n        Returns:\n            The image, target, and filename of the item at index.\n\n        \"\"\"\n        fname = self.index_to_filename(self.dataset, index)\n        sample, target = self.dataset.__getitem__(index)\n\n        return sample, target, fname",
  "def __len__(self):\n        \"\"\"Returns the length of the dataset.\n\n        \"\"\"\n        return len(self.dataset)",
  "def __add__(self, other):\n        \"\"\"Adds another item to the dataset.\n\n        \"\"\"\n        raise NotImplementedError()",
  "def get_filenames(self) -> List[str]:\n        \"\"\"Returns all filenames in the dataset.\n\n        \"\"\"\n        list_of_filenames = []\n        for index in range(len(self)):\n            fname = self.index_to_filename(self.dataset, index)\n            list_of_filenames.append(fname)\n        return list_of_filenames",
  "def dump(self,\n             output_dir: str,\n             filenames: Union[List[str], None] = None,\n             format: Union[str, None] = None):\n        \"\"\"Saves images in the dataset to the output directory.\n\n        Will copy the images from the input directory to the output directory\n        if possible. If not (e.g. for VideoDatasets), will load the images and\n        then save them to the output directory with the specified format.\n\n        Args:\n            output_dir:\n                Output directory where the image is stored.\n            filenames:\n                Filenames of the images to store. If None, stores all images.\n            format:\n                Image format. Can be any pillow image format (png, jpg, ...).\n                By default we try to use the same format as the input data. If\n                not possible (e.g. for videos) we dump the image \n                as a png image to prevent compression artifacts.\n\n        \"\"\"\n\n        if self.dataset.transform is not None:\n            raise RuntimeError('Cannot dump dataset which applies transforms!')\n\n        # create directory if it doesn't exist yet\n        os.makedirs(output_dir, exist_ok=True)\n\n        # dump all the files if no filenames were passed, otherwise dump only\n        # the ones referenced in the list\n        if filenames is None:\n            indices = [i for i in range(self.__len__())]\n            filenames = self.get_filenames()\n        else:\n            indices = []\n            all_filenames = self.get_filenames()\n            for i in range(len(filenames)):\n                if filenames[i] in all_filenames:\n                    indices.append(i)\n\n        # dump images\n        for i, filename in zip(indices, filenames):\n            _dump_image(self.dataset, output_dir, filename, i, fmt=format)",
  "def transform(self):\n        \"\"\"Getter for the transform of the dataset.\n\n        \"\"\"\n        return self.dataset.transform",
  "def transform(self, t):\n        \"\"\"Setter for the transform of the dataset.\n\n        \"\"\"\n        self.dataset.transform = t",
  "class BaseCollateFunction(nn.Module):\n    \"\"\"Base class for other collate implementations.\n\n    Takes a batch of images as input and transforms each image into two \n    different augmentations with the help of random transforms. The images are\n    then concatenated such that the output batch is exactly twice the length \n    of the input batch.\n\n    Attributes:\n        transform:\n            A set of torchvision transforms which are randomly applied to\n            each image.\n\n    \"\"\"\n\n    def __init__(self, transform: torchvision.transforms.Compose):\n\n        super(BaseCollateFunction, self).__init__()\n        self.transform = transform\n\n    def forward(self, batch: List[tuple]):\n        \"\"\"Turns a batch of tuples into a tuple of batches.\n\n            Args:\n                batch:\n                    A batch of tuples of images, labels, and filenames which\n                    is automatically provided if the dataloader is built from \n                    a LightlyDataset.\n\n            Returns:\n                A tuple of images, labels, and filenames. The images consist of \n                two batches corresponding to the two transformations of the\n                input images.\n\n            Examples:\n                >>> # define a random transformation and the collate function\n                >>> transform = ... # some random augmentations\n                >>> collate_fn = BaseCollateFunction(transform)\n                >>>\n                >>> # input is a batch of tuples (here, batch_size = 1)\n                >>> input = [(img, 0, 'my-image.png')]\n                >>> output = collate_fn(input)\n                >>>\n                >>> # output consists of two random transforms of the images,\n                >>> # the labels, and the filenames in the batch\n                >>> (img_t0, img_t1), label, filename = output\n\n        \"\"\"\n        batch_size = len(batch)\n\n        # list of transformed images\n        transforms = [self.transform(batch[i % batch_size][0]).unsqueeze_(0)\n                      for i in range(2 * batch_size)]\n        # list of labels\n        labels = torch.LongTensor([item[1] for item in batch])\n        # list of filenames\n        fnames = [item[2] for item in batch]\n\n        # tuple of transforms\n        transforms = (\n            torch.cat(transforms[:batch_size], 0),\n            torch.cat(transforms[batch_size:], 0)\n        )\n\n        return transforms, labels, fnames",
  "class ImageCollateFunction(BaseCollateFunction):\n    \"\"\"Implementation of a collate function for images.\n\n    This is an implementation of the BaseCollateFunction with a concrete\n    set of transforms.\n\n    The set of transforms is inspired by the SimCLR paper as it has shown\n    to produce powerful embeddings. \n\n    Attributes:\n        input_size:\n            Size of the input image in pixels.\n        cj_prob:\n            Probability that color jitter is applied.\n        cj_bright:\n            How much to jitter brightness.\n        cj_contrast:\n            How much to jitter constrast.\n        cj_sat:\n            How much to jitter saturation.\n        cj_hue:\n            How much to jitter hue.\n        min_scale:\n            Minimum size of the randomized crop relative to the input_size.\n        random_gray_scale:\n            Probability of conversion to grayscale.\n        gaussian_blur:\n            Probability of Gaussian blur.\n        kernel_size:\n            Sigma of gaussian blur is kernel_size * input_size.\n        vf_prob:\n            Probability that vertical flip is applied.\n        hf_prob:\n            Probability that horizontal flip is applied.\n        rr_prob:\n            Probability that random (+90 degree) rotation is applied.\n        normalize:\n            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.\n\n    \"\"\"\n\n    def __init__(self,\n                 input_size: int = 64,\n                 cj_prob: float = 0.8,\n                 cj_bright: float = 0.7,\n                 cj_contrast: float = 0.7,\n                 cj_sat: float = 0.7,\n                 cj_hue: float = 0.2,\n                 min_scale: float = 0.15,\n                 random_gray_scale: float = 0.2,\n                 gaussian_blur: float = 0.5,\n                 kernel_size: float = 0.1,\n                 vf_prob: float = 0.0,\n                 hf_prob: float = 0.5,\n                 rr_prob: float = 0.0,\n                 normalize: dict = imagenet_normalize):\n\n        if isinstance(input_size, tuple):\n            input_size_ = max(input_size)\n        else:\n            input_size_ = input_size\n\n        color_jitter = T.ColorJitter(\n            cj_bright, cj_contrast, cj_sat, cj_hue\n        )\n\n        transform = [T.RandomResizedCrop(size=input_size,\n                                         scale=(min_scale, 1.0)),\n             RandomRotate(prob=rr_prob),\n             T.RandomHorizontalFlip(p=hf_prob),\n             T.RandomVerticalFlip(p=vf_prob),\n             T.RandomApply([color_jitter], p=cj_prob),\n             T.RandomGrayscale(p=random_gray_scale),\n             GaussianBlur(\n                 kernel_size=kernel_size * input_size_,\n                 prob=gaussian_blur),\n             T.ToTensor()\n        ]\n\n        if normalize:\n            transform += [\n             T.Normalize(\n                mean=normalize['mean'],\n                std=normalize['std'])\n             ]\n           \n        transform = T.Compose(transform)\n\n        super(ImageCollateFunction, self).__init__(transform)",
  "class SimCLRCollateFunction(ImageCollateFunction):\n    \"\"\"Implements the transformations for SimCLR.\n\n    Attributes:\n        input_size:\n            Size of the input image in pixels.\n        cj_prob:\n            Probability that color jitter is applied.\n        cj_strength:\n            Strength of the color jitter.\n        min_scale:\n            Minimum size of the randomized crop relative to the input_size.\n        random_gray_scale:\n            Probability of conversion to grayscale.\n        gaussian_blur:\n            Probability of Gaussian blur.\n        kernel_size:\n            Sigma of gaussian blur is kernel_size * input_size.\n        vf_prob:\n            Probability that vertical flip is applied.\n        hf_prob:\n            Probability that horizontal flip is applied.\n        rr_prob:\n            Probability that random (+90 degree) rotation is applied.\n        normalize:\n            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.\n\n    Examples:\n\n        >>> # SimCLR for ImageNet\n        >>> collate_fn = SimCLRCollateFunction()\n        >>> \n        >>> # SimCLR for CIFAR-10\n        >>> collate_fn = SimCLRCollateFunction(\n        >>>     input_size=32,\n        >>>     gaussian_blur=0.,\n        >>> )\n\n    \"\"\"\n\n    def __init__(self,\n                 input_size: int = 224,\n                 cj_prob: float = 0.8,\n                 cj_strength: float = 0.5,\n                 min_scale: float = 0.08,\n                 random_gray_scale: float = 0.2,\n                 gaussian_blur: float = 0.5,\n                 kernel_size: float = 0.1,\n                 vf_prob: float = 0.0,\n                 hf_prob: float = 0.5,\n                 rr_prob: float = 0.0,\n                 normalize: dict = imagenet_normalize):\n\n        super(SimCLRCollateFunction, self).__init__(\n            input_size=input_size,\n            cj_prob=cj_prob,\n            cj_bright=cj_strength * 0.8,\n            cj_contrast=cj_strength * 0.8,\n            cj_sat=cj_strength * 0.8,\n            cj_hue=cj_strength * 0.2,\n            min_scale=min_scale,\n            random_gray_scale=random_gray_scale,\n            gaussian_blur=gaussian_blur,\n            kernel_size=kernel_size,\n            vf_prob=vf_prob,\n            hf_prob=hf_prob,\n            rr_prob=rr_prob,\n            normalize=normalize,\n        )",
  "class MoCoCollateFunction(ImageCollateFunction):\n    \"\"\"Implements the transformations for MoCo v1.\n\n    For MoCo v2, simply use the SimCLR settings.\n\n    Attributes:\n        input_size:\n            Size of the input image in pixels.\n        cj_prob:\n            Probability that color jitter is applied.\n        cj_strength:\n            Strength of the color jitter.\n        min_scale:\n            Minimum size of the randomized crop relative to the input_size.\n        random_gray_scale:\n            Probability of conversion to grayscale.\n        gaussian_blur:\n            Probability of Gaussian blur.\n        kernel_size:\n            Sigma of gaussian blur is kernel_size * input_size.\n        vf_prob:\n            Probability that vertical flip is applied.\n        hf_prob:\n            Probability that horizontal flip is applied.\n        rr_prob:\n            Probability that random (+90 degree) rotation is applied.\n        normalize:\n            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.\n\n    Examples:\n\n        >>> # MoCo v1 for ImageNet\n        >>> collate_fn = MoCoCollateFunction()\n        >>> \n        >>> # MoCo v1 for CIFAR-10\n        >>> collate_fn = MoCoCollateFunction(\n        >>>     input_size=32,\n        >>> )\n\n    \"\"\"\n\n    def __init__(self,\n                 input_size: int = 224,\n                 cj_prob: float = 0.8,\n                 cj_strength: float = 0.4,\n                 min_scale: float = 0.2,\n                 random_gray_scale: float = 0.2,\n                 gaussian_blur: float = 0.,\n                 kernel_size: float = 0.1,\n                 vf_prob: float = 0.0,\n                 hf_prob: float = 0.5,\n                 rr_prob: float = 0.0,\n                 normalize: dict = imagenet_normalize):\n\n        super(MoCoCollateFunction, self).__init__(\n            input_size=input_size,\n            cj_prob=cj_prob,\n            cj_bright=cj_strength,\n            cj_contrast=cj_strength,\n            cj_sat=cj_strength,\n            cj_hue=cj_strength,\n            min_scale=min_scale,\n            random_gray_scale=random_gray_scale,\n            gaussian_blur=gaussian_blur,\n            kernel_size=kernel_size,\n            vf_prob=vf_prob,\n            hf_prob=hf_prob,\n            rr_prob=rr_prob,\n            normalize=imagenet_normalize,\n        )",
  "def __init__(self, transform: torchvision.transforms.Compose):\n\n        super(BaseCollateFunction, self).__init__()\n        self.transform = transform",
  "def forward(self, batch: List[tuple]):\n        \"\"\"Turns a batch of tuples into a tuple of batches.\n\n            Args:\n                batch:\n                    A batch of tuples of images, labels, and filenames which\n                    is automatically provided if the dataloader is built from \n                    a LightlyDataset.\n\n            Returns:\n                A tuple of images, labels, and filenames. The images consist of \n                two batches corresponding to the two transformations of the\n                input images.\n\n            Examples:\n                >>> # define a random transformation and the collate function\n                >>> transform = ... # some random augmentations\n                >>> collate_fn = BaseCollateFunction(transform)\n                >>>\n                >>> # input is a batch of tuples (here, batch_size = 1)\n                >>> input = [(img, 0, 'my-image.png')]\n                >>> output = collate_fn(input)\n                >>>\n                >>> # output consists of two random transforms of the images,\n                >>> # the labels, and the filenames in the batch\n                >>> (img_t0, img_t1), label, filename = output\n\n        \"\"\"\n        batch_size = len(batch)\n\n        # list of transformed images\n        transforms = [self.transform(batch[i % batch_size][0]).unsqueeze_(0)\n                      for i in range(2 * batch_size)]\n        # list of labels\n        labels = torch.LongTensor([item[1] for item in batch])\n        # list of filenames\n        fnames = [item[2] for item in batch]\n\n        # tuple of transforms\n        transforms = (\n            torch.cat(transforms[:batch_size], 0),\n            torch.cat(transforms[batch_size:], 0)\n        )\n\n        return transforms, labels, fnames",
  "def __init__(self,\n                 input_size: int = 64,\n                 cj_prob: float = 0.8,\n                 cj_bright: float = 0.7,\n                 cj_contrast: float = 0.7,\n                 cj_sat: float = 0.7,\n                 cj_hue: float = 0.2,\n                 min_scale: float = 0.15,\n                 random_gray_scale: float = 0.2,\n                 gaussian_blur: float = 0.5,\n                 kernel_size: float = 0.1,\n                 vf_prob: float = 0.0,\n                 hf_prob: float = 0.5,\n                 rr_prob: float = 0.0,\n                 normalize: dict = imagenet_normalize):\n\n        if isinstance(input_size, tuple):\n            input_size_ = max(input_size)\n        else:\n            input_size_ = input_size\n\n        color_jitter = T.ColorJitter(\n            cj_bright, cj_contrast, cj_sat, cj_hue\n        )\n\n        transform = [T.RandomResizedCrop(size=input_size,\n                                         scale=(min_scale, 1.0)),\n             RandomRotate(prob=rr_prob),\n             T.RandomHorizontalFlip(p=hf_prob),\n             T.RandomVerticalFlip(p=vf_prob),\n             T.RandomApply([color_jitter], p=cj_prob),\n             T.RandomGrayscale(p=random_gray_scale),\n             GaussianBlur(\n                 kernel_size=kernel_size * input_size_,\n                 prob=gaussian_blur),\n             T.ToTensor()\n        ]\n\n        if normalize:\n            transform += [\n             T.Normalize(\n                mean=normalize['mean'],\n                std=normalize['std'])\n             ]\n           \n        transform = T.Compose(transform)\n\n        super(ImageCollateFunction, self).__init__(transform)",
  "def __init__(self,\n                 input_size: int = 224,\n                 cj_prob: float = 0.8,\n                 cj_strength: float = 0.5,\n                 min_scale: float = 0.08,\n                 random_gray_scale: float = 0.2,\n                 gaussian_blur: float = 0.5,\n                 kernel_size: float = 0.1,\n                 vf_prob: float = 0.0,\n                 hf_prob: float = 0.5,\n                 rr_prob: float = 0.0,\n                 normalize: dict = imagenet_normalize):\n\n        super(SimCLRCollateFunction, self).__init__(\n            input_size=input_size,\n            cj_prob=cj_prob,\n            cj_bright=cj_strength * 0.8,\n            cj_contrast=cj_strength * 0.8,\n            cj_sat=cj_strength * 0.8,\n            cj_hue=cj_strength * 0.2,\n            min_scale=min_scale,\n            random_gray_scale=random_gray_scale,\n            gaussian_blur=gaussian_blur,\n            kernel_size=kernel_size,\n            vf_prob=vf_prob,\n            hf_prob=hf_prob,\n            rr_prob=rr_prob,\n            normalize=normalize,\n        )",
  "def __init__(self,\n                 input_size: int = 224,\n                 cj_prob: float = 0.8,\n                 cj_strength: float = 0.4,\n                 min_scale: float = 0.2,\n                 random_gray_scale: float = 0.2,\n                 gaussian_blur: float = 0.,\n                 kernel_size: float = 0.1,\n                 vf_prob: float = 0.0,\n                 hf_prob: float = 0.5,\n                 rr_prob: float = 0.0,\n                 normalize: dict = imagenet_normalize):\n\n        super(MoCoCollateFunction, self).__init__(\n            input_size=input_size,\n            cj_prob=cj_prob,\n            cj_bright=cj_strength,\n            cj_contrast=cj_strength,\n            cj_sat=cj_strength,\n            cj_hue=cj_strength,\n            min_scale=min_scale,\n            random_gray_scale=random_gray_scale,\n            gaussian_blur=gaussian_blur,\n            kernel_size=kernel_size,\n            vf_prob=vf_prob,\n            hf_prob=hf_prob,\n            rr_prob=rr_prob,\n            normalize=imagenet_normalize,\n        )",
  "def pil_loader(path):\n    # open path as file to avoid ResourceWarning\n    # (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, 'rb') as f:\n        img = Image.open(f)\n        return img.convert('RGB')",
  "def accimage_loader(path):\n    try:\n        import accimage\n        return accimage.Image(path)\n    except IOError:\n        # Potentially a decoding problem, fall back to PIL.Image\n        return pil_loader(path)",
  "def default_loader(path):\n    from torchvision import get_image_backend\n    if get_image_backend() == 'accimage':\n        return accimage_loader(path)\n    else:\n        return pil_loader(path)",
  "def _make_dataset(directory, extensions=None, is_valid_file=None):\n    \"\"\"Returns a list of all image files with targets in the directory.\n\n    Args:\n        directory:\n            Root directory path (should not contain subdirectories!).\n        extensions:\n            Tuple of valid extensions.\n        is_valid_file:\n            Used to find valid files.\n\n    Returns:\n        List of instance tuples: (path_i, target_i = 0).\n\n    \"\"\"\n\n    # handle is_valid_file and extensions the same way torchvision handles them:\n    # https://pytorch.org/docs/stable/_modules/torchvision/datasets/folder.html#ImageFolder\n    both_none = extensions is None and is_valid_file is None\n    both_something = extensions is not None and is_valid_file is not None\n    if both_none or both_something:\n        raise ValueError('Both extensions and is_valid_file cannot be None or '\n                         'not None at the same time')\n\n    if extensions is not None:\n        def _is_valid_file(filename):\n            return filename.lower().endswith(extensions)\n\n    if is_valid_file is not None:\n        _is_valid_file = is_valid_file\n\n    instances = []\n    for fname in os.listdir(directory):\n\n        if not _is_valid_file(fname):\n            continue\n\n        path = os.path.join(directory, fname)\n        item = (path, 0)\n        instances.append(item)\n\n    return instances",
  "class DatasetFolder(datasets.VisionDataset):\n    \"\"\"Implements a dataset folder.\n    \n    DatasetFolder based on torchvisions implementation.\n    (https://pytorch.org/docs/stable/torchvision/datasets.html#datasetfolder)\n\n    Attributes:\n        root:\n            Root directory path\n        loader:\n            Function that loads file at path\n        extensions:\n            Tuple of allowed extensions\n        transform:\n            Function that takes a PIL image and returns transformed version\n        target_transform:\n            As transform but for targets\n        is_valid_file:\n            Used to check corrupt files\n\n    Raises:\n        RuntimeError: If no supported files are found in root.\n\n    \"\"\"\n\n    def __init__(self,\n                 root: str,\n                 loader=default_loader,\n                 extensions=None,\n                 transform=None,\n                 target_transform=None,\n                 is_valid_file=None):\n\n        super(DatasetFolder, self).__init__(root,\n                                            transform=transform,\n                                            target_transform=target_transform)\n\n        samples = _make_dataset(self.root, extensions, is_valid_file)\n        if len(samples) == 0:\n            msg = 'Found 0 files in folder: {}\\n'.format(self.root)\n            if extensions is not None:\n                msg += 'Supported extensions are: {}'.format(\n                    ','.join(extensions))\n            raise RuntimeError(msg)\n\n        self.loader = loader\n        self.extensions = extensions\n\n        self.samples = samples\n        self.targets = [s[1] for s in samples]\n\n    def __getitem__(self, index: int):\n        \"\"\"Returns item at index.\n\n        Args:\n            index:\n                Index of the sample to retrieve.\n\n        Returns:\n            A tuple (sample, target) where target is 0.\n\n        \"\"\"\n\n        path, target = self.samples[index]\n        sample = self.loader(path)\n        if self.transform is not None:\n            sample = self.transform(sample)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return sample, target\n\n    def __len__(self):\n        \"\"\"Returns the number of samples in the dataset.\n\n        \"\"\"\n        return len(self.samples)",
  "def __init__(self,\n                 root: str,\n                 loader=default_loader,\n                 extensions=None,\n                 transform=None,\n                 target_transform=None,\n                 is_valid_file=None):\n\n        super(DatasetFolder, self).__init__(root,\n                                            transform=transform,\n                                            target_transform=target_transform)\n\n        samples = _make_dataset(self.root, extensions, is_valid_file)\n        if len(samples) == 0:\n            msg = 'Found 0 files in folder: {}\\n'.format(self.root)\n            if extensions is not None:\n                msg += 'Supported extensions are: {}'.format(\n                    ','.join(extensions))\n            raise RuntimeError(msg)\n\n        self.loader = loader\n        self.extensions = extensions\n\n        self.samples = samples\n        self.targets = [s[1] for s in samples]",
  "def __getitem__(self, index: int):\n        \"\"\"Returns item at index.\n\n        Args:\n            index:\n                Index of the sample to retrieve.\n\n        Returns:\n            A tuple (sample, target) where target is 0.\n\n        \"\"\"\n\n        path, target = self.samples[index]\n        sample = self.loader(path)\n        if self.transform is not None:\n            sample = self.transform(sample)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return sample, target",
  "def __len__(self):\n        \"\"\"Returns the number of samples in the dataset.\n\n        \"\"\"\n        return len(self.samples)",
  "def _is_valid_file(filename):\n            return filename.lower().endswith(extensions)",
  "def check_images(data_dir: str) -> Tuple[List[str], List[str]]:\n    '''Iterate through a directory of images and find corrupt images\n\n    Args:\n        data_dir: Path to the directory containing the images\n\n    Returns:\n        (healthy_images, corrupt_images)\n    '''\n    dataset = LightlyDataset(input_dir=data_dir)\n    filenames = dataset.get_filenames()\n\n    def _is_corrupt(filename):\n        try:\n            image = Image.open(\n                os.path.join(data_dir, filename)\n            )\n            image.load()\n        except (IOError, UnidentifiedImageError):\n            return True\n        else:\n            return False\n\n    mapped = concurrent.thread_map(\n        _is_corrupt,\n        filenames,\n        chunksize=min(32, len(filenames))\n    )\n    healthy_images = [f for f, is_corrupt\n                      in zip(filenames, mapped) if not is_corrupt]\n    corrupt_images = [f for f, is_corrupt\n                      in zip(filenames, mapped) if is_corrupt]\n    return healthy_images, corrupt_images",
  "def _is_corrupt(filename):\n        try:\n            image = Image.open(\n                os.path.join(data_dir, filename)\n            )\n            image.load()\n        except (IOError, UnidentifiedImageError):\n            return True\n        else:\n            return False",
  "class LightlySubset(LightlyDataset):\n    def __init__(self, base_dataset: LightlyDataset, filenames_subset: List[str]):\n        \"\"\"Creates a subset of a LightlyDataset.\n\n        Args:\n            base_dataset:\n                The dataset to subset from.\n            filenames_subset:\n                The filenames of the samples to be part of the subset.\n        \"\"\"\n        self.base_dataset = base_dataset\n        self.filenames_subset = filenames_subset\n\n        dict_base_dataset_filename_index: Dict[str, int] = dict()\n        for index in range(len(base_dataset)):\n            fname = base_dataset.index_to_filename(self.dataset, index)\n            dict_base_dataset_filename_index[fname] = index\n\n        self.mapping_subset_index_to_baseset_index = \\\n            [dict_base_dataset_filename_index[filename] for filename in filenames_subset]\n\n    def __getitem__(self, index_subset: int) -> Tuple[object, object, str]:\n        \"\"\"An overwrite for indexing.\n\n        Args:\n            index_subset:\n                The index of a sample w.r.t. to the subset.\n                E.g. if index_subset == 0, the sample belonging to\n                the first filename in self.filenames_subset is returned.\n\n        Returns:\n            A tuple of the sample, its target and its filename.\n\n        \"\"\"\n        index_baseset = self.mapping_subset_index_to_baseset_index[index_subset]\n        sample, target, fname = self.base_dataset.__getitem__(index_baseset)\n        return sample, target, fname\n\n    def __len__(self) -> int:\n        \"\"\"Overwrites the len(...) function.\n\n        Returns:\n            The number of samples in the subset.\n        \"\"\"\n        return len(self.filenames_subset)\n\n    def index_to_filename(self, dataset, index_subset: int):\n        \"\"\"Maps from an index of a sample to its filename.\n\n        Args:\n            dataset:\n                Unused, but specified by the overwritten\n                function of the parent class.\n            index_subset:\n                The index of the sample w.r.t. the subset.\n\n        Returns:\n            The filename of the sample.\n        \"\"\"\n        fname = self.filenames_subset[index_subset]\n        return fname\n\n    @property\n    def input_dir(self):\n        return self.base_dataset.input_dir\n\n    @property\n    def dataset(self):\n        return self.base_dataset.dataset",
  "def __init__(self, base_dataset: LightlyDataset, filenames_subset: List[str]):\n        \"\"\"Creates a subset of a LightlyDataset.\n\n        Args:\n            base_dataset:\n                The dataset to subset from.\n            filenames_subset:\n                The filenames of the samples to be part of the subset.\n        \"\"\"\n        self.base_dataset = base_dataset\n        self.filenames_subset = filenames_subset\n\n        dict_base_dataset_filename_index: Dict[str, int] = dict()\n        for index in range(len(base_dataset)):\n            fname = base_dataset.index_to_filename(self.dataset, index)\n            dict_base_dataset_filename_index[fname] = index\n\n        self.mapping_subset_index_to_baseset_index = \\\n            [dict_base_dataset_filename_index[filename] for filename in filenames_subset]",
  "def __getitem__(self, index_subset: int) -> Tuple[object, object, str]:\n        \"\"\"An overwrite for indexing.\n\n        Args:\n            index_subset:\n                The index of a sample w.r.t. to the subset.\n                E.g. if index_subset == 0, the sample belonging to\n                the first filename in self.filenames_subset is returned.\n\n        Returns:\n            A tuple of the sample, its target and its filename.\n\n        \"\"\"\n        index_baseset = self.mapping_subset_index_to_baseset_index[index_subset]\n        sample, target, fname = self.base_dataset.__getitem__(index_baseset)\n        return sample, target, fname",
  "def __len__(self) -> int:\n        \"\"\"Overwrites the len(...) function.\n\n        Returns:\n            The number of samples in the subset.\n        \"\"\"\n        return len(self.filenames_subset)",
  "def index_to_filename(self, dataset, index_subset: int):\n        \"\"\"Maps from an index of a sample to its filename.\n\n        Args:\n            dataset:\n                Unused, but specified by the overwritten\n                function of the parent class.\n            index_subset:\n                The index of the sample w.r.t. the subset.\n\n        Returns:\n            The filename of the sample.\n        \"\"\"\n        fname = self.filenames_subset[index_subset]\n        return fname",
  "def input_dir(self):\n        return self.base_dataset.input_dir",
  "def dataset(self):\n        return self.base_dataset.dataset",
  "class VideoLoader():\n    \"\"\"Implementation of VideoLoader.\n\n    The VideoLoader is a wrapper around the torchvision video interface. With\n    the VideoLoader you can read specific frames or the next frames of a video.\n    It automatically switches to the `video_loader` backend if available. Reading\n    sequential frames is significantly faster since it uses the VideoReader \n    class from torchvision.\n\n    The video loader automatically detects if you read out subsequent frames and\n    will use the fast read method if possible. \n\n    Attributes:\n        path:\n            Root directory path.\n        timestamps:\n            Function that loads file at path.\n        backend:\n            Tuple of allowed extensions.\n        transform:\n            Function that takes a PIL image and returns transformed version\n        target_transform:\n            As transform but for targets\n        is_valid_file:\n            Used to check corrupt files\n\n    Examples:\n        >>> from torchvision import io\n        >>>\n        >>> # get timestamps\n        >>> ts, fps = io.read_video_timestamps('myvideo.mp4', pts_unit = 'sec')\n        >>>\n        >>> # create a VideoLoader\n        >>> video_loader = VideoLoader('myvideo.mp4', ts)\n        >>>\n        >>> # get frame at specific timestamp\n        >>> frame = video_loader.read_frame(ts[21])\n        >>>\n        >>> # get next frame\n        >>> frame = video_loader.read_frame()\n    \"\"\"\n    def __init__(self, path: str, timestamps: List[float], backend: str = 'video_reader'):\n        self.path = path\n        self.timestamps = timestamps\n        self.current_timestamp_idx = 0\n        self.last_timestamp_idx = 0\n        self.pts_unit='sec'\n        self.backend = backend\n\n        has_video_reader = io._HAS_VIDEO_OPT and hasattr(io, 'VideoReader')\n\n        if has_video_reader and self.backend == 'video_reader':\n            self.reader = io.VideoReader(path = self.path)\n        else:\n            self.reader = None\n    \n    def read_frame(self, timestamp = None):\n        \"\"\"Reads the next frame or from timestamp.\n\n        If no timestamp is provided this method just returns the next frame from\n        the video. This is significantly (up to 10x) faster if the `video_loader` \n        backend is available. If a timestamp is provided we first have to seek\n        to the right position and then load the frame.\n        \n        Args:\n            timestamp: Specific timestamp of frame in seconds or None (default: None)\n\n        Returns:\n            A PIL Image\n\n        \"\"\"\n        if timestamp is not None:\n            self.current_timestamp_idx = self.timestamps.index(timestamp)\n        else:\n            # no timestamp provided -> set current timestamp index to next frame\n            if self.current_timestamp_idx < len(self.timestamps):\n                self.current_timestamp_idx += 1\n\n        if self.reader:\n            if timestamp is not None:\n                # Calling seek is slow. If we read next frame we can skip it!\n                if self.timestamps.index(timestamp) != self.last_timestamp_idx + 1:\n                    self.reader.seek(timestamp)\n\n            # make sure we have the tensor in correct shape (we want H x W x C)\n            frame = next(self.reader)['data'].permute(1,2,0)\n            self.last_timestamp_idx = self.current_timestamp_idx\n\n        else: # fallback on pyav\n            if timestamp is None:\n                # read next frame if no timestamp is provided\n                timestamp = self.timestamps[self.current_timestamp_idx]\n            frame, _, _ = io.read_video(self.path,\n                                        start_pts=timestamp,\n                                        end_pts=timestamp,\n                                        pts_unit=self.pts_unit)    \n            self.last_timestamp_idx = self.timestamps.index(timestamp)    \n        \n        \n        if len(frame.shape) < 3:\n            raise ValueError('Unexpected error during loading of frame')\n\n        # sometimes torchvision returns multiple frames for one timestamp (bug?)\n        if len(frame.shape) > 3 and frame.shape[0] > 1:\n            frame = frame[0]\n\n        # make sure we return a H x W x C tensor and not (1 x H x W x C)\n        if len(frame.shape) == 4:\n            frame = frame.squeeze()\n\n        # convert to PIL image\n        image = Image.fromarray(frame.numpy())\n        return image",
  "def _make_dataset(directory,\n                  extensions=None,\n                  is_valid_file=None,\n                  pts_unit='sec'):\n    \"\"\"Returns a list of all video files, timestamps, and offsets.\n\n    Args:\n        directory:\n            Root directory path (should not contain subdirectories).\n        extensions:\n            Tuple of valid extensions.\n        is_valid_file:\n            Used to find valid files.\n        pts_unit:\n            Unit of the timestamps.\n\n    Returns:\n        A list of video files, timestamps, frame offsets, and fps.\n\n    \"\"\"\n\n    # handle is_valid_file and extensions the same way torchvision handles them:\n    # https://pytorch.org/docs/stable/_modules/torchvision/datasets/folder.html#ImageFolder\n    both_none = extensions is None and is_valid_file is None\n    both_something = extensions is not None and is_valid_file is not None\n    if both_none or both_something:\n        raise ValueError('Both extensions and is_valid_file cannot be None or '\n                         'not None at the same time')\n\n    # use filename to find valid files\n    if extensions is not None:\n        def _is_valid_file(filename):\n            return filename.lower().endswith(extensions)\n\n    # overwrite function to find valid files\n    if is_valid_file is not None:\n        _is_valid_file = is_valid_file\n\n    # find all instances (no subdirectories)\n    instances = []\n    for fname in os.listdir(directory):\n\n        # skip invalid files\n        if not _is_valid_file(fname):\n            continue\n\n        # keep track of valid files\n        path = os.path.join(directory, fname)\n        instances.append(path)\n\n    # get timestamps\n    timestamps, fpss = [], []\n    for instance in instances:\n\n        if AV_AVAILABLE and torchvision.get_video_backend() == 'pyav':\n            # This is a hacky solution to estimate the timestamps.\n            # When using the video_reader this approach fails because the \n            # estimated timestamps are not correct.\n            with av.open(instance) as av_video:\n                stream = av_video.streams.video[0]\n                duration = stream.duration * stream.time_base\n                fps = stream.base_rate\n                n_frames = int(int(duration) * fps)\n\n            timestamps.append([Fraction(i, fps) for i in range(n_frames)])\n            fpss.append(fps)\n        else:\n            ts, fps = io.read_video_timestamps(instance, pts_unit=pts_unit)\n            timestamps.append(ts)\n            fpss.append(fps)\n\n\n    # get frame offsets\n    offsets = [len(ts) for ts in timestamps]\n    offsets = [0] + offsets[:-1]\n    for i in range(1, len(offsets)):\n        offsets[i] = offsets[i-1] + offsets[i] # cumsum\n\n    return instances, timestamps, offsets, fpss",
  "class VideoDataset(datasets.VisionDataset):\n    \"\"\"Implementation of a video dataset.\n\n    The VideoDataset allows random reads from a video file without extracting\n    all frames beforehand. This is more storage efficient but is slower.\n\n    Attributes:\n        root:\n            Root directory path.\n        extensions:\n            Tuple of allowed extensions.\n        transform:\n            Function that takes a PIL image and returns transformed version\n        target_transform:\n            As transform but for targets\n        is_valid_file:\n            Used to check corrupt files\n\n    \"\"\"\n\n    def __init__(self,\n                 root,\n                 extensions=None,\n                 transform=None,\n                 target_transform=None,\n                 is_valid_file=None):\n        \n        super(VideoDataset, self).__init__(root,\n                                           transform=transform,\n                                           target_transform=target_transform)\n\n        videos, video_timestamps, offsets, fps = _make_dataset(\n            self.root, extensions, is_valid_file)\n        \n        if len(videos) == 0:\n            msg = 'Found 0 videos in folder: {}\\n'.format(self.root)\n            if extensions is not None:\n                msg += 'Supported extensions are: {}'.format(\n                    ','.join(extensions))\n            raise RuntimeError(msg)\n\n        self.extensions = extensions\n\n        backend = torchvision.get_video_backend()\n        self.video_loaders = \\\n            [VideoLoader(video, timestamps, backend=backend) for video, timestamps in zip(videos, video_timestamps)]\n\n        self.videos = videos\n        self.video_timestamps = video_timestamps\n        # offsets[i] indicates the index of the first frame of the i-th video.\n        # e.g. for two videos of length 10 and 20, the offsets will be [0, 10].\n        self.offsets = offsets\n        self.fps = fps\n\n    def __getitem__(self, index):\n        \"\"\"Returns item at index.\n\n        Finds the video of the frame at index with the help of the frame \n        offsets. Then, loads the frame from the video, applies the transforms,\n        and returns the frame along with the index of the video (as target).\n\n        For example, if there are two videos with 10 and 20 frames respectively\n        in the input directory:\n\n        Requesting the 5th sample returns the 5th frame from the first video and\n        the target indicates the index of the source video which is 0.\n        >>> dataset[5]\n        >>> > <PIL Image>, 0\n\n        Requesting the 20th sample returns the 10th frame from the second video\n        and the target indicates the index of the source video which is 1.\n        >>> dataset[20]\n        >>> > <PIL Image>, 1\n\n        Args:\n            index:\n                Index of the sample to retrieve.\n\n        Returns:\n            A tuple (sample, target) where target indicates the video index.\n\n        Raises:\n            IndexError if index is out of bounds.\n\n        \"\"\"\n        if index < 0 or index >= self.__len__():\n            raise IndexError(f'Index {index} is out of bounds for VideoDataset'\n                             f' of size {self.__len__()}.')\n\n        # each sample belongs to a video, to load the sample at index, we need\n        # to find the video to which the sample belongs and then read the frame\n        # from this video on the disk.\n        i = len(self.offsets) - 1\n        while (self.offsets[i] > index):\n            i = i - 1\n\n        # find and return the frame as PIL image\n        timestamp_idx = index - self.offsets[i]\n        frame_timestamp = self.video_timestamps[i][timestamp_idx]\n        sample = self.video_loaders[i].read_frame(frame_timestamp)\n\n        target = i\n        if self.transform is not None:\n            sample = self.transform(sample)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return sample, target\n\n    def __len__(self):\n        \"\"\"Returns the number of samples (frames) in the dataset.\n\n        \"\"\"\n        return sum((len(ts) for ts in self.video_timestamps))\n\n    def get_filename(self, index):\n        \"\"\"Returns a filename for the frame at index.\n\n        The filename is created from the video filename, the frame number, and\n        the video format. The frame number will be zero padded to make sure \n        all filenames have the same length and can easily be sorted.\n        E.g. when retrieving a sample from the video\n        `my_video.mp4` at frame 153, the filename will be:\n\n        >>> my_video-153-mp4.png\n    \n        Args:\n            index:\n                Index of the frame to retrieve.\n\n        Returns:\n            The filename of the frame as described above.\n                \n        \"\"\"\n        if index < 0 or index >= self.__len__():\n            raise IndexError(f'Index {index} is out of bounds for VideoDataset'\n                             f' of size {self.__len__()}.')\n    \n        # each sample belongs to a video, to load the sample at index, we need\n        # to find the video to which the sample belongs and then read the frame\n        # from this video on the disk.\n        i = len(self.offsets) - 1\n        while (self.offsets[i] > index):\n            i = i - 1\n\n        # get filename of the video file\n        filename = self.videos[i]\n        filename = os.path.relpath(filename, self.root)\n\n        # get video format and video name\n        splits = filename.split('.')\n        video_format = splits[-1]\n        video_name = '.'.join(splits[:-1])\n\n        # get frame number\n        frame_number = index - self.offsets[i]\n        if i < len(self.offsets) - 1:\n            n_frames = self.offsets[i+1] - self.offsets[i]\n        else:\n            n_frames = self.__len__() - self.offsets[i]\n        \n        return f'{video_name}-{frame_number:0{len(str(n_frames))}}-{video_format}.png'",
  "def __init__(self, path: str, timestamps: List[float], backend: str = 'video_reader'):\n        self.path = path\n        self.timestamps = timestamps\n        self.current_timestamp_idx = 0\n        self.last_timestamp_idx = 0\n        self.pts_unit='sec'\n        self.backend = backend\n\n        has_video_reader = io._HAS_VIDEO_OPT and hasattr(io, 'VideoReader')\n\n        if has_video_reader and self.backend == 'video_reader':\n            self.reader = io.VideoReader(path = self.path)\n        else:\n            self.reader = None",
  "def read_frame(self, timestamp = None):\n        \"\"\"Reads the next frame or from timestamp.\n\n        If no timestamp is provided this method just returns the next frame from\n        the video. This is significantly (up to 10x) faster if the `video_loader` \n        backend is available. If a timestamp is provided we first have to seek\n        to the right position and then load the frame.\n        \n        Args:\n            timestamp: Specific timestamp of frame in seconds or None (default: None)\n\n        Returns:\n            A PIL Image\n\n        \"\"\"\n        if timestamp is not None:\n            self.current_timestamp_idx = self.timestamps.index(timestamp)\n        else:\n            # no timestamp provided -> set current timestamp index to next frame\n            if self.current_timestamp_idx < len(self.timestamps):\n                self.current_timestamp_idx += 1\n\n        if self.reader:\n            if timestamp is not None:\n                # Calling seek is slow. If we read next frame we can skip it!\n                if self.timestamps.index(timestamp) != self.last_timestamp_idx + 1:\n                    self.reader.seek(timestamp)\n\n            # make sure we have the tensor in correct shape (we want H x W x C)\n            frame = next(self.reader)['data'].permute(1,2,0)\n            self.last_timestamp_idx = self.current_timestamp_idx\n\n        else: # fallback on pyav\n            if timestamp is None:\n                # read next frame if no timestamp is provided\n                timestamp = self.timestamps[self.current_timestamp_idx]\n            frame, _, _ = io.read_video(self.path,\n                                        start_pts=timestamp,\n                                        end_pts=timestamp,\n                                        pts_unit=self.pts_unit)    \n            self.last_timestamp_idx = self.timestamps.index(timestamp)    \n        \n        \n        if len(frame.shape) < 3:\n            raise ValueError('Unexpected error during loading of frame')\n\n        # sometimes torchvision returns multiple frames for one timestamp (bug?)\n        if len(frame.shape) > 3 and frame.shape[0] > 1:\n            frame = frame[0]\n\n        # make sure we return a H x W x C tensor and not (1 x H x W x C)\n        if len(frame.shape) == 4:\n            frame = frame.squeeze()\n\n        # convert to PIL image\n        image = Image.fromarray(frame.numpy())\n        return image",
  "def __init__(self,\n                 root,\n                 extensions=None,\n                 transform=None,\n                 target_transform=None,\n                 is_valid_file=None):\n        \n        super(VideoDataset, self).__init__(root,\n                                           transform=transform,\n                                           target_transform=target_transform)\n\n        videos, video_timestamps, offsets, fps = _make_dataset(\n            self.root, extensions, is_valid_file)\n        \n        if len(videos) == 0:\n            msg = 'Found 0 videos in folder: {}\\n'.format(self.root)\n            if extensions is not None:\n                msg += 'Supported extensions are: {}'.format(\n                    ','.join(extensions))\n            raise RuntimeError(msg)\n\n        self.extensions = extensions\n\n        backend = torchvision.get_video_backend()\n        self.video_loaders = \\\n            [VideoLoader(video, timestamps, backend=backend) for video, timestamps in zip(videos, video_timestamps)]\n\n        self.videos = videos\n        self.video_timestamps = video_timestamps\n        # offsets[i] indicates the index of the first frame of the i-th video.\n        # e.g. for two videos of length 10 and 20, the offsets will be [0, 10].\n        self.offsets = offsets\n        self.fps = fps",
  "def __getitem__(self, index):\n        \"\"\"Returns item at index.\n\n        Finds the video of the frame at index with the help of the frame \n        offsets. Then, loads the frame from the video, applies the transforms,\n        and returns the frame along with the index of the video (as target).\n\n        For example, if there are two videos with 10 and 20 frames respectively\n        in the input directory:\n\n        Requesting the 5th sample returns the 5th frame from the first video and\n        the target indicates the index of the source video which is 0.\n        >>> dataset[5]\n        >>> > <PIL Image>, 0\n\n        Requesting the 20th sample returns the 10th frame from the second video\n        and the target indicates the index of the source video which is 1.\n        >>> dataset[20]\n        >>> > <PIL Image>, 1\n\n        Args:\n            index:\n                Index of the sample to retrieve.\n\n        Returns:\n            A tuple (sample, target) where target indicates the video index.\n\n        Raises:\n            IndexError if index is out of bounds.\n\n        \"\"\"\n        if index < 0 or index >= self.__len__():\n            raise IndexError(f'Index {index} is out of bounds for VideoDataset'\n                             f' of size {self.__len__()}.')\n\n        # each sample belongs to a video, to load the sample at index, we need\n        # to find the video to which the sample belongs and then read the frame\n        # from this video on the disk.\n        i = len(self.offsets) - 1\n        while (self.offsets[i] > index):\n            i = i - 1\n\n        # find and return the frame as PIL image\n        timestamp_idx = index - self.offsets[i]\n        frame_timestamp = self.video_timestamps[i][timestamp_idx]\n        sample = self.video_loaders[i].read_frame(frame_timestamp)\n\n        target = i\n        if self.transform is not None:\n            sample = self.transform(sample)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return sample, target",
  "def __len__(self):\n        \"\"\"Returns the number of samples (frames) in the dataset.\n\n        \"\"\"\n        return sum((len(ts) for ts in self.video_timestamps))",
  "def get_filename(self, index):\n        \"\"\"Returns a filename for the frame at index.\n\n        The filename is created from the video filename, the frame number, and\n        the video format. The frame number will be zero padded to make sure \n        all filenames have the same length and can easily be sorted.\n        E.g. when retrieving a sample from the video\n        `my_video.mp4` at frame 153, the filename will be:\n\n        >>> my_video-153-mp4.png\n    \n        Args:\n            index:\n                Index of the frame to retrieve.\n\n        Returns:\n            The filename of the frame as described above.\n                \n        \"\"\"\n        if index < 0 or index >= self.__len__():\n            raise IndexError(f'Index {index} is out of bounds for VideoDataset'\n                             f' of size {self.__len__()}.')\n    \n        # each sample belongs to a video, to load the sample at index, we need\n        # to find the video to which the sample belongs and then read the frame\n        # from this video on the disk.\n        i = len(self.offsets) - 1\n        while (self.offsets[i] > index):\n            i = i - 1\n\n        # get filename of the video file\n        filename = self.videos[i]\n        filename = os.path.relpath(filename, self.root)\n\n        # get video format and video name\n        splits = filename.split('.')\n        video_format = splits[-1]\n        video_name = '.'.join(splits[:-1])\n\n        # get frame number\n        frame_number = index - self.offsets[i]\n        if i < len(self.offsets) - 1:\n            n_frames = self.offsets[i+1] - self.offsets[i]\n        else:\n            n_frames = self.__len__() - self.offsets[i]\n        \n        return f'{video_name}-{frame_number:0{len(str(n_frames))}}-{video_format}.png'",
  "def _is_valid_file(filename):\n            return filename.lower().endswith(extensions)",
  "def _contains_videos(root: str, extensions: tuple):\n    \"\"\"Checks whether directory contains video files.\n\n    Args:\n        root: Root directory path.\n\n    Returns:\n        True if root contains subdirectories else false.\n    \"\"\"\n    list_dir = os.listdir(root)\n    is_video = \\\n        [f.lower().endswith(extensions) for f in list_dir]\n    return any(is_video)",
  "def _is_lightly_output_dir(dirname: str):\n    \"\"\"Checks whether the directory is a lightly_output directory.\n\n    Args:\n        dirname: Directory to check.\n\n    Returns:\n        True if dirname is \"lightly_outputs\" else false.\n\n    \"\"\"\n    return 'lightly_outputs' in dirname",
  "def _contains_subdirs(root: str):\n    \"\"\"Checks whether directory contains subdirectories.\n\n    Args:\n        root: Root directory path.\n\n    Returns:\n        True if root contains subdirectories else false.\n\n    \"\"\"\n    list_dir = os.listdir(root)\n    list_dir = list(filter(lambda x: not _is_lightly_output_dir(x), list_dir))\n    is_dir = \\\n        [os.path.isdir(os.path.join(root, f)) for f in list_dir]\n    return any(is_dir)",
  "def _load_dataset_from_folder(root: str, transform):\n    \"\"\"Initializes dataset from folder.\n\n    Args:\n        root: (str) Root directory path\n        transform: (torchvision.transforms.Compose) image transformations\n\n    Returns:\n        Dataset consisting of images in the root directory.\n\n    \"\"\"\n\n    # if there is a video in the input directory but we do not have\n    # the right dependencies, raise a ValueError\n    contains_videos = _contains_videos(root, VIDEO_EXTENSIONS)\n    if contains_videos and not VIDEO_DATASET_AVAILABLE:\n        raise ValueError(f'The input directory {root} contains videos '\n                         'but the VideoDataset is not available. \\n'\n                         'Make sure you have installed the right '\n                         'dependencies. The error from the imported '\n                         f'module was: {VIDEO_DATASET_ERRORMSG}')\n\n    if contains_videos:\n        # root contains videos -> create a video dataset\n        dataset = VideoDataset(root,\n                               extensions=VIDEO_EXTENSIONS,\n                               transform=transform)\n    elif _contains_subdirs(root):\n        # root contains subdirectories -> create an image folder dataset\n        dataset = datasets.ImageFolder(root,\n                                       transform=transform)\n    else:\n        # root contains plain images -> create a folder dataset\n        dataset = DatasetFolder(root,\n                                extensions=IMG_EXTENSIONS,\n                                transform=transform)\n\n    return dataset",
  "def _load_dataset(input_dir: str,\n                  transform=None):\n    \"\"\"Initializes dataset from torchvision or from folder.\n\n    Args:\n        root: (str) Directory where dataset is stored\n        name: (str) Name of the dataset (e.g. cifar10, cifar100)\n        train: (bool) Use the training set\n        download: (bool) Download the dataset\n        transform: (torchvision.transforms.Compose) image transformations\n        from_folder: (str) Path to directory holding the images to load.\n\n    Returns:\n        A torchvision dataset\n\n    Raises:\n        ValueError: If the specified dataset doesn't exist\n\n    \"\"\"\n\n    if not os.path.exists(input_dir):\n        raise ValueError(f'The input directory {input_dir} does not exist!')\n\n    return _load_dataset_from_folder(input_dir, transform)",
  "class RESTResponse(io.IOBase):\n\n    def __init__(self, resp):\n        self.urllib3_response = resp\n        self.status = resp.status\n        self.reason = resp.reason\n        self.data = resp.data\n\n    def getheaders(self):\n        \"\"\"Returns a dictionary of the response headers.\"\"\"\n        return self.urllib3_response.getheaders()\n\n    def getheader(self, name, default=None):\n        \"\"\"Returns a given response header.\"\"\"\n        return self.urllib3_response.getheader(name, default)",
  "class RESTClientObject(object):\n\n    def __init__(self, configuration, pools_size=4, maxsize=None):\n        # urllib3.PoolManager will pass all kw parameters to connectionpool\n        # https://github.com/shazow/urllib3/blob/f9409436f83aeb79fbaf090181cd81b784f1b8ce/urllib3/poolmanager.py#L75  # noqa: E501\n        # https://github.com/shazow/urllib3/blob/f9409436f83aeb79fbaf090181cd81b784f1b8ce/urllib3/connectionpool.py#L680  # noqa: E501\n        # maxsize is the number of requests to host that are allowed in parallel  # noqa: E501\n        # Custom SSL certificates and client certificates: http://urllib3.readthedocs.io/en/latest/advanced-usage.html  # noqa: E501\n\n        # cert_reqs\n        if configuration.verify_ssl:\n            cert_reqs = ssl.CERT_REQUIRED\n        else:\n            cert_reqs = ssl.CERT_NONE\n\n        # ca_certs\n        if configuration.ssl_ca_cert:\n            ca_certs = configuration.ssl_ca_cert\n        else:\n            # if not set certificate file, use Mozilla's root certificates.\n            ca_certs = certifi.where()\n\n        addition_pool_args = {}\n        if configuration.assert_hostname is not None:\n            addition_pool_args['assert_hostname'] = configuration.assert_hostname  # noqa: E501\n\n        if maxsize is None:\n            if configuration.connection_pool_maxsize is not None:\n                maxsize = configuration.connection_pool_maxsize\n            else:\n                maxsize = 4\n\n        # https pool manager\n        if configuration.proxy:\n            self.pool_manager = urllib3.ProxyManager(\n                num_pools=pools_size,\n                maxsize=maxsize,\n                cert_reqs=cert_reqs,\n                ca_certs=ca_certs,\n                cert_file=configuration.cert_file,\n                key_file=configuration.key_file,\n                proxy_url=configuration.proxy,\n                **addition_pool_args\n            )\n        else:\n            self.pool_manager = urllib3.PoolManager(\n                num_pools=pools_size,\n                maxsize=maxsize,\n                cert_reqs=cert_reqs,\n                ca_certs=ca_certs,\n                cert_file=configuration.cert_file,\n                key_file=configuration.key_file,\n                **addition_pool_args\n            )\n\n    def request(self, method, url, query_params=None, headers=None,\n                body=None, post_params=None, _preload_content=True,\n                _request_timeout=None):\n        \"\"\"Perform requests.\n\n        :param method: http request method\n        :param url: http request url\n        :param query_params: query parameters in the url\n        :param headers: http request headers\n        :param body: request json body, for `application/json`\n        :param post_params: request post parameters,\n                            `application/x-www-form-urlencoded`\n                            and `multipart/form-data`\n        :param _preload_content: if False, the urllib3.HTTPResponse object will\n                                 be returned without reading/decoding response\n                                 data. Default is True.\n        :param _request_timeout: timeout setting for this request. If one\n                                 number provided, it will be total request\n                                 timeout. It can also be a pair (tuple) of\n                                 (connection, read) timeouts.\n        \"\"\"\n        method = method.upper()\n        assert method in ['GET', 'HEAD', 'DELETE', 'POST', 'PUT',\n                          'PATCH', 'OPTIONS']\n\n        if post_params and body:\n            raise ValueError(\n                \"body parameter cannot be used with post_params parameter.\"\n            )\n\n        post_params = post_params or {}\n        headers = headers or {}\n\n        timeout = None\n        if _request_timeout:\n            if isinstance(_request_timeout, (int, ) if six.PY3 else (int, long)):  # noqa: E501,F821\n                timeout = urllib3.Timeout(total=_request_timeout)\n            elif (isinstance(_request_timeout, tuple) and\n                  len(_request_timeout) == 2):\n                timeout = urllib3.Timeout(\n                    connect=_request_timeout[0], read=_request_timeout[1])\n\n        if 'Content-Type' not in headers:\n            headers['Content-Type'] = 'application/json'\n\n        try:\n            # For `POST`, `PUT`, `PATCH`, `OPTIONS`, `DELETE`\n            if method in ['POST', 'PUT', 'PATCH', 'OPTIONS', 'DELETE']:\n                if query_params:\n                    url += '?' + urlencode(query_params)\n                if re.search('json', headers['Content-Type'], re.IGNORECASE):\n                    request_body = '{}'\n                    if body is not None:\n                        request_body = json.dumps(body)\n                    r = self.pool_manager.request(\n                        method, url,\n                        body=request_body,\n                        preload_content=_preload_content,\n                        timeout=timeout,\n                        headers=headers)\n                elif headers['Content-Type'] == 'application/x-www-form-urlencoded':  # noqa: E501\n                    r = self.pool_manager.request(\n                        method, url,\n                        fields=post_params,\n                        encode_multipart=False,\n                        preload_content=_preload_content,\n                        timeout=timeout,\n                        headers=headers)\n                elif headers['Content-Type'] == 'multipart/form-data':\n                    # must del headers['Content-Type'], or the correct\n                    # Content-Type which generated by urllib3 will be\n                    # overwritten.\n                    del headers['Content-Type']\n                    r = self.pool_manager.request(\n                        method, url,\n                        fields=post_params,\n                        encode_multipart=True,\n                        preload_content=_preload_content,\n                        timeout=timeout,\n                        headers=headers)\n                # Pass a `string` parameter directly in the body to support\n                # other content types than Json when `body` argument is\n                # provided in serialized form\n                elif isinstance(body, str):\n                    request_body = body\n                    r = self.pool_manager.request(\n                        method, url,\n                        body=request_body,\n                        preload_content=_preload_content,\n                        timeout=timeout,\n                        headers=headers)\n                else:\n                    # Cannot generate the request from given parameters\n                    msg = \"\"\"Cannot prepare a request message for provided\n                             arguments. Please check that your arguments match\n                             declared content type.\"\"\"\n                    raise ApiException(status=0, reason=msg)\n            # For `GET`, `HEAD`\n            else:\n                r = self.pool_manager.request(method, url,\n                                              fields=query_params,\n                                              preload_content=_preload_content,\n                                              timeout=timeout,\n                                              headers=headers)\n        except urllib3.exceptions.SSLError as e:\n            msg = \"{0}\\n{1}\".format(type(e).__name__, str(e))\n            raise ApiException(status=0, reason=msg)\n\n        if _preload_content:\n            r = RESTResponse(r)\n\n            # In the python 3, the response.data is bytes.\n            # we need to decode it to string.\n            if six.PY3:\n                r.data = r.data.decode('utf8')\n\n            # log response body\n            logger.debug(\"response body: %s\", r.data)\n\n        if not 200 <= r.status <= 299:\n            raise ApiException(http_resp=r)\n\n        return r\n\n    def GET(self, url, headers=None, query_params=None, _preload_content=True,\n            _request_timeout=None):\n        return self.request(\"GET\", url,\n                            headers=headers,\n                            _preload_content=_preload_content,\n                            _request_timeout=_request_timeout,\n                            query_params=query_params)\n\n    def HEAD(self, url, headers=None, query_params=None, _preload_content=True,\n             _request_timeout=None):\n        return self.request(\"HEAD\", url,\n                            headers=headers,\n                            _preload_content=_preload_content,\n                            _request_timeout=_request_timeout,\n                            query_params=query_params)\n\n    def OPTIONS(self, url, headers=None, query_params=None, post_params=None,\n                body=None, _preload_content=True, _request_timeout=None):\n        return self.request(\"OPTIONS\", url,\n                            headers=headers,\n                            query_params=query_params,\n                            post_params=post_params,\n                            _preload_content=_preload_content,\n                            _request_timeout=_request_timeout,\n                            body=body)\n\n    def DELETE(self, url, headers=None, query_params=None, body=None,\n               _preload_content=True, _request_timeout=None):\n        return self.request(\"DELETE\", url,\n                            headers=headers,\n                            query_params=query_params,\n                            _preload_content=_preload_content,\n                            _request_timeout=_request_timeout,\n                            body=body)\n\n    def POST(self, url, headers=None, query_params=None, post_params=None,\n             body=None, _preload_content=True, _request_timeout=None):\n        return self.request(\"POST\", url,\n                            headers=headers,\n                            query_params=query_params,\n                            post_params=post_params,\n                            _preload_content=_preload_content,\n                            _request_timeout=_request_timeout,\n                            body=body)\n\n    def PUT(self, url, headers=None, query_params=None, post_params=None,\n            body=None, _preload_content=True, _request_timeout=None):\n        return self.request(\"PUT\", url,\n                            headers=headers,\n                            query_params=query_params,\n                            post_params=post_params,\n                            _preload_content=_preload_content,\n                            _request_timeout=_request_timeout,\n                            body=body)\n\n    def PATCH(self, url, headers=None, query_params=None, post_params=None,\n              body=None, _preload_content=True, _request_timeout=None):\n        return self.request(\"PATCH\", url,\n                            headers=headers,\n                            query_params=query_params,\n                            post_params=post_params,\n                            _preload_content=_preload_content,\n                            _request_timeout=_request_timeout,\n                            body=body)",
  "class ApiException(Exception):\n\n    def __init__(self, status=None, reason=None, http_resp=None):\n        if http_resp:\n            self.status = http_resp.status\n            self.reason = http_resp.reason\n            self.body = http_resp.data\n            self.headers = http_resp.getheaders()\n        else:\n            self.status = status\n            self.reason = reason\n            self.body = None\n            self.headers = None\n\n    def __str__(self):\n        \"\"\"Custom error messages for exception\"\"\"\n        error_message = \"({0})\\n\"\\\n                        \"Reason: {1}\\n\".format(self.status, self.reason)\n        if self.headers:\n            error_message += \"HTTP response headers: {0}\\n\".format(\n                self.headers)\n\n        if self.body:\n            error_message += \"HTTP response body: {0}\\n\".format(self.body)\n\n        return error_message",
  "def __init__(self, resp):\n        self.urllib3_response = resp\n        self.status = resp.status\n        self.reason = resp.reason\n        self.data = resp.data",
  "def getheaders(self):\n        \"\"\"Returns a dictionary of the response headers.\"\"\"\n        return self.urllib3_response.getheaders()",
  "def getheader(self, name, default=None):\n        \"\"\"Returns a given response header.\"\"\"\n        return self.urllib3_response.getheader(name, default)",
  "def __init__(self, configuration, pools_size=4, maxsize=None):\n        # urllib3.PoolManager will pass all kw parameters to connectionpool\n        # https://github.com/shazow/urllib3/blob/f9409436f83aeb79fbaf090181cd81b784f1b8ce/urllib3/poolmanager.py#L75  # noqa: E501\n        # https://github.com/shazow/urllib3/blob/f9409436f83aeb79fbaf090181cd81b784f1b8ce/urllib3/connectionpool.py#L680  # noqa: E501\n        # maxsize is the number of requests to host that are allowed in parallel  # noqa: E501\n        # Custom SSL certificates and client certificates: http://urllib3.readthedocs.io/en/latest/advanced-usage.html  # noqa: E501\n\n        # cert_reqs\n        if configuration.verify_ssl:\n            cert_reqs = ssl.CERT_REQUIRED\n        else:\n            cert_reqs = ssl.CERT_NONE\n\n        # ca_certs\n        if configuration.ssl_ca_cert:\n            ca_certs = configuration.ssl_ca_cert\n        else:\n            # if not set certificate file, use Mozilla's root certificates.\n            ca_certs = certifi.where()\n\n        addition_pool_args = {}\n        if configuration.assert_hostname is not None:\n            addition_pool_args['assert_hostname'] = configuration.assert_hostname  # noqa: E501\n\n        if maxsize is None:\n            if configuration.connection_pool_maxsize is not None:\n                maxsize = configuration.connection_pool_maxsize\n            else:\n                maxsize = 4\n\n        # https pool manager\n        if configuration.proxy:\n            self.pool_manager = urllib3.ProxyManager(\n                num_pools=pools_size,\n                maxsize=maxsize,\n                cert_reqs=cert_reqs,\n                ca_certs=ca_certs,\n                cert_file=configuration.cert_file,\n                key_file=configuration.key_file,\n                proxy_url=configuration.proxy,\n                **addition_pool_args\n            )\n        else:\n            self.pool_manager = urllib3.PoolManager(\n                num_pools=pools_size,\n                maxsize=maxsize,\n                cert_reqs=cert_reqs,\n                ca_certs=ca_certs,\n                cert_file=configuration.cert_file,\n                key_file=configuration.key_file,\n                **addition_pool_args\n            )",
  "def request(self, method, url, query_params=None, headers=None,\n                body=None, post_params=None, _preload_content=True,\n                _request_timeout=None):\n        \"\"\"Perform requests.\n\n        :param method: http request method\n        :param url: http request url\n        :param query_params: query parameters in the url\n        :param headers: http request headers\n        :param body: request json body, for `application/json`\n        :param post_params: request post parameters,\n                            `application/x-www-form-urlencoded`\n                            and `multipart/form-data`\n        :param _preload_content: if False, the urllib3.HTTPResponse object will\n                                 be returned without reading/decoding response\n                                 data. Default is True.\n        :param _request_timeout: timeout setting for this request. If one\n                                 number provided, it will be total request\n                                 timeout. It can also be a pair (tuple) of\n                                 (connection, read) timeouts.\n        \"\"\"\n        method = method.upper()\n        assert method in ['GET', 'HEAD', 'DELETE', 'POST', 'PUT',\n                          'PATCH', 'OPTIONS']\n\n        if post_params and body:\n            raise ValueError(\n                \"body parameter cannot be used with post_params parameter.\"\n            )\n\n        post_params = post_params or {}\n        headers = headers or {}\n\n        timeout = None\n        if _request_timeout:\n            if isinstance(_request_timeout, (int, ) if six.PY3 else (int, long)):  # noqa: E501,F821\n                timeout = urllib3.Timeout(total=_request_timeout)\n            elif (isinstance(_request_timeout, tuple) and\n                  len(_request_timeout) == 2):\n                timeout = urllib3.Timeout(\n                    connect=_request_timeout[0], read=_request_timeout[1])\n\n        if 'Content-Type' not in headers:\n            headers['Content-Type'] = 'application/json'\n\n        try:\n            # For `POST`, `PUT`, `PATCH`, `OPTIONS`, `DELETE`\n            if method in ['POST', 'PUT', 'PATCH', 'OPTIONS', 'DELETE']:\n                if query_params:\n                    url += '?' + urlencode(query_params)\n                if re.search('json', headers['Content-Type'], re.IGNORECASE):\n                    request_body = '{}'\n                    if body is not None:\n                        request_body = json.dumps(body)\n                    r = self.pool_manager.request(\n                        method, url,\n                        body=request_body,\n                        preload_content=_preload_content,\n                        timeout=timeout,\n                        headers=headers)\n                elif headers['Content-Type'] == 'application/x-www-form-urlencoded':  # noqa: E501\n                    r = self.pool_manager.request(\n                        method, url,\n                        fields=post_params,\n                        encode_multipart=False,\n                        preload_content=_preload_content,\n                        timeout=timeout,\n                        headers=headers)\n                elif headers['Content-Type'] == 'multipart/form-data':\n                    # must del headers['Content-Type'], or the correct\n                    # Content-Type which generated by urllib3 will be\n                    # overwritten.\n                    del headers['Content-Type']\n                    r = self.pool_manager.request(\n                        method, url,\n                        fields=post_params,\n                        encode_multipart=True,\n                        preload_content=_preload_content,\n                        timeout=timeout,\n                        headers=headers)\n                # Pass a `string` parameter directly in the body to support\n                # other content types than Json when `body` argument is\n                # provided in serialized form\n                elif isinstance(body, str):\n                    request_body = body\n                    r = self.pool_manager.request(\n                        method, url,\n                        body=request_body,\n                        preload_content=_preload_content,\n                        timeout=timeout,\n                        headers=headers)\n                else:\n                    # Cannot generate the request from given parameters\n                    msg = \"\"\"Cannot prepare a request message for provided\n                             arguments. Please check that your arguments match\n                             declared content type.\"\"\"\n                    raise ApiException(status=0, reason=msg)\n            # For `GET`, `HEAD`\n            else:\n                r = self.pool_manager.request(method, url,\n                                              fields=query_params,\n                                              preload_content=_preload_content,\n                                              timeout=timeout,\n                                              headers=headers)\n        except urllib3.exceptions.SSLError as e:\n            msg = \"{0}\\n{1}\".format(type(e).__name__, str(e))\n            raise ApiException(status=0, reason=msg)\n\n        if _preload_content:\n            r = RESTResponse(r)\n\n            # In the python 3, the response.data is bytes.\n            # we need to decode it to string.\n            if six.PY3:\n                r.data = r.data.decode('utf8')\n\n            # log response body\n            logger.debug(\"response body: %s\", r.data)\n\n        if not 200 <= r.status <= 299:\n            raise ApiException(http_resp=r)\n\n        return r",
  "def GET(self, url, headers=None, query_params=None, _preload_content=True,\n            _request_timeout=None):\n        return self.request(\"GET\", url,\n                            headers=headers,\n                            _preload_content=_preload_content,\n                            _request_timeout=_request_timeout,\n                            query_params=query_params)",
  "def HEAD(self, url, headers=None, query_params=None, _preload_content=True,\n             _request_timeout=None):\n        return self.request(\"HEAD\", url,\n                            headers=headers,\n                            _preload_content=_preload_content,\n                            _request_timeout=_request_timeout,\n                            query_params=query_params)",
  "def OPTIONS(self, url, headers=None, query_params=None, post_params=None,\n                body=None, _preload_content=True, _request_timeout=None):\n        return self.request(\"OPTIONS\", url,\n                            headers=headers,\n                            query_params=query_params,\n                            post_params=post_params,\n                            _preload_content=_preload_content,\n                            _request_timeout=_request_timeout,\n                            body=body)",
  "def DELETE(self, url, headers=None, query_params=None, body=None,\n               _preload_content=True, _request_timeout=None):\n        return self.request(\"DELETE\", url,\n                            headers=headers,\n                            query_params=query_params,\n                            _preload_content=_preload_content,\n                            _request_timeout=_request_timeout,\n                            body=body)",
  "def POST(self, url, headers=None, query_params=None, post_params=None,\n             body=None, _preload_content=True, _request_timeout=None):\n        return self.request(\"POST\", url,\n                            headers=headers,\n                            query_params=query_params,\n                            post_params=post_params,\n                            _preload_content=_preload_content,\n                            _request_timeout=_request_timeout,\n                            body=body)",
  "def PUT(self, url, headers=None, query_params=None, post_params=None,\n            body=None, _preload_content=True, _request_timeout=None):\n        return self.request(\"PUT\", url,\n                            headers=headers,\n                            query_params=query_params,\n                            post_params=post_params,\n                            _preload_content=_preload_content,\n                            _request_timeout=_request_timeout,\n                            body=body)",
  "def PATCH(self, url, headers=None, query_params=None, post_params=None,\n              body=None, _preload_content=True, _request_timeout=None):\n        return self.request(\"PATCH\", url,\n                            headers=headers,\n                            query_params=query_params,\n                            post_params=post_params,\n                            _preload_content=_preload_content,\n                            _request_timeout=_request_timeout,\n                            body=body)",
  "def __init__(self, status=None, reason=None, http_resp=None):\n        if http_resp:\n            self.status = http_resp.status\n            self.reason = http_resp.reason\n            self.body = http_resp.data\n            self.headers = http_resp.getheaders()\n        else:\n            self.status = status\n            self.reason = reason\n            self.body = None\n            self.headers = None",
  "def __str__(self):\n        \"\"\"Custom error messages for exception\"\"\"\n        error_message = \"({0})\\n\"\\\n                        \"Reason: {1}\\n\".format(self.status, self.reason)\n        if self.headers:\n            error_message += \"HTTP response headers: {0}\\n\".format(\n                self.headers)\n\n        if self.body:\n            error_message += \"HTTP response body: {0}\\n\".format(self.body)\n\n        return error_message",
  "class Configuration(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Ref: https://github.com/swagger-api/swagger-codegen\n    Do not edit the class manually.\n    \"\"\"\n\n    _default = None\n\n    def __init__(self):\n        \"\"\"Constructor\"\"\"\n        if self._default:\n            for key in self._default.__dict__.keys():\n                self.__dict__[key] = copy.copy(self._default.__dict__[key])\n            return\n\n        # Default Base url\n        self.host = \"https://api.lightly.ai\"\n        # Temp file folder for downloading files\n        self.temp_folder_path = None\n\n        # Authentication Settings\n        # dict to store API key(s)\n        self.api_key = {}\n        # dict to store API prefix (e.g. Bearer)\n        self.api_key_prefix = {}\n        # function to refresh API key if expired\n        self.refresh_api_key_hook = None\n        # Username for HTTP basic authentication\n        self.username = \"\"\n        # Password for HTTP basic authentication\n        self.password = \"\"\n\n        # Logging Settings\n        self.logger = {}\n        self.logger[\"package_logger\"] = logging.getLogger(\"lightly.openapi_generated.swagger_client\")\n        self.logger[\"urllib3_logger\"] = logging.getLogger(\"urllib3\")\n        # Log format\n        self.logger_format = '%(asctime)s %(levelname)s %(message)s'\n        # Log stream handler\n        self.logger_stream_handler = None\n        # Log file handler\n        self.logger_file_handler = None\n        # Debug file location\n        self.logger_file = None\n        # Debug switch\n        self.debug = False\n\n        # SSL/TLS verification\n        # Set this to false to skip verifying SSL certificate when calling API\n        # from https server.\n        self.verify_ssl = True\n        # Set this to customize the certificate file to verify the peer.\n        self.ssl_ca_cert = None\n        # client certificate file\n        self.cert_file = None\n        # client key file\n        self.key_file = None\n        # Set this to True/False to enable/disable SSL hostname verification.\n        self.assert_hostname = None\n\n        # urllib3 connection pool's maximum number of connections saved\n        # per pool. urllib3 uses 1 connection as default value, but this is\n        # not the best value when you are making a lot of possibly parallel\n        # requests to the same host, which is often the case here.\n        # cpu_count * 5 is used as default value to increase performance.\n        self.connection_pool_maxsize = multiprocessing.cpu_count() * 5\n\n        # Proxy URL\n        self.proxy = None\n        # Safe chars for path_param\n        self.safe_chars_for_path_param = ''\n\n        # Disable client side validation\n        self.client_side_validation = True\n\n    @classmethod\n    def set_default(cls, default):\n        cls._default = default\n\n    @property\n    def logger_file(self):\n        \"\"\"The logger file.\n\n        If the logger_file is None, then add stream handler and remove file\n        handler. Otherwise, add file handler and remove stream handler.\n\n        :param value: The logger_file path.\n        :type: str\n        \"\"\"\n        return self.__logger_file\n\n    @logger_file.setter\n    def logger_file(self, value):\n        \"\"\"The logger file.\n\n        If the logger_file is None, then add stream handler and remove file\n        handler. Otherwise, add file handler and remove stream handler.\n\n        :param value: The logger_file path.\n        :type: str\n        \"\"\"\n        self.__logger_file = value\n        if self.__logger_file:\n            # If set logging file,\n            # then add file handler and remove stream handler.\n            self.logger_file_handler = logging.FileHandler(self.__logger_file)\n            self.logger_file_handler.setFormatter(self.logger_formatter)\n            for _, logger in six.iteritems(self.logger):\n                logger.addHandler(self.logger_file_handler)\n                if self.logger_stream_handler:\n                    logger.removeHandler(self.logger_stream_handler)\n        else:\n            # If not set logging file,\n            # then add stream handler and remove file handler.\n            self.logger_stream_handler = logging.StreamHandler()\n            self.logger_stream_handler.setFormatter(self.logger_formatter)\n            for _, logger in six.iteritems(self.logger):\n                logger.addHandler(self.logger_stream_handler)\n                if self.logger_file_handler:\n                    logger.removeHandler(self.logger_file_handler)\n\n    @property\n    def debug(self):\n        \"\"\"Debug status\n\n        :param value: The debug status, True or False.\n        :type: bool\n        \"\"\"\n        return self.__debug\n\n    @debug.setter\n    def debug(self, value):\n        \"\"\"Debug status\n\n        :param value: The debug status, True or False.\n        :type: bool\n        \"\"\"\n        self.__debug = value\n        if self.__debug:\n            # if debug status is True, turn on debug logging\n            for _, logger in six.iteritems(self.logger):\n                logger.setLevel(logging.DEBUG)\n            # turn on httplib debug\n            httplib.HTTPConnection.debuglevel = 1\n        else:\n            # if debug status is False, turn off debug logging,\n            # setting log level to default `logging.WARNING`\n            for _, logger in six.iteritems(self.logger):\n                logger.setLevel(logging.WARNING)\n            # turn off httplib debug\n            httplib.HTTPConnection.debuglevel = 0\n\n    @property\n    def logger_format(self):\n        \"\"\"The logger format.\n\n        The logger_formatter will be updated when sets logger_format.\n\n        :param value: The format string.\n        :type: str\n        \"\"\"\n        return self.__logger_format\n\n    @logger_format.setter\n    def logger_format(self, value):\n        \"\"\"The logger format.\n\n        The logger_formatter will be updated when sets logger_format.\n\n        :param value: The format string.\n        :type: str\n        \"\"\"\n        self.__logger_format = value\n        self.logger_formatter = logging.Formatter(self.__logger_format)\n\n    def get_api_key_with_prefix(self, identifier):\n        \"\"\"Gets API key (with prefix if set).\n\n        :param identifier: The identifier of apiKey.\n        :return: The token for api key authentication.\n        \"\"\"\n\n        if self.refresh_api_key_hook:\n            self.refresh_api_key_hook(self)\n\n        key = self.api_key.get(identifier)\n        if key:\n            prefix = self.api_key_prefix.get(identifier)\n            if prefix:\n                return \"%s %s\" % (prefix, key)\n            else:\n                return key\n\n    def get_basic_auth_token(self):\n        \"\"\"Gets HTTP basic authentication header (string).\n\n        :return: The token for basic HTTP authentication.\n        \"\"\"\n        return urllib3.util.make_headers(\n            basic_auth=self.username + ':' + self.password\n        ).get('authorization')\n\n    def auth_settings(self):\n        \"\"\"Gets Auth Settings dict for api client.\n\n        :return: The Auth Settings information dict.\n        \"\"\"\n        return {\n            'ApiKeyAuth':\n                {\n                    'type': 'api_key',\n                    'in': 'query',\n                    'key': 'token',\n                    'value': self.get_api_key_with_prefix('token')\n                },\n            'InternalKeyAuth':\n                {\n                    'type': 'api_key',\n                    'in': 'query',\n                    'key': 'secret',\n                    'value': self.get_api_key_with_prefix('secret')\n                },\n\n        }\n\n    def to_debug_report(self):\n        \"\"\"Gets the essential information for debugging.\n\n        :return: The report for debugging.\n        \"\"\"\n        return \"Python SDK Debug Report:\\n\"\\\n               \"OS: {env}\\n\"\\\n               \"Python Version: {pyversion}\\n\"\\\n               \"Version of the API: 1.0.0\\n\"\\\n               \"SDK Package Version: 1.0.0\".\\\n               format(env=sys.platform, pyversion=sys.version)",
  "def __init__(self):\n        \"\"\"Constructor\"\"\"\n        if self._default:\n            for key in self._default.__dict__.keys():\n                self.__dict__[key] = copy.copy(self._default.__dict__[key])\n            return\n\n        # Default Base url\n        self.host = \"https://api.lightly.ai\"\n        # Temp file folder for downloading files\n        self.temp_folder_path = None\n\n        # Authentication Settings\n        # dict to store API key(s)\n        self.api_key = {}\n        # dict to store API prefix (e.g. Bearer)\n        self.api_key_prefix = {}\n        # function to refresh API key if expired\n        self.refresh_api_key_hook = None\n        # Username for HTTP basic authentication\n        self.username = \"\"\n        # Password for HTTP basic authentication\n        self.password = \"\"\n\n        # Logging Settings\n        self.logger = {}\n        self.logger[\"package_logger\"] = logging.getLogger(\"lightly.openapi_generated.swagger_client\")\n        self.logger[\"urllib3_logger\"] = logging.getLogger(\"urllib3\")\n        # Log format\n        self.logger_format = '%(asctime)s %(levelname)s %(message)s'\n        # Log stream handler\n        self.logger_stream_handler = None\n        # Log file handler\n        self.logger_file_handler = None\n        # Debug file location\n        self.logger_file = None\n        # Debug switch\n        self.debug = False\n\n        # SSL/TLS verification\n        # Set this to false to skip verifying SSL certificate when calling API\n        # from https server.\n        self.verify_ssl = True\n        # Set this to customize the certificate file to verify the peer.\n        self.ssl_ca_cert = None\n        # client certificate file\n        self.cert_file = None\n        # client key file\n        self.key_file = None\n        # Set this to True/False to enable/disable SSL hostname verification.\n        self.assert_hostname = None\n\n        # urllib3 connection pool's maximum number of connections saved\n        # per pool. urllib3 uses 1 connection as default value, but this is\n        # not the best value when you are making a lot of possibly parallel\n        # requests to the same host, which is often the case here.\n        # cpu_count * 5 is used as default value to increase performance.\n        self.connection_pool_maxsize = multiprocessing.cpu_count() * 5\n\n        # Proxy URL\n        self.proxy = None\n        # Safe chars for path_param\n        self.safe_chars_for_path_param = ''\n\n        # Disable client side validation\n        self.client_side_validation = True",
  "def set_default(cls, default):\n        cls._default = default",
  "def logger_file(self):\n        \"\"\"The logger file.\n\n        If the logger_file is None, then add stream handler and remove file\n        handler. Otherwise, add file handler and remove stream handler.\n\n        :param value: The logger_file path.\n        :type: str\n        \"\"\"\n        return self.__logger_file",
  "def logger_file(self, value):\n        \"\"\"The logger file.\n\n        If the logger_file is None, then add stream handler and remove file\n        handler. Otherwise, add file handler and remove stream handler.\n\n        :param value: The logger_file path.\n        :type: str\n        \"\"\"\n        self.__logger_file = value\n        if self.__logger_file:\n            # If set logging file,\n            # then add file handler and remove stream handler.\n            self.logger_file_handler = logging.FileHandler(self.__logger_file)\n            self.logger_file_handler.setFormatter(self.logger_formatter)\n            for _, logger in six.iteritems(self.logger):\n                logger.addHandler(self.logger_file_handler)\n                if self.logger_stream_handler:\n                    logger.removeHandler(self.logger_stream_handler)\n        else:\n            # If not set logging file,\n            # then add stream handler and remove file handler.\n            self.logger_stream_handler = logging.StreamHandler()\n            self.logger_stream_handler.setFormatter(self.logger_formatter)\n            for _, logger in six.iteritems(self.logger):\n                logger.addHandler(self.logger_stream_handler)\n                if self.logger_file_handler:\n                    logger.removeHandler(self.logger_file_handler)",
  "def debug(self):\n        \"\"\"Debug status\n\n        :param value: The debug status, True or False.\n        :type: bool\n        \"\"\"\n        return self.__debug",
  "def debug(self, value):\n        \"\"\"Debug status\n\n        :param value: The debug status, True or False.\n        :type: bool\n        \"\"\"\n        self.__debug = value\n        if self.__debug:\n            # if debug status is True, turn on debug logging\n            for _, logger in six.iteritems(self.logger):\n                logger.setLevel(logging.DEBUG)\n            # turn on httplib debug\n            httplib.HTTPConnection.debuglevel = 1\n        else:\n            # if debug status is False, turn off debug logging,\n            # setting log level to default `logging.WARNING`\n            for _, logger in six.iteritems(self.logger):\n                logger.setLevel(logging.WARNING)\n            # turn off httplib debug\n            httplib.HTTPConnection.debuglevel = 0",
  "def logger_format(self):\n        \"\"\"The logger format.\n\n        The logger_formatter will be updated when sets logger_format.\n\n        :param value: The format string.\n        :type: str\n        \"\"\"\n        return self.__logger_format",
  "def logger_format(self, value):\n        \"\"\"The logger format.\n\n        The logger_formatter will be updated when sets logger_format.\n\n        :param value: The format string.\n        :type: str\n        \"\"\"\n        self.__logger_format = value\n        self.logger_formatter = logging.Formatter(self.__logger_format)",
  "def get_api_key_with_prefix(self, identifier):\n        \"\"\"Gets API key (with prefix if set).\n\n        :param identifier: The identifier of apiKey.\n        :return: The token for api key authentication.\n        \"\"\"\n\n        if self.refresh_api_key_hook:\n            self.refresh_api_key_hook(self)\n\n        key = self.api_key.get(identifier)\n        if key:\n            prefix = self.api_key_prefix.get(identifier)\n            if prefix:\n                return \"%s %s\" % (prefix, key)\n            else:\n                return key",
  "def get_basic_auth_token(self):\n        \"\"\"Gets HTTP basic authentication header (string).\n\n        :return: The token for basic HTTP authentication.\n        \"\"\"\n        return urllib3.util.make_headers(\n            basic_auth=self.username + ':' + self.password\n        ).get('authorization')",
  "def auth_settings(self):\n        \"\"\"Gets Auth Settings dict for api client.\n\n        :return: The Auth Settings information dict.\n        \"\"\"\n        return {\n            'ApiKeyAuth':\n                {\n                    'type': 'api_key',\n                    'in': 'query',\n                    'key': 'token',\n                    'value': self.get_api_key_with_prefix('token')\n                },\n            'InternalKeyAuth':\n                {\n                    'type': 'api_key',\n                    'in': 'query',\n                    'key': 'secret',\n                    'value': self.get_api_key_with_prefix('secret')\n                },\n\n        }",
  "def to_debug_report(self):\n        \"\"\"Gets the essential information for debugging.\n\n        :return: The report for debugging.\n        \"\"\"\n        return \"Python SDK Debug Report:\\n\"\\\n               \"OS: {env}\\n\"\\\n               \"Python Version: {pyversion}\\n\"\\\n               \"Version of the API: 1.0.0\\n\"\\\n               \"SDK Package Version: 1.0.0\".\\\n               format(env=sys.platform, pyversion=sys.version)",
  "class ApiClient(object):\n    \"\"\"Generic API client for Swagger client library builds.\n\n    Swagger generic API client. This client handles the client-\n    server communication, and is invariant across implementations. Specifics of\n    the methods and models for each application are generated from the Swagger\n    templates.\n\n    NOTE: This class is auto generated by the swagger code generator program.\n    Ref: https://github.com/swagger-api/swagger-codegen\n    Do not edit the class manually.\n\n    :param configuration: .Configuration object for this client\n    :param header_name: a header to pass when making calls to the API.\n    :param header_value: a header value to pass when making calls to\n        the API.\n    :param cookie: a cookie to include in the header when making calls\n        to the API\n    \"\"\"\n\n    PRIMITIVE_TYPES = (float, bool, bytes, six.text_type) + six.integer_types\n    NATIVE_TYPES_MAPPING = {\n        'int': int,\n        'long': int if six.PY3 else long,  # noqa: F821\n        'float': float,\n        'str': str,\n        'bool': bool,\n        'date': datetime.date,\n        'datetime': datetime.datetime,\n        'object': object,\n    }\n\n    def __init__(self, configuration=None, header_name=None, header_value=None,\n                 cookie=None):\n        if configuration is None:\n            configuration = Configuration()\n        self.configuration = configuration\n\n        # Use the pool property to lazily initialize the ThreadPool.\n        self._pool = None\n        self.rest_client = rest.RESTClientObject(configuration)\n        self.default_headers = {}\n        if header_name is not None:\n            self.default_headers[header_name] = header_value\n        self.cookie = cookie\n        # Set default User-Agent.\n        self.user_agent = 'Swagger-Codegen/1.0.0/python'\n        self.client_side_validation = configuration.client_side_validation\n\n    def __del__(self):\n        if self._pool is not None:\n            self._pool.close()\n            self._pool.join()\n\n    @property\n    def pool(self):\n        if self._pool is None:\n            self._pool = ThreadPool()\n        return self._pool\n\n    @property\n    def user_agent(self):\n        \"\"\"User agent for this API client\"\"\"\n        return self.default_headers['User-Agent']\n\n    @user_agent.setter\n    def user_agent(self, value):\n        self.default_headers['User-Agent'] = value\n\n    def set_default_header(self, header_name, header_value):\n        self.default_headers[header_name] = header_value\n\n    def __call_api(\n            self, resource_path, method, path_params=None,\n            query_params=None, header_params=None, body=None, post_params=None,\n            files=None, response_type=None, auth_settings=None,\n            _return_http_data_only=None, collection_formats=None,\n            _preload_content=True, _request_timeout=None):\n\n        config = self.configuration\n\n        # header parameters\n        header_params = header_params or {}\n        header_params.update(self.default_headers)\n        if self.cookie:\n            header_params['Cookie'] = self.cookie\n        if header_params:\n            header_params = self.sanitize_for_serialization(header_params)\n            header_params = dict(self.parameters_to_tuples(header_params,\n                                                           collection_formats))\n\n        # path parameters\n        if path_params:\n            path_params = self.sanitize_for_serialization(path_params)\n            path_params = self.parameters_to_tuples(path_params,\n                                                    collection_formats)\n            for k, v in path_params:\n                # specified safe chars, encode everything\n                resource_path = resource_path.replace(\n                    '{%s}' % k,\n                    quote(str(v), safe=config.safe_chars_for_path_param)\n                )\n\n        # query parameters\n        if query_params:\n            query_params = self.sanitize_for_serialization(query_params)\n            query_params = self.parameters_to_tuples(query_params,\n                                                     collection_formats)\n\n        # post parameters\n        if post_params or files:\n            post_params = self.prepare_post_parameters(post_params, files)\n            post_params = self.sanitize_for_serialization(post_params)\n            post_params = self.parameters_to_tuples(post_params,\n                                                    collection_formats)\n\n        # auth setting\n        self.update_params_for_auth(header_params, query_params, auth_settings)\n\n        # body\n        if body:\n            body = self.sanitize_for_serialization(body)\n\n        # request url\n        url = self.configuration.host + resource_path\n\n        # perform request and return response\n        response_data = self.request(\n            method, url, query_params=query_params, headers=header_params,\n            post_params=post_params, body=body,\n            _preload_content=_preload_content,\n            _request_timeout=_request_timeout)\n\n        self.last_response = response_data\n\n        return_data = response_data\n        if _preload_content:\n            # deserialize response data\n            if response_type:\n                return_data = self.deserialize(response_data, response_type)\n            else:\n                return_data = None\n\n        if _return_http_data_only:\n            return (return_data)\n        else:\n            return (return_data, response_data.status,\n                    response_data.getheaders())\n\n    def sanitize_for_serialization(self, obj):\n        \"\"\"Builds a JSON POST object.\n\n        If obj is None, return None.\n        If obj is str, int, long, float, bool, return directly.\n        If obj is datetime.datetime, datetime.date\n            convert to string in iso8601 format.\n        If obj is list, sanitize each element in the list.\n        If obj is dict, return the dict.\n        If obj is swagger model, return the properties dict.\n\n        :param obj: The data to serialize.\n        :return: The serialized form of data.\n        \"\"\"\n        if obj is None:\n            return None\n        elif isinstance(obj, self.PRIMITIVE_TYPES):\n            return obj\n        elif isinstance(obj, list):\n            return [self.sanitize_for_serialization(sub_obj)\n                    for sub_obj in obj]\n        elif isinstance(obj, tuple):\n            return tuple(self.sanitize_for_serialization(sub_obj)\n                         for sub_obj in obj)\n        elif isinstance(obj, (datetime.datetime, datetime.date)):\n            return obj.isoformat()\n\n        if isinstance(obj, dict):\n            obj_dict = obj\n        else:\n            # Convert model obj to dict except\n            # attributes `swagger_types`, `attribute_map`\n            # and attributes which value is not None.\n            # Convert attribute name to json key in\n            # model definition for request.\n            obj_dict = {obj.attribute_map[attr]: getattr(obj, attr)\n                        for attr, _ in six.iteritems(obj.swagger_types)\n                        if getattr(obj, attr) is not None}\n\n        return {key: self.sanitize_for_serialization(val)\n                for key, val in six.iteritems(obj_dict)}\n\n    def deserialize(self, response, response_type):\n        \"\"\"Deserializes response into an object.\n\n        :param response: RESTResponse object to be deserialized.\n        :param response_type: class literal for\n            deserialized object, or string of class name.\n\n        :return: deserialized object.\n        \"\"\"\n        # handle file downloading\n        # save response body into a tmp file and return the instance\n        if response_type == \"file\":\n            return self.__deserialize_file(response)\n\n        # fetch data from response object\n        try:\n            data = json.loads(response.data)\n        except ValueError:\n            data = response.data\n\n        return self.__deserialize(data, response_type)\n\n    def __deserialize(self, data, klass):\n        \"\"\"Deserializes dict, list, str into an object.\n\n        :param data: dict, list or str.\n        :param klass: class literal, or string of class name.\n\n        :return: object.\n        \"\"\"\n        if data is None:\n            return None\n\n        if type(klass) == str:\n            if klass.startswith('list['):\n                sub_kls = re.match(r'list\\[(.*)\\]', klass).group(1)\n                return [self.__deserialize(sub_data, sub_kls)\n                        for sub_data in data]\n\n            if klass.startswith('dict('):\n                sub_kls = re.match(r'dict\\(([^,]*), (.*)\\)', klass).group(2)\n                return {k: self.__deserialize(v, sub_kls)\n                        for k, v in six.iteritems(data)}\n\n            # convert str to class\n            if klass in self.NATIVE_TYPES_MAPPING:\n                klass = self.NATIVE_TYPES_MAPPING[klass]\n            else:\n                klass = getattr(lightly.openapi_generated.swagger_client.models, klass)\n\n        if klass in self.PRIMITIVE_TYPES:\n            return self.__deserialize_primitive(data, klass)\n        elif klass == object:\n            return self.__deserialize_object(data)\n        elif klass == datetime.date:\n            return self.__deserialize_date(data)\n        elif klass == datetime.datetime:\n            return self.__deserialize_datatime(data)\n        else:\n            return self.__deserialize_model(data, klass)\n\n    def call_api(self, resource_path, method,\n                 path_params=None, query_params=None, header_params=None,\n                 body=None, post_params=None, files=None,\n                 response_type=None, auth_settings=None, async_req=None,\n                 _return_http_data_only=None, collection_formats=None,\n                 _preload_content=True, _request_timeout=None):\n        \"\"\"Makes the HTTP request (synchronous) and returns deserialized data.\n\n        To make an async request, set the async_req parameter.\n\n        :param resource_path: Path to method endpoint.\n        :param method: Method to call.\n        :param path_params: Path parameters in the url.\n        :param query_params: Query parameters in the url.\n        :param header_params: Header parameters to be\n            placed in the request header.\n        :param body: Request body.\n        :param post_params dict: Request post form parameters,\n            for `application/x-www-form-urlencoded`, `multipart/form-data`.\n        :param auth_settings list: Auth Settings names for the request.\n        :param response: Response data type.\n        :param files dict: key -> filename, value -> filepath,\n            for `multipart/form-data`.\n        :param async_req bool: execute request asynchronously\n        :param _return_http_data_only: response data without head status code\n                                       and headers\n        :param collection_formats: dict of collection formats for path, query,\n            header, and post parameters.\n        :param _preload_content: if False, the urllib3.HTTPResponse object will\n                                 be returned without reading/decoding response\n                                 data. Default is True.\n        :param _request_timeout: timeout setting for this request. If one\n                                 number provided, it will be total request\n                                 timeout. It can also be a pair (tuple) of\n                                 (connection, read) timeouts.\n        :return:\n            If async_req parameter is True,\n            the request will be called asynchronously.\n            The method will return the request thread.\n            If parameter async_req is False or missing,\n            then the method will return the response directly.\n        \"\"\"\n        if not async_req:\n            return self.__call_api(resource_path, method,\n                                   path_params, query_params, header_params,\n                                   body, post_params, files,\n                                   response_type, auth_settings,\n                                   _return_http_data_only, collection_formats,\n                                   _preload_content, _request_timeout)\n        else:\n            thread = self.pool.apply_async(self.__call_api, (resource_path,\n                                           method, path_params, query_params,\n                                           header_params, body,\n                                           post_params, files,\n                                           response_type, auth_settings,\n                                           _return_http_data_only,\n                                           collection_formats,\n                                           _preload_content, _request_timeout))\n        return thread\n\n    def request(self, method, url, query_params=None, headers=None,\n                post_params=None, body=None, _preload_content=True,\n                _request_timeout=None):\n        \"\"\"Makes the HTTP request using RESTClient.\"\"\"\n        if method == \"GET\":\n            return self.rest_client.GET(url,\n                                        query_params=query_params,\n                                        _preload_content=_preload_content,\n                                        _request_timeout=_request_timeout,\n                                        headers=headers)\n        elif method == \"HEAD\":\n            return self.rest_client.HEAD(url,\n                                         query_params=query_params,\n                                         _preload_content=_preload_content,\n                                         _request_timeout=_request_timeout,\n                                         headers=headers)\n        elif method == \"OPTIONS\":\n            return self.rest_client.OPTIONS(url,\n                                            query_params=query_params,\n                                            headers=headers,\n                                            post_params=post_params,\n                                            _preload_content=_preload_content,\n                                            _request_timeout=_request_timeout,\n                                            body=body)\n        elif method == \"POST\":\n            return self.rest_client.POST(url,\n                                         query_params=query_params,\n                                         headers=headers,\n                                         post_params=post_params,\n                                         _preload_content=_preload_content,\n                                         _request_timeout=_request_timeout,\n                                         body=body)\n        elif method == \"PUT\":\n            return self.rest_client.PUT(url,\n                                        query_params=query_params,\n                                        headers=headers,\n                                        post_params=post_params,\n                                        _preload_content=_preload_content,\n                                        _request_timeout=_request_timeout,\n                                        body=body)\n        elif method == \"PATCH\":\n            return self.rest_client.PATCH(url,\n                                          query_params=query_params,\n                                          headers=headers,\n                                          post_params=post_params,\n                                          _preload_content=_preload_content,\n                                          _request_timeout=_request_timeout,\n                                          body=body)\n        elif method == \"DELETE\":\n            return self.rest_client.DELETE(url,\n                                           query_params=query_params,\n                                           headers=headers,\n                                           _preload_content=_preload_content,\n                                           _request_timeout=_request_timeout,\n                                           body=body)\n        else:\n            raise ValueError(\n                \"http method must be `GET`, `HEAD`, `OPTIONS`,\"\n                \" `POST`, `PATCH`, `PUT` or `DELETE`.\"\n            )\n\n    def parameters_to_tuples(self, params, collection_formats):\n        \"\"\"Get parameters as list of tuples, formatting collections.\n\n        :param params: Parameters as dict or list of two-tuples\n        :param dict collection_formats: Parameter collection formats\n        :return: Parameters as list of tuples, collections formatted\n        \"\"\"\n        new_params = []\n        if collection_formats is None:\n            collection_formats = {}\n        for k, v in six.iteritems(params) if isinstance(params, dict) else params:  # noqa: E501\n            if k in collection_formats:\n                collection_format = collection_formats[k]\n                if collection_format == 'multi':\n                    new_params.extend((k, value) for value in v)\n                else:\n                    if collection_format == 'ssv':\n                        delimiter = ' '\n                    elif collection_format == 'tsv':\n                        delimiter = '\\t'\n                    elif collection_format == 'pipes':\n                        delimiter = '|'\n                    else:  # csv is the default\n                        delimiter = ','\n                    new_params.append(\n                        (k, delimiter.join(str(value) for value in v)))\n            else:\n                new_params.append((k, v))\n        return new_params\n\n    def prepare_post_parameters(self, post_params=None, files=None):\n        \"\"\"Builds form parameters.\n\n        :param post_params: Normal form parameters.\n        :param files: File parameters.\n        :return: Form parameters with files.\n        \"\"\"\n        params = []\n\n        if post_params:\n            params = post_params\n\n        if files:\n            for k, v in six.iteritems(files):\n                if not v:\n                    continue\n                file_names = v if type(v) is list else [v]\n                for n in file_names:\n                    with open(n, 'rb') as f:\n                        filename = os.path.basename(f.name)\n                        filedata = f.read()\n                        mimetype = (mimetypes.guess_type(filename)[0] or\n                                    'application/octet-stream')\n                        params.append(\n                            tuple([k, tuple([filename, filedata, mimetype])]))\n\n        return params\n\n    def select_header_accept(self, accepts):\n        \"\"\"Returns `Accept` based on an array of accepts provided.\n\n        :param accepts: List of headers.\n        :return: Accept (e.g. application/json).\n        \"\"\"\n        if not accepts:\n            return\n\n        accepts = [x.lower() for x in accepts]\n\n        if 'application/json' in accepts:\n            return 'application/json'\n        else:\n            return ', '.join(accepts)\n\n    def select_header_content_type(self, content_types):\n        \"\"\"Returns `Content-Type` based on an array of content_types provided.\n\n        :param content_types: List of content-types.\n        :return: Content-Type (e.g. application/json).\n        \"\"\"\n        if not content_types:\n            return 'application/json'\n\n        content_types = [x.lower() for x in content_types]\n\n        if 'application/json' in content_types or '*/*' in content_types:\n            return 'application/json'\n        else:\n            return content_types[0]\n\n    def update_params_for_auth(self, headers, querys, auth_settings):\n        \"\"\"Updates header and query params based on authentication setting.\n\n        :param headers: Header parameters dict to be updated.\n        :param querys: Query parameters tuple list to be updated.\n        :param auth_settings: Authentication setting identifiers list.\n        \"\"\"\n        if not auth_settings:\n            return\n\n        for auth in auth_settings:\n            auth_setting = self.configuration.auth_settings().get(auth)\n            if auth_setting:\n                if not auth_setting['value']:\n                    continue\n                elif auth_setting['in'] == 'header':\n                    headers[auth_setting['key']] = auth_setting['value']\n                elif auth_setting['in'] == 'query':\n                    querys.append((auth_setting['key'], auth_setting['value']))\n                else:\n                    raise ValueError(\n                        'Authentication token must be in `query` or `header`'\n                    )\n\n    def __deserialize_file(self, response):\n        \"\"\"Deserializes body to file\n\n        Saves response body into a file in a temporary folder,\n        using the filename from the `Content-Disposition` header if provided.\n\n        :param response:  RESTResponse.\n        :return: file path.\n        \"\"\"\n        fd, path = tempfile.mkstemp(dir=self.configuration.temp_folder_path)\n        os.close(fd)\n        os.remove(path)\n\n        content_disposition = response.getheader(\"Content-Disposition\")\n        if content_disposition:\n            filename = re.search(r'filename=[\\'\"]?([^\\'\"\\s]+)[\\'\"]?',\n                                 content_disposition).group(1)\n            path = os.path.join(os.path.dirname(path), filename)\n\n        with open(path, \"w\") as f:\n            f.write(response.data)\n\n        return path\n\n    def __deserialize_primitive(self, data, klass):\n        \"\"\"Deserializes string to primitive type.\n\n        :param data: str.\n        :param klass: class literal.\n\n        :return: int, long, float, str, bool.\n        \"\"\"\n        try:\n            return klass(data)\n        except UnicodeEncodeError:\n            return six.text_type(data)\n        except TypeError:\n            return data\n\n    def __deserialize_object(self, value):\n        \"\"\"Return a original value.\n\n        :return: object.\n        \"\"\"\n        return value\n\n    def __deserialize_date(self, string):\n        \"\"\"Deserializes string to date.\n\n        :param string: str.\n        :return: date.\n        \"\"\"\n        try:\n            from dateutil.parser import parse\n            return parse(string).date()\n        except ImportError:\n            return string\n        except ValueError:\n            raise rest.ApiException(\n                status=0,\n                reason=\"Failed to parse `{0}` as date object\".format(string)\n            )\n\n    def __deserialize_datatime(self, string):\n        \"\"\"Deserializes string to datetime.\n\n        The string should be in iso8601 datetime format.\n\n        :param string: str.\n        :return: datetime.\n        \"\"\"\n        try:\n            from dateutil.parser import parse\n            return parse(string)\n        except ImportError:\n            return string\n        except ValueError:\n            raise rest.ApiException(\n                status=0,\n                reason=(\n                    \"Failed to parse `{0}` as datetime object\"\n                    .format(string)\n                )\n            )\n\n    def __hasattr(self, object, name):\n        return name in object.__class__.__dict__\n\n    def __deserialize_model(self, data, klass):\n        \"\"\"Deserializes list or dict to model.\n\n        :param data: dict, list.\n        :param klass: class literal.\n        :return: model object.\n        \"\"\"\n\n        if (not klass.swagger_types and\n                not self.__hasattr(klass, 'get_real_child_model')):\n            return data\n\n        kwargs = {}\n        if klass.swagger_types is not None:\n            for attr, attr_type in six.iteritems(klass.swagger_types):\n                if (data is not None and\n                        klass.attribute_map[attr] in data and\n                        isinstance(data, (list, dict))):\n                    value = data[klass.attribute_map[attr]]\n                    kwargs[attr] = self.__deserialize(value, attr_type)\n\n        instance = klass(**kwargs)\n\n        if (isinstance(instance, dict) and\n                klass.swagger_types is not None and\n                isinstance(data, dict)):\n            for key, value in data.items():\n                if key not in klass.swagger_types:\n                    instance[key] = value\n        if self.__hasattr(instance, 'get_real_child_model'):\n            klass_name = instance.get_real_child_model(data)\n            if klass_name:\n                instance = self.__deserialize(data, klass_name)\n        return instance",
  "def __init__(self, configuration=None, header_name=None, header_value=None,\n                 cookie=None):\n        if configuration is None:\n            configuration = Configuration()\n        self.configuration = configuration\n\n        # Use the pool property to lazily initialize the ThreadPool.\n        self._pool = None\n        self.rest_client = rest.RESTClientObject(configuration)\n        self.default_headers = {}\n        if header_name is not None:\n            self.default_headers[header_name] = header_value\n        self.cookie = cookie\n        # Set default User-Agent.\n        self.user_agent = 'Swagger-Codegen/1.0.0/python'\n        self.client_side_validation = configuration.client_side_validation",
  "def __del__(self):\n        if self._pool is not None:\n            self._pool.close()\n            self._pool.join()",
  "def pool(self):\n        if self._pool is None:\n            self._pool = ThreadPool()\n        return self._pool",
  "def user_agent(self):\n        \"\"\"User agent for this API client\"\"\"\n        return self.default_headers['User-Agent']",
  "def user_agent(self, value):\n        self.default_headers['User-Agent'] = value",
  "def set_default_header(self, header_name, header_value):\n        self.default_headers[header_name] = header_value",
  "def __call_api(\n            self, resource_path, method, path_params=None,\n            query_params=None, header_params=None, body=None, post_params=None,\n            files=None, response_type=None, auth_settings=None,\n            _return_http_data_only=None, collection_formats=None,\n            _preload_content=True, _request_timeout=None):\n\n        config = self.configuration\n\n        # header parameters\n        header_params = header_params or {}\n        header_params.update(self.default_headers)\n        if self.cookie:\n            header_params['Cookie'] = self.cookie\n        if header_params:\n            header_params = self.sanitize_for_serialization(header_params)\n            header_params = dict(self.parameters_to_tuples(header_params,\n                                                           collection_formats))\n\n        # path parameters\n        if path_params:\n            path_params = self.sanitize_for_serialization(path_params)\n            path_params = self.parameters_to_tuples(path_params,\n                                                    collection_formats)\n            for k, v in path_params:\n                # specified safe chars, encode everything\n                resource_path = resource_path.replace(\n                    '{%s}' % k,\n                    quote(str(v), safe=config.safe_chars_for_path_param)\n                )\n\n        # query parameters\n        if query_params:\n            query_params = self.sanitize_for_serialization(query_params)\n            query_params = self.parameters_to_tuples(query_params,\n                                                     collection_formats)\n\n        # post parameters\n        if post_params or files:\n            post_params = self.prepare_post_parameters(post_params, files)\n            post_params = self.sanitize_for_serialization(post_params)\n            post_params = self.parameters_to_tuples(post_params,\n                                                    collection_formats)\n\n        # auth setting\n        self.update_params_for_auth(header_params, query_params, auth_settings)\n\n        # body\n        if body:\n            body = self.sanitize_for_serialization(body)\n\n        # request url\n        url = self.configuration.host + resource_path\n\n        # perform request and return response\n        response_data = self.request(\n            method, url, query_params=query_params, headers=header_params,\n            post_params=post_params, body=body,\n            _preload_content=_preload_content,\n            _request_timeout=_request_timeout)\n\n        self.last_response = response_data\n\n        return_data = response_data\n        if _preload_content:\n            # deserialize response data\n            if response_type:\n                return_data = self.deserialize(response_data, response_type)\n            else:\n                return_data = None\n\n        if _return_http_data_only:\n            return (return_data)\n        else:\n            return (return_data, response_data.status,\n                    response_data.getheaders())",
  "def sanitize_for_serialization(self, obj):\n        \"\"\"Builds a JSON POST object.\n\n        If obj is None, return None.\n        If obj is str, int, long, float, bool, return directly.\n        If obj is datetime.datetime, datetime.date\n            convert to string in iso8601 format.\n        If obj is list, sanitize each element in the list.\n        If obj is dict, return the dict.\n        If obj is swagger model, return the properties dict.\n\n        :param obj: The data to serialize.\n        :return: The serialized form of data.\n        \"\"\"\n        if obj is None:\n            return None\n        elif isinstance(obj, self.PRIMITIVE_TYPES):\n            return obj\n        elif isinstance(obj, list):\n            return [self.sanitize_for_serialization(sub_obj)\n                    for sub_obj in obj]\n        elif isinstance(obj, tuple):\n            return tuple(self.sanitize_for_serialization(sub_obj)\n                         for sub_obj in obj)\n        elif isinstance(obj, (datetime.datetime, datetime.date)):\n            return obj.isoformat()\n\n        if isinstance(obj, dict):\n            obj_dict = obj\n        else:\n            # Convert model obj to dict except\n            # attributes `swagger_types`, `attribute_map`\n            # and attributes which value is not None.\n            # Convert attribute name to json key in\n            # model definition for request.\n            obj_dict = {obj.attribute_map[attr]: getattr(obj, attr)\n                        for attr, _ in six.iteritems(obj.swagger_types)\n                        if getattr(obj, attr) is not None}\n\n        return {key: self.sanitize_for_serialization(val)\n                for key, val in six.iteritems(obj_dict)}",
  "def deserialize(self, response, response_type):\n        \"\"\"Deserializes response into an object.\n\n        :param response: RESTResponse object to be deserialized.\n        :param response_type: class literal for\n            deserialized object, or string of class name.\n\n        :return: deserialized object.\n        \"\"\"\n        # handle file downloading\n        # save response body into a tmp file and return the instance\n        if response_type == \"file\":\n            return self.__deserialize_file(response)\n\n        # fetch data from response object\n        try:\n            data = json.loads(response.data)\n        except ValueError:\n            data = response.data\n\n        return self.__deserialize(data, response_type)",
  "def __deserialize(self, data, klass):\n        \"\"\"Deserializes dict, list, str into an object.\n\n        :param data: dict, list or str.\n        :param klass: class literal, or string of class name.\n\n        :return: object.\n        \"\"\"\n        if data is None:\n            return None\n\n        if type(klass) == str:\n            if klass.startswith('list['):\n                sub_kls = re.match(r'list\\[(.*)\\]', klass).group(1)\n                return [self.__deserialize(sub_data, sub_kls)\n                        for sub_data in data]\n\n            if klass.startswith('dict('):\n                sub_kls = re.match(r'dict\\(([^,]*), (.*)\\)', klass).group(2)\n                return {k: self.__deserialize(v, sub_kls)\n                        for k, v in six.iteritems(data)}\n\n            # convert str to class\n            if klass in self.NATIVE_TYPES_MAPPING:\n                klass = self.NATIVE_TYPES_MAPPING[klass]\n            else:\n                klass = getattr(lightly.openapi_generated.swagger_client.models, klass)\n\n        if klass in self.PRIMITIVE_TYPES:\n            return self.__deserialize_primitive(data, klass)\n        elif klass == object:\n            return self.__deserialize_object(data)\n        elif klass == datetime.date:\n            return self.__deserialize_date(data)\n        elif klass == datetime.datetime:\n            return self.__deserialize_datatime(data)\n        else:\n            return self.__deserialize_model(data, klass)",
  "def call_api(self, resource_path, method,\n                 path_params=None, query_params=None, header_params=None,\n                 body=None, post_params=None, files=None,\n                 response_type=None, auth_settings=None, async_req=None,\n                 _return_http_data_only=None, collection_formats=None,\n                 _preload_content=True, _request_timeout=None):\n        \"\"\"Makes the HTTP request (synchronous) and returns deserialized data.\n\n        To make an async request, set the async_req parameter.\n\n        :param resource_path: Path to method endpoint.\n        :param method: Method to call.\n        :param path_params: Path parameters in the url.\n        :param query_params: Query parameters in the url.\n        :param header_params: Header parameters to be\n            placed in the request header.\n        :param body: Request body.\n        :param post_params dict: Request post form parameters,\n            for `application/x-www-form-urlencoded`, `multipart/form-data`.\n        :param auth_settings list: Auth Settings names for the request.\n        :param response: Response data type.\n        :param files dict: key -> filename, value -> filepath,\n            for `multipart/form-data`.\n        :param async_req bool: execute request asynchronously\n        :param _return_http_data_only: response data without head status code\n                                       and headers\n        :param collection_formats: dict of collection formats for path, query,\n            header, and post parameters.\n        :param _preload_content: if False, the urllib3.HTTPResponse object will\n                                 be returned without reading/decoding response\n                                 data. Default is True.\n        :param _request_timeout: timeout setting for this request. If one\n                                 number provided, it will be total request\n                                 timeout. It can also be a pair (tuple) of\n                                 (connection, read) timeouts.\n        :return:\n            If async_req parameter is True,\n            the request will be called asynchronously.\n            The method will return the request thread.\n            If parameter async_req is False or missing,\n            then the method will return the response directly.\n        \"\"\"\n        if not async_req:\n            return self.__call_api(resource_path, method,\n                                   path_params, query_params, header_params,\n                                   body, post_params, files,\n                                   response_type, auth_settings,\n                                   _return_http_data_only, collection_formats,\n                                   _preload_content, _request_timeout)\n        else:\n            thread = self.pool.apply_async(self.__call_api, (resource_path,\n                                           method, path_params, query_params,\n                                           header_params, body,\n                                           post_params, files,\n                                           response_type, auth_settings,\n                                           _return_http_data_only,\n                                           collection_formats,\n                                           _preload_content, _request_timeout))\n        return thread",
  "def request(self, method, url, query_params=None, headers=None,\n                post_params=None, body=None, _preload_content=True,\n                _request_timeout=None):\n        \"\"\"Makes the HTTP request using RESTClient.\"\"\"\n        if method == \"GET\":\n            return self.rest_client.GET(url,\n                                        query_params=query_params,\n                                        _preload_content=_preload_content,\n                                        _request_timeout=_request_timeout,\n                                        headers=headers)\n        elif method == \"HEAD\":\n            return self.rest_client.HEAD(url,\n                                         query_params=query_params,\n                                         _preload_content=_preload_content,\n                                         _request_timeout=_request_timeout,\n                                         headers=headers)\n        elif method == \"OPTIONS\":\n            return self.rest_client.OPTIONS(url,\n                                            query_params=query_params,\n                                            headers=headers,\n                                            post_params=post_params,\n                                            _preload_content=_preload_content,\n                                            _request_timeout=_request_timeout,\n                                            body=body)\n        elif method == \"POST\":\n            return self.rest_client.POST(url,\n                                         query_params=query_params,\n                                         headers=headers,\n                                         post_params=post_params,\n                                         _preload_content=_preload_content,\n                                         _request_timeout=_request_timeout,\n                                         body=body)\n        elif method == \"PUT\":\n            return self.rest_client.PUT(url,\n                                        query_params=query_params,\n                                        headers=headers,\n                                        post_params=post_params,\n                                        _preload_content=_preload_content,\n                                        _request_timeout=_request_timeout,\n                                        body=body)\n        elif method == \"PATCH\":\n            return self.rest_client.PATCH(url,\n                                          query_params=query_params,\n                                          headers=headers,\n                                          post_params=post_params,\n                                          _preload_content=_preload_content,\n                                          _request_timeout=_request_timeout,\n                                          body=body)\n        elif method == \"DELETE\":\n            return self.rest_client.DELETE(url,\n                                           query_params=query_params,\n                                           headers=headers,\n                                           _preload_content=_preload_content,\n                                           _request_timeout=_request_timeout,\n                                           body=body)\n        else:\n            raise ValueError(\n                \"http method must be `GET`, `HEAD`, `OPTIONS`,\"\n                \" `POST`, `PATCH`, `PUT` or `DELETE`.\"\n            )",
  "def parameters_to_tuples(self, params, collection_formats):\n        \"\"\"Get parameters as list of tuples, formatting collections.\n\n        :param params: Parameters as dict or list of two-tuples\n        :param dict collection_formats: Parameter collection formats\n        :return: Parameters as list of tuples, collections formatted\n        \"\"\"\n        new_params = []\n        if collection_formats is None:\n            collection_formats = {}\n        for k, v in six.iteritems(params) if isinstance(params, dict) else params:  # noqa: E501\n            if k in collection_formats:\n                collection_format = collection_formats[k]\n                if collection_format == 'multi':\n                    new_params.extend((k, value) for value in v)\n                else:\n                    if collection_format == 'ssv':\n                        delimiter = ' '\n                    elif collection_format == 'tsv':\n                        delimiter = '\\t'\n                    elif collection_format == 'pipes':\n                        delimiter = '|'\n                    else:  # csv is the default\n                        delimiter = ','\n                    new_params.append(\n                        (k, delimiter.join(str(value) for value in v)))\n            else:\n                new_params.append((k, v))\n        return new_params",
  "def prepare_post_parameters(self, post_params=None, files=None):\n        \"\"\"Builds form parameters.\n\n        :param post_params: Normal form parameters.\n        :param files: File parameters.\n        :return: Form parameters with files.\n        \"\"\"\n        params = []\n\n        if post_params:\n            params = post_params\n\n        if files:\n            for k, v in six.iteritems(files):\n                if not v:\n                    continue\n                file_names = v if type(v) is list else [v]\n                for n in file_names:\n                    with open(n, 'rb') as f:\n                        filename = os.path.basename(f.name)\n                        filedata = f.read()\n                        mimetype = (mimetypes.guess_type(filename)[0] or\n                                    'application/octet-stream')\n                        params.append(\n                            tuple([k, tuple([filename, filedata, mimetype])]))\n\n        return params",
  "def select_header_accept(self, accepts):\n        \"\"\"Returns `Accept` based on an array of accepts provided.\n\n        :param accepts: List of headers.\n        :return: Accept (e.g. application/json).\n        \"\"\"\n        if not accepts:\n            return\n\n        accepts = [x.lower() for x in accepts]\n\n        if 'application/json' in accepts:\n            return 'application/json'\n        else:\n            return ', '.join(accepts)",
  "def select_header_content_type(self, content_types):\n        \"\"\"Returns `Content-Type` based on an array of content_types provided.\n\n        :param content_types: List of content-types.\n        :return: Content-Type (e.g. application/json).\n        \"\"\"\n        if not content_types:\n            return 'application/json'\n\n        content_types = [x.lower() for x in content_types]\n\n        if 'application/json' in content_types or '*/*' in content_types:\n            return 'application/json'\n        else:\n            return content_types[0]",
  "def update_params_for_auth(self, headers, querys, auth_settings):\n        \"\"\"Updates header and query params based on authentication setting.\n\n        :param headers: Header parameters dict to be updated.\n        :param querys: Query parameters tuple list to be updated.\n        :param auth_settings: Authentication setting identifiers list.\n        \"\"\"\n        if not auth_settings:\n            return\n\n        for auth in auth_settings:\n            auth_setting = self.configuration.auth_settings().get(auth)\n            if auth_setting:\n                if not auth_setting['value']:\n                    continue\n                elif auth_setting['in'] == 'header':\n                    headers[auth_setting['key']] = auth_setting['value']\n                elif auth_setting['in'] == 'query':\n                    querys.append((auth_setting['key'], auth_setting['value']))\n                else:\n                    raise ValueError(\n                        'Authentication token must be in `query` or `header`'\n                    )",
  "def __deserialize_file(self, response):\n        \"\"\"Deserializes body to file\n\n        Saves response body into a file in a temporary folder,\n        using the filename from the `Content-Disposition` header if provided.\n\n        :param response:  RESTResponse.\n        :return: file path.\n        \"\"\"\n        fd, path = tempfile.mkstemp(dir=self.configuration.temp_folder_path)\n        os.close(fd)\n        os.remove(path)\n\n        content_disposition = response.getheader(\"Content-Disposition\")\n        if content_disposition:\n            filename = re.search(r'filename=[\\'\"]?([^\\'\"\\s]+)[\\'\"]?',\n                                 content_disposition).group(1)\n            path = os.path.join(os.path.dirname(path), filename)\n\n        with open(path, \"w\") as f:\n            f.write(response.data)\n\n        return path",
  "def __deserialize_primitive(self, data, klass):\n        \"\"\"Deserializes string to primitive type.\n\n        :param data: str.\n        :param klass: class literal.\n\n        :return: int, long, float, str, bool.\n        \"\"\"\n        try:\n            return klass(data)\n        except UnicodeEncodeError:\n            return six.text_type(data)\n        except TypeError:\n            return data",
  "def __deserialize_object(self, value):\n        \"\"\"Return a original value.\n\n        :return: object.\n        \"\"\"\n        return value",
  "def __deserialize_date(self, string):\n        \"\"\"Deserializes string to date.\n\n        :param string: str.\n        :return: date.\n        \"\"\"\n        try:\n            from dateutil.parser import parse\n            return parse(string).date()\n        except ImportError:\n            return string\n        except ValueError:\n            raise rest.ApiException(\n                status=0,\n                reason=\"Failed to parse `{0}` as date object\".format(string)\n            )",
  "def __deserialize_datatime(self, string):\n        \"\"\"Deserializes string to datetime.\n\n        The string should be in iso8601 datetime format.\n\n        :param string: str.\n        :return: datetime.\n        \"\"\"\n        try:\n            from dateutil.parser import parse\n            return parse(string)\n        except ImportError:\n            return string\n        except ValueError:\n            raise rest.ApiException(\n                status=0,\n                reason=(\n                    \"Failed to parse `{0}` as datetime object\"\n                    .format(string)\n                )\n            )",
  "def __hasattr(self, object, name):\n        return name in object.__class__.__dict__",
  "def __deserialize_model(self, data, klass):\n        \"\"\"Deserializes list or dict to model.\n\n        :param data: dict, list.\n        :param klass: class literal.\n        :return: model object.\n        \"\"\"\n\n        if (not klass.swagger_types and\n                not self.__hasattr(klass, 'get_real_child_model')):\n            return data\n\n        kwargs = {}\n        if klass.swagger_types is not None:\n            for attr, attr_type in six.iteritems(klass.swagger_types):\n                if (data is not None and\n                        klass.attribute_map[attr] in data and\n                        isinstance(data, (list, dict))):\n                    value = data[klass.attribute_map[attr]]\n                    kwargs[attr] = self.__deserialize(value, attr_type)\n\n        instance = klass(**kwargs)\n\n        if (isinstance(instance, dict) and\n                klass.swagger_types is not None and\n                isinstance(data, dict)):\n            for key, value in data.items():\n                if key not in klass.swagger_types:\n                    instance[key] = value\n        if self.__hasattr(instance, 'get_real_child_model'):\n            klass_name = instance.get_real_child_model(data)\n            if klass_name:\n                instance = self.__deserialize(data, klass_name)\n        return instance",
  "class DatasetsApi(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    Ref: https://github.com/swagger-api/swagger-codegen\n    \"\"\"\n\n    def __init__(self, api_client=None):\n        if api_client is None:\n            api_client = ApiClient()\n        self.api_client = api_client\n\n    def create_dataset(self, body, **kwargs):  # noqa: E501\n        \"\"\"create_dataset  # noqa: E501\n\n        Creates a new dataset for a user  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_dataset(body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param DatasetCreateRequest body: (required)\n        :return: CreateEntityResponse\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_dataset_with_http_info(body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_dataset_with_http_info(body, **kwargs)  # noqa: E501\n            return data\n\n    def create_dataset_with_http_info(self, body, **kwargs):  # noqa: E501\n        \"\"\"create_dataset  # noqa: E501\n\n        Creates a new dataset for a user  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_dataset_with_http_info(body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param DatasetCreateRequest body: (required)\n        :return: CreateEntityResponse\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['body']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method create_dataset\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'body' is set\n        if self.api_client.client_side_validation and ('body' not in params or\n                                                       params['body'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `body` when calling `create_dataset`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        if 'body' in params:\n            body_params = params['body']\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # HTTP header `Content-Type`\n        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/', 'POST',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='CreateEntityResponse',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)\n\n    def delete_dataset_by_id(self, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"delete_dataset_by_id  # noqa: E501\n\n        Delete a specific dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_dataset_by_id(dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: None\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_dataset_by_id_with_http_info(dataset_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_dataset_by_id_with_http_info(dataset_id, **kwargs)  # noqa: E501\n            return data\n\n    def delete_dataset_by_id_with_http_info(self, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"delete_dataset_by_id  # noqa: E501\n\n        Delete a specific dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_dataset_by_id_with_http_info(dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: None\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method delete_dataset_by_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `delete_dataset_by_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}', 'DELETE',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type=None,  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)\n\n    def get_dataset_by_id(self, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"get_dataset_by_id  # noqa: E501\n\n        Get a specific dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_dataset_by_id(dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: DatasetData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_dataset_by_id_with_http_info(dataset_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_dataset_by_id_with_http_info(dataset_id, **kwargs)  # noqa: E501\n            return data\n\n    def get_dataset_by_id_with_http_info(self, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"get_dataset_by_id  # noqa: E501\n\n        Get a specific dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_dataset_by_id_with_http_info(dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: DatasetData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_dataset_by_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_dataset_by_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='DatasetData',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)\n\n    def get_datasets(self, **kwargs):  # noqa: E501\n        \"\"\"get_datasets  # noqa: E501\n\n        Get all datasets for a user  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_datasets(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :return: list[DatasetData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_datasets_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.get_datasets_with_http_info(**kwargs)  # noqa: E501\n            return data\n\n    def get_datasets_with_http_info(self, **kwargs):  # noqa: E501\n        \"\"\"get_datasets  # noqa: E501\n\n        Get all datasets for a user  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_datasets_with_http_info(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :return: list[DatasetData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = []  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_datasets\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n\n        collection_formats = {}\n\n        path_params = {}\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='list[DatasetData]',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)\n\n    def get_datasets_enriched(self, **kwargs):  # noqa: E501\n        \"\"\"get_datasets_enriched  # noqa: E501\n\n        Get all datasets for a user but enriched with additional information as nTags, nEmbeddings, samples  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_datasets_enriched(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :return: list[DatasetDataEnriched]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_datasets_enriched_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.get_datasets_enriched_with_http_info(**kwargs)  # noqa: E501\n            return data\n\n    def get_datasets_enriched_with_http_info(self, **kwargs):  # noqa: E501\n        \"\"\"get_datasets_enriched  # noqa: E501\n\n        Get all datasets for a user but enriched with additional information as nTags, nEmbeddings, samples  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_datasets_enriched_with_http_info(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :return: list[DatasetDataEnriched]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = []  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_datasets_enriched\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n\n        collection_formats = {}\n\n        path_params = {}\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/enriched', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='list[DatasetDataEnriched]',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)\n\n    def update_dataset_by_id(self, body, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"update_dataset_by_id  # noqa: E501\n\n        Update a specific dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.update_dataset_by_id(body, dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param DatasetUpdateRequest body: updated data for dataset (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: None\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.update_dataset_by_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.update_dataset_by_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501\n            return data\n\n    def update_dataset_by_id_with_http_info(self, body, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"update_dataset_by_id  # noqa: E501\n\n        Update a specific dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.update_dataset_by_id_with_http_info(body, dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param DatasetUpdateRequest body: updated data for dataset (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: None\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['body', 'dataset_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method update_dataset_by_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'body' is set\n        if self.api_client.client_side_validation and ('body' not in params or\n                                                       params['body'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `body` when calling `update_dataset_by_id`\")  # noqa: E501\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `update_dataset_by_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        if 'body' in params:\n            body_params = params['body']\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # HTTP header `Content-Type`\n        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}', 'PUT',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type=None,  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def __init__(self, api_client=None):\n        if api_client is None:\n            api_client = ApiClient()\n        self.api_client = api_client",
  "def create_dataset(self, body, **kwargs):  # noqa: E501\n        \"\"\"create_dataset  # noqa: E501\n\n        Creates a new dataset for a user  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_dataset(body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param DatasetCreateRequest body: (required)\n        :return: CreateEntityResponse\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_dataset_with_http_info(body, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_dataset_with_http_info(body, **kwargs)  # noqa: E501\n            return data",
  "def create_dataset_with_http_info(self, body, **kwargs):  # noqa: E501\n        \"\"\"create_dataset  # noqa: E501\n\n        Creates a new dataset for a user  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_dataset_with_http_info(body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param DatasetCreateRequest body: (required)\n        :return: CreateEntityResponse\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['body']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method create_dataset\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'body' is set\n        if self.api_client.client_side_validation and ('body' not in params or\n                                                       params['body'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `body` when calling `create_dataset`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        if 'body' in params:\n            body_params = params['body']\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # HTTP header `Content-Type`\n        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/', 'POST',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='CreateEntityResponse',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def delete_dataset_by_id(self, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"delete_dataset_by_id  # noqa: E501\n\n        Delete a specific dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_dataset_by_id(dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: None\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_dataset_by_id_with_http_info(dataset_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_dataset_by_id_with_http_info(dataset_id, **kwargs)  # noqa: E501\n            return data",
  "def delete_dataset_by_id_with_http_info(self, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"delete_dataset_by_id  # noqa: E501\n\n        Delete a specific dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_dataset_by_id_with_http_info(dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: None\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method delete_dataset_by_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `delete_dataset_by_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}', 'DELETE',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type=None,  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def get_dataset_by_id(self, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"get_dataset_by_id  # noqa: E501\n\n        Get a specific dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_dataset_by_id(dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: DatasetData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_dataset_by_id_with_http_info(dataset_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_dataset_by_id_with_http_info(dataset_id, **kwargs)  # noqa: E501\n            return data",
  "def get_dataset_by_id_with_http_info(self, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"get_dataset_by_id  # noqa: E501\n\n        Get a specific dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_dataset_by_id_with_http_info(dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: DatasetData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_dataset_by_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_dataset_by_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='DatasetData',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def get_datasets(self, **kwargs):  # noqa: E501\n        \"\"\"get_datasets  # noqa: E501\n\n        Get all datasets for a user  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_datasets(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :return: list[DatasetData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_datasets_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.get_datasets_with_http_info(**kwargs)  # noqa: E501\n            return data",
  "def get_datasets_with_http_info(self, **kwargs):  # noqa: E501\n        \"\"\"get_datasets  # noqa: E501\n\n        Get all datasets for a user  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_datasets_with_http_info(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :return: list[DatasetData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = []  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_datasets\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n\n        collection_formats = {}\n\n        path_params = {}\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='list[DatasetData]',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def get_datasets_enriched(self, **kwargs):  # noqa: E501\n        \"\"\"get_datasets_enriched  # noqa: E501\n\n        Get all datasets for a user but enriched with additional information as nTags, nEmbeddings, samples  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_datasets_enriched(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :return: list[DatasetDataEnriched]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_datasets_enriched_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.get_datasets_enriched_with_http_info(**kwargs)  # noqa: E501\n            return data",
  "def get_datasets_enriched_with_http_info(self, **kwargs):  # noqa: E501\n        \"\"\"get_datasets_enriched  # noqa: E501\n\n        Get all datasets for a user but enriched with additional information as nTags, nEmbeddings, samples  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_datasets_enriched_with_http_info(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :return: list[DatasetDataEnriched]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = []  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_datasets_enriched\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n\n        collection_formats = {}\n\n        path_params = {}\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/enriched', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='list[DatasetDataEnriched]',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def update_dataset_by_id(self, body, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"update_dataset_by_id  # noqa: E501\n\n        Update a specific dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.update_dataset_by_id(body, dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param DatasetUpdateRequest body: updated data for dataset (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: None\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.update_dataset_by_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.update_dataset_by_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501\n            return data",
  "def update_dataset_by_id_with_http_info(self, body, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"update_dataset_by_id  # noqa: E501\n\n        Update a specific dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.update_dataset_by_id_with_http_info(body, dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param DatasetUpdateRequest body: updated data for dataset (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: None\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['body', 'dataset_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method update_dataset_by_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'body' is set\n        if self.api_client.client_side_validation and ('body' not in params or\n                                                       params['body'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `body` when calling `update_dataset_by_id`\")  # noqa: E501\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `update_dataset_by_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        if 'body' in params:\n            body_params = params['body']\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # HTTP header `Content-Type`\n        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}', 'PUT',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type=None,  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "class QuotaApi(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    Ref: https://github.com/swagger-api/swagger-codegen\n    \"\"\"\n\n    def __init__(self, api_client=None):\n        if api_client is None:\n            api_client = ApiClient()\n        self.api_client = api_client\n\n    def get_quota_maximum_dataset_size(self, **kwargs):  # noqa: E501\n        \"\"\"get_quota_maximum_dataset_size  # noqa: E501\n\n        Get quota of the current user for the maximum dataset size  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_quota_maximum_dataset_size(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_quota_maximum_dataset_size_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.get_quota_maximum_dataset_size_with_http_info(**kwargs)  # noqa: E501\n            return data\n\n    def get_quota_maximum_dataset_size_with_http_info(self, **kwargs):  # noqa: E501\n        \"\"\"get_quota_maximum_dataset_size  # noqa: E501\n\n        Get quota of the current user for the maximum dataset size  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_quota_maximum_dataset_size_with_http_info(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = []  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_quota_maximum_dataset_size\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n\n        collection_formats = {}\n\n        path_params = {}\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/quota', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='str',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def __init__(self, api_client=None):\n        if api_client is None:\n            api_client = ApiClient()\n        self.api_client = api_client",
  "def get_quota_maximum_dataset_size(self, **kwargs):  # noqa: E501\n        \"\"\"get_quota_maximum_dataset_size  # noqa: E501\n\n        Get quota of the current user for the maximum dataset size  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_quota_maximum_dataset_size(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_quota_maximum_dataset_size_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.get_quota_maximum_dataset_size_with_http_info(**kwargs)  # noqa: E501\n            return data",
  "def get_quota_maximum_dataset_size_with_http_info(self, **kwargs):  # noqa: E501\n        \"\"\"get_quota_maximum_dataset_size  # noqa: E501\n\n        Get quota of the current user for the maximum dataset size  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_quota_maximum_dataset_size_with_http_info(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = []  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_quota_maximum_dataset_size\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n\n        collection_formats = {}\n\n        path_params = {}\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/quota', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='str',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "class MappingsApi(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    Ref: https://github.com/swagger-api/swagger-codegen\n    \"\"\"\n\n    def __init__(self, api_client=None):\n        if api_client is None:\n            api_client = ApiClient()\n        self.api_client = api_client\n\n    def get_sample_mappings_by_dataset_id(self, dataset_id, field, **kwargs):  # noqa: E501\n        \"\"\"get_sample_mappings_by_dataset_id  # noqa: E501\n\n        Get all samples of a dataset as a list. List index is the index of the sample2bitmask mapping and the value is the 'field' you wanted (e.g _id, fileName)  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_sample_mappings_by_dataset_id(dataset_id, field, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param str field: the field to return as the value (required)\n        :return: list[str]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_sample_mappings_by_dataset_id_with_http_info(dataset_id, field, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_sample_mappings_by_dataset_id_with_http_info(dataset_id, field, **kwargs)  # noqa: E501\n            return data\n\n    def get_sample_mappings_by_dataset_id_with_http_info(self, dataset_id, field, **kwargs):  # noqa: E501\n        \"\"\"get_sample_mappings_by_dataset_id  # noqa: E501\n\n        Get all samples of a dataset as a list. List index is the index of the sample2bitmask mapping and the value is the 'field' you wanted (e.g _id, fileName)  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_sample_mappings_by_dataset_id_with_http_info(dataset_id, field, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param str field: the field to return as the value (required)\n        :return: list[str]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id', 'field']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_sample_mappings_by_dataset_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_sample_mappings_by_dataset_id`\")  # noqa: E501\n        # verify the required parameter 'field' is set\n        if self.api_client.client_side_validation and ('field' not in params or\n                                                       params['field'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `field` when calling `get_sample_mappings_by_dataset_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n\n        query_params = []\n        if 'field' in params:\n            query_params.append(('field', params['field']))  # noqa: E501\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/mappings', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='list[str]',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def __init__(self, api_client=None):\n        if api_client is None:\n            api_client = ApiClient()\n        self.api_client = api_client",
  "def get_sample_mappings_by_dataset_id(self, dataset_id, field, **kwargs):  # noqa: E501\n        \"\"\"get_sample_mappings_by_dataset_id  # noqa: E501\n\n        Get all samples of a dataset as a list. List index is the index of the sample2bitmask mapping and the value is the 'field' you wanted (e.g _id, fileName)  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_sample_mappings_by_dataset_id(dataset_id, field, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param str field: the field to return as the value (required)\n        :return: list[str]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_sample_mappings_by_dataset_id_with_http_info(dataset_id, field, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_sample_mappings_by_dataset_id_with_http_info(dataset_id, field, **kwargs)  # noqa: E501\n            return data",
  "def get_sample_mappings_by_dataset_id_with_http_info(self, dataset_id, field, **kwargs):  # noqa: E501\n        \"\"\"get_sample_mappings_by_dataset_id  # noqa: E501\n\n        Get all samples of a dataset as a list. List index is the index of the sample2bitmask mapping and the value is the 'field' you wanted (e.g _id, fileName)  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_sample_mappings_by_dataset_id_with_http_info(dataset_id, field, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param str field: the field to return as the value (required)\n        :return: list[str]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id', 'field']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_sample_mappings_by_dataset_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_sample_mappings_by_dataset_id`\")  # noqa: E501\n        # verify the required parameter 'field' is set\n        if self.api_client.client_side_validation and ('field' not in params or\n                                                       params['field'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `field` when calling `get_sample_mappings_by_dataset_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n\n        query_params = []\n        if 'field' in params:\n            query_params.append(('field', params['field']))  # noqa: E501\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/mappings', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='list[str]',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "class EmbeddingsApi(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    Ref: https://github.com/swagger-api/swagger-codegen\n    \"\"\"\n\n    def __init__(self, api_client=None):\n        if api_client is None:\n            api_client = ApiClient()\n        self.api_client = api_client\n\n    def get_embeddings_by_dataset_id(self, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"get_embeddings_by_dataset_id  # noqa: E501\n\n        Get all annotations of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_embeddings_by_dataset_id(dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: list[DatasetEmbeddingData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_embeddings_by_dataset_id_with_http_info(dataset_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_embeddings_by_dataset_id_with_http_info(dataset_id, **kwargs)  # noqa: E501\n            return data\n\n    def get_embeddings_by_dataset_id_with_http_info(self, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"get_embeddings_by_dataset_id  # noqa: E501\n\n        Get all annotations of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_embeddings_by_dataset_id_with_http_info(dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: list[DatasetEmbeddingData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_embeddings_by_dataset_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_embeddings_by_dataset_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/embeddings', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='list[DatasetEmbeddingData]',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)\n\n    def get_embeddings_by_sample_id(self, dataset_id, sample_id, **kwargs):  # noqa: E501\n        \"\"\"get_embeddings_by_sample_id  # noqa: E501\n\n        Get all embeddings of a datasets sample  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_embeddings_by_sample_id(dataset_id, sample_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID sample_id: ObjectId of the sample (required)\n        :param str mode: if we want everything (full) or just the summaries\n        :return: list[EmbeddingData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_embeddings_by_sample_id_with_http_info(dataset_id, sample_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_embeddings_by_sample_id_with_http_info(dataset_id, sample_id, **kwargs)  # noqa: E501\n            return data\n\n    def get_embeddings_by_sample_id_with_http_info(self, dataset_id, sample_id, **kwargs):  # noqa: E501\n        \"\"\"get_embeddings_by_sample_id  # noqa: E501\n\n        Get all embeddings of a datasets sample  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_embeddings_by_sample_id_with_http_info(dataset_id, sample_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID sample_id: ObjectId of the sample (required)\n        :param str mode: if we want everything (full) or just the summaries\n        :return: list[EmbeddingData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id', 'sample_id', 'mode']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_embeddings_by_sample_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_embeddings_by_sample_id`\")  # noqa: E501\n        # verify the required parameter 'sample_id' is set\n        if self.api_client.client_side_validation and ('sample_id' not in params or\n                                                       params['sample_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `sample_id` when calling `get_embeddings_by_sample_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'sample_id' in params:\n            path_params['sampleId'] = params['sample_id']  # noqa: E501\n\n        query_params = []\n        if 'mode' in params:\n            query_params.append(('mode', params['mode']))  # noqa: E501\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/users/datasets/{datasetId}/samples/{sampleId}/embeddings', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='list[EmbeddingData]',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)\n\n    def get_embeddings_csv_write_url_by_id(self, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"get_embeddings_csv_write_url_by_id  # noqa: E501\n\n        Get the signed url to upload an CSVembedding to for a specific dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_embeddings_csv_write_url_by_id(dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param str name: the sampling requests name to create a signed url for\n        :return: WriteCSVUrlData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_embeddings_csv_write_url_by_id_with_http_info(dataset_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_embeddings_csv_write_url_by_id_with_http_info(dataset_id, **kwargs)  # noqa: E501\n            return data\n\n    def get_embeddings_csv_write_url_by_id_with_http_info(self, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"get_embeddings_csv_write_url_by_id  # noqa: E501\n\n        Get the signed url to upload an CSVembedding to for a specific dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_embeddings_csv_write_url_by_id_with_http_info(dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param str name: the sampling requests name to create a signed url for\n        :return: WriteCSVUrlData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id', 'name']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_embeddings_csv_write_url_by_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_embeddings_csv_write_url_by_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n\n        query_params = []\n        if 'name' in params:\n            query_params.append(('name', params['name']))  # noqa: E501\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/embeddings/writeCSVUrl', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='WriteCSVUrlData',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)\n\n    def set_embeddings_is_processed_flag_by_id(self, body, dataset_id, embedding_id, **kwargs):  # noqa: E501\n        \"\"\"set_embeddings_is_processed_flag_by_id  # noqa: E501\n\n        Sets the isProcessed flag of the specified embedding  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.set_embeddings_is_processed_flag_by_id(body, dataset_id, embedding_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param Body body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID embedding_id: ObjectId of the embedding (required)\n        :return: None\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.set_embeddings_is_processed_flag_by_id_with_http_info(body, dataset_id, embedding_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.set_embeddings_is_processed_flag_by_id_with_http_info(body, dataset_id, embedding_id, **kwargs)  # noqa: E501\n            return data\n\n    def set_embeddings_is_processed_flag_by_id_with_http_info(self, body, dataset_id, embedding_id, **kwargs):  # noqa: E501\n        \"\"\"set_embeddings_is_processed_flag_by_id  # noqa: E501\n\n        Sets the isProcessed flag of the specified embedding  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.set_embeddings_is_processed_flag_by_id_with_http_info(body, dataset_id, embedding_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param Body body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID embedding_id: ObjectId of the embedding (required)\n        :return: None\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['body', 'dataset_id', 'embedding_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method set_embeddings_is_processed_flag_by_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'body' is set\n        if self.api_client.client_side_validation and ('body' not in params or\n                                                       params['body'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `body` when calling `set_embeddings_is_processed_flag_by_id`\")  # noqa: E501\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `set_embeddings_is_processed_flag_by_id`\")  # noqa: E501\n        # verify the required parameter 'embedding_id' is set\n        if self.api_client.client_side_validation and ('embedding_id' not in params or\n                                                       params['embedding_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `embedding_id` when calling `set_embeddings_is_processed_flag_by_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'embedding_id' in params:\n            path_params['embeddingId'] = params['embedding_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        if 'body' in params:\n            body_params = params['body']\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # HTTP header `Content-Type`\n        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/embeddings/{embeddingId}/isProcessed', 'PUT',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type=None,  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def __init__(self, api_client=None):\n        if api_client is None:\n            api_client = ApiClient()\n        self.api_client = api_client",
  "def get_embeddings_by_dataset_id(self, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"get_embeddings_by_dataset_id  # noqa: E501\n\n        Get all annotations of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_embeddings_by_dataset_id(dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: list[DatasetEmbeddingData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_embeddings_by_dataset_id_with_http_info(dataset_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_embeddings_by_dataset_id_with_http_info(dataset_id, **kwargs)  # noqa: E501\n            return data",
  "def get_embeddings_by_dataset_id_with_http_info(self, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"get_embeddings_by_dataset_id  # noqa: E501\n\n        Get all annotations of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_embeddings_by_dataset_id_with_http_info(dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: list[DatasetEmbeddingData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_embeddings_by_dataset_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_embeddings_by_dataset_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/embeddings', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='list[DatasetEmbeddingData]',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def get_embeddings_by_sample_id(self, dataset_id, sample_id, **kwargs):  # noqa: E501\n        \"\"\"get_embeddings_by_sample_id  # noqa: E501\n\n        Get all embeddings of a datasets sample  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_embeddings_by_sample_id(dataset_id, sample_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID sample_id: ObjectId of the sample (required)\n        :param str mode: if we want everything (full) or just the summaries\n        :return: list[EmbeddingData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_embeddings_by_sample_id_with_http_info(dataset_id, sample_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_embeddings_by_sample_id_with_http_info(dataset_id, sample_id, **kwargs)  # noqa: E501\n            return data",
  "def get_embeddings_by_sample_id_with_http_info(self, dataset_id, sample_id, **kwargs):  # noqa: E501\n        \"\"\"get_embeddings_by_sample_id  # noqa: E501\n\n        Get all embeddings of a datasets sample  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_embeddings_by_sample_id_with_http_info(dataset_id, sample_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID sample_id: ObjectId of the sample (required)\n        :param str mode: if we want everything (full) or just the summaries\n        :return: list[EmbeddingData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id', 'sample_id', 'mode']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_embeddings_by_sample_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_embeddings_by_sample_id`\")  # noqa: E501\n        # verify the required parameter 'sample_id' is set\n        if self.api_client.client_side_validation and ('sample_id' not in params or\n                                                       params['sample_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `sample_id` when calling `get_embeddings_by_sample_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'sample_id' in params:\n            path_params['sampleId'] = params['sample_id']  # noqa: E501\n\n        query_params = []\n        if 'mode' in params:\n            query_params.append(('mode', params['mode']))  # noqa: E501\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/users/datasets/{datasetId}/samples/{sampleId}/embeddings', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='list[EmbeddingData]',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def get_embeddings_csv_write_url_by_id(self, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"get_embeddings_csv_write_url_by_id  # noqa: E501\n\n        Get the signed url to upload an CSVembedding to for a specific dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_embeddings_csv_write_url_by_id(dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param str name: the sampling requests name to create a signed url for\n        :return: WriteCSVUrlData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_embeddings_csv_write_url_by_id_with_http_info(dataset_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_embeddings_csv_write_url_by_id_with_http_info(dataset_id, **kwargs)  # noqa: E501\n            return data",
  "def get_embeddings_csv_write_url_by_id_with_http_info(self, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"get_embeddings_csv_write_url_by_id  # noqa: E501\n\n        Get the signed url to upload an CSVembedding to for a specific dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_embeddings_csv_write_url_by_id_with_http_info(dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param str name: the sampling requests name to create a signed url for\n        :return: WriteCSVUrlData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id', 'name']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_embeddings_csv_write_url_by_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_embeddings_csv_write_url_by_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n\n        query_params = []\n        if 'name' in params:\n            query_params.append(('name', params['name']))  # noqa: E501\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/embeddings/writeCSVUrl', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='WriteCSVUrlData',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def set_embeddings_is_processed_flag_by_id(self, body, dataset_id, embedding_id, **kwargs):  # noqa: E501\n        \"\"\"set_embeddings_is_processed_flag_by_id  # noqa: E501\n\n        Sets the isProcessed flag of the specified embedding  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.set_embeddings_is_processed_flag_by_id(body, dataset_id, embedding_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param Body body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID embedding_id: ObjectId of the embedding (required)\n        :return: None\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.set_embeddings_is_processed_flag_by_id_with_http_info(body, dataset_id, embedding_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.set_embeddings_is_processed_flag_by_id_with_http_info(body, dataset_id, embedding_id, **kwargs)  # noqa: E501\n            return data",
  "def set_embeddings_is_processed_flag_by_id_with_http_info(self, body, dataset_id, embedding_id, **kwargs):  # noqa: E501\n        \"\"\"set_embeddings_is_processed_flag_by_id  # noqa: E501\n\n        Sets the isProcessed flag of the specified embedding  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.set_embeddings_is_processed_flag_by_id_with_http_info(body, dataset_id, embedding_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param Body body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID embedding_id: ObjectId of the embedding (required)\n        :return: None\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['body', 'dataset_id', 'embedding_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method set_embeddings_is_processed_flag_by_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'body' is set\n        if self.api_client.client_side_validation and ('body' not in params or\n                                                       params['body'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `body` when calling `set_embeddings_is_processed_flag_by_id`\")  # noqa: E501\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `set_embeddings_is_processed_flag_by_id`\")  # noqa: E501\n        # verify the required parameter 'embedding_id' is set\n        if self.api_client.client_side_validation and ('embedding_id' not in params or\n                                                       params['embedding_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `embedding_id` when calling `set_embeddings_is_processed_flag_by_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'embedding_id' in params:\n            path_params['embeddingId'] = params['embedding_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        if 'body' in params:\n            body_params = params['body']\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # HTTP header `Content-Type`\n        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/embeddings/{embeddingId}/isProcessed', 'PUT',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type=None,  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "class Embeddings2dApi(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    Ref: https://github.com/swagger-api/swagger-codegen\n    \"\"\"\n\n    def __init__(self, api_client=None):\n        if api_client is None:\n            api_client = ApiClient()\n        self.api_client = api_client\n\n    def create_embeddings2d_by_embedding_id(self, body, dataset_id, embedding_id, **kwargs):  # noqa: E501\n        \"\"\"create_embeddings2d_by_embedding_id  # noqa: E501\n\n        Create a new 2d embedding  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_embeddings2d_by_embedding_id(body, dataset_id, embedding_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param Embedding2dCreateRequest body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID embedding_id: ObjectId of the embedding (required)\n        :return: CreateEntityResponse\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_embeddings2d_by_embedding_id_with_http_info(body, dataset_id, embedding_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_embeddings2d_by_embedding_id_with_http_info(body, dataset_id, embedding_id, **kwargs)  # noqa: E501\n            return data\n\n    def create_embeddings2d_by_embedding_id_with_http_info(self, body, dataset_id, embedding_id, **kwargs):  # noqa: E501\n        \"\"\"create_embeddings2d_by_embedding_id  # noqa: E501\n\n        Create a new 2d embedding  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_embeddings2d_by_embedding_id_with_http_info(body, dataset_id, embedding_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param Embedding2dCreateRequest body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID embedding_id: ObjectId of the embedding (required)\n        :return: CreateEntityResponse\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['body', 'dataset_id', 'embedding_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method create_embeddings2d_by_embedding_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'body' is set\n        if self.api_client.client_side_validation and ('body' not in params or\n                                                       params['body'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `body` when calling `create_embeddings2d_by_embedding_id`\")  # noqa: E501\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `create_embeddings2d_by_embedding_id`\")  # noqa: E501\n        # verify the required parameter 'embedding_id' is set\n        if self.api_client.client_side_validation and ('embedding_id' not in params or\n                                                       params['embedding_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `embedding_id` when calling `create_embeddings2d_by_embedding_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'embedding_id' in params:\n            path_params['embeddingId'] = params['embedding_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        if 'body' in params:\n            body_params = params['body']\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # HTTP header `Content-Type`\n        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/embeddings/{embeddingId}/2d', 'POST',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='CreateEntityResponse',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)\n\n    def get_embedding2d_by_id(self, dataset_id, embedding_id, embedding2d_id, **kwargs):  # noqa: E501\n        \"\"\"get_embedding2d_by_id  # noqa: E501\n\n        Get the 2d embeddings by id  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_embedding2d_by_id(dataset_id, embedding_id, embedding2d_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID embedding_id: ObjectId of the embedding (required)\n        :param MongoObjectID embedding2d_id: ObjectId of the 2d embedding (required)\n        :return: Embedding2dData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_embedding2d_by_id_with_http_info(dataset_id, embedding_id, embedding2d_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_embedding2d_by_id_with_http_info(dataset_id, embedding_id, embedding2d_id, **kwargs)  # noqa: E501\n            return data\n\n    def get_embedding2d_by_id_with_http_info(self, dataset_id, embedding_id, embedding2d_id, **kwargs):  # noqa: E501\n        \"\"\"get_embedding2d_by_id  # noqa: E501\n\n        Get the 2d embeddings by id  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_embedding2d_by_id_with_http_info(dataset_id, embedding_id, embedding2d_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID embedding_id: ObjectId of the embedding (required)\n        :param MongoObjectID embedding2d_id: ObjectId of the 2d embedding (required)\n        :return: Embedding2dData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id', 'embedding_id', 'embedding2d_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_embedding2d_by_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_embedding2d_by_id`\")  # noqa: E501\n        # verify the required parameter 'embedding_id' is set\n        if self.api_client.client_side_validation and ('embedding_id' not in params or\n                                                       params['embedding_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `embedding_id` when calling `get_embedding2d_by_id`\")  # noqa: E501\n        # verify the required parameter 'embedding2d_id' is set\n        if self.api_client.client_side_validation and ('embedding2d_id' not in params or\n                                                       params['embedding2d_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `embedding2d_id` when calling `get_embedding2d_by_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'embedding_id' in params:\n            path_params['embeddingId'] = params['embedding_id']  # noqa: E501\n        if 'embedding2d_id' in params:\n            path_params['embedding2dId'] = params['embedding2d_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/embeddings/{embeddingId}/2d/{embedding2dId}', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='Embedding2dData',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)\n\n    def get_embeddings2d_by_embedding_id(self, dataset_id, embedding_id, **kwargs):  # noqa: E501\n        \"\"\"get_embeddings2d_by_embedding_id  # noqa: E501\n\n        Get all 2d embeddings of an embedding  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_embeddings2d_by_embedding_id(dataset_id, embedding_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID embedding_id: ObjectId of the embedding (required)\n        :return: list[Embedding2dData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_embeddings2d_by_embedding_id_with_http_info(dataset_id, embedding_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_embeddings2d_by_embedding_id_with_http_info(dataset_id, embedding_id, **kwargs)  # noqa: E501\n            return data\n\n    def get_embeddings2d_by_embedding_id_with_http_info(self, dataset_id, embedding_id, **kwargs):  # noqa: E501\n        \"\"\"get_embeddings2d_by_embedding_id  # noqa: E501\n\n        Get all 2d embeddings of an embedding  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_embeddings2d_by_embedding_id_with_http_info(dataset_id, embedding_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID embedding_id: ObjectId of the embedding (required)\n        :return: list[Embedding2dData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id', 'embedding_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_embeddings2d_by_embedding_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_embeddings2d_by_embedding_id`\")  # noqa: E501\n        # verify the required parameter 'embedding_id' is set\n        if self.api_client.client_side_validation and ('embedding_id' not in params or\n                                                       params['embedding_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `embedding_id` when calling `get_embeddings2d_by_embedding_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'embedding_id' in params:\n            path_params['embeddingId'] = params['embedding_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/embeddings/{embeddingId}/2d', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='list[Embedding2dData]',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def __init__(self, api_client=None):\n        if api_client is None:\n            api_client = ApiClient()\n        self.api_client = api_client",
  "def create_embeddings2d_by_embedding_id(self, body, dataset_id, embedding_id, **kwargs):  # noqa: E501\n        \"\"\"create_embeddings2d_by_embedding_id  # noqa: E501\n\n        Create a new 2d embedding  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_embeddings2d_by_embedding_id(body, dataset_id, embedding_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param Embedding2dCreateRequest body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID embedding_id: ObjectId of the embedding (required)\n        :return: CreateEntityResponse\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_embeddings2d_by_embedding_id_with_http_info(body, dataset_id, embedding_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_embeddings2d_by_embedding_id_with_http_info(body, dataset_id, embedding_id, **kwargs)  # noqa: E501\n            return data",
  "def create_embeddings2d_by_embedding_id_with_http_info(self, body, dataset_id, embedding_id, **kwargs):  # noqa: E501\n        \"\"\"create_embeddings2d_by_embedding_id  # noqa: E501\n\n        Create a new 2d embedding  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_embeddings2d_by_embedding_id_with_http_info(body, dataset_id, embedding_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param Embedding2dCreateRequest body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID embedding_id: ObjectId of the embedding (required)\n        :return: CreateEntityResponse\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['body', 'dataset_id', 'embedding_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method create_embeddings2d_by_embedding_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'body' is set\n        if self.api_client.client_side_validation and ('body' not in params or\n                                                       params['body'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `body` when calling `create_embeddings2d_by_embedding_id`\")  # noqa: E501\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `create_embeddings2d_by_embedding_id`\")  # noqa: E501\n        # verify the required parameter 'embedding_id' is set\n        if self.api_client.client_side_validation and ('embedding_id' not in params or\n                                                       params['embedding_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `embedding_id` when calling `create_embeddings2d_by_embedding_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'embedding_id' in params:\n            path_params['embeddingId'] = params['embedding_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        if 'body' in params:\n            body_params = params['body']\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # HTTP header `Content-Type`\n        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/embeddings/{embeddingId}/2d', 'POST',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='CreateEntityResponse',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def get_embedding2d_by_id(self, dataset_id, embedding_id, embedding2d_id, **kwargs):  # noqa: E501\n        \"\"\"get_embedding2d_by_id  # noqa: E501\n\n        Get the 2d embeddings by id  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_embedding2d_by_id(dataset_id, embedding_id, embedding2d_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID embedding_id: ObjectId of the embedding (required)\n        :param MongoObjectID embedding2d_id: ObjectId of the 2d embedding (required)\n        :return: Embedding2dData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_embedding2d_by_id_with_http_info(dataset_id, embedding_id, embedding2d_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_embedding2d_by_id_with_http_info(dataset_id, embedding_id, embedding2d_id, **kwargs)  # noqa: E501\n            return data",
  "def get_embedding2d_by_id_with_http_info(self, dataset_id, embedding_id, embedding2d_id, **kwargs):  # noqa: E501\n        \"\"\"get_embedding2d_by_id  # noqa: E501\n\n        Get the 2d embeddings by id  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_embedding2d_by_id_with_http_info(dataset_id, embedding_id, embedding2d_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID embedding_id: ObjectId of the embedding (required)\n        :param MongoObjectID embedding2d_id: ObjectId of the 2d embedding (required)\n        :return: Embedding2dData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id', 'embedding_id', 'embedding2d_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_embedding2d_by_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_embedding2d_by_id`\")  # noqa: E501\n        # verify the required parameter 'embedding_id' is set\n        if self.api_client.client_side_validation and ('embedding_id' not in params or\n                                                       params['embedding_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `embedding_id` when calling `get_embedding2d_by_id`\")  # noqa: E501\n        # verify the required parameter 'embedding2d_id' is set\n        if self.api_client.client_side_validation and ('embedding2d_id' not in params or\n                                                       params['embedding2d_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `embedding2d_id` when calling `get_embedding2d_by_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'embedding_id' in params:\n            path_params['embeddingId'] = params['embedding_id']  # noqa: E501\n        if 'embedding2d_id' in params:\n            path_params['embedding2dId'] = params['embedding2d_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/embeddings/{embeddingId}/2d/{embedding2dId}', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='Embedding2dData',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def get_embeddings2d_by_embedding_id(self, dataset_id, embedding_id, **kwargs):  # noqa: E501\n        \"\"\"get_embeddings2d_by_embedding_id  # noqa: E501\n\n        Get all 2d embeddings of an embedding  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_embeddings2d_by_embedding_id(dataset_id, embedding_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID embedding_id: ObjectId of the embedding (required)\n        :return: list[Embedding2dData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_embeddings2d_by_embedding_id_with_http_info(dataset_id, embedding_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_embeddings2d_by_embedding_id_with_http_info(dataset_id, embedding_id, **kwargs)  # noqa: E501\n            return data",
  "def get_embeddings2d_by_embedding_id_with_http_info(self, dataset_id, embedding_id, **kwargs):  # noqa: E501\n        \"\"\"get_embeddings2d_by_embedding_id  # noqa: E501\n\n        Get all 2d embeddings of an embedding  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_embeddings2d_by_embedding_id_with_http_info(dataset_id, embedding_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID embedding_id: ObjectId of the embedding (required)\n        :return: list[Embedding2dData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id', 'embedding_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_embeddings2d_by_embedding_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_embeddings2d_by_embedding_id`\")  # noqa: E501\n        # verify the required parameter 'embedding_id' is set\n        if self.api_client.client_side_validation and ('embedding_id' not in params or\n                                                       params['embedding_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `embedding_id` when calling `get_embeddings2d_by_embedding_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'embedding_id' in params:\n            path_params['embeddingId'] = params['embedding_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/embeddings/{embeddingId}/2d', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='list[Embedding2dData]',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "class ScoresApi(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    Ref: https://github.com/swagger-api/swagger-codegen\n    \"\"\"\n\n    def __init__(self, api_client=None):\n        if api_client is None:\n            api_client = ApiClient()\n        self.api_client = api_client\n\n    def create_or_update_active_learning_score_by_tag_id(self, body, dataset_id, tag_id, **kwargs):  # noqa: E501\n        \"\"\"create_or_update_active_learning_score_by_tag_id  # noqa: E501\n\n        Create or update active learning score object by tag id  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_or_update_active_learning_score_by_tag_id(body, dataset_id, tag_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param ActiveLearningScoreCreateRequest body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID tag_id: ObjectId of the tag (required)\n        :return: CreateEntityResponse\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_or_update_active_learning_score_by_tag_id_with_http_info(body, dataset_id, tag_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_or_update_active_learning_score_by_tag_id_with_http_info(body, dataset_id, tag_id, **kwargs)  # noqa: E501\n            return data\n\n    def create_or_update_active_learning_score_by_tag_id_with_http_info(self, body, dataset_id, tag_id, **kwargs):  # noqa: E501\n        \"\"\"create_or_update_active_learning_score_by_tag_id  # noqa: E501\n\n        Create or update active learning score object by tag id  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_or_update_active_learning_score_by_tag_id_with_http_info(body, dataset_id, tag_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param ActiveLearningScoreCreateRequest body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID tag_id: ObjectId of the tag (required)\n        :return: CreateEntityResponse\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['body', 'dataset_id', 'tag_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method create_or_update_active_learning_score_by_tag_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'body' is set\n        if self.api_client.client_side_validation and ('body' not in params or\n                                                       params['body'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `body` when calling `create_or_update_active_learning_score_by_tag_id`\")  # noqa: E501\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `create_or_update_active_learning_score_by_tag_id`\")  # noqa: E501\n        # verify the required parameter 'tag_id' is set\n        if self.api_client.client_side_validation and ('tag_id' not in params or\n                                                       params['tag_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `tag_id` when calling `create_or_update_active_learning_score_by_tag_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'tag_id' in params:\n            path_params['tagId'] = params['tag_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        if 'body' in params:\n            body_params = params['body']\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # HTTP header `Content-Type`\n        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/tags/{tagId}/scores', 'POST',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='CreateEntityResponse',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)\n\n    def get_active_learning_score_by_score_id(self, dataset_id, tag_id, score_id, **kwargs):  # noqa: E501\n        \"\"\"get_active_learning_score_by_score_id  # noqa: E501\n\n        Get active learning score object by id  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_active_learning_score_by_score_id(dataset_id, tag_id, score_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID tag_id: ObjectId of the tag (required)\n        :param MongoObjectID score_id: ObjectId of the scores (required)\n        :return: ActiveLearningScoreData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_active_learning_score_by_score_id_with_http_info(dataset_id, tag_id, score_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_active_learning_score_by_score_id_with_http_info(dataset_id, tag_id, score_id, **kwargs)  # noqa: E501\n            return data\n\n    def get_active_learning_score_by_score_id_with_http_info(self, dataset_id, tag_id, score_id, **kwargs):  # noqa: E501\n        \"\"\"get_active_learning_score_by_score_id  # noqa: E501\n\n        Get active learning score object by id  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_active_learning_score_by_score_id_with_http_info(dataset_id, tag_id, score_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID tag_id: ObjectId of the tag (required)\n        :param MongoObjectID score_id: ObjectId of the scores (required)\n        :return: ActiveLearningScoreData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id', 'tag_id', 'score_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_active_learning_score_by_score_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_active_learning_score_by_score_id`\")  # noqa: E501\n        # verify the required parameter 'tag_id' is set\n        if self.api_client.client_side_validation and ('tag_id' not in params or\n                                                       params['tag_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `tag_id` when calling `get_active_learning_score_by_score_id`\")  # noqa: E501\n        # verify the required parameter 'score_id' is set\n        if self.api_client.client_side_validation and ('score_id' not in params or\n                                                       params['score_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `score_id` when calling `get_active_learning_score_by_score_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'tag_id' in params:\n            path_params['tagId'] = params['tag_id']  # noqa: E501\n        if 'score_id' in params:\n            path_params['scoreId'] = params['score_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/tags/{tagId}/scores/{scoreId}', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='ActiveLearningScoreData',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)\n\n    def get_active_learning_scores_by_tag_id(self, dataset_id, tag_id, **kwargs):  # noqa: E501\n        \"\"\"get_active_learning_scores_by_tag_id  # noqa: E501\n\n        Get all scoreIds for the given tag  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_active_learning_scores_by_tag_id(dataset_id, tag_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID tag_id: ObjectId of the tag (required)\n        :return: list[TagActiveLearningScoresData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_active_learning_scores_by_tag_id_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_active_learning_scores_by_tag_id_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501\n            return data\n\n    def get_active_learning_scores_by_tag_id_with_http_info(self, dataset_id, tag_id, **kwargs):  # noqa: E501\n        \"\"\"get_active_learning_scores_by_tag_id  # noqa: E501\n\n        Get all scoreIds for the given tag  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_active_learning_scores_by_tag_id_with_http_info(dataset_id, tag_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID tag_id: ObjectId of the tag (required)\n        :return: list[TagActiveLearningScoresData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id', 'tag_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_active_learning_scores_by_tag_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_active_learning_scores_by_tag_id`\")  # noqa: E501\n        # verify the required parameter 'tag_id' is set\n        if self.api_client.client_side_validation and ('tag_id' not in params or\n                                                       params['tag_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `tag_id` when calling `get_active_learning_scores_by_tag_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'tag_id' in params:\n            path_params['tagId'] = params['tag_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/tags/{tagId}/scores', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='list[TagActiveLearningScoresData]',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def __init__(self, api_client=None):\n        if api_client is None:\n            api_client = ApiClient()\n        self.api_client = api_client",
  "def create_or_update_active_learning_score_by_tag_id(self, body, dataset_id, tag_id, **kwargs):  # noqa: E501\n        \"\"\"create_or_update_active_learning_score_by_tag_id  # noqa: E501\n\n        Create or update active learning score object by tag id  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_or_update_active_learning_score_by_tag_id(body, dataset_id, tag_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param ActiveLearningScoreCreateRequest body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID tag_id: ObjectId of the tag (required)\n        :return: CreateEntityResponse\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_or_update_active_learning_score_by_tag_id_with_http_info(body, dataset_id, tag_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_or_update_active_learning_score_by_tag_id_with_http_info(body, dataset_id, tag_id, **kwargs)  # noqa: E501\n            return data",
  "def create_or_update_active_learning_score_by_tag_id_with_http_info(self, body, dataset_id, tag_id, **kwargs):  # noqa: E501\n        \"\"\"create_or_update_active_learning_score_by_tag_id  # noqa: E501\n\n        Create or update active learning score object by tag id  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_or_update_active_learning_score_by_tag_id_with_http_info(body, dataset_id, tag_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param ActiveLearningScoreCreateRequest body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID tag_id: ObjectId of the tag (required)\n        :return: CreateEntityResponse\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['body', 'dataset_id', 'tag_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method create_or_update_active_learning_score_by_tag_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'body' is set\n        if self.api_client.client_side_validation and ('body' not in params or\n                                                       params['body'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `body` when calling `create_or_update_active_learning_score_by_tag_id`\")  # noqa: E501\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `create_or_update_active_learning_score_by_tag_id`\")  # noqa: E501\n        # verify the required parameter 'tag_id' is set\n        if self.api_client.client_side_validation and ('tag_id' not in params or\n                                                       params['tag_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `tag_id` when calling `create_or_update_active_learning_score_by_tag_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'tag_id' in params:\n            path_params['tagId'] = params['tag_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        if 'body' in params:\n            body_params = params['body']\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # HTTP header `Content-Type`\n        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/tags/{tagId}/scores', 'POST',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='CreateEntityResponse',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def get_active_learning_score_by_score_id(self, dataset_id, tag_id, score_id, **kwargs):  # noqa: E501\n        \"\"\"get_active_learning_score_by_score_id  # noqa: E501\n\n        Get active learning score object by id  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_active_learning_score_by_score_id(dataset_id, tag_id, score_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID tag_id: ObjectId of the tag (required)\n        :param MongoObjectID score_id: ObjectId of the scores (required)\n        :return: ActiveLearningScoreData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_active_learning_score_by_score_id_with_http_info(dataset_id, tag_id, score_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_active_learning_score_by_score_id_with_http_info(dataset_id, tag_id, score_id, **kwargs)  # noqa: E501\n            return data",
  "def get_active_learning_score_by_score_id_with_http_info(self, dataset_id, tag_id, score_id, **kwargs):  # noqa: E501\n        \"\"\"get_active_learning_score_by_score_id  # noqa: E501\n\n        Get active learning score object by id  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_active_learning_score_by_score_id_with_http_info(dataset_id, tag_id, score_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID tag_id: ObjectId of the tag (required)\n        :param MongoObjectID score_id: ObjectId of the scores (required)\n        :return: ActiveLearningScoreData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id', 'tag_id', 'score_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_active_learning_score_by_score_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_active_learning_score_by_score_id`\")  # noqa: E501\n        # verify the required parameter 'tag_id' is set\n        if self.api_client.client_side_validation and ('tag_id' not in params or\n                                                       params['tag_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `tag_id` when calling `get_active_learning_score_by_score_id`\")  # noqa: E501\n        # verify the required parameter 'score_id' is set\n        if self.api_client.client_side_validation and ('score_id' not in params or\n                                                       params['score_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `score_id` when calling `get_active_learning_score_by_score_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'tag_id' in params:\n            path_params['tagId'] = params['tag_id']  # noqa: E501\n        if 'score_id' in params:\n            path_params['scoreId'] = params['score_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/tags/{tagId}/scores/{scoreId}', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='ActiveLearningScoreData',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def get_active_learning_scores_by_tag_id(self, dataset_id, tag_id, **kwargs):  # noqa: E501\n        \"\"\"get_active_learning_scores_by_tag_id  # noqa: E501\n\n        Get all scoreIds for the given tag  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_active_learning_scores_by_tag_id(dataset_id, tag_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID tag_id: ObjectId of the tag (required)\n        :return: list[TagActiveLearningScoresData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_active_learning_scores_by_tag_id_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_active_learning_scores_by_tag_id_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501\n            return data",
  "def get_active_learning_scores_by_tag_id_with_http_info(self, dataset_id, tag_id, **kwargs):  # noqa: E501\n        \"\"\"get_active_learning_scores_by_tag_id  # noqa: E501\n\n        Get all scoreIds for the given tag  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_active_learning_scores_by_tag_id_with_http_info(dataset_id, tag_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID tag_id: ObjectId of the tag (required)\n        :return: list[TagActiveLearningScoresData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id', 'tag_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_active_learning_scores_by_tag_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_active_learning_scores_by_tag_id`\")  # noqa: E501\n        # verify the required parameter 'tag_id' is set\n        if self.api_client.client_side_validation and ('tag_id' not in params or\n                                                       params['tag_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `tag_id` when calling `get_active_learning_scores_by_tag_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'tag_id' in params:\n            path_params['tagId'] = params['tag_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/tags/{tagId}/scores', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='list[TagActiveLearningScoresData]',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "class TagsApi(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    Ref: https://github.com/swagger-api/swagger-codegen\n    \"\"\"\n\n    def __init__(self, api_client=None):\n        if api_client is None:\n            api_client = ApiClient()\n        self.api_client = api_client\n\n    def create_initial_tag_by_dataset_id(self, body, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"create_initial_tag_by_dataset_id  # noqa: E501\n\n        create the intitial tag for a dataset which then locks the dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_initial_tag_by_dataset_id(body, dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param InitialTagCreateRequest body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: CreateEntityResponse\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_initial_tag_by_dataset_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_initial_tag_by_dataset_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501\n            return data\n\n    def create_initial_tag_by_dataset_id_with_http_info(self, body, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"create_initial_tag_by_dataset_id  # noqa: E501\n\n        create the intitial tag for a dataset which then locks the dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_initial_tag_by_dataset_id_with_http_info(body, dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param InitialTagCreateRequest body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: CreateEntityResponse\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['body', 'dataset_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method create_initial_tag_by_dataset_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'body' is set\n        if self.api_client.client_side_validation and ('body' not in params or\n                                                       params['body'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `body` when calling `create_initial_tag_by_dataset_id`\")  # noqa: E501\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `create_initial_tag_by_dataset_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        if 'body' in params:\n            body_params = params['body']\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # HTTP header `Content-Type`\n        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/tags/initial', 'POST',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='CreateEntityResponse',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)\n\n    def create_tag_by_dataset_id(self, body, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"create_tag_by_dataset_id  # noqa: E501\n\n        create new tag for dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_tag_by_dataset_id(body, dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param TagCreateRequest body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: CreateEntityResponse\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_tag_by_dataset_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_tag_by_dataset_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501\n            return data\n\n    def create_tag_by_dataset_id_with_http_info(self, body, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"create_tag_by_dataset_id  # noqa: E501\n\n        create new tag for dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_tag_by_dataset_id_with_http_info(body, dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param TagCreateRequest body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: CreateEntityResponse\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['body', 'dataset_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method create_tag_by_dataset_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'body' is set\n        if self.api_client.client_side_validation and ('body' not in params or\n                                                       params['body'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `body` when calling `create_tag_by_dataset_id`\")  # noqa: E501\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `create_tag_by_dataset_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        if 'body' in params:\n            body_params = params['body']\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # HTTP header `Content-Type`\n        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/tags', 'POST',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='CreateEntityResponse',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)\n\n    def get_filenames_by_tag_id(self, dataset_id, tag_id, **kwargs):  # noqa: E501\n        \"\"\"get_filenames_by_tag_id  # noqa: E501\n\n        Get list of filenames by tag  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_filenames_by_tag_id(dataset_id, tag_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID tag_id: ObjectId of the tag (required)\n        :return: TagFilenamesData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_filenames_by_tag_id_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_filenames_by_tag_id_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501\n            return data\n\n    def get_filenames_by_tag_id_with_http_info(self, dataset_id, tag_id, **kwargs):  # noqa: E501\n        \"\"\"get_filenames_by_tag_id  # noqa: E501\n\n        Get list of filenames by tag  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_filenames_by_tag_id_with_http_info(dataset_id, tag_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID tag_id: ObjectId of the tag (required)\n        :return: TagFilenamesData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id', 'tag_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_filenames_by_tag_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_filenames_by_tag_id`\")  # noqa: E501\n        # verify the required parameter 'tag_id' is set\n        if self.api_client.client_side_validation and ('tag_id' not in params or\n                                                       params['tag_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `tag_id` when calling `get_filenames_by_tag_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'tag_id' in params:\n            path_params['tagId'] = params['tag_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/tags/{tagId}/filenames', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='TagFilenamesData',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)\n\n    def get_tag_by_tag_id(self, dataset_id, tag_id, **kwargs):  # noqa: E501\n        \"\"\"get_tag_by_tag_id  # noqa: E501\n\n        Get information about a specific tag  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_tag_by_tag_id(dataset_id, tag_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID tag_id: ObjectId of the tag (required)\n        :return: TagData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_tag_by_tag_id_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_tag_by_tag_id_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501\n            return data\n\n    def get_tag_by_tag_id_with_http_info(self, dataset_id, tag_id, **kwargs):  # noqa: E501\n        \"\"\"get_tag_by_tag_id  # noqa: E501\n\n        Get information about a specific tag  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_tag_by_tag_id_with_http_info(dataset_id, tag_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID tag_id: ObjectId of the tag (required)\n        :return: TagData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id', 'tag_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_tag_by_tag_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_tag_by_tag_id`\")  # noqa: E501\n        # verify the required parameter 'tag_id' is set\n        if self.api_client.client_side_validation and ('tag_id' not in params or\n                                                       params['tag_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `tag_id` when calling `get_tag_by_tag_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'tag_id' in params:\n            path_params['tagId'] = params['tag_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/tags/{tagId}', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='TagData',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)\n\n    def get_tags_by_dataset_id(self, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"get_tags_by_dataset_id  # noqa: E501\n\n        Get all tags of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_tags_by_dataset_id(dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: list[TagData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_tags_by_dataset_id_with_http_info(dataset_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_tags_by_dataset_id_with_http_info(dataset_id, **kwargs)  # noqa: E501\n            return data\n\n    def get_tags_by_dataset_id_with_http_info(self, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"get_tags_by_dataset_id  # noqa: E501\n\n        Get all tags of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_tags_by_dataset_id_with_http_info(dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: list[TagData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_tags_by_dataset_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_tags_by_dataset_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/tags', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='list[TagData]',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def __init__(self, api_client=None):\n        if api_client is None:\n            api_client = ApiClient()\n        self.api_client = api_client",
  "def create_initial_tag_by_dataset_id(self, body, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"create_initial_tag_by_dataset_id  # noqa: E501\n\n        create the intitial tag for a dataset which then locks the dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_initial_tag_by_dataset_id(body, dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param InitialTagCreateRequest body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: CreateEntityResponse\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_initial_tag_by_dataset_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_initial_tag_by_dataset_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501\n            return data",
  "def create_initial_tag_by_dataset_id_with_http_info(self, body, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"create_initial_tag_by_dataset_id  # noqa: E501\n\n        create the intitial tag for a dataset which then locks the dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_initial_tag_by_dataset_id_with_http_info(body, dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param InitialTagCreateRequest body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: CreateEntityResponse\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['body', 'dataset_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method create_initial_tag_by_dataset_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'body' is set\n        if self.api_client.client_side_validation and ('body' not in params or\n                                                       params['body'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `body` when calling `create_initial_tag_by_dataset_id`\")  # noqa: E501\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `create_initial_tag_by_dataset_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        if 'body' in params:\n            body_params = params['body']\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # HTTP header `Content-Type`\n        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/tags/initial', 'POST',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='CreateEntityResponse',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def create_tag_by_dataset_id(self, body, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"create_tag_by_dataset_id  # noqa: E501\n\n        create new tag for dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_tag_by_dataset_id(body, dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param TagCreateRequest body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: CreateEntityResponse\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_tag_by_dataset_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_tag_by_dataset_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501\n            return data",
  "def create_tag_by_dataset_id_with_http_info(self, body, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"create_tag_by_dataset_id  # noqa: E501\n\n        create new tag for dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_tag_by_dataset_id_with_http_info(body, dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param TagCreateRequest body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: CreateEntityResponse\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['body', 'dataset_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method create_tag_by_dataset_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'body' is set\n        if self.api_client.client_side_validation and ('body' not in params or\n                                                       params['body'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `body` when calling `create_tag_by_dataset_id`\")  # noqa: E501\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `create_tag_by_dataset_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        if 'body' in params:\n            body_params = params['body']\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # HTTP header `Content-Type`\n        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/tags', 'POST',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='CreateEntityResponse',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def get_filenames_by_tag_id(self, dataset_id, tag_id, **kwargs):  # noqa: E501\n        \"\"\"get_filenames_by_tag_id  # noqa: E501\n\n        Get list of filenames by tag  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_filenames_by_tag_id(dataset_id, tag_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID tag_id: ObjectId of the tag (required)\n        :return: TagFilenamesData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_filenames_by_tag_id_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_filenames_by_tag_id_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501\n            return data",
  "def get_filenames_by_tag_id_with_http_info(self, dataset_id, tag_id, **kwargs):  # noqa: E501\n        \"\"\"get_filenames_by_tag_id  # noqa: E501\n\n        Get list of filenames by tag  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_filenames_by_tag_id_with_http_info(dataset_id, tag_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID tag_id: ObjectId of the tag (required)\n        :return: TagFilenamesData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id', 'tag_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_filenames_by_tag_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_filenames_by_tag_id`\")  # noqa: E501\n        # verify the required parameter 'tag_id' is set\n        if self.api_client.client_side_validation and ('tag_id' not in params or\n                                                       params['tag_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `tag_id` when calling `get_filenames_by_tag_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'tag_id' in params:\n            path_params['tagId'] = params['tag_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/tags/{tagId}/filenames', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='TagFilenamesData',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def get_tag_by_tag_id(self, dataset_id, tag_id, **kwargs):  # noqa: E501\n        \"\"\"get_tag_by_tag_id  # noqa: E501\n\n        Get information about a specific tag  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_tag_by_tag_id(dataset_id, tag_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID tag_id: ObjectId of the tag (required)\n        :return: TagData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_tag_by_tag_id_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_tag_by_tag_id_with_http_info(dataset_id, tag_id, **kwargs)  # noqa: E501\n            return data",
  "def get_tag_by_tag_id_with_http_info(self, dataset_id, tag_id, **kwargs):  # noqa: E501\n        \"\"\"get_tag_by_tag_id  # noqa: E501\n\n        Get information about a specific tag  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_tag_by_tag_id_with_http_info(dataset_id, tag_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID tag_id: ObjectId of the tag (required)\n        :return: TagData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id', 'tag_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_tag_by_tag_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_tag_by_tag_id`\")  # noqa: E501\n        # verify the required parameter 'tag_id' is set\n        if self.api_client.client_side_validation and ('tag_id' not in params or\n                                                       params['tag_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `tag_id` when calling `get_tag_by_tag_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'tag_id' in params:\n            path_params['tagId'] = params['tag_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/tags/{tagId}', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='TagData',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def get_tags_by_dataset_id(self, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"get_tags_by_dataset_id  # noqa: E501\n\n        Get all tags of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_tags_by_dataset_id(dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: list[TagData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_tags_by_dataset_id_with_http_info(dataset_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_tags_by_dataset_id_with_http_info(dataset_id, **kwargs)  # noqa: E501\n            return data",
  "def get_tags_by_dataset_id_with_http_info(self, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"get_tags_by_dataset_id  # noqa: E501\n\n        Get all tags of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_tags_by_dataset_id_with_http_info(dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: list[TagData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_tags_by_dataset_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_tags_by_dataset_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/tags', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='list[TagData]',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "class VersioningApi(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    Ref: https://github.com/swagger-api/swagger-codegen\n    \"\"\"\n\n    def __init__(self, api_client=None):\n        if api_client is None:\n            api_client = ApiClient()\n        self.api_client = api_client\n\n    def get_latest_pip_version(self, **kwargs):  # noqa: E501\n        \"\"\"get_latest_pip_version  # noqa: E501\n\n        Get latest pip version available  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_latest_pip_version(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str current_version:\n        :return: VersionNumber\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_latest_pip_version_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.get_latest_pip_version_with_http_info(**kwargs)  # noqa: E501\n            return data\n\n    def get_latest_pip_version_with_http_info(self, **kwargs):  # noqa: E501\n        \"\"\"get_latest_pip_version  # noqa: E501\n\n        Get latest pip version available  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_latest_pip_version_with_http_info(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str current_version:\n        :return: VersionNumber\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['current_version']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_latest_pip_version\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n\n        collection_formats = {}\n\n        path_params = {}\n\n        query_params = []\n        if 'current_version' in params:\n            query_params.append(('currentVersion', params['current_version']))  # noqa: E501\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = []  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/versions/pip/latest', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='VersionNumber',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)\n\n    def get_minimum_compatible_pip_version(self, **kwargs):  # noqa: E501\n        \"\"\"get_minimum_compatible_pip_version  # noqa: E501\n\n        Get minimum pip version needed for compatability  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_minimum_compatible_pip_version(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :return: VersionNumber\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_minimum_compatible_pip_version_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.get_minimum_compatible_pip_version_with_http_info(**kwargs)  # noqa: E501\n            return data\n\n    def get_minimum_compatible_pip_version_with_http_info(self, **kwargs):  # noqa: E501\n        \"\"\"get_minimum_compatible_pip_version  # noqa: E501\n\n        Get minimum pip version needed for compatability  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_minimum_compatible_pip_version_with_http_info(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :return: VersionNumber\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = []  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_minimum_compatible_pip_version\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n\n        collection_formats = {}\n\n        path_params = {}\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = []  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/versions/pip/minimum', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='VersionNumber',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def __init__(self, api_client=None):\n        if api_client is None:\n            api_client = ApiClient()\n        self.api_client = api_client",
  "def get_latest_pip_version(self, **kwargs):  # noqa: E501\n        \"\"\"get_latest_pip_version  # noqa: E501\n\n        Get latest pip version available  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_latest_pip_version(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str current_version:\n        :return: VersionNumber\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_latest_pip_version_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.get_latest_pip_version_with_http_info(**kwargs)  # noqa: E501\n            return data",
  "def get_latest_pip_version_with_http_info(self, **kwargs):  # noqa: E501\n        \"\"\"get_latest_pip_version  # noqa: E501\n\n        Get latest pip version available  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_latest_pip_version_with_http_info(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str current_version:\n        :return: VersionNumber\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['current_version']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_latest_pip_version\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n\n        collection_formats = {}\n\n        path_params = {}\n\n        query_params = []\n        if 'current_version' in params:\n            query_params.append(('currentVersion', params['current_version']))  # noqa: E501\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = []  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/versions/pip/latest', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='VersionNumber',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def get_minimum_compatible_pip_version(self, **kwargs):  # noqa: E501\n        \"\"\"get_minimum_compatible_pip_version  # noqa: E501\n\n        Get minimum pip version needed for compatability  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_minimum_compatible_pip_version(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :return: VersionNumber\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_minimum_compatible_pip_version_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.get_minimum_compatible_pip_version_with_http_info(**kwargs)  # noqa: E501\n            return data",
  "def get_minimum_compatible_pip_version_with_http_info(self, **kwargs):  # noqa: E501\n        \"\"\"get_minimum_compatible_pip_version  # noqa: E501\n\n        Get minimum pip version needed for compatability  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_minimum_compatible_pip_version_with_http_info(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :return: VersionNumber\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = []  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_minimum_compatible_pip_version\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n\n        collection_formats = {}\n\n        path_params = {}\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = []  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/versions/pip/minimum', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='VersionNumber',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "class SamplingsApi(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    Ref: https://github.com/swagger-api/swagger-codegen\n    \"\"\"\n\n    def __init__(self, api_client=None):\n        if api_client is None:\n            api_client = ApiClient()\n        self.api_client = api_client\n\n    def trigger_sampling_by_id(self, body, dataset_id, embedding_id, **kwargs):  # noqa: E501\n        \"\"\"trigger_sampling_by_id  # noqa: E501\n\n        Trigger a sampling on a specific tag of a dataset with specific prior uploaded csv embedding  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.trigger_sampling_by_id(body, dataset_id, embedding_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param SamplingCreateRequest body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID embedding_id: ObjectId of the embedding (required)\n        :return: AsyncTaskData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.trigger_sampling_by_id_with_http_info(body, dataset_id, embedding_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.trigger_sampling_by_id_with_http_info(body, dataset_id, embedding_id, **kwargs)  # noqa: E501\n            return data\n\n    def trigger_sampling_by_id_with_http_info(self, body, dataset_id, embedding_id, **kwargs):  # noqa: E501\n        \"\"\"trigger_sampling_by_id  # noqa: E501\n\n        Trigger a sampling on a specific tag of a dataset with specific prior uploaded csv embedding  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.trigger_sampling_by_id_with_http_info(body, dataset_id, embedding_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param SamplingCreateRequest body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID embedding_id: ObjectId of the embedding (required)\n        :return: AsyncTaskData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['body', 'dataset_id', 'embedding_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method trigger_sampling_by_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'body' is set\n        if self.api_client.client_side_validation and ('body' not in params or\n                                                       params['body'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `body` when calling `trigger_sampling_by_id`\")  # noqa: E501\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `trigger_sampling_by_id`\")  # noqa: E501\n        # verify the required parameter 'embedding_id' is set\n        if self.api_client.client_side_validation and ('embedding_id' not in params or\n                                                       params['embedding_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `embedding_id` when calling `trigger_sampling_by_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'embedding_id' in params:\n            path_params['embeddingId'] = params['embedding_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        if 'body' in params:\n            body_params = params['body']\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # HTTP header `Content-Type`\n        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/embeddings/{embeddingId}/sampling', 'POST',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='AsyncTaskData',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def __init__(self, api_client=None):\n        if api_client is None:\n            api_client = ApiClient()\n        self.api_client = api_client",
  "def trigger_sampling_by_id(self, body, dataset_id, embedding_id, **kwargs):  # noqa: E501\n        \"\"\"trigger_sampling_by_id  # noqa: E501\n\n        Trigger a sampling on a specific tag of a dataset with specific prior uploaded csv embedding  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.trigger_sampling_by_id(body, dataset_id, embedding_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param SamplingCreateRequest body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID embedding_id: ObjectId of the embedding (required)\n        :return: AsyncTaskData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.trigger_sampling_by_id_with_http_info(body, dataset_id, embedding_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.trigger_sampling_by_id_with_http_info(body, dataset_id, embedding_id, **kwargs)  # noqa: E501\n            return data",
  "def trigger_sampling_by_id_with_http_info(self, body, dataset_id, embedding_id, **kwargs):  # noqa: E501\n        \"\"\"trigger_sampling_by_id  # noqa: E501\n\n        Trigger a sampling on a specific tag of a dataset with specific prior uploaded csv embedding  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.trigger_sampling_by_id_with_http_info(body, dataset_id, embedding_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param SamplingCreateRequest body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID embedding_id: ObjectId of the embedding (required)\n        :return: AsyncTaskData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['body', 'dataset_id', 'embedding_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method trigger_sampling_by_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'body' is set\n        if self.api_client.client_side_validation and ('body' not in params or\n                                                       params['body'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `body` when calling `trigger_sampling_by_id`\")  # noqa: E501\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `trigger_sampling_by_id`\")  # noqa: E501\n        # verify the required parameter 'embedding_id' is set\n        if self.api_client.client_side_validation and ('embedding_id' not in params or\n                                                       params['embedding_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `embedding_id` when calling `trigger_sampling_by_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'embedding_id' in params:\n            path_params['embeddingId'] = params['embedding_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        if 'body' in params:\n            body_params = params['body']\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # HTTP header `Content-Type`\n        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/embeddings/{embeddingId}/sampling', 'POST',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='AsyncTaskData',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "class JobsApi(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    Ref: https://github.com/swagger-api/swagger-codegen\n    \"\"\"\n\n    def __init__(self, api_client=None):\n        if api_client is None:\n            api_client = ApiClient()\n        self.api_client = api_client\n\n    def get_job_status_by_id(self, job_id, **kwargs):  # noqa: E501\n        \"\"\"get_job_status_by_id  # noqa: E501\n\n        Get status of a specific job  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_job_status_by_id(job_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str job_id: id of the job (required)\n        :return: JobStatusData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_job_status_by_id_with_http_info(job_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_job_status_by_id_with_http_info(job_id, **kwargs)  # noqa: E501\n            return data\n\n    def get_job_status_by_id_with_http_info(self, job_id, **kwargs):  # noqa: E501\n        \"\"\"get_job_status_by_id  # noqa: E501\n\n        Get status of a specific job  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_job_status_by_id_with_http_info(job_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str job_id: id of the job (required)\n        :return: JobStatusData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['job_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_job_status_by_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'job_id' is set\n        if self.api_client.client_side_validation and ('job_id' not in params or\n                                                       params['job_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `job_id` when calling `get_job_status_by_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'job_id' in params:\n            path_params['jobId'] = params['job_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/jobs/{jobId}', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='JobStatusData',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)\n\n    def get_jobs(self, **kwargs):  # noqa: E501\n        \"\"\"get_jobs  # noqa: E501\n\n        Get all jobs you have created  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_jobs(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :return: list[JobsData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_jobs_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.get_jobs_with_http_info(**kwargs)  # noqa: E501\n            return data\n\n    def get_jobs_with_http_info(self, **kwargs):  # noqa: E501\n        \"\"\"get_jobs  # noqa: E501\n\n        Get all jobs you have created  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_jobs_with_http_info(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :return: list[JobsData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = []  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_jobs\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n\n        collection_formats = {}\n\n        path_params = {}\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/jobs', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='list[JobsData]',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def __init__(self, api_client=None):\n        if api_client is None:\n            api_client = ApiClient()\n        self.api_client = api_client",
  "def get_job_status_by_id(self, job_id, **kwargs):  # noqa: E501\n        \"\"\"get_job_status_by_id  # noqa: E501\n\n        Get status of a specific job  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_job_status_by_id(job_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str job_id: id of the job (required)\n        :return: JobStatusData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_job_status_by_id_with_http_info(job_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_job_status_by_id_with_http_info(job_id, **kwargs)  # noqa: E501\n            return data",
  "def get_job_status_by_id_with_http_info(self, job_id, **kwargs):  # noqa: E501\n        \"\"\"get_job_status_by_id  # noqa: E501\n\n        Get status of a specific job  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_job_status_by_id_with_http_info(job_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str job_id: id of the job (required)\n        :return: JobStatusData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['job_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_job_status_by_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'job_id' is set\n        if self.api_client.client_side_validation and ('job_id' not in params or\n                                                       params['job_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `job_id` when calling `get_job_status_by_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'job_id' in params:\n            path_params['jobId'] = params['job_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/jobs/{jobId}', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='JobStatusData',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def get_jobs(self, **kwargs):  # noqa: E501\n        \"\"\"get_jobs  # noqa: E501\n\n        Get all jobs you have created  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_jobs(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :return: list[JobsData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_jobs_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.get_jobs_with_http_info(**kwargs)  # noqa: E501\n            return data",
  "def get_jobs_with_http_info(self, **kwargs):  # noqa: E501\n        \"\"\"get_jobs  # noqa: E501\n\n        Get all jobs you have created  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_jobs_with_http_info(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :return: list[JobsData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = []  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_jobs\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n\n        collection_formats = {}\n\n        path_params = {}\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/jobs', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='list[JobsData]',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "class SamplesApi(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    Ref: https://github.com/swagger-api/swagger-codegen\n    \"\"\"\n\n    def __init__(self, api_client=None):\n        if api_client is None:\n            api_client = ApiClient()\n        self.api_client = api_client\n\n    def create_sample_by_dataset_id(self, body, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"create_sample_by_dataset_id  # noqa: E501\n\n        Create a new sample in a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_sample_by_dataset_id(body, dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param SampleCreateRequest body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: CreateEntityResponse\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_sample_by_dataset_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_sample_by_dataset_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501\n            return data\n\n    def create_sample_by_dataset_id_with_http_info(self, body, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"create_sample_by_dataset_id  # noqa: E501\n\n        Create a new sample in a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_sample_by_dataset_id_with_http_info(body, dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param SampleCreateRequest body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: CreateEntityResponse\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['body', 'dataset_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method create_sample_by_dataset_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'body' is set\n        if self.api_client.client_side_validation and ('body' not in params or\n                                                       params['body'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `body` when calling `create_sample_by_dataset_id`\")  # noqa: E501\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `create_sample_by_dataset_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        if 'body' in params:\n            body_params = params['body']\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # HTTP header `Content-Type`\n        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/samples', 'POST',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='CreateEntityResponse',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)\n\n    def get_sample_by_id(self, dataset_id, sample_id, **kwargs):  # noqa: E501\n        \"\"\"get_sample_by_id  # noqa: E501\n\n        Get a specific sample of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_sample_by_id(dataset_id, sample_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID sample_id: ObjectId of the sample (required)\n        :return: SampleData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_sample_by_id_with_http_info(dataset_id, sample_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_sample_by_id_with_http_info(dataset_id, sample_id, **kwargs)  # noqa: E501\n            return data\n\n    def get_sample_by_id_with_http_info(self, dataset_id, sample_id, **kwargs):  # noqa: E501\n        \"\"\"get_sample_by_id  # noqa: E501\n\n        Get a specific sample of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_sample_by_id_with_http_info(dataset_id, sample_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID sample_id: ObjectId of the sample (required)\n        :return: SampleData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id', 'sample_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_sample_by_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_sample_by_id`\")  # noqa: E501\n        # verify the required parameter 'sample_id' is set\n        if self.api_client.client_side_validation and ('sample_id' not in params or\n                                                       params['sample_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `sample_id` when calling `get_sample_by_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'sample_id' in params:\n            path_params['sampleId'] = params['sample_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/samples/{sampleId}', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='SampleData',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)\n\n    def get_sample_image_read_url_by_id(self, dataset_id, sample_id, **kwargs):  # noqa: E501\n        \"\"\"get_sample_image_read_url_by_id  # noqa: E501\n\n        Get the image path of a specific sample of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_sample_image_read_url_by_id(dataset_id, sample_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID sample_id: ObjectId of the sample (required)\n        :param str type: if we want to get the full image or just the thumbnail\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_sample_image_read_url_by_id_with_http_info(dataset_id, sample_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_sample_image_read_url_by_id_with_http_info(dataset_id, sample_id, **kwargs)  # noqa: E501\n            return data\n\n    def get_sample_image_read_url_by_id_with_http_info(self, dataset_id, sample_id, **kwargs):  # noqa: E501\n        \"\"\"get_sample_image_read_url_by_id  # noqa: E501\n\n        Get the image path of a specific sample of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_sample_image_read_url_by_id_with_http_info(dataset_id, sample_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID sample_id: ObjectId of the sample (required)\n        :param str type: if we want to get the full image or just the thumbnail\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id', 'sample_id', 'type']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_sample_image_read_url_by_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_sample_image_read_url_by_id`\")  # noqa: E501\n        # verify the required parameter 'sample_id' is set\n        if self.api_client.client_side_validation and ('sample_id' not in params or\n                                                       params['sample_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `sample_id` when calling `get_sample_image_read_url_by_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'sample_id' in params:\n            path_params['sampleId'] = params['sample_id']  # noqa: E501\n\n        query_params = []\n        if 'type' in params:\n            query_params.append(('type', params['type']))  # noqa: E501\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/samples/{sampleId}/readurl', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='str',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)\n\n    def get_sample_image_write_url_by_id(self, dataset_id, sample_id, is_thumbnail, **kwargs):  # noqa: E501\n        \"\"\"get_sample_image_write_url_by_id  # noqa: E501\n\n        Get the signed url to upload an image to for a specific sample of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_sample_image_write_url_by_id(dataset_id, sample_id, is_thumbnail, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID sample_id: ObjectId of the sample (required)\n        :param bool is_thumbnail: Whether or not the image to upload is a thumbnail (required)\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_sample_image_write_url_by_id_with_http_info(dataset_id, sample_id, is_thumbnail, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_sample_image_write_url_by_id_with_http_info(dataset_id, sample_id, is_thumbnail, **kwargs)  # noqa: E501\n            return data\n\n    def get_sample_image_write_url_by_id_with_http_info(self, dataset_id, sample_id, is_thumbnail, **kwargs):  # noqa: E501\n        \"\"\"get_sample_image_write_url_by_id  # noqa: E501\n\n        Get the signed url to upload an image to for a specific sample of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_sample_image_write_url_by_id_with_http_info(dataset_id, sample_id, is_thumbnail, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID sample_id: ObjectId of the sample (required)\n        :param bool is_thumbnail: Whether or not the image to upload is a thumbnail (required)\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id', 'sample_id', 'is_thumbnail']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_sample_image_write_url_by_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_sample_image_write_url_by_id`\")  # noqa: E501\n        # verify the required parameter 'sample_id' is set\n        if self.api_client.client_side_validation and ('sample_id' not in params or\n                                                       params['sample_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `sample_id` when calling `get_sample_image_write_url_by_id`\")  # noqa: E501\n        # verify the required parameter 'is_thumbnail' is set\n        if self.api_client.client_side_validation and ('is_thumbnail' not in params or\n                                                       params['is_thumbnail'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `is_thumbnail` when calling `get_sample_image_write_url_by_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'sample_id' in params:\n            path_params['sampleId'] = params['sample_id']  # noqa: E501\n\n        query_params = []\n        if 'is_thumbnail' in params:\n            query_params.append(('isThumbnail', params['is_thumbnail']))  # noqa: E501\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/samples/{sampleId}/writeurl', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='str',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)\n\n    def get_samples_by_dataset_id(self, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"get_samples_by_dataset_id  # noqa: E501\n\n        Get all samples of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_samples_by_dataset_id(dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param str mode: if we want everything (full) or just the ObjectIds\n        :param str file_name: filter the samples by filename\n        :return: list[SampleData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_samples_by_dataset_id_with_http_info(dataset_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_samples_by_dataset_id_with_http_info(dataset_id, **kwargs)  # noqa: E501\n            return data\n\n    def get_samples_by_dataset_id_with_http_info(self, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"get_samples_by_dataset_id  # noqa: E501\n\n        Get all samples of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_samples_by_dataset_id_with_http_info(dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param str mode: if we want everything (full) or just the ObjectIds\n        :param str file_name: filter the samples by filename\n        :return: list[SampleData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id', 'mode', 'file_name']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_samples_by_dataset_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_samples_by_dataset_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n\n        query_params = []\n        if 'mode' in params:\n            query_params.append(('mode', params['mode']))  # noqa: E501\n        if 'file_name' in params:\n            query_params.append(('fileName', params['file_name']))  # noqa: E501\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/samples', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='list[SampleData]',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)\n\n    def update_sample_by_id(self, body, dataset_id, sample_id, **kwargs):  # noqa: E501\n        \"\"\"update_sample_by_id  # noqa: E501\n\n        update a specific sample of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.update_sample_by_id(body, dataset_id, sample_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param SampleUpdateRequest body: The updated sample to set (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID sample_id: ObjectId of the sample (required)\n        :param bool enable_dataset_update:\n        :return: None\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.update_sample_by_id_with_http_info(body, dataset_id, sample_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.update_sample_by_id_with_http_info(body, dataset_id, sample_id, **kwargs)  # noqa: E501\n            return data\n\n    def update_sample_by_id_with_http_info(self, body, dataset_id, sample_id, **kwargs):  # noqa: E501\n        \"\"\"update_sample_by_id  # noqa: E501\n\n        update a specific sample of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.update_sample_by_id_with_http_info(body, dataset_id, sample_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param SampleUpdateRequest body: The updated sample to set (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID sample_id: ObjectId of the sample (required)\n        :param bool enable_dataset_update:\n        :return: None\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['body', 'dataset_id', 'sample_id', 'enable_dataset_update']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method update_sample_by_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'body' is set\n        if self.api_client.client_side_validation and ('body' not in params or\n                                                       params['body'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `body` when calling `update_sample_by_id`\")  # noqa: E501\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `update_sample_by_id`\")  # noqa: E501\n        # verify the required parameter 'sample_id' is set\n        if self.api_client.client_side_validation and ('sample_id' not in params or\n                                                       params['sample_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `sample_id` when calling `update_sample_by_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'sample_id' in params:\n            path_params['sampleId'] = params['sample_id']  # noqa: E501\n\n        query_params = []\n        if 'enable_dataset_update' in params:\n            query_params.append(('enableDatasetUpdate', params['enable_dataset_update']))  # noqa: E501\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        if 'body' in params:\n            body_params = params['body']\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # HTTP header `Content-Type`\n        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/samples/{sampleId}', 'PUT',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type=None,  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def __init__(self, api_client=None):\n        if api_client is None:\n            api_client = ApiClient()\n        self.api_client = api_client",
  "def create_sample_by_dataset_id(self, body, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"create_sample_by_dataset_id  # noqa: E501\n\n        Create a new sample in a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_sample_by_dataset_id(body, dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param SampleCreateRequest body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: CreateEntityResponse\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_sample_by_dataset_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.create_sample_by_dataset_id_with_http_info(body, dataset_id, **kwargs)  # noqa: E501\n            return data",
  "def create_sample_by_dataset_id_with_http_info(self, body, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"create_sample_by_dataset_id  # noqa: E501\n\n        Create a new sample in a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_sample_by_dataset_id_with_http_info(body, dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param SampleCreateRequest body: (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :return: CreateEntityResponse\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['body', 'dataset_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method create_sample_by_dataset_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'body' is set\n        if self.api_client.client_side_validation and ('body' not in params or\n                                                       params['body'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `body` when calling `create_sample_by_dataset_id`\")  # noqa: E501\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `create_sample_by_dataset_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        if 'body' in params:\n            body_params = params['body']\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # HTTP header `Content-Type`\n        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/samples', 'POST',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='CreateEntityResponse',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def get_sample_by_id(self, dataset_id, sample_id, **kwargs):  # noqa: E501\n        \"\"\"get_sample_by_id  # noqa: E501\n\n        Get a specific sample of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_sample_by_id(dataset_id, sample_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID sample_id: ObjectId of the sample (required)\n        :return: SampleData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_sample_by_id_with_http_info(dataset_id, sample_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_sample_by_id_with_http_info(dataset_id, sample_id, **kwargs)  # noqa: E501\n            return data",
  "def get_sample_by_id_with_http_info(self, dataset_id, sample_id, **kwargs):  # noqa: E501\n        \"\"\"get_sample_by_id  # noqa: E501\n\n        Get a specific sample of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_sample_by_id_with_http_info(dataset_id, sample_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID sample_id: ObjectId of the sample (required)\n        :return: SampleData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id', 'sample_id']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_sample_by_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_sample_by_id`\")  # noqa: E501\n        # verify the required parameter 'sample_id' is set\n        if self.api_client.client_side_validation and ('sample_id' not in params or\n                                                       params['sample_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `sample_id` when calling `get_sample_by_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'sample_id' in params:\n            path_params['sampleId'] = params['sample_id']  # noqa: E501\n\n        query_params = []\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/samples/{sampleId}', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='SampleData',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def get_sample_image_read_url_by_id(self, dataset_id, sample_id, **kwargs):  # noqa: E501\n        \"\"\"get_sample_image_read_url_by_id  # noqa: E501\n\n        Get the image path of a specific sample of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_sample_image_read_url_by_id(dataset_id, sample_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID sample_id: ObjectId of the sample (required)\n        :param str type: if we want to get the full image or just the thumbnail\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_sample_image_read_url_by_id_with_http_info(dataset_id, sample_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_sample_image_read_url_by_id_with_http_info(dataset_id, sample_id, **kwargs)  # noqa: E501\n            return data",
  "def get_sample_image_read_url_by_id_with_http_info(self, dataset_id, sample_id, **kwargs):  # noqa: E501\n        \"\"\"get_sample_image_read_url_by_id  # noqa: E501\n\n        Get the image path of a specific sample of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_sample_image_read_url_by_id_with_http_info(dataset_id, sample_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID sample_id: ObjectId of the sample (required)\n        :param str type: if we want to get the full image or just the thumbnail\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id', 'sample_id', 'type']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_sample_image_read_url_by_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_sample_image_read_url_by_id`\")  # noqa: E501\n        # verify the required parameter 'sample_id' is set\n        if self.api_client.client_side_validation and ('sample_id' not in params or\n                                                       params['sample_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `sample_id` when calling `get_sample_image_read_url_by_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'sample_id' in params:\n            path_params['sampleId'] = params['sample_id']  # noqa: E501\n\n        query_params = []\n        if 'type' in params:\n            query_params.append(('type', params['type']))  # noqa: E501\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/samples/{sampleId}/readurl', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='str',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def get_sample_image_write_url_by_id(self, dataset_id, sample_id, is_thumbnail, **kwargs):  # noqa: E501\n        \"\"\"get_sample_image_write_url_by_id  # noqa: E501\n\n        Get the signed url to upload an image to for a specific sample of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_sample_image_write_url_by_id(dataset_id, sample_id, is_thumbnail, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID sample_id: ObjectId of the sample (required)\n        :param bool is_thumbnail: Whether or not the image to upload is a thumbnail (required)\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_sample_image_write_url_by_id_with_http_info(dataset_id, sample_id, is_thumbnail, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_sample_image_write_url_by_id_with_http_info(dataset_id, sample_id, is_thumbnail, **kwargs)  # noqa: E501\n            return data",
  "def get_sample_image_write_url_by_id_with_http_info(self, dataset_id, sample_id, is_thumbnail, **kwargs):  # noqa: E501\n        \"\"\"get_sample_image_write_url_by_id  # noqa: E501\n\n        Get the signed url to upload an image to for a specific sample of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_sample_image_write_url_by_id_with_http_info(dataset_id, sample_id, is_thumbnail, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID sample_id: ObjectId of the sample (required)\n        :param bool is_thumbnail: Whether or not the image to upload is a thumbnail (required)\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id', 'sample_id', 'is_thumbnail']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_sample_image_write_url_by_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_sample_image_write_url_by_id`\")  # noqa: E501\n        # verify the required parameter 'sample_id' is set\n        if self.api_client.client_side_validation and ('sample_id' not in params or\n                                                       params['sample_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `sample_id` when calling `get_sample_image_write_url_by_id`\")  # noqa: E501\n        # verify the required parameter 'is_thumbnail' is set\n        if self.api_client.client_side_validation and ('is_thumbnail' not in params or\n                                                       params['is_thumbnail'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `is_thumbnail` when calling `get_sample_image_write_url_by_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'sample_id' in params:\n            path_params['sampleId'] = params['sample_id']  # noqa: E501\n\n        query_params = []\n        if 'is_thumbnail' in params:\n            query_params.append(('isThumbnail', params['is_thumbnail']))  # noqa: E501\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/samples/{sampleId}/writeurl', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='str',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def get_samples_by_dataset_id(self, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"get_samples_by_dataset_id  # noqa: E501\n\n        Get all samples of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_samples_by_dataset_id(dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param str mode: if we want everything (full) or just the ObjectIds\n        :param str file_name: filter the samples by filename\n        :return: list[SampleData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_samples_by_dataset_id_with_http_info(dataset_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.get_samples_by_dataset_id_with_http_info(dataset_id, **kwargs)  # noqa: E501\n            return data",
  "def get_samples_by_dataset_id_with_http_info(self, dataset_id, **kwargs):  # noqa: E501\n        \"\"\"get_samples_by_dataset_id  # noqa: E501\n\n        Get all samples of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_samples_by_dataset_id_with_http_info(dataset_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param str mode: if we want everything (full) or just the ObjectIds\n        :param str file_name: filter the samples by filename\n        :return: list[SampleData]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['dataset_id', 'mode', 'file_name']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method get_samples_by_dataset_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `get_samples_by_dataset_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n\n        query_params = []\n        if 'mode' in params:\n            query_params.append(('mode', params['mode']))  # noqa: E501\n        if 'file_name' in params:\n            query_params.append(('fileName', params['file_name']))  # noqa: E501\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/samples', 'GET',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type='list[SampleData]',  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "def update_sample_by_id(self, body, dataset_id, sample_id, **kwargs):  # noqa: E501\n        \"\"\"update_sample_by_id  # noqa: E501\n\n        update a specific sample of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.update_sample_by_id(body, dataset_id, sample_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param SampleUpdateRequest body: The updated sample to set (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID sample_id: ObjectId of the sample (required)\n        :param bool enable_dataset_update:\n        :return: None\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.update_sample_by_id_with_http_info(body, dataset_id, sample_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.update_sample_by_id_with_http_info(body, dataset_id, sample_id, **kwargs)  # noqa: E501\n            return data",
  "def update_sample_by_id_with_http_info(self, body, dataset_id, sample_id, **kwargs):  # noqa: E501\n        \"\"\"update_sample_by_id  # noqa: E501\n\n        update a specific sample of a dataset  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.update_sample_by_id_with_http_info(body, dataset_id, sample_id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param SampleUpdateRequest body: The updated sample to set (required)\n        :param MongoObjectID dataset_id: ObjectId of the dataset (required)\n        :param MongoObjectID sample_id: ObjectId of the sample (required)\n        :param bool enable_dataset_update:\n        :return: None\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['body', 'dataset_id', 'sample_id', 'enable_dataset_update']  # noqa: E501\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n        all_params.append('_preload_content')\n        all_params.append('_request_timeout')\n\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            if key not in all_params:\n                raise TypeError(\n                    \"Got an unexpected keyword argument '%s'\"\n                    \" to method update_sample_by_id\" % key\n                )\n            params[key] = val\n        del params['kwargs']\n        # verify the required parameter 'body' is set\n        if self.api_client.client_side_validation and ('body' not in params or\n                                                       params['body'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `body` when calling `update_sample_by_id`\")  # noqa: E501\n        # verify the required parameter 'dataset_id' is set\n        if self.api_client.client_side_validation and ('dataset_id' not in params or\n                                                       params['dataset_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `dataset_id` when calling `update_sample_by_id`\")  # noqa: E501\n        # verify the required parameter 'sample_id' is set\n        if self.api_client.client_side_validation and ('sample_id' not in params or\n                                                       params['sample_id'] is None):  # noqa: E501\n            raise ValueError(\"Missing the required parameter `sample_id` when calling `update_sample_by_id`\")  # noqa: E501\n\n        collection_formats = {}\n\n        path_params = {}\n        if 'dataset_id' in params:\n            path_params['datasetId'] = params['dataset_id']  # noqa: E501\n        if 'sample_id' in params:\n            path_params['sampleId'] = params['sample_id']  # noqa: E501\n\n        query_params = []\n        if 'enable_dataset_update' in params:\n            query_params.append(('enableDatasetUpdate', params['enable_dataset_update']))  # noqa: E501\n\n        header_params = {}\n\n        form_params = []\n        local_var_files = {}\n\n        body_params = None\n        if 'body' in params:\n            body_params = params['body']\n        # HTTP header `Accept`\n        header_params['Accept'] = self.api_client.select_header_accept(\n            ['application/json'])  # noqa: E501\n\n        # HTTP header `Content-Type`\n        header_params['Content-Type'] = self.api_client.select_header_content_type(  # noqa: E501\n            ['application/json'])  # noqa: E501\n\n        # Authentication setting\n        auth_settings = ['ApiKeyAuth', 'auth0Bearer']  # noqa: E501\n\n        return self.api_client.call_api(\n            '/v1/datasets/{datasetId}/samples/{sampleId}', 'PUT',\n            path_params,\n            query_params,\n            header_params,\n            body=body_params,\n            post_params=form_params,\n            files=local_var_files,\n            response_type=None,  # noqa: E501\n            auth_settings=auth_settings,\n            async_req=params.get('async_req'),\n            _return_http_data_only=params.get('_return_http_data_only'),\n            _preload_content=params.get('_preload_content', True),\n            _request_timeout=params.get('_request_timeout'),\n            collection_formats=collection_formats)",
  "class DatasetCreateRequest(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'name': 'DatasetName',\n        'type': 'DatasetType',\n        'img_type': 'ImageType'\n    }\n\n    attribute_map = {\n        'name': 'name',\n        'type': 'type',\n        'img_type': 'imgType'\n    }\n\n    def __init__(self, name=None, type=None, img_type=None, _configuration=None):  # noqa: E501\n        \"\"\"DatasetCreateRequest - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._name = None\n        self._type = None\n        self._img_type = None\n        self.discriminator = None\n\n        self.name = name\n        if type is not None:\n            self.type = type\n        if img_type is not None:\n            self.img_type = img_type\n\n    @property\n    def name(self):\n        \"\"\"Gets the name of this DatasetCreateRequest.  # noqa: E501\n\n\n        :return: The name of this DatasetCreateRequest.  # noqa: E501\n        :rtype: DatasetName\n        \"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        \"\"\"Sets the name of this DatasetCreateRequest.\n\n\n        :param name: The name of this DatasetCreateRequest.  # noqa: E501\n        :type: DatasetName\n        \"\"\"\n        if self._configuration.client_side_validation and name is None:\n            raise ValueError(\"Invalid value for `name`, must not be `None`\")  # noqa: E501\n\n        self._name = name\n\n    @property\n    def type(self):\n        \"\"\"Gets the type of this DatasetCreateRequest.  # noqa: E501\n\n\n        :return: The type of this DatasetCreateRequest.  # noqa: E501\n        :rtype: DatasetType\n        \"\"\"\n        return self._type\n\n    @type.setter\n    def type(self, type):\n        \"\"\"Sets the type of this DatasetCreateRequest.\n\n\n        :param type: The type of this DatasetCreateRequest.  # noqa: E501\n        :type: DatasetType\n        \"\"\"\n\n        self._type = type\n\n    @property\n    def img_type(self):\n        \"\"\"Gets the img_type of this DatasetCreateRequest.  # noqa: E501\n\n\n        :return: The img_type of this DatasetCreateRequest.  # noqa: E501\n        :rtype: ImageType\n        \"\"\"\n        return self._img_type\n\n    @img_type.setter\n    def img_type(self, img_type):\n        \"\"\"Sets the img_type of this DatasetCreateRequest.\n\n\n        :param img_type: The img_type of this DatasetCreateRequest.  # noqa: E501\n        :type: ImageType\n        \"\"\"\n\n        self._img_type = img_type\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(DatasetCreateRequest, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, DatasetCreateRequest):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, DatasetCreateRequest):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, name=None, type=None, img_type=None, _configuration=None):  # noqa: E501\n        \"\"\"DatasetCreateRequest - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._name = None\n        self._type = None\n        self._img_type = None\n        self.discriminator = None\n\n        self.name = name\n        if type is not None:\n            self.type = type\n        if img_type is not None:\n            self.img_type = img_type",
  "def name(self):\n        \"\"\"Gets the name of this DatasetCreateRequest.  # noqa: E501\n\n\n        :return: The name of this DatasetCreateRequest.  # noqa: E501\n        :rtype: DatasetName\n        \"\"\"\n        return self._name",
  "def name(self, name):\n        \"\"\"Sets the name of this DatasetCreateRequest.\n\n\n        :param name: The name of this DatasetCreateRequest.  # noqa: E501\n        :type: DatasetName\n        \"\"\"\n        if self._configuration.client_side_validation and name is None:\n            raise ValueError(\"Invalid value for `name`, must not be `None`\")  # noqa: E501\n\n        self._name = name",
  "def type(self):\n        \"\"\"Gets the type of this DatasetCreateRequest.  # noqa: E501\n\n\n        :return: The type of this DatasetCreateRequest.  # noqa: E501\n        :rtype: DatasetType\n        \"\"\"\n        return self._type",
  "def type(self, type):\n        \"\"\"Sets the type of this DatasetCreateRequest.\n\n\n        :param type: The type of this DatasetCreateRequest.  # noqa: E501\n        :type: DatasetType\n        \"\"\"\n\n        self._type = type",
  "def img_type(self):\n        \"\"\"Gets the img_type of this DatasetCreateRequest.  # noqa: E501\n\n\n        :return: The img_type of this DatasetCreateRequest.  # noqa: E501\n        :rtype: ImageType\n        \"\"\"\n        return self._img_type",
  "def img_type(self, img_type):\n        \"\"\"Sets the img_type of this DatasetCreateRequest.\n\n\n        :param img_type: The img_type of this DatasetCreateRequest.  # noqa: E501\n        :type: ImageType\n        \"\"\"\n\n        self._img_type = img_type",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(DatasetCreateRequest, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, DatasetCreateRequest):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, DatasetCreateRequest):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class SampleData(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'id': 'MongoObjectID',\n        'dataset_id': 'MongoObjectID',\n        'file_name': 'str',\n        'thumb_name': 'str',\n        'exif': 'dict(str, object)',\n        'meta_data': 'SampleMetaData',\n        'index': 'int',\n        'created_at': 'Timestamp',\n        'last_modified_at': 'Timestamp'\n    }\n\n    attribute_map = {\n        'id': 'id',\n        'dataset_id': 'datasetId',\n        'file_name': 'fileName',\n        'thumb_name': 'thumbName',\n        'exif': 'exif',\n        'meta_data': 'metaData',\n        'index': 'index',\n        'created_at': 'createdAt',\n        'last_modified_at': 'lastModifiedAt'\n    }\n\n    def __init__(self, id=None, dataset_id=None, file_name=None, thumb_name=None, exif=None, meta_data=None, index=None, created_at=None, last_modified_at=None, _configuration=None):  # noqa: E501\n        \"\"\"SampleData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._id = None\n        self._dataset_id = None\n        self._file_name = None\n        self._thumb_name = None\n        self._exif = None\n        self._meta_data = None\n        self._index = None\n        self._created_at = None\n        self._last_modified_at = None\n        self.discriminator = None\n\n        self.id = id\n        if dataset_id is not None:\n            self.dataset_id = dataset_id\n        self.file_name = file_name\n        if thumb_name is not None:\n            self.thumb_name = thumb_name\n        if exif is not None:\n            self.exif = exif\n        if meta_data is not None:\n            self.meta_data = meta_data\n        if index is not None:\n            self.index = index\n        if created_at is not None:\n            self.created_at = created_at\n        if last_modified_at is not None:\n            self.last_modified_at = last_modified_at\n\n    @property\n    def id(self):\n        \"\"\"Gets the id of this SampleData.  # noqa: E501\n\n\n        :return: The id of this SampleData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._id\n\n    @id.setter\n    def id(self, id):\n        \"\"\"Sets the id of this SampleData.\n\n\n        :param id: The id of this SampleData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and id is None:\n            raise ValueError(\"Invalid value for `id`, must not be `None`\")  # noqa: E501\n\n        self._id = id\n\n    @property\n    def dataset_id(self):\n        \"\"\"Gets the dataset_id of this SampleData.  # noqa: E501\n\n\n        :return: The dataset_id of this SampleData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._dataset_id\n\n    @dataset_id.setter\n    def dataset_id(self, dataset_id):\n        \"\"\"Sets the dataset_id of this SampleData.\n\n\n        :param dataset_id: The dataset_id of this SampleData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n\n        self._dataset_id = dataset_id\n\n    @property\n    def file_name(self):\n        \"\"\"Gets the file_name of this SampleData.  # noqa: E501\n\n\n        :return: The file_name of this SampleData.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._file_name\n\n    @file_name.setter\n    def file_name(self, file_name):\n        \"\"\"Sets the file_name of this SampleData.\n\n\n        :param file_name: The file_name of this SampleData.  # noqa: E501\n        :type: str\n        \"\"\"\n        if self._configuration.client_side_validation and file_name is None:\n            raise ValueError(\"Invalid value for `file_name`, must not be `None`\")  # noqa: E501\n\n        self._file_name = file_name\n\n    @property\n    def thumb_name(self):\n        \"\"\"Gets the thumb_name of this SampleData.  # noqa: E501\n\n\n        :return: The thumb_name of this SampleData.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._thumb_name\n\n    @thumb_name.setter\n    def thumb_name(self, thumb_name):\n        \"\"\"Sets the thumb_name of this SampleData.\n\n\n        :param thumb_name: The thumb_name of this SampleData.  # noqa: E501\n        :type: str\n        \"\"\"\n\n        self._thumb_name = thumb_name\n\n    @property\n    def exif(self):\n        \"\"\"Gets the exif of this SampleData.  # noqa: E501\n\n\n        :return: The exif of this SampleData.  # noqa: E501\n        :rtype: dict(str, object)\n        \"\"\"\n        return self._exif\n\n    @exif.setter\n    def exif(self, exif):\n        \"\"\"Sets the exif of this SampleData.\n\n\n        :param exif: The exif of this SampleData.  # noqa: E501\n        :type: dict(str, object)\n        \"\"\"\n\n        self._exif = exif\n\n    @property\n    def meta_data(self):\n        \"\"\"Gets the meta_data of this SampleData.  # noqa: E501\n\n\n        :return: The meta_data of this SampleData.  # noqa: E501\n        :rtype: SampleMetaData\n        \"\"\"\n        return self._meta_data\n\n    @meta_data.setter\n    def meta_data(self, meta_data):\n        \"\"\"Sets the meta_data of this SampleData.\n\n\n        :param meta_data: The meta_data of this SampleData.  # noqa: E501\n        :type: SampleMetaData\n        \"\"\"\n\n        self._meta_data = meta_data\n\n    @property\n    def index(self):\n        \"\"\"Gets the index of this SampleData.  # noqa: E501\n\n\n        :return: The index of this SampleData.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._index\n\n    @index.setter\n    def index(self, index):\n        \"\"\"Sets the index of this SampleData.\n\n\n        :param index: The index of this SampleData.  # noqa: E501\n        :type: int\n        \"\"\"\n\n        self._index = index\n\n    @property\n    def created_at(self):\n        \"\"\"Gets the created_at of this SampleData.  # noqa: E501\n\n\n        :return: The created_at of this SampleData.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._created_at\n\n    @created_at.setter\n    def created_at(self, created_at):\n        \"\"\"Sets the created_at of this SampleData.\n\n\n        :param created_at: The created_at of this SampleData.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n\n        self._created_at = created_at\n\n    @property\n    def last_modified_at(self):\n        \"\"\"Gets the last_modified_at of this SampleData.  # noqa: E501\n\n\n        :return: The last_modified_at of this SampleData.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._last_modified_at\n\n    @last_modified_at.setter\n    def last_modified_at(self, last_modified_at):\n        \"\"\"Sets the last_modified_at of this SampleData.\n\n\n        :param last_modified_at: The last_modified_at of this SampleData.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n\n        self._last_modified_at = last_modified_at\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(SampleData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, SampleData):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, SampleData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, id=None, dataset_id=None, file_name=None, thumb_name=None, exif=None, meta_data=None, index=None, created_at=None, last_modified_at=None, _configuration=None):  # noqa: E501\n        \"\"\"SampleData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._id = None\n        self._dataset_id = None\n        self._file_name = None\n        self._thumb_name = None\n        self._exif = None\n        self._meta_data = None\n        self._index = None\n        self._created_at = None\n        self._last_modified_at = None\n        self.discriminator = None\n\n        self.id = id\n        if dataset_id is not None:\n            self.dataset_id = dataset_id\n        self.file_name = file_name\n        if thumb_name is not None:\n            self.thumb_name = thumb_name\n        if exif is not None:\n            self.exif = exif\n        if meta_data is not None:\n            self.meta_data = meta_data\n        if index is not None:\n            self.index = index\n        if created_at is not None:\n            self.created_at = created_at\n        if last_modified_at is not None:\n            self.last_modified_at = last_modified_at",
  "def id(self):\n        \"\"\"Gets the id of this SampleData.  # noqa: E501\n\n\n        :return: The id of this SampleData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._id",
  "def id(self, id):\n        \"\"\"Sets the id of this SampleData.\n\n\n        :param id: The id of this SampleData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and id is None:\n            raise ValueError(\"Invalid value for `id`, must not be `None`\")  # noqa: E501\n\n        self._id = id",
  "def dataset_id(self):\n        \"\"\"Gets the dataset_id of this SampleData.  # noqa: E501\n\n\n        :return: The dataset_id of this SampleData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._dataset_id",
  "def dataset_id(self, dataset_id):\n        \"\"\"Sets the dataset_id of this SampleData.\n\n\n        :param dataset_id: The dataset_id of this SampleData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n\n        self._dataset_id = dataset_id",
  "def file_name(self):\n        \"\"\"Gets the file_name of this SampleData.  # noqa: E501\n\n\n        :return: The file_name of this SampleData.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._file_name",
  "def file_name(self, file_name):\n        \"\"\"Sets the file_name of this SampleData.\n\n\n        :param file_name: The file_name of this SampleData.  # noqa: E501\n        :type: str\n        \"\"\"\n        if self._configuration.client_side_validation and file_name is None:\n            raise ValueError(\"Invalid value for `file_name`, must not be `None`\")  # noqa: E501\n\n        self._file_name = file_name",
  "def thumb_name(self):\n        \"\"\"Gets the thumb_name of this SampleData.  # noqa: E501\n\n\n        :return: The thumb_name of this SampleData.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._thumb_name",
  "def thumb_name(self, thumb_name):\n        \"\"\"Sets the thumb_name of this SampleData.\n\n\n        :param thumb_name: The thumb_name of this SampleData.  # noqa: E501\n        :type: str\n        \"\"\"\n\n        self._thumb_name = thumb_name",
  "def exif(self):\n        \"\"\"Gets the exif of this SampleData.  # noqa: E501\n\n\n        :return: The exif of this SampleData.  # noqa: E501\n        :rtype: dict(str, object)\n        \"\"\"\n        return self._exif",
  "def exif(self, exif):\n        \"\"\"Sets the exif of this SampleData.\n\n\n        :param exif: The exif of this SampleData.  # noqa: E501\n        :type: dict(str, object)\n        \"\"\"\n\n        self._exif = exif",
  "def meta_data(self):\n        \"\"\"Gets the meta_data of this SampleData.  # noqa: E501\n\n\n        :return: The meta_data of this SampleData.  # noqa: E501\n        :rtype: SampleMetaData\n        \"\"\"\n        return self._meta_data",
  "def meta_data(self, meta_data):\n        \"\"\"Sets the meta_data of this SampleData.\n\n\n        :param meta_data: The meta_data of this SampleData.  # noqa: E501\n        :type: SampleMetaData\n        \"\"\"\n\n        self._meta_data = meta_data",
  "def index(self):\n        \"\"\"Gets the index of this SampleData.  # noqa: E501\n\n\n        :return: The index of this SampleData.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._index",
  "def index(self, index):\n        \"\"\"Sets the index of this SampleData.\n\n\n        :param index: The index of this SampleData.  # noqa: E501\n        :type: int\n        \"\"\"\n\n        self._index = index",
  "def created_at(self):\n        \"\"\"Gets the created_at of this SampleData.  # noqa: E501\n\n\n        :return: The created_at of this SampleData.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._created_at",
  "def created_at(self, created_at):\n        \"\"\"Sets the created_at of this SampleData.\n\n\n        :param created_at: The created_at of this SampleData.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n\n        self._created_at = created_at",
  "def last_modified_at(self):\n        \"\"\"Gets the last_modified_at of this SampleData.  # noqa: E501\n\n\n        :return: The last_modified_at of this SampleData.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._last_modified_at",
  "def last_modified_at(self, last_modified_at):\n        \"\"\"Sets the last_modified_at of this SampleData.\n\n\n        :param last_modified_at: The last_modified_at of this SampleData.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n\n        self._last_modified_at = last_modified_at",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(SampleData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, SampleData):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, SampleData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class InitialTagCreateRequest(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'name': 'TagName',\n        'creator': 'TagCreator',\n        'img_type': 'ImageType'\n    }\n\n    attribute_map = {\n        'name': 'name',\n        'creator': 'creator',\n        'img_type': 'imgType'\n    }\n\n    def __init__(self, name=None, creator=None, img_type=None, _configuration=None):  # noqa: E501\n        \"\"\"InitialTagCreateRequest - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._name = None\n        self._creator = None\n        self._img_type = None\n        self.discriminator = None\n\n        if name is not None:\n            self.name = name\n        if creator is not None:\n            self.creator = creator\n        self.img_type = img_type\n\n    @property\n    def name(self):\n        \"\"\"Gets the name of this InitialTagCreateRequest.  # noqa: E501\n\n\n        :return: The name of this InitialTagCreateRequest.  # noqa: E501\n        :rtype: TagName\n        \"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        \"\"\"Sets the name of this InitialTagCreateRequest.\n\n\n        :param name: The name of this InitialTagCreateRequest.  # noqa: E501\n        :type: TagName\n        \"\"\"\n\n        self._name = name\n\n    @property\n    def creator(self):\n        \"\"\"Gets the creator of this InitialTagCreateRequest.  # noqa: E501\n\n\n        :return: The creator of this InitialTagCreateRequest.  # noqa: E501\n        :rtype: TagCreator\n        \"\"\"\n        return self._creator\n\n    @creator.setter\n    def creator(self, creator):\n        \"\"\"Sets the creator of this InitialTagCreateRequest.\n\n\n        :param creator: The creator of this InitialTagCreateRequest.  # noqa: E501\n        :type: TagCreator\n        \"\"\"\n\n        self._creator = creator\n\n    @property\n    def img_type(self):\n        \"\"\"Gets the img_type of this InitialTagCreateRequest.  # noqa: E501\n\n\n        :return: The img_type of this InitialTagCreateRequest.  # noqa: E501\n        :rtype: ImageType\n        \"\"\"\n        return self._img_type\n\n    @img_type.setter\n    def img_type(self, img_type):\n        \"\"\"Sets the img_type of this InitialTagCreateRequest.\n\n\n        :param img_type: The img_type of this InitialTagCreateRequest.  # noqa: E501\n        :type: ImageType\n        \"\"\"\n        if self._configuration.client_side_validation and img_type is None:\n            raise ValueError(\"Invalid value for `img_type`, must not be `None`\")  # noqa: E501\n\n        self._img_type = img_type\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(InitialTagCreateRequest, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, InitialTagCreateRequest):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, InitialTagCreateRequest):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, name=None, creator=None, img_type=None, _configuration=None):  # noqa: E501\n        \"\"\"InitialTagCreateRequest - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._name = None\n        self._creator = None\n        self._img_type = None\n        self.discriminator = None\n\n        if name is not None:\n            self.name = name\n        if creator is not None:\n            self.creator = creator\n        self.img_type = img_type",
  "def name(self):\n        \"\"\"Gets the name of this InitialTagCreateRequest.  # noqa: E501\n\n\n        :return: The name of this InitialTagCreateRequest.  # noqa: E501\n        :rtype: TagName\n        \"\"\"\n        return self._name",
  "def name(self, name):\n        \"\"\"Sets the name of this InitialTagCreateRequest.\n\n\n        :param name: The name of this InitialTagCreateRequest.  # noqa: E501\n        :type: TagName\n        \"\"\"\n\n        self._name = name",
  "def creator(self):\n        \"\"\"Gets the creator of this InitialTagCreateRequest.  # noqa: E501\n\n\n        :return: The creator of this InitialTagCreateRequest.  # noqa: E501\n        :rtype: TagCreator\n        \"\"\"\n        return self._creator",
  "def creator(self, creator):\n        \"\"\"Sets the creator of this InitialTagCreateRequest.\n\n\n        :param creator: The creator of this InitialTagCreateRequest.  # noqa: E501\n        :type: TagCreator\n        \"\"\"\n\n        self._creator = creator",
  "def img_type(self):\n        \"\"\"Gets the img_type of this InitialTagCreateRequest.  # noqa: E501\n\n\n        :return: The img_type of this InitialTagCreateRequest.  # noqa: E501\n        :rtype: ImageType\n        \"\"\"\n        return self._img_type",
  "def img_type(self, img_type):\n        \"\"\"Sets the img_type of this InitialTagCreateRequest.\n\n\n        :param img_type: The img_type of this InitialTagCreateRequest.  # noqa: E501\n        :type: ImageType\n        \"\"\"\n        if self._configuration.client_side_validation and img_type is None:\n            raise ValueError(\"Invalid value for `img_type`, must not be `None`\")  # noqa: E501\n\n        self._img_type = img_type",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(InitialTagCreateRequest, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, InitialTagCreateRequest):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, InitialTagCreateRequest):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class JobStatusMeta(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'total': 'int',\n        'processed': 'int'\n    }\n\n    attribute_map = {\n        'total': 'total',\n        'processed': 'processed'\n    }\n\n    def __init__(self, total=None, processed=None, _configuration=None):  # noqa: E501\n        \"\"\"JobStatusMeta - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._total = None\n        self._processed = None\n        self.discriminator = None\n\n        self.total = total\n        self.processed = processed\n\n    @property\n    def total(self):\n        \"\"\"Gets the total of this JobStatusMeta.  # noqa: E501\n\n\n        :return: The total of this JobStatusMeta.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._total\n\n    @total.setter\n    def total(self, total):\n        \"\"\"Sets the total of this JobStatusMeta.\n\n\n        :param total: The total of this JobStatusMeta.  # noqa: E501\n        :type: int\n        \"\"\"\n        if self._configuration.client_side_validation and total is None:\n            raise ValueError(\"Invalid value for `total`, must not be `None`\")  # noqa: E501\n\n        self._total = total\n\n    @property\n    def processed(self):\n        \"\"\"Gets the processed of this JobStatusMeta.  # noqa: E501\n\n\n        :return: The processed of this JobStatusMeta.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._processed\n\n    @processed.setter\n    def processed(self, processed):\n        \"\"\"Sets the processed of this JobStatusMeta.\n\n\n        :param processed: The processed of this JobStatusMeta.  # noqa: E501\n        :type: int\n        \"\"\"\n        if self._configuration.client_side_validation and processed is None:\n            raise ValueError(\"Invalid value for `processed`, must not be `None`\")  # noqa: E501\n\n        self._processed = processed\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(JobStatusMeta, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, JobStatusMeta):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, JobStatusMeta):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, total=None, processed=None, _configuration=None):  # noqa: E501\n        \"\"\"JobStatusMeta - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._total = None\n        self._processed = None\n        self.discriminator = None\n\n        self.total = total\n        self.processed = processed",
  "def total(self):\n        \"\"\"Gets the total of this JobStatusMeta.  # noqa: E501\n\n\n        :return: The total of this JobStatusMeta.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._total",
  "def total(self, total):\n        \"\"\"Sets the total of this JobStatusMeta.\n\n\n        :param total: The total of this JobStatusMeta.  # noqa: E501\n        :type: int\n        \"\"\"\n        if self._configuration.client_side_validation and total is None:\n            raise ValueError(\"Invalid value for `total`, must not be `None`\")  # noqa: E501\n\n        self._total = total",
  "def processed(self):\n        \"\"\"Gets the processed of this JobStatusMeta.  # noqa: E501\n\n\n        :return: The processed of this JobStatusMeta.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._processed",
  "def processed(self, processed):\n        \"\"\"Sets the processed of this JobStatusMeta.\n\n\n        :param processed: The processed of this JobStatusMeta.  # noqa: E501\n        :type: int\n        \"\"\"\n        if self._configuration.client_side_validation and processed is None:\n            raise ValueError(\"Invalid value for `processed`, must not be `None`\")  # noqa: E501\n\n        self._processed = processed",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(JobStatusMeta, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, JobStatusMeta):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, JobStatusMeta):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class TagActiveLearningScoresData(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'id': 'MongoObjectID',\n        'tag_id': 'MongoObjectID',\n        'score_type': 'ActiveLearningScoreType',\n        'created_at': 'Timestamp'\n    }\n\n    attribute_map = {\n        'id': 'id',\n        'tag_id': 'tagId',\n        'score_type': 'scoreType',\n        'created_at': 'createdAt'\n    }\n\n    def __init__(self, id=None, tag_id=None, score_type=None, created_at=None, _configuration=None):  # noqa: E501\n        \"\"\"TagActiveLearningScoresData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._id = None\n        self._tag_id = None\n        self._score_type = None\n        self._created_at = None\n        self.discriminator = None\n\n        self.id = id\n        self.tag_id = tag_id\n        self.score_type = score_type\n        self.created_at = created_at\n\n    @property\n    def id(self):\n        \"\"\"Gets the id of this TagActiveLearningScoresData.  # noqa: E501\n\n\n        :return: The id of this TagActiveLearningScoresData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._id\n\n    @id.setter\n    def id(self, id):\n        \"\"\"Sets the id of this TagActiveLearningScoresData.\n\n\n        :param id: The id of this TagActiveLearningScoresData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and id is None:\n            raise ValueError(\"Invalid value for `id`, must not be `None`\")  # noqa: E501\n\n        self._id = id\n\n    @property\n    def tag_id(self):\n        \"\"\"Gets the tag_id of this TagActiveLearningScoresData.  # noqa: E501\n\n\n        :return: The tag_id of this TagActiveLearningScoresData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._tag_id\n\n    @tag_id.setter\n    def tag_id(self, tag_id):\n        \"\"\"Sets the tag_id of this TagActiveLearningScoresData.\n\n\n        :param tag_id: The tag_id of this TagActiveLearningScoresData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and tag_id is None:\n            raise ValueError(\"Invalid value for `tag_id`, must not be `None`\")  # noqa: E501\n\n        self._tag_id = tag_id\n\n    @property\n    def score_type(self):\n        \"\"\"Gets the score_type of this TagActiveLearningScoresData.  # noqa: E501\n\n\n        :return: The score_type of this TagActiveLearningScoresData.  # noqa: E501\n        :rtype: ActiveLearningScoreType\n        \"\"\"\n        return self._score_type\n\n    @score_type.setter\n    def score_type(self, score_type):\n        \"\"\"Sets the score_type of this TagActiveLearningScoresData.\n\n\n        :param score_type: The score_type of this TagActiveLearningScoresData.  # noqa: E501\n        :type: ActiveLearningScoreType\n        \"\"\"\n        if self._configuration.client_side_validation and score_type is None:\n            raise ValueError(\"Invalid value for `score_type`, must not be `None`\")  # noqa: E501\n\n        self._score_type = score_type\n\n    @property\n    def created_at(self):\n        \"\"\"Gets the created_at of this TagActiveLearningScoresData.  # noqa: E501\n\n\n        :return: The created_at of this TagActiveLearningScoresData.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._created_at\n\n    @created_at.setter\n    def created_at(self, created_at):\n        \"\"\"Sets the created_at of this TagActiveLearningScoresData.\n\n\n        :param created_at: The created_at of this TagActiveLearningScoresData.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n        if self._configuration.client_side_validation and created_at is None:\n            raise ValueError(\"Invalid value for `created_at`, must not be `None`\")  # noqa: E501\n\n        self._created_at = created_at\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(TagActiveLearningScoresData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, TagActiveLearningScoresData):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, TagActiveLearningScoresData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, id=None, tag_id=None, score_type=None, created_at=None, _configuration=None):  # noqa: E501\n        \"\"\"TagActiveLearningScoresData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._id = None\n        self._tag_id = None\n        self._score_type = None\n        self._created_at = None\n        self.discriminator = None\n\n        self.id = id\n        self.tag_id = tag_id\n        self.score_type = score_type\n        self.created_at = created_at",
  "def id(self):\n        \"\"\"Gets the id of this TagActiveLearningScoresData.  # noqa: E501\n\n\n        :return: The id of this TagActiveLearningScoresData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._id",
  "def id(self, id):\n        \"\"\"Sets the id of this TagActiveLearningScoresData.\n\n\n        :param id: The id of this TagActiveLearningScoresData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and id is None:\n            raise ValueError(\"Invalid value for `id`, must not be `None`\")  # noqa: E501\n\n        self._id = id",
  "def tag_id(self):\n        \"\"\"Gets the tag_id of this TagActiveLearningScoresData.  # noqa: E501\n\n\n        :return: The tag_id of this TagActiveLearningScoresData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._tag_id",
  "def tag_id(self, tag_id):\n        \"\"\"Sets the tag_id of this TagActiveLearningScoresData.\n\n\n        :param tag_id: The tag_id of this TagActiveLearningScoresData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and tag_id is None:\n            raise ValueError(\"Invalid value for `tag_id`, must not be `None`\")  # noqa: E501\n\n        self._tag_id = tag_id",
  "def score_type(self):\n        \"\"\"Gets the score_type of this TagActiveLearningScoresData.  # noqa: E501\n\n\n        :return: The score_type of this TagActiveLearningScoresData.  # noqa: E501\n        :rtype: ActiveLearningScoreType\n        \"\"\"\n        return self._score_type",
  "def score_type(self, score_type):\n        \"\"\"Sets the score_type of this TagActiveLearningScoresData.\n\n\n        :param score_type: The score_type of this TagActiveLearningScoresData.  # noqa: E501\n        :type: ActiveLearningScoreType\n        \"\"\"\n        if self._configuration.client_side_validation and score_type is None:\n            raise ValueError(\"Invalid value for `score_type`, must not be `None`\")  # noqa: E501\n\n        self._score_type = score_type",
  "def created_at(self):\n        \"\"\"Gets the created_at of this TagActiveLearningScoresData.  # noqa: E501\n\n\n        :return: The created_at of this TagActiveLearningScoresData.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._created_at",
  "def created_at(self, created_at):\n        \"\"\"Sets the created_at of this TagActiveLearningScoresData.\n\n\n        :param created_at: The created_at of this TagActiveLearningScoresData.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n        if self._configuration.client_side_validation and created_at is None:\n            raise ValueError(\"Invalid value for `created_at`, must not be `None`\")  # noqa: E501\n\n        self._created_at = created_at",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(TagActiveLearningScoresData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, TagActiveLearningScoresData):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, TagActiveLearningScoresData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class MongoObjectID(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n    }\n\n    attribute_map = {\n    }\n\n    def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"MongoObjectID - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(MongoObjectID, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, MongoObjectID):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, MongoObjectID):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"MongoObjectID - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(MongoObjectID, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, MongoObjectID):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, MongoObjectID):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class JobState(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    allowed enum values\n    \"\"\"\n    UNKNOWN = \"UNKNOWN\"\n    WAITING = \"WAITING\"\n    RUNNING = \"RUNNING\"\n    FAILED = \"FAILED\"\n    FINISHED = \"FINISHED\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n    }\n\n    attribute_map = {\n    }\n\n    def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"JobState - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(JobState, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, JobState):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, JobState):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"JobState - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(JobState, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, JobState):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, JobState):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class EmbeddingData(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'id': 'MongoObjectID',\n        'dataset': 'MongoObjectID',\n        'name': 'str'\n    }\n\n    attribute_map = {\n        'id': 'id',\n        'dataset': 'dataset',\n        'name': 'name'\n    }\n\n    def __init__(self, id=None, dataset=None, name=None, _configuration=None):  # noqa: E501\n        \"\"\"EmbeddingData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._id = None\n        self._dataset = None\n        self._name = None\n        self.discriminator = None\n\n        self.id = id\n        self.dataset = dataset\n        self.name = name\n\n    @property\n    def id(self):\n        \"\"\"Gets the id of this EmbeddingData.  # noqa: E501\n\n\n        :return: The id of this EmbeddingData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._id\n\n    @id.setter\n    def id(self, id):\n        \"\"\"Sets the id of this EmbeddingData.\n\n\n        :param id: The id of this EmbeddingData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and id is None:\n            raise ValueError(\"Invalid value for `id`, must not be `None`\")  # noqa: E501\n\n        self._id = id\n\n    @property\n    def dataset(self):\n        \"\"\"Gets the dataset of this EmbeddingData.  # noqa: E501\n\n\n        :return: The dataset of this EmbeddingData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._dataset\n\n    @dataset.setter\n    def dataset(self, dataset):\n        \"\"\"Sets the dataset of this EmbeddingData.\n\n\n        :param dataset: The dataset of this EmbeddingData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and dataset is None:\n            raise ValueError(\"Invalid value for `dataset`, must not be `None`\")  # noqa: E501\n\n        self._dataset = dataset\n\n    @property\n    def name(self):\n        \"\"\"Gets the name of this EmbeddingData.  # noqa: E501\n\n\n        :return: The name of this EmbeddingData.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        \"\"\"Sets the name of this EmbeddingData.\n\n\n        :param name: The name of this EmbeddingData.  # noqa: E501\n        :type: str\n        \"\"\"\n        if self._configuration.client_side_validation and name is None:\n            raise ValueError(\"Invalid value for `name`, must not be `None`\")  # noqa: E501\n\n        self._name = name\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(EmbeddingData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, EmbeddingData):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, EmbeddingData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, id=None, dataset=None, name=None, _configuration=None):  # noqa: E501\n        \"\"\"EmbeddingData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._id = None\n        self._dataset = None\n        self._name = None\n        self.discriminator = None\n\n        self.id = id\n        self.dataset = dataset\n        self.name = name",
  "def id(self):\n        \"\"\"Gets the id of this EmbeddingData.  # noqa: E501\n\n\n        :return: The id of this EmbeddingData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._id",
  "def id(self, id):\n        \"\"\"Sets the id of this EmbeddingData.\n\n\n        :param id: The id of this EmbeddingData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and id is None:\n            raise ValueError(\"Invalid value for `id`, must not be `None`\")  # noqa: E501\n\n        self._id = id",
  "def dataset(self):\n        \"\"\"Gets the dataset of this EmbeddingData.  # noqa: E501\n\n\n        :return: The dataset of this EmbeddingData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._dataset",
  "def dataset(self, dataset):\n        \"\"\"Sets the dataset of this EmbeddingData.\n\n\n        :param dataset: The dataset of this EmbeddingData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and dataset is None:\n            raise ValueError(\"Invalid value for `dataset`, must not be `None`\")  # noqa: E501\n\n        self._dataset = dataset",
  "def name(self):\n        \"\"\"Gets the name of this EmbeddingData.  # noqa: E501\n\n\n        :return: The name of this EmbeddingData.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._name",
  "def name(self, name):\n        \"\"\"Sets the name of this EmbeddingData.\n\n\n        :param name: The name of this EmbeddingData.  # noqa: E501\n        :type: str\n        \"\"\"\n        if self._configuration.client_side_validation and name is None:\n            raise ValueError(\"Invalid value for `name`, must not be `None`\")  # noqa: E501\n\n        self._name = name",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(EmbeddingData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, EmbeddingData):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, EmbeddingData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class DimensionalityReductionMethod(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    allowed enum values\n    \"\"\"\n    PCA = \"PCA\"\n    UMAP = \"UMAP\"\n    TSNE = \"TSNE\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n    }\n\n    attribute_map = {\n    }\n\n    def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"DimensionalityReductionMethod - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(DimensionalityReductionMethod, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, DimensionalityReductionMethod):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, DimensionalityReductionMethod):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"DimensionalityReductionMethod - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(DimensionalityReductionMethod, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, DimensionalityReductionMethod):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, DimensionalityReductionMethod):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class WriteCSVUrlData(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'signed_write_url': 'str',\n        'embedding_id': 'str'\n    }\n\n    attribute_map = {\n        'signed_write_url': 'signedWriteUrl',\n        'embedding_id': 'embeddingId'\n    }\n\n    def __init__(self, signed_write_url=None, embedding_id=None, _configuration=None):  # noqa: E501\n        \"\"\"WriteCSVUrlData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._signed_write_url = None\n        self._embedding_id = None\n        self.discriminator = None\n\n        self.signed_write_url = signed_write_url\n        self.embedding_id = embedding_id\n\n    @property\n    def signed_write_url(self):\n        \"\"\"Gets the signed_write_url of this WriteCSVUrlData.  # noqa: E501\n\n\n        :return: The signed_write_url of this WriteCSVUrlData.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._signed_write_url\n\n    @signed_write_url.setter\n    def signed_write_url(self, signed_write_url):\n        \"\"\"Sets the signed_write_url of this WriteCSVUrlData.\n\n\n        :param signed_write_url: The signed_write_url of this WriteCSVUrlData.  # noqa: E501\n        :type: str\n        \"\"\"\n        if self._configuration.client_side_validation and signed_write_url is None:\n            raise ValueError(\"Invalid value for `signed_write_url`, must not be `None`\")  # noqa: E501\n\n        self._signed_write_url = signed_write_url\n\n    @property\n    def embedding_id(self):\n        \"\"\"Gets the embedding_id of this WriteCSVUrlData.  # noqa: E501\n\n\n        :return: The embedding_id of this WriteCSVUrlData.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._embedding_id\n\n    @embedding_id.setter\n    def embedding_id(self, embedding_id):\n        \"\"\"Sets the embedding_id of this WriteCSVUrlData.\n\n\n        :param embedding_id: The embedding_id of this WriteCSVUrlData.  # noqa: E501\n        :type: str\n        \"\"\"\n        if self._configuration.client_side_validation and embedding_id is None:\n            raise ValueError(\"Invalid value for `embedding_id`, must not be `None`\")  # noqa: E501\n\n        self._embedding_id = embedding_id\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(WriteCSVUrlData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, WriteCSVUrlData):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, WriteCSVUrlData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, signed_write_url=None, embedding_id=None, _configuration=None):  # noqa: E501\n        \"\"\"WriteCSVUrlData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._signed_write_url = None\n        self._embedding_id = None\n        self.discriminator = None\n\n        self.signed_write_url = signed_write_url\n        self.embedding_id = embedding_id",
  "def signed_write_url(self):\n        \"\"\"Gets the signed_write_url of this WriteCSVUrlData.  # noqa: E501\n\n\n        :return: The signed_write_url of this WriteCSVUrlData.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._signed_write_url",
  "def signed_write_url(self, signed_write_url):\n        \"\"\"Sets the signed_write_url of this WriteCSVUrlData.\n\n\n        :param signed_write_url: The signed_write_url of this WriteCSVUrlData.  # noqa: E501\n        :type: str\n        \"\"\"\n        if self._configuration.client_side_validation and signed_write_url is None:\n            raise ValueError(\"Invalid value for `signed_write_url`, must not be `None`\")  # noqa: E501\n\n        self._signed_write_url = signed_write_url",
  "def embedding_id(self):\n        \"\"\"Gets the embedding_id of this WriteCSVUrlData.  # noqa: E501\n\n\n        :return: The embedding_id of this WriteCSVUrlData.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._embedding_id",
  "def embedding_id(self, embedding_id):\n        \"\"\"Sets the embedding_id of this WriteCSVUrlData.\n\n\n        :param embedding_id: The embedding_id of this WriteCSVUrlData.  # noqa: E501\n        :type: str\n        \"\"\"\n        if self._configuration.client_side_validation and embedding_id is None:\n            raise ValueError(\"Invalid value for `embedding_id`, must not be `None`\")  # noqa: E501\n\n        self._embedding_id = embedding_id",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(WriteCSVUrlData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, WriteCSVUrlData):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, WriteCSVUrlData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class JobStatusDataResult(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'type': 'JobResultType',\n        'data': 'GeneralJobResult'\n    }\n\n    attribute_map = {\n        'type': 'type',\n        'data': 'data'\n    }\n\n    def __init__(self, type=None, data=None, _configuration=None):  # noqa: E501\n        \"\"\"JobStatusDataResult - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._type = None\n        self._data = None\n        self.discriminator = None\n\n        self.type = type\n        if data is not None:\n            self.data = data\n\n    @property\n    def type(self):\n        \"\"\"Gets the type of this JobStatusDataResult.  # noqa: E501\n\n\n        :return: The type of this JobStatusDataResult.  # noqa: E501\n        :rtype: JobResultType\n        \"\"\"\n        return self._type\n\n    @type.setter\n    def type(self, type):\n        \"\"\"Sets the type of this JobStatusDataResult.\n\n\n        :param type: The type of this JobStatusDataResult.  # noqa: E501\n        :type: JobResultType\n        \"\"\"\n        if self._configuration.client_side_validation and type is None:\n            raise ValueError(\"Invalid value for `type`, must not be `None`\")  # noqa: E501\n\n        self._type = type\n\n    @property\n    def data(self):\n        \"\"\"Gets the data of this JobStatusDataResult.  # noqa: E501\n\n\n        :return: The data of this JobStatusDataResult.  # noqa: E501\n        :rtype: GeneralJobResult\n        \"\"\"\n        return self._data\n\n    @data.setter\n    def data(self, data):\n        \"\"\"Sets the data of this JobStatusDataResult.\n\n\n        :param data: The data of this JobStatusDataResult.  # noqa: E501\n        :type: GeneralJobResult\n        \"\"\"\n\n        self._data = data\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(JobStatusDataResult, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, JobStatusDataResult):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, JobStatusDataResult):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, type=None, data=None, _configuration=None):  # noqa: E501\n        \"\"\"JobStatusDataResult - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._type = None\n        self._data = None\n        self.discriminator = None\n\n        self.type = type\n        if data is not None:\n            self.data = data",
  "def type(self):\n        \"\"\"Gets the type of this JobStatusDataResult.  # noqa: E501\n\n\n        :return: The type of this JobStatusDataResult.  # noqa: E501\n        :rtype: JobResultType\n        \"\"\"\n        return self._type",
  "def type(self, type):\n        \"\"\"Sets the type of this JobStatusDataResult.\n\n\n        :param type: The type of this JobStatusDataResult.  # noqa: E501\n        :type: JobResultType\n        \"\"\"\n        if self._configuration.client_side_validation and type is None:\n            raise ValueError(\"Invalid value for `type`, must not be `None`\")  # noqa: E501\n\n        self._type = type",
  "def data(self):\n        \"\"\"Gets the data of this JobStatusDataResult.  # noqa: E501\n\n\n        :return: The data of this JobStatusDataResult.  # noqa: E501\n        :rtype: GeneralJobResult\n        \"\"\"\n        return self._data",
  "def data(self, data):\n        \"\"\"Sets the data of this JobStatusDataResult.\n\n\n        :param data: The data of this JobStatusDataResult.  # noqa: E501\n        :type: GeneralJobResult\n        \"\"\"\n\n        self._data = data",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(JobStatusDataResult, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, JobStatusDataResult):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, JobStatusDataResult):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class ImageType(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    allowed enum values\n    \"\"\"\n    FULL = \"full\"\n    THUMBNAIL = \"thumbnail\"\n    META = \"meta\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n    }\n\n    attribute_map = {\n    }\n\n    def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"ImageType - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(ImageType, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, ImageType):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, ImageType):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"ImageType - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(ImageType, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, ImageType):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, ImageType):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class SamplingMethod(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    allowed enum values\n    \"\"\"\n    ACTIVE_LEARNING = \"ACTIVE_LEARNING\"\n    CORAL = \"CORAL\"\n    CORESET = \"CORESET\"\n    RANDOM = \"RANDOM\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n    }\n\n    attribute_map = {\n    }\n\n    def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"SamplingMethod - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(SamplingMethod, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, SamplingMethod):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, SamplingMethod):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"SamplingMethod - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(SamplingMethod, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, SamplingMethod):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, SamplingMethod):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class ActiveLearningScoreCreateRequest(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'score_type': 'ActiveLearningScoreType',\n        'scores': 'ActiveLearningScores'\n    }\n\n    attribute_map = {\n        'score_type': 'scoreType',\n        'scores': 'scores'\n    }\n\n    def __init__(self, score_type=None, scores=None, _configuration=None):  # noqa: E501\n        \"\"\"ActiveLearningScoreCreateRequest - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._score_type = None\n        self._scores = None\n        self.discriminator = None\n\n        self.score_type = score_type\n        self.scores = scores\n\n    @property\n    def score_type(self):\n        \"\"\"Gets the score_type of this ActiveLearningScoreCreateRequest.  # noqa: E501\n\n\n        :return: The score_type of this ActiveLearningScoreCreateRequest.  # noqa: E501\n        :rtype: ActiveLearningScoreType\n        \"\"\"\n        return self._score_type\n\n    @score_type.setter\n    def score_type(self, score_type):\n        \"\"\"Sets the score_type of this ActiveLearningScoreCreateRequest.\n\n\n        :param score_type: The score_type of this ActiveLearningScoreCreateRequest.  # noqa: E501\n        :type: ActiveLearningScoreType\n        \"\"\"\n        if self._configuration.client_side_validation and score_type is None:\n            raise ValueError(\"Invalid value for `score_type`, must not be `None`\")  # noqa: E501\n\n        self._score_type = score_type\n\n    @property\n    def scores(self):\n        \"\"\"Gets the scores of this ActiveLearningScoreCreateRequest.  # noqa: E501\n\n\n        :return: The scores of this ActiveLearningScoreCreateRequest.  # noqa: E501\n        :rtype: ActiveLearningScores\n        \"\"\"\n        return self._scores\n\n    @scores.setter\n    def scores(self, scores):\n        \"\"\"Sets the scores of this ActiveLearningScoreCreateRequest.\n\n\n        :param scores: The scores of this ActiveLearningScoreCreateRequest.  # noqa: E501\n        :type: ActiveLearningScores\n        \"\"\"\n        if self._configuration.client_side_validation and scores is None:\n            raise ValueError(\"Invalid value for `scores`, must not be `None`\")  # noqa: E501\n\n        self._scores = scores\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(ActiveLearningScoreCreateRequest, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, ActiveLearningScoreCreateRequest):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, ActiveLearningScoreCreateRequest):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, score_type=None, scores=None, _configuration=None):  # noqa: E501\n        \"\"\"ActiveLearningScoreCreateRequest - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._score_type = None\n        self._scores = None\n        self.discriminator = None\n\n        self.score_type = score_type\n        self.scores = scores",
  "def score_type(self):\n        \"\"\"Gets the score_type of this ActiveLearningScoreCreateRequest.  # noqa: E501\n\n\n        :return: The score_type of this ActiveLearningScoreCreateRequest.  # noqa: E501\n        :rtype: ActiveLearningScoreType\n        \"\"\"\n        return self._score_type",
  "def score_type(self, score_type):\n        \"\"\"Sets the score_type of this ActiveLearningScoreCreateRequest.\n\n\n        :param score_type: The score_type of this ActiveLearningScoreCreateRequest.  # noqa: E501\n        :type: ActiveLearningScoreType\n        \"\"\"\n        if self._configuration.client_side_validation and score_type is None:\n            raise ValueError(\"Invalid value for `score_type`, must not be `None`\")  # noqa: E501\n\n        self._score_type = score_type",
  "def scores(self):\n        \"\"\"Gets the scores of this ActiveLearningScoreCreateRequest.  # noqa: E501\n\n\n        :return: The scores of this ActiveLearningScoreCreateRequest.  # noqa: E501\n        :rtype: ActiveLearningScores\n        \"\"\"\n        return self._scores",
  "def scores(self, scores):\n        \"\"\"Sets the scores of this ActiveLearningScoreCreateRequest.\n\n\n        :param scores: The scores of this ActiveLearningScoreCreateRequest.  # noqa: E501\n        :type: ActiveLearningScores\n        \"\"\"\n        if self._configuration.client_side_validation and scores is None:\n            raise ValueError(\"Invalid value for `scores`, must not be `None`\")  # noqa: E501\n\n        self._scores = scores",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(ActiveLearningScoreCreateRequest, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, ActiveLearningScoreCreateRequest):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, ActiveLearningScoreCreateRequest):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class SamplingConfig(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'stopping_condition': 'SamplingConfigStoppingCondition'\n    }\n\n    attribute_map = {\n        'stopping_condition': 'stoppingCondition'\n    }\n\n    def __init__(self, stopping_condition=None, _configuration=None):  # noqa: E501\n        \"\"\"SamplingConfig - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._stopping_condition = None\n        self.discriminator = None\n\n        if stopping_condition is not None:\n            self.stopping_condition = stopping_condition\n\n    @property\n    def stopping_condition(self):\n        \"\"\"Gets the stopping_condition of this SamplingConfig.  # noqa: E501\n\n\n        :return: The stopping_condition of this SamplingConfig.  # noqa: E501\n        :rtype: SamplingConfigStoppingCondition\n        \"\"\"\n        return self._stopping_condition\n\n    @stopping_condition.setter\n    def stopping_condition(self, stopping_condition):\n        \"\"\"Sets the stopping_condition of this SamplingConfig.\n\n\n        :param stopping_condition: The stopping_condition of this SamplingConfig.  # noqa: E501\n        :type: SamplingConfigStoppingCondition\n        \"\"\"\n\n        self._stopping_condition = stopping_condition\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(SamplingConfig, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, SamplingConfig):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, SamplingConfig):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, stopping_condition=None, _configuration=None):  # noqa: E501\n        \"\"\"SamplingConfig - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._stopping_condition = None\n        self.discriminator = None\n\n        if stopping_condition is not None:\n            self.stopping_condition = stopping_condition",
  "def stopping_condition(self):\n        \"\"\"Gets the stopping_condition of this SamplingConfig.  # noqa: E501\n\n\n        :return: The stopping_condition of this SamplingConfig.  # noqa: E501\n        :rtype: SamplingConfigStoppingCondition\n        \"\"\"\n        return self._stopping_condition",
  "def stopping_condition(self, stopping_condition):\n        \"\"\"Sets the stopping_condition of this SamplingConfig.\n\n\n        :param stopping_condition: The stopping_condition of this SamplingConfig.  # noqa: E501\n        :type: SamplingConfigStoppingCondition\n        \"\"\"\n\n        self._stopping_condition = stopping_condition",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(SamplingConfig, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, SamplingConfig):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, SamplingConfig):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class JobResultType(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    allowed enum values\n    \"\"\"\n    DATASET_PROCESSING = \"DATASET_PROCESSING\"\n    SAMPLING = \"SAMPLING\"\n    IMAGEMETA = \"IMAGEMETA\"\n    EMBEDDINGS2D = \"EMBEDDINGS2D\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n    }\n\n    attribute_map = {\n    }\n\n    def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"JobResultType - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(JobResultType, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, JobResultType):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, JobResultType):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"JobResultType - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(JobResultType, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, JobResultType):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, JobResultType):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class Embedding2dCoordinates(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n    }\n\n    attribute_map = {\n    }\n\n    def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"Embedding2dCoordinates - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(Embedding2dCoordinates, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, Embedding2dCoordinates):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, Embedding2dCoordinates):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"Embedding2dCoordinates - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(Embedding2dCoordinates, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, Embedding2dCoordinates):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, Embedding2dCoordinates):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class Body(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'row_count': 'float'\n    }\n\n    attribute_map = {\n        'row_count': 'rowCount'\n    }\n\n    def __init__(self, row_count=None, _configuration=None):  # noqa: E501\n        \"\"\"Body - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._row_count = None\n        self.discriminator = None\n\n        self.row_count = row_count\n\n    @property\n    def row_count(self):\n        \"\"\"Gets the row_count of this Body.  # noqa: E501\n\n        Number of rows in the embeddings file  # noqa: E501\n\n        :return: The row_count of this Body.  # noqa: E501\n        :rtype: float\n        \"\"\"\n        return self._row_count\n\n    @row_count.setter\n    def row_count(self, row_count):\n        \"\"\"Sets the row_count of this Body.\n\n        Number of rows in the embeddings file  # noqa: E501\n\n        :param row_count: The row_count of this Body.  # noqa: E501\n        :type: float\n        \"\"\"\n        if self._configuration.client_side_validation and row_count is None:\n            raise ValueError(\"Invalid value for `row_count`, must not be `None`\")  # noqa: E501\n\n        self._row_count = row_count\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(Body, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, Body):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, Body):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, row_count=None, _configuration=None):  # noqa: E501\n        \"\"\"Body - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._row_count = None\n        self.discriminator = None\n\n        self.row_count = row_count",
  "def row_count(self):\n        \"\"\"Gets the row_count of this Body.  # noqa: E501\n\n        Number of rows in the embeddings file  # noqa: E501\n\n        :return: The row_count of this Body.  # noqa: E501\n        :rtype: float\n        \"\"\"\n        return self._row_count",
  "def row_count(self, row_count):\n        \"\"\"Sets the row_count of this Body.\n\n        Number of rows in the embeddings file  # noqa: E501\n\n        :param row_count: The row_count of this Body.  # noqa: E501\n        :type: float\n        \"\"\"\n        if self._configuration.client_side_validation and row_count is None:\n            raise ValueError(\"Invalid value for `row_count`, must not be `None`\")  # noqa: E501\n\n        self._row_count = row_count",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(Body, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, Body):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, Body):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class ActiveLearningScoreType(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    allowed enum values\n    \"\"\"\n    PREDICTION_ENTROPY = \"prediction-entropy\"\n    PREDICTION_MARGIN = \"prediction-margin\"\n    OBJECT_FREQUENCY = \"object-frequency\"\n    BALD = \"BALD\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n    }\n\n    attribute_map = {\n    }\n\n    def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"ActiveLearningScoreType - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(ActiveLearningScoreType, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, ActiveLearningScoreType):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, ActiveLearningScoreType):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"ActiveLearningScoreType - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(ActiveLearningScoreType, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, ActiveLearningScoreType):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, ActiveLearningScoreType):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class DatasetData(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'id': 'MongoObjectID',\n        'name': 'DatasetName',\n        'type': 'DatasetType',\n        'img_type': 'ImageType',\n        'n_samples': 'int',\n        'size_in_bytes': 'int',\n        'created_at': 'Timestamp',\n        'last_modified_at': 'Timestamp'\n    }\n\n    attribute_map = {\n        'id': 'id',\n        'name': 'name',\n        'type': 'type',\n        'img_type': 'imgType',\n        'n_samples': 'nSamples',\n        'size_in_bytes': 'sizeInBytes',\n        'created_at': 'createdAt',\n        'last_modified_at': 'lastModifiedAt'\n    }\n\n    def __init__(self, id=None, name=None, type=None, img_type=None, n_samples=None, size_in_bytes=None, created_at=None, last_modified_at=None, _configuration=None):  # noqa: E501\n        \"\"\"DatasetData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._id = None\n        self._name = None\n        self._type = None\n        self._img_type = None\n        self._n_samples = None\n        self._size_in_bytes = None\n        self._created_at = None\n        self._last_modified_at = None\n        self.discriminator = None\n\n        self.id = id\n        self.name = name\n        self.type = type\n        if img_type is not None:\n            self.img_type = img_type\n        self.n_samples = n_samples\n        self.size_in_bytes = size_in_bytes\n        self.created_at = created_at\n        self.last_modified_at = last_modified_at\n\n    @property\n    def id(self):\n        \"\"\"Gets the id of this DatasetData.  # noqa: E501\n\n\n        :return: The id of this DatasetData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._id\n\n    @id.setter\n    def id(self, id):\n        \"\"\"Sets the id of this DatasetData.\n\n\n        :param id: The id of this DatasetData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and id is None:\n            raise ValueError(\"Invalid value for `id`, must not be `None`\")  # noqa: E501\n\n        self._id = id\n\n    @property\n    def name(self):\n        \"\"\"Gets the name of this DatasetData.  # noqa: E501\n\n\n        :return: The name of this DatasetData.  # noqa: E501\n        :rtype: DatasetName\n        \"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        \"\"\"Sets the name of this DatasetData.\n\n\n        :param name: The name of this DatasetData.  # noqa: E501\n        :type: DatasetName\n        \"\"\"\n        if self._configuration.client_side_validation and name is None:\n            raise ValueError(\"Invalid value for `name`, must not be `None`\")  # noqa: E501\n\n        self._name = name\n\n    @property\n    def type(self):\n        \"\"\"Gets the type of this DatasetData.  # noqa: E501\n\n\n        :return: The type of this DatasetData.  # noqa: E501\n        :rtype: DatasetType\n        \"\"\"\n        return self._type\n\n    @type.setter\n    def type(self, type):\n        \"\"\"Sets the type of this DatasetData.\n\n\n        :param type: The type of this DatasetData.  # noqa: E501\n        :type: DatasetType\n        \"\"\"\n        if self._configuration.client_side_validation and type is None:\n            raise ValueError(\"Invalid value for `type`, must not be `None`\")  # noqa: E501\n\n        self._type = type\n\n    @property\n    def img_type(self):\n        \"\"\"Gets the img_type of this DatasetData.  # noqa: E501\n\n\n        :return: The img_type of this DatasetData.  # noqa: E501\n        :rtype: ImageType\n        \"\"\"\n        return self._img_type\n\n    @img_type.setter\n    def img_type(self, img_type):\n        \"\"\"Sets the img_type of this DatasetData.\n\n\n        :param img_type: The img_type of this DatasetData.  # noqa: E501\n        :type: ImageType\n        \"\"\"\n\n        self._img_type = img_type\n\n    @property\n    def n_samples(self):\n        \"\"\"Gets the n_samples of this DatasetData.  # noqa: E501\n\n\n        :return: The n_samples of this DatasetData.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._n_samples\n\n    @n_samples.setter\n    def n_samples(self, n_samples):\n        \"\"\"Sets the n_samples of this DatasetData.\n\n\n        :param n_samples: The n_samples of this DatasetData.  # noqa: E501\n        :type: int\n        \"\"\"\n        if self._configuration.client_side_validation and n_samples is None:\n            raise ValueError(\"Invalid value for `n_samples`, must not be `None`\")  # noqa: E501\n\n        self._n_samples = n_samples\n\n    @property\n    def size_in_bytes(self):\n        \"\"\"Gets the size_in_bytes of this DatasetData.  # noqa: E501\n\n\n        :return: The size_in_bytes of this DatasetData.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._size_in_bytes\n\n    @size_in_bytes.setter\n    def size_in_bytes(self, size_in_bytes):\n        \"\"\"Sets the size_in_bytes of this DatasetData.\n\n\n        :param size_in_bytes: The size_in_bytes of this DatasetData.  # noqa: E501\n        :type: int\n        \"\"\"\n        if self._configuration.client_side_validation and size_in_bytes is None:\n            raise ValueError(\"Invalid value for `size_in_bytes`, must not be `None`\")  # noqa: E501\n\n        self._size_in_bytes = size_in_bytes\n\n    @property\n    def created_at(self):\n        \"\"\"Gets the created_at of this DatasetData.  # noqa: E501\n\n\n        :return: The created_at of this DatasetData.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._created_at\n\n    @created_at.setter\n    def created_at(self, created_at):\n        \"\"\"Sets the created_at of this DatasetData.\n\n\n        :param created_at: The created_at of this DatasetData.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n        if self._configuration.client_side_validation and created_at is None:\n            raise ValueError(\"Invalid value for `created_at`, must not be `None`\")  # noqa: E501\n\n        self._created_at = created_at\n\n    @property\n    def last_modified_at(self):\n        \"\"\"Gets the last_modified_at of this DatasetData.  # noqa: E501\n\n\n        :return: The last_modified_at of this DatasetData.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._last_modified_at\n\n    @last_modified_at.setter\n    def last_modified_at(self, last_modified_at):\n        \"\"\"Sets the last_modified_at of this DatasetData.\n\n\n        :param last_modified_at: The last_modified_at of this DatasetData.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n        if self._configuration.client_side_validation and last_modified_at is None:\n            raise ValueError(\"Invalid value for `last_modified_at`, must not be `None`\")  # noqa: E501\n\n        self._last_modified_at = last_modified_at\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(DatasetData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, DatasetData):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, DatasetData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, id=None, name=None, type=None, img_type=None, n_samples=None, size_in_bytes=None, created_at=None, last_modified_at=None, _configuration=None):  # noqa: E501\n        \"\"\"DatasetData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._id = None\n        self._name = None\n        self._type = None\n        self._img_type = None\n        self._n_samples = None\n        self._size_in_bytes = None\n        self._created_at = None\n        self._last_modified_at = None\n        self.discriminator = None\n\n        self.id = id\n        self.name = name\n        self.type = type\n        if img_type is not None:\n            self.img_type = img_type\n        self.n_samples = n_samples\n        self.size_in_bytes = size_in_bytes\n        self.created_at = created_at\n        self.last_modified_at = last_modified_at",
  "def id(self):\n        \"\"\"Gets the id of this DatasetData.  # noqa: E501\n\n\n        :return: The id of this DatasetData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._id",
  "def id(self, id):\n        \"\"\"Sets the id of this DatasetData.\n\n\n        :param id: The id of this DatasetData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and id is None:\n            raise ValueError(\"Invalid value for `id`, must not be `None`\")  # noqa: E501\n\n        self._id = id",
  "def name(self):\n        \"\"\"Gets the name of this DatasetData.  # noqa: E501\n\n\n        :return: The name of this DatasetData.  # noqa: E501\n        :rtype: DatasetName\n        \"\"\"\n        return self._name",
  "def name(self, name):\n        \"\"\"Sets the name of this DatasetData.\n\n\n        :param name: The name of this DatasetData.  # noqa: E501\n        :type: DatasetName\n        \"\"\"\n        if self._configuration.client_side_validation and name is None:\n            raise ValueError(\"Invalid value for `name`, must not be `None`\")  # noqa: E501\n\n        self._name = name",
  "def type(self):\n        \"\"\"Gets the type of this DatasetData.  # noqa: E501\n\n\n        :return: The type of this DatasetData.  # noqa: E501\n        :rtype: DatasetType\n        \"\"\"\n        return self._type",
  "def type(self, type):\n        \"\"\"Sets the type of this DatasetData.\n\n\n        :param type: The type of this DatasetData.  # noqa: E501\n        :type: DatasetType\n        \"\"\"\n        if self._configuration.client_side_validation and type is None:\n            raise ValueError(\"Invalid value for `type`, must not be `None`\")  # noqa: E501\n\n        self._type = type",
  "def img_type(self):\n        \"\"\"Gets the img_type of this DatasetData.  # noqa: E501\n\n\n        :return: The img_type of this DatasetData.  # noqa: E501\n        :rtype: ImageType\n        \"\"\"\n        return self._img_type",
  "def img_type(self, img_type):\n        \"\"\"Sets the img_type of this DatasetData.\n\n\n        :param img_type: The img_type of this DatasetData.  # noqa: E501\n        :type: ImageType\n        \"\"\"\n\n        self._img_type = img_type",
  "def n_samples(self):\n        \"\"\"Gets the n_samples of this DatasetData.  # noqa: E501\n\n\n        :return: The n_samples of this DatasetData.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._n_samples",
  "def n_samples(self, n_samples):\n        \"\"\"Sets the n_samples of this DatasetData.\n\n\n        :param n_samples: The n_samples of this DatasetData.  # noqa: E501\n        :type: int\n        \"\"\"\n        if self._configuration.client_side_validation and n_samples is None:\n            raise ValueError(\"Invalid value for `n_samples`, must not be `None`\")  # noqa: E501\n\n        self._n_samples = n_samples",
  "def size_in_bytes(self):\n        \"\"\"Gets the size_in_bytes of this DatasetData.  # noqa: E501\n\n\n        :return: The size_in_bytes of this DatasetData.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._size_in_bytes",
  "def size_in_bytes(self, size_in_bytes):\n        \"\"\"Sets the size_in_bytes of this DatasetData.\n\n\n        :param size_in_bytes: The size_in_bytes of this DatasetData.  # noqa: E501\n        :type: int\n        \"\"\"\n        if self._configuration.client_side_validation and size_in_bytes is None:\n            raise ValueError(\"Invalid value for `size_in_bytes`, must not be `None`\")  # noqa: E501\n\n        self._size_in_bytes = size_in_bytes",
  "def created_at(self):\n        \"\"\"Gets the created_at of this DatasetData.  # noqa: E501\n\n\n        :return: The created_at of this DatasetData.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._created_at",
  "def created_at(self, created_at):\n        \"\"\"Sets the created_at of this DatasetData.\n\n\n        :param created_at: The created_at of this DatasetData.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n        if self._configuration.client_side_validation and created_at is None:\n            raise ValueError(\"Invalid value for `created_at`, must not be `None`\")  # noqa: E501\n\n        self._created_at = created_at",
  "def last_modified_at(self):\n        \"\"\"Gets the last_modified_at of this DatasetData.  # noqa: E501\n\n\n        :return: The last_modified_at of this DatasetData.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._last_modified_at",
  "def last_modified_at(self, last_modified_at):\n        \"\"\"Sets the last_modified_at of this DatasetData.\n\n\n        :param last_modified_at: The last_modified_at of this DatasetData.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n        if self._configuration.client_side_validation and last_modified_at is None:\n            raise ValueError(\"Invalid value for `last_modified_at`, must not be `None`\")  # noqa: E501\n\n        self._last_modified_at = last_modified_at",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(DatasetData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, DatasetData):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, DatasetData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class GeneralJobResult(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n    }\n\n    attribute_map = {\n    }\n\n    def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"GeneralJobResult - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(GeneralJobResult, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, GeneralJobResult):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, GeneralJobResult):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"GeneralJobResult - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(GeneralJobResult, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, GeneralJobResult):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, GeneralJobResult):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class TagData(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'id': 'MongoObjectID',\n        'dataset_id': 'MongoObjectID',\n        'prev_tag_id': 'str',\n        'name': 'TagName',\n        'bit_mask_data': 'TagBitMaskData',\n        'tot_size': 'int',\n        'created_at': 'Timestamp',\n        'changes': 'TagChangeData'\n    }\n\n    attribute_map = {\n        'id': 'id',\n        'dataset_id': 'datasetId',\n        'prev_tag_id': 'prevTagId',\n        'name': 'name',\n        'bit_mask_data': 'bitMaskData',\n        'tot_size': 'totSize',\n        'created_at': 'createdAt',\n        'changes': 'changes'\n    }\n\n    def __init__(self, id=None, dataset_id=None, prev_tag_id=None, name=None, bit_mask_data=None, tot_size=None, created_at=None, changes=None, _configuration=None):  # noqa: E501\n        \"\"\"TagData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._id = None\n        self._dataset_id = None\n        self._prev_tag_id = None\n        self._name = None\n        self._bit_mask_data = None\n        self._tot_size = None\n        self._created_at = None\n        self._changes = None\n        self.discriminator = None\n\n        self.id = id\n        self.dataset_id = dataset_id\n        self.prev_tag_id = prev_tag_id\n        self.name = name\n        self.bit_mask_data = bit_mask_data\n        self.tot_size = tot_size\n        self.created_at = created_at\n        if changes is not None:\n            self.changes = changes\n\n    @property\n    def id(self):\n        \"\"\"Gets the id of this TagData.  # noqa: E501\n\n\n        :return: The id of this TagData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._id\n\n    @id.setter\n    def id(self, id):\n        \"\"\"Sets the id of this TagData.\n\n\n        :param id: The id of this TagData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and id is None:\n            raise ValueError(\"Invalid value for `id`, must not be `None`\")  # noqa: E501\n\n        self._id = id\n\n    @property\n    def dataset_id(self):\n        \"\"\"Gets the dataset_id of this TagData.  # noqa: E501\n\n\n        :return: The dataset_id of this TagData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._dataset_id\n\n    @dataset_id.setter\n    def dataset_id(self, dataset_id):\n        \"\"\"Sets the dataset_id of this TagData.\n\n\n        :param dataset_id: The dataset_id of this TagData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and dataset_id is None:\n            raise ValueError(\"Invalid value for `dataset_id`, must not be `None`\")  # noqa: E501\n\n        self._dataset_id = dataset_id\n\n    @property\n    def prev_tag_id(self):\n        \"\"\"Gets the prev_tag_id of this TagData.  # noqa: E501\n\n        MongoObjectID or null  # noqa: E501\n\n        :return: The prev_tag_id of this TagData.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._prev_tag_id\n\n    @prev_tag_id.setter\n    def prev_tag_id(self, prev_tag_id):\n        \"\"\"Sets the prev_tag_id of this TagData.\n\n        MongoObjectID or null  # noqa: E501\n\n        :param prev_tag_id: The prev_tag_id of this TagData.  # noqa: E501\n        :type: str\n        \"\"\"\n\n        self._prev_tag_id = prev_tag_id\n\n    @property\n    def name(self):\n        \"\"\"Gets the name of this TagData.  # noqa: E501\n\n\n        :return: The name of this TagData.  # noqa: E501\n        :rtype: TagName\n        \"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        \"\"\"Sets the name of this TagData.\n\n\n        :param name: The name of this TagData.  # noqa: E501\n        :type: TagName\n        \"\"\"\n        if self._configuration.client_side_validation and name is None:\n            raise ValueError(\"Invalid value for `name`, must not be `None`\")  # noqa: E501\n\n        self._name = name\n\n    @property\n    def bit_mask_data(self):\n        \"\"\"Gets the bit_mask_data of this TagData.  # noqa: E501\n\n\n        :return: The bit_mask_data of this TagData.  # noqa: E501\n        :rtype: TagBitMaskData\n        \"\"\"\n        return self._bit_mask_data\n\n    @bit_mask_data.setter\n    def bit_mask_data(self, bit_mask_data):\n        \"\"\"Sets the bit_mask_data of this TagData.\n\n\n        :param bit_mask_data: The bit_mask_data of this TagData.  # noqa: E501\n        :type: TagBitMaskData\n        \"\"\"\n        if self._configuration.client_side_validation and bit_mask_data is None:\n            raise ValueError(\"Invalid value for `bit_mask_data`, must not be `None`\")  # noqa: E501\n\n        self._bit_mask_data = bit_mask_data\n\n    @property\n    def tot_size(self):\n        \"\"\"Gets the tot_size of this TagData.  # noqa: E501\n\n\n        :return: The tot_size of this TagData.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._tot_size\n\n    @tot_size.setter\n    def tot_size(self, tot_size):\n        \"\"\"Sets the tot_size of this TagData.\n\n\n        :param tot_size: The tot_size of this TagData.  # noqa: E501\n        :type: int\n        \"\"\"\n        if self._configuration.client_side_validation and tot_size is None:\n            raise ValueError(\"Invalid value for `tot_size`, must not be `None`\")  # noqa: E501\n\n        self._tot_size = tot_size\n\n    @property\n    def created_at(self):\n        \"\"\"Gets the created_at of this TagData.  # noqa: E501\n\n\n        :return: The created_at of this TagData.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._created_at\n\n    @created_at.setter\n    def created_at(self, created_at):\n        \"\"\"Sets the created_at of this TagData.\n\n\n        :param created_at: The created_at of this TagData.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n        if self._configuration.client_side_validation and created_at is None:\n            raise ValueError(\"Invalid value for `created_at`, must not be `None`\")  # noqa: E501\n\n        self._created_at = created_at\n\n    @property\n    def changes(self):\n        \"\"\"Gets the changes of this TagData.  # noqa: E501\n\n\n        :return: The changes of this TagData.  # noqa: E501\n        :rtype: TagChangeData\n        \"\"\"\n        return self._changes\n\n    @changes.setter\n    def changes(self, changes):\n        \"\"\"Sets the changes of this TagData.\n\n\n        :param changes: The changes of this TagData.  # noqa: E501\n        :type: TagChangeData\n        \"\"\"\n\n        self._changes = changes\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(TagData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, TagData):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, TagData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, id=None, dataset_id=None, prev_tag_id=None, name=None, bit_mask_data=None, tot_size=None, created_at=None, changes=None, _configuration=None):  # noqa: E501\n        \"\"\"TagData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._id = None\n        self._dataset_id = None\n        self._prev_tag_id = None\n        self._name = None\n        self._bit_mask_data = None\n        self._tot_size = None\n        self._created_at = None\n        self._changes = None\n        self.discriminator = None\n\n        self.id = id\n        self.dataset_id = dataset_id\n        self.prev_tag_id = prev_tag_id\n        self.name = name\n        self.bit_mask_data = bit_mask_data\n        self.tot_size = tot_size\n        self.created_at = created_at\n        if changes is not None:\n            self.changes = changes",
  "def id(self):\n        \"\"\"Gets the id of this TagData.  # noqa: E501\n\n\n        :return: The id of this TagData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._id",
  "def id(self, id):\n        \"\"\"Sets the id of this TagData.\n\n\n        :param id: The id of this TagData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and id is None:\n            raise ValueError(\"Invalid value for `id`, must not be `None`\")  # noqa: E501\n\n        self._id = id",
  "def dataset_id(self):\n        \"\"\"Gets the dataset_id of this TagData.  # noqa: E501\n\n\n        :return: The dataset_id of this TagData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._dataset_id",
  "def dataset_id(self, dataset_id):\n        \"\"\"Sets the dataset_id of this TagData.\n\n\n        :param dataset_id: The dataset_id of this TagData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and dataset_id is None:\n            raise ValueError(\"Invalid value for `dataset_id`, must not be `None`\")  # noqa: E501\n\n        self._dataset_id = dataset_id",
  "def prev_tag_id(self):\n        \"\"\"Gets the prev_tag_id of this TagData.  # noqa: E501\n\n        MongoObjectID or null  # noqa: E501\n\n        :return: The prev_tag_id of this TagData.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._prev_tag_id",
  "def prev_tag_id(self, prev_tag_id):\n        \"\"\"Sets the prev_tag_id of this TagData.\n\n        MongoObjectID or null  # noqa: E501\n\n        :param prev_tag_id: The prev_tag_id of this TagData.  # noqa: E501\n        :type: str\n        \"\"\"\n\n        self._prev_tag_id = prev_tag_id",
  "def name(self):\n        \"\"\"Gets the name of this TagData.  # noqa: E501\n\n\n        :return: The name of this TagData.  # noqa: E501\n        :rtype: TagName\n        \"\"\"\n        return self._name",
  "def name(self, name):\n        \"\"\"Sets the name of this TagData.\n\n\n        :param name: The name of this TagData.  # noqa: E501\n        :type: TagName\n        \"\"\"\n        if self._configuration.client_side_validation and name is None:\n            raise ValueError(\"Invalid value for `name`, must not be `None`\")  # noqa: E501\n\n        self._name = name",
  "def bit_mask_data(self):\n        \"\"\"Gets the bit_mask_data of this TagData.  # noqa: E501\n\n\n        :return: The bit_mask_data of this TagData.  # noqa: E501\n        :rtype: TagBitMaskData\n        \"\"\"\n        return self._bit_mask_data",
  "def bit_mask_data(self, bit_mask_data):\n        \"\"\"Sets the bit_mask_data of this TagData.\n\n\n        :param bit_mask_data: The bit_mask_data of this TagData.  # noqa: E501\n        :type: TagBitMaskData\n        \"\"\"\n        if self._configuration.client_side_validation and bit_mask_data is None:\n            raise ValueError(\"Invalid value for `bit_mask_data`, must not be `None`\")  # noqa: E501\n\n        self._bit_mask_data = bit_mask_data",
  "def tot_size(self):\n        \"\"\"Gets the tot_size of this TagData.  # noqa: E501\n\n\n        :return: The tot_size of this TagData.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._tot_size",
  "def tot_size(self, tot_size):\n        \"\"\"Sets the tot_size of this TagData.\n\n\n        :param tot_size: The tot_size of this TagData.  # noqa: E501\n        :type: int\n        \"\"\"\n        if self._configuration.client_side_validation and tot_size is None:\n            raise ValueError(\"Invalid value for `tot_size`, must not be `None`\")  # noqa: E501\n\n        self._tot_size = tot_size",
  "def created_at(self):\n        \"\"\"Gets the created_at of this TagData.  # noqa: E501\n\n\n        :return: The created_at of this TagData.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._created_at",
  "def created_at(self, created_at):\n        \"\"\"Sets the created_at of this TagData.\n\n\n        :param created_at: The created_at of this TagData.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n        if self._configuration.client_side_validation and created_at is None:\n            raise ValueError(\"Invalid value for `created_at`, must not be `None`\")  # noqa: E501\n\n        self._created_at = created_at",
  "def changes(self):\n        \"\"\"Gets the changes of this TagData.  # noqa: E501\n\n\n        :return: The changes of this TagData.  # noqa: E501\n        :rtype: TagChangeData\n        \"\"\"\n        return self._changes",
  "def changes(self, changes):\n        \"\"\"Sets the changes of this TagData.\n\n\n        :param changes: The changes of this TagData.  # noqa: E501\n        :type: TagChangeData\n        \"\"\"\n\n        self._changes = changes",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(TagData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, TagData):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, TagData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class TagBitMaskData(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n    }\n\n    attribute_map = {\n    }\n\n    def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"TagBitMaskData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(TagBitMaskData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, TagBitMaskData):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, TagBitMaskData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"TagBitMaskData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(TagBitMaskData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, TagBitMaskData):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, TagBitMaskData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class TagFilenamesData(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n    }\n\n    attribute_map = {\n    }\n\n    def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"TagFilenamesData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(TagFilenamesData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, TagFilenamesData):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, TagFilenamesData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"TagFilenamesData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(TagFilenamesData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, TagFilenamesData):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, TagFilenamesData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class Timestamp(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n    }\n\n    attribute_map = {\n    }\n\n    def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"Timestamp - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(Timestamp, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, Timestamp):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, Timestamp):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"Timestamp - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(Timestamp, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, Timestamp):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, Timestamp):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class SampleCreateRequest(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'file_name': 'str',\n        'thumb_name': 'str',\n        'exif': 'dict(str, object)',\n        'meta_data': 'SampleMetaData'\n    }\n\n    attribute_map = {\n        'file_name': 'fileName',\n        'thumb_name': 'thumbName',\n        'exif': 'exif',\n        'meta_data': 'metaData'\n    }\n\n    def __init__(self, file_name=None, thumb_name=None, exif=None, meta_data=None, _configuration=None):  # noqa: E501\n        \"\"\"SampleCreateRequest - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._file_name = None\n        self._thumb_name = None\n        self._exif = None\n        self._meta_data = None\n        self.discriminator = None\n\n        self.file_name = file_name\n        if thumb_name is not None:\n            self.thumb_name = thumb_name\n        if exif is not None:\n            self.exif = exif\n        if meta_data is not None:\n            self.meta_data = meta_data\n\n    @property\n    def file_name(self):\n        \"\"\"Gets the file_name of this SampleCreateRequest.  # noqa: E501\n\n\n        :return: The file_name of this SampleCreateRequest.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._file_name\n\n    @file_name.setter\n    def file_name(self, file_name):\n        \"\"\"Sets the file_name of this SampleCreateRequest.\n\n\n        :param file_name: The file_name of this SampleCreateRequest.  # noqa: E501\n        :type: str\n        \"\"\"\n        if self._configuration.client_side_validation and file_name is None:\n            raise ValueError(\"Invalid value for `file_name`, must not be `None`\")  # noqa: E501\n\n        self._file_name = file_name\n\n    @property\n    def thumb_name(self):\n        \"\"\"Gets the thumb_name of this SampleCreateRequest.  # noqa: E501\n\n\n        :return: The thumb_name of this SampleCreateRequest.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._thumb_name\n\n    @thumb_name.setter\n    def thumb_name(self, thumb_name):\n        \"\"\"Sets the thumb_name of this SampleCreateRequest.\n\n\n        :param thumb_name: The thumb_name of this SampleCreateRequest.  # noqa: E501\n        :type: str\n        \"\"\"\n\n        self._thumb_name = thumb_name\n\n    @property\n    def exif(self):\n        \"\"\"Gets the exif of this SampleCreateRequest.  # noqa: E501\n\n\n        :return: The exif of this SampleCreateRequest.  # noqa: E501\n        :rtype: dict(str, object)\n        \"\"\"\n        return self._exif\n\n    @exif.setter\n    def exif(self, exif):\n        \"\"\"Sets the exif of this SampleCreateRequest.\n\n\n        :param exif: The exif of this SampleCreateRequest.  # noqa: E501\n        :type: dict(str, object)\n        \"\"\"\n\n        self._exif = exif\n\n    @property\n    def meta_data(self):\n        \"\"\"Gets the meta_data of this SampleCreateRequest.  # noqa: E501\n\n\n        :return: The meta_data of this SampleCreateRequest.  # noqa: E501\n        :rtype: SampleMetaData\n        \"\"\"\n        return self._meta_data\n\n    @meta_data.setter\n    def meta_data(self, meta_data):\n        \"\"\"Sets the meta_data of this SampleCreateRequest.\n\n\n        :param meta_data: The meta_data of this SampleCreateRequest.  # noqa: E501\n        :type: SampleMetaData\n        \"\"\"\n\n        self._meta_data = meta_data\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(SampleCreateRequest, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, SampleCreateRequest):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, SampleCreateRequest):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, file_name=None, thumb_name=None, exif=None, meta_data=None, _configuration=None):  # noqa: E501\n        \"\"\"SampleCreateRequest - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._file_name = None\n        self._thumb_name = None\n        self._exif = None\n        self._meta_data = None\n        self.discriminator = None\n\n        self.file_name = file_name\n        if thumb_name is not None:\n            self.thumb_name = thumb_name\n        if exif is not None:\n            self.exif = exif\n        if meta_data is not None:\n            self.meta_data = meta_data",
  "def file_name(self):\n        \"\"\"Gets the file_name of this SampleCreateRequest.  # noqa: E501\n\n\n        :return: The file_name of this SampleCreateRequest.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._file_name",
  "def file_name(self, file_name):\n        \"\"\"Sets the file_name of this SampleCreateRequest.\n\n\n        :param file_name: The file_name of this SampleCreateRequest.  # noqa: E501\n        :type: str\n        \"\"\"\n        if self._configuration.client_side_validation and file_name is None:\n            raise ValueError(\"Invalid value for `file_name`, must not be `None`\")  # noqa: E501\n\n        self._file_name = file_name",
  "def thumb_name(self):\n        \"\"\"Gets the thumb_name of this SampleCreateRequest.  # noqa: E501\n\n\n        :return: The thumb_name of this SampleCreateRequest.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._thumb_name",
  "def thumb_name(self, thumb_name):\n        \"\"\"Sets the thumb_name of this SampleCreateRequest.\n\n\n        :param thumb_name: The thumb_name of this SampleCreateRequest.  # noqa: E501\n        :type: str\n        \"\"\"\n\n        self._thumb_name = thumb_name",
  "def exif(self):\n        \"\"\"Gets the exif of this SampleCreateRequest.  # noqa: E501\n\n\n        :return: The exif of this SampleCreateRequest.  # noqa: E501\n        :rtype: dict(str, object)\n        \"\"\"\n        return self._exif",
  "def exif(self, exif):\n        \"\"\"Sets the exif of this SampleCreateRequest.\n\n\n        :param exif: The exif of this SampleCreateRequest.  # noqa: E501\n        :type: dict(str, object)\n        \"\"\"\n\n        self._exif = exif",
  "def meta_data(self):\n        \"\"\"Gets the meta_data of this SampleCreateRequest.  # noqa: E501\n\n\n        :return: The meta_data of this SampleCreateRequest.  # noqa: E501\n        :rtype: SampleMetaData\n        \"\"\"\n        return self._meta_data",
  "def meta_data(self, meta_data):\n        \"\"\"Sets the meta_data of this SampleCreateRequest.\n\n\n        :param meta_data: The meta_data of this SampleCreateRequest.  # noqa: E501\n        :type: SampleMetaData\n        \"\"\"\n\n        self._meta_data = meta_data",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(SampleCreateRequest, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, SampleCreateRequest):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, SampleCreateRequest):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class SampleUpdateRequest(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'file_name': 'str',\n        'thumb_name': 'str',\n        'exif': 'dict(str, object)',\n        'meta_data': 'SampleMetaData'\n    }\n\n    attribute_map = {\n        'file_name': 'fileName',\n        'thumb_name': 'thumbName',\n        'exif': 'exif',\n        'meta_data': 'metaData'\n    }\n\n    def __init__(self, file_name=None, thumb_name=None, exif=None, meta_data=None, _configuration=None):  # noqa: E501\n        \"\"\"SampleUpdateRequest - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._file_name = None\n        self._thumb_name = None\n        self._exif = None\n        self._meta_data = None\n        self.discriminator = None\n\n        self.file_name = file_name\n        self.thumb_name = thumb_name\n        if exif is not None:\n            self.exif = exif\n        self.meta_data = meta_data\n\n    @property\n    def file_name(self):\n        \"\"\"Gets the file_name of this SampleUpdateRequest.  # noqa: E501\n\n\n        :return: The file_name of this SampleUpdateRequest.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._file_name\n\n    @file_name.setter\n    def file_name(self, file_name):\n        \"\"\"Sets the file_name of this SampleUpdateRequest.\n\n\n        :param file_name: The file_name of this SampleUpdateRequest.  # noqa: E501\n        :type: str\n        \"\"\"\n        if self._configuration.client_side_validation and file_name is None:\n            raise ValueError(\"Invalid value for `file_name`, must not be `None`\")  # noqa: E501\n\n        self._file_name = file_name\n\n    @property\n    def thumb_name(self):\n        \"\"\"Gets the thumb_name of this SampleUpdateRequest.  # noqa: E501\n\n\n        :return: The thumb_name of this SampleUpdateRequest.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._thumb_name\n\n    @thumb_name.setter\n    def thumb_name(self, thumb_name):\n        \"\"\"Sets the thumb_name of this SampleUpdateRequest.\n\n\n        :param thumb_name: The thumb_name of this SampleUpdateRequest.  # noqa: E501\n        :type: str\n        \"\"\"\n        if self._configuration.client_side_validation and thumb_name is None:\n            raise ValueError(\"Invalid value for `thumb_name`, must not be `None`\")  # noqa: E501\n\n        self._thumb_name = thumb_name\n\n    @property\n    def exif(self):\n        \"\"\"Gets the exif of this SampleUpdateRequest.  # noqa: E501\n\n\n        :return: The exif of this SampleUpdateRequest.  # noqa: E501\n        :rtype: dict(str, object)\n        \"\"\"\n        return self._exif\n\n    @exif.setter\n    def exif(self, exif):\n        \"\"\"Sets the exif of this SampleUpdateRequest.\n\n\n        :param exif: The exif of this SampleUpdateRequest.  # noqa: E501\n        :type: dict(str, object)\n        \"\"\"\n\n        self._exif = exif\n\n    @property\n    def meta_data(self):\n        \"\"\"Gets the meta_data of this SampleUpdateRequest.  # noqa: E501\n\n\n        :return: The meta_data of this SampleUpdateRequest.  # noqa: E501\n        :rtype: SampleMetaData\n        \"\"\"\n        return self._meta_data\n\n    @meta_data.setter\n    def meta_data(self, meta_data):\n        \"\"\"Sets the meta_data of this SampleUpdateRequest.\n\n\n        :param meta_data: The meta_data of this SampleUpdateRequest.  # noqa: E501\n        :type: SampleMetaData\n        \"\"\"\n        if self._configuration.client_side_validation and meta_data is None:\n            raise ValueError(\"Invalid value for `meta_data`, must not be `None`\")  # noqa: E501\n\n        self._meta_data = meta_data\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(SampleUpdateRequest, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, SampleUpdateRequest):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, SampleUpdateRequest):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, file_name=None, thumb_name=None, exif=None, meta_data=None, _configuration=None):  # noqa: E501\n        \"\"\"SampleUpdateRequest - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._file_name = None\n        self._thumb_name = None\n        self._exif = None\n        self._meta_data = None\n        self.discriminator = None\n\n        self.file_name = file_name\n        self.thumb_name = thumb_name\n        if exif is not None:\n            self.exif = exif\n        self.meta_data = meta_data",
  "def file_name(self):\n        \"\"\"Gets the file_name of this SampleUpdateRequest.  # noqa: E501\n\n\n        :return: The file_name of this SampleUpdateRequest.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._file_name",
  "def file_name(self, file_name):\n        \"\"\"Sets the file_name of this SampleUpdateRequest.\n\n\n        :param file_name: The file_name of this SampleUpdateRequest.  # noqa: E501\n        :type: str\n        \"\"\"\n        if self._configuration.client_side_validation and file_name is None:\n            raise ValueError(\"Invalid value for `file_name`, must not be `None`\")  # noqa: E501\n\n        self._file_name = file_name",
  "def thumb_name(self):\n        \"\"\"Gets the thumb_name of this SampleUpdateRequest.  # noqa: E501\n\n\n        :return: The thumb_name of this SampleUpdateRequest.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._thumb_name",
  "def thumb_name(self, thumb_name):\n        \"\"\"Sets the thumb_name of this SampleUpdateRequest.\n\n\n        :param thumb_name: The thumb_name of this SampleUpdateRequest.  # noqa: E501\n        :type: str\n        \"\"\"\n        if self._configuration.client_side_validation and thumb_name is None:\n            raise ValueError(\"Invalid value for `thumb_name`, must not be `None`\")  # noqa: E501\n\n        self._thumb_name = thumb_name",
  "def exif(self):\n        \"\"\"Gets the exif of this SampleUpdateRequest.  # noqa: E501\n\n\n        :return: The exif of this SampleUpdateRequest.  # noqa: E501\n        :rtype: dict(str, object)\n        \"\"\"\n        return self._exif",
  "def exif(self, exif):\n        \"\"\"Sets the exif of this SampleUpdateRequest.\n\n\n        :param exif: The exif of this SampleUpdateRequest.  # noqa: E501\n        :type: dict(str, object)\n        \"\"\"\n\n        self._exif = exif",
  "def meta_data(self):\n        \"\"\"Gets the meta_data of this SampleUpdateRequest.  # noqa: E501\n\n\n        :return: The meta_data of this SampleUpdateRequest.  # noqa: E501\n        :rtype: SampleMetaData\n        \"\"\"\n        return self._meta_data",
  "def meta_data(self, meta_data):\n        \"\"\"Sets the meta_data of this SampleUpdateRequest.\n\n\n        :param meta_data: The meta_data of this SampleUpdateRequest.  # noqa: E501\n        :type: SampleMetaData\n        \"\"\"\n        if self._configuration.client_side_validation and meta_data is None:\n            raise ValueError(\"Invalid value for `meta_data`, must not be `None`\")  # noqa: E501\n\n        self._meta_data = meta_data",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(SampleUpdateRequest, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, SampleUpdateRequest):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, SampleUpdateRequest):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class ObjectId(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n    }\n\n    attribute_map = {\n    }\n\n    def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"ObjectId - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(ObjectId, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, ObjectId):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, ObjectId):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"ObjectId - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(ObjectId, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, ObjectId):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, ObjectId):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class TagChangeData(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n    }\n\n    attribute_map = {\n    }\n\n    def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"TagChangeData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(TagChangeData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, TagChangeData):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, TagChangeData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"TagChangeData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(TagChangeData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, TagChangeData):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, TagChangeData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class ActiveLearningScores(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n    }\n\n    attribute_map = {\n    }\n\n    def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"ActiveLearningScores - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(ActiveLearningScores, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, ActiveLearningScores):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, ActiveLearningScores):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"ActiveLearningScores - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(ActiveLearningScores, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, ActiveLearningScores):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, ActiveLearningScores):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class DatasetName(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n    }\n\n    attribute_map = {\n    }\n\n    def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"DatasetName - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(DatasetName, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, DatasetName):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, DatasetName):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"DatasetName - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(DatasetName, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, DatasetName):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, DatasetName):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class DatasetEmbeddingData(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'id': 'MongoObjectID',\n        'name': 'str',\n        'is_processed': 'bool',\n        'created_at': 'Timestamp',\n        'is2d': 'bool'\n    }\n\n    attribute_map = {\n        'id': 'id',\n        'name': 'name',\n        'is_processed': 'isProcessed',\n        'created_at': 'createdAt',\n        'is2d': 'is2d'\n    }\n\n    def __init__(self, id=None, name=None, is_processed=None, created_at=None, is2d=None, _configuration=None):  # noqa: E501\n        \"\"\"DatasetEmbeddingData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._id = None\n        self._name = None\n        self._is_processed = None\n        self._created_at = None\n        self._is2d = None\n        self.discriminator = None\n\n        self.id = id\n        self.name = name\n        self.is_processed = is_processed\n        self.created_at = created_at\n        if is2d is not None:\n            self.is2d = is2d\n\n    @property\n    def id(self):\n        \"\"\"Gets the id of this DatasetEmbeddingData.  # noqa: E501\n\n\n        :return: The id of this DatasetEmbeddingData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._id\n\n    @id.setter\n    def id(self, id):\n        \"\"\"Sets the id of this DatasetEmbeddingData.\n\n\n        :param id: The id of this DatasetEmbeddingData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and id is None:\n            raise ValueError(\"Invalid value for `id`, must not be `None`\")  # noqa: E501\n\n        self._id = id\n\n    @property\n    def name(self):\n        \"\"\"Gets the name of this DatasetEmbeddingData.  # noqa: E501\n\n        name of the embedding chosen by the user calling writeCSVUrl  # noqa: E501\n\n        :return: The name of this DatasetEmbeddingData.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        \"\"\"Sets the name of this DatasetEmbeddingData.\n\n        name of the embedding chosen by the user calling writeCSVUrl  # noqa: E501\n\n        :param name: The name of this DatasetEmbeddingData.  # noqa: E501\n        :type: str\n        \"\"\"\n        if self._configuration.client_side_validation and name is None:\n            raise ValueError(\"Invalid value for `name`, must not be `None`\")  # noqa: E501\n\n        self._name = name\n\n    @property\n    def is_processed(self):\n        \"\"\"Gets the is_processed of this DatasetEmbeddingData.  # noqa: E501\n\n        indicator whether embeddings have already been processed by a background worker  # noqa: E501\n\n        :return: The is_processed of this DatasetEmbeddingData.  # noqa: E501\n        :rtype: bool\n        \"\"\"\n        return self._is_processed\n\n    @is_processed.setter\n    def is_processed(self, is_processed):\n        \"\"\"Sets the is_processed of this DatasetEmbeddingData.\n\n        indicator whether embeddings have already been processed by a background worker  # noqa: E501\n\n        :param is_processed: The is_processed of this DatasetEmbeddingData.  # noqa: E501\n        :type: bool\n        \"\"\"\n        if self._configuration.client_side_validation and is_processed is None:\n            raise ValueError(\"Invalid value for `is_processed`, must not be `None`\")  # noqa: E501\n\n        self._is_processed = is_processed\n\n    @property\n    def created_at(self):\n        \"\"\"Gets the created_at of this DatasetEmbeddingData.  # noqa: E501\n\n\n        :return: The created_at of this DatasetEmbeddingData.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._created_at\n\n    @created_at.setter\n    def created_at(self, created_at):\n        \"\"\"Sets the created_at of this DatasetEmbeddingData.\n\n\n        :param created_at: The created_at of this DatasetEmbeddingData.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n        if self._configuration.client_side_validation and created_at is None:\n            raise ValueError(\"Invalid value for `created_at`, must not be `None`\")  # noqa: E501\n\n        self._created_at = created_at\n\n    @property\n    def is2d(self):\n        \"\"\"Gets the is2d of this DatasetEmbeddingData.  # noqa: E501\n\n        flag set by the background worker if the embedding is 2d  # noqa: E501\n\n        :return: The is2d of this DatasetEmbeddingData.  # noqa: E501\n        :rtype: bool\n        \"\"\"\n        return self._is2d\n\n    @is2d.setter\n    def is2d(self, is2d):\n        \"\"\"Sets the is2d of this DatasetEmbeddingData.\n\n        flag set by the background worker if the embedding is 2d  # noqa: E501\n\n        :param is2d: The is2d of this DatasetEmbeddingData.  # noqa: E501\n        :type: bool\n        \"\"\"\n\n        self._is2d = is2d\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(DatasetEmbeddingData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, DatasetEmbeddingData):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, DatasetEmbeddingData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, id=None, name=None, is_processed=None, created_at=None, is2d=None, _configuration=None):  # noqa: E501\n        \"\"\"DatasetEmbeddingData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._id = None\n        self._name = None\n        self._is_processed = None\n        self._created_at = None\n        self._is2d = None\n        self.discriminator = None\n\n        self.id = id\n        self.name = name\n        self.is_processed = is_processed\n        self.created_at = created_at\n        if is2d is not None:\n            self.is2d = is2d",
  "def id(self):\n        \"\"\"Gets the id of this DatasetEmbeddingData.  # noqa: E501\n\n\n        :return: The id of this DatasetEmbeddingData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._id",
  "def id(self, id):\n        \"\"\"Sets the id of this DatasetEmbeddingData.\n\n\n        :param id: The id of this DatasetEmbeddingData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and id is None:\n            raise ValueError(\"Invalid value for `id`, must not be `None`\")  # noqa: E501\n\n        self._id = id",
  "def name(self):\n        \"\"\"Gets the name of this DatasetEmbeddingData.  # noqa: E501\n\n        name of the embedding chosen by the user calling writeCSVUrl  # noqa: E501\n\n        :return: The name of this DatasetEmbeddingData.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._name",
  "def name(self, name):\n        \"\"\"Sets the name of this DatasetEmbeddingData.\n\n        name of the embedding chosen by the user calling writeCSVUrl  # noqa: E501\n\n        :param name: The name of this DatasetEmbeddingData.  # noqa: E501\n        :type: str\n        \"\"\"\n        if self._configuration.client_side_validation and name is None:\n            raise ValueError(\"Invalid value for `name`, must not be `None`\")  # noqa: E501\n\n        self._name = name",
  "def is_processed(self):\n        \"\"\"Gets the is_processed of this DatasetEmbeddingData.  # noqa: E501\n\n        indicator whether embeddings have already been processed by a background worker  # noqa: E501\n\n        :return: The is_processed of this DatasetEmbeddingData.  # noqa: E501\n        :rtype: bool\n        \"\"\"\n        return self._is_processed",
  "def is_processed(self, is_processed):\n        \"\"\"Sets the is_processed of this DatasetEmbeddingData.\n\n        indicator whether embeddings have already been processed by a background worker  # noqa: E501\n\n        :param is_processed: The is_processed of this DatasetEmbeddingData.  # noqa: E501\n        :type: bool\n        \"\"\"\n        if self._configuration.client_side_validation and is_processed is None:\n            raise ValueError(\"Invalid value for `is_processed`, must not be `None`\")  # noqa: E501\n\n        self._is_processed = is_processed",
  "def created_at(self):\n        \"\"\"Gets the created_at of this DatasetEmbeddingData.  # noqa: E501\n\n\n        :return: The created_at of this DatasetEmbeddingData.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._created_at",
  "def created_at(self, created_at):\n        \"\"\"Sets the created_at of this DatasetEmbeddingData.\n\n\n        :param created_at: The created_at of this DatasetEmbeddingData.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n        if self._configuration.client_side_validation and created_at is None:\n            raise ValueError(\"Invalid value for `created_at`, must not be `None`\")  # noqa: E501\n\n        self._created_at = created_at",
  "def is2d(self):\n        \"\"\"Gets the is2d of this DatasetEmbeddingData.  # noqa: E501\n\n        flag set by the background worker if the embedding is 2d  # noqa: E501\n\n        :return: The is2d of this DatasetEmbeddingData.  # noqa: E501\n        :rtype: bool\n        \"\"\"\n        return self._is2d",
  "def is2d(self, is2d):\n        \"\"\"Sets the is2d of this DatasetEmbeddingData.\n\n        flag set by the background worker if the embedding is 2d  # noqa: E501\n\n        :param is2d: The is2d of this DatasetEmbeddingData.  # noqa: E501\n        :type: bool\n        \"\"\"\n\n        self._is2d = is2d",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(DatasetEmbeddingData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, DatasetEmbeddingData):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, DatasetEmbeddingData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class SampleMetaData(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'custom': 'dict(str, object)',\n        'sharpness': 'float',\n        'size_in_bytes': 'int',\n        'snr': 'float',\n        'mean': 'list[float]',\n        'shape': 'list[int]',\n        'std': 'list[float]',\n        'sum_of_squares': 'list[float]',\n        'sum_of_values': 'list[float]'\n    }\n\n    attribute_map = {\n        'custom': 'custom',\n        'sharpness': 'sharpness',\n        'size_in_bytes': 'sizeInBytes',\n        'snr': 'snr',\n        'mean': 'mean',\n        'shape': 'shape',\n        'std': 'std',\n        'sum_of_squares': 'sumOfSquares',\n        'sum_of_values': 'sumOfValues'\n    }\n\n    def __init__(self, custom=None, sharpness=None, size_in_bytes=None, snr=None, mean=None, shape=None, std=None, sum_of_squares=None, sum_of_values=None, _configuration=None):  # noqa: E501\n        \"\"\"SampleMetaData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._custom = None\n        self._sharpness = None\n        self._size_in_bytes = None\n        self._snr = None\n        self._mean = None\n        self._shape = None\n        self._std = None\n        self._sum_of_squares = None\n        self._sum_of_values = None\n        self.discriminator = None\n\n        if custom is not None:\n            self.custom = custom\n        if sharpness is not None:\n            self.sharpness = sharpness\n        if size_in_bytes is not None:\n            self.size_in_bytes = size_in_bytes\n        if snr is not None:\n            self.snr = snr\n        if mean is not None:\n            self.mean = mean\n        if shape is not None:\n            self.shape = shape\n        if std is not None:\n            self.std = std\n        if sum_of_squares is not None:\n            self.sum_of_squares = sum_of_squares\n        if sum_of_values is not None:\n            self.sum_of_values = sum_of_values\n\n    @property\n    def custom(self):\n        \"\"\"Gets the custom of this SampleMetaData.  # noqa: E501\n\n\n        :return: The custom of this SampleMetaData.  # noqa: E501\n        :rtype: dict(str, object)\n        \"\"\"\n        return self._custom\n\n    @custom.setter\n    def custom(self, custom):\n        \"\"\"Sets the custom of this SampleMetaData.\n\n\n        :param custom: The custom of this SampleMetaData.  # noqa: E501\n        :type: dict(str, object)\n        \"\"\"\n\n        self._custom = custom\n\n    @property\n    def sharpness(self):\n        \"\"\"Gets the sharpness of this SampleMetaData.  # noqa: E501\n\n\n        :return: The sharpness of this SampleMetaData.  # noqa: E501\n        :rtype: float\n        \"\"\"\n        return self._sharpness\n\n    @sharpness.setter\n    def sharpness(self, sharpness):\n        \"\"\"Sets the sharpness of this SampleMetaData.\n\n\n        :param sharpness: The sharpness of this SampleMetaData.  # noqa: E501\n        :type: float\n        \"\"\"\n\n        self._sharpness = sharpness\n\n    @property\n    def size_in_bytes(self):\n        \"\"\"Gets the size_in_bytes of this SampleMetaData.  # noqa: E501\n\n\n        :return: The size_in_bytes of this SampleMetaData.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._size_in_bytes\n\n    @size_in_bytes.setter\n    def size_in_bytes(self, size_in_bytes):\n        \"\"\"Sets the size_in_bytes of this SampleMetaData.\n\n\n        :param size_in_bytes: The size_in_bytes of this SampleMetaData.  # noqa: E501\n        :type: int\n        \"\"\"\n\n        self._size_in_bytes = size_in_bytes\n\n    @property\n    def snr(self):\n        \"\"\"Gets the snr of this SampleMetaData.  # noqa: E501\n\n\n        :return: The snr of this SampleMetaData.  # noqa: E501\n        :rtype: float\n        \"\"\"\n        return self._snr\n\n    @snr.setter\n    def snr(self, snr):\n        \"\"\"Sets the snr of this SampleMetaData.\n\n\n        :param snr: The snr of this SampleMetaData.  # noqa: E501\n        :type: float\n        \"\"\"\n\n        self._snr = snr\n\n    @property\n    def mean(self):\n        \"\"\"Gets the mean of this SampleMetaData.  # noqa: E501\n\n\n        :return: The mean of this SampleMetaData.  # noqa: E501\n        :rtype: list[float]\n        \"\"\"\n        return self._mean\n\n    @mean.setter\n    def mean(self, mean):\n        \"\"\"Sets the mean of this SampleMetaData.\n\n\n        :param mean: The mean of this SampleMetaData.  # noqa: E501\n        :type: list[float]\n        \"\"\"\n\n        self._mean = mean\n\n    @property\n    def shape(self):\n        \"\"\"Gets the shape of this SampleMetaData.  # noqa: E501\n\n\n        :return: The shape of this SampleMetaData.  # noqa: E501\n        :rtype: list[int]\n        \"\"\"\n        return self._shape\n\n    @shape.setter\n    def shape(self, shape):\n        \"\"\"Sets the shape of this SampleMetaData.\n\n\n        :param shape: The shape of this SampleMetaData.  # noqa: E501\n        :type: list[int]\n        \"\"\"\n\n        self._shape = shape\n\n    @property\n    def std(self):\n        \"\"\"Gets the std of this SampleMetaData.  # noqa: E501\n\n\n        :return: The std of this SampleMetaData.  # noqa: E501\n        :rtype: list[float]\n        \"\"\"\n        return self._std\n\n    @std.setter\n    def std(self, std):\n        \"\"\"Sets the std of this SampleMetaData.\n\n\n        :param std: The std of this SampleMetaData.  # noqa: E501\n        :type: list[float]\n        \"\"\"\n\n        self._std = std\n\n    @property\n    def sum_of_squares(self):\n        \"\"\"Gets the sum_of_squares of this SampleMetaData.  # noqa: E501\n\n\n        :return: The sum_of_squares of this SampleMetaData.  # noqa: E501\n        :rtype: list[float]\n        \"\"\"\n        return self._sum_of_squares\n\n    @sum_of_squares.setter\n    def sum_of_squares(self, sum_of_squares):\n        \"\"\"Sets the sum_of_squares of this SampleMetaData.\n\n\n        :param sum_of_squares: The sum_of_squares of this SampleMetaData.  # noqa: E501\n        :type: list[float]\n        \"\"\"\n\n        self._sum_of_squares = sum_of_squares\n\n    @property\n    def sum_of_values(self):\n        \"\"\"Gets the sum_of_values of this SampleMetaData.  # noqa: E501\n\n\n        :return: The sum_of_values of this SampleMetaData.  # noqa: E501\n        :rtype: list[float]\n        \"\"\"\n        return self._sum_of_values\n\n    @sum_of_values.setter\n    def sum_of_values(self, sum_of_values):\n        \"\"\"Sets the sum_of_values of this SampleMetaData.\n\n\n        :param sum_of_values: The sum_of_values of this SampleMetaData.  # noqa: E501\n        :type: list[float]\n        \"\"\"\n\n        self._sum_of_values = sum_of_values\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(SampleMetaData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, SampleMetaData):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, SampleMetaData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, custom=None, sharpness=None, size_in_bytes=None, snr=None, mean=None, shape=None, std=None, sum_of_squares=None, sum_of_values=None, _configuration=None):  # noqa: E501\n        \"\"\"SampleMetaData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._custom = None\n        self._sharpness = None\n        self._size_in_bytes = None\n        self._snr = None\n        self._mean = None\n        self._shape = None\n        self._std = None\n        self._sum_of_squares = None\n        self._sum_of_values = None\n        self.discriminator = None\n\n        if custom is not None:\n            self.custom = custom\n        if sharpness is not None:\n            self.sharpness = sharpness\n        if size_in_bytes is not None:\n            self.size_in_bytes = size_in_bytes\n        if snr is not None:\n            self.snr = snr\n        if mean is not None:\n            self.mean = mean\n        if shape is not None:\n            self.shape = shape\n        if std is not None:\n            self.std = std\n        if sum_of_squares is not None:\n            self.sum_of_squares = sum_of_squares\n        if sum_of_values is not None:\n            self.sum_of_values = sum_of_values",
  "def custom(self):\n        \"\"\"Gets the custom of this SampleMetaData.  # noqa: E501\n\n\n        :return: The custom of this SampleMetaData.  # noqa: E501\n        :rtype: dict(str, object)\n        \"\"\"\n        return self._custom",
  "def custom(self, custom):\n        \"\"\"Sets the custom of this SampleMetaData.\n\n\n        :param custom: The custom of this SampleMetaData.  # noqa: E501\n        :type: dict(str, object)\n        \"\"\"\n\n        self._custom = custom",
  "def sharpness(self):\n        \"\"\"Gets the sharpness of this SampleMetaData.  # noqa: E501\n\n\n        :return: The sharpness of this SampleMetaData.  # noqa: E501\n        :rtype: float\n        \"\"\"\n        return self._sharpness",
  "def sharpness(self, sharpness):\n        \"\"\"Sets the sharpness of this SampleMetaData.\n\n\n        :param sharpness: The sharpness of this SampleMetaData.  # noqa: E501\n        :type: float\n        \"\"\"\n\n        self._sharpness = sharpness",
  "def size_in_bytes(self):\n        \"\"\"Gets the size_in_bytes of this SampleMetaData.  # noqa: E501\n\n\n        :return: The size_in_bytes of this SampleMetaData.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._size_in_bytes",
  "def size_in_bytes(self, size_in_bytes):\n        \"\"\"Sets the size_in_bytes of this SampleMetaData.\n\n\n        :param size_in_bytes: The size_in_bytes of this SampleMetaData.  # noqa: E501\n        :type: int\n        \"\"\"\n\n        self._size_in_bytes = size_in_bytes",
  "def snr(self):\n        \"\"\"Gets the snr of this SampleMetaData.  # noqa: E501\n\n\n        :return: The snr of this SampleMetaData.  # noqa: E501\n        :rtype: float\n        \"\"\"\n        return self._snr",
  "def snr(self, snr):\n        \"\"\"Sets the snr of this SampleMetaData.\n\n\n        :param snr: The snr of this SampleMetaData.  # noqa: E501\n        :type: float\n        \"\"\"\n\n        self._snr = snr",
  "def mean(self):\n        \"\"\"Gets the mean of this SampleMetaData.  # noqa: E501\n\n\n        :return: The mean of this SampleMetaData.  # noqa: E501\n        :rtype: list[float]\n        \"\"\"\n        return self._mean",
  "def mean(self, mean):\n        \"\"\"Sets the mean of this SampleMetaData.\n\n\n        :param mean: The mean of this SampleMetaData.  # noqa: E501\n        :type: list[float]\n        \"\"\"\n\n        self._mean = mean",
  "def shape(self):\n        \"\"\"Gets the shape of this SampleMetaData.  # noqa: E501\n\n\n        :return: The shape of this SampleMetaData.  # noqa: E501\n        :rtype: list[int]\n        \"\"\"\n        return self._shape",
  "def shape(self, shape):\n        \"\"\"Sets the shape of this SampleMetaData.\n\n\n        :param shape: The shape of this SampleMetaData.  # noqa: E501\n        :type: list[int]\n        \"\"\"\n\n        self._shape = shape",
  "def std(self):\n        \"\"\"Gets the std of this SampleMetaData.  # noqa: E501\n\n\n        :return: The std of this SampleMetaData.  # noqa: E501\n        :rtype: list[float]\n        \"\"\"\n        return self._std",
  "def std(self, std):\n        \"\"\"Sets the std of this SampleMetaData.\n\n\n        :param std: The std of this SampleMetaData.  # noqa: E501\n        :type: list[float]\n        \"\"\"\n\n        self._std = std",
  "def sum_of_squares(self):\n        \"\"\"Gets the sum_of_squares of this SampleMetaData.  # noqa: E501\n\n\n        :return: The sum_of_squares of this SampleMetaData.  # noqa: E501\n        :rtype: list[float]\n        \"\"\"\n        return self._sum_of_squares",
  "def sum_of_squares(self, sum_of_squares):\n        \"\"\"Sets the sum_of_squares of this SampleMetaData.\n\n\n        :param sum_of_squares: The sum_of_squares of this SampleMetaData.  # noqa: E501\n        :type: list[float]\n        \"\"\"\n\n        self._sum_of_squares = sum_of_squares",
  "def sum_of_values(self):\n        \"\"\"Gets the sum_of_values of this SampleMetaData.  # noqa: E501\n\n\n        :return: The sum_of_values of this SampleMetaData.  # noqa: E501\n        :rtype: list[float]\n        \"\"\"\n        return self._sum_of_values",
  "def sum_of_values(self, sum_of_values):\n        \"\"\"Sets the sum_of_values of this SampleMetaData.\n\n\n        :param sum_of_values: The sum_of_values of this SampleMetaData.  # noqa: E501\n        :type: list[float]\n        \"\"\"\n\n        self._sum_of_values = sum_of_values",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(SampleMetaData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, SampleMetaData):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, SampleMetaData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class VersionNumber(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n    }\n\n    attribute_map = {\n    }\n\n    def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"VersionNumber - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(VersionNumber, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, VersionNumber):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, VersionNumber):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"VersionNumber - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(VersionNumber, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, VersionNumber):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, VersionNumber):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class TagName(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n    }\n\n    attribute_map = {\n    }\n\n    def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"TagName - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(TagName, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, TagName):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, TagName):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"TagName - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(TagName, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, TagName):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, TagName):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class ActiveLearningScoreData(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'id': 'MongoObjectID',\n        'tag_id': 'MongoObjectID',\n        'score_type': 'ActiveLearningScoreType',\n        'scores': 'ActiveLearningScores',\n        'created_at': 'Timestamp'\n    }\n\n    attribute_map = {\n        'id': 'id',\n        'tag_id': 'tagId',\n        'score_type': 'scoreType',\n        'scores': 'scores',\n        'created_at': 'createdAt'\n    }\n\n    def __init__(self, id=None, tag_id=None, score_type=None, scores=None, created_at=None, _configuration=None):  # noqa: E501\n        \"\"\"ActiveLearningScoreData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._id = None\n        self._tag_id = None\n        self._score_type = None\n        self._scores = None\n        self._created_at = None\n        self.discriminator = None\n\n        self.id = id\n        self.tag_id = tag_id\n        self.score_type = score_type\n        self.scores = scores\n        self.created_at = created_at\n\n    @property\n    def id(self):\n        \"\"\"Gets the id of this ActiveLearningScoreData.  # noqa: E501\n\n\n        :return: The id of this ActiveLearningScoreData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._id\n\n    @id.setter\n    def id(self, id):\n        \"\"\"Sets the id of this ActiveLearningScoreData.\n\n\n        :param id: The id of this ActiveLearningScoreData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and id is None:\n            raise ValueError(\"Invalid value for `id`, must not be `None`\")  # noqa: E501\n\n        self._id = id\n\n    @property\n    def tag_id(self):\n        \"\"\"Gets the tag_id of this ActiveLearningScoreData.  # noqa: E501\n\n\n        :return: The tag_id of this ActiveLearningScoreData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._tag_id\n\n    @tag_id.setter\n    def tag_id(self, tag_id):\n        \"\"\"Sets the tag_id of this ActiveLearningScoreData.\n\n\n        :param tag_id: The tag_id of this ActiveLearningScoreData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and tag_id is None:\n            raise ValueError(\"Invalid value for `tag_id`, must not be `None`\")  # noqa: E501\n\n        self._tag_id = tag_id\n\n    @property\n    def score_type(self):\n        \"\"\"Gets the score_type of this ActiveLearningScoreData.  # noqa: E501\n\n\n        :return: The score_type of this ActiveLearningScoreData.  # noqa: E501\n        :rtype: ActiveLearningScoreType\n        \"\"\"\n        return self._score_type\n\n    @score_type.setter\n    def score_type(self, score_type):\n        \"\"\"Sets the score_type of this ActiveLearningScoreData.\n\n\n        :param score_type: The score_type of this ActiveLearningScoreData.  # noqa: E501\n        :type: ActiveLearningScoreType\n        \"\"\"\n        if self._configuration.client_side_validation and score_type is None:\n            raise ValueError(\"Invalid value for `score_type`, must not be `None`\")  # noqa: E501\n\n        self._score_type = score_type\n\n    @property\n    def scores(self):\n        \"\"\"Gets the scores of this ActiveLearningScoreData.  # noqa: E501\n\n\n        :return: The scores of this ActiveLearningScoreData.  # noqa: E501\n        :rtype: ActiveLearningScores\n        \"\"\"\n        return self._scores\n\n    @scores.setter\n    def scores(self, scores):\n        \"\"\"Sets the scores of this ActiveLearningScoreData.\n\n\n        :param scores: The scores of this ActiveLearningScoreData.  # noqa: E501\n        :type: ActiveLearningScores\n        \"\"\"\n        if self._configuration.client_side_validation and scores is None:\n            raise ValueError(\"Invalid value for `scores`, must not be `None`\")  # noqa: E501\n\n        self._scores = scores\n\n    @property\n    def created_at(self):\n        \"\"\"Gets the created_at of this ActiveLearningScoreData.  # noqa: E501\n\n\n        :return: The created_at of this ActiveLearningScoreData.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._created_at\n\n    @created_at.setter\n    def created_at(self, created_at):\n        \"\"\"Sets the created_at of this ActiveLearningScoreData.\n\n\n        :param created_at: The created_at of this ActiveLearningScoreData.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n        if self._configuration.client_side_validation and created_at is None:\n            raise ValueError(\"Invalid value for `created_at`, must not be `None`\")  # noqa: E501\n\n        self._created_at = created_at\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(ActiveLearningScoreData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, ActiveLearningScoreData):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, ActiveLearningScoreData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, id=None, tag_id=None, score_type=None, scores=None, created_at=None, _configuration=None):  # noqa: E501\n        \"\"\"ActiveLearningScoreData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._id = None\n        self._tag_id = None\n        self._score_type = None\n        self._scores = None\n        self._created_at = None\n        self.discriminator = None\n\n        self.id = id\n        self.tag_id = tag_id\n        self.score_type = score_type\n        self.scores = scores\n        self.created_at = created_at",
  "def id(self):\n        \"\"\"Gets the id of this ActiveLearningScoreData.  # noqa: E501\n\n\n        :return: The id of this ActiveLearningScoreData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._id",
  "def id(self, id):\n        \"\"\"Sets the id of this ActiveLearningScoreData.\n\n\n        :param id: The id of this ActiveLearningScoreData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and id is None:\n            raise ValueError(\"Invalid value for `id`, must not be `None`\")  # noqa: E501\n\n        self._id = id",
  "def tag_id(self):\n        \"\"\"Gets the tag_id of this ActiveLearningScoreData.  # noqa: E501\n\n\n        :return: The tag_id of this ActiveLearningScoreData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._tag_id",
  "def tag_id(self, tag_id):\n        \"\"\"Sets the tag_id of this ActiveLearningScoreData.\n\n\n        :param tag_id: The tag_id of this ActiveLearningScoreData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and tag_id is None:\n            raise ValueError(\"Invalid value for `tag_id`, must not be `None`\")  # noqa: E501\n\n        self._tag_id = tag_id",
  "def score_type(self):\n        \"\"\"Gets the score_type of this ActiveLearningScoreData.  # noqa: E501\n\n\n        :return: The score_type of this ActiveLearningScoreData.  # noqa: E501\n        :rtype: ActiveLearningScoreType\n        \"\"\"\n        return self._score_type",
  "def score_type(self, score_type):\n        \"\"\"Sets the score_type of this ActiveLearningScoreData.\n\n\n        :param score_type: The score_type of this ActiveLearningScoreData.  # noqa: E501\n        :type: ActiveLearningScoreType\n        \"\"\"\n        if self._configuration.client_side_validation and score_type is None:\n            raise ValueError(\"Invalid value for `score_type`, must not be `None`\")  # noqa: E501\n\n        self._score_type = score_type",
  "def scores(self):\n        \"\"\"Gets the scores of this ActiveLearningScoreData.  # noqa: E501\n\n\n        :return: The scores of this ActiveLearningScoreData.  # noqa: E501\n        :rtype: ActiveLearningScores\n        \"\"\"\n        return self._scores",
  "def scores(self, scores):\n        \"\"\"Sets the scores of this ActiveLearningScoreData.\n\n\n        :param scores: The scores of this ActiveLearningScoreData.  # noqa: E501\n        :type: ActiveLearningScores\n        \"\"\"\n        if self._configuration.client_side_validation and scores is None:\n            raise ValueError(\"Invalid value for `scores`, must not be `None`\")  # noqa: E501\n\n        self._scores = scores",
  "def created_at(self):\n        \"\"\"Gets the created_at of this ActiveLearningScoreData.  # noqa: E501\n\n\n        :return: The created_at of this ActiveLearningScoreData.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._created_at",
  "def created_at(self, created_at):\n        \"\"\"Sets the created_at of this ActiveLearningScoreData.\n\n\n        :param created_at: The created_at of this ActiveLearningScoreData.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n        if self._configuration.client_side_validation and created_at is None:\n            raise ValueError(\"Invalid value for `created_at`, must not be `None`\")  # noqa: E501\n\n        self._created_at = created_at",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(ActiveLearningScoreData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, ActiveLearningScoreData):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, ActiveLearningScoreData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class SamplingCreateRequest(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'new_tag_name': 'TagName',\n        'method': 'SamplingMethod',\n        'config': 'SamplingConfig',\n        'preselected_tag_id': 'MongoObjectID',\n        'query_tag_id': 'MongoObjectID',\n        'row_count': 'float'\n    }\n\n    attribute_map = {\n        'new_tag_name': 'newTagName',\n        'method': 'method',\n        'config': 'config',\n        'preselected_tag_id': 'preselectedTagId',\n        'query_tag_id': 'queryTagId',\n        'row_count': 'rowCount'\n    }\n\n    def __init__(self, new_tag_name=None, method=None, config=None, preselected_tag_id=None, query_tag_id=None, row_count=None, _configuration=None):  # noqa: E501\n        \"\"\"SamplingCreateRequest - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._new_tag_name = None\n        self._method = None\n        self._config = None\n        self._preselected_tag_id = None\n        self._query_tag_id = None\n        self._row_count = None\n        self.discriminator = None\n\n        self.new_tag_name = new_tag_name\n        self.method = method\n        self.config = config\n        if preselected_tag_id is not None:\n            self.preselected_tag_id = preselected_tag_id\n        if query_tag_id is not None:\n            self.query_tag_id = query_tag_id\n        if row_count is not None:\n            self.row_count = row_count\n\n    @property\n    def new_tag_name(self):\n        \"\"\"Gets the new_tag_name of this SamplingCreateRequest.  # noqa: E501\n\n\n        :return: The new_tag_name of this SamplingCreateRequest.  # noqa: E501\n        :rtype: TagName\n        \"\"\"\n        return self._new_tag_name\n\n    @new_tag_name.setter\n    def new_tag_name(self, new_tag_name):\n        \"\"\"Sets the new_tag_name of this SamplingCreateRequest.\n\n\n        :param new_tag_name: The new_tag_name of this SamplingCreateRequest.  # noqa: E501\n        :type: TagName\n        \"\"\"\n        if self._configuration.client_side_validation and new_tag_name is None:\n            raise ValueError(\"Invalid value for `new_tag_name`, must not be `None`\")  # noqa: E501\n\n        self._new_tag_name = new_tag_name\n\n    @property\n    def method(self):\n        \"\"\"Gets the method of this SamplingCreateRequest.  # noqa: E501\n\n\n        :return: The method of this SamplingCreateRequest.  # noqa: E501\n        :rtype: SamplingMethod\n        \"\"\"\n        return self._method\n\n    @method.setter\n    def method(self, method):\n        \"\"\"Sets the method of this SamplingCreateRequest.\n\n\n        :param method: The method of this SamplingCreateRequest.  # noqa: E501\n        :type: SamplingMethod\n        \"\"\"\n        if self._configuration.client_side_validation and method is None:\n            raise ValueError(\"Invalid value for `method`, must not be `None`\")  # noqa: E501\n\n        self._method = method\n\n    @property\n    def config(self):\n        \"\"\"Gets the config of this SamplingCreateRequest.  # noqa: E501\n\n\n        :return: The config of this SamplingCreateRequest.  # noqa: E501\n        :rtype: SamplingConfig\n        \"\"\"\n        return self._config\n\n    @config.setter\n    def config(self, config):\n        \"\"\"Sets the config of this SamplingCreateRequest.\n\n\n        :param config: The config of this SamplingCreateRequest.  # noqa: E501\n        :type: SamplingConfig\n        \"\"\"\n        if self._configuration.client_side_validation and config is None:\n            raise ValueError(\"Invalid value for `config`, must not be `None`\")  # noqa: E501\n\n        self._config = config\n\n    @property\n    def preselected_tag_id(self):\n        \"\"\"Gets the preselected_tag_id of this SamplingCreateRequest.  # noqa: E501\n\n\n        :return: The preselected_tag_id of this SamplingCreateRequest.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._preselected_tag_id\n\n    @preselected_tag_id.setter\n    def preselected_tag_id(self, preselected_tag_id):\n        \"\"\"Sets the preselected_tag_id of this SamplingCreateRequest.\n\n\n        :param preselected_tag_id: The preselected_tag_id of this SamplingCreateRequest.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n\n        self._preselected_tag_id = preselected_tag_id\n\n    @property\n    def query_tag_id(self):\n        \"\"\"Gets the query_tag_id of this SamplingCreateRequest.  # noqa: E501\n\n\n        :return: The query_tag_id of this SamplingCreateRequest.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._query_tag_id\n\n    @query_tag_id.setter\n    def query_tag_id(self, query_tag_id):\n        \"\"\"Sets the query_tag_id of this SamplingCreateRequest.\n\n\n        :param query_tag_id: The query_tag_id of this SamplingCreateRequest.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n\n        self._query_tag_id = query_tag_id\n\n    @property\n    def row_count(self):\n        \"\"\"Gets the row_count of this SamplingCreateRequest.  # noqa: E501\n\n        temporary rowCount until the API/DB is aware how many they are..  # noqa: E501\n\n        :return: The row_count of this SamplingCreateRequest.  # noqa: E501\n        :rtype: float\n        \"\"\"\n        return self._row_count\n\n    @row_count.setter\n    def row_count(self, row_count):\n        \"\"\"Sets the row_count of this SamplingCreateRequest.\n\n        temporary rowCount until the API/DB is aware how many they are..  # noqa: E501\n\n        :param row_count: The row_count of this SamplingCreateRequest.  # noqa: E501\n        :type: float\n        \"\"\"\n\n        self._row_count = row_count\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(SamplingCreateRequest, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, SamplingCreateRequest):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, SamplingCreateRequest):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, new_tag_name=None, method=None, config=None, preselected_tag_id=None, query_tag_id=None, row_count=None, _configuration=None):  # noqa: E501\n        \"\"\"SamplingCreateRequest - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._new_tag_name = None\n        self._method = None\n        self._config = None\n        self._preselected_tag_id = None\n        self._query_tag_id = None\n        self._row_count = None\n        self.discriminator = None\n\n        self.new_tag_name = new_tag_name\n        self.method = method\n        self.config = config\n        if preselected_tag_id is not None:\n            self.preselected_tag_id = preselected_tag_id\n        if query_tag_id is not None:\n            self.query_tag_id = query_tag_id\n        if row_count is not None:\n            self.row_count = row_count",
  "def new_tag_name(self):\n        \"\"\"Gets the new_tag_name of this SamplingCreateRequest.  # noqa: E501\n\n\n        :return: The new_tag_name of this SamplingCreateRequest.  # noqa: E501\n        :rtype: TagName\n        \"\"\"\n        return self._new_tag_name",
  "def new_tag_name(self, new_tag_name):\n        \"\"\"Sets the new_tag_name of this SamplingCreateRequest.\n\n\n        :param new_tag_name: The new_tag_name of this SamplingCreateRequest.  # noqa: E501\n        :type: TagName\n        \"\"\"\n        if self._configuration.client_side_validation and new_tag_name is None:\n            raise ValueError(\"Invalid value for `new_tag_name`, must not be `None`\")  # noqa: E501\n\n        self._new_tag_name = new_tag_name",
  "def method(self):\n        \"\"\"Gets the method of this SamplingCreateRequest.  # noqa: E501\n\n\n        :return: The method of this SamplingCreateRequest.  # noqa: E501\n        :rtype: SamplingMethod\n        \"\"\"\n        return self._method",
  "def method(self, method):\n        \"\"\"Sets the method of this SamplingCreateRequest.\n\n\n        :param method: The method of this SamplingCreateRequest.  # noqa: E501\n        :type: SamplingMethod\n        \"\"\"\n        if self._configuration.client_side_validation and method is None:\n            raise ValueError(\"Invalid value for `method`, must not be `None`\")  # noqa: E501\n\n        self._method = method",
  "def config(self):\n        \"\"\"Gets the config of this SamplingCreateRequest.  # noqa: E501\n\n\n        :return: The config of this SamplingCreateRequest.  # noqa: E501\n        :rtype: SamplingConfig\n        \"\"\"\n        return self._config",
  "def config(self, config):\n        \"\"\"Sets the config of this SamplingCreateRequest.\n\n\n        :param config: The config of this SamplingCreateRequest.  # noqa: E501\n        :type: SamplingConfig\n        \"\"\"\n        if self._configuration.client_side_validation and config is None:\n            raise ValueError(\"Invalid value for `config`, must not be `None`\")  # noqa: E501\n\n        self._config = config",
  "def preselected_tag_id(self):\n        \"\"\"Gets the preselected_tag_id of this SamplingCreateRequest.  # noqa: E501\n\n\n        :return: The preselected_tag_id of this SamplingCreateRequest.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._preselected_tag_id",
  "def preselected_tag_id(self, preselected_tag_id):\n        \"\"\"Sets the preselected_tag_id of this SamplingCreateRequest.\n\n\n        :param preselected_tag_id: The preselected_tag_id of this SamplingCreateRequest.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n\n        self._preselected_tag_id = preselected_tag_id",
  "def query_tag_id(self):\n        \"\"\"Gets the query_tag_id of this SamplingCreateRequest.  # noqa: E501\n\n\n        :return: The query_tag_id of this SamplingCreateRequest.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._query_tag_id",
  "def query_tag_id(self, query_tag_id):\n        \"\"\"Sets the query_tag_id of this SamplingCreateRequest.\n\n\n        :param query_tag_id: The query_tag_id of this SamplingCreateRequest.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n\n        self._query_tag_id = query_tag_id",
  "def row_count(self):\n        \"\"\"Gets the row_count of this SamplingCreateRequest.  # noqa: E501\n\n        temporary rowCount until the API/DB is aware how many they are..  # noqa: E501\n\n        :return: The row_count of this SamplingCreateRequest.  # noqa: E501\n        :rtype: float\n        \"\"\"\n        return self._row_count",
  "def row_count(self, row_count):\n        \"\"\"Sets the row_count of this SamplingCreateRequest.\n\n        temporary rowCount until the API/DB is aware how many they are..  # noqa: E501\n\n        :param row_count: The row_count of this SamplingCreateRequest.  # noqa: E501\n        :type: float\n        \"\"\"\n\n        self._row_count = row_count",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(SamplingCreateRequest, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, SamplingCreateRequest):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, SamplingCreateRequest):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class JobsData(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'id': 'MongoObjectID',\n        'job_id': 'str',\n        'job_type': 'JobResultType',\n        'status': 'JobState',\n        'finished_at': 'Timestamp',\n        'created_at': 'Timestamp'\n    }\n\n    attribute_map = {\n        'id': 'id',\n        'job_id': 'jobId',\n        'job_type': 'jobType',\n        'status': 'status',\n        'finished_at': 'finishedAt',\n        'created_at': 'createdAt'\n    }\n\n    def __init__(self, id=None, job_id=None, job_type=None, status=None, finished_at=None, created_at=None, _configuration=None):  # noqa: E501\n        \"\"\"JobsData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._id = None\n        self._job_id = None\n        self._job_type = None\n        self._status = None\n        self._finished_at = None\n        self._created_at = None\n        self.discriminator = None\n\n        self.id = id\n        self.job_id = job_id\n        self.job_type = job_type\n        self.status = status\n        if finished_at is not None:\n            self.finished_at = finished_at\n        self.created_at = created_at\n\n    @property\n    def id(self):\n        \"\"\"Gets the id of this JobsData.  # noqa: E501\n\n\n        :return: The id of this JobsData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._id\n\n    @id.setter\n    def id(self, id):\n        \"\"\"Sets the id of this JobsData.\n\n\n        :param id: The id of this JobsData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and id is None:\n            raise ValueError(\"Invalid value for `id`, must not be `None`\")  # noqa: E501\n\n        self._id = id\n\n    @property\n    def job_id(self):\n        \"\"\"Gets the job_id of this JobsData.  # noqa: E501\n\n\n        :return: The job_id of this JobsData.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._job_id\n\n    @job_id.setter\n    def job_id(self, job_id):\n        \"\"\"Sets the job_id of this JobsData.\n\n\n        :param job_id: The job_id of this JobsData.  # noqa: E501\n        :type: str\n        \"\"\"\n        if self._configuration.client_side_validation and job_id is None:\n            raise ValueError(\"Invalid value for `job_id`, must not be `None`\")  # noqa: E501\n\n        self._job_id = job_id\n\n    @property\n    def job_type(self):\n        \"\"\"Gets the job_type of this JobsData.  # noqa: E501\n\n\n        :return: The job_type of this JobsData.  # noqa: E501\n        :rtype: JobResultType\n        \"\"\"\n        return self._job_type\n\n    @job_type.setter\n    def job_type(self, job_type):\n        \"\"\"Sets the job_type of this JobsData.\n\n\n        :param job_type: The job_type of this JobsData.  # noqa: E501\n        :type: JobResultType\n        \"\"\"\n        if self._configuration.client_side_validation and job_type is None:\n            raise ValueError(\"Invalid value for `job_type`, must not be `None`\")  # noqa: E501\n\n        self._job_type = job_type\n\n    @property\n    def status(self):\n        \"\"\"Gets the status of this JobsData.  # noqa: E501\n\n\n        :return: The status of this JobsData.  # noqa: E501\n        :rtype: JobState\n        \"\"\"\n        return self._status\n\n    @status.setter\n    def status(self, status):\n        \"\"\"Sets the status of this JobsData.\n\n\n        :param status: The status of this JobsData.  # noqa: E501\n        :type: JobState\n        \"\"\"\n        if self._configuration.client_side_validation and status is None:\n            raise ValueError(\"Invalid value for `status`, must not be `None`\")  # noqa: E501\n\n        self._status = status\n\n    @property\n    def finished_at(self):\n        \"\"\"Gets the finished_at of this JobsData.  # noqa: E501\n\n\n        :return: The finished_at of this JobsData.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._finished_at\n\n    @finished_at.setter\n    def finished_at(self, finished_at):\n        \"\"\"Sets the finished_at of this JobsData.\n\n\n        :param finished_at: The finished_at of this JobsData.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n\n        self._finished_at = finished_at\n\n    @property\n    def created_at(self):\n        \"\"\"Gets the created_at of this JobsData.  # noqa: E501\n\n\n        :return: The created_at of this JobsData.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._created_at\n\n    @created_at.setter\n    def created_at(self, created_at):\n        \"\"\"Sets the created_at of this JobsData.\n\n\n        :param created_at: The created_at of this JobsData.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n        if self._configuration.client_side_validation and created_at is None:\n            raise ValueError(\"Invalid value for `created_at`, must not be `None`\")  # noqa: E501\n\n        self._created_at = created_at\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(JobsData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, JobsData):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, JobsData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, id=None, job_id=None, job_type=None, status=None, finished_at=None, created_at=None, _configuration=None):  # noqa: E501\n        \"\"\"JobsData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._id = None\n        self._job_id = None\n        self._job_type = None\n        self._status = None\n        self._finished_at = None\n        self._created_at = None\n        self.discriminator = None\n\n        self.id = id\n        self.job_id = job_id\n        self.job_type = job_type\n        self.status = status\n        if finished_at is not None:\n            self.finished_at = finished_at\n        self.created_at = created_at",
  "def id(self):\n        \"\"\"Gets the id of this JobsData.  # noqa: E501\n\n\n        :return: The id of this JobsData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._id",
  "def id(self, id):\n        \"\"\"Sets the id of this JobsData.\n\n\n        :param id: The id of this JobsData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and id is None:\n            raise ValueError(\"Invalid value for `id`, must not be `None`\")  # noqa: E501\n\n        self._id = id",
  "def job_id(self):\n        \"\"\"Gets the job_id of this JobsData.  # noqa: E501\n\n\n        :return: The job_id of this JobsData.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._job_id",
  "def job_id(self, job_id):\n        \"\"\"Sets the job_id of this JobsData.\n\n\n        :param job_id: The job_id of this JobsData.  # noqa: E501\n        :type: str\n        \"\"\"\n        if self._configuration.client_side_validation and job_id is None:\n            raise ValueError(\"Invalid value for `job_id`, must not be `None`\")  # noqa: E501\n\n        self._job_id = job_id",
  "def job_type(self):\n        \"\"\"Gets the job_type of this JobsData.  # noqa: E501\n\n\n        :return: The job_type of this JobsData.  # noqa: E501\n        :rtype: JobResultType\n        \"\"\"\n        return self._job_type",
  "def job_type(self, job_type):\n        \"\"\"Sets the job_type of this JobsData.\n\n\n        :param job_type: The job_type of this JobsData.  # noqa: E501\n        :type: JobResultType\n        \"\"\"\n        if self._configuration.client_side_validation and job_type is None:\n            raise ValueError(\"Invalid value for `job_type`, must not be `None`\")  # noqa: E501\n\n        self._job_type = job_type",
  "def status(self):\n        \"\"\"Gets the status of this JobsData.  # noqa: E501\n\n\n        :return: The status of this JobsData.  # noqa: E501\n        :rtype: JobState\n        \"\"\"\n        return self._status",
  "def status(self, status):\n        \"\"\"Sets the status of this JobsData.\n\n\n        :param status: The status of this JobsData.  # noqa: E501\n        :type: JobState\n        \"\"\"\n        if self._configuration.client_side_validation and status is None:\n            raise ValueError(\"Invalid value for `status`, must not be `None`\")  # noqa: E501\n\n        self._status = status",
  "def finished_at(self):\n        \"\"\"Gets the finished_at of this JobsData.  # noqa: E501\n\n\n        :return: The finished_at of this JobsData.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._finished_at",
  "def finished_at(self, finished_at):\n        \"\"\"Sets the finished_at of this JobsData.\n\n\n        :param finished_at: The finished_at of this JobsData.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n\n        self._finished_at = finished_at",
  "def created_at(self):\n        \"\"\"Gets the created_at of this JobsData.  # noqa: E501\n\n\n        :return: The created_at of this JobsData.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._created_at",
  "def created_at(self, created_at):\n        \"\"\"Sets the created_at of this JobsData.\n\n\n        :param created_at: The created_at of this JobsData.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n        if self._configuration.client_side_validation and created_at is None:\n            raise ValueError(\"Invalid value for `created_at`, must not be `None`\")  # noqa: E501\n\n        self._created_at = created_at",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(JobsData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, JobsData):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, JobsData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class Embedding2dData(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'id': 'MongoObjectID',\n        'dataset_id': 'MongoObjectID',\n        'embedding_id': 'MongoObjectID',\n        'name': 'str',\n        'created_at': 'Timestamp',\n        'dimensionality_reduction_method': 'DimensionalityReductionMethod',\n        'coordinates_dimension1': 'Embedding2dCoordinates',\n        'coordinates_dimension2': 'Embedding2dCoordinates'\n    }\n\n    attribute_map = {\n        'id': 'id',\n        'dataset_id': 'datasetId',\n        'embedding_id': 'embeddingId',\n        'name': 'name',\n        'created_at': 'createdAt',\n        'dimensionality_reduction_method': 'dimensionalityReductionMethod',\n        'coordinates_dimension1': 'coordinatesDimension1',\n        'coordinates_dimension2': 'coordinatesDimension2'\n    }\n\n    def __init__(self, id=None, dataset_id=None, embedding_id=None, name=None, created_at=None, dimensionality_reduction_method=None, coordinates_dimension1=None, coordinates_dimension2=None, _configuration=None):  # noqa: E501\n        \"\"\"Embedding2dData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._id = None\n        self._dataset_id = None\n        self._embedding_id = None\n        self._name = None\n        self._created_at = None\n        self._dimensionality_reduction_method = None\n        self._coordinates_dimension1 = None\n        self._coordinates_dimension2 = None\n        self.discriminator = None\n\n        self.id = id\n        self.dataset_id = dataset_id\n        self.embedding_id = embedding_id\n        self.name = name\n        self.created_at = created_at\n        self.dimensionality_reduction_method = dimensionality_reduction_method\n        if coordinates_dimension1 is not None:\n            self.coordinates_dimension1 = coordinates_dimension1\n        if coordinates_dimension2 is not None:\n            self.coordinates_dimension2 = coordinates_dimension2\n\n    @property\n    def id(self):\n        \"\"\"Gets the id of this Embedding2dData.  # noqa: E501\n\n\n        :return: The id of this Embedding2dData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._id\n\n    @id.setter\n    def id(self, id):\n        \"\"\"Sets the id of this Embedding2dData.\n\n\n        :param id: The id of this Embedding2dData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and id is None:\n            raise ValueError(\"Invalid value for `id`, must not be `None`\")  # noqa: E501\n\n        self._id = id\n\n    @property\n    def dataset_id(self):\n        \"\"\"Gets the dataset_id of this Embedding2dData.  # noqa: E501\n\n\n        :return: The dataset_id of this Embedding2dData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._dataset_id\n\n    @dataset_id.setter\n    def dataset_id(self, dataset_id):\n        \"\"\"Sets the dataset_id of this Embedding2dData.\n\n\n        :param dataset_id: The dataset_id of this Embedding2dData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and dataset_id is None:\n            raise ValueError(\"Invalid value for `dataset_id`, must not be `None`\")  # noqa: E501\n\n        self._dataset_id = dataset_id\n\n    @property\n    def embedding_id(self):\n        \"\"\"Gets the embedding_id of this Embedding2dData.  # noqa: E501\n\n\n        :return: The embedding_id of this Embedding2dData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._embedding_id\n\n    @embedding_id.setter\n    def embedding_id(self, embedding_id):\n        \"\"\"Sets the embedding_id of this Embedding2dData.\n\n\n        :param embedding_id: The embedding_id of this Embedding2dData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and embedding_id is None:\n            raise ValueError(\"Invalid value for `embedding_id`, must not be `None`\")  # noqa: E501\n\n        self._embedding_id = embedding_id\n\n    @property\n    def name(self):\n        \"\"\"Gets the name of this Embedding2dData.  # noqa: E501\n\n        Name of the 2d embedding (default is embedding name + __2d)  # noqa: E501\n\n        :return: The name of this Embedding2dData.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        \"\"\"Sets the name of this Embedding2dData.\n\n        Name of the 2d embedding (default is embedding name + __2d)  # noqa: E501\n\n        :param name: The name of this Embedding2dData.  # noqa: E501\n        :type: str\n        \"\"\"\n        if self._configuration.client_side_validation and name is None:\n            raise ValueError(\"Invalid value for `name`, must not be `None`\")  # noqa: E501\n\n        self._name = name\n\n    @property\n    def created_at(self):\n        \"\"\"Gets the created_at of this Embedding2dData.  # noqa: E501\n\n\n        :return: The created_at of this Embedding2dData.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._created_at\n\n    @created_at.setter\n    def created_at(self, created_at):\n        \"\"\"Sets the created_at of this Embedding2dData.\n\n\n        :param created_at: The created_at of this Embedding2dData.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n        if self._configuration.client_side_validation and created_at is None:\n            raise ValueError(\"Invalid value for `created_at`, must not be `None`\")  # noqa: E501\n\n        self._created_at = created_at\n\n    @property\n    def dimensionality_reduction_method(self):\n        \"\"\"Gets the dimensionality_reduction_method of this Embedding2dData.  # noqa: E501\n\n\n        :return: The dimensionality_reduction_method of this Embedding2dData.  # noqa: E501\n        :rtype: DimensionalityReductionMethod\n        \"\"\"\n        return self._dimensionality_reduction_method\n\n    @dimensionality_reduction_method.setter\n    def dimensionality_reduction_method(self, dimensionality_reduction_method):\n        \"\"\"Sets the dimensionality_reduction_method of this Embedding2dData.\n\n\n        :param dimensionality_reduction_method: The dimensionality_reduction_method of this Embedding2dData.  # noqa: E501\n        :type: DimensionalityReductionMethod\n        \"\"\"\n        if self._configuration.client_side_validation and dimensionality_reduction_method is None:\n            raise ValueError(\"Invalid value for `dimensionality_reduction_method`, must not be `None`\")  # noqa: E501\n\n        self._dimensionality_reduction_method = dimensionality_reduction_method\n\n    @property\n    def coordinates_dimension1(self):\n        \"\"\"Gets the coordinates_dimension1 of this Embedding2dData.  # noqa: E501\n\n\n        :return: The coordinates_dimension1 of this Embedding2dData.  # noqa: E501\n        :rtype: Embedding2dCoordinates\n        \"\"\"\n        return self._coordinates_dimension1\n\n    @coordinates_dimension1.setter\n    def coordinates_dimension1(self, coordinates_dimension1):\n        \"\"\"Sets the coordinates_dimension1 of this Embedding2dData.\n\n\n        :param coordinates_dimension1: The coordinates_dimension1 of this Embedding2dData.  # noqa: E501\n        :type: Embedding2dCoordinates\n        \"\"\"\n\n        self._coordinates_dimension1 = coordinates_dimension1\n\n    @property\n    def coordinates_dimension2(self):\n        \"\"\"Gets the coordinates_dimension2 of this Embedding2dData.  # noqa: E501\n\n\n        :return: The coordinates_dimension2 of this Embedding2dData.  # noqa: E501\n        :rtype: Embedding2dCoordinates\n        \"\"\"\n        return self._coordinates_dimension2\n\n    @coordinates_dimension2.setter\n    def coordinates_dimension2(self, coordinates_dimension2):\n        \"\"\"Sets the coordinates_dimension2 of this Embedding2dData.\n\n\n        :param coordinates_dimension2: The coordinates_dimension2 of this Embedding2dData.  # noqa: E501\n        :type: Embedding2dCoordinates\n        \"\"\"\n\n        self._coordinates_dimension2 = coordinates_dimension2\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(Embedding2dData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, Embedding2dData):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, Embedding2dData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, id=None, dataset_id=None, embedding_id=None, name=None, created_at=None, dimensionality_reduction_method=None, coordinates_dimension1=None, coordinates_dimension2=None, _configuration=None):  # noqa: E501\n        \"\"\"Embedding2dData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._id = None\n        self._dataset_id = None\n        self._embedding_id = None\n        self._name = None\n        self._created_at = None\n        self._dimensionality_reduction_method = None\n        self._coordinates_dimension1 = None\n        self._coordinates_dimension2 = None\n        self.discriminator = None\n\n        self.id = id\n        self.dataset_id = dataset_id\n        self.embedding_id = embedding_id\n        self.name = name\n        self.created_at = created_at\n        self.dimensionality_reduction_method = dimensionality_reduction_method\n        if coordinates_dimension1 is not None:\n            self.coordinates_dimension1 = coordinates_dimension1\n        if coordinates_dimension2 is not None:\n            self.coordinates_dimension2 = coordinates_dimension2",
  "def id(self):\n        \"\"\"Gets the id of this Embedding2dData.  # noqa: E501\n\n\n        :return: The id of this Embedding2dData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._id",
  "def id(self, id):\n        \"\"\"Sets the id of this Embedding2dData.\n\n\n        :param id: The id of this Embedding2dData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and id is None:\n            raise ValueError(\"Invalid value for `id`, must not be `None`\")  # noqa: E501\n\n        self._id = id",
  "def dataset_id(self):\n        \"\"\"Gets the dataset_id of this Embedding2dData.  # noqa: E501\n\n\n        :return: The dataset_id of this Embedding2dData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._dataset_id",
  "def dataset_id(self, dataset_id):\n        \"\"\"Sets the dataset_id of this Embedding2dData.\n\n\n        :param dataset_id: The dataset_id of this Embedding2dData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and dataset_id is None:\n            raise ValueError(\"Invalid value for `dataset_id`, must not be `None`\")  # noqa: E501\n\n        self._dataset_id = dataset_id",
  "def embedding_id(self):\n        \"\"\"Gets the embedding_id of this Embedding2dData.  # noqa: E501\n\n\n        :return: The embedding_id of this Embedding2dData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._embedding_id",
  "def embedding_id(self, embedding_id):\n        \"\"\"Sets the embedding_id of this Embedding2dData.\n\n\n        :param embedding_id: The embedding_id of this Embedding2dData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and embedding_id is None:\n            raise ValueError(\"Invalid value for `embedding_id`, must not be `None`\")  # noqa: E501\n\n        self._embedding_id = embedding_id",
  "def name(self):\n        \"\"\"Gets the name of this Embedding2dData.  # noqa: E501\n\n        Name of the 2d embedding (default is embedding name + __2d)  # noqa: E501\n\n        :return: The name of this Embedding2dData.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._name",
  "def name(self, name):\n        \"\"\"Sets the name of this Embedding2dData.\n\n        Name of the 2d embedding (default is embedding name + __2d)  # noqa: E501\n\n        :param name: The name of this Embedding2dData.  # noqa: E501\n        :type: str\n        \"\"\"\n        if self._configuration.client_side_validation and name is None:\n            raise ValueError(\"Invalid value for `name`, must not be `None`\")  # noqa: E501\n\n        self._name = name",
  "def created_at(self):\n        \"\"\"Gets the created_at of this Embedding2dData.  # noqa: E501\n\n\n        :return: The created_at of this Embedding2dData.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._created_at",
  "def created_at(self, created_at):\n        \"\"\"Sets the created_at of this Embedding2dData.\n\n\n        :param created_at: The created_at of this Embedding2dData.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n        if self._configuration.client_side_validation and created_at is None:\n            raise ValueError(\"Invalid value for `created_at`, must not be `None`\")  # noqa: E501\n\n        self._created_at = created_at",
  "def dimensionality_reduction_method(self):\n        \"\"\"Gets the dimensionality_reduction_method of this Embedding2dData.  # noqa: E501\n\n\n        :return: The dimensionality_reduction_method of this Embedding2dData.  # noqa: E501\n        :rtype: DimensionalityReductionMethod\n        \"\"\"\n        return self._dimensionality_reduction_method",
  "def dimensionality_reduction_method(self, dimensionality_reduction_method):\n        \"\"\"Sets the dimensionality_reduction_method of this Embedding2dData.\n\n\n        :param dimensionality_reduction_method: The dimensionality_reduction_method of this Embedding2dData.  # noqa: E501\n        :type: DimensionalityReductionMethod\n        \"\"\"\n        if self._configuration.client_side_validation and dimensionality_reduction_method is None:\n            raise ValueError(\"Invalid value for `dimensionality_reduction_method`, must not be `None`\")  # noqa: E501\n\n        self._dimensionality_reduction_method = dimensionality_reduction_method",
  "def coordinates_dimension1(self):\n        \"\"\"Gets the coordinates_dimension1 of this Embedding2dData.  # noqa: E501\n\n\n        :return: The coordinates_dimension1 of this Embedding2dData.  # noqa: E501\n        :rtype: Embedding2dCoordinates\n        \"\"\"\n        return self._coordinates_dimension1",
  "def coordinates_dimension1(self, coordinates_dimension1):\n        \"\"\"Sets the coordinates_dimension1 of this Embedding2dData.\n\n\n        :param coordinates_dimension1: The coordinates_dimension1 of this Embedding2dData.  # noqa: E501\n        :type: Embedding2dCoordinates\n        \"\"\"\n\n        self._coordinates_dimension1 = coordinates_dimension1",
  "def coordinates_dimension2(self):\n        \"\"\"Gets the coordinates_dimension2 of this Embedding2dData.  # noqa: E501\n\n\n        :return: The coordinates_dimension2 of this Embedding2dData.  # noqa: E501\n        :rtype: Embedding2dCoordinates\n        \"\"\"\n        return self._coordinates_dimension2",
  "def coordinates_dimension2(self, coordinates_dimension2):\n        \"\"\"Sets the coordinates_dimension2 of this Embedding2dData.\n\n\n        :param coordinates_dimension2: The coordinates_dimension2 of this Embedding2dData.  # noqa: E501\n        :type: Embedding2dCoordinates\n        \"\"\"\n\n        self._coordinates_dimension2 = coordinates_dimension2",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(Embedding2dData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, Embedding2dData):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, Embedding2dData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class ApiErrorCode(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    allowed enum values\n    \"\"\"\n    BAD_REQUEST = \"BAD_REQUEST\"\n    NOT_IMPLEMENTED = \"NOT_IMPLEMENTED\"\n    FORBIDDEN = \"FORBIDDEN\"\n    UNAUTHORIZED = \"UNAUTHORIZED\"\n    NOT_FOUND = \"NOT_FOUND\"\n    MALFORMED_REQUEST = \"MALFORMED_REQUEST\"\n    MALFORMED_RESPONSE = \"MALFORMED_RESPONSE\"\n    JOB_CREATION_FAILED = \"JOB_CREATION_FAILED\"\n    USER_NOT_KNOWN = \"USER_NOT_KNOWN\"\n    USER_ACCOUNT_DEACTIVATED = \"USER_ACCOUNT_DEACTIVATED\"\n    USER_ACCOUNT_BLOCKED = \"USER_ACCOUNT_BLOCKED\"\n    TEAM_ACCOUNT_PLAN_INSUFFICIENT = \"TEAM_ACCOUNT_PLAN_INSUFFICIENT\"\n    DATASET_UNKNOWN = \"DATASET_UNKNOWN\"\n    DATASET_TAG_INVALID = \"DATASET_TAG_INVALID\"\n    DATASET_NAME_EXISTS = \"DATASET_NAME_EXISTS\"\n    DATASET_AT_MAX_CAPACITY = \"DATASET_AT_MAX_CAPACITY\"\n    EMBEDDING_UNKNOWN = \"EMBEDDING_UNKNOWN\"\n    EMBEDDING_NAME_EXISTS = \"EMBEDDING_NAME_EXISTS\"\n    EMBEDDING_INVALID = \"EMBEDDING_INVALID\"\n    EMBEDDING_NOT_READY = \"EMBEDDING_NOT_READY\"\n    EMBEDDING_ROW_COUNT_UNKNOWN = \"EMBEDDING_ROW_COUNT_UNKNOWN\"\n    EMBEDDING_ROW_COUNT_INVALID = \"EMBEDDING_ROW_COUNT_INVALID\"\n    EMBEDDING_2D_UNKNOWN = \"EMBEDDING_2D_UNKNOWN\"\n    TAG_UNKNOWN = \"TAG_UNKNOWN\"\n    TAG_NAME_EXISTS = \"TAG_NAME_EXISTS\"\n    TAG_INITIAL_EXISTS = \"TAG_INITIAL_EXISTS\"\n    TAG_PREVTAG_NOT_OF_DATASET = \"TAG_PREVTAG_NOT_OF_DATASET\"\n    SAMPLE_UNKNOWN = \"SAMPLE_UNKNOWN\"\n    SAMPLE_THUMBNAME_UNKNOWN = \"SAMPLE_THUMBNAME_UNKNOWN\"\n    SCORE_UNKNOWN = \"SCORE_UNKNOWN\"\n    DOCKER_RUN_UNKNOWN = \"DOCKER_RUN_UNKNOWN\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n    }\n\n    attribute_map = {\n    }\n\n    def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"ApiErrorCode - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(ApiErrorCode, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, ApiErrorCode):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, ApiErrorCode):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"ApiErrorCode - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(ApiErrorCode, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, ApiErrorCode):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, ApiErrorCode):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class ApiErrorResponse(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'code': 'ApiErrorCode',\n        'message': 'str'\n    }\n\n    attribute_map = {\n        'code': 'code',\n        'message': 'message'\n    }\n\n    def __init__(self, code=None, message=None, _configuration=None):  # noqa: E501\n        \"\"\"ApiErrorResponse - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._code = None\n        self._message = None\n        self.discriminator = None\n\n        self.code = code\n        self.message = message\n\n    @property\n    def code(self):\n        \"\"\"Gets the code of this ApiErrorResponse.  # noqa: E501\n\n\n        :return: The code of this ApiErrorResponse.  # noqa: E501\n        :rtype: ApiErrorCode\n        \"\"\"\n        return self._code\n\n    @code.setter\n    def code(self, code):\n        \"\"\"Sets the code of this ApiErrorResponse.\n\n\n        :param code: The code of this ApiErrorResponse.  # noqa: E501\n        :type: ApiErrorCode\n        \"\"\"\n        if self._configuration.client_side_validation and code is None:\n            raise ValueError(\"Invalid value for `code`, must not be `None`\")  # noqa: E501\n\n        self._code = code\n\n    @property\n    def message(self):\n        \"\"\"Gets the message of this ApiErrorResponse.  # noqa: E501\n\n\n        :return: The message of this ApiErrorResponse.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._message\n\n    @message.setter\n    def message(self, message):\n        \"\"\"Sets the message of this ApiErrorResponse.\n\n\n        :param message: The message of this ApiErrorResponse.  # noqa: E501\n        :type: str\n        \"\"\"\n        if self._configuration.client_side_validation and message is None:\n            raise ValueError(\"Invalid value for `message`, must not be `None`\")  # noqa: E501\n\n        self._message = message\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(ApiErrorResponse, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, ApiErrorResponse):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, ApiErrorResponse):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, code=None, message=None, _configuration=None):  # noqa: E501\n        \"\"\"ApiErrorResponse - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._code = None\n        self._message = None\n        self.discriminator = None\n\n        self.code = code\n        self.message = message",
  "def code(self):\n        \"\"\"Gets the code of this ApiErrorResponse.  # noqa: E501\n\n\n        :return: The code of this ApiErrorResponse.  # noqa: E501\n        :rtype: ApiErrorCode\n        \"\"\"\n        return self._code",
  "def code(self, code):\n        \"\"\"Sets the code of this ApiErrorResponse.\n\n\n        :param code: The code of this ApiErrorResponse.  # noqa: E501\n        :type: ApiErrorCode\n        \"\"\"\n        if self._configuration.client_side_validation and code is None:\n            raise ValueError(\"Invalid value for `code`, must not be `None`\")  # noqa: E501\n\n        self._code = code",
  "def message(self):\n        \"\"\"Gets the message of this ApiErrorResponse.  # noqa: E501\n\n\n        :return: The message of this ApiErrorResponse.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._message",
  "def message(self, message):\n        \"\"\"Sets the message of this ApiErrorResponse.\n\n\n        :param message: The message of this ApiErrorResponse.  # noqa: E501\n        :type: str\n        \"\"\"\n        if self._configuration.client_side_validation and message is None:\n            raise ValueError(\"Invalid value for `message`, must not be `None`\")  # noqa: E501\n\n        self._message = message",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(ApiErrorResponse, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, ApiErrorResponse):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, ApiErrorResponse):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class Embedding2dCreateRequest(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'name': 'str',\n        'dimensionality_reduction_method': 'DimensionalityReductionMethod',\n        'coordinates_dimension1': 'Embedding2dCoordinates',\n        'coordinates_dimension2': 'Embedding2dCoordinates'\n    }\n\n    attribute_map = {\n        'name': 'name',\n        'dimensionality_reduction_method': 'dimensionalityReductionMethod',\n        'coordinates_dimension1': 'coordinatesDimension1',\n        'coordinates_dimension2': 'coordinatesDimension2'\n    }\n\n    def __init__(self, name=None, dimensionality_reduction_method=None, coordinates_dimension1=None, coordinates_dimension2=None, _configuration=None):  # noqa: E501\n        \"\"\"Embedding2dCreateRequest - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._name = None\n        self._dimensionality_reduction_method = None\n        self._coordinates_dimension1 = None\n        self._coordinates_dimension2 = None\n        self.discriminator = None\n\n        self.name = name\n        self.dimensionality_reduction_method = dimensionality_reduction_method\n        self.coordinates_dimension1 = coordinates_dimension1\n        self.coordinates_dimension2 = coordinates_dimension2\n\n    @property\n    def name(self):\n        \"\"\"Gets the name of this Embedding2dCreateRequest.  # noqa: E501\n\n        Name of the 2d embedding (default is embedding name + __2d)  # noqa: E501\n\n        :return: The name of this Embedding2dCreateRequest.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        \"\"\"Sets the name of this Embedding2dCreateRequest.\n\n        Name of the 2d embedding (default is embedding name + __2d)  # noqa: E501\n\n        :param name: The name of this Embedding2dCreateRequest.  # noqa: E501\n        :type: str\n        \"\"\"\n        if self._configuration.client_side_validation and name is None:\n            raise ValueError(\"Invalid value for `name`, must not be `None`\")  # noqa: E501\n\n        self._name = name\n\n    @property\n    def dimensionality_reduction_method(self):\n        \"\"\"Gets the dimensionality_reduction_method of this Embedding2dCreateRequest.  # noqa: E501\n\n\n        :return: The dimensionality_reduction_method of this Embedding2dCreateRequest.  # noqa: E501\n        :rtype: DimensionalityReductionMethod\n        \"\"\"\n        return self._dimensionality_reduction_method\n\n    @dimensionality_reduction_method.setter\n    def dimensionality_reduction_method(self, dimensionality_reduction_method):\n        \"\"\"Sets the dimensionality_reduction_method of this Embedding2dCreateRequest.\n\n\n        :param dimensionality_reduction_method: The dimensionality_reduction_method of this Embedding2dCreateRequest.  # noqa: E501\n        :type: DimensionalityReductionMethod\n        \"\"\"\n        if self._configuration.client_side_validation and dimensionality_reduction_method is None:\n            raise ValueError(\"Invalid value for `dimensionality_reduction_method`, must not be `None`\")  # noqa: E501\n\n        self._dimensionality_reduction_method = dimensionality_reduction_method\n\n    @property\n    def coordinates_dimension1(self):\n        \"\"\"Gets the coordinates_dimension1 of this Embedding2dCreateRequest.  # noqa: E501\n\n\n        :return: The coordinates_dimension1 of this Embedding2dCreateRequest.  # noqa: E501\n        :rtype: Embedding2dCoordinates\n        \"\"\"\n        return self._coordinates_dimension1\n\n    @coordinates_dimension1.setter\n    def coordinates_dimension1(self, coordinates_dimension1):\n        \"\"\"Sets the coordinates_dimension1 of this Embedding2dCreateRequest.\n\n\n        :param coordinates_dimension1: The coordinates_dimension1 of this Embedding2dCreateRequest.  # noqa: E501\n        :type: Embedding2dCoordinates\n        \"\"\"\n        if self._configuration.client_side_validation and coordinates_dimension1 is None:\n            raise ValueError(\"Invalid value for `coordinates_dimension1`, must not be `None`\")  # noqa: E501\n\n        self._coordinates_dimension1 = coordinates_dimension1\n\n    @property\n    def coordinates_dimension2(self):\n        \"\"\"Gets the coordinates_dimension2 of this Embedding2dCreateRequest.  # noqa: E501\n\n\n        :return: The coordinates_dimension2 of this Embedding2dCreateRequest.  # noqa: E501\n        :rtype: Embedding2dCoordinates\n        \"\"\"\n        return self._coordinates_dimension2\n\n    @coordinates_dimension2.setter\n    def coordinates_dimension2(self, coordinates_dimension2):\n        \"\"\"Sets the coordinates_dimension2 of this Embedding2dCreateRequest.\n\n\n        :param coordinates_dimension2: The coordinates_dimension2 of this Embedding2dCreateRequest.  # noqa: E501\n        :type: Embedding2dCoordinates\n        \"\"\"\n        if self._configuration.client_side_validation and coordinates_dimension2 is None:\n            raise ValueError(\"Invalid value for `coordinates_dimension2`, must not be `None`\")  # noqa: E501\n\n        self._coordinates_dimension2 = coordinates_dimension2\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(Embedding2dCreateRequest, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, Embedding2dCreateRequest):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, Embedding2dCreateRequest):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, name=None, dimensionality_reduction_method=None, coordinates_dimension1=None, coordinates_dimension2=None, _configuration=None):  # noqa: E501\n        \"\"\"Embedding2dCreateRequest - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._name = None\n        self._dimensionality_reduction_method = None\n        self._coordinates_dimension1 = None\n        self._coordinates_dimension2 = None\n        self.discriminator = None\n\n        self.name = name\n        self.dimensionality_reduction_method = dimensionality_reduction_method\n        self.coordinates_dimension1 = coordinates_dimension1\n        self.coordinates_dimension2 = coordinates_dimension2",
  "def name(self):\n        \"\"\"Gets the name of this Embedding2dCreateRequest.  # noqa: E501\n\n        Name of the 2d embedding (default is embedding name + __2d)  # noqa: E501\n\n        :return: The name of this Embedding2dCreateRequest.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._name",
  "def name(self, name):\n        \"\"\"Sets the name of this Embedding2dCreateRequest.\n\n        Name of the 2d embedding (default is embedding name + __2d)  # noqa: E501\n\n        :param name: The name of this Embedding2dCreateRequest.  # noqa: E501\n        :type: str\n        \"\"\"\n        if self._configuration.client_side_validation and name is None:\n            raise ValueError(\"Invalid value for `name`, must not be `None`\")  # noqa: E501\n\n        self._name = name",
  "def dimensionality_reduction_method(self):\n        \"\"\"Gets the dimensionality_reduction_method of this Embedding2dCreateRequest.  # noqa: E501\n\n\n        :return: The dimensionality_reduction_method of this Embedding2dCreateRequest.  # noqa: E501\n        :rtype: DimensionalityReductionMethod\n        \"\"\"\n        return self._dimensionality_reduction_method",
  "def dimensionality_reduction_method(self, dimensionality_reduction_method):\n        \"\"\"Sets the dimensionality_reduction_method of this Embedding2dCreateRequest.\n\n\n        :param dimensionality_reduction_method: The dimensionality_reduction_method of this Embedding2dCreateRequest.  # noqa: E501\n        :type: DimensionalityReductionMethod\n        \"\"\"\n        if self._configuration.client_side_validation and dimensionality_reduction_method is None:\n            raise ValueError(\"Invalid value for `dimensionality_reduction_method`, must not be `None`\")  # noqa: E501\n\n        self._dimensionality_reduction_method = dimensionality_reduction_method",
  "def coordinates_dimension1(self):\n        \"\"\"Gets the coordinates_dimension1 of this Embedding2dCreateRequest.  # noqa: E501\n\n\n        :return: The coordinates_dimension1 of this Embedding2dCreateRequest.  # noqa: E501\n        :rtype: Embedding2dCoordinates\n        \"\"\"\n        return self._coordinates_dimension1",
  "def coordinates_dimension1(self, coordinates_dimension1):\n        \"\"\"Sets the coordinates_dimension1 of this Embedding2dCreateRequest.\n\n\n        :param coordinates_dimension1: The coordinates_dimension1 of this Embedding2dCreateRequest.  # noqa: E501\n        :type: Embedding2dCoordinates\n        \"\"\"\n        if self._configuration.client_side_validation and coordinates_dimension1 is None:\n            raise ValueError(\"Invalid value for `coordinates_dimension1`, must not be `None`\")  # noqa: E501\n\n        self._coordinates_dimension1 = coordinates_dimension1",
  "def coordinates_dimension2(self):\n        \"\"\"Gets the coordinates_dimension2 of this Embedding2dCreateRequest.  # noqa: E501\n\n\n        :return: The coordinates_dimension2 of this Embedding2dCreateRequest.  # noqa: E501\n        :rtype: Embedding2dCoordinates\n        \"\"\"\n        return self._coordinates_dimension2",
  "def coordinates_dimension2(self, coordinates_dimension2):\n        \"\"\"Sets the coordinates_dimension2 of this Embedding2dCreateRequest.\n\n\n        :param coordinates_dimension2: The coordinates_dimension2 of this Embedding2dCreateRequest.  # noqa: E501\n        :type: Embedding2dCoordinates\n        \"\"\"\n        if self._configuration.client_side_validation and coordinates_dimension2 is None:\n            raise ValueError(\"Invalid value for `coordinates_dimension2`, must not be `None`\")  # noqa: E501\n\n        self._coordinates_dimension2 = coordinates_dimension2",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(Embedding2dCreateRequest, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, Embedding2dCreateRequest):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, Embedding2dCreateRequest):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class CreateEntityResponse(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'id': 'MongoObjectID'\n    }\n\n    attribute_map = {\n        'id': 'id'\n    }\n\n    def __init__(self, id=None, _configuration=None):  # noqa: E501\n        \"\"\"CreateEntityResponse - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._id = None\n        self.discriminator = None\n\n        self.id = id\n\n    @property\n    def id(self):\n        \"\"\"Gets the id of this CreateEntityResponse.  # noqa: E501\n\n\n        :return: The id of this CreateEntityResponse.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._id\n\n    @id.setter\n    def id(self, id):\n        \"\"\"Sets the id of this CreateEntityResponse.\n\n\n        :param id: The id of this CreateEntityResponse.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and id is None:\n            raise ValueError(\"Invalid value for `id`, must not be `None`\")  # noqa: E501\n\n        self._id = id\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(CreateEntityResponse, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, CreateEntityResponse):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, CreateEntityResponse):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, id=None, _configuration=None):  # noqa: E501\n        \"\"\"CreateEntityResponse - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._id = None\n        self.discriminator = None\n\n        self.id = id",
  "def id(self):\n        \"\"\"Gets the id of this CreateEntityResponse.  # noqa: E501\n\n\n        :return: The id of this CreateEntityResponse.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._id",
  "def id(self, id):\n        \"\"\"Sets the id of this CreateEntityResponse.\n\n\n        :param id: The id of this CreateEntityResponse.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and id is None:\n            raise ValueError(\"Invalid value for `id`, must not be `None`\")  # noqa: E501\n\n        self._id = id",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(CreateEntityResponse, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, CreateEntityResponse):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, CreateEntityResponse):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class TagCreateRequest(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'name': 'TagName',\n        'prev_tag_id': 'MongoObjectID',\n        'bit_mask_data': 'TagBitMaskData',\n        'tot_size': 'int',\n        'creator': 'TagCreator',\n        'changes': 'TagChangeData'\n    }\n\n    attribute_map = {\n        'name': 'name',\n        'prev_tag_id': 'prevTagId',\n        'bit_mask_data': 'bitMaskData',\n        'tot_size': 'totSize',\n        'creator': 'creator',\n        'changes': 'changes'\n    }\n\n    def __init__(self, name=None, prev_tag_id=None, bit_mask_data=None, tot_size=None, creator=None, changes=None, _configuration=None):  # noqa: E501\n        \"\"\"TagCreateRequest - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._name = None\n        self._prev_tag_id = None\n        self._bit_mask_data = None\n        self._tot_size = None\n        self._creator = None\n        self._changes = None\n        self.discriminator = None\n\n        self.name = name\n        self.prev_tag_id = prev_tag_id\n        self.bit_mask_data = bit_mask_data\n        self.tot_size = tot_size\n        if creator is not None:\n            self.creator = creator\n        if changes is not None:\n            self.changes = changes\n\n    @property\n    def name(self):\n        \"\"\"Gets the name of this TagCreateRequest.  # noqa: E501\n\n\n        :return: The name of this TagCreateRequest.  # noqa: E501\n        :rtype: TagName\n        \"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        \"\"\"Sets the name of this TagCreateRequest.\n\n\n        :param name: The name of this TagCreateRequest.  # noqa: E501\n        :type: TagName\n        \"\"\"\n        if self._configuration.client_side_validation and name is None:\n            raise ValueError(\"Invalid value for `name`, must not be `None`\")  # noqa: E501\n\n        self._name = name\n\n    @property\n    def prev_tag_id(self):\n        \"\"\"Gets the prev_tag_id of this TagCreateRequest.  # noqa: E501\n\n\n        :return: The prev_tag_id of this TagCreateRequest.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._prev_tag_id\n\n    @prev_tag_id.setter\n    def prev_tag_id(self, prev_tag_id):\n        \"\"\"Sets the prev_tag_id of this TagCreateRequest.\n\n\n        :param prev_tag_id: The prev_tag_id of this TagCreateRequest.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and prev_tag_id is None:\n            raise ValueError(\"Invalid value for `prev_tag_id`, must not be `None`\")  # noqa: E501\n\n        self._prev_tag_id = prev_tag_id\n\n    @property\n    def bit_mask_data(self):\n        \"\"\"Gets the bit_mask_data of this TagCreateRequest.  # noqa: E501\n\n\n        :return: The bit_mask_data of this TagCreateRequest.  # noqa: E501\n        :rtype: TagBitMaskData\n        \"\"\"\n        return self._bit_mask_data\n\n    @bit_mask_data.setter\n    def bit_mask_data(self, bit_mask_data):\n        \"\"\"Sets the bit_mask_data of this TagCreateRequest.\n\n\n        :param bit_mask_data: The bit_mask_data of this TagCreateRequest.  # noqa: E501\n        :type: TagBitMaskData\n        \"\"\"\n        if self._configuration.client_side_validation and bit_mask_data is None:\n            raise ValueError(\"Invalid value for `bit_mask_data`, must not be `None`\")  # noqa: E501\n\n        self._bit_mask_data = bit_mask_data\n\n    @property\n    def tot_size(self):\n        \"\"\"Gets the tot_size of this TagCreateRequest.  # noqa: E501\n\n\n        :return: The tot_size of this TagCreateRequest.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._tot_size\n\n    @tot_size.setter\n    def tot_size(self, tot_size):\n        \"\"\"Sets the tot_size of this TagCreateRequest.\n\n\n        :param tot_size: The tot_size of this TagCreateRequest.  # noqa: E501\n        :type: int\n        \"\"\"\n        if self._configuration.client_side_validation and tot_size is None:\n            raise ValueError(\"Invalid value for `tot_size`, must not be `None`\")  # noqa: E501\n\n        self._tot_size = tot_size\n\n    @property\n    def creator(self):\n        \"\"\"Gets the creator of this TagCreateRequest.  # noqa: E501\n\n\n        :return: The creator of this TagCreateRequest.  # noqa: E501\n        :rtype: TagCreator\n        \"\"\"\n        return self._creator\n\n    @creator.setter\n    def creator(self, creator):\n        \"\"\"Sets the creator of this TagCreateRequest.\n\n\n        :param creator: The creator of this TagCreateRequest.  # noqa: E501\n        :type: TagCreator\n        \"\"\"\n\n        self._creator = creator\n\n    @property\n    def changes(self):\n        \"\"\"Gets the changes of this TagCreateRequest.  # noqa: E501\n\n\n        :return: The changes of this TagCreateRequest.  # noqa: E501\n        :rtype: TagChangeData\n        \"\"\"\n        return self._changes\n\n    @changes.setter\n    def changes(self, changes):\n        \"\"\"Sets the changes of this TagCreateRequest.\n\n\n        :param changes: The changes of this TagCreateRequest.  # noqa: E501\n        :type: TagChangeData\n        \"\"\"\n\n        self._changes = changes\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(TagCreateRequest, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, TagCreateRequest):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, TagCreateRequest):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, name=None, prev_tag_id=None, bit_mask_data=None, tot_size=None, creator=None, changes=None, _configuration=None):  # noqa: E501\n        \"\"\"TagCreateRequest - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._name = None\n        self._prev_tag_id = None\n        self._bit_mask_data = None\n        self._tot_size = None\n        self._creator = None\n        self._changes = None\n        self.discriminator = None\n\n        self.name = name\n        self.prev_tag_id = prev_tag_id\n        self.bit_mask_data = bit_mask_data\n        self.tot_size = tot_size\n        if creator is not None:\n            self.creator = creator\n        if changes is not None:\n            self.changes = changes",
  "def name(self):\n        \"\"\"Gets the name of this TagCreateRequest.  # noqa: E501\n\n\n        :return: The name of this TagCreateRequest.  # noqa: E501\n        :rtype: TagName\n        \"\"\"\n        return self._name",
  "def name(self, name):\n        \"\"\"Sets the name of this TagCreateRequest.\n\n\n        :param name: The name of this TagCreateRequest.  # noqa: E501\n        :type: TagName\n        \"\"\"\n        if self._configuration.client_side_validation and name is None:\n            raise ValueError(\"Invalid value for `name`, must not be `None`\")  # noqa: E501\n\n        self._name = name",
  "def prev_tag_id(self):\n        \"\"\"Gets the prev_tag_id of this TagCreateRequest.  # noqa: E501\n\n\n        :return: The prev_tag_id of this TagCreateRequest.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._prev_tag_id",
  "def prev_tag_id(self, prev_tag_id):\n        \"\"\"Sets the prev_tag_id of this TagCreateRequest.\n\n\n        :param prev_tag_id: The prev_tag_id of this TagCreateRequest.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and prev_tag_id is None:\n            raise ValueError(\"Invalid value for `prev_tag_id`, must not be `None`\")  # noqa: E501\n\n        self._prev_tag_id = prev_tag_id",
  "def bit_mask_data(self):\n        \"\"\"Gets the bit_mask_data of this TagCreateRequest.  # noqa: E501\n\n\n        :return: The bit_mask_data of this TagCreateRequest.  # noqa: E501\n        :rtype: TagBitMaskData\n        \"\"\"\n        return self._bit_mask_data",
  "def bit_mask_data(self, bit_mask_data):\n        \"\"\"Sets the bit_mask_data of this TagCreateRequest.\n\n\n        :param bit_mask_data: The bit_mask_data of this TagCreateRequest.  # noqa: E501\n        :type: TagBitMaskData\n        \"\"\"\n        if self._configuration.client_side_validation and bit_mask_data is None:\n            raise ValueError(\"Invalid value for `bit_mask_data`, must not be `None`\")  # noqa: E501\n\n        self._bit_mask_data = bit_mask_data",
  "def tot_size(self):\n        \"\"\"Gets the tot_size of this TagCreateRequest.  # noqa: E501\n\n\n        :return: The tot_size of this TagCreateRequest.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._tot_size",
  "def tot_size(self, tot_size):\n        \"\"\"Sets the tot_size of this TagCreateRequest.\n\n\n        :param tot_size: The tot_size of this TagCreateRequest.  # noqa: E501\n        :type: int\n        \"\"\"\n        if self._configuration.client_side_validation and tot_size is None:\n            raise ValueError(\"Invalid value for `tot_size`, must not be `None`\")  # noqa: E501\n\n        self._tot_size = tot_size",
  "def creator(self):\n        \"\"\"Gets the creator of this TagCreateRequest.  # noqa: E501\n\n\n        :return: The creator of this TagCreateRequest.  # noqa: E501\n        :rtype: TagCreator\n        \"\"\"\n        return self._creator",
  "def creator(self, creator):\n        \"\"\"Sets the creator of this TagCreateRequest.\n\n\n        :param creator: The creator of this TagCreateRequest.  # noqa: E501\n        :type: TagCreator\n        \"\"\"\n\n        self._creator = creator",
  "def changes(self):\n        \"\"\"Gets the changes of this TagCreateRequest.  # noqa: E501\n\n\n        :return: The changes of this TagCreateRequest.  # noqa: E501\n        :rtype: TagChangeData\n        \"\"\"\n        return self._changes",
  "def changes(self, changes):\n        \"\"\"Sets the changes of this TagCreateRequest.\n\n\n        :param changes: The changes of this TagCreateRequest.  # noqa: E501\n        :type: TagChangeData\n        \"\"\"\n\n        self._changes = changes",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(TagCreateRequest, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, TagCreateRequest):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, TagCreateRequest):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class DatasetUpdateRequest(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'name': 'DatasetName'\n    }\n\n    attribute_map = {\n        'name': 'name'\n    }\n\n    def __init__(self, name=None, _configuration=None):  # noqa: E501\n        \"\"\"DatasetUpdateRequest - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._name = None\n        self.discriminator = None\n\n        self.name = name\n\n    @property\n    def name(self):\n        \"\"\"Gets the name of this DatasetUpdateRequest.  # noqa: E501\n\n\n        :return: The name of this DatasetUpdateRequest.  # noqa: E501\n        :rtype: DatasetName\n        \"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        \"\"\"Sets the name of this DatasetUpdateRequest.\n\n\n        :param name: The name of this DatasetUpdateRequest.  # noqa: E501\n        :type: DatasetName\n        \"\"\"\n        if self._configuration.client_side_validation and name is None:\n            raise ValueError(\"Invalid value for `name`, must not be `None`\")  # noqa: E501\n\n        self._name = name\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(DatasetUpdateRequest, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, DatasetUpdateRequest):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, DatasetUpdateRequest):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, name=None, _configuration=None):  # noqa: E501\n        \"\"\"DatasetUpdateRequest - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._name = None\n        self.discriminator = None\n\n        self.name = name",
  "def name(self):\n        \"\"\"Gets the name of this DatasetUpdateRequest.  # noqa: E501\n\n\n        :return: The name of this DatasetUpdateRequest.  # noqa: E501\n        :rtype: DatasetName\n        \"\"\"\n        return self._name",
  "def name(self, name):\n        \"\"\"Sets the name of this DatasetUpdateRequest.\n\n\n        :param name: The name of this DatasetUpdateRequest.  # noqa: E501\n        :type: DatasetName\n        \"\"\"\n        if self._configuration.client_side_validation and name is None:\n            raise ValueError(\"Invalid value for `name`, must not be `None`\")  # noqa: E501\n\n        self._name = name",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(DatasetUpdateRequest, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, DatasetUpdateRequest):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, DatasetUpdateRequest):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class DatasetDataEnriched(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'id': 'MongoObjectID',\n        'name': 'DatasetName',\n        'type': 'DatasetType',\n        'img_type': 'ImageType',\n        'n_samples': 'int',\n        'size_in_bytes': 'int',\n        'created_at': 'Timestamp',\n        'last_modified_at': 'Timestamp',\n        'samples': 'list[MongoObjectID]',\n        'n_tags': 'int',\n        'n_embeddings': 'int'\n    }\n\n    attribute_map = {\n        'id': 'id',\n        'name': 'name',\n        'type': 'type',\n        'img_type': 'imgType',\n        'n_samples': 'nSamples',\n        'size_in_bytes': 'sizeInBytes',\n        'created_at': 'createdAt',\n        'last_modified_at': 'lastModifiedAt',\n        'samples': 'samples',\n        'n_tags': 'nTags',\n        'n_embeddings': 'nEmbeddings'\n    }\n\n    def __init__(self, id=None, name=None, type=None, img_type=None, n_samples=None, size_in_bytes=None, created_at=None, last_modified_at=None, samples=None, n_tags=None, n_embeddings=None, _configuration=None):  # noqa: E501\n        \"\"\"DatasetDataEnriched - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._id = None\n        self._name = None\n        self._type = None\n        self._img_type = None\n        self._n_samples = None\n        self._size_in_bytes = None\n        self._created_at = None\n        self._last_modified_at = None\n        self._samples = None\n        self._n_tags = None\n        self._n_embeddings = None\n        self.discriminator = None\n\n        self.id = id\n        self.name = name\n        self.type = type\n        if img_type is not None:\n            self.img_type = img_type\n        self.n_samples = n_samples\n        self.size_in_bytes = size_in_bytes\n        self.created_at = created_at\n        self.last_modified_at = last_modified_at\n        self.samples = samples\n        self.n_tags = n_tags\n        self.n_embeddings = n_embeddings\n\n    @property\n    def id(self):\n        \"\"\"Gets the id of this DatasetDataEnriched.  # noqa: E501\n\n\n        :return: The id of this DatasetDataEnriched.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._id\n\n    @id.setter\n    def id(self, id):\n        \"\"\"Sets the id of this DatasetDataEnriched.\n\n\n        :param id: The id of this DatasetDataEnriched.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and id is None:\n            raise ValueError(\"Invalid value for `id`, must not be `None`\")  # noqa: E501\n\n        self._id = id\n\n    @property\n    def name(self):\n        \"\"\"Gets the name of this DatasetDataEnriched.  # noqa: E501\n\n\n        :return: The name of this DatasetDataEnriched.  # noqa: E501\n        :rtype: DatasetName\n        \"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        \"\"\"Sets the name of this DatasetDataEnriched.\n\n\n        :param name: The name of this DatasetDataEnriched.  # noqa: E501\n        :type: DatasetName\n        \"\"\"\n        if self._configuration.client_side_validation and name is None:\n            raise ValueError(\"Invalid value for `name`, must not be `None`\")  # noqa: E501\n\n        self._name = name\n\n    @property\n    def type(self):\n        \"\"\"Gets the type of this DatasetDataEnriched.  # noqa: E501\n\n\n        :return: The type of this DatasetDataEnriched.  # noqa: E501\n        :rtype: DatasetType\n        \"\"\"\n        return self._type\n\n    @type.setter\n    def type(self, type):\n        \"\"\"Sets the type of this DatasetDataEnriched.\n\n\n        :param type: The type of this DatasetDataEnriched.  # noqa: E501\n        :type: DatasetType\n        \"\"\"\n        if self._configuration.client_side_validation and type is None:\n            raise ValueError(\"Invalid value for `type`, must not be `None`\")  # noqa: E501\n\n        self._type = type\n\n    @property\n    def img_type(self):\n        \"\"\"Gets the img_type of this DatasetDataEnriched.  # noqa: E501\n\n\n        :return: The img_type of this DatasetDataEnriched.  # noqa: E501\n        :rtype: ImageType\n        \"\"\"\n        return self._img_type\n\n    @img_type.setter\n    def img_type(self, img_type):\n        \"\"\"Sets the img_type of this DatasetDataEnriched.\n\n\n        :param img_type: The img_type of this DatasetDataEnriched.  # noqa: E501\n        :type: ImageType\n        \"\"\"\n\n        self._img_type = img_type\n\n    @property\n    def n_samples(self):\n        \"\"\"Gets the n_samples of this DatasetDataEnriched.  # noqa: E501\n\n\n        :return: The n_samples of this DatasetDataEnriched.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._n_samples\n\n    @n_samples.setter\n    def n_samples(self, n_samples):\n        \"\"\"Sets the n_samples of this DatasetDataEnriched.\n\n\n        :param n_samples: The n_samples of this DatasetDataEnriched.  # noqa: E501\n        :type: int\n        \"\"\"\n        if self._configuration.client_side_validation and n_samples is None:\n            raise ValueError(\"Invalid value for `n_samples`, must not be `None`\")  # noqa: E501\n\n        self._n_samples = n_samples\n\n    @property\n    def size_in_bytes(self):\n        \"\"\"Gets the size_in_bytes of this DatasetDataEnriched.  # noqa: E501\n\n\n        :return: The size_in_bytes of this DatasetDataEnriched.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._size_in_bytes\n\n    @size_in_bytes.setter\n    def size_in_bytes(self, size_in_bytes):\n        \"\"\"Sets the size_in_bytes of this DatasetDataEnriched.\n\n\n        :param size_in_bytes: The size_in_bytes of this DatasetDataEnriched.  # noqa: E501\n        :type: int\n        \"\"\"\n        if self._configuration.client_side_validation and size_in_bytes is None:\n            raise ValueError(\"Invalid value for `size_in_bytes`, must not be `None`\")  # noqa: E501\n\n        self._size_in_bytes = size_in_bytes\n\n    @property\n    def created_at(self):\n        \"\"\"Gets the created_at of this DatasetDataEnriched.  # noqa: E501\n\n\n        :return: The created_at of this DatasetDataEnriched.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._created_at\n\n    @created_at.setter\n    def created_at(self, created_at):\n        \"\"\"Sets the created_at of this DatasetDataEnriched.\n\n\n        :param created_at: The created_at of this DatasetDataEnriched.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n        if self._configuration.client_side_validation and created_at is None:\n            raise ValueError(\"Invalid value for `created_at`, must not be `None`\")  # noqa: E501\n\n        self._created_at = created_at\n\n    @property\n    def last_modified_at(self):\n        \"\"\"Gets the last_modified_at of this DatasetDataEnriched.  # noqa: E501\n\n\n        :return: The last_modified_at of this DatasetDataEnriched.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._last_modified_at\n\n    @last_modified_at.setter\n    def last_modified_at(self, last_modified_at):\n        \"\"\"Sets the last_modified_at of this DatasetDataEnriched.\n\n\n        :param last_modified_at: The last_modified_at of this DatasetDataEnriched.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n        if self._configuration.client_side_validation and last_modified_at is None:\n            raise ValueError(\"Invalid value for `last_modified_at`, must not be `None`\")  # noqa: E501\n\n        self._last_modified_at = last_modified_at\n\n    @property\n    def samples(self):\n        \"\"\"Gets the samples of this DatasetDataEnriched.  # noqa: E501\n\n\n        :return: The samples of this DatasetDataEnriched.  # noqa: E501\n        :rtype: list[MongoObjectID]\n        \"\"\"\n        return self._samples\n\n    @samples.setter\n    def samples(self, samples):\n        \"\"\"Sets the samples of this DatasetDataEnriched.\n\n\n        :param samples: The samples of this DatasetDataEnriched.  # noqa: E501\n        :type: list[MongoObjectID]\n        \"\"\"\n        if self._configuration.client_side_validation and samples is None:\n            raise ValueError(\"Invalid value for `samples`, must not be `None`\")  # noqa: E501\n\n        self._samples = samples\n\n    @property\n    def n_tags(self):\n        \"\"\"Gets the n_tags of this DatasetDataEnriched.  # noqa: E501\n\n\n        :return: The n_tags of this DatasetDataEnriched.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._n_tags\n\n    @n_tags.setter\n    def n_tags(self, n_tags):\n        \"\"\"Sets the n_tags of this DatasetDataEnriched.\n\n\n        :param n_tags: The n_tags of this DatasetDataEnriched.  # noqa: E501\n        :type: int\n        \"\"\"\n        if self._configuration.client_side_validation and n_tags is None:\n            raise ValueError(\"Invalid value for `n_tags`, must not be `None`\")  # noqa: E501\n\n        self._n_tags = n_tags\n\n    @property\n    def n_embeddings(self):\n        \"\"\"Gets the n_embeddings of this DatasetDataEnriched.  # noqa: E501\n\n\n        :return: The n_embeddings of this DatasetDataEnriched.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._n_embeddings\n\n    @n_embeddings.setter\n    def n_embeddings(self, n_embeddings):\n        \"\"\"Sets the n_embeddings of this DatasetDataEnriched.\n\n\n        :param n_embeddings: The n_embeddings of this DatasetDataEnriched.  # noqa: E501\n        :type: int\n        \"\"\"\n        if self._configuration.client_side_validation and n_embeddings is None:\n            raise ValueError(\"Invalid value for `n_embeddings`, must not be `None`\")  # noqa: E501\n\n        self._n_embeddings = n_embeddings\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(DatasetDataEnriched, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, DatasetDataEnriched):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, DatasetDataEnriched):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, id=None, name=None, type=None, img_type=None, n_samples=None, size_in_bytes=None, created_at=None, last_modified_at=None, samples=None, n_tags=None, n_embeddings=None, _configuration=None):  # noqa: E501\n        \"\"\"DatasetDataEnriched - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._id = None\n        self._name = None\n        self._type = None\n        self._img_type = None\n        self._n_samples = None\n        self._size_in_bytes = None\n        self._created_at = None\n        self._last_modified_at = None\n        self._samples = None\n        self._n_tags = None\n        self._n_embeddings = None\n        self.discriminator = None\n\n        self.id = id\n        self.name = name\n        self.type = type\n        if img_type is not None:\n            self.img_type = img_type\n        self.n_samples = n_samples\n        self.size_in_bytes = size_in_bytes\n        self.created_at = created_at\n        self.last_modified_at = last_modified_at\n        self.samples = samples\n        self.n_tags = n_tags\n        self.n_embeddings = n_embeddings",
  "def id(self):\n        \"\"\"Gets the id of this DatasetDataEnriched.  # noqa: E501\n\n\n        :return: The id of this DatasetDataEnriched.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._id",
  "def id(self, id):\n        \"\"\"Sets the id of this DatasetDataEnriched.\n\n\n        :param id: The id of this DatasetDataEnriched.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and id is None:\n            raise ValueError(\"Invalid value for `id`, must not be `None`\")  # noqa: E501\n\n        self._id = id",
  "def name(self):\n        \"\"\"Gets the name of this DatasetDataEnriched.  # noqa: E501\n\n\n        :return: The name of this DatasetDataEnriched.  # noqa: E501\n        :rtype: DatasetName\n        \"\"\"\n        return self._name",
  "def name(self, name):\n        \"\"\"Sets the name of this DatasetDataEnriched.\n\n\n        :param name: The name of this DatasetDataEnriched.  # noqa: E501\n        :type: DatasetName\n        \"\"\"\n        if self._configuration.client_side_validation and name is None:\n            raise ValueError(\"Invalid value for `name`, must not be `None`\")  # noqa: E501\n\n        self._name = name",
  "def type(self):\n        \"\"\"Gets the type of this DatasetDataEnriched.  # noqa: E501\n\n\n        :return: The type of this DatasetDataEnriched.  # noqa: E501\n        :rtype: DatasetType\n        \"\"\"\n        return self._type",
  "def type(self, type):\n        \"\"\"Sets the type of this DatasetDataEnriched.\n\n\n        :param type: The type of this DatasetDataEnriched.  # noqa: E501\n        :type: DatasetType\n        \"\"\"\n        if self._configuration.client_side_validation and type is None:\n            raise ValueError(\"Invalid value for `type`, must not be `None`\")  # noqa: E501\n\n        self._type = type",
  "def img_type(self):\n        \"\"\"Gets the img_type of this DatasetDataEnriched.  # noqa: E501\n\n\n        :return: The img_type of this DatasetDataEnriched.  # noqa: E501\n        :rtype: ImageType\n        \"\"\"\n        return self._img_type",
  "def img_type(self, img_type):\n        \"\"\"Sets the img_type of this DatasetDataEnriched.\n\n\n        :param img_type: The img_type of this DatasetDataEnriched.  # noqa: E501\n        :type: ImageType\n        \"\"\"\n\n        self._img_type = img_type",
  "def n_samples(self):\n        \"\"\"Gets the n_samples of this DatasetDataEnriched.  # noqa: E501\n\n\n        :return: The n_samples of this DatasetDataEnriched.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._n_samples",
  "def n_samples(self, n_samples):\n        \"\"\"Sets the n_samples of this DatasetDataEnriched.\n\n\n        :param n_samples: The n_samples of this DatasetDataEnriched.  # noqa: E501\n        :type: int\n        \"\"\"\n        if self._configuration.client_side_validation and n_samples is None:\n            raise ValueError(\"Invalid value for `n_samples`, must not be `None`\")  # noqa: E501\n\n        self._n_samples = n_samples",
  "def size_in_bytes(self):\n        \"\"\"Gets the size_in_bytes of this DatasetDataEnriched.  # noqa: E501\n\n\n        :return: The size_in_bytes of this DatasetDataEnriched.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._size_in_bytes",
  "def size_in_bytes(self, size_in_bytes):\n        \"\"\"Sets the size_in_bytes of this DatasetDataEnriched.\n\n\n        :param size_in_bytes: The size_in_bytes of this DatasetDataEnriched.  # noqa: E501\n        :type: int\n        \"\"\"\n        if self._configuration.client_side_validation and size_in_bytes is None:\n            raise ValueError(\"Invalid value for `size_in_bytes`, must not be `None`\")  # noqa: E501\n\n        self._size_in_bytes = size_in_bytes",
  "def created_at(self):\n        \"\"\"Gets the created_at of this DatasetDataEnriched.  # noqa: E501\n\n\n        :return: The created_at of this DatasetDataEnriched.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._created_at",
  "def created_at(self, created_at):\n        \"\"\"Sets the created_at of this DatasetDataEnriched.\n\n\n        :param created_at: The created_at of this DatasetDataEnriched.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n        if self._configuration.client_side_validation and created_at is None:\n            raise ValueError(\"Invalid value for `created_at`, must not be `None`\")  # noqa: E501\n\n        self._created_at = created_at",
  "def last_modified_at(self):\n        \"\"\"Gets the last_modified_at of this DatasetDataEnriched.  # noqa: E501\n\n\n        :return: The last_modified_at of this DatasetDataEnriched.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._last_modified_at",
  "def last_modified_at(self, last_modified_at):\n        \"\"\"Sets the last_modified_at of this DatasetDataEnriched.\n\n\n        :param last_modified_at: The last_modified_at of this DatasetDataEnriched.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n        if self._configuration.client_side_validation and last_modified_at is None:\n            raise ValueError(\"Invalid value for `last_modified_at`, must not be `None`\")  # noqa: E501\n\n        self._last_modified_at = last_modified_at",
  "def samples(self):\n        \"\"\"Gets the samples of this DatasetDataEnriched.  # noqa: E501\n\n\n        :return: The samples of this DatasetDataEnriched.  # noqa: E501\n        :rtype: list[MongoObjectID]\n        \"\"\"\n        return self._samples",
  "def samples(self, samples):\n        \"\"\"Sets the samples of this DatasetDataEnriched.\n\n\n        :param samples: The samples of this DatasetDataEnriched.  # noqa: E501\n        :type: list[MongoObjectID]\n        \"\"\"\n        if self._configuration.client_side_validation and samples is None:\n            raise ValueError(\"Invalid value for `samples`, must not be `None`\")  # noqa: E501\n\n        self._samples = samples",
  "def n_tags(self):\n        \"\"\"Gets the n_tags of this DatasetDataEnriched.  # noqa: E501\n\n\n        :return: The n_tags of this DatasetDataEnriched.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._n_tags",
  "def n_tags(self, n_tags):\n        \"\"\"Sets the n_tags of this DatasetDataEnriched.\n\n\n        :param n_tags: The n_tags of this DatasetDataEnriched.  # noqa: E501\n        :type: int\n        \"\"\"\n        if self._configuration.client_side_validation and n_tags is None:\n            raise ValueError(\"Invalid value for `n_tags`, must not be `None`\")  # noqa: E501\n\n        self._n_tags = n_tags",
  "def n_embeddings(self):\n        \"\"\"Gets the n_embeddings of this DatasetDataEnriched.  # noqa: E501\n\n\n        :return: The n_embeddings of this DatasetDataEnriched.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._n_embeddings",
  "def n_embeddings(self, n_embeddings):\n        \"\"\"Sets the n_embeddings of this DatasetDataEnriched.\n\n\n        :param n_embeddings: The n_embeddings of this DatasetDataEnriched.  # noqa: E501\n        :type: int\n        \"\"\"\n        if self._configuration.client_side_validation and n_embeddings is None:\n            raise ValueError(\"Invalid value for `n_embeddings`, must not be `None`\")  # noqa: E501\n\n        self._n_embeddings = n_embeddings",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(DatasetDataEnriched, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, DatasetDataEnriched):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, DatasetDataEnriched):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class DatasetType(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    allowed enum values\n    \"\"\"\n    IMAGES = \"Images\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n    }\n\n    attribute_map = {\n    }\n\n    def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"DatasetType - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(DatasetType, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, DatasetType):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, DatasetType):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"DatasetType - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(DatasetType, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, DatasetType):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, DatasetType):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class AsyncTaskData(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'job_id': 'str'\n    }\n\n    attribute_map = {\n        'job_id': 'jobId'\n    }\n\n    def __init__(self, job_id=None, _configuration=None):  # noqa: E501\n        \"\"\"AsyncTaskData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._job_id = None\n        self.discriminator = None\n\n        self.job_id = job_id\n\n    @property\n    def job_id(self):\n        \"\"\"Gets the job_id of this AsyncTaskData.  # noqa: E501\n\n\n        :return: The job_id of this AsyncTaskData.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._job_id\n\n    @job_id.setter\n    def job_id(self, job_id):\n        \"\"\"Sets the job_id of this AsyncTaskData.\n\n\n        :param job_id: The job_id of this AsyncTaskData.  # noqa: E501\n        :type: str\n        \"\"\"\n        if self._configuration.client_side_validation and job_id is None:\n            raise ValueError(\"Invalid value for `job_id`, must not be `None`\")  # noqa: E501\n\n        self._job_id = job_id\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(AsyncTaskData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, AsyncTaskData):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, AsyncTaskData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, job_id=None, _configuration=None):  # noqa: E501\n        \"\"\"AsyncTaskData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._job_id = None\n        self.discriminator = None\n\n        self.job_id = job_id",
  "def job_id(self):\n        \"\"\"Gets the job_id of this AsyncTaskData.  # noqa: E501\n\n\n        :return: The job_id of this AsyncTaskData.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._job_id",
  "def job_id(self, job_id):\n        \"\"\"Sets the job_id of this AsyncTaskData.\n\n\n        :param job_id: The job_id of this AsyncTaskData.  # noqa: E501\n        :type: str\n        \"\"\"\n        if self._configuration.client_side_validation and job_id is None:\n            raise ValueError(\"Invalid value for `job_id`, must not be `None`\")  # noqa: E501\n\n        self._job_id = job_id",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(AsyncTaskData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, AsyncTaskData):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, AsyncTaskData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class TagCreator(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    allowed enum values\n    \"\"\"\n    UNKNOWN = \"UNKNOWN\"\n    USER_WEBAPP = \"USER_WEBAPP\"\n    USER_PIP = \"USER_PIP\"\n    SAMPLER_ACTIVE_LEARNING = \"SAMPLER_ACTIVE_LEARNING\"\n    SAMPLER_CORAL = \"SAMPLER_CORAL\"\n    SAMPLER_CORESET = \"SAMPLER_CORESET\"\n    SAMPLER_RANDOM = \"SAMPLER_RANDOM\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n    }\n\n    attribute_map = {\n    }\n\n    def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"TagCreator - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(TagCreator, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, TagCreator):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, TagCreator):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, _configuration=None):  # noqa: E501\n        \"\"\"TagCreator - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n        self.discriminator = None",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(TagCreator, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, TagCreator):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, TagCreator):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class JobStatusData(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'id': 'MongoObjectID',\n        'status': 'JobState',\n        'meta': 'JobStatusMeta',\n        'wait_time_till_next_poll': 'int',\n        'created_at': 'Timestamp',\n        'finished_at': 'Timestamp',\n        'error': 'str',\n        'result': 'JobStatusDataResult'\n    }\n\n    attribute_map = {\n        'id': 'id',\n        'status': 'status',\n        'meta': 'meta',\n        'wait_time_till_next_poll': 'waitTimeTillNextPoll',\n        'created_at': 'createdAt',\n        'finished_at': 'finishedAt',\n        'error': 'error',\n        'result': 'result'\n    }\n\n    def __init__(self, id=None, status=None, meta=None, wait_time_till_next_poll=None, created_at=None, finished_at=None, error=None, result=None, _configuration=None):  # noqa: E501\n        \"\"\"JobStatusData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._id = None\n        self._status = None\n        self._meta = None\n        self._wait_time_till_next_poll = None\n        self._created_at = None\n        self._finished_at = None\n        self._error = None\n        self._result = None\n        self.discriminator = None\n\n        self.id = id\n        self.status = status\n        if meta is not None:\n            self.meta = meta\n        self.wait_time_till_next_poll = wait_time_till_next_poll\n        self.created_at = created_at\n        if finished_at is not None:\n            self.finished_at = finished_at\n        if error is not None:\n            self.error = error\n        if result is not None:\n            self.result = result\n\n    @property\n    def id(self):\n        \"\"\"Gets the id of this JobStatusData.  # noqa: E501\n\n\n        :return: The id of this JobStatusData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._id\n\n    @id.setter\n    def id(self, id):\n        \"\"\"Sets the id of this JobStatusData.\n\n\n        :param id: The id of this JobStatusData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and id is None:\n            raise ValueError(\"Invalid value for `id`, must not be `None`\")  # noqa: E501\n\n        self._id = id\n\n    @property\n    def status(self):\n        \"\"\"Gets the status of this JobStatusData.  # noqa: E501\n\n\n        :return: The status of this JobStatusData.  # noqa: E501\n        :rtype: JobState\n        \"\"\"\n        return self._status\n\n    @status.setter\n    def status(self, status):\n        \"\"\"Sets the status of this JobStatusData.\n\n\n        :param status: The status of this JobStatusData.  # noqa: E501\n        :type: JobState\n        \"\"\"\n        if self._configuration.client_side_validation and status is None:\n            raise ValueError(\"Invalid value for `status`, must not be `None`\")  # noqa: E501\n\n        self._status = status\n\n    @property\n    def meta(self):\n        \"\"\"Gets the meta of this JobStatusData.  # noqa: E501\n\n\n        :return: The meta of this JobStatusData.  # noqa: E501\n        :rtype: JobStatusMeta\n        \"\"\"\n        return self._meta\n\n    @meta.setter\n    def meta(self, meta):\n        \"\"\"Sets the meta of this JobStatusData.\n\n\n        :param meta: The meta of this JobStatusData.  # noqa: E501\n        :type: JobStatusMeta\n        \"\"\"\n\n        self._meta = meta\n\n    @property\n    def wait_time_till_next_poll(self):\n        \"\"\"Gets the wait_time_till_next_poll of this JobStatusData.  # noqa: E501\n\n        The time in seconds the client should wait before doing the next poll.  # noqa: E501\n\n        :return: The wait_time_till_next_poll of this JobStatusData.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._wait_time_till_next_poll\n\n    @wait_time_till_next_poll.setter\n    def wait_time_till_next_poll(self, wait_time_till_next_poll):\n        \"\"\"Sets the wait_time_till_next_poll of this JobStatusData.\n\n        The time in seconds the client should wait before doing the next poll.  # noqa: E501\n\n        :param wait_time_till_next_poll: The wait_time_till_next_poll of this JobStatusData.  # noqa: E501\n        :type: int\n        \"\"\"\n        if self._configuration.client_side_validation and wait_time_till_next_poll is None:\n            raise ValueError(\"Invalid value for `wait_time_till_next_poll`, must not be `None`\")  # noqa: E501\n\n        self._wait_time_till_next_poll = wait_time_till_next_poll\n\n    @property\n    def created_at(self):\n        \"\"\"Gets the created_at of this JobStatusData.  # noqa: E501\n\n\n        :return: The created_at of this JobStatusData.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._created_at\n\n    @created_at.setter\n    def created_at(self, created_at):\n        \"\"\"Sets the created_at of this JobStatusData.\n\n\n        :param created_at: The created_at of this JobStatusData.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n        if self._configuration.client_side_validation and created_at is None:\n            raise ValueError(\"Invalid value for `created_at`, must not be `None`\")  # noqa: E501\n\n        self._created_at = created_at\n\n    @property\n    def finished_at(self):\n        \"\"\"Gets the finished_at of this JobStatusData.  # noqa: E501\n\n\n        :return: The finished_at of this JobStatusData.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._finished_at\n\n    @finished_at.setter\n    def finished_at(self, finished_at):\n        \"\"\"Sets the finished_at of this JobStatusData.\n\n\n        :param finished_at: The finished_at of this JobStatusData.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n\n        self._finished_at = finished_at\n\n    @property\n    def error(self):\n        \"\"\"Gets the error of this JobStatusData.  # noqa: E501\n\n\n        :return: The error of this JobStatusData.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._error\n\n    @error.setter\n    def error(self, error):\n        \"\"\"Sets the error of this JobStatusData.\n\n\n        :param error: The error of this JobStatusData.  # noqa: E501\n        :type: str\n        \"\"\"\n\n        self._error = error\n\n    @property\n    def result(self):\n        \"\"\"Gets the result of this JobStatusData.  # noqa: E501\n\n\n        :return: The result of this JobStatusData.  # noqa: E501\n        :rtype: JobStatusDataResult\n        \"\"\"\n        return self._result\n\n    @result.setter\n    def result(self, result):\n        \"\"\"Sets the result of this JobStatusData.\n\n\n        :param result: The result of this JobStatusData.  # noqa: E501\n        :type: JobStatusDataResult\n        \"\"\"\n\n        self._result = result\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(JobStatusData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, JobStatusData):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, JobStatusData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, id=None, status=None, meta=None, wait_time_till_next_poll=None, created_at=None, finished_at=None, error=None, result=None, _configuration=None):  # noqa: E501\n        \"\"\"JobStatusData - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._id = None\n        self._status = None\n        self._meta = None\n        self._wait_time_till_next_poll = None\n        self._created_at = None\n        self._finished_at = None\n        self._error = None\n        self._result = None\n        self.discriminator = None\n\n        self.id = id\n        self.status = status\n        if meta is not None:\n            self.meta = meta\n        self.wait_time_till_next_poll = wait_time_till_next_poll\n        self.created_at = created_at\n        if finished_at is not None:\n            self.finished_at = finished_at\n        if error is not None:\n            self.error = error\n        if result is not None:\n            self.result = result",
  "def id(self):\n        \"\"\"Gets the id of this JobStatusData.  # noqa: E501\n\n\n        :return: The id of this JobStatusData.  # noqa: E501\n        :rtype: MongoObjectID\n        \"\"\"\n        return self._id",
  "def id(self, id):\n        \"\"\"Sets the id of this JobStatusData.\n\n\n        :param id: The id of this JobStatusData.  # noqa: E501\n        :type: MongoObjectID\n        \"\"\"\n        if self._configuration.client_side_validation and id is None:\n            raise ValueError(\"Invalid value for `id`, must not be `None`\")  # noqa: E501\n\n        self._id = id",
  "def status(self):\n        \"\"\"Gets the status of this JobStatusData.  # noqa: E501\n\n\n        :return: The status of this JobStatusData.  # noqa: E501\n        :rtype: JobState\n        \"\"\"\n        return self._status",
  "def status(self, status):\n        \"\"\"Sets the status of this JobStatusData.\n\n\n        :param status: The status of this JobStatusData.  # noqa: E501\n        :type: JobState\n        \"\"\"\n        if self._configuration.client_side_validation and status is None:\n            raise ValueError(\"Invalid value for `status`, must not be `None`\")  # noqa: E501\n\n        self._status = status",
  "def meta(self):\n        \"\"\"Gets the meta of this JobStatusData.  # noqa: E501\n\n\n        :return: The meta of this JobStatusData.  # noqa: E501\n        :rtype: JobStatusMeta\n        \"\"\"\n        return self._meta",
  "def meta(self, meta):\n        \"\"\"Sets the meta of this JobStatusData.\n\n\n        :param meta: The meta of this JobStatusData.  # noqa: E501\n        :type: JobStatusMeta\n        \"\"\"\n\n        self._meta = meta",
  "def wait_time_till_next_poll(self):\n        \"\"\"Gets the wait_time_till_next_poll of this JobStatusData.  # noqa: E501\n\n        The time in seconds the client should wait before doing the next poll.  # noqa: E501\n\n        :return: The wait_time_till_next_poll of this JobStatusData.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._wait_time_till_next_poll",
  "def wait_time_till_next_poll(self, wait_time_till_next_poll):\n        \"\"\"Sets the wait_time_till_next_poll of this JobStatusData.\n\n        The time in seconds the client should wait before doing the next poll.  # noqa: E501\n\n        :param wait_time_till_next_poll: The wait_time_till_next_poll of this JobStatusData.  # noqa: E501\n        :type: int\n        \"\"\"\n        if self._configuration.client_side_validation and wait_time_till_next_poll is None:\n            raise ValueError(\"Invalid value for `wait_time_till_next_poll`, must not be `None`\")  # noqa: E501\n\n        self._wait_time_till_next_poll = wait_time_till_next_poll",
  "def created_at(self):\n        \"\"\"Gets the created_at of this JobStatusData.  # noqa: E501\n\n\n        :return: The created_at of this JobStatusData.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._created_at",
  "def created_at(self, created_at):\n        \"\"\"Sets the created_at of this JobStatusData.\n\n\n        :param created_at: The created_at of this JobStatusData.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n        if self._configuration.client_side_validation and created_at is None:\n            raise ValueError(\"Invalid value for `created_at`, must not be `None`\")  # noqa: E501\n\n        self._created_at = created_at",
  "def finished_at(self):\n        \"\"\"Gets the finished_at of this JobStatusData.  # noqa: E501\n\n\n        :return: The finished_at of this JobStatusData.  # noqa: E501\n        :rtype: Timestamp\n        \"\"\"\n        return self._finished_at",
  "def finished_at(self, finished_at):\n        \"\"\"Sets the finished_at of this JobStatusData.\n\n\n        :param finished_at: The finished_at of this JobStatusData.  # noqa: E501\n        :type: Timestamp\n        \"\"\"\n\n        self._finished_at = finished_at",
  "def error(self):\n        \"\"\"Gets the error of this JobStatusData.  # noqa: E501\n\n\n        :return: The error of this JobStatusData.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._error",
  "def error(self, error):\n        \"\"\"Sets the error of this JobStatusData.\n\n\n        :param error: The error of this JobStatusData.  # noqa: E501\n        :type: str\n        \"\"\"\n\n        self._error = error",
  "def result(self):\n        \"\"\"Gets the result of this JobStatusData.  # noqa: E501\n\n\n        :return: The result of this JobStatusData.  # noqa: E501\n        :rtype: JobStatusDataResult\n        \"\"\"\n        return self._result",
  "def result(self, result):\n        \"\"\"Sets the result of this JobStatusData.\n\n\n        :param result: The result of this JobStatusData.  # noqa: E501\n        :type: JobStatusDataResult\n        \"\"\"\n\n        self._result = result",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(JobStatusData, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, JobStatusData):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, JobStatusData):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class SamplingConfigStoppingCondition(object):\n    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually.\n    \"\"\"\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'n_samples': 'float',\n        'min_distance': 'int'\n    }\n\n    attribute_map = {\n        'n_samples': 'nSamples',\n        'min_distance': 'minDistance'\n    }\n\n    def __init__(self, n_samples=None, min_distance=None, _configuration=None):  # noqa: E501\n        \"\"\"SamplingConfigStoppingCondition - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._n_samples = None\n        self._min_distance = None\n        self.discriminator = None\n\n        if n_samples is not None:\n            self.n_samples = n_samples\n        if min_distance is not None:\n            self.min_distance = min_distance\n\n    @property\n    def n_samples(self):\n        \"\"\"Gets the n_samples of this SamplingConfigStoppingCondition.  # noqa: E501\n\n        How many samples/images should be used for the sampling. 0-1 represents a percentage of all. 1-N are absolute numbers  # noqa: E501\n\n        :return: The n_samples of this SamplingConfigStoppingCondition.  # noqa: E501\n        :rtype: float\n        \"\"\"\n        return self._n_samples\n\n    @n_samples.setter\n    def n_samples(self, n_samples):\n        \"\"\"Sets the n_samples of this SamplingConfigStoppingCondition.\n\n        How many samples/images should be used for the sampling. 0-1 represents a percentage of all. 1-N are absolute numbers  # noqa: E501\n\n        :param n_samples: The n_samples of this SamplingConfigStoppingCondition.  # noqa: E501\n        :type: float\n        \"\"\"\n\n        self._n_samples = n_samples\n\n    @property\n    def min_distance(self):\n        \"\"\"Gets the min_distance of this SamplingConfigStoppingCondition.  # noqa: E501\n\n        How far should forest gump run  # noqa: E501\n\n        :return: The min_distance of this SamplingConfigStoppingCondition.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._min_distance\n\n    @min_distance.setter\n    def min_distance(self, min_distance):\n        \"\"\"Sets the min_distance of this SamplingConfigStoppingCondition.\n\n        How far should forest gump run  # noqa: E501\n\n        :param min_distance: The min_distance of this SamplingConfigStoppingCondition.  # noqa: E501\n        :type: int\n        \"\"\"\n\n        self._min_distance = min_distance\n\n    def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(SamplingConfigStoppingCondition, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, SamplingConfigStoppingCondition):\n            return False\n\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, SamplingConfigStoppingCondition):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "def __init__(self, n_samples=None, min_distance=None, _configuration=None):  # noqa: E501\n        \"\"\"SamplingConfigStoppingCondition - a model defined in Swagger\"\"\"  # noqa: E501\n        if _configuration is None:\n            _configuration = Configuration()\n        self._configuration = _configuration\n\n        self._n_samples = None\n        self._min_distance = None\n        self.discriminator = None\n\n        if n_samples is not None:\n            self.n_samples = n_samples\n        if min_distance is not None:\n            self.min_distance = min_distance",
  "def n_samples(self):\n        \"\"\"Gets the n_samples of this SamplingConfigStoppingCondition.  # noqa: E501\n\n        How many samples/images should be used for the sampling. 0-1 represents a percentage of all. 1-N are absolute numbers  # noqa: E501\n\n        :return: The n_samples of this SamplingConfigStoppingCondition.  # noqa: E501\n        :rtype: float\n        \"\"\"\n        return self._n_samples",
  "def n_samples(self, n_samples):\n        \"\"\"Sets the n_samples of this SamplingConfigStoppingCondition.\n\n        How many samples/images should be used for the sampling. 0-1 represents a percentage of all. 1-N are absolute numbers  # noqa: E501\n\n        :param n_samples: The n_samples of this SamplingConfigStoppingCondition.  # noqa: E501\n        :type: float\n        \"\"\"\n\n        self._n_samples = n_samples",
  "def min_distance(self):\n        \"\"\"Gets the min_distance of this SamplingConfigStoppingCondition.  # noqa: E501\n\n        How far should forest gump run  # noqa: E501\n\n        :return: The min_distance of this SamplingConfigStoppingCondition.  # noqa: E501\n        :rtype: int\n        \"\"\"\n        return self._min_distance",
  "def min_distance(self, min_distance):\n        \"\"\"Sets the min_distance of this SamplingConfigStoppingCondition.\n\n        How far should forest gump run  # noqa: E501\n\n        :param min_distance: The min_distance of this SamplingConfigStoppingCondition.  # noqa: E501\n        :type: int\n        \"\"\"\n\n        self._min_distance = min_distance",
  "def to_dict(self):\n        \"\"\"Returns the model properties as a dict\"\"\"\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(SamplingConfigStoppingCondition, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result",
  "def to_str(self):\n        \"\"\"Returns the string representation of the model\"\"\"\n        return pprint.pformat(self.to_dict())",
  "def __repr__(self):\n        \"\"\"For `print` and `pprint`\"\"\"\n        return self.to_str()",
  "def __eq__(self, other):\n        \"\"\"Returns true if both objects are equal\"\"\"\n        if not isinstance(other, SamplingConfigStoppingCondition):\n            return False\n\n        return self.to_dict() == other.to_dict()",
  "def __ne__(self, other):\n        \"\"\"Returns true if both objects are not equal\"\"\"\n        if not isinstance(other, SamplingConfigStoppingCondition):\n            return True\n\n        return self.to_dict() != other.to_dict()",
  "class RandomRotate(object):\n    \"\"\"Implementation of random rotation.\n\n    Randomly rotates an input image by a fixed angle. By default, we rotate\n    the image by 90 degrees with a probability of 50%.\n\n    This augmentation can be very useful for rotation invariant images such as\n    in medical imaging or satellite imaginary.\n\n    Attributes:\n        prob:\n            Probability with which image is rotated.\n        angle:\n            Angle by which the image is rotated. We recommend multiples of 90\n            to prevent rasterization artifacts. If you pick numbers like\n            90, 180, 270 the tensor will be rotated without introducing \n            any artifacts.\n    \n    \"\"\"\n\n    def __init__(self, prob: float = 0.5, angle: int = 90):\n        self.prob = prob\n        self.angle = angle\n\n    def __call__(self, sample):\n        \"\"\"Rotates the images with a given probability.\n\n        Args:\n            sample:\n                PIL image which will be rotated.\n        \n        Returns:\n            Rotated image or original image.\n\n        \"\"\"\n        prob = np.random.random_sample()\n        if prob < self.prob:\n            sample = TF.rotate(sample, self.angle)\n        return sample",
  "def __init__(self, prob: float = 0.5, angle: int = 90):\n        self.prob = prob\n        self.angle = angle",
  "def __call__(self, sample):\n        \"\"\"Rotates the images with a given probability.\n\n        Args:\n            sample:\n                PIL image which will be rotated.\n        \n        Returns:\n            Rotated image or original image.\n\n        \"\"\"\n        prob = np.random.random_sample()\n        if prob < self.prob:\n            sample = TF.rotate(sample, self.angle)\n        return sample",
  "class RandomSolarization(object):\n    \"\"\"Implementation of random image Solarization.\n\n    Utilizes the integrated image operation `solarize` from Pillow. Solarization\n    inverts all pixel values above a threshold (default: 128).\n\n    Attributes:\n        probability:\n            Probability to apply the transformation\n        threshold:\n            Threshold for solarization.\n    \"\"\"\n\n    def __init__(self, prob: float = 0.5,\n                 threshold: int = 128):\n        self.prob = prob\n        self.threshold = threshold\n\n    def __call__(self, sample):\n        \"\"\"Solarizes the given input image\n\n        Args:\n            sample:\n                PIL image to which solarize will be applied.\n\n        Returns:\n            Solarized image or original image.\n\n        \"\"\"\n        prob = np.random.random_sample()\n        if prob < self.prob:\n            # return solarized image\n            return ImageOps.solarize(sample, threshold=self.threshold)\n        # return original image\n        return sample",
  "def __init__(self, prob: float = 0.5,\n                 threshold: int = 128):\n        self.prob = prob\n        self.threshold = threshold",
  "def __call__(self, sample):\n        \"\"\"Solarizes the given input image\n\n        Args:\n            sample:\n                PIL image to which solarize will be applied.\n\n        Returns:\n            Solarized image or original image.\n\n        \"\"\"\n        prob = np.random.random_sample()\n        if prob < self.prob:\n            # return solarized image\n            return ImageOps.solarize(sample, threshold=self.threshold)\n        # return original image\n        return sample",
  "class GaussianBlur(object):\n    \"\"\"Implementation of random Gaussian blur.\n\n    Utilizes the built-in ImageFilter method from PIL to apply a Gaussian \n    blur to the input image with a certain probability. The blur is further\n    randomized as the kernel size is chosen randomly around a mean specified\n    by the user.\n\n    Attributes:\n        kernel_size:\n            Mean kernel size for the Gaussian blur.\n        prob:\n            Probability with which the blur is applied.\n        scale:\n            Fraction of the kernel size which is used for upper and lower\n            limits of the randomized kernel size.\n\n    \"\"\"\n\n    def __init__(self, kernel_size: float, prob: float = 0.5,\n                 scale: float = 0.2):\n        self.prob = prob\n        self.scale = scale\n        # limits for random kernel sizes\n        self.min_size = (1 - scale) * kernel_size\n        self.max_size = (1 + scale) * kernel_size\n        self.kernel_size = kernel_size\n\n    def __call__(self, sample):\n        \"\"\"Blurs the image with a given probability.\n\n        Args:\n            sample:\n                PIL image to which blur will be applied.\n        \n        Returns:\n            Blurred image or original image.\n\n        \"\"\"\n        prob = np.random.random_sample()\n        if prob < self.prob:\n            # choose randomized kernel size\n            kernel_size = np.random.normal(\n                self.kernel_size, self.scale * self.kernel_size\n            )\n            kernel_size = max(self.min_size, kernel_size)\n            kernel_size = min(self.max_size, kernel_size)\n            radius = int(kernel_size / 2)\n            #\u00a0return blurred image\n            return sample.filter(ImageFilter.GaussianBlur(radius=radius))\n        # return original image\n        return sample",
  "def __init__(self, kernel_size: float, prob: float = 0.5,\n                 scale: float = 0.2):\n        self.prob = prob\n        self.scale = scale\n        # limits for random kernel sizes\n        self.min_size = (1 - scale) * kernel_size\n        self.max_size = (1 + scale) * kernel_size\n        self.kernel_size = kernel_size",
  "def __call__(self, sample):\n        \"\"\"Blurs the image with a given probability.\n\n        Args:\n            sample:\n                PIL image to which blur will be applied.\n        \n        Returns:\n            Blurred image or original image.\n\n        \"\"\"\n        prob = np.random.random_sample()\n        if prob < self.prob:\n            # choose randomized kernel size\n            kernel_size = np.random.normal(\n                self.kernel_size, self.scale * self.kernel_size\n            )\n            kernel_size = max(self.min_size, kernel_size)\n            kernel_size = min(self.max_size, kernel_size)\n            radius = int(kernel_size / 2)\n            #\u00a0return blurred image\n            return sample.filter(ImageFilter.GaussianBlur(radius=radius))\n        # return original image\n        return sample",
  "def _deactivate_requires_grad(params):\n    \"\"\"Deactivates the requires_grad flag for all parameters.\n\n    \"\"\"\n    for param in params:\n        param.requires_grad = False",
  "def _do_momentum_update(prev_params, params, m):\n    \"\"\"Updates the weights of the previous parameters.\n\n    \"\"\"\n    for prev_param, param in zip(prev_params, params):\n        prev_param.data = prev_param.data * m + param.data * (1. - m)",
  "class _MomentumEncoderMixin:\n    \"\"\"Mixin to provide momentum encoder functionalities.\n\n    Provides the following functionalities:\n        - Momentum encoder initialization.\n        - Momentum updates.\n        - Batch shuffling and unshuffling.\n\n    To make use of the mixin, simply inherit from it:\n\n    >>> class MyMoCo(nn.Module, _MomentumEncoderMixin):\n    >>>\n    >>>     def __init__(self, backbone):\n    >>>         super(MyMoCo, self).__init__()\n    >>>\n    >>>         self.backbone = backbone\n    >>>         self.projection_head = get_projection_head()\n    >>>\n    >>>         # initialize momentum_backbone and momentum_projection_head\n    >>>         self._init_momentum_encoder()\n    >>>\n    >>>     def forward(self, x: torch.Tensor):\n    >>>\n    >>>         # do the momentum update\n    >>>         self._momentum_update(0.999)\n    >>>\n    >>>         # use momentum backbone\n    >>>         y = self.momentum_backbone(x)\n    >>>         y = self.momentum_projection_head(y)\n\n    \"\"\"\n\n    m: float\n    backbone: nn.Module\n    projection_head: nn.Module\n    momentum_backbone: nn.Module\n    momentum_projection_head: nn.Module\n\n    def _init_momentum_encoder(self):\n        \"\"\"Initializes momentum backbone and a momentum projection head.\n\n        \"\"\"\n        assert self.backbone is not None\n        assert self.projection_head is not None\n\n        self.momentum_backbone = copy.deepcopy(self.backbone)\n        self.momentum_projection_head = copy.deepcopy(self.projection_head)\n        \n        _deactivate_requires_grad(self.momentum_backbone.parameters())\n        _deactivate_requires_grad(self.momentum_projection_head.parameters())\n\n    @torch.no_grad()\n    def _momentum_update(self, m: float = 0.999):\n        \"\"\"Performs the momentum update for the backbone and projection head.\n\n        \"\"\"\n        _do_momentum_update(\n            self.momentum_backbone.parameters(),\n            self.backbone.parameters(),\n            m=m,\n        )\n        _do_momentum_update(\n            self.momentum_projection_head.parameters(),\n            self.projection_head.parameters(),\n            m=m,\n        )\n\n    @torch.no_grad()\n    def _batch_shuffle(self, batch: torch.Tensor):\n        \"\"\"Returns the shuffled batch and the indices to undo.\n\n        \"\"\"\n        batch_size = batch.shape[0]\n        shuffle = torch.randperm(batch_size).to(batch.device)\n        return batch[shuffle], shuffle\n\n    @torch.no_grad()\n    def _batch_unshuffle(self, batch: torch.Tensor, shuffle: torch.Tensor):\n        \"\"\"Returns the unshuffled batch.\n\n        \"\"\"\n        unshuffle = torch.argsort(shuffle).to(batch.device)\n        return batch[unshuffle]",
  "def _init_momentum_encoder(self):\n        \"\"\"Initializes momentum backbone and a momentum projection head.\n\n        \"\"\"\n        assert self.backbone is not None\n        assert self.projection_head is not None\n\n        self.momentum_backbone = copy.deepcopy(self.backbone)\n        self.momentum_projection_head = copy.deepcopy(self.projection_head)\n        \n        _deactivate_requires_grad(self.momentum_backbone.parameters())\n        _deactivate_requires_grad(self.momentum_projection_head.parameters())",
  "def _momentum_update(self, m: float = 0.999):\n        \"\"\"Performs the momentum update for the backbone and projection head.\n\n        \"\"\"\n        _do_momentum_update(\n            self.momentum_backbone.parameters(),\n            self.backbone.parameters(),\n            m=m,\n        )\n        _do_momentum_update(\n            self.momentum_projection_head.parameters(),\n            self.projection_head.parameters(),\n            m=m,\n        )",
  "def _batch_shuffle(self, batch: torch.Tensor):\n        \"\"\"Returns the shuffled batch and the indices to undo.\n\n        \"\"\"\n        batch_size = batch.shape[0]\n        shuffle = torch.randperm(batch_size).to(batch.device)\n        return batch[shuffle], shuffle",
  "def _batch_unshuffle(self, batch: torch.Tensor, shuffle: torch.Tensor):\n        \"\"\"Returns the unshuffled batch.\n\n        \"\"\"\n        unshuffle = torch.argsort(shuffle).to(batch.device)\n        return batch[unshuffle]",
  "def _prediction_mlp(in_dims: int, \n                    h_dims: int, \n                    out_dims: int) -> nn.Sequential:\n    \"\"\"Prediction MLP. The original paper's implementation has 2 layers, with \n    BN applied to its hidden fc layers but no BN or ReLU on the output fc layer.\n\n    Note that the hidden dimensions should be smaller than the input/output \n    dimensions (bottleneck structure). The default implementation using a \n    ResNet50 backbone has an input dimension of 2048, hidden dimension of 512, \n    and output dimension of 2048\n\n    Args:\n        in_dims:\n            Input dimension of the first linear layer.\n        h_dims: \n            Hidden dimension of all the fully connected layers (should be a\n            bottleneck!)\n        out_dims: \n            Output Dimension of the final linear layer.\n\n    Returns:\n        nn.Sequential:\n            The projection head.\n    \"\"\"\n    l1 = nn.Sequential(nn.Linear(in_dims, h_dims),\n                       nn.BatchNorm1d(h_dims),\n                       nn.ReLU(inplace=True))\n\n    l2 = nn.Linear(h_dims, out_dims)\n\n    prediction = nn.Sequential(l1, l2)\n    return prediction",
  "def _projection_mlp(in_dims: int,\n                    h_dims: int,\n                    out_dims: int,\n                    num_layers: int = 3) -> nn.Sequential:\n    \"\"\"Projection MLP. The original paper's implementation has 3 layers, with \n    BN applied to its hidden fc layers but no ReLU on the output fc layer. \n    The CIFAR-10 study used a MLP with only two layers.\n\n    Args:\n        in_dims:\n            Input dimension of the first linear layer.\n        h_dims: \n            Hidden dimension of all the fully connected layers.\n        out_dims: \n            Output Dimension of the final linear layer.\n        num_layers:\n            Controls the number of layers; must be 2 or 3. Defaults to 3.\n\n    Returns:\n        nn.Sequential:\n            The projection head.\n    \"\"\"\n    l1 = nn.Sequential(nn.Linear(in_dims, h_dims),\n                       nn.BatchNorm1d(h_dims),\n                       nn.ReLU(inplace=True))\n\n    l2 = nn.Sequential(nn.Linear(h_dims, h_dims),\n                       nn.BatchNorm1d(h_dims),\n                       nn.ReLU(inplace=True))\n\n    l3 = nn.Sequential(nn.Linear(h_dims, out_dims),\n                       nn.BatchNorm1d(out_dims))\n\n    if num_layers == 3:\n        projection = nn.Sequential(l1, l2, l3)\n    elif num_layers == 2:\n        projection = nn.Sequential(l1, l3)\n    else:\n        raise NotImplementedError(\"Only MLPs with 2 and 3 layers are implemented.\")\n\n    return projection",
  "class SimSiam(nn.Module):\n    \"\"\"Implementation of SimSiam[0] network\n\n    Recommended loss: :py:class:`lightly.loss.sym_neg_cos_sim_loss.SymNegCosineSimilarityLoss`\n\n    [0] SimSiam, 2020, https://arxiv.org/abs/2011.10566\n\n    Attributes:\n        backbone:\n            Backbone model to extract features from images.\n        num_ftrs:\n            Dimension of the embedding (before the projection head).\n        proj_hidden_dim:\n            Dimension of the hidden layer of the projection head. This should\n            be the same size as `num_ftrs`.\n        pred_hidden_dim:\n            Dimension of the hidden layer of the predicion head. This should\n            be `num_ftrs` / 4.\n        out_dim:\n            Dimension of the output (after the projection head).\n\n    \"\"\"\n\n    def __init__(self,\n                 backbone: nn.Module,\n                 num_ftrs: int = 2048,\n                 proj_hidden_dim: int = 2048,\n                 pred_hidden_dim: int = 512,\n                 out_dim: int = 2048,\n                 num_mlp_layers: int = 3):\n\n        super(SimSiam, self).__init__()\n\n        self.backbone = backbone\n        self.num_ftrs = num_ftrs\n        self.proj_hidden_dim = proj_hidden_dim\n        self.pred_hidden_dim = pred_hidden_dim\n        self.out_dim = out_dim\n\n        self.projection_mlp = \\\n            _projection_mlp(num_ftrs, proj_hidden_dim, out_dim, num_mlp_layers)\n\n        self.prediction_mlp = \\\n            _prediction_mlp(out_dim, pred_hidden_dim, out_dim)\n        \n    def forward(self, \n                x0: torch.Tensor, \n                x1: torch.Tensor = None,\n                return_features: bool = False):\n        \"\"\"Forward pass through SimSiam.\n\n        Extracts features with the backbone and applies the projection\n        head and prediction head to the output space. If both x0 and x1 are not\n        None, both will be passed through the backbone, projection, and\n        prediction head. If x1 is None, only x0 will be forwarded.\n\n        Args:\n            x0:\n                Tensor of shape bsz x channels x W x H.\n            x1:\n                Tensor of shape bsz x channels x W x H.\n            return_features:\n                Whether or not to return the intermediate features backbone(x).\n\n        Returns:\n            The output prediction and projection of x0 and (if x1 is not None)\n            the output prediction and projection of x1. If return_features is\n            True, the output for each x is a tuple (out, f) where f are the\n            features before the projection head.\n            \n        Examples:\n            >>> # single input, single output\n            >>> out = model(x) \n            >>> \n            >>> # single input with return_features=True\n            >>> out, f = model(x, return_features=True)\n            >>>\n            >>> # two inputs, two outputs\n            >>> out0, out1 = model(x0, x1)\n            >>>\n            >>> # two inputs, two outputs with return_features=True\n            >>> (out0, f0), (out1, f1) = model(x0, x1, return_features=True)\n        \"\"\"\n        f0 = self.backbone(x0).squeeze()\n        z0 = self.projection_mlp(f0)\n        p0 = self.prediction_mlp(z0)\n\n        out0 = (z0, p0)\n\n        # append features if requested\n        if return_features:\n            out0 = (out0, f0)\n\n        if x1 is None:\n            return out0\n        \n        f1 = self.backbone(x1).squeeze()\n        z1 = self.projection_mlp(f1)\n        p1 = self.prediction_mlp(z1)\n\n        out1 = (z1, p1)\n\n        # append features if requested\n        if return_features:\n            out1 = (out1, f1)\n\n        return out0, out1",
  "def __init__(self,\n                 backbone: nn.Module,\n                 num_ftrs: int = 2048,\n                 proj_hidden_dim: int = 2048,\n                 pred_hidden_dim: int = 512,\n                 out_dim: int = 2048,\n                 num_mlp_layers: int = 3):\n\n        super(SimSiam, self).__init__()\n\n        self.backbone = backbone\n        self.num_ftrs = num_ftrs\n        self.proj_hidden_dim = proj_hidden_dim\n        self.pred_hidden_dim = pred_hidden_dim\n        self.out_dim = out_dim\n\n        self.projection_mlp = \\\n            _projection_mlp(num_ftrs, proj_hidden_dim, out_dim, num_mlp_layers)\n\n        self.prediction_mlp = \\\n            _prediction_mlp(out_dim, pred_hidden_dim, out_dim)",
  "def forward(self, \n                x0: torch.Tensor, \n                x1: torch.Tensor = None,\n                return_features: bool = False):\n        \"\"\"Forward pass through SimSiam.\n\n        Extracts features with the backbone and applies the projection\n        head and prediction head to the output space. If both x0 and x1 are not\n        None, both will be passed through the backbone, projection, and\n        prediction head. If x1 is None, only x0 will be forwarded.\n\n        Args:\n            x0:\n                Tensor of shape bsz x channels x W x H.\n            x1:\n                Tensor of shape bsz x channels x W x H.\n            return_features:\n                Whether or not to return the intermediate features backbone(x).\n\n        Returns:\n            The output prediction and projection of x0 and (if x1 is not None)\n            the output prediction and projection of x1. If return_features is\n            True, the output for each x is a tuple (out, f) where f are the\n            features before the projection head.\n            \n        Examples:\n            >>> # single input, single output\n            >>> out = model(x) \n            >>> \n            >>> # single input with return_features=True\n            >>> out, f = model(x, return_features=True)\n            >>>\n            >>> # two inputs, two outputs\n            >>> out0, out1 = model(x0, x1)\n            >>>\n            >>> # two inputs, two outputs with return_features=True\n            >>> (out0, f0), (out1, f1) = model(x0, x1, return_features=True)\n        \"\"\"\n        f0 = self.backbone(x0).squeeze()\n        z0 = self.projection_mlp(f0)\n        p0 = self.prediction_mlp(z0)\n\n        out0 = (z0, p0)\n\n        # append features if requested\n        if return_features:\n            out0 = (out0, f0)\n\n        if x1 is None:\n            return out0\n        \n        f1 = self.backbone(x1).squeeze()\n        z1 = self.projection_mlp(f1)\n        p1 = self.prediction_mlp(z1)\n\n        out1 = (z1, p1)\n\n        # append features if requested\n        if return_features:\n            out1 = (out1, f1)\n\n        return out0, out1",
  "def _get_moco_projection_head(num_ftrs: int, out_dim: int):\n    \"\"\"Returns a 2-layer projection head.\n\n    Reference (07.12.2020):\n    https://github.com/facebookresearch/moco/blob/master/moco/builder.py\n\n    \"\"\"\n    modules = [\n        nn.Linear(num_ftrs, num_ftrs),\n        nn.ReLU(),\n        nn.Linear(num_ftrs, out_dim)\n    ]\n    return nn.Sequential(*modules)",
  "class MoCo(nn.Module, _MomentumEncoderMixin):\n    \"\"\"Implementation of the MoCo (Momentum Contrast)[0] architecture.\n\n    Recommended loss: :py:class:`lightly.loss.ntx_ent_loss.NTXentLoss` with \n    a memory bank.\n\n    [0] MoCo, 2020, https://arxiv.org/abs/1911.05722\n\n    Attributes:\n        backbone:\n            Backbone model to extract features from images.\n        num_ftrs:\n            Dimension of the embedding (before the projection head).\n        out_dim:\n            Dimension of the output (after the projection head).\n        m:\n            Momentum for momentum update of the key-encoder.\n\n    \"\"\"\n\n    def __init__(self,\n                 backbone: nn.Module,\n                 num_ftrs: int = 32,\n                 out_dim: int = 128,\n                 m: float = 0.999,\n                 batch_shuffle: bool = False):\n\n        super(MoCo, self).__init__()\n\n        self.backbone = backbone\n        self.projection_head = _get_moco_projection_head(num_ftrs, out_dim)\n        self.momentum_features = None\n        self.momentum_projection_head = None\n\n        self.m = m\n        self.batch_shuffle = batch_shuffle\n\n        # initialize momentum features and momentum projection head\n        self._init_momentum_encoder()\n\n    def forward(self,\n                x0: torch.Tensor,\n                x1: torch.Tensor = None,\n                return_features: bool = False):\n        \"\"\"Embeds and projects the input image.\n\n        Performs the momentum update, extracts features with the backbone and \n        applies the projection head to the output space. If both x0 and x1 are\n        not None, both will be passed through the backbone and projection head.\n        If x1 is None, only x0 will be forwarded.\n\n        Args:\n            x0:\n                Tensor of shape bsz x channels x W x H.\n            x1:\n                Tensor of shape bsz x channels x W x H.\n            return_features:\n                Whether or not to return the intermediate features backbone(x).\n\n        Returns:\n            The output projection of x0 and (if x1 is not None) the output\n            projection of x1. If return_features is True, the output for each x\n            is a tuple (out, f) where f are the features before the projection\n            head.\n\n        Examples:\n            >>> # single input, single output\n            >>> out = model(x) \n            >>> \n            >>> # single input with return_features=True\n            >>> out, f = model(x, return_features=True)\n            >>>\n            >>> # two inputs, two outputs\n            >>> out0, out1 = model(x0, x1)\n            >>>\n            >>> # two inputs, two outputs with return_features=True\n            >>> (out0, f0), (out1, f1) = model(x0, x1, return_features=True)\n\n        \"\"\"\n        self._momentum_update(self.m)\n        \n        # forward pass of first input x0\n        f0 = self.backbone(x0).squeeze()\n        out0 = self.projection_head(f0)\n\n        # append features if requested\n        if return_features:\n            out0 = (out0, f0)\n\n        # return out0 if x1 is None\n        if x1 is None:\n            return out0\n\n        # forward pass of second input x1\n        with torch.no_grad():\n\n            # shuffle for batchnorm\n            if self.batch_shuffle:\n                x1, shuffle = self._batch_shuffle(x1)\n\n            # run x1 through momentum encoder\n            f1 = self.momentum_backbone(x1).squeeze()\n            out1 = self.momentum_projection_head(f1).detach()\n        \n            # unshuffle for batchnorm\n            if self.batch_shuffle:\n                f1 = self._batch_unshuffle(f1, shuffle)\n                out1 = self._batch_unshuffle(out1, shuffle)\n\n            # append features if requested\n            if return_features:\n                out1 = (out1, f1)\n\n        return out0, out1",
  "def __init__(self,\n                 backbone: nn.Module,\n                 num_ftrs: int = 32,\n                 out_dim: int = 128,\n                 m: float = 0.999,\n                 batch_shuffle: bool = False):\n\n        super(MoCo, self).__init__()\n\n        self.backbone = backbone\n        self.projection_head = _get_moco_projection_head(num_ftrs, out_dim)\n        self.momentum_features = None\n        self.momentum_projection_head = None\n\n        self.m = m\n        self.batch_shuffle = batch_shuffle\n\n        # initialize momentum features and momentum projection head\n        self._init_momentum_encoder()",
  "def forward(self,\n                x0: torch.Tensor,\n                x1: torch.Tensor = None,\n                return_features: bool = False):\n        \"\"\"Embeds and projects the input image.\n\n        Performs the momentum update, extracts features with the backbone and \n        applies the projection head to the output space. If both x0 and x1 are\n        not None, both will be passed through the backbone and projection head.\n        If x1 is None, only x0 will be forwarded.\n\n        Args:\n            x0:\n                Tensor of shape bsz x channels x W x H.\n            x1:\n                Tensor of shape bsz x channels x W x H.\n            return_features:\n                Whether or not to return the intermediate features backbone(x).\n\n        Returns:\n            The output projection of x0 and (if x1 is not None) the output\n            projection of x1. If return_features is True, the output for each x\n            is a tuple (out, f) where f are the features before the projection\n            head.\n\n        Examples:\n            >>> # single input, single output\n            >>> out = model(x) \n            >>> \n            >>> # single input with return_features=True\n            >>> out, f = model(x, return_features=True)\n            >>>\n            >>> # two inputs, two outputs\n            >>> out0, out1 = model(x0, x1)\n            >>>\n            >>> # two inputs, two outputs with return_features=True\n            >>> (out0, f0), (out1, f1) = model(x0, x1, return_features=True)\n\n        \"\"\"\n        self._momentum_update(self.m)\n        \n        # forward pass of first input x0\n        f0 = self.backbone(x0).squeeze()\n        out0 = self.projection_head(f0)\n\n        # append features if requested\n        if return_features:\n            out0 = (out0, f0)\n\n        # return out0 if x1 is None\n        if x1 is None:\n            return out0\n\n        # forward pass of second input x1\n        with torch.no_grad():\n\n            # shuffle for batchnorm\n            if self.batch_shuffle:\n                x1, shuffle = self._batch_shuffle(x1)\n\n            # run x1 through momentum encoder\n            f1 = self.momentum_backbone(x1).squeeze()\n            out1 = self.momentum_projection_head(f1).detach()\n        \n            # unshuffle for batchnorm\n            if self.batch_shuffle:\n                f1 = self._batch_unshuffle(f1, shuffle)\n                out1 = self._batch_unshuffle(out1, shuffle)\n\n            # append features if requested\n            if return_features:\n                out1 = (out1, f1)\n\n        return out0, out1",
  "def _projection_head_barlow(in_dims: int,\n                    h_dims: int = 8192,\n                    out_dims: int = 8192,\n                    num_layers: int = 3) -> nn.Sequential:\n    \"\"\"\n    Projection MLP. The original paper's implementation [0] has 3 layers, with\n    8192 output units each layer. BN and ReLU applied to first and second layer.\n\n    Args:\n        in_dims:\n            Input dimension of the first linear layer.\n        h_dims:\n            Hidden dimension of all the fully connected layers.\n            8192 on [0].\n        out_dims:\n            Output Dimension of the final linear layer.\n            Dimension of the latent space. 8192 on [0].\n        num_layers:\n            Controls the number of layers; must be 2 or 3. Defaults to 3.\n\n    Returns:\n        nn.Sequential:\n            The projection head.\n    \"\"\"\n    l1 = nn.Sequential(nn.Linear(in_dims, h_dims),\n                       nn.BatchNorm1d(h_dims),\n                       nn.ReLU(inplace=True))\n\n    l2 = nn.Sequential(nn.Linear(h_dims, h_dims),\n                       nn.BatchNorm1d(h_dims),\n                       nn.ReLU(inplace=True))\n\n    l3 = nn.Sequential(nn.Linear(h_dims, out_dims))\n    #SimSiam and BarlowTwins only differs in one BN layer\n\n    if num_layers == 3:\n        projection = nn.Sequential(l1, l2, l3)\n    elif num_layers == 2:\n        projection = nn.Sequential(l1, l3)\n    else:\n        raise NotImplementedError(\"Only MLPs with 2 and 3 layers are implemented.\")\n\n    return projection",
  "class BarlowTwins(nn.Module):\n    \"\"\"Implementation of BarlowTwins[0] network.\n\n    Recommended loss: :py:class:`lightly.loss.barlow_twins_loss.BarlowTwinsLoss`\n\n    Default params are the ones explained in the original paper [0].\n    [0] Zbontar,J. et.al. 2021. Barlow Twins... https://arxiv.org/abs/2103.03230\n\n    Attributes:\n        backbone:\n            Backbone model to extract features from images.\n            ResNet-50 in original paper [0].\n        num_ftrs:\n            Dimension of the embedding (before the projection head).\n        proj_hidden_dim:\n            Dimension of the hidden layer of the projection head. This should\n            be the same size as `num_ftrs`.\n        out_dim:\n            Dimension of the output (after the projection head).\n\n    \"\"\"\n\n    def __init__(self,\n                 backbone: nn.Module,\n                 num_ftrs: int = 2048,\n                 proj_hidden_dim: int = 8192,\n                 out_dim: int = 8192,\n                 num_mlp_layers: int = 3):\n\n        super(BarlowTwins, self).__init__()\n\n        self.backbone = backbone\n        self.num_ftrs = num_ftrs\n        self.proj_hidden_dim = proj_hidden_dim\n        self.out_dim = out_dim\n\n        self.projection_mlp = \\\n            _projection_head_barlow(num_ftrs, proj_hidden_dim, out_dim, num_mlp_layers)\n\n    def forward(self,\n                x0: torch.Tensor,\n                x1: torch.Tensor = None,\n                return_features: bool = False):\n\n        \"\"\"Forward pass through BarlowTwins.\n\n        Extracts features with the backbone and applies the projection\n        head to the output space. If both x0 and x1 are not None, both will be\n        passed through the backbone and projection. If x1 is None, only x0 will\n        be forwarded.\n        Barlow Twins only implement a projection head unlike SimSiam.\n\n        Args:\n            x0:\n                Tensor of shape bsz x channels x W x H.\n            x1:\n                Tensor of shape bsz x channels x W x H.\n            return_features:\n                Whether or not to return the intermediate features backbone(x).\n\n        Returns:\n            The output projection of x0 and (if x1 is not None)\n            the output projection of x1. If return_features is\n            True, the output for each x is a tuple (out, f) where f are the\n            features before the projection head.\n\n        Examples:\n            >>> # single input, single output\n            >>> out = model(x)\n            >>>\n            >>> # single input with return_features=True\n            >>> out, f = model(x, return_features=True)\n            >>>\n            >>> # two inputs, two outputs\n            >>> out0, out1 = model(x0, x1)\n            >>>\n            >>> # two inputs, two outputs with return_features=True\n            >>> (out0, f0), (out1, f1) = model(x0, x1, return_features=True)\n        \"\"\"\n        # forward pass first input\n        f0 = self.backbone(x0).squeeze()\n        out0 = self.projection_mlp(f0)\n\n        # append features if requested\n        if return_features:\n            out0 = (out0, f0)\n\n        if x1 is None:\n            return out0\n\n        # forward pass second input\n        f1 = self.backbone(x1).squeeze()\n        out1 = self.projection_mlp(f1)\n\n        # append features if requested\n        if return_features:\n            out1 = (out1, f1)\n\n        return out0, out1",
  "def __init__(self,\n                 backbone: nn.Module,\n                 num_ftrs: int = 2048,\n                 proj_hidden_dim: int = 8192,\n                 out_dim: int = 8192,\n                 num_mlp_layers: int = 3):\n\n        super(BarlowTwins, self).__init__()\n\n        self.backbone = backbone\n        self.num_ftrs = num_ftrs\n        self.proj_hidden_dim = proj_hidden_dim\n        self.out_dim = out_dim\n\n        self.projection_mlp = \\\n            _projection_head_barlow(num_ftrs, proj_hidden_dim, out_dim, num_mlp_layers)",
  "def forward(self,\n                x0: torch.Tensor,\n                x1: torch.Tensor = None,\n                return_features: bool = False):\n\n        \"\"\"Forward pass through BarlowTwins.\n\n        Extracts features with the backbone and applies the projection\n        head to the output space. If both x0 and x1 are not None, both will be\n        passed through the backbone and projection. If x1 is None, only x0 will\n        be forwarded.\n        Barlow Twins only implement a projection head unlike SimSiam.\n\n        Args:\n            x0:\n                Tensor of shape bsz x channels x W x H.\n            x1:\n                Tensor of shape bsz x channels x W x H.\n            return_features:\n                Whether or not to return the intermediate features backbone(x).\n\n        Returns:\n            The output projection of x0 and (if x1 is not None)\n            the output projection of x1. If return_features is\n            True, the output for each x is a tuple (out, f) where f are the\n            features before the projection head.\n\n        Examples:\n            >>> # single input, single output\n            >>> out = model(x)\n            >>>\n            >>> # single input with return_features=True\n            >>> out, f = model(x, return_features=True)\n            >>>\n            >>> # two inputs, two outputs\n            >>> out0, out1 = model(x0, x1)\n            >>>\n            >>> # two inputs, two outputs with return_features=True\n            >>> (out0, f0), (out1, f1) = model(x0, x1, return_features=True)\n        \"\"\"\n        # forward pass first input\n        f0 = self.backbone(x0).squeeze()\n        out0 = self.projection_mlp(f0)\n\n        # append features if requested\n        if return_features:\n            out0 = (out0, f0)\n\n        if x1 is None:\n            return out0\n\n        # forward pass second input\n        f1 = self.backbone(x1).squeeze()\n        out1 = self.projection_mlp(f1)\n\n        # append features if requested\n        if return_features:\n            out1 = (out1, f1)\n\n        return out0, out1",
  "class SplitBatchNorm(nn.BatchNorm2d):\n    \"\"\"Simulates multi-gpu behaviour of BatchNorm in one gpu by splitting.\n\n    Implementation was adapted from:\n    https://github.com/davidcpage/cifar10-fast/blob/master/torch_backend.py\n\n    Attributes:\n        num_features:\n            Number of input features.\n        num_splits:\n            Number of splits.\n\n    \"\"\"\n    def __init__(self, num_features, num_splits, **kw):\n        super().__init__(num_features, **kw)\n        self.num_splits = num_splits\n        self.register_buffer(\n            'running_mean', torch.zeros(num_features*self.num_splits)\n        )\n        self.register_buffer(\n            'running_var', torch.ones(num_features*self.num_splits)\n        )\n\n    def train(self, mode=True):\n        # lazily collate stats when we are going to use them\n        if (self.training is True) and (mode is False):\n            self.running_mean = \\\n                torch.mean(\n                    self.running_mean.view(self.num_splits, self.num_features),\n                    dim=0\n                ).repeat(self.num_splits)\n            self.running_var = \\\n                torch.mean(\n                    self.running_var.view(self.num_splits, self.num_features),\n                    dim=0\n                ).repeat(self.num_splits)\n\n        return super().train(mode)\n\n    def forward(self, input):\n        \"\"\"Computes the SplitBatchNorm on the input.\n\n        \"\"\"\n        # get input shape\n        N, C, H, W = input.shape\n\n        # during training, use different stats for each split and otherwise\n        # use the stats from the first split\n        if self.training or not self.track_running_stats:\n            result = nn.functional.batch_norm(\n                input.view(-1, C*self.num_splits, H, W),\n                self.running_mean, self.running_var, \n                self.weight.repeat(self.num_splits),\n                self.bias.repeat(self.num_splits),\n                True,\n                self.momentum,\n                self.eps\n            ).view(N, C, H, W)\n        else:\n            result = nn.functional.batch_norm(\n                input,\n                self.running_mean[:self.num_features],\n                self.running_var[:self.num_features], \n                self.weight,\n                self.bias,\n                False,\n                self.momentum, \n                self.eps\n            )\n        \n        return result",
  "def get_norm_layer(num_features: int, num_splits: int, **kw):\n    \"\"\"Utility to switch between BatchNorm2d and SplitBatchNorm.\n\n    \"\"\"\n    if num_splits > 0:\n        return SplitBatchNorm(num_features, num_splits)\n    else:\n        return nn.BatchNorm2d(num_features)",
  "def __init__(self, num_features, num_splits, **kw):\n        super().__init__(num_features, **kw)\n        self.num_splits = num_splits\n        self.register_buffer(\n            'running_mean', torch.zeros(num_features*self.num_splits)\n        )\n        self.register_buffer(\n            'running_var', torch.ones(num_features*self.num_splits)\n        )",
  "def train(self, mode=True):\n        # lazily collate stats when we are going to use them\n        if (self.training is True) and (mode is False):\n            self.running_mean = \\\n                torch.mean(\n                    self.running_mean.view(self.num_splits, self.num_features),\n                    dim=0\n                ).repeat(self.num_splits)\n            self.running_var = \\\n                torch.mean(\n                    self.running_var.view(self.num_splits, self.num_features),\n                    dim=0\n                ).repeat(self.num_splits)\n\n        return super().train(mode)",
  "def forward(self, input):\n        \"\"\"Computes the SplitBatchNorm on the input.\n\n        \"\"\"\n        # get input shape\n        N, C, H, W = input.shape\n\n        # during training, use different stats for each split and otherwise\n        # use the stats from the first split\n        if self.training or not self.track_running_stats:\n            result = nn.functional.batch_norm(\n                input.view(-1, C*self.num_splits, H, W),\n                self.running_mean, self.running_var, \n                self.weight.repeat(self.num_splits),\n                self.bias.repeat(self.num_splits),\n                True,\n                self.momentum,\n                self.eps\n            ).view(N, C, H, W)\n        else:\n            result = nn.functional.batch_norm(\n                input,\n                self.running_mean[:self.num_features],\n                self.running_var[:self.num_features], \n                self.weight,\n                self.bias,\n                False,\n                self.momentum, \n                self.eps\n            )\n        \n        return result",
  "def _get_simclr_projection_head(num_ftrs: int, out_dim: int):\n    \"\"\"Returns a 2-layer projection head.\n\n    Reference (07.12.2020):\n    https://github.com/google-research/simclr/blob/master/model_util.py#L141\n\n    \"\"\"\n    modules = [\n        nn.Linear(num_ftrs, num_ftrs),\n        #nn.BatchNorm1d(num_ftrs),\n        nn.ReLU(),\n        nn.Linear(num_ftrs, out_dim)\n    ]\n    return nn.Sequential(*modules)",
  "class SimCLR(nn.Module):\n    \"\"\"Implementation of the SimCLR[0] architecture\n\n    Recommended loss: :py:class:`lightly.loss.ntx_ent_loss.NTXentLoss`\n\n    [0] SimCLR, 2020, https://arxiv.org/abs/2002.05709\n\n    Attributes:\n        backbone:\n            Backbone model to extract features from images.\n        num_ftrs:\n            Dimension of the embedding (before the projection head).\n        out_dim:\n            Dimension of the output (after the projection head).\n\n    \"\"\"\n\n    def __init__(self,\n                 backbone: nn.Module,\n                 num_ftrs: int = 32,\n                 out_dim: int = 128):\n\n        super(SimCLR, self).__init__()\n\n        self.backbone = backbone\n        self.projection_head = _get_simclr_projection_head(num_ftrs, out_dim)\n\n    def forward(self,\n                x0: torch.Tensor,\n                x1: torch.Tensor = None,\n                return_features: bool = False):\n        \"\"\"Embeds and projects the input images.\n\n        Extracts features with the backbone and applies the projection\n        head to the output space. If both x0 and x1 are not None, both will be\n        passed through the backbone and projection head. If x1 is None, only\n        x0 will be forwarded.\n\n        Args:\n            x0:\n                Tensor of shape bsz x channels x W x H.\n            x1:\n                Tensor of shape bsz x channels x W x H.\n            return_features:\n                Whether or not to return the intermediate features backbone(x).\n\n        Returns:\n            The output projection of x0 and (if x1 is not None) the output\n            projection of x1. If return_features is True, the output for each x\n            is a tuple (out, f) where f are the features before the projection\n            head.\n\n        Examples:\n            >>> # single input, single output\n            >>> out = model(x) \n            >>> \n            >>> # single input with return_features=True\n            >>> out, f = model(x, return_features=True)\n            >>>\n            >>> # two inputs, two outputs\n            >>> out0, out1 = model(x0, x1)\n            >>>\n            >>> # two inputs, two outputs with return_features=True\n            >>> (out0, f0), (out1, f1) = model(x0, x1, return_features=True)\n\n        \"\"\"\n        \n        # forward pass of first input x0\n        f0 = self.backbone(x0).squeeze()\n        out0 = self.projection_head(f0)\n\n        # append features if requested\n        if return_features:\n            out0 = (out0, f0)\n\n        # return out0 if x1 is None\n        if x1 is None:\n            return out0\n\n        # forward pass of second input x1\n        f1 = self.backbone(x1).squeeze()\n        out1 = self.projection_head(f1)\n\n        # append features if requested\n        if return_features:\n            out1 = (out1, f1)\n\n        # return both outputs\n        return out0, out1",
  "def __init__(self,\n                 backbone: nn.Module,\n                 num_ftrs: int = 32,\n                 out_dim: int = 128):\n\n        super(SimCLR, self).__init__()\n\n        self.backbone = backbone\n        self.projection_head = _get_simclr_projection_head(num_ftrs, out_dim)",
  "def forward(self,\n                x0: torch.Tensor,\n                x1: torch.Tensor = None,\n                return_features: bool = False):\n        \"\"\"Embeds and projects the input images.\n\n        Extracts features with the backbone and applies the projection\n        head to the output space. If both x0 and x1 are not None, both will be\n        passed through the backbone and projection head. If x1 is None, only\n        x0 will be forwarded.\n\n        Args:\n            x0:\n                Tensor of shape bsz x channels x W x H.\n            x1:\n                Tensor of shape bsz x channels x W x H.\n            return_features:\n                Whether or not to return the intermediate features backbone(x).\n\n        Returns:\n            The output projection of x0 and (if x1 is not None) the output\n            projection of x1. If return_features is True, the output for each x\n            is a tuple (out, f) where f are the features before the projection\n            head.\n\n        Examples:\n            >>> # single input, single output\n            >>> out = model(x) \n            >>> \n            >>> # single input with return_features=True\n            >>> out, f = model(x, return_features=True)\n            >>>\n            >>> # two inputs, two outputs\n            >>> out0, out1 = model(x0, x1)\n            >>>\n            >>> # two inputs, two outputs with return_features=True\n            >>> (out0, f0), (out1, f1) = model(x0, x1, return_features=True)\n\n        \"\"\"\n        \n        # forward pass of first input x0\n        f0 = self.backbone(x0).squeeze()\n        out0 = self.projection_head(f0)\n\n        # append features if requested\n        if return_features:\n            out0 = (out0, f0)\n\n        # return out0 if x1 is None\n        if x1 is None:\n            return out0\n\n        # forward pass of second input x1\n        f1 = self.backbone(x1).squeeze()\n        out1 = self.projection_head(f1)\n\n        # append features if requested\n        if return_features:\n            out1 = (out1, f1)\n\n        # return both outputs\n        return out0, out1",
  "class BasicBlock(nn.Module):\n    \"\"\" Implementation of the ResNet Basic Block.\n\n     Attributes:\n        in_planes:\n            Number of input channels.\n        planes:\n            Number of channels.\n        stride:\n            Stride of the first convolutional.\n    \"\"\"\n    expansion = 1\n\n    def __init__(self, in_planes: int, planes: int, stride: int = 1, num_splits: int = 0):\n\n        super(BasicBlock, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_planes,\n                               planes,\n                               kernel_size=3,\n                               stride=stride,\n                               padding=1,\n                               bias=False)\n        self.bn1 = get_norm_layer(planes, num_splits)\n\n        self.conv2 = nn.Conv2d(planes,\n                               planes,\n                               kernel_size=3,\n                               stride=1,\n                               padding=1,\n                               bias=False)\n        self.bn2 = get_norm_layer(planes, num_splits)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes,\n                          self.expansion*planes,\n                          kernel_size=1,\n                          stride=stride,\n                          bias=False),\n                get_norm_layer(self.expansion * planes, num_splits)\n            )\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"Forward pass through basic ResNet block.\n\n        Args:\n            x:\n                Tensor of shape bsz x channels x W x H\n\n        Returns:\n            Tensor of shape bsz x channels x W x H\n        \"\"\"\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = F.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        out += self.shortcut(x)\n        out = F.relu(out)\n\n        return out",
  "class Bottleneck(nn.Module):\n    \"\"\" Implementation of the ResNet Bottleneck Block.\n\n    Attributes:\n        in_planes:\n            Number of input channels.\n        planes:\n            Number of channels.\n        stride:\n            Stride of the first convolutional.\n\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, in_planes: int, planes: int, stride: int = 1, num_splits: int = 0):\n\n        super(Bottleneck, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = get_norm_layer(planes, num_splits)\n\n        self.conv2 = nn.Conv2d(planes,\n                               planes,\n                               kernel_size=3,\n                               stride=stride,\n                               padding=1,\n                               bias=False)\n        self.bn2 = get_norm_layer(planes, num_splits)\n\n        self.conv3 = nn.Conv2d(planes,\n                               self.expansion*planes,\n                               kernel_size=1,\n                               bias=False)\n        self.bn3 = get_norm_layer(self.expansion * planes, num_splits)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes,\n                          self.expansion*planes,\n                          kernel_size=1,\n                          stride=stride,\n                          bias=False),\n                get_norm_layer(self.expansion * planes, num_splits)\n            )\n\n    def forward(self, x):\n        \"\"\"Forward pass through bottleneck ResNet block.\n\n        Args:\n            x:\n                Tensor of shape bsz x channels x W x H\n\n        Returns:\n            Tensor of shape bsz x channels x W x H\n        \"\"\"\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = F.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = F.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        out += self.shortcut(x)\n        out = F.relu(out)\n\n        return out",
  "class ResNet(nn.Module):\n    \"\"\"ResNet implementation.\n\n    [1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\n    Attributes:\n        block:\n            ResNet building block type.\n        layers:\n            List of blocks per layer.\n        num_classes:\n            Number of classes in final softmax layer.\n        width:\n            Multiplier for ResNet width.\n    \"\"\"\n\n    def __init__(self,\n                 block: nn.Module = BasicBlock,\n                 layers: List[int] = [2, 2, 2, 2],\n                 num_classes: int = 10,\n                 width: float = 1.,\n                 num_splits: int = 0):\n\n        super(ResNet, self).__init__()\n        self.in_planes = int(64 * width)\n\n        self.base = int(64 * width)\n\n        self.conv1 = nn.Conv2d(3,\n                               self.base,\n                               kernel_size=3,\n                               stride=1,\n                               padding=1,\n                               bias=False)\n        self.bn1 = get_norm_layer(self.base, num_splits)\n        self.layer1 = self._make_layer(block, self.base, layers[0], stride=1, num_splits=num_splits)\n        self.layer2 = self._make_layer(block, self.base*2, layers[1], stride=2, num_splits=num_splits)\n        self.layer3 = self._make_layer(block, self.base*4, layers[2], stride=2, num_splits=num_splits)\n        self.layer4 = self._make_layer(block, self.base*8, layers[3], stride=2, num_splits=num_splits)\n        self.linear = nn.Linear(self.base*8*block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, layers, stride, num_splits):\n        strides = [stride] + [1]*(layers-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride, num_splits))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"Forward pass through ResNet.\n\n        Args:\n            x:\n                Tensor of shape bsz x channels x W x H\n        \n        Returns:\n            Output tensor of shape bsz x num_classes\n\n        \"\"\"\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out",
  "def ResNetGenerator(name: str = 'resnet-18',\n                    width: float = 1,\n                    num_classes: int = 10,\n                    num_splits: int = 0):\n    \"\"\"Builds and returns the specified ResNet.\n\n    Args:\n        name:\n            ResNet version from resnet-{9, 18, 34, 50, 101, 152}.\n        width:\n            ResNet width.\n        num_classes:\n            Output dim of the last layer.\n        num_splits:\n            Number of splits to use for SplitBatchNorm (for MoCo model).\n            Increase this number to simulate multi-gpu behavior.\n            E.g. `num_splits=8` simulates a 8-GPU cluster.\n            `num_splits=0` uses normal PyTorch BatchNorm.\n\n    Returns:\n        ResNet as nn.Module.\n\n    Examples:\n        >>> # binary classifier with ResNet-34\n        >>> from lightly.models import ResNetGenerator\n        >>> resnet = ResNetGenerator('resnet-34', num_classes=2)\n\n    \"\"\"\n\n    model_params = {\n        'resnet-9': {'block': BasicBlock, 'layers': [1, 1, 1, 1]},\n        'resnet-18': {'block': BasicBlock, 'layers': [2, 2, 2, 2]},\n        'resnet-34': {'block': BasicBlock, 'layers': [3, 4, 6, 3]},\n        'resnet-50': {'block': Bottleneck, 'layers': [3, 4, 6, 3]},\n        'resnet-101': {'block': Bottleneck, 'layers': [3, 4, 23, 3]},\n        'resnet-152': {'block': Bottleneck, 'layers': [3, 8, 36, 3]},\n    }\n\n    if name not in model_params.keys():\n        raise ValueError('Illegal name: {%s}. \\\n        Try resnet-9, resnet-18, resnet-34, resnet-50, resnet-101, resnet-152.' % (name))\n\n    return ResNet(**model_params[name], width=width, num_classes=10, num_splits=num_splits)",
  "def __init__(self, in_planes: int, planes: int, stride: int = 1, num_splits: int = 0):\n\n        super(BasicBlock, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_planes,\n                               planes,\n                               kernel_size=3,\n                               stride=stride,\n                               padding=1,\n                               bias=False)\n        self.bn1 = get_norm_layer(planes, num_splits)\n\n        self.conv2 = nn.Conv2d(planes,\n                               planes,\n                               kernel_size=3,\n                               stride=1,\n                               padding=1,\n                               bias=False)\n        self.bn2 = get_norm_layer(planes, num_splits)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes,\n                          self.expansion*planes,\n                          kernel_size=1,\n                          stride=stride,\n                          bias=False),\n                get_norm_layer(self.expansion * planes, num_splits)\n            )",
  "def forward(self, x: torch.Tensor):\n        \"\"\"Forward pass through basic ResNet block.\n\n        Args:\n            x:\n                Tensor of shape bsz x channels x W x H\n\n        Returns:\n            Tensor of shape bsz x channels x W x H\n        \"\"\"\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = F.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        out += self.shortcut(x)\n        out = F.relu(out)\n\n        return out",
  "def __init__(self, in_planes: int, planes: int, stride: int = 1, num_splits: int = 0):\n\n        super(Bottleneck, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = get_norm_layer(planes, num_splits)\n\n        self.conv2 = nn.Conv2d(planes,\n                               planes,\n                               kernel_size=3,\n                               stride=stride,\n                               padding=1,\n                               bias=False)\n        self.bn2 = get_norm_layer(planes, num_splits)\n\n        self.conv3 = nn.Conv2d(planes,\n                               self.expansion*planes,\n                               kernel_size=1,\n                               bias=False)\n        self.bn3 = get_norm_layer(self.expansion * planes, num_splits)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes,\n                          self.expansion*planes,\n                          kernel_size=1,\n                          stride=stride,\n                          bias=False),\n                get_norm_layer(self.expansion * planes, num_splits)\n            )",
  "def forward(self, x):\n        \"\"\"Forward pass through bottleneck ResNet block.\n\n        Args:\n            x:\n                Tensor of shape bsz x channels x W x H\n\n        Returns:\n            Tensor of shape bsz x channels x W x H\n        \"\"\"\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = F.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = F.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        out += self.shortcut(x)\n        out = F.relu(out)\n\n        return out",
  "def __init__(self,\n                 block: nn.Module = BasicBlock,\n                 layers: List[int] = [2, 2, 2, 2],\n                 num_classes: int = 10,\n                 width: float = 1.,\n                 num_splits: int = 0):\n\n        super(ResNet, self).__init__()\n        self.in_planes = int(64 * width)\n\n        self.base = int(64 * width)\n\n        self.conv1 = nn.Conv2d(3,\n                               self.base,\n                               kernel_size=3,\n                               stride=1,\n                               padding=1,\n                               bias=False)\n        self.bn1 = get_norm_layer(self.base, num_splits)\n        self.layer1 = self._make_layer(block, self.base, layers[0], stride=1, num_splits=num_splits)\n        self.layer2 = self._make_layer(block, self.base*2, layers[1], stride=2, num_splits=num_splits)\n        self.layer3 = self._make_layer(block, self.base*4, layers[2], stride=2, num_splits=num_splits)\n        self.layer4 = self._make_layer(block, self.base*8, layers[3], stride=2, num_splits=num_splits)\n        self.linear = nn.Linear(self.base*8*block.expansion, num_classes)",
  "def _make_layer(self, block, planes, layers, stride, num_splits):\n        strides = [stride] + [1]*(layers-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride, num_splits))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)",
  "def forward(self, x: torch.Tensor):\n        \"\"\"Forward pass through ResNet.\n\n        Args:\n            x:\n                Tensor of shape bsz x channels x W x H\n        \n        Returns:\n            Output tensor of shape bsz x num_classes\n\n        \"\"\"\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out",
  "def checkpoints():\n    \"\"\"Returns the Lightly model zoo as a list of checkpoints.\n\n    Checkpoints:\n        ResNet-9:\n            SimCLR with width = 0.0625 and num_ftrs = 16\n        ResNet-9:\n            SimCLR with width = 0.125 and num_ftrs = 16\n        ResNet-18:\n            SimCLR with width = 1.0 and num_ftrs = 16\n        ResNet-18:\n            SimCLR with width = 1.0 and num_ftrs = 32\n        ResNet-34:\n            SimCLR with width = 1.0 and num_ftrs = 16\n        ResNet-34:\n            SimCLR with width = 1.0 and num_ftrs = 32\n\n    Returns:\n        A list of available checkpoints as URLs.\n\n    \"\"\"\n    return [item for key, item in ZOO.items()]",
  "class SelfSupervisedEmbedding(BaseEmbedding):\n    \"\"\"Implementation of self-supervised embedding models.\n\n    Implements an embedding strategy based on self-supervised learning. A\n    model backbone, self-supervised criterion, optimizer, and dataloader are\n    passed to the constructor. The embedding itself is a pytorch-lightning\n    module.\n\n    The implementation is based on contrastive learning.\n\n    * SimCLR: https://arxiv.org/abs/2002.05709\n    * MoCo: https://arxiv.org/abs/1911.05722\n    * SimSiam: https://arxiv.org/abs/2011.10566\n\n    Attributes:\n        model:\n            A backbone convolutional network with a projection head.\n        criterion:\n            A contrastive loss function.\n        optimizer:\n            A PyTorch optimizer.\n        dataloader:\n            A torchvision dataloader.\n        scheduler:\n            A PyTorch learning rate scheduler.\n\n    Examples:\n        >>> # define a model, criterion, optimizer, and dataloader above\n        >>> import lightly.embedding as embedding\n        >>> encoder = SelfSupervisedEmbedding(\n        >>>     model,\n        >>>     criterion,\n        >>>     optimizer,\n        >>>     dataloader,\n        >>> )\n        >>> #\u00a0train the self-supervised embedding with default settings\n        >>> encoder.train_embedding()\n        >>> #\u00a0pass pytorch-lightning trainer arguments as kwargs\n        >>> encoder.train_embedding(max_epochs=10)\n\n    \"\"\"\n\n    def __init__(self,\n                 model: torch.nn.Module,\n                 criterion: torch.nn.Module,\n                 optimizer: torch.optim.Optimizer,\n                 dataloader: torch.utils.data.DataLoader,\n                 scheduler=None):\n\n        super(SelfSupervisedEmbedding, self).__init__(\n            model, criterion, optimizer, dataloader, scheduler)\n\n    def embed(self,\n              dataloader: torch.utils.data.DataLoader,\n              device: torch.device = None,\n              to_numpy: bool = True):\n        \"\"\"Embeds images in a vector space.\n\n        Args:\n            dataloader:\n                A PyTorch dataloader.\n            device:\n                Selected device (`cpu`, `cuda`, see PyTorch documentation)\n            to_numpy:\n                Whether to return the embeddings as numpy array.\n\n        Returns:\n            A tuple consisting of a tensor or ndarray of embeddings\n            with shape n_images x num_ftrs and labels, fnames\n\n        Examples:\n            >>> # embed images in vector space\n            >>> embeddings, labels, fnames = encoder.embed(dataloader)\n\n        \"\"\"\n\n        self.model.eval()\n        embeddings, labels, fnames = None, None, []\n\n        if lightly._is_prefetch_generator_available():\n            pbar = tqdm(BackgroundGenerator(dataloader, max_prefetch=3),\n                        total=len(dataloader))\n        else:\n            pbar = tqdm(dataloader, total=len(dataloader))\n\n        efficiency = 0.\n        embeddings = []\n        labels = []\n        with torch.no_grad():\n\n            start_time = time.time()\n            for (img, label, fname) in pbar:\n\n                img = img.to(device)\n                label = label.to(device)\n\n                fnames += [*fname]\n\n                batch_size = img.shape[0]\n                prepare_time = time.time()\n\n                emb = self.model.backbone(img)\n                emb = emb.detach().reshape(batch_size, -1)\n\n                embeddings.append(emb)\n                labels.append(label)\n\n                process_time = time.time()\n\n                efficiency = \\\n                    (process_time - prepare_time) / (process_time - start_time)\n                pbar.set_description(\n                    \"Compute efficiency: {:.2f}\".format(efficiency))\n                start_time = time.time()\n\n            embeddings = torch.cat(embeddings, 0)\n            labels = torch.cat(labels, 0)\n            if to_numpy:\n                embeddings = embeddings.cpu().numpy()\n                labels = labels.cpu().numpy()\n\n        return embeddings, labels, fnames",
  "def __init__(self,\n                 model: torch.nn.Module,\n                 criterion: torch.nn.Module,\n                 optimizer: torch.optim.Optimizer,\n                 dataloader: torch.utils.data.DataLoader,\n                 scheduler=None):\n\n        super(SelfSupervisedEmbedding, self).__init__(\n            model, criterion, optimizer, dataloader, scheduler)",
  "def embed(self,\n              dataloader: torch.utils.data.DataLoader,\n              device: torch.device = None,\n              to_numpy: bool = True):\n        \"\"\"Embeds images in a vector space.\n\n        Args:\n            dataloader:\n                A PyTorch dataloader.\n            device:\n                Selected device (`cpu`, `cuda`, see PyTorch documentation)\n            to_numpy:\n                Whether to return the embeddings as numpy array.\n\n        Returns:\n            A tuple consisting of a tensor or ndarray of embeddings\n            with shape n_images x num_ftrs and labels, fnames\n\n        Examples:\n            >>> # embed images in vector space\n            >>> embeddings, labels, fnames = encoder.embed(dataloader)\n\n        \"\"\"\n\n        self.model.eval()\n        embeddings, labels, fnames = None, None, []\n\n        if lightly._is_prefetch_generator_available():\n            pbar = tqdm(BackgroundGenerator(dataloader, max_prefetch=3),\n                        total=len(dataloader))\n        else:\n            pbar = tqdm(dataloader, total=len(dataloader))\n\n        efficiency = 0.\n        embeddings = []\n        labels = []\n        with torch.no_grad():\n\n            start_time = time.time()\n            for (img, label, fname) in pbar:\n\n                img = img.to(device)\n                label = label.to(device)\n\n                fnames += [*fname]\n\n                batch_size = img.shape[0]\n                prepare_time = time.time()\n\n                emb = self.model.backbone(img)\n                emb = emb.detach().reshape(batch_size, -1)\n\n                embeddings.append(emb)\n                labels.append(label)\n\n                process_time = time.time()\n\n                efficiency = \\\n                    (process_time - prepare_time) / (process_time - start_time)\n                pbar.set_description(\n                    \"Compute efficiency: {:.2f}\".format(efficiency))\n                start_time = time.time()\n\n            embeddings = torch.cat(embeddings, 0)\n            labels = torch.cat(labels, 0)\n            if to_numpy:\n                embeddings = embeddings.cpu().numpy()\n                labels = labels.cpu().numpy()\n\n        return embeddings, labels, fnames",
  "class BaseEmbedding(lightning.LightningModule):\n    \"\"\"All trainable embeddings must inherit from BaseEmbedding.\n\n    \"\"\"\n\n    def __init__(self,\n                 model,\n                 criterion,\n                 optimizer,\n                 dataloader,\n                 scheduler=None):\n        \"\"\" Constructor\n\n        Args:\n            model: (torch.nn.Module)\n            criterion: (torch.nn.Module)\n            optimizer: (torch.optim.Optimizer)\n            dataloader: (torch.utils.data.DataLoader)\n\n        \"\"\"\n\n        super(BaseEmbedding, self).__init__()\n        self.model = model\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.dataloader = dataloader\n        self.scheduler = scheduler\n        self.checkpoint = None\n        self.cwd = os.getcwd()\n\n        self.checkpoint_callback = None\n        self.init_checkpoint_callback()\n\n    def forward(self, x0, x1):\n        return self.model(x0, x1)\n\n    def training_step(self, batch, batch_idx):\n\n        # get the two image transformations\n        (x0, x1), _, _ = batch\n        # forward pass of the transformations\n        y0, y1 = self(x0, x1)\n        # calculate loss\n        loss = self.criterion(y0, y1)\n        # log loss and return\n        self.log('loss', loss)\n        return loss\n\n    def configure_optimizers(self):\n        if self.scheduler is None:\n            return self.optimizer\n        else:\n            return [self.optimizer], [self.scheduler]\n\n    def train_dataloader(self):\n        return self.dataloader\n\n    def train_embedding(self, **kwargs):\n        \"\"\" Train the model on the provided dataset.\n\n        Args:\n            **kwargs: pylightning_trainer arguments, examples include:\n                min_epochs: (int) Minimum number of epochs to train\n                max_epochs: (int) Maximum number of epochs to train\n                gpus: (int) number of gpus to use\n\n        Returns:\n            A trained encoder, ready for embedding datasets.\n\n        \"\"\"\n\n        trainer = pl.Trainer(**kwargs, callbacks=[self.checkpoint_callback])\n\n        trainer.fit(self)\n\n        self.checkpoint = self.checkpoint_callback.best_model_path\n        self.checkpoint = os.path.join(self.cwd, self.checkpoint)\n\n    def embed(self, *args, **kwargs):\n        \"\"\"Must be implemented by classes which inherit from BaseEmbedding.\n\n        \"\"\"\n        raise NotImplementedError()\n\n    def init_checkpoint_callback(self,\n                                 save_last=False,\n                                 save_top_k=0,\n                                 monitor='loss',\n                                 dirpath=None):\n        \"\"\"Initializes the checkpoint callback.\n\n        Args:\n            save_last:\n                Whether or not to save the checkpoint of the last epoch.\n            save_top_k:\n                Save the top_k model checkpoints.\n            monitor:\n                Which quantity to monitor.\n            dirpath:\n                Where to save the checkpoint.\n\n        \"\"\"\n        # initialize custom model checkpoint\n        self.checkpoint_callback = CustomModelCheckpoint()\n        self.checkpoint_callback.save_last = save_last\n        self.checkpoint_callback.save_top_k = save_top_k\n        self.checkpoint_callback.monitor = monitor\n\n        dirpath = self.cwd if dirpath is None else dirpath\n        self.checkpoint_callback.dirpath = dirpath",
  "def __init__(self,\n                 model,\n                 criterion,\n                 optimizer,\n                 dataloader,\n                 scheduler=None):\n        \"\"\" Constructor\n\n        Args:\n            model: (torch.nn.Module)\n            criterion: (torch.nn.Module)\n            optimizer: (torch.optim.Optimizer)\n            dataloader: (torch.utils.data.DataLoader)\n\n        \"\"\"\n\n        super(BaseEmbedding, self).__init__()\n        self.model = model\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.dataloader = dataloader\n        self.scheduler = scheduler\n        self.checkpoint = None\n        self.cwd = os.getcwd()\n\n        self.checkpoint_callback = None\n        self.init_checkpoint_callback()",
  "def forward(self, x0, x1):\n        return self.model(x0, x1)",
  "def training_step(self, batch, batch_idx):\n\n        # get the two image transformations\n        (x0, x1), _, _ = batch\n        # forward pass of the transformations\n        y0, y1 = self(x0, x1)\n        # calculate loss\n        loss = self.criterion(y0, y1)\n        # log loss and return\n        self.log('loss', loss)\n        return loss",
  "def configure_optimizers(self):\n        if self.scheduler is None:\n            return self.optimizer\n        else:\n            return [self.optimizer], [self.scheduler]",
  "def train_dataloader(self):\n        return self.dataloader",
  "def train_embedding(self, **kwargs):\n        \"\"\" Train the model on the provided dataset.\n\n        Args:\n            **kwargs: pylightning_trainer arguments, examples include:\n                min_epochs: (int) Minimum number of epochs to train\n                max_epochs: (int) Maximum number of epochs to train\n                gpus: (int) number of gpus to use\n\n        Returns:\n            A trained encoder, ready for embedding datasets.\n\n        \"\"\"\n\n        trainer = pl.Trainer(**kwargs, callbacks=[self.checkpoint_callback])\n\n        trainer.fit(self)\n\n        self.checkpoint = self.checkpoint_callback.best_model_path\n        self.checkpoint = os.path.join(self.cwd, self.checkpoint)",
  "def embed(self, *args, **kwargs):\n        \"\"\"Must be implemented by classes which inherit from BaseEmbedding.\n\n        \"\"\"\n        raise NotImplementedError()",
  "def init_checkpoint_callback(self,\n                                 save_last=False,\n                                 save_top_k=0,\n                                 monitor='loss',\n                                 dirpath=None):\n        \"\"\"Initializes the checkpoint callback.\n\n        Args:\n            save_last:\n                Whether or not to save the checkpoint of the last epoch.\n            save_top_k:\n                Save the top_k model checkpoints.\n            monitor:\n                Which quantity to monitor.\n            dirpath:\n                Where to save the checkpoint.\n\n        \"\"\"\n        # initialize custom model checkpoint\n        self.checkpoint_callback = CustomModelCheckpoint()\n        self.checkpoint_callback.save_last = save_last\n        self.checkpoint_callback.save_top_k = save_top_k\n        self.checkpoint_callback.monitor = monitor\n\n        dirpath = self.cwd if dirpath is None else dirpath\n        self.checkpoint_callback.dirpath = dirpath",
  "class CustomModelCheckpoint(cb.ModelCheckpoint):\n    \"\"\"Custom implementation of the Pytorch Lightning ModelCheckpoint.\n\n    Attributes:\n        checkpoint_fmt:\n            String which determines the format of the checkpoint name.\n            Default leads to, e.g.\n\n            >>> epoch_10.ckpt\n    \"\"\"\n\n    def __init__(self, checkpoint_fmt: str = 'lightly_epoch_{epoch}.ckpt'):\n        # use default initialization to prevent compatability\n        # issues in case pytorch_lightning changes attributes\n        super(CustomModelCheckpoint, self).__init__()\n        self.checkpoint_fmt = checkpoint_fmt\n\n    def format_checkpoint_name(\n        self, epoch: int, metrics: Dict[str, Any], ver: Optional[int] = None\n    ) -> str:\n        \"\"\"Formats the format string to an actual checkpoint name.\n\n        Args:\n            epoch:\n                Training epoch of the checkpoint.\n\n        \"\"\"\n        # use custom template to prevent the = in the checkpoint name\n        return self.checkpoint_fmt.format(epoch=epoch)",
  "def __init__(self, checkpoint_fmt: str = 'lightly_epoch_{epoch}.ckpt'):\n        # use default initialization to prevent compatability\n        # issues in case pytorch_lightning changes attributes\n        super(CustomModelCheckpoint, self).__init__()\n        self.checkpoint_fmt = checkpoint_fmt",
  "def format_checkpoint_name(\n        self, epoch: int, metrics: Dict[str, Any], ver: Optional[int] = None\n    ) -> str:\n        \"\"\"Formats the format string to an actual checkpoint name.\n\n        Args:\n            epoch:\n                Training epoch of the checkpoint.\n\n        \"\"\"\n        # use custom template to prevent the = in the checkpoint name\n        return self.checkpoint_fmt.format(epoch=epoch)",
  "class PCA(object):\n    \"\"\"Handmade PCA to bypass sklearn dependency.\n\n    Attributes:\n        n_components:\n            Number of principal components to keep.\n        eps:\n            Epsilon for numerical stability.\n    \"\"\"\n\n    def __init__(self, n_components: int = 2, eps: float = 1e-10):\n        self.n_components = n_components\n        self.mean = None\n        self.w = None\n        self.eps = eps\n\n    def fit(self, X: np.ndarray):\n        \"\"\"Fits PCA to data in X.\n\n        Args:\n            X:\n                Datapoints stored in numpy array of size n x d.\n\n        Returns:\n            PCA object to transform datapoints.\n\n        \"\"\"\n        X = X.astype(np.float32)\n        self.mean = X.mean(axis=0)\n        X = X - self.mean + self.eps\n        cov = np.cov(X.T) / X.shape[0]\n        v, w = np.linalg.eig(cov)\n        idx = v.argsort()[::-1]\n        v, w = v[idx], w[:, idx]\n        self.w = w\n        return self\n\n    def transform(self, X: np.ndarray):\n        \"\"\"Uses PCA to transform data in X.\n\n        Args:\n            X:\n                Datapoints stored in numpy array of size n x d.\n\n        Returns:\n            Numpy array of n x p datapoints where p <= d.\n\n        \"\"\"\n        X = X.astype(np.float32)\n        X = X - self.mean + self.eps\n        return X.dot(self.w)[:, :self.n_components]",
  "def fit_pca(embeddings: np.ndarray, n_components: int = 2, fraction: float = None):\n    \"\"\"Fits PCA to randomly selected subset of embeddings.\n\n    For large datasets, it can be unfeasible to perform PCA on the whole data.\n    This method can fit a PCA on a fraction of the embeddings in order to save\n    computational resources.\n\n    Args:\n        embeddings:\n            Datapoints stored in numpy array of size n x d.\n        n_components:\n            Number of principal components to keep.\n        fraction:\n            Fraction of the dataset to fit PCA on.\n\n    Returns:\n        A transformer which can be used to transform embeddings\n        to lower dimensions.\n\n    Raises:\n        ValueError if fraction < 0 or fraction > 1.\n\n    \"\"\"\n    if fraction is not None:\n        if fraction < 0. or fraction > 1.:\n            msg = f'fraction must be in [0, 1] but was {fraction}.'\n            raise ValueError(msg)\n\n    N = embeddings.shape[0]\n    n = N if fraction is None else min(N, int(N * fraction))\n    X = embeddings[np.random.permutation(N)][:n]\n    return PCA(n_components=n_components).fit(X)",
  "def __init__(self, n_components: int = 2, eps: float = 1e-10):\n        self.n_components = n_components\n        self.mean = None\n        self.w = None\n        self.eps = eps",
  "def fit(self, X: np.ndarray):\n        \"\"\"Fits PCA to data in X.\n\n        Args:\n            X:\n                Datapoints stored in numpy array of size n x d.\n\n        Returns:\n            PCA object to transform datapoints.\n\n        \"\"\"\n        X = X.astype(np.float32)\n        self.mean = X.mean(axis=0)\n        X = X - self.mean + self.eps\n        cov = np.cov(X.T) / X.shape[0]\n        v, w = np.linalg.eig(cov)\n        idx = v.argsort()[::-1]\n        v, w = v[idx], w[:, idx]\n        self.w = w\n        return self",
  "def transform(self, X: np.ndarray):\n        \"\"\"Uses PCA to transform data in X.\n\n        Args:\n            X:\n                Datapoints stored in numpy array of size n x d.\n\n        Returns:\n            Numpy array of n x p datapoints where p <= d.\n\n        \"\"\"\n        X = X.astype(np.float32)\n        X = X - self.mean + self.eps\n        return X.dot(self.w)[:, :self.n_components]",
  "def save_embeddings(path: str,\n                    embeddings: np.ndarray,\n                    labels: List[int],\n                    filenames: List[str]):\n    \"\"\"Saves embeddings in a csv file in a Lightly compatible format.\n\n    Creates a csv file at the location specified by path and saves embeddings,\n    labels, and filenames.\n\n    Args:\n        path:\n            Path to the csv file.\n        embeddings:\n            Embeddings of the images as a numpy array (n x d).\n        labels:\n            List of integer labels.\n        filenames:\n            List of filenames.\n\n    Raises:\n        ValueError if embeddings, labels, and filenames have different lengths.\n\n    Examples:\n        >>> import lightly.utils.io as io\n        >>> io.save_embeddings(\n        >>>     'path/to/my/embeddings.csv',\n        >>>     embeddings,\n        >>>     labels,\n        >>>     filenames)\n    \"\"\"\n    n_embeddings = len(embeddings)\n    n_filenames = len(filenames)\n    n_labels = len(labels)\n    \n    if n_embeddings != n_labels or n_filenames != n_labels:\n        msg = 'Length of embeddings, labels, and filenames should be equal '\n        msg += f' but are not: ({n_embeddings}, {n_filenames}, {n_labels})'\n        raise ValueError(msg)\n\n    header = ['filenames']\n    header = header + [f'embedding_{i}' for i in range(embeddings.shape[-1])]\n    header = header + ['labels']\n    with open(path, 'w', newline='') as csv_file:\n        writer = csv.writer(csv_file, delimiter=',')\n        writer.writerow(header)\n        for filename, embedding, label in zip(filenames, embeddings, labels):\n            writer.writerow([filename] + list(embedding) + [label])",
  "def load_embeddings(path: str):\n    \"\"\"Loads embeddings from a csv file in a Lightly compatible format.\n\n    Args:\n        path:\n            Path to the csv file.\n\n    Returns:\n        The embeddings as a numpy array, labels as a list of integers, and\n        filenames as a list of strings in the order they were saved.\n\n        The embeddings will always be of the Float32 datatype.\n\n    Examples:\n        >>> import lightly.utils.io as io\n        >>> embeddings, labels, filenames = io.load_embeddings(\n        >>>     'path/to/my/embeddings.csv')\n\n    \"\"\"\n    filenames, labels = [], []\n    embeddings = []\n    with open(path, 'r', newline='') as csv_file:\n        reader = csv.reader(csv_file, delimiter=',')\n        for i, row in enumerate(reader):\n            # skip header\n            if i == 0:\n                continue\n            #\u00a0read filenames and labels\n            filenames.append(row[0])\n            labels.append(int(row[-1]))\n            # read embeddings\n            embeddings.append(row[1:-1])\n\n    embeddings = np.array(embeddings).astype(np.float32)\n    return embeddings, labels, filenames",
  "def load_embeddings_as_dict(path: str,\n                            embedding_name: str = 'default',\n                            return_all: bool = False):\n    \"\"\"Loads embeddings from csv and store it in a dictionary for transfer.\n\n    Loads embeddings to a dictionary which can be serialized and sent to the\n    Lightly servers. It is recommended that the embedding_name is always\n    specified because the Lightly web-app does not allow two embeddings with\n    the same name.\n    \n    Args:\n        path:\n            Path to the csv file.\n        embedding_name:\n            Name of the embedding for the platform.\n        return_all:\n            If true, return embeddings, labels, and filenames, too.\n\n    Returns:\n        A dictionary containing the embedding information (see load_embeddings)\n\n    Examples:\n        >>> import lightly.utils.io as io\n        >>> embedding_dict = io.load_embeddings_as_dict(\n        >>>     'path/to/my/embeddings.csv',\n        >>>     embedding_name='MyEmbeddings')\n        >>>\n        >>> result = io.load_embeddings_as_dict(\n        >>>     'path/to/my/embeddings.csv',\n        >>>     embedding_name='MyEmbeddings',\n        >>>     return_all=True)\n        >>> embedding_dict, embeddings, labels, filenames = result\n\n    \"\"\"\n    embeddings, labels, filenames = load_embeddings(path)\n\n    # build dictionary\n    data = {}\n    data['embeddingName'] = embedding_name\n    data['embeddings'] = []\n    for embedding, filename, label in zip(embeddings, filenames, labels):\n        item = {}\n        item['fileName'] = filename\n        item['value'] = embedding.tolist()\n        item['label'] = label\n        data['embeddings'].append(item)\n\n    # return embeddings along with dictionary\n    if return_all:\n        return data, embeddings, labels, filenames\n    else:\n        return data",
  "def knn_predict(feature: torch.Tensor,\n                feature_bank: torch.Tensor,\n                feature_labels: torch.Tensor, \n                num_classes: int,\n                knn_k: int=200,\n                knn_t: float=0.1) -> torch.Tensor:\n    \"\"\"Run kNN predictions on features based on a feature bank\n\n    This method is commonly used to monitor performance of self-supervised\n    learning methods.\n\n    The default parameters are the ones\n    used in https://arxiv.org/pdf/1805.01978v1.pdf.\n\n    Args:\n        feature: \n            Tensor of shape [N, D] for which you want predictions\n        feature_bank: \n            Tensor of a database of features used for kNN\n        feature_labels: \n            Labels for the features in our feature_bank\n        num_classes: \n            Number of classes (e.g. `10` for CIFAR-10)\n        knn_k: \n            Number of k neighbors used for kNN\n        knn_t: \n            Temperature parameter to reweights similarities for kNN\n\n    Returns:\n        A tensor containing the kNN predictions\n\n    Examples:\n        >>> images, targets, _ = batch\n        >>> feature = backbone(images).squeeze()\n        >>> # we recommend to normalize the features\n        >>> feature = F.normalize(feature, dim=1)\n        >>> pred_labels = knn_predict(\n        >>>     feature,\n        >>>     feature_bank,\n        >>>     targets_bank,\n        >>>     num_classes=10,\n        >>> )\n    \"\"\"\n\n    # compute cos similarity between each feature vector and feature bank ---> [B, N]\n    sim_matrix = torch.mm(feature, feature_bank)\n    # [B, K]\n    sim_weight, sim_indices = sim_matrix.topk(k=knn_k, dim=-1)\n    # [B, K]\n    sim_labels = torch.gather(feature_labels.expand(\n        feature.size(0), -1), dim=-1, index=sim_indices)\n    # we do a reweighting of the similarities\n    sim_weight = (sim_weight / knn_t).exp()\n    # counts for each class\n    one_hot_label = torch.zeros(feature.size(\n        0) * knn_k, num_classes, device=sim_labels.device)\n    # [B*K, C]\n    one_hot_label = one_hot_label.scatter(\n        dim=-1, index=sim_labels.view(-1, 1), value=1.0)\n    # weighted score ---> [B, C]\n    pred_scores = torch.sum(one_hot_label.view(feature.size(\n        0), -1, num_classes) * sim_weight.unsqueeze(dim=-1), dim=1)\n    pred_labels = pred_scores.argsort(dim=-1, descending=True)\n    return pred_labels",
  "class BenchmarkModule(pl.LightningModule):\n    \"\"\"A PyTorch Lightning Module for automated kNN callback\n\n    At the end of every training epoch we create a feature bank by feeding the\n    `dataloader_kNN` passed to the module through the backbone.\n    At every validation step we predict features on the validation data.\n    After all predictions on validation data (validation_epoch_end) we evaluate\n    the predictions on a kNN classifier on the validation data using the\n    feature_bank features from the train data.\n\n    We can access the highest test accuracy during a kNN prediction \n    using the `max_accuracy` attribute.\n\n    Attributes:\n        backbone:\n            The backbone model used for kNN validation. Make sure that you set the\n            backbone when inheriting from `BenchmarkModule`.\n        max_accuracy:\n            Floating point number between 0.0 and 1.0 representing the maximum\n            test accuracy the benchmarked model has achieved.\n        dataloader_kNN:\n            Dataloader to be used after each training epoch to create feature bank.\n        num_classes:\n            Number of classes. E.g. for cifar10 we have 10 classes. (default: 10)\n        knn_k:\n            Number of nearest neighbors for kNN\n        knn_t:\n            Temperature parameter for kNN\n\n    Examples:\n        >>> class SimSiamModel(BenchmarkingModule):\n        >>>     def __init__(dataloader_kNN, num_classes):\n        >>>         super().__init__(dataloader_kNN, num_classes)\n        >>>         resnet = lightly.models.ResNetGenerator('resnet-18')\n        >>>         self.backbone = nn.Sequential(\n        >>>             *list(resnet.children())[:-1],\n        >>>             nn.AdaptiveAvgPool2d(1),\n        >>>         )\n        >>>         self.resnet_simsiam = \n        >>>             lightly.models.SimSiam(self.backbone, num_ftrs=512)\n        >>>         self.criterion = lightly.loss.SymNegCosineSimilarityLoss()\n        >>>\n        >>>     def forward(self, x):\n        >>>         self.resnet_simsiam(x)\n        >>>\n        >>>     def training_step(self, batch, batch_idx):\n        >>>         (x0, x1), _, _ = batch\n        >>>         x0, x1 = self.resnet_simsiam(x0, x1)\n        >>>         loss = self.criterion(x0, x1)\n        >>>         return loss\n        >>>     def configure_optimizers(self):\n        >>>         optim = torch.optim.SGD(\n        >>>             self.resnet_simsiam.parameters(), lr=6e-2, momentum=0.9\n        >>>         )\n        >>>         return [optim]\n        >>>\n        >>> model = SimSiamModel(dataloader_train_kNN)\n        >>> trainer = pl.Trainer()\n        >>> trainer.fit(\n        >>>     model,\n        >>>     train_dataloader=dataloader_train_ssl,\n        >>>     val_dataloaders=dataloader_test\n        >>> )\n        >>> # you can get the peak accuracy using\n        >>> print(model.max_accuracy)\n\n    \"\"\"\n\n    def __init__(self,\n                 dataloader_kNN: DataLoader,\n                 num_classes: int,\n                 knn_k: int=200,\n                 knn_t: float=0.1):\n        super().__init__()\n        self.backbone = nn.Module()\n        self.max_accuracy = 0.0\n        self.dataloader_kNN = dataloader_kNN\n        self.num_classes = num_classes\n        self.knn_k = knn_k\n        self.knn_t = knn_t\n\n        # create dummy param to keep track of the device the model is using\n        self.dummy_param = nn.Parameter(torch.empty(0))\n\n    def training_epoch_end(self, outputs):\n        # update feature bank at the end of each training epoch\n        self.backbone.eval()\n        self.feature_bank = []\n        self.targets_bank = []\n        with torch.no_grad():\n            for data in self.dataloader_kNN:\n                img, target, _ = data\n                img = img.to(self.dummy_param.device)\n                target = target.to(self.dummy_param.device)\n                feature = self.backbone(img).squeeze()\n                feature = F.normalize(feature, dim=1)\n                self.feature_bank.append(feature)\n                self.targets_bank.append(target)\n        self.feature_bank = torch.cat(\n            self.feature_bank, dim=0).t().contiguous()\n        self.targets_bank = torch.cat(\n            self.targets_bank, dim=0).t().contiguous()\n        self.backbone.train()\n\n    def validation_step(self, batch, batch_idx):\n        # we can only do kNN predictions once we have a feature bank\n        if hasattr(self, 'feature_bank') and hasattr(self, 'targets_bank'):\n            images, targets, _ = batch\n            feature = self.backbone(images).squeeze()\n            feature = F.normalize(feature, dim=1)\n            pred_labels = knn_predict(\n                feature,\n                self.feature_bank,\n                self.targets_bank,\n                self.num_classes,\n                self.knn_k,\n                self.knn_t\n            )\n            num = images.size()\n            top1 = (pred_labels[:, 0] == targets).float().sum()\n            return (num, top1)\n\n    def validation_epoch_end(self, outputs):\n        device = self.dummy_param.device\n        if outputs:\n            total_num = torch.Tensor([0]).to(device)\n            total_top1 = torch.Tensor([0.]).to(device)\n            for (num, top1) in outputs:\n                total_num += num[0]\n                total_top1 += top1\n             \n            if dist.is_initialized() and dist.get_world_size() > 1:\n                dist.all_reduce(total_num)\n                dist.all_reduce(total_top1)\n\n            acc = float(total_top1.item() / total_num.item())\n            if acc > self.max_accuracy:\n                self.max_accuracy = acc\n            self.log('kNN_accuracy', acc * 100.0, prog_bar=True)",
  "def __init__(self,\n                 dataloader_kNN: DataLoader,\n                 num_classes: int,\n                 knn_k: int=200,\n                 knn_t: float=0.1):\n        super().__init__()\n        self.backbone = nn.Module()\n        self.max_accuracy = 0.0\n        self.dataloader_kNN = dataloader_kNN\n        self.num_classes = num_classes\n        self.knn_k = knn_k\n        self.knn_t = knn_t\n\n        # create dummy param to keep track of the device the model is using\n        self.dummy_param = nn.Parameter(torch.empty(0))",
  "def training_epoch_end(self, outputs):\n        # update feature bank at the end of each training epoch\n        self.backbone.eval()\n        self.feature_bank = []\n        self.targets_bank = []\n        with torch.no_grad():\n            for data in self.dataloader_kNN:\n                img, target, _ = data\n                img = img.to(self.dummy_param.device)\n                target = target.to(self.dummy_param.device)\n                feature = self.backbone(img).squeeze()\n                feature = F.normalize(feature, dim=1)\n                self.feature_bank.append(feature)\n                self.targets_bank.append(target)\n        self.feature_bank = torch.cat(\n            self.feature_bank, dim=0).t().contiguous()\n        self.targets_bank = torch.cat(\n            self.targets_bank, dim=0).t().contiguous()\n        self.backbone.train()",
  "def validation_step(self, batch, batch_idx):\n        # we can only do kNN predictions once we have a feature bank\n        if hasattr(self, 'feature_bank') and hasattr(self, 'targets_bank'):\n            images, targets, _ = batch\n            feature = self.backbone(images).squeeze()\n            feature = F.normalize(feature, dim=1)\n            pred_labels = knn_predict(\n                feature,\n                self.feature_bank,\n                self.targets_bank,\n                self.num_classes,\n                self.knn_k,\n                self.knn_t\n            )\n            num = images.size()\n            top1 = (pred_labels[:, 0] == targets).float().sum()\n            return (num, top1)",
  "def validation_epoch_end(self, outputs):\n        device = self.dummy_param.device\n        if outputs:\n            total_num = torch.Tensor([0]).to(device)\n            total_top1 = torch.Tensor([0.]).to(device)\n            for (num, top1) in outputs:\n                total_num += num[0]\n                total_top1 += top1\n             \n            if dist.is_initialized() and dist.get_world_size() > 1:\n                dist.all_reduce(total_num)\n                dist.all_reduce(total_top1)\n\n            acc = float(total_top1.item() / total_num.item())\n            if acc > self.max_accuracy:\n                self.max_accuracy = acc\n            self.log('kNN_accuracy', acc * 100.0, prog_bar=True)",
  "def _download_cli(cfg, is_cli_call=True):\n\n    tag_name = cfg['tag_name']\n    dataset_id = cfg['dataset_id']\n    token = cfg['token']\n\n    if not tag_name:\n        print('Please specify a tag name')\n        print('For help, try: lightly-download --help')\n        return\n\n    if not token or not dataset_id:\n        print('Please specify your access token and dataset id')\n        print('For help, try: lightly-download --help')\n        return\n\n    api_workflow_client = ApiWorkflowClient(\n        token=token, dataset_id=dataset_id\n    )\n\n    # get tag id\n    tag_name_id_dict = dict([tag.name, tag.id] for tag in api_workflow_client._get_all_tags())\n    tag_id = tag_name_id_dict.get(tag_name, None)\n    if tag_id is None:\n        print(f'The specified tag {tag_name} does not exist.')\n        return\n\n    # get tag data\n    tag_data = api_workflow_client.tags_api.get_tag_by_tag_id(\n        dataset_id=dataset_id, tag_id=tag_id\n    )\n    \n    # get samples\n    chosen_samples_ids = BitMask.from_hex(tag_data.bit_mask_data).to_indices()\n    samples = [api_workflow_client.filenames_on_server[i] for i in chosen_samples_ids]\n\n    # store sample names in a .txt file\n    with open(cfg['tag_name'] + '.txt', 'w') as f:\n        for item in samples:\n            f.write(\"%s\\n\" % item)\n\n    msg = 'The list of files in tag {} is stored at: '.format(cfg['tag_name'])\n    msg += os.path.join(os.getcwd(), cfg['tag_name'] + '.txt')\n    print(msg, flush=True)\n\n    if not cfg['input_dir'] and cfg['output_dir']:\n        # download full images from api\n        output_dir = fix_input_path(cfg['output_dir'])\n        api_workflow_client.download_dataset(output_dir, tag_name=tag_name)\n\n    elif cfg['input_dir'] and cfg['output_dir']:\n        input_dir = fix_input_path(cfg['input_dir'])\n        output_dir = fix_input_path(cfg['output_dir'])\n        print(f'Copying files from {input_dir} to {output_dir}.')\n\n        # create a dataset from the input directory\n        dataset = data.LightlyDataset(input_dir=input_dir)\n\n        # dump the dataset in the output directory\n        dataset.dump(output_dir, samples)",
  "def download_cli(cfg):\n    \"\"\"Download images from the Lightly platform.\n\n    Args:\n        cfg:\n            The default configs are loaded from the config file.\n            To overwrite them please see the section on the config file \n            (.config.config.yaml).\n    \n    Command-Line Args:\n        tag_name:\n            Download all images from the requested tag. Use initial-tag\n            to get all images from the dataset.\n        token:\n            User access token to the Lightly platform. If dataset_id\n            and token are specified, the images and embeddings are \n            uploaded to the platform.\n        dataset_id:\n            Identifier of the dataset on the Lightly platform. If \n            dataset_id and token are specified, the images and \n            embeddings are uploaded to the platform.\n        input_dir:\n            If input_dir and output_dir are specified, lightly will copy\n            all images belonging to the tag from the input_dir to the \n            output_dir.\n        output_dir:\n            If input_dir and output_dir are specified, lightly will copy\n            all images belonging to the tag from the input_dir to the \n            output_dir.\n\n    Examples:\n        >>> #\u00a0download list of all files in the dataset from the Lightly platform\n        >>> lightly-download token='123' dataset_id='XYZ'\n        >>> \n        >>> # download list of all files in tag 'my-tag' from the Lightly platform\n        >>> lightly-download token='123' dataset_id='XYZ' tag_name='my-tag'\n        >>>\n        >>> # download all images in tag 'my-tag' from the Lightly platform\n        >>> lightly-download token='123' dataset_id='XYZ' tag_name='my-tag' output_dir='my_data/'\n        >>>\n        >>> # copy all files in 'my-tag' to a new directory\n        >>> lightly-download token='123' dataset_id='XYZ' tag_name='my-tag' input_dir='data/' output_dir='my_data/'\n\n\n    \"\"\"\n    _download_cli(cfg)",
  "def entry():\n    download_cli()",
  "def _train_cli(cfg, is_cli_call=True):\n\n    input_dir = cfg['input_dir']\n    if input_dir and is_cli_call:\n        input_dir = fix_input_path(input_dir)\n\n    if 'seed' in cfg.keys():\n        seed = cfg['seed']\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n    if torch.cuda.is_available():\n        device = 'cuda'\n    elif cfg['trainer'] and cfg['trainer']['gpus']:\n        device = 'cpu'\n        cfg['trainer']['gpus'] = 0\n\n    if cfg['loader']['batch_size'] < 64:\n        msg = 'Training a self-supervised model with a small batch size: {}! '\n        msg = msg.format(cfg['loader']['batch_size'])\n        msg += 'Small batch size may harm embedding quality. '\n        msg += 'You can specify the batch size via the loader key-word: '\n        msg += 'loader.batch_size=BSZ'\n        warnings.warn(msg)\n\n    state_dict = None\n    checkpoint = cfg['checkpoint']\n    if cfg['pre_trained'] and not checkpoint:\n        # if checkpoint wasn't specified explicitly and pre_trained is True\n        # try to load the checkpoint from the model zoo\n        checkpoint, key = get_ptmodel_from_config(cfg['model'])\n        if not checkpoint:\n            msg = 'Cannot download checkpoint for key {} '.format(key)\n            msg += 'because it does not exist! '\n            msg += 'Model will be trained from scratch.'\n            warnings.warn(msg)\n    elif checkpoint:\n        checkpoint = fix_input_path(checkpoint) if is_cli_call else checkpoint\n    \n    if checkpoint:\n        # load the PyTorch state dictionary and map it to the current device\n        if is_url(checkpoint):\n            state_dict = load_state_dict_from_url(\n                checkpoint, map_location=device\n            )['state_dict']\n        else:\n            state_dict = torch.load(\n                checkpoint, map_location=device\n            )['state_dict']\n\n    # load model\n    resnet = ResNetGenerator(cfg['model']['name'], cfg['model']['width'])\n    last_conv_channels = list(resnet.children())[-1].in_features\n    features = nn.Sequential(\n        get_norm_layer(3, 0),\n        *list(resnet.children())[:-1],\n        nn.Conv2d(last_conv_channels, cfg['model']['num_ftrs'], 1),\n        nn.AdaptiveAvgPool2d(1),\n    )\n\n    model = SimCLR(\n        features,\n        num_ftrs=cfg['model']['num_ftrs'],\n        out_dim=cfg['model']['out_dim']\n    )\n    if state_dict is not None:\n        load_from_state_dict(model, state_dict)\n\n    criterion = NTXentLoss(**cfg['criterion'])\n    optimizer = torch.optim.SGD(model.parameters(), **cfg['optimizer'])\n\n    dataset = LightlyDataset(input_dir)\n\n    cfg['loader']['batch_size'] = min(\n        cfg['loader']['batch_size'],\n        len(dataset)\n    )\n\n    collate_fn = ImageCollateFunction(**cfg['collate'])\n    dataloader = torch.utils.data.DataLoader(dataset,\n                                             **cfg['loader'],\n                                             collate_fn=collate_fn)\n\n    encoder = SelfSupervisedEmbedding(model, criterion, optimizer, dataloader)\n    encoder.init_checkpoint_callback(**cfg['checkpoint_callback'])\n    encoder.train_embedding(**cfg['trainer'])\n\n    print('Best model is stored at: %s' % (encoder.checkpoint))\n    return encoder.checkpoint",
  "def train_cli(cfg):\n    \"\"\"Train a self-supervised model from the command-line.\n\n    Args:\n        cfg:\n            The default configs are loaded from the config file.\n            To overwrite them please see the section on the config file \n            (.config.config.yaml).\n    \n    Command-Line Args:\n        input_dir:\n            Path to the input directory where images are stored.\n\n    Examples:\n        >>> #\u00a0train model with default settings\n        >>> lightly-train input_dir=data/\n        >>>\n        >>> # train model with batches of size 128\n        >>> lightly-train input_dir=data/ loader.batch_size=128\n        >>>\n        >>> # train model for 10 epochs\n        >>> lightly-train input_dir=data/ trainer.max_epochs=10\n\n    \"\"\"\n    return _train_cli(cfg)",
  "def entry():\n    train_cli()",
  "def _embed_cli(cfg, is_cli_call=True):\n\n    checkpoint = cfg['checkpoint']\n\n    input_dir = cfg['input_dir']\n    if input_dir and is_cli_call:\n        input_dir = fix_input_path(input_dir)\n\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n    else:\n        device = torch.device('cpu')\n\n    transform = torchvision.transforms.Compose([\n        torchvision.transforms.Resize((cfg['collate']['input_size'],\n                                       cfg['collate']['input_size'])),\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225])\n    ])\n\n    dataset = LightlyDataset(input_dir, transform=transform)\n\n    cfg['loader']['drop_last'] = False\n    cfg['loader']['shuffle'] = False\n    cfg['loader']['batch_size'] = min(\n        cfg['loader']['batch_size'],\n        len(dataset)\n    )\n    dataloader = torch.utils.data.DataLoader(dataset, **cfg['loader'])\n\n    # load the PyTorch state dictionary and map it to the current device    \n    state_dict = None\n    if not checkpoint:\n        checkpoint, key = get_ptmodel_from_config(cfg['model'])\n        if not checkpoint:\n            msg = 'Cannot download checkpoint for key {} '.format(key)\n            msg += 'because it does not exist!'\n            raise RuntimeError(msg)\n        state_dict = load_state_dict_from_url(\n            checkpoint, map_location=device\n        )['state_dict']\n    else:\n        checkpoint = fix_input_path(checkpoint) if is_cli_call else checkpoint\n        state_dict = torch.load(\n            checkpoint, map_location=device\n        )['state_dict']\n\n    # load model\n    resnet = ResNetGenerator(cfg['model']['name'], cfg['model']['width'])\n    last_conv_channels = list(resnet.children())[-1].in_features\n    features = nn.Sequential(\n        get_norm_layer(3, 0),\n        *list(resnet.children())[:-1],\n        nn.Conv2d(last_conv_channels, cfg['model']['num_ftrs'], 1),\n        nn.AdaptiveAvgPool2d(1),\n    )\n\n    model = SimCLR(\n        features,\n        num_ftrs=cfg['model']['num_ftrs'],\n        out_dim=cfg['model']['out_dim']\n    ).to(device)\n\n    if state_dict is not None:\n        load_from_state_dict(model, state_dict)\n\n    encoder = SelfSupervisedEmbedding(model, None, None, None)\n    embeddings, labels, filenames = encoder.embed(dataloader, device=device)\n\n    if is_cli_call:\n        path = os.path.join(os.getcwd(), 'embeddings.csv')\n        save_embeddings(path, embeddings, labels, filenames)\n        print('Embeddings are stored at %s' % (path))\n        return path\n\n    return embeddings, labels, filenames",
  "def embed_cli(cfg):\n    \"\"\"Embed images from the command-line.\n\n    Args:\n        cfg:\n            The default configs are loaded from the config file.\n            To overwrite them please see the section on the config file \n            (.config.config.yaml).\n    \n    Command-Line Args:\n        input_dir:\n            Path to the input directory where images are stored.\n        checkpoint:\n            Path to the checkpoint of a pretrained model. If left\n            empty, a pretrained model by lightly is used.\n\n    Examples:\n        >>> #\u00a0embed images with default settings and a lightly model\n        >>> lightly-embed input_dir=data/\n        >>>\n        >>> # embed images with default settings and a custom checkpoint\n        >>> lightly-embed input_dir=data/ checkpoint=my_checkpoint.ckpt\n        >>>\n        >>> # embed images with custom settings\n        >>> lightly-embed input_dir=data/ model.num_ftrs=32\n\n    \"\"\"\n    return _embed_cli(cfg)",
  "def entry():\n    embed_cli()",
  "def _version_cli():\n    version = lightly.__version__\n    print(f'lightly version {version}', flush=True)",
  "def version_cli(cfg):\n    \"\"\"Prints the version of the used lightly package to the terminal.\n\n    \"\"\"\n    _version_cli()",
  "def entry():\n    version_cli()",
  "def _upload_cli(cfg, is_cli_call=True):\n\n    input_dir = cfg['input_dir']\n    if input_dir and is_cli_call:\n        input_dir = fix_input_path(input_dir)\n\n    path_to_embeddings = cfg['embeddings']\n    if path_to_embeddings and is_cli_call:\n        path_to_embeddings = fix_input_path(path_to_embeddings)\n\n    dataset_id = cfg['dataset_id']\n    token = cfg['token']\n\n    size = cfg['resize']\n    if not isinstance(size, int):\n        size = tuple(size)\n    transform = None\n    if isinstance(size, tuple) or size > 0:\n        transform = torchvision.transforms.Resize(size)\n\n    if not token or not dataset_id:\n        print('Please specify your access token and dataset id.')\n        print('For help, try: lightly-upload --help')\n        return\n\n    api_workflow_client = ApiWorkflowClient(\n        token=token, dataset_id=dataset_id\n    )\n\n    if input_dir:\n        mode = cfg['upload']\n        dataset = LightlyDataset(input_dir=input_dir, transform=transform)\n        api_workflow_client.upload_dataset(\n            input=dataset, mode=mode\n        )\n\n    if path_to_embeddings:\n        name = cfg['embedding_name']\n        api_workflow_client.upload_embeddings(\n            path_to_embeddings_csv=path_to_embeddings, name=name\n        )",
  "def upload_cli(cfg):\n    \"\"\"Upload images/embeddings from the command-line to the Lightly platform.\n\n    Args:\n        cfg:\n            The default configs are loaded from the config file.\n            To overwrite them please see the section on the config file \n            (.config.config.yaml).\n    \n    Command-Line Args:\n        input_dir:\n            Path to the input directory where images are stored.\n        embeddings:\n            Path to the csv file storing the embeddings generated by\n            lightly.\n        token:\n            User access token to the Lightly platform. If dataset_id\n            and token are specified, the images and embeddings are \n            uploaded to the platform.\n        dataset_id:\n            Identifier of the dataset on the Lightly platform. If \n            dataset_id and token are specified, the images and \n            embeddings are uploaded to the platform.\n        upload:\n            String to determine whether to upload the full images, \n            thumbnails only, or metadata only.\n\n            Must be one of ['full', 'thumbnails', 'metadata']\n        embedding_name:\n            Assign the embedding a name in order to identify it on the \n            Lightly platform.\n        resize:\n            Desired size of the uploaded images. If negative, default size is used.\n            If size is a sequence like (h, w), output size will be matched to \n            this. If size is an int, smaller edge of the image will be matched \n            to this number. i.e, if height > width, then image will be rescaled\n            to (size * height / width, size).\n\n    Examples:\n        >>> #\u00a0upload thumbnails to the Lightly platform\n        >>> lightly-upload input_dir=data/ token='123' dataset_id='XYZ'\n        >>> \n        >>> # upload full images to the Lightly platform\n        >>> lightly-upload input_dir=data/ token='123' dataset_id='XYZ' upload='full'\n        >>>\n        >>> # upload metadata to the Lightly platform\n        >>> lightly-upload input_dir=data/ token='123' dataset_id='XYZ' upload='metadata'\n        >>>\n        >>> # upload embeddings to the Lightly platform (must have uploaded images beforehand)\n        >>> lightly-upload embeddings=embeddings.csv token='123' dataset_id='XYZ'\n        >>>\n        >>> # upload both, images and embeddings in a single command\n        >>> lightly-upload input_dir=data/ embeddings=embeddings.csv upload='full' \\\\\n        >>>     token='123' dataset_id='XYZ'\n\n    \"\"\"\n    _upload_cli(cfg)",
  "def entry():\n    upload_cli()",
  "def _lightly_cli(cfg, is_cli_call=True):\n\n    cfg['loader']['shuffle'] = True\n    cfg['loader']['drop_last'] = True\n    if cfg['trainer']['max_epochs'] > 0:\n        checkpoint = _train_cli(cfg, is_cli_call)\n    else:\n        checkpoint = ''\n\n    cfg['loader']['shuffle'] = False\n    cfg['loader']['drop_last'] = False\n    cfg['checkpoint'] = checkpoint\n\n    embeddings = _embed_cli(cfg, is_cli_call)\n    cfg['embeddings'] = embeddings\n\n    if cfg['token'] and cfg['dataset_id']:\n        _upload_cli(cfg)",
  "def lightly_cli(cfg):\n    \"\"\"Train a self-supervised model and use it to embed your dataset.\n\n    Args:\n        cfg:\n            The default configs are loaded from the config file.\n            To overwrite them please see the section on the config file \n            (.config.config.yaml).\n    \n    Command-Line Args:\n        input_dir:\n            Path to the input directory where images are stored.\n        token:\n            User access token to the Lightly platform. If dataset_id\n            and token are specified, the images and embeddings are \n            uploaded to the platform.\n\n            (Required for upload)\n        dataset_id:\n            Identifier of the dataset on the Lightly platform. If \n            dataset_id and token are specified, the images and \n            embeddings are uploaded to the platform.\n\n            (Required for upload)\n\n    Examples:\n        >>> #\u00a0train model and embed images with default settings\n        >>> lightly-magic input_dir=data/\n        >>>\n        >>> # train model for 10 epochs and embed images\n        >>> lightly-magic input_dir=data/ trainer.max_epochs=10\n        >>>\n        >>> # train model, embed images, and upload to the Lightly platform\n        >>> lightly-magic input_dir=data/ token='123' dataset_id='XYZ'\n\n    \"\"\"\n    return _lightly_cli(cfg)",
  "def entry():\n    lightly_cli()",
  "def fix_input_path(path):\n    \"\"\"Fix broken relative paths.\n\n    \"\"\"\n    if not os.path.isabs(path):\n        path = utils.to_absolute_path(path)\n    return path",
  "def is_url(checkpoint):\n    \"\"\"Check whether the checkpoint is a url or not.\n\n    \"\"\"\n    is_url = ('https://storage.googleapis.com' in checkpoint)\n    return is_url",
  "def get_ptmodel_from_config(model):\n    \"\"\"Get a pre-trained model from the lightly model zoo.\n\n    \"\"\"\n    key = model['name']\n    key += '/simclr'\n    key += '/d' + str(model['num_ftrs'])\n    key += '/w' + str(float(model['width']))\n\n    if key in model_zoo.keys():\n        return model_zoo[key], key\n    else:\n        return '', key",
  "def load_state_dict_from_url(url, map_location=None):\n    \"\"\"Try to load the checkopint from the given url.\n\n    \"\"\"\n    try:\n        state_dict = torch.hub.load_state_dict_from_url(\n            url, map_location=map_location\n        )\n        return state_dict\n    except Exception:\n        print('Not able to load state dict from %s' % (url))\n        print('Retrying with http:// prefix')\n    try:\n        url = url.replace('https', 'http')\n        state_dict = torch.hub.load_state_dict_from_url(\n            url, map_location=map_location\n        )\n        return state_dict\n    except Exception:\n        print('Not able to load state dict from %s' % (url))\n\n    # in this case downloading the pre-trained model was not possible\n    # notify the user and return\n    return {'state_dict': None}",
  "def _maybe_expand_batchnorm_weights(model_dict, state_dict, num_splits):\n    \"\"\"Expands the weights of the BatchNorm2d to the size of SplitBatchNorm.\n\n    \"\"\"\n    running_mean = 'running_mean'\n    running_var = 'running_var'\n\n    for key, item in model_dict.items():\n        # not batchnorm -> continue\n        if not running_mean in key and not running_var in key:\n            continue\n        \n        state = state_dict.get(key, None)\n        # not in dict -> continue\n        if state is None:\n            continue\n        # same shape -> continue\n        if item.shape == state.shape:\n            continue\n\n        # found running mean or running var with different shapes\n        state_dict[key] = state.repeat(num_splits)\n\n    return state_dict",
  "def _filter_state_dict(state_dict, remove_model_prefix_offset: int = 1):\n    \"\"\"Makes the state_dict compatible with the model.\n    \n    Prevents unexpected key error when loading PyTorch-Lightning checkpoints.\n    Allows backwards compatability to checkpoints before v1.0.6.\n\n    \"\"\"\n\n    prev_backbone = 'features'\n    curr_backbone = 'backbone'\n\n    new_state_dict = {}\n    for key, item in state_dict.items():\n        # remove the \"model.\" prefix from the state dict key\n        key_parts = key.split('.')[remove_model_prefix_offset:]\n        # with v1.0.6 the backbone of the models will be renamed from\n        # \"features\" to \"backbone\", ensure compatability with old ckpts\n        key_parts = \\\n            [k if k != prev_backbone else curr_backbone for k in key_parts]\n\n        new_key = '.'.join(key_parts)\n        new_state_dict[new_key] = item\n\n    return new_state_dict",
  "def load_from_state_dict(model,\n                         state_dict,\n                         strict: bool = True,\n                         apply_filter: bool = True,\n                         num_splits: int = 0):\n    \"\"\"Loads the model weights from the state dictionary.\n\n    \"\"\"\n    # step 1: filter state dict\n    if apply_filter:\n        state_dict = _filter_state_dict(state_dict)\n \n    # step 2: expand batchnorm weights\n    state_dict = \\\n        _maybe_expand_batchnorm_weights(model.state_dict(), state_dict, num_splits)\n\n    # step 3: load from checkpoint\n    model.load_state_dict(state_dict, strict=strict)"
]