[
  "def is_new_osx():\n    \"\"\"Check whether we're on OSX >= 10.10\"\"\"\n    name = distutils.util.get_platform()\n    if sys.platform != \"darwin\":\n        return False\n    elif name.startswith(\"macosx-10\"):\n        minor_version = int(name.split(\"-\")[1].split(\".\")[1])\n        if minor_version >= 7:\n            return True\n        else:\n            return False\n    else:\n        return False",
  "class build_ext_options:\n    def build_options(self):\n        src_dir = os.path.join(os.path.dirname(__file__), \"thinc\", \"_files\")\n        if hasattr(self.compiler, \"initialize\"):\n            self.compiler.initialize()\n        self.compiler.platform = sys.platform[:6]\n        for e in self.extensions:\n            e.extra_compile_args = COMPILE_OPTIONS.get(\n                self.compiler.compiler_type, COMPILE_OPTIONS[\"other\"]\n            )\n            e.extra_link_args = LINK_OPTIONS.get(\n                self.compiler.compiler_type, LINK_OPTIONS[\"other\"]\n            )",
  "class build_ext_subclass(build_ext, build_ext_options):\n    def build_extensions(self):\n        build_ext_options.build_options(self)\n        build_ext.build_extensions(self)",
  "def generate_cython(root, source):\n    print(\"Cythonizing sources\")\n    p = subprocess.call(\n        [sys.executable, os.path.join(root, \"bin\", \"cythonize.py\"), source],\n        env=os.environ,\n    )\n    if p != 0:\n        raise RuntimeError(\"Running cythonize failed\")",
  "def find_in_path(name, path):\n    \"Find a file in a search path\"\n    # adapted fom http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = os.path.join(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None",
  "def is_source_release(path):\n    return os.path.exists(os.path.join(path, \"PKG-INFO\"))",
  "def clean(path):\n    for name in MOD_NAMES:\n        name = name.replace(\".\", \"/\")\n        for ext in [\".so\", \".html\", \".cpp\", \".c\"]:\n            file_path = os.path.join(path, name + ext)\n            if os.path.exists(file_path):\n                os.unlink(file_path)",
  "def chdir(new_dir):\n    old_dir = os.getcwd()\n    try:\n        os.chdir(new_dir)\n        sys.path.insert(0, new_dir)\n        yield\n    finally:\n        del sys.path[0]\n        os.chdir(old_dir)",
  "def setup_package():\n    root = os.path.abspath(os.path.dirname(__file__))\n\n    if len(sys.argv) > 1 and sys.argv[1] == \"clean\":\n        return clean(root)\n\n    with chdir(root):\n        with open(os.path.join(root, \"thinc\", \"about.py\")) as f:\n            about = {}\n            exec(f.read(), about)\n\n        with io.open(os.path.join(root, \"README.md\"), encoding=\"utf8\") as f:\n            readme = f.read()\n\n        include_dirs = [\n            get_python_inc(plat_specific=True),\n            os.path.join(root, \"include\"),\n        ]\n\n        if (\n            ccompiler.new_compiler().compiler_type == \"msvc\"\n            and msvccompiler.get_build_version() == 9\n        ):\n            include_dirs.append(os.path.join(root, \"include\", \"msvc9\"))\n\n        ext_modules = []\n        for mod_name in MOD_NAMES:\n            mod_path = mod_name.replace(\".\", \"/\") + \".cpp\"\n            if mod_name.endswith(\"gpu_ops\"):\n                continue\n            mod_path = mod_name.replace(\".\", \"/\") + \".cpp\"\n            ext_modules.append(\n                Extension(\n                    mod_name, [mod_path], language=\"c++\", include_dirs=include_dirs\n                )\n            )\n        ext_modules.append(\n            Extension(\n                \"thinc.extra.wrapt._wrappers\",\n                [\"thinc/extra/wrapt/_wrappers.c\"],\n                include_dirs=include_dirs,\n            )\n        )\n\n        if not is_source_release(root):\n            generate_cython(root, \"thinc\")\n\n        setup(\n            name=\"thinc\",\n            zip_safe=False,\n            packages=PACKAGES,\n            package_data={\"\": [\"*.pyx\", \"*.pxd\", \"*.pxi\", \"*.cpp\"]},\n            description=about[\"__summary__\"],\n            long_description=readme,\n            long_description_content_type=\"text/markdown\",\n            author=about[\"__author__\"],\n            author_email=about[\"__email__\"],\n            version=about[\"__version__\"],\n            url=about[\"__uri__\"],\n            license=about[\"__license__\"],\n            ext_modules=ext_modules,\n            setup_requires=[\"numpy>=1.7.0\"],\n            install_requires=[\n                # Explosion-provided dependencies\n                \"murmurhash>=0.28.0,<1.1.0\",\n                \"cymem>=2.0.2,<2.1.0\",\n                \"preshed>=1.0.1,<3.1.0\",\n                \"blis>=0.4.0,<0.5.0\",\n                \"wasabi>=0.0.9,<1.1.0\",\n                \"srsly>=0.0.6,<1.1.0\",\n                # Third-party dependencies\n                \"numpy>=1.7.0\",\n                \"plac>=0.9.6,<1.0.0\",\n                \"tqdm>=4.10.0,<5.0.0\",\n                'pathlib==1.0.1; python_version < \"3.4\"',\n            ],\n            extras_require={\n                \"cuda\": [\"cupy>=5.0.0b4\"],\n                \"cuda80\": [\"cupy-cuda80>=5.0.0b4\"],\n                \"cuda90\": [\"cupy-cuda90>=5.0.0b4\"],\n                \"cuda91\": [\"cupy-cuda91>=5.0.0b4\"],\n                \"cuda92\": [\"cupy-cuda92>=5.0.0b4\"],\n                \"cuda100\": [\"cupy-cuda100>=5.0.0b4\"],\n                \"cuda110\": [\"cupy-cuda110>=5.0.0b4\"],\n            },\n            classifiers=[\n                \"Development Status :: 5 - Production/Stable\",\n                \"Environment :: Console\",\n                \"Intended Audience :: Developers\",\n                \"Intended Audience :: Science/Research\",\n                \"License :: OSI Approved :: MIT License\",\n                \"Operating System :: POSIX :: Linux\",\n                \"Operating System :: MacOS :: MacOS X\",\n                \"Operating System :: Microsoft :: Windows\",\n                \"Programming Language :: Cython\",\n                \"Programming Language :: Python :: 2.6\",\n                \"Programming Language :: Python :: 2.7\",\n                \"Programming Language :: Python :: 3.3\",\n                \"Programming Language :: Python :: 3.4\",\n                \"Programming Language :: Python :: 3.5\",\n                \"Programming Language :: Python :: 3.6\",\n                \"Programming Language :: Python :: 3.7\",\n                \"Topic :: Scientific/Engineering\",\n            ],\n            cmdclass={\"build_ext\": build_ext_subclass},\n        )",
  "def build_options(self):\n        src_dir = os.path.join(os.path.dirname(__file__), \"thinc\", \"_files\")\n        if hasattr(self.compiler, \"initialize\"):\n            self.compiler.initialize()\n        self.compiler.platform = sys.platform[:6]\n        for e in self.extensions:\n            e.extra_compile_args = COMPILE_OPTIONS.get(\n                self.compiler.compiler_type, COMPILE_OPTIONS[\"other\"]\n            )\n            e.extra_link_args = LINK_OPTIONS.get(\n                self.compiler.compiler_type, LINK_OPTIONS[\"other\"]\n            )",
  "def build_extensions(self):\n        build_ext_options.build_options(self)\n        build_ext.build_extensions(self)",
  "def iter_files(giga_dir):\n    i = 0\n    for subdir in os.listdir(giga_dir):\n        if not path.isdir(path.join(giga_dir, subdir)):\n            continue\n        for filename in os.listdir(path.join(giga_dir, subdir)):\n            if filename.endswith('gz'):\n                print(filename)\n                yield path.join(giga_dir, subdir, filename)\n                i += 1\n                if i >= 1:\n                    break\n        break",
  "def main(giga_dir):\n    ops = NumpyOps()\n    vectors = defaultdict(lambda: ops.allocate((300,)))\n    W = ops.allocate((200, 300))\n    embed = Embed(vectors=vectors, W=W, ops=ops)\n    nr_word = 0\n    for loc in iter_files(giga_dir):\n        with gzip.open(loc, 'r') as file_:\n            text = file_.read()\n        words = text.split()\n        vectors = embed.predict_batch(words)\n        for word in words:\n            if word not in embed.vectors:\n                embed.vectors[word] = embed.ops.allocate((300,))\n        nr_word += len(words)\n    print(nr_word)",
  "def main(loc):\n    model = thinc.linear.avgtron.AveragedPerceptron([])\n    model.load(loc)",
  "def process_pyx(fromfile, tofile, language_level='-2'):\n    try:\n        from Cython.Compiler.Version import version as cython_version\n        from distutils.version import LooseVersion\n        if LooseVersion(cython_version) < LooseVersion('0.19'):\n            raise Exception('Building %s requires Cython >= 0.19' % VENDOR)\n\n    except ImportError:\n        pass\n\n    flags = ['--fast-fail', language_level]\n    if tofile.endswith('.cpp'):\n        flags += ['--cplus']\n\n    try:\n        try:\n            r = subprocess.call(['cython'] + flags + [\"-o\", tofile, fromfile], env=os.environ)\n            if r != 0:\n                raise Exception('Cython failed')\n        except OSError:\n            # There are ways of installing Cython that don't result in a cython\n            # executable on the path, see gh-2397.\n            r = subprocess.call([sys.executable, '-c',\n                                 'import sys; from Cython.Compiler.Main import '\n                                 'setuptools_main as main; sys.exit(main())'] + flags +\n                                 [\"-o\", tofile, fromfile], env=os.environ)\n            if r != 0:\n                raise Exception('Cython failed')\n    except OSError:\n        raise OSError('Cython needs to be installed')",
  "def process_tempita_pyx(fromfile, tofile):\n    try:\n        try:\n            from Cython import Tempita as tempita\n        except ImportError:\n            import tempita\n    except ImportError:\n        raise Exception('Building %s requires Tempita: '\n                        'pip install --user Tempita' % VENDOR)\n    with open(fromfile, \"r\") as f:\n        tmpl = f.read()\n    pyxcontent = tempita.sub(tmpl)\n    assert fromfile.endswith('.pyx.in')\n    pyxfile = fromfile[:-len('.pyx.in')] + '.pyx'\n    with open(pyxfile, \"w\") as f:\n        f.write(pyxcontent)\n    process_pyx(pyxfile, tofile)",
  "def load_hashes(filename):\n    # Return { filename : (sha1 of input, sha1 of output) }\n    if os.path.isfile(filename):\n        hashes = {}\n        with open(filename, 'r') as f:\n            for line in f:\n                filename, inhash, outhash = line.split()\n                hashes[filename] = (inhash, outhash)\n    else:\n        hashes = {}\n    return hashes",
  "def save_hashes(hash_db, filename):\n    with open(filename, 'w') as f:\n        for key, value in sorted(hash_db.items()):\n            f.write(\"%s %s %s\\n\" % (key, value[0], value[1]))",
  "def sha1_of_file(filename):\n    h = hashlib.sha1()\n    with open(filename, \"rb\") as f:\n        h.update(f.read())\n    return h.hexdigest()",
  "def normpath(path):\n    path = path.replace(os.sep, '/')\n    if path.startswith('./'):\n        path = path[2:]\n    return path",
  "def get_hash(frompath, topath):\n    from_hash = sha1_of_file(frompath)\n    to_hash = sha1_of_file(topath) if os.path.exists(topath) else None\n    return (from_hash, to_hash)",
  "def process(path, fromfile, tofile, processor_function, hash_db):\n    fullfrompath = os.path.join(path, fromfile)\n    fulltopath = os.path.join(path, tofile)\n    current_hash = get_hash(fullfrompath, fulltopath)\n    if current_hash == hash_db.get(normpath(fullfrompath), None):\n        print('%s has not changed' % fullfrompath)\n        return\n\n    orig_cwd = os.getcwd()\n    try:\n        os.chdir(path)\n        print('Processing %s' % fullfrompath)\n        processor_function(fromfile, tofile)\n    finally:\n        os.chdir(orig_cwd)\n    # changed target file, recompute hash\n    current_hash = get_hash(fullfrompath, fulltopath)\n    # store hash in db\n    hash_db[normpath(fullfrompath)] = current_hash",
  "def find_process_files(root_dir):\n    hash_db = load_hashes(HASH_FILE)\n    for cur_dir, dirs, files in os.walk(root_dir):\n        for filename in files:\n            in_file = os.path.join(cur_dir, filename + \".in\")\n            if filename.endswith('.pyx') and os.path.isfile(in_file):\n                continue\n            for fromext, function in rules.items():\n                if filename.endswith(fromext):\n                    toext = \".cpp\"\n                    # with open(os.path.join(cur_dir, filename), 'rb') as f:\n                    #     data = f.read()\n                    #     m = re.search(br\"^\\s*#\\s*distutils:\\s*language\\s*=\\s*c\\+\\+\\s*$\", data, re.I|re.M)\n                    #     if m:\n                    #         toext = \".cxx\"\n                    fromfile = filename\n                    tofile = filename[:-len(fromext)] + toext\n                    process(cur_dir, fromfile, tofile, function, hash_db)\n                    save_hashes(hash_db, HASH_FILE)",
  "def main():\n    try:\n        root_dir = sys.argv[1]\n    except IndexError:\n        root_dir = DEFAULT_ROOT\n    find_process_files(root_dir)",
  "def mnist():\n    with virtualenv(VENV_DIR), lcd(PWD), shell_env(PYTHONPATH=PWD):\n        local('python examples/mnist_mlp.py')",
  "def basic_tagger():\n    with virtualenv(VENV_DIR), lcd(PWD), shell_env(PYTHONPATH=PWD):\n        local('python examples/basic_tagger.py')",
  "def cnn_tagger():\n    with virtualenv(VENV_DIR), lcd(PWD), shell_env(PYTHONPATH=PWD):\n        local('python examples/cnn_tagger.py')",
  "def quora():\n    with virtualenv(VENV_DIR), lcd(PWD), shell_env(PYTHONPATH=PWD):\n        local('pip install spacy')\n        local('python -m spacy.en.download')\n        local('python examples/quora_similarity.py')",
  "def spacy_tagger():\n    with virtualenv(VENV_DIR), lcd(PWD), shell_env(PYTHONPATH=PWD):\n        local('python examples/spacy_tagger.py')",
  "def env():\n    if file_exists(VENV_DIR):\n        local('rm -rf {env}'.format(env=VENV_DIR))\n    local('python -m virtualenv {env}'.format(env=VENV_DIR))\n    with virtualenv(VENV_DIR):\n        local('python -m pip install --upgrade setuptools')\n        local('python -m pip install -r requirements.txt')\n        local('python -m pip install pytest')",
  "def make():\n    with virtualenv(VENV_DIR):\n        with lcd(PWD):\n            local('python setup.py build_ext --inplace')",
  "def clean():\n    with lcd(PWD):\n        local('python setup.py clean --all')",
  "def test():\n    with virtualenv(VENV_DIR):\n        with lcd(PWD):\n            local('python -m pytest -x thinc')",
  "def env(lang='python2.7'):\n    if path.exists(VENV_DIR):\n        local('rm -rf {env}'.format(env=VENV_DIR))\n    local('pip install virtualenv')\n    local('python -m virtualenv -p {lang} {env}'.format(lang=lang, env=VENV_DIR))",
  "def install():\n    with virtualenv(VENV_DIR):\n        local('pip install --upgrade setuptools')\n        local('pip install dist/*.tar.gz')\n        local('pip install pytest')",
  "def make():\n    with virtualenv(VENV_DIR):\n        with lcd(PWD):\n            local('pip install cython')\n            local('pip install murmurhash')\n            local('pip install -r requirements.txt')\n            local('python setup.py build_ext --inplace')",
  "def sdist():\n    with virtualenv(VENV_DIR):\n        with lcd(PWD):\n            local('python setup.py sdist')",
  "def clean():\n    with lcd(PWD):\n        local('python setup.py clean --all')",
  "def test():\n    with virtualenv(VENV_DIR):\n        with lcd(PWD):\n            local('py.test -x spacy/tests')",
  "def decaying(base_rate, decay, t=0):\n    \"\"\"Yield an infinite series of linearly decaying values,\n    following the schedule:\n\n        base_rate * 1/(1+decay*t)\n\n    Example:\n\n        >>> learn_rates = linear_decay(0.001, 1e-4)\n        >>> next(learn_rates)\n        0.001\n        >>> next(learn_rates)\n        0.00999\n    \"\"\"\n    while True:\n        yield base_rate * (1.0 / (1.0 + decay * t))\n        t += 1",
  "def compounding(start, stop, compound, t=0.0):\n    \"\"\"Yield an infinite series of compounding values. Each time the\n    generator is called, a value is produced by multiplying the previous\n    value by the compound rate.\n\n    EXAMPLE:\n      >>> sizes = compounding(1., 10., 1.5)\n      >>> assert next(sizes) == 1.\n      >>> assert next(sizes) == 1 * 1.5\n      >>> assert next(sizes) == 1.5 * 1.5\n    \"\"\"\n    curr = float(start)\n    while True:\n        yield _clip(curr, start, stop)\n        curr *= compound",
  "def _clip(value, start, stop):\n    return max(value, stop) if (start > stop) else min(value, stop)",
  "def annealing(rate, decay, decay_steps, t=0.0):\n    while True:\n        if decay == 0.0:\n            yield rate\n        else:\n            yield rate * decay ** (t / decay_steps)\n            t += 1",
  "def slanted_triangular(max_rate, num_steps, cut_frac=0.1, ratio=32, decay=1, t=0.0):\n    \"\"\"Yield an infinite series of values according to Howard and Ruder's\n    \"slanted triangular learning rate\" schedule.\n    \"\"\"\n    cut = int(num_steps * cut_frac)\n    while True:\n        t += 1\n        if t < cut:\n            p = t / cut\n        else:\n            p = 1 - ((t - cut) / (cut * (1 / cut_frac - 1)))\n        learn_rate = max_rate * (1 + p * (ratio - 1)) * (1 / ratio)\n        yield learn_rate",
  "def layerize(begin_update=None, predict=None, *args, **kwargs):\n    \"\"\"Wrap a function into a layer\"\"\"\n    if begin_update is not None:\n        return FunctionLayer(begin_update, predict=predict, *args, **kwargs)\n\n    def wrapper(begin_update):\n        return FunctionLayer(begin_update, *args, **kwargs)\n\n    return wrapper",
  "def metalayerize(user_func):\n    \"\"\"Wrap a function over a sequence of layers and an input into a layer.\"\"\"\n\n    def returned(layers, *args, **kwargs):\n        def begin_update(X, *args, **kwargs):\n            return user_func(layers, X, *args, **kwargs)\n\n        return FunctionLayer(begin_update, *args, **kwargs)\n\n    return returned",
  "def flatten_add_lengths(seqs, pad=0, drop=0.0):\n    ops = Model.ops\n    lengths = ops.asarray([len(seq) for seq in seqs], dtype=\"i\")\n\n    def finish_update(d_X, sgd=None):\n        return ops.unflatten(d_X, lengths, pad=pad)\n\n    X = ops.flatten(seqs, pad=pad)\n    return (X, lengths), finish_update",
  "def unflatten(X_lengths, drop=0.):\n    ops = Model.ops\n    X, lengths = X_lengths\n    Xs = ops.unflatten(X, lengths)\n\n    def backprop_unflatten(dXs, sgd=None):\n        dX = ops.flatten(dXs, pad=0)\n        return dX\n\n    return Xs, backprop_unflatten",
  "def remap_ids(ops=None, column=0):\n    id_map = {0: 0}\n\n    def remap_ids_fwd(ids, drop=0.0):\n        ids = ids[:, column]\n        if not isinstance(ids, numpy.ndarray):\n            ids = ids.get()\n        n_vector = len(id_map)\n        for i, id_ in enumerate(ids):\n            id_ = int(id_)\n            if id_ not in id_map:\n                id_map[id_] = n_vector\n                n_vector += 1\n            ids[i] = id_map[id_]\n        return ops.asarray(ids), None\n\n    model = layerize(remap_ids_fwd)\n    if ops is None:\n        ops = model.ops\n    return model",
  "def with_reshape(layer):\n    def with_reshape_forward(X, drop=0.0):\n        initial_shape = X.shape\n        final_shape = list(initial_shape[:-1]) + [layer.nO]\n        nB = X.shape[0]\n        nT = X.shape[1]\n        X2d = X.reshape(-1, X.shape[2])\n        X2d = X2d.astype(layer.ops.xp.float32)\n        Y2d, Y2d_backprop = layer.begin_update(X2d, drop=drop)\n        Y = Y2d.reshape(final_shape)\n\n        def with_reshape_backward(dY, sgd=None):\n            dY = dY.reshape(nB * nT, -1).astype(layer.ops.xp.float32)\n            return Y2d_backprop(dY, sgd=sgd).reshape(initial_shape)\n\n        return Y, with_reshape_backward\n\n    return wrap(with_reshape_forward, layer)",
  "def with_getitem(idx, layer):\n    def begin_update(items, drop=0.0):\n        X, finish = layer.begin_update(items[idx], drop=drop)\n        return items[:idx] + (X,) + items[idx + 1 :], finish\n\n    model = layerize(begin_update)\n    model._layers.append(layer)\n\n    def on_data(self, items, y):\n        for hook in layer.on_data_hooks:\n            hook(layer, items[idx], y)\n\n    model.on_data_hooks.append(on_data)\n    return model",
  "def noop(*layers):\n    \"\"\"Transform a sequences of layers into a null operation.\"\"\"\n\n    def begin_update(X, drop=0.0):\n        return X, lambda D, *a, **k: D\n\n    return begin_update",
  "def chain(*layers):\n    \"\"\"Compose two models `f` and `g` such that they become layers of a single\n    feed-forward model that computes `g(f(x))`.\n\n    Raises exception if their dimensions don't match.\n    \"\"\"\n    if len(layers) == 0:\n        return FeedForward([])\n    elif len(layers) == 1:\n        return layers[0]\n    else:\n        return FeedForward(layers)",
  "def clone(orig, n):\n    \"\"\"Construct `n` copies of a layer, with distinct weights.\n\n    i.e. `clone(f, 3)(x)` computes `f(f'(f''(x)))`.\n    \"\"\"\n    if n == 0:\n        return layerize(noop())\n    layers = [orig]\n    for i in range(n - 1):\n        layers.append(copy.deepcopy(orig))\n        layers[-1].set_id()\n    return FeedForward(layers)",
  "def concatenate(*layers):  # pragma: no cover\n    \"\"\"Compose two or more models `f`, `g`, etc, such that their outputs are\n    concatenated, i.e. `concatenate(f, g)(x)` computes `hstack(f(x), g(x))`\n    \"\"\"\n    if not layers:\n        return noop()\n    ops = layers[0].ops\n\n    def begin_update(X, *a, **k):\n        forward, backward = split_backward(layers)\n        values = [fwd(X, *a, **k) for fwd in forward]\n\n        output = ops.xp.hstack(values)\n        shapes = [val.shape for val in values]\n\n        def finish_update(gradient, *args, **kwargs):\n            layer_grads = []\n            start = 0\n            for bwd, shape in zip(backward, shapes):\n                end = start + shape[1]\n                if bwd is not None:\n                    d = bwd(\n                        ops.xp.ascontiguousarray(gradient[:, start:end]),\n                        *args,\n                        **kwargs\n                    )\n                    if d is not None and hasattr(X, \"shape\"):\n                        if not layer_grads:\n                            layer_grads.append(d)\n                        else:\n                            layer_grads[-1] += d\n                start = end\n            if layer_grads:\n                return ops.asarray(layer_grads[-1])\n            else:\n                return None\n\n        return output, finish_update\n\n    layer = FunctionLayer(begin_update)\n    layer._layers = list(layers)\n\n    def on_data(self, X, y=None):\n        for layer in self._layers:\n            for hook in layer.on_data_hooks:\n                hook(layer, X, y)\n\n    layer.on_data_hooks.append(on_data)\n    return layer",
  "def add(*layers):\n    if not layers:\n        return noop()\n\n    def forward(X, drop=0.0):\n        outs, callbacks = zip(*[lyr.begin_update(X, drop=drop) for lyr in layers])\n        out = outs[0]\n        for o in outs:\n            out += o\n\n        def backward(d_out, sgd=None):\n            grads = [bp(d_out, sgd=sgd) for bp in callbacks if bp is not None]\n            grads = [g for g in grads if g is not None]\n            if grads:\n                total = grads[0]\n                for g in grads:\n                    total += g\n                return total\n            else:\n                return None\n\n        return out, backward\n\n    model = layerize(forward)\n    model._layers = list(layers)\n\n    def on_data(self, X, y):\n        for layer in layers:\n            for hook in layer.on_data_hooks:\n                hook(layer, X, y)\n\n    model.on_data_hooks.append(on_data)\n    return model",
  "def split_backward(layers):  # pragma: no cover\n    \"\"\"Separate a sequence of layers' `begin_update` methods into two lists of\n    functions: one that computes the forward values, and the other that completes\n    the backward pass. The backward sequence is only populated after the forward\n    functions have been applied.\n    \"\"\"\n    backward = []\n    forward = [sink_return(op.begin_update, backward.append) for op in layers]\n    return forward, backward",
  "def sink_return(func, sink, splitter=None):  # pragma: no cover\n    \"\"\"Transform a function `func` that returns tuples into a function that returns\n    single values. Call a function `sink` on the unused values.\n    \"\"\"\n\n    def wrap(*args, **kwargs):\n        output = func(*args, **kwargs)\n        if splitter is None:\n            to_keep, to_sink = output\n        else:\n            to_keep, to_sink = splitter(*output)\n        sink(to_sink)\n        return to_keep\n\n    return wrap",
  "def Arg(i):\n    @layerize\n    def begin_update(batched_inputs, drop=0.0):\n        inputs = list(zip(*batched_inputs))\n        return inputs[i], None\n\n    return begin_update",
  "def with_square_sequences(model):\n    def padded_forward(seqs_in, drop=0.0):\n        padded_in, _, unpad = model.ops.square_sequences(seqs_in)\n        (padded_out, _), backprop_model = model.begin_update(padded_in, drop=drop)\n        seqs_out = unpad(padded_out)\n\n        def backprop_padding(d_seqs_out, sgd=None):\n            d_padded_out, sizes_at_t, unpad = model.ops.square_sequences(d_seqs_out)\n            d_padded_in = backprop_model((d_padded_out, None), sgd=sgd)\n            return unpad(d_padded_in)\n\n        return seqs_out, backprop_padding\n\n    return wrap(padded_forward, model)",
  "def with_flatten(layer, pad=0, ndim=4):\n    def begin_update(seqs_in, drop=0.0):\n        lengths = layer.ops.asarray([len(seq) for seq in seqs_in])\n        X, bp_layer = layer.begin_update(layer.ops.flatten(seqs_in, pad=pad), drop=drop)\n        if bp_layer is None:\n            return layer.ops.unflatten(X, lengths, pad=pad), None\n\n        def finish_update(d_seqs_out, sgd=None):\n            d_X = bp_layer(layer.ops.flatten(d_seqs_out, pad=pad), sgd=sgd)\n            if d_X is None:\n                return None\n            else:\n                return layer.ops.unflatten(d_X, lengths, pad=pad)\n\n        return layer.ops.unflatten(X, lengths, pad=pad), finish_update\n\n    def predict(seqs_in):\n        lengths = layer.ops.asarray([len(seq) for seq in seqs_in])\n        X = layer(layer.ops.flatten(seqs_in, pad=pad))\n        return layer.ops.unflatten(X, lengths, pad=pad)\n\n    model = layerize(begin_update, predict=predict)\n    model._layers.append(layer)\n    model.on_data_hooks.append(_with_flatten_on_data)\n    model.name = \"flatten\"\n    return model",
  "def _with_flatten_on_data(model, X, y):\n    X = model.ops.flatten(X)\n    for layer in model._layers:\n        for hook in layer.on_data_hooks:\n            hook(layer, X, y)\n        X = layer(X)",
  "def get_word_ids(ops, pad=1, token_drop=0.0, ignore=None):\n    # TODO: Is this made obsolete by the FeatureExtractor?\n    def forward(docs, drop=0.0):\n        \"\"\"Get word forms.\"\"\"\n        seqs = []\n        ops = Model.ops\n        for doc in docs:\n            if ignore is not None:\n                doc = [token for token in doc if not ignore(token)]\n            # seq = [0] * pad\n            seq = [(token.lex_id or token.orth) for token in doc]\n            # seq += [0] * pad\n            seqs.append(ops.asarray(seq, dtype=\"uint64\"))\n        return seqs, None\n\n    return layerize(forward)",
  "def wrap(func, *child_layers):\n    model = layerize(func)\n    model._layers.extend(child_layers)\n\n    def on_data(self, X, y):\n        for child in self._layers:\n            for hook in child.on_data_hooks:\n                hook(child, X, y)\n\n    model.on_data_hooks.append(on_data)\n    return model",
  "def uniqued(layer, column=0):\n    \"\"\"Group inputs to a layer, so that the layer only has to compute\n    for the unique values. The data is transformed back before output, and the same\n    transformation is applied for the gradient. Effectively, this is a cache\n    local to each minibatch.\n\n    The uniqued wrapper is useful for word inputs, because common words are\n    seen often, but we may want to compute complicated features for the words,\n    using e.g. character LSTM.\n    \"\"\"\n\n    def uniqued_fwd(X, drop=0.0):\n        keys = X[:, column]\n        keys = layer.ops.xp.ascontiguousarray(keys)\n        if not isinstance(keys, numpy.ndarray):\n            keys = keys.get()\n        uniq_keys, ind, inv, counts = numpy.unique(\n            keys, return_index=True, return_inverse=True, return_counts=True\n        )\n        X_uniq = layer.ops.xp.ascontiguousarray(X[ind])\n        Y_uniq, bp_Y_uniq = layer.begin_update(X_uniq, drop=drop)\n        Y = Y_uniq[inv].reshape((X.shape[0],) + Y_uniq.shape[1:])\n\n        def uniqued_bwd(dY, sgd=None):\n            dY_uniq = layer.ops.allocate(Y_uniq.shape, dtype=\"f\")\n            layer.ops.scatter_add(dY_uniq, layer.ops.asarray(inv, dtype=\"i\"), dY)\n            d_uniques = bp_Y_uniq(dY_uniq, sgd=sgd)\n            if d_uniques is not None:\n                dX = (d_uniques / counts)[inv]\n                return dX\n            else:\n                return None\n\n        return Y, uniqued_bwd\n\n    model = wrap(uniqued_fwd, layer)\n    return model",
  "def foreach(layer, drop_factor=1.0):\n    \"\"\"Map a layer across list items\"\"\"\n\n    def foreach_fwd(docs, drop=0.0):\n        sents = []\n        lengths = []\n        for doc in docs:\n            doc_sents = [sent for sent in doc if len(sent)]\n            assert len(doc_sents)\n            if drop:\n                subset = [\n                    s for s in doc_sents if numpy.random.random() >= drop * drop_factor\n                ]\n            else:\n                subset = list(doc_sents)\n            if subset:\n                sents.extend(subset)\n                lengths.append(len(subset))\n            else:\n                numpy.random.shuffle(doc_sents)\n                sents.append(doc_sents[0])\n                lengths.append(1)\n        assert len(sents)\n        flat, bp_flat = layer.begin_update(sents, drop=0.0)\n        output = layer.ops.unflatten(flat, lengths)\n\n        def foreach_bwd(d_output, sgd=None):\n            d_flat = layer.ops.flatten(d_output)\n            d_sents = bp_flat(d_flat, sgd=sgd)\n            if d_sents is None:\n                return d_sents\n            else:\n                return layer.ops.unflatten(d_sents, lengths)\n\n        return output, foreach_bwd\n\n    model = wrap(foreach_fwd, layer)\n\n    def _run_foreach_child_hooks(model, X, y):\n        for layer in model._layers:\n            for hook in layer.on_data_hooks:\n                hook(layer, X[0], y[0])\n\n    model.on_data_hooks = [_run_foreach_child_hooks]\n\n    return model",
  "def foreach_sentence(layer, drop_factor=1.0):\n    \"\"\"Map a layer across sentences (assumes spaCy-esque .sents interface)\"\"\"\n\n    def sentence_fwd(docs, drop=0.0):\n        sents = []\n        lengths = []\n        for doc in docs:\n            doc_sents = [sent for sent in doc.sents if len(sent)]\n            subset = [\n                s for s in doc_sents if numpy.random.random() >= drop * drop_factor\n            ]\n            if subset:\n                sents.extend(subset)\n                lengths.append(len(subset))\n            else:\n                numpy.random.shuffle(doc_sents)\n                sents.append(doc_sents[0])\n                lengths.append(1)\n        flat, bp_flat = layer.begin_update(sents, drop=0.0)\n        output = layer.ops.unflatten(flat, lengths)\n\n        def sentence_bwd(d_output, sgd=None):\n            d_flat = layer.ops.flatten(d_output)\n            d_sents = bp_flat(d_flat, sgd=sgd)\n            if d_sents is None:\n                return d_sents\n            else:\n                return layer.ops.unflatten(d_sents, lengths)\n\n        return output, sentence_bwd\n\n    model = wrap(sentence_fwd, layer)\n    return model",
  "def with_pad_and_mask(layer):\n    def create_model_input_forward(Xs, drop=0.):\n        nX = model.ops.asarray([x.shape[0] for x in Xs], dtype='i')\n        nL = nX.max()\n        X, unpad_X = pad_sequences(model.ops, Xs, pad_to=nL)\n        X_mask = _get_mask(X, nX)\n        Y, bp_Y = layer.begin_update((X.astype(\"float32\"), X_mask, None), drop=drop)\n        def create_model_input_backward(dYs, sgd=None):\n            dY, _ = pad_sequences(model.ops, dYs, pad_to=nL)\n            dX = bp_Y(dY, sgd=sgd)\n            return unpad_X(dX)\n        return unpad_X(Y), create_model_input_backward\n    model = layerize(create_model_input_forward)\n    return model",
  "def pad_sequences(ops, seqs_in, pad_to=None):\n    lengths = ops.asarray([len(seq) for seq in seqs_in], dtype='i')\n    nB = len(seqs_in)\n    if pad_to is None:\n        pad_to = lengths.max()\n    arr = ops.allocate((nB, int(pad_to)) + seqs_in[0].shape[1:], dtype=seqs_in[0].dtype)\n    for arr_i, seq in enumerate(seqs_in):\n        arr[arr_i, :seq.shape[0]] = ops.asarray(seq)\n    \n    def unpad(padded):\n        unpadded = [None] * len(lengths)\n        for i in range(padded.shape[0]):\n            unpadded[i] = padded[i, :lengths[i]]\n        return unpadded\n    return arr, unpad",
  "def _get_mask(X, nX):\n    nB = X.shape[0]\n    nL = X.shape[1]\n    X_mask = Model.ops.allocate((nB, nL, nL))\n    for i, length in enumerate(nX):\n        X_mask[i, :, :length] = 1.0\n    return X_mask",
  "def wrapper(begin_update):\n        return FunctionLayer(begin_update, *args, **kwargs)",
  "def returned(layers, *args, **kwargs):\n        def begin_update(X, *args, **kwargs):\n            return user_func(layers, X, *args, **kwargs)\n\n        return FunctionLayer(begin_update, *args, **kwargs)",
  "def finish_update(d_X, sgd=None):\n        return ops.unflatten(d_X, lengths, pad=pad)",
  "def backprop_unflatten(dXs, sgd=None):\n        dX = ops.flatten(dXs, pad=0)\n        return dX",
  "def remap_ids_fwd(ids, drop=0.0):\n        ids = ids[:, column]\n        if not isinstance(ids, numpy.ndarray):\n            ids = ids.get()\n        n_vector = len(id_map)\n        for i, id_ in enumerate(ids):\n            id_ = int(id_)\n            if id_ not in id_map:\n                id_map[id_] = n_vector\n                n_vector += 1\n            ids[i] = id_map[id_]\n        return ops.asarray(ids), None",
  "def with_reshape_forward(X, drop=0.0):\n        initial_shape = X.shape\n        final_shape = list(initial_shape[:-1]) + [layer.nO]\n        nB = X.shape[0]\n        nT = X.shape[1]\n        X2d = X.reshape(-1, X.shape[2])\n        X2d = X2d.astype(layer.ops.xp.float32)\n        Y2d, Y2d_backprop = layer.begin_update(X2d, drop=drop)\n        Y = Y2d.reshape(final_shape)\n\n        def with_reshape_backward(dY, sgd=None):\n            dY = dY.reshape(nB * nT, -1).astype(layer.ops.xp.float32)\n            return Y2d_backprop(dY, sgd=sgd).reshape(initial_shape)\n\n        return Y, with_reshape_backward",
  "def begin_update(items, drop=0.0):\n        X, finish = layer.begin_update(items[idx], drop=drop)\n        return items[:idx] + (X,) + items[idx + 1 :], finish",
  "def on_data(self, items, y):\n        for hook in layer.on_data_hooks:\n            hook(layer, items[idx], y)",
  "def begin_update(X, drop=0.0):\n        return X, lambda D, *a, **k: D",
  "def begin_update(X, *a, **k):\n        forward, backward = split_backward(layers)\n        values = [fwd(X, *a, **k) for fwd in forward]\n\n        output = ops.xp.hstack(values)\n        shapes = [val.shape for val in values]\n\n        def finish_update(gradient, *args, **kwargs):\n            layer_grads = []\n            start = 0\n            for bwd, shape in zip(backward, shapes):\n                end = start + shape[1]\n                if bwd is not None:\n                    d = bwd(\n                        ops.xp.ascontiguousarray(gradient[:, start:end]),\n                        *args,\n                        **kwargs\n                    )\n                    if d is not None and hasattr(X, \"shape\"):\n                        if not layer_grads:\n                            layer_grads.append(d)\n                        else:\n                            layer_grads[-1] += d\n                start = end\n            if layer_grads:\n                return ops.asarray(layer_grads[-1])\n            else:\n                return None\n\n        return output, finish_update",
  "def on_data(self, X, y=None):\n        for layer in self._layers:\n            for hook in layer.on_data_hooks:\n                hook(layer, X, y)",
  "def forward(X, drop=0.0):\n        outs, callbacks = zip(*[lyr.begin_update(X, drop=drop) for lyr in layers])\n        out = outs[0]\n        for o in outs:\n            out += o\n\n        def backward(d_out, sgd=None):\n            grads = [bp(d_out, sgd=sgd) for bp in callbacks if bp is not None]\n            grads = [g for g in grads if g is not None]\n            if grads:\n                total = grads[0]\n                for g in grads:\n                    total += g\n                return total\n            else:\n                return None\n\n        return out, backward",
  "def on_data(self, X, y):\n        for layer in layers:\n            for hook in layer.on_data_hooks:\n                hook(layer, X, y)",
  "def wrap(*args, **kwargs):\n        output = func(*args, **kwargs)\n        if splitter is None:\n            to_keep, to_sink = output\n        else:\n            to_keep, to_sink = splitter(*output)\n        sink(to_sink)\n        return to_keep",
  "def begin_update(batched_inputs, drop=0.0):\n        inputs = list(zip(*batched_inputs))\n        return inputs[i], None",
  "def padded_forward(seqs_in, drop=0.0):\n        padded_in, _, unpad = model.ops.square_sequences(seqs_in)\n        (padded_out, _), backprop_model = model.begin_update(padded_in, drop=drop)\n        seqs_out = unpad(padded_out)\n\n        def backprop_padding(d_seqs_out, sgd=None):\n            d_padded_out, sizes_at_t, unpad = model.ops.square_sequences(d_seqs_out)\n            d_padded_in = backprop_model((d_padded_out, None), sgd=sgd)\n            return unpad(d_padded_in)\n\n        return seqs_out, backprop_padding",
  "def begin_update(seqs_in, drop=0.0):\n        lengths = layer.ops.asarray([len(seq) for seq in seqs_in])\n        X, bp_layer = layer.begin_update(layer.ops.flatten(seqs_in, pad=pad), drop=drop)\n        if bp_layer is None:\n            return layer.ops.unflatten(X, lengths, pad=pad), None\n\n        def finish_update(d_seqs_out, sgd=None):\n            d_X = bp_layer(layer.ops.flatten(d_seqs_out, pad=pad), sgd=sgd)\n            if d_X is None:\n                return None\n            else:\n                return layer.ops.unflatten(d_X, lengths, pad=pad)\n\n        return layer.ops.unflatten(X, lengths, pad=pad), finish_update",
  "def predict(seqs_in):\n        lengths = layer.ops.asarray([len(seq) for seq in seqs_in])\n        X = layer(layer.ops.flatten(seqs_in, pad=pad))\n        return layer.ops.unflatten(X, lengths, pad=pad)",
  "def forward(docs, drop=0.0):\n        \"\"\"Get word forms.\"\"\"\n        seqs = []\n        ops = Model.ops\n        for doc in docs:\n            if ignore is not None:\n                doc = [token for token in doc if not ignore(token)]\n            # seq = [0] * pad\n            seq = [(token.lex_id or token.orth) for token in doc]\n            # seq += [0] * pad\n            seqs.append(ops.asarray(seq, dtype=\"uint64\"))\n        return seqs, None",
  "def on_data(self, X, y):\n        for child in self._layers:\n            for hook in child.on_data_hooks:\n                hook(child, X, y)",
  "def uniqued_fwd(X, drop=0.0):\n        keys = X[:, column]\n        keys = layer.ops.xp.ascontiguousarray(keys)\n        if not isinstance(keys, numpy.ndarray):\n            keys = keys.get()\n        uniq_keys, ind, inv, counts = numpy.unique(\n            keys, return_index=True, return_inverse=True, return_counts=True\n        )\n        X_uniq = layer.ops.xp.ascontiguousarray(X[ind])\n        Y_uniq, bp_Y_uniq = layer.begin_update(X_uniq, drop=drop)\n        Y = Y_uniq[inv].reshape((X.shape[0],) + Y_uniq.shape[1:])\n\n        def uniqued_bwd(dY, sgd=None):\n            dY_uniq = layer.ops.allocate(Y_uniq.shape, dtype=\"f\")\n            layer.ops.scatter_add(dY_uniq, layer.ops.asarray(inv, dtype=\"i\"), dY)\n            d_uniques = bp_Y_uniq(dY_uniq, sgd=sgd)\n            if d_uniques is not None:\n                dX = (d_uniques / counts)[inv]\n                return dX\n            else:\n                return None\n\n        return Y, uniqued_bwd",
  "def foreach_fwd(docs, drop=0.0):\n        sents = []\n        lengths = []\n        for doc in docs:\n            doc_sents = [sent for sent in doc if len(sent)]\n            assert len(doc_sents)\n            if drop:\n                subset = [\n                    s for s in doc_sents if numpy.random.random() >= drop * drop_factor\n                ]\n            else:\n                subset = list(doc_sents)\n            if subset:\n                sents.extend(subset)\n                lengths.append(len(subset))\n            else:\n                numpy.random.shuffle(doc_sents)\n                sents.append(doc_sents[0])\n                lengths.append(1)\n        assert len(sents)\n        flat, bp_flat = layer.begin_update(sents, drop=0.0)\n        output = layer.ops.unflatten(flat, lengths)\n\n        def foreach_bwd(d_output, sgd=None):\n            d_flat = layer.ops.flatten(d_output)\n            d_sents = bp_flat(d_flat, sgd=sgd)\n            if d_sents is None:\n                return d_sents\n            else:\n                return layer.ops.unflatten(d_sents, lengths)\n\n        return output, foreach_bwd",
  "def _run_foreach_child_hooks(model, X, y):\n        for layer in model._layers:\n            for hook in layer.on_data_hooks:\n                hook(layer, X[0], y[0])",
  "def sentence_fwd(docs, drop=0.0):\n        sents = []\n        lengths = []\n        for doc in docs:\n            doc_sents = [sent for sent in doc.sents if len(sent)]\n            subset = [\n                s for s in doc_sents if numpy.random.random() >= drop * drop_factor\n            ]\n            if subset:\n                sents.extend(subset)\n                lengths.append(len(subset))\n            else:\n                numpy.random.shuffle(doc_sents)\n                sents.append(doc_sents[0])\n                lengths.append(1)\n        flat, bp_flat = layer.begin_update(sents, drop=0.0)\n        output = layer.ops.unflatten(flat, lengths)\n\n        def sentence_bwd(d_output, sgd=None):\n            d_flat = layer.ops.flatten(d_output)\n            d_sents = bp_flat(d_flat, sgd=sgd)\n            if d_sents is None:\n                return d_sents\n            else:\n                return layer.ops.unflatten(d_sents, lengths)\n\n        return output, sentence_bwd",
  "def create_model_input_forward(Xs, drop=0.):\n        nX = model.ops.asarray([x.shape[0] for x in Xs], dtype='i')\n        nL = nX.max()\n        X, unpad_X = pad_sequences(model.ops, Xs, pad_to=nL)\n        X_mask = _get_mask(X, nX)\n        Y, bp_Y = layer.begin_update((X.astype(\"float32\"), X_mask, None), drop=drop)\n        def create_model_input_backward(dYs, sgd=None):\n            dY, _ = pad_sequences(model.ops, dYs, pad_to=nL)\n            dX = bp_Y(dY, sgd=sgd)\n            return unpad_X(dX)\n        return unpad_X(Y), create_model_input_backward",
  "def unpad(padded):\n        unpadded = [None] * len(lengths)\n        for i in range(padded.shape[0]):\n            unpadded[i] = padded[i, :lengths[i]]\n        return unpadded",
  "def begin_update(X, *args, **kwargs):\n            return user_func(layers, X, *args, **kwargs)",
  "def with_reshape_backward(dY, sgd=None):\n            dY = dY.reshape(nB * nT, -1).astype(layer.ops.xp.float32)\n            return Y2d_backprop(dY, sgd=sgd).reshape(initial_shape)",
  "def finish_update(gradient, *args, **kwargs):\n            layer_grads = []\n            start = 0\n            for bwd, shape in zip(backward, shapes):\n                end = start + shape[1]\n                if bwd is not None:\n                    d = bwd(\n                        ops.xp.ascontiguousarray(gradient[:, start:end]),\n                        *args,\n                        **kwargs\n                    )\n                    if d is not None and hasattr(X, \"shape\"):\n                        if not layer_grads:\n                            layer_grads.append(d)\n                        else:\n                            layer_grads[-1] += d\n                start = end\n            if layer_grads:\n                return ops.asarray(layer_grads[-1])\n            else:\n                return None",
  "def backward(d_out, sgd=None):\n            grads = [bp(d_out, sgd=sgd) for bp in callbacks if bp is not None]\n            grads = [g for g in grads if g is not None]\n            if grads:\n                total = grads[0]\n                for g in grads:\n                    total += g\n                return total\n            else:\n                return None",
  "def backprop_padding(d_seqs_out, sgd=None):\n            d_padded_out, sizes_at_t, unpad = model.ops.square_sequences(d_seqs_out)\n            d_padded_in = backprop_model((d_padded_out, None), sgd=sgd)\n            return unpad(d_padded_in)",
  "def finish_update(d_seqs_out, sgd=None):\n            d_X = bp_layer(layer.ops.flatten(d_seqs_out, pad=pad), sgd=sgd)\n            if d_X is None:\n                return None\n            else:\n                return layer.ops.unflatten(d_X, lengths, pad=pad)",
  "def uniqued_bwd(dY, sgd=None):\n            dY_uniq = layer.ops.allocate(Y_uniq.shape, dtype=\"f\")\n            layer.ops.scatter_add(dY_uniq, layer.ops.asarray(inv, dtype=\"i\"), dY)\n            d_uniques = bp_Y_uniq(dY_uniq, sgd=sgd)\n            if d_uniques is not None:\n                dX = (d_uniques / counts)[inv]\n                return dX\n            else:\n                return None",
  "def foreach_bwd(d_output, sgd=None):\n            d_flat = layer.ops.flatten(d_output)\n            d_sents = bp_flat(d_flat, sgd=sgd)\n            if d_sents is None:\n                return d_sents\n            else:\n                return layer.ops.unflatten(d_sents, lengths)",
  "def sentence_bwd(d_output, sgd=None):\n            d_flat = layer.ops.flatten(d_output)\n            d_sents = bp_flat(d_flat, sgd=sgd)\n            if d_sents is None:\n                return d_sents\n            else:\n                return layer.ops.unflatten(d_sents, lengths)",
  "def create_model_input_backward(dYs, sgd=None):\n            dY, _ = pad_sequences(model.ops, dYs, pad_to=nL)\n            dX = bp_Y(dY, sgd=sgd)\n            return unpad_X(dX)",
  "class UndefinedOperatorError(TypeError):\n    def __init__(self, op, arg1, arg2, operators):\n        self.tb = traceback.extract_stack()\n        TypeError.__init__(\n            self,\n            get_error(\n                \"Undefined operator: {op}\".format(op=op),\n                \"Called by ({arg1}, {arg2})\".format(arg1=arg1, arg2=arg2),\n                \"Available: {ops}\".format(ops=\", \".join(operators.keys())),\n                tb=self.tb,\n                highlight=op,\n            ),\n        )",
  "class OutsideRangeError(ValueError):\n    def __init__(self, arg, val, operator):\n        self.tb = traceback.extract_stack()\n        ValueError.__init__(\n            self,\n            get_error(\n                \"Outside range: {v} needs to be {o} {v2}\".format(\n                    v=format_repr(arg), o=operator, v2=format_repr(val)\n                ),\n                tb=self.tb,\n            ),\n        )",
  "class DifferentLengthError(ValueError):\n    def __init__(self, lengths, arg):\n        self.tb = traceback.extract_stack()\n        ValueError.__init__(\n            self,\n            get_error(\n                \"Values need to be equal length: {v}\".format(v=format_repr(lengths)),\n                tb=self.tb,\n            ),\n        )",
  "class ShapeMismatchError(ValueError):\n    def __init__(self, shape, dim, shape_names):\n        self.tb = traceback.extract_stack()\n        shape = format_repr(shape)\n        dim = format_repr(dim)\n        ValueError.__init__(\n            self,\n            get_error(\n                \"Shape mismatch: input {s} not compatible with {d}.\".format(\n                    s=shape, d=dim\n                ),\n                tb=self.tb,\n            ),\n        )",
  "class TooFewDimensionsError(ValueError):\n    def __init__(self, shape, axis):\n        self.tb = traceback.extract_stack()\n        ValueError.__init__(\n            self,\n            get_error(\n                \"Shape mismatch: input {s} has too short for axis {d}.\".format(\n                    s=format_repr(shape), d=axis\n                ),\n                tb=self.tb,\n            ),\n        )",
  "class ExpectedTypeError(TypeError):\n    max_to_print_of_value = 200\n\n    def __init__(self, bad_type, expected):\n        if isinstance(expected, str):\n            expected = [expected]\n        self.tb = traceback.extract_stack()\n        TypeError.__init__(\n            self,\n            get_error(\n                \"Expected type {e}, but got: {v} ({t})\".format(\n                    e=\"/\".join(expected), v=format_repr(bad_type), t=type(bad_type)\n                ),\n                tb=self.tb,\n                highlight=format_repr(bad_type),\n            ),\n        )",
  "def __init__(self, op, arg1, arg2, operators):\n        self.tb = traceback.extract_stack()\n        TypeError.__init__(\n            self,\n            get_error(\n                \"Undefined operator: {op}\".format(op=op),\n                \"Called by ({arg1}, {arg2})\".format(arg1=arg1, arg2=arg2),\n                \"Available: {ops}\".format(ops=\", \".join(operators.keys())),\n                tb=self.tb,\n                highlight=op,\n            ),\n        )",
  "def __init__(self, arg, val, operator):\n        self.tb = traceback.extract_stack()\n        ValueError.__init__(\n            self,\n            get_error(\n                \"Outside range: {v} needs to be {o} {v2}\".format(\n                    v=format_repr(arg), o=operator, v2=format_repr(val)\n                ),\n                tb=self.tb,\n            ),\n        )",
  "def __init__(self, lengths, arg):\n        self.tb = traceback.extract_stack()\n        ValueError.__init__(\n            self,\n            get_error(\n                \"Values need to be equal length: {v}\".format(v=format_repr(lengths)),\n                tb=self.tb,\n            ),\n        )",
  "def __init__(self, shape, dim, shape_names):\n        self.tb = traceback.extract_stack()\n        shape = format_repr(shape)\n        dim = format_repr(dim)\n        ValueError.__init__(\n            self,\n            get_error(\n                \"Shape mismatch: input {s} not compatible with {d}.\".format(\n                    s=shape, d=dim\n                ),\n                tb=self.tb,\n            ),\n        )",
  "def __init__(self, shape, axis):\n        self.tb = traceback.extract_stack()\n        ValueError.__init__(\n            self,\n            get_error(\n                \"Shape mismatch: input {s} has too short for axis {d}.\".format(\n                    s=format_repr(shape), d=axis\n                ),\n                tb=self.tb,\n            ),\n        )",
  "def __init__(self, bad_type, expected):\n        if isinstance(expected, str):\n            expected = [expected]\n        self.tb = traceback.extract_stack()\n        TypeError.__init__(\n            self,\n            get_error(\n                \"Expected type {e}, but got: {v} ({t})\".format(\n                    e=\"/\".join(expected), v=format_repr(bad_type), t=type(bad_type)\n                ),\n                tb=self.tb,\n                highlight=format_repr(bad_type),\n            ),\n        )",
  "class AttributeDescription(object):\n    def __init__(self, text, value=None, *args, **kwargs):\n        self.name = None\n        self.text = text\n        self.value = value\n\n    def __call__(self, attr, model):\n        self.name = attr\n\n    def __get__(self, obj, type=None):  # pragma: no cover\n        return self.value\n\n    def __set__(self, obj, val):  # pragma: no cover\n        self.value = val",
  "class Dimension(AttributeDescription):\n    def __get__(self, obj, type=None):\n        return obj._dims.get(self.name, None)\n\n    def __set__(self, obj, value):\n        obj._dims[self.name] = value",
  "class Weights(AttributeDescription):\n    def __init__(self, text, get_shape, init=None):\n        self.name = None\n        self.text = text\n        self.get_shape = get_shape\n        self.init = init\n\n    def __get__(self, obj, type=None):\n        key = (obj.id, self.name)\n        if key in obj._mem:\n            return obj._mem[key]\n        else:\n            shape = self.get_shape(obj)\n            data = obj._mem.add(key, shape)\n            if self.init is not None:\n                self.init(data, obj.ops)\n            return data\n\n    def __set__(self, obj, val):\n        data = obj._mem.get((obj.id, self.name))\n        data[:] = val",
  "class Gradient(AttributeDescription):\n    def __init__(self, param_name):\n        self.name = None\n        self.text = \"Gradient of %s\" % param_name\n        self.param_name = param_name\n\n    def __get__(self, obj, type=None):\n        key = (obj.id, self.name)\n        if key in obj._mem:\n            return obj._mem.get(key)\n        else:\n            param_key = (obj.id, self.param_name)\n            grad = obj._mem.add_gradient(key, param_key)\n            return grad\n\n    def __set__(self, obj, val):\n        data = obj._mem.get((obj.id, self.name))\n        data[:] = val",
  "class Synapses(Weights):\n    pass",
  "class Biases(Weights):\n    pass",
  "class Moment(Weights):\n    pass",
  "def attributes(**specs):\n    if not specs:  # pragma: no cover\n        raise ValueError(\"Must describe at least one attribute\")\n\n    def wrapped(cls):\n        cls.descriptions = dict(cls.descriptions)\n        cls.descriptions.update(specs)\n        for attr, desc in cls.descriptions.items():\n            setattr(cls, attr, desc)\n            desc.name = attr\n        return cls\n\n    return wrapped",
  "def on_init(*callbacks):\n    def wrapped(cls):\n        cls.on_init_hooks = list(cls.on_init_hooks)\n        cls.on_init_hooks.extend(callbacks)\n        return cls\n\n    return wrapped",
  "def on_data(*callbacks):\n    def wrapped(cls):\n        cls.on_data_hooks = list(cls.on_data_hooks)\n        cls.on_data_hooks.extend(callbacks)\n        return cls\n\n    return wrapped",
  "def input(getter):\n    def wrapped(cls):\n        cls.describe_input = getter\n        return cls\n\n    return wrapped",
  "def output(getter):\n    def wrapped(cls):\n        cls.describe_output = getter\n        return cls\n\n    return wrapped",
  "def __init__(self, text, value=None, *args, **kwargs):\n        self.name = None\n        self.text = text\n        self.value = value",
  "def __call__(self, attr, model):\n        self.name = attr",
  "def __get__(self, obj, type=None):  # pragma: no cover\n        return self.value",
  "def __set__(self, obj, val):  # pragma: no cover\n        self.value = val",
  "def __get__(self, obj, type=None):\n        return obj._dims.get(self.name, None)",
  "def __set__(self, obj, value):\n        obj._dims[self.name] = value",
  "def __init__(self, text, get_shape, init=None):\n        self.name = None\n        self.text = text\n        self.get_shape = get_shape\n        self.init = init",
  "def __get__(self, obj, type=None):\n        key = (obj.id, self.name)\n        if key in obj._mem:\n            return obj._mem[key]\n        else:\n            shape = self.get_shape(obj)\n            data = obj._mem.add(key, shape)\n            if self.init is not None:\n                self.init(data, obj.ops)\n            return data",
  "def __set__(self, obj, val):\n        data = obj._mem.get((obj.id, self.name))\n        data[:] = val",
  "def __init__(self, param_name):\n        self.name = None\n        self.text = \"Gradient of %s\" % param_name\n        self.param_name = param_name",
  "def __get__(self, obj, type=None):\n        key = (obj.id, self.name)\n        if key in obj._mem:\n            return obj._mem.get(key)\n        else:\n            param_key = (obj.id, self.param_name)\n            grad = obj._mem.add_gradient(key, param_key)\n            return grad",
  "def __set__(self, obj, val):\n        data = obj._mem.get((obj.id, self.name))\n        data[:] = val",
  "def wrapped(cls):\n        cls.descriptions = dict(cls.descriptions)\n        cls.descriptions.update(specs)\n        for attr, desc in cls.descriptions.items():\n            setattr(cls, attr, desc)\n            desc.name = attr\n        return cls",
  "def wrapped(cls):\n        cls.on_init_hooks = list(cls.on_init_hooks)\n        cls.on_init_hooks.extend(callbacks)\n        return cls",
  "def wrapped(cls):\n        cls.on_data_hooks = list(cls.on_data_hooks)\n        cls.on_data_hooks.extend(callbacks)\n        return cls",
  "def wrapped(cls):\n        cls.describe_input = getter\n        return cls",
  "def wrapped(cls):\n        cls.describe_output = getter\n        return cls",
  "def categorical_crossentropy(scores, labels):\n    xp = get_array_module(scores)\n    target = xp.zeros(scores.shape, dtype=\"float32\")\n    loss = 0.0\n    for i in range(len(labels)):\n        target[i, int(labels[i])] = 1.0\n        loss += (1.0 - scores[i, int(labels[i])]) ** 2\n    return scores - target, loss",
  "def L1_distance(vec1, vec2, labels, margin=0.2):\n    xp = get_array_module(vec1)\n    dist = xp.abs(vec1 - vec2).sum(axis=1)\n    loss = (dist > margin) - labels\n    return (vec1 - vec2) * loss, (vec2 - vec1) * loss, loss",
  "def get_array_module(*a, **k):\n        return numpy",
  "def is_docs(arg_id, args, kwargs):\n    from spacy.tokens.doc import Doc\n\n    docs = args[arg_id]\n    if not isinstance(docs, Sequence):\n        raise ExpectedTypeError(type(docs), [\"Sequence\"])\n    if not isinstance(docs[0], Doc):\n        raise ExpectedTypeError(type(docs[0]), [\"spacy.tokens.doc.Doc\"])",
  "def equal_length(*args):\n    \"\"\"Check that arguments have the same length.\n    \"\"\"\n    for i, arg in enumerate(args):\n        if not isinstance(arg, Sized):\n            raise ExpectedTypeError(arg, [\"Sized\"])\n        if i >= 1 and len(arg) != len(args[0]):\n            raise DifferentLengthError(args, arg)",
  "def equal_axis(*args, **axis):\n    \"\"\"Check that elements have the same dimension on specified axis.\n    \"\"\"\n    axis = axis.get(\"axis\", -1)\n    for i, arg in enumerate(args):\n        if not isinstance(arg, ndarray):\n            raise ExpectedTypeError(arg, [\"ndarray\"])\n        if axis >= 0 and (axis + 1) < arg.shape[axis]:\n            raise ShapeMismatchError(arg.shape[axis], axis, [])\n        if i >= 1 and arg.shape[axis] != args[0].shape[axis]:\n            lengths = [a.shape[axis] for a in args]\n            raise DifferentLengthError(lengths, arg)",
  "def has_shape(shape):\n    \"\"\"Check that a particular argument is an array with a given shape. The\n    shape may contain string attributes, which will be fetched from arg0 to\n    the function (usually self).\n    \"\"\"\n\n    def has_shape_inner(arg_id, args, kwargs):\n        self = args[0]\n        arg = args[arg_id]\n        if not hasattr(arg, \"shape\"):\n            raise ExpectedTypeError(arg, [\"array\"])\n        shape_values = []\n        for dim in shape:\n            if not isinstance(dim, integer_types):\n                dim = getattr(self, dim, None)\n            shape_values.append(dim)\n        if len(shape) != len(arg.shape):\n            raise ShapeMismatchError(arg.shape, tuple(shape_values), shape)\n        for i, dim in enumerate(shape_values):\n            # Allow underspecified dimensions\n            if dim is not None and arg.shape[i] != dim:\n                raise ShapeMismatchError(arg.shape, shape_values, shape)\n\n    return has_shape_inner",
  "def is_shape(arg_id, args, func_kwargs, **kwargs):\n    arg = args[arg_id]\n    if not isinstance(arg, Iterable):\n        raise ExpectedTypeError(arg, [\"iterable\"])\n    for value in arg:\n        if not isinstance(value, integer_types) or value < 0:\n            raise ExpectedTypeError(arg, [\"valid shape (positive ints)\"])",
  "def is_sequence(arg_id, args, kwargs):\n    arg = args[arg_id]\n    if not isinstance(arg, Iterable) and not hasattr(arg, \"__getitem__\"):\n        raise ExpectedTypeError(arg, [\"iterable\"])",
  "def is_float(arg_id, args, func_kwargs, **kwargs):\n    arg = args[arg_id]\n    if not isinstance(arg, float):\n        raise ExpectedTypeError(arg, [\"float\"])\n    if \"min\" in kwargs and arg < kwargs[\"min\"]:\n        raise OutsideRangeError(arg, kwargs[\"min\"], \">=\")\n    if \"max\" in kwargs and arg > kwargs[\"max\"]:\n        raise OutsideRangeError(arg, kwargs[\"max\"], \"<=\")",
  "def is_int(arg_id, args, func_kwargs, **kwargs):\n    arg = args[arg_id]\n    if not isinstance(arg, integer_types):\n        raise ExpectedTypeError(arg, [\"int\"])\n    if \"min\" in kwargs and arg < kwargs[\"min\"]:\n        raise OutsideRangeError(arg, kwargs[\"min\"], \">=\")\n    if \"max\" in kwargs and arg > kwargs[\"max\"]:\n        raise OutsideRangeError(arg, kwargs[\"max\"], \"<=\")",
  "def is_array(arg_id, args, func_kwargs, **kwargs):\n    arg = args[arg_id]\n    if not isinstance(arg, ndarray):\n        raise ExpectedTypeError(arg, [\"ndarray\"])",
  "def is_int_array(arg_id, args, func_kwargs, **kwargs):\n    arg = args[arg_id]\n    if not isinstance(arg, ndarray) or \"i\" not in arg.dtype.kind:\n        raise ExpectedTypeError(arg, [\"ndarray[int]\"])",
  "def operator_is_defined(op):\n    @wrapt.decorator\n    def checker(wrapped, instance, args, kwargs):\n        if instance is None:\n            instance = args[0]\n        if instance is None:\n            raise ExpectedTypeError(instance, [\"Model\"])\n        if op not in instance._operators:\n            raise UndefinedOperatorError(op, instance, args[0], instance._operators)\n        else:\n            return wrapped(*args, **kwargs)\n\n    return checker",
  "def arg(arg_id, *constraints):\n    @wrapt.decorator\n    def checked_function(wrapped, instance, args, kwargs):\n        # for partial functions or other C-compiled functions\n        if not hasattr(wrapped, \"checks\"):  # pragma: no cover\n            return wrapped(*args, **kwargs)\n        if instance is not None:\n            fix_args = [instance] + list(args)\n        else:\n            fix_args = list(args)\n        for arg_id, checks in wrapped.checks.items():\n            for check in checks:\n                if not isinstance(check, Callable):\n                    raise ExpectedTypeError(check, [\"Callable\"])\n                check(arg_id, fix_args, kwargs)\n        return wrapped(*args, **kwargs)\n\n    def arg_check_adder(func):\n        if hasattr(func, \"checks\"):\n            func.checks.setdefault(arg_id, []).extend(constraints)\n            return func\n        else:\n            wrapped = checked_function(func)\n            wrapped.checks = {arg_id: list(constraints)}\n            return wrapped\n\n    return arg_check_adder",
  "def args(*constraints):\n    @wrapt.decorator\n    def arg_check_adder(wrapped, instance, args, kwargs):\n        for check in constraints:\n            if not isinstance(check, Callable):\n                raise ExpectedTypeError(check, [\"Callable\"])\n            check(*args)\n        return wrapped(*args, **kwargs)\n\n    return arg_check_adder",
  "def has_shape_inner(arg_id, args, kwargs):\n        self = args[0]\n        arg = args[arg_id]\n        if not hasattr(arg, \"shape\"):\n            raise ExpectedTypeError(arg, [\"array\"])\n        shape_values = []\n        for dim in shape:\n            if not isinstance(dim, integer_types):\n                dim = getattr(self, dim, None)\n            shape_values.append(dim)\n        if len(shape) != len(arg.shape):\n            raise ShapeMismatchError(arg.shape, tuple(shape_values), shape)\n        for i, dim in enumerate(shape_values):\n            # Allow underspecified dimensions\n            if dim is not None and arg.shape[i] != dim:\n                raise ShapeMismatchError(arg.shape, shape_values, shape)",
  "def checker(wrapped, instance, args, kwargs):\n        if instance is None:\n            instance = args[0]\n        if instance is None:\n            raise ExpectedTypeError(instance, [\"Model\"])\n        if op not in instance._operators:\n            raise UndefinedOperatorError(op, instance, args[0], instance._operators)\n        else:\n            return wrapped(*args, **kwargs)",
  "def checked_function(wrapped, instance, args, kwargs):\n        # for partial functions or other C-compiled functions\n        if not hasattr(wrapped, \"checks\"):  # pragma: no cover\n            return wrapped(*args, **kwargs)\n        if instance is not None:\n            fix_args = [instance] + list(args)\n        else:\n            fix_args = list(args)\n        for arg_id, checks in wrapped.checks.items():\n            for check in checks:\n                if not isinstance(check, Callable):\n                    raise ExpectedTypeError(check, [\"Callable\"])\n                check(arg_id, fix_args, kwargs)\n        return wrapped(*args, **kwargs)",
  "def arg_check_adder(func):\n        if hasattr(func, \"checks\"):\n            func.checks.setdefault(arg_id, []).extend(constraints)\n            return func\n        else:\n            wrapped = checked_function(func)\n            wrapped.checks = {arg_id: list(constraints)}\n            return wrapped",
  "def arg_check_adder(wrapped, instance, args, kwargs):\n        for check in constraints:\n            if not isinstance(check, Callable):\n                raise ExpectedTypeError(check, [\"Callable\"])\n            check(*args)\n        return wrapped(*args, **kwargs)",
  "def is_cupy_array(arr):\n    \"\"\"Check whether an array is a cupy array\"\"\"\n    if cupy is None:\n        return False\n    elif isinstance(arr, cupy.ndarray):\n        return True\n    else:\n        return False",
  "def is_numpy_array(arr):\n    \"\"\"Check whether an array is a numpy array\"\"\"\n    if isinstance(arr, numpy.ndarray):\n        return True\n    else:\n        return False",
  "def get_ops(ops):\n    from .ops import NumpyOps, CupyOps\n\n    if ops in (\"numpy\", \"cpu\") or (isinstance(ops, int) and ops < 0):\n        return NumpyOps\n    elif ops in (\"cupy\", \"gpu\") or (isinstance(ops, int) and ops >= 0):\n        return CupyOps\n    else:\n        raise ValueError(\"Invalid ops (or device) description: %s\" % ops)",
  "def set_active_gpu(gpu_id):\n    import cupy.cuda.device\n\n    device = cupy.cuda.device.Device(gpu_id)\n    device.use()\n    try:\n        import torch\n\n        torch.cuda.set_device(gpu_id)\n        torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n    except ImportError:\n        pass\n    return device",
  "def prefer_gpu(gpu_id=0):\n    \"\"\"Use GPU if it's available. Returns True if so, False otherwise.\"\"\"\n    from .ops import CupyOps\n\n    if CupyOps.xp is None:\n        return False\n    else:\n        require_gpu(gpu_id=gpu_id)\n        return True",
  "def require_gpu(gpu_id=0):\n    from ._classes.model import Model\n    from .ops import CupyOps\n\n    if CupyOps.xp is None:\n        raise ValueError(\"GPU is not accessible. Was the library installed correctly?\")\n    Model.Ops = CupyOps\n    Model.ops = CupyOps()\n    set_active_gpu(gpu_id)\n    return True",
  "def minibatch(items, size=8):\n    \"\"\"Iterate over batches of items. `size` may be an iterator,\n    so that batch-size can vary on each step.\n    \"\"\"\n    if isinstance(size, int):\n        size_ = itertools.repeat(size)\n    else:\n        size_ = size\n    items = iter(items)\n    while True:\n        batch_size = next(size_)\n        batch = list(itertools.islice(items, int(batch_size)))\n        if len(batch) == 0:\n            break\n        yield list(batch)",
  "def mark_sentence_boundaries(sequences, drop=0.0):  # pragma: no cover\n    \"\"\"Pad sentence sequences with EOL markers.\"\"\"\n    for sequence in sequences:\n        sequence.insert(0, \"-EOL-\")\n        sequence.insert(0, \"-EOL-\")\n        sequence.append(\"-EOL-\")\n        sequence.append(\"-EOL-\")\n    return sequences, None",
  "def remap_ids(ops):\n    id_map = {0: 0}\n\n    def begin_update(ids, drop=0.0):\n        n_vector = len(id_map)\n        for i, id_ in enumerate(ids):\n            if id_ not in id_map:\n                id_map[id_] = n_vector\n                n_vector += 1\n            ids[i] = id_map[id_]\n        return ids, None\n\n    return begin_update",
  "def copy_array(dst, src, casting=\"same_kind\", where=None):\n    if isinstance(dst, numpy.ndarray) and isinstance(src, numpy.ndarray):\n        dst[:] = src\n    elif is_cupy_array(dst):\n        src = cupy.array(src, copy=False)\n        cupy.copyto(dst, src)\n    else:\n        numpy.copyto(dst, src)",
  "def ensure_path(path):\n    if isinstance(path, unicode) or isinstance(path, str):\n        return Path(path)\n    else:\n        return path",
  "def to_categorical(y, nb_classes=None):\n    # From keras\n    xp = get_array_module(y)\n    if xp is cupy:\n        y = y.get()\n    y = numpy.array(y, dtype=\"int\").ravel()\n    if not nb_classes:\n        nb_classes = numpy.max(y) + 1\n    n = y.shape[0]\n    categorical = numpy.zeros((n, nb_classes), dtype=\"float32\")\n    categorical[numpy.arange(n), y] = 1\n    return xp.asarray(categorical)",
  "def flatten_sequences(sequences, drop=0.0):  # pragma: no cover\n    xp = get_array_module(sequences[0])\n    return xp.concatenate(sequences), None",
  "def partition(examples, split_size):  # pragma: no cover\n    examples = list(examples)\n    numpy.random.shuffle(examples)\n    n_docs = len(examples)\n    split = int(n_docs * split_size)\n    return examples[:split], examples[split:]",
  "def begin_update(ids, drop=0.0):\n        n_vector = len(id_map)\n        for i, id_ in enumerate(ids):\n            if id_ not in id_map:\n                id_map[id_] = n_vector\n                n_vector += 1\n            ids[i] = id_map[id_]\n        return ids, None",
  "def parse_kernels(src):\n    kernels = {}\n    for kernel in kernel_re.findall(src):\n        name = name_re.search(kernel).group()\n        kernels[name] = kernel\n    return kernels",
  "def compile_kernels(src):\n    if cupy is None:\n        return defaultdict(lambda: None)\n    kernels = parse_kernels(src)\n    return {name: cupy.RawKernel(src, name) for name, src in kernels.items()}",
  "def compile_mmh(src):\n    if cupy is None:\n        return None\n    return cupy.RawKernel(src, \"hash_data\")",
  "def sum_pool(X, lengths, out=None, threads_per_block=128):\n    if out is None:\n        out = cupy.zeros((len(lengths), X.shape[1]), dtype=\"f\")\n    B = len(lengths)\n    T = X.shape[0]\n    O = X.shape[1]\n    num_blocks = min(1, B // threads_per_block)\n    sum_pool_kernel((num_blocks,), (threads_per_block,), (out, X, lengths, B, T, O))\n    return out",
  "def mean_pool(X, lengths, out=None, threads_per_block=128):\n    if out is None:\n        out = cupy.zeros((len(lengths), X.shape[1]), dtype=\"f\")\n    B = len(lengths)\n    T = X.shape[0]\n    O = X.shape[1]\n    num_blocks = min(1, B // threads_per_block)\n    sum_pool_kernel((num_blocks,), (threads_per_block,), (out, X, lengths, B, T, O))\n    out /= lengths\n    return out",
  "def max_pool(X, lengths, out=None, threads_per_block=128):\n    if out is None:\n        maxes = cupy.zeros((len(lengths), X.shape[1]), dtype=\"f\")\n        which = cupy.zeros((len(lengths), X.shape[1]), dtype=\"i\")\n    else:\n        maxes, which = out\n    B = len(lengths)\n    T = X.shape[0]\n    O = X.shape[1]\n    num_blocks = max(1, B // threads_per_block)\n    max_pool_kernel((num_blocks,), (threads_per_block,),\n        (maxes, which, X, lengths, B, T, O))\n    return maxes, which",
  "def backprop_sum_pool(d_sum, lengths, out=None, threads_per_block=128):\n    B = len(lengths)\n    T = int(lengths.sum())\n    O = d_sum.shape[1]\n    if out is None:\n        out = cupy.zeros((T, O), dtype=\"f\")\n\n    num_blocks = max(1, T // threads_per_block)\n    backprop_sum_pool_kernel((num_blocks,), (threads_per_block,),\n        (out, d_sum, lengths, B, T, O))\n    return out",
  "def backprop_mean_pool(d_mean, lengths, out=None, threads_per_block=128):\n    out = backprop_sum_pool(d_mean, lengths, out=out,\n            threads_per_block=threads_per_block)\n    out /= lengths\n    return out",
  "def backprop_max_pool(d_maxes, which, lengths, out=None, threads_per_block=128):\n    B = len(lengths)\n    T = int(lengths.sum())\n    O = d_maxes.shape[1]\n    if out is None:\n        out = cupy.zeros((T, O), dtype=\"f\")\n\n    num_blocks = max(1, T // threads_per_block)\n    backprop_max_pool_kernel((num_blocks,), (threads_per_block,),\n        (out, d_maxes, which, lengths, B, T, O))\n    return out",
  "def hash(ids, seed, out=None, threads_per_block=128):\n    if out is None:\n        out = cupy.zeros((ids.shape[0], 4), dtype=\"uint32\")\n    # sizeof(uint32_t) * 4\n    out_size = 4 * 4\n    in_size = 8 # sizeof(uint64_t)\n    T = ids.shape[0]\n    # Having trouble executing this in parallel? Shrug\n    hash_data_kernel((T,), (1,),\n        (out, ids, out_size, in_size, ids.shape[0], seed))\n    return out",
  "class Memory(object):\n    def __init__(self, ops, size=128):\n        if size < 0:\n            raise ValueError(\"TODO error re negative size %d\" % size)\n        self.ops = ops\n        self._mem = self.ops.allocate((2, size))\n        self._offsets = {}\n        self._sizes = {}\n        self._i = 0\n\n    @property\n    def weights(self):\n        return self._mem[0, : self._i]\n\n    @property\n    def gradient(self):\n        return self._mem[1, : self._i]\n\n    def __contains__(self, name):\n        return name in self._offsets\n\n    def __getitem__(self, name):\n        offset, col, shape = self._offsets[name]\n        size = self._sizes[name]\n        return self._mem[col, offset : offset + size].reshape(shape)\n\n    def get(self, name, default=None):\n        return self[name] if name in self._offsets else default\n\n    def set(self, value):\n        self._mem[0, : self._i] = value\n\n    @check.arg(2, is_shape)\n    def add(self, name, shape):\n        assert name not in self._offsets, \"TODO error\"\n        self._offsets[name] = (self._i, 0, shape)\n        size = prod(shape)\n        self._sizes[name] = size\n        blob = self._get_blob(size)\n        return blob[0].reshape(shape)\n\n    def add_gradient(self, grad_name, param_name):\n        assert grad_name not in self._offsets, \"TODO error\"\n        offset, _, shape = self._offsets[param_name]\n        size = self._sizes[param_name]\n        self._offsets[grad_name] = (offset, 1, shape)\n        self._sizes[grad_name] = size\n        return self._mem[1, offset : offset + size].reshape(shape)\n\n    def _get_blob(self, nr_req):\n        nr_avail = self._mem.shape[1] - (self._i + 1)\n        if nr_avail < nr_req:\n            self._realloc(max(self._mem.shape[1], nr_req) * 2)\n        blob = self._mem[:, self._i : self._i + nr_req]\n        self._i += nr_req\n        return blob\n\n    def _realloc(self, new_size):\n        new_mem = self.ops.allocate((self._mem.shape[0], new_size))\n        new_mem[:, : self._i + 1] = self._mem[:, : self._i + 1]\n        self._mem = new_mem",
  "def __init__(self, ops, size=128):\n        if size < 0:\n            raise ValueError(\"TODO error re negative size %d\" % size)\n        self.ops = ops\n        self._mem = self.ops.allocate((2, size))\n        self._offsets = {}\n        self._sizes = {}\n        self._i = 0",
  "def weights(self):\n        return self._mem[0, : self._i]",
  "def gradient(self):\n        return self._mem[1, : self._i]",
  "def __contains__(self, name):\n        return name in self._offsets",
  "def __getitem__(self, name):\n        offset, col, shape = self._offsets[name]\n        size = self._sizes[name]\n        return self._mem[col, offset : offset + size].reshape(shape)",
  "def get(self, name, default=None):\n        return self[name] if name in self._offsets else default",
  "def set(self, value):\n        self._mem[0, : self._i] = value",
  "def add(self, name, shape):\n        assert name not in self._offsets, \"TODO error\"\n        self._offsets[name] = (self._i, 0, shape)\n        size = prod(shape)\n        self._sizes[name] = size\n        blob = self._get_blob(size)\n        return blob[0].reshape(shape)",
  "def add_gradient(self, grad_name, param_name):\n        assert grad_name not in self._offsets, \"TODO error\"\n        offset, _, shape = self._offsets[param_name]\n        size = self._sizes[param_name]\n        self._offsets[grad_name] = (offset, 1, shape)\n        self._sizes[grad_name] = size\n        return self._mem[1, offset : offset + size].reshape(shape)",
  "def _get_blob(self, nr_req):\n        nr_avail = self._mem.shape[1] - (self._i + 1)\n        if nr_avail < nr_req:\n            self._realloc(max(self._mem.shape[1], nr_req) * 2)\n        blob = self._mem[:, self._i : self._i + nr_req]\n        self._i += nr_req\n        return blob",
  "def _realloc(self, new_size):\n        new_mem = self.ops.allocate((self._mem.shape[0], new_size))\n        new_mem[:, : self._i + 1] = self._mem[:, : self._i + 1]\n        self._mem = new_mem",
  "class Trainer(object):\n    def __init__(self, model, **cfg):\n        self.ops = model.ops\n        self.model = model\n        self.L2 = cfg.get(\"L2\", 0.0)\n        self.optimizer = Adam(model.ops, 0.001, decay=0.0, eps=1e-8, L2=self.L2)\n        self.batch_size = cfg.get(\"batch_size\", 128)\n        self.nb_epoch = cfg.get(\"nb_epoch\", 20)\n        self.i = 0\n        self.dropout = cfg.get(\"dropout\", 0.0)\n        self.dropout_decay = cfg.get(\"dropout_decay\", 0.0)\n        self.each_epoch = []\n\n    def __enter__(self):\n        return self, self.optimizer\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.model.use_params(self.optimizer.averages)\n\n    def iterate(self, train_X, train_y, progress_bar=True):\n        orig_dropout = self.dropout\n        for i in range(self.nb_epoch):\n            indices = numpy.arange(len(train_X))\n            numpy.random.shuffle(indices)\n            indices = self.ops.asarray(indices)\n            j = 0\n            with tqdm(total=indices.shape[0], leave=False) as pbar:\n                while j < indices.shape[0]:\n                    slice_ = indices[j : j + self.batch_size]\n                    X = _take_slice(train_X, slice_)\n                    y = _take_slice(train_y, slice_)\n                    yield X, y\n                    self.dropout = linear_decay(orig_dropout, self.dropout_decay, j)\n                    j += self.batch_size\n                    if progress_bar:\n                        pbar.update(self.batch_size)\n            for func in self.each_epoch:\n                func()",
  "def _take_slice(data, slice_):\n    if isinstance(data, list) or isinstance(data, tuple):\n        return [data[int(i)] for i in slice_]\n    else:\n        return data[slice_]",
  "def __init__(self, model, **cfg):\n        self.ops = model.ops\n        self.model = model\n        self.L2 = cfg.get(\"L2\", 0.0)\n        self.optimizer = Adam(model.ops, 0.001, decay=0.0, eps=1e-8, L2=self.L2)\n        self.batch_size = cfg.get(\"batch_size\", 128)\n        self.nb_epoch = cfg.get(\"nb_epoch\", 20)\n        self.i = 0\n        self.dropout = cfg.get(\"dropout\", 0.0)\n        self.dropout_decay = cfg.get(\"dropout_decay\", 0.0)\n        self.each_epoch = []",
  "def __enter__(self):\n        return self, self.optimizer",
  "def __exit__(self, exc_type, exc_val, exc_tb):\n        self.model.use_params(self.optimizer.averages)",
  "def iterate(self, train_X, train_y, progress_bar=True):\n        orig_dropout = self.dropout\n        for i in range(self.nb_epoch):\n            indices = numpy.arange(len(train_X))\n            numpy.random.shuffle(indices)\n            indices = self.ops.asarray(indices)\n            j = 0\n            with tqdm(total=indices.shape[0], leave=False) as pbar:\n                while j < indices.shape[0]:\n                    slice_ = indices[j : j + self.batch_size]\n                    X = _take_slice(train_X, slice_)\n                    y = _take_slice(train_y, slice_)\n                    yield X, y\n                    self.dropout = linear_decay(orig_dropout, self.dropout_decay, j)\n                    j += self.batch_size\n                    if progress_bar:\n                        pbar.update(self.batch_size)\n            for func in self.each_epoch:\n                func()",
  "def Pooling(*funcs, **kwargs):\n    ops = kwargs[\"ops\"] if \"ops\" in kwargs else Model.ops\n    F = len(funcs)\n    drop_factor = kwargs.get(\"drop_factor\", 1.0)\n\n    def begin_update(X_lengths, drop=0.0):\n        if drop is not None:\n            drop *= drop_factor\n        X, lengths = X_lengths\n        T, O = X.shape  # noqa: E741\n        pooled = ops.allocate((len(lengths), F * O))\n        bp_funcs = [None] * F\n        for i, func in enumerate(funcs):\n            res, bp_res = func.begin_update((X, lengths))\n            pooled[:, i * O : i * O + O] = res\n            bp_funcs[i] = bp_res\n        pooled, bp_dropout = ops.dropout(pooled, drop)\n\n        def finish_update(d_pooled, sgd=None):\n            dX = ops.allocate(X.shape)\n            for i, bp_func in enumerate(bp_funcs):\n                dX += bp_func(d_pooled[:, i * O : i * O + O])\n            return dX\n\n        return pooled, bp_dropout(finish_update)\n\n    return layerize(begin_update)",
  "def mean_pool(X_lengths, drop=0.0):\n    X, lengths = X_lengths\n    if isinstance(X, numpy.ndarray):\n        ops = NumpyOps()\n    else:\n        ops = CupyOps()\n    output = ops.mean_pool(X, lengths)\n\n    def finish_update(d_output, sgd=None):\n        d_output = ops.xp.ascontiguousarray(d_output)\n        return ops.backprop_mean_pool(d_output, lengths)\n\n    return output, finish_update",
  "def sum_pool(X_lengths, drop=0.0):\n    X, lengths = X_lengths\n    if isinstance(X, numpy.ndarray):\n        ops = NumpyOps()\n    else:\n        ops = CupyOps()\n\n    output = ops.sum_pool(X, lengths)\n\n    def finish_update(d_output, sgd=None):\n        d_output = ops.xp.ascontiguousarray(d_output)\n        return ops.backprop_sum_pool(d_output, lengths)\n\n    return output, finish_update",
  "def max_pool(X_lengths, drop=0.0):\n    X, lengths = X_lengths\n    ops = Model.ops\n\n    best, which = ops.max_pool(X, lengths)\n\n    def finish_update(d_output, sgd=None):\n        d_output = ops.xp.ascontiguousarray(d_output)\n        return ops.backprop_max_pool(d_output, which, lengths)\n\n    return best, finish_update",
  "def begin_update(X_lengths, drop=0.0):\n        if drop is not None:\n            drop *= drop_factor\n        X, lengths = X_lengths\n        T, O = X.shape  # noqa: E741\n        pooled = ops.allocate((len(lengths), F * O))\n        bp_funcs = [None] * F\n        for i, func in enumerate(funcs):\n            res, bp_res = func.begin_update((X, lengths))\n            pooled[:, i * O : i * O + O] = res\n            bp_funcs[i] = bp_res\n        pooled, bp_dropout = ops.dropout(pooled, drop)\n\n        def finish_update(d_pooled, sgd=None):\n            dX = ops.allocate(X.shape)\n            for i, bp_func in enumerate(bp_funcs):\n                dX += bp_func(d_pooled[:, i * O : i * O + O])\n            return dX\n\n        return pooled, bp_dropout(finish_update)",
  "def finish_update(d_output, sgd=None):\n        d_output = ops.xp.ascontiguousarray(d_output)\n        return ops.backprop_mean_pool(d_output, lengths)",
  "def finish_update(d_output, sgd=None):\n        d_output = ops.xp.ascontiguousarray(d_output)\n        return ops.backprop_sum_pool(d_output, lengths)",
  "def finish_update(d_output, sgd=None):\n        d_output = ops.xp.ascontiguousarray(d_output)\n        return ops.backprop_max_pool(d_output, which, lengths)",
  "def finish_update(d_pooled, sgd=None):\n            dX = ops.allocate(X.shape)\n            for i, bp_func in enumerate(bp_funcs):\n                dX += bp_func(d_pooled[:, i * O : i * O + O])\n            return dX",
  "def Pooling(*funcs, **kwargs):\n    ops = kwargs[\"ops\"] if \"ops\" in kwargs else funcs[0].ops\n    F = len(funcs)\n\n    def begin_update(X_lengths, drop=0.0):\n        X, lengths = X_lengths\n        X, bp_dropout = ops.dropout(X, drop)\n        B, O = X.shape  # noqa: E741\n        pooled = ops.allocate((F, len(lengths), O))\n        bp_funcs = [None] * F\n        for i, func in enumerate(funcs):\n            pooled[i], bp_funcs[i] = func.begin_update((X, lengths))\n\n        def finish_update(d_pooled, sgd=None):\n            d_pooled = d_pooled.reshape((len(lengths), F, O))\n            d_pooled = d_pooled.transpose((1, 0, 2))\n            dX = ops.allocate(X.shape)\n            for i, bp_func in enumerate(bp_funcs):\n                dX += bp_func(d_pooled[i])\n            return dX\n\n        pooled = pooled.transpose((1, 0, 2))\n        pooled = pooled.reshape((len(lengths), F * O))\n        return pooled, finish_update\n\n    return layerize(begin_update)",
  "def mean_pool(X_lengths, drop=0.0):\n    X, lengths = X_lengths\n    xp = get_array_module(X)\n    output = xp.zeros((len(lengths), X.shape[1]), dtype=\"float32\")\n    start = 0\n    for i, length in enumerate(lengths):\n        end = start + length\n        output[i] = X[start:end].mean(axis=0)\n        start = end\n\n    def finish_update(d_output, sgd=None):\n        d_X = xp.zeros((X.shape[0], X.shape[1]), dtype=\"float32\")\n        start = 0\n        for i, length in enumerate(lengths):\n            end = start + length\n            d_X[start:end] += d_output[i] / (end - start)\n            start = end\n        return d_X\n\n    return output, finish_update",
  "def max_pool(X_lengths, drop=0.0):\n    X, lengths = X_lengths\n    xp = get_array_module(X)\n    maxes = xp.zeros((len(lengths), X.shape[1]), dtype=\"float32\")\n    start = 0\n    for i, length in enumerate(lengths):\n        end = start + length\n        maxes[i] = X[start:end].max(axis=0)\n        start = end\n\n    def finish_update(d_maxes, sgd=None):\n        d_X = xp.zeros((X.shape[0], X.shape[1]), dtype=\"float32\")\n        start = 0\n        for i, length in enumerate(lengths):\n            end = start + length\n            d_X[start:end] += d_maxes[i] * (d_maxes[i] == maxes[i])\n            start = end\n        return d_X\n\n    return maxes, finish_update",
  "def begin_update(X_lengths, drop=0.0):\n        X, lengths = X_lengths\n        X, bp_dropout = ops.dropout(X, drop)\n        B, O = X.shape  # noqa: E741\n        pooled = ops.allocate((F, len(lengths), O))\n        bp_funcs = [None] * F\n        for i, func in enumerate(funcs):\n            pooled[i], bp_funcs[i] = func.begin_update((X, lengths))\n\n        def finish_update(d_pooled, sgd=None):\n            d_pooled = d_pooled.reshape((len(lengths), F, O))\n            d_pooled = d_pooled.transpose((1, 0, 2))\n            dX = ops.allocate(X.shape)\n            for i, bp_func in enumerate(bp_funcs):\n                dX += bp_func(d_pooled[i])\n            return dX\n\n        pooled = pooled.transpose((1, 0, 2))\n        pooled = pooled.reshape((len(lengths), F * O))\n        return pooled, finish_update",
  "def finish_update(d_output, sgd=None):\n        d_X = xp.zeros((X.shape[0], X.shape[1]), dtype=\"float32\")\n        start = 0\n        for i, length in enumerate(lengths):\n            end = start + length\n            d_X[start:end] += d_output[i] / (end - start)\n            start = end\n        return d_X",
  "def finish_update(d_maxes, sgd=None):\n        d_X = xp.zeros((X.shape[0], X.shape[1]), dtype=\"float32\")\n        start = 0\n        for i, length in enumerate(lengths):\n            end = start + length\n            d_X[start:end] += d_maxes[i] * (d_maxes[i] == maxes[i])\n            start = end\n        return d_X",
  "def finish_update(d_pooled, sgd=None):\n            d_pooled = d_pooled.reshape((len(lengths), F, O))\n            d_pooled = d_pooled.transpose((1, 0, 2))\n            dX = ops.allocate(X.shape)\n            for i, bp_func in enumerate(bp_funcs):\n                dX += bp_func(d_pooled[i])\n            return dX",
  "def svd_orthonormal(shape):\n    if len(shape) < 2:  # pragma: no cover\n        raise RuntimeError(\"Only shapes of length 2 or more are supported.\")\n    flat_shape = (shape[0], np.prod(shape[1:]))\n    a = np.random.standard_normal(flat_shape)\n    u, _, v = np.linalg.svd(a, full_matrices=False)\n    q = u if u.shape == flat_shape else v\n    q = q.reshape(shape)\n    return q",
  "def do_lsuv(ops, weights, predict, X):\n    if id(predict) in _initialized:\n        return\n    _initialized.add(id(predict))\n    copy_array(weights, svd_orthonormal(weights.shape))\n    X_copy = ops.xp.ascontiguousarray(X)\n    tol_var = 0.1\n    t_max = 10\n    t_i = 0\n    while True:\n        acts1 = predict(X_copy)\n        var = np.var(acts1)\n        if abs(var - 1.0) < tol_var or t_i > t_max:\n            break\n        weights /= ops.xp.sqrt(var)\n        t_i += 1\n    return predict(X_copy)",
  "def LSUVinit(model, X, y=None):\n    if model.name == \"batchnorm\":  # pragma: no cover\n        model = model._layers[0]\n    if model.name in \"softmax\":  # pragma: no cover\n        return\n    if hasattr(model, \"lsuv\") and not model.lsuv:\n        return\n    if model.id in _initialized:\n        return\n    _initialized.add(model.id)\n    return do_lsuv(model.ops, model.W, model, X)",
  "class FeatureExtracter(Model):\n    def __init__(self, attrs):\n        Model.__init__(self)\n        self.attrs = attrs\n\n    def begin_update(self, docs, drop=0.0):\n        # Handle spans\n        features = [self._get_feats(doc) for doc in docs]\n        return features, _feature_extracter_bwd\n\n    def _get_feats(self, doc):\n        if hasattr(doc, \"to_array\"):\n            arr = doc.to_array(self.attrs)\n        else:\n            arr = doc.doc.to_array(self.attrs)[doc.start : doc.end]\n        return self.ops.asarray(arr, dtype=\"uint64\")",
  "def _feature_extracter_bwd(d_features, sgd=None):\n    return d_features",
  "def __init__(self, attrs):\n        Model.__init__(self)\n        self.attrs = attrs",
  "def begin_update(self, docs, drop=0.0):\n        # Handle spans\n        features = [self._get_feats(doc) for doc in docs]\n        return features, _feature_extracter_bwd",
  "def _get_feats(self, doc):\n        if hasattr(doc, \"to_array\"):\n            arr = doc.to_array(self.attrs)\n        else:\n            arr = doc.doc.to_array(self.attrs)[doc.start : doc.end]\n        return self.ops.asarray(arr, dtype=\"uint64\")",
  "class Model(object):\n    \"\"\"Model base class.\"\"\"\n\n    name = \"model\"\n    id = 0\n    lsuv = False\n    ops = NumpyOps()\n    Ops = NumpyOps\n    Trainer = Trainer\n    drop_factor = 1.0\n    descriptions = []\n    on_data_hooks = []\n    on_init_hooks = []  # Use this to add layers\n    _operators = THREAD_LOCAL.operators\n\n    @classmethod\n    @contextlib.contextmanager\n    def define_operators(cls, operators):\n        \"\"\"Bind operators to specified functions for the scope of the context:\n\n        Example\n        -------\n\n            model = Model()\n            other = Model()\n            with Model.define_operators({\"+\": lambda self, other: \"plus\"}):\n                print(model + other)\n                # \"plus\"\n            print(model + other)\n            # Raises TypeError --- binding limited to scope of with block.\n        \"\"\"\n        old_ops = dict(cls._operators)\n        for op, func in operators.items():\n            cls._operators[op] = func\n        yield\n        cls._operators = old_ops\n\n    @classmethod\n    @contextlib.contextmanager\n    def use_device(cls, device):\n        \"\"\"Change the device to execute on for the scope of the block.\"\"\"\n        if device == cls.ops.device:\n            yield\n        else:\n            curr_Ops, curr_ops = (cls.Ops, cls.ops)\n            cls.Ops = get_ops(device)\n            cls.ops = cls.Ops()\n            yield\n            cls.Ops = curr_Ops\n            cls.ops = curr_ops\n\n    @property\n    def input_shape(self):\n        raise NotImplementedError\n\n    @property\n    def output_shape(self):\n        raise NotImplementedError\n\n    def __init__(self, *args, **kwargs):\n        self.name = self.__class__.name\n        self.ops = self.Ops()\n        kwargs = self._update_defaults(args, kwargs)\n        self._mem = Memory(self.ops)\n        self._dims = {}\n        if not hasattr(self, \"_layers\"):\n            self._layers = []\n        self.descriptions = dict(self.descriptions)\n        self.on_init_hooks = list(self.on_init_hooks)\n        self.on_data_hooks = list(self.on_data_hooks)\n\n        for attr, install in self.descriptions.items():\n            install(attr, self)\n        for hook in self.on_init_hooks:\n            hook(self, *args, **kwargs)\n        self.set_id()\n\n    def __getstate__(self):\n        return srsly.pickle_dumps(self.__dict__)\n\n    def __setstate__(self, state_data):\n        self.__dict__ = srsly.pickle_loads(state_data)\n\n    def _update_defaults(self, args, kwargs):\n        new_kwargs = {}\n        for key, value in kwargs.items():\n            if hasattr(self, key):\n                setattr(self, key, value)\n            else:\n                new_kwargs[key] = value\n        return new_kwargs\n\n    def set_id(self):\n        Model.id += 1\n        self.id = Model.id\n        for layer in self._layers:\n            layer.set_id()\n\n    # @check.args(equal_length)\n    @check.arg(1, is_sequence)\n    def begin_training(self, train_X, train_y=None, **trainer_cfg):\n        for hook in self.on_data_hooks:\n            hook(self, train_X, train_y)\n        return self.Trainer(self, **trainer_cfg)\n\n    @check.arg(2, is_float)\n    @check.arg(1, has_shape((\"nB\", \"nI\")))\n    def begin_update(self, X, drop=0.0):\n        raise NotImplementedError\n\n    def predict(self, X):\n        y, _ = self.begin_update(X, drop=None)\n        return y\n\n    def predict_one(self, x):\n        X = self.ops.expand_dims(x, axis=0)\n        return self.predict(X)[0]\n\n    @contextlib.contextmanager\n    def use_params(self, params):  # pragma: no cover\n        backup = None\n        weights = self._mem.weights\n        if self.id in params:\n            param = params[self.id]\n            backup = weights.copy()\n            copy_array(weights, param)\n        if hasattr(self, \"_layers\"):\n            contexts = [layer.use_params(params) for layer in self._layers]\n            for context in contexts:\n                next(context.gen)\n        yield\n        if backup is not None:\n            copy_array(self._mem.weights, backup)\n        for i, context in enumerate(contexts):\n            # This is ridiculous, but apparently it's what you\n            # have to do to make this work across Python 2/3?\n            try:\n                next(context.gen)\n            except StopIteration:\n                pass\n\n    def __call__(self, x):\n        \"\"\"\n        x\n            Must match expected type\n            Must match expected shape\n        \"\"\"\n        return self.predict(x)\n\n    def pipe(self, stream, batch_size=128):\n        for batch in util.minibatch(stream, batch_size):\n            ys = self.predict(batch)\n            for y in ys:\n                yield y\n\n    def update(self, stream, batch_size=1000):\n        for X, y in util.minibatch(stream, batch_size=batch_size):\n            output, finish_update = self.begin_update(X)\n            gradient = finish_update(y)\n            yield gradient\n\n    def to_gpu(self, device_num):\n        import cupy.cuda.device\n\n        device = cupy.cuda.device.Device(device_num)\n        device.use()\n        queue = [self]\n        for layer in queue:\n            layer.ops = CupyOps()\n            layer.Ops = CupyOps\n            if hasattr(layer, \"_mem\"):\n                layer._mem._mem = self.ops.xp.asarray(layer._mem._mem)\n                layer._mem.ops = layer.ops\n            if hasattr(layer, \"_layers\"):\n                queue.extend(layer._layers)\n        return device\n\n    def to_cpu(self):\n        queue = [self]\n        for layer in queue:\n            layer.ops = NumpyOps()\n            layer.Ops = NumpyOps\n            if hasattr(layer, \"_mem\"):\n                if hasattr(layer._mem._mem, \"get\"):\n                    layer._mem._mem = layer._mem._mem.get()\n                layer._mem.ops = layer.ops\n            if hasattr(layer, \"_layers\"):\n                queue.extend(layer._layers)\n\n    def evaluate(self, X, y):\n        \"\"\"\n        x\n            Must match expected type\n            Must match expected shape\n        y\n            Must match expected type\n        \"\"\"\n        scores = self.ops.flatten(list(self.pipe(X)))\n        if not hasattr(y, \"shape\"):\n            y = self.ops.flatten(y)\n        scores = scores.reshape(y.shape)\n        if len(scores.shape) == 1:\n            correct = ((scores >= 0.5) == (y >= 0.5)).sum()\n        else:\n            correct = (scores.argmax(axis=1) == y.argmax(axis=1)).sum()\n        return correct / y.shape[0]\n\n    def evaluate_logloss(self, X, y, minimum=None, maximum=None):\n        yh = self.ops.xp.vstack(self.pipe(X))\n        yh = yh.reshape(y.shape)\n        if minimum is not None:\n            yh = self.ops.xp.maximum(yh, minimum)\n        if maximum is not None:\n            yh = self.ops.xp.minimum(yh, maximum)\n        assert len(yh.shape) == 1\n        losses = -y * self.ops.xp.log(yh + 1e-8) - (1 - y) * self.ops.xp.log(\n            (1 - yh) + 1e-8\n        )\n        return losses.mean()\n\n    @check.operator_is_defined(\"+\")\n    def __add__(self, other):\n        \"\"\"Apply the function bound to the '+' operator.\"\"\"\n        return self._operators[\"+\"](self, other)\n\n    @check.operator_is_defined(\"-\")\n    def __sub__(self, other):\n        \"\"\"Apply the function bound to the '-' operator.\"\"\"\n        return self._operators[\"-\"](self, other)\n\n    @check.operator_is_defined(\"*\")\n    def __mul__(self, other):\n        \"\"\"Apply the function bound to the '*' operator.\"\"\"\n        return self._operators[\"*\"](self, other)\n\n    @check.operator_is_defined(\"@\")\n    def __matmul__(self, other):\n        \"\"\"Apply the function bound to the '@' operator.\"\"\"\n        return self._operators[\"@\"](self, other)\n\n    @check.operator_is_defined(\"/\")\n    def __div__(self, other):\n        \"\"\"Apply the function bound to the '/' operator.\"\"\"\n        return self._operators[\"/\"](self, other)\n\n    @check.operator_is_defined(\"/\")\n    def __truediv__(self, other):  # pragma: no cover\n        \"\"\"Apply the function bound to the '/' operator.\"\"\"\n        return self._operators[\"/\"](self, other)\n\n    @check.operator_is_defined(\"//\")\n    def __floordiv__(self, other):\n        \"\"\"Apply the function bound to the '//' operator.\"\"\"\n        return self._operators[\"//\"](self, other)\n\n    @check.operator_is_defined(\"%\")\n    def __mod__(self, other):\n        \"\"\"Apply the function bound to the '%' operator.\"\"\"\n        return self._operators[\"%\"](self, other)\n\n    @check.operator_is_defined(\"**\")\n    def __pow__(self, other, modulo=None):\n        \"\"\"Apply the function bound to the '**' operator.\"\"\"\n        return self._operators[\"**\"](self, other)\n\n    @check.operator_is_defined(\"<<\")\n    def __lshift__(self, other):\n        \"\"\"Apply the function bound to the '<<' operator.\"\"\"\n        return self._operators[\"<<\"](self, other)\n\n    @check.operator_is_defined(\">>\")\n    def __rshift__(self, other):\n        \"\"\"Apply the function bound to the '>>' operator.\"\"\"\n        return self._operators[\">>\"](self, other)\n\n    @check.operator_is_defined(\"&\")\n    def __and__(self, other):\n        \"\"\"Apply the function bound to the '&' operator.\"\"\"\n        return self._operators[\"&\"](self, other)\n\n    @check.operator_is_defined(\"^\")\n    def __xor__(self, other):\n        \"\"\"Apply the function bound to the '^' operator.\"\"\"\n        return self._operators[\"^\"](self, other)\n\n    @check.operator_is_defined(\"|\")\n    def __or__(self, other):\n        \"\"\"Apply the function bound to the '|' operator.\"\"\"\n        return self._operators[\"|\"](self, other)\n\n    def to_bytes(self):\n        weights = []\n        queue = [self]\n        i = 0\n        for layer in queue:\n            # Hack to support saving/loading PyTorch models. TODO: Improve\n            if hasattr(layer, \"_model\") and not isinstance(layer._model, Model):\n                weights.append(layer.to_bytes())\n            elif hasattr(layer, \"_mem\"):\n                weights.append(\n                    OrderedDict(\n                        (\n                            (b\"dims\", OrderedDict(sorted(layer._dims.items()))),\n                            (b\"params\", []),\n                        )\n                    )\n                )\n                if hasattr(layer, \"seed\"):\n                    weights[-1][b\"seed\"] = layer.seed\n\n                offsets = sorted(layer._mem._offsets.items())\n                for (id_, name), (start, row, shape) in offsets:\n                    if row == 1:\n                        continue\n                    param = layer._mem.get((id_, name))\n                    if not isinstance(layer._mem.weights, numpy.ndarray):\n                        param = param.get()\n                    weights[-1][b\"params\"].append(\n                        OrderedDict(\n                            (\n                                (b\"name\", name),\n                                (b\"offset\", start),\n                                (b\"shape\", shape),\n                                (b\"value\", param),\n                            )\n                        )\n                    )\n                i += 1\n            if hasattr(layer, \"_layers\"):\n                queue.extend(layer._layers)\n        return srsly.msgpack_dumps({b\"weights\": weights})\n\n    def from_bytes(self, bytes_data):\n        data = srsly.msgpack_loads(bytes_data)\n        weights = data[b\"weights\"]\n        queue = [self]\n        i = 0\n        for layer in queue:\n            # Hack to support saving/loading PyTorch models. TODO: Improve\n            if hasattr(layer, \"_model\") and not isinstance(layer._model, Model):\n                layer.from_bytes(weights[i])\n                i += 1\n            elif hasattr(layer, \"_mem\"):\n                if b\"seed\" in weights[i]:\n                    layer.seed = weights[i][b\"seed\"]\n                for dim, value in weights[i][b\"dims\"].items():\n                    if isinstance(dim, bytes):\n                        dim = dim.decode(\"utf8\")\n                    setattr(layer, dim, value)\n                for param in weights[i][b\"params\"]:\n                    name = param[b\"name\"]\n                    if isinstance(name, bytes):\n                        name = name.decode(\"utf8\")\n                    dest = getattr(layer, name)\n                    copy_array(dest, param[b\"value\"])\n                i += 1\n            if hasattr(layer, \"_layers\"):\n                queue.extend(layer._layers)\n        return self\n\n    def to_disk(self, path):\n        path = util.ensure_path(path)\n        with path.open(\"wb\") as file_:\n            file_.write(self.to_bytes())\n\n    def from_disk(self, path):\n        path = util.ensure_path(path)\n        with path.open(\"rb\") as file_:\n            bytes_data = file_.read()\n        return self.from_bytes(bytes_data)",
  "def define_operators(cls, operators):\n        \"\"\"Bind operators to specified functions for the scope of the context:\n\n        Example\n        -------\n\n            model = Model()\n            other = Model()\n            with Model.define_operators({\"+\": lambda self, other: \"plus\"}):\n                print(model + other)\n                # \"plus\"\n            print(model + other)\n            # Raises TypeError --- binding limited to scope of with block.\n        \"\"\"\n        old_ops = dict(cls._operators)\n        for op, func in operators.items():\n            cls._operators[op] = func\n        yield\n        cls._operators = old_ops",
  "def use_device(cls, device):\n        \"\"\"Change the device to execute on for the scope of the block.\"\"\"\n        if device == cls.ops.device:\n            yield\n        else:\n            curr_Ops, curr_ops = (cls.Ops, cls.ops)\n            cls.Ops = get_ops(device)\n            cls.ops = cls.Ops()\n            yield\n            cls.Ops = curr_Ops\n            cls.ops = curr_ops",
  "def input_shape(self):\n        raise NotImplementedError",
  "def output_shape(self):\n        raise NotImplementedError",
  "def __init__(self, *args, **kwargs):\n        self.name = self.__class__.name\n        self.ops = self.Ops()\n        kwargs = self._update_defaults(args, kwargs)\n        self._mem = Memory(self.ops)\n        self._dims = {}\n        if not hasattr(self, \"_layers\"):\n            self._layers = []\n        self.descriptions = dict(self.descriptions)\n        self.on_init_hooks = list(self.on_init_hooks)\n        self.on_data_hooks = list(self.on_data_hooks)\n\n        for attr, install in self.descriptions.items():\n            install(attr, self)\n        for hook in self.on_init_hooks:\n            hook(self, *args, **kwargs)\n        self.set_id()",
  "def __getstate__(self):\n        return srsly.pickle_dumps(self.__dict__)",
  "def __setstate__(self, state_data):\n        self.__dict__ = srsly.pickle_loads(state_data)",
  "def _update_defaults(self, args, kwargs):\n        new_kwargs = {}\n        for key, value in kwargs.items():\n            if hasattr(self, key):\n                setattr(self, key, value)\n            else:\n                new_kwargs[key] = value\n        return new_kwargs",
  "def set_id(self):\n        Model.id += 1\n        self.id = Model.id\n        for layer in self._layers:\n            layer.set_id()",
  "def begin_training(self, train_X, train_y=None, **trainer_cfg):\n        for hook in self.on_data_hooks:\n            hook(self, train_X, train_y)\n        return self.Trainer(self, **trainer_cfg)",
  "def begin_update(self, X, drop=0.0):\n        raise NotImplementedError",
  "def predict(self, X):\n        y, _ = self.begin_update(X, drop=None)\n        return y",
  "def predict_one(self, x):\n        X = self.ops.expand_dims(x, axis=0)\n        return self.predict(X)[0]",
  "def use_params(self, params):  # pragma: no cover\n        backup = None\n        weights = self._mem.weights\n        if self.id in params:\n            param = params[self.id]\n            backup = weights.copy()\n            copy_array(weights, param)\n        if hasattr(self, \"_layers\"):\n            contexts = [layer.use_params(params) for layer in self._layers]\n            for context in contexts:\n                next(context.gen)\n        yield\n        if backup is not None:\n            copy_array(self._mem.weights, backup)\n        for i, context in enumerate(contexts):\n            # This is ridiculous, but apparently it's what you\n            # have to do to make this work across Python 2/3?\n            try:\n                next(context.gen)\n            except StopIteration:\n                pass",
  "def __call__(self, x):\n        \"\"\"\n        x\n            Must match expected type\n            Must match expected shape\n        \"\"\"\n        return self.predict(x)",
  "def pipe(self, stream, batch_size=128):\n        for batch in util.minibatch(stream, batch_size):\n            ys = self.predict(batch)\n            for y in ys:\n                yield y",
  "def update(self, stream, batch_size=1000):\n        for X, y in util.minibatch(stream, batch_size=batch_size):\n            output, finish_update = self.begin_update(X)\n            gradient = finish_update(y)\n            yield gradient",
  "def to_gpu(self, device_num):\n        import cupy.cuda.device\n\n        device = cupy.cuda.device.Device(device_num)\n        device.use()\n        queue = [self]\n        for layer in queue:\n            layer.ops = CupyOps()\n            layer.Ops = CupyOps\n            if hasattr(layer, \"_mem\"):\n                layer._mem._mem = self.ops.xp.asarray(layer._mem._mem)\n                layer._mem.ops = layer.ops\n            if hasattr(layer, \"_layers\"):\n                queue.extend(layer._layers)\n        return device",
  "def to_cpu(self):\n        queue = [self]\n        for layer in queue:\n            layer.ops = NumpyOps()\n            layer.Ops = NumpyOps\n            if hasattr(layer, \"_mem\"):\n                if hasattr(layer._mem._mem, \"get\"):\n                    layer._mem._mem = layer._mem._mem.get()\n                layer._mem.ops = layer.ops\n            if hasattr(layer, \"_layers\"):\n                queue.extend(layer._layers)",
  "def evaluate(self, X, y):\n        \"\"\"\n        x\n            Must match expected type\n            Must match expected shape\n        y\n            Must match expected type\n        \"\"\"\n        scores = self.ops.flatten(list(self.pipe(X)))\n        if not hasattr(y, \"shape\"):\n            y = self.ops.flatten(y)\n        scores = scores.reshape(y.shape)\n        if len(scores.shape) == 1:\n            correct = ((scores >= 0.5) == (y >= 0.5)).sum()\n        else:\n            correct = (scores.argmax(axis=1) == y.argmax(axis=1)).sum()\n        return correct / y.shape[0]",
  "def evaluate_logloss(self, X, y, minimum=None, maximum=None):\n        yh = self.ops.xp.vstack(self.pipe(X))\n        yh = yh.reshape(y.shape)\n        if minimum is not None:\n            yh = self.ops.xp.maximum(yh, minimum)\n        if maximum is not None:\n            yh = self.ops.xp.minimum(yh, maximum)\n        assert len(yh.shape) == 1\n        losses = -y * self.ops.xp.log(yh + 1e-8) - (1 - y) * self.ops.xp.log(\n            (1 - yh) + 1e-8\n        )\n        return losses.mean()",
  "def __add__(self, other):\n        \"\"\"Apply the function bound to the '+' operator.\"\"\"\n        return self._operators[\"+\"](self, other)",
  "def __sub__(self, other):\n        \"\"\"Apply the function bound to the '-' operator.\"\"\"\n        return self._operators[\"-\"](self, other)",
  "def __mul__(self, other):\n        \"\"\"Apply the function bound to the '*' operator.\"\"\"\n        return self._operators[\"*\"](self, other)",
  "def __matmul__(self, other):\n        \"\"\"Apply the function bound to the '@' operator.\"\"\"\n        return self._operators[\"@\"](self, other)",
  "def __div__(self, other):\n        \"\"\"Apply the function bound to the '/' operator.\"\"\"\n        return self._operators[\"/\"](self, other)",
  "def __truediv__(self, other):  # pragma: no cover\n        \"\"\"Apply the function bound to the '/' operator.\"\"\"\n        return self._operators[\"/\"](self, other)",
  "def __floordiv__(self, other):\n        \"\"\"Apply the function bound to the '//' operator.\"\"\"\n        return self._operators[\"//\"](self, other)",
  "def __mod__(self, other):\n        \"\"\"Apply the function bound to the '%' operator.\"\"\"\n        return self._operators[\"%\"](self, other)",
  "def __pow__(self, other, modulo=None):\n        \"\"\"Apply the function bound to the '**' operator.\"\"\"\n        return self._operators[\"**\"](self, other)",
  "def __lshift__(self, other):\n        \"\"\"Apply the function bound to the '<<' operator.\"\"\"\n        return self._operators[\"<<\"](self, other)",
  "def __rshift__(self, other):\n        \"\"\"Apply the function bound to the '>>' operator.\"\"\"\n        return self._operators[\">>\"](self, other)",
  "def __and__(self, other):\n        \"\"\"Apply the function bound to the '&' operator.\"\"\"\n        return self._operators[\"&\"](self, other)",
  "def __xor__(self, other):\n        \"\"\"Apply the function bound to the '^' operator.\"\"\"\n        return self._operators[\"^\"](self, other)",
  "def __or__(self, other):\n        \"\"\"Apply the function bound to the '|' operator.\"\"\"\n        return self._operators[\"|\"](self, other)",
  "def to_bytes(self):\n        weights = []\n        queue = [self]\n        i = 0\n        for layer in queue:\n            # Hack to support saving/loading PyTorch models. TODO: Improve\n            if hasattr(layer, \"_model\") and not isinstance(layer._model, Model):\n                weights.append(layer.to_bytes())\n            elif hasattr(layer, \"_mem\"):\n                weights.append(\n                    OrderedDict(\n                        (\n                            (b\"dims\", OrderedDict(sorted(layer._dims.items()))),\n                            (b\"params\", []),\n                        )\n                    )\n                )\n                if hasattr(layer, \"seed\"):\n                    weights[-1][b\"seed\"] = layer.seed\n\n                offsets = sorted(layer._mem._offsets.items())\n                for (id_, name), (start, row, shape) in offsets:\n                    if row == 1:\n                        continue\n                    param = layer._mem.get((id_, name))\n                    if not isinstance(layer._mem.weights, numpy.ndarray):\n                        param = param.get()\n                    weights[-1][b\"params\"].append(\n                        OrderedDict(\n                            (\n                                (b\"name\", name),\n                                (b\"offset\", start),\n                                (b\"shape\", shape),\n                                (b\"value\", param),\n                            )\n                        )\n                    )\n                i += 1\n            if hasattr(layer, \"_layers\"):\n                queue.extend(layer._layers)\n        return srsly.msgpack_dumps({b\"weights\": weights})",
  "def from_bytes(self, bytes_data):\n        data = srsly.msgpack_loads(bytes_data)\n        weights = data[b\"weights\"]\n        queue = [self]\n        i = 0\n        for layer in queue:\n            # Hack to support saving/loading PyTorch models. TODO: Improve\n            if hasattr(layer, \"_model\") and not isinstance(layer._model, Model):\n                layer.from_bytes(weights[i])\n                i += 1\n            elif hasattr(layer, \"_mem\"):\n                if b\"seed\" in weights[i]:\n                    layer.seed = weights[i][b\"seed\"]\n                for dim, value in weights[i][b\"dims\"].items():\n                    if isinstance(dim, bytes):\n                        dim = dim.decode(\"utf8\")\n                    setattr(layer, dim, value)\n                for param in weights[i][b\"params\"]:\n                    name = param[b\"name\"]\n                    if isinstance(name, bytes):\n                        name = name.decode(\"utf8\")\n                    dest = getattr(layer, name)\n                    copy_array(dest, param[b\"value\"])\n                i += 1\n            if hasattr(layer, \"_layers\"):\n                queue.extend(layer._layers)\n        return self",
  "def to_disk(self, path):\n        path = util.ensure_path(path)\n        with path.open(\"wb\") as file_:\n            file_.write(self.to_bytes())",
  "def from_disk(self, path):\n        path = util.ensure_path(path)\n        with path.open(\"rb\") as file_:\n            bytes_data = file_.read()\n        return self.from_bytes(bytes_data)",
  "class FunctionLayer(Model):\n    \"\"\"Wrap functions into weightless Model instances, for use as network\n    components.\"\"\"\n\n    def __init__(\n        self,\n        begin_update,\n        predict=None,\n        predict_one=None,\n        nI=None,\n        nO=None,\n        *args,\n        **kwargs\n    ):\n        self.begin_update = begin_update\n        if predict is not None:\n            self.predict = predict\n        if predict_one is not None:\n            self.predict_one = predict_one\n        self.nI = nI\n        self.nO = nO\n        Model.__init__(self)",
  "def __init__(\n        self,\n        begin_update,\n        predict=None,\n        predict_one=None,\n        nI=None,\n        nO=None,\n        *args,\n        **kwargs\n    ):\n        self.begin_update = begin_update\n        if predict is not None:\n            self.predict = predict\n        if predict_one is not None:\n            self.predict_one = predict_one\n        self.nI = nI\n        self.nO = nO\n        Model.__init__(self)",
  "class EncoderDecoder(Model):\n    def __init__(self, nS=1, nH=6, nM=300, nTGT=10000, device=\"cpu\"):\n        \"\"\"\n        EncoderDecoder consists of an encoder stack, a decoder stack and an\n        output layer which is a linear + softmax.\n        Parameters explanation:\n            nS: the number of encoders/decoders in the stack\n            nH: the number of heads in the multiheaded attention\n            nM: the token's embedding size\n            nTGT: the number of unique words in output vocabulary\n        \"\"\"\n        Model.__init__(self)\n        self.nS = nS\n        self.nH = nH\n        self.nM = nM\n        self.nTGT = nTGT\n        self.device = device\n        self.enc = Encoder(nM=nM, nH=nH, device=device, nS=nS)\n        self.norm = PyTorchWrapper(PytorchLayerNorm(nM=nM, device=device))\n        self.dec = clone(DecoderLayer(nM=nM, nH=nH, device=device), nS)\n        self.proj = with_reshape(Softmax(nO=nTGT, nI=nM))\n        self._layers = [self.enc, self.dec, self.proj]\n\n    def begin_update(self, inputs, drop=0.1):\n        \"\"\"\n        A batch object flows through the network. It contains input, output and\n        corresponding masks. Input changes while the object travels through\n        the network. Output is the golden output.\n        Input: nB x nL x nM\n        \"\"\"\n        X0, Xmask, Y0, Ymask = inputs\n        X1, backprop_encode = self.enc.begin_update((X0, Xmask), drop=drop)\n        (Y1, _, _, _), backprop_decode = self.dec.begin_update(\n            (Y0, X1, Xmask, Ymask), drop=drop\n        )\n        Y2, b_Y2 = self.norm.begin_update(Y1)\n        word_probs, backprop_output = self.proj.begin_update(Y2, drop=drop)\n\n        def finish_update(d_word_probs, sgd=None):\n            dY2 = backprop_output(d_word_probs, sgd=sgd)\n            dY1 = b_Y2(dY2, sgd=sgd)\n            zeros = Model.ops.xp.zeros(X0.shape, dtype=Model.ops.xp.float32)\n            dY0, dX1 = backprop_decode((dY1, zeros), sgd=sgd)\n            dX0 = backprop_encode(dX1, sgd=sgd)\n            return (dX0, dY0)\n\n        return (word_probs, Xmask), finish_update",
  "class PytorchLayerNorm(PyTorchModule):\n    def __init__(self, nM=300, eps=1e-6, device=\"cpu\"):\n        super(PytorchLayerNorm, self).__init__()\n        self.a_2 = nn.Parameter(torch.ones(nM).to(device))\n        self.b_2 = nn.Parameter(torch.zeros(nM).to(device))\n        self.eps = eps\n        self.device = device\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True).to(self.device)\n        std = x.std(-1, keepdim=True).to(self.device)\n        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2",
  "class Encoder(Model):\n    def __init__(self, nM=300, nH=6, nS=6, device=\"cpu\"):\n        Model.__init__(self)\n        self.stack = clone(EncoderLayer(nM=nM, nH=nH, device=device), nS)\n        self.norm = PyTorchWrapper(PytorchLayerNorm(nM=nM, device=device))\n\n    def begin_update(self, input, drop=0.1):\n        X0, mask = input\n        (X1, _), b_X1 = self.stack.begin_update((X0, mask), drop=0.1)\n        X2, b_X2 = self.norm.begin_update(X1)\n\n        def finish_update(dX2, sgd=None):\n            dX1 = b_X2(dX2, sgd=sgd)\n            dX0 = b_X1(dX1, sgd=sgd)\n            return dX0\n\n        return X2, finish_update",
  "class EncoderLayer(Model):\n    def __init__(self, nM=300, nH=6, device=\"cpu\"):\n        Model.__init__(self)\n        self.attn = MultiHeadedAttention(nM=nM, nH=nH)\n        self.ffd = PositionwiseFeedForward(nM, 4 * nM)\n        self.norm = PyTorchWrapper(PytorchLayerNorm(nM, device=device))\n        self.nM = nM\n        self.layers_ = [self.attn, self.ffd, self.norm]\n\n    def begin_update(self, input, drop=0.1):\n        X0, mask = input\n        X1, b_X1 = self.attn.begin_update((X0, mask, None), drop=drop)\n        X2, b_X2 = self.norm.begin_update(X1)\n        X3 = X0 + X2\n\n        X4, b_X4 = self.ffd.begin_update(X3, drop=drop)\n        X5, b_X5 = self.norm.begin_update(X4)\n        X6 = X3 + X5\n\n        def finish_update(dX6, sgd=None):\n            dX5 = dX6\n            dX4 = b_X5(dX5, sgd=sgd)\n            dX3 = b_X4(dX4, sgd=sgd)\n            dX3 += dX6\n\n            dX2 = dX3\n            dX1 = b_X2(dX2, sgd=sgd)\n            dX0 = b_X1(dX1, sgd=sgd)\n\n            dX0 += dX3\n            return X0\n\n        return (X6, mask), finish_update",
  "class DecoderLayer(Model):\n    def __init__(self, nM=300, nH=6, device=\"cpu\"):\n        Model.__init__(self)\n        self.y_attn = MultiHeadedAttention(nM=nM, nH=nH)\n        self.x_attn = MultiHeadedAttention(nM=nM, nH=nH)\n        self.norm = PyTorchWrapper(PytorchLayerNorm(nM, device=device))\n        self.ffd = PositionwiseFeedForward(nM, 4 * nM)\n        self.layers_ = [self.norm, self.y_attn, self.x_attn, self.ffd]\n\n    def begin_update(self, input, drop=0.1):\n        Y0, X0, X_mask, Y_mask = input\n        Y1, b_Y1 = self.y_attn.begin_update((Y0, Y_mask, None), drop=drop)\n        Y2, b_Y2 = self.norm.begin_update(Y1)\n        Y3 = Y0 + Y2\n        Y4, b_Y4 = self.x_attn.begin_update((Y3, X0, X_mask, None, None), drop=drop)\n        Y5, b_Y5 = self.norm.begin_update(Y4)\n        Y6 = Y3 + Y5\n        Y7, b_Y7 = self.ffd.begin_update(Y6, drop=drop)\n\n        def finish_update(dI, sgd=None):\n            dY7, dX = dI\n            dY6 = b_Y7(dY7, sgd=sgd)\n            dY5 = dY6\n            dY4 = b_Y5(dY5, sgd=sgd)\n            dY3, dX0 = b_Y4(dY4, sgd=sgd)\n            dY3 += dY6\n            dY2 = dY3\n            dY1 = b_Y2(dY2, sgd=sgd)\n            dY0 = b_Y1(dY1, sgd=sgd)\n            dY0 += dY3\n            dX0 += dX\n            return (dY0, dX0)\n\n        return (Y7, X0, X_mask, Y_mask), finish_update",
  "def __init__(self, nS=1, nH=6, nM=300, nTGT=10000, device=\"cpu\"):\n        \"\"\"\n        EncoderDecoder consists of an encoder stack, a decoder stack and an\n        output layer which is a linear + softmax.\n        Parameters explanation:\n            nS: the number of encoders/decoders in the stack\n            nH: the number of heads in the multiheaded attention\n            nM: the token's embedding size\n            nTGT: the number of unique words in output vocabulary\n        \"\"\"\n        Model.__init__(self)\n        self.nS = nS\n        self.nH = nH\n        self.nM = nM\n        self.nTGT = nTGT\n        self.device = device\n        self.enc = Encoder(nM=nM, nH=nH, device=device, nS=nS)\n        self.norm = PyTorchWrapper(PytorchLayerNorm(nM=nM, device=device))\n        self.dec = clone(DecoderLayer(nM=nM, nH=nH, device=device), nS)\n        self.proj = with_reshape(Softmax(nO=nTGT, nI=nM))\n        self._layers = [self.enc, self.dec, self.proj]",
  "def begin_update(self, inputs, drop=0.1):\n        \"\"\"\n        A batch object flows through the network. It contains input, output and\n        corresponding masks. Input changes while the object travels through\n        the network. Output is the golden output.\n        Input: nB x nL x nM\n        \"\"\"\n        X0, Xmask, Y0, Ymask = inputs\n        X1, backprop_encode = self.enc.begin_update((X0, Xmask), drop=drop)\n        (Y1, _, _, _), backprop_decode = self.dec.begin_update(\n            (Y0, X1, Xmask, Ymask), drop=drop\n        )\n        Y2, b_Y2 = self.norm.begin_update(Y1)\n        word_probs, backprop_output = self.proj.begin_update(Y2, drop=drop)\n\n        def finish_update(d_word_probs, sgd=None):\n            dY2 = backprop_output(d_word_probs, sgd=sgd)\n            dY1 = b_Y2(dY2, sgd=sgd)\n            zeros = Model.ops.xp.zeros(X0.shape, dtype=Model.ops.xp.float32)\n            dY0, dX1 = backprop_decode((dY1, zeros), sgd=sgd)\n            dX0 = backprop_encode(dX1, sgd=sgd)\n            return (dX0, dY0)\n\n        return (word_probs, Xmask), finish_update",
  "def __init__(self, nM=300, eps=1e-6, device=\"cpu\"):\n        super(PytorchLayerNorm, self).__init__()\n        self.a_2 = nn.Parameter(torch.ones(nM).to(device))\n        self.b_2 = nn.Parameter(torch.zeros(nM).to(device))\n        self.eps = eps\n        self.device = device",
  "def forward(self, x):\n        mean = x.mean(-1, keepdim=True).to(self.device)\n        std = x.std(-1, keepdim=True).to(self.device)\n        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2",
  "def __init__(self, nM=300, nH=6, nS=6, device=\"cpu\"):\n        Model.__init__(self)\n        self.stack = clone(EncoderLayer(nM=nM, nH=nH, device=device), nS)\n        self.norm = PyTorchWrapper(PytorchLayerNorm(nM=nM, device=device))",
  "def begin_update(self, input, drop=0.1):\n        X0, mask = input\n        (X1, _), b_X1 = self.stack.begin_update((X0, mask), drop=0.1)\n        X2, b_X2 = self.norm.begin_update(X1)\n\n        def finish_update(dX2, sgd=None):\n            dX1 = b_X2(dX2, sgd=sgd)\n            dX0 = b_X1(dX1, sgd=sgd)\n            return dX0\n\n        return X2, finish_update",
  "def __init__(self, nM=300, nH=6, device=\"cpu\"):\n        Model.__init__(self)\n        self.attn = MultiHeadedAttention(nM=nM, nH=nH)\n        self.ffd = PositionwiseFeedForward(nM, 4 * nM)\n        self.norm = PyTorchWrapper(PytorchLayerNorm(nM, device=device))\n        self.nM = nM\n        self.layers_ = [self.attn, self.ffd, self.norm]",
  "def begin_update(self, input, drop=0.1):\n        X0, mask = input\n        X1, b_X1 = self.attn.begin_update((X0, mask, None), drop=drop)\n        X2, b_X2 = self.norm.begin_update(X1)\n        X3 = X0 + X2\n\n        X4, b_X4 = self.ffd.begin_update(X3, drop=drop)\n        X5, b_X5 = self.norm.begin_update(X4)\n        X6 = X3 + X5\n\n        def finish_update(dX6, sgd=None):\n            dX5 = dX6\n            dX4 = b_X5(dX5, sgd=sgd)\n            dX3 = b_X4(dX4, sgd=sgd)\n            dX3 += dX6\n\n            dX2 = dX3\n            dX1 = b_X2(dX2, sgd=sgd)\n            dX0 = b_X1(dX1, sgd=sgd)\n\n            dX0 += dX3\n            return X0\n\n        return (X6, mask), finish_update",
  "def __init__(self, nM=300, nH=6, device=\"cpu\"):\n        Model.__init__(self)\n        self.y_attn = MultiHeadedAttention(nM=nM, nH=nH)\n        self.x_attn = MultiHeadedAttention(nM=nM, nH=nH)\n        self.norm = PyTorchWrapper(PytorchLayerNorm(nM, device=device))\n        self.ffd = PositionwiseFeedForward(nM, 4 * nM)\n        self.layers_ = [self.norm, self.y_attn, self.x_attn, self.ffd]",
  "def begin_update(self, input, drop=0.1):\n        Y0, X0, X_mask, Y_mask = input\n        Y1, b_Y1 = self.y_attn.begin_update((Y0, Y_mask, None), drop=drop)\n        Y2, b_Y2 = self.norm.begin_update(Y1)\n        Y3 = Y0 + Y2\n        Y4, b_Y4 = self.x_attn.begin_update((Y3, X0, X_mask, None, None), drop=drop)\n        Y5, b_Y5 = self.norm.begin_update(Y4)\n        Y6 = Y3 + Y5\n        Y7, b_Y7 = self.ffd.begin_update(Y6, drop=drop)\n\n        def finish_update(dI, sgd=None):\n            dY7, dX = dI\n            dY6 = b_Y7(dY7, sgd=sgd)\n            dY5 = dY6\n            dY4 = b_Y5(dY5, sgd=sgd)\n            dY3, dX0 = b_Y4(dY4, sgd=sgd)\n            dY3 += dY6\n            dY2 = dY3\n            dY1 = b_Y2(dY2, sgd=sgd)\n            dY0 = b_Y1(dY1, sgd=sgd)\n            dY0 += dY3\n            dX0 += dX\n            return (dY0, dX0)\n\n        return (Y7, X0, X_mask, Y_mask), finish_update",
  "def finish_update(d_word_probs, sgd=None):\n            dY2 = backprop_output(d_word_probs, sgd=sgd)\n            dY1 = b_Y2(dY2, sgd=sgd)\n            zeros = Model.ops.xp.zeros(X0.shape, dtype=Model.ops.xp.float32)\n            dY0, dX1 = backprop_decode((dY1, zeros), sgd=sgd)\n            dX0 = backprop_encode(dX1, sgd=sgd)\n            return (dX0, dY0)",
  "def finish_update(dX2, sgd=None):\n            dX1 = b_X2(dX2, sgd=sgd)\n            dX0 = b_X1(dX1, sgd=sgd)\n            return dX0",
  "def finish_update(dX6, sgd=None):\n            dX5 = dX6\n            dX4 = b_X5(dX5, sgd=sgd)\n            dX3 = b_X4(dX4, sgd=sgd)\n            dX3 += dX6\n\n            dX2 = dX3\n            dX1 = b_X2(dX2, sgd=sgd)\n            dX0 = b_X1(dX1, sgd=sgd)\n\n            dX0 += dX3\n            return X0",
  "def finish_update(dI, sgd=None):\n            dY7, dX = dI\n            dY6 = b_Y7(dY7, sgd=sgd)\n            dY5 = dY6\n            dY4 = b_Y5(dY5, sgd=sgd)\n            dY3, dX0 = b_Y4(dY4, sgd=sgd)\n            dY3 += dY6\n            dY2 = dY3\n            dY1 = b_Y2(dY2, sgd=sgd)\n            dY0 = b_Y1(dY1, sgd=sgd)\n            dY0 += dY3\n            dX0 += dX\n            return (dY0, dX0)",
  "def inverse(total):\n    inverse = 1.0 / (1 + total)\n\n    def backward(d_inverse):\n        result = d_inverse * (-1 / (total + 1) ** 2)\n        return result\n\n    return inverse, backward",
  "def _get_mask(ops, shape, drop):\n    return ops.xp.random.uniform(0.0, 1.0, shape) > drop",
  "def Siamese(layer, similarity):\n    def begin_update(inputs, drop=0.0):\n        ops = layer.ops\n        if drop not in (None, 0.0):\n            dropped = []\n            for in1, in2 in inputs:\n                if in1.size > in2.size:\n                    mask = _get_mask(ops, in1.shape, drop)\n                else:\n                    mask = _get_mask(ops, in2.shape, drop)\n                in1 = in1 * mask[: in1.shape[0]]\n                in2 = in2 * mask[: in2.shape[0]]\n                dropped.append((in1, in2))\n            inputs = dropped\n\n        input1, input2 = zip(*inputs)\n        vec1, bp_vec1 = layer.begin_update(input1, drop=0.0)\n        vec2, bp_vec2 = layer.begin_update(input2, drop=0.0)\n        output, bp_output = similarity.begin_update((vec1, vec2), drop=0.0)\n\n        def finish_update(d_output, sgd=None):\n            d_vec1, d_vec2 = bp_output(d_output, sgd)\n            # Remember that this is the same layer --\n            # Is this bad? Are we making bp_vec2 stale?\n            d_input1 = bp_vec1(d_vec1, lambda *args, **kwargs: None)\n            d_input2 = bp_vec2(d_vec2, sgd)\n            return (d_input1, d_input2)\n\n        return output, finish_update\n\n    model = layerize(begin_update)\n\n    model._layers.append(layer)\n    model._layers.append(similarity)\n\n    def on_data(self, X, y):\n        input1, input2 = zip(*X)\n        for hook in layer.on_data_hooks:\n            hook(layer, input1, y)\n\n    model.on_data_hooks.append(on_data)\n    return model",
  "def unit_init(W, ops):\n    W.fill(1)",
  "class CauchySimilarity(Model):\n    # From chen (2013)\n    def __init__(self, length):\n        Model.__init__(self)\n        self.nO = length\n\n    def begin_update(self, vec1_vec2, drop=0.0):\n        weights = self.W\n        vec1, vec2 = vec1_vec2\n        diff = vec1 - vec2\n        square_diff = diff ** 2\n        total = (weights * square_diff).sum(axis=1)\n        sim, bp_sim = inverse(total)\n        total = total.reshape((vec1.shape[0], 1))\n\n        def finish_update(d_sim, sgd=None):\n            d_total = bp_sim(d_sim)\n            d_total = d_total.reshape(total.shape)\n            self.d_W += (d_total * square_diff).sum(axis=0)\n            d_square_diff = weights * d_total\n            d_diff = 2 * d_square_diff * diff\n            d_vec1 = d_diff\n            d_vec2 = -d_diff\n            sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return (d_vec1, d_vec2)\n\n        return sim, finish_update",
  "def backward(d_inverse):\n        result = d_inverse * (-1 / (total + 1) ** 2)\n        return result",
  "def begin_update(inputs, drop=0.0):\n        ops = layer.ops\n        if drop not in (None, 0.0):\n            dropped = []\n            for in1, in2 in inputs:\n                if in1.size > in2.size:\n                    mask = _get_mask(ops, in1.shape, drop)\n                else:\n                    mask = _get_mask(ops, in2.shape, drop)\n                in1 = in1 * mask[: in1.shape[0]]\n                in2 = in2 * mask[: in2.shape[0]]\n                dropped.append((in1, in2))\n            inputs = dropped\n\n        input1, input2 = zip(*inputs)\n        vec1, bp_vec1 = layer.begin_update(input1, drop=0.0)\n        vec2, bp_vec2 = layer.begin_update(input2, drop=0.0)\n        output, bp_output = similarity.begin_update((vec1, vec2), drop=0.0)\n\n        def finish_update(d_output, sgd=None):\n            d_vec1, d_vec2 = bp_output(d_output, sgd)\n            # Remember that this is the same layer --\n            # Is this bad? Are we making bp_vec2 stale?\n            d_input1 = bp_vec1(d_vec1, lambda *args, **kwargs: None)\n            d_input2 = bp_vec2(d_vec2, sgd)\n            return (d_input1, d_input2)\n\n        return output, finish_update",
  "def on_data(self, X, y):\n        input1, input2 = zip(*X)\n        for hook in layer.on_data_hooks:\n            hook(layer, input1, y)",
  "def __init__(self, length):\n        Model.__init__(self)\n        self.nO = length",
  "def begin_update(self, vec1_vec2, drop=0.0):\n        weights = self.W\n        vec1, vec2 = vec1_vec2\n        diff = vec1 - vec2\n        square_diff = diff ** 2\n        total = (weights * square_diff).sum(axis=1)\n        sim, bp_sim = inverse(total)\n        total = total.reshape((vec1.shape[0], 1))\n\n        def finish_update(d_sim, sgd=None):\n            d_total = bp_sim(d_sim)\n            d_total = d_total.reshape(total.shape)\n            self.d_W += (d_total * square_diff).sum(axis=0)\n            d_square_diff = weights * d_total\n            d_diff = 2 * d_square_diff * diff\n            d_vec1 = d_diff\n            d_vec2 = -d_diff\n            sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return (d_vec1, d_vec2)\n\n        return sim, finish_update",
  "def finish_update(d_output, sgd=None):\n            d_vec1, d_vec2 = bp_output(d_output, sgd)\n            # Remember that this is the same layer --\n            # Is this bad? Are we making bp_vec2 stale?\n            d_input1 = bp_vec1(d_vec1, lambda *args, **kwargs: None)\n            d_input2 = bp_vec2(d_vec2, sgd)\n            return (d_input1, d_input2)",
  "def finish_update(d_sim, sgd=None):\n            d_total = bp_sim(d_sim)\n            d_total = d_total.reshape(total.shape)\n            self.d_W += (d_total * square_diff).sum(axis=0)\n            d_square_diff = weights * d_total\n            d_diff = 2 * d_square_diff * diff\n            d_vec1 = d_diff\n            d_vec2 = -d_diff\n            sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return (d_vec1, d_vec2)",
  "def _set_dimensions_if_needed(model, X, y=None):\n    if model.nI is None:\n        model.nI = X.shape[1]\n    if model.nO is None and y is not None:  # pragma: no cover\n        model.nO = int(y.max()) + 1",
  "def xavier_uniform_init(W, ops):\n    if (W ** 2).sum() != 0:\n        return\n    xp = get_array_module(W)\n    scale = xp.sqrt(6.0 / (W.shape[0] + W.shape[2]))\n    shape = (W.shape[0], W.shape[2])\n    for i in range(W.shape[1]):\n        xp.copyto(W[:, i], xp.random.uniform(-scale, scale, shape))",
  "def normal_init(W, ops):\n    if (W ** 2).sum() != 0:\n        return\n    xp = get_array_module(W)\n    scale = xp.sqrt(1.0 / W.shape[-1])\n    shape = (W.shape[0], W.shape[-1])\n    size = xp.prod(shape)\n    for i in range(W.shape[1]):\n        xp.copyto(\n            W[:, i], xp.random.normal(loc=0, scale=scale, size=size).reshape(shape)\n        )",
  "class Maxout(Model):\n    name = \"maxout\"\n\n    def __init__(self, nO=None, nI=None, pieces=2, **kwargs):\n        Model.__init__(self, **kwargs)\n        self.nO = nO\n        self.nI = nI\n        self.nP = pieces\n        self.drop_factor = kwargs.get(\"drop_factor\", 1.0)\n\n    def predict(self, X__BI):\n        W = self.W.reshape((self.nO * self.nP, self.nI))\n        X__BOP = self.ops.gemm(X__BI, W, trans2=True)\n        X__BOP += self.b.reshape((self.nO * self.nP,))\n        X__BOP = X__BOP.reshape((X__BOP.shape[0], self.nO, self.nP))\n        best__BO, _ = self.ops.maxout(X__BOP)\n        return best__BO\n\n    def begin_update(self, X__bi, drop=0.0):\n        W = self.W.reshape((self.nO * self.nP, self.nI))\n        if drop is not None:\n            drop *= self.drop_factor\n        output__boc = self.ops.gemm(X__bi, W, trans2=True)\n        output__boc += self.b.reshape((self.nO * self.nP,))\n        output__boc = output__boc.reshape((output__boc.shape[0], self.nO, self.nP))\n        best__bo, which__bo = self.ops.maxout(output__boc)\n        best__bo, bp_dropout = self.ops.dropout(best__bo, drop)\n\n        def finish_update(dX__bo, sgd=None):\n            dX__bop = self.ops.backprop_maxout(dX__bo, which__bo, self.nP)\n            self.d_b += dX__bop.sum(axis=0)\n            dX__bop = dX__bop.reshape((dX__bop.shape[0], self.nO * self.nP))\n            d_W = self.ops.gemm(dX__bop, X__bi, trans1=True)\n            self.d_W += d_W.reshape((self.nO, self.nP, self.nI))\n            # Bop,opi->Bi\n            dX__bi = self.ops.gemm(\n                dX__bop, self.W.reshape((self.nO * self.nP, self.nI))\n            )\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return dX__bi\n\n        return best__bo, bp_dropout(finish_update)",
  "def __init__(self, nO=None, nI=None, pieces=2, **kwargs):\n        Model.__init__(self, **kwargs)\n        self.nO = nO\n        self.nI = nI\n        self.nP = pieces\n        self.drop_factor = kwargs.get(\"drop_factor\", 1.0)",
  "def predict(self, X__BI):\n        W = self.W.reshape((self.nO * self.nP, self.nI))\n        X__BOP = self.ops.gemm(X__BI, W, trans2=True)\n        X__BOP += self.b.reshape((self.nO * self.nP,))\n        X__BOP = X__BOP.reshape((X__BOP.shape[0], self.nO, self.nP))\n        best__BO, _ = self.ops.maxout(X__BOP)\n        return best__BO",
  "def begin_update(self, X__bi, drop=0.0):\n        W = self.W.reshape((self.nO * self.nP, self.nI))\n        if drop is not None:\n            drop *= self.drop_factor\n        output__boc = self.ops.gemm(X__bi, W, trans2=True)\n        output__boc += self.b.reshape((self.nO * self.nP,))\n        output__boc = output__boc.reshape((output__boc.shape[0], self.nO, self.nP))\n        best__bo, which__bo = self.ops.maxout(output__boc)\n        best__bo, bp_dropout = self.ops.dropout(best__bo, drop)\n\n        def finish_update(dX__bo, sgd=None):\n            dX__bop = self.ops.backprop_maxout(dX__bo, which__bo, self.nP)\n            self.d_b += dX__bop.sum(axis=0)\n            dX__bop = dX__bop.reshape((dX__bop.shape[0], self.nO * self.nP))\n            d_W = self.ops.gemm(dX__bop, X__bi, trans1=True)\n            self.d_W += d_W.reshape((self.nO, self.nP, self.nI))\n            # Bop,opi->Bi\n            dX__bi = self.ops.gemm(\n                dX__bop, self.W.reshape((self.nO * self.nP, self.nI))\n            )\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return dX__bi\n\n        return best__bo, bp_dropout(finish_update)",
  "def finish_update(dX__bo, sgd=None):\n            dX__bop = self.ops.backprop_maxout(dX__bo, which__bo, self.nP)\n            self.d_b += dX__bop.sum(axis=0)\n            dX__bop = dX__bop.reshape((dX__bop.shape[0], self.nO * self.nP))\n            d_W = self.ops.gemm(dX__bop, X__bi, trans1=True)\n            self.d_W += d_W.reshape((self.nO, self.nP, self.nI))\n            # Bop,opi->Bi\n            dX__bi = self.ops.gemm(\n                dX__bop, self.W.reshape((self.nO * self.nP, self.nI))\n            )\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return dX__bi",
  "class Softmax(Affine):\n    name = \"softmax\"\n\n    @check.arg(1, has_shape((\"nB\", \"nI\")))\n    def predict(self, input__BI):\n        output__BO = self.ops.affine(self.W, self.b, input__BI)\n        self.ops.softmax(output__BO, inplace=True)\n        return output__BO\n\n    @check.arg(1, has_shape((\"nB\", \"nI\")))\n    def begin_update(self, input__BI, drop=0.0):\n        output__BO = self.predict(input__BI)\n\n        @check.arg(0, has_shape((\"nB\", \"nO\")))\n        def finish_update(grad__BO, sgd=None):\n            self.ops.gemm(grad__BO, input__BI, trans1=True, out=self.d_W)\n            self.d_b += grad__BO.sum(axis=0)\n            grad__BI = self.ops.gemm(grad__BO, self.W)\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return grad__BI\n\n        return output__BO, finish_update",
  "def predict(self, input__BI):\n        output__BO = self.ops.affine(self.W, self.b, input__BI)\n        self.ops.softmax(output__BO, inplace=True)\n        return output__BO",
  "def begin_update(self, input__BI, drop=0.0):\n        output__BO = self.predict(input__BI)\n\n        @check.arg(0, has_shape((\"nB\", \"nO\")))\n        def finish_update(grad__BO, sgd=None):\n            self.ops.gemm(grad__BO, input__BI, trans1=True, out=self.d_W)\n            self.d_b += grad__BO.sum(axis=0)\n            grad__BI = self.ops.gemm(grad__BO, self.W)\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return grad__BI\n\n        return output__BO, finish_update",
  "def finish_update(grad__BO, sgd=None):\n            self.ops.gemm(grad__BO, input__BI, trans1=True, out=self.d_W)\n            self.d_b += grad__BO.sum(axis=0)\n            grad__BI = self.ops.gemm(grad__BO, self.W)\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return grad__BI",
  "class ELU(Affine):\n    def predict(self, input__bi):\n        output__bo = Affine.predict(self, input__bi)\n        self.ops.elu(output__bo, inplace=True)\n        return output__bo\n\n    def begin_update(self, input__bi, drop=0.0):\n        output__bo, finish_affine = Affine.begin_update(self, input__bi, drop=drop)\n\n        output_copy = self.ops.xp.ascontiguousarray(output__bo, dtype=\"f\")\n        self.ops.elu(output_copy, inplace=True)\n\n        def finish_update(gradient, sgd=None):\n            gradient = self.ops.xp.ascontiguousarray(gradient, dtype=\"f\")\n            self.ops.backprop_elu(gradient, output_copy, inplace=True)\n            return finish_affine(gradient, sgd)\n\n        output__bo[:] = output_copy\n        output__bo, bp_dropout = self.ops.dropout(output__bo, drop, inplace=True)\n        return output__bo, bp_dropout(finish_update)",
  "def predict(self, input__bi):\n        output__bo = Affine.predict(self, input__bi)\n        self.ops.elu(output__bo, inplace=True)\n        return output__bo",
  "def begin_update(self, input__bi, drop=0.0):\n        output__bo, finish_affine = Affine.begin_update(self, input__bi, drop=drop)\n\n        output_copy = self.ops.xp.ascontiguousarray(output__bo, dtype=\"f\")\n        self.ops.elu(output_copy, inplace=True)\n\n        def finish_update(gradient, sgd=None):\n            gradient = self.ops.xp.ascontiguousarray(gradient, dtype=\"f\")\n            self.ops.backprop_elu(gradient, output_copy, inplace=True)\n            return finish_affine(gradient, sgd)\n\n        output__bo[:] = output_copy\n        output__bo, bp_dropout = self.ops.dropout(output__bo, drop, inplace=True)\n        return output__bo, bp_dropout(finish_update)",
  "def finish_update(gradient, sgd=None):\n            gradient = self.ops.xp.ascontiguousarray(gradient, dtype=\"f\")\n            self.ops.backprop_elu(gradient, output_copy, inplace=True)\n            return finish_affine(gradient, sgd)",
  "def _run_child_hooks(model, X, y):\n    for layer in model._layers:\n        for hook in layer.on_data_hooks:\n            hook(layer, X, y)\n        X = layer(X)",
  "class FeedForward(Model):\n    \"\"\"A feed-forward network, that chains multiple Model instances together.\"\"\"\n\n    name = \"feed-forward\"\n\n    def __init__(self, layers, **kwargs):\n        self._layers = []\n        for layer in layers:\n            if isinstance(layer, FeedForward):\n                self._layers.extend(layer._layers)\n            else:\n                self._layers.append(layer)\n        Model.__init__(self, **kwargs)\n\n    @property\n    def input_shape(self):\n        return self._layers[0].input_shape\n\n    @property\n    def output_shape(self):\n        return self._layers[-1].output_shape\n\n    def predict(self, X):\n        for layer in self._layers:\n            X = layer(X)\n        return X\n\n    def begin_update(self, X, drop=0.0):\n        callbacks = []\n        for layer in self._layers:\n            X, inc_layer_grad = layer.begin_update(X, drop=drop)\n            callbacks.append(inc_layer_grad)\n\n        def continue_update(gradient, sgd=None):\n            for callback in reversed(callbacks):\n                if gradient is None or callback is None:\n                    break\n                gradient = callback(gradient, sgd)\n            return gradient\n\n        return X, continue_update",
  "def __init__(self, layers, **kwargs):\n        self._layers = []\n        for layer in layers:\n            if isinstance(layer, FeedForward):\n                self._layers.extend(layer._layers)\n            else:\n                self._layers.append(layer)\n        Model.__init__(self, **kwargs)",
  "def input_shape(self):\n        return self._layers[0].input_shape",
  "def output_shape(self):\n        return self._layers[-1].output_shape",
  "def predict(self, X):\n        for layer in self._layers:\n            X = layer(X)\n        return X",
  "def begin_update(self, X, drop=0.0):\n        callbacks = []\n        for layer in self._layers:\n            X, inc_layer_grad = layer.begin_update(X, drop=drop)\n            callbacks.append(inc_layer_grad)\n\n        def continue_update(gradient, sgd=None):\n            for callback in reversed(callbacks):\n                if gradient is None or callback is None:\n                    break\n                gradient = callback(gradient, sgd)\n            return gradient\n\n        return X, continue_update",
  "def continue_update(gradient, sgd=None):\n            for callback in reversed(callbacks):\n                if gradient is None or callback is None:\n                    break\n                gradient = callback(gradient, sgd)\n            return gradient",
  "class ExtractWindow(Model):\n    \"\"\"Add context to vectors in a sequence by concatenating n surrounding\n    vectors.\n\n    If the input is (10, 32) and n=1, the output will be (10, 96), with\n    output[i] made up of (input[i-1], input[i], input[i+1]).\n    \"\"\"\n\n    name = \"extract_window\"\n\n    def __init__(self, nW=2, gap=0):\n        assert gap == 0\n        Model.__init__(self)\n        self.nW = nW\n        self.gap = gap\n\n    def predict(self, X):\n        return self.ops.seq2col(X, self.nW)\n\n    def begin_update(self, X__bi, drop=0.0):\n        X__bo = self.ops.seq2col(X__bi, self.nW)\n        finish_update = self._get_finish_update()\n        return X__bo, finish_update\n\n    def _get_finish_update(self):\n        def finish_update(gradient, sgd=None):\n            return self.ops.backprop_seq2col(gradient, self.nW)\n\n        return finish_update",
  "def __init__(self, nW=2, gap=0):\n        assert gap == 0\n        Model.__init__(self)\n        self.nW = nW\n        self.gap = gap",
  "def predict(self, X):\n        return self.ops.seq2col(X, self.nW)",
  "def begin_update(self, X__bi, drop=0.0):\n        X__bo = self.ops.seq2col(X__bi, self.nW)\n        finish_update = self._get_finish_update()\n        return X__bo, finish_update",
  "def _get_finish_update(self):\n        def finish_update(gradient, sgd=None):\n            return self.ops.backprop_seq2col(gradient, self.nW)\n\n        return finish_update",
  "def finish_update(gradient, sgd=None):\n            return self.ops.backprop_seq2col(gradient, self.nW)",
  "def _init_to_one(W, ops):\n    W.fill(1.0)",
  "def _run_child_hooks(model, X, y=None):\n    for hook in model.child.on_data_hooks:\n        hook(model.child, X, y)",
  "class BatchNorm(Model):\n    name = \"batchnorm\"\n\n    def __init__(self, child, **kwargs):\n        self.child = child\n        self._layers = [child]\n        if \"nO\" in kwargs:\n            self.nO = kwargs[\"nO\"]\n        elif getattr(child, \"nO\", None):\n            self.nO = child.nO\n        self.nr_upd = 0\n        self.eps = kwargs.get(\"eps\", 1e-5)\n        self.alpha = self.ops.xp.asarray([0.1], dtype=\"float32\")\n        self.rmax = kwargs.get(\"rmax\", 3.0)\n        self.dmax = kwargs.get(\"dmax\", 5.0)\n        Model.__init__(self, **kwargs)\n\n    def predict(self, X):\n        X = self.child.predict(X)\n        Xh = _forward(self.ops, X, self.m, self.v + self.eps)\n        y = Xh * self.G + self.b\n        return y\n\n    def begin_update(self, X, drop=0.0):\n        if drop is None:\n            return self.predict(X), None\n        assert X.dtype == \"float32\"\n        X, backprop_child = self.child.begin_update(X, drop=0.0)\n        N, mu, var = _get_moments(self.ops, X)\n        var += self.eps\n        r = self.ops.xp.clip(var / self.v, 1.0 / self.rmax, self.rmax)\n        d = self.ops.xp.clip((mu - self.m) / self.v, -self.dmax, self.dmax)\n        self.nr_upd += 1\n\n        # I'm not sure this is the best thing to do --\n        # Should we consider a sample be the instance, or the batch?\n        # If we want the variance of the inputs it should be like:\n        \"\"\"\n        diff = X - self.m\n        incr = (1-alpha) * diff\n        self.m += incr.mean(axis=0)\n        self.v += (diff * incr).mean(axis=0)\n        self.v *= alpha\n        \"\"\"\n        self.m += self.alpha * (mu - self.m)\n        self.v += self.alpha * (var - self.v)\n        Xhat = _forward(self.ops, X, mu, var)\n        Xhat *= r\n        Xhat += d\n\n        y, backprop_rescale = self._begin_update_scale_shift(Xhat)\n\n        def finish_update(dy, sgd=None):\n            dy = backprop_rescale(dy, sgd)\n            dist, sum_dy, sum_dy_dist = _get_d_moments(self.ops, dy, X, mu)\n            d_xhat = N * dy - sum_dy - dist * (1.0 / var) * sum_dy_dist\n            d_xhat *= var ** (-1.0 / 2)\n            d_xhat /= N\n            return backprop_child(d_xhat, sgd)\n\n        if drop is not None:\n            drop *= getattr(self.child, \"drop_factor\", 1.0)\n        y, bp_dropout = self.ops.dropout(y, drop)\n        assert y.dtype == \"float32\"\n        return y, bp_dropout(finish_update)\n\n    def _begin_update_scale_shift(self, input__BI):\n        def finish_update(gradient__BI, sgd=None):\n            self.d_b += gradient__BI.sum(axis=0)\n            d_G = self.d_G\n            d_G += (gradient__BI * input__BI).sum(axis=0)\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return gradient__BI * self.G\n\n        return input__BI * self.G + self.b, finish_update",
  "def _get_moments(ops, X):\n    mu = X.mean(axis=0)\n    var = X.var(axis=0) + 1e-08\n    return ops.asarray([X.shape[0]], dtype=\"float32\"), mu, var",
  "def _get_d_moments(ops, dy, X, mu):\n    dist = X - mu\n    return dist, ops.xp.sum(dy, axis=0), ops.xp.sum(dy * dist, axis=0)",
  "def _forward(ops, X, mu, var):\n    return (X - mu) * var ** (-1.0 / 2.0)",
  "def __init__(self, child, **kwargs):\n        self.child = child\n        self._layers = [child]\n        if \"nO\" in kwargs:\n            self.nO = kwargs[\"nO\"]\n        elif getattr(child, \"nO\", None):\n            self.nO = child.nO\n        self.nr_upd = 0\n        self.eps = kwargs.get(\"eps\", 1e-5)\n        self.alpha = self.ops.xp.asarray([0.1], dtype=\"float32\")\n        self.rmax = kwargs.get(\"rmax\", 3.0)\n        self.dmax = kwargs.get(\"dmax\", 5.0)\n        Model.__init__(self, **kwargs)",
  "def predict(self, X):\n        X = self.child.predict(X)\n        Xh = _forward(self.ops, X, self.m, self.v + self.eps)\n        y = Xh * self.G + self.b\n        return y",
  "def begin_update(self, X, drop=0.0):\n        if drop is None:\n            return self.predict(X), None\n        assert X.dtype == \"float32\"\n        X, backprop_child = self.child.begin_update(X, drop=0.0)\n        N, mu, var = _get_moments(self.ops, X)\n        var += self.eps\n        r = self.ops.xp.clip(var / self.v, 1.0 / self.rmax, self.rmax)\n        d = self.ops.xp.clip((mu - self.m) / self.v, -self.dmax, self.dmax)\n        self.nr_upd += 1\n\n        # I'm not sure this is the best thing to do --\n        # Should we consider a sample be the instance, or the batch?\n        # If we want the variance of the inputs it should be like:\n        \"\"\"\n        diff = X - self.m\n        incr = (1-alpha) * diff\n        self.m += incr.mean(axis=0)\n        self.v += (diff * incr).mean(axis=0)\n        self.v *= alpha\n        \"\"\"\n        self.m += self.alpha * (mu - self.m)\n        self.v += self.alpha * (var - self.v)\n        Xhat = _forward(self.ops, X, mu, var)\n        Xhat *= r\n        Xhat += d\n\n        y, backprop_rescale = self._begin_update_scale_shift(Xhat)\n\n        def finish_update(dy, sgd=None):\n            dy = backprop_rescale(dy, sgd)\n            dist, sum_dy, sum_dy_dist = _get_d_moments(self.ops, dy, X, mu)\n            d_xhat = N * dy - sum_dy - dist * (1.0 / var) * sum_dy_dist\n            d_xhat *= var ** (-1.0 / 2)\n            d_xhat /= N\n            return backprop_child(d_xhat, sgd)\n\n        if drop is not None:\n            drop *= getattr(self.child, \"drop_factor\", 1.0)\n        y, bp_dropout = self.ops.dropout(y, drop)\n        assert y.dtype == \"float32\"\n        return y, bp_dropout(finish_update)",
  "def _begin_update_scale_shift(self, input__BI):\n        def finish_update(gradient__BI, sgd=None):\n            self.d_b += gradient__BI.sum(axis=0)\n            d_G = self.d_G\n            d_G += (gradient__BI * input__BI).sum(axis=0)\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return gradient__BI * self.G\n\n        return input__BI * self.G + self.b, finish_update",
  "def finish_update(dy, sgd=None):\n            dy = backprop_rescale(dy, sgd)\n            dist, sum_dy, sum_dy_dist = _get_d_moments(self.ops, dy, X, mu)\n            d_xhat = N * dy - sum_dy - dist * (1.0 / var) * sum_dy_dist\n            d_xhat *= var ** (-1.0 / 2)\n            d_xhat /= N\n            return backprop_child(d_xhat, sgd)",
  "def finish_update(gradient__BI, sgd=None):\n            self.d_b += gradient__BI.sum(axis=0)\n            d_G = self.d_G\n            d_G += (gradient__BI * input__BI).sum(axis=0)\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return gradient__BI * self.G",
  "def BiLSTM(nO, nI):\n    \"\"\"Create a bidirectional LSTM layer. Args: number out, number in\"\"\"\n    return Bidirectional(LSTM(nO // 2, nI), LSTM(nO // 2, nI))",
  "def LSTM(nO, nI):\n    \"\"\"Create an LSTM layer. Args: number out, number in\"\"\"\n    weights = LSTM_weights(nO, nI)\n    gates = LSTM_gates(weights.ops)\n    return Recurrent(RNN_step(weights, gates))",
  "def Bidirectional(l2r, r2l):\n    \"\"\"Stitch two RNN models into a bidirectional layer.\"\"\"\n    nO = l2r.nO\n\n    def birnn_fwd(Xs, drop=0.0):\n        l2r_Zs, bp_l2r_Zs = l2r.begin_update(Xs, drop=drop)\n        r2l_Zs, bp_r2l_Zs = r2l.begin_update(\n            [l2r.ops.xp.ascontiguousarray(X[::-1]) for X in Xs]\n        )\n\n        def birnn_bwd(dZs, sgd=None):\n            d_l2r_Zs = []\n            d_r2l_Zs = []\n            for dZ in dZs:\n                l2r_fwd = dZ[:, :nO]\n                r2l_fwd = dZ[:, nO:]\n                d_l2r_Zs.append(l2r.ops.xp.ascontiguousarray(l2r_fwd))\n                d_r2l_Zs.append(l2r.ops.xp.ascontiguousarray(r2l_fwd[::-1]))\n            dXs_l2r = bp_l2r_Zs(d_l2r_Zs, sgd=sgd)\n            dXs_r2l = bp_r2l_Zs(d_r2l_Zs, sgd=sgd)\n            dXs = [dXf + dXb[::-1] for dXf, dXb in zip(dXs_l2r, dXs_r2l)]\n            return dXs\n\n        Zs = [l2r.ops.xp.hstack((Zf, Zb[::-1])) for Zf, Zb in zip(l2r_Zs, r2l_Zs)]\n        return Zs, birnn_bwd\n\n    return wrap(birnn_fwd, l2r, r2l)",
  "def Recurrent(step_model):\n    \"\"\"Apply a stepwise model over a sequence, maintaining state. For RNNs\"\"\"\n    ops = step_model.ops\n\n    def recurrent_fwd(seqs, drop=0.0):\n        lengths = [len(X) for X in seqs]\n        X, size_at_t, unpad = ops.square_sequences(seqs)\n        Y = ops.allocate((X.shape[0], X.shape[1], step_model.nO))\n        cell_drop = ops.get_dropout_mask((len(seqs), step_model.nO), 0.0)\n        hidden_drop = ops.get_dropout_mask((len(seqs), step_model.nO), 0.0)\n        out_drop = ops.get_dropout_mask((len(seqs), step_model.nO), 0.0)\n        backprops = [None] * max(lengths)\n        state = step_model.weights.get_initial_state(len(seqs))\n        for t in range(max(lengths)):\n            state = list(state)\n            size = size_at_t[t]\n            Xt = X[t, :size]\n            state[0] = state[0][:size]\n            state[1] = state[1][:size]\n            if cell_drop is not None:\n                state[0] *= cell_drop\n            if hidden_drop is not None:\n                state[1] *= hidden_drop\n            inputs = (state, Xt)\n            (state, Y[t, :size]), backprops[t] = step_model.begin_update(inputs)\n            if out_drop is not None:\n                Y[t, :size] *= out_drop\n        outputs = unpad(Y)\n\n        def recurrent_bwd(d_outputs, sgd=None):\n            dY, size_at_t, unpad = step_model.ops.square_sequences(d_outputs)\n            d_state = [\n                step_model.ops.allocate((dY.shape[1], step_model.nO)),\n                step_model.ops.allocate((dY.shape[1], step_model.nO)),\n            ]\n            updates = {}\n\n            def gather_updates(weights, gradient, key=None):\n                updates[key] = (weights, gradient)\n\n            dX = step_model.ops.allocate(\n                (dY.shape[0], dY.shape[1], step_model.weights.nI)\n            )\n            for t in range(max(lengths) - 1, -1, -1):\n                if out_drop is not None:\n                    dY[t] *= out_drop\n                d_state_t, dXt = backprops[t]((d_state, dY[t]), sgd=gather_updates)\n                d_state[0][: d_state_t[0].shape[0]] = d_state_t[0]\n                d_state[1][: d_state_t[1].shape[0]] = d_state_t[1]\n                dX[t, : dXt.shape[0]] = dXt\n                if cell_drop is not None:\n                    d_state[0] *= cell_drop\n                if hidden_drop is not None:\n                    d_state[1] *= hidden_drop\n            d_cell, d_hidden = d_state\n            step_model.weights.d_initial_cells += d_cell.sum(axis=0)\n            step_model.weights.d_initial_hiddens += d_hidden.sum(axis=0)\n            if sgd is not None:\n                for key, (weights, gradient) in updates.items():\n                    sgd(weights, gradient, key=key)\n            return unpad(dX)\n\n        return outputs, recurrent_bwd\n\n    model = wrap(recurrent_fwd, step_model)\n    model.nO = step_model.nO\n    return model",
  "def RNN_step(weights, gates):\n    \"\"\"Create a step model for an RNN, given weights and gates functions.\"\"\"\n\n    def rnn_step_fwd(prevstate_inputs, drop=0.0):\n        prevstate, inputs = prevstate_inputs\n        cell_tm1, hidden_tm1 = prevstate\n\n        acts, bp_acts = weights.begin_update((inputs, hidden_tm1), drop=drop)\n        (cells, hiddens), bp_gates = gates.begin_update((acts, cell_tm1), drop=drop)\n\n        def rnn_step_bwd(d_state_d_hiddens, sgd=None):\n            (d_cells, d_hiddens), d_hiddens = d_state_d_hiddens\n            d_acts, d_cell_tm1 = bp_gates((d_cells, d_hiddens), sgd=sgd)\n            d_inputs, d_hidden_tm1 = bp_acts(d_acts, sgd=sgd)\n            return (d_cell_tm1, d_hidden_tm1), d_inputs\n\n        return ((cells, hiddens), hiddens), rnn_step_bwd\n\n    model = wrap(rnn_step_fwd, weights, gates)\n    model.nO = weights.nO\n    model.nI = weights.nI\n    model.weights = weights\n    model.gates = gates\n    return model",
  "def LSTM_gates(ops):\n    def lstm_gates_fwd(acts_prev_cells, drop=0.0):\n        acts, prev_cells = acts_prev_cells\n        new_cells = ops.allocate(prev_cells.shape)\n        new_hiddens = ops.allocate(prev_cells.shape)\n        ops.lstm(new_hiddens, new_cells, acts, prev_cells)\n        state = (new_cells, new_hiddens)\n        size = new_cells.shape[0]\n\n        def lstm_gates_bwd(d_state, sgd=None):\n            d_cells, d_hiddens = d_state\n            d_cells = d_cells[:size]\n            d_hiddens = d_hiddens[:size]\n            d_acts = [ops.allocate(act.shape) for act in acts]\n            d_prev_cells = ops.allocate(prev_cells.shape)\n            ops.backprop_lstm(\n                d_cells, d_prev_cells, d_acts, d_hiddens, acts, new_cells, prev_cells\n            )\n            return d_acts, d_prev_cells\n\n        return state, lstm_gates_bwd\n\n    return layerize(lstm_gates_fwd)",
  "def _uniform_init(lo, hi):\n    def wrapped(W, ops):\n        copy_array(W, ops.xp.random.uniform(lo, hi, W.shape))\n\n    return wrapped",
  "class LSTM_weights(Model):\n    def __init__(self, nO, nI):\n        Model.__init__(self)\n        self.nO = nO\n        self.nI = nI\n\n    def begin_update(self, inputs_hidden, drop=0.0):\n        inputs, hidden = inputs_hidden\n        assert inputs.dtype == \"float32\"\n        X = self.ops.xp.hstack([inputs, hidden])\n        acts = self.ops.gemm(X, self.W, trans2=True) + self.b\n        acts = self._split_activations(acts)\n        acts[0] += self.forget_bias\n\n        def bwd_lstm_weights(d_acts, sgd=None):\n            self.d_forget_bias += d_acts[0].sum(axis=0)\n            d_acts = self._merge_activations(d_acts)\n            dX = self.ops.gemm(d_acts, self.W)\n            self.d_W += self.ops.gemm(d_acts, X, trans1=True)\n            self.d_b += d_acts.sum(axis=0)\n            d_input = dX[:, : self.nI]\n            d_hidden = dX[:, self.nI :]\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return d_input, d_hidden\n\n        return acts, bwd_lstm_weights\n\n    def get_initial_state(self, n):\n        initial_cells = self.ops.allocate((n, self.nO))\n        initial_hiddens = self.ops.allocate((n, self.nO))\n        initial_cells += self.initial_cells\n        initial_hiddens += self.initial_hiddens\n        return (initial_cells, initial_hiddens)\n\n    def _split_activations(self, acts):\n        acts = acts.reshape((acts.shape[0], 4, self.nO))\n        acts = self.ops.xp.ascontiguousarray(acts.transpose((1, 0, 2)))\n        return [acts[0], acts[1], acts[2], acts[3]]\n\n    def _merge_activations(self, act_pieces):\n        return self.ops.xp.hstack(act_pieces)",
  "def birnn_fwd(Xs, drop=0.0):\n        l2r_Zs, bp_l2r_Zs = l2r.begin_update(Xs, drop=drop)\n        r2l_Zs, bp_r2l_Zs = r2l.begin_update(\n            [l2r.ops.xp.ascontiguousarray(X[::-1]) for X in Xs]\n        )\n\n        def birnn_bwd(dZs, sgd=None):\n            d_l2r_Zs = []\n            d_r2l_Zs = []\n            for dZ in dZs:\n                l2r_fwd = dZ[:, :nO]\n                r2l_fwd = dZ[:, nO:]\n                d_l2r_Zs.append(l2r.ops.xp.ascontiguousarray(l2r_fwd))\n                d_r2l_Zs.append(l2r.ops.xp.ascontiguousarray(r2l_fwd[::-1]))\n            dXs_l2r = bp_l2r_Zs(d_l2r_Zs, sgd=sgd)\n            dXs_r2l = bp_r2l_Zs(d_r2l_Zs, sgd=sgd)\n            dXs = [dXf + dXb[::-1] for dXf, dXb in zip(dXs_l2r, dXs_r2l)]\n            return dXs\n\n        Zs = [l2r.ops.xp.hstack((Zf, Zb[::-1])) for Zf, Zb in zip(l2r_Zs, r2l_Zs)]\n        return Zs, birnn_bwd",
  "def recurrent_fwd(seqs, drop=0.0):\n        lengths = [len(X) for X in seqs]\n        X, size_at_t, unpad = ops.square_sequences(seqs)\n        Y = ops.allocate((X.shape[0], X.shape[1], step_model.nO))\n        cell_drop = ops.get_dropout_mask((len(seqs), step_model.nO), 0.0)\n        hidden_drop = ops.get_dropout_mask((len(seqs), step_model.nO), 0.0)\n        out_drop = ops.get_dropout_mask((len(seqs), step_model.nO), 0.0)\n        backprops = [None] * max(lengths)\n        state = step_model.weights.get_initial_state(len(seqs))\n        for t in range(max(lengths)):\n            state = list(state)\n            size = size_at_t[t]\n            Xt = X[t, :size]\n            state[0] = state[0][:size]\n            state[1] = state[1][:size]\n            if cell_drop is not None:\n                state[0] *= cell_drop\n            if hidden_drop is not None:\n                state[1] *= hidden_drop\n            inputs = (state, Xt)\n            (state, Y[t, :size]), backprops[t] = step_model.begin_update(inputs)\n            if out_drop is not None:\n                Y[t, :size] *= out_drop\n        outputs = unpad(Y)\n\n        def recurrent_bwd(d_outputs, sgd=None):\n            dY, size_at_t, unpad = step_model.ops.square_sequences(d_outputs)\n            d_state = [\n                step_model.ops.allocate((dY.shape[1], step_model.nO)),\n                step_model.ops.allocate((dY.shape[1], step_model.nO)),\n            ]\n            updates = {}\n\n            def gather_updates(weights, gradient, key=None):\n                updates[key] = (weights, gradient)\n\n            dX = step_model.ops.allocate(\n                (dY.shape[0], dY.shape[1], step_model.weights.nI)\n            )\n            for t in range(max(lengths) - 1, -1, -1):\n                if out_drop is not None:\n                    dY[t] *= out_drop\n                d_state_t, dXt = backprops[t]((d_state, dY[t]), sgd=gather_updates)\n                d_state[0][: d_state_t[0].shape[0]] = d_state_t[0]\n                d_state[1][: d_state_t[1].shape[0]] = d_state_t[1]\n                dX[t, : dXt.shape[0]] = dXt\n                if cell_drop is not None:\n                    d_state[0] *= cell_drop\n                if hidden_drop is not None:\n                    d_state[1] *= hidden_drop\n            d_cell, d_hidden = d_state\n            step_model.weights.d_initial_cells += d_cell.sum(axis=0)\n            step_model.weights.d_initial_hiddens += d_hidden.sum(axis=0)\n            if sgd is not None:\n                for key, (weights, gradient) in updates.items():\n                    sgd(weights, gradient, key=key)\n            return unpad(dX)\n\n        return outputs, recurrent_bwd",
  "def rnn_step_fwd(prevstate_inputs, drop=0.0):\n        prevstate, inputs = prevstate_inputs\n        cell_tm1, hidden_tm1 = prevstate\n\n        acts, bp_acts = weights.begin_update((inputs, hidden_tm1), drop=drop)\n        (cells, hiddens), bp_gates = gates.begin_update((acts, cell_tm1), drop=drop)\n\n        def rnn_step_bwd(d_state_d_hiddens, sgd=None):\n            (d_cells, d_hiddens), d_hiddens = d_state_d_hiddens\n            d_acts, d_cell_tm1 = bp_gates((d_cells, d_hiddens), sgd=sgd)\n            d_inputs, d_hidden_tm1 = bp_acts(d_acts, sgd=sgd)\n            return (d_cell_tm1, d_hidden_tm1), d_inputs\n\n        return ((cells, hiddens), hiddens), rnn_step_bwd",
  "def lstm_gates_fwd(acts_prev_cells, drop=0.0):\n        acts, prev_cells = acts_prev_cells\n        new_cells = ops.allocate(prev_cells.shape)\n        new_hiddens = ops.allocate(prev_cells.shape)\n        ops.lstm(new_hiddens, new_cells, acts, prev_cells)\n        state = (new_cells, new_hiddens)\n        size = new_cells.shape[0]\n\n        def lstm_gates_bwd(d_state, sgd=None):\n            d_cells, d_hiddens = d_state\n            d_cells = d_cells[:size]\n            d_hiddens = d_hiddens[:size]\n            d_acts = [ops.allocate(act.shape) for act in acts]\n            d_prev_cells = ops.allocate(prev_cells.shape)\n            ops.backprop_lstm(\n                d_cells, d_prev_cells, d_acts, d_hiddens, acts, new_cells, prev_cells\n            )\n            return d_acts, d_prev_cells\n\n        return state, lstm_gates_bwd",
  "def wrapped(W, ops):\n        copy_array(W, ops.xp.random.uniform(lo, hi, W.shape))",
  "def __init__(self, nO, nI):\n        Model.__init__(self)\n        self.nO = nO\n        self.nI = nI",
  "def begin_update(self, inputs_hidden, drop=0.0):\n        inputs, hidden = inputs_hidden\n        assert inputs.dtype == \"float32\"\n        X = self.ops.xp.hstack([inputs, hidden])\n        acts = self.ops.gemm(X, self.W, trans2=True) + self.b\n        acts = self._split_activations(acts)\n        acts[0] += self.forget_bias\n\n        def bwd_lstm_weights(d_acts, sgd=None):\n            self.d_forget_bias += d_acts[0].sum(axis=0)\n            d_acts = self._merge_activations(d_acts)\n            dX = self.ops.gemm(d_acts, self.W)\n            self.d_W += self.ops.gemm(d_acts, X, trans1=True)\n            self.d_b += d_acts.sum(axis=0)\n            d_input = dX[:, : self.nI]\n            d_hidden = dX[:, self.nI :]\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return d_input, d_hidden\n\n        return acts, bwd_lstm_weights",
  "def get_initial_state(self, n):\n        initial_cells = self.ops.allocate((n, self.nO))\n        initial_hiddens = self.ops.allocate((n, self.nO))\n        initial_cells += self.initial_cells\n        initial_hiddens += self.initial_hiddens\n        return (initial_cells, initial_hiddens)",
  "def _split_activations(self, acts):\n        acts = acts.reshape((acts.shape[0], 4, self.nO))\n        acts = self.ops.xp.ascontiguousarray(acts.transpose((1, 0, 2)))\n        return [acts[0], acts[1], acts[2], acts[3]]",
  "def _merge_activations(self, act_pieces):\n        return self.ops.xp.hstack(act_pieces)",
  "def birnn_bwd(dZs, sgd=None):\n            d_l2r_Zs = []\n            d_r2l_Zs = []\n            for dZ in dZs:\n                l2r_fwd = dZ[:, :nO]\n                r2l_fwd = dZ[:, nO:]\n                d_l2r_Zs.append(l2r.ops.xp.ascontiguousarray(l2r_fwd))\n                d_r2l_Zs.append(l2r.ops.xp.ascontiguousarray(r2l_fwd[::-1]))\n            dXs_l2r = bp_l2r_Zs(d_l2r_Zs, sgd=sgd)\n            dXs_r2l = bp_r2l_Zs(d_r2l_Zs, sgd=sgd)\n            dXs = [dXf + dXb[::-1] for dXf, dXb in zip(dXs_l2r, dXs_r2l)]\n            return dXs",
  "def recurrent_bwd(d_outputs, sgd=None):\n            dY, size_at_t, unpad = step_model.ops.square_sequences(d_outputs)\n            d_state = [\n                step_model.ops.allocate((dY.shape[1], step_model.nO)),\n                step_model.ops.allocate((dY.shape[1], step_model.nO)),\n            ]\n            updates = {}\n\n            def gather_updates(weights, gradient, key=None):\n                updates[key] = (weights, gradient)\n\n            dX = step_model.ops.allocate(\n                (dY.shape[0], dY.shape[1], step_model.weights.nI)\n            )\n            for t in range(max(lengths) - 1, -1, -1):\n                if out_drop is not None:\n                    dY[t] *= out_drop\n                d_state_t, dXt = backprops[t]((d_state, dY[t]), sgd=gather_updates)\n                d_state[0][: d_state_t[0].shape[0]] = d_state_t[0]\n                d_state[1][: d_state_t[1].shape[0]] = d_state_t[1]\n                dX[t, : dXt.shape[0]] = dXt\n                if cell_drop is not None:\n                    d_state[0] *= cell_drop\n                if hidden_drop is not None:\n                    d_state[1] *= hidden_drop\n            d_cell, d_hidden = d_state\n            step_model.weights.d_initial_cells += d_cell.sum(axis=0)\n            step_model.weights.d_initial_hiddens += d_hidden.sum(axis=0)\n            if sgd is not None:\n                for key, (weights, gradient) in updates.items():\n                    sgd(weights, gradient, key=key)\n            return unpad(dX)",
  "def rnn_step_bwd(d_state_d_hiddens, sgd=None):\n            (d_cells, d_hiddens), d_hiddens = d_state_d_hiddens\n            d_acts, d_cell_tm1 = bp_gates((d_cells, d_hiddens), sgd=sgd)\n            d_inputs, d_hidden_tm1 = bp_acts(d_acts, sgd=sgd)\n            return (d_cell_tm1, d_hidden_tm1), d_inputs",
  "def lstm_gates_bwd(d_state, sgd=None):\n            d_cells, d_hiddens = d_state\n            d_cells = d_cells[:size]\n            d_hiddens = d_hiddens[:size]\n            d_acts = [ops.allocate(act.shape) for act in acts]\n            d_prev_cells = ops.allocate(prev_cells.shape)\n            ops.backprop_lstm(\n                d_cells, d_prev_cells, d_acts, d_hiddens, acts, new_cells, prev_cells\n            )\n            return d_acts, d_prev_cells",
  "def bwd_lstm_weights(d_acts, sgd=None):\n            self.d_forget_bias += d_acts[0].sum(axis=0)\n            d_acts = self._merge_activations(d_acts)\n            dX = self.ops.gemm(d_acts, self.W)\n            self.d_W += self.ops.gemm(d_acts, X, trans1=True)\n            self.d_b += d_acts.sum(axis=0)\n            d_input = dX[:, : self.nI]\n            d_hidden = dX[:, self.nI :]\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return d_input, d_hidden",
  "def gather_updates(weights, gradient, key=None):\n                updates[key] = (weights, gradient)",
  "def _set_dimensions_if_needed(model, X, y=None):\n    if model.nI is None:\n        model.nI = X.shape[1]\n    if model.nO is None and y is not None:\n        if len(y.shape) == 2:\n            model.nO = y.shape[1]\n        else:\n            model.nO = int(y.max()) + 1",
  "class Affine(Model):\n    \"\"\"Computes the linear transform Y = (W @ X) + b.\"\"\"\n\n    name = \"affine\"\n\n    @property\n    def input_shape(self):\n        return (self.nB, self.nI)\n\n    @property\n    def output_shape(self):\n        return (self.nB, self.nO)\n\n    def __init__(self, nO=None, nI=None, **kwargs):\n        Model.__init__(self, **kwargs)\n        self.nO = nO\n        self.nI = nI\n        self.drop_factor = kwargs.get(\"drop_factor\", 1.0)\n\n    @check.arg(1, has_shape((\"nB\", \"nI\")))\n    def predict(self, input__BI):\n        output = self.ops.gemm(input__BI, self.W, trans2=True)\n        output += self.b\n        return output\n\n    @check.arg(1, has_shape((\"nB\", \"nI\")))\n    def begin_update(self, input__BI, drop=0.0):\n        input__BI = self.ops.xp.ascontiguousarray(input__BI)\n        output__BO = self.predict(input__BI)\n\n        def finish_update(grad__BO, sgd=None):\n            grad__BO = self.ops.xp.ascontiguousarray(grad__BO)\n            self.ops.gemm(grad__BO, input__BI, trans1=True, out=self.d_W)\n            self.d_b += grad__BO.sum(axis=0)\n            grad__BI = self.ops.gemm(grad__BO, self.W)\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return grad__BI\n\n        if drop is not None:\n            drop *= self.drop_factor\n        output__BO, bp_dropout = self.ops.dropout(output__BO, drop, inplace=True)\n        return output__BO, bp_dropout(finish_update)",
  "def input_shape(self):\n        return (self.nB, self.nI)",
  "def output_shape(self):\n        return (self.nB, self.nO)",
  "def __init__(self, nO=None, nI=None, **kwargs):\n        Model.__init__(self, **kwargs)\n        self.nO = nO\n        self.nI = nI\n        self.drop_factor = kwargs.get(\"drop_factor\", 1.0)",
  "def predict(self, input__BI):\n        output = self.ops.gemm(input__BI, self.W, trans2=True)\n        output += self.b\n        return output",
  "def begin_update(self, input__BI, drop=0.0):\n        input__BI = self.ops.xp.ascontiguousarray(input__BI)\n        output__BO = self.predict(input__BI)\n\n        def finish_update(grad__BO, sgd=None):\n            grad__BO = self.ops.xp.ascontiguousarray(grad__BO)\n            self.ops.gemm(grad__BO, input__BI, trans1=True, out=self.d_W)\n            self.d_b += grad__BO.sum(axis=0)\n            grad__BI = self.ops.gemm(grad__BO, self.W)\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return grad__BI\n\n        if drop is not None:\n            drop *= self.drop_factor\n        output__BO, bp_dropout = self.ops.dropout(output__BO, drop, inplace=True)\n        return output__BO, bp_dropout(finish_update)",
  "def finish_update(grad__BO, sgd=None):\n            grad__BO = self.ops.xp.ascontiguousarray(grad__BO)\n            self.ops.gemm(grad__BO, input__BI, trans1=True, out=self.d_W)\n            self.d_b += grad__BO.sum(axis=0)\n            grad__BI = self.ops.gemm(grad__BO, self.W)\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return grad__BI",
  "class StaticVectors(Model):\n    \"\"\"Load a static embedding table, and learn a linear projection from it.\n\n    Out-of-vocabulary items are modded into the table, receiving an arbitrary\n    vector (but the same word will always receive the same vector).\n    \"\"\"\n\n    name = \"static-vectors\"\n\n    def __init__(self, lang, nO, drop_factor=0.0, column=0):\n        Model.__init__(self)\n        self.column = column\n        self.nO = nO\n        # This doesn't seem the cleverest solution,\n        # but it ensures multiple models load the\n        # same copy of spaCy if they're deserialised.\n        self.lang = lang\n        vectors = self.get_vectors()\n        self.nM = vectors.shape[1]\n        self.drop_factor = drop_factor\n        self.column = column\n        if self.nM == 0:\n            raise ValueError(\n                \"Cannot create vectors table with dimension 0.\\n\"\n                \"If you're using pre-trained vectors, are the vectors loaded?\"\n            )\n        self.nV = vectors.shape[0]\n\n    def get_vectors(self):\n        return get_vectors(self.ops, self.lang)\n\n    def begin_update(self, ids, drop=0.0):\n        if ids.ndim >= 2:\n            ids = self.ops.xp.ascontiguousarray(ids[:, self.column])\n        vector_table = self.get_vectors()\n        vectors = vector_table[ids * (ids < vector_table.shape[0])]\n        vectors = self.ops.xp.ascontiguousarray(vectors)\n        assert vectors.shape[0] == ids.shape[0]\n\n        def finish_update(gradients, sgd=None):\n            if mask is not None:\n                gradients *= mask\n            self.d_W += self.ops.gemm(gradients, vectors, trans1=True)\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return None\n\n        dotted = self.ops.gemm(vectors, self.W, trans2=True)\n        mask = self.ops.get_dropout_mask((dotted.shape[1],), drop)\n        if mask is not None:\n            dotted *= mask\n        return dotted, finish_update",
  "def __init__(self, lang, nO, drop_factor=0.0, column=0):\n        Model.__init__(self)\n        self.column = column\n        self.nO = nO\n        # This doesn't seem the cleverest solution,\n        # but it ensures multiple models load the\n        # same copy of spaCy if they're deserialised.\n        self.lang = lang\n        vectors = self.get_vectors()\n        self.nM = vectors.shape[1]\n        self.drop_factor = drop_factor\n        self.column = column\n        if self.nM == 0:\n            raise ValueError(\n                \"Cannot create vectors table with dimension 0.\\n\"\n                \"If you're using pre-trained vectors, are the vectors loaded?\"\n            )\n        self.nV = vectors.shape[0]",
  "def get_vectors(self):\n        return get_vectors(self.ops, self.lang)",
  "def begin_update(self, ids, drop=0.0):\n        if ids.ndim >= 2:\n            ids = self.ops.xp.ascontiguousarray(ids[:, self.column])\n        vector_table = self.get_vectors()\n        vectors = vector_table[ids * (ids < vector_table.shape[0])]\n        vectors = self.ops.xp.ascontiguousarray(vectors)\n        assert vectors.shape[0] == ids.shape[0]\n\n        def finish_update(gradients, sgd=None):\n            if mask is not None:\n                gradients *= mask\n            self.d_W += self.ops.gemm(gradients, vectors, trans1=True)\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return None\n\n        dotted = self.ops.gemm(vectors, self.W, trans2=True)\n        mask = self.ops.get_dropout_mask((dotted.shape[1],), drop)\n        if mask is not None:\n            dotted *= mask\n        return dotted, finish_update",
  "def finish_update(gradients, sgd=None):\n            if mask is not None:\n                gradients *= mask\n            self.d_W += self.ops.gemm(gradients, vectors, trans1=True)\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return None",
  "def _set_dimensions_if_needed(model, X, y=None):\n    if model.nI is None:\n        model.nI = X.shape[1]\n    if model.nO is None and y is not None:\n        if len(y.shape) == 2:\n            model.nO = y.shape[1]\n        else:\n            model.nO = int(y.max()) + 1",
  "class SELU(Model):\n    name = \"selu\"\n\n    @property\n    def input_shape(self):\n        return (self.nB, self.nI)\n\n    @property\n    def output_shape(self):\n        return (self.nB, self.nO)\n\n    def __init__(self, nO=None, nI=None, **kwargs):\n        Model.__init__(self, **kwargs)\n        self.nO = nO\n        self.nI = nI\n        self.drop_factor = kwargs.get(\"drop_factor\", 1.0)\n\n    def predict(self, input__bi):\n        output__bo = self.ops.affine(self.W, self.b, input__bi)\n        self.ops.selu(output__bo, inplace=True)\n        return output__bo\n\n    def begin_update(self, input__bi, drop=0.0):\n        output__bo = self.predict(input__bi)\n        output_copy = self.ops.xp.ascontiguousarray(output__bo, dtype=\"f\")\n        self.ops.selu(output_copy, inplace=True)\n\n        def finish_update(grad__bo, sgd=None):\n            grad__bo = self.ops.xp.ascontiguousarray(grad__bo, dtype=\"f\")\n            self.ops.backprop_selu(grad__bo, output_copy, inplace=True)\n            self.d_W += self.ops.batch_outer(grad__bo, input__bi)\n            self.d_b += grad__bo.sum(axis=0)\n            grad__BI = self.ops.batch_dot(grad__bo, self.W.T)\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return grad__BI\n\n        if drop is not None:\n            drop *= self.drop_factor\n        return self.dropout(output__bo, finish_update, drop)\n\n    def dropout(self, X, finish_update, drop):\n        if drop <= 0:\n            return X, finish_update\n        alpha = -1.75809934\n        q = 1.0 - drop\n        a = (q * (1.0 + alpha * alpha * (1.0 - q))) ** -0.5\n        b = -a * (1.0 - q) * alpha\n        mask = self.ops.xp.random.uniform(0.0, 1.0, X.shape)\n\n        def backprop_selu_dropout(d_dropped, sgd=None):\n            return finish_update(a * d_dropped * mask, sgd=sgd)\n\n        dropped = self.ops.xp.where(mask >= drop, X, alpha)\n        return a * dropped + b, backprop_selu_dropout",
  "def input_shape(self):\n        return (self.nB, self.nI)",
  "def output_shape(self):\n        return (self.nB, self.nO)",
  "def __init__(self, nO=None, nI=None, **kwargs):\n        Model.__init__(self, **kwargs)\n        self.nO = nO\n        self.nI = nI\n        self.drop_factor = kwargs.get(\"drop_factor\", 1.0)",
  "def predict(self, input__bi):\n        output__bo = self.ops.affine(self.W, self.b, input__bi)\n        self.ops.selu(output__bo, inplace=True)\n        return output__bo",
  "def begin_update(self, input__bi, drop=0.0):\n        output__bo = self.predict(input__bi)\n        output_copy = self.ops.xp.ascontiguousarray(output__bo, dtype=\"f\")\n        self.ops.selu(output_copy, inplace=True)\n\n        def finish_update(grad__bo, sgd=None):\n            grad__bo = self.ops.xp.ascontiguousarray(grad__bo, dtype=\"f\")\n            self.ops.backprop_selu(grad__bo, output_copy, inplace=True)\n            self.d_W += self.ops.batch_outer(grad__bo, input__bi)\n            self.d_b += grad__bo.sum(axis=0)\n            grad__BI = self.ops.batch_dot(grad__bo, self.W.T)\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return grad__BI\n\n        if drop is not None:\n            drop *= self.drop_factor\n        return self.dropout(output__bo, finish_update, drop)",
  "def dropout(self, X, finish_update, drop):\n        if drop <= 0:\n            return X, finish_update\n        alpha = -1.75809934\n        q = 1.0 - drop\n        a = (q * (1.0 + alpha * alpha * (1.0 - q))) ** -0.5\n        b = -a * (1.0 - q) * alpha\n        mask = self.ops.xp.random.uniform(0.0, 1.0, X.shape)\n\n        def backprop_selu_dropout(d_dropped, sgd=None):\n            return finish_update(a * d_dropped * mask, sgd=sgd)\n\n        dropped = self.ops.xp.where(mask >= drop, X, alpha)\n        return a * dropped + b, backprop_selu_dropout",
  "def finish_update(grad__bo, sgd=None):\n            grad__bo = self.ops.xp.ascontiguousarray(grad__bo, dtype=\"f\")\n            self.ops.backprop_selu(grad__bo, output_copy, inplace=True)\n            self.d_W += self.ops.batch_outer(grad__bo, input__bi)\n            self.d_b += grad__bo.sum(axis=0)\n            grad__BI = self.ops.batch_dot(grad__bo, self.W.T)\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return grad__BI",
  "def backprop_selu_dropout(d_dropped, sgd=None):\n            return finish_update(a * d_dropped * mask, sgd=sgd)",
  "def prepare_self_attention(affine, window=None, nM=300, nH=6):\n    nD = nM // nH\n    if window is not None:\n        get_mask = window_mask(window)\n    else:\n        get_mask = None\n    def qkv_sa_forward(Xs, drop=0.0):\n        X = affine.ops.flatten(Xs)\n        lengths = [len(x) for x in Xs]\n        QKV, get_dX = affine.begin_update(X, drop=drop)\n        Qs, Ks, Vs = _split_seqs(QKV, lengths, nH, nD)\n\n        def qkv_sa_backward(dQs_dKs_dVs, sgd=None):\n            dQs, dKs, dVs = dQs_dKs_dVs\n            dQKV = _join_seqs(dQs, dKs, dVs, nH, nD)\n            dX = get_dX(dQKV, sgd=sgd)\n            return affine.ops.unflatten(dX, lengths)\n        if get_mask is not None:\n            xp = get_array_module(X)\n            masks = [get_mask(xp, length, length) for length in lengths]\n        else:\n            masks = [None for _ in lengths]\n        return (Qs, Ks, Vs, masks), qkv_sa_backward\n    return wrap(qkv_sa_forward, affine)",
  "def window_mask(n):\n    def get_mask(xp, nX, nY):\n        mask = xp.zeros((nX, nY), dtype='f')\n        for i in range(nX):\n            mask[i, i-n:i+n] = 1\n        return mask\n    return get_mask",
  "def _split_seqs(QKV, lengths, nH, nD):\n    assert sum(lengths) == QKV.shape[0], (sum(lengths), QKV.shape[0])\n    Qs = []\n    Ks = []\n    Vs = []\n    i = 0\n    xp = get_array_module(QKV)\n    for length in lengths:\n        qkv = QKV[i:i+length]\n        qkv = qkv.reshape((length, 3, nH*nD))\n        queries = xp.ascontiguousarray(qkv[:, 0])\n        keys = xp.ascontiguousarray(qkv[:, 1])\n        values = xp.ascontiguousarray(qkv[:, 2])\n        Qs.append(queries.reshape((-1, nH, nD)))\n        Ks.append(keys.reshape((-1, nH, nD)))\n        Vs.append(values.reshape((-1, nH, nD)))\n        i += length\n    return Qs, Ks, Vs",
  "def _join_seqs(Qs, Ks, Vs, nH, nD):\n    xp = get_array_module(Qs[0])\n    Q = xp.vstack(Qs).reshape((-1, nH*nD))\n    K = xp.vstack(Ks).reshape((-1, nH*nD))\n    V = xp.vstack(Vs).reshape((-1, nH*nD))\n    assert Q.shape[0] == K.shape[0] == V.shape[0]\n    return xp.hstack((Q, K, V))",
  "class MultiHeadedAttention(Model):\n    \"\"\"Multi-headed attention. Requires a preprocessor to prepare (Qs, Ks, Vs, masks)\n    triples, such as the prepare_self_attention() preprocessor. A layer\n    should run after this to do the projection as well.\"\"\"\n    def __init__(self):\n        Model.__init__(self)\n\n    def begin_update(self, Qs_Ks_Vs_masks, drop=0.0):\n        Qs, Ks, Vs, masks = Qs_Ks_Vs_masks\n        if masks is None:\n            masks = [None for _ in Qs]\n        assert len(Qs) == len(Ks) == len(Vs)\n        return self._attend_seqs(Qs, Ks, Vs, masks)\n        \n    def _attend_seqs(self, Qs, Ks, Vs, masks):\n        outputs = []\n        backprops = []\n        assert len(Qs) == len(Ks) == len(Vs) == len(masks), (len(Qs), len(Ks), len(Vs), len(masks))\n        for Q, K, V, mask in zip(Qs, Ks, Vs, masks):\n            output, backprop = self._attend(Q, K, V, mask)\n            outputs.append(output)\n            backprops.append(backprop)\n            assert output.shape[0] == Q.shape[0]\n\n        def backprop_attend_seqs(d_outputs, sgd=None):\n            dQs = []\n            dKs = []\n            dVs = []\n            for d_output, backprop in zip(d_outputs, backprops):\n                dQ, dK, dV = backprop(d_output, sgd=sgd)\n                dQs.append(dQ)\n                dKs.append(dK)\n                dVs.append(dV)\n            return dQs, dKs, dVs\n        assert len(outputs) == len(Qs), len(Qs)\n        return outputs, backprop_attend_seqs\n\n    def _attend(self, Q, K, V, mask):\n        \"\"\"\n        Compute attention on a (query, key, value) triplet.\n        The similarity of the (Q, K) pairs are used to\n        compute an attention matrix, which is used to rescale\n        V.\n        \"\"\"\n        attn, get_dQ_dK = self._get_attn_weights(Q, K, mask)\n        output, get_d_attn_dV = self._apply_attn(attn, V)\n\n        def backprop_attend(d_output, sgd=None):\n            d_attn, dV = get_d_attn_dV(d_output)\n            dQ, dK = get_dQ_dK(d_attn)\n            return (dQ, dK, dV)\n\n        return output, backprop_attend\n\n    def _get_attn_weights(self, Q0, K0, mask):\n        nQ, nK, nH, nD = (Q0.shape[0], K0.shape[0], Q0.shape[1], Q0.shape[2])\n        assert Q0.shape == (nQ, nH, nD)\n        assert K0.shape == (nK, nH, nD)\n        sqrtM = self.ops.xp.sqrt(nH*nD).astype(\"f\")\n        Q1 = _trans(Q0, 1, 0, 2)\n        assert Q1.shape == (nH, nQ, nD)\n        K1 = _trans(K0, 1, 2, 0)\n        assert K1.shape == (nH, nD, nK)\n        K1 /= sqrtM\n        attn0 = self.ops.matmul(Q1, K1)\n        assert attn0.shape == (nH, nQ, nK)\n        attn1, backprop_mask = self._apply_mask(attn0, mask)\n        attn2 = self.ops.softmax(attn1, axis=-1)\n        assert attn2.shape == (nH, nQ, nK)\n\n        def backprop_attn1(d_attn2, sgd=None):\n            assert d_attn2.shape == (nH, nQ, nK)\n            d_attn1 = self.ops.backprop_softmax(attn2, d_attn2, axis=-1)\n            d_attn0 = backprop_mask(d_attn1)\n            assert d_attn0.shape == (nH, nQ, nK)\n            dQ1 = self.ops.matmul(d_attn0, _trans(K1, 0, 2, 1))\n            assert dQ1.shape == (nH, nQ, nD)\n            dK1 = self.ops.matmul(_trans(Q1, 0, 2, 1), d_attn0)\n            assert dK1.shape == (nH, nD, nK)\n            dK0 = _trans(dK1, 2, 0, 1)\n            dK0 /= sqrtM\n            assert dK0.shape == (nK, nH, nD)\n            dQ0 = _trans(dQ1, 1, 0, 2)\n            assert dQ0.shape == (nQ, nH, nD)\n            return dQ0, dK0\n\n        return attn2, backprop_attn1\n\n    def _apply_mask(self, attn, mask):\n        def backprop_apply_mask(d_attn, sgd=None):\n            if mask is None:\n                return d_attn\n            else:\n                return d_attn * mask\n\n        if mask is None:\n            return attn, backprop_apply_mask\n        else:\n            return attn - (1-mask)*1e9, backprop_apply_mask\n\n    def _apply_attn(self, attn, V0):\n        \"\"\" Multiplication with values \"\"\"\n        nH, nQ, nV = attn.shape\n        nD = V0.shape[-1]\n        assert V0.shape == (nV, nH, nD)\n        V1 = _trans(V0, 1, 0, 2)\n        assert V1.shape == (nH, nV, nD)\n        # (nH, nQ, nV) @ (nH, nV, nD) = (nH, nQ, nD)\n        S0 = self.ops.matmul(attn, V1)\n        assert S0.shape == (nH, nQ, nD)\n        S1 = _trans(S0, 1, 0, 2)\n        assert S1.shape == (nQ, nH, nD)\n        S2 = S1.reshape((nQ, nH*nD))\n\n        def backprop_apply_attn(dS2):\n            assert dS2.shape == (nQ, nH*nD)\n            dS1 = dS2.reshape((nQ, nH, nD))\n            dS0 = dS1.transpose((1, 0, 2))\n            assert dS0.shape == (nH, nQ, nD)\n            dS0 = self.ops.xp.ascontiguousarray(dS0)\n            # (nH, nQ, nD) @ (nH, nD, nV) --> (nH, nQ, nV)\n            d_attn = self.ops.matmul(dS0, _trans(V1, 0, 2, 1))\n            assert d_attn.shape == (nH, nQ, nV)\n            # (nH, nV, nQ) @ (nH, nQ, nD) --> (nH, nV, nD)\n            dV1 = self.ops.matmul(_trans(attn, 0, 2, 1), dS0)\n            assert dV1.shape == (nH, nV, nD)\n            dV0 = dV1.transpose((1, 0, 2))\n            assert dV0.shape == (nV, nH, nD)\n            return d_attn, dV0\n\n        return S2, backprop_apply_attn",
  "def _trans(X, *order):\n    \"\"\"Transpose and make contiguous\"\"\"\n    xp = get_array_module(X)\n    return xp.ascontiguousarray(X.transpose(order))",
  "def qkv_sa_forward(Xs, drop=0.0):\n        X = affine.ops.flatten(Xs)\n        lengths = [len(x) for x in Xs]\n        QKV, get_dX = affine.begin_update(X, drop=drop)\n        Qs, Ks, Vs = _split_seqs(QKV, lengths, nH, nD)\n\n        def qkv_sa_backward(dQs_dKs_dVs, sgd=None):\n            dQs, dKs, dVs = dQs_dKs_dVs\n            dQKV = _join_seqs(dQs, dKs, dVs, nH, nD)\n            dX = get_dX(dQKV, sgd=sgd)\n            return affine.ops.unflatten(dX, lengths)\n        if get_mask is not None:\n            xp = get_array_module(X)\n            masks = [get_mask(xp, length, length) for length in lengths]\n        else:\n            masks = [None for _ in lengths]\n        return (Qs, Ks, Vs, masks), qkv_sa_backward",
  "def get_mask(xp, nX, nY):\n        mask = xp.zeros((nX, nY), dtype='f')\n        for i in range(nX):\n            mask[i, i-n:i+n] = 1\n        return mask",
  "def __init__(self):\n        Model.__init__(self)",
  "def begin_update(self, Qs_Ks_Vs_masks, drop=0.0):\n        Qs, Ks, Vs, masks = Qs_Ks_Vs_masks\n        if masks is None:\n            masks = [None for _ in Qs]\n        assert len(Qs) == len(Ks) == len(Vs)\n        return self._attend_seqs(Qs, Ks, Vs, masks)",
  "def _attend_seqs(self, Qs, Ks, Vs, masks):\n        outputs = []\n        backprops = []\n        assert len(Qs) == len(Ks) == len(Vs) == len(masks), (len(Qs), len(Ks), len(Vs), len(masks))\n        for Q, K, V, mask in zip(Qs, Ks, Vs, masks):\n            output, backprop = self._attend(Q, K, V, mask)\n            outputs.append(output)\n            backprops.append(backprop)\n            assert output.shape[0] == Q.shape[0]\n\n        def backprop_attend_seqs(d_outputs, sgd=None):\n            dQs = []\n            dKs = []\n            dVs = []\n            for d_output, backprop in zip(d_outputs, backprops):\n                dQ, dK, dV = backprop(d_output, sgd=sgd)\n                dQs.append(dQ)\n                dKs.append(dK)\n                dVs.append(dV)\n            return dQs, dKs, dVs\n        assert len(outputs) == len(Qs), len(Qs)\n        return outputs, backprop_attend_seqs",
  "def _attend(self, Q, K, V, mask):\n        \"\"\"\n        Compute attention on a (query, key, value) triplet.\n        The similarity of the (Q, K) pairs are used to\n        compute an attention matrix, which is used to rescale\n        V.\n        \"\"\"\n        attn, get_dQ_dK = self._get_attn_weights(Q, K, mask)\n        output, get_d_attn_dV = self._apply_attn(attn, V)\n\n        def backprop_attend(d_output, sgd=None):\n            d_attn, dV = get_d_attn_dV(d_output)\n            dQ, dK = get_dQ_dK(d_attn)\n            return (dQ, dK, dV)\n\n        return output, backprop_attend",
  "def _get_attn_weights(self, Q0, K0, mask):\n        nQ, nK, nH, nD = (Q0.shape[0], K0.shape[0], Q0.shape[1], Q0.shape[2])\n        assert Q0.shape == (nQ, nH, nD)\n        assert K0.shape == (nK, nH, nD)\n        sqrtM = self.ops.xp.sqrt(nH*nD).astype(\"f\")\n        Q1 = _trans(Q0, 1, 0, 2)\n        assert Q1.shape == (nH, nQ, nD)\n        K1 = _trans(K0, 1, 2, 0)\n        assert K1.shape == (nH, nD, nK)\n        K1 /= sqrtM\n        attn0 = self.ops.matmul(Q1, K1)\n        assert attn0.shape == (nH, nQ, nK)\n        attn1, backprop_mask = self._apply_mask(attn0, mask)\n        attn2 = self.ops.softmax(attn1, axis=-1)\n        assert attn2.shape == (nH, nQ, nK)\n\n        def backprop_attn1(d_attn2, sgd=None):\n            assert d_attn2.shape == (nH, nQ, nK)\n            d_attn1 = self.ops.backprop_softmax(attn2, d_attn2, axis=-1)\n            d_attn0 = backprop_mask(d_attn1)\n            assert d_attn0.shape == (nH, nQ, nK)\n            dQ1 = self.ops.matmul(d_attn0, _trans(K1, 0, 2, 1))\n            assert dQ1.shape == (nH, nQ, nD)\n            dK1 = self.ops.matmul(_trans(Q1, 0, 2, 1), d_attn0)\n            assert dK1.shape == (nH, nD, nK)\n            dK0 = _trans(dK1, 2, 0, 1)\n            dK0 /= sqrtM\n            assert dK0.shape == (nK, nH, nD)\n            dQ0 = _trans(dQ1, 1, 0, 2)\n            assert dQ0.shape == (nQ, nH, nD)\n            return dQ0, dK0\n\n        return attn2, backprop_attn1",
  "def _apply_mask(self, attn, mask):\n        def backprop_apply_mask(d_attn, sgd=None):\n            if mask is None:\n                return d_attn\n            else:\n                return d_attn * mask\n\n        if mask is None:\n            return attn, backprop_apply_mask\n        else:\n            return attn - (1-mask)*1e9, backprop_apply_mask",
  "def _apply_attn(self, attn, V0):\n        \"\"\" Multiplication with values \"\"\"\n        nH, nQ, nV = attn.shape\n        nD = V0.shape[-1]\n        assert V0.shape == (nV, nH, nD)\n        V1 = _trans(V0, 1, 0, 2)\n        assert V1.shape == (nH, nV, nD)\n        # (nH, nQ, nV) @ (nH, nV, nD) = (nH, nQ, nD)\n        S0 = self.ops.matmul(attn, V1)\n        assert S0.shape == (nH, nQ, nD)\n        S1 = _trans(S0, 1, 0, 2)\n        assert S1.shape == (nQ, nH, nD)\n        S2 = S1.reshape((nQ, nH*nD))\n\n        def backprop_apply_attn(dS2):\n            assert dS2.shape == (nQ, nH*nD)\n            dS1 = dS2.reshape((nQ, nH, nD))\n            dS0 = dS1.transpose((1, 0, 2))\n            assert dS0.shape == (nH, nQ, nD)\n            dS0 = self.ops.xp.ascontiguousarray(dS0)\n            # (nH, nQ, nD) @ (nH, nD, nV) --> (nH, nQ, nV)\n            d_attn = self.ops.matmul(dS0, _trans(V1, 0, 2, 1))\n            assert d_attn.shape == (nH, nQ, nV)\n            # (nH, nV, nQ) @ (nH, nQ, nD) --> (nH, nV, nD)\n            dV1 = self.ops.matmul(_trans(attn, 0, 2, 1), dS0)\n            assert dV1.shape == (nH, nV, nD)\n            dV0 = dV1.transpose((1, 0, 2))\n            assert dV0.shape == (nV, nH, nD)\n            return d_attn, dV0\n\n        return S2, backprop_apply_attn",
  "def qkv_sa_backward(dQs_dKs_dVs, sgd=None):\n            dQs, dKs, dVs = dQs_dKs_dVs\n            dQKV = _join_seqs(dQs, dKs, dVs, nH, nD)\n            dX = get_dX(dQKV, sgd=sgd)\n            return affine.ops.unflatten(dX, lengths)",
  "def backprop_attend_seqs(d_outputs, sgd=None):\n            dQs = []\n            dKs = []\n            dVs = []\n            for d_output, backprop in zip(d_outputs, backprops):\n                dQ, dK, dV = backprop(d_output, sgd=sgd)\n                dQs.append(dQ)\n                dKs.append(dK)\n                dVs.append(dV)\n            return dQs, dKs, dVs",
  "def backprop_attend(d_output, sgd=None):\n            d_attn, dV = get_d_attn_dV(d_output)\n            dQ, dK = get_dQ_dK(d_attn)\n            return (dQ, dK, dV)",
  "def backprop_attn1(d_attn2, sgd=None):\n            assert d_attn2.shape == (nH, nQ, nK)\n            d_attn1 = self.ops.backprop_softmax(attn2, d_attn2, axis=-1)\n            d_attn0 = backprop_mask(d_attn1)\n            assert d_attn0.shape == (nH, nQ, nK)\n            dQ1 = self.ops.matmul(d_attn0, _trans(K1, 0, 2, 1))\n            assert dQ1.shape == (nH, nQ, nD)\n            dK1 = self.ops.matmul(_trans(Q1, 0, 2, 1), d_attn0)\n            assert dK1.shape == (nH, nD, nK)\n            dK0 = _trans(dK1, 2, 0, 1)\n            dK0 /= sqrtM\n            assert dK0.shape == (nK, nH, nD)\n            dQ0 = _trans(dQ1, 1, 0, 2)\n            assert dQ0.shape == (nQ, nH, nD)\n            return dQ0, dK0",
  "def backprop_apply_mask(d_attn, sgd=None):\n            if mask is None:\n                return d_attn\n            else:\n                return d_attn * mask",
  "def backprop_apply_attn(dS2):\n            assert dS2.shape == (nQ, nH*nD)\n            dS1 = dS2.reshape((nQ, nH, nD))\n            dS0 = dS1.transpose((1, 0, 2))\n            assert dS0.shape == (nH, nQ, nD)\n            dS0 = self.ops.xp.ascontiguousarray(dS0)\n            # (nH, nQ, nD) @ (nH, nD, nV) --> (nH, nQ, nV)\n            d_attn = self.ops.matmul(dS0, _trans(V1, 0, 2, 1))\n            assert d_attn.shape == (nH, nQ, nV)\n            # (nH, nV, nQ) @ (nH, nQ, nD) --> (nH, nV, nD)\n            dV1 = self.ops.matmul(_trans(attn, 0, 2, 1), dS0)\n            assert dV1.shape == (nH, nV, nD)\n            dV0 = dV1.transpose((1, 0, 2))\n            assert dV0.shape == (nV, nH, nD)\n            return d_attn, dV0",
  "def LSUVinit(model, X, y=None):\n    if model.vectors is not None:\n        do_lsuv(model.ops, model.vectors, model, X)\n    return X",
  "def _uniform_init(lo, hi):\n    def wrapped(W, ops):\n        if (W ** 2).sum() == 0.0:\n            copy_array(W, ops.xp.random.uniform(lo, hi, W.shape))\n\n    return wrapped",
  "class HashEmbed(Model):\n    name = \"hash-embed\"\n\n    def __init__(self, nO, nV, seed=None, **kwargs):\n        Model.__init__(self, **kwargs)\n        self.column = kwargs.get(\"column\", 0)\n        self.nO = nO\n        self.nV = nV\n        \n        if seed is not None:\n            self.seed = seed\n        else:\n            self.seed = self.id\n\n    def predict(self, ids):\n        if ids.ndim >= 2:\n            ids = self.ops.xp.ascontiguousarray(ids[:, self.column], dtype=\"uint64\")\n        keys = self.ops.hash(ids, self.seed) % self.nV\n        vectors = self.vectors[keys]\n        summed = vectors.sum(axis=1)\n        return summed\n\n    def begin_update(self, ids, drop=0.0):\n        if ids.ndim >= 2:\n            ids = self.ops.xp.ascontiguousarray(ids[:, self.column], dtype=\"uint64\")\n        keys = self.ops.hash(ids, self.seed) % self.nV\n        vectors = self.vectors[keys].sum(axis=1)\n        mask = self.ops.get_dropout_mask((vectors.shape[1],), drop)\n        if mask is not None:\n            vectors *= mask\n\n        def finish_update(delta, sgd=None):\n            if mask is not None:\n                delta *= mask\n            keys = self.ops.hash(ids, self.seed) % self.nV\n            d_vectors = self.d_vectors\n            keys = self.ops.xp.ascontiguousarray(keys.T, dtype=\"i\")\n            for i in range(keys.shape[0]):\n                self.ops.scatter_add(d_vectors, keys[i], delta)\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return None\n\n        return vectors, finish_update",
  "def wrapped(W, ops):\n        if (W ** 2).sum() == 0.0:\n            copy_array(W, ops.xp.random.uniform(lo, hi, W.shape))",
  "def __init__(self, nO, nV, seed=None, **kwargs):\n        Model.__init__(self, **kwargs)\n        self.column = kwargs.get(\"column\", 0)\n        self.nO = nO\n        self.nV = nV\n        \n        if seed is not None:\n            self.seed = seed\n        else:\n            self.seed = self.id",
  "def predict(self, ids):\n        if ids.ndim >= 2:\n            ids = self.ops.xp.ascontiguousarray(ids[:, self.column], dtype=\"uint64\")\n        keys = self.ops.hash(ids, self.seed) % self.nV\n        vectors = self.vectors[keys]\n        summed = vectors.sum(axis=1)\n        return summed",
  "def begin_update(self, ids, drop=0.0):\n        if ids.ndim >= 2:\n            ids = self.ops.xp.ascontiguousarray(ids[:, self.column], dtype=\"uint64\")\n        keys = self.ops.hash(ids, self.seed) % self.nV\n        vectors = self.vectors[keys].sum(axis=1)\n        mask = self.ops.get_dropout_mask((vectors.shape[1],), drop)\n        if mask is not None:\n            vectors *= mask\n\n        def finish_update(delta, sgd=None):\n            if mask is not None:\n                delta *= mask\n            keys = self.ops.hash(ids, self.seed) % self.nV\n            d_vectors = self.d_vectors\n            keys = self.ops.xp.ascontiguousarray(keys.T, dtype=\"i\")\n            for i in range(keys.shape[0]):\n                self.ops.scatter_add(d_vectors, keys[i], delta)\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return None\n\n        return vectors, finish_update",
  "def finish_update(delta, sgd=None):\n            if mask is not None:\n                delta *= mask\n            keys = self.ops.hash(ids, self.seed) % self.nV\n            d_vectors = self.d_vectors\n            keys = self.ops.xp.ascontiguousarray(keys.T, dtype=\"i\")\n            for i in range(keys.shape[0]):\n                self.ops.scatter_add(d_vectors, keys[i], delta)\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return None",
  "class ParametricAttention(Model):\n    \"\"\"Weight inputs by similarity to a learned vector\"\"\"\n\n    name = \"para-attn\"\n\n    def __init__(self, nO=None, hard=False, **kwargs):\n        Model.__init__(self, **kwargs)\n        self.nO = nO\n        self.hard = hard\n        self.drop_factor = kwargs.get(\"drop_factor\", 1.0)\n\n    def begin_update(self, Xs_lengths, drop=0.0):\n        Xs, lengths = Xs_lengths\n        attention, bp_attention = self._get_attention(self.Q, Xs, lengths)\n        output, bp_output = self._apply_attention(attention, Xs, lengths)\n\n        def attention_bwd(d_output, sgd=None):\n            dXs, d_attention = bp_output(d_output)\n            dQ, dXs2 = bp_attention(d_attention)\n            self.dQ += dQ\n            dXs += dXs2\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return dXs\n\n        return (output, lengths), attention_bwd\n\n    def _get_attention(self, Q, Xs, lengths):\n        attention = Xs.dot(Q)\n        if self.hard:\n            start = 0\n            for i, length in enumerate(lengths):\n                argmax = attention[start : start + length].argmax()\n                attention[start : start + length] = 0\n                attention[start + argmax] = 1.0\n                start += length\n        else:\n            attention = self.ops.softmax_sequences(attention, lengths)\n\n        def get_attention_bwd(d_attention):\n            if self.hard:\n                d_attention *= attention\n            else:\n                d_attention = self.ops.backprop_softmax_sequences(\n                    d_attention, attention, lengths\n                )\n            dQ = self.ops.gemm(Xs, d_attention, trans1=True)\n            dXs = self.ops.xp.outer(d_attention, Q)\n            return dQ, dXs\n\n        return attention, get_attention_bwd\n\n    def _apply_attention(self, attention, Xs, lengths):\n        output = Xs * attention\n\n        def apply_attention_bwd(d_output):\n            d_attention = (Xs * d_output).sum(axis=1, keepdims=True)\n            dXs = d_output * attention\n            return dXs, d_attention\n\n        return output, apply_attention_bwd",
  "def __init__(self, nO=None, hard=False, **kwargs):\n        Model.__init__(self, **kwargs)\n        self.nO = nO\n        self.hard = hard\n        self.drop_factor = kwargs.get(\"drop_factor\", 1.0)",
  "def begin_update(self, Xs_lengths, drop=0.0):\n        Xs, lengths = Xs_lengths\n        attention, bp_attention = self._get_attention(self.Q, Xs, lengths)\n        output, bp_output = self._apply_attention(attention, Xs, lengths)\n\n        def attention_bwd(d_output, sgd=None):\n            dXs, d_attention = bp_output(d_output)\n            dQ, dXs2 = bp_attention(d_attention)\n            self.dQ += dQ\n            dXs += dXs2\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return dXs\n\n        return (output, lengths), attention_bwd",
  "def _get_attention(self, Q, Xs, lengths):\n        attention = Xs.dot(Q)\n        if self.hard:\n            start = 0\n            for i, length in enumerate(lengths):\n                argmax = attention[start : start + length].argmax()\n                attention[start : start + length] = 0\n                attention[start + argmax] = 1.0\n                start += length\n        else:\n            attention = self.ops.softmax_sequences(attention, lengths)\n\n        def get_attention_bwd(d_attention):\n            if self.hard:\n                d_attention *= attention\n            else:\n                d_attention = self.ops.backprop_softmax_sequences(\n                    d_attention, attention, lengths\n                )\n            dQ = self.ops.gemm(Xs, d_attention, trans1=True)\n            dXs = self.ops.xp.outer(d_attention, Q)\n            return dQ, dXs\n\n        return attention, get_attention_bwd",
  "def _apply_attention(self, attention, Xs, lengths):\n        output = Xs * attention\n\n        def apply_attention_bwd(d_output):\n            d_attention = (Xs * d_output).sum(axis=1, keepdims=True)\n            dXs = d_output * attention\n            return dXs, d_attention\n\n        return output, apply_attention_bwd",
  "def attention_bwd(d_output, sgd=None):\n            dXs, d_attention = bp_output(d_output)\n            dQ, dXs2 = bp_attention(d_attention)\n            self.dQ += dQ\n            dXs += dXs2\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return dXs",
  "def get_attention_bwd(d_attention):\n            if self.hard:\n                d_attention *= attention\n            else:\n                d_attention = self.ops.backprop_softmax_sequences(\n                    d_attention, attention, lengths\n                )\n            dQ = self.ops.gemm(Xs, d_attention, trans1=True)\n            dXs = self.ops.xp.outer(d_attention, Q)\n            return dQ, dXs",
  "def apply_attention_bwd(d_output):\n            d_attention = (Xs * d_output).sum(axis=1, keepdims=True)\n            dXs = d_output * attention\n            return dXs, d_attention",
  "def set_compat_six_eight(flag_value):\n    \"\"\"Allow backwards compatibility with calculations bug from Thinc 6.8\"\"\"\n    global REPRODUCE_BUG\n    REPRODUCE_BUG = flag_value",
  "def _init_to_one(W, ops):\n    W.fill(1.0)",
  "def _run_child_hooks(model, X, y=None):\n    if model.child:\n        for hook in model.child.on_data_hooks:\n            hook(model.child, X, y)",
  "class LayerNorm(Model):\n    name = \"layernorm\"\n\n    def __init__(self, child=None, **kwargs):\n        self.child = child\n        if child is not None:\n            self._layers = [child]\n        else:\n            self._layers = []\n        Model.__init__(self, **kwargs)\n        if \"nO\" in kwargs:\n            self.nO = kwargs[\"nO\"]\n        elif getattr(child, \"nO\", None):\n            self.nO = child.nO\n        self.nr_upd = 0\n\n    def predict(self, X):\n        if self.child is not None:\n            X = self.child.predict(X)\n        N, mu, var = _get_moments(self.ops, X)\n        Xh = _forward(self.ops, X, mu, var)\n        y = Xh * self.G + self.b\n        return y\n\n    def begin_update(self, X, drop=0.0):\n        if self.child is not None:\n            X, backprop_child = self.child.begin_update(X, drop=0.0)\n        else:\n            backprop_child = None\n        N, mu, var = _get_moments(self.ops, X)\n\n        Xhat = _forward(self.ops, X, mu, var)\n\n        y, backprop_rescale = self._begin_update_scale_shift(Xhat)\n\n        def finish_update(dy, sgd=None):\n            dy = backprop_rescale(dy, sgd)\n            dist, sum_dy, sum_dy_dist = _get_d_moments(self.ops, dy, X, mu)\n            d_xhat = N * dy - sum_dy - dist * var ** (-1.0) * sum_dy_dist\n            d_xhat *= var ** (-1.0 / 2)\n            d_xhat /= N\n            if backprop_child is not None:\n                return backprop_child(d_xhat, sgd)\n            else:\n                return d_xhat\n\n        if drop is not None:\n            drop *= getattr(\n                self.child, \"drop_factor\", self.ops.asarray([1.0], dtype=\"f\")\n            )\n\n        y, bp_dropout = self.ops.dropout(y, drop)\n        assert y.dtype == \"float32\"\n\n        return y, bp_dropout(finish_update)\n\n    def _begin_update_scale_shift(self, input__BI):\n        def finish_update(gradient__BI, sgd=None):\n            self.d_b += gradient__BI.sum(axis=0)\n            d_G = self.d_G\n            d_G += (gradient__BI * input__BI).sum(axis=0)\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return gradient__BI * self.G\n\n        return input__BI * self.G + self.b, finish_update",
  "def _get_moments(ops, X):\n    if REPRODUCE_BUG:\n        return _get_moments_reproduce_bug(ops, X)\n    mu = X.mean(axis=1, keepdims=True)\n    var = X.var(axis=1, keepdims=True) + 1e-08\n    return ops.asarray([X.shape[1]], dtype=\"f\"), mu, var",
  "def _get_moments_reproduce_bug(ops, X):\n    \"\"\"Replicate bug from Thinc 6.8, for backwards compatibility.\"\"\"\n    mu = X.mean(axis=1, keepdims=True)\n    var = X.var(axis=1, keepdims=True) + 1e-08\n    return ops.asarray([X.shape[0]], dtype=\"f\"), mu, var",
  "def _get_d_moments(ops, dy, X, mu):\n    dist = X - mu\n    return (\n        dist,\n        ops.xp.sum(dy, axis=1, keepdims=True),\n        ops.xp.sum(dy * dist, axis=1, keepdims=True),\n    )",
  "def _forward(ops, X, mu, var):\n    return (X - mu) * var ** (-1.0 / 2.0)",
  "def __init__(self, child=None, **kwargs):\n        self.child = child\n        if child is not None:\n            self._layers = [child]\n        else:\n            self._layers = []\n        Model.__init__(self, **kwargs)\n        if \"nO\" in kwargs:\n            self.nO = kwargs[\"nO\"]\n        elif getattr(child, \"nO\", None):\n            self.nO = child.nO\n        self.nr_upd = 0",
  "def predict(self, X):\n        if self.child is not None:\n            X = self.child.predict(X)\n        N, mu, var = _get_moments(self.ops, X)\n        Xh = _forward(self.ops, X, mu, var)\n        y = Xh * self.G + self.b\n        return y",
  "def begin_update(self, X, drop=0.0):\n        if self.child is not None:\n            X, backprop_child = self.child.begin_update(X, drop=0.0)\n        else:\n            backprop_child = None\n        N, mu, var = _get_moments(self.ops, X)\n\n        Xhat = _forward(self.ops, X, mu, var)\n\n        y, backprop_rescale = self._begin_update_scale_shift(Xhat)\n\n        def finish_update(dy, sgd=None):\n            dy = backprop_rescale(dy, sgd)\n            dist, sum_dy, sum_dy_dist = _get_d_moments(self.ops, dy, X, mu)\n            d_xhat = N * dy - sum_dy - dist * var ** (-1.0) * sum_dy_dist\n            d_xhat *= var ** (-1.0 / 2)\n            d_xhat /= N\n            if backprop_child is not None:\n                return backprop_child(d_xhat, sgd)\n            else:\n                return d_xhat\n\n        if drop is not None:\n            drop *= getattr(\n                self.child, \"drop_factor\", self.ops.asarray([1.0], dtype=\"f\")\n            )\n\n        y, bp_dropout = self.ops.dropout(y, drop)\n        assert y.dtype == \"float32\"\n\n        return y, bp_dropout(finish_update)",
  "def _begin_update_scale_shift(self, input__BI):\n        def finish_update(gradient__BI, sgd=None):\n            self.d_b += gradient__BI.sum(axis=0)\n            d_G = self.d_G\n            d_G += (gradient__BI * input__BI).sum(axis=0)\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return gradient__BI * self.G\n\n        return input__BI * self.G + self.b, finish_update",
  "def finish_update(dy, sgd=None):\n            dy = backprop_rescale(dy, sgd)\n            dist, sum_dy, sum_dy_dist = _get_d_moments(self.ops, dy, X, mu)\n            d_xhat = N * dy - sum_dy - dist * var ** (-1.0) * sum_dy_dist\n            d_xhat *= var ** (-1.0 / 2)\n            d_xhat /= N\n            if backprop_child is not None:\n                return backprop_child(d_xhat, sgd)\n            else:\n                return d_xhat",
  "def finish_update(gradient__BI, sgd=None):\n            self.d_b += gradient__BI.sum(axis=0)\n            d_G = self.d_G\n            d_G += (gradient__BI * input__BI).sum(axis=0)\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return gradient__BI * self.G",
  "class ReLu(Affine):\n    @check.arg(1, has_shape((\"nB\", \"nI\")))\n    def predict(self, input__BI):\n        output__BO = Affine.predict(self, input__BI)\n        output__BO = self.ops.relu(output__BO, inplace=False)\n        return output__BO\n\n    @check.arg(1, has_shape((\"nB\", \"nI\")))\n    def begin_update(self, input__BI, drop=0.0):\n        output__BO, finish_affine = Affine.begin_update(self, input__BI, drop=0.0)\n        output__BO = self.ops.relu(output__BO)\n\n        @check.arg(0, has_shape((\"nB\", \"nO\")))\n        def finish_update(gradient, sgd=None):\n            gradient = self.ops.backprop_relu(gradient, output__BO)\n            return finish_affine(gradient, sgd)\n\n        dropped, bp_dropout = self.ops.dropout(output__BO, drop, inplace=False)\n        return dropped, bp_dropout(finish_update)",
  "def predict(self, input__BI):\n        output__BO = Affine.predict(self, input__BI)\n        output__BO = self.ops.relu(output__BO, inplace=False)\n        return output__BO",
  "def begin_update(self, input__BI, drop=0.0):\n        output__BO, finish_affine = Affine.begin_update(self, input__BI, drop=0.0)\n        output__BO = self.ops.relu(output__BO)\n\n        @check.arg(0, has_shape((\"nB\", \"nO\")))\n        def finish_update(gradient, sgd=None):\n            gradient = self.ops.backprop_relu(gradient, output__BO)\n            return finish_affine(gradient, sgd)\n\n        dropped, bp_dropout = self.ops.dropout(output__BO, drop, inplace=False)\n        return dropped, bp_dropout(finish_update)",
  "def finish_update(gradient, sgd=None):\n            gradient = self.ops.backprop_relu(gradient, output__BO)\n            return finish_affine(gradient, sgd)",
  "class Residual(Model):\n    def __init__(self, layer):\n        Model.__init__(self)\n        self._layers.append(layer)\n        self.on_data_hooks.append(on_data)\n\n    def predict(self, X):\n        Y = self._layers[0](X)\n        if isinstance(X, list) or isinstance(X, tuple):\n            return [X[i] + Y[i] for i in range(len(X))]\n        elif isinstance(X, tuple) and isinstance(Y, tuple) and len(X) == 2:\n            assert X[1].sum() == Y[1].sum()\n            assert Y[1].sum() == Y[0].shape[0], (Y[1].sum(), Y[0].shape[0])\n            return (X[0] + Y[0], Y[1])\n        else:\n            return X + Y\n\n    def begin_update(self, X, drop=0.0):\n        y, bp_y = self._layers[0].begin_update(X, drop=drop)\n        if isinstance(X, list):\n            output = [X[i] + y[i] for i in range(len(X))]\n        elif isinstance(X, tuple) and isinstance(y, tuple) and len(X) == 2:\n            # Handle case where we have (data, lengths) tuple\n            assert X[1].sum() == y[1].sum()\n            assert y[1].sum() == y[0].shape[0], (y[1].sum(), y[0].shape[0])\n            output = (X[0] + y[0], y[1])\n        else:\n            output = X + y\n\n        def residual_bwd(d_output, sgd=None):\n            dX = bp_y(d_output, sgd)\n            if isinstance(d_output, list) or isinstance(d_output, tuple):\n                return [d_output[i] + dX[i] for i in range(len(d_output))]\n            else:\n                return d_output + dX\n\n        return output, residual_bwd",
  "def on_data(self, X, y=None):\n    for layer in self._layers:\n        for hook in layer.on_data_hooks:\n            hook(layer, X, y)\n        if hasattr(layer, \"W\"):\n            layer.W.fill(0)",
  "def __init__(self, layer):\n        Model.__init__(self)\n        self._layers.append(layer)\n        self.on_data_hooks.append(on_data)",
  "def predict(self, X):\n        Y = self._layers[0](X)\n        if isinstance(X, list) or isinstance(X, tuple):\n            return [X[i] + Y[i] for i in range(len(X))]\n        elif isinstance(X, tuple) and isinstance(Y, tuple) and len(X) == 2:\n            assert X[1].sum() == Y[1].sum()\n            assert Y[1].sum() == Y[0].shape[0], (Y[1].sum(), Y[0].shape[0])\n            return (X[0] + Y[0], Y[1])\n        else:\n            return X + Y",
  "def begin_update(self, X, drop=0.0):\n        y, bp_y = self._layers[0].begin_update(X, drop=drop)\n        if isinstance(X, list):\n            output = [X[i] + y[i] for i in range(len(X))]\n        elif isinstance(X, tuple) and isinstance(y, tuple) and len(X) == 2:\n            # Handle case where we have (data, lengths) tuple\n            assert X[1].sum() == y[1].sum()\n            assert y[1].sum() == y[0].shape[0], (y[1].sum(), y[0].shape[0])\n            output = (X[0] + y[0], y[1])\n        else:\n            output = X + y\n\n        def residual_bwd(d_output, sgd=None):\n            dX = bp_y(d_output, sgd)\n            if isinstance(d_output, list) or isinstance(d_output, tuple):\n                return [d_output[i] + dX[i] for i in range(len(d_output))]\n            else:\n                return d_output + dX\n\n        return output, residual_bwd",
  "def residual_bwd(d_output, sgd=None):\n            dX = bp_y(d_output, sgd)\n            if isinstance(d_output, list) or isinstance(d_output, tuple):\n                return [d_output[i] + dX[i] for i in range(len(d_output))]\n            else:\n                return d_output + dX",
  "def _set_dimensions_if_needed(model, X, y=None):\n    if model.nV is None:\n        max_id = int(X.max()) + 1\n        if max_id >= 10000000:  # pragma: no cover\n            raise ValueError(\"TODO error --- really want us to make 1m vectors?\")\n        model.nV = max_id",
  "def _uniform_init(lo, hi):\n    def wrapped(W, ops):\n        copy_array(W, ops.xp.random.uniform(lo, hi, W.shape))\n\n    return wrapped",
  "def LSUVinit(model, X, y=None):\n    if model.vectors is not None and model.W is not None:\n        do_lsuv(model.ops, model.W, model, X)\n    return X",
  "class Embed(Model):\n    name = \"embed\"\n\n    # @property\n    # def input_shape(self):\n    #    return (self.nB,)\n\n    # @property\n    # def output_shape(self):\n    #    return (self.nB, self.nO)\n\n    @check.arg(1, is_int)\n    def __init__(self, nO, nM=None, nV=None, **kwargs):\n        Model.__init__(self, **kwargs)\n        self.is_static = kwargs.get(\"is_static\", False)\n        self.column = kwargs.get(\"column\", 0)\n        self.nO = nO\n        self.nM = nM\n        self.nV = nV\n\n    # @check.arg(1, is_int_array)\n    def predict(self, ids):\n        if ids.ndim == 2:\n            ids = ids[:, self.column]\n        if len(ids) < 1000 or isinstance(self.ops, CupyOps):\n            vectors = self._embed(ids)\n            dotted = self.ops.batch_dot(vectors, self.W)\n            return dotted\n        uniques, positions = self.ops.xp.unique(ids, return_inverse=True)\n        vectors = self._embed(uniques)\n        dotted_uniq = self.ops.gemm(vectors, self.W, trans2=True)\n        output = dotted_uniq[positions]\n        return self.ops.xp.ascontiguousarray(output)\n\n    def begin_update(self, ids, drop=0.0):\n        if ids.ndim == 2:\n            ids = ids[:, self.column]\n        mask = self.ops.get_dropout_mask(ids.shape[0], drop)\n        if mask is not None:\n            ids = ids * (mask > 0)\n        vectors = self._embed(ids)\n        dotted = self.ops.gemm(vectors, self.W, trans2=True)\n\n        def finish_update(gradients, sgd=None):\n            self.d_W += self.ops.gemm(gradients, vectors, trans1=True)\n            if not self.is_static:\n                gradients = self.ops.gemm(gradients, self.W)\n                d_vectors = self.d_vectors\n                if hasattr(self.ops.xp, \"scatter_add\"):\n                    self.ops.xp.scatter_add(d_vectors, ids % self.nV, gradients)\n                else:\n                    self.ops.xp.add.at(d_vectors, ids % self.nV, gradients)\n            if sgd is not None:\n                if self.is_static:\n                    sgd(self.W.ravel(), self.d_W.ravel(), key=self.id)\n                else:\n                    sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return None\n\n        return dotted, finish_update\n\n    @contextlib.contextmanager\n    def use_params(self, params):\n        if self.is_static:\n            yield\n        else:\n            backup = None\n            weights = self._mem.weights\n            if self.id in params:\n                param = params[self.id]\n                backup = weights.copy()\n                weights[:] = param\n            yield\n            if backup is not None:\n                weights[:] = backup\n\n    def _embed(self, ids):\n        vectors = self.vectors\n        return vectors[ids % self.nV]",
  "def wrapped(W, ops):\n        copy_array(W, ops.xp.random.uniform(lo, hi, W.shape))",
  "def __init__(self, nO, nM=None, nV=None, **kwargs):\n        Model.__init__(self, **kwargs)\n        self.is_static = kwargs.get(\"is_static\", False)\n        self.column = kwargs.get(\"column\", 0)\n        self.nO = nO\n        self.nM = nM\n        self.nV = nV",
  "def predict(self, ids):\n        if ids.ndim == 2:\n            ids = ids[:, self.column]\n        if len(ids) < 1000 or isinstance(self.ops, CupyOps):\n            vectors = self._embed(ids)\n            dotted = self.ops.batch_dot(vectors, self.W)\n            return dotted\n        uniques, positions = self.ops.xp.unique(ids, return_inverse=True)\n        vectors = self._embed(uniques)\n        dotted_uniq = self.ops.gemm(vectors, self.W, trans2=True)\n        output = dotted_uniq[positions]\n        return self.ops.xp.ascontiguousarray(output)",
  "def begin_update(self, ids, drop=0.0):\n        if ids.ndim == 2:\n            ids = ids[:, self.column]\n        mask = self.ops.get_dropout_mask(ids.shape[0], drop)\n        if mask is not None:\n            ids = ids * (mask > 0)\n        vectors = self._embed(ids)\n        dotted = self.ops.gemm(vectors, self.W, trans2=True)\n\n        def finish_update(gradients, sgd=None):\n            self.d_W += self.ops.gemm(gradients, vectors, trans1=True)\n            if not self.is_static:\n                gradients = self.ops.gemm(gradients, self.W)\n                d_vectors = self.d_vectors\n                if hasattr(self.ops.xp, \"scatter_add\"):\n                    self.ops.xp.scatter_add(d_vectors, ids % self.nV, gradients)\n                else:\n                    self.ops.xp.add.at(d_vectors, ids % self.nV, gradients)\n            if sgd is not None:\n                if self.is_static:\n                    sgd(self.W.ravel(), self.d_W.ravel(), key=self.id)\n                else:\n                    sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return None\n\n        return dotted, finish_update",
  "def use_params(self, params):\n        if self.is_static:\n            yield\n        else:\n            backup = None\n            weights = self._mem.weights\n            if self.id in params:\n                param = params[self.id]\n                backup = weights.copy()\n                weights[:] = param\n            yield\n            if backup is not None:\n                weights[:] = backup",
  "def _embed(self, ids):\n        vectors = self.vectors\n        return vectors[ids % self.nV]",
  "def finish_update(gradients, sgd=None):\n            self.d_W += self.ops.gemm(gradients, vectors, trans1=True)\n            if not self.is_static:\n                gradients = self.ops.gemm(gradients, self.W)\n                d_vectors = self.d_vectors\n                if hasattr(self.ops.xp, \"scatter_add\"):\n                    self.ops.xp.scatter_add(d_vectors, ids % self.nV, gradients)\n                else:\n                    self.ops.xp.add.at(d_vectors, ids % self.nV, gradients)\n            if sgd is not None:\n                if self.is_static:\n                    sgd(self.W.ravel(), self.d_W.ravel(), key=self.id)\n                else:\n                    sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return None",
  "def minibatch(train_X, train_y, size=16, nr_update=1000):\n    with tqdm.tqdm(total=nr_update * size, leave=False) as pbar:\n        while nr_update >= 0:\n            indices = numpy.arange(len(train_X))\n            numpy.random.shuffle(indices)\n            j = 0\n            while j < indices.shape[0]:\n                slice_ = indices[j : j + size]\n                X = _take_slice(train_X, slice_)\n                y = _take_slice(train_y, slice_)\n                yield X, y\n                j += size\n                nr_update -= 1\n                if nr_update <= 0:\n                    break\n                pbar.update(size)",
  "def _take_slice(data, slice_):\n    if isinstance(data, list) or isinstance(data, tuple):\n        return [data[int(i)] for i in slice_]\n    else:\n        return data[slice_]",
  "class BestFirstFinder(object):\n    def __init__(self, **param_values):\n        self.queue = []\n        self.limit = 16\n        self.params = param_values\n        self.best_acc = 0.0\n        self.best_i = 0\n        self.i = 0\n        self.j = 0\n        self.best_model = None\n        self.temperature = 0.0\n\n    @property\n    def configs(self):\n        keys, value_groups = zip(*self.params.items())\n        for values in itertools.product(*value_groups):\n            config = dict(zip(keys, values))\n            yield config\n\n    def enqueue(self, model, train_acc, check_acc):\n        fom = check_acc * min(check_acc / train_acc, 1.0)\n        self.queue.append([fom, self.i, 0, model])\n        if check_acc >= self.best_acc:\n            self.best_acc = check_acc\n            self.best_i = self.i\n            self.best_model = model\n            self.temperature = 0.0\n        else:\n            self.temperature += 0.01\n        self.j = 0\n        self.queue.sort(reverse=True)\n        self.queue = self.queue[: self.limit]\n\n    def __iter__(self):\n        self.queue.sort(reverse=True)\n        self.queue = self.queue[: self.limit]\n        for i in range(len(self.queue)):\n            self.queue[i][0] = self.queue[i][0] - 0.01\n            self.queue[i][-1][2][\"parent\"] = self.queue[i][2]\n            self.queue[i][2] += 1\n            yield self.queue[i][-1]\n\n    @property\n    def best(self):\n        return self.best_model",
  "def resample_hyper_params(hparams, temperature):\n    hparams = dict(hparams)\n    hparams[\"epochs\"] = hparams.get(\"epochs\", 0) + 1\n    hparams[\"learn_rate\"] = resample(hparams[\"learn_rate\"], 1e-6, 0.1, temperature)\n    # hparams['beta1'] = resample(hparams.get('beta1', 0.9), 0.8, 1.0, temperature)\n    # hparams['beta2'] = resample(hparams.get('beta2', 0.9), 0.8, 1.0, temperature)\n    # hparams['L2'] = resample(hparams['L2'], 0.0, 1e-3, temperature)\n    # hparams['batch_size'] = int(resample(hparams['batch_size'], 10, 256, temperature))\n    # hparams['dropout'] = resample(hparams['dropout'], 0.05, 0.7, temperature)\n    return hparams",
  "def resample(curr, min_, max_, temperature):\n    if temperature == 0.0:\n        return curr\n    scale = (max_ - min_) * temperature\n    next_ = numpy.random.normal(loc=curr, scale=scale)\n    return min(max_, max(min_, next_))",
  "def train_epoch(\n    model, sgd, hparams, train_X, train_y, dev_X, dev_y, device_id=-1, temperature=0.0\n):\n    model, sgd, hparams = srsly.pickle_loads(srsly.pickle_dumps((model, sgd, hparams)))\n    if device_id >= 0:\n        model.to_gpu(device_id)\n        sgd.ops = model.ops\n        sgd.to_gpu()\n        if isinstance(train_y, numpy.ndarray):\n            train_y = model.ops.asarray(train_y)\n            dev_y = model.ops.asarray(dev_y)\n    hparams = resample_hyper_params(hparams, temperature)\n    sgd.learn_rate = hparams[\"learn_rate\"]\n    sgd.beta1 = hparams[\"beta1\"]\n    sgd.beta2 = hparams[\"beta2\"]\n    sgd.L2 = hparams[\"L2\"]\n    train_acc = 0.0\n    train_n = 0\n    for X, y in minibatch(\n        train_X, train_y, size=hparams[\"batch_size\"], nr_update=hparams[\"nr_update\"]\n    ):\n        yh, finish_update = model.begin_update(X, drop=hparams[\"dropout\"])\n        if hasattr(y, \"shape\"):\n            dy = (yh - y) / y.shape[0]\n            train_acc += (y.argmax(axis=1) == yh.argmax(axis=1)).sum()\n            train_n += y.shape[0]\n        else:\n            n_y = sum(len(y_i) for y_i in y)\n            dy = [(yh[i] - y[i]) / n_y for i in range(len(yh))]\n            for i in range(len(y)):\n                train_acc += (y[i].argmax(axis=1) == yh[i].argmax(axis=1)).sum()\n            train_n += n_y\n        finish_update(dy, sgd=sgd)\n    train_acc /= train_n\n    with model.use_params(sgd.averages):\n        dev_acc = model.evaluate(dev_X, dev_y)\n    model.to_cpu()\n    sgd.to_cpu()\n    return device_id, ((model, sgd, hparams), float(train_acc), float(dev_acc))",
  "class DevicePool(object):\n    \"\"\"Synchronize GPU usage\"\"\"\n\n    def __init__(self, n):\n        self.devices = {i: None for i in range(n)}\n\n    def acquire(self):\n        for i, device in self.devices.items():\n            if device is None:\n                self.devices[i] = True\n                return i\n        else:\n            return None\n\n    def release(self, i):\n        if i in self.devices:\n            self.devices[i] = None",
  "def __init__(self, **param_values):\n        self.queue = []\n        self.limit = 16\n        self.params = param_values\n        self.best_acc = 0.0\n        self.best_i = 0\n        self.i = 0\n        self.j = 0\n        self.best_model = None\n        self.temperature = 0.0",
  "def configs(self):\n        keys, value_groups = zip(*self.params.items())\n        for values in itertools.product(*value_groups):\n            config = dict(zip(keys, values))\n            yield config",
  "def enqueue(self, model, train_acc, check_acc):\n        fom = check_acc * min(check_acc / train_acc, 1.0)\n        self.queue.append([fom, self.i, 0, model])\n        if check_acc >= self.best_acc:\n            self.best_acc = check_acc\n            self.best_i = self.i\n            self.best_model = model\n            self.temperature = 0.0\n        else:\n            self.temperature += 0.01\n        self.j = 0\n        self.queue.sort(reverse=True)\n        self.queue = self.queue[: self.limit]",
  "def __iter__(self):\n        self.queue.sort(reverse=True)\n        self.queue = self.queue[: self.limit]\n        for i in range(len(self.queue)):\n            self.queue[i][0] = self.queue[i][0] - 0.01\n            self.queue[i][-1][2][\"parent\"] = self.queue[i][2]\n            self.queue[i][2] += 1\n            yield self.queue[i][-1]",
  "def best(self):\n        return self.best_model",
  "def __init__(self, n):\n        self.devices = {i: None for i in range(n)}",
  "def acquire(self):\n        for i, device in self.devices.items():\n            if device is None:\n                self.devices[i] = True\n                return i\n        else:\n            return None",
  "def release(self, i):\n        if i in self.devices:\n            self.devices[i] = None",
  "def ancora_pos_tags(encode_words=False):  # pragma: no cover\n    data_dir = get_file(\"UD_Spanish-AnCora-r1.4\", ANCORA_1_4_ZIP, unzip=True)\n    train_loc = os.path.join(data_dir, \"es_ancora-ud-train.conllu\")\n    dev_loc = os.path.join(data_dir, \"es_ancora-ud-dev.conllu\")\n    return ud_pos_tags(train_loc, dev_loc, encode_words=encode_words)",
  "def ewtb_pos_tags(encode_tags=False, encode_words=False):  # pragma: no cover\n    data_dir = get_file(\"UD_English-EWT-r1.4\", EWTB_1_4_ZIP, unzip=True)\n    train_loc = os.path.join(data_dir, \"en-ud-train.conllu\")\n    dev_loc = os.path.join(data_dir, \"en-ud-dev.conllu\")\n    return ud_pos_tags(\n        train_loc, dev_loc, encode_tags=encode_tags, encode_words=encode_words\n    )",
  "def ud_pos_tags(\n    train_loc, dev_loc, encode_tags=True, encode_words=True\n):  # pragma: no cover\n    train_sents = list(read_conll(train_loc))\n    dev_sents = list(read_conll(dev_loc))\n    tagmap = {}\n    freqs = Counter()\n    for words, tags in train_sents:\n        for tag in tags:\n            tagmap.setdefault(tag, len(tagmap))\n        for word in words:\n            freqs[word] += 1\n    vocab = {\n        word: i for i, (word, freq) in enumerate(freqs.most_common()) if (freq >= 5)\n    }\n\n    def _encode(sents):\n        X = []\n        y = []\n        for words, tags in sents:\n            if encode_words:\n                X.append(\n                    numpy.asarray(\n                        [vocab.get(word, len(vocab)) for word in words], dtype=\"uint64\"\n                    )\n                )\n            else:\n                X.append(words)\n            if encode_tags:\n                y.append(numpy.asarray([tagmap[tag] for tag in tags], dtype=\"int32\"))\n            else:\n                y.append(tags)\n        return zip(X, y)\n\n    return _encode(train_sents), _encode(dev_sents), len(tagmap)",
  "def imdb(loc=None, limit=0):\n    if loc is None:\n        loc = get_file(\"aclImdb\", IMDB_URL, untar=True, unzip=True)\n    train_loc = Path(loc) / \"train\"\n    test_loc = Path(loc) / \"test\"\n    return read_imdb(train_loc, limit=limit), read_imdb(test_loc, limit=limit)",
  "def read_wikiner(file_, tagmap=None):\n    Xs = []\n    ys = []\n    for line in file_:\n        if not line.strip():\n            continue\n        tokens = [t.rsplit(\"|\", 2) for t in line.split()]\n        words, _, tags = zip(*tokens)\n        if tagmap is not None:\n            tags = [tagmap.setdefault(tag, len(tagmap)) for tag in tags]\n        Xs.append(words)\n        ys.append(tags)\n    return zip(Xs, ys)",
  "def read_imdb(data_dir, limit=0):\n    examples = []\n    for subdir, label in ((\"pos\", 1), (\"neg\", 0)):\n        for filename in (data_dir / subdir).iterdir():\n            with filename.open(\"r\", encoding=\"utf8\") as file_:\n                text = file_.read()\n            text = text.replace(\"<br />\", \"\\n\\n\")\n            if text.strip():\n                examples.append((text, label))\n    random.shuffle(examples)\n    if limit >= 1:\n        examples = examples[:limit]\n    return examples",
  "def read_conll(loc):  # pragma: no cover\n    with io.open(loc, encoding=\"utf8\") as file_:\n        sent_strs = file_.read().strip().split(\"\\n\\n\")\n    for sent_str in sent_strs:\n        lines = [\n            line.split() for line in sent_str.split(\"\\n\") if not line.startswith(\"#\")\n        ]\n        words = []\n        tags = []\n        for i, pieces in enumerate(lines):\n            if len(pieces) == 4:\n                word, pos, head, label = pieces\n            else:\n                idx, word, lemma, pos1, pos, morph, head, label, _, _2 = pieces\n            if \"-\" in idx:\n                continue\n            words.append(word)\n            tags.append(pos)\n        yield words, tags",
  "def read_csv(csv_loc, label_col=0, text_col=-1):\n    with csv_loc.open() as file_:\n        for row in csv.reader(file_):\n            label_str = row[label_col]\n            text = row[text_col]\n            yield text, label_str",
  "def mnist():  # pragma: no cover\n    from ._vendorized.keras_datasets import load_mnist\n\n    # the data, shuffled and split between tran and test sets\n    (X_train, y_train), (X_test, y_test) = load_mnist()\n\n    X_train = X_train.reshape(60000, 784)\n    X_test = X_test.reshape(10000, 784)\n    X_train = X_train.astype(\"float32\")\n    X_test = X_test.astype(\"float32\")\n\n    X_train /= 255.0\n    X_test /= 255.0\n    train_data = list(zip(X_train, y_train))\n    nr_train = X_train.shape[0]\n    random.shuffle(train_data)\n    heldout_data = train_data[: int(nr_train * 0.1)]\n    train_data = train_data[len(heldout_data) :]\n    test_data = list(zip(X_test, y_test))\n    return train_data, heldout_data, test_data",
  "def reuters():  # pragma: no cover\n    from ._vendorized.keras_datasets import load_reuters\n\n    (X_train, y_train), (X_test, y_test) = load_reuters()\n    return (X_train, y_train), (X_test, y_test)",
  "def quora_questions(loc=None):\n    if loc is None:\n        loc = get_file(\"quora_similarity.tsv\", QUORA_QUESTIONS_URL)\n    if isinstance(loc, basestring):\n        loc = Path(loc)\n    is_header = True\n    lines = []\n    with loc.open(\"r\", encoding=\"utf8\") as file_:\n        for row in csv.reader(file_, delimiter=\"\\t\"):\n            if is_header:\n                is_header = False\n                continue\n            id_, qid1, qid2, sent1, sent2, is_duplicate = row\n            if not isinstance(sent1, unicode):\n                sent1 = sent1.decode(\"utf8\").strip()\n            if not isinstance(sent2, unicode):\n                sent2 = sent2.decode(\"utf8\").strip()\n            if sent1 and sent2:\n                lines.append(((sent1, sent2), int(is_duplicate)))\n    train, dev = partition(lines, 0.9)\n    return train, dev",
  "def snli(loc=None, ternary=False):\n    label_scheme = THREE_LABELS if ternary else TWO_LABELS\n    if loc is None:\n        loc = get_file(\"snli_1.0\", SNLI_URL, unzip=True)\n    if isinstance(loc, basestring):\n        loc = Path(loc)\n\n    train = read_snli(Path(loc) / \"snli_1.0_train.jsonl\", label_scheme)\n    dev = read_snli(Path(loc) / \"snli_1.0_dev.jsonl\", label_scheme)\n    return train, dev",
  "def stack_exchange(loc=None):\n    if loc is None:\n        raise ValueError(\"No default path for Stack Exchange yet\")\n    rows = []\n    with loc.open(\"r\", encoding=\"utf8\") as file_:\n        for line in file_:\n            eg = json.loads(line)\n            rows.append(((eg[\"text1\"], eg[\"text2\"]), int(eg[\"label\"])))\n    train, dev = partition(rows, 0.7)\n    return train, dev",
  "def read_snli(loc, label_scheme):\n    rows = []\n    with loc.open(\"r\", encoding=\"utf8\") as file_:\n        for line in file_:\n            eg = json.loads(line)\n            label = eg[\"gold_label\"]\n            if label == \"-\":\n                continue\n            rows.append(((eg[\"sentence1\"], eg[\"sentence2\"]), label_scheme[label]))\n    return rows",
  "def get_word_index(path=\"reuters_word_index.pkl\"):  # pragma: no cover\n    path = get_file(\n        path, origin=\"https://s3.amazonaws.com/text-datasets/reuters_word_index.pkl\"\n    )\n    f = open(path, \"rb\")\n\n    if sys.version_info < (3,):\n        data = pickle.load(f)\n    else:\n        data = pickle.load(f, encoding=\"latin1\")\n\n    f.close()\n    return data",
  "def _encode(sents):\n        X = []\n        y = []\n        for words, tags in sents:\n            if encode_words:\n                X.append(\n                    numpy.asarray(\n                        [vocab.get(word, len(vocab)) for word in words], dtype=\"uint64\"\n                    )\n                )\n            else:\n                X.append(words)\n            if encode_tags:\n                y.append(numpy.asarray([tagmap[tag] for tag in tags], dtype=\"int32\"))\n            else:\n                y.append(tags)\n        return zip(X, y)",
  "def visualize_attention(x, y, weights, layer=\"Encoder\", self_attn=True):\n    \"\"\"\n        Visualize self/outer attention\n        Args:\n            x: sentence\n            y: sentence\n            weights: (nH, nL, nL)\n    \"\"\"\n\n    def heatmap(x, y, data, ax):\n        seaborn.heatmap(\n            data,\n            square=True,\n            xticklabels=y,\n            yticklabels=x,\n            vmin=0.0,\n            vmax=1.0,\n            cbar_kws=dict(use_gridspec=False, location=\"top\"),\n            ax=ax,\n        )\n\n    num = min(weights.shape[0], 4)\n    fig, axs = plt.subplots(1, num)\n    attn_type = \"self attention\" if self_attn else \"outer attention\"\n    fig.suptitle(\"{} {} for all the heads\".format(layer, attn_type))\n    if len(weights.shape) == 3:\n        for i in range(num):\n            heatmap(x, y, weights[i], axs[i])\n    else:\n        raise ValueError(\"Wrong input weights dimensions\")\n    plt.show()",
  "def heatmap(x, y, data, ax):\n        seaborn.heatmap(\n            data,\n            square=True,\n            xticklabels=y,\n            yticklabels=x,\n            vmin=0.0,\n            vmax=1.0,\n            cbar_kws=dict(use_gridspec=False, location=\"top\"),\n            ax=ax,\n        )",
  "def xp2torch(xp_tensor):\n    if hasattr(xp_tensor, \"toDlpack\"):\n        return torch.utils.dlpack.from_dlpack(xp_tensor.toDlpack())\n    else:\n        return torch.from_numpy(xp_tensor)",
  "def torch2xp(torch_tensor):\n    if torch_tensor.is_cuda:\n        return cupy.fromDlpack(torch.utils.dlpack.to_dlpack(torch_tensor))\n    else:\n        return torch_tensor.detach().numpy()",
  "class PyTorchWrapper(Model):\n    \"\"\"Wrap a PyTorch model, so that it has the same API as Thinc models.\n    To optimize the model, you'll need to create a PyTorch optimizer and call\n    optimizer.step() after each batch --- see examples/wrap_pytorch.py\n    \"\"\"\n\n    def __init__(self, model):\n        Model.__init__(self)\n        self._model = model\n        self._optimizer = None\n\n    def prepare_input(self, x_data, is_update=True):\n        x_var = torch.autograd.Variable(xp2torch(x_data), requires_grad=is_update)\n        return (x_var,), {}\n\n    def prepare_output(self, y_var):\n        return torch2xp(y_var)\n\n    def prepare_backward_input(self, dy_data, y_var):\n        dy_var = xp2torch(dy_data)\n        return (y_var,), {\"grad_tensors\": (dy_var,)}\n\n    def prepare_backward_output(self, x_args, x_kwargs):\n        x_var = x_args[0]\n        return torch2xp(x_var.grad)\n\n    def predict(self, x_data):\n        self._model.eval()\n        x_args, x_kwargs = self.prepare_input(x_data, is_update=False)\n        with torch.no_grad():\n            y_var = self._model(*x_args, **x_kwargs)\n        self._model.train()\n        return self.prepare_output(y_var)\n\n    def begin_update(self, x_data, drop=0.0):\n        \"\"\"Return the output of the wrapped PyTorch model for the given input,\n        along with a callback to handle the backward pass.\n        \"\"\"\n        if drop is None:\n            return self.predict(x_data), None\n        self._model.train()\n        fwd_args, fwd_kwargs = self.prepare_input(x_data, is_update=True)\n        y_var = self._model(*fwd_args, **fwd_kwargs)\n        y = self.prepare_output(y_var)\n\n        def backward_pytorch(dy_data, sgd=None):\n            d_args, d_kwargs = self.prepare_backward_input(dy_data, y_var)\n            torch.autograd.backward(*d_args, **d_kwargs)\n            if sgd is not None:\n                if self._optimizer is None:\n                    self._optimizer = self._create_optimizer(sgd)\n                self._optimizer.step()\n                self._optimizer.zero_grad()\n            return self.prepare_backward_output(fwd_args, fwd_kwargs)\n\n        return y, backward_pytorch\n\n    def _create_optimizer(self, sgd):\n        params = self._model.parameters()\n        if sgd.b1 != 0 and sgd.b2 != 0:\n            optimizer = torch.optim.Adam(params, lr=sgd.alpha, betas=(sgd.b1, sgd.b2))\n        elif sgd.b2 == 0:\n            optimizer = torch.optim.SGD(params, lr=sgd.alpha, momentum=sgd.b1)\n        else:\n            raise NotImplementedError\n        return optimizer\n\n    @contextlib.contextmanager\n    def use_params(self, params):\n        key_prefix = f\"pytorch_{self.id}_\"\n        state_dict = {}\n        for k, v in params.items():\n            if hasattr(k, \"startswith\") and k.startswith(key_prefix):\n                state_dict[k.replace(key_prefix, \"\")] = xp2torch(v)\n        if state_dict:\n            backup = {k: v.clone() for k, v in self._model.state_dict().items()}\n            self._model.load_state_dict(state_dict)\n            yield\n            self._model.load_state_dict(backup)\n        else:\n            yield\n\n    def _update_pytorch_averages(self, sgd, *, init_steps=1):\n        if getattr(sgd, \"averages\", None) is None:\n            return\n        # Collect parameters if we don't have them\n        for name, param in self._model.state_dict().items():\n            key = f\"pytorch_{self.id}_{name}\"\n            sgd.nr_update[key] += 1\n            xp_param = torch2xp(param)\n            if key in sgd.averages:\n                self.ops.update_averages(\n                    sgd.averages[key], xp_param, sgd.nr_update[key]\n                )\n            else:\n                sgd.averages[key] = xp_param.copy()\n                sgd.nr_update[key] = init_steps\n\n    def to_disk(self, path):\n        torch.save(self._model.state_dict(), str(path))\n\n    def from_disk(self, path):\n        if self.ops.device == \"cpu\":\n            map_location = \"cpu\"\n        else:\n            device_id = torch.cuda.current_device()\n            map_location = \"cuda:%d\" % device_id\n        self._model.load_state_dict(torch.load(path, map_location=map_location))\n        self._model.to(map_location)\n\n    def to_bytes(self):\n        filelike = BytesIO()\n        torch.save(self._model.state_dict(), filelike)\n        filelike.seek(0)\n        return filelike.getvalue()\n\n    def from_bytes(self, data):\n        filelike = BytesIO(data)\n        filelike.seek(0)\n        if self.ops.device == \"cpu\":\n            map_location = \"cpu\"\n        else:\n            device_id = torch.cuda.current_device()\n            map_location = \"cuda:%d\" % device_id\n        self._model.load_state_dict(torch.load(filelike, map_location=map_location))\n        self._model.to(map_location)\n\n    def to_gpu(self, device_num):\n        self._model.cuda(device_num)\n\n    def to_cpu(self):\n        self._model.cpu()\n\n    def resize_output(self, new_dim):\n        # self.weight = nn.Parameter(F.pad(self.weight, ...)) # add classes\n        # self.weight = nn.Parameter(F.pad(model.weight, ...)) # add classes\n        raise NotImplementedError\n\n    def resize_input(self):\n        raise NotImplementedError",
  "class PyTorchWrapperRNN(PyTorchWrapper):\n    \"\"\"Wrap a PyTorch RNN model\"\"\"\n\n    def prepare_input(self, inputs, is_update=False):\n        if isinstance(inputs, tuple):\n            x_data, h_0 = inputs\n        else:\n            x_data = inputs\n            h_0 = None\n        x_var = torch.autograd.Variable(xp2torch(x_data), requires_grad=is_update)\n        return (x_var, h_0), {}\n\n    def prepare_output(self, torch_outputs):\n        y_var, h_n = torch_outputs\n        return torch2xp(y_var), h_n\n\n    def prepare_backward_input(self, dy_data, y_var):\n        dy, _ = dy_data\n        dy_var = xp2torch(dy)\n        y_var, _ = y_var\n        return (y_var,), {\"grad_tensors\": (dy_var,)}\n\n    def prepare_backward_output(self, x_args, x_kwargs):\n        x_var, _ = x_args\n        return torch2xp(x_var.grad)",
  "def __init__(self, model):\n        Model.__init__(self)\n        self._model = model\n        self._optimizer = None",
  "def prepare_input(self, x_data, is_update=True):\n        x_var = torch.autograd.Variable(xp2torch(x_data), requires_grad=is_update)\n        return (x_var,), {}",
  "def prepare_output(self, y_var):\n        return torch2xp(y_var)",
  "def prepare_backward_input(self, dy_data, y_var):\n        dy_var = xp2torch(dy_data)\n        return (y_var,), {\"grad_tensors\": (dy_var,)}",
  "def prepare_backward_output(self, x_args, x_kwargs):\n        x_var = x_args[0]\n        return torch2xp(x_var.grad)",
  "def predict(self, x_data):\n        self._model.eval()\n        x_args, x_kwargs = self.prepare_input(x_data, is_update=False)\n        with torch.no_grad():\n            y_var = self._model(*x_args, **x_kwargs)\n        self._model.train()\n        return self.prepare_output(y_var)",
  "def begin_update(self, x_data, drop=0.0):\n        \"\"\"Return the output of the wrapped PyTorch model for the given input,\n        along with a callback to handle the backward pass.\n        \"\"\"\n        if drop is None:\n            return self.predict(x_data), None\n        self._model.train()\n        fwd_args, fwd_kwargs = self.prepare_input(x_data, is_update=True)\n        y_var = self._model(*fwd_args, **fwd_kwargs)\n        y = self.prepare_output(y_var)\n\n        def backward_pytorch(dy_data, sgd=None):\n            d_args, d_kwargs = self.prepare_backward_input(dy_data, y_var)\n            torch.autograd.backward(*d_args, **d_kwargs)\n            if sgd is not None:\n                if self._optimizer is None:\n                    self._optimizer = self._create_optimizer(sgd)\n                self._optimizer.step()\n                self._optimizer.zero_grad()\n            return self.prepare_backward_output(fwd_args, fwd_kwargs)\n\n        return y, backward_pytorch",
  "def _create_optimizer(self, sgd):\n        params = self._model.parameters()\n        if sgd.b1 != 0 and sgd.b2 != 0:\n            optimizer = torch.optim.Adam(params, lr=sgd.alpha, betas=(sgd.b1, sgd.b2))\n        elif sgd.b2 == 0:\n            optimizer = torch.optim.SGD(params, lr=sgd.alpha, momentum=sgd.b1)\n        else:\n            raise NotImplementedError\n        return optimizer",
  "def use_params(self, params):\n        key_prefix = f\"pytorch_{self.id}_\"\n        state_dict = {}\n        for k, v in params.items():\n            if hasattr(k, \"startswith\") and k.startswith(key_prefix):\n                state_dict[k.replace(key_prefix, \"\")] = xp2torch(v)\n        if state_dict:\n            backup = {k: v.clone() for k, v in self._model.state_dict().items()}\n            self._model.load_state_dict(state_dict)\n            yield\n            self._model.load_state_dict(backup)\n        else:\n            yield",
  "def _update_pytorch_averages(self, sgd, *, init_steps=1):\n        if getattr(sgd, \"averages\", None) is None:\n            return\n        # Collect parameters if we don't have them\n        for name, param in self._model.state_dict().items():\n            key = f\"pytorch_{self.id}_{name}\"\n            sgd.nr_update[key] += 1\n            xp_param = torch2xp(param)\n            if key in sgd.averages:\n                self.ops.update_averages(\n                    sgd.averages[key], xp_param, sgd.nr_update[key]\n                )\n            else:\n                sgd.averages[key] = xp_param.copy()\n                sgd.nr_update[key] = init_steps",
  "def to_disk(self, path):\n        torch.save(self._model.state_dict(), str(path))",
  "def from_disk(self, path):\n        if self.ops.device == \"cpu\":\n            map_location = \"cpu\"\n        else:\n            device_id = torch.cuda.current_device()\n            map_location = \"cuda:%d\" % device_id\n        self._model.load_state_dict(torch.load(path, map_location=map_location))\n        self._model.to(map_location)",
  "def to_bytes(self):\n        filelike = BytesIO()\n        torch.save(self._model.state_dict(), filelike)\n        filelike.seek(0)\n        return filelike.getvalue()",
  "def from_bytes(self, data):\n        filelike = BytesIO(data)\n        filelike.seek(0)\n        if self.ops.device == \"cpu\":\n            map_location = \"cpu\"\n        else:\n            device_id = torch.cuda.current_device()\n            map_location = \"cuda:%d\" % device_id\n        self._model.load_state_dict(torch.load(filelike, map_location=map_location))\n        self._model.to(map_location)",
  "def to_gpu(self, device_num):\n        self._model.cuda(device_num)",
  "def to_cpu(self):\n        self._model.cpu()",
  "def resize_output(self, new_dim):\n        # self.weight = nn.Parameter(F.pad(self.weight, ...)) # add classes\n        # self.weight = nn.Parameter(F.pad(model.weight, ...)) # add classes\n        raise NotImplementedError",
  "def resize_input(self):\n        raise NotImplementedError",
  "def prepare_input(self, inputs, is_update=False):\n        if isinstance(inputs, tuple):\n            x_data, h_0 = inputs\n        else:\n            x_data = inputs\n            h_0 = None\n        x_var = torch.autograd.Variable(xp2torch(x_data), requires_grad=is_update)\n        return (x_var, h_0), {}",
  "def prepare_output(self, torch_outputs):\n        y_var, h_n = torch_outputs\n        return torch2xp(y_var), h_n",
  "def prepare_backward_input(self, dy_data, y_var):\n        dy, _ = dy_data\n        dy_var = xp2torch(dy)\n        y_var, _ = y_var\n        return (y_var,), {\"grad_tensors\": (dy_var,)}",
  "def prepare_backward_output(self, x_args, x_kwargs):\n        x_var, _ = x_args\n        return torch2xp(x_var.grad)",
  "def backward_pytorch(dy_data, sgd=None):\n            d_args, d_kwargs = self.prepare_backward_input(dy_data, y_var)\n            torch.autograd.backward(*d_args, **d_kwargs)\n            if sgd is not None:\n                if self._optimizer is None:\n                    self._optimizer = self._create_optimizer(sgd)\n                self._optimizer.step()\n                self._optimizer.zero_grad()\n            return self.prepare_backward_output(fwd_args, fwd_kwargs)",
  "def get_spacy(lang, **kwargs):\n    global SPACY_MODELS\n    import spacy\n\n    if lang not in SPACY_MODELS:\n        SPACY_MODELS[lang] = spacy.load(lang, **kwargs)\n    return SPACY_MODELS[lang]",
  "def register_vectors(ops, lang, data):\n    key = (ops.device, lang)\n    VECTORS[key] = data",
  "def get_vectors(ops, lang):\n    global VECTORS\n    key = (ops.device, lang)\n    if key not in VECTORS:\n        nlp = get_spacy(lang)\n        VECTORS[key] = nlp.vocab.vectors.data\n    return VECTORS[key]",
  "def load_mnist(path=\"mnist.pkl.gz\"):\n    path = get_file(path, origin=\"https://s3.amazonaws.com/img-datasets/mnist.pkl.gz\")\n\n    if path.endswith(\".gz\"):\n        f = gzip.open(path, \"rb\")\n    else:\n        f = open(path, \"rb\")\n\n    if sys.version_info < (3,):\n        data = cPickle.load(f)\n    else:\n        data = cPickle.load(f, encoding=\"bytes\")\n\n    f.close()\n    return data",
  "def load_reuters(\n    path=\"reuters.pkl\",\n    nb_words=None,\n    skip_top=0,\n    maxlen=None,\n    test_split=0.2,\n    seed=113,\n    start_char=1,\n    oov_char=2,\n    index_from=3,\n):\n    \"\"\"Loads the Reuters newswire classification dataset.\n\n    # Arguments\n        path: where to store the data (in `/.keras/dataset`)\n        nb_words: max number of words to include. Words are ranked\n            by how often they occur (in the training set) and only\n            the most frequent words are kept\n        skip_top: skip the top N most frequently occuring words\n            (which may not be informative).\n        maxlen: truncate sequences after this length.\n        test_split: Fraction of the dataset to be used as test data.\n        seed: random seed for sample shuffling.\n        start_char: The start of a sequence will be marked with this character.\n            Set to 1 because 0 is usually the padding character.\n        oov_char: words that were cut out because of the `nb_words`\n            or `skip_top` limit will be replaced with this character.\n        index_from: index actual words with this index and higher.\n\n    Note that the 'out of vocabulary' character is only used for\n    words that were present in the training set but are not included\n    because they're not making the `nb_words` cut here.\n    Words that were not seen in the trining set but are in the test set\n    have simply been skipped.\n    \"\"\"\n\n    path = get_file(path, origin=\"https://s3.amazonaws.com/text-datasets/reuters.pkl\")\n    f = open(path, \"rb\")\n    X, labels = cPickle.load(f)\n    f.close()\n\n    np.random.seed(seed)\n    np.random.shuffle(X)\n    np.random.seed(seed)\n    np.random.shuffle(labels)\n\n    if start_char is not None:\n        X = [[start_char] + [w + index_from for w in x] for x in X]\n    elif index_from:\n        X = [[w + index_from for w in x] for x in X]\n\n    if maxlen:\n        new_X = []\n        new_labels = []\n        for x, y in zip(X, labels):\n            if len(x) < maxlen:\n                new_X.append(x)\n                new_labels.append(y)\n        X = new_X\n        labels = new_labels\n\n    if not nb_words:\n        nb_words = max([max(x) for x in X])\n\n    # by convention, use 2 as OOV word\n    # reserve 'index_from' (=3 by default) characters: 0 (padding), 1 (start), 2 (OOV)\n    if oov_char is not None:\n        X = [[oov_char if (w >= nb_words or w < skip_top) else w for w in x] for x in X]\n    else:\n        nX = []\n        for x in X:\n            nx = []\n            for w in x:\n                if w >= nb_words or w < skip_top:\n                    nx.append(w)\n            nX.append(nx)\n        X = nX\n\n    X_train = X[: int(len(X) * (1 - test_split))]\n    y_train = labels[: int(len(X) * (1 - test_split))]\n\n    X_test = X[int(len(X) * (1 - test_split)) :]\n    y_test = labels[int(len(X) * (1 - test_split)) :]\n\n    return (X_train, y_train), (X_test, y_test)",
  "def get_word_index(path=\"reuters_word_index.pkl\"):\n    path = get_file(\n        path, origin=\"https://s3.amazonaws.com/text-datasets/reuters_word_index.pkl\"\n    )\n    f = open(path, \"rb\")\n\n    if sys.version_info < (3,):\n        data = cPickle.load(f)\n    else:\n        data = cPickle.load(f, encoding=\"latin1\")\n\n    f.close()\n    return data",
  "def get_file(\n    fname, origin, untar=False, unzip=False, md5_hash=None, cache_subdir=\"datasets\"\n):\n    \"\"\"Downloads a file from a URL if it not already in the cache.\n\n    Passing the MD5 hash will verify the file after download as well as if it is already present in the cache.\n\n    # Arguments\n        fname: name of the file\n        origin: original URL of the file\n        untar: boolean, whether the file should be decompressed\n        md5_hash: MD5 hash of the file for verification\n        cache_subdir: directory being used as the cache\n\n    # Returns\n        Path to the downloaded file\n    \"\"\"\n    datadir_base = os.path.expanduser(os.path.join(\"~\", \".keras\"))\n    if not os.access(datadir_base, os.W_OK):\n        datadir_base = os.path.join(\"/tmp\", \".keras\")\n    datadir = os.path.join(datadir_base, cache_subdir)\n    if not os.path.exists(datadir):\n        os.makedirs(datadir)\n\n    if untar or unzip:\n        untar_fpath = os.path.join(datadir, fname)\n        if unzip:\n            fpath = untar_fpath + \".zip\"\n        else:\n            fpath = untar_fpath + \".tar.gz\"\n    else:\n        fpath = os.path.join(datadir, fname)\n\n    download = False\n    if os.path.exists(fpath):\n        # file found; verify integrity if a hash was provided\n        if md5_hash is not None:\n            if not validate_file(fpath, md5_hash):\n                print(\n                    \"A local file was found, but it seems to be \"\n                    \"incomplete or outdated.\"\n                )\n                download = True\n    else:\n        download = True\n\n    if download:\n        print(\"Downloading data from\", origin)\n        global progbar\n        progbar = None\n\n        def dl_progress(count, block_size, total_size):\n            global progbar\n            if progbar is None:\n                progbar = Progbar(total_size)\n            else:\n                progbar.update(count * block_size)\n\n        error_msg = \"URL fetch failure on {}: {} -- {}\"\n        try:\n            try:\n                urlretrieve(origin, fpath, dl_progress)\n            except URLError as e:\n                raise Exception(error_msg.format(origin, e.errno, e.reason))\n            except HTTPError as e:\n                raise Exception(error_msg.format(origin, e.code, e.msg))\n        except (Exception, KeyboardInterrupt) as e:\n            if os.path.exists(fpath):\n                os.remove(fpath)\n            raise\n        progbar = None\n\n    if untar:\n        if not os.path.exists(untar_fpath):\n            print(\"Untaring file...\")\n            tfile = tarfile.open(fpath, \"r:gz\")\n            try:\n                tfile.extractall(path=datadir)\n            except (Exception, KeyboardInterrupt) as e:\n                if os.path.exists(untar_fpath):\n                    if os.path.isfile(untar_fpath):\n                        os.remove(untar_fpath)\n                    else:\n                        shutil.rmtree(untar_fpath)\n                raise\n            tfile.close()\n        return untar_fpath\n    elif unzip:\n        if not os.path.exists(untar_fpath):\n            print(\"Unzipping file...\")\n            with zipfile.ZipFile(fpath) as file_:\n                try:\n                    file_.extractall(path=datadir)\n                except (Exception, KeyboardInterrupt) as e:\n                    if os.path.exists(untar_fpath):\n                        if os.path.isfile(untar_fpath):\n                            os.remove(untar_fpath)\n                        else:\n                            shutil.rmtree(untar_fpath)\n                    raise\n        return untar_fpath\n\n    return fpath",
  "def validate_file(fpath, md5_hash):\n    \"\"\"Validates a file against a MD5 hash\n\n    # Arguments\n        fpath: path to the file being validated\n        md5_hash: the MD5 hash being validated against\n\n    # Returns\n        Whether the file is valid\n    \"\"\"\n    hasher = hashlib.md5()\n    with open(fpath, \"rb\") as f:\n        buf = f.read()\n        hasher.update(buf)\n    if str(hasher.hexdigest()) == str(md5_hash):\n        return True\n    else:\n        return False",
  "def urlretrieve(url, filename, reporthook=None, data=None):\n        def chunk_read(response, chunk_size=8192, reporthook=None):\n            total_size = response.info().get(\"Content-Length\").strip()\n            total_size = int(total_size)\n            count = 0\n            while 1:\n                chunk = response.read(chunk_size)\n                count += 1\n                if not chunk:\n                    reporthook(count, total_size, total_size)\n                    break\n                if reporthook:\n                    reporthook(count, chunk_size, total_size)\n                yield chunk\n\n        response = urlopen(url, data)\n        with open(filename, \"wb\") as fd:\n            for chunk in chunk_read(response, reporthook=reporthook):\n                fd.write(chunk)",
  "def chunk_read(response, chunk_size=8192, reporthook=None):\n            total_size = response.info().get(\"Content-Length\").strip()\n            total_size = int(total_size)\n            count = 0\n            while 1:\n                chunk = response.read(chunk_size)\n                count += 1\n                if not chunk:\n                    reporthook(count, total_size, total_size)\n                    break\n                if reporthook:\n                    reporthook(count, chunk_size, total_size)\n                yield chunk",
  "def dl_progress(count, block_size, total_size):\n            global progbar\n            if progbar is None:\n                progbar = Progbar(total_size)\n            else:\n                progbar.update(count * block_size)",
  "def get_from_module(\n    identifier, module_params, module_name, instantiate=False, kwargs=None\n):\n    if isinstance(identifier, string_types):\n        res = module_params.get(identifier)\n        if not res:\n            raise ValueError(\"Invalid \" + str(module_name) + \": \" + str(identifier))\n        if instantiate and not kwargs:\n            return res()\n        elif instantiate and kwargs:\n            return res(**kwargs)\n        else:\n            return res\n    elif isinstance(identifier, dict):\n        name = identifier.pop(\"name\")\n        res = module_params.get(name)\n        if res:\n            return res(**identifier)\n        else:\n            raise ValueError(\"Invalid \" + str(module_name) + \": \" + str(identifier))\n    return identifier",
  "def make_tuple(*args):\n    return args",
  "def func_dump(func):\n    \"\"\"Serialize user defined function.\"\"\"\n    code = marshal.dumps(func.__code__).decode(\"raw_unicode_escape\")\n    defaults = func.__defaults__\n    if func.__closure__:\n        closure = tuple(c.cell_contents for c in func.__closure__)\n    else:\n        closure = None\n    return code, defaults, closure",
  "def func_load(code, defaults=None, closure=None, globs=None):\n    \"\"\"Deserialize user defined function.\"\"\"\n    if isinstance(code, (tuple, list)):  # unpack previous dump\n        code, defaults, closure = code\n    code = marshal.loads(code.encode(\"raw_unicode_escape\"))\n    if globs is None:\n        globs = globals()\n    return python_types.FunctionType(\n        code, globs, name=code.co_name, argdefs=defaults, closure=closure\n    )",
  "class Progbar(object):\n    def __init__(self, target, width=30, verbose=1, interval=0.01):\n        \"\"\"Dislays a progress bar.\n\n        # Arguments:\n            target: Total number of steps expected.\n            interval: Minimum visual progress update interval (in seconds).\n        \"\"\"\n        self.width = width\n        self.target = target\n        self.sum_values = {}\n        self.unique_values = []\n        self.start = time.time()\n        self.last_update = 0\n        self.interval = interval\n        self.total_width = 0\n        self.seen_so_far = 0\n        self.verbose = verbose\n\n    def update(self, current, values=[], force=False):\n        \"\"\"Updates the progress bar.\n\n        # Arguments\n            current: Index of current step.\n            values: List of tuples (name, value_for_last_step).\n                The progress bar will display averages for these values.\n            force: Whether to force visual progress update.\n        \"\"\"\n        for k, v in values:\n            if k not in self.sum_values:\n                self.sum_values[k] = [\n                    v * (current - self.seen_so_far),\n                    current - self.seen_so_far,\n                ]\n                self.unique_values.append(k)\n            else:\n                self.sum_values[k][0] += v * (current - self.seen_so_far)\n                self.sum_values[k][1] += current - self.seen_so_far\n        self.seen_so_far = current\n\n        now = time.time()\n        if self.verbose == 1:\n            if not force and (now - self.last_update) < self.interval:\n                return\n\n            prev_total_width = self.total_width\n            sys.stdout.write(\"\\b\" * prev_total_width)\n            sys.stdout.write(\"\\r\")\n            if self.target == -1:\n                numdigits = 0\n            else:\n                numdigits = int(np.floor(np.log10(self.target))) + 1\n            barstr = \"%%%dd/%%%dd [\" % (numdigits, numdigits)\n            bar = barstr % (current, self.target)\n            prog = float(current) / self.target\n            prog_width = int(self.width * prog)\n            if prog_width > 0:\n                bar += \"=\" * (prog_width - 1)\n                if current < self.target:\n                    bar += \">\"\n                else:\n                    bar += \"=\"\n            bar += \".\" * (self.width - prog_width)\n            bar += \"]\"\n            sys.stdout.write(bar)\n            self.total_width = len(bar)\n\n            if current:\n                time_per_unit = (now - self.start) / current\n            else:\n                time_per_unit = 0\n            eta = time_per_unit * (self.target - current)\n            info = \"\"\n            if current < self.target:\n                info += \" - ETA: %ds \" % eta\n            else:\n                info += \" - %ds\" % (now - self.start)\n            for k in self.unique_values:\n                info += \" - %s:\" % k\n                if isinstance(self.sum_values[k], list):\n                    avg = self.sum_values[k][0] / max(1, self.sum_values[k][1])\n                    if abs(avg) > 1e-3:\n                        info += \" %.4f\" % avg\n                    else:\n                        info += \" %.4e\" % avg\n                else:\n                    info += \" %s\" % self.sum_values[k]\n\n            self.total_width += len(info)\n            if prev_total_width > self.total_width:\n                info += (prev_total_width - self.total_width) * \" \"\n\n            sys.stdout.write(info)\n            sys.stdout.flush()\n\n            if current >= self.target:\n                sys.stdout.write(\"\\n\")\n\n        if self.verbose == 2:\n            if current >= self.target:\n                info = \"%ds\" % (now - self.start)\n                for k in self.unique_values:\n                    info += \" - %s:\" % k\n                    avg = self.sum_values[k][0] / max(1, self.sum_values[k][1])\n                    if avg > 1e-3:\n                        info += \" %.4f\" % avg\n                    else:\n                        info += \" %.4e\" % avg\n                sys.stdout.write(info + \"\\n\")\n\n        self.last_update = now\n\n    def add(self, n, values=[]):\n        self.update(self.seen_so_far + n, values)",
  "def display_table(rows, positions):\n    def display_row(objects, positions):\n        line = \"\"\n        for i in range(len(objects)):\n            line += str(objects[i])\n            line = line[: positions[i]]\n            line += \" \" * (positions[i] - len(line))\n        print(line)\n\n    for objects in rows:\n        display_row(objects, positions)",
  "def __init__(self, target, width=30, verbose=1, interval=0.01):\n        \"\"\"Dislays a progress bar.\n\n        # Arguments:\n            target: Total number of steps expected.\n            interval: Minimum visual progress update interval (in seconds).\n        \"\"\"\n        self.width = width\n        self.target = target\n        self.sum_values = {}\n        self.unique_values = []\n        self.start = time.time()\n        self.last_update = 0\n        self.interval = interval\n        self.total_width = 0\n        self.seen_so_far = 0\n        self.verbose = verbose",
  "def update(self, current, values=[], force=False):\n        \"\"\"Updates the progress bar.\n\n        # Arguments\n            current: Index of current step.\n            values: List of tuples (name, value_for_last_step).\n                The progress bar will display averages for these values.\n            force: Whether to force visual progress update.\n        \"\"\"\n        for k, v in values:\n            if k not in self.sum_values:\n                self.sum_values[k] = [\n                    v * (current - self.seen_so_far),\n                    current - self.seen_so_far,\n                ]\n                self.unique_values.append(k)\n            else:\n                self.sum_values[k][0] += v * (current - self.seen_so_far)\n                self.sum_values[k][1] += current - self.seen_so_far\n        self.seen_so_far = current\n\n        now = time.time()\n        if self.verbose == 1:\n            if not force and (now - self.last_update) < self.interval:\n                return\n\n            prev_total_width = self.total_width\n            sys.stdout.write(\"\\b\" * prev_total_width)\n            sys.stdout.write(\"\\r\")\n            if self.target == -1:\n                numdigits = 0\n            else:\n                numdigits = int(np.floor(np.log10(self.target))) + 1\n            barstr = \"%%%dd/%%%dd [\" % (numdigits, numdigits)\n            bar = barstr % (current, self.target)\n            prog = float(current) / self.target\n            prog_width = int(self.width * prog)\n            if prog_width > 0:\n                bar += \"=\" * (prog_width - 1)\n                if current < self.target:\n                    bar += \">\"\n                else:\n                    bar += \"=\"\n            bar += \".\" * (self.width - prog_width)\n            bar += \"]\"\n            sys.stdout.write(bar)\n            self.total_width = len(bar)\n\n            if current:\n                time_per_unit = (now - self.start) / current\n            else:\n                time_per_unit = 0\n            eta = time_per_unit * (self.target - current)\n            info = \"\"\n            if current < self.target:\n                info += \" - ETA: %ds \" % eta\n            else:\n                info += \" - %ds\" % (now - self.start)\n            for k in self.unique_values:\n                info += \" - %s:\" % k\n                if isinstance(self.sum_values[k], list):\n                    avg = self.sum_values[k][0] / max(1, self.sum_values[k][1])\n                    if abs(avg) > 1e-3:\n                        info += \" %.4f\" % avg\n                    else:\n                        info += \" %.4e\" % avg\n                else:\n                    info += \" %s\" % self.sum_values[k]\n\n            self.total_width += len(info)\n            if prev_total_width > self.total_width:\n                info += (prev_total_width - self.total_width) * \" \"\n\n            sys.stdout.write(info)\n            sys.stdout.flush()\n\n            if current >= self.target:\n                sys.stdout.write(\"\\n\")\n\n        if self.verbose == 2:\n            if current >= self.target:\n                info = \"%ds\" % (now - self.start)\n                for k in self.unique_values:\n                    info += \" - %s:\" % k\n                    avg = self.sum_values[k][0] / max(1, self.sum_values[k][1])\n                    if avg > 1e-3:\n                        info += \" %.4f\" % avg\n                    else:\n                        info += \" %.4e\" % avg\n                sys.stdout.write(info + \"\\n\")\n\n        self.last_update = now",
  "def add(self, n, values=[]):\n        self.update(self.seen_so_far + n, values)",
  "def display_row(objects, positions):\n        line = \"\"\n        for i in range(len(objects)):\n            line += str(objects[i])\n            line = line[: positions[i]]\n            line += \" \" * (positions[i] - len(line))\n        print(line)",
  "class _AdapterFunctionCode(CallableObjectProxy):\n    def __init__(self, wrapped_code, adapter_code):\n        super(_AdapterFunctionCode, self).__init__(wrapped_code)\n        self._self_adapter_code = adapter_code\n\n    @property\n    def co_argcount(self):\n        return self._self_adapter_code.co_argcount\n\n    @property\n    def co_code(self):\n        return self._self_adapter_code.co_code\n\n    @property\n    def co_flags(self):\n        return self._self_adapter_code.co_flags\n\n    @property\n    def co_kwonlyargcount(self):\n        return self._self_adapter_code.co_kwonlyargcount\n\n    @property\n    def co_varnames(self):\n        return self._self_adapter_code.co_varnames",
  "class _AdapterFunctionSurrogate(CallableObjectProxy):\n    def __init__(self, wrapped, adapter):\n        super(_AdapterFunctionSurrogate, self).__init__(wrapped)\n        self._self_adapter = adapter\n\n    @property\n    def __code__(self):\n        return _AdapterFunctionCode(\n            self.__wrapped__.__code__, self._self_adapter.__code__\n        )\n\n    @property\n    def __defaults__(self):\n        return self._self_adapter.__defaults__\n\n    @property\n    def __kwdefaults__(self):\n        return self._self_adapter.__kwdefaults__\n\n    @property\n    def __signature__(self):\n        if \"signature\" not in globals():\n            return self._self_adapter.__signature__\n        else:\n            # Can't allow this to fail on Python 3 else it falls\n            # through to using __wrapped__, but that will be the\n            # wrong function we want to derive the signature\n            # from. Thus generate the signature ourselves.\n\n            return signature(self._self_adapter)\n\n    if PY2:\n        func_code = __code__\n        func_defaults = __defaults__",
  "class _BoundAdapterWrapper(BoundFunctionWrapper):\n    @property\n    def __func__(self):\n        return _AdapterFunctionSurrogate(\n            self.__wrapped__.__func__, self._self_parent._self_adapter\n        )\n\n    if PY2:\n        im_func = __func__",
  "class AdapterWrapper(FunctionWrapper):\n\n    __bound_function_wrapper__ = _BoundAdapterWrapper\n\n    def __init__(self, *args, **kwargs):\n        adapter = kwargs.pop(\"adapter\")\n        super(AdapterWrapper, self).__init__(*args, **kwargs)\n        self._self_surrogate = _AdapterFunctionSurrogate(self.__wrapped__, adapter)\n        self._self_adapter = adapter\n\n    @property\n    def __code__(self):\n        return self._self_surrogate.__code__\n\n    @property\n    def __defaults__(self):\n        return self._self_surrogate.__defaults__\n\n    @property\n    def __kwdefaults__(self):\n        return self._self_surrogate.__kwdefaults__\n\n    if PY2:\n        func_code = __code__\n        func_defaults = __defaults__\n\n    @property\n    def __signature__(self):\n        return self._self_surrogate.__signature__",
  "class AdapterFactory(object):\n    def __call__(self, wrapped):\n        raise NotImplementedError()",
  "class DelegatedAdapterFactory(AdapterFactory):\n    def __init__(self, factory):\n        super(DelegatedAdapterFactory, self).__init__()\n        self.factory = factory\n\n    def __call__(self, wrapped):\n        return self.factory(wrapped)",
  "def decorator(wrapper=None, enabled=None, adapter=None):\n    # The decorator should be supplied with a single positional argument\n    # which is the wrapper function to be used to implement the\n    # decorator. This may be preceded by a step whereby the keyword\n    # arguments are supplied to customise the behaviour of the\n    # decorator. The 'adapter' argument is used to optionally denote a\n    # separate function which is notionally used by an adapter\n    # decorator. In that case parts of the function '__code__' and\n    # '__defaults__' attributes are used from the adapter function\n    # rather than those of the wrapped function. This allows for the\n    # argument specification from inspect.getargspec() and similar\n    # functions to be overridden with a prototype for a different\n    # function than what was wrapped. The 'enabled' argument provides a\n    # way to enable/disable the use of the decorator. If the type of\n    # 'enabled' is a boolean, then it is evaluated immediately and the\n    # wrapper not even applied if it is False. If not a boolean, it will\n    # be evaluated when the wrapper is called for an unbound wrapper,\n    # and when binding occurs for a bound wrapper. When being evaluated,\n    # if 'enabled' is callable it will be called to obtain the value to\n    # be checked. If False, the wrapper will not be called and instead\n    # the original wrapped function will be called directly instead.\n\n    if wrapper is not None:\n        # Helper function for creating wrapper of the appropriate\n        # time when we need it down below.\n\n        def _build(wrapped, wrapper, enabled=None, adapter=None):\n            if adapter:\n                if isinstance(adapter, AdapterFactory):\n                    adapter = adapter(wrapped)\n\n                if not callable(adapter):\n                    ns = {}\n                    if not isinstance(adapter, string_types):\n                        adapter = formatargspec(*adapter)\n                    exec_(\"def adapter{}: pass\".format(adapter), ns, ns)\n                    adapter = ns[\"adapter\"]\n\n                return AdapterWrapper(\n                    wrapped=wrapped, wrapper=wrapper, enabled=enabled, adapter=adapter\n                )\n\n            return FunctionWrapper(wrapped=wrapped, wrapper=wrapper, enabled=enabled)\n\n        # The wrapper has been provided so return the final decorator.\n        # The decorator is itself one of our function wrappers so we\n        # can determine when it is applied to functions, instance methods\n        # or class methods. This allows us to bind the instance or class\n        # method so the appropriate self or cls attribute is supplied\n        # when it is finally called.\n\n        def _wrapper(wrapped, instance, args, kwargs):\n            # We first check for the case where the decorator was applied\n            # to a class type.\n            #\n            #     @decorator\n            #     class mydecoratorclass(object):\n            #         def __init__(self, arg=None):\n            #             self.arg = arg\n            #         def __call__(self, wrapped, instance, args, kwargs):\n            #             return wrapped(*args, **kwargs)\n            #\n            #     @mydecoratorclass(arg=1)\n            #     def function():\n            #         pass\n            #\n            # In this case an instance of the class is to be used as the\n            # decorator wrapper function. If args was empty at this point,\n            # then it means that there were optional keyword arguments\n            # supplied to be used when creating an instance of the class\n            # to be used as the wrapper function.\n\n            if instance is None and isclass(wrapped) and not args:\n                # We still need to be passed the target function to be\n                # wrapped as yet, so we need to return a further function\n                # to be able to capture it.\n\n                def _capture(target_wrapped):\n                    # Now have the target function to be wrapped and need\n                    # to create an instance of the class which is to act\n                    # as the decorator wrapper function. Before we do that,\n                    # we need to first check that use of the decorator\n                    # hadn't been disabled by a simple boolean. If it was,\n                    # the target function to be wrapped is returned instead.\n\n                    _enabled = enabled\n                    if type(_enabled) is bool:\n                        if not _enabled:\n                            return target_wrapped\n                        _enabled = None\n\n                    # Now create an instance of the class which is to act\n                    # as the decorator wrapper function. Any arguments had\n                    # to be supplied as keyword only arguments so that is\n                    # all we pass when creating it.\n\n                    target_wrapper = wrapped(**kwargs)\n\n                    # Finally build the wrapper itself and return it.\n\n                    return _build(target_wrapped, target_wrapper, _enabled, adapter)\n\n                return _capture\n\n            # We should always have the target function to be wrapped at\n            # this point as the first (and only) value in args.\n\n            target_wrapped = args[0]\n\n            # Need to now check that use of the decorator hadn't been\n            # disabled by a simple boolean. If it was, then target\n            # function to be wrapped is returned instead.\n\n            _enabled = enabled\n            if type(_enabled) is bool:\n                if not _enabled:\n                    return target_wrapped\n                _enabled = None\n\n            # We now need to build the wrapper, but there are a couple of\n            # different cases we need to consider.\n\n            if instance is None:\n                if isclass(wrapped):\n                    # In this case the decorator was applied to a class\n                    # type but optional keyword arguments were not supplied\n                    # for initialising an instance of the class to be used\n                    # as the decorator wrapper function.\n                    #\n                    #     @decorator\n                    #     class mydecoratorclass(object):\n                    #         def __init__(self, arg=None):\n                    #             self.arg = arg\n                    #         def __call__(self, wrapped, instance,\n                    #                 args, kwargs):\n                    #             return wrapped(*args, **kwargs)\n                    #\n                    #     @mydecoratorclass\n                    #     def function():\n                    #         pass\n                    #\n                    # We still need to create an instance of the class to\n                    # be used as the decorator wrapper function, but no\n                    # arguments are pass.\n\n                    target_wrapper = wrapped()\n\n                else:\n                    # In this case the decorator was applied to a normal\n                    # function, or possibly a static method of a class.\n                    #\n                    #     @decorator\n                    #     def mydecoratorfuntion(wrapped, instance,\n                    #             args, kwargs):\n                    #         return wrapped(*args, **kwargs)\n                    #\n                    #     @mydecoratorfunction\n                    #     def function():\n                    #         pass\n                    #\n                    # That normal function becomes the decorator wrapper\n                    # function.\n\n                    target_wrapper = wrapper\n\n            else:\n                if isclass(instance):\n                    # In this case the decorator was applied to a class\n                    # method.\n                    #\n                    #     class myclass(object):\n                    #         @decorator\n                    #         @classmethod\n                    #         def decoratorclassmethod(cls, wrapped,\n                    #                 instance, args, kwargs):\n                    #             return wrapped(*args, **kwargs)\n                    #\n                    #     instance = myclass()\n                    #\n                    #     @instance.decoratorclassmethod\n                    #     def function():\n                    #         pass\n                    #\n                    # This one is a bit strange because binding was actually\n                    # performed on the wrapper created by our decorator\n                    # factory. We need to apply that binding to the decorator\n                    # wrapper function which which the decorator factory\n                    # was applied to.\n\n                    target_wrapper = wrapper.__get__(None, instance)\n\n                else:\n                    # In this case the decorator was applied to an instance\n                    # method.\n                    #\n                    #     class myclass(object):\n                    #         @decorator\n                    #         def decoratorclassmethod(self, wrapped,\n                    #                 instance, args, kwargs):\n                    #             return wrapped(*args, **kwargs)\n                    #\n                    #     instance = myclass()\n                    #\n                    #     @instance.decoratorclassmethod\n                    #     def function():\n                    #         pass\n                    #\n                    # This one is a bit strange because binding was actually\n                    # performed on the wrapper created by our decorator\n                    # factory. We need to apply that binding to the decorator\n                    # wrapper function which which the decorator factory\n                    # was applied to.\n\n                    target_wrapper = wrapper.__get__(instance, type(instance))\n\n            # Finally build the wrapper itself and return it.\n\n            return _build(target_wrapped, target_wrapper, _enabled, adapter)\n\n        # We first return our magic function wrapper here so we can\n        # determine in what context the decorator factory was used. In\n        # other words, it is itself a universal decorator.\n\n        return _build(wrapper, _wrapper)\n\n    else:\n        # The wrapper still has not been provided, so we are just\n        # collecting the optional keyword arguments. Return the\n        # decorator again wrapped in a partial using the collected\n        # arguments.\n\n        return partial(decorator, enabled=enabled, adapter=adapter)",
  "def synchronized(wrapped):\n    # Determine if being passed an object which is a synchronization\n    # primitive. We can't check by type for Lock, RLock, Semaphore etc,\n    # as the means of creating them isn't the type. Therefore use the\n    # existence of acquire() and release() methods. This is more\n    # extensible anyway as it allows custom synchronization mechanisms.\n\n    if hasattr(wrapped, \"acquire\") and hasattr(wrapped, \"release\"):\n        # We remember what the original lock is and then return a new\n        # decorator which accesses and locks it. When returning the new\n        # decorator we wrap it with an object proxy so we can override\n        # the context manager methods in case it is being used to wrap\n        # synchronized statements with a 'with' statement.\n\n        lock = wrapped\n\n        @decorator\n        def _synchronized(wrapped, instance, args, kwargs):\n            # Execute the wrapped function while the original supplied\n            # lock is held.\n\n            with lock:\n                return wrapped(*args, **kwargs)\n\n        class _PartialDecorator(CallableObjectProxy):\n            def __enter__(self):\n                lock.acquire()\n                return lock\n\n            def __exit__(self, *args):\n                lock.release()\n\n        return _PartialDecorator(wrapped=_synchronized)\n\n    # Following only apply when the lock is being created automatically\n    # based on the context of what was supplied. In this case we supply\n    # a final decorator, but need to use FunctionWrapper directly as we\n    # want to derive from it to add context manager methods in case it is\n    # being used to wrap synchronized statements with a 'with' statement.\n\n    def _synchronized_lock(context):\n        # Attempt to retrieve the lock for the specific context.\n\n        lock = vars(context).get(\"_synchronized_lock\", None)\n\n        if lock is None:\n            # There is no existing lock defined for the context we\n            # are dealing with so we need to create one. This needs\n            # to be done in a way to guarantee there is only one\n            # created, even if multiple threads try and create it at\n            # the same time. We can't always use the setdefault()\n            # method on the __dict__ for the context. This is the\n            # case where the context is a class, as __dict__ is\n            # actually a dictproxy. What we therefore do is use a\n            # meta lock on this wrapper itself, to control the\n            # creation and assignment of the lock attribute against\n            # the context.\n\n            with synchronized._synchronized_meta_lock:\n                # We need to check again for whether the lock we want\n                # exists in case two threads were trying to create it\n                # at the same time and were competing to create the\n                # meta lock.\n\n                lock = vars(context).get(\"_synchronized_lock\", None)\n\n                if lock is None:\n                    lock = RLock()\n                    setattr(context, \"_synchronized_lock\", lock)\n\n        return lock\n\n    def _synchronized_wrapper(wrapped, instance, args, kwargs):\n        # Execute the wrapped function while the lock for the\n        # desired context is held. If instance is None then the\n        # wrapped function is used as the context.\n\n        with _synchronized_lock(instance or wrapped):\n            return wrapped(*args, **kwargs)\n\n    class _FinalDecorator(FunctionWrapper):\n        def __enter__(self):\n            self._self_lock = _synchronized_lock(self.__wrapped__)\n            self._self_lock.acquire()\n            return self._self_lock\n\n        def __exit__(self, *args):\n            self._self_lock.release()\n\n    return _FinalDecorator(wrapped=wrapped, wrapper=_synchronized_wrapper)",
  "def exec_(_code_, _globs_=None, _locs_=None):\n        \"\"\"Execute code in a namespace.\"\"\"\n        if _globs_ is None:\n            frame = sys._getframe(1)\n            _globs_ = frame.f_globals\n            if _locs_ is None:\n                _locs_ = frame.f_locals\n            del frame\n        elif _locs_ is None:\n            _locs_ = _globs_\n        exec(\"\"\"exec _code_ in _globs_, _locs_\"\"\")",
  "def __init__(self, wrapped_code, adapter_code):\n        super(_AdapterFunctionCode, self).__init__(wrapped_code)\n        self._self_adapter_code = adapter_code",
  "def co_argcount(self):\n        return self._self_adapter_code.co_argcount",
  "def co_code(self):\n        return self._self_adapter_code.co_code",
  "def co_flags(self):\n        return self._self_adapter_code.co_flags",
  "def co_kwonlyargcount(self):\n        return self._self_adapter_code.co_kwonlyargcount",
  "def co_varnames(self):\n        return self._self_adapter_code.co_varnames",
  "def __init__(self, wrapped, adapter):\n        super(_AdapterFunctionSurrogate, self).__init__(wrapped)\n        self._self_adapter = adapter",
  "def __code__(self):\n        return _AdapterFunctionCode(\n            self.__wrapped__.__code__, self._self_adapter.__code__\n        )",
  "def __defaults__(self):\n        return self._self_adapter.__defaults__",
  "def __kwdefaults__(self):\n        return self._self_adapter.__kwdefaults__",
  "def __signature__(self):\n        if \"signature\" not in globals():\n            return self._self_adapter.__signature__\n        else:\n            # Can't allow this to fail on Python 3 else it falls\n            # through to using __wrapped__, but that will be the\n            # wrong function we want to derive the signature\n            # from. Thus generate the signature ourselves.\n\n            return signature(self._self_adapter)",
  "def __func__(self):\n        return _AdapterFunctionSurrogate(\n            self.__wrapped__.__func__, self._self_parent._self_adapter\n        )",
  "def __init__(self, *args, **kwargs):\n        adapter = kwargs.pop(\"adapter\")\n        super(AdapterWrapper, self).__init__(*args, **kwargs)\n        self._self_surrogate = _AdapterFunctionSurrogate(self.__wrapped__, adapter)\n        self._self_adapter = adapter",
  "def __code__(self):\n        return self._self_surrogate.__code__",
  "def __defaults__(self):\n        return self._self_surrogate.__defaults__",
  "def __kwdefaults__(self):\n        return self._self_surrogate.__kwdefaults__",
  "def __signature__(self):\n        return self._self_surrogate.__signature__",
  "def __call__(self, wrapped):\n        raise NotImplementedError()",
  "def __init__(self, factory):\n        super(DelegatedAdapterFactory, self).__init__()\n        self.factory = factory",
  "def __call__(self, wrapped):\n        return self.factory(wrapped)",
  "def _synchronized_lock(context):\n        # Attempt to retrieve the lock for the specific context.\n\n        lock = vars(context).get(\"_synchronized_lock\", None)\n\n        if lock is None:\n            # There is no existing lock defined for the context we\n            # are dealing with so we need to create one. This needs\n            # to be done in a way to guarantee there is only one\n            # created, even if multiple threads try and create it at\n            # the same time. We can't always use the setdefault()\n            # method on the __dict__ for the context. This is the\n            # case where the context is a class, as __dict__ is\n            # actually a dictproxy. What we therefore do is use a\n            # meta lock on this wrapper itself, to control the\n            # creation and assignment of the lock attribute against\n            # the context.\n\n            with synchronized._synchronized_meta_lock:\n                # We need to check again for whether the lock we want\n                # exists in case two threads were trying to create it\n                # at the same time and were competing to create the\n                # meta lock.\n\n                lock = vars(context).get(\"_synchronized_lock\", None)\n\n                if lock is None:\n                    lock = RLock()\n                    setattr(context, \"_synchronized_lock\", lock)\n\n        return lock",
  "def _synchronized_wrapper(wrapped, instance, args, kwargs):\n        # Execute the wrapped function while the lock for the\n        # desired context is held. If instance is None then the\n        # wrapped function is used as the context.\n\n        with _synchronized_lock(instance or wrapped):\n            return wrapped(*args, **kwargs)",
  "class _FinalDecorator(FunctionWrapper):\n        def __enter__(self):\n            self._self_lock = _synchronized_lock(self.__wrapped__)\n            self._self_lock.acquire()\n            return self._self_lock\n\n        def __exit__(self, *args):\n            self._self_lock.release()",
  "def _build(wrapped, wrapper, enabled=None, adapter=None):\n            if adapter:\n                if isinstance(adapter, AdapterFactory):\n                    adapter = adapter(wrapped)\n\n                if not callable(adapter):\n                    ns = {}\n                    if not isinstance(adapter, string_types):\n                        adapter = formatargspec(*adapter)\n                    exec_(\"def adapter{}: pass\".format(adapter), ns, ns)\n                    adapter = ns[\"adapter\"]\n\n                return AdapterWrapper(\n                    wrapped=wrapped, wrapper=wrapper, enabled=enabled, adapter=adapter\n                )\n\n            return FunctionWrapper(wrapped=wrapped, wrapper=wrapper, enabled=enabled)",
  "def _wrapper(wrapped, instance, args, kwargs):\n            # We first check for the case where the decorator was applied\n            # to a class type.\n            #\n            #     @decorator\n            #     class mydecoratorclass(object):\n            #         def __init__(self, arg=None):\n            #             self.arg = arg\n            #         def __call__(self, wrapped, instance, args, kwargs):\n            #             return wrapped(*args, **kwargs)\n            #\n            #     @mydecoratorclass(arg=1)\n            #     def function():\n            #         pass\n            #\n            # In this case an instance of the class is to be used as the\n            # decorator wrapper function. If args was empty at this point,\n            # then it means that there were optional keyword arguments\n            # supplied to be used when creating an instance of the class\n            # to be used as the wrapper function.\n\n            if instance is None and isclass(wrapped) and not args:\n                # We still need to be passed the target function to be\n                # wrapped as yet, so we need to return a further function\n                # to be able to capture it.\n\n                def _capture(target_wrapped):\n                    # Now have the target function to be wrapped and need\n                    # to create an instance of the class which is to act\n                    # as the decorator wrapper function. Before we do that,\n                    # we need to first check that use of the decorator\n                    # hadn't been disabled by a simple boolean. If it was,\n                    # the target function to be wrapped is returned instead.\n\n                    _enabled = enabled\n                    if type(_enabled) is bool:\n                        if not _enabled:\n                            return target_wrapped\n                        _enabled = None\n\n                    # Now create an instance of the class which is to act\n                    # as the decorator wrapper function. Any arguments had\n                    # to be supplied as keyword only arguments so that is\n                    # all we pass when creating it.\n\n                    target_wrapper = wrapped(**kwargs)\n\n                    # Finally build the wrapper itself and return it.\n\n                    return _build(target_wrapped, target_wrapper, _enabled, adapter)\n\n                return _capture\n\n            # We should always have the target function to be wrapped at\n            # this point as the first (and only) value in args.\n\n            target_wrapped = args[0]\n\n            # Need to now check that use of the decorator hadn't been\n            # disabled by a simple boolean. If it was, then target\n            # function to be wrapped is returned instead.\n\n            _enabled = enabled\n            if type(_enabled) is bool:\n                if not _enabled:\n                    return target_wrapped\n                _enabled = None\n\n            # We now need to build the wrapper, but there are a couple of\n            # different cases we need to consider.\n\n            if instance is None:\n                if isclass(wrapped):\n                    # In this case the decorator was applied to a class\n                    # type but optional keyword arguments were not supplied\n                    # for initialising an instance of the class to be used\n                    # as the decorator wrapper function.\n                    #\n                    #     @decorator\n                    #     class mydecoratorclass(object):\n                    #         def __init__(self, arg=None):\n                    #             self.arg = arg\n                    #         def __call__(self, wrapped, instance,\n                    #                 args, kwargs):\n                    #             return wrapped(*args, **kwargs)\n                    #\n                    #     @mydecoratorclass\n                    #     def function():\n                    #         pass\n                    #\n                    # We still need to create an instance of the class to\n                    # be used as the decorator wrapper function, but no\n                    # arguments are pass.\n\n                    target_wrapper = wrapped()\n\n                else:\n                    # In this case the decorator was applied to a normal\n                    # function, or possibly a static method of a class.\n                    #\n                    #     @decorator\n                    #     def mydecoratorfuntion(wrapped, instance,\n                    #             args, kwargs):\n                    #         return wrapped(*args, **kwargs)\n                    #\n                    #     @mydecoratorfunction\n                    #     def function():\n                    #         pass\n                    #\n                    # That normal function becomes the decorator wrapper\n                    # function.\n\n                    target_wrapper = wrapper\n\n            else:\n                if isclass(instance):\n                    # In this case the decorator was applied to a class\n                    # method.\n                    #\n                    #     class myclass(object):\n                    #         @decorator\n                    #         @classmethod\n                    #         def decoratorclassmethod(cls, wrapped,\n                    #                 instance, args, kwargs):\n                    #             return wrapped(*args, **kwargs)\n                    #\n                    #     instance = myclass()\n                    #\n                    #     @instance.decoratorclassmethod\n                    #     def function():\n                    #         pass\n                    #\n                    # This one is a bit strange because binding was actually\n                    # performed on the wrapper created by our decorator\n                    # factory. We need to apply that binding to the decorator\n                    # wrapper function which which the decorator factory\n                    # was applied to.\n\n                    target_wrapper = wrapper.__get__(None, instance)\n\n                else:\n                    # In this case the decorator was applied to an instance\n                    # method.\n                    #\n                    #     class myclass(object):\n                    #         @decorator\n                    #         def decoratorclassmethod(self, wrapped,\n                    #                 instance, args, kwargs):\n                    #             return wrapped(*args, **kwargs)\n                    #\n                    #     instance = myclass()\n                    #\n                    #     @instance.decoratorclassmethod\n                    #     def function():\n                    #         pass\n                    #\n                    # This one is a bit strange because binding was actually\n                    # performed on the wrapper created by our decorator\n                    # factory. We need to apply that binding to the decorator\n                    # wrapper function which which the decorator factory\n                    # was applied to.\n\n                    target_wrapper = wrapper.__get__(instance, type(instance))\n\n            # Finally build the wrapper itself and return it.\n\n            return _build(target_wrapped, target_wrapper, _enabled, adapter)",
  "def _synchronized(wrapped, instance, args, kwargs):\n            # Execute the wrapped function while the original supplied\n            # lock is held.\n\n            with lock:\n                return wrapped(*args, **kwargs)",
  "class _PartialDecorator(CallableObjectProxy):\n            def __enter__(self):\n                lock.acquire()\n                return lock\n\n            def __exit__(self, *args):\n                lock.release()",
  "def __enter__(self):\n            self._self_lock = _synchronized_lock(self.__wrapped__)\n            self._self_lock.acquire()\n            return self._self_lock",
  "def __exit__(self, *args):\n            self._self_lock.release()",
  "def __enter__(self):\n                lock.acquire()\n                return lock",
  "def __exit__(self, *args):\n                lock.release()",
  "def _capture(target_wrapped):\n                    # Now have the target function to be wrapped and need\n                    # to create an instance of the class which is to act\n                    # as the decorator wrapper function. Before we do that,\n                    # we need to first check that use of the decorator\n                    # hadn't been disabled by a simple boolean. If it was,\n                    # the target function to be wrapped is returned instead.\n\n                    _enabled = enabled\n                    if type(_enabled) is bool:\n                        if not _enabled:\n                            return target_wrapped\n                        _enabled = None\n\n                    # Now create an instance of the class which is to act\n                    # as the decorator wrapper function. Any arguments had\n                    # to be supplied as keyword only arguments so that is\n                    # all we pass when creating it.\n\n                    target_wrapper = wrapped(**kwargs)\n\n                    # Finally build the wrapper itself and return it.\n\n                    return _build(target_wrapped, target_wrapper, _enabled, adapter)",
  "def _create_import_hook_from_string(name):\n    def import_hook(module):\n        module_name, function = name.split(\":\")\n        attrs = function.split(\".\")\n        __import__(module_name)\n        callback = sys.modules[module_name]\n        for attr in attrs:\n            callback = getattr(callback, attr)\n        return callback(module)\n\n    return import_hook",
  "def register_post_import_hook(hook, name):\n    # Create a deferred import hook if hook is a string name rather than\n    # a callable function.\n\n    if isinstance(hook, string_types):\n        hook = _create_import_hook_from_string(hook)\n\n    # Automatically install the import hook finder if it has not already\n    # been installed.\n\n    global _post_import_hooks_init\n\n    if not _post_import_hooks_init:\n        _post_import_hooks_init = True\n        sys.meta_path.insert(0, ImportHookFinder())\n\n    # Determine if any prior registration of a post import hook for\n    # the target modules has occurred and act appropriately.\n\n    hooks = _post_import_hooks.get(name, None)\n\n    if hooks is None:\n        # No prior registration of post import hooks for the target\n        # module. We need to check whether the module has already been\n        # imported. If it has we fire the hook immediately and add an\n        # empty list to the registry to indicate that the module has\n        # already been imported and hooks have fired. Otherwise add\n        # the post import hook to the registry.\n\n        module = sys.modules.get(name, None)\n\n        if module is not None:\n            _post_import_hooks[name] = []\n            hook(module)\n\n        else:\n            _post_import_hooks[name] = [hook]\n\n    elif hooks == []:\n        # A prior registration of port import hooks for the target\n        # module was done and the hooks already fired. Fire the hook\n        # immediately.\n\n        module = sys.modules[name]\n        hook(module)\n\n    else:\n        # A prior registration of port import hooks for the target\n        # module was done but the module has not yet been imported.\n\n        _post_import_hooks[name].append(hook)",
  "def _create_import_hook_from_entrypoint(entrypoint):\n    def import_hook(module):\n        __import__(entrypoint.module_name)\n        callback = sys.modules[entrypoint.module_name]\n        for attr in entrypoint.attrs:\n            callback = getattr(callback, attr)\n        return callback(module)\n\n    return import_hook",
  "def discover_post_import_hooks(group):\n    try:\n        import pkg_resources\n    except ImportError:\n        return\n\n    for entrypoint in pkg_resources.iter_entry_points(group=group):\n        callback = _create_import_hook_from_entrypoint(entrypoint)\n        register_post_import_hook(callback, entrypoint.name)",
  "def notify_module_loaded(module):\n    name = getattr(module, \"__name__\", None)\n    hooks = _post_import_hooks.get(name, None)\n\n    if hooks:\n        _post_import_hooks[name] = []\n\n        for hook in hooks:\n            hook(module)",
  "class _ImportHookLoader:\n    def load_module(self, fullname):\n        module = sys.modules[fullname]\n        notify_module_loaded(module)\n\n        return module",
  "class _ImportHookChainedLoader:\n    def __init__(self, loader):\n        self.loader = loader\n\n    def load_module(self, fullname):\n        module = self.loader.load_module(fullname)\n        notify_module_loaded(module)\n\n        return module",
  "class ImportHookFinder:\n    def __init__(self):\n        self.in_progress = {}\n\n    @synchronized(_post_import_hooks_lock)\n    def find_module(self, fullname, path=None):\n        # If the module being imported is not one we have registered\n        # post import hooks for, we can return immediately. We will\n        # take no further part in the importing of this module.\n\n        if not fullname in _post_import_hooks:\n            return None\n\n        # When we are interested in a specific module, we will call back\n        # into the import system a second time to defer to the import\n        # finder that is supposed to handle the importing of the module.\n        # We set an in progress flag for the target module so that on\n        # the second time through we don't trigger another call back\n        # into the import system and cause a infinite loop.\n\n        if fullname in self.in_progress:\n            return None\n\n        self.in_progress[fullname] = True\n\n        # Now call back into the import system again.\n\n        try:\n            if PY3:\n                # For Python 3 we need to use find_spec().loader\n                # from the importlib.util module. It doesn't actually\n                # import the target module and only finds the\n                # loader. If a loader is found, we need to return\n                # our own loader which will then in turn call the\n                # real loader to import the module and invoke the\n                # post import hooks.\n                try:\n                    import importlib.util\n\n                    loader = importlib.util.find_spec(fullname).loader\n                except (ImportError, AttributeError):\n                    loader = importlib.find_loader(fullname, path)\n                if loader:\n                    return _ImportHookChainedLoader(loader)\n\n            else:\n                # For Python 2 we don't have much choice but to\n                # call back in to __import__(). This will\n                # actually cause the module to be imported. If no\n                # module could be found then ImportError will be\n                # raised. Otherwise we return a loader which\n                # returns the already loaded module and invokes\n                # the post import hooks.\n\n                __import__(fullname)\n\n                return _ImportHookLoader()\n\n        finally:\n            del self.in_progress[fullname]",
  "def when_imported(name):\n    def register(hook):\n        register_post_import_hook(hook, name)\n        return hook\n\n    return register",
  "def import_hook(module):\n        module_name, function = name.split(\":\")\n        attrs = function.split(\".\")\n        __import__(module_name)\n        callback = sys.modules[module_name]\n        for attr in attrs:\n            callback = getattr(callback, attr)\n        return callback(module)",
  "def import_hook(module):\n        __import__(entrypoint.module_name)\n        callback = sys.modules[entrypoint.module_name]\n        for attr in entrypoint.attrs:\n            callback = getattr(callback, attr)\n        return callback(module)",
  "def load_module(self, fullname):\n        module = sys.modules[fullname]\n        notify_module_loaded(module)\n\n        return module",
  "def __init__(self, loader):\n        self.loader = loader",
  "def load_module(self, fullname):\n        module = self.loader.load_module(fullname)\n        notify_module_loaded(module)\n\n        return module",
  "def __init__(self):\n        self.in_progress = {}",
  "def find_module(self, fullname, path=None):\n        # If the module being imported is not one we have registered\n        # post import hooks for, we can return immediately. We will\n        # take no further part in the importing of this module.\n\n        if not fullname in _post_import_hooks:\n            return None\n\n        # When we are interested in a specific module, we will call back\n        # into the import system a second time to defer to the import\n        # finder that is supposed to handle the importing of the module.\n        # We set an in progress flag for the target module so that on\n        # the second time through we don't trigger another call back\n        # into the import system and cause a infinite loop.\n\n        if fullname in self.in_progress:\n            return None\n\n        self.in_progress[fullname] = True\n\n        # Now call back into the import system again.\n\n        try:\n            if PY3:\n                # For Python 3 we need to use find_spec().loader\n                # from the importlib.util module. It doesn't actually\n                # import the target module and only finds the\n                # loader. If a loader is found, we need to return\n                # our own loader which will then in turn call the\n                # real loader to import the module and invoke the\n                # post import hooks.\n                try:\n                    import importlib.util\n\n                    loader = importlib.util.find_spec(fullname).loader\n                except (ImportError, AttributeError):\n                    loader = importlib.find_loader(fullname, path)\n                if loader:\n                    return _ImportHookChainedLoader(loader)\n\n            else:\n                # For Python 2 we don't have much choice but to\n                # call back in to __import__(). This will\n                # actually cause the module to be imported. If no\n                # module could be found then ImportError will be\n                # raised. Otherwise we return a loader which\n                # returns the already loaded module and invokes\n                # the post import hooks.\n\n                __import__(fullname)\n\n                return _ImportHookLoader()\n\n        finally:\n            del self.in_progress[fullname]",
  "def register(hook):\n        register_post_import_hook(hook, name)\n        return hook",
  "def with_metaclass(meta, *bases):\n    \"\"\"Create a base class with a metaclass.\"\"\"\n    return meta(\"NewBase\", bases, {})",
  "class _ObjectProxyMethods(object):\n\n    # We use properties to override the values of __module__ and\n    # __doc__. If we add these in ObjectProxy, the derived class\n    # __dict__ will still be setup to have string variants of these\n    # attributes and the rules of descriptors means that they appear to\n    # take precedence over the properties in the base class. To avoid\n    # that, we copy the properties into the derived class type itself\n    # via a meta class. In that way the properties will always take\n    # precedence.\n\n    @property\n    def __module__(self):\n        return self.__wrapped__.__module__\n\n    @__module__.setter\n    def __module__(self, value):\n        self.__wrapped__.__module__ = value\n\n    @property\n    def __doc__(self):\n        return self.__wrapped__.__doc__\n\n    @__doc__.setter\n    def __doc__(self, value):\n        self.__wrapped__.__doc__ = value\n\n    # We similar use a property for __dict__. We need __dict__ to be\n    # explicit to ensure that vars() works as expected.\n\n    @property\n    def __dict__(self):\n        return self.__wrapped__.__dict__\n\n    # Need to also propagate the special __weakref__ attribute for case\n    # where decorating classes which will define this. If do not define\n    # it and use a function like inspect.getmembers() on a decorator\n    # class it will fail. This can't be in the derived classes.\n\n    @property\n    def __weakref__(self):\n        return self.__wrapped__.__weakref__",
  "class _ObjectProxyMetaType(type):\n    def __new__(cls, name, bases, dictionary):\n        # Copy our special properties into the class so that they\n        # always take precedence over attributes of the same name added\n        # during construction of a derived class. This is to save\n        # duplicating the implementation for them in all derived classes.\n\n        dictionary.update(vars(_ObjectProxyMethods))\n\n        return type.__new__(cls, name, bases, dictionary)",
  "class ObjectProxy(with_metaclass(_ObjectProxyMetaType)):\n\n    __slots__ = \"__wrapped__\"\n\n    def __init__(self, wrapped):\n        object.__setattr__(self, \"__wrapped__\", wrapped)\n\n        # Python 3.2+ has the __qualname__ attribute, but it does not\n        # allow it to be overridden using a property and it must instead\n        # be an actual string object instead.\n\n        try:\n            object.__setattr__(self, \"__qualname__\", wrapped.__qualname__)\n        except AttributeError:\n            pass\n\n    @property\n    def __name__(self):\n        return self.__wrapped__.__name__\n\n    @__name__.setter\n    def __name__(self, value):\n        self.__wrapped__.__name__ = value\n\n    @property\n    def __class__(self):\n        return self.__wrapped__.__class__\n\n    @__class__.setter\n    def __class__(self, value):\n        self.__wrapped__.__class__ = value\n\n    @property\n    def __annotations__(self):\n        return self.__wrapped__.__annotations__\n\n    @__annotations__.setter\n    def __annotations__(self, value):\n        self.__wrapped__.__annotations__ = value\n\n    def __dir__(self):\n        return dir(self.__wrapped__)\n\n    def __str__(self):\n        return str(self.__wrapped__)\n\n    if PY3:\n\n        def __bytes__(self):\n            return bytes(self.__wrapped__)\n\n    def __repr__(self):\n        return \"<{} at 0x{:x} for {} at 0x{:x}>\".format(\n            type(self).__name__,\n            id(self),\n            type(self.__wrapped__).__name__,\n            id(self.__wrapped__),\n        )\n\n    def __reversed__(self):\n        return reversed(self.__wrapped__)\n\n    if PY3:\n\n        def __round__(self):\n            return round(self.__wrapped__)\n\n    def __lt__(self, other):\n        return self.__wrapped__ < other\n\n    def __le__(self, other):\n        return self.__wrapped__ <= other\n\n    def __eq__(self, other):\n        return self.__wrapped__ == other\n\n    def __ne__(self, other):\n        return self.__wrapped__ != other\n\n    def __gt__(self, other):\n        return self.__wrapped__ > other\n\n    def __ge__(self, other):\n        return self.__wrapped__ >= other\n\n    def __hash__(self):\n        return hash(self.__wrapped__)\n\n    def __nonzero__(self):\n        return bool(self.__wrapped__)\n\n    def __bool__(self):\n        return bool(self.__wrapped__)\n\n    def __setattr__(self, name, value):\n        if name.startswith(\"_self_\"):\n            object.__setattr__(self, name, value)\n\n        elif name == \"__wrapped__\":\n            object.__setattr__(self, name, value)\n            try:\n                object.__delattr__(self, \"__qualname__\")\n            except AttributeError:\n                pass\n            try:\n                object.__setattr__(self, \"__qualname__\", value.__qualname__)\n            except AttributeError:\n                pass\n\n        elif name == \"__qualname__\":\n            setattr(self.__wrapped__, name, value)\n            object.__setattr__(self, name, value)\n\n        elif hasattr(type(self), name):\n            object.__setattr__(self, name, value)\n\n        else:\n            setattr(self.__wrapped__, name, value)\n\n    def __getattr__(self, name):\n        # If we are being to lookup '__wrapped__' then the\n        # '__init__()' method cannot have been called.\n\n        if name == \"__wrapped__\":\n            raise ValueError(\"wrapper has not been initialised\")\n\n        return getattr(self.__wrapped__, name)\n\n    def __delattr__(self, name):\n        if name.startswith(\"_self_\"):\n            object.__delattr__(self, name)\n\n        elif name == \"__wrapped__\":\n            raise TypeError(\"__wrapped__ must be an object\")\n\n        elif name == \"__qualname__\":\n            object.__delattr__(self, name)\n            delattr(self.__wrapped__, name)\n\n        elif hasattr(type(self), name):\n            object.__delattr__(self, name)\n\n        else:\n            delattr(self.__wrapped__, name)\n\n    def __add__(self, other):\n        return self.__wrapped__ + other\n\n    def __sub__(self, other):\n        return self.__wrapped__ - other\n\n    def __mul__(self, other):\n        return self.__wrapped__ * other\n\n    def __div__(self, other):\n        return operator.div(self.__wrapped__, other)\n\n    def __truediv__(self, other):\n        return operator.truediv(self.__wrapped__, other)\n\n    def __floordiv__(self, other):\n        return self.__wrapped__ // other\n\n    def __mod__(self, other):\n        return self.__wrapped__ % other\n\n    def __divmod__(self, other):\n        return divmod(self.__wrapped__, other)\n\n    def __pow__(self, other, *args):\n        return pow(self.__wrapped__, other, *args)\n\n    def __lshift__(self, other):\n        return self.__wrapped__ << other\n\n    def __rshift__(self, other):\n        return self.__wrapped__ >> other\n\n    def __and__(self, other):\n        return self.__wrapped__ & other\n\n    def __xor__(self, other):\n        return self.__wrapped__ ^ other\n\n    def __or__(self, other):\n        return self.__wrapped__ | other\n\n    def __radd__(self, other):\n        return other + self.__wrapped__\n\n    def __rsub__(self, other):\n        return other - self.__wrapped__\n\n    def __rmul__(self, other):\n        return other * self.__wrapped__\n\n    def __rdiv__(self, other):\n        return operator.div(other, self.__wrapped__)\n\n    def __rtruediv__(self, other):\n        return operator.truediv(other, self.__wrapped__)\n\n    def __rfloordiv__(self, other):\n        return other // self.__wrapped__\n\n    def __rmod__(self, other):\n        return other % self.__wrapped__\n\n    def __rdivmod__(self, other):\n        return divmod(other, self.__wrapped__)\n\n    def __rpow__(self, other, *args):\n        return pow(other, self.__wrapped__, *args)\n\n    def __rlshift__(self, other):\n        return other << self.__wrapped__\n\n    def __rrshift__(self, other):\n        return other >> self.__wrapped__\n\n    def __rand__(self, other):\n        return other & self.__wrapped__\n\n    def __rxor__(self, other):\n        return other ^ self.__wrapped__\n\n    def __ror__(self, other):\n        return other | self.__wrapped__\n\n    def __iadd__(self, other):\n        self.__wrapped__ += other\n        return self\n\n    def __isub__(self, other):\n        self.__wrapped__ -= other\n        return self\n\n    def __imul__(self, other):\n        self.__wrapped__ *= other\n        return self\n\n    def __idiv__(self, other):\n        self.__wrapped__ = operator.idiv(self.__wrapped__, other)\n        return self\n\n    def __itruediv__(self, other):\n        self.__wrapped__ = operator.itruediv(self.__wrapped__, other)\n        return self\n\n    def __ifloordiv__(self, other):\n        self.__wrapped__ //= other\n        return self\n\n    def __imod__(self, other):\n        self.__wrapped__ %= other\n        return self\n\n    def __ipow__(self, other):\n        self.__wrapped__ **= other\n        return self\n\n    def __ilshift__(self, other):\n        self.__wrapped__ <<= other\n        return self\n\n    def __irshift__(self, other):\n        self.__wrapped__ >>= other\n        return self\n\n    def __iand__(self, other):\n        self.__wrapped__ &= other\n        return self\n\n    def __ixor__(self, other):\n        self.__wrapped__ ^= other\n        return self\n\n    def __ior__(self, other):\n        self.__wrapped__ |= other\n        return self\n\n    def __neg__(self):\n        return -self.__wrapped__\n\n    def __pos__(self):\n        return +self.__wrapped__\n\n    def __abs__(self):\n        return abs(self.__wrapped__)\n\n    def __invert__(self):\n        return ~self.__wrapped__\n\n    def __int__(self):\n        return int(self.__wrapped__)\n\n    def __long__(self):\n        return long(self.__wrapped__)\n\n    def __float__(self):\n        return float(self.__wrapped__)\n\n    def __complex__(self):\n        return complex(self.__wrapped__)\n\n    def __oct__(self):\n        return oct(self.__wrapped__)\n\n    def __hex__(self):\n        return hex(self.__wrapped__)\n\n    def __index__(self):\n        return operator.index(self.__wrapped__)\n\n    def __len__(self):\n        return len(self.__wrapped__)\n\n    def __contains__(self, value):\n        return value in self.__wrapped__\n\n    def __getitem__(self, key):\n        return self.__wrapped__[key]\n\n    def __setitem__(self, key, value):\n        self.__wrapped__[key] = value\n\n    def __delitem__(self, key):\n        del self.__wrapped__[key]\n\n    def __getslice__(self, i, j):\n        return self.__wrapped__[i:j]\n\n    def __setslice__(self, i, j, value):\n        self.__wrapped__[i:j] = value\n\n    def __delslice__(self, i, j):\n        del self.__wrapped__[i:j]\n\n    def __enter__(self):\n        return self.__wrapped__.__enter__()\n\n    def __exit__(self, *args, **kwargs):\n        return self.__wrapped__.__exit__(*args, **kwargs)\n\n    def __iter__(self):\n        return iter(self.__wrapped__)\n\n    def __copy__(self):\n        raise NotImplementedError(\"object proxy must define __copy__()\")\n\n    def __deepcopy__(self, memo):\n        raise NotImplementedError(\"object proxy must define __deepcopy__()\")\n\n    def __reduce__(self):\n        raise NotImplementedError(\"object proxy must define __reduce_ex__()\")\n\n    def __reduce_ex__(self, protocol):\n        raise NotImplementedError(\"object proxy must define __reduce_ex__()\")",
  "class CallableObjectProxy(ObjectProxy):\n    def __call__(self, *args, **kwargs):\n        return self.__wrapped__(*args, **kwargs)",
  "class PartialCallableObjectProxy(ObjectProxy):\n    def __init__(self, *args, **kwargs):\n        if len(args) < 1:\n            raise TypeError(\"partial type takes at least one argument\")\n\n        wrapped, args = args[0], args[1:]\n\n        if not callable(wrapped):\n            raise TypeError(\"the first argument must be callable\")\n\n        super(PartialCallableObjectProxy, self).__init__(wrapped)\n\n        self._self_args = args\n        self._self_kwargs = kwargs\n\n    def __call__(self, *args, **kwargs):\n        _args = self._self_args + args\n\n        _kwargs = dict(self._self_kwargs)\n        _kwargs.update(kwargs)\n\n        return self.__wrapped__(*_args, **_kwargs)",
  "class _FunctionWrapperBase(ObjectProxy):\n\n    __slots__ = (\n        \"_self_instance\",\n        \"_self_wrapper\",\n        \"_self_enabled\",\n        \"_self_binding\",\n        \"_self_parent\",\n    )\n\n    def __init__(\n        self, wrapped, instance, wrapper, enabled=None, binding=\"function\", parent=None\n    ):\n\n        super(_FunctionWrapperBase, self).__init__(wrapped)\n\n        object.__setattr__(self, \"_self_instance\", instance)\n        object.__setattr__(self, \"_self_wrapper\", wrapper)\n        object.__setattr__(self, \"_self_enabled\", enabled)\n        object.__setattr__(self, \"_self_binding\", binding)\n        object.__setattr__(self, \"_self_parent\", parent)\n\n    def __get__(self, instance, owner):\n        # This method is actually doing double duty for both unbound and\n        # bound derived wrapper classes. It should possibly be broken up\n        # and the distinct functionality moved into the derived classes.\n        # Can't do that straight away due to some legacy code which is\n        # relying on it being here in this base class.\n        #\n        # The distinguishing attribute which determines whether we are\n        # being called in an unbound or bound wrapper is the parent\n        # attribute. If binding has never occurred, then the parent will\n        # be None.\n        #\n        # First therefore, is if we are called in an unbound wrapper. In\n        # this case we perform the binding.\n        #\n        # We have one special case to worry about here. This is where we\n        # are decorating a nested class. In this case the wrapped class\n        # would not have a __get__() method to call. In that case we\n        # simply return self.\n        #\n        # Note that we otherwise still do binding even if instance is\n        # None and accessing an unbound instance method from a class.\n        # This is because we need to be able to later detect that\n        # specific case as we will need to extract the instance from the\n        # first argument of those passed in.\n\n        if self._self_parent is None:\n            if not inspect.isclass(self.__wrapped__):\n                descriptor = self.__wrapped__.__get__(instance, owner)\n\n                return self.__bound_function_wrapper__(\n                    descriptor,\n                    instance,\n                    self._self_wrapper,\n                    self._self_enabled,\n                    self._self_binding,\n                    self,\n                )\n\n            return self\n\n        # Now we have the case of binding occurring a second time on what\n        # was already a bound function. In this case we would usually\n        # return ourselves again. This mirrors what Python does.\n        #\n        # The special case this time is where we were originally bound\n        # with an instance of None and we were likely an instance\n        # method. In that case we rebind against the original wrapped\n        # function from the parent again.\n\n        if self._self_instance is None and self._self_binding == \"function\":\n            descriptor = self._self_parent.__wrapped__.__get__(instance, owner)\n\n            return self._self_parent.__bound_function_wrapper__(\n                descriptor,\n                instance,\n                self._self_wrapper,\n                self._self_enabled,\n                self._self_binding,\n                self._self_parent,\n            )\n\n        return self\n\n    def __call__(self, *args, **kwargs):\n        # If enabled has been specified, then evaluate it at this point\n        # and if the wrapper is not to be executed, then simply return\n        # the bound function rather than a bound wrapper for the bound\n        # function. When evaluating enabled, if it is callable we call\n        # it, otherwise we evaluate it as a boolean.\n\n        if self._self_enabled is not None:\n            if callable(self._self_enabled):\n                if not self._self_enabled():\n                    return self.__wrapped__(*args, **kwargs)\n            elif not self._self_enabled:\n                return self.__wrapped__(*args, **kwargs)\n\n        # This can occur where initial function wrapper was applied to\n        # a function that was already bound to an instance. In that case\n        # we want to extract the instance from the function and use it.\n\n        if self._self_binding == \"function\":\n            if self._self_instance is None:\n                instance = getattr(self.__wrapped__, \"__self__\", None)\n                if instance is not None:\n                    return self._self_wrapper(self.__wrapped__, instance, args, kwargs)\n\n        # This is generally invoked when the wrapped function is being\n        # called as a normal function and is not bound to a class as an\n        # instance method. This is also invoked in the case where the\n        # wrapped function was a method, but this wrapper was in turn\n        # wrapped using the staticmethod decorator.\n\n        return self._self_wrapper(self.__wrapped__, self._self_instance, args, kwargs)",
  "class BoundFunctionWrapper(_FunctionWrapperBase):\n    def __call__(self, *args, **kwargs):\n        # If enabled has been specified, then evaluate it at this point\n        # and if the wrapper is not to be executed, then simply return\n        # the bound function rather than a bound wrapper for the bound\n        # function. When evaluating enabled, if it is callable we call\n        # it, otherwise we evaluate it as a boolean.\n\n        if self._self_enabled is not None:\n            if callable(self._self_enabled):\n                if not self._self_enabled():\n                    return self.__wrapped__(*args, **kwargs)\n            elif not self._self_enabled:\n                return self.__wrapped__(*args, **kwargs)\n\n        # We need to do things different depending on whether we are\n        # likely wrapping an instance method vs a static method or class\n        # method.\n\n        if self._self_binding == \"function\":\n            if self._self_instance is None:\n                # This situation can occur where someone is calling the\n                # instancemethod via the class type and passing the instance\n                # as the first argument. We need to shift the args before\n                # making the call to the wrapper and effectively bind the\n                # instance to the wrapped function using a partial so the\n                # wrapper doesn't see anything as being different.\n\n                if not args:\n                    raise TypeError(\"missing 1 required positional argument\")\n\n                instance, args = args[0], args[1:]\n                wrapped = PartialCallableObjectProxy(self.__wrapped__, instance)\n                return self._self_wrapper(wrapped, instance, args, kwargs)\n\n            return self._self_wrapper(\n                self.__wrapped__, self._self_instance, args, kwargs\n            )\n\n        else:\n            # As in this case we would be dealing with a classmethod or\n            # staticmethod, then _self_instance will only tell us whether\n            # when calling the classmethod or staticmethod they did it via an\n            # instance of the class it is bound to and not the case where\n            # done by the class type itself. We thus ignore _self_instance\n            # and use the __self__ attribute of the bound function instead.\n            # For a classmethod, this means instance will be the class type\n            # and for a staticmethod it will be None. This is probably the\n            # more useful thing we can pass through even though we loose\n            # knowledge of whether they were called on the instance vs the\n            # class type, as it reflects what they have available in the\n            # decoratored function.\n\n            instance = getattr(self.__wrapped__, \"__self__\", None)\n\n            return self._self_wrapper(self.__wrapped__, instance, args, kwargs)",
  "class FunctionWrapper(_FunctionWrapperBase):\n\n    __bound_function_wrapper__ = BoundFunctionWrapper\n\n    def __init__(self, wrapped, wrapper, enabled=None):\n        # What it is we are wrapping here could be anything. We need to\n        # try and detect specific cases though. In particular, we need\n        # to detect when we are given something that is a method of a\n        # class. Further, we need to know when it is likely an instance\n        # method, as opposed to a class or static method. This can\n        # become problematic though as there isn't strictly a fool proof\n        # method of knowing.\n        #\n        # The situations we could encounter when wrapping a method are:\n        #\n        # 1. The wrapper is being applied as part of a decorator which\n        # is a part of the class definition. In this case what we are\n        # given is the raw unbound function, classmethod or staticmethod\n        # wrapper objects.\n        #\n        # The problem here is that we will not know we are being applied\n        # in the context of the class being set up. This becomes\n        # important later for the case of an instance method, because in\n        # that case we just see it as a raw function and can't\n        # distinguish it from wrapping a normal function outside of\n        # a class context.\n        #\n        # 2. The wrapper is being applied when performing monkey\n        # patching of the class type afterwards and the method to be\n        # wrapped was retrieved direct from the __dict__ of the class\n        # type. This is effectively the same as (1) above.\n        #\n        # 3. The wrapper is being applied when performing monkey\n        # patching of the class type afterwards and the method to be\n        # wrapped was retrieved from the class type. In this case\n        # binding will have been performed where the instance against\n        # which the method is bound will be None at that point.\n        #\n        # This case is a problem because we can no longer tell if the\n        # method was a static method, plus if using Python3, we cannot\n        # tell if it was an instance method as the concept of an\n        # unnbound method no longer exists.\n        #\n        # 4. The wrapper is being applied when performing monkey\n        # patching of an instance of a class. In this case binding will\n        # have been perfomed where the instance was not None.\n        #\n        # This case is a problem because we can no longer tell if the\n        # method was a static method.\n        #\n        # Overall, the best we can do is look at the original type of the\n        # object which was wrapped prior to any binding being done and\n        # see if it is an instance of classmethod or staticmethod. In\n        # the case where other decorators are between us and them, if\n        # they do not propagate the __class__  attribute so that the\n        # isinstance() checks works, then likely this will do the wrong\n        # thing where classmethod and staticmethod are used.\n        #\n        # Since it is likely to be very rare that anyone even puts\n        # decorators around classmethod and staticmethod, likelihood of\n        # that being an issue is very small, so we accept it and suggest\n        # that those other decorators be fixed. It is also only an issue\n        # if a decorator wants to actually do things with the arguments.\n        #\n        # As to not being able to identify static methods properly, we\n        # just hope that that isn't something people are going to want\n        # to wrap, or if they do suggest they do it the correct way by\n        # ensuring that it is decorated in the class definition itself,\n        # or patch it in the __dict__ of the class type.\n        #\n        # So to get the best outcome we can, whenever we aren't sure what\n        # it is, we label it as a 'function'. If it was already bound and\n        # that is rebound later, we assume that it will be an instance\n        # method and try an cope with the possibility that the 'self'\n        # argument it being passed as an explicit argument and shuffle\n        # the arguments around to extract 'self' for use as the instance.\n\n        if isinstance(wrapped, classmethod):\n            binding = \"classmethod\"\n\n        elif isinstance(wrapped, staticmethod):\n            binding = \"staticmethod\"\n\n        elif hasattr(wrapped, \"__self__\"):\n            if inspect.isclass(wrapped.__self__):\n                binding = \"classmethod\"\n            else:\n                binding = \"function\"\n\n        else:\n            binding = \"function\"\n\n        super(FunctionWrapper, self).__init__(wrapped, None, wrapper, enabled, binding)",
  "def resolve_path(module, name):\n    if isinstance(module, string_types):\n        __import__(module)\n        module = sys.modules[module]\n\n    parent = module\n\n    path = name.split(\".\")\n    attribute = path[0]\n\n    original = getattr(parent, attribute)\n    for attribute in path[1:]:\n        parent = original\n\n        # We can't just always use getattr() because in doing\n        # that on a class it will cause binding to occur which\n        # will complicate things later and cause some things not\n        # to work. For the case of a class we therefore access\n        # the __dict__ directly. To cope though with the wrong\n        # class being given to us, or a method being moved into\n        # a base class, we need to walk the class hierarchy to\n        # work out exactly which __dict__ the method was defined\n        # in, as accessing it from __dict__ will fail if it was\n        # not actually on the class given. Fallback to using\n        # getattr() if we can't find it. If it truly doesn't\n        # exist, then that will fail.\n\n        if inspect.isclass(original):\n            for cls in inspect.getmro(original):\n                if attribute in vars(cls):\n                    original = vars(cls)[attribute]\n                    break\n            else:\n                original = getattr(original, attribute)\n\n        else:\n            original = getattr(original, attribute)\n\n    return (parent, attribute, original)",
  "def apply_patch(parent, attribute, replacement):\n    setattr(parent, attribute, replacement)",
  "def wrap_object(module, name, factory, args=(), kwargs={}):\n    (parent, attribute, original) = resolve_path(module, name)\n    wrapper = factory(original, *args, **kwargs)\n    apply_patch(parent, attribute, wrapper)\n    return wrapper",
  "class AttributeWrapper(object):\n    def __init__(self, attribute, factory, args, kwargs):\n        self.attribute = attribute\n        self.factory = factory\n        self.args = args\n        self.kwargs = kwargs\n\n    def __get__(self, instance, owner):\n        value = instance.__dict__[self.attribute]\n        return self.factory(value, *self.args, **self.kwargs)\n\n    def __set__(self, instance, value):\n        instance.__dict__[self.attribute] = value\n\n    def __delete__(self, instance):\n        del instance.__dict__[self.attribute]",
  "def wrap_object_attribute(module, name, factory, args=(), kwargs={}):\n    path, attribute = name.rsplit(\".\", 1)\n    parent = resolve_path(module, path)[2]\n    wrapper = AttributeWrapper(attribute, factory, args, kwargs)\n    apply_patch(parent, attribute, wrapper)\n    return wrapper",
  "def function_wrapper(wrapper):\n    def _wrapper(wrapped, instance, args, kwargs):\n        target_wrapped = args[0]\n        if instance is None:\n            target_wrapper = wrapper\n        elif inspect.isclass(instance):\n            target_wrapper = wrapper.__get__(None, instance)\n        else:\n            target_wrapper = wrapper.__get__(instance, type(instance))\n        return FunctionWrapper(target_wrapped, target_wrapper)\n\n    return FunctionWrapper(wrapper, _wrapper)",
  "def wrap_function_wrapper(module, name, wrapper):\n    return wrap_object(module, name, FunctionWrapper, (wrapper,))",
  "def patch_function_wrapper(module, name):\n    def _wrapper(wrapper):\n        return wrap_object(module, name, FunctionWrapper, (wrapper,))\n\n    return _wrapper",
  "def transient_function_wrapper(module, name):\n    def _decorator(wrapper):\n        def _wrapper(wrapped, instance, args, kwargs):\n            target_wrapped = args[0]\n            if instance is None:\n                target_wrapper = wrapper\n            elif inspect.isclass(instance):\n                target_wrapper = wrapper.__get__(None, instance)\n            else:\n                target_wrapper = wrapper.__get__(instance, type(instance))\n\n            def _execute(wrapped, instance, args, kwargs):\n                (parent, attribute, original) = resolve_path(module, name)\n                replacement = FunctionWrapper(original, target_wrapper)\n                setattr(parent, attribute, replacement)\n                try:\n                    return wrapped(*args, **kwargs)\n                finally:\n                    setattr(parent, attribute, original)\n\n            return FunctionWrapper(target_wrapped, _execute)\n\n        return FunctionWrapper(wrapper, _wrapper)\n\n    return _decorator",
  "def _weak_function_proxy_callback(ref, proxy, callback):\n    if proxy._self_expired:\n        return\n\n    proxy._self_expired = True\n\n    # This could raise an exception. We let it propagate back and let\n    # the weakref.proxy() deal with it, at which point it generally\n    # prints out a short error message direct to stderr and keeps going.\n\n    if callback is not None:\n        callback(proxy)",
  "class WeakFunctionProxy(ObjectProxy):\n\n    __slots__ = (\"_self_expired\", \"_self_instance\")\n\n    def __init__(self, wrapped, callback=None):\n        # We need to determine if the wrapped function is actually a\n        # bound method. In the case of a bound method, we need to keep a\n        # reference to the original unbound function and the instance.\n        # This is necessary because if we hold a reference to the bound\n        # function, it will be the only reference and given it is a\n        # temporary object, it will almost immediately expire and\n        # the weakref callback triggered. So what is done is that we\n        # hold a reference to the instance and unbound function and\n        # when called bind the function to the instance once again and\n        # then call it. Note that we avoid using a nested function for\n        # the callback here so as not to cause any odd reference cycles.\n\n        _callback = callback and functools.partial(\n            _weak_function_proxy_callback, proxy=self, callback=callback\n        )\n\n        self._self_expired = False\n\n        if isinstance(wrapped, _FunctionWrapperBase):\n            self._self_instance = weakref.ref(wrapped._self_instance, _callback)\n\n            if wrapped._self_parent is not None:\n                super(WeakFunctionProxy, self).__init__(\n                    weakref.proxy(wrapped._self_parent, _callback)\n                )\n\n            else:\n                super(WeakFunctionProxy, self).__init__(\n                    weakref.proxy(wrapped, _callback)\n                )\n\n            return\n\n        try:\n            self._self_instance = weakref.ref(wrapped.__self__, _callback)\n\n            super(WeakFunctionProxy, self).__init__(\n                weakref.proxy(wrapped.__func__, _callback)\n            )\n\n        except AttributeError:\n            self._self_instance = None\n\n            super(WeakFunctionProxy, self).__init__(weakref.proxy(wrapped, _callback))\n\n    def __call__(self, *args, **kwargs):\n        # We perform a boolean check here on the instance and wrapped\n        # function as that will trigger the reference error prior to\n        # calling if the reference had expired.\n\n        instance = self._self_instance and self._self_instance()\n        function = self.__wrapped__ and self.__wrapped__\n\n        # If the wrapped function was originally a bound function, for\n        # which we retained a reference to the instance and the unbound\n        # function we need to rebind the function and then call it. If\n        # not just called the wrapped function.\n\n        if instance is None:\n            return self.__wrapped__(*args, **kwargs)\n\n        return function.__get__(instance, type(instance))(*args, **kwargs)",
  "def __module__(self):\n        return self.__wrapped__.__module__",
  "def __module__(self, value):\n        self.__wrapped__.__module__ = value",
  "def __doc__(self):\n        return self.__wrapped__.__doc__",
  "def __doc__(self, value):\n        self.__wrapped__.__doc__ = value",
  "def __dict__(self):\n        return self.__wrapped__.__dict__",
  "def __weakref__(self):\n        return self.__wrapped__.__weakref__",
  "def __new__(cls, name, bases, dictionary):\n        # Copy our special properties into the class so that they\n        # always take precedence over attributes of the same name added\n        # during construction of a derived class. This is to save\n        # duplicating the implementation for them in all derived classes.\n\n        dictionary.update(vars(_ObjectProxyMethods))\n\n        return type.__new__(cls, name, bases, dictionary)",
  "def __init__(self, wrapped):\n        object.__setattr__(self, \"__wrapped__\", wrapped)\n\n        # Python 3.2+ has the __qualname__ attribute, but it does not\n        # allow it to be overridden using a property and it must instead\n        # be an actual string object instead.\n\n        try:\n            object.__setattr__(self, \"__qualname__\", wrapped.__qualname__)\n        except AttributeError:\n            pass",
  "def __name__(self):\n        return self.__wrapped__.__name__",
  "def __name__(self, value):\n        self.__wrapped__.__name__ = value",
  "def __class__(self):\n        return self.__wrapped__.__class__",
  "def __class__(self, value):\n        self.__wrapped__.__class__ = value",
  "def __annotations__(self):\n        return self.__wrapped__.__annotations__",
  "def __annotations__(self, value):\n        self.__wrapped__.__annotations__ = value",
  "def __dir__(self):\n        return dir(self.__wrapped__)",
  "def __str__(self):\n        return str(self.__wrapped__)",
  "def __repr__(self):\n        return \"<{} at 0x{:x} for {} at 0x{:x}>\".format(\n            type(self).__name__,\n            id(self),\n            type(self.__wrapped__).__name__,\n            id(self.__wrapped__),\n        )",
  "def __reversed__(self):\n        return reversed(self.__wrapped__)",
  "def __lt__(self, other):\n        return self.__wrapped__ < other",
  "def __le__(self, other):\n        return self.__wrapped__ <= other",
  "def __eq__(self, other):\n        return self.__wrapped__ == other",
  "def __ne__(self, other):\n        return self.__wrapped__ != other",
  "def __gt__(self, other):\n        return self.__wrapped__ > other",
  "def __ge__(self, other):\n        return self.__wrapped__ >= other",
  "def __hash__(self):\n        return hash(self.__wrapped__)",
  "def __nonzero__(self):\n        return bool(self.__wrapped__)",
  "def __bool__(self):\n        return bool(self.__wrapped__)",
  "def __setattr__(self, name, value):\n        if name.startswith(\"_self_\"):\n            object.__setattr__(self, name, value)\n\n        elif name == \"__wrapped__\":\n            object.__setattr__(self, name, value)\n            try:\n                object.__delattr__(self, \"__qualname__\")\n            except AttributeError:\n                pass\n            try:\n                object.__setattr__(self, \"__qualname__\", value.__qualname__)\n            except AttributeError:\n                pass\n\n        elif name == \"__qualname__\":\n            setattr(self.__wrapped__, name, value)\n            object.__setattr__(self, name, value)\n\n        elif hasattr(type(self), name):\n            object.__setattr__(self, name, value)\n\n        else:\n            setattr(self.__wrapped__, name, value)",
  "def __getattr__(self, name):\n        # If we are being to lookup '__wrapped__' then the\n        # '__init__()' method cannot have been called.\n\n        if name == \"__wrapped__\":\n            raise ValueError(\"wrapper has not been initialised\")\n\n        return getattr(self.__wrapped__, name)",
  "def __delattr__(self, name):\n        if name.startswith(\"_self_\"):\n            object.__delattr__(self, name)\n\n        elif name == \"__wrapped__\":\n            raise TypeError(\"__wrapped__ must be an object\")\n\n        elif name == \"__qualname__\":\n            object.__delattr__(self, name)\n            delattr(self.__wrapped__, name)\n\n        elif hasattr(type(self), name):\n            object.__delattr__(self, name)\n\n        else:\n            delattr(self.__wrapped__, name)",
  "def __add__(self, other):\n        return self.__wrapped__ + other",
  "def __sub__(self, other):\n        return self.__wrapped__ - other",
  "def __mul__(self, other):\n        return self.__wrapped__ * other",
  "def __div__(self, other):\n        return operator.div(self.__wrapped__, other)",
  "def __truediv__(self, other):\n        return operator.truediv(self.__wrapped__, other)",
  "def __floordiv__(self, other):\n        return self.__wrapped__ // other",
  "def __mod__(self, other):\n        return self.__wrapped__ % other",
  "def __divmod__(self, other):\n        return divmod(self.__wrapped__, other)",
  "def __pow__(self, other, *args):\n        return pow(self.__wrapped__, other, *args)",
  "def __lshift__(self, other):\n        return self.__wrapped__ << other",
  "def __rshift__(self, other):\n        return self.__wrapped__ >> other",
  "def __and__(self, other):\n        return self.__wrapped__ & other",
  "def __xor__(self, other):\n        return self.__wrapped__ ^ other",
  "def __or__(self, other):\n        return self.__wrapped__ | other",
  "def __radd__(self, other):\n        return other + self.__wrapped__",
  "def __rsub__(self, other):\n        return other - self.__wrapped__",
  "def __rmul__(self, other):\n        return other * self.__wrapped__",
  "def __rdiv__(self, other):\n        return operator.div(other, self.__wrapped__)",
  "def __rtruediv__(self, other):\n        return operator.truediv(other, self.__wrapped__)",
  "def __rfloordiv__(self, other):\n        return other // self.__wrapped__",
  "def __rmod__(self, other):\n        return other % self.__wrapped__",
  "def __rdivmod__(self, other):\n        return divmod(other, self.__wrapped__)",
  "def __rpow__(self, other, *args):\n        return pow(other, self.__wrapped__, *args)",
  "def __rlshift__(self, other):\n        return other << self.__wrapped__",
  "def __rrshift__(self, other):\n        return other >> self.__wrapped__",
  "def __rand__(self, other):\n        return other & self.__wrapped__",
  "def __rxor__(self, other):\n        return other ^ self.__wrapped__",
  "def __ror__(self, other):\n        return other | self.__wrapped__",
  "def __iadd__(self, other):\n        self.__wrapped__ += other\n        return self",
  "def __isub__(self, other):\n        self.__wrapped__ -= other\n        return self",
  "def __imul__(self, other):\n        self.__wrapped__ *= other\n        return self",
  "def __idiv__(self, other):\n        self.__wrapped__ = operator.idiv(self.__wrapped__, other)\n        return self",
  "def __itruediv__(self, other):\n        self.__wrapped__ = operator.itruediv(self.__wrapped__, other)\n        return self",
  "def __ifloordiv__(self, other):\n        self.__wrapped__ //= other\n        return self",
  "def __imod__(self, other):\n        self.__wrapped__ %= other\n        return self",
  "def __ipow__(self, other):\n        self.__wrapped__ **= other\n        return self",
  "def __ilshift__(self, other):\n        self.__wrapped__ <<= other\n        return self",
  "def __irshift__(self, other):\n        self.__wrapped__ >>= other\n        return self",
  "def __iand__(self, other):\n        self.__wrapped__ &= other\n        return self",
  "def __ixor__(self, other):\n        self.__wrapped__ ^= other\n        return self",
  "def __ior__(self, other):\n        self.__wrapped__ |= other\n        return self",
  "def __neg__(self):\n        return -self.__wrapped__",
  "def __pos__(self):\n        return +self.__wrapped__",
  "def __abs__(self):\n        return abs(self.__wrapped__)",
  "def __invert__(self):\n        return ~self.__wrapped__",
  "def __int__(self):\n        return int(self.__wrapped__)",
  "def __long__(self):\n        return long(self.__wrapped__)",
  "def __float__(self):\n        return float(self.__wrapped__)",
  "def __complex__(self):\n        return complex(self.__wrapped__)",
  "def __oct__(self):\n        return oct(self.__wrapped__)",
  "def __hex__(self):\n        return hex(self.__wrapped__)",
  "def __index__(self):\n        return operator.index(self.__wrapped__)",
  "def __len__(self):\n        return len(self.__wrapped__)",
  "def __contains__(self, value):\n        return value in self.__wrapped__",
  "def __getitem__(self, key):\n        return self.__wrapped__[key]",
  "def __setitem__(self, key, value):\n        self.__wrapped__[key] = value",
  "def __delitem__(self, key):\n        del self.__wrapped__[key]",
  "def __getslice__(self, i, j):\n        return self.__wrapped__[i:j]",
  "def __setslice__(self, i, j, value):\n        self.__wrapped__[i:j] = value",
  "def __delslice__(self, i, j):\n        del self.__wrapped__[i:j]",
  "def __enter__(self):\n        return self.__wrapped__.__enter__()",
  "def __exit__(self, *args, **kwargs):\n        return self.__wrapped__.__exit__(*args, **kwargs)",
  "def __iter__(self):\n        return iter(self.__wrapped__)",
  "def __copy__(self):\n        raise NotImplementedError(\"object proxy must define __copy__()\")",
  "def __deepcopy__(self, memo):\n        raise NotImplementedError(\"object proxy must define __deepcopy__()\")",
  "def __reduce__(self):\n        raise NotImplementedError(\"object proxy must define __reduce_ex__()\")",
  "def __reduce_ex__(self, protocol):\n        raise NotImplementedError(\"object proxy must define __reduce_ex__()\")",
  "def __call__(self, *args, **kwargs):\n        return self.__wrapped__(*args, **kwargs)",
  "def __init__(self, *args, **kwargs):\n        if len(args) < 1:\n            raise TypeError(\"partial type takes at least one argument\")\n\n        wrapped, args = args[0], args[1:]\n\n        if not callable(wrapped):\n            raise TypeError(\"the first argument must be callable\")\n\n        super(PartialCallableObjectProxy, self).__init__(wrapped)\n\n        self._self_args = args\n        self._self_kwargs = kwargs",
  "def __call__(self, *args, **kwargs):\n        _args = self._self_args + args\n\n        _kwargs = dict(self._self_kwargs)\n        _kwargs.update(kwargs)\n\n        return self.__wrapped__(*_args, **_kwargs)",
  "def __init__(\n        self, wrapped, instance, wrapper, enabled=None, binding=\"function\", parent=None\n    ):\n\n        super(_FunctionWrapperBase, self).__init__(wrapped)\n\n        object.__setattr__(self, \"_self_instance\", instance)\n        object.__setattr__(self, \"_self_wrapper\", wrapper)\n        object.__setattr__(self, \"_self_enabled\", enabled)\n        object.__setattr__(self, \"_self_binding\", binding)\n        object.__setattr__(self, \"_self_parent\", parent)",
  "def __get__(self, instance, owner):\n        # This method is actually doing double duty for both unbound and\n        # bound derived wrapper classes. It should possibly be broken up\n        # and the distinct functionality moved into the derived classes.\n        # Can't do that straight away due to some legacy code which is\n        # relying on it being here in this base class.\n        #\n        # The distinguishing attribute which determines whether we are\n        # being called in an unbound or bound wrapper is the parent\n        # attribute. If binding has never occurred, then the parent will\n        # be None.\n        #\n        # First therefore, is if we are called in an unbound wrapper. In\n        # this case we perform the binding.\n        #\n        # We have one special case to worry about here. This is where we\n        # are decorating a nested class. In this case the wrapped class\n        # would not have a __get__() method to call. In that case we\n        # simply return self.\n        #\n        # Note that we otherwise still do binding even if instance is\n        # None and accessing an unbound instance method from a class.\n        # This is because we need to be able to later detect that\n        # specific case as we will need to extract the instance from the\n        # first argument of those passed in.\n\n        if self._self_parent is None:\n            if not inspect.isclass(self.__wrapped__):\n                descriptor = self.__wrapped__.__get__(instance, owner)\n\n                return self.__bound_function_wrapper__(\n                    descriptor,\n                    instance,\n                    self._self_wrapper,\n                    self._self_enabled,\n                    self._self_binding,\n                    self,\n                )\n\n            return self\n\n        # Now we have the case of binding occurring a second time on what\n        # was already a bound function. In this case we would usually\n        # return ourselves again. This mirrors what Python does.\n        #\n        # The special case this time is where we were originally bound\n        # with an instance of None and we were likely an instance\n        # method. In that case we rebind against the original wrapped\n        # function from the parent again.\n\n        if self._self_instance is None and self._self_binding == \"function\":\n            descriptor = self._self_parent.__wrapped__.__get__(instance, owner)\n\n            return self._self_parent.__bound_function_wrapper__(\n                descriptor,\n                instance,\n                self._self_wrapper,\n                self._self_enabled,\n                self._self_binding,\n                self._self_parent,\n            )\n\n        return self",
  "def __call__(self, *args, **kwargs):\n        # If enabled has been specified, then evaluate it at this point\n        # and if the wrapper is not to be executed, then simply return\n        # the bound function rather than a bound wrapper for the bound\n        # function. When evaluating enabled, if it is callable we call\n        # it, otherwise we evaluate it as a boolean.\n\n        if self._self_enabled is not None:\n            if callable(self._self_enabled):\n                if not self._self_enabled():\n                    return self.__wrapped__(*args, **kwargs)\n            elif not self._self_enabled:\n                return self.__wrapped__(*args, **kwargs)\n\n        # This can occur where initial function wrapper was applied to\n        # a function that was already bound to an instance. In that case\n        # we want to extract the instance from the function and use it.\n\n        if self._self_binding == \"function\":\n            if self._self_instance is None:\n                instance = getattr(self.__wrapped__, \"__self__\", None)\n                if instance is not None:\n                    return self._self_wrapper(self.__wrapped__, instance, args, kwargs)\n\n        # This is generally invoked when the wrapped function is being\n        # called as a normal function and is not bound to a class as an\n        # instance method. This is also invoked in the case where the\n        # wrapped function was a method, but this wrapper was in turn\n        # wrapped using the staticmethod decorator.\n\n        return self._self_wrapper(self.__wrapped__, self._self_instance, args, kwargs)",
  "def __call__(self, *args, **kwargs):\n        # If enabled has been specified, then evaluate it at this point\n        # and if the wrapper is not to be executed, then simply return\n        # the bound function rather than a bound wrapper for the bound\n        # function. When evaluating enabled, if it is callable we call\n        # it, otherwise we evaluate it as a boolean.\n\n        if self._self_enabled is not None:\n            if callable(self._self_enabled):\n                if not self._self_enabled():\n                    return self.__wrapped__(*args, **kwargs)\n            elif not self._self_enabled:\n                return self.__wrapped__(*args, **kwargs)\n\n        # We need to do things different depending on whether we are\n        # likely wrapping an instance method vs a static method or class\n        # method.\n\n        if self._self_binding == \"function\":\n            if self._self_instance is None:\n                # This situation can occur where someone is calling the\n                # instancemethod via the class type and passing the instance\n                # as the first argument. We need to shift the args before\n                # making the call to the wrapper and effectively bind the\n                # instance to the wrapped function using a partial so the\n                # wrapper doesn't see anything as being different.\n\n                if not args:\n                    raise TypeError(\"missing 1 required positional argument\")\n\n                instance, args = args[0], args[1:]\n                wrapped = PartialCallableObjectProxy(self.__wrapped__, instance)\n                return self._self_wrapper(wrapped, instance, args, kwargs)\n\n            return self._self_wrapper(\n                self.__wrapped__, self._self_instance, args, kwargs\n            )\n\n        else:\n            # As in this case we would be dealing with a classmethod or\n            # staticmethod, then _self_instance will only tell us whether\n            # when calling the classmethod or staticmethod they did it via an\n            # instance of the class it is bound to and not the case where\n            # done by the class type itself. We thus ignore _self_instance\n            # and use the __self__ attribute of the bound function instead.\n            # For a classmethod, this means instance will be the class type\n            # and for a staticmethod it will be None. This is probably the\n            # more useful thing we can pass through even though we loose\n            # knowledge of whether they were called on the instance vs the\n            # class type, as it reflects what they have available in the\n            # decoratored function.\n\n            instance = getattr(self.__wrapped__, \"__self__\", None)\n\n            return self._self_wrapper(self.__wrapped__, instance, args, kwargs)",
  "def __init__(self, wrapped, wrapper, enabled=None):\n        # What it is we are wrapping here could be anything. We need to\n        # try and detect specific cases though. In particular, we need\n        # to detect when we are given something that is a method of a\n        # class. Further, we need to know when it is likely an instance\n        # method, as opposed to a class or static method. This can\n        # become problematic though as there isn't strictly a fool proof\n        # method of knowing.\n        #\n        # The situations we could encounter when wrapping a method are:\n        #\n        # 1. The wrapper is being applied as part of a decorator which\n        # is a part of the class definition. In this case what we are\n        # given is the raw unbound function, classmethod or staticmethod\n        # wrapper objects.\n        #\n        # The problem here is that we will not know we are being applied\n        # in the context of the class being set up. This becomes\n        # important later for the case of an instance method, because in\n        # that case we just see it as a raw function and can't\n        # distinguish it from wrapping a normal function outside of\n        # a class context.\n        #\n        # 2. The wrapper is being applied when performing monkey\n        # patching of the class type afterwards and the method to be\n        # wrapped was retrieved direct from the __dict__ of the class\n        # type. This is effectively the same as (1) above.\n        #\n        # 3. The wrapper is being applied when performing monkey\n        # patching of the class type afterwards and the method to be\n        # wrapped was retrieved from the class type. In this case\n        # binding will have been performed where the instance against\n        # which the method is bound will be None at that point.\n        #\n        # This case is a problem because we can no longer tell if the\n        # method was a static method, plus if using Python3, we cannot\n        # tell if it was an instance method as the concept of an\n        # unnbound method no longer exists.\n        #\n        # 4. The wrapper is being applied when performing monkey\n        # patching of an instance of a class. In this case binding will\n        # have been perfomed where the instance was not None.\n        #\n        # This case is a problem because we can no longer tell if the\n        # method was a static method.\n        #\n        # Overall, the best we can do is look at the original type of the\n        # object which was wrapped prior to any binding being done and\n        # see if it is an instance of classmethod or staticmethod. In\n        # the case where other decorators are between us and them, if\n        # they do not propagate the __class__  attribute so that the\n        # isinstance() checks works, then likely this will do the wrong\n        # thing where classmethod and staticmethod are used.\n        #\n        # Since it is likely to be very rare that anyone even puts\n        # decorators around classmethod and staticmethod, likelihood of\n        # that being an issue is very small, so we accept it and suggest\n        # that those other decorators be fixed. It is also only an issue\n        # if a decorator wants to actually do things with the arguments.\n        #\n        # As to not being able to identify static methods properly, we\n        # just hope that that isn't something people are going to want\n        # to wrap, or if they do suggest they do it the correct way by\n        # ensuring that it is decorated in the class definition itself,\n        # or patch it in the __dict__ of the class type.\n        #\n        # So to get the best outcome we can, whenever we aren't sure what\n        # it is, we label it as a 'function'. If it was already bound and\n        # that is rebound later, we assume that it will be an instance\n        # method and try an cope with the possibility that the 'self'\n        # argument it being passed as an explicit argument and shuffle\n        # the arguments around to extract 'self' for use as the instance.\n\n        if isinstance(wrapped, classmethod):\n            binding = \"classmethod\"\n\n        elif isinstance(wrapped, staticmethod):\n            binding = \"staticmethod\"\n\n        elif hasattr(wrapped, \"__self__\"):\n            if inspect.isclass(wrapped.__self__):\n                binding = \"classmethod\"\n            else:\n                binding = \"function\"\n\n        else:\n            binding = \"function\"\n\n        super(FunctionWrapper, self).__init__(wrapped, None, wrapper, enabled, binding)",
  "def __init__(self, attribute, factory, args, kwargs):\n        self.attribute = attribute\n        self.factory = factory\n        self.args = args\n        self.kwargs = kwargs",
  "def __get__(self, instance, owner):\n        value = instance.__dict__[self.attribute]\n        return self.factory(value, *self.args, **self.kwargs)",
  "def __set__(self, instance, value):\n        instance.__dict__[self.attribute] = value",
  "def __delete__(self, instance):\n        del instance.__dict__[self.attribute]",
  "def _wrapper(wrapped, instance, args, kwargs):\n        target_wrapped = args[0]\n        if instance is None:\n            target_wrapper = wrapper\n        elif inspect.isclass(instance):\n            target_wrapper = wrapper.__get__(None, instance)\n        else:\n            target_wrapper = wrapper.__get__(instance, type(instance))\n        return FunctionWrapper(target_wrapped, target_wrapper)",
  "def _wrapper(wrapper):\n        return wrap_object(module, name, FunctionWrapper, (wrapper,))",
  "def _decorator(wrapper):\n        def _wrapper(wrapped, instance, args, kwargs):\n            target_wrapped = args[0]\n            if instance is None:\n                target_wrapper = wrapper\n            elif inspect.isclass(instance):\n                target_wrapper = wrapper.__get__(None, instance)\n            else:\n                target_wrapper = wrapper.__get__(instance, type(instance))\n\n            def _execute(wrapped, instance, args, kwargs):\n                (parent, attribute, original) = resolve_path(module, name)\n                replacement = FunctionWrapper(original, target_wrapper)\n                setattr(parent, attribute, replacement)\n                try:\n                    return wrapped(*args, **kwargs)\n                finally:\n                    setattr(parent, attribute, original)\n\n            return FunctionWrapper(target_wrapped, _execute)\n\n        return FunctionWrapper(wrapper, _wrapper)",
  "def __init__(self, wrapped, callback=None):\n        # We need to determine if the wrapped function is actually a\n        # bound method. In the case of a bound method, we need to keep a\n        # reference to the original unbound function and the instance.\n        # This is necessary because if we hold a reference to the bound\n        # function, it will be the only reference and given it is a\n        # temporary object, it will almost immediately expire and\n        # the weakref callback triggered. So what is done is that we\n        # hold a reference to the instance and unbound function and\n        # when called bind the function to the instance once again and\n        # then call it. Note that we avoid using a nested function for\n        # the callback here so as not to cause any odd reference cycles.\n\n        _callback = callback and functools.partial(\n            _weak_function_proxy_callback, proxy=self, callback=callback\n        )\n\n        self._self_expired = False\n\n        if isinstance(wrapped, _FunctionWrapperBase):\n            self._self_instance = weakref.ref(wrapped._self_instance, _callback)\n\n            if wrapped._self_parent is not None:\n                super(WeakFunctionProxy, self).__init__(\n                    weakref.proxy(wrapped._self_parent, _callback)\n                )\n\n            else:\n                super(WeakFunctionProxy, self).__init__(\n                    weakref.proxy(wrapped, _callback)\n                )\n\n            return\n\n        try:\n            self._self_instance = weakref.ref(wrapped.__self__, _callback)\n\n            super(WeakFunctionProxy, self).__init__(\n                weakref.proxy(wrapped.__func__, _callback)\n            )\n\n        except AttributeError:\n            self._self_instance = None\n\n            super(WeakFunctionProxy, self).__init__(weakref.proxy(wrapped, _callback))",
  "def __call__(self, *args, **kwargs):\n        # We perform a boolean check here on the instance and wrapped\n        # function as that will trigger the reference error prior to\n        # calling if the reference had expired.\n\n        instance = self._self_instance and self._self_instance()\n        function = self.__wrapped__ and self.__wrapped__\n\n        # If the wrapped function was originally a bound function, for\n        # which we retained a reference to the instance and the unbound\n        # function we need to rebind the function and then call it. If\n        # not just called the wrapped function.\n\n        if instance is None:\n            return self.__wrapped__(*args, **kwargs)\n\n        return function.__get__(instance, type(instance))(*args, **kwargs)",
  "def __bytes__(self):\n            return bytes(self.__wrapped__)",
  "def __round__(self):\n            return round(self.__wrapped__)",
  "def _wrapper(wrapped, instance, args, kwargs):\n            target_wrapped = args[0]\n            if instance is None:\n                target_wrapper = wrapper\n            elif inspect.isclass(instance):\n                target_wrapper = wrapper.__get__(None, instance)\n            else:\n                target_wrapper = wrapper.__get__(instance, type(instance))\n\n            def _execute(wrapped, instance, args, kwargs):\n                (parent, attribute, original) = resolve_path(module, name)\n                replacement = FunctionWrapper(original, target_wrapper)\n                setattr(parent, attribute, replacement)\n                try:\n                    return wrapped(*args, **kwargs)\n                finally:\n                    setattr(parent, attribute, original)\n\n            return FunctionWrapper(target_wrapped, _execute)",
  "def _execute(wrapped, instance, args, kwargs):\n                (parent, attribute, original) = resolve_path(module, name)\n                replacement = FunctionWrapper(original, target_wrapper)\n                setattr(parent, attribute, replacement)\n                try:\n                    return wrapped(*args, **kwargs)\n                finally:\n                    setattr(parent, attribute, original)"
]