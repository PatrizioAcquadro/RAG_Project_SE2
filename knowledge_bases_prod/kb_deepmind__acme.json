[
  "def generate_requirements_file(path=None):\n  \"\"\"Generates requirements.txt file with the Acme's dependencies.\n\n  It is used by Launchpad GCP runtime to generate Acme requirements to be\n  installed inside the docker image. Acme itself is not installed from pypi,\n  but instead sources are copied over to reflect any local changes made to\n  the codebase.\n\n  Args:\n    path: path to the requirements.txt file to generate.\n  \"\"\"\n  if not path:\n    path = os.path.join(os.path.dirname(__file__), 'acme/requirements.txt')\n  with open(path, 'w') as f:\n    for package in set(core_requirements + jax_requirements + tf_requirements +\n                       envs_requirements):\n      f.write(f'{package}\\n')",
  "class BuildPy(setuptools.command.build_py.build_py):\n\n  def run(self):\n    generate_requirements_file()\n    setuptools.command.build_py.build_py.run(self)",
  "class Develop(setuptools.command.develop.develop):\n\n  def run(self):\n    generate_requirements_file()\n    setuptools.command.develop.develop.run(self)",
  "def run(self):\n    generate_requirements_file()\n    setuptools.command.build_py.build_py.run(self)",
  "def run(self):\n    generate_requirements_file()\n    setuptools.command.develop.develop.run(self)",
  "class EnvironmentLoop(core.Worker):\n  \"\"\"A simple RL environment loop.\n\n  This takes `Environment` and `Actor` instances and coordinates their\n  interaction. Agent is updated if `should_update=True`. This can be used as:\n\n    loop = EnvironmentLoop(environment, actor)\n    loop.run(num_episodes)\n\n  A `Counter` instance can optionally be given in order to maintain counts\n  between different Acme components. If not given a local Counter will be\n  created to maintain counts between calls to the `run` method.\n\n  A `Logger` instance can also be passed in order to control the output of the\n  loop. If not given a platform-specific default logger will be used as defined\n  by utils.loggers.make_default_logger. A string `label` can be passed to easily\n  change the label associated with the default logger; this is ignored if a\n  `Logger` instance is given.\n\n  A list of 'Observer' instances can be specified to generate additional metrics\n  to be logged by the logger. They have access to the 'Environment' instance,\n  the current timestep datastruct and the current action.\n  \"\"\"\n\n  def __init__(\n      self,\n      environment: dm_env.Environment,\n      actor: core.Actor,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      should_update: bool = True,\n      label: str = 'environment_loop',\n      observers: Sequence[observers_lib.EnvLoopObserver] = (),\n  ):\n    # Internalize agent and environment.\n    self._environment = environment\n    self._actor = actor\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger(\n        label, steps_key=self._counter.get_steps_key())\n    self._should_update = should_update\n    self._observers = observers\n\n  def run_episode(self) -> loggers.LoggingData:\n    \"\"\"Run one episode.\n\n    Each episode is a loop which interacts first with the environment to get an\n    observation and then give that observation to the agent in order to retrieve\n    an action.\n\n    Returns:\n      An instance of `loggers.LoggingData`.\n    \"\"\"\n    # Reset any counts and start the environment.\n    episode_start_time = time.time()\n    select_action_durations: List[float] = []\n    env_step_durations: List[float] = []\n    episode_steps: int = 0\n\n    # For evaluation, this keeps track of the total undiscounted reward\n    # accumulated during the episode.\n    episode_return = tree.map_structure(_generate_zeros_from_spec,\n                                        self._environment.reward_spec())\n    env_reset_start = time.time()\n    timestep = self._environment.reset()\n    env_reset_duration = time.time() - env_reset_start\n    # Make the first observation.\n    self._actor.observe_first(timestep)\n    for observer in self._observers:\n      # Initialize the observer with the current state of the env after reset\n      # and the initial timestep.\n      observer.observe_first(self._environment, timestep)\n\n    # Run an episode.\n    while not timestep.last():\n      # Book-keeping.\n      episode_steps += 1\n\n      # Generate an action from the agent's policy.\n      select_action_start = time.time()\n      action = self._actor.select_action(timestep.observation)\n      select_action_durations.append(time.time() - select_action_start)\n\n      # Step the environment with the agent's selected action.\n      env_step_start = time.time()\n      timestep = self._environment.step(action)\n      env_step_durations.append(time.time() - env_step_start)\n\n      # Have the agent and observers observe the timestep.\n      self._actor.observe(action, next_timestep=timestep)\n      for observer in self._observers:\n        # One environment step was completed. Observe the current state of the\n        # environment, the current timestep and the action.\n        observer.observe(self._environment, timestep, action)\n\n      # Give the actor the opportunity to update itself.\n      if self._should_update:\n        self._actor.update()\n\n      # Equivalent to: episode_return += timestep.reward\n      # We capture the return value because if timestep.reward is a JAX\n      # DeviceArray, episode_return will not be mutated in-place. (In all other\n      # cases, the returned episode_return will be the same object as the\n      # argument episode_return.)\n      episode_return = tree.map_structure(operator.iadd,\n                                          episode_return,\n                                          timestep.reward)\n\n    # Record counts.\n    counts = self._counter.increment(episodes=1, steps=episode_steps)\n\n    # Collect the results and combine with counts.\n    steps_per_second = episode_steps / (time.time() - episode_start_time)\n    result = {\n        'episode_length': episode_steps,\n        'episode_return': episode_return,\n        'steps_per_second': steps_per_second,\n        'env_reset_duration_sec': env_reset_duration,\n        'select_action_duration_sec': np.mean(select_action_durations),\n        'env_step_duration_sec': np.mean(env_step_durations),\n    }\n    result.update(counts)\n    for observer in self._observers:\n      result.update(observer.get_metrics())\n    return result\n\n  def run(\n      self,\n      num_episodes: Optional[int] = None,\n      num_steps: Optional[int] = None,\n  ) -> int:\n    \"\"\"Perform the run loop.\n\n    Run the environment loop either for `num_episodes` episodes or for at\n    least `num_steps` steps (the last episode is always run until completion,\n    so the total number of steps may be slightly more than `num_steps`).\n    At least one of these two arguments has to be None.\n\n    Upon termination of an episode a new episode will be started. If the number\n    of episodes and the number of steps are not given then this will interact\n    with the environment infinitely.\n\n    Args:\n      num_episodes: number of episodes to run the loop for.\n      num_steps: minimal number of steps to run the loop for.\n\n    Returns:\n      Actual number of steps the loop executed.\n\n    Raises:\n      ValueError: If both 'num_episodes' and 'num_steps' are not None.\n    \"\"\"\n\n    if not (num_episodes is None or num_steps is None):\n      raise ValueError('Either \"num_episodes\" or \"num_steps\" should be None.')\n\n    def should_terminate(episode_count: int, step_count: int) -> bool:\n      return ((num_episodes is not None and episode_count >= num_episodes) or\n              (num_steps is not None and step_count >= num_steps))\n\n    episode_count: int = 0\n    step_count: int = 0\n    with signals.runtime_terminator():\n      while not should_terminate(episode_count, step_count):\n        episode_start = time.time()\n        result = self.run_episode()\n        result = {**result, **{'episode_duration': time.time() - episode_start}}\n        episode_count += 1\n        step_count += int(result['episode_length'])\n        # Log the given episode results.\n        self._logger.write(result)\n\n    return step_count",
  "def _generate_zeros_from_spec(spec: specs.Array) -> np.ndarray:\n  return np.zeros(spec.shape, spec.dtype)",
  "def __init__(\n      self,\n      environment: dm_env.Environment,\n      actor: core.Actor,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      should_update: bool = True,\n      label: str = 'environment_loop',\n      observers: Sequence[observers_lib.EnvLoopObserver] = (),\n  ):\n    # Internalize agent and environment.\n    self._environment = environment\n    self._actor = actor\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger(\n        label, steps_key=self._counter.get_steps_key())\n    self._should_update = should_update\n    self._observers = observers",
  "def run_episode(self) -> loggers.LoggingData:\n    \"\"\"Run one episode.\n\n    Each episode is a loop which interacts first with the environment to get an\n    observation and then give that observation to the agent in order to retrieve\n    an action.\n\n    Returns:\n      An instance of `loggers.LoggingData`.\n    \"\"\"\n    # Reset any counts and start the environment.\n    episode_start_time = time.time()\n    select_action_durations: List[float] = []\n    env_step_durations: List[float] = []\n    episode_steps: int = 0\n\n    # For evaluation, this keeps track of the total undiscounted reward\n    # accumulated during the episode.\n    episode_return = tree.map_structure(_generate_zeros_from_spec,\n                                        self._environment.reward_spec())\n    env_reset_start = time.time()\n    timestep = self._environment.reset()\n    env_reset_duration = time.time() - env_reset_start\n    # Make the first observation.\n    self._actor.observe_first(timestep)\n    for observer in self._observers:\n      # Initialize the observer with the current state of the env after reset\n      # and the initial timestep.\n      observer.observe_first(self._environment, timestep)\n\n    # Run an episode.\n    while not timestep.last():\n      # Book-keeping.\n      episode_steps += 1\n\n      # Generate an action from the agent's policy.\n      select_action_start = time.time()\n      action = self._actor.select_action(timestep.observation)\n      select_action_durations.append(time.time() - select_action_start)\n\n      # Step the environment with the agent's selected action.\n      env_step_start = time.time()\n      timestep = self._environment.step(action)\n      env_step_durations.append(time.time() - env_step_start)\n\n      # Have the agent and observers observe the timestep.\n      self._actor.observe(action, next_timestep=timestep)\n      for observer in self._observers:\n        # One environment step was completed. Observe the current state of the\n        # environment, the current timestep and the action.\n        observer.observe(self._environment, timestep, action)\n\n      # Give the actor the opportunity to update itself.\n      if self._should_update:\n        self._actor.update()\n\n      # Equivalent to: episode_return += timestep.reward\n      # We capture the return value because if timestep.reward is a JAX\n      # DeviceArray, episode_return will not be mutated in-place. (In all other\n      # cases, the returned episode_return will be the same object as the\n      # argument episode_return.)\n      episode_return = tree.map_structure(operator.iadd,\n                                          episode_return,\n                                          timestep.reward)\n\n    # Record counts.\n    counts = self._counter.increment(episodes=1, steps=episode_steps)\n\n    # Collect the results and combine with counts.\n    steps_per_second = episode_steps / (time.time() - episode_start_time)\n    result = {\n        'episode_length': episode_steps,\n        'episode_return': episode_return,\n        'steps_per_second': steps_per_second,\n        'env_reset_duration_sec': env_reset_duration,\n        'select_action_duration_sec': np.mean(select_action_durations),\n        'env_step_duration_sec': np.mean(env_step_durations),\n    }\n    result.update(counts)\n    for observer in self._observers:\n      result.update(observer.get_metrics())\n    return result",
  "def run(\n      self,\n      num_episodes: Optional[int] = None,\n      num_steps: Optional[int] = None,\n  ) -> int:\n    \"\"\"Perform the run loop.\n\n    Run the environment loop either for `num_episodes` episodes or for at\n    least `num_steps` steps (the last episode is always run until completion,\n    so the total number of steps may be slightly more than `num_steps`).\n    At least one of these two arguments has to be None.\n\n    Upon termination of an episode a new episode will be started. If the number\n    of episodes and the number of steps are not given then this will interact\n    with the environment infinitely.\n\n    Args:\n      num_episodes: number of episodes to run the loop for.\n      num_steps: minimal number of steps to run the loop for.\n\n    Returns:\n      Actual number of steps the loop executed.\n\n    Raises:\n      ValueError: If both 'num_episodes' and 'num_steps' are not None.\n    \"\"\"\n\n    if not (num_episodes is None or num_steps is None):\n      raise ValueError('Either \"num_episodes\" or \"num_steps\" should be None.')\n\n    def should_terminate(episode_count: int, step_count: int) -> bool:\n      return ((num_episodes is not None and episode_count >= num_episodes) or\n              (num_steps is not None and step_count >= num_steps))\n\n    episode_count: int = 0\n    step_count: int = 0\n    with signals.runtime_terminator():\n      while not should_terminate(episode_count, step_count):\n        episode_start = time.time()\n        result = self.run_episode()\n        result = {**result, **{'episode_duration': time.time() - episode_start}}\n        episode_count += 1\n        step_count += int(result['episode_length'])\n        # Log the given episode results.\n        self._logger.write(result)\n\n    return step_count",
  "def should_terminate(episode_count: int, step_count: int) -> bool:\n      return ((num_episodes is not None and episode_count >= num_episodes) or\n              (num_steps is not None and step_count >= num_steps))",
  "class Batches(int):\n  \"\"\"Helper class for specification of quantities in units of batches.\n\n  Example usage:\n\n      # Configure the batch size and replay size in units of batches.\n      config.batch_size = 32\n      config.replay_size = Batches(4)\n\n      # ...\n\n      # Convert the replay size at runtime.\n      if isinstance(config.replay_size, Batches):\n        config.replay_size = config.replay_size * config.batch_size  # int: 128\n\n  \"\"\"",
  "class Transition(NamedTuple):\n  \"\"\"Container for a transition.\"\"\"\n  observation: NestedArray\n  action: NestedArray\n  reward: NestedArray\n  discount: NestedArray\n  next_observation: NestedArray\n  extras: NestedArray = ()",
  "class StepCountingLearner(core.Learner):\n  \"\"\"A learner which counts `num_steps` and then raises `StopIteration`.\"\"\"\n\n  def __init__(self, num_steps: int):\n    self.step_count = 0\n    self.num_steps = num_steps\n\n  def step(self):\n    self.step_count += 1\n    if self.step_count >= self.num_steps:\n      raise StopIteration()\n\n  def get_variables(self, unused: List[str]) -> List[types.NestedArray]:\n    del unused\n    return []",
  "class CoreTest(absltest.TestCase):\n\n  def test_learner_run_with_limit(self):\n    learner = StepCountingLearner(100)\n    learner.run(7)\n    self.assertEqual(learner.step_count, 7)\n\n  def test_learner_run_no_limit(self):\n    learner = StepCountingLearner(100)\n    with self.assertRaises(StopIteration):\n      learner.run()\n    self.assertEqual(learner.step_count, 100)",
  "def __init__(self, num_steps: int):\n    self.step_count = 0\n    self.num_steps = num_steps",
  "def step(self):\n    self.step_count += 1\n    if self.step_count >= self.num_steps:\n      raise StopIteration()",
  "def get_variables(self, unused: List[str]) -> List[types.NestedArray]:\n    del unused\n    return []",
  "def test_learner_run_with_limit(self):\n    learner = StepCountingLearner(100)\n    learner.run(7)\n    self.assertEqual(learner.step_count, 7)",
  "def test_learner_run_no_limit(self):\n    learner = StepCountingLearner(100)\n    with self.assertRaises(StopIteration):\n      learner.run()\n    self.assertEqual(learner.step_count, 100)",
  "class Actor(abc.ABC):\n  \"\"\"Interface for an agent that can act.\n\n  This interface defines an API for an Actor to interact with an EnvironmentLoop\n  (see acme.environment_loop), e.g. a simple RL loop where each step is of the\n  form:\n\n    # Make the first observation.\n    timestep = env.reset()\n    actor.observe_first(timestep)\n\n    # Take a step and observe.\n    action = actor.select_action(timestep.observation)\n    next_timestep = env.step(action)\n    actor.observe(action, next_timestep)\n\n    # Update the actor policy/parameters.\n    actor.update()\n  \"\"\"\n\n  @abc.abstractmethod\n  def select_action(self, observation: types.NestedArray) -> types.NestedArray:\n    \"\"\"Samples from the policy and returns an action.\"\"\"\n\n  @abc.abstractmethod\n  def observe_first(self, timestep: dm_env.TimeStep):\n    \"\"\"Make a first observation from the environment.\n\n    Note that this need not be an initial state, it is merely beginning the\n    recording of a trajectory.\n\n    Args:\n      timestep: first timestep.\n    \"\"\"\n\n  @abc.abstractmethod\n  def observe(\n      self,\n      action: types.NestedArray,\n      next_timestep: dm_env.TimeStep,\n  ):\n    \"\"\"Make an observation of timestep data from the environment.\n\n    Args:\n      action: action taken in the environment.\n      next_timestep: timestep produced by the environment given the action.\n    \"\"\"\n\n  @abc.abstractmethod\n  def update(self, wait: bool = False):\n    \"\"\"Perform an update of the actor parameters from past observations.\n\n    Args:\n      wait: if True, the update will be blocking.\n    \"\"\"",
  "class VariableSource(abc.ABC):\n  \"\"\"Abstract source of variables.\n\n  Objects which implement this interface provide a source of variables, returned\n  as a collection of (nested) numpy arrays. Generally this will be used to\n  provide variables to some learned policy/etc.\n  \"\"\"\n\n  @abc.abstractmethod\n  def get_variables(self, names: Sequence[str]) -> List[types.NestedArray]:\n    \"\"\"Return the named variables as a collection of (nested) numpy arrays.\n\n    Args:\n      names: args where each name is a string identifying a predefined subset of\n        the variables.\n\n    Returns:\n      A list of (nested) numpy arrays `variables` such that `variables[i]`\n      corresponds to the collection named by `names[i]`.\n    \"\"\"",
  "class Worker(abc.ABC):\n  \"\"\"An interface for (potentially) distributed workers.\"\"\"\n\n  @abc.abstractmethod\n  def run(self):\n    \"\"\"Runs the worker.\"\"\"",
  "class Saveable(abc.ABC, Generic[T]):\n  \"\"\"An interface for saveable objects.\"\"\"\n\n  @abc.abstractmethod\n  def save(self) -> T:\n    \"\"\"Returns the state from the object to be saved.\"\"\"\n\n  @abc.abstractmethod\n  def restore(self, state: T):\n    \"\"\"Given the state, restores the object.\"\"\"",
  "class Learner(VariableSource, Worker, Saveable):\n  \"\"\"Abstract learner object.\n\n  This corresponds to an object which implements a learning loop. A single step\n  of learning should be implemented via the `step` method and this step\n  is generally interacted with via the `run` method which runs update\n  continuously.\n\n  All objects implementing this interface should also be able to take in an\n  external dataset (see acme.datasets) and run updates using data from this\n  dataset. This can be accomplished by explicitly running `learner.step()`\n  inside a for/while loop or by using the `learner.run()` convenience function.\n  Data will be read from this dataset asynchronously and this is primarily\n  useful when the dataset is filled by an external process.\n  \"\"\"\n\n  @abc.abstractmethod\n  def step(self):\n    \"\"\"Perform an update step of the learner's parameters.\"\"\"\n\n  def run(self, num_steps: Optional[int] = None) -> None:\n    \"\"\"Run the update loop; typically an infinite loop which calls step.\"\"\"\n\n    iterator = range(num_steps) if num_steps is not None else itertools.count()\n\n    for _ in iterator:\n      self.step()\n\n  def save(self):\n    raise NotImplementedError('Method \"save\" is not implemented.')\n\n  def restore(self, state):\n    raise NotImplementedError('Method \"restore\" is not implemented.')",
  "class PrefetchingIterator(Iterator[T], abc.ABC):\n  \"\"\"Abstract iterator object which supports `ready` method.\"\"\"\n\n  @abc.abstractmethod\n  def ready(self) -> bool:\n    \"\"\"Is there any data waiting for processing.\"\"\"\n\n  @abc.abstractmethod\n  def retrieved_elements(self) -> int:\n    \"\"\"How many elements were retrieved from the iterator.\"\"\"",
  "def select_action(self, observation: types.NestedArray) -> types.NestedArray:\n    \"\"\"Samples from the policy and returns an action.\"\"\"",
  "def observe_first(self, timestep: dm_env.TimeStep):\n    \"\"\"Make a first observation from the environment.\n\n    Note that this need not be an initial state, it is merely beginning the\n    recording of a trajectory.\n\n    Args:\n      timestep: first timestep.\n    \"\"\"",
  "def observe(\n      self,\n      action: types.NestedArray,\n      next_timestep: dm_env.TimeStep,\n  ):\n    \"\"\"Make an observation of timestep data from the environment.\n\n    Args:\n      action: action taken in the environment.\n      next_timestep: timestep produced by the environment given the action.\n    \"\"\"",
  "def update(self, wait: bool = False):\n    \"\"\"Perform an update of the actor parameters from past observations.\n\n    Args:\n      wait: if True, the update will be blocking.\n    \"\"\"",
  "def get_variables(self, names: Sequence[str]) -> List[types.NestedArray]:\n    \"\"\"Return the named variables as a collection of (nested) numpy arrays.\n\n    Args:\n      names: args where each name is a string identifying a predefined subset of\n        the variables.\n\n    Returns:\n      A list of (nested) numpy arrays `variables` such that `variables[i]`\n      corresponds to the collection named by `names[i]`.\n    \"\"\"",
  "def run(self):\n    \"\"\"Runs the worker.\"\"\"",
  "def save(self) -> T:\n    \"\"\"Returns the state from the object to be saved.\"\"\"",
  "def restore(self, state: T):\n    \"\"\"Given the state, restores the object.\"\"\"",
  "def step(self):\n    \"\"\"Perform an update step of the learner's parameters.\"\"\"",
  "def run(self, num_steps: Optional[int] = None) -> None:\n    \"\"\"Run the update loop; typically an infinite loop which calls step.\"\"\"\n\n    iterator = range(num_steps) if num_steps is not None else itertools.count()\n\n    for _ in iterator:\n      self.step()",
  "def save(self):\n    raise NotImplementedError('Method \"save\" is not implemented.')",
  "def restore(self, state):\n    raise NotImplementedError('Method \"restore\" is not implemented.')",
  "def ready(self) -> bool:\n    \"\"\"Is there any data waiting for processing.\"\"\"",
  "def retrieved_elements(self) -> int:\n    \"\"\"How many elements were retrieved from the iterator.\"\"\"",
  "class EnvironmentSpec(NamedTuple):\n  \"\"\"Full specification of the domains used by a given environment.\"\"\"\n  # TODO(b/144758674): Use NestedSpec type here.\n  observations: Any\n  actions: Any\n  rewards: Any\n  discounts: Any",
  "def make_environment_spec(environment: dm_env.Environment) -> EnvironmentSpec:\n  \"\"\"Returns an `EnvironmentSpec` describing values used by an environment.\"\"\"\n  return EnvironmentSpec(\n      observations=environment.observation_spec(),\n      actions=environment.action_spec(),\n      rewards=environment.reward_spec(),\n      discounts=environment.discount_spec())",
  "class EnvironmentLoopTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(*TEST_CASES)\n  def test_one_episode(self, discount_spec, reward_spec):\n    _, loop = _parameterized_setup(discount_spec, reward_spec)\n    result = loop.run_episode()\n    self.assertIn('episode_length', result)\n    self.assertEqual(EPISODE_LENGTH, result['episode_length'])\n    self.assertIn('episode_return', result)\n    self.assertIn('steps_per_second', result)\n\n  @parameterized.named_parameters(*TEST_CASES)\n  def test_run_episodes(self, discount_spec, reward_spec):\n    actor, loop = _parameterized_setup(discount_spec, reward_spec)\n\n    # Run the loop. There should be EPISODE_LENGTH update calls per episode.\n    loop.run(num_episodes=10)\n    self.assertEqual(actor.num_updates, 10 * EPISODE_LENGTH)\n\n  @parameterized.named_parameters(*TEST_CASES)\n  def test_run_steps(self, discount_spec, reward_spec):\n    actor, loop = _parameterized_setup(discount_spec, reward_spec)\n\n    # Run the loop. This will run 2 episodes so that total number of steps is\n    # at least 15.\n    loop.run(num_steps=EPISODE_LENGTH + 5)\n    self.assertEqual(actor.num_updates, 2 * EPISODE_LENGTH)",
  "def _parameterized_setup(discount_spec: Optional[types.NestedSpec] = None,\n                         reward_spec: Optional[types.NestedSpec] = None):\n  \"\"\"Common setup code that, unlike self.setUp, takes arguments.\n\n  Args:\n    discount_spec: None, or a (nested) specs.BoundedArray.\n    reward_spec: None, or a (nested) specs.Array.\n  Returns:\n    environment, actor, loop\n  \"\"\"\n  env_kwargs = {'episode_length': EPISODE_LENGTH}\n  if discount_spec:\n    env_kwargs['discount_spec'] = discount_spec\n  if reward_spec:\n    env_kwargs['reward_spec'] = reward_spec\n\n  environment = fakes.DiscreteEnvironment(**env_kwargs)\n  actor = fakes.Actor(specs.make_environment_spec(environment))\n  loop = environment_loop.EnvironmentLoop(environment, actor)\n  return actor, loop",
  "def test_one_episode(self, discount_spec, reward_spec):\n    _, loop = _parameterized_setup(discount_spec, reward_spec)\n    result = loop.run_episode()\n    self.assertIn('episode_length', result)\n    self.assertEqual(EPISODE_LENGTH, result['episode_length'])\n    self.assertIn('episode_return', result)\n    self.assertIn('steps_per_second', result)",
  "def test_run_episodes(self, discount_spec, reward_spec):\n    actor, loop = _parameterized_setup(discount_spec, reward_spec)\n\n    # Run the loop. There should be EPISODE_LENGTH update calls per episode.\n    loop.run(num_episodes=10)\n    self.assertEqual(actor.num_updates, 10 * EPISODE_LENGTH)",
  "def test_run_steps(self, discount_spec, reward_spec):\n    actor, loop = _parameterized_setup(discount_spec, reward_spec)\n\n    # Run the loop. This will run 2 episodes so that total number of steps is\n    # at least 15.\n    loop.run(num_steps=EPISODE_LENGTH + 5)\n    self.assertEqual(actor.num_updates, 2 * EPISODE_LENGTH)",
  "def get_agent_spec(env_spec: specs.EnvironmentSpec,\n                   agent_id: types.AgentID) -> specs.EnvironmentSpec:\n  \"\"\"Returns a single agent spec from environment spec.\n\n  Args:\n    env_spec: environment spec, wherein observation, action, and reward specs\n      are simply lists (with each entry specifying the respective spec for the\n      given agent index). Discounts are scalars shared amongst agents.\n    agent_id: agent index.\n  \"\"\"\n  return specs.EnvironmentSpec(\n      actions=env_spec.actions[agent_id],\n      discounts=env_spec.discounts,\n      observations=env_spec.observations[agent_id],\n      rewards=env_spec.rewards[agent_id])",
  "def get_agent_timestep(timestep: dm_env.TimeStep,\n                       agent_id: types.AgentID) -> dm_env.TimeStep:\n  \"\"\"Returns the extracted timestep for a particular agent.\"\"\"\n  # Discounts are assumed to be shared amongst agents\n  reward = None if timestep.reward is None else timestep.reward[agent_id]\n  return dm_env.TimeStep(\n      observation=timestep.observation[agent_id],\n      reward=reward,\n      discount=timestep.discount,\n      step_type=timestep.step_type)",
  "class UtilsTest(absltest.TestCase):\n\n  def test_get_agent_spec(self):\n    agent_indices = ['a', '99', 'Z']\n    spec = multiagent_fakes.make_multiagent_environment_spec(agent_indices)\n    for agent_id in spec.actions.keys():\n      single_agent_spec = multiagent_utils.get_agent_spec(\n          spec, agent_id=agent_id)\n      expected_spec = specs.EnvironmentSpec(\n          actions=spec.actions[agent_id],\n          discounts=spec.discounts,\n          observations=spec.observations[agent_id],\n          rewards=spec.rewards[agent_id]\n      )\n      self.assertEqual(single_agent_spec, expected_spec)\n\n  def test_get_agent_timestep(self):\n    agent_indices = ['a', '99', 'Z']\n    spec = multiagent_fakes.make_multiagent_environment_spec(agent_indices)\n    env = fakes.Environment(spec)\n    timestep = env.reset()\n    for agent_id in spec.actions.keys():\n      single_agent_timestep = multiagent_utils.get_agent_timestep(\n          timestep, agent_id)\n      expected_timestep = dm_env.TimeStep(\n          observation=timestep.observation[agent_id],\n          reward=None,\n          discount=None,\n          step_type=timestep.step_type\n      )\n      self.assertEqual(single_agent_timestep, expected_timestep)",
  "def test_get_agent_spec(self):\n    agent_indices = ['a', '99', 'Z']\n    spec = multiagent_fakes.make_multiagent_environment_spec(agent_indices)\n    for agent_id in spec.actions.keys():\n      single_agent_spec = multiagent_utils.get_agent_spec(\n          spec, agent_id=agent_id)\n      expected_spec = specs.EnvironmentSpec(\n          actions=spec.actions[agent_id],\n          discounts=spec.discounts,\n          observations=spec.observations[agent_id],\n          rewards=spec.rewards[agent_id]\n      )\n      self.assertEqual(single_agent_spec, expected_spec)",
  "def test_get_agent_timestep(self):\n    agent_indices = ['a', '99', 'Z']\n    spec = multiagent_fakes.make_multiagent_environment_spec(agent_indices)\n    env = fakes.Environment(spec)\n    timestep = env.reset()\n    for agent_id in spec.actions.keys():\n      single_agent_timestep = multiagent_utils.get_agent_timestep(\n          timestep, agent_id)\n      expected_timestep = dm_env.TimeStep(\n          observation=timestep.observation[agent_id],\n          reward=None,\n          discount=None,\n          step_type=timestep.step_type\n      )\n      self.assertEqual(single_agent_timestep, expected_timestep)",
  "class AtariWrapperTest(parameterized.TestCase):\n\n  @parameterized.parameters(True, False)\n  def test_pong(self, zero_discount_on_life_loss: bool):\n    env = gym.make('PongNoFrameskip-v4', full_action_space=True)\n    env = gym_wrapper.GymAtariAdapter(env)\n    env = atari_wrapper.AtariWrapper(\n        env, zero_discount_on_life_loss=zero_discount_on_life_loss)\n\n    # Test converted observation spec.\n    observation_spec = env.observation_spec()\n    self.assertEqual(type(observation_spec), specs.Array)\n\n    # Test converted action spec.\n    action_spec: specs.DiscreteArray = env.action_spec()\n    self.assertEqual(type(action_spec), specs.DiscreteArray)\n    self.assertEqual(action_spec.shape, ())\n    self.assertEqual(action_spec.minimum, 0)\n    self.assertEqual(action_spec.maximum, 17)\n    self.assertEqual(action_spec.num_values, 18)\n    self.assertEqual(action_spec.dtype, np.dtype('int32'))\n\n    # Check that the `render` call gets delegated to the underlying Gym env.\n    env.render('rgb_array')\n\n    # Test step.\n    timestep = env.reset()\n    self.assertTrue(timestep.first())\n    _ = env.step(0)\n    env.close()",
  "def test_pong(self, zero_discount_on_life_loss: bool):\n    env = gym.make('PongNoFrameskip-v4', full_action_space=True)\n    env = gym_wrapper.GymAtariAdapter(env)\n    env = atari_wrapper.AtariWrapper(\n        env, zero_discount_on_life_loss=zero_discount_on_life_loss)\n\n    # Test converted observation spec.\n    observation_spec = env.observation_spec()\n    self.assertEqual(type(observation_spec), specs.Array)\n\n    # Test converted action spec.\n    action_spec: specs.DiscreteArray = env.action_spec()\n    self.assertEqual(type(action_spec), specs.DiscreteArray)\n    self.assertEqual(action_spec.shape, ())\n    self.assertEqual(action_spec.minimum, 0)\n    self.assertEqual(action_spec.maximum, 17)\n    self.assertEqual(action_spec.num_values, 18)\n    self.assertEqual(action_spec.dtype, np.dtype('int32'))\n\n    # Check that the `render` call gets delegated to the underlying Gym env.\n    env.render('rgb_array')\n\n    # Test step.\n    timestep = env.reset()\n    self.assertTrue(timestep.first())\n    _ = env.step(0)\n    env.close()",
  "class OLT(NamedTuple):\n  \"\"\"Container for (observation, legal_actions, terminal) tuples.\"\"\"\n  observation: types.Nest\n  legal_actions: types.Nest\n  terminal: types.Nest",
  "class OpenSpielWrapper(dm_env.Environment):\n  \"\"\"Environment wrapper for OpenSpiel RL environments.\"\"\"\n\n  # Note: we don't inherit from base.EnvironmentWrapper because that class\n  # assumes that the wrapped environment is a dm_env.Environment.\n\n  def __init__(self, environment: rl_environment.Environment):\n    self._environment = environment\n    self._reset_next_step = True\n    if not environment.is_turn_based:\n      raise ValueError(\"Currently only supports turn based games.\")\n\n  def reset(self) -> dm_env.TimeStep:\n    \"\"\"Resets the episode.\"\"\"\n    self._reset_next_step = False\n    open_spiel_timestep = self._environment.reset()\n    observations = self._convert_observation(open_spiel_timestep)\n    return dm_env.restart(observations)\n\n  def step(self, action: types.NestedArray) -> dm_env.TimeStep:\n    \"\"\"Steps the environment.\"\"\"\n    if self._reset_next_step:\n      return self.reset()\n\n    open_spiel_timestep = self._environment.step(action)\n\n    if open_spiel_timestep.step_type == rl_environment.StepType.LAST:\n      self._reset_next_step = True\n\n    observations = self._convert_observation(open_spiel_timestep)\n    rewards = np.asarray(open_spiel_timestep.rewards)\n    discounts = np.asarray(open_spiel_timestep.discounts)\n    step_type = open_spiel_timestep.step_type\n\n    if step_type == rl_environment.StepType.FIRST:\n      step_type = dm_env.StepType.FIRST\n    elif step_type == rl_environment.StepType.MID:\n      step_type = dm_env.StepType.MID\n    elif step_type == rl_environment.StepType.LAST:\n      step_type = dm_env.StepType.LAST\n    else:\n      raise ValueError(\n          \"Did not recognize OpenSpiel StepType: {}\".format(step_type))\n\n    return dm_env.TimeStep(observation=observations,\n                           reward=rewards,\n                           discount=discounts,\n                           step_type=step_type)\n\n  # Convert OpenSpiel observation so it's dm_env compatible. Also, the list\n  # of legal actions must be converted to a legal actions mask.\n  def _convert_observation(\n      self, open_spiel_timestep: rl_environment.TimeStep) -> List[OLT]:\n    observations = []\n    for pid in range(self._environment.num_players):\n      legals = np.zeros(self._environment.game.num_distinct_actions(),\n                        dtype=np.float32)\n      legals[open_spiel_timestep.observations[\"legal_actions\"][pid]] = 1.0\n      player_observation = OLT(observation=np.asarray(\n          open_spiel_timestep.observations[\"info_state\"][pid],\n          dtype=np.float32),\n                               legal_actions=legals,\n                               terminal=np.asarray([open_spiel_timestep.last()],\n                                                   dtype=np.float32))\n      observations.append(player_observation)\n    return observations\n\n  def observation_spec(self) -> OLT:\n    # Observation spec depends on whether the OpenSpiel environment is using\n    # observation/information_state tensors.\n    if self._environment.use_observation:\n      return OLT(observation=specs.Array(\n          (self._environment.game.observation_tensor_size(),), np.float32),\n                 legal_actions=specs.Array(\n                     (self._environment.game.num_distinct_actions(),),\n                     np.float32),\n                 terminal=specs.Array((1,), np.float32))\n    else:\n      return OLT(observation=specs.Array(\n          (self._environment.game.information_state_tensor_size(),),\n          np.float32),\n                 legal_actions=specs.Array(\n                     (self._environment.game.num_distinct_actions(),),\n                     np.float32),\n                 terminal=specs.Array((1,), np.float32))\n\n  def action_spec(self) -> specs.DiscreteArray:\n    return specs.DiscreteArray(self._environment.game.num_distinct_actions())\n\n  def reward_spec(self) -> specs.BoundedArray:\n    return specs.BoundedArray((),\n                              np.float32,\n                              minimum=self._environment.game.min_utility(),\n                              maximum=self._environment.game.max_utility())\n\n  def discount_spec(self) -> specs.BoundedArray:\n    return specs.BoundedArray((), np.float32, minimum=0, maximum=1.0)\n\n  @property\n  def environment(self) -> rl_environment.Environment:\n    \"\"\"Returns the wrapped environment.\"\"\"\n    return self._environment\n\n  @property\n  def current_player(self) -> int:\n    return self._environment.get_state.current_player()\n\n  def __getattr__(self, name: str):\n    \"\"\"Expose any other attributes of the underlying environment.\"\"\"\n    if name.startswith(\"__\"):\n      raise AttributeError(\n          \"attempted to get missing private attribute '{}'\".format(name))\n    return getattr(self._environment, name)",
  "def __init__(self, environment: rl_environment.Environment):\n    self._environment = environment\n    self._reset_next_step = True\n    if not environment.is_turn_based:\n      raise ValueError(\"Currently only supports turn based games.\")",
  "def reset(self) -> dm_env.TimeStep:\n    \"\"\"Resets the episode.\"\"\"\n    self._reset_next_step = False\n    open_spiel_timestep = self._environment.reset()\n    observations = self._convert_observation(open_spiel_timestep)\n    return dm_env.restart(observations)",
  "def step(self, action: types.NestedArray) -> dm_env.TimeStep:\n    \"\"\"Steps the environment.\"\"\"\n    if self._reset_next_step:\n      return self.reset()\n\n    open_spiel_timestep = self._environment.step(action)\n\n    if open_spiel_timestep.step_type == rl_environment.StepType.LAST:\n      self._reset_next_step = True\n\n    observations = self._convert_observation(open_spiel_timestep)\n    rewards = np.asarray(open_spiel_timestep.rewards)\n    discounts = np.asarray(open_spiel_timestep.discounts)\n    step_type = open_spiel_timestep.step_type\n\n    if step_type == rl_environment.StepType.FIRST:\n      step_type = dm_env.StepType.FIRST\n    elif step_type == rl_environment.StepType.MID:\n      step_type = dm_env.StepType.MID\n    elif step_type == rl_environment.StepType.LAST:\n      step_type = dm_env.StepType.LAST\n    else:\n      raise ValueError(\n          \"Did not recognize OpenSpiel StepType: {}\".format(step_type))\n\n    return dm_env.TimeStep(observation=observations,\n                           reward=rewards,\n                           discount=discounts,\n                           step_type=step_type)",
  "def _convert_observation(\n      self, open_spiel_timestep: rl_environment.TimeStep) -> List[OLT]:\n    observations = []\n    for pid in range(self._environment.num_players):\n      legals = np.zeros(self._environment.game.num_distinct_actions(),\n                        dtype=np.float32)\n      legals[open_spiel_timestep.observations[\"legal_actions\"][pid]] = 1.0\n      player_observation = OLT(observation=np.asarray(\n          open_spiel_timestep.observations[\"info_state\"][pid],\n          dtype=np.float32),\n                               legal_actions=legals,\n                               terminal=np.asarray([open_spiel_timestep.last()],\n                                                   dtype=np.float32))\n      observations.append(player_observation)\n    return observations",
  "def observation_spec(self) -> OLT:\n    # Observation spec depends on whether the OpenSpiel environment is using\n    # observation/information_state tensors.\n    if self._environment.use_observation:\n      return OLT(observation=specs.Array(\n          (self._environment.game.observation_tensor_size(),), np.float32),\n                 legal_actions=specs.Array(\n                     (self._environment.game.num_distinct_actions(),),\n                     np.float32),\n                 terminal=specs.Array((1,), np.float32))\n    else:\n      return OLT(observation=specs.Array(\n          (self._environment.game.information_state_tensor_size(),),\n          np.float32),\n                 legal_actions=specs.Array(\n                     (self._environment.game.num_distinct_actions(),),\n                     np.float32),\n                 terminal=specs.Array((1,), np.float32))",
  "def action_spec(self) -> specs.DiscreteArray:\n    return specs.DiscreteArray(self._environment.game.num_distinct_actions())",
  "def reward_spec(self) -> specs.BoundedArray:\n    return specs.BoundedArray((),\n                              np.float32,\n                              minimum=self._environment.game.min_utility(),\n                              maximum=self._environment.game.max_utility())",
  "def discount_spec(self) -> specs.BoundedArray:\n    return specs.BoundedArray((), np.float32, minimum=0, maximum=1.0)",
  "def environment(self) -> rl_environment.Environment:\n    \"\"\"Returns the wrapped environment.\"\"\"\n    return self._environment",
  "def current_player(self) -> int:\n    return self._environment.get_state.current_player()",
  "def __getattr__(self, name: str):\n    \"\"\"Expose any other attributes of the underlying environment.\"\"\"\n    if name.startswith(\"__\"):\n      raise AttributeError(\n          \"attempted to get missing private attribute '{}'\".format(name))\n    return getattr(self._environment, name)",
  "class EnvironmentWrapper(dm_env.Environment):\n  \"\"\"Environment that wraps another environment.\n\n  This exposes the wrapped environment with the `.environment` property and also\n  defines `__getattr__` so that attributes are invisibly forwarded to the\n  wrapped environment (and hence enabling duck-typing).\n  \"\"\"\n\n  _environment: dm_env.Environment\n\n  def __init__(self, environment: dm_env.Environment):\n    self._environment = environment\n\n  def __getattr__(self, name):\n    if name.startswith(\"__\"):\n      raise AttributeError(\n          \"attempted to get missing private attribute '{}'\".format(name))\n    return getattr(self._environment, name)\n\n  @property\n  def environment(self) -> dm_env.Environment:\n    return self._environment\n\n  # The following lines are necessary because methods defined in\n  # `dm_env.Environment` are not delegated through `__getattr__`, which would\n  # only be used to expose methods or properties that are not defined in the\n  # base `dm_env.Environment` class.\n\n  def step(self, action) -> dm_env.TimeStep:\n    return self._environment.step(action)\n\n  def reset(self) -> dm_env.TimeStep:\n    return self._environment.reset()\n\n  def action_spec(self):\n    return self._environment.action_spec()\n\n  def discount_spec(self):\n    return self._environment.discount_spec()\n\n  def observation_spec(self):\n    return self._environment.observation_spec()\n\n  def reward_spec(self):\n    return self._environment.reward_spec()\n\n  def close(self):\n    return self._environment.close()",
  "def wrap_all(\n    environment: dm_env.Environment,\n    wrappers: Sequence[Callable[[dm_env.Environment], dm_env.Environment]],\n) -> dm_env.Environment:\n  \"\"\"Given an environment, wrap it in a list of wrappers.\"\"\"\n  for w in wrappers:\n    environment = w(environment)\n\n  return environment",
  "def __init__(self, environment: dm_env.Environment):\n    self._environment = environment",
  "def __getattr__(self, name):\n    if name.startswith(\"__\"):\n      raise AttributeError(\n          \"attempted to get missing private attribute '{}'\".format(name))\n    return getattr(self._environment, name)",
  "def environment(self) -> dm_env.Environment:\n    return self._environment",
  "def step(self, action) -> dm_env.TimeStep:\n    return self._environment.step(action)",
  "def reset(self) -> dm_env.TimeStep:\n    return self._environment.reset()",
  "def action_spec(self):\n    return self._environment.action_spec()",
  "def discount_spec(self):\n    return self._environment.discount_spec()",
  "def observation_spec(self):\n    return self._environment.observation_spec()",
  "def reward_spec(self):\n    return self._environment.reward_spec()",
  "def close(self):\n    return self._environment.close()",
  "class BaseAtariWrapper(abc.ABC, base.EnvironmentWrapper):\n  \"\"\"Abstract base class for Atari wrappers.\n\n  This assumes that the input environment is a dm_env.Environment instance in\n  which observations are tuples whose first element is an RGB observation and\n  the second element is the lives count.\n\n  The wrapper itself performs the following modifications:\n\n    1. Soft-termination (setting discount to zero) on loss of life.\n    2. Action repeats.\n    3. Frame pooling for action repeats.\n    4. Conversion to grayscale and downscaling.\n    5. Reward clipping.\n    6. Observation stacking.\n\n  The details of grayscale conversion, downscaling, and frame pooling are\n  delegated to the concrete subclasses.\n\n  This wrapper will raise an error if the underlying Atari environment does not:\n\n  - Exposes RGB observations in interleaved format (shape `(H, W, C)`).\n  - Expose zero-indexed actions.\n\n  Note that this class does not expose a configurable rescale method (defaults\n  to bilinear internally).\n\n  This class also exposes an additional option `to_float` that doesn't feature\n  in other wrappers, which rescales pixel values to floats in the range [0, 1].\n  \"\"\"\n\n  def __init__(self,\n               environment: dm_env.Environment,\n               *,\n               max_abs_reward: Optional[float] = None,\n               scale_dims: Optional[Tuple[int, int]] = (84, 84),\n               action_repeats: int = 4,\n               pooled_frames: int = 2,\n               zero_discount_on_life_loss: bool = False,\n               expose_lives_observation: bool = False,\n               num_stacked_frames: int = 4,\n               flatten_frame_stack: bool = False,\n               max_episode_len: Optional[int] = None,\n               to_float: bool = False,\n               grayscaling: bool = True):\n    \"\"\"Initializes a new AtariWrapper.\n\n    Args:\n      environment: An Atari environment.\n      max_abs_reward: Maximum absolute reward value before clipping is applied.\n        If set to `None` (default), no clipping is applied.\n      scale_dims: Image size for the rescaling step after grayscaling, given as\n        `(height, width)`. Set to `None` to disable resizing.\n      action_repeats: Number of times to step wrapped environment for each given\n        action.\n      pooled_frames: Number of observations to pool over. Set to 1 to disable\n        frame pooling.\n      zero_discount_on_life_loss: If `True`, sets the discount to zero when the\n        number of lives decreases in in Atari environment.\n      expose_lives_observation: If `False`, the `lives` part of the observation\n        is discarded, otherwise it is kept as part of an observation tuple. This\n        does not affect the `zero_discount_on_life_loss` feature. When enabled,\n        the observation consists of a single pixel array, otherwise it is a\n        tuple (pixel_array, lives).\n      num_stacked_frames: Number of recent (pooled) observations to stack into\n        the returned observation.\n      flatten_frame_stack: Whether to flatten the stack of frames such that\n        the channel (RGB) and stacking dimensions are merged.\n      max_episode_len: Number of frames before truncating episode. By default,\n        there is no maximum length.\n      to_float: If `True`, rescales RGB observations to floats in [0, 1].\n      grayscaling: If `True` returns a grayscale version of the observations. In\n        this case, the observation is 3D (H, W, num_stacked_frames). If `False`\n        the observations are RGB and have shape (H, W, C, num_stacked_frames).\n\n    Raises:\n      ValueError: For various invalid inputs.\n    \"\"\"\n    if not 1 <= pooled_frames <= action_repeats:\n      raise ValueError(\"pooled_frames ({}) must be between 1 and \"\n                       \"action_repeats ({}) inclusive\".format(\n                           pooled_frames, action_repeats))\n\n    if zero_discount_on_life_loss:\n      super().__init__(_ZeroDiscountOnLifeLoss(environment))\n    else:\n      super().__init__(environment)\n\n    if not max_episode_len:\n      max_episode_len = np.inf\n\n    self._frame_stacker = frame_stacking.FrameStacker(\n        num_frames=num_stacked_frames, flatten=flatten_frame_stack)\n    self._action_repeats = action_repeats\n    self._pooled_frames = pooled_frames\n    self._scale_dims = scale_dims\n    self._max_abs_reward = max_abs_reward or np.inf\n    self._to_float = to_float\n    self._expose_lives_observation = expose_lives_observation\n\n    if scale_dims:\n      self._height, self._width = scale_dims\n    else:\n      spec = environment.observation_spec()\n      self._height, self._width = spec[RGB_INDEX].shape[:2]\n\n    self._episode_len = 0\n    self._max_episode_len = max_episode_len\n    self._reset_next_step = True\n\n    self._grayscaling = grayscaling\n\n    # Based on underlying observation spec, decide whether lives are to be\n    # included in output observations.\n    observation_spec = self._environment.observation_spec()\n    spec_names = [spec.name for spec in observation_spec]\n    if \"lives\" in spec_names and spec_names.index(\"lives\") != 1:\n      raise ValueError(\"`lives` observation needs to have index 1 in Atari.\")\n\n    self._observation_spec = self._init_observation_spec()\n\n    self._raw_observation = None\n\n  def _init_observation_spec(self):\n    \"\"\"Computes the observation spec for the pixel observations.\n\n    Returns:\n      An `Array` specification for the pixel observations.\n    \"\"\"\n    if self._to_float:\n      pixels_dtype = float\n    else:\n      pixels_dtype = np.uint8\n\n    if self._grayscaling:\n      pixels_spec_shape = (self._height, self._width)\n      pixels_spec_name = \"grayscale\"\n    else:\n      pixels_spec_shape = (self._height, self._width, NUM_COLOR_CHANNELS)\n      pixels_spec_name = \"RGB\"\n\n    pixel_spec = specs.Array(\n        shape=pixels_spec_shape, dtype=pixels_dtype, name=pixels_spec_name)\n    pixel_spec = self._frame_stacker.update_spec(pixel_spec)\n\n    if self._expose_lives_observation:\n      return (pixel_spec,) + self._environment.observation_spec()[1:]\n    return pixel_spec\n\n  def reset(self) -> dm_env.TimeStep:\n    \"\"\"Resets environment and provides the first timestep.\"\"\"\n    self._reset_next_step = False\n    self._episode_len = 0\n    self._frame_stacker.reset()\n    timestep = self._environment.reset()\n\n    observation = self._observation_from_timestep_stack([timestep])\n\n    return self._postprocess_observation(\n        timestep._replace(observation=observation))\n\n  def step(self, action: int) -> dm_env.TimeStep:\n    \"\"\"Steps up to action_repeat times and returns a post-processed step.\"\"\"\n    if self._reset_next_step:\n      return self.reset()\n\n    timestep_stack = []\n\n    # Step on environment multiple times for each selected action.\n    for _ in range(self._action_repeats):\n      timestep = self._environment.step([np.array([action])])\n\n      self._episode_len += 1\n      if self._episode_len == self._max_episode_len:\n        timestep = timestep._replace(step_type=dm_env.StepType.LAST)\n\n      timestep_stack.append(timestep)\n\n      if timestep.last():\n        # Action repeat frames should not span episode boundaries. Also, no need\n        # to pad with zero-valued observations as all the reductions in\n        # _postprocess_observation work gracefully for any non-zero size of\n        # timestep_stack.\n        self._reset_next_step = True\n        break\n\n    # Determine a single step type. We let FIRST take priority over LAST, since\n    # we think it's more likely algorithm code will be set up to deal with that,\n    # due to environments supporting reset() (which emits a FIRST).\n    # Note we'll never have LAST then FIRST in timestep_stack here.\n    step_type = dm_env.StepType.MID\n    for timestep in timestep_stack:\n      if timestep.first():\n        step_type = dm_env.StepType.FIRST\n        break\n      elif timestep.last():\n        step_type = dm_env.StepType.LAST\n        break\n\n    if timestep_stack[0].first():\n      # Update first timestep to have identity effect on reward and discount.\n      timestep_stack[0] = timestep_stack[0]._replace(reward=0., discount=1.)\n\n    # Sum reward over stack.\n    reward = sum(timestep_t.reward for timestep_t in timestep_stack)\n\n    # Multiply discount over stack (will either be 0. or 1.).\n    discount = np.prod([timestep_t.discount for timestep_t in timestep_stack])\n\n    observation = self._observation_from_timestep_stack(timestep_stack)\n\n    timestep = dm_env.TimeStep(\n        step_type=step_type,\n        reward=reward,\n        observation=observation,\n        discount=discount)\n\n    return self._postprocess_observation(timestep)\n\n  @abc.abstractmethod\n  def _preprocess_pixels(self, timestep_stack: List[dm_env.TimeStep]):\n    \"\"\"Process Atari pixels.\"\"\"\n\n  def _observation_from_timestep_stack(self,\n                                       timestep_stack: List[dm_env.TimeStep]):\n    \"\"\"Compute the observation for a stack of timesteps.\"\"\"\n    self._raw_observation = timestep_stack[-1].observation[RGB_INDEX].copy()\n    processed_pixels = self._preprocess_pixels(timestep_stack)\n\n    if self._to_float:\n      stacked_observation = self._frame_stacker.step(processed_pixels / 255.0)\n    else:\n      stacked_observation = self._frame_stacker.step(processed_pixels)\n\n    # We use last timestep for lives only.\n    observation = timestep_stack[-1].observation\n    if self._expose_lives_observation:\n      return (stacked_observation,) + observation[1:]\n\n    return stacked_observation\n\n  def _postprocess_observation(self,\n                               timestep: dm_env.TimeStep) -> dm_env.TimeStep:\n    \"\"\"Observation processing applied after action repeat consolidation.\"\"\"\n\n    if timestep.first():\n      return dm_env.restart(timestep.observation)\n\n    reward = np.clip(timestep.reward, -self._max_abs_reward,\n                     self._max_abs_reward)\n\n    return timestep._replace(reward=reward)\n\n  def action_spec(self) -> specs.DiscreteArray:\n    raw_spec = self._environment.action_spec()[0]\n    return specs.DiscreteArray(num_values=raw_spec.maximum.item() -\n                               raw_spec.minimum.item() + 1)\n\n  def observation_spec(self) -> Union[specs.Array, Sequence[specs.Array]]:\n    return self._observation_spec\n\n  def reward_spec(self) -> specs.Array:\n    return specs.Array(shape=(), dtype=float)\n\n  @property\n  def raw_observation(self) -> np.ndarray:\n    \"\"\"Returns the raw observation, after any pooling has been applied.\"\"\"\n    return self._raw_observation",
  "class AtariWrapper(BaseAtariWrapper):\n  \"\"\"Standard \"Nature Atari\" wrapper for Python environments.\n\n  Before being fed to a neural network, Atari frames go through a prepocessing,\n  implemented in this wrapper. For historical reasons, there were different\n  choices in the method to apply there between what was done in the Dopamine\n  library and what is done in Acme. During the processing of\n  Atari frames, three operations need to happen. Images are\n  transformed from RGB to grayscale, we perform a max-pooling on the time scale,\n  and images are resized to 84x84.\n\n  1. The `standard` style (this one, matches previous acme versions):\n     - does max pooling, then rgb -> grayscale\n     - uses Pillow inter area interpolation for resizing\n  2. The `dopamine` style:\n     - does rgb -> grayscale, then max pooling\n     - uses opencv bilinear interpolation for resizing.\n\n  This can change the behavior of RL agents on some games. The recommended\n  setting is to use the standard style with this class. The Dopamine setting is\n  available in `atari_wrapper_dopamine.py` for the\n  user that wishes to compare agents between librairies.\n  \"\"\"\n\n  def _preprocess_pixels(self, timestep_stack: List[dm_env.TimeStep]):\n    \"\"\"Preprocess Atari frames.\"\"\"\n    # 1. Max pooling\n    processed_pixels = np.max(\n        np.stack([\n            s.observation[RGB_INDEX]\n            for s in timestep_stack[-self._pooled_frames:]\n        ]),\n        axis=0)\n\n    # 2. RGB to grayscale\n    if self._grayscaling:\n      processed_pixels = np.tensordot(processed_pixels,\n                                      [0.299, 0.587, 1 - (0.299 + 0.587)],\n                                      (-1, 0))\n\n    # 3. Resize\n    processed_pixels = processed_pixels.astype(np.uint8, copy=False)\n    if self._scale_dims != processed_pixels.shape[:2]:\n      processed_pixels = Image.fromarray(processed_pixels).resize(\n          (self._width, self._height), Image.Resampling.BILINEAR)\n      processed_pixels = np.array(processed_pixels, dtype=np.uint8)\n\n    return processed_pixels",
  "class _ZeroDiscountOnLifeLoss(base.EnvironmentWrapper):\n  \"\"\"Implements soft-termination (zero discount) on life loss.\"\"\"\n\n  def __init__(self, environment: dm_env.Environment):\n    \"\"\"Initializes a new `_ZeroDiscountOnLifeLoss` wrapper.\n\n    Args:\n      environment: An Atari environment.\n\n    Raises:\n      ValueError: If the environment does not expose a lives observation.\n    \"\"\"\n    super().__init__(environment)\n    self._reset_next_step = True\n    self._last_num_lives = None\n\n  def reset(self) -> dm_env.TimeStep:\n    timestep = self._environment.reset()\n    self._reset_next_step = False\n    self._last_num_lives = timestep.observation[LIVES_INDEX]\n    return timestep\n\n  def step(self, action: int) -> dm_env.TimeStep:\n    if self._reset_next_step:\n      return self.reset()\n\n    timestep = self._environment.step(action)\n    lives = timestep.observation[LIVES_INDEX]\n\n    is_life_loss = True\n    # We have a life loss when:\n    # The wrapped environment is in a regular (MID) transition.\n    is_life_loss &= timestep.mid()\n    # Lives have decreased since last time `step` was called.\n    is_life_loss &= lives < self._last_num_lives\n\n    self._last_num_lives = lives\n    if is_life_loss:\n      return timestep._replace(discount=0.0)\n    return timestep",
  "def __init__(self,\n               environment: dm_env.Environment,\n               *,\n               max_abs_reward: Optional[float] = None,\n               scale_dims: Optional[Tuple[int, int]] = (84, 84),\n               action_repeats: int = 4,\n               pooled_frames: int = 2,\n               zero_discount_on_life_loss: bool = False,\n               expose_lives_observation: bool = False,\n               num_stacked_frames: int = 4,\n               flatten_frame_stack: bool = False,\n               max_episode_len: Optional[int] = None,\n               to_float: bool = False,\n               grayscaling: bool = True):\n    \"\"\"Initializes a new AtariWrapper.\n\n    Args:\n      environment: An Atari environment.\n      max_abs_reward: Maximum absolute reward value before clipping is applied.\n        If set to `None` (default), no clipping is applied.\n      scale_dims: Image size for the rescaling step after grayscaling, given as\n        `(height, width)`. Set to `None` to disable resizing.\n      action_repeats: Number of times to step wrapped environment for each given\n        action.\n      pooled_frames: Number of observations to pool over. Set to 1 to disable\n        frame pooling.\n      zero_discount_on_life_loss: If `True`, sets the discount to zero when the\n        number of lives decreases in in Atari environment.\n      expose_lives_observation: If `False`, the `lives` part of the observation\n        is discarded, otherwise it is kept as part of an observation tuple. This\n        does not affect the `zero_discount_on_life_loss` feature. When enabled,\n        the observation consists of a single pixel array, otherwise it is a\n        tuple (pixel_array, lives).\n      num_stacked_frames: Number of recent (pooled) observations to stack into\n        the returned observation.\n      flatten_frame_stack: Whether to flatten the stack of frames such that\n        the channel (RGB) and stacking dimensions are merged.\n      max_episode_len: Number of frames before truncating episode. By default,\n        there is no maximum length.\n      to_float: If `True`, rescales RGB observations to floats in [0, 1].\n      grayscaling: If `True` returns a grayscale version of the observations. In\n        this case, the observation is 3D (H, W, num_stacked_frames). If `False`\n        the observations are RGB and have shape (H, W, C, num_stacked_frames).\n\n    Raises:\n      ValueError: For various invalid inputs.\n    \"\"\"\n    if not 1 <= pooled_frames <= action_repeats:\n      raise ValueError(\"pooled_frames ({}) must be between 1 and \"\n                       \"action_repeats ({}) inclusive\".format(\n                           pooled_frames, action_repeats))\n\n    if zero_discount_on_life_loss:\n      super().__init__(_ZeroDiscountOnLifeLoss(environment))\n    else:\n      super().__init__(environment)\n\n    if not max_episode_len:\n      max_episode_len = np.inf\n\n    self._frame_stacker = frame_stacking.FrameStacker(\n        num_frames=num_stacked_frames, flatten=flatten_frame_stack)\n    self._action_repeats = action_repeats\n    self._pooled_frames = pooled_frames\n    self._scale_dims = scale_dims\n    self._max_abs_reward = max_abs_reward or np.inf\n    self._to_float = to_float\n    self._expose_lives_observation = expose_lives_observation\n\n    if scale_dims:\n      self._height, self._width = scale_dims\n    else:\n      spec = environment.observation_spec()\n      self._height, self._width = spec[RGB_INDEX].shape[:2]\n\n    self._episode_len = 0\n    self._max_episode_len = max_episode_len\n    self._reset_next_step = True\n\n    self._grayscaling = grayscaling\n\n    # Based on underlying observation spec, decide whether lives are to be\n    # included in output observations.\n    observation_spec = self._environment.observation_spec()\n    spec_names = [spec.name for spec in observation_spec]\n    if \"lives\" in spec_names and spec_names.index(\"lives\") != 1:\n      raise ValueError(\"`lives` observation needs to have index 1 in Atari.\")\n\n    self._observation_spec = self._init_observation_spec()\n\n    self._raw_observation = None",
  "def _init_observation_spec(self):\n    \"\"\"Computes the observation spec for the pixel observations.\n\n    Returns:\n      An `Array` specification for the pixel observations.\n    \"\"\"\n    if self._to_float:\n      pixels_dtype = float\n    else:\n      pixels_dtype = np.uint8\n\n    if self._grayscaling:\n      pixels_spec_shape = (self._height, self._width)\n      pixels_spec_name = \"grayscale\"\n    else:\n      pixels_spec_shape = (self._height, self._width, NUM_COLOR_CHANNELS)\n      pixels_spec_name = \"RGB\"\n\n    pixel_spec = specs.Array(\n        shape=pixels_spec_shape, dtype=pixels_dtype, name=pixels_spec_name)\n    pixel_spec = self._frame_stacker.update_spec(pixel_spec)\n\n    if self._expose_lives_observation:\n      return (pixel_spec,) + self._environment.observation_spec()[1:]\n    return pixel_spec",
  "def reset(self) -> dm_env.TimeStep:\n    \"\"\"Resets environment and provides the first timestep.\"\"\"\n    self._reset_next_step = False\n    self._episode_len = 0\n    self._frame_stacker.reset()\n    timestep = self._environment.reset()\n\n    observation = self._observation_from_timestep_stack([timestep])\n\n    return self._postprocess_observation(\n        timestep._replace(observation=observation))",
  "def step(self, action: int) -> dm_env.TimeStep:\n    \"\"\"Steps up to action_repeat times and returns a post-processed step.\"\"\"\n    if self._reset_next_step:\n      return self.reset()\n\n    timestep_stack = []\n\n    # Step on environment multiple times for each selected action.\n    for _ in range(self._action_repeats):\n      timestep = self._environment.step([np.array([action])])\n\n      self._episode_len += 1\n      if self._episode_len == self._max_episode_len:\n        timestep = timestep._replace(step_type=dm_env.StepType.LAST)\n\n      timestep_stack.append(timestep)\n\n      if timestep.last():\n        # Action repeat frames should not span episode boundaries. Also, no need\n        # to pad with zero-valued observations as all the reductions in\n        # _postprocess_observation work gracefully for any non-zero size of\n        # timestep_stack.\n        self._reset_next_step = True\n        break\n\n    # Determine a single step type. We let FIRST take priority over LAST, since\n    # we think it's more likely algorithm code will be set up to deal with that,\n    # due to environments supporting reset() (which emits a FIRST).\n    # Note we'll never have LAST then FIRST in timestep_stack here.\n    step_type = dm_env.StepType.MID\n    for timestep in timestep_stack:\n      if timestep.first():\n        step_type = dm_env.StepType.FIRST\n        break\n      elif timestep.last():\n        step_type = dm_env.StepType.LAST\n        break\n\n    if timestep_stack[0].first():\n      # Update first timestep to have identity effect on reward and discount.\n      timestep_stack[0] = timestep_stack[0]._replace(reward=0., discount=1.)\n\n    # Sum reward over stack.\n    reward = sum(timestep_t.reward for timestep_t in timestep_stack)\n\n    # Multiply discount over stack (will either be 0. or 1.).\n    discount = np.prod([timestep_t.discount for timestep_t in timestep_stack])\n\n    observation = self._observation_from_timestep_stack(timestep_stack)\n\n    timestep = dm_env.TimeStep(\n        step_type=step_type,\n        reward=reward,\n        observation=observation,\n        discount=discount)\n\n    return self._postprocess_observation(timestep)",
  "def _preprocess_pixels(self, timestep_stack: List[dm_env.TimeStep]):\n    \"\"\"Process Atari pixels.\"\"\"",
  "def _observation_from_timestep_stack(self,\n                                       timestep_stack: List[dm_env.TimeStep]):\n    \"\"\"Compute the observation for a stack of timesteps.\"\"\"\n    self._raw_observation = timestep_stack[-1].observation[RGB_INDEX].copy()\n    processed_pixels = self._preprocess_pixels(timestep_stack)\n\n    if self._to_float:\n      stacked_observation = self._frame_stacker.step(processed_pixels / 255.0)\n    else:\n      stacked_observation = self._frame_stacker.step(processed_pixels)\n\n    # We use last timestep for lives only.\n    observation = timestep_stack[-1].observation\n    if self._expose_lives_observation:\n      return (stacked_observation,) + observation[1:]\n\n    return stacked_observation",
  "def _postprocess_observation(self,\n                               timestep: dm_env.TimeStep) -> dm_env.TimeStep:\n    \"\"\"Observation processing applied after action repeat consolidation.\"\"\"\n\n    if timestep.first():\n      return dm_env.restart(timestep.observation)\n\n    reward = np.clip(timestep.reward, -self._max_abs_reward,\n                     self._max_abs_reward)\n\n    return timestep._replace(reward=reward)",
  "def action_spec(self) -> specs.DiscreteArray:\n    raw_spec = self._environment.action_spec()[0]\n    return specs.DiscreteArray(num_values=raw_spec.maximum.item() -\n                               raw_spec.minimum.item() + 1)",
  "def observation_spec(self) -> Union[specs.Array, Sequence[specs.Array]]:\n    return self._observation_spec",
  "def reward_spec(self) -> specs.Array:\n    return specs.Array(shape=(), dtype=float)",
  "def raw_observation(self) -> np.ndarray:\n    \"\"\"Returns the raw observation, after any pooling has been applied.\"\"\"\n    return self._raw_observation",
  "def _preprocess_pixels(self, timestep_stack: List[dm_env.TimeStep]):\n    \"\"\"Preprocess Atari frames.\"\"\"\n    # 1. Max pooling\n    processed_pixels = np.max(\n        np.stack([\n            s.observation[RGB_INDEX]\n            for s in timestep_stack[-self._pooled_frames:]\n        ]),\n        axis=0)\n\n    # 2. RGB to grayscale\n    if self._grayscaling:\n      processed_pixels = np.tensordot(processed_pixels,\n                                      [0.299, 0.587, 1 - (0.299 + 0.587)],\n                                      (-1, 0))\n\n    # 3. Resize\n    processed_pixels = processed_pixels.astype(np.uint8, copy=False)\n    if self._scale_dims != processed_pixels.shape[:2]:\n      processed_pixels = Image.fromarray(processed_pixels).resize(\n          (self._width, self._height), Image.Resampling.BILINEAR)\n      processed_pixels = np.array(processed_pixels, dtype=np.uint8)\n\n    return processed_pixels",
  "def __init__(self, environment: dm_env.Environment):\n    \"\"\"Initializes a new `_ZeroDiscountOnLifeLoss` wrapper.\n\n    Args:\n      environment: An Atari environment.\n\n    Raises:\n      ValueError: If the environment does not expose a lives observation.\n    \"\"\"\n    super().__init__(environment)\n    self._reset_next_step = True\n    self._last_num_lives = None",
  "def reset(self) -> dm_env.TimeStep:\n    timestep = self._environment.reset()\n    self._reset_next_step = False\n    self._last_num_lives = timestep.observation[LIVES_INDEX]\n    return timestep",
  "def step(self, action: int) -> dm_env.TimeStep:\n    if self._reset_next_step:\n      return self.reset()\n\n    timestep = self._environment.step(action)\n    lives = timestep.observation[LIVES_INDEX]\n\n    is_life_loss = True\n    # We have a life loss when:\n    # The wrapped environment is in a regular (MID) transition.\n    is_life_loss &= timestep.mid()\n    # Lives have decreased since last time `step` was called.\n    is_life_loss &= lives < self._last_num_lives\n\n    self._last_num_lives = lives\n    if is_life_loss:\n      return timestep._replace(discount=0.0)\n    return timestep",
  "class FrameStackingWrapper(base.EnvironmentWrapper):\n  \"\"\"Wrapper that stacks observations along a new final axis.\"\"\"\n\n  def __init__(self, environment: dm_env.Environment, num_frames: int = 4,\n               flatten: bool = False):\n    \"\"\"Initializes a new FrameStackingWrapper.\n\n    Args:\n      environment: Environment.\n      num_frames: Number frames to stack.\n      flatten: Whether to flatten the channel and stack dimensions together.\n    \"\"\"\n    self._environment = environment\n    original_spec = self._environment.observation_spec()\n    self._stackers = tree.map_structure(\n        lambda _: FrameStacker(num_frames=num_frames, flatten=flatten),\n        self._environment.observation_spec())\n    self._observation_spec = tree.map_structure(\n        lambda stacker, spec: stacker.update_spec(spec),\n        self._stackers, original_spec)\n\n  def _process_timestep(self, timestep: dm_env.TimeStep) -> dm_env.TimeStep:\n    observation = tree.map_structure(lambda stacker, x: stacker.step(x),\n                                     self._stackers, timestep.observation)\n    return timestep._replace(observation=observation)\n\n  def reset(self) -> dm_env.TimeStep:\n    for stacker in tree.flatten(self._stackers):\n      stacker.reset()\n    return self._process_timestep(self._environment.reset())\n\n  def step(self, action: int) -> dm_env.TimeStep:\n    return self._process_timestep(self._environment.step(action))\n\n  def observation_spec(self) -> types.NestedSpec:\n    return self._observation_spec",
  "class FrameStacker:\n  \"\"\"Simple class for frame-stacking observations.\"\"\"\n\n  def __init__(self, num_frames: int, flatten: bool = False):\n    self._num_frames = num_frames\n    self._flatten = flatten\n    self.reset()\n\n  @property\n  def num_frames(self) -> int:\n    return self._num_frames\n\n  def reset(self):\n    self._stack = collections.deque(maxlen=self._num_frames)\n\n  def step(self, frame: np.ndarray) -> np.ndarray:\n    \"\"\"Append frame to stack and return the stack.\"\"\"\n    if not self._stack:\n      # Fill stack with blank frames if empty.\n      self._stack.extend([np.zeros_like(frame)] * (self._num_frames - 1))\n    self._stack.append(frame)\n    stacked_frames = np.stack(self._stack, axis=-1)\n\n    if not self._flatten:\n      return stacked_frames\n    else:\n      new_shape = stacked_frames.shape[:-2] + (-1,)\n      return stacked_frames.reshape(*new_shape)\n\n  def update_spec(self, spec: dm_env_specs.Array) -> dm_env_specs.Array:\n    if not self._flatten:\n      new_shape = spec.shape + (self._num_frames,)\n    else:\n      new_shape = spec.shape[:-1] + (self._num_frames * spec.shape[-1],)\n    return dm_env_specs.Array(shape=new_shape, dtype=spec.dtype, name=spec.name)",
  "def __init__(self, environment: dm_env.Environment, num_frames: int = 4,\n               flatten: bool = False):\n    \"\"\"Initializes a new FrameStackingWrapper.\n\n    Args:\n      environment: Environment.\n      num_frames: Number frames to stack.\n      flatten: Whether to flatten the channel and stack dimensions together.\n    \"\"\"\n    self._environment = environment\n    original_spec = self._environment.observation_spec()\n    self._stackers = tree.map_structure(\n        lambda _: FrameStacker(num_frames=num_frames, flatten=flatten),\n        self._environment.observation_spec())\n    self._observation_spec = tree.map_structure(\n        lambda stacker, spec: stacker.update_spec(spec),\n        self._stackers, original_spec)",
  "def _process_timestep(self, timestep: dm_env.TimeStep) -> dm_env.TimeStep:\n    observation = tree.map_structure(lambda stacker, x: stacker.step(x),\n                                     self._stackers, timestep.observation)\n    return timestep._replace(observation=observation)",
  "def reset(self) -> dm_env.TimeStep:\n    for stacker in tree.flatten(self._stackers):\n      stacker.reset()\n    return self._process_timestep(self._environment.reset())",
  "def step(self, action: int) -> dm_env.TimeStep:\n    return self._process_timestep(self._environment.step(action))",
  "def observation_spec(self) -> types.NestedSpec:\n    return self._observation_spec",
  "def __init__(self, num_frames: int, flatten: bool = False):\n    self._num_frames = num_frames\n    self._flatten = flatten\n    self.reset()",
  "def num_frames(self) -> int:\n    return self._num_frames",
  "def reset(self):\n    self._stack = collections.deque(maxlen=self._num_frames)",
  "def step(self, frame: np.ndarray) -> np.ndarray:\n    \"\"\"Append frame to stack and return the stack.\"\"\"\n    if not self._stack:\n      # Fill stack with blank frames if empty.\n      self._stack.extend([np.zeros_like(frame)] * (self._num_frames - 1))\n    self._stack.append(frame)\n    stacked_frames = np.stack(self._stack, axis=-1)\n\n    if not self._flatten:\n      return stacked_frames\n    else:\n      new_shape = stacked_frames.shape[:-2] + (-1,)\n      return stacked_frames.reshape(*new_shape)",
  "def update_spec(self, spec: dm_env_specs.Array) -> dm_env_specs.Array:\n    if not self._flatten:\n      new_shape = spec.shape + (self._num_frames,)\n    else:\n      new_shape = spec.shape[:-1] + (self._num_frames * spec.shape[-1],)\n    return dm_env_specs.Array(shape=new_shape, dtype=spec.dtype, name=spec.name)",
  "class OpenSpielWrapperTest(absltest.TestCase):\n\n  def test_tic_tac_toe(self):\n    raw_env = rl_environment.Environment('tic_tac_toe')\n    env = open_spiel_wrapper.OpenSpielWrapper(raw_env)\n\n    # Test converted observation spec.\n    observation_spec = env.observation_spec()\n    self.assertEqual(type(observation_spec), open_spiel_wrapper.OLT)\n    self.assertEqual(type(observation_spec.observation), specs.Array)\n    self.assertEqual(type(observation_spec.legal_actions), specs.Array)\n    self.assertEqual(type(observation_spec.terminal), specs.Array)\n\n    # Test converted action spec.\n    action_spec: specs.DiscreteArray = env.action_spec()\n    self.assertEqual(type(action_spec), specs.DiscreteArray)\n    self.assertEqual(action_spec.shape, ())\n    self.assertEqual(action_spec.minimum, 0)\n    self.assertEqual(action_spec.maximum, 8)\n    self.assertEqual(action_spec.num_values, 9)\n    self.assertEqual(action_spec.dtype, np.dtype('int32'))\n\n    # Test step.\n    timestep = env.reset()\n    self.assertTrue(timestep.first())\n    _ = env.step([0])\n    env.close()",
  "def test_tic_tac_toe(self):\n    raw_env = rl_environment.Environment('tic_tac_toe')\n    env = open_spiel_wrapper.OpenSpielWrapper(raw_env)\n\n    # Test converted observation spec.\n    observation_spec = env.observation_spec()\n    self.assertEqual(type(observation_spec), open_spiel_wrapper.OLT)\n    self.assertEqual(type(observation_spec.observation), specs.Array)\n    self.assertEqual(type(observation_spec.legal_actions), specs.Array)\n    self.assertEqual(type(observation_spec.terminal), specs.Array)\n\n    # Test converted action spec.\n    action_spec: specs.DiscreteArray = env.action_spec()\n    self.assertEqual(type(action_spec), specs.DiscreteArray)\n    self.assertEqual(action_spec.shape, ())\n    self.assertEqual(action_spec.minimum, 0)\n    self.assertEqual(action_spec.maximum, 8)\n    self.assertEqual(action_spec.num_values, 9)\n    self.assertEqual(action_spec.dtype, np.dtype('int32'))\n\n    # Test step.\n    timestep = env.reset()\n    self.assertTrue(timestep.first())\n    _ = env.step([0])\n    env.close()",
  "class SinglePrecisionWrapper(base.EnvironmentWrapper):\n  \"\"\"Wrapper which converts environments from double- to single-precision.\"\"\"\n\n  def _convert_timestep(self, timestep: dm_env.TimeStep) -> dm_env.TimeStep:\n    return timestep._replace(\n        reward=_convert_value(timestep.reward),\n        discount=_convert_value(timestep.discount),\n        observation=_convert_value(timestep.observation))\n\n  def step(self, action) -> dm_env.TimeStep:\n    return self._convert_timestep(self._environment.step(action))\n\n  def reset(self) -> dm_env.TimeStep:\n    return self._convert_timestep(self._environment.reset())\n\n  def action_spec(self):\n    return _convert_spec(self._environment.action_spec())\n\n  def discount_spec(self):\n    return _convert_spec(self._environment.discount_spec())\n\n  def observation_spec(self):\n    return _convert_spec(self._environment.observation_spec())\n\n  def reward_spec(self):\n    return _convert_spec(self._environment.reward_spec())",
  "def _convert_spec(nested_spec: types.NestedSpec) -> types.NestedSpec:\n  \"\"\"Convert a nested spec.\"\"\"\n\n  def _convert_single_spec(spec: specs.Array):\n    \"\"\"Convert a single spec.\"\"\"\n    if spec.dtype == 'O':\n      # Pass StringArray objects through unmodified.\n      return spec\n    if np.issubdtype(spec.dtype, np.float64):\n      dtype = np.float32\n    elif np.issubdtype(spec.dtype, np.int64):\n      dtype = np.int32\n    else:\n      dtype = spec.dtype\n    return spec.replace(dtype=dtype)\n\n  return tree.map_structure(_convert_single_spec, nested_spec)",
  "def _convert_value(nested_value: types.Nest) -> types.Nest:\n  \"\"\"Convert a nested value given a desired nested spec.\"\"\"\n\n  def _convert_single_value(value):\n    if value is not None:\n      value = np.array(value, copy=False)\n      if np.issubdtype(value.dtype, np.float64):\n        value = np.array(value, copy=False, dtype=np.float32)\n      elif np.issubdtype(value.dtype, np.int64):\n        value = np.array(value, copy=False, dtype=np.int32)\n    return value\n\n  return tree.map_structure(_convert_single_value, nested_value)",
  "def _convert_timestep(self, timestep: dm_env.TimeStep) -> dm_env.TimeStep:\n    return timestep._replace(\n        reward=_convert_value(timestep.reward),\n        discount=_convert_value(timestep.discount),\n        observation=_convert_value(timestep.observation))",
  "def step(self, action) -> dm_env.TimeStep:\n    return self._convert_timestep(self._environment.step(action))",
  "def reset(self) -> dm_env.TimeStep:\n    return self._convert_timestep(self._environment.reset())",
  "def action_spec(self):\n    return _convert_spec(self._environment.action_spec())",
  "def discount_spec(self):\n    return _convert_spec(self._environment.discount_spec())",
  "def observation_spec(self):\n    return _convert_spec(self._environment.observation_spec())",
  "def reward_spec(self):\n    return _convert_spec(self._environment.reward_spec())",
  "def _convert_single_spec(spec: specs.Array):\n    \"\"\"Convert a single spec.\"\"\"\n    if spec.dtype == 'O':\n      # Pass StringArray objects through unmodified.\n      return spec\n    if np.issubdtype(spec.dtype, np.float64):\n      dtype = np.float32\n    elif np.issubdtype(spec.dtype, np.int64):\n      dtype = np.int32\n    else:\n      dtype = spec.dtype\n    return spec.replace(dtype=dtype)",
  "def _convert_single_value(value):\n    if value is not None:\n      value = np.array(value, copy=False)\n      if np.issubdtype(value.dtype, np.float64):\n        value = np.array(value, copy=False, dtype=np.float32)\n      elif np.issubdtype(value.dtype, np.int64):\n        value = np.array(value, copy=False, dtype=np.int32)\n    return value",
  "class BaseTest(absltest.TestCase):\n\n  def test_pickle_unpickle(self):\n    test_env = base.EnvironmentWrapper(environment=fakes.DiscreteEnvironment())\n\n    test_env_pickled = pickle.dumps(test_env)\n    test_env_restored = pickle.loads(test_env_pickled)\n    self.assertEqual(\n        test_env.observation_spec(),\n        test_env_restored.observation_spec(),\n    )\n\n  def test_deepcopy(self):\n    test_env = base.EnvironmentWrapper(environment=fakes.DiscreteEnvironment())\n    copied_env = copy.deepcopy(test_env)\n    del copied_env",
  "def test_pickle_unpickle(self):\n    test_env = base.EnvironmentWrapper(environment=fakes.DiscreteEnvironment())\n\n    test_env_pickled = pickle.dumps(test_env)\n    test_env_restored = pickle.loads(test_env_pickled)\n    self.assertEqual(\n        test_env.observation_spec(),\n        test_env_restored.observation_spec(),\n    )",
  "def test_deepcopy(self):\n    test_env = base.EnvironmentWrapper(environment=fakes.DiscreteEnvironment())\n    copied_env = copy.deepcopy(test_env)\n    del copied_env",
  "class MultigridWrapper(dm_env.Environment):\n  \"\"\"Environment wrapper for Multigrid environments.\n\n  Note: the main difference with vanilla GymWrapper is that reward_spec() is\n  overridden and rewards are cast to np.arrays in step()\n  \"\"\"\n\n  def __init__(self, environment: multigrid.MultiGridEnv):\n    \"\"\"Initializes environment.\n\n    Args:\n      environment: the environment.\n    \"\"\"\n    self._environment = environment\n    self._reset_next_step = True\n    self._last_info = None\n    self.num_agents = environment.n_agents  # pytype: disable=attribute-error\n\n    # Convert action and observation specs.\n    obs_space = self._environment.observation_space\n    act_space = self._environment.action_space\n    self._observation_spec = _convert_to_spec(\n        obs_space, self.num_agents, name='observation')\n    self._action_spec = _convert_to_spec(\n        act_space, self.num_agents, name='action')\n\n  def process_obs(self, observation: types.NestedArray) -> types.NestedArray:\n    # Convert observations to agent-index-first format\n    observation = dict_obs_to_list_obs(observation)\n\n    # Assign dtypes to multigrid observations (some of which are lists by\n    # default, so do not have a precise dtype that matches their observation\n    # spec. This ensures no replay signature mismatch issues occur).\n    observation = tree.map_structure(lambda x, t: np.asarray(x, dtype=t.dtype),\n                                     observation, self.observation_spec())\n    return observation\n\n  def reset(self) -> dm_env.TimeStep:\n    \"\"\"Resets the episode.\"\"\"\n    self._reset_next_step = False\n    observation = self.process_obs(self._environment.reset())\n\n    # Reset the diagnostic information.\n    self._last_info = None\n    return dm_env.restart(observation)\n\n  def step(self, action: types.NestedArray) -> dm_env.TimeStep:\n    \"\"\"Steps the environment.\"\"\"\n    if self._reset_next_step:\n      return self.reset()\n\n    observation, reward, done, info = self._environment.step(action)\n    observation = self.process_obs(observation)\n\n    self._reset_next_step = done\n    self._last_info = info\n\n    def _map_reward_spec(x, t):\n      if np.isscalar(x):\n        return t.dtype.type(x)\n      return np.asarray(x, dtype=t.dtype)\n\n    reward = tree.map_structure(\n        _map_reward_spec,\n        reward,\n        self.reward_spec())\n\n    if done:\n      truncated = info.get('TimeLimit.truncated', False)\n      if truncated:\n        return dm_env.truncation(reward, observation)\n      return dm_env.termination(reward, observation)\n    return dm_env.transition(reward, observation)\n\n  def observation_spec(self) -> types.NestedSpec:  # Internal pytype check.\n    return self._observation_spec\n\n  def action_spec(self) -> types.NestedSpec:  # Internal pytype check.\n    return self._action_spec\n\n  def reward_spec(self) -> types.NestedSpec:  # Internal pytype check.\n    return [specs.Array(shape=(), dtype=float, name='rewards')\n           ] * self._environment.n_agents\n\n  def get_info(self) -> Optional[Dict[str, Any]]:\n    \"\"\"Returns the last info returned from env.step(action).\n\n    Returns:\n      info: dictionary of diagnostic information from the last environment step\n    \"\"\"\n    return self._last_info\n\n  @property\n  def environment(self) -> gym.Env:\n    \"\"\"Returns the wrapped environment.\"\"\"\n    return self._environment\n\n  def __getattr__(self, name: str) -> Any:\n    \"\"\"Returns any other attributes of the underlying environment.\"\"\"\n    return getattr(self._environment, name)\n\n  def close(self):\n    self._environment.close()",
  "def _get_single_agent_spec(spec):\n  \"\"\"Returns a single-agent spec from multiagent multigrid spec.\n\n  Primarily used for converting multigrid specs to multiagent Acme specs,\n  wherein actions and observations specs are expected to be lists (each entry\n  corresponding to the spec of that particular agent). Note that this function\n  assumes homogeneous observation / action specs across all agents, which is the\n  case in multigrid.\n\n  Args:\n    spec: multigrid environment spec.\n  \"\"\"\n  def make_single_agent_spec(spec):\n    if not spec.shape:  # Rewards & discounts\n      shape = ()\n    elif len(spec.shape) == 1:  # Actions\n      shape = ()\n    else:  # Observations\n      shape = spec.shape[1:]\n\n    if isinstance(spec, specs.BoundedArray):\n      # Bounded rewards and discounts often have no dimensions as they are\n      # amongst the agents, whereas observations are of shape [num_agents, ...].\n      # The following pair of if statements handle both cases accordingly.\n      minimum = spec.minimum if spec.minimum.ndim == 0 else spec.minimum[0]\n      maximum = spec.maximum if spec.maximum.ndim == 0 else spec.maximum[0]\n      return specs.BoundedArray(\n          shape=shape,\n          name=spec.name,\n          minimum=minimum,\n          maximum=maximum,\n          dtype=spec.dtype)\n    elif isinstance(spec, specs.DiscreteArray):\n      return specs.DiscreteArray(\n          num_values=spec.num_values, dtype=spec.dtype, name=spec.name)\n    elif isinstance(spec, specs.Array):\n      return specs.Array(shape=shape, dtype=spec.dtype, name=spec.name)\n    else:\n      raise ValueError(f'Unexpected spec type {type(spec)}.')\n\n  single_agent_spec = jax.tree_map(make_single_agent_spec, spec)\n  return single_agent_spec",
  "def _gym_to_spec(space: gym.Space,\n                 name: Optional[str] = None) -> types.NestedSpec:\n  \"\"\"Converts an OpenAI Gym space to a dm_env spec or nested structure of specs.\n\n  Box, MultiBinary and MultiDiscrete Gym spaces are converted to BoundedArray\n  specs. Discrete OpenAI spaces are converted to DiscreteArray specs. Tuple and\n  Dict spaces are recursively converted to tuples and dictionaries of specs.\n\n  Args:\n    space: The Gym space to convert.\n    name: Optional name to apply to all return spec(s).\n\n  Returns:\n    A dm_env spec or nested structure of specs, corresponding to the input\n    space.\n  \"\"\"\n  if isinstance(space, spaces.Discrete):\n    return specs.DiscreteArray(num_values=space.n, dtype=space.dtype, name=name)\n\n  elif isinstance(space, spaces.Box):\n    return specs.BoundedArray(\n        shape=space.shape,\n        dtype=space.dtype,\n        minimum=space.low,\n        maximum=space.high,\n        name=name)\n\n  elif isinstance(space, spaces.MultiBinary):\n    return specs.BoundedArray(\n        shape=space.shape,\n        dtype=space.dtype,\n        minimum=0.0,\n        maximum=1.0,\n        name=name)\n\n  elif isinstance(space, spaces.MultiDiscrete):\n    return specs.BoundedArray(\n        shape=space.shape,\n        dtype=space.dtype,\n        minimum=np.zeros(space.shape),\n        maximum=space.nvec - 1,\n        name=name)\n\n  elif isinstance(space, spaces.Tuple):\n    return tuple(_gym_to_spec(s, name) for s in space.spaces)\n\n  elif isinstance(space, spaces.Dict):\n    return {\n        key: _gym_to_spec(value, key) for key, value in space.spaces.items()\n    }\n\n  else:\n    raise ValueError('Unexpected gym space: {}'.format(space))",
  "def _convert_to_spec(space: gym.Space,\n                     num_agents: int,\n                     name: Optional[str] = None) -> types.NestedSpec:\n  \"\"\"Converts multigrid Gym space to an Acme multiagent spec.\n\n  Args:\n    space: The Gym space to convert.\n    num_agents: the number of agents.\n    name: Optional name to apply to all return spec(s).\n\n  Returns:\n    A dm_env spec or nested structure of specs, corresponding to the input\n    space.\n  \"\"\"\n  # Convert gym specs to acme specs\n  spec = _gym_to_spec(space, name)\n  # Then change spec indexing from observation-key-first to agent-index-first\n  return [_get_single_agent_spec(spec)] * num_agents",
  "def dict_obs_to_list_obs(\n    observation: types.NestedArray\n) -> List[Dict[ma_types.AgentID, types.NestedArray]]:\n  \"\"\"Returns multigrid observations converted to agent-index-first format.\n\n  By default, multigrid observations are structured as:\n    observation['image'][agent_index]\n    observation['direction'][agent_index]\n    ...\n\n  However, multiagent Acme expects observations with agent indices first:\n    observation[agent_index]['image']\n    observation[agent_index]['direction']\n\n  This function simply converts multigrid observations to the latter format.\n\n  Args:\n    observation:\n  \"\"\"\n  return [dict(zip(observation, v)) for v in zip(*observation.values())]",
  "def make_multigrid_environment(\n    env_name: str = 'MultiGrid-Empty-5x5-v0') -> dm_env.Environment:\n  \"\"\"Returns Multigrid Multiagent Gym environment.\n\n  Args:\n    env_name: name of multigrid task. See social_rl.gym_multigrid.envs for the\n      available environments.\n  \"\"\"\n  # Load the gym environment.\n  env = gym.make(env_name)\n\n  # Make sure the environment obeys the dm_env.Environment interface.\n  env = MultigridWrapper(env)\n  env = wrappers.SinglePrecisionWrapper(env)\n  env = multiagent_dict_key_wrapper.MultiagentDictKeyWrapper(env)\n  return env",
  "def __init__(self, environment: multigrid.MultiGridEnv):\n    \"\"\"Initializes environment.\n\n    Args:\n      environment: the environment.\n    \"\"\"\n    self._environment = environment\n    self._reset_next_step = True\n    self._last_info = None\n    self.num_agents = environment.n_agents  # pytype: disable=attribute-error\n\n    # Convert action and observation specs.\n    obs_space = self._environment.observation_space\n    act_space = self._environment.action_space\n    self._observation_spec = _convert_to_spec(\n        obs_space, self.num_agents, name='observation')\n    self._action_spec = _convert_to_spec(\n        act_space, self.num_agents, name='action')",
  "def process_obs(self, observation: types.NestedArray) -> types.NestedArray:\n    # Convert observations to agent-index-first format\n    observation = dict_obs_to_list_obs(observation)\n\n    # Assign dtypes to multigrid observations (some of which are lists by\n    # default, so do not have a precise dtype that matches their observation\n    # spec. This ensures no replay signature mismatch issues occur).\n    observation = tree.map_structure(lambda x, t: np.asarray(x, dtype=t.dtype),\n                                     observation, self.observation_spec())\n    return observation",
  "def reset(self) -> dm_env.TimeStep:\n    \"\"\"Resets the episode.\"\"\"\n    self._reset_next_step = False\n    observation = self.process_obs(self._environment.reset())\n\n    # Reset the diagnostic information.\n    self._last_info = None\n    return dm_env.restart(observation)",
  "def step(self, action: types.NestedArray) -> dm_env.TimeStep:\n    \"\"\"Steps the environment.\"\"\"\n    if self._reset_next_step:\n      return self.reset()\n\n    observation, reward, done, info = self._environment.step(action)\n    observation = self.process_obs(observation)\n\n    self._reset_next_step = done\n    self._last_info = info\n\n    def _map_reward_spec(x, t):\n      if np.isscalar(x):\n        return t.dtype.type(x)\n      return np.asarray(x, dtype=t.dtype)\n\n    reward = tree.map_structure(\n        _map_reward_spec,\n        reward,\n        self.reward_spec())\n\n    if done:\n      truncated = info.get('TimeLimit.truncated', False)\n      if truncated:\n        return dm_env.truncation(reward, observation)\n      return dm_env.termination(reward, observation)\n    return dm_env.transition(reward, observation)",
  "def observation_spec(self) -> types.NestedSpec:  # Internal pytype check.\n    return self._observation_spec",
  "def action_spec(self) -> types.NestedSpec:  # Internal pytype check.\n    return self._action_spec",
  "def reward_spec(self) -> types.NestedSpec:  # Internal pytype check.\n    return [specs.Array(shape=(), dtype=float, name='rewards')\n           ] * self._environment.n_agents",
  "def get_info(self) -> Optional[Dict[str, Any]]:\n    \"\"\"Returns the last info returned from env.step(action).\n\n    Returns:\n      info: dictionary of diagnostic information from the last environment step\n    \"\"\"\n    return self._last_info",
  "def environment(self) -> gym.Env:\n    \"\"\"Returns the wrapped environment.\"\"\"\n    return self._environment",
  "def __getattr__(self, name: str) -> Any:\n    \"\"\"Returns any other attributes of the underlying environment.\"\"\"\n    return getattr(self._environment, name)",
  "def close(self):\n    self._environment.close()",
  "def make_single_agent_spec(spec):\n    if not spec.shape:  # Rewards & discounts\n      shape = ()\n    elif len(spec.shape) == 1:  # Actions\n      shape = ()\n    else:  # Observations\n      shape = spec.shape[1:]\n\n    if isinstance(spec, specs.BoundedArray):\n      # Bounded rewards and discounts often have no dimensions as they are\n      # amongst the agents, whereas observations are of shape [num_agents, ...].\n      # The following pair of if statements handle both cases accordingly.\n      minimum = spec.minimum if spec.minimum.ndim == 0 else spec.minimum[0]\n      maximum = spec.maximum if spec.maximum.ndim == 0 else spec.maximum[0]\n      return specs.BoundedArray(\n          shape=shape,\n          name=spec.name,\n          minimum=minimum,\n          maximum=maximum,\n          dtype=spec.dtype)\n    elif isinstance(spec, specs.DiscreteArray):\n      return specs.DiscreteArray(\n          num_values=spec.num_values, dtype=spec.dtype, name=spec.name)\n    elif isinstance(spec, specs.Array):\n      return specs.Array(shape=shape, dtype=spec.dtype, name=spec.name)\n    else:\n      raise ValueError(f'Unexpected spec type {type(spec)}.')",
  "def _map_reward_spec(x, t):\n      if np.isscalar(x):\n        return t.dtype.type(x)\n      return np.asarray(x, dtype=t.dtype)",
  "class GymWrapperTest(absltest.TestCase):\n\n  def test_gym_cartpole(self):\n    env = gym_wrapper.GymWrapper(gym.make('CartPole-v0'))\n\n    # Test converted observation spec.\n    observation_spec: specs.BoundedArray = env.observation_spec()\n    self.assertEqual(type(observation_spec), specs.BoundedArray)\n    self.assertEqual(observation_spec.shape, (4,))\n    self.assertEqual(observation_spec.minimum.shape, (4,))\n    self.assertEqual(observation_spec.maximum.shape, (4,))\n    self.assertEqual(observation_spec.dtype, np.dtype('float32'))\n\n    # Test converted action spec.\n    action_spec: specs.BoundedArray = env.action_spec()\n    self.assertEqual(type(action_spec), specs.DiscreteArray)\n    self.assertEqual(action_spec.shape, ())\n    self.assertEqual(action_spec.minimum, 0)\n    self.assertEqual(action_spec.maximum, 1)\n    self.assertEqual(action_spec.num_values, 2)\n    self.assertEqual(action_spec.dtype, np.dtype('int64'))\n\n    # Test step.\n    timestep = env.reset()\n    self.assertTrue(timestep.first())\n    timestep = env.step(1)\n    self.assertEqual(timestep.reward, 1.0)\n    self.assertTrue(np.isscalar(timestep.reward))\n    self.assertEqual(timestep.observation.shape, (4,))\n    env.close()\n\n  def test_early_truncation(self):\n    # Pendulum has no early termination condition. Recent versions of gym force\n    # to use v1. We try both in case an earlier version is installed.\n    try:\n      gym_env = gym.make('Pendulum-v1')\n    except:  # pylint: disable=bare-except\n      gym_env = gym.make('Pendulum-v0')\n    env = gym_wrapper.GymWrapper(gym_env)\n    ts = env.reset()\n    while not ts.last():\n      ts = env.step(env.action_spec().generate_value())\n    self.assertEqual(ts.discount, 1.0)\n    self.assertTrue(np.isscalar(ts.reward))\n    env.close()\n\n  def test_multi_discrete(self):\n    space = gym.spaces.MultiDiscrete([2, 3])\n    spec = gym_wrapper._convert_to_spec(space)\n\n    spec.validate([0, 0])\n    spec.validate([1, 2])\n\n    self.assertRaises(ValueError, spec.validate, [2, 2])\n    self.assertRaises(ValueError, spec.validate, [1, 3])",
  "class AtariGymWrapperTest(absltest.TestCase):\n\n  def test_pong(self):\n    env = gym.make('PongNoFrameskip-v4', full_action_space=True)\n    env = gym_wrapper.GymAtariAdapter(env)\n\n    # Test converted observation spec. This should expose (RGB, LIVES).\n    observation_spec = env.observation_spec()\n    self.assertEqual(type(observation_spec[0]), specs.BoundedArray)\n    self.assertEqual(type(observation_spec[1]), specs.Array)\n\n    # Test converted action spec.\n    action_spec: specs.DiscreteArray = env.action_spec()[0]\n    self.assertEqual(type(action_spec), specs.DiscreteArray)\n    self.assertEqual(action_spec.shape, ())\n    self.assertEqual(action_spec.minimum, 0)\n    self.assertEqual(action_spec.maximum, 17)\n    self.assertEqual(action_spec.num_values, 18)\n    self.assertEqual(action_spec.dtype, np.dtype('int64'))\n\n    # Test step.\n    timestep = env.reset()\n    self.assertTrue(timestep.first())\n    _ = env.step([np.array(0)])\n    env.close()",
  "def test_gym_cartpole(self):\n    env = gym_wrapper.GymWrapper(gym.make('CartPole-v0'))\n\n    # Test converted observation spec.\n    observation_spec: specs.BoundedArray = env.observation_spec()\n    self.assertEqual(type(observation_spec), specs.BoundedArray)\n    self.assertEqual(observation_spec.shape, (4,))\n    self.assertEqual(observation_spec.minimum.shape, (4,))\n    self.assertEqual(observation_spec.maximum.shape, (4,))\n    self.assertEqual(observation_spec.dtype, np.dtype('float32'))\n\n    # Test converted action spec.\n    action_spec: specs.BoundedArray = env.action_spec()\n    self.assertEqual(type(action_spec), specs.DiscreteArray)\n    self.assertEqual(action_spec.shape, ())\n    self.assertEqual(action_spec.minimum, 0)\n    self.assertEqual(action_spec.maximum, 1)\n    self.assertEqual(action_spec.num_values, 2)\n    self.assertEqual(action_spec.dtype, np.dtype('int64'))\n\n    # Test step.\n    timestep = env.reset()\n    self.assertTrue(timestep.first())\n    timestep = env.step(1)\n    self.assertEqual(timestep.reward, 1.0)\n    self.assertTrue(np.isscalar(timestep.reward))\n    self.assertEqual(timestep.observation.shape, (4,))\n    env.close()",
  "def test_early_truncation(self):\n    # Pendulum has no early termination condition. Recent versions of gym force\n    # to use v1. We try both in case an earlier version is installed.\n    try:\n      gym_env = gym.make('Pendulum-v1')\n    except:  # pylint: disable=bare-except\n      gym_env = gym.make('Pendulum-v0')\n    env = gym_wrapper.GymWrapper(gym_env)\n    ts = env.reset()\n    while not ts.last():\n      ts = env.step(env.action_spec().generate_value())\n    self.assertEqual(ts.discount, 1.0)\n    self.assertTrue(np.isscalar(ts.reward))\n    env.close()",
  "def test_multi_discrete(self):\n    space = gym.spaces.MultiDiscrete([2, 3])\n    spec = gym_wrapper._convert_to_spec(space)\n\n    spec.validate([0, 0])\n    spec.validate([1, 2])\n\n    self.assertRaises(ValueError, spec.validate, [2, 2])\n    self.assertRaises(ValueError, spec.validate, [1, 3])",
  "def test_pong(self):\n    env = gym.make('PongNoFrameskip-v4', full_action_space=True)\n    env = gym_wrapper.GymAtariAdapter(env)\n\n    # Test converted observation spec. This should expose (RGB, LIVES).\n    observation_spec = env.observation_spec()\n    self.assertEqual(type(observation_spec[0]), specs.BoundedArray)\n    self.assertEqual(type(observation_spec[1]), specs.Array)\n\n    # Test converted action spec.\n    action_spec: specs.DiscreteArray = env.action_spec()[0]\n    self.assertEqual(type(action_spec), specs.DiscreteArray)\n    self.assertEqual(action_spec.shape, ())\n    self.assertEqual(action_spec.minimum, 0)\n    self.assertEqual(action_spec.maximum, 17)\n    self.assertEqual(action_spec.num_values, 18)\n    self.assertEqual(action_spec.dtype, np.dtype('int64'))\n\n    # Test step.\n    timestep = env.reset()\n    self.assertTrue(timestep.first())\n    _ = env.step([np.array(0)])\n    env.close()",
  "class MultiagentDictKeyWrapper(base.EnvironmentWrapper):\n  \"\"\"Wrapper that converts list-indexed multiagent environments to dict-indexed.\n\n  Specifically, if the underlying environment observation and actions are:\n    observation = [observation_agent_0, observation_agent_1, ...]\n    action = [action_agent_0, action_agent_1, ...]\n\n  They are converted instead to:\n    observation = {'0': observation_agent_0, '1': observation_agent_1, ...}\n    action = {'0': action_agent_0, '1': action_agent_1, ...}\n\n  This can be helpful in situations where dict-based structures are natively\n  supported, whereas lists are not (e.g., in tfds, where ragged observation data\n  can directly be supported if dicts, but not natively supported as lists).\n  \"\"\"\n\n  def __init__(self, environment: dm_env.Environment):\n    self._environment = environment\n    # Convert action and observation specs.\n    self._action_spec = self._list_to_dict(self._environment.action_spec())\n    self._discount_spec = self._list_to_dict(self._environment.discount_spec())\n    self._observation_spec = self._list_to_dict(\n        self._environment.observation_spec())\n    self._reward_spec = self._list_to_dict(self._environment.reward_spec())\n\n  def _list_to_dict(self, data: Union[List[V], V]) -> Union[Dict[str, V], V]:\n    \"\"\"Convert list-indexed data to dict-indexed, otherwise passthrough.\"\"\"\n    if isinstance(data, list):\n      return {str(k): v for k, v in enumerate(data)}\n    return data\n\n  def _dict_to_list(self, data: Union[Dict[str, V], V]) -> Union[List[V], V]:\n    \"\"\"Convert dict-indexed data to list-indexed, otherwise passthrough.\"\"\"\n    if isinstance(data, dict):\n      return [data[str(i_agent)]\n              for i_agent in range(self._environment.num_agents)]   # pytype: disable=attribute-error\n    return data\n\n  def _convert_timestep(self, timestep: dm_env.TimeStep) -> dm_env.TimeStep:\n    return timestep._replace(\n        reward=self._list_to_dict(timestep.reward),\n        discount=self._list_to_dict(timestep.discount),\n        observation=self._list_to_dict(timestep.observation))\n\n  def step(self, action: Dict[int, Any]) -> dm_env.TimeStep:\n    return self._convert_timestep(\n        self._environment.step(self._dict_to_list(action)))\n\n  def reset(self) -> dm_env.TimeStep:\n    return self._convert_timestep(self._environment.reset())\n\n  def action_spec(self) -> types.NestedSpec:  # Internal pytype check.\n    return self._action_spec\n\n  def discount_spec(self) -> types.NestedSpec:  # Internal pytype check.\n    return self._discount_spec\n\n  def observation_spec(self) -> types.NestedSpec:  # Internal pytype check.\n    return self._observation_spec\n\n  def reward_spec(self) -> types.NestedSpec:  # Internal pytype check.\n    return self._reward_spec",
  "def __init__(self, environment: dm_env.Environment):\n    self._environment = environment\n    # Convert action and observation specs.\n    self._action_spec = self._list_to_dict(self._environment.action_spec())\n    self._discount_spec = self._list_to_dict(self._environment.discount_spec())\n    self._observation_spec = self._list_to_dict(\n        self._environment.observation_spec())\n    self._reward_spec = self._list_to_dict(self._environment.reward_spec())",
  "def _list_to_dict(self, data: Union[List[V], V]) -> Union[Dict[str, V], V]:\n    \"\"\"Convert list-indexed data to dict-indexed, otherwise passthrough.\"\"\"\n    if isinstance(data, list):\n      return {str(k): v for k, v in enumerate(data)}\n    return data",
  "def _dict_to_list(self, data: Union[Dict[str, V], V]) -> Union[List[V], V]:\n    \"\"\"Convert dict-indexed data to list-indexed, otherwise passthrough.\"\"\"\n    if isinstance(data, dict):\n      return [data[str(i_agent)]\n              for i_agent in range(self._environment.num_agents)]   # pytype: disable=attribute-error\n    return data",
  "def _convert_timestep(self, timestep: dm_env.TimeStep) -> dm_env.TimeStep:\n    return timestep._replace(\n        reward=self._list_to_dict(timestep.reward),\n        discount=self._list_to_dict(timestep.discount),\n        observation=self._list_to_dict(timestep.observation))",
  "def step(self, action: Dict[int, Any]) -> dm_env.TimeStep:\n    return self._convert_timestep(\n        self._environment.step(self._dict_to_list(action)))",
  "def reset(self) -> dm_env.TimeStep:\n    return self._convert_timestep(self._environment.reset())",
  "def action_spec(self) -> types.NestedSpec:  # Internal pytype check.\n    return self._action_spec",
  "def discount_spec(self) -> types.NestedSpec:  # Internal pytype check.\n    return self._discount_spec",
  "def observation_spec(self) -> types.NestedSpec:  # Internal pytype check.\n    return self._observation_spec",
  "def reward_spec(self) -> types.NestedSpec:  # Internal pytype check.\n    return self._reward_spec",
  "class NoopStartsWrapper(base.EnvironmentWrapper):\n  \"\"\"Implements random noop starts to episodes.\n\n  This introduces randomness into an otherwise deterministic environment.\n\n  Note that the base environment must support a no-op action and the value\n  of this action must be known and provided to this wrapper.\n  \"\"\"\n\n  def __init__(self,\n               environment: dm_env.Environment,\n               noop_action: types.NestedArray = 0,\n               noop_max: int = 30,\n               seed: Optional[int] = None):\n    \"\"\"Initializes a `NoopStartsWrapper` wrapper.\n\n    Args:\n      environment: An environment conforming to the dm_env.Environment\n        interface.\n      noop_action: The noop action used to step the environment for random\n        initialisation.\n      noop_max: The maximal number of noop actions at the start of an episode.\n      seed: The random seed used to sample the number of noops.\n    \"\"\"\n    if noop_max < 0:\n      raise ValueError(\n          'Maximal number of no-ops after reset cannot be negative. '\n          f'Received noop_max={noop_max}')\n\n    super().__init__(environment)\n    self.np_random = np.random.RandomState(seed)\n    self._noop_max = noop_max\n    self._noop_action = noop_action\n\n  def reset(self) -> dm_env.TimeStep:\n    \"\"\"Resets environment and provides the first timestep.\"\"\"\n    noops = self.np_random.randint(self._noop_max + 1)\n    timestep = self.environment.reset()\n    for _ in range(noops):\n      timestep = self.environment.step(self._noop_action)\n      if timestep.last():\n        timestep = self.environment.reset()\n\n    return timestep._replace(step_type=dm_env.StepType.FIRST)",
  "def __init__(self,\n               environment: dm_env.Environment,\n               noop_action: types.NestedArray = 0,\n               noop_max: int = 30,\n               seed: Optional[int] = None):\n    \"\"\"Initializes a `NoopStartsWrapper` wrapper.\n\n    Args:\n      environment: An environment conforming to the dm_env.Environment\n        interface.\n      noop_action: The noop action used to step the environment for random\n        initialisation.\n      noop_max: The maximal number of noop actions at the start of an episode.\n      seed: The random seed used to sample the number of noops.\n    \"\"\"\n    if noop_max < 0:\n      raise ValueError(\n          'Maximal number of no-ops after reset cannot be negative. '\n          f'Received noop_max={noop_max}')\n\n    super().__init__(environment)\n    self.np_random = np.random.RandomState(seed)\n    self._noop_max = noop_max\n    self._noop_action = noop_action",
  "def reset(self) -> dm_env.TimeStep:\n    \"\"\"Resets environment and provides the first timestep.\"\"\"\n    noops = self.np_random.randint(self._noop_max + 1)\n    timestep = self.environment.reset()\n    for _ in range(noops):\n      timestep = self.environment.step(self._noop_action)\n      if timestep.last():\n        timestep = self.environment.reset()\n\n    return timestep._replace(step_type=dm_env.StepType.FIRST)",
  "class GymWrapper(dm_env.Environment):\n  \"\"\"Environment wrapper for OpenAI Gym environments.\"\"\"\n\n  # Note: we don't inherit from base.EnvironmentWrapper because that class\n  # assumes that the wrapped environment is a dm_env.Environment.\n\n  def __init__(self, environment: gym.Env):\n\n    self._environment = environment\n    self._reset_next_step = True\n    self._last_info = None\n\n    # Convert action and observation specs.\n    obs_space = self._environment.observation_space\n    act_space = self._environment.action_space\n    self._observation_spec = _convert_to_spec(obs_space, name='observation')\n    self._action_spec = _convert_to_spec(act_space, name='action')\n\n  def reset(self) -> dm_env.TimeStep:\n    \"\"\"Resets the episode.\"\"\"\n    self._reset_next_step = False\n    observation = self._environment.reset()\n    # Reset the diagnostic information.\n    self._last_info = None\n    return dm_env.restart(observation)\n\n  def step(self, action: types.NestedArray) -> dm_env.TimeStep:\n    \"\"\"Steps the environment.\"\"\"\n    if self._reset_next_step:\n      return self.reset()\n\n    observation, reward, done, info = self._environment.step(action)\n    self._reset_next_step = done\n    self._last_info = info\n\n    # Convert the type of the reward based on the spec, respecting the scalar or\n    # array property.\n    reward = tree.map_structure(\n        lambda x, t: (  # pylint: disable=g-long-lambda\n            t.dtype.type(x)\n            if np.isscalar(x) else np.asarray(x, dtype=t.dtype)),\n        reward,\n        self.reward_spec())\n\n    if done:\n      truncated = info.get('TimeLimit.truncated', False)\n      if truncated:\n        return dm_env.truncation(reward, observation)\n      return dm_env.termination(reward, observation)\n    return dm_env.transition(reward, observation)\n\n  def observation_spec(self) -> types.NestedSpec:\n    return self._observation_spec\n\n  def action_spec(self) -> types.NestedSpec:\n    return self._action_spec\n\n  def get_info(self) -> Optional[Dict[str, Any]]:\n    \"\"\"Returns the last info returned from env.step(action).\n\n    Returns:\n      info: dictionary of diagnostic information from the last environment step\n    \"\"\"\n    return self._last_info\n\n  @property\n  def environment(self) -> gym.Env:\n    \"\"\"Returns the wrapped environment.\"\"\"\n    return self._environment\n\n  def __getattr__(self, name: str):\n    if name.startswith('__'):\n      raise AttributeError(\n          \"attempted to get missing private attribute '{}'\".format(name))\n    return getattr(self._environment, name)\n\n  def close(self):\n    self._environment.close()",
  "def _convert_to_spec(space: gym.Space,\n                     name: Optional[str] = None) -> types.NestedSpec:\n  \"\"\"Converts an OpenAI Gym space to a dm_env spec or nested structure of specs.\n\n  Box, MultiBinary and MultiDiscrete Gym spaces are converted to BoundedArray\n  specs. Discrete OpenAI spaces are converted to DiscreteArray specs. Tuple and\n  Dict spaces are recursively converted to tuples and dictionaries of specs.\n\n  Args:\n    space: The Gym space to convert.\n    name: Optional name to apply to all return spec(s).\n\n  Returns:\n    A dm_env spec or nested structure of specs, corresponding to the input\n    space.\n  \"\"\"\n  if isinstance(space, spaces.Discrete):\n    return specs.DiscreteArray(num_values=space.n, dtype=space.dtype, name=name)\n\n  elif isinstance(space, spaces.Box):\n    return specs.BoundedArray(\n        shape=space.shape,\n        dtype=space.dtype,\n        minimum=space.low,\n        maximum=space.high,\n        name=name)\n\n  elif isinstance(space, spaces.MultiBinary):\n    return specs.BoundedArray(\n        shape=space.shape,\n        dtype=space.dtype,\n        minimum=0.0,\n        maximum=1.0,\n        name=name)\n\n  elif isinstance(space, spaces.MultiDiscrete):\n    return specs.BoundedArray(\n        shape=space.shape,\n        dtype=space.dtype,\n        minimum=np.zeros(space.shape),\n        maximum=space.nvec - 1,\n        name=name)\n\n  elif isinstance(space, spaces.Tuple):\n    return tuple(_convert_to_spec(s, name) for s in space.spaces)\n\n  elif isinstance(space, spaces.Dict):\n    return {\n        key: _convert_to_spec(value, key)\n        for key, value in space.spaces.items()\n    }\n\n  else:\n    raise ValueError('Unexpected gym space: {}'.format(space))",
  "class GymAtariAdapter(GymWrapper):\n  \"\"\"Specialized wrapper exposing a Gym Atari environment.\n\n  This wraps the Gym Atari environment in the same way as GymWrapper, but also\n  exposes the lives count as an observation. The resuling observations are\n  a tuple whose first element is the RGB observations and the second is the\n  lives count.\n  \"\"\"\n\n  def _wrap_observation(self,\n                        observation: types.NestedArray) -> types.NestedArray:\n    # pytype: disable=attribute-error\n    return observation, self._environment.ale.lives()\n    # pytype: enable=attribute-error\n\n  def reset(self) -> dm_env.TimeStep:\n    \"\"\"Resets the episode.\"\"\"\n    self._reset_next_step = False\n    observation = self._environment.reset()\n    observation = self._wrap_observation(observation)\n    return dm_env.restart(observation)\n\n  def step(self, action: List[np.ndarray]) -> dm_env.TimeStep:\n    \"\"\"Steps the environment.\"\"\"\n    if self._reset_next_step:\n      return self.reset()\n\n    observation, reward, done, _ = self._environment.step(action[0].item())\n    self._reset_next_step = done\n\n    observation = self._wrap_observation(observation)\n\n    if done:\n      return dm_env.termination(reward, observation)\n    return dm_env.transition(reward, observation)\n\n  def observation_spec(self) -> types.NestedSpec:\n    return (self._observation_spec,\n            specs.Array(shape=(), dtype=np.dtype('float64'), name='lives'))\n\n  def action_spec(self) -> List[specs.BoundedArray]:\n    return [self._action_spec]",
  "def __init__(self, environment: gym.Env):\n\n    self._environment = environment\n    self._reset_next_step = True\n    self._last_info = None\n\n    # Convert action and observation specs.\n    obs_space = self._environment.observation_space\n    act_space = self._environment.action_space\n    self._observation_spec = _convert_to_spec(obs_space, name='observation')\n    self._action_spec = _convert_to_spec(act_space, name='action')",
  "def reset(self) -> dm_env.TimeStep:\n    \"\"\"Resets the episode.\"\"\"\n    self._reset_next_step = False\n    observation = self._environment.reset()\n    # Reset the diagnostic information.\n    self._last_info = None\n    return dm_env.restart(observation)",
  "def step(self, action: types.NestedArray) -> dm_env.TimeStep:\n    \"\"\"Steps the environment.\"\"\"\n    if self._reset_next_step:\n      return self.reset()\n\n    observation, reward, done, info = self._environment.step(action)\n    self._reset_next_step = done\n    self._last_info = info\n\n    # Convert the type of the reward based on the spec, respecting the scalar or\n    # array property.\n    reward = tree.map_structure(\n        lambda x, t: (  # pylint: disable=g-long-lambda\n            t.dtype.type(x)\n            if np.isscalar(x) else np.asarray(x, dtype=t.dtype)),\n        reward,\n        self.reward_spec())\n\n    if done:\n      truncated = info.get('TimeLimit.truncated', False)\n      if truncated:\n        return dm_env.truncation(reward, observation)\n      return dm_env.termination(reward, observation)\n    return dm_env.transition(reward, observation)",
  "def observation_spec(self) -> types.NestedSpec:\n    return self._observation_spec",
  "def action_spec(self) -> types.NestedSpec:\n    return self._action_spec",
  "def get_info(self) -> Optional[Dict[str, Any]]:\n    \"\"\"Returns the last info returned from env.step(action).\n\n    Returns:\n      info: dictionary of diagnostic information from the last environment step\n    \"\"\"\n    return self._last_info",
  "def environment(self) -> gym.Env:\n    \"\"\"Returns the wrapped environment.\"\"\"\n    return self._environment",
  "def __getattr__(self, name: str):\n    if name.startswith('__'):\n      raise AttributeError(\n          \"attempted to get missing private attribute '{}'\".format(name))\n    return getattr(self._environment, name)",
  "def close(self):\n    self._environment.close()",
  "def _wrap_observation(self,\n                        observation: types.NestedArray) -> types.NestedArray:\n    # pytype: disable=attribute-error\n    return observation, self._environment.ale.lives()",
  "def reset(self) -> dm_env.TimeStep:\n    \"\"\"Resets the episode.\"\"\"\n    self._reset_next_step = False\n    observation = self._environment.reset()\n    observation = self._wrap_observation(observation)\n    return dm_env.restart(observation)",
  "def step(self, action: List[np.ndarray]) -> dm_env.TimeStep:\n    \"\"\"Steps the environment.\"\"\"\n    if self._reset_next_step:\n      return self.reset()\n\n    observation, reward, done, _ = self._environment.step(action[0].item())\n    self._reset_next_step = done\n\n    observation = self._wrap_observation(observation)\n\n    if done:\n      return dm_env.termination(reward, observation)\n    return dm_env.transition(reward, observation)",
  "def observation_spec(self) -> types.NestedSpec:\n    return (self._observation_spec,\n            specs.Array(shape=(), dtype=np.dtype('float64'), name='lives'))",
  "def action_spec(self) -> List[specs.BoundedArray]:\n    return [self._action_spec]",
  "class StepLimitWrapperTest(absltest.TestCase):\n\n  def test_step(self):\n    fake_env = fakes.DiscreteEnvironment(episode_length=5)\n    env = wrappers.StepLimitWrapper(fake_env, step_limit=2)\n\n    env.reset()\n    env.step(ACTION)\n    self.assertTrue(env.step(ACTION).last())\n\n  def test_step_on_new_env(self):\n    fake_env = fakes.DiscreteEnvironment(episode_length=5)\n    env = wrappers.StepLimitWrapper(fake_env, step_limit=2)\n\n    self.assertTrue(env.step(ACTION).first())\n    self.assertFalse(env.step(ACTION).last())\n    self.assertTrue(env.step(ACTION).last())\n\n  def test_step_after_truncation(self):\n    fake_env = fakes.DiscreteEnvironment(episode_length=5)\n    env = wrappers.StepLimitWrapper(fake_env, step_limit=2)\n\n    env.reset()\n    env.step(ACTION)\n    self.assertTrue(env.step(ACTION).last())\n\n    self.assertTrue(env.step(ACTION).first())\n    self.assertFalse(env.step(ACTION).last())\n    self.assertTrue(env.step(ACTION).last())\n\n  def test_step_after_termination(self):\n    fake_env = fakes.DiscreteEnvironment(episode_length=5)\n\n    fake_env.reset()\n    fake_env.step(ACTION)\n    fake_env.step(ACTION)\n    fake_env.step(ACTION)\n    fake_env.step(ACTION)\n    self.assertTrue(fake_env.step(ACTION).last())\n\n    env = wrappers.StepLimitWrapper(fake_env, step_limit=2)\n\n    self.assertTrue(env.step(ACTION).first())\n    self.assertFalse(env.step(ACTION).last())\n    self.assertTrue(env.step(ACTION).last())",
  "def test_step(self):\n    fake_env = fakes.DiscreteEnvironment(episode_length=5)\n    env = wrappers.StepLimitWrapper(fake_env, step_limit=2)\n\n    env.reset()\n    env.step(ACTION)\n    self.assertTrue(env.step(ACTION).last())",
  "def test_step_on_new_env(self):\n    fake_env = fakes.DiscreteEnvironment(episode_length=5)\n    env = wrappers.StepLimitWrapper(fake_env, step_limit=2)\n\n    self.assertTrue(env.step(ACTION).first())\n    self.assertFalse(env.step(ACTION).last())\n    self.assertTrue(env.step(ACTION).last())",
  "def test_step_after_truncation(self):\n    fake_env = fakes.DiscreteEnvironment(episode_length=5)\n    env = wrappers.StepLimitWrapper(fake_env, step_limit=2)\n\n    env.reset()\n    env.step(ACTION)\n    self.assertTrue(env.step(ACTION).last())\n\n    self.assertTrue(env.step(ACTION).first())\n    self.assertFalse(env.step(ACTION).last())\n    self.assertTrue(env.step(ACTION).last())",
  "def test_step_after_termination(self):\n    fake_env = fakes.DiscreteEnvironment(episode_length=5)\n\n    fake_env.reset()\n    fake_env.step(ACTION)\n    fake_env.step(ACTION)\n    fake_env.step(ACTION)\n    fake_env.step(ACTION)\n    self.assertTrue(fake_env.step(ACTION).last())\n\n    env = wrappers.StepLimitWrapper(fake_env, step_limit=2)\n\n    self.assertTrue(env.step(ACTION).first())\n    self.assertFalse(env.step(ACTION).last())\n    self.assertTrue(env.step(ACTION).last())",
  "class NoopStartsTest(absltest.TestCase):\n\n  def test_reset(self):\n    \"\"\"Ensure that noop starts `reset` steps the environment multiple times.\"\"\"\n    noop_action = 0\n    noop_max = 10\n    seed = 24\n\n    base_env = fakes.DiscreteEnvironment(\n        action_dtype=np.int64,\n        obs_dtype=np.int64,\n        reward_spec=specs.Array(dtype=np.float64, shape=()))\n    mock_step_fn = mock.MagicMock()\n    expected_num_step_calls = np.random.RandomState(seed).randint(noop_max + 1)\n\n    with mock.patch.object(base_env, 'step', mock_step_fn):\n      env = wrappers.NoopStartsWrapper(\n          base_env,\n          noop_action=noop_action,\n          noop_max=noop_max,\n          seed=seed,\n      )\n      env.reset()\n\n      # Test environment step called with noop action as part of wrapper.reset\n      mock_step_fn.assert_called_with(noop_action)\n      self.assertEqual(mock_step_fn.call_count, expected_num_step_calls)\n      self.assertEqual(mock_step_fn.call_args, ((noop_action,), {}))\n\n  def test_raises_value_error(self):\n    \"\"\"Ensure that wrapper raises error if noop_max is <0.\"\"\"\n    base_env = fakes.DiscreteEnvironment(\n        action_dtype=np.int64,\n        obs_dtype=np.int64,\n        reward_spec=specs.Array(dtype=np.float64, shape=()))\n\n    with self.assertRaises(ValueError):\n      wrappers.NoopStartsWrapper(base_env, noop_action=0, noop_max=-1, seed=24)",
  "def test_reset(self):\n    \"\"\"Ensure that noop starts `reset` steps the environment multiple times.\"\"\"\n    noop_action = 0\n    noop_max = 10\n    seed = 24\n\n    base_env = fakes.DiscreteEnvironment(\n        action_dtype=np.int64,\n        obs_dtype=np.int64,\n        reward_spec=specs.Array(dtype=np.float64, shape=()))\n    mock_step_fn = mock.MagicMock()\n    expected_num_step_calls = np.random.RandomState(seed).randint(noop_max + 1)\n\n    with mock.patch.object(base_env, 'step', mock_step_fn):\n      env = wrappers.NoopStartsWrapper(\n          base_env,\n          noop_action=noop_action,\n          noop_max=noop_max,\n          seed=seed,\n      )\n      env.reset()\n\n      # Test environment step called with noop action as part of wrapper.reset\n      mock_step_fn.assert_called_with(noop_action)\n      self.assertEqual(mock_step_fn.call_count, expected_num_step_calls)\n      self.assertEqual(mock_step_fn.call_args, ((noop_action,), {}))",
  "def test_raises_value_error(self):\n    \"\"\"Ensure that wrapper raises error if noop_max is <0.\"\"\"\n    base_env = fakes.DiscreteEnvironment(\n        action_dtype=np.int64,\n        obs_dtype=np.int64,\n        reward_spec=specs.Array(dtype=np.float64, shape=()))\n\n    with self.assertRaises(ValueError):\n      wrappers.NoopStartsWrapper(base_env, noop_action=0, noop_max=-1, seed=24)",
  "class ActionRepeatWrapper(base.EnvironmentWrapper):\n  \"\"\"Action repeat wrapper.\"\"\"\n\n  def __init__(self, environment: dm_env.Environment, num_repeats: int = 1):\n    super().__init__(environment)\n    self._num_repeats = num_repeats\n\n  def step(self, action: types.NestedArray) -> dm_env.TimeStep:\n    # Initialize accumulated reward and discount.\n    reward = 0.\n    discount = 1.\n\n    # Step the environment by repeating action.\n    for _ in range(self._num_repeats):\n      timestep = self._environment.step(action)\n\n      # Accumulate reward and discount.\n      reward += timestep.reward * discount\n      discount *= timestep.discount\n\n      # Don't go over episode boundaries.\n      if timestep.last():\n        break\n\n    # Replace the final timestep's reward and discount.\n    return timestep._replace(reward=reward, discount=discount)",
  "def __init__(self, environment: dm_env.Environment, num_repeats: int = 1):\n    super().__init__(environment)\n    self._num_repeats = num_repeats",
  "def step(self, action: types.NestedArray) -> dm_env.TimeStep:\n    # Initialize accumulated reward and discount.\n    reward = 0.\n    discount = 1.\n\n    # Step the environment by repeating action.\n    for _ in range(self._num_repeats):\n      timestep = self._environment.step(action)\n\n      # Accumulate reward and discount.\n      reward += timestep.reward * discount\n      discount *= timestep.discount\n\n      # Don't go over episode boundaries.\n      if timestep.last():\n        break\n\n    # Replace the final timestep's reward and discount.\n    return timestep._replace(reward=reward, discount=discount)",
  "class OAR(NamedTuple):\n  \"\"\"Container for (Observation, Action, Reward) tuples.\"\"\"\n  observation: types.Nest\n  action: types.Nest\n  reward: types.Nest",
  "class ObservationActionRewardWrapper(base.EnvironmentWrapper):\n  \"\"\"A wrapper that puts the previous action and reward into the observation.\"\"\"\n\n  def reset(self) -> dm_env.TimeStep:\n    # Initialize with zeros of the appropriate shape/dtype.\n    action = tree.map_structure(\n        lambda x: x.generate_value(), self._environment.action_spec())\n    reward = tree.map_structure(\n        lambda x: x.generate_value(), self._environment.reward_spec())\n    timestep = self._environment.reset()\n    new_timestep = self._augment_observation(action, reward, timestep)\n    return new_timestep\n\n  def step(self, action: types.NestedArray) -> dm_env.TimeStep:\n    timestep = self._environment.step(action)\n    new_timestep = self._augment_observation(action, timestep.reward, timestep)\n    return new_timestep\n\n  def _augment_observation(self, action: types.NestedArray,\n                           reward: types.NestedArray,\n                           timestep: dm_env.TimeStep) -> dm_env.TimeStep:\n    oar = OAR(observation=timestep.observation,\n              action=action,\n              reward=reward)\n    return timestep._replace(observation=oar)\n\n  def observation_spec(self):\n    return OAR(observation=self._environment.observation_spec(),\n               action=self.action_spec(),\n               reward=self.reward_spec())",
  "def reset(self) -> dm_env.TimeStep:\n    # Initialize with zeros of the appropriate shape/dtype.\n    action = tree.map_structure(\n        lambda x: x.generate_value(), self._environment.action_spec())\n    reward = tree.map_structure(\n        lambda x: x.generate_value(), self._environment.reward_spec())\n    timestep = self._environment.reset()\n    new_timestep = self._augment_observation(action, reward, timestep)\n    return new_timestep",
  "def step(self, action: types.NestedArray) -> dm_env.TimeStep:\n    timestep = self._environment.step(action)\n    new_timestep = self._augment_observation(action, timestep.reward, timestep)\n    return new_timestep",
  "def _augment_observation(self, action: types.NestedArray,\n                           reward: types.NestedArray,\n                           timestep: dm_env.TimeStep) -> dm_env.TimeStep:\n    oar = OAR(observation=timestep.observation,\n              action=action,\n              reward=reward)\n    return timestep._replace(observation=oar)",
  "def observation_spec(self):\n    return OAR(observation=self._environment.observation_spec(),\n               action=self.action_spec(),\n               reward=self.reward_spec())",
  "class MujocoPixelWrapper(base.EnvironmentWrapper):\n  \"\"\"Produces pixel observations from Mujoco environment observations.\"\"\"\n\n  def __init__(self,\n               environment: control.Environment,\n               *,\n               height: int = 84,\n               width: int = 84,\n               camera_id: int = 0):\n    render_kwargs = {'height': height, 'width': width, 'camera_id': camera_id}\n    pixel_environment = pixels.Wrapper(\n        environment, pixels_only=True, render_kwargs=render_kwargs)\n    super().__init__(pixel_environment)\n\n  def step(self, action) -> dm_env.TimeStep:\n    return self._convert_timestep(self._environment.step(action))\n\n  def reset(self) -> dm_env.TimeStep:\n    return self._convert_timestep(self._environment.reset())\n\n  def observation_spec(self):\n    return self._environment.observation_spec()['pixels']\n\n  def _convert_timestep(self, timestep: dm_env.TimeStep) -> dm_env.TimeStep:\n    \"\"\"Removes the pixel observation's OrderedDict wrapper.\"\"\"\n    observation: collections.OrderedDict = timestep.observation\n    return timestep._replace(observation=observation['pixels'])",
  "def __init__(self,\n               environment: control.Environment,\n               *,\n               height: int = 84,\n               width: int = 84,\n               camera_id: int = 0):\n    render_kwargs = {'height': height, 'width': width, 'camera_id': camera_id}\n    pixel_environment = pixels.Wrapper(\n        environment, pixels_only=True, render_kwargs=render_kwargs)\n    super().__init__(pixel_environment)",
  "def step(self, action) -> dm_env.TimeStep:\n    return self._convert_timestep(self._environment.step(action))",
  "def reset(self) -> dm_env.TimeStep:\n    return self._convert_timestep(self._environment.reset())",
  "def observation_spec(self):\n    return self._environment.observation_spec()['pixels']",
  "def _convert_timestep(self, timestep: dm_env.TimeStep) -> dm_env.TimeStep:\n    \"\"\"Removes the pixel observation's OrderedDict wrapper.\"\"\"\n    observation: collections.OrderedDict = timestep.observation\n    return timestep._replace(observation=observation['pixels'])",
  "def _episode_reward(env):\n  timestep = env.reset()\n  action_spec = env.action_spec()\n  rng = np.random.RandomState(seed=1)\n  reward = []\n  while not timestep.last():\n    timestep = env.step(rng.randint(action_spec.num_values))\n    reward.append(timestep.reward)\n  return reward",
  "def _compare_nested_sequences(seq1, seq2):\n  \"\"\"Compare two sequences of arrays.\"\"\"\n  return all([(l == m).all() for l, m in zip(seq1, seq2)])",
  "class _DiscreteEnvironmentOneReward(fakes.DiscreteEnvironment):\n  \"\"\"A fake discrete environement with constant reward of 1.\"\"\"\n\n  def _generate_fake_reward(self) -> Any:\n    return tree.map_structure(lambda s: s.generate_value() + 1.,\n                              self._spec.rewards)",
  "class DelayedRewardTest(parameterized.TestCase):\n\n  def test_noop(self):\n    \"\"\"Ensure when accumulation_period=1 it does not change anything.\"\"\"\n    base_env = _DiscreteEnvironmentOneReward(\n        action_dtype=np.int64,\n        reward_spec=specs.Array(dtype=np.float32, shape=()))\n    wrapped_env = wrappers.DelayedRewardWrapper(base_env, accumulation_period=1)\n    base_episode_reward = _episode_reward(base_env)\n    wrapped_episode_reward = _episode_reward(wrapped_env)\n    self.assertEqual(base_episode_reward, wrapped_episode_reward)\n\n  def test_noop_composite_reward(self):\n    \"\"\"No-op test with composite rewards.\"\"\"\n    base_env = _DiscreteEnvironmentOneReward(\n        action_dtype=np.int64,\n        reward_spec=specs.Array(dtype=np.float32, shape=(2, 1)))\n    wrapped_env = wrappers.DelayedRewardWrapper(base_env, accumulation_period=1)\n    base_episode_reward = _episode_reward(base_env)\n    wrapped_episode_reward = _episode_reward(wrapped_env)\n    self.assertTrue(\n        _compare_nested_sequences(base_episode_reward, wrapped_episode_reward))\n\n  @parameterized.parameters(10, None)\n  def test_same_episode_composite_reward(self, accumulation_period):\n    \"\"\"Ensure that wrapper does not change total reward.\"\"\"\n    base_env = _DiscreteEnvironmentOneReward(\n        action_dtype=np.int64,\n        reward_spec=specs.Array(dtype=np.float32, shape=()))\n    wrapped_env = wrappers.DelayedRewardWrapper(\n        base_env, accumulation_period=accumulation_period)\n    base_episode_reward = _episode_reward(base_env)\n    wrapped_episode_reward = _episode_reward(wrapped_env)\n    self.assertTrue(\n        (sum(base_episode_reward) == sum(wrapped_episode_reward)).all())",
  "def _generate_fake_reward(self) -> Any:\n    return tree.map_structure(lambda s: s.generate_value() + 1.,\n                              self._spec.rewards)",
  "def test_noop(self):\n    \"\"\"Ensure when accumulation_period=1 it does not change anything.\"\"\"\n    base_env = _DiscreteEnvironmentOneReward(\n        action_dtype=np.int64,\n        reward_spec=specs.Array(dtype=np.float32, shape=()))\n    wrapped_env = wrappers.DelayedRewardWrapper(base_env, accumulation_period=1)\n    base_episode_reward = _episode_reward(base_env)\n    wrapped_episode_reward = _episode_reward(wrapped_env)\n    self.assertEqual(base_episode_reward, wrapped_episode_reward)",
  "def test_noop_composite_reward(self):\n    \"\"\"No-op test with composite rewards.\"\"\"\n    base_env = _DiscreteEnvironmentOneReward(\n        action_dtype=np.int64,\n        reward_spec=specs.Array(dtype=np.float32, shape=(2, 1)))\n    wrapped_env = wrappers.DelayedRewardWrapper(base_env, accumulation_period=1)\n    base_episode_reward = _episode_reward(base_env)\n    wrapped_episode_reward = _episode_reward(wrapped_env)\n    self.assertTrue(\n        _compare_nested_sequences(base_episode_reward, wrapped_episode_reward))",
  "def test_same_episode_composite_reward(self, accumulation_period):\n    \"\"\"Ensure that wrapper does not change total reward.\"\"\"\n    base_env = _DiscreteEnvironmentOneReward(\n        action_dtype=np.int64,\n        reward_spec=specs.Array(dtype=np.float32, shape=()))\n    wrapped_env = wrappers.DelayedRewardWrapper(\n        base_env, accumulation_period=accumulation_period)\n    base_episode_reward = _episode_reward(base_env)\n    wrapped_episode_reward = _episode_reward(wrapped_env)\n    self.assertTrue(\n        (sum(base_episode_reward) == sum(wrapped_episode_reward)).all())",
  "class SinglePrecisionTest(absltest.TestCase):\n\n  def test_continuous(self):\n    env = wrappers.SinglePrecisionWrapper(\n        fakes.ContinuousEnvironment(\n            action_dim=0, dtype=np.float64, reward_dtype=np.float64))\n\n    self.assertTrue(np.issubdtype(env.observation_spec().dtype, np.float32))\n    self.assertTrue(np.issubdtype(env.action_spec().dtype, np.float32))\n    self.assertTrue(np.issubdtype(env.reward_spec().dtype, np.float32))\n    self.assertTrue(np.issubdtype(env.discount_spec().dtype, np.float32))\n\n    timestep = env.reset()\n    self.assertIsNone(timestep.reward)\n    self.assertIsNone(timestep.discount)\n    self.assertTrue(np.issubdtype(timestep.observation.dtype, np.float32))\n\n    timestep = env.step(0.0)\n    self.assertTrue(np.issubdtype(timestep.reward.dtype, np.float32))\n    self.assertTrue(np.issubdtype(timestep.discount.dtype, np.float32))\n    self.assertTrue(np.issubdtype(timestep.observation.dtype, np.float32))\n\n  def test_discrete(self):\n    env = wrappers.SinglePrecisionWrapper(\n        fakes.DiscreteEnvironment(\n            action_dtype=np.int64,\n            obs_dtype=np.int64,\n            reward_spec=specs.Array(dtype=np.float64, shape=())))\n\n    self.assertTrue(np.issubdtype(env.observation_spec().dtype, np.int32))\n    self.assertTrue(np.issubdtype(env.action_spec().dtype, np.int32))\n    self.assertTrue(np.issubdtype(env.reward_spec().dtype, np.float32))\n    self.assertTrue(np.issubdtype(env.discount_spec().dtype, np.float32))\n\n    timestep = env.reset()\n    self.assertIsNone(timestep.reward)\n    self.assertIsNone(timestep.discount)\n    self.assertTrue(np.issubdtype(timestep.observation.dtype, np.int32))\n\n    timestep = env.step(0)\n    self.assertTrue(np.issubdtype(timestep.reward.dtype, np.float32))\n    self.assertTrue(np.issubdtype(timestep.discount.dtype, np.float32))\n    self.assertTrue(np.issubdtype(timestep.observation.dtype, np.int32))",
  "def test_continuous(self):\n    env = wrappers.SinglePrecisionWrapper(\n        fakes.ContinuousEnvironment(\n            action_dim=0, dtype=np.float64, reward_dtype=np.float64))\n\n    self.assertTrue(np.issubdtype(env.observation_spec().dtype, np.float32))\n    self.assertTrue(np.issubdtype(env.action_spec().dtype, np.float32))\n    self.assertTrue(np.issubdtype(env.reward_spec().dtype, np.float32))\n    self.assertTrue(np.issubdtype(env.discount_spec().dtype, np.float32))\n\n    timestep = env.reset()\n    self.assertIsNone(timestep.reward)\n    self.assertIsNone(timestep.discount)\n    self.assertTrue(np.issubdtype(timestep.observation.dtype, np.float32))\n\n    timestep = env.step(0.0)\n    self.assertTrue(np.issubdtype(timestep.reward.dtype, np.float32))\n    self.assertTrue(np.issubdtype(timestep.discount.dtype, np.float32))\n    self.assertTrue(np.issubdtype(timestep.observation.dtype, np.float32))",
  "def test_discrete(self):\n    env = wrappers.SinglePrecisionWrapper(\n        fakes.DiscreteEnvironment(\n            action_dtype=np.int64,\n            obs_dtype=np.int64,\n            reward_spec=specs.Array(dtype=np.float64, shape=())))\n\n    self.assertTrue(np.issubdtype(env.observation_spec().dtype, np.int32))\n    self.assertTrue(np.issubdtype(env.action_spec().dtype, np.int32))\n    self.assertTrue(np.issubdtype(env.reward_spec().dtype, np.float32))\n    self.assertTrue(np.issubdtype(env.discount_spec().dtype, np.float32))\n\n    timestep = env.reset()\n    self.assertIsNone(timestep.reward)\n    self.assertIsNone(timestep.discount)\n    self.assertTrue(np.issubdtype(timestep.observation.dtype, np.int32))\n\n    timestep = env.step(0)\n    self.assertTrue(np.issubdtype(timestep.reward.dtype, np.float32))\n    self.assertTrue(np.issubdtype(timestep.discount.dtype, np.float32))\n    self.assertTrue(np.issubdtype(timestep.observation.dtype, np.int32))",
  "class FakeNonZeroObservationEnvironment(fakes.ContinuousEnvironment):\n  \"\"\"Fake environment with non-zero observations.\"\"\"\n\n  def _generate_fake_observation(self):\n    original_observation = super()._generate_fake_observation()\n    return tree.map_structure(np.ones_like, original_observation)",
  "class FrameStackingTest(absltest.TestCase):\n\n  def test_specs(self):\n    original_env = FakeNonZeroObservationEnvironment()\n    env = wrappers.FrameStackingWrapper(original_env, 2)\n\n    original_observation_spec = original_env.observation_spec()\n    expected_shape = original_observation_spec.shape + (2,)\n    observation_spec = env.observation_spec()\n    self.assertEqual(expected_shape, observation_spec.shape)\n\n    expected_action_spec = original_env.action_spec()\n    action_spec = env.action_spec()\n    self.assertEqual(expected_action_spec, action_spec)\n\n    expected_reward_spec = original_env.reward_spec()\n    reward_spec = env.reward_spec()\n    self.assertEqual(expected_reward_spec, reward_spec)\n\n    expected_discount_spec = original_env.discount_spec()\n    discount_spec = env.discount_spec()\n    self.assertEqual(expected_discount_spec, discount_spec)\n\n  def test_step(self):\n    original_env = FakeNonZeroObservationEnvironment()\n    env = wrappers.FrameStackingWrapper(original_env, 2)\n    observation_spec = env.observation_spec()\n    action_spec = env.action_spec()\n\n    timestep = env.reset()\n    self.assertEqual(observation_spec.shape, timestep.observation.shape)\n    self.assertTrue(np.all(timestep.observation[..., 0] == 0))\n\n    timestep = env.step(action_spec.generate_value())\n    self.assertEqual(observation_spec.shape, timestep.observation.shape)\n\n  def test_second_reset(self):\n    original_env = FakeNonZeroObservationEnvironment()\n    env = wrappers.FrameStackingWrapper(original_env, 2)\n    action_spec = env.action_spec()\n\n    env.reset()\n    env.step(action_spec.generate_value())\n    timestep = env.reset()\n    self.assertTrue(np.all(timestep.observation[..., 0] == 0))",
  "def _generate_fake_observation(self):\n    original_observation = super()._generate_fake_observation()\n    return tree.map_structure(np.ones_like, original_observation)",
  "def test_specs(self):\n    original_env = FakeNonZeroObservationEnvironment()\n    env = wrappers.FrameStackingWrapper(original_env, 2)\n\n    original_observation_spec = original_env.observation_spec()\n    expected_shape = original_observation_spec.shape + (2,)\n    observation_spec = env.observation_spec()\n    self.assertEqual(expected_shape, observation_spec.shape)\n\n    expected_action_spec = original_env.action_spec()\n    action_spec = env.action_spec()\n    self.assertEqual(expected_action_spec, action_spec)\n\n    expected_reward_spec = original_env.reward_spec()\n    reward_spec = env.reward_spec()\n    self.assertEqual(expected_reward_spec, reward_spec)\n\n    expected_discount_spec = original_env.discount_spec()\n    discount_spec = env.discount_spec()\n    self.assertEqual(expected_discount_spec, discount_spec)",
  "def test_step(self):\n    original_env = FakeNonZeroObservationEnvironment()\n    env = wrappers.FrameStackingWrapper(original_env, 2)\n    observation_spec = env.observation_spec()\n    action_spec = env.action_spec()\n\n    timestep = env.reset()\n    self.assertEqual(observation_spec.shape, timestep.observation.shape)\n    self.assertTrue(np.all(timestep.observation[..., 0] == 0))\n\n    timestep = env.step(action_spec.generate_value())\n    self.assertEqual(observation_spec.shape, timestep.observation.shape)",
  "def test_second_reset(self):\n    original_env = FakeNonZeroObservationEnvironment()\n    env = wrappers.FrameStackingWrapper(original_env, 2)\n    action_spec = env.action_spec()\n\n    env.reset()\n    env.step(action_spec.generate_value())\n    timestep = env.reset()\n    self.assertTrue(np.all(timestep.observation[..., 0] == 0))",
  "class AtariWrapperDopamine(atari_wrapper.BaseAtariWrapper):\n  \"\"\"Atari wrapper that matches exactly Dopamine's prepocessing.\n\n  Warning: using this wrapper requires that you have opencv and its dependencies\n  installed. In general, opencv is not required for Acme.\n  \"\"\"\n\n  def _preprocess_pixels(self, timestep_stack: List[dm_env.TimeStep]):\n    \"\"\"Preprocess Atari frames.\"\"\"\n\n    # 1. RBG to grayscale\n    def rgb_to_grayscale(obs):\n      if self._grayscaling:\n        return np.tensordot(obs, [0.2989, 0.5870, 0.1140], (-1, 0))\n      return obs\n\n    # 2. Max pooling\n    processed_pixels = np.max(\n        np.stack([\n            rgb_to_grayscale(s.observation[atari_wrapper.RGB_INDEX])\n            for s in timestep_stack[-self._pooled_frames:]\n        ]),\n        axis=0)\n\n    # 3. Resize\n    processed_pixels = np.round(processed_pixels).astype(np.uint8)\n    if self._scale_dims != processed_pixels.shape[:2]:\n      processed_pixels = cv2.resize(\n          processed_pixels, (self._width, self._height),\n          interpolation=cv2.INTER_AREA)\n\n      processed_pixels = np.round(processed_pixels).astype(np.uint8)\n\n    return processed_pixels",
  "def _preprocess_pixels(self, timestep_stack: List[dm_env.TimeStep]):\n    \"\"\"Preprocess Atari frames.\"\"\"\n\n    # 1. RBG to grayscale\n    def rgb_to_grayscale(obs):\n      if self._grayscaling:\n        return np.tensordot(obs, [0.2989, 0.5870, 0.1140], (-1, 0))\n      return obs\n\n    # 2. Max pooling\n    processed_pixels = np.max(\n        np.stack([\n            rgb_to_grayscale(s.observation[atari_wrapper.RGB_INDEX])\n            for s in timestep_stack[-self._pooled_frames:]\n        ]),\n        axis=0)\n\n    # 3. Resize\n    processed_pixels = np.round(processed_pixels).astype(np.uint8)\n    if self._scale_dims != processed_pixels.shape[:2]:\n      processed_pixels = cv2.resize(\n          processed_pixels, (self._width, self._height),\n          interpolation=cv2.INTER_AREA)\n\n      processed_pixels = np.round(processed_pixels).astype(np.uint8)\n\n    return processed_pixels",
  "def rgb_to_grayscale(obs):\n      if self._grayscaling:\n        return np.tensordot(obs, [0.2989, 0.5870, 0.1140], (-1, 0))\n      return obs",
  "def _concat(values: types.NestedArray) -> np.ndarray:\n  \"\"\"Concatenates the leaves of `values` along the leading dimension.\n\n  Treats scalars as 1d arrays and expects that the shapes of all leaves are\n  the same except for the leading dimension.\n\n  Args:\n    values: the nested arrays to concatenate.\n\n  Returns:\n    The concatenated array.\n  \"\"\"\n  leaves = list(map(np.atleast_1d, tree.flatten(values)))\n  return np.concatenate(leaves)",
  "def _zeros_like(nest, dtype=None):\n  \"\"\"Generate a nested NumPy array according to spec.\"\"\"\n  return tree.map_structure(lambda x: np.zeros(x.shape, dtype or x.dtype), nest)",
  "class ConcatObservationWrapper(base.EnvironmentWrapper):\n  \"\"\"Wrapper that concatenates observation fields.\n\n  It takes an environment with nested observations and concatenates the fields\n  in a single tensor. The original fields should be 1-dimensional.\n  Observation fields that are not in name_filter are dropped.\n\n  **NOTE**: The fields in the flattened observations will be in sorted order by\n  their names, see tree.flatten for more information.\n  \"\"\"\n\n  def __init__(self,\n               environment: dm_env.Environment,\n               name_filter: Optional[Sequence[str]] = None):\n    \"\"\"Initializes a new ConcatObservationWrapper.\n\n    Args:\n      environment: Environment to wrap.\n      name_filter: Sequence of observation names to keep. None keeps them all.\n    \"\"\"\n    super().__init__(environment)\n    observation_spec = environment.observation_spec()\n    if name_filter is None:\n      name_filter = list(observation_spec.keys())\n    self._obs_names = [x for x in name_filter if x in observation_spec.keys()]\n\n    dummy_obs = _zeros_like(observation_spec)\n    dummy_obs = self._convert_observation(dummy_obs)\n    self._observation_spec = dm_env.specs.BoundedArray(\n        shape=dummy_obs.shape,\n        dtype=dummy_obs.dtype,\n        minimum=-np.inf,\n        maximum=np.inf,\n        name='state')\n\n  def _convert_observation(self, observation):\n    obs = {k: observation[k] for k in self._obs_names}\n    return _concat(obs)\n\n  def step(self, action) -> dm_env.TimeStep:\n    timestep = self._environment.step(action)\n    return timestep._replace(\n        observation=self._convert_observation(timestep.observation))\n\n  def reset(self) -> dm_env.TimeStep:\n    timestep = self._environment.reset()\n    return timestep._replace(\n        observation=self._convert_observation(timestep.observation))\n\n  def observation_spec(self) -> types.NestedSpec:\n    return self._observation_spec",
  "def __init__(self,\n               environment: dm_env.Environment,\n               name_filter: Optional[Sequence[str]] = None):\n    \"\"\"Initializes a new ConcatObservationWrapper.\n\n    Args:\n      environment: Environment to wrap.\n      name_filter: Sequence of observation names to keep. None keeps them all.\n    \"\"\"\n    super().__init__(environment)\n    observation_spec = environment.observation_spec()\n    if name_filter is None:\n      name_filter = list(observation_spec.keys())\n    self._obs_names = [x for x in name_filter if x in observation_spec.keys()]\n\n    dummy_obs = _zeros_like(observation_spec)\n    dummy_obs = self._convert_observation(dummy_obs)\n    self._observation_spec = dm_env.specs.BoundedArray(\n        shape=dummy_obs.shape,\n        dtype=dummy_obs.dtype,\n        minimum=-np.inf,\n        maximum=np.inf,\n        name='state')",
  "def _convert_observation(self, observation):\n    obs = {k: observation[k] for k in self._obs_names}\n    return _concat(obs)",
  "def step(self, action) -> dm_env.TimeStep:\n    timestep = self._environment.step(action)\n    return timestep._replace(\n        observation=self._convert_observation(timestep.observation))",
  "def reset(self) -> dm_env.TimeStep:\n    timestep = self._environment.reset()\n    return timestep._replace(\n        observation=self._convert_observation(timestep.observation))",
  "def observation_spec(self) -> types.NestedSpec:\n    return self._observation_spec",
  "class StepLimitWrapper(base.EnvironmentWrapper):\n  \"\"\"A wrapper which truncates episodes at the specified step limit.\"\"\"\n\n  def __init__(self, environment: dm_env.Environment,\n               step_limit: Optional[int] = None):\n    super().__init__(environment)\n    self._step_limit = step_limit\n    self._elapsed_steps = 0\n\n  def reset(self) -> dm_env.TimeStep:\n    self._elapsed_steps = 0\n    return self._environment.reset()\n\n  def step(self, action: types.NestedArray) -> dm_env.TimeStep:\n    if self._elapsed_steps == -1:\n      # The previous episode was truncated by the wrapper, so start a new one.\n      timestep = self._environment.reset()\n    else:\n      timestep = self._environment.step(action)\n    # If this is the first timestep, then this `step()` call was done on a new,\n    # terminated or truncated environment instance without calling `reset()`\n    # first. In this case this `step()` call should be treated as `reset()`,\n    # so should not increment step count.\n    if timestep.first():\n      self._elapsed_steps = 0\n      return timestep\n    self._elapsed_steps += 1\n    if self._step_limit is not None and self._elapsed_steps >= self._step_limit:\n      self._elapsed_steps = -1\n      return dm_env.truncation(\n          timestep.reward, timestep.observation, timestep.discount)\n    return timestep",
  "def __init__(self, environment: dm_env.Environment,\n               step_limit: Optional[int] = None):\n    super().__init__(environment)\n    self._step_limit = step_limit\n    self._elapsed_steps = 0",
  "def reset(self) -> dm_env.TimeStep:\n    self._elapsed_steps = 0\n    return self._environment.reset()",
  "def step(self, action: types.NestedArray) -> dm_env.TimeStep:\n    if self._elapsed_steps == -1:\n      # The previous episode was truncated by the wrapper, so start a new one.\n      timestep = self._environment.reset()\n    else:\n      timestep = self._environment.step(action)\n    # If this is the first timestep, then this `step()` call was done on a new,\n    # terminated or truncated environment instance without calling `reset()`\n    # first. In this case this `step()` call should be treated as `reset()`,\n    # so should not increment step count.\n    if timestep.first():\n      self._elapsed_steps = 0\n      return timestep\n    self._elapsed_steps += 1\n    if self._step_limit is not None and self._elapsed_steps >= self._step_limit:\n      self._elapsed_steps = -1\n      return dm_env.truncation(\n          timestep.reward, timestep.observation, timestep.discount)\n    return timestep",
  "class CanonicalSpecWrapper(base.EnvironmentWrapper):\n  \"\"\"Wrapper which converts environments to use canonical action specs.\n\n  This only affects action specs of type `specs.BoundedArray`.\n\n  For bounded action specs, we refer to a canonical action spec as the bounding\n  box [-1, 1]^d where d is the dimensionality of the spec. So the shape and\n  dtype of the spec is unchanged, while the maximum/minimum values are set\n  to +/- 1.\n  \"\"\"\n\n  def __init__(self, environment: dm_env.Environment, clip: bool = False):\n    super().__init__(environment)\n    self._action_spec = environment.action_spec()\n    self._clip = clip\n\n  def step(self, action: types.NestedArray) -> dm_env.TimeStep:\n    scaled_action = _scale_nested_action(action, self._action_spec, self._clip)\n    return self._environment.step(scaled_action)\n\n  def action_spec(self):\n    return _convert_spec(self._environment.action_spec())",
  "def _convert_spec(nested_spec: types.NestedSpec) -> types.NestedSpec:\n  \"\"\"Converts all bounded specs in nested spec to the canonical scale.\"\"\"\n\n  def _convert_single_spec(spec: specs.Array) -> specs.Array:\n    \"\"\"Converts a single spec to canonical if bounded.\"\"\"\n    if isinstance(spec, specs.BoundedArray):\n      return spec.replace(\n          minimum=-np.ones(spec.shape), maximum=np.ones(spec.shape))\n    else:\n      return spec\n\n  return tree.map_structure(_convert_single_spec, nested_spec)",
  "def _scale_nested_action(\n    nested_action: types.NestedArray,\n    nested_spec: types.NestedSpec,\n    clip: bool,\n) -> types.NestedArray:\n  \"\"\"Converts a canonical nested action back to the given nested action spec.\"\"\"\n\n  def _scale_action(action: np.ndarray, spec: specs.Array):\n    \"\"\"Converts a single canonical action back to the given action spec.\"\"\"\n    if isinstance(spec, specs.BoundedArray):\n      # Get scale and offset of output action spec.\n      scale = spec.maximum - spec.minimum\n      offset = spec.minimum\n\n      # Maybe clip the action.\n      if clip:\n        action = np.clip(action, -1.0, 1.0)\n\n      # Map action to [0, 1].\n      action = 0.5 * (action + 1.0)\n\n      # Map action to [spec.minimum, spec.maximum].\n      action *= scale\n      action += offset\n\n    return action\n\n  return tree.map_structure(_scale_action, nested_action, nested_spec)",
  "def __init__(self, environment: dm_env.Environment, clip: bool = False):\n    super().__init__(environment)\n    self._action_spec = environment.action_spec()\n    self._clip = clip",
  "def step(self, action: types.NestedArray) -> dm_env.TimeStep:\n    scaled_action = _scale_nested_action(action, self._action_spec, self._clip)\n    return self._environment.step(scaled_action)",
  "def action_spec(self):\n    return _convert_spec(self._environment.action_spec())",
  "def _convert_single_spec(spec: specs.Array) -> specs.Array:\n    \"\"\"Converts a single spec to canonical if bounded.\"\"\"\n    if isinstance(spec, specs.BoundedArray):\n      return spec.replace(\n          minimum=-np.ones(spec.shape), maximum=np.ones(spec.shape))\n    else:\n      return spec",
  "def _scale_action(action: np.ndarray, spec: specs.Array):\n    \"\"\"Converts a single canonical action back to the given action spec.\"\"\"\n    if isinstance(spec, specs.BoundedArray):\n      # Get scale and offset of output action spec.\n      scale = spec.maximum - spec.minimum\n      offset = spec.minimum\n\n      # Maybe clip the action.\n      if clip:\n        action = np.clip(action, -1.0, 1.0)\n\n      # Map action to [0, 1].\n      action = 0.5 * (action + 1.0)\n\n      # Map action to [spec.minimum, spec.maximum].\n      action *= scale\n      action += offset\n\n    return action",
  "class ExpandScalarObservationShapesWrapper(base.EnvironmentWrapper):\n  \"\"\"Expands scalar shapes in the observation.\n\n  For example, if the observation holds the previous (scalar) action, this\n  wrapper makes sure the environment returns a previous action with shape [1].\n\n  This can be necessary when stacking observations with previous actions.\n  \"\"\"\n\n  def step(self, action: Any) -> dm_env.TimeStep:\n    timestep = self._environment.step(action)\n    expanded_observation = tree.map_structure(_expand_scalar_array_shape,\n                                              timestep.observation)\n    return timestep._replace(observation=expanded_observation)\n\n  def reset(self) -> dm_env.TimeStep:\n    timestep = self._environment.reset()\n    expanded_observation = tree.map_structure(_expand_scalar_array_shape,\n                                              timestep.observation)\n    return timestep._replace(observation=expanded_observation)\n\n  def observation_spec(self) -> specs.Array:\n    return tree.map_structure(_expand_scalar_spec_shape,\n                              self._environment.observation_spec())",
  "def _expand_scalar_spec_shape(spec: specs.Array) -> specs.Array:\n  if not spec.shape:\n    # NOTE: This line upcasts the spec to an Array to avoid edge cases (as in\n    # DiscreteSpec) where we cannot set the spec's shape.\n    spec = specs.Array(shape=(1,), dtype=spec.dtype, name=spec.name)\n  return spec",
  "def _expand_scalar_array_shape(array: np.ndarray) -> np.ndarray:\n  return array if array.shape else np.expand_dims(array, axis=-1)",
  "def step(self, action: Any) -> dm_env.TimeStep:\n    timestep = self._environment.step(action)\n    expanded_observation = tree.map_structure(_expand_scalar_array_shape,\n                                              timestep.observation)\n    return timestep._replace(observation=expanded_observation)",
  "def reset(self) -> dm_env.TimeStep:\n    timestep = self._environment.reset()\n    expanded_observation = tree.map_structure(_expand_scalar_array_shape,\n                                              timestep.observation)\n    return timestep._replace(observation=expanded_observation)",
  "def observation_spec(self) -> specs.Array:\n    return tree.map_structure(_expand_scalar_spec_shape,\n                              self._environment.observation_spec())",
  "class DelayedRewardWrapper(base.EnvironmentWrapper):\n  \"\"\"Implements delayed reward on environments.\n\n  This wrapper sparsifies any environment by adding a reward delay. Instead of\n  returning a reward at each step, the wrapped environment returns the\n  accumulated reward every N steps or at the end of an episode, whichever comes\n  first. This does not change the optimal expected return, but typically makes\n  the environment harder by adding exploration and longer term dependencies.\n  \"\"\"\n\n  def __init__(self,\n               environment: dm_env.Environment,\n               accumulation_period: Optional[int] = 1):\n    \"\"\"Initializes a `DelayedRewardWrapper`.\n\n    Args:\n      environment: An environment conforming to the dm_env.Environment\n        interface.\n     accumulation_period: number of steps to accumulate the reward over. If\n       `accumulation_period` is an integer, reward is accumulated and returned\n       every `accumulation_period` steps, and at the end of an episode. If\n       `accumulation_period` is None, reward is only returned at the end of an\n       episode. If `accumulation_period`=1, this wrapper is a no-op.\n    \"\"\"\n\n    super().__init__(environment)\n    if accumulation_period is not None and accumulation_period < 1:\n      raise ValueError(\n          f'Accumuluation period is {accumulation_period} but should be greater than 1.'\n      )\n    self._accumuation_period = accumulation_period\n    self._delayed_reward = self._zero_reward\n    self._accumulation_counter = 0\n\n  @property\n  def _zero_reward(self):\n    return tree.map_structure(lambda s: np.zeros(s.shape, s.dtype),\n                              self._environment.reward_spec())\n\n  def reset(self) -> dm_env.TimeStep:\n    \"\"\"Resets environment and provides the first timestep.\"\"\"\n    timestep = self.environment.reset()\n    self._delayed_reward = self._zero_reward\n    self._accumulation_counter = 0\n    return timestep\n\n  def step(self, action: types.NestedArray) -> dm_env.TimeStep:\n    \"\"\"Performs one step and maybe returns a reward.\"\"\"\n    timestep = self.environment.step(action)\n    self._delayed_reward = tree.map_structure(operator.iadd,\n                                              self._delayed_reward,\n                                              timestep.reward)\n    self._accumulation_counter += 1\n\n    if (self._accumuation_period is not None and self._accumulation_counter\n        == self._accumuation_period) or timestep.last():\n      timestep = timestep._replace(reward=self._delayed_reward)\n      self._accumulation_counter = 0\n      self._delayed_reward = self._zero_reward\n    else:\n      timestep = timestep._replace(reward=self._zero_reward)\n\n    return timestep",
  "def __init__(self,\n               environment: dm_env.Environment,\n               accumulation_period: Optional[int] = 1):\n    \"\"\"Initializes a `DelayedRewardWrapper`.\n\n    Args:\n      environment: An environment conforming to the dm_env.Environment\n        interface.\n     accumulation_period: number of steps to accumulate the reward over. If\n       `accumulation_period` is an integer, reward is accumulated and returned\n       every `accumulation_period` steps, and at the end of an episode. If\n       `accumulation_period` is None, reward is only returned at the end of an\n       episode. If `accumulation_period`=1, this wrapper is a no-op.\n    \"\"\"\n\n    super().__init__(environment)\n    if accumulation_period is not None and accumulation_period < 1:\n      raise ValueError(\n          f'Accumuluation period is {accumulation_period} but should be greater than 1.'\n      )\n    self._accumuation_period = accumulation_period\n    self._delayed_reward = self._zero_reward\n    self._accumulation_counter = 0",
  "def _zero_reward(self):\n    return tree.map_structure(lambda s: np.zeros(s.shape, s.dtype),\n                              self._environment.reward_spec())",
  "def reset(self) -> dm_env.TimeStep:\n    \"\"\"Resets environment and provides the first timestep.\"\"\"\n    timestep = self.environment.reset()\n    self._delayed_reward = self._zero_reward\n    self._accumulation_counter = 0\n    return timestep",
  "def step(self, action: types.NestedArray) -> dm_env.TimeStep:\n    \"\"\"Performs one step and maybe returns a reward.\"\"\"\n    timestep = self.environment.step(action)\n    self._delayed_reward = tree.map_structure(operator.iadd,\n                                              self._delayed_reward,\n                                              timestep.reward)\n    self._accumulation_counter += 1\n\n    if (self._accumuation_period is not None and self._accumulation_counter\n        == self._accumuation_period) or timestep.last():\n      timestep = timestep._replace(reward=self._delayed_reward)\n      self._accumulation_counter = 0\n      self._delayed_reward = self._zero_reward\n    else:\n      timestep = timestep._replace(reward=self._zero_reward)\n\n    return timestep",
  "def make_animation(\n    frames: Sequence[np.ndarray], frame_rate: float,\n    figsize: Optional[Union[float, Tuple[int, int]]]) -> anim.Animation:\n  \"\"\"Generates a matplotlib animation from a stack of frames.\"\"\"\n\n  # Set animation characteristics.\n  if figsize is None:\n    height, width, _ = frames[0].shape\n  elif isinstance(figsize, tuple):\n    height, width = figsize\n  else:\n    diagonal = figsize\n    height, width, _ = frames[0].shape\n    scale_factor = diagonal / np.sqrt(height**2 + width**2)\n    width *= scale_factor\n    height *= scale_factor\n\n  dpi = 70\n  interval = int(round(1e3 / frame_rate))  # Time (in ms) between frames.\n\n  # Create and configure the figure.\n  fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n  ax.set_axis_off()\n  ax.set_aspect('equal')\n  ax.set_position([0, 0, 1, 1])\n\n  # Initialize the first frame.\n  im = ax.imshow(frames[0])\n\n  # Create the function that will modify the frame, creating an animation.\n  def update(frame):\n    im.set_data(frame)\n    return [im]\n\n  return anim.FuncAnimation(\n      fig=fig,\n      func=update,\n      frames=frames,\n      interval=interval,\n      blit=True,\n      repeat=False)",
  "class VideoWrapper(base.EnvironmentWrapper):\n  \"\"\"Wrapper which creates and records videos from generated observations.\n\n  This will limit itself to recording once every `record_every` episodes and\n  videos will be recorded to the directory `path` + '/<unique id>/videos' where\n  `path` defaults to '~/acme'. Users can specify the size of the screen by\n  passing either a tuple giving height and width or a float giving the size\n  of the diagonal.\n  \"\"\"\n\n  def __init__(\n      self,\n      environment: dm_env.Environment,\n      *,\n      path: str = '~/acme',\n      filename: str = '',\n      process_path: Callable[[str, str], str] = paths.process_path,\n      record_every: int = 100,\n      frame_rate: int = 30,\n      figsize: Optional[Union[float, Tuple[int, int]]] = None,\n      to_html: bool = True,\n  ):\n    super(VideoWrapper, self).__init__(environment)\n    self._path = process_path(path, 'videos')\n    self._filename = filename\n    self._record_every = record_every\n    self._frame_rate = frame_rate\n    self._frames = []\n    self._counter = 0\n    self._figsize = figsize\n    self._to_html = to_html\n\n  def _render_frame(self, observation):\n    \"\"\"Renders a frame from the given environment observation.\"\"\"\n    return observation\n\n  def _write_frames(self):\n    \"\"\"Writes frames to video.\"\"\"\n    if self._counter % self._record_every == 0:\n      animation = make_animation(self._frames, self._frame_rate, self._figsize)\n      path_without_extension = os.path.join(\n          self._path, f'{self._filename}_{self._counter:04d}'\n      )\n      if self._to_html:\n        path = path_without_extension + '.html'\n        video = animation.to_html5_video()\n        with open(path, 'w') as f:\n          f.write(video)\n      else:\n        path = path_without_extension + '.m4v'\n        # Animation.save can save only locally. Save first and copy using\n        # gfile.\n        with tempfile.TemporaryDirectory() as tmp_dir:\n          tmp_path = os.path.join(tmp_dir, 'temp.m4v')\n          animation.save(tmp_path)\n          with open(path, 'wb') as f:\n            with open(tmp_path, 'rb') as g:\n              f.write(g.read())\n\n    # Clear the frame buffer whether a video was generated or not.\n    self._frames = []\n\n  def _append_frame(self, observation):\n    \"\"\"Appends a frame to the sequence of frames.\"\"\"\n    if self._counter % self._record_every == 0:\n      self._frames.append(self._render_frame(observation))\n\n  def step(self, action) -> dm_env.TimeStep:\n    timestep = self.environment.step(action)\n    self._append_frame(timestep.observation)\n    return timestep\n\n  def reset(self) -> dm_env.TimeStep:\n    # If the frame buffer is nonempty, flush it and record video\n    if self._frames:\n      self._write_frames()\n    self._counter += 1\n    timestep = self.environment.reset()\n    self._append_frame(timestep.observation)\n    return timestep\n\n  def make_html_animation(self):\n    if self._frames:\n      return make_animation(self._frames, self._frame_rate,\n                            self._figsize).to_html5_video()\n    else:\n      raise ValueError('make_html_animation should be called after running a '\n                       'trajectory and before calling reset().')\n\n  def close(self):\n    if self._frames:\n      self._write_frames()\n      self._frames = []\n    self.environment.close()",
  "class MujocoVideoWrapper(VideoWrapper):\n  \"\"\"VideoWrapper which generates videos from a mujoco physics object.\n\n  This passes its keyword arguments into the parent `VideoWrapper` class (refer\n  here for any default arguments).\n  \"\"\"\n\n  # Note that since we can be given a wrapped mujoco environment we can't give\n  # the type as dm_control.Environment.\n\n  def __init__(self,\n               environment: dm_env.Environment,\n               *,\n               frame_rate: Optional[int] = None,\n               camera_id: Optional[int] = 0,\n               height: int = 240,\n               width: int = 320,\n               playback_speed: float = 1.,\n               **kwargs):\n\n    # Check that we have a mujoco environment (or a wrapper thereof).\n    if not hasattr(environment, 'physics'):\n      raise ValueError('MujocoVideoWrapper expects an environment which '\n                       'exposes a physics attribute corresponding to a MuJoCo '\n                       'physics engine')\n\n    # Compute frame rate if not set.\n    if frame_rate is None:\n      try:\n        control_timestep = getattr(environment, 'control_timestep')()\n      except AttributeError as e:\n        raise AttributeError('MujocoVideoWrapper expects an environment which '\n                             'exposes a control_timestep method, like '\n                             'dm_control environments, or frame_rate '\n                             'to be specified.') from e\n      frame_rate = int(round(playback_speed / control_timestep))\n\n    super().__init__(environment, frame_rate=frame_rate, **kwargs)\n    self._camera_id = camera_id\n    self._height = height\n    self._width = width\n\n  def _render_frame(self, unused_observation):\n    del unused_observation\n\n    # We've checked above that this attribute should exist. Pytype won't like\n    # it if we just try and do self.environment.physics, so we use the slightly\n    # grosser version below.\n    physics = getattr(self.environment, 'physics')\n\n    if self._camera_id is not None:\n      frame = physics.render(\n          camera_id=self._camera_id, height=self._height, width=self._width)\n    else:\n      # If camera_id is None, we create a minimal canvas that will accommodate\n      # physics.model.ncam frames, and render all of them on a grid.\n      num_cameras = physics.model.ncam\n      num_columns = int(np.ceil(np.sqrt(num_cameras)))\n      num_rows = int(np.ceil(float(num_cameras)/num_columns))\n      height = self._height\n      width = self._width\n\n      # Make a black canvas.\n      frame = np.zeros((num_rows*height, num_columns*width, 3), dtype=np.uint8)\n\n      for col in range(num_columns):\n        for row in range(num_rows):\n\n          camera_id = row*num_columns + col\n\n          if camera_id >= num_cameras:\n            break\n\n          subframe = physics.render(\n              camera_id=camera_id, height=height, width=width)\n\n          # Place the frame in the appropriate rectangle on the pixel canvas.\n          frame[row*height:(row+1)*height, col*width:(col+1)*width] = subframe\n\n    return frame",
  "def update(frame):\n    im.set_data(frame)\n    return [im]",
  "def __init__(\n      self,\n      environment: dm_env.Environment,\n      *,\n      path: str = '~/acme',\n      filename: str = '',\n      process_path: Callable[[str, str], str] = paths.process_path,\n      record_every: int = 100,\n      frame_rate: int = 30,\n      figsize: Optional[Union[float, Tuple[int, int]]] = None,\n      to_html: bool = True,\n  ):\n    super(VideoWrapper, self).__init__(environment)\n    self._path = process_path(path, 'videos')\n    self._filename = filename\n    self._record_every = record_every\n    self._frame_rate = frame_rate\n    self._frames = []\n    self._counter = 0\n    self._figsize = figsize\n    self._to_html = to_html",
  "def _render_frame(self, observation):\n    \"\"\"Renders a frame from the given environment observation.\"\"\"\n    return observation",
  "def _write_frames(self):\n    \"\"\"Writes frames to video.\"\"\"\n    if self._counter % self._record_every == 0:\n      animation = make_animation(self._frames, self._frame_rate, self._figsize)\n      path_without_extension = os.path.join(\n          self._path, f'{self._filename}_{self._counter:04d}'\n      )\n      if self._to_html:\n        path = path_without_extension + '.html'\n        video = animation.to_html5_video()\n        with open(path, 'w') as f:\n          f.write(video)\n      else:\n        path = path_without_extension + '.m4v'\n        # Animation.save can save only locally. Save first and copy using\n        # gfile.\n        with tempfile.TemporaryDirectory() as tmp_dir:\n          tmp_path = os.path.join(tmp_dir, 'temp.m4v')\n          animation.save(tmp_path)\n          with open(path, 'wb') as f:\n            with open(tmp_path, 'rb') as g:\n              f.write(g.read())\n\n    # Clear the frame buffer whether a video was generated or not.\n    self._frames = []",
  "def _append_frame(self, observation):\n    \"\"\"Appends a frame to the sequence of frames.\"\"\"\n    if self._counter % self._record_every == 0:\n      self._frames.append(self._render_frame(observation))",
  "def step(self, action) -> dm_env.TimeStep:\n    timestep = self.environment.step(action)\n    self._append_frame(timestep.observation)\n    return timestep",
  "def reset(self) -> dm_env.TimeStep:\n    # If the frame buffer is nonempty, flush it and record video\n    if self._frames:\n      self._write_frames()\n    self._counter += 1\n    timestep = self.environment.reset()\n    self._append_frame(timestep.observation)\n    return timestep",
  "def make_html_animation(self):\n    if self._frames:\n      return make_animation(self._frames, self._frame_rate,\n                            self._figsize).to_html5_video()\n    else:\n      raise ValueError('make_html_animation should be called after running a '\n                       'trajectory and before calling reset().')",
  "def close(self):\n    if self._frames:\n      self._write_frames()\n      self._frames = []\n    self.environment.close()",
  "def __init__(self,\n               environment: dm_env.Environment,\n               *,\n               frame_rate: Optional[int] = None,\n               camera_id: Optional[int] = 0,\n               height: int = 240,\n               width: int = 320,\n               playback_speed: float = 1.,\n               **kwargs):\n\n    # Check that we have a mujoco environment (or a wrapper thereof).\n    if not hasattr(environment, 'physics'):\n      raise ValueError('MujocoVideoWrapper expects an environment which '\n                       'exposes a physics attribute corresponding to a MuJoCo '\n                       'physics engine')\n\n    # Compute frame rate if not set.\n    if frame_rate is None:\n      try:\n        control_timestep = getattr(environment, 'control_timestep')()\n      except AttributeError as e:\n        raise AttributeError('MujocoVideoWrapper expects an environment which '\n                             'exposes a control_timestep method, like '\n                             'dm_control environments, or frame_rate '\n                             'to be specified.') from e\n      frame_rate = int(round(playback_speed / control_timestep))\n\n    super().__init__(environment, frame_rate=frame_rate, **kwargs)\n    self._camera_id = camera_id\n    self._height = height\n    self._width = width",
  "def _render_frame(self, unused_observation):\n    del unused_observation\n\n    # We've checked above that this attribute should exist. Pytype won't like\n    # it if we just try and do self.environment.physics, so we use the slightly\n    # grosser version below.\n    physics = getattr(self.environment, 'physics')\n\n    if self._camera_id is not None:\n      frame = physics.render(\n          camera_id=self._camera_id, height=self._height, width=self._width)\n    else:\n      # If camera_id is None, we create a minimal canvas that will accommodate\n      # physics.model.ncam frames, and render all of them on a grid.\n      num_cameras = physics.model.ncam\n      num_columns = int(np.ceil(np.sqrt(num_cameras)))\n      num_rows = int(np.ceil(float(num_cameras)/num_columns))\n      height = self._height\n      width = self._width\n\n      # Make a black canvas.\n      frame = np.zeros((num_rows*height, num_columns*width, 3), dtype=np.uint8)\n\n      for col in range(num_columns):\n        for row in range(num_rows):\n\n          camera_id = row*num_columns + col\n\n          if camera_id >= num_cameras:\n            break\n\n          subframe = physics.render(\n              camera_id=camera_id, height=height, width=width)\n\n          # Place the frame in the appropriate rectangle on the pixel canvas.\n          frame[row*height:(row+1)*height, col*width:(col+1)*width] = subframe\n\n    return frame",
  "class NumpyIteratorTest(absltest.TestCase):\n\n  def testBasic(self):\n    ds = tf.data.Dataset.range(3)\n    self.assertEqual([0, 1, 2], list(numpy_iterator.NumpyIterator(ds)))\n\n  def testNestedStructure(self):\n    point = collections.namedtuple('Point', ['x', 'y'])\n    ds = tf.data.Dataset.from_tensor_slices({\n        'a': ([1, 2], [3, 4]),\n        'b': [5, 6],\n        'c': point([7, 8], [9, 10])\n    })\n    self.assertEqual([{\n        'a': (1, 3),\n        'b': 5,\n        'c': point(7, 9)\n    }, {\n        'a': (2, 4),\n        'b': 6,\n        'c': point(8, 10)\n    }], list(numpy_iterator.NumpyIterator(ds)))",
  "def testBasic(self):\n    ds = tf.data.Dataset.range(3)\n    self.assertEqual([0, 1, 2], list(numpy_iterator.NumpyIterator(ds)))",
  "def testNestedStructure(self):\n    point = collections.namedtuple('Point', ['x', 'y'])\n    ds = tf.data.Dataset.from_tensor_slices({\n        'a': ([1, 2], [3, 4]),\n        'b': [5, 6],\n        'c': point([7, 8], [9, 10])\n    })\n    self.assertEqual([{\n        'a': (1, 3),\n        'b': 5,\n        'c': point(7, 9)\n    }, {\n        'a': (2, 4),\n        'b': 6,\n        'c': point(8, 10)\n    }], list(numpy_iterator.NumpyIterator(ds)))",
  "class CropType(enum.Enum):\n  \"\"\"Types of cropping supported by the image aumentation transforms.\n\n  BILINEAR: Continuously randomly located then bilinearly interpolated.\n  ALIGNED: Aligned with input image's pixel grid.\n  \"\"\"\n  BILINEAR = 'bilinear'\n  ALIGNED = 'aligned'",
  "def pad_and_crop(img: tf.Tensor,\n                 pad_size: int = 4,\n                 method: CropType = CropType.ALIGNED) -> tf.Tensor:\n  \"\"\"Pad and crop image to mimic a random translation with mirroring at edges.\n\n  This implements the image augmentation from section 3.1 in (Kostrikov et al.)\n  https://arxiv.org/abs/2004.13649.\n\n  Args:\n    img: The image to pad and crop. Its dimensions are [..., H, W, C] where ...\n      are batch dimensions (if it has any).\n    pad_size: The amount of padding to apply to the image before cropping it.\n    method: The method to use for cropping the image, see `CropType` for\n      details.\n\n  Returns:\n    The image after having been padded and cropped.\n  \"\"\"\n  num_batch_dims = img.shape[:-3].rank\n\n  if img.shape.is_fully_defined():\n    img_shape = img.shape.as_list()\n  else:\n    img_shape = tf.shape(img)\n\n  # Set paddings for height and width only, batches and channels set to [0, 0].\n  paddings = [[0, 0]] * num_batch_dims  # Do not pad batch dims.\n  paddings.extend([[pad_size, pad_size], [pad_size, pad_size], [0, 0]])\n\n  # Pad using symmetric padding.\n  padded_img = tf.pad(img, paddings=paddings, mode='SYMMETRIC')\n\n  # Crop padded image using requested method.\n  if method == CropType.ALIGNED:\n    cropped_img = tf.image.random_crop(padded_img, img_shape)\n  elif method == CropType.BILINEAR:\n    height, width = img_shape[-3:-1]\n    padded_height, padded_width = height + 2 * pad_size, width + 2 * pad_size\n\n    # Pick a top-left point uniformly at random.\n    top_left = tf.random.uniform(\n        shape=(2,), maxval=2 * pad_size + 1, dtype=tf.int32)\n\n    # This single box is applied to the entire batch if a batch is passed.\n    batch_size = tf.shape(padded_img)[0]\n    box = tf.cast(\n        tf.tile(\n            tf.expand_dims([\n                top_left[0] / padded_height,\n                top_left[1] / padded_width,\n                (top_left[0] + height) / padded_height,\n                (top_left[1] + width) / padded_width,\n            ], axis=0), [batch_size, 1]),\n        tf.float32)  # Shape [batch_size, 2].\n\n    # Crop and resize according to `box` then reshape back to input shape.\n    cropped_img = tf.image.crop_and_resize(\n        padded_img,\n        box,\n        tf.range(batch_size),\n        (height, width),\n        method='bilinear')\n    cropped_img = tf.reshape(cropped_img, img_shape)\n\n  return cropped_img",
  "def make_transform(\n    observation_transform: types.TensorTransformation,\n    transform_next_observation: bool = True,\n) -> reverb_dataset.Transform:\n  \"\"\"Creates the appropriate dataset transform for the given signature.\"\"\"\n\n  if transform_next_observation:\n    def transform(x: reverb.ReplaySample) -> reverb.ReplaySample:\n      return x._replace(\n          data=x.data._replace(\n              observation=observation_transform(x.data.observation),\n              next_observation=observation_transform(x.data.next_observation)))\n  else:\n    def transform(x: reverb.ReplaySample) -> reverb.ReplaySample:\n      return x._replace(\n          data=x.data._replace(\n              observation=observation_transform(x.data.observation)))\n\n  return transform",
  "def transform(x: reverb.ReplaySample) -> reverb.ReplaySample:\n      return x._replace(\n          data=x.data._replace(\n              observation=observation_transform(x.data.observation),\n              next_observation=observation_transform(x.data.next_observation)))",
  "def transform(x: reverb.ReplaySample) -> reverb.ReplaySample:\n      return x._replace(\n          data=x.data._replace(\n              observation=observation_transform(x.data.observation)))",
  "def make_reverb_dataset(\n    server_address: str,\n    batch_size: Optional[int] = None,\n    prefetch_size: Optional[int] = None,\n    table: Union[str, Mapping[str, float]] = adders.DEFAULT_PRIORITY_TABLE,\n    num_parallel_calls: Optional[int] = 12,\n    max_in_flight_samples_per_worker: Optional[int] = None,\n    postprocess: Optional[Transform] = None,\n    # Deprecated kwargs.\n    environment_spec: Optional[specs.EnvironmentSpec] = None,\n    extra_spec: Optional[types.NestedSpec] = None,\n    transition_adder: bool = False,\n    convert_zero_size_to_none: bool = False,\n    using_deprecated_adder: bool = False,\n    sequence_length: Optional[int] = None,\n) -> tf.data.Dataset:\n  \"\"\"Make a TensorFlow dataset backed by a Reverb trajectory replay service.\n\n  Arguments:\n    server_address: Address of the Reverb server.\n    batch_size: Batch size of the returned dataset.\n    prefetch_size: The number of elements to prefetch from the original dataset.\n      Note that Reverb may do some internal prefetching in addition to this.\n    table: The name of the Reverb table to use, or a mapping of (table_name,\n      float_weight) for mixing multiple tables in the input (e.g. mixing online\n      and offline experiences).\n    num_parallel_calls: The parralelism to use. Setting it to `tf.data.AUTOTUNE`\n      will allow `tf.data` to automatically find a reasonable value.\n    max_in_flight_samples_per_worker: see reverb.TrajectoryDataset for details.\n    postprocess: User-specified transformation to be applied to the dataset (as\n      `ds.map(postprocess)`).\n    environment_spec: DEPRECATED! Do not use.\n    extra_spec: DEPRECATED! Do not use.\n    transition_adder: DEPRECATED! Do not use.\n    convert_zero_size_to_none: DEPRECATED! Do not use.\n    using_deprecated_adder: DEPRECATED! Do not use.\n    sequence_length: DEPRECATED! Do not use.\n\n  Returns:\n    A `tf.data.Dataset` iterating over the contents of the Reverb table.\n\n  Raises:\n    ValueError if `environment_spec` or `extra_spec` are set, or `table` is a\n    mapping with no positive weight values.\n  \"\"\"\n\n  if environment_spec or extra_spec:\n    raise ValueError(\n        'The make_reverb_dataset factory function no longer requires specs as'\n        ' as they should be passed as a signature to the reverb.Table when it'\n        ' is created. Consider either updating your code or falling back to the'\n        ' deprecated dataset factory in acme/datasets/deprecated.')\n\n  # These are no longer used and are only kept in the call signature for\n  # backward compatibility.\n  del environment_spec\n  del extra_spec\n  del transition_adder\n  del convert_zero_size_to_none\n  del using_deprecated_adder\n  del sequence_length\n\n  # This is the default that used to be set by reverb.TFClient.dataset().\n  if max_in_flight_samples_per_worker is None and batch_size is None:\n    max_in_flight_samples_per_worker = 100\n  elif max_in_flight_samples_per_worker is None:\n    max_in_flight_samples_per_worker = 2 * batch_size\n\n  # Create mapping from tables to non-zero weights.\n  if isinstance(table, str):\n    tables = collections.OrderedDict([(table, 1.)])\n  else:\n    tables = collections.OrderedDict([\n        (name, weight) for name, weight in table.items() if weight > 0.\n    ])\n    if len(tables) <= 0:\n      raise ValueError(f'No positive weights in input tables {tables}')\n\n  # Normalize weights.\n  total_weight = sum(tables.values())\n  tables = collections.OrderedDict([\n      (name, weight / total_weight) for name, weight in tables.items()\n  ])\n\n  def _make_dataset(unused_idx: tf.Tensor) -> tf.data.Dataset:\n    datasets = ()\n    for table_name, weight in tables.items():\n      max_in_flight_samples = max(\n          1, int(max_in_flight_samples_per_worker * weight))\n      dataset = reverb.TrajectoryDataset.from_table_signature(\n          server_address=server_address,\n          table=table_name,\n          max_in_flight_samples_per_worker=max_in_flight_samples)\n      datasets += (dataset,)\n    if len(datasets) > 1:\n      dataset = tf.data.Dataset.sample_from_datasets(\n          datasets, weights=tables.values())\n    else:\n      dataset = datasets[0]\n\n    # Post-process each element if a post-processing function is passed, e.g.\n    # observation-stacking or data augmenting transformations.\n    if postprocess:\n      dataset = dataset.map(postprocess)\n\n    if batch_size:\n      dataset = dataset.batch(batch_size, drop_remainder=True)\n\n    return dataset\n\n  if num_parallel_calls is not None:\n    # Create a datasets and interleaves it to create `num_parallel_calls`\n    # `TrajectoryDataset`s.\n    num_datasets_to_interleave = (\n        os.cpu_count()\n        if num_parallel_calls == tf.data.AUTOTUNE else num_parallel_calls)\n    dataset = tf.data.Dataset.range(num_datasets_to_interleave).interleave(\n        map_func=_make_dataset,\n        cycle_length=num_parallel_calls,\n        num_parallel_calls=num_parallel_calls,\n        deterministic=False)\n  else:\n    dataset = _make_dataset(tf.constant(0))\n\n  if prefetch_size:\n    dataset = dataset.prefetch(prefetch_size)\n\n  return dataset",
  "def _make_dataset(unused_idx: tf.Tensor) -> tf.data.Dataset:\n    datasets = ()\n    for table_name, weight in tables.items():\n      max_in_flight_samples = max(\n          1, int(max_in_flight_samples_per_worker * weight))\n      dataset = reverb.TrajectoryDataset.from_table_signature(\n          server_address=server_address,\n          table=table_name,\n          max_in_flight_samples_per_worker=max_in_flight_samples)\n      datasets += (dataset,)\n    if len(datasets) > 1:\n      dataset = tf.data.Dataset.sample_from_datasets(\n          datasets, weights=tables.values())\n    else:\n      dataset = datasets[0]\n\n    # Post-process each element if a post-processing function is passed, e.g.\n    # observation-stacking or data augmenting transformations.\n    if postprocess:\n      dataset = dataset.map(postprocess)\n\n    if batch_size:\n      dataset = dataset.batch(batch_size, drop_remainder=True)\n\n    return dataset",
  "def make_replay_tables(environment_spec: specs.EnvironmentSpec\n                      ) -> Sequence[reverb.Table]:\n  \"\"\"Create tables to insert data into.\"\"\"\n  return [\n      reverb.Table(\n          name='default',\n          sampler=reverb.selectors.Uniform(),\n          remover=reverb.selectors.Fifo(),\n          max_size=1000000,\n          rate_limiter=rate_limiters.MinSize(1),\n          signature=adders_reverb.NStepTransitionAdder.signature(\n              environment_spec))\n  ]",
  "def make_adder(replay_client: reverb.Client) -> adders.Adder:\n  return adders_reverb.NStepTransitionAdder(\n      priority_fns={'default': None},\n      client=replay_client,\n      n_step=1,\n      discount=1)",
  "def main(_):\n  environment = fakes.ContinuousEnvironment(action_dim=8,\n                                            observation_dim=87,\n                                            episode_length=10000000)\n  spec = specs.make_environment_spec(environment)\n  replay_tables = make_replay_tables(spec)\n  replay_server = reverb.Server(replay_tables, port=None)\n  replay_client = reverb.Client(f'localhost:{replay_server.port}')\n  adder = make_adder(replay_client)\n\n  timestep = environment.reset()\n  adder.add_first(timestep)\n  # TODO(raveman): Consider also filling the table to say 1M (too slow).\n  for steps in range(10000):\n    if steps % 1000 == 0:\n      logging.info('Processed %s steps', steps)\n    action = np.asarray(np.random.uniform(-1, 1, (8,)), dtype=np.float32)\n    next_timestep = environment.step(action)\n    adder.add(action, next_timestep, extras=())\n\n  for batch_size in [256, 256 * 8, 256 * 64]:\n    for prefetch_size in [0, 1, 4]:\n      print(f'Processing batch_size={batch_size} prefetch_size={prefetch_size}')\n      ds = datasets.make_reverb_dataset(\n          table='default',\n          server_address=replay_client.server_address,\n          batch_size=batch_size,\n          prefetch_size=prefetch_size,\n      )\n      it = ds.as_numpy_iterator()\n\n      for iteration in range(3):\n        t = time.time()\n        for _ in range(1000):\n          _ = next(it)\n        print(f'Iteration {iteration} finished in {time.time() - t}s')",
  "def _batched_step_to_transition(step: rlds.BatchedStep) -> types.Transition:\n  return types.Transition(\n      observation=tf.nest.map_structure(lambda x: x[0], step[rlds.OBSERVATION]),\n      action=tf.nest.map_structure(lambda x: x[0], step[rlds.ACTION]),\n      reward=tf.nest.map_structure(lambda x: x[0], step[rlds.REWARD]),\n      discount=1.0 - tf.cast(step[rlds.IS_TERMINAL][1], dtype=tf.float32),\n      # If next step is terminal, then the observation may be arbitrary.\n      next_observation=tf.nest.map_structure(\n          lambda x: x[1], step[rlds.OBSERVATION])\n  )",
  "def _batch_steps(episode: rlds.Episode) -> tf.data.Dataset:\n  return rlds.transformations.batch(\n      episode[rlds.STEPS], size=2, shift=1, drop_remainder=True)",
  "def _dataset_size_upperbound(dataset: tf.data.Dataset) -> int:\n  if dataset.cardinality() != tf.data.experimental.UNKNOWN_CARDINALITY:\n    return dataset.cardinality()\n  return tf.cast(\n      dataset.batch(1000).reduce(0, lambda x, step: x + 1000), tf.int64)",
  "def load_tfds_dataset(\n    dataset_name: str,\n    num_episodes: Optional[int] = None,\n    env_spec: Optional[specs.EnvironmentSpec] = None) -> tf.data.Dataset:\n  \"\"\"Returns a TFDS dataset with the given name.\"\"\"\n  # Used only in tests.\n  del env_spec\n\n  dataset = tfds.load(dataset_name)['train']\n  if num_episodes:\n    dataset = dataset.take(num_episodes)\n  return dataset",
  "def get_tfds_dataset(\n    dataset_name: str,\n    num_episodes: Optional[int] = None,\n    env_spec: Optional[specs.EnvironmentSpec] = None) -> tf.data.Dataset:\n  \"\"\"Returns a TFDS dataset transformed to a dataset of transitions.\"\"\"\n  dataset = load_tfds_dataset(dataset_name, num_episodes, env_spec)\n  batched_steps = dataset.flat_map(_batch_steps)\n  return rlds.transformations.map_steps(batched_steps,\n                                        _batched_step_to_transition)",
  "def _pad(x: jnp.ndarray) -> jnp.ndarray:\n  if len(x.shape) != 2:\n    return x\n  # Find a more scientific way to find this threshold (30). Depending on various\n  # conditions for low enough sizes the excessive copying is not triggered.\n  if x.shape[-1] % _BEST_DIVISOR != 0 and x.shape[-1] > 30:\n    n = _BEST_DIVISOR - (x.shape[-1] % _BEST_DIVISOR)\n    x = np.pad(x, [(0, 0)] * (x.ndim - 1) + [(0, n)], 'constant')\n  return x",
  "def _unpad(x: jnp.ndarray, shape: Sequence[int]) -> jnp.ndarray:\n  if len(shape) == 2 and x.shape[-1] != shape[-1]:\n    return x[..., :shape[-1]]\n  return x",
  "class JaxInMemoryRandomSampleIterator(Iterator[Any]):\n  \"\"\"In memory random sample iterator implemented in JAX.\n\n  Loads the whole dataset in memory and performs random sampling with\n  replacement of batches of `batch_size`.\n  This class provides much faster sampling functionality compared to using\n  an iterator on tf.data.Dataset.\n  \"\"\"\n\n  def __init__(self,\n               dataset: tf.data.Dataset,\n               key: jnp.ndarray,\n               batch_size: int,\n               shard_dataset_across_devices: bool = False):\n    \"\"\"Creates an iterator.\n\n    Args:\n      dataset: underlying tf Dataset\n      key: a key to be used for random number generation\n      batch_size: batch size\n      shard_dataset_across_devices: whether to use all available devices\n        for storing the underlying dataset. The upside is a larger\n        dataset capacity that fits into memory. Downsides are:\n          - execution of pmapped functions is usually slower than jitted\n          - few last elements in the dataset might be dropped (if not multiple)\n          - sampling is not 100% uniform, since each core will be doing sampling\n            only within its data chunk\n        The number of available devices must divide the batch_size evenly.\n    \"\"\"\n    # Read the whole dataset. We use artificially large batch_size to make sure\n    # we capture the whole dataset.\n    size = _dataset_size_upperbound(dataset)\n    data = next(dataset.batch(size).as_numpy_iterator())\n    self._dataset_size = jax.tree_flatten(\n        jax.tree_map(lambda x: x.shape[0], data))[0][0]\n    device = jax_utils._pmap_device_order()\n    if not shard_dataset_across_devices:\n      device = device[:1]\n    should_pmap = len(device) > 1\n    assert batch_size % len(device) == 0\n    self._dataset_size = self._dataset_size - self._dataset_size % len(device)\n    # len(device) needs to divide self._dataset_size evenly.\n    assert self._dataset_size % len(device) == 0\n    logging.info('Trying to load %s elements to %s', self._dataset_size, device)\n    logging.info('Dataset %s %s',\n                 ('before padding' if should_pmap else ''),\n                 jax.tree_map(lambda x: x.shape, data))\n    if should_pmap:\n      shapes = jax.tree_map(lambda x: x.shape, data)\n      # Padding to a multiple of 128 is needed to avoid excessive copying on TPU\n      data = jax.tree_map(_pad, data)\n      logging.info('Dataset after padding %s',\n                   jax.tree_map(lambda x: x.shape, data))\n      def split_and_put(x: jnp.ndarray) -> jnp.ndarray:\n        return jax.device_put_sharded(\n            np.split(x[:self._dataset_size], len(device)), devices=device)\n      self._jax_dataset = jax.tree_map(split_and_put, data)\n    else:\n      self._jax_dataset = jax.tree_map(jax.device_put, data)\n\n    self._key = (jnp.stack(jax.random.split(key, len(device)))\n                 if should_pmap else key)\n\n    def sample_per_shard(data: Any,\n                         key: jnp.ndarray) -> Tuple[Any, jnp.ndarray]:\n      key1, key2 = jax.random.split(key)\n      indices = jax.random.randint(\n          key1, (batch_size // len(device),),\n          minval=0,\n          maxval=self._dataset_size // len(device))\n      data_sample = jax.tree_map(lambda d: jnp.take(d, indices, axis=0), data)\n      return data_sample, key2\n\n    if should_pmap:\n      def sample(data, key):\n        data_sample, key = sample_per_shard(data, key)\n        # Gathering data on TPUs is much more efficient that doing so on a host\n        # since it avoids Host - Device communications.\n        data_sample = jax.lax.all_gather(\n            data_sample, axis_name=_PMAP_AXIS_NAME, axis=0, tiled=True)\n        data_sample = jax.tree_map(_unpad, data_sample, shapes)\n        return data_sample, key\n\n      pmapped_sample = jax.pmap(sample, axis_name=_PMAP_AXIS_NAME)\n\n      def sample_and_postprocess(key: jnp.ndarray) -> Tuple[Any, jnp.ndarray]:\n        data, key = pmapped_sample(self._jax_dataset, key)\n        # All pmapped devices return the same data, so we just take the one from\n        # the first device.\n        return jax.tree_map(lambda x: x[0], data), key\n      self._sample = sample_and_postprocess\n    else:\n      self._sample = jax.jit(\n          lambda key: sample_per_shard(self._jax_dataset, key))\n\n  def __next__(self) -> Any:\n    data, self._key = self._sample(self._key)\n    return data\n\n  @property\n  def dataset_size(self) -> int:\n    \"\"\"An integer of the dataset cardinality.\"\"\"\n    return self._dataset_size",
  "def __init__(self,\n               dataset: tf.data.Dataset,\n               key: jnp.ndarray,\n               batch_size: int,\n               shard_dataset_across_devices: bool = False):\n    \"\"\"Creates an iterator.\n\n    Args:\n      dataset: underlying tf Dataset\n      key: a key to be used for random number generation\n      batch_size: batch size\n      shard_dataset_across_devices: whether to use all available devices\n        for storing the underlying dataset. The upside is a larger\n        dataset capacity that fits into memory. Downsides are:\n          - execution of pmapped functions is usually slower than jitted\n          - few last elements in the dataset might be dropped (if not multiple)\n          - sampling is not 100% uniform, since each core will be doing sampling\n            only within its data chunk\n        The number of available devices must divide the batch_size evenly.\n    \"\"\"\n    # Read the whole dataset. We use artificially large batch_size to make sure\n    # we capture the whole dataset.\n    size = _dataset_size_upperbound(dataset)\n    data = next(dataset.batch(size).as_numpy_iterator())\n    self._dataset_size = jax.tree_flatten(\n        jax.tree_map(lambda x: x.shape[0], data))[0][0]\n    device = jax_utils._pmap_device_order()\n    if not shard_dataset_across_devices:\n      device = device[:1]\n    should_pmap = len(device) > 1\n    assert batch_size % len(device) == 0\n    self._dataset_size = self._dataset_size - self._dataset_size % len(device)\n    # len(device) needs to divide self._dataset_size evenly.\n    assert self._dataset_size % len(device) == 0\n    logging.info('Trying to load %s elements to %s', self._dataset_size, device)\n    logging.info('Dataset %s %s',\n                 ('before padding' if should_pmap else ''),\n                 jax.tree_map(lambda x: x.shape, data))\n    if should_pmap:\n      shapes = jax.tree_map(lambda x: x.shape, data)\n      # Padding to a multiple of 128 is needed to avoid excessive copying on TPU\n      data = jax.tree_map(_pad, data)\n      logging.info('Dataset after padding %s',\n                   jax.tree_map(lambda x: x.shape, data))\n      def split_and_put(x: jnp.ndarray) -> jnp.ndarray:\n        return jax.device_put_sharded(\n            np.split(x[:self._dataset_size], len(device)), devices=device)\n      self._jax_dataset = jax.tree_map(split_and_put, data)\n    else:\n      self._jax_dataset = jax.tree_map(jax.device_put, data)\n\n    self._key = (jnp.stack(jax.random.split(key, len(device)))\n                 if should_pmap else key)\n\n    def sample_per_shard(data: Any,\n                         key: jnp.ndarray) -> Tuple[Any, jnp.ndarray]:\n      key1, key2 = jax.random.split(key)\n      indices = jax.random.randint(\n          key1, (batch_size // len(device),),\n          minval=0,\n          maxval=self._dataset_size // len(device))\n      data_sample = jax.tree_map(lambda d: jnp.take(d, indices, axis=0), data)\n      return data_sample, key2\n\n    if should_pmap:\n      def sample(data, key):\n        data_sample, key = sample_per_shard(data, key)\n        # Gathering data on TPUs is much more efficient that doing so on a host\n        # since it avoids Host - Device communications.\n        data_sample = jax.lax.all_gather(\n            data_sample, axis_name=_PMAP_AXIS_NAME, axis=0, tiled=True)\n        data_sample = jax.tree_map(_unpad, data_sample, shapes)\n        return data_sample, key\n\n      pmapped_sample = jax.pmap(sample, axis_name=_PMAP_AXIS_NAME)\n\n      def sample_and_postprocess(key: jnp.ndarray) -> Tuple[Any, jnp.ndarray]:\n        data, key = pmapped_sample(self._jax_dataset, key)\n        # All pmapped devices return the same data, so we just take the one from\n        # the first device.\n        return jax.tree_map(lambda x: x[0], data), key\n      self._sample = sample_and_postprocess\n    else:\n      self._sample = jax.jit(\n          lambda key: sample_per_shard(self._jax_dataset, key))",
  "def __next__(self) -> Any:\n    data, self._key = self._sample(self._key)\n    return data",
  "def dataset_size(self) -> int:\n    \"\"\"An integer of the dataset cardinality.\"\"\"\n    return self._dataset_size",
  "def sample_per_shard(data: Any,\n                         key: jnp.ndarray) -> Tuple[Any, jnp.ndarray]:\n      key1, key2 = jax.random.split(key)\n      indices = jax.random.randint(\n          key1, (batch_size // len(device),),\n          minval=0,\n          maxval=self._dataset_size // len(device))\n      data_sample = jax.tree_map(lambda d: jnp.take(d, indices, axis=0), data)\n      return data_sample, key2",
  "def split_and_put(x: jnp.ndarray) -> jnp.ndarray:\n        return jax.device_put_sharded(\n            np.split(x[:self._dataset_size], len(device)), devices=device)",
  "def sample(data, key):\n        data_sample, key = sample_per_shard(data, key)\n        # Gathering data on TPUs is much more efficient that doing so on a host\n        # since it avoids Host - Device communications.\n        data_sample = jax.lax.all_gather(\n            data_sample, axis_name=_PMAP_AXIS_NAME, axis=0, tiled=True)\n        data_sample = jax.tree_map(_unpad, data_sample, shapes)\n        return data_sample, key",
  "def sample_and_postprocess(key: jnp.ndarray) -> Tuple[Any, jnp.ndarray]:\n        data, key = pmapped_sample(self._jax_dataset, key)\n        # All pmapped devices return the same data, so we just take the one from\n        # the first device.\n        return jax.tree_map(lambda x: x[0], data), key",
  "class NumpyIterator(Iterator[types.NestedArray]):\n  \"\"\"Iterator over a dataset with elements converted to numpy.\n\n  Note: This iterator returns read-only numpy arrays.\n\n  This iterator (compared to `tf.data.Dataset.as_numpy_iterator()`) does not\n  copy the data when comverting `tf.Tensor`s to `np.ndarray`s.\n\n  TODO(b/178684359): Remove this when it is upstreamed into `tf.data`.\n  \"\"\"\n\n  __slots__ = ['_iterator']\n\n  def __init__(self, dataset):\n    self._iterator: Iterator[types.NestedTensor] = iter(dataset)\n\n  def __iter__(self) -> 'NumpyIterator':\n    return self\n\n  def __next__(self) -> types.NestedArray:\n    return tree.map_structure(lambda t: np.asarray(memoryview(t)),\n                              next(self._iterator))\n\n  def next(self):\n    return self.__next__()",
  "def __init__(self, dataset):\n    self._iterator: Iterator[types.NestedTensor] = iter(dataset)",
  "def __iter__(self) -> 'NumpyIterator':\n    return self",
  "def __next__(self) -> types.NestedArray:\n    return tree.map_structure(lambda t: np.asarray(memoryview(t)),\n                              next(self._iterator))",
  "def next(self):\n    return self.__next__()",
  "class Adder(abc.ABC):\n  \"\"\"The Adder interface.\n\n  An adder packs together data to send to the replay buffer, and potentially\n  performs some reduction/transformation to this data in the process.\n\n  All adders will use this API. Below is an illustrative example of how they\n  are intended to be used in a typical RL run-loop. We assume that the\n  environment conforms to the dm_env environment API.\n\n  ```python\n  # Reset the environment and add the first observation.\n  timestep = env.reset()\n  adder.add_first(timestep.observation)\n\n  while not timestep.last():\n    # Generate an action from the policy and step the environment.\n    action = my_policy(timestep)\n    timestep = env.step(action)\n\n    # Add the action and the resulting timestep.\n    adder.add(action, next_timestep=timestep)\n  ```\n\n  Note that for all adders, the `add()` method expects an action taken and the\n  *resulting* timestep observed after taking this action. Note that this\n  timestep is named `next_timestep` precisely to emphasize this point.\n  \"\"\"\n\n  @abc.abstractmethod\n  def add_first(self, timestep: dm_env.TimeStep):\n    \"\"\"Defines the interface for an adder's `add_first` method.\n\n    We expect this to be called at the beginning of each episode and it will\n    start a trajectory to be added to replay with an initial observation.\n\n    Args:\n      timestep: a dm_env TimeStep corresponding to the first step.\n    \"\"\"\n\n  @abc.abstractmethod\n  def add(\n      self,\n      action: types.NestedArray,\n      next_timestep: dm_env.TimeStep,\n      extras: types.NestedArray = (),\n  ):\n    \"\"\"Defines the adder `add` interface.\n\n    Args:\n      action: A possibly nested structure corresponding to a_t.\n      next_timestep: A dm_env Timestep object corresponding to the resulting\n        data obtained by taking the given action.\n      extras: A possibly nested structure of extra data to add to replay.\n    \"\"\"\n\n  @abc.abstractmethod\n  def reset(self):\n    \"\"\"Resets the adder's buffer.\"\"\"",
  "def add_first(self, timestep: dm_env.TimeStep):\n    \"\"\"Defines the interface for an adder's `add_first` method.\n\n    We expect this to be called at the beginning of each episode and it will\n    start a trajectory to be added to replay with an initial observation.\n\n    Args:\n      timestep: a dm_env TimeStep corresponding to the first step.\n    \"\"\"",
  "def add(\n      self,\n      action: types.NestedArray,\n      next_timestep: dm_env.TimeStep,\n      extras: types.NestedArray = (),\n  ):\n    \"\"\"Defines the adder `add` interface.\n\n    Args:\n      action: A possibly nested structure corresponding to a_t.\n      next_timestep: A dm_env Timestep object corresponding to the resulting\n        data obtained by taking the given action.\n      extras: A possibly nested structure of extra data to add to replay.\n    \"\"\"",
  "def reset(self):\n    \"\"\"Resets the adder's buffer.\"\"\"",
  "class ForkingAdder(base.Adder):\n  \"\"\"An adder that forks data into several other adders.\"\"\"\n\n  def __init__(self, adders: Iterable[base.Adder]):\n    self._adders = adders\n\n  def reset(self):\n    for adder in self._adders:\n      adder.reset()\n\n  def add_first(self, timestep: dm_env.TimeStep):\n    for adder in self._adders:\n      adder.add_first(timestep)\n\n  def add(self,\n          action: types.NestedArray,\n          next_timestep: dm_env.TimeStep,\n          extras: types.NestedArray = ()):\n    for adder in self._adders:\n      adder.add(action, next_timestep, extras)",
  "class IgnoreExtrasAdder(base.Adder):\n  \"\"\"An adder that ignores extras.\"\"\"\n\n  def __init__(self, adder: base.Adder):\n    self._adder = adder\n\n  def reset(self):\n    self._adder.reset()\n\n  def add_first(self, timestep: dm_env.TimeStep):\n    self._adder.add_first(timestep)\n\n  def add(self,\n          action: types.NestedArray,\n          next_timestep: dm_env.TimeStep,\n          extras: types.NestedArray = ()):\n    self._adder.add(action, next_timestep)",
  "def __init__(self, adders: Iterable[base.Adder]):\n    self._adders = adders",
  "def reset(self):\n    for adder in self._adders:\n      adder.reset()",
  "def add_first(self, timestep: dm_env.TimeStep):\n    for adder in self._adders:\n      adder.add_first(timestep)",
  "def add(self,\n          action: types.NestedArray,\n          next_timestep: dm_env.TimeStep,\n          extras: types.NestedArray = ()):\n    for adder in self._adders:\n      adder.add(action, next_timestep, extras)",
  "def __init__(self, adder: base.Adder):\n    self._adder = adder",
  "def reset(self):\n    self._adder.reset()",
  "def add_first(self, timestep: dm_env.TimeStep):\n    self._adder.add_first(timestep)",
  "def add(self,\n          action: types.NestedArray,\n          next_timestep: dm_env.TimeStep,\n          extras: types.NestedArray = ()):\n    self._adder.add(action, next_timestep)",
  "class NStepTransitionAdderTest(test_utils.AdderTestMixin,\n                               parameterized.TestCase):\n\n  @parameterized.named_parameters(*test_cases.TEST_CASES_FOR_TRANSITION_ADDER)\n  def test_adder(self, n_step, additional_discount, first, steps,\n                 expected_transitions):\n    adder = adders.NStepTransitionAdder(self.client, n_step,\n                                        additional_discount)\n    super().run_test_adder(\n        adder=adder,\n        first=first,\n        steps=steps,\n        expected_items=expected_transitions,\n        stack_sequence_fields=False,\n        signature=adder.signature(*test_utils.get_specs(steps[0])))",
  "def test_adder(self, n_step, additional_discount, first, steps,\n                 expected_transitions):\n    adder = adders.NStepTransitionAdder(self.client, n_step,\n                                        additional_discount)\n    super().run_test_adder(\n        adder=adder,\n        first=first,\n        steps=steps,\n        expected_items=expected_transitions,\n        stack_sequence_fields=False,\n        signature=adder.signature(*test_utils.get_specs(steps[0])))",
  "def zeros_like(x: Union[np.ndarray, int, float, np.number]):\n  \"\"\"Returns a zero-filled object of the same (d)type and shape as the input.\n\n  The difference between this and `np.zeros_like()` is that this works well\n  with `np.number`, `int`, `float`, and `jax.numpy.DeviceArray` objects without\n  converting them to `np.ndarray`s.\n\n  Args:\n    x: The object to replace with 0s.\n\n  Returns:\n    A zero-filed object of the same (d)type and shape as the input.\n  \"\"\"\n  if isinstance(x, (int, float, np.number)):\n    return type(x)(0)\n  elif isinstance(x, jax.Array):\n    return jnp.zeros_like(x)\n  elif isinstance(x, np.ndarray):\n    return np.zeros_like(x)\n  else:\n    raise ValueError(\n        f'Input ({type(x)}) must be either a numpy array, an int, or a float.')",
  "def final_step_like(step: base.Step,\n                    next_observation: types.NestedArray) -> base.Step:\n  \"\"\"Return a list of steps with the final step zero-filled.\"\"\"\n  # Make zero-filled components so we can fill out the last step.\n  zero_action, zero_reward, zero_discount, zero_extras = tree.map_structure(\n      zeros_like, (step.action, step.reward, step.discount, step.extras))\n\n  # Return a final step that only has next_observation.\n  return base.Step(\n      observation=next_observation,\n      action=zero_action,\n      reward=zero_reward,\n      discount=zero_discount,\n      start_of_episode=False,\n      extras=zero_extras)",
  "def calculate_priorities(\n    priority_fns: base.PriorityFnMapping,\n    trajectory_or_transition: Union[base.Trajectory, types.Transition],\n) -> Dict[str, float]:\n  \"\"\"Helper used to calculate the priority of a Trajectory or Transition.\n\n  This helper converts the leaves of the Trajectory or Transition from\n  `reverb.TrajectoryColumn` objects into numpy arrays. The converted Trajectory\n  or Transition is then passed into each of the functions in `priority_fns`.\n\n  Args:\n    priority_fns: a mapping from table names to priority functions (i.e. a\n      callable of type PriorityFn). The given function will be used to generate\n      the priority (a float) for the given table.\n    trajectory_or_transition: the trajectory or transition used to compute\n      priorities.\n\n  Returns:\n    A dictionary mapping from table names to the priority (a float) for the\n    given collection Trajectory or Transition.\n  \"\"\"\n  if any([priority_fn is not None for priority_fn in priority_fns.values()]):\n\n    trajectory_or_transition = tree.map_structure(lambda col: col.numpy(),\n                                                  trajectory_or_transition)\n\n  return {\n      table: (priority_fn(trajectory_or_transition) if priority_fn else 1.0)\n      for table, priority_fn in priority_fns.items()\n  }",
  "class Step(NamedTuple):\n  \"\"\"Step class used internally for reverb adders.\"\"\"\n  observation: types.NestedArray\n  action: types.NestedArray\n  reward: types.NestedArray\n  discount: types.NestedArray\n  start_of_episode: StartOfEpisodeType\n  extras: types.NestedArray = ()",
  "class PriorityFnInput(NamedTuple):\n  \"\"\"The input to a priority function consisting of stacked steps.\"\"\"\n  observations: types.NestedArray\n  actions: types.NestedArray\n  rewards: types.NestedArray\n  discounts: types.NestedArray\n  start_of_episode: types.NestedArray\n  extras: types.NestedArray",
  "def spec_like_to_tensor_spec(paths: Iterable[str], spec: specs.Array):\n  return tf.TensorSpec.from_spec(spec, name='/'.join(str(p) for p in paths))",
  "class ReverbAdder(base.Adder):\n  \"\"\"Base class for Reverb adders.\"\"\"\n\n  def __init__(\n      self,\n      client: reverb.Client,\n      max_sequence_length: int,\n      max_in_flight_items: int,\n      delta_encoded: bool = False,\n      priority_fns: Optional[PriorityFnMapping] = None,\n      validate_items: bool = True,\n  ):\n    \"\"\"Initialize a ReverbAdder instance.\n\n    Args:\n      client: A client to the Reverb backend.\n      max_sequence_length: The maximum length of sequences (corresponding to the\n        number of observations) that can be added to replay.\n      max_in_flight_items: The maximum number of items allowed to be \"in flight\"\n        at the same time. See `block_until_num_items` in\n        `reverb.TrajectoryWriter.flush` for more info.\n      delta_encoded: If `True` (False by default) enables delta encoding, see\n        `Client` for more information.\n      priority_fns: A mapping from table names to priority functions; if\n        omitted, all transitions/steps/sequences are given uniform priorities\n        (1.0) and placed in DEFAULT_PRIORITY_TABLE.\n      validate_items: Whether to validate items against the table signature\n        before they are sent to the server. This requires table signature to be\n        fetched from the server and cached locally.\n    \"\"\"\n    if priority_fns:\n      priority_fns = dict(priority_fns)\n    else:\n      priority_fns = {DEFAULT_PRIORITY_TABLE: None}\n\n    self._client = client\n    self._priority_fns = priority_fns\n    self._max_sequence_length = max_sequence_length\n    self._delta_encoded = delta_encoded\n    # TODO(b/206629159): Remove this.\n    self._max_in_flight_items = max_in_flight_items\n    self._add_first_called = False\n\n    # This is exposed as the _writer property in such a way that it will create\n    # a new writer automatically whenever the internal __writer is None. Users\n    # should ONLY ever interact with self._writer.\n    self.__writer = None\n    # Every time a new writer is created, it must fetch the signature from the\n    # Reverb server. If this is set too low it can crash the adders in a\n    # distributed setup where the replay may take a while to spin up.\n    self._validate_items = validate_items\n\n  def __del__(self):\n    if self.__writer is not None:\n      timeout_ms = 10_000\n      # Try flush all appended data before closing to avoid loss of experience.\n      try:\n        self.__writer.flush(0, timeout_ms=timeout_ms)\n      except reverb.DeadlineExceededError as e:\n        logging.error(\n            'Timeout (%d ms) exceeded when flushing the writer before '\n            'deleting it. Caught Reverb exception: %s', timeout_ms, str(e))\n      self.__writer.close()\n      self.__writer = None\n\n  @property\n  def _writer(self) -> reverb.TrajectoryWriter:\n    if self.__writer is None:\n      self.__writer = self._client.trajectory_writer(\n          num_keep_alive_refs=self._max_sequence_length,\n          validate_items=self._validate_items)\n      self._writer_created_timestamp = time.time()\n    return self.__writer\n\n  def add_priority_table(self, table_name: str,\n                         priority_fn: Optional[PriorityFn]):\n    if table_name in self._priority_fns:\n      raise ValueError(\n          f'A priority function already exists for {table_name}. '\n          f'Existing tables: {\", \".join(self._priority_fns.keys())}.'\n      )\n    self._priority_fns[table_name] = priority_fn\n\n  def reset(self, timeout_ms: Optional[int] = None):\n    \"\"\"Resets the adder's buffer.\"\"\"\n    if self.__writer:\n      # Flush all appended data and clear the buffers.\n      self.__writer.end_episode(clear_buffers=True, timeout_ms=timeout_ms)\n\n      # Create a new writer unless the current one is too young.\n      # This is to reduce the relative overhead of creating a new Reverb writer.\n      if (time.time() - self._writer_created_timestamp >\n          _MIN_WRITER_LIFESPAN_SECONDS):\n        self.__writer = None\n    self._add_first_called = False\n\n  def add_first(self, timestep: dm_env.TimeStep):\n    \"\"\"Record the first observation of a trajectory.\"\"\"\n    if not timestep.first():\n      raise ValueError('adder.add_first with an initial timestep (i.e. one for '\n                       'which timestep.first() is True')\n\n    # Record the next observation but leave the history buffer row open by\n    # passing `partial_step=True`.\n    self._writer.append(dict(observation=timestep.observation,\n                             start_of_episode=timestep.first()),\n                        partial_step=True)\n    self._add_first_called = True\n\n  def add(self,\n          action: types.NestedArray,\n          next_timestep: dm_env.TimeStep,\n          extras: types.NestedArray = ()):\n    \"\"\"Record an action and the following timestep.\"\"\"\n\n    if not self._add_first_called:\n      raise ValueError('adder.add_first must be called before adder.add.')\n\n    # Add the timestep to the buffer.\n    has_extras = (len(extras) > 0 if isinstance(extras, Sized)  # pylint: disable=g-explicit-length-test\n                  else extras is not None)\n    current_step = dict(\n        # Observation was passed at the previous add call.\n        action=action,\n        reward=next_timestep.reward,\n        discount=next_timestep.discount,\n        # Start of episode indicator was passed at the previous add call.\n        **({'extras': extras} if has_extras else {})\n    )\n    self._writer.append(current_step)\n\n    # Record the next observation and write.\n    self._writer.append(\n        dict(\n            observation=next_timestep.observation,\n            start_of_episode=next_timestep.first()),\n        partial_step=True)\n    self._write()\n\n    if next_timestep.last():\n      # Complete the row by appending zeros to remaining open fields.\n      # TODO(b/183945808): remove this when fields are no longer expected to be\n      # of equal length on the learner side.\n      dummy_step = tree.map_structure(np.zeros_like, current_step)\n      self._writer.append(dummy_step)\n      self._write_last()\n      self.reset()\n\n  @classmethod\n  def signature(cls, environment_spec: specs.EnvironmentSpec,\n                extras_spec: types.NestedSpec = ()):\n    \"\"\"This is a helper method for generating signatures for Reverb tables.\n\n    Signatures are useful for validating data types and shapes, see Reverb's\n    documentation for details on how they are used.\n\n    Args:\n      environment_spec: A `specs.EnvironmentSpec` whose fields are nested\n        structures with leaf nodes that have `.shape` and `.dtype` attributes.\n        This should come from the environment that will be used to generate\n        the data inserted into the Reverb table.\n      extras_spec: A nested structure with leaf nodes that have `.shape` and\n        `.dtype` attributes. The structure (and shapes/dtypes) of this must\n        be the same as the `extras` passed into `ReverbAdder.add`.\n\n    Returns:\n      A `Step` whose leaf nodes are `tf.TensorSpec` objects.\n    \"\"\"\n    spec_step = Step(\n        observation=environment_spec.observations,\n        action=environment_spec.actions,\n        reward=environment_spec.rewards,\n        discount=environment_spec.discounts,\n        start_of_episode=specs.Array(shape=(), dtype=bool),\n        extras=extras_spec)\n    return tree.map_structure_with_path(spec_like_to_tensor_spec, spec_step)\n\n  @abc.abstractmethod\n  def _write(self):\n    \"\"\"Write data to replay from the buffer.\"\"\"\n\n  @abc.abstractmethod\n  def _write_last(self):\n    \"\"\"Write data to replay from the buffer.\"\"\"",
  "def __init__(\n      self,\n      client: reverb.Client,\n      max_sequence_length: int,\n      max_in_flight_items: int,\n      delta_encoded: bool = False,\n      priority_fns: Optional[PriorityFnMapping] = None,\n      validate_items: bool = True,\n  ):\n    \"\"\"Initialize a ReverbAdder instance.\n\n    Args:\n      client: A client to the Reverb backend.\n      max_sequence_length: The maximum length of sequences (corresponding to the\n        number of observations) that can be added to replay.\n      max_in_flight_items: The maximum number of items allowed to be \"in flight\"\n        at the same time. See `block_until_num_items` in\n        `reverb.TrajectoryWriter.flush` for more info.\n      delta_encoded: If `True` (False by default) enables delta encoding, see\n        `Client` for more information.\n      priority_fns: A mapping from table names to priority functions; if\n        omitted, all transitions/steps/sequences are given uniform priorities\n        (1.0) and placed in DEFAULT_PRIORITY_TABLE.\n      validate_items: Whether to validate items against the table signature\n        before they are sent to the server. This requires table signature to be\n        fetched from the server and cached locally.\n    \"\"\"\n    if priority_fns:\n      priority_fns = dict(priority_fns)\n    else:\n      priority_fns = {DEFAULT_PRIORITY_TABLE: None}\n\n    self._client = client\n    self._priority_fns = priority_fns\n    self._max_sequence_length = max_sequence_length\n    self._delta_encoded = delta_encoded\n    # TODO(b/206629159): Remove this.\n    self._max_in_flight_items = max_in_flight_items\n    self._add_first_called = False\n\n    # This is exposed as the _writer property in such a way that it will create\n    # a new writer automatically whenever the internal __writer is None. Users\n    # should ONLY ever interact with self._writer.\n    self.__writer = None\n    # Every time a new writer is created, it must fetch the signature from the\n    # Reverb server. If this is set too low it can crash the adders in a\n    # distributed setup where the replay may take a while to spin up.\n    self._validate_items = validate_items",
  "def __del__(self):\n    if self.__writer is not None:\n      timeout_ms = 10_000\n      # Try flush all appended data before closing to avoid loss of experience.\n      try:\n        self.__writer.flush(0, timeout_ms=timeout_ms)\n      except reverb.DeadlineExceededError as e:\n        logging.error(\n            'Timeout (%d ms) exceeded when flushing the writer before '\n            'deleting it. Caught Reverb exception: %s', timeout_ms, str(e))\n      self.__writer.close()\n      self.__writer = None",
  "def _writer(self) -> reverb.TrajectoryWriter:\n    if self.__writer is None:\n      self.__writer = self._client.trajectory_writer(\n          num_keep_alive_refs=self._max_sequence_length,\n          validate_items=self._validate_items)\n      self._writer_created_timestamp = time.time()\n    return self.__writer",
  "def add_priority_table(self, table_name: str,\n                         priority_fn: Optional[PriorityFn]):\n    if table_name in self._priority_fns:\n      raise ValueError(\n          f'A priority function already exists for {table_name}. '\n          f'Existing tables: {\", \".join(self._priority_fns.keys())}.'\n      )\n    self._priority_fns[table_name] = priority_fn",
  "def reset(self, timeout_ms: Optional[int] = None):\n    \"\"\"Resets the adder's buffer.\"\"\"\n    if self.__writer:\n      # Flush all appended data and clear the buffers.\n      self.__writer.end_episode(clear_buffers=True, timeout_ms=timeout_ms)\n\n      # Create a new writer unless the current one is too young.\n      # This is to reduce the relative overhead of creating a new Reverb writer.\n      if (time.time() - self._writer_created_timestamp >\n          _MIN_WRITER_LIFESPAN_SECONDS):\n        self.__writer = None\n    self._add_first_called = False",
  "def add_first(self, timestep: dm_env.TimeStep):\n    \"\"\"Record the first observation of a trajectory.\"\"\"\n    if not timestep.first():\n      raise ValueError('adder.add_first with an initial timestep (i.e. one for '\n                       'which timestep.first() is True')\n\n    # Record the next observation but leave the history buffer row open by\n    # passing `partial_step=True`.\n    self._writer.append(dict(observation=timestep.observation,\n                             start_of_episode=timestep.first()),\n                        partial_step=True)\n    self._add_first_called = True",
  "def add(self,\n          action: types.NestedArray,\n          next_timestep: dm_env.TimeStep,\n          extras: types.NestedArray = ()):\n    \"\"\"Record an action and the following timestep.\"\"\"\n\n    if not self._add_first_called:\n      raise ValueError('adder.add_first must be called before adder.add.')\n\n    # Add the timestep to the buffer.\n    has_extras = (len(extras) > 0 if isinstance(extras, Sized)  # pylint: disable=g-explicit-length-test\n                  else extras is not None)\n    current_step = dict(\n        # Observation was passed at the previous add call.\n        action=action,\n        reward=next_timestep.reward,\n        discount=next_timestep.discount,\n        # Start of episode indicator was passed at the previous add call.\n        **({'extras': extras} if has_extras else {})\n    )\n    self._writer.append(current_step)\n\n    # Record the next observation and write.\n    self._writer.append(\n        dict(\n            observation=next_timestep.observation,\n            start_of_episode=next_timestep.first()),\n        partial_step=True)\n    self._write()\n\n    if next_timestep.last():\n      # Complete the row by appending zeros to remaining open fields.\n      # TODO(b/183945808): remove this when fields are no longer expected to be\n      # of equal length on the learner side.\n      dummy_step = tree.map_structure(np.zeros_like, current_step)\n      self._writer.append(dummy_step)\n      self._write_last()\n      self.reset()",
  "def signature(cls, environment_spec: specs.EnvironmentSpec,\n                extras_spec: types.NestedSpec = ()):\n    \"\"\"This is a helper method for generating signatures for Reverb tables.\n\n    Signatures are useful for validating data types and shapes, see Reverb's\n    documentation for details on how they are used.\n\n    Args:\n      environment_spec: A `specs.EnvironmentSpec` whose fields are nested\n        structures with leaf nodes that have `.shape` and `.dtype` attributes.\n        This should come from the environment that will be used to generate\n        the data inserted into the Reverb table.\n      extras_spec: A nested structure with leaf nodes that have `.shape` and\n        `.dtype` attributes. The structure (and shapes/dtypes) of this must\n        be the same as the `extras` passed into `ReverbAdder.add`.\n\n    Returns:\n      A `Step` whose leaf nodes are `tf.TensorSpec` objects.\n    \"\"\"\n    spec_step = Step(\n        observation=environment_spec.observations,\n        action=environment_spec.actions,\n        reward=environment_spec.rewards,\n        discount=environment_spec.discounts,\n        start_of_episode=specs.Array(shape=(), dtype=bool),\n        extras=extras_spec)\n    return tree.map_structure_with_path(spec_like_to_tensor_spec, spec_step)",
  "def _write(self):\n    \"\"\"Write data to replay from the buffer.\"\"\"",
  "def _write_last(self):\n    \"\"\"Write data to replay from the buffer.\"\"\"",
  "class EndBehavior(enum.Enum):\n  \"\"\"Class to enumerate available options for writing behavior at episode ends.\n\n  Example:\n\n    sequence_length = 3\n    period = 2\n\n  Episode steps (digits) and writing events (W):\n\n             1 2 3 4 5 6\n                 W   W\n\n  First two sequences:\n\n             1 2 3\n             . . 3 4 5\n\n  Written sequences for the different end of episode behaviors:\n  Here are the last written sequences for each end of episode behavior:\n\n   WRITE     . . . 4 5 6\n   CONTINUE  . . . . 5 6 F\n   ZERO_PAD  . . . . 5 6 0\n   TRUNCATE  . . . . 5 6\n\n  Key:\n    F: First step of the next episode\n    0: Zero-filled Step\n  \"\"\"\n  WRITE = 'write_buffer'\n  CONTINUE = 'continue_to_next_episode'\n  ZERO_PAD = 'zero_pad_til_next_write'\n  TRUNCATE = 'write_truncated_buffer'",
  "class SequenceAdder(base.ReverbAdder):\n  \"\"\"An adder which adds sequences of fixed length.\"\"\"\n\n  def __init__(\n      self,\n      client: reverb.Client,\n      sequence_length: int,\n      period: int,\n      *,\n      delta_encoded: bool = False,\n      priority_fns: Optional[base.PriorityFnMapping] = None,\n      max_in_flight_items: Optional[int] = 2,\n      end_of_episode_behavior: Optional[EndBehavior] = None,\n      # Deprecated kwargs.\n      chunk_length: Optional[int] = None,\n      pad_end_of_episode: Optional[bool] = None,\n      break_end_of_episode: Optional[bool] = None,\n      validate_items: bool = True,\n  ):\n    \"\"\"Makes a SequenceAdder instance.\n\n    Args:\n      client: See docstring for BaseAdder.\n      sequence_length: The fixed length of sequences we wish to add.\n      period: The period with which we add sequences. If less than\n        sequence_length, overlapping sequences are added. If equal to\n        sequence_length, sequences are exactly non-overlapping.\n      delta_encoded: If `True` (False by default) enables delta encoding, see\n        `Client` for more information.\n      priority_fns: See docstring for BaseAdder.\n      max_in_flight_items: The maximum number of items allowed to be \"in flight\"\n        at the same time. See `block_until_num_items` in\n        `reverb.TrajectoryWriter.flush` for more info.\n      end_of_episode_behavior:  Determines how sequences at the end of the\n        episode are handled (default `EndOfEpisodeBehavior.ZERO_PAD`). See\n        the docstring for `EndOfEpisodeBehavior` for more information.\n      chunk_length: Deprecated and unused.\n      pad_end_of_episode: If True (default) then upon end of episode the current\n        sequence will be padded (with observations, actions, etc... whose values\n        are 0) until its length is `sequence_length`. If False then the last\n        sequence in the episode may have length less than `sequence_length`.\n      break_end_of_episode: If 'False' (True by default) does not break\n        sequences on env reset. In this case 'pad_end_of_episode' is not used.\n      validate_items: Whether to validate items against the table signature\n        before they are sent to the server. This requires table signature to be\n        fetched from the server and cached locally.\n    \"\"\"\n    del chunk_length\n    super().__init__(\n        client=client,\n        # We need an additional space in the buffer for the partial step the\n        # base.ReverbAdder will add with the next observation.\n        max_sequence_length=sequence_length+1,\n        delta_encoded=delta_encoded,\n        priority_fns=priority_fns,\n        max_in_flight_items=max_in_flight_items,\n        validate_items=validate_items)\n\n    if pad_end_of_episode and not break_end_of_episode:\n      raise ValueError(\n          'Can\\'t set pad_end_of_episode=True and break_end_of_episode=False at'\n          ' the same time, since those behaviors are incompatible.')\n\n    self._period = period\n    self._sequence_length = sequence_length\n\n    if end_of_episode_behavior and (pad_end_of_episode is not None or\n                                    break_end_of_episode is not None):\n      raise ValueError(\n          'Using end_of_episode_behavior and either '\n          'pad_end_of_episode or break_end_of_episode is not permitted. '\n          'Please use only end_of_episode_behavior instead.')\n\n    # Set pad_end_of_episode and break_end_of_episode to default values.\n    if end_of_episode_behavior is None and pad_end_of_episode is None:\n      pad_end_of_episode = True\n    if end_of_episode_behavior is None and break_end_of_episode is None:\n      break_end_of_episode = True\n\n    self._end_of_episode_behavior = EndBehavior.ZERO_PAD\n    if pad_end_of_episode is not None or break_end_of_episode is not None:\n      if not break_end_of_episode:\n        self._end_of_episode_behavior = EndBehavior.CONTINUE\n      elif break_end_of_episode and pad_end_of_episode:\n        self._end_of_episode_behavior = EndBehavior.ZERO_PAD\n      elif break_end_of_episode and not pad_end_of_episode:\n        self._end_of_episode_behavior = EndBehavior.TRUNCATE\n      else:\n        raise ValueError(\n            'Reached an unexpected configuration of the SequenceAdder '\n            f'with break_end_of_episode={break_end_of_episode} '\n            f'and pad_end_of_episode={pad_end_of_episode}.')\n    elif isinstance(end_of_episode_behavior, EndBehavior):\n      self._end_of_episode_behavior = end_of_episode_behavior\n    else:\n      raise ValueError('end_of_episod_behavior must be an instance of '\n                       f'EndBehavior, received {end_of_episode_behavior}.')\n\n  def reset(self):  # pytype: disable=signature-mismatch  # overriding-parameter-count-checks\n    \"\"\"Resets the adder's buffer.\"\"\"\n    # If we do not write on end of episode, we should not reset the writer.\n    if self._end_of_episode_behavior is EndBehavior.CONTINUE:\n      return\n\n    super().reset()\n\n  def _write(self):\n    self._maybe_create_item(self._sequence_length)\n\n  def _write_last(self):\n    # Maybe determine the delta to the next time we would write a sequence.\n    if self._end_of_episode_behavior in (EndBehavior.TRUNCATE,\n                                         EndBehavior.ZERO_PAD):\n      delta = self._sequence_length - self._writer.episode_steps\n      if delta < 0:\n        delta = (self._period + delta) % self._period\n\n    # Handle various end-of-episode cases.\n    if self._end_of_episode_behavior is EndBehavior.CONTINUE:\n      self._maybe_create_item(self._sequence_length, end_of_episode=True)\n\n    elif self._end_of_episode_behavior is EndBehavior.WRITE:\n      # Drop episodes that are too short.\n      if self._writer.episode_steps < self._sequence_length:\n        return\n      self._maybe_create_item(\n          self._sequence_length, end_of_episode=True, force=True)\n\n    elif self._end_of_episode_behavior is EndBehavior.TRUNCATE:\n      self._maybe_create_item(\n          self._sequence_length - delta,\n          end_of_episode=True,\n          force=True)\n\n    elif self._end_of_episode_behavior is EndBehavior.ZERO_PAD:\n      zero_step = tree.map_structure(lambda x: np.zeros_like(x[-2].numpy()),\n                                     self._writer.history)\n      for _ in range(delta):\n        self._writer.append(zero_step)\n\n      self._maybe_create_item(\n          self._sequence_length, end_of_episode=True, force=True)\n    else:\n      raise ValueError(\n          f'Unhandled end of episode behavior: {self._end_of_episode_behavior}.'\n          ' This should never happen, please contact Acme dev team.')\n\n  def _maybe_create_item(self,\n                         sequence_length: int,\n                         *,\n                         end_of_episode: bool = False,\n                         force: bool = False):\n\n    # Check conditions under which a new item is created.\n    first_write = self._writer.episode_steps == sequence_length\n    # NOTE(bshahr): the following line assumes that the only way sequence_length\n    # is less than self._sequence_length, is if the episode is shorter than\n    # self._sequence_length.\n    period_reached = (\n        self._writer.episode_steps > self._sequence_length and\n        ((self._writer.episode_steps - self._sequence_length) % self._period\n         == 0))\n\n    if not first_write and not period_reached and not force:\n      return\n\n    # TODO(b/183945808): will need to change to adhere to the new protocol.\n    if not end_of_episode:\n      get_traj = operator.itemgetter(slice(-sequence_length - 1, -1))\n    else:\n      get_traj = operator.itemgetter(slice(-sequence_length, None))\n\n    history = self._writer.history\n    trajectory = base.Trajectory(**tree.map_structure(get_traj, history))\n\n    # Compute priorities for the buffer.\n    table_priorities = utils.calculate_priorities(self._priority_fns,\n                                                  trajectory)\n\n    # Create a prioritized item for each table.\n    for table_name, priority in table_priorities.items():\n      self._writer.create_item(table_name, priority, trajectory)\n      self._writer.flush(self._max_in_flight_items)\n\n  # TODO(bshahr): make this into a standalone method. Class methods should be\n  # used as alternative constructors or when modifying some global state,\n  # neither of which is done here.\n  @classmethod\n  def signature(cls, environment_spec: specs.EnvironmentSpec,\n                extras_spec: types.NestedSpec = (),\n                sequence_length: Optional[int] = None):\n    \"\"\"This is a helper method for generating signatures for Reverb tables.\n\n    Signatures are useful for validating data types and shapes, see Reverb's\n    documentation for details on how they are used.\n\n    Args:\n      environment_spec: A `specs.EnvironmentSpec` whose fields are nested\n        structures with leaf nodes that have `.shape` and `.dtype` attributes.\n        This should come from the environment that will be used to generate\n        the data inserted into the Reverb table.\n      extras_spec: A nested structure with leaf nodes that have `.shape` and\n        `.dtype` attributes. The structure (and shapes/dtypes) of this must\n        be the same as the `extras` passed into `ReverbAdder.add`.\n      sequence_length: An optional integer representing the expected length of\n        sequences that will be added to replay.\n\n    Returns:\n      A `Trajectory` whose leaf nodes are `tf.TensorSpec` objects.\n    \"\"\"\n\n    def add_time_dim(paths: Iterable[str], spec: tf.TensorSpec):\n      return tf.TensorSpec(shape=(sequence_length, *spec.shape),\n                           dtype=spec.dtype,\n                           name='/'.join(str(p) for p in paths))\n\n    trajectory_env_spec, trajectory_extras_spec = tree.map_structure_with_path(\n        add_time_dim, (environment_spec, extras_spec))\n\n    spec_step = base.Trajectory(\n        *trajectory_env_spec,\n        start_of_episode=tf.TensorSpec(\n            shape=(sequence_length,), dtype=tf.bool, name='start_of_episode'),\n        extras=trajectory_extras_spec)\n\n    return spec_step",
  "def __init__(\n      self,\n      client: reverb.Client,\n      sequence_length: int,\n      period: int,\n      *,\n      delta_encoded: bool = False,\n      priority_fns: Optional[base.PriorityFnMapping] = None,\n      max_in_flight_items: Optional[int] = 2,\n      end_of_episode_behavior: Optional[EndBehavior] = None,\n      # Deprecated kwargs.\n      chunk_length: Optional[int] = None,\n      pad_end_of_episode: Optional[bool] = None,\n      break_end_of_episode: Optional[bool] = None,\n      validate_items: bool = True,\n  ):\n    \"\"\"Makes a SequenceAdder instance.\n\n    Args:\n      client: See docstring for BaseAdder.\n      sequence_length: The fixed length of sequences we wish to add.\n      period: The period with which we add sequences. If less than\n        sequence_length, overlapping sequences are added. If equal to\n        sequence_length, sequences are exactly non-overlapping.\n      delta_encoded: If `True` (False by default) enables delta encoding, see\n        `Client` for more information.\n      priority_fns: See docstring for BaseAdder.\n      max_in_flight_items: The maximum number of items allowed to be \"in flight\"\n        at the same time. See `block_until_num_items` in\n        `reverb.TrajectoryWriter.flush` for more info.\n      end_of_episode_behavior:  Determines how sequences at the end of the\n        episode are handled (default `EndOfEpisodeBehavior.ZERO_PAD`). See\n        the docstring for `EndOfEpisodeBehavior` for more information.\n      chunk_length: Deprecated and unused.\n      pad_end_of_episode: If True (default) then upon end of episode the current\n        sequence will be padded (with observations, actions, etc... whose values\n        are 0) until its length is `sequence_length`. If False then the last\n        sequence in the episode may have length less than `sequence_length`.\n      break_end_of_episode: If 'False' (True by default) does not break\n        sequences on env reset. In this case 'pad_end_of_episode' is not used.\n      validate_items: Whether to validate items against the table signature\n        before they are sent to the server. This requires table signature to be\n        fetched from the server and cached locally.\n    \"\"\"\n    del chunk_length\n    super().__init__(\n        client=client,\n        # We need an additional space in the buffer for the partial step the\n        # base.ReverbAdder will add with the next observation.\n        max_sequence_length=sequence_length+1,\n        delta_encoded=delta_encoded,\n        priority_fns=priority_fns,\n        max_in_flight_items=max_in_flight_items,\n        validate_items=validate_items)\n\n    if pad_end_of_episode and not break_end_of_episode:\n      raise ValueError(\n          'Can\\'t set pad_end_of_episode=True and break_end_of_episode=False at'\n          ' the same time, since those behaviors are incompatible.')\n\n    self._period = period\n    self._sequence_length = sequence_length\n\n    if end_of_episode_behavior and (pad_end_of_episode is not None or\n                                    break_end_of_episode is not None):\n      raise ValueError(\n          'Using end_of_episode_behavior and either '\n          'pad_end_of_episode or break_end_of_episode is not permitted. '\n          'Please use only end_of_episode_behavior instead.')\n\n    # Set pad_end_of_episode and break_end_of_episode to default values.\n    if end_of_episode_behavior is None and pad_end_of_episode is None:\n      pad_end_of_episode = True\n    if end_of_episode_behavior is None and break_end_of_episode is None:\n      break_end_of_episode = True\n\n    self._end_of_episode_behavior = EndBehavior.ZERO_PAD\n    if pad_end_of_episode is not None or break_end_of_episode is not None:\n      if not break_end_of_episode:\n        self._end_of_episode_behavior = EndBehavior.CONTINUE\n      elif break_end_of_episode and pad_end_of_episode:\n        self._end_of_episode_behavior = EndBehavior.ZERO_PAD\n      elif break_end_of_episode and not pad_end_of_episode:\n        self._end_of_episode_behavior = EndBehavior.TRUNCATE\n      else:\n        raise ValueError(\n            'Reached an unexpected configuration of the SequenceAdder '\n            f'with break_end_of_episode={break_end_of_episode} '\n            f'and pad_end_of_episode={pad_end_of_episode}.')\n    elif isinstance(end_of_episode_behavior, EndBehavior):\n      self._end_of_episode_behavior = end_of_episode_behavior\n    else:\n      raise ValueError('end_of_episod_behavior must be an instance of '\n                       f'EndBehavior, received {end_of_episode_behavior}.')",
  "def reset(self):  # pytype: disable=signature-mismatch  # overriding-parameter-count-checks\n    \"\"\"Resets the adder's buffer.\"\"\"\n    # If we do not write on end of episode, we should not reset the writer.\n    if self._end_of_episode_behavior is EndBehavior.CONTINUE:\n      return\n\n    super().reset()",
  "def _write(self):\n    self._maybe_create_item(self._sequence_length)",
  "def _write_last(self):\n    # Maybe determine the delta to the next time we would write a sequence.\n    if self._end_of_episode_behavior in (EndBehavior.TRUNCATE,\n                                         EndBehavior.ZERO_PAD):\n      delta = self._sequence_length - self._writer.episode_steps\n      if delta < 0:\n        delta = (self._period + delta) % self._period\n\n    # Handle various end-of-episode cases.\n    if self._end_of_episode_behavior is EndBehavior.CONTINUE:\n      self._maybe_create_item(self._sequence_length, end_of_episode=True)\n\n    elif self._end_of_episode_behavior is EndBehavior.WRITE:\n      # Drop episodes that are too short.\n      if self._writer.episode_steps < self._sequence_length:\n        return\n      self._maybe_create_item(\n          self._sequence_length, end_of_episode=True, force=True)\n\n    elif self._end_of_episode_behavior is EndBehavior.TRUNCATE:\n      self._maybe_create_item(\n          self._sequence_length - delta,\n          end_of_episode=True,\n          force=True)\n\n    elif self._end_of_episode_behavior is EndBehavior.ZERO_PAD:\n      zero_step = tree.map_structure(lambda x: np.zeros_like(x[-2].numpy()),\n                                     self._writer.history)\n      for _ in range(delta):\n        self._writer.append(zero_step)\n\n      self._maybe_create_item(\n          self._sequence_length, end_of_episode=True, force=True)\n    else:\n      raise ValueError(\n          f'Unhandled end of episode behavior: {self._end_of_episode_behavior}.'\n          ' This should never happen, please contact Acme dev team.')",
  "def _maybe_create_item(self,\n                         sequence_length: int,\n                         *,\n                         end_of_episode: bool = False,\n                         force: bool = False):\n\n    # Check conditions under which a new item is created.\n    first_write = self._writer.episode_steps == sequence_length\n    # NOTE(bshahr): the following line assumes that the only way sequence_length\n    # is less than self._sequence_length, is if the episode is shorter than\n    # self._sequence_length.\n    period_reached = (\n        self._writer.episode_steps > self._sequence_length and\n        ((self._writer.episode_steps - self._sequence_length) % self._period\n         == 0))\n\n    if not first_write and not period_reached and not force:\n      return\n\n    # TODO(b/183945808): will need to change to adhere to the new protocol.\n    if not end_of_episode:\n      get_traj = operator.itemgetter(slice(-sequence_length - 1, -1))\n    else:\n      get_traj = operator.itemgetter(slice(-sequence_length, None))\n\n    history = self._writer.history\n    trajectory = base.Trajectory(**tree.map_structure(get_traj, history))\n\n    # Compute priorities for the buffer.\n    table_priorities = utils.calculate_priorities(self._priority_fns,\n                                                  trajectory)\n\n    # Create a prioritized item for each table.\n    for table_name, priority in table_priorities.items():\n      self._writer.create_item(table_name, priority, trajectory)\n      self._writer.flush(self._max_in_flight_items)",
  "def signature(cls, environment_spec: specs.EnvironmentSpec,\n                extras_spec: types.NestedSpec = (),\n                sequence_length: Optional[int] = None):\n    \"\"\"This is a helper method for generating signatures for Reverb tables.\n\n    Signatures are useful for validating data types and shapes, see Reverb's\n    documentation for details on how they are used.\n\n    Args:\n      environment_spec: A `specs.EnvironmentSpec` whose fields are nested\n        structures with leaf nodes that have `.shape` and `.dtype` attributes.\n        This should come from the environment that will be used to generate\n        the data inserted into the Reverb table.\n      extras_spec: A nested structure with leaf nodes that have `.shape` and\n        `.dtype` attributes. The structure (and shapes/dtypes) of this must\n        be the same as the `extras` passed into `ReverbAdder.add`.\n      sequence_length: An optional integer representing the expected length of\n        sequences that will be added to replay.\n\n    Returns:\n      A `Trajectory` whose leaf nodes are `tf.TensorSpec` objects.\n    \"\"\"\n\n    def add_time_dim(paths: Iterable[str], spec: tf.TensorSpec):\n      return tf.TensorSpec(shape=(sequence_length, *spec.shape),\n                           dtype=spec.dtype,\n                           name='/'.join(str(p) for p in paths))\n\n    trajectory_env_spec, trajectory_extras_spec = tree.map_structure_with_path(\n        add_time_dim, (environment_spec, extras_spec))\n\n    spec_step = base.Trajectory(\n        *trajectory_env_spec,\n        start_of_episode=tf.TensorSpec(\n            shape=(sequence_length,), dtype=tf.bool, name='start_of_episode'),\n        extras=trajectory_extras_spec)\n\n    return spec_step",
  "def add_time_dim(paths: Iterable[str], spec: tf.TensorSpec):\n      return tf.TensorSpec(shape=(sequence_length, *spec.shape),\n                           dtype=spec.dtype,\n                           name='/'.join(str(p) for p in paths))",
  "class SequenceAdderTest(test_utils.AdderTestMixin, parameterized.TestCase):\n\n  @parameterized.named_parameters(*test_cases.TEST_CASES_FOR_SEQUENCE_ADDER)\n  def test_adder(self,\n                 sequence_length: int,\n                 period: int,\n                 first,\n                 steps,\n                 expected_sequences,\n                 end_behavior: adders.EndBehavior = adders.EndBehavior.ZERO_PAD,\n                 repeat_episode_times: int = 1):\n    adder = adders.SequenceAdder(\n        self.client,\n        sequence_length=sequence_length,\n        period=period,\n        end_of_episode_behavior=end_behavior)\n    super().run_test_adder(\n        adder=adder,\n        first=first,\n        steps=steps,\n        expected_items=expected_sequences,\n        repeat_episode_times=repeat_episode_times,\n        end_behavior=end_behavior,\n        signature=adder.signature(*test_utils.get_specs(steps[0])))\n\n  @parameterized.parameters(\n      (True, True, adders.EndBehavior.ZERO_PAD),\n      (False, True, adders.EndBehavior.TRUNCATE),\n      (False, False, adders.EndBehavior.CONTINUE),\n  )\n  def test_end_of_episode_behavior_set_correctly(self, pad_end_of_episode,\n                                                 break_end_of_episode,\n                                                 expected_behavior):\n    adder = adders.SequenceAdder(\n        self.client,\n        sequence_length=5,\n        period=3,\n        pad_end_of_episode=pad_end_of_episode,\n        break_end_of_episode=break_end_of_episode)\n    self.assertEqual(adder._end_of_episode_behavior, expected_behavior)",
  "def test_adder(self,\n                 sequence_length: int,\n                 period: int,\n                 first,\n                 steps,\n                 expected_sequences,\n                 end_behavior: adders.EndBehavior = adders.EndBehavior.ZERO_PAD,\n                 repeat_episode_times: int = 1):\n    adder = adders.SequenceAdder(\n        self.client,\n        sequence_length=sequence_length,\n        period=period,\n        end_of_episode_behavior=end_behavior)\n    super().run_test_adder(\n        adder=adder,\n        first=first,\n        steps=steps,\n        expected_items=expected_sequences,\n        repeat_episode_times=repeat_episode_times,\n        end_behavior=end_behavior,\n        signature=adder.signature(*test_utils.get_specs(steps[0])))",
  "def test_end_of_episode_behavior_set_correctly(self, pad_end_of_episode,\n                                                 break_end_of_episode,\n                                                 expected_behavior):\n    adder = adders.SequenceAdder(\n        self.client,\n        sequence_length=5,\n        period=3,\n        pad_end_of_episode=pad_end_of_episode,\n        break_end_of_episode=break_end_of_episode)\n    self.assertEqual(adder._end_of_episode_behavior, expected_behavior)",
  "class EpisodeAdder(base.ReverbAdder):\n  \"\"\"Adder which adds entire episodes as trajectories.\"\"\"\n\n  def __init__(\n      self,\n      client: reverb.Client,\n      max_sequence_length: int,\n      delta_encoded: bool = False,\n      priority_fns: Optional[base.PriorityFnMapping] = None,\n      max_in_flight_items: int = 1,\n      padding_fn: Optional[_PaddingFn] = None,\n      # Deprecated kwargs.\n      chunk_length: Optional[int] = None,\n  ):\n    del chunk_length\n\n    super().__init__(\n        client=client,\n        max_sequence_length=max_sequence_length,\n        delta_encoded=delta_encoded,\n        priority_fns=priority_fns,\n        max_in_flight_items=max_in_flight_items,\n    )\n    self._padding_fn = padding_fn\n\n  def add(\n      self,\n      action: types.NestedArray,\n      next_timestep: dm_env.TimeStep,\n      extras: types.NestedArray = (),\n  ):\n    if self._writer.episode_steps >= self._max_sequence_length - 1:\n      raise ValueError(\n          'The number of observations within the same episode will exceed '\n          'max_sequence_length with the addition of this transition.')\n\n    super().add(action, next_timestep, extras)\n\n  def _write(self):\n    # This adder only writes at the end of the episode, see _write_last()\n    pass\n\n  def _write_last(self):\n    if self._padding_fn is not None and self._writer.episode_steps < self._max_sequence_length:\n      history = self._writer.history\n      padding_step = dict(\n          observation=history['observation'],\n          action=history['action'],\n          reward=history['reward'],\n          discount=history['discount'],\n          extras=history.get('extras', ()))\n      # Get shapes and dtypes from the last element.\n      padding_step = tree.map_structure(\n          lambda col: self._padding_fn(col[-1].shape, col[-1].dtype),\n          padding_step)\n      padding_step['start_of_episode'] = False\n      while self._writer.episode_steps < self._max_sequence_length:\n        self._writer.append(padding_step)\n\n    trajectory = tree.map_structure(lambda x: x[:], self._writer.history)\n\n    # Pack the history into a base.Step structure and get numpy converted\n    # variant for priotiy computation.\n    trajectory = base.Trajectory(**trajectory)\n\n    # Calculate the priority for this episode.\n    table_priorities = utils.calculate_priorities(self._priority_fns,\n                                                  trajectory)\n\n    # Create a prioritized item for each table.\n    for table_name, priority in table_priorities.items():\n      self._writer.create_item(table_name, priority, trajectory)\n      self._writer.flush(self._max_in_flight_items)\n\n  # TODO(b/185309817): make this into a standalone method.\n  @classmethod\n  def signature(cls,\n                environment_spec: specs.EnvironmentSpec,\n                extras_spec: types.NestedSpec = (),\n                sequence_length: Optional[int] = None):\n    \"\"\"This is a helper method for generating signatures for Reverb tables.\n\n    Signatures are useful for validating data types and shapes, see Reverb's\n    documentation for details on how they are used.\n\n    Args:\n      environment_spec: A `specs.EnvironmentSpec` whose fields are nested\n        structures with leaf nodes that have `.shape` and `.dtype` attributes.\n        This should come from the environment that will be used to generate the\n        data inserted into the Reverb table.\n      extras_spec: A nested structure with leaf nodes that have `.shape` and\n        `.dtype` attributes. The structure (and shapes/dtypes) of this must be\n        the same as the `extras` passed into `ReverbAdder.add`.\n      sequence_length: An optional integer representing the expected length of\n        sequences that will be added to replay.\n\n    Returns:\n      A `Step` whose leaf nodes are `tf.TensorSpec` objects.\n    \"\"\"\n\n    def add_time_dim(paths: Iterable[str], spec: tf.TensorSpec):\n      return tf.TensorSpec(\n          shape=(sequence_length, *spec.shape),\n          dtype=spec.dtype,\n          name='/'.join(str(p) for p in paths))\n\n    trajectory_env_spec, trajectory_extras_spec = tree.map_structure_with_path(\n        add_time_dim, (environment_spec, extras_spec))\n\n    trajectory_spec = base.Trajectory(\n        *trajectory_env_spec,\n        start_of_episode=tf.TensorSpec(\n            shape=(sequence_length,), dtype=tf.bool, name='start_of_episode'),\n        extras=trajectory_extras_spec)\n\n    return trajectory_spec",
  "def __init__(\n      self,\n      client: reverb.Client,\n      max_sequence_length: int,\n      delta_encoded: bool = False,\n      priority_fns: Optional[base.PriorityFnMapping] = None,\n      max_in_flight_items: int = 1,\n      padding_fn: Optional[_PaddingFn] = None,\n      # Deprecated kwargs.\n      chunk_length: Optional[int] = None,\n  ):\n    del chunk_length\n\n    super().__init__(\n        client=client,\n        max_sequence_length=max_sequence_length,\n        delta_encoded=delta_encoded,\n        priority_fns=priority_fns,\n        max_in_flight_items=max_in_flight_items,\n    )\n    self._padding_fn = padding_fn",
  "def add(\n      self,\n      action: types.NestedArray,\n      next_timestep: dm_env.TimeStep,\n      extras: types.NestedArray = (),\n  ):\n    if self._writer.episode_steps >= self._max_sequence_length - 1:\n      raise ValueError(\n          'The number of observations within the same episode will exceed '\n          'max_sequence_length with the addition of this transition.')\n\n    super().add(action, next_timestep, extras)",
  "def _write(self):\n    # This adder only writes at the end of the episode, see _write_last()\n    pass",
  "def _write_last(self):\n    if self._padding_fn is not None and self._writer.episode_steps < self._max_sequence_length:\n      history = self._writer.history\n      padding_step = dict(\n          observation=history['observation'],\n          action=history['action'],\n          reward=history['reward'],\n          discount=history['discount'],\n          extras=history.get('extras', ()))\n      # Get shapes and dtypes from the last element.\n      padding_step = tree.map_structure(\n          lambda col: self._padding_fn(col[-1].shape, col[-1].dtype),\n          padding_step)\n      padding_step['start_of_episode'] = False\n      while self._writer.episode_steps < self._max_sequence_length:\n        self._writer.append(padding_step)\n\n    trajectory = tree.map_structure(lambda x: x[:], self._writer.history)\n\n    # Pack the history into a base.Step structure and get numpy converted\n    # variant for priotiy computation.\n    trajectory = base.Trajectory(**trajectory)\n\n    # Calculate the priority for this episode.\n    table_priorities = utils.calculate_priorities(self._priority_fns,\n                                                  trajectory)\n\n    # Create a prioritized item for each table.\n    for table_name, priority in table_priorities.items():\n      self._writer.create_item(table_name, priority, trajectory)\n      self._writer.flush(self._max_in_flight_items)",
  "def signature(cls,\n                environment_spec: specs.EnvironmentSpec,\n                extras_spec: types.NestedSpec = (),\n                sequence_length: Optional[int] = None):\n    \"\"\"This is a helper method for generating signatures for Reverb tables.\n\n    Signatures are useful for validating data types and shapes, see Reverb's\n    documentation for details on how they are used.\n\n    Args:\n      environment_spec: A `specs.EnvironmentSpec` whose fields are nested\n        structures with leaf nodes that have `.shape` and `.dtype` attributes.\n        This should come from the environment that will be used to generate the\n        data inserted into the Reverb table.\n      extras_spec: A nested structure with leaf nodes that have `.shape` and\n        `.dtype` attributes. The structure (and shapes/dtypes) of this must be\n        the same as the `extras` passed into `ReverbAdder.add`.\n      sequence_length: An optional integer representing the expected length of\n        sequences that will be added to replay.\n\n    Returns:\n      A `Step` whose leaf nodes are `tf.TensorSpec` objects.\n    \"\"\"\n\n    def add_time_dim(paths: Iterable[str], spec: tf.TensorSpec):\n      return tf.TensorSpec(\n          shape=(sequence_length, *spec.shape),\n          dtype=spec.dtype,\n          name='/'.join(str(p) for p in paths))\n\n    trajectory_env_spec, trajectory_extras_spec = tree.map_structure_with_path(\n        add_time_dim, (environment_spec, extras_spec))\n\n    trajectory_spec = base.Trajectory(\n        *trajectory_env_spec,\n        start_of_episode=tf.TensorSpec(\n            shape=(sequence_length,), dtype=tf.bool, name='start_of_episode'),\n        extras=trajectory_extras_spec)\n\n    return trajectory_spec",
  "def add_time_dim(paths: Iterable[str], spec: tf.TensorSpec):\n      return tf.TensorSpec(\n          shape=(sequence_length, *spec.shape),\n          dtype=spec.dtype,\n          name='/'.join(str(p) for p in paths))",
  "class StructuredAdder(adders_base.Adder):\n  \"\"\"Generic Adder which writes to Reverb using Reverb's `StructuredWriter`.\n\n  The StructuredAdder is a thin wrapper around Reverb's `StructuredWriter` and\n  its behaviour is determined by the configs to __init__. Much of the behaviour\n  provided by other Adders can be replicated using `StructuredAdder` but there\n  are a few noteworthy differences:\n\n   * The behaviour of `StructuredAdder` can be thought of as the union of all\n     its configs. This means that a single adder is capable of inserting items\n     of different structures into any number of tables WITHOUT any data\n     duplication. Other adders are only capable of writing items of the same\n     structure into multiple tables.\n   * The complete structure of the step must be known at construction time when\n     using the StructuredAdder. This is not the case for other Adders as they\n     allow the structure of the step to become expanded over time.\n   * The `StructuredAdder` assigns all items the same priority (1.0) as it does\n     not currently support custom priority computations.\n   * The StructuredAdder is completely generic and thus does not perform any\n     preprocessing on the data (e.g. cumulative rewards as done by the\n     NStepTransitionAdder) before writing it to Reverb. The user is instead\n     expected to perform preprocessing in the dataset pipeline on the learner.\n  \"\"\"\n\n  def __init__(self, client: reverb.Client, max_in_flight_items: int,\n               configs: Sequence[sw.Config], step_spec: Step):\n    \"\"\"Initialize a StructuredAdder instance.\n\n    Args:\n      client: A client to the Reverb backend.\n      max_in_flight_items: The maximum number of items allowed to be \"in flight\"\n        at the same time. See `block_until_num_items` in\n        `reverb.TrajectoryWriter.flush` for more info.\n      configs: Configurations defining the behaviour of the wrapped Reverb\n        writer.\n      step_spec: spec of the step that is going to be inserted in the Adder. It\n        can be created with `create_step_spec` using the environment spec and\n        and the extras spec.\n    \"\"\"\n\n    # We validate the configs by attempting to infer the signatures of all\n    # targeted tables.\n    for table, table_configs in itertools.groupby(configs, lambda c: c.table):\n      try:\n        sw.infer_signature(list(table_configs), step_spec)\n      except ValueError as e:\n        raise ValueError(\n            f'Received invalid configs for table {table}: {str(e)}') from e\n\n    self._client = client\n    self._configs = tuple(configs)\n    self._none_step: Step = tree.map_structure(lambda _: None, step_spec)\n    self._max_in_flight_items = max_in_flight_items\n\n    self._writer = None\n    self._writer_created_at = None\n\n  def __del__(self):\n    if self._writer is None:\n      return\n\n    # Try flush all appended data before closing to avoid loss of experience.\n    try:\n      self._writer.flush(0, timeout_ms=10_000)\n    except reverb.DeadlineExceededError as e:\n      logging.error(\n          'Timeout (10 s) exceeded when flushing the writer before '\n          'deleting it. Caught Reverb exception: %s', str(e))\n\n  def _make_step(self, **kwargs) -> Step:\n    \"\"\"Complete the step with None in the missing positions.\"\"\"\n    return Step(**{**self._none_step._asdict(), **kwargs})\n\n  @property\n  def configs(self):\n    return self._configs\n\n  def reset(self, timeout_ms: Optional[int] = None):\n    \"\"\"Marks the active episode as completed and flushes pending items.\"\"\"\n    if self._writer is not None:\n      # Flush all pending items.\n      self._writer.end_episode(clear_buffers=True, timeout_ms=timeout_ms)\n\n      # Create a new writer unless the current one is too young.\n      # This is to reduce the relative overhead of creating a new Reverb writer.\n      if time.time() - self._writer_created_at > _RESET_WRITER_EVERY_SECONDS:\n        self._writer = None\n\n  def add_first(self, timestep: dm_env.TimeStep):\n    \"\"\"Record the first observation of an episode.\"\"\"\n    if not timestep.first():\n      raise ValueError(\n          'adder.add_first called with a timestep that was not the first of its'\n          'episode (i.e. one for which timestep.first() is not True)')\n\n    if self._writer is None:\n      self._writer = self._client.structured_writer(self._configs)\n      self._writer_created_at = time.time()\n\n    # Record the next observation but leave the history buffer row open by\n    # passing `partial_step=True`.\n    self._writer.append(\n        data=self._make_step(\n            observation=timestep.observation,\n            start_of_episode=timestep.first()),\n        partial_step=True)\n    self._writer.flush(self._max_in_flight_items)\n\n  def add(self,\n          action: types.NestedArray,\n          next_timestep: dm_env.TimeStep,\n          extras: types.NestedArray = ()):\n    \"\"\"Record an action and the following timestep.\"\"\"\n\n    if self._writer is None or not self._writer.step_is_open:\n      raise ValueError('adder.add_first must be called before adder.add.')\n\n    # Add the timestep to the buffer.\n    has_extras = (\n        len(extras) > 0 if isinstance(extras, Sized)  # pylint: disable=g-explicit-length-test\n        else extras is not None)\n\n    current_step = self._make_step(\n        action=action,\n        reward=next_timestep.reward,\n        discount=next_timestep.discount,\n        extras=extras if has_extras else self._none_step.extras)\n    self._writer.append(current_step)\n\n    # Record the next observation and write.\n    self._writer.append(\n        data=self._make_step(\n            observation=next_timestep.observation,\n            start_of_episode=next_timestep.first()),\n        partial_step=True)\n    self._writer.flush(self._max_in_flight_items)\n\n    if next_timestep.last():\n      # Complete the row by appending zeros to remaining open fields.\n      # TODO(b/183945808): remove this when fields are no longer expected to be\n      # of equal length on the learner side.\n      dummy_step = tree.map_structure(\n          lambda x: None if x is None else np.zeros_like(x), current_step)\n      self._writer.append(dummy_step)\n      self.reset()",
  "def create_step_spec(\n    environment_spec: specs.EnvironmentSpec, extras_spec: types.NestedSpec = ()\n) -> Step:\n  return Step(\n      *environment_spec,\n      start_of_episode=tf.TensorSpec([], tf.bool, 'start_of_episode'),\n      extras=extras_spec)",
  "def _last_n(n: int, step_spec: Step) -> Trajectory:\n  \"\"\"Constructs a sequence with the last n elements of all the Step fields.\"\"\"\n  return Trajectory(*tree.map_structure(lambda x: x[-n:], step_spec))",
  "def create_sequence_config(\n    step_spec: Step,\n    sequence_length: int,\n    period: int,\n    table: str = reverb_base.DEFAULT_PRIORITY_TABLE,\n    end_of_episode_behavior: EndBehavior = EndBehavior.TRUNCATE,\n    sequence_pattern: Callable[[int, Step], Trajectory] = _last_n,\n) -> List[sw.Config]:\n  \"\"\"Generates configs that produces the same behaviour as `SequenceAdder`.\n\n  NOTE! ZERO_PAD is not supported as the same behaviour can be achieved by\n  writing with TRUNCATE and then adding padding in the dataset pipeline on the\n  learner.\n\n  Args:\n    step_spec: The full structure of the data which will be appended to the\n      Reverb `StructuredWriter` in each step. Please use `create_step_spec` to\n      create `step_spec`.\n    sequence_length: The number of steps that each trajectory should span.\n    period: The period with which we add sequences. If less than\n      sequence_length, overlapping sequences are added. If equal to\n      sequence_length, sequences are exactly non-overlapping.\n    table: Name of the Reverb table to write items to. Defaults to the default\n      Acme table.\n    end_of_episode_behavior: Determines how sequences at the end of the episode\n      are handled (default `EndOfEpisodeBehavior.TRUNCATE`). See the docstring\n      of `EndOfEpisodeBehavior` for more information.\n    sequence_pattern: Transformation to obtain a sequence given the length\n      and the shape of the step.\n\n  Returns:\n    A list of configs for `StructuredAdder` to produce the described behaviour.\n\n  Raises:\n    ValueError: If sequence_length is <= 0.\n    NotImplementedError: If `end_of_episod_behavior` is `ZERO_PAD`.\n  \"\"\"\n  if sequence_length <= 0:\n    raise ValueError(f'sequence_length must be > 0 but got {sequence_length}.')\n\n  if end_of_episode_behavior == EndBehavior.ZERO_PAD:\n    raise NotImplementedError(\n        'Zero-padding is not supported. Please use TRUNCATE instead.')\n\n  if end_of_episode_behavior == EndBehavior.CONTINUE:\n    raise NotImplementedError('Merging episodes is not supported.')\n\n  def _sequence_pattern(n: int) -> sw.Pattern:\n    return sw.pattern_from_transform(step_spec,\n                                     lambda step: sequence_pattern(n, step))\n\n  # The base config is considered for all but the last step in the episode. No\n  # trajectories are created for the first `sequence_step-1` steps and then a\n  # new trajectory is inserted every `period` steps.\n  base_config = sw.create_config(\n      pattern=_sequence_pattern(sequence_length),\n      table=table,\n      conditions=[\n          sw.Condition.step_index() >= sequence_length - 1,\n          sw.Condition.step_index() % period == (sequence_length - 1) % period,\n      ])\n\n  end_of_episode_configs = []\n  if end_of_episode_behavior == EndBehavior.WRITE:\n    # Simply write a trajectory in exactly the same way as the base config. The\n    # only difference here is that we ALWAYS create a trajectory even if it\n    # doesn't align with the `period`. The exceptions to the rule are episodes\n    # that are shorter than `sequence_length` steps which are completely\n    # ignored.\n    config = sw.create_config(\n        pattern=_sequence_pattern(sequence_length),\n        table=table,\n        conditions=[\n            sw.Condition.is_end_episode(),\n            sw.Condition.step_index() >= sequence_length - 1,\n        ])\n    end_of_episode_configs.append(config)\n  elif end_of_episode_behavior == EndBehavior.TRUNCATE:\n    # The first trajectory is written at step index `sequence_length - 1` and\n    # then written every `period` step. This means that the\n    # `step_index % period` will always be equal to the below value everytime a\n    # trajectory is written.\n    target = (sequence_length - 1) % period\n\n    # When the episode ends we still want to capture the steps that has been\n    # appended since the last item was created. We do this by creating a config\n    # for all `step_index % period`, except `target`, and condition these\n    # configs so that they only are triggered when `end_episode` is called.\n    for x in range(period):\n      # When the last step is aligned with the period of the inserts then no\n      # action is required as the item was already generated by `base_config`.\n      if x == target:\n        continue\n\n      # If we were to pad the trajectory then we'll need to continue adding\n      # padding until `step_index % period` is equal to `target` again. We can\n      # exploit this relation by conditioning the config to only be applied for\n      # a single value of `step_index % period`. This constraint means that we\n      # can infer the number of padding steps required until the next write\n      # would have occurred if the episode didn't end.\n      #\n      # Now if we assume that the padding instead is added on the dataset (or\n      # the trajectory is simply truncated) then we can infer from the above\n      # that the number of real steps in this padded trajectory will be the\n      # difference between `sequence_length` and number of pad steps.\n      num_pad_steps = (target - x) % period\n      unpadded_length = sequence_length - num_pad_steps\n\n      config = sw.create_config(\n          pattern=_sequence_pattern(unpadded_length),\n          table=table,\n          conditions=[\n              sw.Condition.is_end_episode(),\n              sw.Condition.step_index() % period == x,\n              sw.Condition.step_index() >= sequence_length,\n          ])\n      end_of_episode_configs.append(config)\n\n    # The above configs will capture the \"remainder\" of any episode that is at\n    # least `sequence_length` steps long. However, if the entire episode is\n    # shorter than `sequence_length` then data might still be lost. We avoid\n    # this by simply creating `sequence_length-1` configs that capture the last\n    # `x` steps iff the entire episode is `x` steps long.\n    for x in range(1, sequence_length):\n      config = sw.create_config(\n          pattern=_sequence_pattern(x),\n          table=table,\n          conditions=[\n              sw.Condition.is_end_episode(),\n              sw.Condition.step_index() == x - 1,\n          ])\n      end_of_episode_configs.append(config)\n  else:\n    raise ValueError(\n        f'Unexpected `end_of_episod_behavior`: {end_of_episode_behavior}')\n\n  return [base_config] + end_of_episode_configs",
  "def create_n_step_transition_config(\n    step_spec: Step,\n    n_step: int,\n    table: str = reverb_base.DEFAULT_PRIORITY_TABLE) -> List[sw.Config]:\n  \"\"\"Generates configs that replicates the behaviour of NStepTransitionAdder.\n\n  Please see the docstring of NStepTransitionAdder for more details.\n\n  NOTE! In contrast to NStepTransitionAdder, the trajectories written by the\n  `StructuredWriter` does not include the precomputed cumulative reward and\n  discounts. Instead the trajectory includes the raw rewards and discounts\n  required to comptute these values.\n\n  Args:\n    step_spec: The full structure of the data which will be appended to the\n      Reverb `StructuredWriter` in each step. Please use `create_step_spec` to\n      create `step_spec`.\n    n_step: The \"N\" in N-step transition. See the class docstring for the\n      precise definition of what an N-step transition is. `n_step` must be at\n      least 1, in which case we use the standard one-step transition, i.e. (s_t,\n      a_t, r_t, d_t, s_t+1, e_t).\n    table: Name of the Reverb table to write items to. Defaults to the default\n      Acme table.\n\n  Returns:\n    A list of configs for `StructuredAdder` to produce the described behaviour.\n  \"\"\"\n\n  def _make_pattern(n: int):\n    ref_step = sw.create_reference_step(step_spec)\n\n    get_first = lambda x: x[-(n + 1):-n]\n    get_all = lambda x: x[-(n + 1):-1]\n    get_first_and_last = lambda x: x[-(n + 1)::n]\n\n    tmap = tree.map_structure\n\n    # We use the exact same structure as we done when writing sequences except\n    # we trim the number of steps in each sub tree. This has the benefit that\n    # the postprocessing used to transform these items into N-step transition\n    # structures (cumulative rewards and discounts etc.) can be applied on\n    # full sequence items as well. The only difference being that the latter is\n    # more wasteful than the trimmed down version we write here.\n    return Trajectory(\n        observation=tmap(get_first_and_last, ref_step.observation),\n        action=tmap(get_first, ref_step.action),\n        reward=tmap(get_all, ref_step.reward),\n        discount=tmap(get_all, ref_step.discount),\n        start_of_episode=tmap(get_first, ref_step.start_of_episode),\n        extras=tmap(get_first, ref_step.extras))\n\n  # At the start of the episodes we'll add shorter transitions.\n  start_of_episode_configs = []\n  for n in range(1, n_step):\n    config = sw.create_config(\n        pattern=_make_pattern(n),\n        table=table,\n        conditions=[\n            sw.Condition.step_index() == n,\n        ],\n    )\n    start_of_episode_configs.append(config)\n\n  # During all other steps we'll add a full N-step transition.\n  base_config = sw.create_config(pattern=_make_pattern(n_step), table=table)\n\n  # When the episode ends we'll add shorter transitions.\n  end_of_episode_configs = []\n  for n in range(n_step - 1, 0, -1):\n    config = sw.create_config(\n        pattern=_make_pattern(n),\n        table=table,\n        conditions=[\n            sw.Condition.is_end_episode(),\n            # If the entire episode is shorter than n_step then the episode\n            # start configs will already create an item that covers all the\n            # steps so we add this filter here to avoid adding it again.\n            sw.Condition.step_index() != n,\n        ],\n    )\n    end_of_episode_configs.append(config)\n\n  return start_of_episode_configs + [base_config] + end_of_episode_configs",
  "def __init__(self, client: reverb.Client, max_in_flight_items: int,\n               configs: Sequence[sw.Config], step_spec: Step):\n    \"\"\"Initialize a StructuredAdder instance.\n\n    Args:\n      client: A client to the Reverb backend.\n      max_in_flight_items: The maximum number of items allowed to be \"in flight\"\n        at the same time. See `block_until_num_items` in\n        `reverb.TrajectoryWriter.flush` for more info.\n      configs: Configurations defining the behaviour of the wrapped Reverb\n        writer.\n      step_spec: spec of the step that is going to be inserted in the Adder. It\n        can be created with `create_step_spec` using the environment spec and\n        and the extras spec.\n    \"\"\"\n\n    # We validate the configs by attempting to infer the signatures of all\n    # targeted tables.\n    for table, table_configs in itertools.groupby(configs, lambda c: c.table):\n      try:\n        sw.infer_signature(list(table_configs), step_spec)\n      except ValueError as e:\n        raise ValueError(\n            f'Received invalid configs for table {table}: {str(e)}') from e\n\n    self._client = client\n    self._configs = tuple(configs)\n    self._none_step: Step = tree.map_structure(lambda _: None, step_spec)\n    self._max_in_flight_items = max_in_flight_items\n\n    self._writer = None\n    self._writer_created_at = None",
  "def __del__(self):\n    if self._writer is None:\n      return\n\n    # Try flush all appended data before closing to avoid loss of experience.\n    try:\n      self._writer.flush(0, timeout_ms=10_000)\n    except reverb.DeadlineExceededError as e:\n      logging.error(\n          'Timeout (10 s) exceeded when flushing the writer before '\n          'deleting it. Caught Reverb exception: %s', str(e))",
  "def _make_step(self, **kwargs) -> Step:\n    \"\"\"Complete the step with None in the missing positions.\"\"\"\n    return Step(**{**self._none_step._asdict(), **kwargs})",
  "def configs(self):\n    return self._configs",
  "def reset(self, timeout_ms: Optional[int] = None):\n    \"\"\"Marks the active episode as completed and flushes pending items.\"\"\"\n    if self._writer is not None:\n      # Flush all pending items.\n      self._writer.end_episode(clear_buffers=True, timeout_ms=timeout_ms)\n\n      # Create a new writer unless the current one is too young.\n      # This is to reduce the relative overhead of creating a new Reverb writer.\n      if time.time() - self._writer_created_at > _RESET_WRITER_EVERY_SECONDS:\n        self._writer = None",
  "def add_first(self, timestep: dm_env.TimeStep):\n    \"\"\"Record the first observation of an episode.\"\"\"\n    if not timestep.first():\n      raise ValueError(\n          'adder.add_first called with a timestep that was not the first of its'\n          'episode (i.e. one for which timestep.first() is not True)')\n\n    if self._writer is None:\n      self._writer = self._client.structured_writer(self._configs)\n      self._writer_created_at = time.time()\n\n    # Record the next observation but leave the history buffer row open by\n    # passing `partial_step=True`.\n    self._writer.append(\n        data=self._make_step(\n            observation=timestep.observation,\n            start_of_episode=timestep.first()),\n        partial_step=True)\n    self._writer.flush(self._max_in_flight_items)",
  "def add(self,\n          action: types.NestedArray,\n          next_timestep: dm_env.TimeStep,\n          extras: types.NestedArray = ()):\n    \"\"\"Record an action and the following timestep.\"\"\"\n\n    if self._writer is None or not self._writer.step_is_open:\n      raise ValueError('adder.add_first must be called before adder.add.')\n\n    # Add the timestep to the buffer.\n    has_extras = (\n        len(extras) > 0 if isinstance(extras, Sized)  # pylint: disable=g-explicit-length-test\n        else extras is not None)\n\n    current_step = self._make_step(\n        action=action,\n        reward=next_timestep.reward,\n        discount=next_timestep.discount,\n        extras=extras if has_extras else self._none_step.extras)\n    self._writer.append(current_step)\n\n    # Record the next observation and write.\n    self._writer.append(\n        data=self._make_step(\n            observation=next_timestep.observation,\n            start_of_episode=next_timestep.first()),\n        partial_step=True)\n    self._writer.flush(self._max_in_flight_items)\n\n    if next_timestep.last():\n      # Complete the row by appending zeros to remaining open fields.\n      # TODO(b/183945808): remove this when fields are no longer expected to be\n      # of equal length on the learner side.\n      dummy_step = tree.map_structure(\n          lambda x: None if x is None else np.zeros_like(x), current_step)\n      self._writer.append(dummy_step)\n      self.reset()",
  "def _sequence_pattern(n: int) -> sw.Pattern:\n    return sw.pattern_from_transform(step_spec,\n                                     lambda step: sequence_pattern(n, step))",
  "def _make_pattern(n: int):\n    ref_step = sw.create_reference_step(step_spec)\n\n    get_first = lambda x: x[-(n + 1):-n]\n    get_all = lambda x: x[-(n + 1):-1]\n    get_first_and_last = lambda x: x[-(n + 1)::n]\n\n    tmap = tree.map_structure\n\n    # We use the exact same structure as we done when writing sequences except\n    # we trim the number of steps in each sub tree. This has the benefit that\n    # the postprocessing used to transform these items into N-step transition\n    # structures (cumulative rewards and discounts etc.) can be applied on\n    # full sequence items as well. The only difference being that the latter is\n    # more wasteful than the trimmed down version we write here.\n    return Trajectory(\n        observation=tmap(get_first_and_last, ref_step.observation),\n        action=tmap(get_first, ref_step.action),\n        reward=tmap(get_all, ref_step.reward),\n        discount=tmap(get_all, ref_step.discount),\n        start_of_episode=tmap(get_first, ref_step.start_of_episode),\n        extras=tmap(get_first, ref_step.extras))",
  "def make_trajectory(observations):\n  \"\"\"Make a simple trajectory from a sequence of observations.\n\n  Arguments:\n    observations: a sequence of observations.\n\n  Returns:\n    a tuple (first, steps) where first contains the initial dm_env.TimeStep\n    object and steps contains a list of (action, step) tuples. The length of\n    steps is given by episode_length.\n  \"\"\"\n  first = dm_env.restart(observations[0])\n  middle = [(0, dm_env.transition(reward=0.0, observation=observation))\n            for observation in observations[1:-1]]\n  last = (0, dm_env.termination(reward=0.0, observation=observations[-1]))\n  return first, middle + [last]",
  "def make_sequence(observations):\n  \"\"\"Create a sequence of timesteps of the form `first, [second, ..., last]`.\"\"\"\n  first, steps = make_trajectory(observations)\n  observation = first.observation\n  sequence = []\n  start_of_episode = True\n  for action, timestep in steps:\n    extras = ()\n    sequence.append((observation, action, timestep.reward, timestep.discount,\n                     start_of_episode, extras))\n    observation = timestep.observation\n    start_of_episode = False\n  sequence.append((observation, 0, 0.0, 0.0, False, ()))\n  return sequence",
  "def _numeric_to_spec(x: Union[float, int, np.ndarray]):\n  if isinstance(x, np.ndarray):\n    return specs.Array(shape=x.shape, dtype=x.dtype)\n  elif isinstance(x, (float, int)):\n    return specs.Array(shape=(), dtype=type(x))\n  else:\n    raise ValueError(f'Unsupported numeric: {type(x)}')",
  "def get_specs(step):\n  \"\"\"Infer spec from an example step.\"\"\"\n  env_spec = tree.map_structure(\n      _numeric_to_spec,\n      specs.EnvironmentSpec(\n          observations=step[1].observation,\n          actions=step[0],\n          rewards=step[1].reward,\n          discounts=step[1].discount))\n\n  has_extras = len(step) == 3\n  if has_extras:\n    extras_spec = tree.map_structure(_numeric_to_spec, step[2])\n  else:\n    extras_spec = ()\n\n  return env_spec, extras_spec",
  "class AdderTestMixin(absltest.TestCase):\n  \"\"\"A helper mixin for testing Reverb adders.\n\n  Note that any test inheriting from this mixin must also inherit from something\n  that provides the Python unittest assert methods.\n  \"\"\"\n\n  server: reverb.Server\n  client: reverb.Client\n\n  @classmethod\n  def setUpClass(cls):\n    super().setUpClass()\n\n    replay_table = reverb.Table.queue(adders.DEFAULT_PRIORITY_TABLE, 1000)\n    cls.server = reverb.Server([replay_table])\n    cls.client = reverb.Client(f'localhost:{cls.server.port}')\n\n  def tearDown(self):\n    self.client.reset(adders.DEFAULT_PRIORITY_TABLE)\n    super().tearDown()\n\n  @classmethod\n  def tearDownClass(cls):\n    cls.server.stop()\n    super().tearDownClass()\n\n  def num_episodes(self):\n    info = self.client.server_info(1)[adders.DEFAULT_PRIORITY_TABLE]\n    return info.num_episodes\n\n  def num_items(self):\n    info = self.client.server_info(1)[adders.DEFAULT_PRIORITY_TABLE]\n    return info.current_size\n\n  def items(self):\n    sampler = self.client.sample(\n        table=adders.DEFAULT_PRIORITY_TABLE,\n        num_samples=self.num_items(),\n        emit_timesteps=False)\n    return [sample.data for sample in sampler]  # pytype: disable=attribute-error\n\n  def run_test_adder(\n      self,\n      adder: adders_base.Adder,\n      first: dm_env.TimeStep,\n      steps: Sequence[Step],\n      expected_items: Sequence[Any],\n      signature: types.NestedSpec,\n      pack_expected_items: bool = False,\n      stack_sequence_fields: bool = True,\n      repeat_episode_times: int = 1,\n      end_behavior: adders.EndBehavior = adders.EndBehavior.ZERO_PAD,\n      item_transform: Optional[Callable[[Sequence[np.ndarray]], Any]] = None):\n    \"\"\"Runs a unit test case for the adder.\n\n    Args:\n      adder: The instance of `Adder` that is being tested.\n      first: The first `dm_env.TimeStep` that is used to call\n        `Adder.add_first()`.\n      steps: A sequence of (action, timestep) tuples that are passed to\n        `Adder.add()`.\n      expected_items: The sequence of items that are expected to be created\n        by calling the adder's `add_first()` method on `first` and `add()` on\n        all of the elements in `steps`.\n      signature: Signature that written items must be compatible with.\n      pack_expected_items: Deprecated and not used. If true the expected items\n        are given unpacked and need to be packed in a list before comparison.\n      stack_sequence_fields: Whether to stack the sequence fields of the\n        expected items before comparing to the observed items. Usually False\n        for transition adders and True for both episode and sequence adders.\n      repeat_episode_times: How many times to run an episode.\n      end_behavior: How end of episode should be handled.\n      item_transform: Transformation of item simulating the work done by the\n        dataset pipeline on the learner in a real setup.\n    \"\"\"\n\n    del pack_expected_items\n\n    if not steps:\n      raise ValueError('At least one step must be given.')\n\n    has_extras = len(steps[0]) == 3\n    for _ in range(repeat_episode_times):\n      # Add all the data up to the final step.\n      adder.add_first(first)\n      for step in steps[:-1]:\n        action, ts = step[0], step[1]\n\n        if has_extras:\n          extras = step[2]\n        else:\n          extras = ()\n\n        adder.add(action, next_timestep=ts, extras=extras)\n\n      # Add the final step.\n      adder.add(*steps[-1])\n\n    # Force run the destructor to trigger the flushing of all pending items.\n    getattr(adder, '__del__', lambda: None)()\n\n    # Ending the episode should close the writer. No new writer should yet have\n    # been created as it is constructed lazily.\n    if end_behavior is not adders.EndBehavior.CONTINUE:\n      self.assertEqual(self.num_episodes(), repeat_episode_times)\n\n    # Make sure our expected and observed data match.\n    observed_items = self.items()\n\n    # Check matching number of items.\n    self.assertEqual(len(expected_items), len(observed_items))\n\n    # Check items are matching according to numpy's almost_equal.\n    for expected_item, observed_item in zip(expected_items, observed_items):\n      if stack_sequence_fields:\n        expected_item = tree_utils.stack_sequence_fields(expected_item)\n\n      # Apply the transformation which would be done by the dataset in a real\n      # setup.\n      if item_transform:\n        observed_item = item_transform(observed_item)\n\n      tree.map_structure(np.testing.assert_array_almost_equal,\n                         tree.flatten(expected_item),\n                         tree.flatten(observed_item))\n\n    # Make sure the signature matches was is being written by Reverb.\n    def _check_signature(spec: tf.TensorSpec, value: np.ndarray):\n      self.assertTrue(spec.is_compatible_with(tf.convert_to_tensor(value)))\n\n    # Check that it is possible to unpack observed using the signature.\n    for item in observed_items:\n      tree.map_structure(_check_signature, tree.flatten(signature),\n                         tree.flatten(item))",
  "def setUpClass(cls):\n    super().setUpClass()\n\n    replay_table = reverb.Table.queue(adders.DEFAULT_PRIORITY_TABLE, 1000)\n    cls.server = reverb.Server([replay_table])\n    cls.client = reverb.Client(f'localhost:{cls.server.port}')",
  "def tearDown(self):\n    self.client.reset(adders.DEFAULT_PRIORITY_TABLE)\n    super().tearDown()",
  "def tearDownClass(cls):\n    cls.server.stop()\n    super().tearDownClass()",
  "def num_episodes(self):\n    info = self.client.server_info(1)[adders.DEFAULT_PRIORITY_TABLE]\n    return info.num_episodes",
  "def num_items(self):\n    info = self.client.server_info(1)[adders.DEFAULT_PRIORITY_TABLE]\n    return info.current_size",
  "def items(self):\n    sampler = self.client.sample(\n        table=adders.DEFAULT_PRIORITY_TABLE,\n        num_samples=self.num_items(),\n        emit_timesteps=False)\n    return [sample.data for sample in sampler]",
  "def run_test_adder(\n      self,\n      adder: adders_base.Adder,\n      first: dm_env.TimeStep,\n      steps: Sequence[Step],\n      expected_items: Sequence[Any],\n      signature: types.NestedSpec,\n      pack_expected_items: bool = False,\n      stack_sequence_fields: bool = True,\n      repeat_episode_times: int = 1,\n      end_behavior: adders.EndBehavior = adders.EndBehavior.ZERO_PAD,\n      item_transform: Optional[Callable[[Sequence[np.ndarray]], Any]] = None):\n    \"\"\"Runs a unit test case for the adder.\n\n    Args:\n      adder: The instance of `Adder` that is being tested.\n      first: The first `dm_env.TimeStep` that is used to call\n        `Adder.add_first()`.\n      steps: A sequence of (action, timestep) tuples that are passed to\n        `Adder.add()`.\n      expected_items: The sequence of items that are expected to be created\n        by calling the adder's `add_first()` method on `first` and `add()` on\n        all of the elements in `steps`.\n      signature: Signature that written items must be compatible with.\n      pack_expected_items: Deprecated and not used. If true the expected items\n        are given unpacked and need to be packed in a list before comparison.\n      stack_sequence_fields: Whether to stack the sequence fields of the\n        expected items before comparing to the observed items. Usually False\n        for transition adders and True for both episode and sequence adders.\n      repeat_episode_times: How many times to run an episode.\n      end_behavior: How end of episode should be handled.\n      item_transform: Transformation of item simulating the work done by the\n        dataset pipeline on the learner in a real setup.\n    \"\"\"\n\n    del pack_expected_items\n\n    if not steps:\n      raise ValueError('At least one step must be given.')\n\n    has_extras = len(steps[0]) == 3\n    for _ in range(repeat_episode_times):\n      # Add all the data up to the final step.\n      adder.add_first(first)\n      for step in steps[:-1]:\n        action, ts = step[0], step[1]\n\n        if has_extras:\n          extras = step[2]\n        else:\n          extras = ()\n\n        adder.add(action, next_timestep=ts, extras=extras)\n\n      # Add the final step.\n      adder.add(*steps[-1])\n\n    # Force run the destructor to trigger the flushing of all pending items.\n    getattr(adder, '__del__', lambda: None)()\n\n    # Ending the episode should close the writer. No new writer should yet have\n    # been created as it is constructed lazily.\n    if end_behavior is not adders.EndBehavior.CONTINUE:\n      self.assertEqual(self.num_episodes(), repeat_episode_times)\n\n    # Make sure our expected and observed data match.\n    observed_items = self.items()\n\n    # Check matching number of items.\n    self.assertEqual(len(expected_items), len(observed_items))\n\n    # Check items are matching according to numpy's almost_equal.\n    for expected_item, observed_item in zip(expected_items, observed_items):\n      if stack_sequence_fields:\n        expected_item = tree_utils.stack_sequence_fields(expected_item)\n\n      # Apply the transformation which would be done by the dataset in a real\n      # setup.\n      if item_transform:\n        observed_item = item_transform(observed_item)\n\n      tree.map_structure(np.testing.assert_array_almost_equal,\n                         tree.flatten(expected_item),\n                         tree.flatten(observed_item))\n\n    # Make sure the signature matches was is being written by Reverb.\n    def _check_signature(spec: tf.TensorSpec, value: np.ndarray):\n      self.assertTrue(spec.is_compatible_with(tf.convert_to_tensor(value)))\n\n    # Check that it is possible to unpack observed using the signature.\n    for item in observed_items:\n      tree.map_structure(_check_signature, tree.flatten(signature),\n                         tree.flatten(item))",
  "def _check_signature(spec: tf.TensorSpec, value: np.ndarray):\n      self.assertTrue(spec.is_compatible_with(tf.convert_to_tensor(value)))",
  "class StructuredAdderTest(test_utils.AdderTestMixin, parameterized.TestCase):\n\n  @parameterized.named_parameters(*test_cases.BASE_TEST_CASES_FOR_SEQUENCE_ADDER\n                                 )\n  def test_sequence_adder(self,\n                          sequence_length: int,\n                          period: int,\n                          first,\n                          steps,\n                          expected_sequences,\n                          end_behavior: adders.EndBehavior,\n                          repeat_episode_times: int = 1):\n\n    env_spec, extras_spec = test_utils.get_specs(steps[0])\n    step_spec = structured.create_step_spec(env_spec, extras_spec)\n\n    should_pad_trajectory = end_behavior == adders.EndBehavior.ZERO_PAD\n\n    def _maybe_zero_pad(flat_trajectory):\n      trajectory = tree.unflatten_as(step_spec, flat_trajectory)\n\n      if not should_pad_trajectory:\n        return trajectory\n\n      padding_length = sequence_length - flat_trajectory[0].shape[0]\n      if padding_length == 0:\n        return trajectory\n\n      padding = tree.map_structure(\n          lambda x: np.zeros([padding_length, *x.shape[1:]], x.dtype),\n          trajectory)\n\n      return tree.map_structure(lambda *x: np.concatenate(x), trajectory,\n                                padding)\n\n    # The StructuredAdder does not support adding padding steps as we assume\n    # that the padding will be added on the learner side.\n    if end_behavior == adders.EndBehavior.ZERO_PAD:\n      end_behavior = adders.EndBehavior.TRUNCATE\n\n    configs = structured.create_sequence_config(\n        step_spec=step_spec,\n        sequence_length=sequence_length,\n        period=period,\n        end_of_episode_behavior=end_behavior)\n    adder = structured.StructuredAdder(\n        client=self.client,\n        max_in_flight_items=0,\n        configs=configs,\n        step_spec=step_spec)\n\n    super().run_test_adder(\n        adder=adder,\n        first=first,\n        steps=steps,\n        expected_items=expected_sequences,\n        repeat_episode_times=repeat_episode_times,\n        end_behavior=end_behavior,\n        item_transform=_maybe_zero_pad,\n        signature=sw.infer_signature(configs, step_spec))\n\n  @parameterized.named_parameters(*test_cases.TEST_CASES_FOR_TRANSITION_ADDER)\n  def test_transition_adder(self, n_step: int, additional_discount: float,\n                            first: dm_env.TimeStep,\n                            steps: Sequence[dm_env.TimeStep],\n                            expected_transitions: Sequence[types.Transition]):\n\n    env_spec, extras_spec = test_utils.get_specs(steps[0])\n    step_spec = structured.create_step_spec(env_spec, extras_spec)\n\n    def _as_n_step_transition(flat_trajectory):\n      trajectory = tree.unflatten_as(step_spec, flat_trajectory)\n\n      rewards, discount = _compute_cumulative_quantities(\n          rewards=trajectory.reward,\n          discounts=trajectory.discount,\n          additional_discount=additional_discount,\n          n_step=tree.flatten(trajectory.reward)[0].shape[0])\n\n      tmap = tree.map_structure\n      return types.Transition(\n          observation=tmap(lambda x: x[0], trajectory.observation),\n          action=tmap(lambda x: x[0], trajectory.action),\n          reward=rewards,\n          discount=discount,\n          next_observation=tmap(lambda x: x[-1], trajectory.observation),\n          extras=tmap(lambda x: x[0], trajectory.extras))\n\n    configs = structured.create_n_step_transition_config(\n        step_spec=step_spec, n_step=n_step)\n\n    adder = structured.StructuredAdder(\n        client=self.client,\n        max_in_flight_items=0,\n        configs=configs,\n        step_spec=step_spec)\n\n    super().run_test_adder(\n        adder=adder,\n        first=first,\n        steps=steps,\n        expected_items=expected_transitions,\n        stack_sequence_fields=False,\n        item_transform=_as_n_step_transition,\n        signature=sw.infer_signature(configs, step_spec))",
  "def _compute_cumulative_quantities(rewards: types.NestedArray,\n                                   discounts: types.NestedArray,\n                                   additional_discount: float, n_step: int):\n  \"\"\"Stolen from TransitionAdder.\"\"\"\n\n  # Give the same tree structure to the n-step return accumulator,\n  # n-step discount accumulator, and self.discount, so that they can be\n  # iterated in parallel using tree.map_structure.\n  rewards, discounts, self_discount = tree_utils.broadcast_structures(\n      rewards, discounts, additional_discount)\n  flat_rewards = tree.flatten(rewards)\n  flat_discounts = tree.flatten(discounts)\n  flat_self_discount = tree.flatten(self_discount)\n\n  # Copy total_discount as it is otherwise read-only.\n  total_discount = [np.copy(a[0]) for a in flat_discounts]\n\n  # Broadcast n_step_return to have the broadcasted shape of\n  # reward * discount.\n  n_step_return = [\n      np.copy(np.broadcast_to(r[0],\n                              np.broadcast(r[0], d).shape))\n      for r, d in zip(flat_rewards, total_discount)\n  ]\n\n  # NOTE: total_discount will have one less self_discount applied to it than\n  # the value of self._n_step. This is so that when the learner/update uses\n  # an additional discount we don't apply it twice. Inside the following loop\n  # we will apply this right before summing up the n_step_return.\n  for i in range(1, n_step):\n    for nsr, td, r, d, sd in zip(n_step_return, total_discount, flat_rewards,\n                                 flat_discounts, flat_self_discount):\n      # Equivalent to: `total_discount *= self._discount`.\n      td *= sd\n      # Equivalent to: `n_step_return += reward[i] * total_discount`.\n      nsr += r[i] * td\n      # Equivalent to: `total_discount *= discount[i]`.\n      td *= d[i]\n\n  n_step_return = tree.unflatten_as(rewards, n_step_return)\n  total_discount = tree.unflatten_as(rewards, total_discount)\n  return n_step_return, total_discount",
  "def test_sequence_adder(self,\n                          sequence_length: int,\n                          period: int,\n                          first,\n                          steps,\n                          expected_sequences,\n                          end_behavior: adders.EndBehavior,\n                          repeat_episode_times: int = 1):\n\n    env_spec, extras_spec = test_utils.get_specs(steps[0])\n    step_spec = structured.create_step_spec(env_spec, extras_spec)\n\n    should_pad_trajectory = end_behavior == adders.EndBehavior.ZERO_PAD\n\n    def _maybe_zero_pad(flat_trajectory):\n      trajectory = tree.unflatten_as(step_spec, flat_trajectory)\n\n      if not should_pad_trajectory:\n        return trajectory\n\n      padding_length = sequence_length - flat_trajectory[0].shape[0]\n      if padding_length == 0:\n        return trajectory\n\n      padding = tree.map_structure(\n          lambda x: np.zeros([padding_length, *x.shape[1:]], x.dtype),\n          trajectory)\n\n      return tree.map_structure(lambda *x: np.concatenate(x), trajectory,\n                                padding)\n\n    # The StructuredAdder does not support adding padding steps as we assume\n    # that the padding will be added on the learner side.\n    if end_behavior == adders.EndBehavior.ZERO_PAD:\n      end_behavior = adders.EndBehavior.TRUNCATE\n\n    configs = structured.create_sequence_config(\n        step_spec=step_spec,\n        sequence_length=sequence_length,\n        period=period,\n        end_of_episode_behavior=end_behavior)\n    adder = structured.StructuredAdder(\n        client=self.client,\n        max_in_flight_items=0,\n        configs=configs,\n        step_spec=step_spec)\n\n    super().run_test_adder(\n        adder=adder,\n        first=first,\n        steps=steps,\n        expected_items=expected_sequences,\n        repeat_episode_times=repeat_episode_times,\n        end_behavior=end_behavior,\n        item_transform=_maybe_zero_pad,\n        signature=sw.infer_signature(configs, step_spec))",
  "def test_transition_adder(self, n_step: int, additional_discount: float,\n                            first: dm_env.TimeStep,\n                            steps: Sequence[dm_env.TimeStep],\n                            expected_transitions: Sequence[types.Transition]):\n\n    env_spec, extras_spec = test_utils.get_specs(steps[0])\n    step_spec = structured.create_step_spec(env_spec, extras_spec)\n\n    def _as_n_step_transition(flat_trajectory):\n      trajectory = tree.unflatten_as(step_spec, flat_trajectory)\n\n      rewards, discount = _compute_cumulative_quantities(\n          rewards=trajectory.reward,\n          discounts=trajectory.discount,\n          additional_discount=additional_discount,\n          n_step=tree.flatten(trajectory.reward)[0].shape[0])\n\n      tmap = tree.map_structure\n      return types.Transition(\n          observation=tmap(lambda x: x[0], trajectory.observation),\n          action=tmap(lambda x: x[0], trajectory.action),\n          reward=rewards,\n          discount=discount,\n          next_observation=tmap(lambda x: x[-1], trajectory.observation),\n          extras=tmap(lambda x: x[0], trajectory.extras))\n\n    configs = structured.create_n_step_transition_config(\n        step_spec=step_spec, n_step=n_step)\n\n    adder = structured.StructuredAdder(\n        client=self.client,\n        max_in_flight_items=0,\n        configs=configs,\n        step_spec=step_spec)\n\n    super().run_test_adder(\n        adder=adder,\n        first=first,\n        steps=steps,\n        expected_items=expected_transitions,\n        stack_sequence_fields=False,\n        item_transform=_as_n_step_transition,\n        signature=sw.infer_signature(configs, step_spec))",
  "def _maybe_zero_pad(flat_trajectory):\n      trajectory = tree.unflatten_as(step_spec, flat_trajectory)\n\n      if not should_pad_trajectory:\n        return trajectory\n\n      padding_length = sequence_length - flat_trajectory[0].shape[0]\n      if padding_length == 0:\n        return trajectory\n\n      padding = tree.map_structure(\n          lambda x: np.zeros([padding_length, *x.shape[1:]], x.dtype),\n          trajectory)\n\n      return tree.map_structure(lambda *x: np.concatenate(x), trajectory,\n                                padding)",
  "def _as_n_step_transition(flat_trajectory):\n      trajectory = tree.unflatten_as(step_spec, flat_trajectory)\n\n      rewards, discount = _compute_cumulative_quantities(\n          rewards=trajectory.reward,\n          discounts=trajectory.discount,\n          additional_discount=additional_discount,\n          n_step=tree.flatten(trajectory.reward)[0].shape[0])\n\n      tmap = tree.map_structure\n      return types.Transition(\n          observation=tmap(lambda x: x[0], trajectory.observation),\n          action=tmap(lambda x: x[0], trajectory.action),\n          reward=rewards,\n          discount=discount,\n          next_observation=tmap(lambda x: x[-1], trajectory.observation),\n          extras=tmap(lambda x: x[0], trajectory.extras))",
  "class EpisodeAdderTest(test_utils.AdderTestMixin, parameterized.TestCase):\n\n  @parameterized.parameters(2, 10, 50)\n  def test_adder(self, max_sequence_length):\n    adder = adders.EpisodeAdder(self.client, max_sequence_length)\n\n    # Create a simple trajectory to add.\n    observations = range(max_sequence_length)\n    first, steps = test_utils.make_trajectory(observations)\n\n    expected_episode = test_utils.make_sequence(observations)\n    self.run_test_adder(\n        adder=adder,\n        first=first,\n        steps=steps,\n        expected_items=[expected_episode],\n        signature=adder.signature(*test_utils.get_specs(steps[0])))\n\n  @parameterized.parameters(2, 10, 50)\n  def test_max_sequence_length(self, max_sequence_length):\n    adder = adders.EpisodeAdder(self.client, max_sequence_length)\n\n    first, steps = test_utils.make_trajectory(range(max_sequence_length + 1))\n    adder.add_first(first)\n    for action, step in steps[:-1]:\n      adder.add(action, step)\n\n    # We should have max_sequence_length-1 timesteps that have been written,\n    # where the -1 is due to the dangling observation (ie we have actually\n    # seen max_sequence_length observations).\n    self.assertEqual(self.num_items(), 0)\n\n    # Adding one more step should raise an error.\n    with self.assertRaises(ValueError):\n      action, step = steps[-1]\n      adder.add(action, step)\n\n    # Since the last insert failed it should not affect the internal state.\n    self.assertEqual(self.num_items(), 0)\n\n  @parameterized.parameters((2, 1), (10, 2), (50, 5))\n  def test_padding(self, max_sequence_length, padding):\n    adder = adders.EpisodeAdder(\n        self.client,\n        max_sequence_length + padding,\n        padding_fn=np.zeros)\n\n    # Create a simple trajectory to add.\n    observations = range(max_sequence_length)\n    first, steps = test_utils.make_trajectory(observations)\n\n    expected_episode = test_utils.make_sequence(observations)\n    for _ in range(padding):\n      expected_episode.append((0, 0, 0.0, 0.0, False, ()))\n\n    self.run_test_adder(\n        adder=adder,\n        first=first,\n        steps=steps,\n        expected_items=[expected_episode],\n        signature=adder.signature(*test_utils.get_specs(steps[0])))\n\n  @parameterized.parameters((2, 1), (10, 2), (50, 5))\n  def test_nonzero_padding(self, max_sequence_length, padding):\n    adder = adders.EpisodeAdder(\n        self.client,\n        max_sequence_length + padding,\n        padding_fn=lambda s, d: np.zeros(s, d) - 1)\n\n    # Create a simple trajectory to add.\n    observations = range(max_sequence_length)\n    first, steps = test_utils.make_trajectory(observations)\n\n    expected_episode = test_utils.make_sequence(observations)\n    for _ in range(padding):\n      expected_episode.append((-1, -1, -1.0, -1.0, False, ()))\n\n    self.run_test_adder(\n        adder=adder,\n        first=first,\n        steps=steps,\n        expected_items=[expected_episode],\n        signature=adder.signature(*test_utils.get_specs(steps[0])))",
  "def test_adder(self, max_sequence_length):\n    adder = adders.EpisodeAdder(self.client, max_sequence_length)\n\n    # Create a simple trajectory to add.\n    observations = range(max_sequence_length)\n    first, steps = test_utils.make_trajectory(observations)\n\n    expected_episode = test_utils.make_sequence(observations)\n    self.run_test_adder(\n        adder=adder,\n        first=first,\n        steps=steps,\n        expected_items=[expected_episode],\n        signature=adder.signature(*test_utils.get_specs(steps[0])))",
  "def test_max_sequence_length(self, max_sequence_length):\n    adder = adders.EpisodeAdder(self.client, max_sequence_length)\n\n    first, steps = test_utils.make_trajectory(range(max_sequence_length + 1))\n    adder.add_first(first)\n    for action, step in steps[:-1]:\n      adder.add(action, step)\n\n    # We should have max_sequence_length-1 timesteps that have been written,\n    # where the -1 is due to the dangling observation (ie we have actually\n    # seen max_sequence_length observations).\n    self.assertEqual(self.num_items(), 0)\n\n    # Adding one more step should raise an error.\n    with self.assertRaises(ValueError):\n      action, step = steps[-1]\n      adder.add(action, step)\n\n    # Since the last insert failed it should not affect the internal state.\n    self.assertEqual(self.num_items(), 0)",
  "def test_padding(self, max_sequence_length, padding):\n    adder = adders.EpisodeAdder(\n        self.client,\n        max_sequence_length + padding,\n        padding_fn=np.zeros)\n\n    # Create a simple trajectory to add.\n    observations = range(max_sequence_length)\n    first, steps = test_utils.make_trajectory(observations)\n\n    expected_episode = test_utils.make_sequence(observations)\n    for _ in range(padding):\n      expected_episode.append((0, 0, 0.0, 0.0, False, ()))\n\n    self.run_test_adder(\n        adder=adder,\n        first=first,\n        steps=steps,\n        expected_items=[expected_episode],\n        signature=adder.signature(*test_utils.get_specs(steps[0])))",
  "def test_nonzero_padding(self, max_sequence_length, padding):\n    adder = adders.EpisodeAdder(\n        self.client,\n        max_sequence_length + padding,\n        padding_fn=lambda s, d: np.zeros(s, d) - 1)\n\n    # Create a simple trajectory to add.\n    observations = range(max_sequence_length)\n    first, steps = test_utils.make_trajectory(observations)\n\n    expected_episode = test_utils.make_sequence(observations)\n    for _ in range(padding):\n      expected_episode.append((-1, -1, -1.0, -1.0, False, ()))\n\n    self.run_test_adder(\n        adder=adder,\n        first=first,\n        steps=steps,\n        expected_items=[expected_episode],\n        signature=adder.signature(*test_utils.get_specs(steps[0])))",
  "class NStepTransitionAdder(base.ReverbAdder):\n  \"\"\"An N-step transition adder.\n\n  This will buffer a sequence of N timesteps in order to form a single N-step\n  transition which is added to reverb for future retrieval.\n\n  For N=1 the data added to replay will be a standard one-step transition which\n  takes the form:\n\n        (s_t, a_t, r_t, d_t, s_{t+1}, e_t)\n\n  where:\n\n    s_t = state observation at time t\n    a_t = the action taken from s_t\n    r_t = reward ensuing from action a_t\n    d_t = environment discount ensuing from action a_t. This discount is\n        applied to future rewards after r_t.\n    e_t [Optional] = extra data that the agent persists in replay.\n\n  For N greater than 1, transitions are of the form:\n\n        (s_t, a_t, R_{t:t+n}, D_{t:t+n}, s_{t+N}, e_t),\n\n  where:\n\n    s_t = State (observation) at time t.\n    a_t = Action taken from state s_t.\n    g = the additional discount, used by the agent to discount future returns.\n    R_{t:t+n} = N-step discounted return, i.e. accumulated over N rewards:\n          R_{t:t+n} := r_t + g * d_t * r_{t+1} + ...\n                           + g^{n-1} * d_t * ... * d_{t+n-2} * r_{t+n-1}.\n    D_{t:t+n}: N-step product of agent discounts g_i and environment\n      \"discounts\" d_i.\n          D_{t:t+n} := g^{n-1} * d_{t} * ... * d_{t+n-1},\n      For most environments d_i is 1 for all steps except the last,\n      i.e. it is the episode termination signal.\n    s_{t+n}: The \"arrival\" state, i.e. the state at time t+n.\n    e_t [Optional]: A nested structure of any 'extras' the user wishes to add.\n\n  Notes:\n    - At the beginning and end of episodes, shorter transitions are added.\n      That is, at the beginning of the episode, it will add:\n            (s_0 -> s_1), (s_0 -> s_2), ..., (s_0 -> s_n), (s_1 -> s_{n+1})\n\n      And at the end of the episode, it will add:\n            (s_{T-n+1} -> s_T), (s_{T-n+2} -> s_T), ... (s_{T-1} -> s_T).\n    - We add the *first* `extra` of each transition, not the *last*, i.e.\n        if extras are provided, we get e_t, not e_{t+n}.\n  \"\"\"\n\n  def __init__(\n      self,\n      client: reverb.Client,\n      n_step: int,\n      discount: float,\n      *,\n      priority_fns: Optional[base.PriorityFnMapping] = None,\n      max_in_flight_items: int = 5,\n  ):\n    \"\"\"Creates an N-step transition adder.\n\n    Args:\n      client: A `reverb.Client` to send the data to replay through.\n      n_step: The \"N\" in N-step transition. See the class docstring for the\n        precise definition of what an N-step transition is. `n_step` must be at\n        least 1, in which case we use the standard one-step transition, i.e.\n        (s_t, a_t, r_t, d_t, s_t+1, e_t).\n      discount: Discount factor to apply. This corresponds to the agent's\n        discount in the class docstring.\n      priority_fns: See docstring for BaseAdder.\n      max_in_flight_items: The maximum number of items allowed to be \"in flight\"\n        at the same time. See `block_until_num_items` in\n        `reverb.TrajectoryWriter.flush` for more info.\n\n    Raises:\n      ValueError: If n_step is less than 1.\n    \"\"\"\n    # Makes the additional discount a float32, which means that it will be\n    # upcast if rewards/discounts are float64 and left alone otherwise.\n    self.n_step = n_step\n    self._discount = tree.map_structure(np.float32, discount)\n    self._first_idx = 0\n    self._last_idx = 0\n\n    super().__init__(\n        client=client,\n        max_sequence_length=n_step + 1,\n        priority_fns=priority_fns,\n        max_in_flight_items=max_in_flight_items)\n\n  def add(self, *args, **kwargs):\n    # Increment the indices for the start and end of the window for computing\n    # n-step returns.\n    if self._writer.episode_steps >= self.n_step:\n      self._first_idx += 1\n    self._last_idx += 1\n\n    super().add(*args, **kwargs)\n\n  def reset(self):  # pytype: disable=signature-mismatch  # overriding-parameter-count-checks\n    super().reset()\n    self._first_idx = 0\n    self._last_idx = 0\n\n  @property\n  def _n_step(self) -> int:\n    \"\"\"Effective n-step, which may vary at starts and ends of episodes.\"\"\"\n    return self._last_idx - self._first_idx\n\n  def _write(self):\n    # Convenient getters for use in tree operations.\n    get_first = lambda x: x[self._first_idx]\n    get_last = lambda x: x[self._last_idx]\n    # Note: this getter is meant to be used on a TrajectoryWriter.history to\n    # obtain its numpy values.\n    get_all_np = lambda x: x[self._first_idx:self._last_idx].numpy()\n\n    # Get the state, action, next_state, as well as possibly extras for the\n    # transition that is about to be written.\n    history = self._writer.history\n    s, a = tree.map_structure(get_first,\n                              (history['observation'], history['action']))\n    s_ = tree.map_structure(get_last, history['observation'])\n\n    # Maybe get extras to add to the transition later.\n    if 'extras' in history:\n      extras = tree.map_structure(get_first, history['extras'])\n\n    # Note: at the beginning of an episode we will add the initial N-1\n    # transitions (of size 1, 2, ...) and at the end of an episode (when\n    # called from write_last) we will write the final transitions of size (N,\n    # N-1, ...). See the Note in the docstring.\n    # Get numpy view of the steps to be fed into the priority functions.\n    reward, discount = tree.map_structure(\n        get_all_np, (history['reward'], history['discount']))\n\n    # Compute discounted return and geometric discount over n steps.\n    n_step_return, total_discount = self._compute_cumulative_quantities(\n        reward, discount)\n\n    # Append the computed n-step return and total discount.\n    # Note: if this call to _write() is within a call to _write_last(), then\n    # this is the only data being appended and so it is not a partial append.\n    self._writer.append(\n        dict(n_step_return=n_step_return, total_discount=total_discount),\n        partial_step=self._writer.episode_steps <= self._last_idx)\n    # This should be done immediately after self._writer.append so the history\n    # includes the recently appended data.\n    history = self._writer.history\n\n    # Form the n-step transition by using the following:\n    # the first observation and action in the buffer, along with the cumulative\n    # reward and discount computed above.\n    n_step_return, total_discount = tree.map_structure(\n        lambda x: x[-1], (history['n_step_return'], history['total_discount']))\n    transition = types.Transition(\n        observation=s,\n        action=a,\n        reward=n_step_return,\n        discount=total_discount,\n        next_observation=s_,\n        extras=(extras if 'extras' in history else ()))\n\n    # Calculate the priority for this transition.\n    table_priorities = utils.calculate_priorities(self._priority_fns,\n                                                  transition)\n\n    # Insert the transition into replay along with its priority.\n    for table, priority in table_priorities.items():\n      self._writer.create_item(\n          table=table, priority=priority, trajectory=transition)\n      self._writer.flush(self._max_in_flight_items)\n\n  def _write_last(self):\n    # Write the remaining shorter transitions by alternating writing and\n    # incrementingfirst_idx. Note that last_idx will no longer be incremented\n    # once we're in this method's scope.\n    self._first_idx += 1\n    while self._first_idx < self._last_idx:\n      self._write()\n      self._first_idx += 1\n\n  def _compute_cumulative_quantities(\n      self, rewards: types.NestedArray, discounts: types.NestedArray\n  ) -> Tuple[types.NestedArray, types.NestedArray]:\n\n    # Give the same tree structure to the n-step return accumulator,\n    # n-step discount accumulator, and self.discount, so that they can be\n    # iterated in parallel using tree.map_structure.\n    rewards, discounts, self_discount = tree_utils.broadcast_structures(\n        rewards, discounts, self._discount)\n    flat_rewards = tree.flatten(rewards)\n    flat_discounts = tree.flatten(discounts)\n    flat_self_discount = tree.flatten(self_discount)\n\n    # Copy total_discount as it is otherwise read-only.\n    total_discount = [np.copy(a[0]) for a in flat_discounts]\n\n    # Broadcast n_step_return to have the broadcasted shape of\n    # reward * discount.\n    n_step_return = [\n        np.copy(np.broadcast_to(r[0],\n                                np.broadcast(r[0], d).shape))\n        for r, d in zip(flat_rewards, total_discount)\n    ]\n\n    # NOTE: total_discount will have one less self_discount applied to it than\n    # the value of self._n_step. This is so that when the learner/update uses\n    # an additional discount we don't apply it twice. Inside the following loop\n    # we will apply this right before summing up the n_step_return.\n    for i in range(1, self._n_step):\n      for nsr, td, r, d, sd in zip(n_step_return, total_discount, flat_rewards,\n                                   flat_discounts, flat_self_discount):\n        # Equivalent to: `total_discount *= self._discount`.\n        td *= sd\n        # Equivalent to: `n_step_return += reward[i] * total_discount`.\n        nsr += r[i] * td\n        # Equivalent to: `total_discount *= discount[i]`.\n        td *= d[i]\n\n    n_step_return = tree.unflatten_as(rewards, n_step_return)\n    total_discount = tree.unflatten_as(rewards, total_discount)\n    return n_step_return, total_discount\n\n  # TODO(bshahr): make this into a standalone method. Class methods should be\n  # used as alternative constructors or when modifying some global state,\n  # neither of which is done here.\n  @classmethod\n  def signature(cls,\n                environment_spec: specs.EnvironmentSpec,\n                extras_spec: types.NestedSpec = ()):\n\n    # This function currently assumes that self._discount is a scalar.\n    # If it ever becomes a nested structure and/or a np.ndarray, this method\n    # will need to know its structure / shape. This is because the signature\n    # discount shape is the environment's discount shape and this adder's\n    # discount shape broadcasted together. Also, the reward shape is this\n    # signature discount shape broadcasted together with the environment\n    # reward shape. As long as self._discount is a scalar, it will not affect\n    # either the signature discount shape nor the signature reward shape, so we\n    # can ignore it.\n\n    rewards_spec, step_discounts_spec = tree_utils.broadcast_structures(\n        environment_spec.rewards, environment_spec.discounts)\n    rewards_spec = tree.map_structure(_broadcast_specs, rewards_spec,\n                                      step_discounts_spec)\n    step_discounts_spec = tree.map_structure(copy.deepcopy, step_discounts_spec)\n\n    transition_spec = types.Transition(\n        environment_spec.observations,\n        environment_spec.actions,\n        rewards_spec,\n        step_discounts_spec,\n        environment_spec.observations,  # next_observation\n        extras_spec)\n\n    return tree.map_structure_with_path(base.spec_like_to_tensor_spec,\n                                        transition_spec)",
  "def _broadcast_specs(*args: specs.Array) -> specs.Array:\n  \"\"\"Like np.broadcast, but for specs.Array.\n\n  Args:\n    *args: one or more specs.Array instances.\n\n  Returns:\n    A specs.Array with the broadcasted shape and dtype of the specs in *args.\n  \"\"\"\n  bc_info = np.broadcast(*tuple(a.generate_value() for a in args))\n  dtype = np.result_type(*tuple(a.dtype for a in args))\n  return specs.Array(shape=bc_info.shape, dtype=dtype)",
  "def __init__(\n      self,\n      client: reverb.Client,\n      n_step: int,\n      discount: float,\n      *,\n      priority_fns: Optional[base.PriorityFnMapping] = None,\n      max_in_flight_items: int = 5,\n  ):\n    \"\"\"Creates an N-step transition adder.\n\n    Args:\n      client: A `reverb.Client` to send the data to replay through.\n      n_step: The \"N\" in N-step transition. See the class docstring for the\n        precise definition of what an N-step transition is. `n_step` must be at\n        least 1, in which case we use the standard one-step transition, i.e.\n        (s_t, a_t, r_t, d_t, s_t+1, e_t).\n      discount: Discount factor to apply. This corresponds to the agent's\n        discount in the class docstring.\n      priority_fns: See docstring for BaseAdder.\n      max_in_flight_items: The maximum number of items allowed to be \"in flight\"\n        at the same time. See `block_until_num_items` in\n        `reverb.TrajectoryWriter.flush` for more info.\n\n    Raises:\n      ValueError: If n_step is less than 1.\n    \"\"\"\n    # Makes the additional discount a float32, which means that it will be\n    # upcast if rewards/discounts are float64 and left alone otherwise.\n    self.n_step = n_step\n    self._discount = tree.map_structure(np.float32, discount)\n    self._first_idx = 0\n    self._last_idx = 0\n\n    super().__init__(\n        client=client,\n        max_sequence_length=n_step + 1,\n        priority_fns=priority_fns,\n        max_in_flight_items=max_in_flight_items)",
  "def add(self, *args, **kwargs):\n    # Increment the indices for the start and end of the window for computing\n    # n-step returns.\n    if self._writer.episode_steps >= self.n_step:\n      self._first_idx += 1\n    self._last_idx += 1\n\n    super().add(*args, **kwargs)",
  "def reset(self):  # pytype: disable=signature-mismatch  # overriding-parameter-count-checks\n    super().reset()\n    self._first_idx = 0\n    self._last_idx = 0",
  "def _n_step(self) -> int:\n    \"\"\"Effective n-step, which may vary at starts and ends of episodes.\"\"\"\n    return self._last_idx - self._first_idx",
  "def _write(self):\n    # Convenient getters for use in tree operations.\n    get_first = lambda x: x[self._first_idx]\n    get_last = lambda x: x[self._last_idx]\n    # Note: this getter is meant to be used on a TrajectoryWriter.history to\n    # obtain its numpy values.\n    get_all_np = lambda x: x[self._first_idx:self._last_idx].numpy()\n\n    # Get the state, action, next_state, as well as possibly extras for the\n    # transition that is about to be written.\n    history = self._writer.history\n    s, a = tree.map_structure(get_first,\n                              (history['observation'], history['action']))\n    s_ = tree.map_structure(get_last, history['observation'])\n\n    # Maybe get extras to add to the transition later.\n    if 'extras' in history:\n      extras = tree.map_structure(get_first, history['extras'])\n\n    # Note: at the beginning of an episode we will add the initial N-1\n    # transitions (of size 1, 2, ...) and at the end of an episode (when\n    # called from write_last) we will write the final transitions of size (N,\n    # N-1, ...). See the Note in the docstring.\n    # Get numpy view of the steps to be fed into the priority functions.\n    reward, discount = tree.map_structure(\n        get_all_np, (history['reward'], history['discount']))\n\n    # Compute discounted return and geometric discount over n steps.\n    n_step_return, total_discount = self._compute_cumulative_quantities(\n        reward, discount)\n\n    # Append the computed n-step return and total discount.\n    # Note: if this call to _write() is within a call to _write_last(), then\n    # this is the only data being appended and so it is not a partial append.\n    self._writer.append(\n        dict(n_step_return=n_step_return, total_discount=total_discount),\n        partial_step=self._writer.episode_steps <= self._last_idx)\n    # This should be done immediately after self._writer.append so the history\n    # includes the recently appended data.\n    history = self._writer.history\n\n    # Form the n-step transition by using the following:\n    # the first observation and action in the buffer, along with the cumulative\n    # reward and discount computed above.\n    n_step_return, total_discount = tree.map_structure(\n        lambda x: x[-1], (history['n_step_return'], history['total_discount']))\n    transition = types.Transition(\n        observation=s,\n        action=a,\n        reward=n_step_return,\n        discount=total_discount,\n        next_observation=s_,\n        extras=(extras if 'extras' in history else ()))\n\n    # Calculate the priority for this transition.\n    table_priorities = utils.calculate_priorities(self._priority_fns,\n                                                  transition)\n\n    # Insert the transition into replay along with its priority.\n    for table, priority in table_priorities.items():\n      self._writer.create_item(\n          table=table, priority=priority, trajectory=transition)\n      self._writer.flush(self._max_in_flight_items)",
  "def _write_last(self):\n    # Write the remaining shorter transitions by alternating writing and\n    # incrementingfirst_idx. Note that last_idx will no longer be incremented\n    # once we're in this method's scope.\n    self._first_idx += 1\n    while self._first_idx < self._last_idx:\n      self._write()\n      self._first_idx += 1",
  "def _compute_cumulative_quantities(\n      self, rewards: types.NestedArray, discounts: types.NestedArray\n  ) -> Tuple[types.NestedArray, types.NestedArray]:\n\n    # Give the same tree structure to the n-step return accumulator,\n    # n-step discount accumulator, and self.discount, so that they can be\n    # iterated in parallel using tree.map_structure.\n    rewards, discounts, self_discount = tree_utils.broadcast_structures(\n        rewards, discounts, self._discount)\n    flat_rewards = tree.flatten(rewards)\n    flat_discounts = tree.flatten(discounts)\n    flat_self_discount = tree.flatten(self_discount)\n\n    # Copy total_discount as it is otherwise read-only.\n    total_discount = [np.copy(a[0]) for a in flat_discounts]\n\n    # Broadcast n_step_return to have the broadcasted shape of\n    # reward * discount.\n    n_step_return = [\n        np.copy(np.broadcast_to(r[0],\n                                np.broadcast(r[0], d).shape))\n        for r, d in zip(flat_rewards, total_discount)\n    ]\n\n    # NOTE: total_discount will have one less self_discount applied to it than\n    # the value of self._n_step. This is so that when the learner/update uses\n    # an additional discount we don't apply it twice. Inside the following loop\n    # we will apply this right before summing up the n_step_return.\n    for i in range(1, self._n_step):\n      for nsr, td, r, d, sd in zip(n_step_return, total_discount, flat_rewards,\n                                   flat_discounts, flat_self_discount):\n        # Equivalent to: `total_discount *= self._discount`.\n        td *= sd\n        # Equivalent to: `n_step_return += reward[i] * total_discount`.\n        nsr += r[i] * td\n        # Equivalent to: `total_discount *= discount[i]`.\n        td *= d[i]\n\n    n_step_return = tree.unflatten_as(rewards, n_step_return)\n    total_discount = tree.unflatten_as(rewards, total_discount)\n    return n_step_return, total_discount",
  "def signature(cls,\n                environment_spec: specs.EnvironmentSpec,\n                extras_spec: types.NestedSpec = ()):\n\n    # This function currently assumes that self._discount is a scalar.\n    # If it ever becomes a nested structure and/or a np.ndarray, this method\n    # will need to know its structure / shape. This is because the signature\n    # discount shape is the environment's discount shape and this adder's\n    # discount shape broadcasted together. Also, the reward shape is this\n    # signature discount shape broadcasted together with the environment\n    # reward shape. As long as self._discount is a scalar, it will not affect\n    # either the signature discount shape nor the signature reward shape, so we\n    # can ignore it.\n\n    rewards_spec, step_discounts_spec = tree_utils.broadcast_structures(\n        environment_spec.rewards, environment_spec.discounts)\n    rewards_spec = tree.map_structure(_broadcast_specs, rewards_spec,\n                                      step_discounts_spec)\n    step_discounts_spec = tree.map_structure(copy.deepcopy, step_discounts_spec)\n\n    transition_spec = types.Transition(\n        environment_spec.observations,\n        environment_spec.actions,\n        rewards_spec,\n        step_discounts_spec,\n        environment_spec.observations,  # next_observation\n        extras_spec)\n\n    return tree.map_structure_with_path(base.spec_like_to_tensor_spec,\n                                        transition_spec)",
  "def add_batch_dim(nest: types.NestedTensor) -> types.NestedTensor:\n  \"\"\"Adds a batch dimension to each leaf of a nested structure of Tensors.\"\"\"\n  return tree.map_structure(lambda x: tf.expand_dims(x, axis=0), nest)",
  "def squeeze_batch_dim(nest: types.NestedTensor) -> types.NestedTensor:\n  \"\"\"Squeezes out a batch dimension from each leaf of a nested structure.\"\"\"\n  return tree.map_structure(lambda x: tf.squeeze(x, axis=0), nest)",
  "def batch_concat(inputs: types.NestedTensor) -> tf.Tensor:\n  \"\"\"Concatenate a collection of Tensors while preserving the batch dimension.\n\n  This takes a potentially nested collection of tensors, flattens everything\n  but the batch (first) dimension, and concatenates along the resulting data\n  (second) dimension.\n\n  Args:\n    inputs: a tensor or nested collection of tensors.\n\n  Returns:\n    A concatenated tensor which maintains the batch dimension but concatenates\n    all other data along the flattened second dimension.\n  \"\"\"\n  flat_leaves = tree.map_structure(snt.Flatten(), inputs)\n  return tf.concat(tree.flatten(flat_leaves), axis=-1)",
  "def batch_to_sequence(data: types.NestedTensor) -> types.NestedTensor:\n  \"\"\"Converts data between sequence-major and batch-major format.\"\"\"\n  return tree.map_structure(\n      lambda t: tf.transpose(t, [1, 0] + list(range(2, t.shape.rank))), data)",
  "def tile_tensor(tensor: tf.Tensor, multiple: int) -> tf.Tensor:\n  \"\"\"Tiles `multiple` copies of `tensor` along a new leading axis.\"\"\"\n  rank = len(tensor.shape)\n  multiples = tf.constant([multiple] + [1] * rank, dtype=tf.int32)\n  expanded_tensor = tf.expand_dims(tensor, axis=0)\n  return tf.tile(expanded_tensor, multiples)",
  "def tile_nested(inputs: types.NestedTensor,\n                multiple: int) -> types.NestedTensor:\n  \"\"\"Tiles tensors in a nested structure along a new leading axis.\"\"\"\n  tile = functools.partial(tile_tensor, multiple=multiple)\n  return tree.map_structure(tile, inputs)",
  "def create_variables(\n    network: snt.Module,\n    input_spec: List[Union[types.NestedSpec, tf.TensorSpec]],\n) -> Optional[tf.TensorSpec]:\n  \"\"\"Builds the network with dummy inputs to create the necessary variables.\n\n  Args:\n    network: Sonnet Module whose variables are to be created.\n    input_spec: list of input specs to the network. The length of this list\n      should match the number of arguments expected by `network`.\n\n  Returns:\n    output_spec: only returns an output spec if the output is a tf.Tensor, else\n        it doesn't return anything (None); e.g. if the output is a\n        tfp.distributions.Distribution.\n  \"\"\"\n  # Create a dummy observation with no batch dimension.\n  dummy_input = zeros_like(input_spec)\n\n  # If we have an RNNCore the hidden state will be an additional input.\n  if isinstance(network, snt.RNNCore):\n    initial_state = squeeze_batch_dim(network.initial_state(1))\n    dummy_input += [initial_state]\n\n  # Forward pass of the network which will create variables as a side effect.\n  dummy_output = network(*add_batch_dim(dummy_input))\n\n  # Evaluate the input signature by converting the dummy input into a\n  # TensorSpec. We then save the signature as a property of the network. This is\n  # done so that we can later use it when creating snapshots. We do this here\n  # because the snapshot code may not have access to the precise form of the\n  # inputs.\n  input_signature = tree.map_structure(\n      lambda t: tf.TensorSpec((None,) + t.shape, t.dtype), dummy_input)\n  network._input_signature = input_signature  # pylint: disable=protected-access\n\n  def spec(output):\n    # If the output is not a Tensor, return None as spec is ill-defined.\n    if not isinstance(output, tf.Tensor):\n      return None\n    # If this is not a scalar Tensor, make sure to squeeze out the batch dim.\n    if tf.rank(output) > 0:\n      output = squeeze_batch_dim(output)\n    return tf.TensorSpec(output.shape, output.dtype)\n\n  return tree.map_structure(spec, dummy_output)",
  "class TransformationWrapper(snt.Module):\n  \"\"\"Helper class for to_sonnet_module.\n\n  This wraps arbitrary Tensor-valued callables as a Sonnet module.\n  A use case for this is in agent code that could take either a trainable\n  sonnet module or a hard-coded function as its policy. By wrapping a hard-coded\n  policy with this class, the agent can then treat it as if it were a Sonnet\n  module. This removes the need for \"if is_hard_coded:...\" branches, which you'd\n  otherwise need if e.g. calling get_variables() on the policy.\n  \"\"\"\n\n  def __init__(self,\n               transformation: types.TensorValuedCallable,\n               name: Optional[str] = None):\n    super().__init__(name=name)\n    self._transformation = transformation\n\n  def __call__(self, *args, **kwargs):\n    return self._transformation(*args, **kwargs)",
  "def to_sonnet_module(\n    transformation: types.TensorValuedCallable\n    ) -> snt.Module:\n  \"\"\"Convert a tensor transformation to a Sonnet Module.\n\n  Args:\n    transformation: A Callable that takes one or more (nested) Tensors, and\n      returns one or more (nested) Tensors.\n\n  Returns:\n    A Sonnet Module that wraps the transformation.\n  \"\"\"\n\n  if isinstance(transformation, snt.Module):\n    return transformation\n\n  module = TransformationWrapper(transformation)\n\n  # Wrap the module to allow it to return an empty variable tuple.\n  return snt.allow_empty_variables(module)",
  "def to_numpy(nest: types.NestedTensor) -> types.NestedArray:\n  \"\"\"Converts a nest of Tensors to a nest of numpy arrays.\"\"\"\n  return tree.map_structure(lambda x: x.numpy(), nest)",
  "def to_numpy_squeeze(nest: types.NestedTensor, axis=0) -> types.NestedArray:\n  \"\"\"Converts a nest of Tensors to a nest of numpy arrays and squeeze axis.\"\"\"\n  return tree.map_structure(lambda x: tf.squeeze(x, axis=axis).numpy(), nest)",
  "def zeros_like(nest: types.Nest) -> types.NestedTensor:\n  \"\"\"Given a nest of array-like objects, returns similarly nested tf.zeros.\"\"\"\n  return tree.map_structure(lambda x: tf.zeros(x.shape, x.dtype), nest)",
  "def spec(output):\n    # If the output is not a Tensor, return None as spec is ill-defined.\n    if not isinstance(output, tf.Tensor):\n      return None\n    # If this is not a scalar Tensor, make sure to squeeze out the batch dim.\n    if tf.rank(output) > 0:\n      output = squeeze_batch_dim(output)\n    return tf.TensorSpec(output.shape, output.dtype)",
  "def __init__(self,\n               transformation: types.TensorValuedCallable,\n               name: Optional[str] = None):\n    super().__init__(name=name)\n    self._transformation = transformation",
  "def __call__(self, *args, **kwargs):\n    return self._transformation(*args, **kwargs)",
  "class TFSaveable(abc.ABC):\n  \"\"\"An interface for objects that expose their checkpointable TF state.\"\"\"\n\n  @property\n  @abc.abstractmethod\n  def state(self) -> Mapping[str, Checkpointable]:\n    \"\"\"Returns TensorFlow checkpointable state.\"\"\"",
  "class Checkpointer:\n  \"\"\"Convenience class for periodically checkpointing.\n\n  This can be used to checkpoint any object with trackable state (e.g.\n  tensorflow variables or modules); see tf.train.Checkpoint for\n  details. Objects inheriting from tf.train.experimental.PythonState can also\n  be checkpointed.\n\n  Typically people use Checkpointer to make sure that they can correctly recover\n  from a machine going down during learning. For more permanent storage of self-\n  contained \"networks\" see the Snapshotter object.\n\n  Usage example:\n\n  ```python\n  model = snt.Linear(10)\n  checkpointer = Checkpointer(objects_to_save={'model': model})\n\n  for _ in range(100):\n    # ...\n    checkpointer.save()\n  ```\n  \"\"\"\n\n  def __init__(\n      self,\n      objects_to_save: Mapping[str, Union[Checkpointable, core.Saveable]],\n      *,\n      directory: str = '~/acme/',\n      subdirectory: str = 'default',\n      time_delta_minutes: float = 10.0,\n      enable_checkpointing: bool = True,\n      add_uid: bool = True,\n      max_to_keep: int = 1,\n      checkpoint_ttl_seconds: Optional[int] = _DEFAULT_CHECKPOINT_TTL,\n      keep_checkpoint_every_n_hours: Optional[int] = None,\n  ):\n    \"\"\"Builds the saver object.\n\n    Args:\n      objects_to_save: Mapping specifying what to checkpoint.\n      directory: Which directory to put the checkpoint in.\n      subdirectory: Sub-directory to use (e.g. if multiple checkpoints are being\n        saved).\n      time_delta_minutes: How often to save the checkpoint, in minutes.\n      enable_checkpointing: whether to checkpoint or not.\n      add_uid: If True adds a UID to the checkpoint path, see\n        `paths.get_unique_id()` for how this UID is generated.\n      max_to_keep: The maximum number of checkpoints to keep.\n      checkpoint_ttl_seconds: TTL (time to leave) in seconds for checkpoints.\n      keep_checkpoint_every_n_hours: keep_checkpoint_every_n_hours passed to\n        tf.train.CheckpointManager.\n    \"\"\"\n\n    # Convert `Saveable` objects to TF `Checkpointable` first, if necessary.\n    def to_ckptable(x: Union[Checkpointable, core.Saveable]) -> Checkpointable:\n      if isinstance(x, core.Saveable):\n        return SaveableAdapter(x)\n      return x\n\n    objects_to_save = {k: to_ckptable(v) for k, v in objects_to_save.items()}\n\n    self._time_delta_minutes = time_delta_minutes\n    self._last_saved = 0.\n    self._enable_checkpointing = enable_checkpointing\n    self._checkpoint_manager = None\n\n    if enable_checkpointing:\n      # Checkpoint object that handles saving/restoring.\n      self._checkpoint = tf.train.Checkpoint(**objects_to_save)\n      self._checkpoint_dir = paths.process_path(\n          directory,\n          'checkpoints',\n          subdirectory,\n          ttl_seconds=checkpoint_ttl_seconds,\n          backups=False,\n          add_uid=add_uid)\n\n      # Create a manager to maintain different checkpoints.\n      self._checkpoint_manager = tf.train.CheckpointManager(\n          self._checkpoint,\n          directory=self._checkpoint_dir,\n          max_to_keep=max_to_keep,\n          keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)\n\n      self.restore()\n\n  def save(self, force: bool = False) -> bool:\n    \"\"\"Save the checkpoint if it's the appropriate time, otherwise no-ops.\n\n    Args:\n      force: Whether to force a save regardless of time elapsed since last save.\n\n    Returns:\n      A boolean indicating if a save event happened.\n    \"\"\"\n    if not self._enable_checkpointing:\n      return False\n\n    if (not force and\n        time.time() - self._last_saved < 60 * self._time_delta_minutes):\n      return False\n\n    # Save any checkpoints.\n    logging.info('Saving checkpoint: %s', self._checkpoint_manager.directory)\n    self._checkpoint_manager.save()\n    self._last_saved = time.time()\n\n    return True\n\n  def restore(self):\n    # Restore from the most recent checkpoint (if it exists).\n    checkpoint_to_restore = self._checkpoint_manager.latest_checkpoint\n    logging.info('Attempting to restore checkpoint: %s',\n                 checkpoint_to_restore)\n    self._checkpoint.restore(checkpoint_to_restore)\n\n  @property\n  def directory(self):\n    return self._checkpoint_manager.directory",
  "class CheckpointingRunner(core.Worker):\n  \"\"\"Wrap an object and expose a run method which checkpoints periodically.\n\n  This internally creates a Checkpointer around `wrapped` object and exposes\n  all of the methods of `wrapped`. Additionally, any `**kwargs` passed to the\n  runner are forwarded to the internal Checkpointer.\n  \"\"\"\n\n  def __init__(\n      self,\n      wrapped: Union[Checkpointable, core.Saveable, TFSaveable],\n      key: str = 'wrapped',\n      *,\n      time_delta_minutes: int = 30,\n      **kwargs,\n  ):\n\n    if isinstance(wrapped, TFSaveable):\n      # If the object to be wrapped exposes its TF State, checkpoint that.\n      objects_to_save = wrapped.state\n    else:\n      # Otherwise checkpoint the wrapped object itself.\n      objects_to_save = wrapped\n\n    self._wrapped = wrapped\n    self._time_delta_minutes = time_delta_minutes\n    self._checkpointer = Checkpointer(\n        objects_to_save={key: objects_to_save},\n        time_delta_minutes=time_delta_minutes,\n        **kwargs)\n\n  # Handle preemption signal. Note that this must happen in the main thread.\n  def _signal_handler(self):\n    logging.info('Caught SIGTERM: forcing a checkpoint save.')\n    self._checkpointer.save(force=True)\n\n  def step(self):\n    if isinstance(self._wrapped, core.Learner):\n      # Learners have a step() method, so alternate between that and ckpt call.\n      self._wrapped.step()\n      self._checkpointer.save()\n    else:\n      # Wrapped object doesn't have a run method; set our run method to ckpt.\n      self.checkpoint()\n\n  def run(self):\n    \"\"\"Runs the checkpointer.\"\"\"\n    with signals.runtime_terminator(self._signal_handler):\n      while True:\n        self.step()\n\n  def __dir__(self):\n    return dir(self._wrapped) + ['get_directory']\n\n  # TODO(b/195915583) : Throw when wrapped object has get_directory() method.\n  def __getattr__(self, name):\n    if name == 'get_directory':\n      return self.get_directory\n    return getattr(self._wrapped, name)\n\n  def checkpoint(self):\n    self._checkpointer.save()\n    # Do not sleep for a long period of time to avoid LaunchPad program\n    # termination hangs (time.sleep is not interruptible).\n    for _ in range(self._time_delta_minutes * 60):\n      time.sleep(1)\n\n  def get_directory(self):\n    return self._checkpointer.directory",
  "class Snapshotter:\n  \"\"\"Convenience class for periodically snapshotting.\n\n  Objects which can be snapshotted are limited to Sonnet or tensorflow Modules\n  which implement a __call__ method. This will save the module's graph and\n  variables such that they can be loaded later using `tf.saved_model.load`. See\n  https://www.tensorflow.org/guide/saved_model for more details.\n\n  The Snapshotter is typically used to save infrequent permanent self-contained\n  snapshots which can be loaded later for inspection. For frequent saving of\n  model parameters in order to guard against pre-emption of the learning process\n  see the Checkpointer class.\n\n  Usage example:\n\n  ```python\n  model = snt.Linear(10)\n  snapshotter = Snapshotter(objects_to_save={'model': model})\n\n  for _ in range(100):\n    # ...\n    snapshotter.save()\n  ```\n  \"\"\"\n\n  def __init__(\n      self,\n      objects_to_save: Mapping[str, snt.Module],\n      *,\n      directory: str = '~/acme/',\n      time_delta_minutes: float = 30.0,\n      snapshot_ttl_seconds: int = _DEFAULT_SNAPSHOT_TTL,\n  ):\n    \"\"\"Builds the saver object.\n\n    Args:\n      objects_to_save: Mapping specifying what to snapshot.\n      directory: Which directory to put the snapshot in.\n      time_delta_minutes: How often to save the snapshot, in minutes.\n      snapshot_ttl_seconds: TTL (time to leave) in seconds for snapshots.\n    \"\"\"\n    objects_to_save = objects_to_save or {}\n\n    self._time_delta_minutes = time_delta_minutes\n    self._last_saved = 0.\n    self._snapshots = {}\n\n    # Save the base directory path so we can refer to it if needed.\n    self.directory = paths.process_path(\n        directory, 'snapshots', ttl_seconds=snapshot_ttl_seconds)\n\n    # Save a dictionary mapping paths to snapshot capable models.\n    for name, module in objects_to_save.items():\n      path = os.path.join(self.directory, name)\n      self._snapshots[path] = make_snapshot(module)\n\n  def save(self, force: bool = False) -> bool:\n    \"\"\"Snapshots if it's the appropriate time, otherwise no-ops.\n\n    Args:\n      force: If True, save new snapshot no matter how long it's been since the\n        last one.\n\n    Returns:\n      A boolean indicating if a save event happened.\n    \"\"\"\n    seconds_since_last = time.time() - self._last_saved\n    if (self._snapshots and\n        (force or seconds_since_last >= 60 * self._time_delta_minutes)):\n      # Save any snapshots.\n      for path, snapshot in self._snapshots.items():\n        tf.saved_model.save(snapshot, path)\n\n      # Record the time we finished saving.\n      self._last_saved = time.time()\n\n      return True\n\n    return False",
  "class Snapshot(tf.Module):\n  \"\"\"Thin wrapper which allows the module to be saved.\"\"\"\n\n  def __init__(self):\n    super().__init__()\n    self._module = None\n    self._variables = None\n    self._trainable_variables = None\n\n  @tf.function\n  def __call__(self, *args, **kwargs):\n    return self._module(*args, **kwargs)\n\n  @property\n  def submodules(self):\n    return [self._module]\n\n  @property\n  def variables(self):\n    return self._variables\n\n  @property\n  def trainable_variables(self):\n    return self._trainable_variables",
  "def make_snapshot(module: snt.Module):\n  \"\"\"Create a thin wrapper around a module to make it snapshottable.\"\"\"\n  # Get the input signature as long as it has been created.\n  input_signature = _get_input_signature(module)\n  if input_signature is None:\n    raise ValueError(\n        ('module instance \"{}\" has no input_signature attribute, '\n         'which is required for snapshotting; run '\n         'create_variables to add this annotation.').format(module.name))\n\n  # Wrap the module up in tf.function so we can process it properly.\n  @tf.function\n  def wrapped_module(*args, **kwargs):\n    return module(*args, **kwargs)\n\n  # pylint: disable=protected-access\n  snapshot = Snapshot()\n  snapshot._module = wrapped_module\n  snapshot._variables = module.variables\n  snapshot._trainable_variables = module.trainable_variables\n  # pylint: disable=protected-access\n\n  # Make sure the snapshot has the proper input signature.\n  snapshot.__call__.get_concrete_function(*input_signature)\n\n  # If we are an RNN also save the initial-state generating function.\n  if isinstance(module, snt.RNNCore):\n    snapshot.initial_state = tf.function(module.initial_state)\n    snapshot.initial_state.get_concrete_function(\n        tf.TensorSpec(shape=(), dtype=tf.int32))\n\n  return snapshot",
  "def _get_input_signature(module: snt.Module) -> Optional[tf.TensorSpec]:\n  \"\"\"Get module input signature.\n\n  Works even if the module with signature is wrapper into snt.Sequentual or\n  snt.DeepRNN.\n\n  Args:\n    module: the module which input signature we need to get. The module has to\n      either have input_signature itself (i.e. you have to run create_variables\n      on the module), or it has to be a module (with input_signature) wrapped in\n      (one or multiple) snt.Sequential or snt.DeepRNNs.\n\n  Returns:\n    Input signature of the module or None if it's not available.\n  \"\"\"\n  if hasattr(module, '_input_signature'):\n    return module._input_signature  # pylint: disable=protected-access\n\n  if isinstance(module, snt.Sequential):\n    first_layer = module._layers[0]  # pylint: disable=protected-access\n    return _get_input_signature(first_layer)\n\n  if isinstance(module, snt.DeepRNN):\n    first_layer = module._layers[0]  # pylint: disable=protected-access\n    input_signature = _get_input_signature(first_layer)\n\n    # Wrapping a module in DeepRNN changes its state shape, so we need to bring\n    # it up to date.\n    state = module.initial_state(1)\n    input_signature[-1] = tree.map_structure(\n        lambda t: tf.TensorSpec((None,) + t.shape[1:], t.dtype), state)\n\n    return input_signature\n\n  return None",
  "class SaveableAdapter(tf.train.experimental.PythonState):\n  \"\"\"Adapter which allows `Saveable` object to be checkpointed by TensorFlow.\"\"\"\n\n  def __init__(self, object_to_save: core.Saveable):\n    self._object_to_save = object_to_save\n\n  def serialize(self):\n    state = self._object_to_save.save()\n    return pickle.dumps(state)\n\n  def deserialize(self, pickled: bytes):\n    state = pickle.loads(pickled)\n    self._object_to_save.restore(state)",
  "def state(self) -> Mapping[str, Checkpointable]:\n    \"\"\"Returns TensorFlow checkpointable state.\"\"\"",
  "def __init__(\n      self,\n      objects_to_save: Mapping[str, Union[Checkpointable, core.Saveable]],\n      *,\n      directory: str = '~/acme/',\n      subdirectory: str = 'default',\n      time_delta_minutes: float = 10.0,\n      enable_checkpointing: bool = True,\n      add_uid: bool = True,\n      max_to_keep: int = 1,\n      checkpoint_ttl_seconds: Optional[int] = _DEFAULT_CHECKPOINT_TTL,\n      keep_checkpoint_every_n_hours: Optional[int] = None,\n  ):\n    \"\"\"Builds the saver object.\n\n    Args:\n      objects_to_save: Mapping specifying what to checkpoint.\n      directory: Which directory to put the checkpoint in.\n      subdirectory: Sub-directory to use (e.g. if multiple checkpoints are being\n        saved).\n      time_delta_minutes: How often to save the checkpoint, in minutes.\n      enable_checkpointing: whether to checkpoint or not.\n      add_uid: If True adds a UID to the checkpoint path, see\n        `paths.get_unique_id()` for how this UID is generated.\n      max_to_keep: The maximum number of checkpoints to keep.\n      checkpoint_ttl_seconds: TTL (time to leave) in seconds for checkpoints.\n      keep_checkpoint_every_n_hours: keep_checkpoint_every_n_hours passed to\n        tf.train.CheckpointManager.\n    \"\"\"\n\n    # Convert `Saveable` objects to TF `Checkpointable` first, if necessary.\n    def to_ckptable(x: Union[Checkpointable, core.Saveable]) -> Checkpointable:\n      if isinstance(x, core.Saveable):\n        return SaveableAdapter(x)\n      return x\n\n    objects_to_save = {k: to_ckptable(v) for k, v in objects_to_save.items()}\n\n    self._time_delta_minutes = time_delta_minutes\n    self._last_saved = 0.\n    self._enable_checkpointing = enable_checkpointing\n    self._checkpoint_manager = None\n\n    if enable_checkpointing:\n      # Checkpoint object that handles saving/restoring.\n      self._checkpoint = tf.train.Checkpoint(**objects_to_save)\n      self._checkpoint_dir = paths.process_path(\n          directory,\n          'checkpoints',\n          subdirectory,\n          ttl_seconds=checkpoint_ttl_seconds,\n          backups=False,\n          add_uid=add_uid)\n\n      # Create a manager to maintain different checkpoints.\n      self._checkpoint_manager = tf.train.CheckpointManager(\n          self._checkpoint,\n          directory=self._checkpoint_dir,\n          max_to_keep=max_to_keep,\n          keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)\n\n      self.restore()",
  "def save(self, force: bool = False) -> bool:\n    \"\"\"Save the checkpoint if it's the appropriate time, otherwise no-ops.\n\n    Args:\n      force: Whether to force a save regardless of time elapsed since last save.\n\n    Returns:\n      A boolean indicating if a save event happened.\n    \"\"\"\n    if not self._enable_checkpointing:\n      return False\n\n    if (not force and\n        time.time() - self._last_saved < 60 * self._time_delta_minutes):\n      return False\n\n    # Save any checkpoints.\n    logging.info('Saving checkpoint: %s', self._checkpoint_manager.directory)\n    self._checkpoint_manager.save()\n    self._last_saved = time.time()\n\n    return True",
  "def restore(self):\n    # Restore from the most recent checkpoint (if it exists).\n    checkpoint_to_restore = self._checkpoint_manager.latest_checkpoint\n    logging.info('Attempting to restore checkpoint: %s',\n                 checkpoint_to_restore)\n    self._checkpoint.restore(checkpoint_to_restore)",
  "def directory(self):\n    return self._checkpoint_manager.directory",
  "def __init__(\n      self,\n      wrapped: Union[Checkpointable, core.Saveable, TFSaveable],\n      key: str = 'wrapped',\n      *,\n      time_delta_minutes: int = 30,\n      **kwargs,\n  ):\n\n    if isinstance(wrapped, TFSaveable):\n      # If the object to be wrapped exposes its TF State, checkpoint that.\n      objects_to_save = wrapped.state\n    else:\n      # Otherwise checkpoint the wrapped object itself.\n      objects_to_save = wrapped\n\n    self._wrapped = wrapped\n    self._time_delta_minutes = time_delta_minutes\n    self._checkpointer = Checkpointer(\n        objects_to_save={key: objects_to_save},\n        time_delta_minutes=time_delta_minutes,\n        **kwargs)",
  "def _signal_handler(self):\n    logging.info('Caught SIGTERM: forcing a checkpoint save.')\n    self._checkpointer.save(force=True)",
  "def step(self):\n    if isinstance(self._wrapped, core.Learner):\n      # Learners have a step() method, so alternate between that and ckpt call.\n      self._wrapped.step()\n      self._checkpointer.save()\n    else:\n      # Wrapped object doesn't have a run method; set our run method to ckpt.\n      self.checkpoint()",
  "def run(self):\n    \"\"\"Runs the checkpointer.\"\"\"\n    with signals.runtime_terminator(self._signal_handler):\n      while True:\n        self.step()",
  "def __dir__(self):\n    return dir(self._wrapped) + ['get_directory']",
  "def __getattr__(self, name):\n    if name == 'get_directory':\n      return self.get_directory\n    return getattr(self._wrapped, name)",
  "def checkpoint(self):\n    self._checkpointer.save()\n    # Do not sleep for a long period of time to avoid LaunchPad program\n    # termination hangs (time.sleep is not interruptible).\n    for _ in range(self._time_delta_minutes * 60):\n      time.sleep(1)",
  "def get_directory(self):\n    return self._checkpointer.directory",
  "def __init__(\n      self,\n      objects_to_save: Mapping[str, snt.Module],\n      *,\n      directory: str = '~/acme/',\n      time_delta_minutes: float = 30.0,\n      snapshot_ttl_seconds: int = _DEFAULT_SNAPSHOT_TTL,\n  ):\n    \"\"\"Builds the saver object.\n\n    Args:\n      objects_to_save: Mapping specifying what to snapshot.\n      directory: Which directory to put the snapshot in.\n      time_delta_minutes: How often to save the snapshot, in minutes.\n      snapshot_ttl_seconds: TTL (time to leave) in seconds for snapshots.\n    \"\"\"\n    objects_to_save = objects_to_save or {}\n\n    self._time_delta_minutes = time_delta_minutes\n    self._last_saved = 0.\n    self._snapshots = {}\n\n    # Save the base directory path so we can refer to it if needed.\n    self.directory = paths.process_path(\n        directory, 'snapshots', ttl_seconds=snapshot_ttl_seconds)\n\n    # Save a dictionary mapping paths to snapshot capable models.\n    for name, module in objects_to_save.items():\n      path = os.path.join(self.directory, name)\n      self._snapshots[path] = make_snapshot(module)",
  "def save(self, force: bool = False) -> bool:\n    \"\"\"Snapshots if it's the appropriate time, otherwise no-ops.\n\n    Args:\n      force: If True, save new snapshot no matter how long it's been since the\n        last one.\n\n    Returns:\n      A boolean indicating if a save event happened.\n    \"\"\"\n    seconds_since_last = time.time() - self._last_saved\n    if (self._snapshots and\n        (force or seconds_since_last >= 60 * self._time_delta_minutes)):\n      # Save any snapshots.\n      for path, snapshot in self._snapshots.items():\n        tf.saved_model.save(snapshot, path)\n\n      # Record the time we finished saving.\n      self._last_saved = time.time()\n\n      return True\n\n    return False",
  "def __init__(self):\n    super().__init__()\n    self._module = None\n    self._variables = None\n    self._trainable_variables = None",
  "def __call__(self, *args, **kwargs):\n    return self._module(*args, **kwargs)",
  "def submodules(self):\n    return [self._module]",
  "def variables(self):\n    return self._variables",
  "def trainable_variables(self):\n    return self._trainable_variables",
  "def wrapped_module(*args, **kwargs):\n    return module(*args, **kwargs)",
  "def __init__(self, object_to_save: core.Saveable):\n    self._object_to_save = object_to_save",
  "def serialize(self):\n    state = self._object_to_save.save()\n    return pickle.dumps(state)",
  "def deserialize(self, pickled: bytes):\n    state = pickle.loads(pickled)\n    self._object_to_save.restore(state)",
  "def to_ckptable(x: Union[Checkpointable, core.Saveable]) -> Checkpointable:\n      if isinstance(x, core.Saveable):\n        return SaveableAdapter(x)\n      return x",
  "class VariableClientTest(tf.test.TestCase):\n\n  def setUp(self):\n    super().setUp()\n\n    # Create two instances of the same model.\n    self._actor_model = snt.nets.MLP(_MLP_LAYERS)\n    self._learner_model = snt.nets.MLP(_MLP_LAYERS)\n\n    # Create variables first.\n    input_spec = tf.TensorSpec(shape=(_INPUT_SIZE,), dtype=tf.float32)\n    tf2_utils.create_variables(self._actor_model, [input_spec])\n    tf2_utils.create_variables(self._learner_model, [input_spec])\n\n  def test_update_and_wait(self):\n    # Create a variable source (emulating the learner).\n    np_learner_variables = tf2_utils.to_numpy(self._learner_model.variables)\n    variable_source = fakes.VariableSource(np_learner_variables)\n\n    # Create a variable client (emulating the actor).\n    variable_client = tf2_variable_utils.VariableClient(\n        variable_source, {'policy': self._actor_model.variables})\n\n    # Create some random batch of test input:\n    x = tf.random.normal(shape=(_BATCH_SIZE, _INPUT_SIZE))\n\n    # Before copying variables, the models have different outputs.\n    self.assertNotAllClose(self._actor_model(x), self._learner_model(x))\n\n    # Update the variable client.\n    variable_client.update_and_wait()\n\n    # After copying variables (by updating the client), the models are the same.\n    self.assertAllClose(self._actor_model(x), self._learner_model(x))\n\n  def test_update(self):\n    # Create a barrier to be shared between the test body and the variable\n    # source. The barrier will block until, in this case, two threads call\n    # wait(). Note that the (fake) variable source will call it within its\n    # get_variables() call.\n    barrier = threading.Barrier(2)\n\n    # Create a variable source (emulating the learner).\n    np_learner_variables = tf2_utils.to_numpy(self._learner_model.variables)\n    variable_source = fakes.VariableSource(np_learner_variables, barrier)\n\n    # Create a variable client (emulating the actor).\n    variable_client = tf2_variable_utils.VariableClient(\n        variable_source, {'policy': self._actor_model.variables},\n        update_period=_UPDATE_PERIOD)\n\n    # Create some random batch of test input:\n    x = tf.random.normal(shape=(_BATCH_SIZE, _INPUT_SIZE))\n\n    # Create variables by doing the computation once.\n    learner_output = self._learner_model(x)\n    actor_output = self._actor_model(x)\n    del learner_output, actor_output\n\n    for _ in range(_UPDATE_PERIOD):\n      # Before the update period is reached, the models have different outputs.\n      self.assertNotAllClose(self._actor_model.variables,\n                             self._learner_model.variables)\n\n      # Before the update period is reached, the variable client should not make\n      # any requests for variables.\n      self.assertIsNone(variable_client._future)\n\n      variable_client.update()\n\n    # Make sure the last call created a request for variables and reset the\n    # internal call counter.\n    self.assertIsNotNone(variable_client._future)\n    self.assertEqual(variable_client._call_counter, 0)\n    future = variable_client._future\n\n    for _ in range(_UPDATE_PERIOD):\n      # Before the barrier allows the variables to be released, the models have\n      # different outputs.\n      self.assertNotAllClose(self._actor_model.variables,\n                             self._learner_model.variables)\n\n      variable_client.update()\n\n      # Make sure no new requests are made.\n      self.assertEqual(variable_client._future, future)\n\n    # Calling wait() on the barrier will now allow the variables to be copied\n    # over from source to client.\n    barrier.wait()\n\n    # Update once more to ensure the variables are copied over.\n    while variable_client._future is not None:\n      variable_client.update()\n\n    # After a number of update calls, the variables should be the same.\n    self.assertAllClose(self._actor_model.variables,\n                        self._learner_model.variables)",
  "def setUp(self):\n    super().setUp()\n\n    # Create two instances of the same model.\n    self._actor_model = snt.nets.MLP(_MLP_LAYERS)\n    self._learner_model = snt.nets.MLP(_MLP_LAYERS)\n\n    # Create variables first.\n    input_spec = tf.TensorSpec(shape=(_INPUT_SIZE,), dtype=tf.float32)\n    tf2_utils.create_variables(self._actor_model, [input_spec])\n    tf2_utils.create_variables(self._learner_model, [input_spec])",
  "def test_update_and_wait(self):\n    # Create a variable source (emulating the learner).\n    np_learner_variables = tf2_utils.to_numpy(self._learner_model.variables)\n    variable_source = fakes.VariableSource(np_learner_variables)\n\n    # Create a variable client (emulating the actor).\n    variable_client = tf2_variable_utils.VariableClient(\n        variable_source, {'policy': self._actor_model.variables})\n\n    # Create some random batch of test input:\n    x = tf.random.normal(shape=(_BATCH_SIZE, _INPUT_SIZE))\n\n    # Before copying variables, the models have different outputs.\n    self.assertNotAllClose(self._actor_model(x), self._learner_model(x))\n\n    # Update the variable client.\n    variable_client.update_and_wait()\n\n    # After copying variables (by updating the client), the models are the same.\n    self.assertAllClose(self._actor_model(x), self._learner_model(x))",
  "def test_update(self):\n    # Create a barrier to be shared between the test body and the variable\n    # source. The barrier will block until, in this case, two threads call\n    # wait(). Note that the (fake) variable source will call it within its\n    # get_variables() call.\n    barrier = threading.Barrier(2)\n\n    # Create a variable source (emulating the learner).\n    np_learner_variables = tf2_utils.to_numpy(self._learner_model.variables)\n    variable_source = fakes.VariableSource(np_learner_variables, barrier)\n\n    # Create a variable client (emulating the actor).\n    variable_client = tf2_variable_utils.VariableClient(\n        variable_source, {'policy': self._actor_model.variables},\n        update_period=_UPDATE_PERIOD)\n\n    # Create some random batch of test input:\n    x = tf.random.normal(shape=(_BATCH_SIZE, _INPUT_SIZE))\n\n    # Create variables by doing the computation once.\n    learner_output = self._learner_model(x)\n    actor_output = self._actor_model(x)\n    del learner_output, actor_output\n\n    for _ in range(_UPDATE_PERIOD):\n      # Before the update period is reached, the models have different outputs.\n      self.assertNotAllClose(self._actor_model.variables,\n                             self._learner_model.variables)\n\n      # Before the update period is reached, the variable client should not make\n      # any requests for variables.\n      self.assertIsNone(variable_client._future)\n\n      variable_client.update()\n\n    # Make sure the last call created a request for variables and reset the\n    # internal call counter.\n    self.assertIsNotNone(variable_client._future)\n    self.assertEqual(variable_client._call_counter, 0)\n    future = variable_client._future\n\n    for _ in range(_UPDATE_PERIOD):\n      # Before the barrier allows the variables to be released, the models have\n      # different outputs.\n      self.assertNotAllClose(self._actor_model.variables,\n                             self._learner_model.variables)\n\n      variable_client.update()\n\n      # Make sure no new requests are made.\n      self.assertEqual(variable_client._future, future)\n\n    # Calling wait() on the barrier will now allow the variables to be copied\n    # over from source to client.\n    barrier.wait()\n\n    # Update once more to ensure the variables are copied over.\n    while variable_client._future is not None:\n      variable_client.update()\n\n    # After a number of update calls, the variables should be the same.\n    self.assertAllClose(self._actor_model.variables,\n                        self._learner_model.variables)",
  "class DummySaveable(tf2_savers.TFSaveable):\n\n  _state: tf.Variable\n\n  def __init__(self):\n    self._state = tf.Variable(0, dtype=tf.int32)\n\n  @property\n  def state(self):\n    return {'state': self._state}",
  "class CheckpointerTest(test_utils.TestCase):\n\n  def test_save_and_restore(self):\n    \"\"\"Test that checkpointer correctly calls save and restore.\"\"\"\n\n    x = tf.Variable(0, dtype=tf.int32)\n    directory = self.get_tempdir()\n    checkpointer = tf2_savers.Checkpointer(\n        objects_to_save={'x': x}, time_delta_minutes=0., directory=directory)\n\n    for _ in range(10):\n      saved = checkpointer.save()\n      self.assertTrue(saved)\n      x.assign_add(1)\n      checkpointer.restore()\n      np.testing.assert_array_equal(x.numpy(), np.int32(0))\n\n  def test_save_and_new_restore(self):\n    \"\"\"Tests that a fresh checkpointer correctly restores an existing ckpt.\"\"\"\n    with mock.patch.object(paths, 'get_unique_id') as mock_unique_id:\n      mock_unique_id.return_value = ('test',)\n      x = tf.Variable(0, dtype=tf.int32)\n      directory = self.get_tempdir()\n      checkpointer1 = tf2_savers.Checkpointer(\n          objects_to_save={'x': x}, time_delta_minutes=0., directory=directory)\n      checkpointer1.save()\n      x.assign_add(1)\n      # Simulate a preemption: x is changed, and we make a new Checkpointer.\n      checkpointer2 = tf2_savers.Checkpointer(\n          objects_to_save={'x': x}, time_delta_minutes=0., directory=directory)\n      checkpointer2.restore()\n      np.testing.assert_array_equal(x.numpy(), np.int32(0))\n\n  def test_save_and_restore_time_based(self):\n    \"\"\"Test that checkpointer correctly calls save and restore based on time.\"\"\"\n\n    x = tf.Variable(0, dtype=tf.int32)\n    directory = self.get_tempdir()\n    checkpointer = tf2_savers.Checkpointer(\n        objects_to_save={'x': x}, time_delta_minutes=1., directory=directory)\n\n    with mock.patch.object(time, 'time') as mock_time:\n      mock_time.return_value = 0.\n      self.assertFalse(checkpointer.save())\n\n      mock_time.return_value = 40.\n      self.assertFalse(checkpointer.save())\n\n      mock_time.return_value = 70.\n      self.assertTrue(checkpointer.save())\n    x.assign_add(1)\n    checkpointer.restore()\n    np.testing.assert_array_equal(x.numpy(), np.int32(0))\n\n  def test_no_checkpoint(self):\n    \"\"\"Test that checkpointer does nothing when checkpoint=False.\"\"\"\n    num_steps = tf.Variable(0)\n    checkpointer = tf2_savers.Checkpointer(\n        objects_to_save={'num_steps': num_steps}, enable_checkpointing=False)\n\n    for _ in range(10):\n      self.assertFalse(checkpointer.save())\n    self.assertIsNone(checkpointer._checkpoint_manager)\n\n  def test_tf_saveable(self):\n    x = DummySaveable()\n\n    directory = self.get_tempdir()\n    checkpoint_runner = tf2_savers.CheckpointingRunner(\n        x, time_delta_minutes=0, directory=directory)\n    checkpoint_runner._checkpointer.save()\n\n    x._state.assign_add(1)\n    checkpoint_runner._checkpointer.restore()\n\n    np.testing.assert_array_equal(x._state.numpy(), np.int32(0))",
  "class CheckpointingRunnerTest(test_utils.TestCase):\n\n  def test_signal_handling(self):\n    x = DummySaveable()\n\n    # Increment the value of DummySavable.\n    x.state['state'].assign_add(1)\n\n    directory = self.get_tempdir()\n\n    # Patch signals.add_handler so the registered signal handler sets the event.\n    with mock.patch.object(\n        launchpad, 'register_stop_handler') as mock_register_stop_handler:\n      def add_handler(fn):\n        fn()\n      mock_register_stop_handler.side_effect = add_handler\n\n      runner = tf2_savers.CheckpointingRunner(\n          wrapped=x,\n          time_delta_minutes=0,\n          directory=directory)\n      with self.assertRaises(SystemExit):\n        runner.run()\n\n    # Recreate DummySavable(), its tf.Variable is initialized to 0.\n    x = DummySaveable()\n    # Recreate the CheckpointingRunner, which will restore DummySavable() to 1.\n    tf2_savers.CheckpointingRunner(\n        wrapped=x,\n        time_delta_minutes=0,\n        directory=directory)\n    # Check DummyVariable() was restored properly.\n    np.testing.assert_array_equal(x.state['state'].numpy(), np.int32(1))\n\n  def test_checkpoint_dir(self):\n    directory = self.get_tempdir()\n    ckpt_runner = tf2_savers.CheckpointingRunner(\n        wrapped=DummySaveable(),\n        time_delta_minutes=0,\n        directory=directory)\n    expected_dir_re = f'{directory}/[a-z0-9-]*/checkpoints/default'\n    regexp = re.compile(expected_dir_re)\n    self.assertIsNotNone(regexp.fullmatch(ckpt_runner.get_directory()))",
  "class SnapshotterTest(test_utils.TestCase):\n\n  def test_snapshot(self):\n    \"\"\"Test that snapshotter correctly calls saves/restores snapshots.\"\"\"\n    # Create a test network.\n    net1 = networks.LayerNormMLP([10, 10])\n    spec = specs.Array([10], dtype=np.float32)\n    tf2_utils.create_variables(net1, [spec])\n\n    # Save the test network.\n    directory = self.get_tempdir()\n    objects_to_save = {'net': net1}\n    snapshotter = tf2_savers.Snapshotter(objects_to_save, directory=directory)\n    snapshotter.save()\n\n    # Reload the test network.\n    net2 = tf.saved_model.load(os.path.join(snapshotter.directory, 'net'))\n    inputs = tf2_utils.add_batch_dim(tf2_utils.zeros_like(spec))\n\n    with tf.GradientTape() as tape:\n      outputs1 = net1(inputs)\n      loss1 = tf.math.reduce_sum(outputs1)\n      grads1 = tape.gradient(loss1, net1.trainable_variables)\n\n    with tf.GradientTape() as tape:\n      outputs2 = net2(inputs)\n      loss2 = tf.math.reduce_sum(outputs2)\n      grads2 = tape.gradient(loss2, net2.trainable_variables)\n\n    assert np.allclose(outputs1, outputs2)\n    assert all(tree.map_structure(np.allclose, list(grads1), list(grads2)))\n\n  def test_snapshot_distribution(self):\n    \"\"\"Test that snapshotter correctly calls saves/restores snapshots.\"\"\"\n    # Create a test network.\n    net1 = snt.Sequential([\n        networks.LayerNormMLP([10, 10]),\n        networks.MultivariateNormalDiagHead(1)\n    ])\n    spec = specs.Array([10], dtype=np.float32)\n    tf2_utils.create_variables(net1, [spec])\n\n    # Save the test network.\n    directory = self.get_tempdir()\n    objects_to_save = {'net': net1}\n    snapshotter = tf2_savers.Snapshotter(objects_to_save, directory=directory)\n    snapshotter.save()\n\n    # Reload the test network.\n    net2 = tf.saved_model.load(os.path.join(snapshotter.directory, 'net'))\n    inputs = tf2_utils.add_batch_dim(tf2_utils.zeros_like(spec))\n\n    with tf.GradientTape() as tape:\n      dist1 = net1(inputs)\n      loss1 = tf.math.reduce_sum(dist1.mean() + dist1.variance())\n      grads1 = tape.gradient(loss1, net1.trainable_variables)\n\n    with tf.GradientTape() as tape:\n      dist2 = net2(inputs)\n      loss2 = tf.math.reduce_sum(dist2.mean() + dist2.variance())\n      grads2 = tape.gradient(loss2, net2.trainable_variables)\n\n    assert all(tree.map_structure(np.allclose, list(grads1), list(grads2)))\n\n  def test_force_snapshot(self):\n    \"\"\"Test that the force feature in Snapshotter.save() works correctly.\"\"\"\n    # Create a test network.\n    net = snt.Linear(10)\n    spec = specs.Array([10], dtype=np.float32)\n    tf2_utils.create_variables(net, [spec])\n\n    # Save the test network.\n    directory = self.get_tempdir()\n    objects_to_save = {'net': net}\n    # Very long time_delta_minutes.\n    snapshotter = tf2_savers.Snapshotter(objects_to_save, directory=directory,\n                                         time_delta_minutes=1000)\n    self.assertTrue(snapshotter.save(force=False))\n\n    # Due to the long time_delta_minutes, only force=True will create a new\n    # snapshot. This also checks the default is force=False.\n    self.assertFalse(snapshotter.save())\n    self.assertTrue(snapshotter.save(force=True))\n\n  def test_rnn_snapshot(self):\n    \"\"\"Test that snapshotter correctly calls saves/restores snapshots on RNNs.\"\"\"\n    # Create a test network.\n    net = snt.LSTM(10)\n    spec = specs.Array([10], dtype=np.float32)\n    tf2_utils.create_variables(net, [spec])\n\n    # Test that if you add some postprocessing without rerunning\n    # create_variables, it still works.\n    wrapped_net = snt.DeepRNN([net, lambda x: x])\n\n    for net1 in [net, wrapped_net]:\n      # Save the test network.\n      directory = self.get_tempdir()\n      objects_to_save = {'net': net1}\n      snapshotter = tf2_savers.Snapshotter(objects_to_save, directory=directory)\n      snapshotter.save()\n\n      # Reload the test network.\n      net2 = tf.saved_model.load(os.path.join(snapshotter.directory, 'net'))\n      inputs = tf2_utils.add_batch_dim(tf2_utils.zeros_like(spec))\n\n      with tf.GradientTape() as tape:\n        outputs1, next_state1 = net1(inputs, net1.initial_state(1))\n        loss1 = tf.math.reduce_sum(outputs1)\n        grads1 = tape.gradient(loss1, net1.trainable_variables)\n\n      with tf.GradientTape() as tape:\n        outputs2, next_state2 = net2(inputs, net2.initial_state(1))\n        loss2 = tf.math.reduce_sum(outputs2)\n        grads2 = tape.gradient(loss2, net2.trainable_variables)\n\n      assert np.allclose(outputs1, outputs2)\n      assert np.allclose(tree.flatten(next_state1), tree.flatten(next_state2))\n      assert all(tree.map_structure(np.allclose, list(grads1), list(grads2)))",
  "def __init__(self):\n    self._state = tf.Variable(0, dtype=tf.int32)",
  "def state(self):\n    return {'state': self._state}",
  "def test_save_and_restore(self):\n    \"\"\"Test that checkpointer correctly calls save and restore.\"\"\"\n\n    x = tf.Variable(0, dtype=tf.int32)\n    directory = self.get_tempdir()\n    checkpointer = tf2_savers.Checkpointer(\n        objects_to_save={'x': x}, time_delta_minutes=0., directory=directory)\n\n    for _ in range(10):\n      saved = checkpointer.save()\n      self.assertTrue(saved)\n      x.assign_add(1)\n      checkpointer.restore()\n      np.testing.assert_array_equal(x.numpy(), np.int32(0))",
  "def test_save_and_new_restore(self):\n    \"\"\"Tests that a fresh checkpointer correctly restores an existing ckpt.\"\"\"\n    with mock.patch.object(paths, 'get_unique_id') as mock_unique_id:\n      mock_unique_id.return_value = ('test',)\n      x = tf.Variable(0, dtype=tf.int32)\n      directory = self.get_tempdir()\n      checkpointer1 = tf2_savers.Checkpointer(\n          objects_to_save={'x': x}, time_delta_minutes=0., directory=directory)\n      checkpointer1.save()\n      x.assign_add(1)\n      # Simulate a preemption: x is changed, and we make a new Checkpointer.\n      checkpointer2 = tf2_savers.Checkpointer(\n          objects_to_save={'x': x}, time_delta_minutes=0., directory=directory)\n      checkpointer2.restore()\n      np.testing.assert_array_equal(x.numpy(), np.int32(0))",
  "def test_save_and_restore_time_based(self):\n    \"\"\"Test that checkpointer correctly calls save and restore based on time.\"\"\"\n\n    x = tf.Variable(0, dtype=tf.int32)\n    directory = self.get_tempdir()\n    checkpointer = tf2_savers.Checkpointer(\n        objects_to_save={'x': x}, time_delta_minutes=1., directory=directory)\n\n    with mock.patch.object(time, 'time') as mock_time:\n      mock_time.return_value = 0.\n      self.assertFalse(checkpointer.save())\n\n      mock_time.return_value = 40.\n      self.assertFalse(checkpointer.save())\n\n      mock_time.return_value = 70.\n      self.assertTrue(checkpointer.save())\n    x.assign_add(1)\n    checkpointer.restore()\n    np.testing.assert_array_equal(x.numpy(), np.int32(0))",
  "def test_no_checkpoint(self):\n    \"\"\"Test that checkpointer does nothing when checkpoint=False.\"\"\"\n    num_steps = tf.Variable(0)\n    checkpointer = tf2_savers.Checkpointer(\n        objects_to_save={'num_steps': num_steps}, enable_checkpointing=False)\n\n    for _ in range(10):\n      self.assertFalse(checkpointer.save())\n    self.assertIsNone(checkpointer._checkpoint_manager)",
  "def test_tf_saveable(self):\n    x = DummySaveable()\n\n    directory = self.get_tempdir()\n    checkpoint_runner = tf2_savers.CheckpointingRunner(\n        x, time_delta_minutes=0, directory=directory)\n    checkpoint_runner._checkpointer.save()\n\n    x._state.assign_add(1)\n    checkpoint_runner._checkpointer.restore()\n\n    np.testing.assert_array_equal(x._state.numpy(), np.int32(0))",
  "def test_signal_handling(self):\n    x = DummySaveable()\n\n    # Increment the value of DummySavable.\n    x.state['state'].assign_add(1)\n\n    directory = self.get_tempdir()\n\n    # Patch signals.add_handler so the registered signal handler sets the event.\n    with mock.patch.object(\n        launchpad, 'register_stop_handler') as mock_register_stop_handler:\n      def add_handler(fn):\n        fn()\n      mock_register_stop_handler.side_effect = add_handler\n\n      runner = tf2_savers.CheckpointingRunner(\n          wrapped=x,\n          time_delta_minutes=0,\n          directory=directory)\n      with self.assertRaises(SystemExit):\n        runner.run()\n\n    # Recreate DummySavable(), its tf.Variable is initialized to 0.\n    x = DummySaveable()\n    # Recreate the CheckpointingRunner, which will restore DummySavable() to 1.\n    tf2_savers.CheckpointingRunner(\n        wrapped=x,\n        time_delta_minutes=0,\n        directory=directory)\n    # Check DummyVariable() was restored properly.\n    np.testing.assert_array_equal(x.state['state'].numpy(), np.int32(1))",
  "def test_checkpoint_dir(self):\n    directory = self.get_tempdir()\n    ckpt_runner = tf2_savers.CheckpointingRunner(\n        wrapped=DummySaveable(),\n        time_delta_minutes=0,\n        directory=directory)\n    expected_dir_re = f'{directory}/[a-z0-9-]*/checkpoints/default'\n    regexp = re.compile(expected_dir_re)\n    self.assertIsNotNone(regexp.fullmatch(ckpt_runner.get_directory()))",
  "def test_snapshot(self):\n    \"\"\"Test that snapshotter correctly calls saves/restores snapshots.\"\"\"\n    # Create a test network.\n    net1 = networks.LayerNormMLP([10, 10])\n    spec = specs.Array([10], dtype=np.float32)\n    tf2_utils.create_variables(net1, [spec])\n\n    # Save the test network.\n    directory = self.get_tempdir()\n    objects_to_save = {'net': net1}\n    snapshotter = tf2_savers.Snapshotter(objects_to_save, directory=directory)\n    snapshotter.save()\n\n    # Reload the test network.\n    net2 = tf.saved_model.load(os.path.join(snapshotter.directory, 'net'))\n    inputs = tf2_utils.add_batch_dim(tf2_utils.zeros_like(spec))\n\n    with tf.GradientTape() as tape:\n      outputs1 = net1(inputs)\n      loss1 = tf.math.reduce_sum(outputs1)\n      grads1 = tape.gradient(loss1, net1.trainable_variables)\n\n    with tf.GradientTape() as tape:\n      outputs2 = net2(inputs)\n      loss2 = tf.math.reduce_sum(outputs2)\n      grads2 = tape.gradient(loss2, net2.trainable_variables)\n\n    assert np.allclose(outputs1, outputs2)\n    assert all(tree.map_structure(np.allclose, list(grads1), list(grads2)))",
  "def test_snapshot_distribution(self):\n    \"\"\"Test that snapshotter correctly calls saves/restores snapshots.\"\"\"\n    # Create a test network.\n    net1 = snt.Sequential([\n        networks.LayerNormMLP([10, 10]),\n        networks.MultivariateNormalDiagHead(1)\n    ])\n    spec = specs.Array([10], dtype=np.float32)\n    tf2_utils.create_variables(net1, [spec])\n\n    # Save the test network.\n    directory = self.get_tempdir()\n    objects_to_save = {'net': net1}\n    snapshotter = tf2_savers.Snapshotter(objects_to_save, directory=directory)\n    snapshotter.save()\n\n    # Reload the test network.\n    net2 = tf.saved_model.load(os.path.join(snapshotter.directory, 'net'))\n    inputs = tf2_utils.add_batch_dim(tf2_utils.zeros_like(spec))\n\n    with tf.GradientTape() as tape:\n      dist1 = net1(inputs)\n      loss1 = tf.math.reduce_sum(dist1.mean() + dist1.variance())\n      grads1 = tape.gradient(loss1, net1.trainable_variables)\n\n    with tf.GradientTape() as tape:\n      dist2 = net2(inputs)\n      loss2 = tf.math.reduce_sum(dist2.mean() + dist2.variance())\n      grads2 = tape.gradient(loss2, net2.trainable_variables)\n\n    assert all(tree.map_structure(np.allclose, list(grads1), list(grads2)))",
  "def test_force_snapshot(self):\n    \"\"\"Test that the force feature in Snapshotter.save() works correctly.\"\"\"\n    # Create a test network.\n    net = snt.Linear(10)\n    spec = specs.Array([10], dtype=np.float32)\n    tf2_utils.create_variables(net, [spec])\n\n    # Save the test network.\n    directory = self.get_tempdir()\n    objects_to_save = {'net': net}\n    # Very long time_delta_minutes.\n    snapshotter = tf2_savers.Snapshotter(objects_to_save, directory=directory,\n                                         time_delta_minutes=1000)\n    self.assertTrue(snapshotter.save(force=False))\n\n    # Due to the long time_delta_minutes, only force=True will create a new\n    # snapshot. This also checks the default is force=False.\n    self.assertFalse(snapshotter.save())\n    self.assertTrue(snapshotter.save(force=True))",
  "def test_rnn_snapshot(self):\n    \"\"\"Test that snapshotter correctly calls saves/restores snapshots on RNNs.\"\"\"\n    # Create a test network.\n    net = snt.LSTM(10)\n    spec = specs.Array([10], dtype=np.float32)\n    tf2_utils.create_variables(net, [spec])\n\n    # Test that if you add some postprocessing without rerunning\n    # create_variables, it still works.\n    wrapped_net = snt.DeepRNN([net, lambda x: x])\n\n    for net1 in [net, wrapped_net]:\n      # Save the test network.\n      directory = self.get_tempdir()\n      objects_to_save = {'net': net1}\n      snapshotter = tf2_savers.Snapshotter(objects_to_save, directory=directory)\n      snapshotter.save()\n\n      # Reload the test network.\n      net2 = tf.saved_model.load(os.path.join(snapshotter.directory, 'net'))\n      inputs = tf2_utils.add_batch_dim(tf2_utils.zeros_like(spec))\n\n      with tf.GradientTape() as tape:\n        outputs1, next_state1 = net1(inputs, net1.initial_state(1))\n        loss1 = tf.math.reduce_sum(outputs1)\n        grads1 = tape.gradient(loss1, net1.trainable_variables)\n\n      with tf.GradientTape() as tape:\n        outputs2, next_state2 = net2(inputs, net2.initial_state(1))\n        loss2 = tf.math.reduce_sum(outputs2)\n        grads2 = tape.gradient(loss2, net2.trainable_variables)\n\n      assert np.allclose(outputs1, outputs2)\n      assert np.allclose(tree.flatten(next_state1), tree.flatten(next_state2))\n      assert all(tree.map_structure(np.allclose, list(grads1), list(grads2)))",
  "def add_handler(fn):\n        fn()",
  "class VariableClient:\n  \"\"\"A variable client for updating variables from a remote source.\"\"\"\n\n  def __init__(self,\n               client: core.VariableSource,\n               variables: Mapping[str, Sequence[tf.Variable]],\n               update_period: int = 1):\n    self._keys = list(variables.keys())\n    self._variables = tree.flatten(list(variables.values()))\n    self._call_counter = 0\n    self._update_period = update_period\n    self._client = client\n    self._request = lambda: client.get_variables(self._keys)\n\n    # Create a single background thread to fetch variables without necessarily\n    # blocking the actor.\n    self._executor = futures.ThreadPoolExecutor(max_workers=1)\n    self._async_request = lambda: self._executor.submit(self._request)\n\n    # Initialize this client's future to None to indicate to the `update()`\n    # method that there is no pending/running request.\n    self._future: Optional[futures.Future] = None\n\n  def update(self, wait: bool = False):\n    \"\"\"Periodically updates the variables with the latest copy from the source.\n\n    This stateful update method keeps track of the number of calls to it and,\n    every `update_period` call, sends a request to its server to retrieve the\n    latest variables.\n\n    If wait is True, a blocking request is executed. Any active request will be\n    cancelled.\n    If wait is False, this method makes an asynchronous request for variables\n    and returns. Unless the request is immediately fulfilled, the variables are\n    only copied _within a subsequent call to_ `update()`, whenever the request\n    is fulfilled by the `VariableSource`. If there is an existing fulfilled\n    request when this method is called, the resulting variables are immediately\n    copied.\n\n    Args:\n      wait: if True, executes blocking update.\n    \"\"\"\n    # Track the number of calls (we only update periodically).\n    if self._call_counter < self._update_period:\n      self._call_counter += 1\n\n    period_reached: bool = self._call_counter >= self._update_period\n\n    if period_reached and wait:\n      # Cancel any active request.\n      self._future: Optional[futures.Future] = None\n      self.update_and_wait()\n      self._call_counter = 0\n      return\n\n    if period_reached and self._future is None:\n      # The update period has been reached and no request has been sent yet, so\n      # making an asynchronous request now.\n      self._future = self._async_request()\n      self._call_counter = 0\n\n    if self._future is not None and self._future.done():\n      # The active request is done so copy the result and remove the future.\n      self._copy(self._future.result())\n      self._future: Optional[futures.Future] = None\n    else:\n      # There is either a pending/running request or we're between update\n      # periods, so just carry on.\n      return\n\n  def update_and_wait(self):\n    \"\"\"Immediately update and block until we get the result.\"\"\"\n    self._copy(self._request())\n\n  def _copy(self, new_variables: Sequence[Sequence[tf.Variable]]):\n    \"\"\"Copies the new variables to the old ones.\"\"\"\n\n    new_variables = tree.flatten(new_variables)\n    if len(self._variables) != len(new_variables):\n      raise ValueError('Length mismatch between old variables and new.')\n\n    for new, old in zip(new_variables, self._variables):\n      old.assign(new)",
  "def __init__(self,\n               client: core.VariableSource,\n               variables: Mapping[str, Sequence[tf.Variable]],\n               update_period: int = 1):\n    self._keys = list(variables.keys())\n    self._variables = tree.flatten(list(variables.values()))\n    self._call_counter = 0\n    self._update_period = update_period\n    self._client = client\n    self._request = lambda: client.get_variables(self._keys)\n\n    # Create a single background thread to fetch variables without necessarily\n    # blocking the actor.\n    self._executor = futures.ThreadPoolExecutor(max_workers=1)\n    self._async_request = lambda: self._executor.submit(self._request)\n\n    # Initialize this client's future to None to indicate to the `update()`\n    # method that there is no pending/running request.\n    self._future: Optional[futures.Future] = None",
  "def update(self, wait: bool = False):\n    \"\"\"Periodically updates the variables with the latest copy from the source.\n\n    This stateful update method keeps track of the number of calls to it and,\n    every `update_period` call, sends a request to its server to retrieve the\n    latest variables.\n\n    If wait is True, a blocking request is executed. Any active request will be\n    cancelled.\n    If wait is False, this method makes an asynchronous request for variables\n    and returns. Unless the request is immediately fulfilled, the variables are\n    only copied _within a subsequent call to_ `update()`, whenever the request\n    is fulfilled by the `VariableSource`. If there is an existing fulfilled\n    request when this method is called, the resulting variables are immediately\n    copied.\n\n    Args:\n      wait: if True, executes blocking update.\n    \"\"\"\n    # Track the number of calls (we only update periodically).\n    if self._call_counter < self._update_period:\n      self._call_counter += 1\n\n    period_reached: bool = self._call_counter >= self._update_period\n\n    if period_reached and wait:\n      # Cancel any active request.\n      self._future: Optional[futures.Future] = None\n      self.update_and_wait()\n      self._call_counter = 0\n      return\n\n    if period_reached and self._future is None:\n      # The update period has been reached and no request has been sent yet, so\n      # making an asynchronous request now.\n      self._future = self._async_request()\n      self._call_counter = 0\n\n    if self._future is not None and self._future.done():\n      # The active request is done so copy the result and remove the future.\n      self._copy(self._future.result())\n      self._future: Optional[futures.Future] = None\n    else:\n      # There is either a pending/running request or we're between update\n      # periods, so just carry on.\n      return",
  "def update_and_wait(self):\n    \"\"\"Immediately update and block until we get the result.\"\"\"\n    self._copy(self._request())",
  "def _copy(self, new_variables: Sequence[Sequence[tf.Variable]]):\n    \"\"\"Copies the new variables to the old ones.\"\"\"\n\n    new_variables = tree.flatten(new_variables)\n    if len(self._variables) != len(new_variables):\n      raise ValueError('Length mismatch between old variables and new.')\n\n    for new, old in zip(new_variables, self._variables):\n      old.assign(new)",
  "class PolicyValueHead(snt.Module):\n  \"\"\"A network with two linear layers, for policy and value respectively.\"\"\"\n\n  def __init__(self, num_actions: int):\n    super().__init__(name='policy_value_network')\n    self._policy_layer = snt.Linear(num_actions)\n    self._value_layer = snt.Linear(1)\n\n  def __call__(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n    \"\"\"Returns a (Logits, Value) tuple.\"\"\"\n    logits = self._policy_layer(inputs)  # [B, A]\n    value = tf.squeeze(self._value_layer(inputs), axis=-1)  # [B]\n\n    return logits, value",
  "class CreateVariableTest(parameterized.TestCase):\n  \"\"\"Tests for tf2_utils.create_variables method.\"\"\"\n\n  @parameterized.parameters([True, False])\n  def test_feedforward(self, recurrent: bool):\n    model = snt.Linear(42)\n    if recurrent:\n      model = snt.DeepRNN([model])\n    input_spec = specs.Array(shape=(10,), dtype=np.float32)\n    tf2_utils.create_variables(model, [input_spec])\n    variables: Sequence[tf.Variable] = model.variables\n    shapes = [v.shape.as_list() for v in variables]\n    self.assertSequenceEqual(shapes, [[42], [10, 42]])\n\n  @parameterized.parameters([True, False])\n  def test_output_spec_feedforward(self, recurrent: bool):\n    input_spec = specs.Array(shape=(10,), dtype=np.float32)\n    model = snt.Linear(42)\n    expected_spec = tf.TensorSpec(shape=(42,), dtype=tf.float32)\n    if recurrent:\n      model = snt.DeepRNN([model])\n      expected_spec = (expected_spec, ())\n\n    output_spec = tf2_utils.create_variables(model, [input_spec])\n    self.assertEqual(output_spec, expected_spec)\n\n  def test_multiple_outputs(self):\n    model = PolicyValueHead(42)\n    input_spec = specs.Array(shape=(10,), dtype=np.float32)\n    expected_spec = (tf.TensorSpec(shape=(42,), dtype=tf.float32),\n                     tf.TensorSpec(shape=(), dtype=tf.float32))\n    output_spec = tf2_utils.create_variables(model, [input_spec])\n    variables: Sequence[tf.Variable] = model.variables\n    shapes = [v.shape.as_list() for v in variables]\n    self.assertSequenceEqual(shapes, [[42], [10, 42], [1], [10, 1]])\n    self.assertSequenceEqual(output_spec, expected_spec)\n\n  def test_scalar_output(self):\n    model = tf2_utils.to_sonnet_module(tf.reduce_sum)\n    input_spec = specs.Array(shape=(10,), dtype=np.float32)\n    expected_spec = tf.TensorSpec(shape=(), dtype=tf.float32)\n    output_spec = tf2_utils.create_variables(model, [input_spec])\n    self.assertEqual(model.variables, ())\n    self.assertEqual(output_spec, expected_spec)\n\n  def test_none_output(self):\n    model = tf2_utils.to_sonnet_module(lambda x: None)\n    input_spec = specs.Array(shape=(10,), dtype=np.float32)\n    expected_spec = None\n    output_spec = tf2_utils.create_variables(model, [input_spec])\n    self.assertEqual(model.variables, ())\n    self.assertEqual(output_spec, expected_spec)\n\n  def test_multiple_inputs_and_outputs(self):\n    def transformation(aa, bb, cc):\n      return (tf.concat([aa, bb, cc], axis=-1),\n              tf.concat([bb, cc], axis=-1))\n\n    model = tf2_utils.to_sonnet_module(transformation)\n    dtype = np.float32\n    input_spec = [specs.Array(shape=(2,), dtype=dtype),\n                  specs.Array(shape=(3,), dtype=dtype),\n                  specs.Array(shape=(4,), dtype=dtype)]\n    expected_output_spec = (tf.TensorSpec(shape=(9,), dtype=dtype),\n                            tf.TensorSpec(shape=(7,), dtype=dtype))\n    output_spec = tf2_utils.create_variables(model, input_spec)\n    self.assertEqual(model.variables, ())\n    self.assertEqual(output_spec, expected_output_spec)",
  "class Tf2UtilsTest(parameterized.TestCase):\n  \"\"\"Tests for tf2_utils methods.\"\"\"\n\n  def test_batch_concat(self):\n    batch_size = 32\n    inputs = [\n        tf.zeros(shape=(batch_size, 2)),\n        {\n            'foo': tf.zeros(shape=(batch_size, 5, 3))\n        },\n        [tf.zeros(shape=(batch_size, 1))],\n    ]\n\n    output_shape = tf2_utils.batch_concat(inputs).shape.as_list()\n    expected_shape = [batch_size, 2 + 5 * 3 + 1]\n    self.assertSequenceEqual(output_shape, expected_shape)",
  "def __init__(self, num_actions: int):\n    super().__init__(name='policy_value_network')\n    self._policy_layer = snt.Linear(num_actions)\n    self._value_layer = snt.Linear(1)",
  "def __call__(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n    \"\"\"Returns a (Logits, Value) tuple.\"\"\"\n    logits = self._policy_layer(inputs)  # [B, A]\n    value = tf.squeeze(self._value_layer(inputs), axis=-1)  # [B]\n\n    return logits, value",
  "def test_feedforward(self, recurrent: bool):\n    model = snt.Linear(42)\n    if recurrent:\n      model = snt.DeepRNN([model])\n    input_spec = specs.Array(shape=(10,), dtype=np.float32)\n    tf2_utils.create_variables(model, [input_spec])\n    variables: Sequence[tf.Variable] = model.variables\n    shapes = [v.shape.as_list() for v in variables]\n    self.assertSequenceEqual(shapes, [[42], [10, 42]])",
  "def test_output_spec_feedforward(self, recurrent: bool):\n    input_spec = specs.Array(shape=(10,), dtype=np.float32)\n    model = snt.Linear(42)\n    expected_spec = tf.TensorSpec(shape=(42,), dtype=tf.float32)\n    if recurrent:\n      model = snt.DeepRNN([model])\n      expected_spec = (expected_spec, ())\n\n    output_spec = tf2_utils.create_variables(model, [input_spec])\n    self.assertEqual(output_spec, expected_spec)",
  "def test_multiple_outputs(self):\n    model = PolicyValueHead(42)\n    input_spec = specs.Array(shape=(10,), dtype=np.float32)\n    expected_spec = (tf.TensorSpec(shape=(42,), dtype=tf.float32),\n                     tf.TensorSpec(shape=(), dtype=tf.float32))\n    output_spec = tf2_utils.create_variables(model, [input_spec])\n    variables: Sequence[tf.Variable] = model.variables\n    shapes = [v.shape.as_list() for v in variables]\n    self.assertSequenceEqual(shapes, [[42], [10, 42], [1], [10, 1]])\n    self.assertSequenceEqual(output_spec, expected_spec)",
  "def test_scalar_output(self):\n    model = tf2_utils.to_sonnet_module(tf.reduce_sum)\n    input_spec = specs.Array(shape=(10,), dtype=np.float32)\n    expected_spec = tf.TensorSpec(shape=(), dtype=tf.float32)\n    output_spec = tf2_utils.create_variables(model, [input_spec])\n    self.assertEqual(model.variables, ())\n    self.assertEqual(output_spec, expected_spec)",
  "def test_none_output(self):\n    model = tf2_utils.to_sonnet_module(lambda x: None)\n    input_spec = specs.Array(shape=(10,), dtype=np.float32)\n    expected_spec = None\n    output_spec = tf2_utils.create_variables(model, [input_spec])\n    self.assertEqual(model.variables, ())\n    self.assertEqual(output_spec, expected_spec)",
  "def test_multiple_inputs_and_outputs(self):\n    def transformation(aa, bb, cc):\n      return (tf.concat([aa, bb, cc], axis=-1),\n              tf.concat([bb, cc], axis=-1))\n\n    model = tf2_utils.to_sonnet_module(transformation)\n    dtype = np.float32\n    input_spec = [specs.Array(shape=(2,), dtype=dtype),\n                  specs.Array(shape=(3,), dtype=dtype),\n                  specs.Array(shape=(4,), dtype=dtype)]\n    expected_output_spec = (tf.TensorSpec(shape=(9,), dtype=dtype),\n                            tf.TensorSpec(shape=(7,), dtype=dtype))\n    output_spec = tf2_utils.create_variables(model, input_spec)\n    self.assertEqual(model.variables, ())\n    self.assertEqual(output_spec, expected_output_spec)",
  "def test_batch_concat(self):\n    batch_size = 32\n    inputs = [\n        tf.zeros(shape=(batch_size, 2)),\n        {\n            'foo': tf.zeros(shape=(batch_size, 5, 3))\n        },\n        [tf.zeros(shape=(batch_size, 1))],\n    ]\n\n    output_shape = tf2_utils.batch_concat(inputs).shape.as_list()\n    expected_shape = [batch_size, 2 + 5 * 3 + 1]\n    self.assertSequenceEqual(output_shape, expected_shape)",
  "def transformation(aa, bb, cc):\n      return (tf.concat([aa, bb, cc], axis=-1),\n              tf.concat([bb, cc], axis=-1))",
  "class MPO(snt.Module):\n  \"\"\"MPO loss with decoupled KL constraints as in (Abdolmaleki et al., 2018).\n\n  This implementation of the MPO loss includes the following features, as\n  options:\n  - Satisfying the KL-constraint on a per-dimension basis (on by default);\n  - Penalizing actions that fall outside of [-1, 1] (on by default) as a\n      special case of multi-objective MPO (MO-MPO; Abdolmaleki et al., 2020).\n  For best results on the control suite, keep both of these on.\n\n  (Abdolmaleki et al., 2018): https://arxiv.org/pdf/1812.02256.pdf\n  (Abdolmaleki et al., 2020): https://arxiv.org/pdf/2005.07513.pdf\n  \"\"\"\n\n  def __init__(self,\n               epsilon: float,\n               epsilon_mean: float,\n               epsilon_stddev: float,\n               init_log_temperature: float,\n               init_log_alpha_mean: float,\n               init_log_alpha_stddev: float,\n               per_dim_constraining: bool = True,\n               action_penalization: bool = True,\n               epsilon_penalty: float = 0.001,\n               name: str = \"MPO\"):\n    \"\"\"Initialize and configure the MPO loss.\n\n    Args:\n      epsilon: KL constraint on the non-parametric auxiliary policy, the one\n        associated with the dual variable called temperature.\n      epsilon_mean: KL constraint on the mean of the Gaussian policy, the one\n        associated with the dual variable called alpha_mean.\n      epsilon_stddev: KL constraint on the stddev of the Gaussian policy, the\n        one associated with the dual variable called alpha_mean.\n      init_log_temperature: initial value for the temperature in log-space, note\n        a softplus (rather than an exp) will be used to transform this.\n      init_log_alpha_mean: initial value for the alpha_mean in log-space, note\n        a softplus (rather than an exp) will be used to transform this.\n      init_log_alpha_stddev: initial value for the alpha_stddev in log-space,\n        note a softplus (rather than an exp) will be used to transform this.\n      per_dim_constraining: whether to enforce the KL constraint on each\n        dimension independently; this is the default. Otherwise the overall KL\n        is constrained, which allows some dimensions to change more at the\n        expense of others staying put.\n      action_penalization: whether to use a KL constraint to penalize actions\n        via the MO-MPO algorithm.\n      epsilon_penalty: KL constraint on the probability of violating the action\n        constraint.\n      name: a name for the module, passed directly to snt.Module.\n\n    \"\"\"\n    super().__init__(name=name)\n\n    # MPO constrain thresholds.\n    self._epsilon = tf.constant(epsilon)\n    self._epsilon_mean = tf.constant(epsilon_mean)\n    self._epsilon_stddev = tf.constant(epsilon_stddev)\n\n    # Initial values for the constraints' dual variables.\n    self._init_log_temperature = init_log_temperature\n    self._init_log_alpha_mean = init_log_alpha_mean\n    self._init_log_alpha_stddev = init_log_alpha_stddev\n\n    # Whether to penalize out-of-bound actions via MO-MPO and its corresponding\n    # constraint threshold.\n    self._action_penalization = action_penalization\n    self._epsilon_penalty = tf.constant(epsilon_penalty)\n\n    # Whether to ensure per-dimension KL constraint satisfication.\n    self._per_dim_constraining = per_dim_constraining\n\n  @snt.once\n  def create_dual_variables_once(self, shape: tf.TensorShape, dtype: tf.DType):\n    \"\"\"Creates the dual variables the first time the loss module is called.\"\"\"\n\n    # Create the dual variables.\n    self._log_temperature = tf.Variable(\n        initial_value=[self._init_log_temperature],\n        dtype=dtype,\n        name=\"log_temperature\",\n        shape=(1,))\n    self._log_alpha_mean = tf.Variable(\n        initial_value=tf.fill(shape, self._init_log_alpha_mean),\n        dtype=dtype,\n        name=\"log_alpha_mean\",\n        shape=shape)\n    self._log_alpha_stddev = tf.Variable(\n        initial_value=tf.fill(shape, self._init_log_alpha_stddev),\n        dtype=dtype,\n        name=\"log_alpha_stddev\",\n        shape=shape)\n\n    # Cast constraint thresholds to the expected dtype.\n    self._epsilon = tf.cast(self._epsilon, dtype)\n    self._epsilon_mean = tf.cast(self._epsilon_mean, dtype)\n    self._epsilon_stddev = tf.cast(self._epsilon_stddev, dtype)\n\n    # Maybe create the action penalization dual variable.\n    if self._action_penalization:\n      self._epsilon_penalty = tf.cast(self._epsilon_penalty, dtype)\n      self._log_penalty_temperature = tf.Variable(\n          initial_value=[self._init_log_temperature],\n          dtype=dtype,\n          name=\"log_penalty_temperature\",\n          shape=(1,))\n\n  def __call__(\n      self,\n      online_action_distribution: Union[tfd.MultivariateNormalDiag,\n                                        tfd.Independent],\n      target_action_distribution: Union[tfd.MultivariateNormalDiag,\n                                        tfd.Independent],\n      actions: tf.Tensor,  # Shape [N, B, D].\n      q_values: tf.Tensor,  # Shape [N, B].\n  ) -> Tuple[tf.Tensor, Dict[str, tf.Tensor]]:\n    \"\"\"Computes the decoupled MPO loss.\n\n    Args:\n      online_action_distribution: online distribution returned by the online\n        policy network; expects batch_dims of [B] and event_dims of [D].\n      target_action_distribution: target distribution returned by the target\n        policy network; expects same shapes as online distribution.\n      actions: actions sampled from the target policy; expects shape [N, B, D].\n      q_values: Q-values associated with each action; expects shape [N, B].\n\n    Returns:\n      Loss, combining the policy loss, KL penalty, and dual losses required to\n        adapt the dual variables.\n      Stats, for diagnostics and tracking performance.\n    \"\"\"\n\n    # Cast `MultivariateNormalDiag`s to Independent Normals.\n    # The latter allows us to satisfy KL constraints per-dimension.\n    if isinstance(target_action_distribution, tfd.MultivariateNormalDiag):\n      target_action_distribution = tfd.Independent(\n          tfd.Normal(target_action_distribution.mean(),\n                     target_action_distribution.stddev()))\n      online_action_distribution = tfd.Independent(\n          tfd.Normal(online_action_distribution.mean(),\n                     online_action_distribution.stddev()))\n\n    # Infer the shape and dtype of dual variables.\n    scalar_dtype = q_values.dtype\n    if self._per_dim_constraining:\n      dual_variable_shape = target_action_distribution.distribution.kl_divergence(\n          online_action_distribution.distribution).shape[1:]  # Should be [D].\n    else:\n      dual_variable_shape = target_action_distribution.kl_divergence(\n          online_action_distribution).shape[1:]  # Should be [1].\n\n    # Create dual variables for the KL constraints; only happens the first call.\n    self.create_dual_variables_once(dual_variable_shape, scalar_dtype)\n\n    # Project dual variables to ensure they stay positive.\n    min_log_temperature = tf.constant(-18.0, scalar_dtype)\n    min_log_alpha = tf.constant(-18.0, scalar_dtype)\n    self._log_temperature.assign(\n        tf.maximum(min_log_temperature, self._log_temperature))\n    self._log_alpha_mean.assign(tf.maximum(min_log_alpha, self._log_alpha_mean))\n    self._log_alpha_stddev.assign(\n        tf.maximum(min_log_alpha, self._log_alpha_stddev))\n\n    # Transform dual variables from log-space.\n    # Note: using softplus instead of exponential for numerical stability.\n    temperature = tf.math.softplus(self._log_temperature) + _MPO_FLOAT_EPSILON\n    alpha_mean = tf.math.softplus(self._log_alpha_mean) + _MPO_FLOAT_EPSILON\n    alpha_stddev = tf.math.softplus(self._log_alpha_stddev) + _MPO_FLOAT_EPSILON\n\n    # Get online and target means and stddevs in preparation for decomposition.\n    online_mean = online_action_distribution.distribution.mean()\n    online_scale = online_action_distribution.distribution.stddev()\n    target_mean = target_action_distribution.distribution.mean()\n    target_scale = target_action_distribution.distribution.stddev()\n\n    # Compute normalized importance weights, used to compute expectations with\n    # respect to the non-parametric policy; and the temperature loss, used to\n    # adapt the tempering of Q-values.\n    normalized_weights, loss_temperature = compute_weights_and_temperature_loss(\n        q_values, self._epsilon, temperature)\n\n    # Only needed for diagnostics: Compute estimated actualized KL between the\n    # non-parametric and current target policies.\n    kl_nonparametric = compute_nonparametric_kl_from_normalized_weights(\n        normalized_weights)\n\n    if self._action_penalization:\n      # Project and transform action penalization temperature.\n      self._log_penalty_temperature.assign(\n          tf.maximum(min_log_temperature, self._log_penalty_temperature))\n      penalty_temperature = tf.math.softplus(\n          self._log_penalty_temperature) + _MPO_FLOAT_EPSILON\n\n      # Compute action penalization cost.\n      # Note: the cost is zero in [-1, 1] and quadratic beyond.\n      diff_out_of_bound = actions - tf.clip_by_value(actions, -1.0, 1.0)\n      cost_out_of_bound = -tf.norm(diff_out_of_bound, axis=-1)\n\n      penalty_normalized_weights, loss_penalty_temperature = compute_weights_and_temperature_loss(\n          cost_out_of_bound, self._epsilon_penalty, penalty_temperature)\n\n      # Only needed for diagnostics: Compute estimated actualized KL between the\n      # non-parametric and current target policies.\n      penalty_kl_nonparametric = compute_nonparametric_kl_from_normalized_weights(\n          penalty_normalized_weights)\n\n      # Combine normalized weights.\n      normalized_weights += penalty_normalized_weights\n      loss_temperature += loss_penalty_temperature\n    # Decompose the online policy into fixed-mean & fixed-stddev distributions.\n    # This has been documented as having better performance in bandit settings,\n    # see e.g. https://arxiv.org/pdf/1812.02256.pdf.\n    fixed_stddev_distribution = tfd.Independent(\n        tfd.Normal(loc=online_mean, scale=target_scale))\n    fixed_mean_distribution = tfd.Independent(\n        tfd.Normal(loc=target_mean, scale=online_scale))\n\n    # Compute the decomposed policy losses.\n    loss_policy_mean = compute_cross_entropy_loss(\n        actions, normalized_weights, fixed_stddev_distribution)\n    loss_policy_stddev = compute_cross_entropy_loss(\n        actions, normalized_weights, fixed_mean_distribution)\n\n    # Compute the decomposed KL between the target and online policies.\n    if self._per_dim_constraining:\n      kl_mean = target_action_distribution.distribution.kl_divergence(\n          fixed_stddev_distribution.distribution)  # Shape [B, D].\n      kl_stddev = target_action_distribution.distribution.kl_divergence(\n          fixed_mean_distribution.distribution)  # Shape [B, D].\n    else:\n      kl_mean = target_action_distribution.kl_divergence(\n          fixed_stddev_distribution)  # Shape [B].\n      kl_stddev = target_action_distribution.kl_divergence(\n          fixed_mean_distribution)  # Shape [B].\n\n    # Compute the alpha-weighted KL-penalty and dual losses to adapt the alphas.\n    loss_kl_mean, loss_alpha_mean = compute_parametric_kl_penalty_and_dual_loss(\n        kl_mean, alpha_mean, self._epsilon_mean)\n    loss_kl_stddev, loss_alpha_stddev = compute_parametric_kl_penalty_and_dual_loss(\n        kl_stddev, alpha_stddev, self._epsilon_stddev)\n\n    # Combine losses.\n    loss_policy = loss_policy_mean + loss_policy_stddev\n    loss_kl_penalty = loss_kl_mean + loss_kl_stddev\n    loss_dual = loss_alpha_mean + loss_alpha_stddev + loss_temperature\n    loss = loss_policy + loss_kl_penalty + loss_dual\n\n    stats = {}\n    # Dual Variables.\n    stats[\"dual_alpha_mean\"] = tf.reduce_mean(alpha_mean)\n    stats[\"dual_alpha_stddev\"] = tf.reduce_mean(alpha_stddev)\n    stats[\"dual_temperature\"] = tf.reduce_mean(temperature)\n    # Losses.\n    stats[\"loss_policy\"] = tf.reduce_mean(loss)\n    stats[\"loss_alpha\"] = tf.reduce_mean(loss_alpha_mean + loss_alpha_stddev)\n    stats[\"loss_temperature\"] = tf.reduce_mean(loss_temperature)\n    # KL measurements.\n    stats[\"kl_q_rel\"] = tf.reduce_mean(kl_nonparametric) / self._epsilon\n\n    if self._action_penalization:\n      stats[\"penalty_kl_q_rel\"] = tf.reduce_mean(\n          penalty_kl_nonparametric) / self._epsilon_penalty\n\n    stats[\"kl_mean_rel\"] = tf.reduce_mean(kl_mean) / self._epsilon_mean\n    stats[\"kl_stddev_rel\"] = tf.reduce_mean(kl_stddev) / self._epsilon_stddev\n    if self._per_dim_constraining:\n      # When KL is constrained per-dimension, we also log per-dimension min and\n      # max of mean/std of the realized KL costs.\n      stats[\"kl_mean_rel_min\"] = tf.reduce_min(tf.reduce_mean(\n          kl_mean, axis=0)) / self._epsilon_mean\n      stats[\"kl_mean_rel_max\"] = tf.reduce_max(tf.reduce_mean(\n          kl_mean, axis=0)) / self._epsilon_mean\n      stats[\"kl_stddev_rel_min\"] = tf.reduce_min(\n          tf.reduce_mean(kl_stddev, axis=0)) / self._epsilon_stddev\n      stats[\"kl_stddev_rel_max\"] = tf.reduce_max(\n          tf.reduce_mean(kl_stddev, axis=0)) / self._epsilon_stddev\n    # Q measurements.\n    stats[\"q_min\"] = tf.reduce_mean(tf.reduce_min(q_values, axis=0))\n    stats[\"q_max\"] = tf.reduce_mean(tf.reduce_max(q_values, axis=0))\n    # If the policy has standard deviation, log summary stats for this as well.\n    pi_stddev = online_action_distribution.distribution.stddev()\n    stats[\"pi_stddev_min\"] = tf.reduce_mean(tf.reduce_min(pi_stddev, axis=-1))\n    stats[\"pi_stddev_max\"] = tf.reduce_mean(tf.reduce_max(pi_stddev, axis=-1))\n    # Condition number of the diagonal covariance (actually, stddev) matrix.\n    stats[\"pi_stddev_cond\"] = tf.reduce_mean(\n        tf.reduce_max(pi_stddev, axis=-1) / tf.reduce_min(pi_stddev, axis=-1))\n\n    return loss, stats",
  "def compute_weights_and_temperature_loss(\n    q_values: tf.Tensor,\n    epsilon: float,\n    temperature: tf.Variable,\n) -> Tuple[tf.Tensor, tf.Tensor]:\n  \"\"\"Computes normalized importance weights for the policy optimization.\n\n  Args:\n    q_values: Q-values associated with the actions sampled from the target\n      policy; expected shape [N, B].\n    epsilon: Desired constraint on the KL between the target and non-parametric\n      policies.\n    temperature: Scalar used to temper the Q-values before computing normalized\n      importance weights from them. This is really the Lagrange dual variable\n      in the constrained optimization problem, the solution of which is the\n      non-parametric policy targeted by the policy loss.\n\n  Returns:\n    Normalized importance weights, used for policy optimization.\n    Temperature loss, used to adapt the temperature.\n  \"\"\"\n\n  # Temper the given Q-values using the current temperature.\n  tempered_q_values = tf.stop_gradient(q_values) / temperature\n\n  # Compute the normalized importance weights used to compute expectations with\n  # respect to the non-parametric policy.\n  normalized_weights = tf.nn.softmax(tempered_q_values, axis=0)\n  normalized_weights = tf.stop_gradient(normalized_weights)\n\n  # Compute the temperature loss (dual of the E-step optimization problem).\n  q_logsumexp = tf.reduce_logsumexp(tempered_q_values, axis=0)\n  log_num_actions = tf.math.log(tf.cast(q_values.shape[0], tf.float32))\n  loss_temperature = epsilon + tf.reduce_mean(q_logsumexp) - log_num_actions\n  loss_temperature = temperature * loss_temperature\n\n  return normalized_weights, loss_temperature",
  "def compute_nonparametric_kl_from_normalized_weights(\n    normalized_weights: tf.Tensor) -> tf.Tensor:\n  \"\"\"Estimate the actualized KL between the non-parametric and target policies.\"\"\"\n\n  # Compute integrand.\n  num_action_samples = tf.cast(normalized_weights.shape[0], tf.float32)\n  integrand = tf.math.log(num_action_samples * normalized_weights + 1e-8)\n\n  # Return the expectation with respect to the non-parametric policy.\n  return tf.reduce_sum(normalized_weights * integrand, axis=0)",
  "def compute_cross_entropy_loss(\n    sampled_actions: tf.Tensor,\n    normalized_weights: tf.Tensor,\n    online_action_distribution: tfp.distributions.Distribution,\n) -> tf.Tensor:\n  \"\"\"Compute cross-entropy online and the reweighted target policy.\n\n  Args:\n    sampled_actions: samples used in the Monte Carlo integration in the policy\n      loss. Expected shape is [N, B, ...], where N is the number of sampled\n      actions and B is the number of sampled states.\n    normalized_weights: target policy multiplied by the exponentiated Q values\n      and normalized; expected shape is [N, B].\n    online_action_distribution: policy to be optimized.\n\n  Returns:\n    loss_policy_gradient: the cross-entropy loss that, when differentiated,\n      produces the policy gradient.\n  \"\"\"\n\n  # Compute the M-step loss.\n  log_prob = online_action_distribution.log_prob(sampled_actions)\n\n  # Compute the weighted average log-prob using the normalized weights.\n  loss_policy_gradient = -tf.reduce_sum(log_prob * normalized_weights, axis=0)\n\n  # Return the mean loss over the batch of states.\n  return tf.reduce_mean(loss_policy_gradient, axis=0)",
  "def compute_parametric_kl_penalty_and_dual_loss(\n    kl: tf.Tensor,\n    alpha: tf.Variable,\n    epsilon: float,\n) -> Tuple[tf.Tensor, tf.Tensor]:\n  \"\"\"Computes the KL cost to be added to the Lagragian and its dual loss.\n\n  The KL cost is simply the alpha-weighted KL divergence and it is added as a\n  regularizer to the policy loss. The dual variable alpha itself has a loss that\n  can be minimized to adapt the strength of the regularizer to keep the KL\n  between consecutive updates at the desired target value of epsilon.\n\n  Args:\n    kl: KL divergence between the target and online policies.\n    alpha: Lagrange multipliers (dual variables) for the KL constraints.\n    epsilon: Desired value for the KL.\n\n  Returns:\n    loss_kl: alpha-weighted KL regularization to be added to the policy loss.\n    loss_alpha: The Lagrange dual loss minimized to adapt alpha.\n  \"\"\"\n\n  # Compute the mean KL over the batch.\n  mean_kl = tf.reduce_mean(kl, axis=0)\n\n  # Compute the regularization.\n  loss_kl = tf.reduce_sum(tf.stop_gradient(alpha) * mean_kl)\n\n  # Compute the dual loss.\n  loss_alpha = tf.reduce_sum(alpha * (epsilon - tf.stop_gradient(mean_kl)))\n\n  return loss_kl, loss_alpha",
  "def __init__(self,\n               epsilon: float,\n               epsilon_mean: float,\n               epsilon_stddev: float,\n               init_log_temperature: float,\n               init_log_alpha_mean: float,\n               init_log_alpha_stddev: float,\n               per_dim_constraining: bool = True,\n               action_penalization: bool = True,\n               epsilon_penalty: float = 0.001,\n               name: str = \"MPO\"):\n    \"\"\"Initialize and configure the MPO loss.\n\n    Args:\n      epsilon: KL constraint on the non-parametric auxiliary policy, the one\n        associated with the dual variable called temperature.\n      epsilon_mean: KL constraint on the mean of the Gaussian policy, the one\n        associated with the dual variable called alpha_mean.\n      epsilon_stddev: KL constraint on the stddev of the Gaussian policy, the\n        one associated with the dual variable called alpha_mean.\n      init_log_temperature: initial value for the temperature in log-space, note\n        a softplus (rather than an exp) will be used to transform this.\n      init_log_alpha_mean: initial value for the alpha_mean in log-space, note\n        a softplus (rather than an exp) will be used to transform this.\n      init_log_alpha_stddev: initial value for the alpha_stddev in log-space,\n        note a softplus (rather than an exp) will be used to transform this.\n      per_dim_constraining: whether to enforce the KL constraint on each\n        dimension independently; this is the default. Otherwise the overall KL\n        is constrained, which allows some dimensions to change more at the\n        expense of others staying put.\n      action_penalization: whether to use a KL constraint to penalize actions\n        via the MO-MPO algorithm.\n      epsilon_penalty: KL constraint on the probability of violating the action\n        constraint.\n      name: a name for the module, passed directly to snt.Module.\n\n    \"\"\"\n    super().__init__(name=name)\n\n    # MPO constrain thresholds.\n    self._epsilon = tf.constant(epsilon)\n    self._epsilon_mean = tf.constant(epsilon_mean)\n    self._epsilon_stddev = tf.constant(epsilon_stddev)\n\n    # Initial values for the constraints' dual variables.\n    self._init_log_temperature = init_log_temperature\n    self._init_log_alpha_mean = init_log_alpha_mean\n    self._init_log_alpha_stddev = init_log_alpha_stddev\n\n    # Whether to penalize out-of-bound actions via MO-MPO and its corresponding\n    # constraint threshold.\n    self._action_penalization = action_penalization\n    self._epsilon_penalty = tf.constant(epsilon_penalty)\n\n    # Whether to ensure per-dimension KL constraint satisfication.\n    self._per_dim_constraining = per_dim_constraining",
  "def create_dual_variables_once(self, shape: tf.TensorShape, dtype: tf.DType):\n    \"\"\"Creates the dual variables the first time the loss module is called.\"\"\"\n\n    # Create the dual variables.\n    self._log_temperature = tf.Variable(\n        initial_value=[self._init_log_temperature],\n        dtype=dtype,\n        name=\"log_temperature\",\n        shape=(1,))\n    self._log_alpha_mean = tf.Variable(\n        initial_value=tf.fill(shape, self._init_log_alpha_mean),\n        dtype=dtype,\n        name=\"log_alpha_mean\",\n        shape=shape)\n    self._log_alpha_stddev = tf.Variable(\n        initial_value=tf.fill(shape, self._init_log_alpha_stddev),\n        dtype=dtype,\n        name=\"log_alpha_stddev\",\n        shape=shape)\n\n    # Cast constraint thresholds to the expected dtype.\n    self._epsilon = tf.cast(self._epsilon, dtype)\n    self._epsilon_mean = tf.cast(self._epsilon_mean, dtype)\n    self._epsilon_stddev = tf.cast(self._epsilon_stddev, dtype)\n\n    # Maybe create the action penalization dual variable.\n    if self._action_penalization:\n      self._epsilon_penalty = tf.cast(self._epsilon_penalty, dtype)\n      self._log_penalty_temperature = tf.Variable(\n          initial_value=[self._init_log_temperature],\n          dtype=dtype,\n          name=\"log_penalty_temperature\",\n          shape=(1,))",
  "def __call__(\n      self,\n      online_action_distribution: Union[tfd.MultivariateNormalDiag,\n                                        tfd.Independent],\n      target_action_distribution: Union[tfd.MultivariateNormalDiag,\n                                        tfd.Independent],\n      actions: tf.Tensor,  # Shape [N, B, D].\n      q_values: tf.Tensor,  # Shape [N, B].\n  ) -> Tuple[tf.Tensor, Dict[str, tf.Tensor]]:\n    \"\"\"Computes the decoupled MPO loss.\n\n    Args:\n      online_action_distribution: online distribution returned by the online\n        policy network; expects batch_dims of [B] and event_dims of [D].\n      target_action_distribution: target distribution returned by the target\n        policy network; expects same shapes as online distribution.\n      actions: actions sampled from the target policy; expects shape [N, B, D].\n      q_values: Q-values associated with each action; expects shape [N, B].\n\n    Returns:\n      Loss, combining the policy loss, KL penalty, and dual losses required to\n        adapt the dual variables.\n      Stats, for diagnostics and tracking performance.\n    \"\"\"\n\n    # Cast `MultivariateNormalDiag`s to Independent Normals.\n    # The latter allows us to satisfy KL constraints per-dimension.\n    if isinstance(target_action_distribution, tfd.MultivariateNormalDiag):\n      target_action_distribution = tfd.Independent(\n          tfd.Normal(target_action_distribution.mean(),\n                     target_action_distribution.stddev()))\n      online_action_distribution = tfd.Independent(\n          tfd.Normal(online_action_distribution.mean(),\n                     online_action_distribution.stddev()))\n\n    # Infer the shape and dtype of dual variables.\n    scalar_dtype = q_values.dtype\n    if self._per_dim_constraining:\n      dual_variable_shape = target_action_distribution.distribution.kl_divergence(\n          online_action_distribution.distribution).shape[1:]  # Should be [D].\n    else:\n      dual_variable_shape = target_action_distribution.kl_divergence(\n          online_action_distribution).shape[1:]  # Should be [1].\n\n    # Create dual variables for the KL constraints; only happens the first call.\n    self.create_dual_variables_once(dual_variable_shape, scalar_dtype)\n\n    # Project dual variables to ensure they stay positive.\n    min_log_temperature = tf.constant(-18.0, scalar_dtype)\n    min_log_alpha = tf.constant(-18.0, scalar_dtype)\n    self._log_temperature.assign(\n        tf.maximum(min_log_temperature, self._log_temperature))\n    self._log_alpha_mean.assign(tf.maximum(min_log_alpha, self._log_alpha_mean))\n    self._log_alpha_stddev.assign(\n        tf.maximum(min_log_alpha, self._log_alpha_stddev))\n\n    # Transform dual variables from log-space.\n    # Note: using softplus instead of exponential for numerical stability.\n    temperature = tf.math.softplus(self._log_temperature) + _MPO_FLOAT_EPSILON\n    alpha_mean = tf.math.softplus(self._log_alpha_mean) + _MPO_FLOAT_EPSILON\n    alpha_stddev = tf.math.softplus(self._log_alpha_stddev) + _MPO_FLOAT_EPSILON\n\n    # Get online and target means and stddevs in preparation for decomposition.\n    online_mean = online_action_distribution.distribution.mean()\n    online_scale = online_action_distribution.distribution.stddev()\n    target_mean = target_action_distribution.distribution.mean()\n    target_scale = target_action_distribution.distribution.stddev()\n\n    # Compute normalized importance weights, used to compute expectations with\n    # respect to the non-parametric policy; and the temperature loss, used to\n    # adapt the tempering of Q-values.\n    normalized_weights, loss_temperature = compute_weights_and_temperature_loss(\n        q_values, self._epsilon, temperature)\n\n    # Only needed for diagnostics: Compute estimated actualized KL between the\n    # non-parametric and current target policies.\n    kl_nonparametric = compute_nonparametric_kl_from_normalized_weights(\n        normalized_weights)\n\n    if self._action_penalization:\n      # Project and transform action penalization temperature.\n      self._log_penalty_temperature.assign(\n          tf.maximum(min_log_temperature, self._log_penalty_temperature))\n      penalty_temperature = tf.math.softplus(\n          self._log_penalty_temperature) + _MPO_FLOAT_EPSILON\n\n      # Compute action penalization cost.\n      # Note: the cost is zero in [-1, 1] and quadratic beyond.\n      diff_out_of_bound = actions - tf.clip_by_value(actions, -1.0, 1.0)\n      cost_out_of_bound = -tf.norm(diff_out_of_bound, axis=-1)\n\n      penalty_normalized_weights, loss_penalty_temperature = compute_weights_and_temperature_loss(\n          cost_out_of_bound, self._epsilon_penalty, penalty_temperature)\n\n      # Only needed for diagnostics: Compute estimated actualized KL between the\n      # non-parametric and current target policies.\n      penalty_kl_nonparametric = compute_nonparametric_kl_from_normalized_weights(\n          penalty_normalized_weights)\n\n      # Combine normalized weights.\n      normalized_weights += penalty_normalized_weights\n      loss_temperature += loss_penalty_temperature\n    # Decompose the online policy into fixed-mean & fixed-stddev distributions.\n    # This has been documented as having better performance in bandit settings,\n    # see e.g. https://arxiv.org/pdf/1812.02256.pdf.\n    fixed_stddev_distribution = tfd.Independent(\n        tfd.Normal(loc=online_mean, scale=target_scale))\n    fixed_mean_distribution = tfd.Independent(\n        tfd.Normal(loc=target_mean, scale=online_scale))\n\n    # Compute the decomposed policy losses.\n    loss_policy_mean = compute_cross_entropy_loss(\n        actions, normalized_weights, fixed_stddev_distribution)\n    loss_policy_stddev = compute_cross_entropy_loss(\n        actions, normalized_weights, fixed_mean_distribution)\n\n    # Compute the decomposed KL between the target and online policies.\n    if self._per_dim_constraining:\n      kl_mean = target_action_distribution.distribution.kl_divergence(\n          fixed_stddev_distribution.distribution)  # Shape [B, D].\n      kl_stddev = target_action_distribution.distribution.kl_divergence(\n          fixed_mean_distribution.distribution)  # Shape [B, D].\n    else:\n      kl_mean = target_action_distribution.kl_divergence(\n          fixed_stddev_distribution)  # Shape [B].\n      kl_stddev = target_action_distribution.kl_divergence(\n          fixed_mean_distribution)  # Shape [B].\n\n    # Compute the alpha-weighted KL-penalty and dual losses to adapt the alphas.\n    loss_kl_mean, loss_alpha_mean = compute_parametric_kl_penalty_and_dual_loss(\n        kl_mean, alpha_mean, self._epsilon_mean)\n    loss_kl_stddev, loss_alpha_stddev = compute_parametric_kl_penalty_and_dual_loss(\n        kl_stddev, alpha_stddev, self._epsilon_stddev)\n\n    # Combine losses.\n    loss_policy = loss_policy_mean + loss_policy_stddev\n    loss_kl_penalty = loss_kl_mean + loss_kl_stddev\n    loss_dual = loss_alpha_mean + loss_alpha_stddev + loss_temperature\n    loss = loss_policy + loss_kl_penalty + loss_dual\n\n    stats = {}\n    # Dual Variables.\n    stats[\"dual_alpha_mean\"] = tf.reduce_mean(alpha_mean)\n    stats[\"dual_alpha_stddev\"] = tf.reduce_mean(alpha_stddev)\n    stats[\"dual_temperature\"] = tf.reduce_mean(temperature)\n    # Losses.\n    stats[\"loss_policy\"] = tf.reduce_mean(loss)\n    stats[\"loss_alpha\"] = tf.reduce_mean(loss_alpha_mean + loss_alpha_stddev)\n    stats[\"loss_temperature\"] = tf.reduce_mean(loss_temperature)\n    # KL measurements.\n    stats[\"kl_q_rel\"] = tf.reduce_mean(kl_nonparametric) / self._epsilon\n\n    if self._action_penalization:\n      stats[\"penalty_kl_q_rel\"] = tf.reduce_mean(\n          penalty_kl_nonparametric) / self._epsilon_penalty\n\n    stats[\"kl_mean_rel\"] = tf.reduce_mean(kl_mean) / self._epsilon_mean\n    stats[\"kl_stddev_rel\"] = tf.reduce_mean(kl_stddev) / self._epsilon_stddev\n    if self._per_dim_constraining:\n      # When KL is constrained per-dimension, we also log per-dimension min and\n      # max of mean/std of the realized KL costs.\n      stats[\"kl_mean_rel_min\"] = tf.reduce_min(tf.reduce_mean(\n          kl_mean, axis=0)) / self._epsilon_mean\n      stats[\"kl_mean_rel_max\"] = tf.reduce_max(tf.reduce_mean(\n          kl_mean, axis=0)) / self._epsilon_mean\n      stats[\"kl_stddev_rel_min\"] = tf.reduce_min(\n          tf.reduce_mean(kl_stddev, axis=0)) / self._epsilon_stddev\n      stats[\"kl_stddev_rel_max\"] = tf.reduce_max(\n          tf.reduce_mean(kl_stddev, axis=0)) / self._epsilon_stddev\n    # Q measurements.\n    stats[\"q_min\"] = tf.reduce_mean(tf.reduce_min(q_values, axis=0))\n    stats[\"q_max\"] = tf.reduce_mean(tf.reduce_max(q_values, axis=0))\n    # If the policy has standard deviation, log summary stats for this as well.\n    pi_stddev = online_action_distribution.distribution.stddev()\n    stats[\"pi_stddev_min\"] = tf.reduce_mean(tf.reduce_min(pi_stddev, axis=-1))\n    stats[\"pi_stddev_max\"] = tf.reduce_mean(tf.reduce_max(pi_stddev, axis=-1))\n    # Condition number of the diagonal covariance (actually, stddev) matrix.\n    stats[\"pi_stddev_cond\"] = tf.reduce_mean(\n        tf.reduce_max(pi_stddev, axis=-1) / tf.reduce_min(pi_stddev, axis=-1))\n\n    return loss, stats",
  "class QuantileDistribution(NamedTuple):\n  values: tf.Tensor\n  logits: tf.Tensor",
  "class NonUniformQuantileRegression(snt.Module):\n  \"\"\"Compute the quantile regression loss for the distributional TD error.\"\"\"\n\n  def __init__(\n      self,\n      huber_param: float = 0.,\n      name: str = 'NUQuantileRegression'):\n    \"\"\"Initializes the module.\n\n    Args:\n      huber_param: The point where the huber loss function changes from a\n        quadratic to linear.\n      name: name to use for grouping operations.\n    \"\"\"\n    super().__init__(name=name)\n    self._huber_param = huber_param\n\n  def __call__(\n      self,\n      q_tm1: QuantileDistribution,\n      r_t: tf.Tensor,\n      pcont_t: tf.Tensor,\n      q_t: QuantileDistribution,\n      tau: tf.Tensor,\n  ) -> tf.Tensor:\n    \"\"\"Calculates the loss.\n\n    Note that this is only defined for discrete quantile-valued distributions.\n    In particular we assume that the distributions define q.logits and\n    q.values.\n\n    Args:\n      q_tm1: the distribution at time t-1.\n      r_t: the reward at time t.\n      pcont_t: the discount factor at time t.\n      q_t: the target distribution.\n      tau: the quantile regression targets.\n\n    Returns:\n      Value of the loss.\n    \"\"\"\n    # Distributional Bellman update\n    values_t = (tf.reshape(r_t, (-1, 1)) +\n                tf.reshape(pcont_t, (-1, 1)) * q_t.values)\n    values_t = tf.stop_gradient(values_t)\n    probs_t = tf.nn.softmax(q_t.logits)\n\n    # Quantile regression loss\n    # Tau gives the quantile regression targets, where in the sample\n    # space [0, 1] each output should train towards\n    # Tau applies along the second dimension in delta (below)\n    tau = tf.expand_dims(tau, -1)\n\n    # quantile td-error and assymmetric weighting\n    delta = values_t[:, None, :] - q_tm1.values[:, :, None]\n    delta_neg = tf.cast(delta < 0., dtype=tf.float32)\n    # This stop_gradient is very important, do not remove\n    weight = tf.stop_gradient(tf.abs(tau - delta_neg))\n\n    # loss\n    loss = huber(delta, self._huber_param) * weight\n    loss = tf.reduce_sum(loss * probs_t[:, None, :], 2)\n\n    # Have not been able to get quite as good performance with mean vs. sum\n    loss = tf.reduce_sum(loss, -1)\n    return loss",
  "def __init__(\n      self,\n      huber_param: float = 0.,\n      name: str = 'NUQuantileRegression'):\n    \"\"\"Initializes the module.\n\n    Args:\n      huber_param: The point where the huber loss function changes from a\n        quadratic to linear.\n      name: name to use for grouping operations.\n    \"\"\"\n    super().__init__(name=name)\n    self._huber_param = huber_param",
  "def __call__(\n      self,\n      q_tm1: QuantileDistribution,\n      r_t: tf.Tensor,\n      pcont_t: tf.Tensor,\n      q_t: QuantileDistribution,\n      tau: tf.Tensor,\n  ) -> tf.Tensor:\n    \"\"\"Calculates the loss.\n\n    Note that this is only defined for discrete quantile-valued distributions.\n    In particular we assume that the distributions define q.logits and\n    q.values.\n\n    Args:\n      q_tm1: the distribution at time t-1.\n      r_t: the reward at time t.\n      pcont_t: the discount factor at time t.\n      q_t: the target distribution.\n      tau: the quantile regression targets.\n\n    Returns:\n      Value of the loss.\n    \"\"\"\n    # Distributional Bellman update\n    values_t = (tf.reshape(r_t, (-1, 1)) +\n                tf.reshape(pcont_t, (-1, 1)) * q_t.values)\n    values_t = tf.stop_gradient(values_t)\n    probs_t = tf.nn.softmax(q_t.logits)\n\n    # Quantile regression loss\n    # Tau gives the quantile regression targets, where in the sample\n    # space [0, 1] each output should train towards\n    # Tau applies along the second dimension in delta (below)\n    tau = tf.expand_dims(tau, -1)\n\n    # quantile td-error and assymmetric weighting\n    delta = values_t[:, None, :] - q_tm1.values[:, :, None]\n    delta_neg = tf.cast(delta < 0., dtype=tf.float32)\n    # This stop_gradient is very important, do not remove\n    weight = tf.stop_gradient(tf.abs(tau - delta_neg))\n\n    # loss\n    loss = huber(delta, self._huber_param) * weight\n    loss = tf.reduce_sum(loss * probs_t[:, None, :], 2)\n\n    # Have not been able to get quite as good performance with mean vs. sum\n    loss = tf.reduce_sum(loss, -1)\n    return loss",
  "def _reference_l2_project(src_support, src_probs, dst_support):\n  \"\"\"Multi-axis l2_project, implemented using single-axis l2_project.\n\n  This is for testing multiaxis_l2_project's consistency with l2_project,\n  when used with multi-axis support vs single-axis support.\n\n  Args:\n    src_support: Zp in l2_project.\n    src_probs: P in l2_project.\n    dst_support: Zq in l2_project.\n\n  Returns:\n    src_probs, projected onto dst_support.\n  \"\"\"\n  assert src_support.shape == src_probs.shape\n\n  # Remove the batch and value axes, and broadcast the rest to a common shape.\n  common_shape = np.broadcast(src_support[0, ..., 0],\n                              dst_support[..., 0]).shape\n\n  # If src_* have fewer internal axes than len(common_shape), insert size-1\n  # axes.\n  while src_support.ndim-2 < len(common_shape):\n    src_support = src_support[:, None, ...]\n\n  src_probs = np.reshape(src_probs, src_support.shape)\n\n  # Broadcast args' non-batch, non-value axes to common_shape.\n  src_support = np.broadcast_to(\n      src_support,\n      src_support.shape[:1] + common_shape + src_support.shape[-1:])\n  src_probs = np.broadcast_to(src_probs, src_support.shape)\n  dst_support = np.broadcast_to(\n      dst_support,\n      common_shape + dst_support.shape[-1:])\n\n  output_shape = (src_support.shape[0],) + dst_support.shape\n\n  # Collapse all but the first (batch) and last (atom) axes.\n  src_support = src_support.reshape(\n      [src_support.shape[0], -1, src_support.shape[-1]])\n  src_probs = src_probs.reshape(\n      [src_probs.shape[0], -1, src_probs.shape[-1]])\n\n  # Collapse all but the last (atom) axes.\n  dst_support = dst_support.reshape([-1, dst_support.shape[-1]])\n\n  dst_probs = np.zeros(src_support.shape[:1] + dst_support.shape,\n                       dtype=src_probs.dtype)\n\n  # iterate over all supports\n  for i in range(src_support.shape[1]):\n    s_support = tf.convert_to_tensor(src_support[:, i, :])\n    s_probs = tf.convert_to_tensor(src_probs[:, i, :])\n    d_support = tf.convert_to_tensor(dst_support[i, :])\n    d_probs = distributional.l2_project(s_support, s_probs, d_support)\n    dst_probs[:, i, :] = d_probs.numpy()\n\n  return dst_probs.reshape(output_shape)",
  "class L2ProjectTest(parameterized.TestCase):\n\n  @parameterized.parameters(\n      [(2, 11), (11,)],  # C = (), D = (), matching num_atoms (11 and 11)\n      [(2, 11), (5,)],  # C = (), D = (), differing num_atoms (11 and 5).\n      [(2, 3, 11), (3, 5)],  # C = (3,), D = (3,)\n      [(2, 1, 11), (3, 5)],  # C = (1,), D = (3,)\n      [(2, 3, 11), (1, 5)],  # (C = (3,), D = (1,)\n      [(2, 3, 4, 11), (3, 4, 5)],  # C = (3, 4), D = (3, 4)\n      [(2, 3, 4, 11), (4, 5)],  # C = (3, 4), D = (4,)\n      [(2, 4, 11), (3, 4, 5)],  # C = (4,), D = (3, 4)\n  )\n  def test_multiaxis(self, src_shape, dst_shape):\n    \"\"\"Tests consistency between multi-axis and single-axis l2_project.\n\n    This calls l2_project on multi-axis supports, and checks that it gets\n    the same outcomes as many calls to single-axis supports.\n\n    Args:\n      src_shape: Shape of source support. Includes a leading batch axis.\n      dst_shape: Shape of destination support.\n        Does not include a leading batch axis.\n    \"\"\"\n    # src_shape includes a leading batch axis, whereas dst_shape does not.\n    # assert len(src_shape) >= (1 + len(dst_shape))\n\n    def make_support(shape, minimum):\n      \"\"\"Creates a ndarray of supports.\"\"\"\n      values = np.linspace(start=minimum, stop=minimum+100, num=shape[-1])\n      offsets = np.arange(np.prod(shape[:-1]))\n      result = values[None, :] + offsets[:, None]\n      return result.reshape(shape)\n\n    src_support = make_support(src_shape, -1)\n    dst_support = make_support(dst_shape, -.75)\n\n    rng = np.random.RandomState(1)\n    src_probs = rng.uniform(low=1.0, high=2.0, size=src_shape)\n    src_probs /= src_probs.sum()\n\n    # Repeated calls to l2_project using single-axis supports.\n    expected_dst_probs = _reference_l2_project(src_support,\n                                               src_probs,\n                                               dst_support)\n\n    # A single call to l2_project, with multi-axis supports.\n    dst_probs = distributional.multiaxis_l2_project(\n        tf.convert_to_tensor(src_support),\n        tf.convert_to_tensor(src_probs),\n        tf.convert_to_tensor(dst_support)).numpy()\n\n    npt.assert_allclose(dst_probs, expected_dst_probs)\n\n  @parameterized.parameters(\n      # Same src and dst support shape, dst support is shifted by +.25\n      ([[0., 1, 2, 3]],\n       [[0., 1, 0, 0]],\n       [.25, 1.25, 2.25, 3.25],\n       [[.25, .75, 0, 0]]),\n      # Similar to above, but with batched src.\n      ([[0., 1, 2, 3],\n        [0., 1, 2, 3]],\n       [[0., 1, 0, 0],\n        [0., 0, 1, 0]],\n       [.25, 1.25, 2.25, 3.25],\n       [[.25, .75, 0, 0],\n        [0., .25, .75, 0]]),\n      # Similar to above, but src_probs has two 0.5's, instead of being one-hot.\n      ([[0., 1, 2, 3]],\n       [[0., .5, .5, 0]],\n       [.25, 1.25, 2.25, 3.25],\n       0.5 * (np.array([[.25, .75, 0, 0]]) + np.array([[0., .25, .75, 0]]))),\n      # src and dst support have differing sizes\n      ([[0., 1, 2, 3]],\n       [[0., 1, 0, 0]],\n       [0.00, 0.25, 0.50, 0.75, 1.00, 1.25, 1.50, 1.75, 2.00, 2.25, 2.50],\n       [[0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]]),\n      )\n  def test_l2_projection(\n      self, src_support, src_probs, dst_support, expected_dst_probs):\n\n    dst_probs = distributional.multiaxis_l2_project(\n        tf.convert_to_tensor(src_support),\n        tf.convert_to_tensor(src_probs),\n        tf.convert_to_tensor(dst_support)).numpy()\n    npt.assert_allclose(dst_probs, expected_dst_probs)",
  "def test_multiaxis(self, src_shape, dst_shape):\n    \"\"\"Tests consistency between multi-axis and single-axis l2_project.\n\n    This calls l2_project on multi-axis supports, and checks that it gets\n    the same outcomes as many calls to single-axis supports.\n\n    Args:\n      src_shape: Shape of source support. Includes a leading batch axis.\n      dst_shape: Shape of destination support.\n        Does not include a leading batch axis.\n    \"\"\"\n    # src_shape includes a leading batch axis, whereas dst_shape does not.\n    # assert len(src_shape) >= (1 + len(dst_shape))\n\n    def make_support(shape, minimum):\n      \"\"\"Creates a ndarray of supports.\"\"\"\n      values = np.linspace(start=minimum, stop=minimum+100, num=shape[-1])\n      offsets = np.arange(np.prod(shape[:-1]))\n      result = values[None, :] + offsets[:, None]\n      return result.reshape(shape)\n\n    src_support = make_support(src_shape, -1)\n    dst_support = make_support(dst_shape, -.75)\n\n    rng = np.random.RandomState(1)\n    src_probs = rng.uniform(low=1.0, high=2.0, size=src_shape)\n    src_probs /= src_probs.sum()\n\n    # Repeated calls to l2_project using single-axis supports.\n    expected_dst_probs = _reference_l2_project(src_support,\n                                               src_probs,\n                                               dst_support)\n\n    # A single call to l2_project, with multi-axis supports.\n    dst_probs = distributional.multiaxis_l2_project(\n        tf.convert_to_tensor(src_support),\n        tf.convert_to_tensor(src_probs),\n        tf.convert_to_tensor(dst_support)).numpy()\n\n    npt.assert_allclose(dst_probs, expected_dst_probs)",
  "def test_l2_projection(\n      self, src_support, src_probs, dst_support, expected_dst_probs):\n\n    dst_probs = distributional.multiaxis_l2_project(\n        tf.convert_to_tensor(src_support),\n        tf.convert_to_tensor(src_probs),\n        tf.convert_to_tensor(dst_support)).numpy()\n    npt.assert_allclose(dst_probs, expected_dst_probs)",
  "def make_support(shape, minimum):\n      \"\"\"Creates a ndarray of supports.\"\"\"\n      values = np.linspace(start=minimum, stop=minimum+100, num=shape[-1])\n      offsets = np.arange(np.prod(shape[:-1]))\n      result = values[None, :] + offsets[:, None]\n      return result.reshape(shape)",
  "class KLConstraint:\n  \"\"\"Defines a per-objective policy improvement step constraint for MO-MPO.\"\"\"\n\n  name: str\n  value: float\n\n  def __post_init__(self):\n    if self.value < 0:\n      raise ValueError(\"KL constraint epsilon must be non-negative.\")",
  "class MultiObjectiveMPO(snt.Module):\n  \"\"\"Multi-objective MPO loss with decoupled KL constraints.\n\n  This implementation of the MO-MPO loss is based on the approach proposed in\n  (Abdolmaleki, Huang et al., 2020). The following features are included as\n  options:\n  - Satisfying the KL-constraint on a per-dimension basis (on by default)\n\n  (Abdolmaleki, Huang et al., 2020): https://arxiv.org/pdf/2005.07513.pdf\n  \"\"\"\n\n  def __init__(self,\n               epsilons: Sequence[KLConstraint],\n               epsilon_mean: float,\n               epsilon_stddev: float,\n               init_log_temperature: float,\n               init_log_alpha_mean: float,\n               init_log_alpha_stddev: float,\n               per_dim_constraining: bool = True,\n               name: str = \"MOMPO\"):\n    \"\"\"Initialize and configure the MPO loss.\n\n    Args:\n      epsilons: per-objective KL constraints on the non-parametric auxiliary\n        policy, the one associated with the dual variables called temperature;\n        expected length K.\n      epsilon_mean: KL constraint on the mean of the Gaussian policy, the one\n        associated with the dual variable called alpha_mean.\n      epsilon_stddev: KL constraint on the stddev of the Gaussian policy, the\n        one associated with the dual variable called alpha_mean.\n      init_log_temperature: initial value for the temperature in log-space, note\n        a softplus (rather than an exp) will be used to transform this.\n      init_log_alpha_mean: initial value for the alpha_mean in log-space, note\n        a softplus (rather than an exp) will be used to transform this.\n      init_log_alpha_stddev: initial value for the alpha_stddev in log-space,\n        note a softplus (rather than an exp) will be used to transform this.\n      per_dim_constraining: whether to enforce the KL constraint on each\n        dimension independently; this is the default. Otherwise the overall KL\n        is constrained, which allows some dimensions to change more at the\n        expense of others staying put.\n      name: a name for the module, passed directly to snt.Module.\n\n    \"\"\"\n    super().__init__(name=name)\n\n    # MO-MPO constraint thresholds.\n    self._epsilons = tf.constant([x.value for x in epsilons])\n    self._epsilon_mean = tf.constant(epsilon_mean)\n    self._epsilon_stddev = tf.constant(epsilon_stddev)\n\n    # Initial values for the constraints' dual variables.\n    self._init_log_temperature = init_log_temperature\n    self._init_log_alpha_mean = init_log_alpha_mean\n    self._init_log_alpha_stddev = init_log_alpha_stddev\n\n    # Whether to ensure per-dimension KL constraint satisfication.\n    self._per_dim_constraining = per_dim_constraining\n\n    # Remember the number of objectives\n    self._num_objectives = len(epsilons)  # K = number of objectives\n    self._objective_names = [x.name for x in epsilons]\n\n    # Make sure there are no duplicate objective names\n    if len(self._objective_names) != len(set(self._objective_names)):\n      raise ValueError(\"Duplicate objective names are not allowed.\")\n\n  @property\n  def objective_names(self):\n    return self._objective_names\n\n  @snt.once\n  def create_dual_variables_once(self, shape: tf.TensorShape, dtype: tf.DType):\n    \"\"\"Creates the dual variables the first time the loss module is called.\"\"\"\n\n    # Create the dual variables.\n    self._log_temperature = tf.Variable(\n        initial_value=[self._init_log_temperature] * self._num_objectives,\n        dtype=dtype,\n        name=\"log_temperature\",\n        shape=(self._num_objectives,))\n    self._log_alpha_mean = tf.Variable(\n        initial_value=tf.fill(shape, self._init_log_alpha_mean),\n        dtype=dtype,\n        name=\"log_alpha_mean\",\n        shape=shape)\n    self._log_alpha_stddev = tf.Variable(\n        initial_value=tf.fill(shape, self._init_log_alpha_stddev),\n        dtype=dtype,\n        name=\"log_alpha_stddev\",\n        shape=shape)\n\n    # Cast constraint thresholds to the expected dtype.\n    self._epsilons = tf.cast(self._epsilons, dtype)\n    self._epsilon_mean = tf.cast(self._epsilon_mean, dtype)\n    self._epsilon_stddev = tf.cast(self._epsilon_stddev, dtype)\n\n  def __call__(\n      self,\n      online_action_distribution: Union[tfd.MultivariateNormalDiag,\n                                        tfd.Independent],\n      target_action_distribution: Union[tfd.MultivariateNormalDiag,\n                                        tfd.Independent],\n      actions: tf.Tensor,  # Shape [N, B, D].\n      q_values: tf.Tensor,  # Shape [N, B, K].\n  ) -> Tuple[tf.Tensor, Dict[str, tf.Tensor]]:\n    \"\"\"Computes the decoupled MO-MPO loss.\n\n    Args:\n      online_action_distribution: online distribution returned by the online\n        policy network; expects batch_dims of [B] and event_dims of [D].\n      target_action_distribution: target distribution returned by the target\n        policy network; expects same shapes as online distribution.\n      actions: actions sampled from the target policy; expects shape [N, B, D].\n      q_values: Q-values associated with each action; expects shape [N, B, K].\n\n    Returns:\n      Loss, combining the policy loss, KL penalty, and dual losses required to\n        adapt the dual variables.\n      Stats, for diagnostics and tracking performance.\n    \"\"\"\n\n    # Make sure the Q-values are per-objective\n    q_values.get_shape().assert_has_rank(3)\n    if q_values.get_shape()[-1] != self._num_objectives:\n      raise ValueError(\"Q-values do not match expected number of objectives.\")\n\n    # Cast `MultivariateNormalDiag`s to Independent Normals.\n    # The latter allows us to satisfy KL constraints per-dimension.\n    if isinstance(target_action_distribution, tfd.MultivariateNormalDiag):\n      target_action_distribution = tfd.Independent(\n          tfd.Normal(target_action_distribution.mean(),\n                     target_action_distribution.stddev()))\n      online_action_distribution = tfd.Independent(\n          tfd.Normal(online_action_distribution.mean(),\n                     online_action_distribution.stddev()))\n\n    # Infer the shape and dtype of dual variables.\n    scalar_dtype = q_values.dtype\n    if self._per_dim_constraining:\n      dual_variable_shape = target_action_distribution.distribution.kl_divergence(\n          online_action_distribution.distribution).shape[1:]  # Should be [D].\n    else:\n      dual_variable_shape = target_action_distribution.kl_divergence(\n          online_action_distribution).shape[1:]  # Should be [1].\n\n    # Create dual variables for the KL constraints; only happens the first call.\n    self.create_dual_variables_once(dual_variable_shape, scalar_dtype)\n\n    # Project dual variables to ensure they stay positive.\n    min_log_temperature = tf.constant(-18.0, scalar_dtype)\n    min_log_alpha = tf.constant(-18.0, scalar_dtype)\n    self._log_temperature.assign(\n        tf.maximum(min_log_temperature, self._log_temperature))\n    self._log_alpha_mean.assign(tf.maximum(min_log_alpha, self._log_alpha_mean))\n    self._log_alpha_stddev.assign(\n        tf.maximum(min_log_alpha, self._log_alpha_stddev))\n\n    # Transform dual variables from log-space.\n    # Note: using softplus instead of exponential for numerical stability.\n    temperature = tf.math.softplus(self._log_temperature) + _MPO_FLOAT_EPSILON\n    alpha_mean = tf.math.softplus(self._log_alpha_mean) + _MPO_FLOAT_EPSILON\n    alpha_stddev = tf.math.softplus(self._log_alpha_stddev) + _MPO_FLOAT_EPSILON\n\n    # Get online and target means and stddevs in preparation for decomposition.\n    online_mean = online_action_distribution.distribution.mean()\n    online_scale = online_action_distribution.distribution.stddev()\n    target_mean = target_action_distribution.distribution.mean()\n    target_scale = target_action_distribution.distribution.stddev()\n\n    # Compute normalized importance weights, used to compute expectations with\n    # respect to the non-parametric policy; and the temperature loss, used to\n    # adapt the tempering of Q-values.\n    normalized_weights, loss_temperature = compute_weights_and_temperature_loss(\n        q_values, self._epsilons, temperature)  # Shapes [N, B, K] and [1, K].\n    normalized_weights_sum = tf.reduce_sum(normalized_weights, axis=-1)\n    loss_temperature_mean = tf.reduce_mean(loss_temperature)\n\n    # Only needed for diagnostics: Compute estimated actualized KL between the\n    # non-parametric and current target policies.\n    kl_nonparametric = mpo.compute_nonparametric_kl_from_normalized_weights(\n        normalized_weights)\n\n    # Decompose the online policy into fixed-mean & fixed-stddev distributions.\n    # This has been documented as having better performance in bandit settings,\n    # see e.g. https://arxiv.org/pdf/1812.02256.pdf.\n    fixed_stddev_distribution = tfd.Independent(\n        tfd.Normal(loc=online_mean, scale=target_scale))\n    fixed_mean_distribution = tfd.Independent(\n        tfd.Normal(loc=target_mean, scale=online_scale))\n\n    # Compute the decomposed policy losses.\n    loss_policy_mean = mpo.compute_cross_entropy_loss(\n        actions, normalized_weights_sum, fixed_stddev_distribution)\n    loss_policy_stddev = mpo.compute_cross_entropy_loss(\n        actions, normalized_weights_sum, fixed_mean_distribution)\n\n    # Compute the decomposed KL between the target and online policies.\n    if self._per_dim_constraining:\n      kl_mean = target_action_distribution.distribution.kl_divergence(\n          fixed_stddev_distribution.distribution)  # Shape [B, D].\n      kl_stddev = target_action_distribution.distribution.kl_divergence(\n          fixed_mean_distribution.distribution)  # Shape [B, D].\n    else:\n      kl_mean = target_action_distribution.kl_divergence(\n          fixed_stddev_distribution)  # Shape [B].\n      kl_stddev = target_action_distribution.kl_divergence(\n          fixed_mean_distribution)  # Shape [B].\n\n    # Compute the alpha-weighted KL-penalty and dual losses to adapt the alphas.\n    loss_kl_mean, loss_alpha_mean = mpo.compute_parametric_kl_penalty_and_dual_loss(\n        kl_mean, alpha_mean, self._epsilon_mean)\n    loss_kl_stddev, loss_alpha_stddev = mpo.compute_parametric_kl_penalty_and_dual_loss(\n        kl_stddev, alpha_stddev, self._epsilon_stddev)\n\n    # Combine losses.\n    loss_policy = loss_policy_mean + loss_policy_stddev\n    loss_kl_penalty = loss_kl_mean + loss_kl_stddev\n    loss_dual = loss_alpha_mean + loss_alpha_stddev + loss_temperature_mean\n    loss = loss_policy + loss_kl_penalty + loss_dual\n\n    stats = {}\n    # Dual Variables.\n    stats[\"dual_alpha_mean\"] = tf.reduce_mean(alpha_mean)\n    stats[\"dual_alpha_stddev\"] = tf.reduce_mean(alpha_stddev)\n    # Losses.\n    stats[\"loss_policy\"] = tf.reduce_mean(loss)\n    stats[\"loss_alpha\"] = tf.reduce_mean(loss_alpha_mean + loss_alpha_stddev)\n    # KL measurements.\n    stats[\"kl_mean_rel\"] = tf.reduce_mean(kl_mean, axis=0) / self._epsilon_mean\n    stats[\"kl_stddev_rel\"] = tf.reduce_mean(\n        kl_stddev, axis=0) / self._epsilon_stddev\n    # If the policy has standard deviation, log summary stats for this as well.\n    pi_stddev = online_action_distribution.distribution.stddev()\n    stats[\"pi_stddev_min\"] = tf.reduce_mean(tf.reduce_min(pi_stddev, axis=-1))\n    stats[\"pi_stddev_max\"] = tf.reduce_mean(tf.reduce_max(pi_stddev, axis=-1))\n\n    # Condition number of the diagonal covariance (actually, stddev) matrix.\n    stats[\"pi_stddev_cond\"] = tf.reduce_mean(\n        tf.reduce_max(pi_stddev, axis=-1) / tf.reduce_min(pi_stddev, axis=-1))\n\n    # Log per-objective values.\n    for i, name in enumerate(self._objective_names):\n      stats[\"{}_dual_temperature\".format(name)] = temperature[i]\n      stats[\"{}_loss_temperature\".format(name)] = loss_temperature[i]\n      stats[\"{}_kl_q_rel\".format(name)] = tf.reduce_mean(\n          kl_nonparametric[:, i]) / self._epsilons[i]\n\n      # Q measurements.\n      stats[\"{}_q_min\".format(name)] = tf.reduce_mean(tf.reduce_min(\n          q_values, axis=0)[:, i])\n      stats[\"{}_q_mean\".format(name)] = tf.reduce_mean(tf.reduce_mean(\n          q_values, axis=0)[:, i])\n      stats[\"{}_q_max\".format(name)] = tf.reduce_mean(tf.reduce_max(\n          q_values, axis=0)[:, i])\n\n    return loss, stats",
  "def compute_weights_and_temperature_loss(\n    q_values: tf.Tensor,\n    epsilons: tf.Tensor,\n    temperature: tf.Variable,\n) -> Tuple[tf.Tensor, tf.Tensor]:\n  \"\"\"Computes normalized importance weights for the policy optimization.\n\n  Args:\n    q_values: Q-values associated with the actions sampled from the target\n      policy; expected shape [N, B, K].\n    epsilons: Desired per-objective constraints on the KL between the target\n      and non-parametric policies; expected shape [K].\n    temperature: Per-objective scalar used to temper the Q-values before\n      computing normalized importance weights from them; expected shape [K].\n      This is really the Lagrange dual variable in the constrained optimization\n      problem, the solution of which is the non-parametric policy targeted by\n      the policy loss.\n\n  Returns:\n    Normalized importance weights, used for policy optimization; shape [N,B,K].\n    Temperature loss, used to adapt the temperature; shape [1, K].\n  \"\"\"\n\n  # Temper the given Q-values using the current temperature.\n  tempered_q_values = tf.stop_gradient(q_values) / temperature[None, None, :]\n\n  # Compute the normalized importance weights used to compute expectations with\n  # respect to the non-parametric policy.\n  normalized_weights = tf.nn.softmax(tempered_q_values, axis=0)\n  normalized_weights = tf.stop_gradient(normalized_weights)\n\n  # Compute the temperature loss (dual of the E-step optimization problem).\n  q_logsumexp = tf.reduce_logsumexp(tempered_q_values, axis=0)\n  log_num_actions = tf.math.log(tf.cast(q_values.shape[0], tf.float32))\n  loss_temperature = (\n      epsilons + tf.reduce_mean(q_logsumexp, axis=0) - log_num_actions)\n  loss_temperature = temperature * loss_temperature\n\n  return normalized_weights, loss_temperature",
  "def __post_init__(self):\n    if self.value < 0:\n      raise ValueError(\"KL constraint epsilon must be non-negative.\")",
  "def __init__(self,\n               epsilons: Sequence[KLConstraint],\n               epsilon_mean: float,\n               epsilon_stddev: float,\n               init_log_temperature: float,\n               init_log_alpha_mean: float,\n               init_log_alpha_stddev: float,\n               per_dim_constraining: bool = True,\n               name: str = \"MOMPO\"):\n    \"\"\"Initialize and configure the MPO loss.\n\n    Args:\n      epsilons: per-objective KL constraints on the non-parametric auxiliary\n        policy, the one associated with the dual variables called temperature;\n        expected length K.\n      epsilon_mean: KL constraint on the mean of the Gaussian policy, the one\n        associated with the dual variable called alpha_mean.\n      epsilon_stddev: KL constraint on the stddev of the Gaussian policy, the\n        one associated with the dual variable called alpha_mean.\n      init_log_temperature: initial value for the temperature in log-space, note\n        a softplus (rather than an exp) will be used to transform this.\n      init_log_alpha_mean: initial value for the alpha_mean in log-space, note\n        a softplus (rather than an exp) will be used to transform this.\n      init_log_alpha_stddev: initial value for the alpha_stddev in log-space,\n        note a softplus (rather than an exp) will be used to transform this.\n      per_dim_constraining: whether to enforce the KL constraint on each\n        dimension independently; this is the default. Otherwise the overall KL\n        is constrained, which allows some dimensions to change more at the\n        expense of others staying put.\n      name: a name for the module, passed directly to snt.Module.\n\n    \"\"\"\n    super().__init__(name=name)\n\n    # MO-MPO constraint thresholds.\n    self._epsilons = tf.constant([x.value for x in epsilons])\n    self._epsilon_mean = tf.constant(epsilon_mean)\n    self._epsilon_stddev = tf.constant(epsilon_stddev)\n\n    # Initial values for the constraints' dual variables.\n    self._init_log_temperature = init_log_temperature\n    self._init_log_alpha_mean = init_log_alpha_mean\n    self._init_log_alpha_stddev = init_log_alpha_stddev\n\n    # Whether to ensure per-dimension KL constraint satisfication.\n    self._per_dim_constraining = per_dim_constraining\n\n    # Remember the number of objectives\n    self._num_objectives = len(epsilons)  # K = number of objectives\n    self._objective_names = [x.name for x in epsilons]\n\n    # Make sure there are no duplicate objective names\n    if len(self._objective_names) != len(set(self._objective_names)):\n      raise ValueError(\"Duplicate objective names are not allowed.\")",
  "def objective_names(self):\n    return self._objective_names",
  "def create_dual_variables_once(self, shape: tf.TensorShape, dtype: tf.DType):\n    \"\"\"Creates the dual variables the first time the loss module is called.\"\"\"\n\n    # Create the dual variables.\n    self._log_temperature = tf.Variable(\n        initial_value=[self._init_log_temperature] * self._num_objectives,\n        dtype=dtype,\n        name=\"log_temperature\",\n        shape=(self._num_objectives,))\n    self._log_alpha_mean = tf.Variable(\n        initial_value=tf.fill(shape, self._init_log_alpha_mean),\n        dtype=dtype,\n        name=\"log_alpha_mean\",\n        shape=shape)\n    self._log_alpha_stddev = tf.Variable(\n        initial_value=tf.fill(shape, self._init_log_alpha_stddev),\n        dtype=dtype,\n        name=\"log_alpha_stddev\",\n        shape=shape)\n\n    # Cast constraint thresholds to the expected dtype.\n    self._epsilons = tf.cast(self._epsilons, dtype)\n    self._epsilon_mean = tf.cast(self._epsilon_mean, dtype)\n    self._epsilon_stddev = tf.cast(self._epsilon_stddev, dtype)",
  "def __call__(\n      self,\n      online_action_distribution: Union[tfd.MultivariateNormalDiag,\n                                        tfd.Independent],\n      target_action_distribution: Union[tfd.MultivariateNormalDiag,\n                                        tfd.Independent],\n      actions: tf.Tensor,  # Shape [N, B, D].\n      q_values: tf.Tensor,  # Shape [N, B, K].\n  ) -> Tuple[tf.Tensor, Dict[str, tf.Tensor]]:\n    \"\"\"Computes the decoupled MO-MPO loss.\n\n    Args:\n      online_action_distribution: online distribution returned by the online\n        policy network; expects batch_dims of [B] and event_dims of [D].\n      target_action_distribution: target distribution returned by the target\n        policy network; expects same shapes as online distribution.\n      actions: actions sampled from the target policy; expects shape [N, B, D].\n      q_values: Q-values associated with each action; expects shape [N, B, K].\n\n    Returns:\n      Loss, combining the policy loss, KL penalty, and dual losses required to\n        adapt the dual variables.\n      Stats, for diagnostics and tracking performance.\n    \"\"\"\n\n    # Make sure the Q-values are per-objective\n    q_values.get_shape().assert_has_rank(3)\n    if q_values.get_shape()[-1] != self._num_objectives:\n      raise ValueError(\"Q-values do not match expected number of objectives.\")\n\n    # Cast `MultivariateNormalDiag`s to Independent Normals.\n    # The latter allows us to satisfy KL constraints per-dimension.\n    if isinstance(target_action_distribution, tfd.MultivariateNormalDiag):\n      target_action_distribution = tfd.Independent(\n          tfd.Normal(target_action_distribution.mean(),\n                     target_action_distribution.stddev()))\n      online_action_distribution = tfd.Independent(\n          tfd.Normal(online_action_distribution.mean(),\n                     online_action_distribution.stddev()))\n\n    # Infer the shape and dtype of dual variables.\n    scalar_dtype = q_values.dtype\n    if self._per_dim_constraining:\n      dual_variable_shape = target_action_distribution.distribution.kl_divergence(\n          online_action_distribution.distribution).shape[1:]  # Should be [D].\n    else:\n      dual_variable_shape = target_action_distribution.kl_divergence(\n          online_action_distribution).shape[1:]  # Should be [1].\n\n    # Create dual variables for the KL constraints; only happens the first call.\n    self.create_dual_variables_once(dual_variable_shape, scalar_dtype)\n\n    # Project dual variables to ensure they stay positive.\n    min_log_temperature = tf.constant(-18.0, scalar_dtype)\n    min_log_alpha = tf.constant(-18.0, scalar_dtype)\n    self._log_temperature.assign(\n        tf.maximum(min_log_temperature, self._log_temperature))\n    self._log_alpha_mean.assign(tf.maximum(min_log_alpha, self._log_alpha_mean))\n    self._log_alpha_stddev.assign(\n        tf.maximum(min_log_alpha, self._log_alpha_stddev))\n\n    # Transform dual variables from log-space.\n    # Note: using softplus instead of exponential for numerical stability.\n    temperature = tf.math.softplus(self._log_temperature) + _MPO_FLOAT_EPSILON\n    alpha_mean = tf.math.softplus(self._log_alpha_mean) + _MPO_FLOAT_EPSILON\n    alpha_stddev = tf.math.softplus(self._log_alpha_stddev) + _MPO_FLOAT_EPSILON\n\n    # Get online and target means and stddevs in preparation for decomposition.\n    online_mean = online_action_distribution.distribution.mean()\n    online_scale = online_action_distribution.distribution.stddev()\n    target_mean = target_action_distribution.distribution.mean()\n    target_scale = target_action_distribution.distribution.stddev()\n\n    # Compute normalized importance weights, used to compute expectations with\n    # respect to the non-parametric policy; and the temperature loss, used to\n    # adapt the tempering of Q-values.\n    normalized_weights, loss_temperature = compute_weights_and_temperature_loss(\n        q_values, self._epsilons, temperature)  # Shapes [N, B, K] and [1, K].\n    normalized_weights_sum = tf.reduce_sum(normalized_weights, axis=-1)\n    loss_temperature_mean = tf.reduce_mean(loss_temperature)\n\n    # Only needed for diagnostics: Compute estimated actualized KL between the\n    # non-parametric and current target policies.\n    kl_nonparametric = mpo.compute_nonparametric_kl_from_normalized_weights(\n        normalized_weights)\n\n    # Decompose the online policy into fixed-mean & fixed-stddev distributions.\n    # This has been documented as having better performance in bandit settings,\n    # see e.g. https://arxiv.org/pdf/1812.02256.pdf.\n    fixed_stddev_distribution = tfd.Independent(\n        tfd.Normal(loc=online_mean, scale=target_scale))\n    fixed_mean_distribution = tfd.Independent(\n        tfd.Normal(loc=target_mean, scale=online_scale))\n\n    # Compute the decomposed policy losses.\n    loss_policy_mean = mpo.compute_cross_entropy_loss(\n        actions, normalized_weights_sum, fixed_stddev_distribution)\n    loss_policy_stddev = mpo.compute_cross_entropy_loss(\n        actions, normalized_weights_sum, fixed_mean_distribution)\n\n    # Compute the decomposed KL between the target and online policies.\n    if self._per_dim_constraining:\n      kl_mean = target_action_distribution.distribution.kl_divergence(\n          fixed_stddev_distribution.distribution)  # Shape [B, D].\n      kl_stddev = target_action_distribution.distribution.kl_divergence(\n          fixed_mean_distribution.distribution)  # Shape [B, D].\n    else:\n      kl_mean = target_action_distribution.kl_divergence(\n          fixed_stddev_distribution)  # Shape [B].\n      kl_stddev = target_action_distribution.kl_divergence(\n          fixed_mean_distribution)  # Shape [B].\n\n    # Compute the alpha-weighted KL-penalty and dual losses to adapt the alphas.\n    loss_kl_mean, loss_alpha_mean = mpo.compute_parametric_kl_penalty_and_dual_loss(\n        kl_mean, alpha_mean, self._epsilon_mean)\n    loss_kl_stddev, loss_alpha_stddev = mpo.compute_parametric_kl_penalty_and_dual_loss(\n        kl_stddev, alpha_stddev, self._epsilon_stddev)\n\n    # Combine losses.\n    loss_policy = loss_policy_mean + loss_policy_stddev\n    loss_kl_penalty = loss_kl_mean + loss_kl_stddev\n    loss_dual = loss_alpha_mean + loss_alpha_stddev + loss_temperature_mean\n    loss = loss_policy + loss_kl_penalty + loss_dual\n\n    stats = {}\n    # Dual Variables.\n    stats[\"dual_alpha_mean\"] = tf.reduce_mean(alpha_mean)\n    stats[\"dual_alpha_stddev\"] = tf.reduce_mean(alpha_stddev)\n    # Losses.\n    stats[\"loss_policy\"] = tf.reduce_mean(loss)\n    stats[\"loss_alpha\"] = tf.reduce_mean(loss_alpha_mean + loss_alpha_stddev)\n    # KL measurements.\n    stats[\"kl_mean_rel\"] = tf.reduce_mean(kl_mean, axis=0) / self._epsilon_mean\n    stats[\"kl_stddev_rel\"] = tf.reduce_mean(\n        kl_stddev, axis=0) / self._epsilon_stddev\n    # If the policy has standard deviation, log summary stats for this as well.\n    pi_stddev = online_action_distribution.distribution.stddev()\n    stats[\"pi_stddev_min\"] = tf.reduce_mean(tf.reduce_min(pi_stddev, axis=-1))\n    stats[\"pi_stddev_max\"] = tf.reduce_mean(tf.reduce_max(pi_stddev, axis=-1))\n\n    # Condition number of the diagonal covariance (actually, stddev) matrix.\n    stats[\"pi_stddev_cond\"] = tf.reduce_mean(\n        tf.reduce_max(pi_stddev, axis=-1) / tf.reduce_min(pi_stddev, axis=-1))\n\n    # Log per-objective values.\n    for i, name in enumerate(self._objective_names):\n      stats[\"{}_dual_temperature\".format(name)] = temperature[i]\n      stats[\"{}_loss_temperature\".format(name)] = loss_temperature[i]\n      stats[\"{}_kl_q_rel\".format(name)] = tf.reduce_mean(\n          kl_nonparametric[:, i]) / self._epsilons[i]\n\n      # Q measurements.\n      stats[\"{}_q_min\".format(name)] = tf.reduce_mean(tf.reduce_min(\n          q_values, axis=0)[:, i])\n      stats[\"{}_q_mean\".format(name)] = tf.reduce_mean(tf.reduce_mean(\n          q_values, axis=0)[:, i])\n      stats[\"{}_q_max\".format(name)] = tf.reduce_mean(tf.reduce_max(\n          q_values, axis=0)[:, i])\n\n    return loss, stats",
  "def categorical(q_tm1: networks.DiscreteValuedDistribution, r_t: tf.Tensor,\n                d_t: tf.Tensor,\n                q_t: networks.DiscreteValuedDistribution) -> tf.Tensor:\n  \"\"\"Implements the Categorical Distributional TD(0)-learning loss.\"\"\"\n\n  z_t = tf.reshape(r_t, (-1, 1)) + tf.reshape(d_t, (-1, 1)) * q_t.values\n  p_t = tf.nn.softmax(q_t.logits)\n\n  # Performs L2 projection.\n  target = tf.stop_gradient(l2_project(z_t, p_t, q_t.values))\n\n  # Calculates loss.\n  loss = tf.nn.softmax_cross_entropy_with_logits(\n      logits=q_tm1.logits, labels=target)\n\n  return loss",
  "def l2_project(  # pylint: disable=invalid-name\n    Zp: tf.Tensor,\n    P: tf.Tensor,\n    Zq: tf.Tensor,\n) -> tf.Tensor:\n  \"\"\"Project distribution (Zp, P) onto support Zq under the L2-metric over CDFs.\n\n  This projection works for any support Zq.\n  Let Kq be len(Zq) and Kp be len(Zp).\n\n  Args:\n    Zp: (batch_size, Kp) Support of distribution P\n    P:  (batch_size, Kp) Probability values for P(Zp[i])\n    Zq: (Kp,) Support to project onto\n\n  Returns:\n    L2 projection of (Zp, P) onto Zq.\n  \"\"\"\n\n  # Asserts that Zq has no leading dimension of size 1.\n  if Zq.get_shape().ndims > 1:\n    Zq = tf.squeeze(Zq, axis=0)\n\n  # Extracts vmin and vmax and construct helper tensors from Zq.\n  vmin, vmax = Zq[0], Zq[-1]\n  d_pos = tf.concat([Zq, vmin[None]], 0)[1:]\n  d_neg = tf.concat([vmax[None], Zq], 0)[:-1]\n\n  # Clips Zp to be in new support range (vmin, vmax).\n  clipped_zp = tf.clip_by_value(Zp, vmin, vmax)[:, None, :]\n  clipped_zq = Zq[None, :, None]\n\n  # Gets the distance between atom values in support.\n  d_pos = (d_pos - Zq)[None, :, None]  # Zq[i+1] - Zq[i]\n  d_neg = (Zq - d_neg)[None, :, None]  # Zq[i] - Zq[i-1]\n\n  delta_qp = clipped_zp - clipped_zq  # Zp[j] - Zq[i]\n\n  d_sign = tf.cast(delta_qp >= 0., dtype=P.dtype)\n  delta_hat = (d_sign * delta_qp / d_pos) - ((1. - d_sign) * delta_qp / d_neg)\n  P = P[:, None, :]\n  return tf.reduce_sum(tf.clip_by_value(1. - delta_hat, 0., 1.) * P, 2)",
  "def multiaxis_categorical(  # pylint: disable=invalid-name\n    q_tm1: networks.DiscreteValuedDistribution,\n    r_t: tf.Tensor,\n    d_t: tf.Tensor,\n    q_t: networks.DiscreteValuedDistribution) -> tf.Tensor:\n  \"\"\"Implements a multi-axis categorical distributional TD(0)-learning loss.\n\n  All arguments may have a leading batch axis, but q_tm1.logits, and one of\n  r_t or d_t *must* have a leading batch axis.\n\n  Args:\n    q_tm1: Previous timestep's value distribution.\n    r_t: Reward.\n    d_t: Discount.\n    q_t: Current timestep's value distribution.\n\n  Returns:\n    Cross-entropy Bellman loss between q_tm1 and q_t + r_t * d_t.\n    Shape: (B, *E), where\n      B is the batch size.\n      E is the broadcasted shape of r_t, d_t, and q_t.values[:-1].\n  \"\"\"\n  tf.assert_equal(tf.rank(r_t), tf.rank(d_t))\n\n  # Append a singleton axis corresponding to the axis that indexes the atoms in\n  # q_t.values.\n  r_t = r_t[..., None]  # shape: (B, *R, 1)\n  d_t = d_t[..., None]  # shape: (B, *D, 1)\n\n  z_t = r_t + d_t * q_t.values  # shape: (B, *E, N)\n\n  p_t = tf.nn.softmax(q_t.logits)\n\n  # Performs L2 projection.\n  target = tf.stop_gradient(multiaxis_l2_project(z_t, p_t, q_t.values))\n\n  # Calculates loss.\n  loss = tf.nn.softmax_cross_entropy_with_logits(\n      logits=q_tm1.logits, labels=target)\n\n  return loss",
  "def multiaxis_l2_project(  # pylint: disable=invalid-name\n    Zp: tf.Tensor,\n    P: tf.Tensor,\n    Zq: tf.Tensor,\n) -> tf.Tensor:\n  \"\"\"Project distribution (Zp, P) onto support Zq under the L2-metric over CDFs.\n\n  Let source support Zp's shape be described as (B, *C, M), where:\n    B is the batch size.\n    C contains the sizes of any axes in between the first and last axes.\n    M is the number of atoms in the support.\n\n  Let destination support Zq's shape be described as (*D, N), where:\n    D contains the sizes of any axes before the last axis.\n    N is the number of atoms in the support.\n\n  Shapes C and D must have the same number of dimensions, and must be\n  broadcastable with each other.\n\n  Args:\n    Zp: Support of source distribution. Shape: (B, *C, M).\n    P:  Probability values of source distribution p(Zp[i]). Shape: (B, *C, M).\n    Zq: Support to project P onto. Shape: (*D, N).\n\n  Returns:\n    The L2 projection of P from support Zp to support Zq.\n    Shape: (B, *E, N), where E is the broadcast-merged shape of C and D.\n  \"\"\"\n\n  tf.assert_equal(tf.shape(Zp), tf.shape(P))\n\n  # Shapes C, D, and E as defined in the docstring above.\n  shape_c = tf.shape(Zp)[1:-1]  # drop the batch and atom axes\n  shape_d = tf.shape(Zq)[:-1]  # drop the atom axis\n  shape_e = tf.broadcast_dynamic_shape(shape_c, shape_d)\n\n  # If Zq has fewer inner axes than the broadcasted output shape, insert some\n  # size-1 axes to broadcast.\n  ndim_c = tf.size(shape_c)\n  ndim_e = tf.size(shape_e)\n  Zp = tf.reshape(\n      Zp,\n      tf.concat([tf.shape(Zp)[:1],  # B\n                 tf.ones(tf.math.maximum(ndim_e - ndim_c, 0), dtype=tf.int32),\n                 shape_c,  # C\n                 tf.shape(Zp)[-1:]],  # M\n                axis=0))\n  P = tf.reshape(P, tf.shape(Zp))\n\n  # Broadcast Zp, P, and Zq's common axes to the same shape: E.\n  #\n  # Normally it'd be sufficient to ensure that these args have the same number\n  # of axes, then let the arithmetic operators broadcast as necessary. Instead,\n  # we need to explicitly broadcast them here, because there's a call to\n  # tf.clip_by_value(t, vmin, vmax) below, which doesn't allow t's dimensions\n  # to be expanded to match vmin and vmax.\n\n  # Shape: (B, *E, M)\n  Zp = tf.broadcast_to(\n      Zp,\n      tf.concat([tf.shape(Zp)[:1],  # B\n                 shape_e,  # E\n                 tf.shape(Zp)[-1:]],  # M\n                axis=0))\n\n  # Shape: (B, *E, M)\n  P = tf.broadcast_to(P, tf.shape(Zp))\n\n  # Shape: (*E, N)\n  Zq = tf.broadcast_to(Zq, tf.concat([shape_e, tf.shape(Zq)[-1:]], axis=0))\n\n  # Extracts vmin and vmax and construct helper tensors from Zq.\n  # These have shape shape_q, except the last axis has size 1.\n  # Shape: (*E, 1)\n  vmin, vmax = Zq[..., :1], Zq[..., -1:]\n\n  # The distances between neighboring atom values in the target support.\n  # Shape: (*E, N)\n  d_pos = tf.roll(Zq, shift=-1, axis=-1) - Zq  # d_pos[i] := Zq[i+1] - Zq[i]\n  d_neg = Zq - tf.roll(Zq, shift=1, axis=-1)   # d_neg[i] := Zq[i] - Zq[i-1]\n\n  # Clips Zp to be in new support range (vmin, vmax).\n  # Shape: (B, *E, 1, M)\n  clipped_zp = tf.clip_by_value(Zp, vmin, vmax)[..., None, :]\n\n  # Shape: (1, *E, N, 1)\n  clipped_zq = Zq[None, ..., :, None]\n\n  # Shape: (B, *E, N, M)\n  delta_qp = clipped_zp - clipped_zq  # Zp[j] - Zq[i]\n\n  # Shape: (B, *E, N, M)\n  d_sign = tf.cast(delta_qp >= 0., dtype=P.dtype)\n\n  # Insert singleton axes to d_pos and d_neg to maintain the same shape as\n  # clipped_zq.\n  # Shape: (1, *E, N, 1)\n  d_pos = d_pos[None, ..., :, None]\n  d_neg = d_neg[None, ..., :, None]\n\n  # Shape: (B, *E, N, M)\n  delta_hat = (d_sign * delta_qp / d_pos) - ((1. - d_sign) * delta_qp / d_neg)\n\n  # Shape: (B, *E, 1, M)\n  P = P[..., None, :]\n\n  # Shape: (B, *E, N)\n  return tf.reduce_sum(tf.clip_by_value(1. - delta_hat, 0., 1.) * P, axis=-1)",
  "def huber(inputs: tf.Tensor, quadratic_linear_boundary: float) -> tf.Tensor:\n  \"\"\"Calculates huber loss of `inputs`.\n\n  For each value x in `inputs`, the following is calculated:\n\n  ```\n    0.5 * x^2                  if |x| <= d\n    0.5 * d^2 + d * (|x| - d)  if |x| > d\n  ```\n\n  where d is `quadratic_linear_boundary`.\n\n  Args:\n    inputs: Input Tensor to calculate the huber loss on.\n    quadratic_linear_boundary: The point where the huber loss function changes\n      from a quadratic to linear.\n\n  Returns:\n    `Tensor` of the same shape as `inputs`, containing values calculated\n    in the manner described above.\n\n  Raises:\n    ValueError: if quadratic_linear_boundary < 0.\n  \"\"\"\n  if quadratic_linear_boundary < 0:\n    raise ValueError(\"quadratic_linear_boundary must be >= 0.\")\n\n  abs_x = tf.abs(inputs)\n  delta = tf.constant(quadratic_linear_boundary)\n  quad = tf.minimum(abs_x, delta)\n  # The following expression is the same in value as\n  # tf.maximum(abs_x - delta, 0), but importantly the gradient for the\n  # expression when abs_x == delta is 0 (for tf.maximum it would be 1). This\n  # is necessary to avoid doubling the gradient, since there is already a\n  # nonzero contribution to the gradient from the quadratic term.\n  lin = (abs_x - quad)\n  return 0.5 * quad**2 + delta * lin",
  "class LossCoreExtra(NamedTuple):\n  targets: tf.Tensor\n  errors: tf.Tensor",
  "def transformed_n_step_loss(\n    qs: tf.Tensor,\n    targnet_qs: tf.Tensor,\n    actions: tf.Tensor,\n    rewards: tf.Tensor,\n    pcontinues: tf.Tensor,\n    target_policy_probs: tf.Tensor,\n    bootstrap_n: int,\n    stop_targnet_gradients: bool = True,\n    name: str = 'transformed_n_step_loss',\n) -> trfl.base_ops.LossOutput:\n  \"\"\"Helper function for computing transformed loss on sequences.\n\n  Args:\n    qs: 3-D tensor corresponding to the Q-values to be learned. Shape is [T+1,\n      B, A].\n    targnet_qs: Like `qs`, but in the target network setting, these values\n      should be computed by the target network. Shape is [T+1, B, A].\n    actions: 2-D tensor holding the indices of actions executed during the\n      transition that corresponds to each major index. Shape is [T+1, B].\n    rewards: 2-D tensor holding rewards received during the transition that\n      corresponds to each major index. Shape is [T, B].\n    pcontinues: 2-D tensor holding pcontinue values received during the\n      transition that corresponds to each major index. Shape is [T, B].\n    target_policy_probs: 3-D tensor holding per-action policy probabilities for\n      the states encountered just before taking the transitions that correspond\n      to each major index, according to the target policy (i.e. the policy we\n      wish to learn). For standard Q-learning the probabilities should form a\n      one-hot vector over actions where the nonzero index corresponds to the max\n      Q. Shape is [T+1, B, A].\n    bootstrap_n: Transition length for N-step bootstrapping.\n    stop_targnet_gradients: `bool` indicating whether to apply tf.stop_gradients\n      to the target values. This should usually be True.\n    name: name to prefix ops created by this function.\n\n  Returns:\n    a tuple of:\n    * `loss`: the transformed Q-learning loss summed over `T`.\n    * `LossCoreExtra`: namedtuple containing the fields `targets` and `errors`.\n  \"\"\"\n\n  with tf.name_scope(name):\n    # Require correct tensor ranks---as long as we have shape information\n    # available to check. If there isn't any, we print a warning.\n    def check_rank(tensors: Iterable[tf.Tensor], ranks: Sequence[int]):\n      for i, (tensor, rank) in enumerate(zip(tensors, ranks)):\n        if tensor.get_shape():\n          trfl.assert_rank_and_shape_compatibility([tensor], rank)\n        else:\n          raise ValueError(\n              f'Tensor \"{tensor.name}\", which was offered as transformed_n_step_loss'\n              f'parameter {i+1}, has no rank at construction time, so cannot verify'\n              f'that it has the necessary rank of {rank}')\n\n    check_rank(\n        [qs, targnet_qs, actions, rewards, pcontinues, target_policy_probs],\n        [3, 3, 2, 2, 2, 3])\n\n    # Construct arguments to compute bootstrap target.\n    a_tm1 = actions[:-1]  # (0:T) x B\n    r_t, pcont_t = rewards, pcontinues  # (1:T+1) x B\n    q_tm1 = qs[:-1]  # (0:T) x B x A\n    target_policy_t = target_policy_probs[1:]  # (1:T+1) x B x A\n    targnet_q_t = targnet_qs[1:]  # (1:T+1) x B x A\n\n    bootstrap_value = tf.reduce_sum(\n        target_policy_t * _signed_parabolic_tx(targnet_q_t), -1)\n    target = _compute_n_step_sequence_targets(\n        r_t=r_t,\n        pcont_t=pcont_t,\n        bootstrap_value=bootstrap_value,\n        n=bootstrap_n)\n\n    if stop_targnet_gradients:\n      target = tf.stop_gradient(target)\n\n    # tx/inv_tx may result in numerical instabilities so mask any NaNs.\n    finite_mask = tf.math.is_finite(target)\n    target = tf.where(finite_mask, target, tf.zeros_like(target))\n\n    qa_tm1 = trfl.batched_index(q_tm1, a_tm1)\n    errors = qa_tm1 - _signed_hyperbolic_tx(target)\n\n    # Only compute n-step errors w.r.t. finite targets.\n    errors = tf.where(finite_mask, errors, tf.zeros_like(errors))\n\n    # Sum over time dimension.\n    loss = 0.5 * tf.reduce_sum(tf.square(errors), axis=0)\n\n    return trfl.base_ops.LossOutput(\n        loss, LossCoreExtra(targets=target, errors=errors))",
  "def _compute_n_step_sequence_targets(\n    r_t: tf.Tensor,\n    pcont_t: tf.Tensor,\n    bootstrap_value: tf.Tensor,\n    n: int,\n) -> tf.Tensor:\n  \"\"\"Computes n-step bootstrapped returns over a sequence.\n\n  Args:\n    r_t: 2-D tensor of shape [T, B] corresponding to rewards.\n    pcont_t: 2-D tensor of shape [T, B] corresponding to pcontinues.\n    bootstrap_value: 2-D tensor of shape [T, B] corresponding to bootstrap\n      values.\n    n: number of steps over which to accumulate reward before bootstrapping.\n\n  Returns:\n    2-D tensor of shape [T, B] corresponding to bootstrapped returns.\n  \"\"\"\n  time_size, batch_size = r_t.shape.as_list()\n\n  # Pad r_t and pcont_t so we can use static slice shapes in scan.\n  r_t = tf.concat([r_t, tf.zeros((n - 1, batch_size))], 0)\n  pcont_t = tf.concat([pcont_t, tf.ones((n - 1, batch_size))], 0)\n\n  # We need to use tf.slice with static shapes for TPU compatibility.\n  def _slice(tensor, index, size):\n    return tf.slice(tensor, [index, 0], [size, batch_size])\n\n  # Construct correct bootstrap targets for each time slice t, which are exactly\n  # the target values at timestep min(t+n-1, time_size-1).\n  last_bootstrap_value = _slice(bootstrap_value, time_size - 1, 1)\n  if time_size > n - 1:\n    full_bootstrap_steps = [_slice(bootstrap_value, n - 1, time_size - (n - 1))]\n    truncated_bootstrap_steps = [last_bootstrap_value] * (n - 1)\n  else:\n    # Only truncated steps, since n > time_size.\n    full_bootstrap_steps = []\n    truncated_bootstrap_steps = [last_bootstrap_value] * time_size\n  bootstrap_value = tf.concat(full_bootstrap_steps + truncated_bootstrap_steps,\n                              0)\n\n  # Iterate backwards for n steps to construct n-step return targets.\n  targets = bootstrap_value\n  for i in range(n - 1, -1, -1):\n    this_pcont_t = _slice(pcont_t, i, time_size)\n    this_r_t = _slice(r_t, i, time_size)\n    targets = this_r_t + this_pcont_t * targets\n  return targets",
  "def _signed_hyperbolic_tx(x: tf.Tensor, eps: float = 1e-3) -> tf.Tensor:\n  \"\"\"Signed hyperbolic transform, inverse of signed_parabolic.\"\"\"\n  return tf.sign(x) * (tf.sqrt(abs(x) + 1) - 1) + eps * x",
  "def _signed_parabolic_tx(x: tf.Tensor, eps: float = 1e-3) -> tf.Tensor:\n  \"\"\"Signed parabolic transform, inverse of signed_hyperbolic.\"\"\"\n  z = tf.sqrt(1 + 4 * eps * (eps + 1 + abs(x))) / 2 / eps - 1 / 2 / eps\n  return tf.sign(x) * (tf.square(z) - 1)",
  "def _slice(tensor, index, size):\n    return tf.slice(tensor, [index, 0], [size, batch_size])",
  "def check_rank(tensors: Iterable[tf.Tensor], ranks: Sequence[int]):\n      for i, (tensor, rank) in enumerate(zip(tensors, ranks)):\n        if tensor.get_shape():\n          trfl.assert_rank_and_shape_compatibility([tensor], rank)\n        else:\n          raise ValueError(\n              f'Tensor \"{tensor.name}\", which was offered as transformed_n_step_loss'\n              f'parameter {i+1}, has no rank at construction time, so cannot verify'\n              f'that it has the necessary rank of {rank}')",
  "def dpg(\n    q_max: tf.Tensor,\n    a_max: tf.Tensor,\n    tape: tf.GradientTape,\n    dqda_clipping: Optional[float] = None,\n    clip_norm: bool = False,\n) -> tf.Tensor:\n  \"\"\"Deterministic policy gradient loss, similar to trfl.dpg.\"\"\"\n\n  # Calculate the gradient dq/da.\n  dqda = tape.gradient([q_max], [a_max])[0]\n\n  if dqda is None:\n    raise ValueError('q_max needs to be a function of a_max.')\n\n  # Clipping the gradient dq/da.\n  if dqda_clipping is not None:\n    if dqda_clipping <= 0:\n      raise ValueError('dqda_clipping should be bigger than 0, {} found'.format(\n          dqda_clipping))\n    if clip_norm:\n      dqda = tf.clip_by_norm(dqda, dqda_clipping, axes=-1)\n    else:\n      dqda = tf.clip_by_value(dqda, -1. * dqda_clipping, dqda_clipping)\n\n  # Target_a ensures correct gradient calculated during backprop.\n  target_a = dqda + a_max\n  # Stop the gradient going through Q network when backprop.\n  target_a = tf.stop_gradient(target_a)\n  # Gradient only go through actor network.\n  loss = 0.5 * tf.reduce_sum(tf.square(target_a - a_max), axis=-1)\n  # This recovers the DPG because (letting w be the actor network weights):\n  # d(loss)/dw = 0.5 * (2 * (target_a - a_max) * d(target_a - a_max)/dw)\n  #            = (target_a - a_max) * [d(target_a)/dw  - d(a_max)/dw]\n  #            = dq/da * [d(target_a)/dw  - d(a_max)/dw]  # by defn of target_a\n  #            = dq/da * [0 - d(a_max)/dw]                # by stop_gradient\n  #            = - dq/da * da/dw\n\n  return loss",
  "class MaskedSequential(snt.Module):\n  \"\"\"Applies a legal actions mask to a linear chain of modules / callables.\n\n  It is assumed the trailing dimension of the final layer (representing\n  action values) is the same as the trailing dimension of legal_actions.\n  \"\"\"\n\n  def __init__(self,\n               layers: Optional[Iterable[Callable[..., Any]]] = None,\n               name: str = 'MaskedSequential'):\n    super().__init__(name=name)\n    self._layers = list(layers) if layers is not None else []\n    self._illegal_action_penalty = -1e9\n    # Note: illegal_action_penalty cannot be -np.inf because trfl's qlearning\n    # ops utilize a batched_index function that returns NaN whenever -np.inf\n    # is present among action values.\n\n  def __call__(self, inputs: open_spiel_wrapper.OLT) -> tf.Tensor:\n    # Extract observation, legal actions, and terminal\n    outputs = inputs.observation\n    legal_actions = inputs.legal_actions\n    terminal = inputs.terminal\n\n    for mod in self._layers:\n      outputs = mod(outputs)\n\n    # Apply legal actions mask\n    outputs = tf.where(tf.equal(legal_actions, 1), outputs,\n                       tf.fill(tf.shape(outputs), self._illegal_action_penalty))\n\n    # When computing the Q-learning target (r_t + d_t * max q_t) we need to\n    # ensure max q_t = 0 in terminal states.\n    outputs = tf.where(tf.equal(terminal, 1), tf.zeros_like(outputs), outputs)\n\n    return outputs",
  "class EpsilonGreedy(snt.Module):\n  \"\"\"Computes an epsilon-greedy distribution over actions.\n\n  This policy does the following:\n  - With probability 1 - epsilon, take the action corresponding to the highest\n  action value, breaking ties uniformly at random.\n  - With probability epsilon, take an action uniformly at random.\n  \"\"\"\n\n  def __init__(self,\n               epsilon: Union[tf.Tensor, float],\n               threshold: float,\n               name: str = 'EpsilonGreedy'):\n    \"\"\"Initialize the policy.\n\n    Args:\n      epsilon: Exploratory param with value between 0 and 1.\n      threshold: Action values must exceed this value to qualify as a legal\n        action and possibly be selected by the policy.\n      name: Name of the network.\n\n    Returns:\n      policy: tfp.distributions.Categorical distribution representing the\n        policy.\n    \"\"\"\n    super().__init__(name=name)\n    self._epsilon = tf.Variable(epsilon, trainable=False)\n    self._threshold = threshold\n\n  def __call__(self, action_values: tf.Tensor) -> tfd.Categorical:\n    legal_actions_mask = tf.where(\n        tf.math.less_equal(action_values, self._threshold),\n        tf.fill(tf.shape(action_values), 0.),\n        tf.fill(tf.shape(action_values), 1.))\n\n    # Dithering action distribution.\n    dither_probs = 1 / tf.reduce_sum(legal_actions_mask, axis=-1,\n                                     keepdims=True) * legal_actions_mask\n    masked_action_values = tf.where(tf.equal(legal_actions_mask, 1),\n                                    action_values,\n                                    tf.fill(tf.shape(action_values), -np.inf))\n    # Greedy action distribution, breaking ties uniformly at random.\n    max_value = tf.reduce_max(masked_action_values, axis=-1, keepdims=True)\n    greedy_probs = tf.cast(\n        tf.equal(action_values * legal_actions_mask, max_value),\n        action_values.dtype)\n\n    greedy_probs /= tf.reduce_sum(greedy_probs, axis=-1, keepdims=True)\n\n    # Epsilon-greedy action distribution.\n    probs = self._epsilon * dither_probs + (1 - self._epsilon) * greedy_probs\n\n    # Make the policy object.\n    policy = tfd.Categorical(probs=probs)\n\n    return policy",
  "def __init__(self,\n               layers: Optional[Iterable[Callable[..., Any]]] = None,\n               name: str = 'MaskedSequential'):\n    super().__init__(name=name)\n    self._layers = list(layers) if layers is not None else []\n    self._illegal_action_penalty = -1e9",
  "def __call__(self, inputs: open_spiel_wrapper.OLT) -> tf.Tensor:\n    # Extract observation, legal actions, and terminal\n    outputs = inputs.observation\n    legal_actions = inputs.legal_actions\n    terminal = inputs.terminal\n\n    for mod in self._layers:\n      outputs = mod(outputs)\n\n    # Apply legal actions mask\n    outputs = tf.where(tf.equal(legal_actions, 1), outputs,\n                       tf.fill(tf.shape(outputs), self._illegal_action_penalty))\n\n    # When computing the Q-learning target (r_t + d_t * max q_t) we need to\n    # ensure max q_t = 0 in terminal states.\n    outputs = tf.where(tf.equal(terminal, 1), tf.zeros_like(outputs), outputs)\n\n    return outputs",
  "def __init__(self,\n               epsilon: Union[tf.Tensor, float],\n               threshold: float,\n               name: str = 'EpsilonGreedy'):\n    \"\"\"Initialize the policy.\n\n    Args:\n      epsilon: Exploratory param with value between 0 and 1.\n      threshold: Action values must exceed this value to qualify as a legal\n        action and possibly be selected by the policy.\n      name: Name of the network.\n\n    Returns:\n      policy: tfp.distributions.Categorical distribution representing the\n        policy.\n    \"\"\"\n    super().__init__(name=name)\n    self._epsilon = tf.Variable(epsilon, trainable=False)\n    self._threshold = threshold",
  "def __call__(self, action_values: tf.Tensor) -> tfd.Categorical:\n    legal_actions_mask = tf.where(\n        tf.math.less_equal(action_values, self._threshold),\n        tf.fill(tf.shape(action_values), 0.),\n        tf.fill(tf.shape(action_values), 1.))\n\n    # Dithering action distribution.\n    dither_probs = 1 / tf.reduce_sum(legal_actions_mask, axis=-1,\n                                     keepdims=True) * legal_actions_mask\n    masked_action_values = tf.where(tf.equal(legal_actions_mask, 1),\n                                    action_values,\n                                    tf.fill(tf.shape(action_values), -np.inf))\n    # Greedy action distribution, breaking ties uniformly at random.\n    max_value = tf.reduce_max(masked_action_values, axis=-1, keepdims=True)\n    greedy_probs = tf.cast(\n        tf.equal(action_values * legal_actions_mask, max_value),\n        action_values.dtype)\n\n    greedy_probs /= tf.reduce_sum(greedy_probs, axis=-1, keepdims=True)\n\n    # Epsilon-greedy action distribution.\n    probs = self._epsilon * dither_probs + (1 - self._epsilon) * greedy_probs\n\n    # Make the policy object.\n    policy = tfd.Categorical(probs=probs)\n\n    return policy",
  "class OAREmbedding(snt.Module):\n  \"\"\"Module for embedding (observation, action, reward) inputs together.\"\"\"\n\n  def __init__(self, torso: base.Module, num_actions: int):\n    super().__init__(name='oar_embedding')\n    self._num_actions = num_actions\n    self._torso = torso\n\n  def __call__(self, inputs: observation_action_reward.OAR) -> tf.Tensor:\n    \"\"\"Embed each of the (observation, action, reward) inputs & concatenate.\"\"\"\n\n    # Add dummy trailing dimension to rewards if necessary.\n    if len(inputs.reward.shape.dims) == 1:\n      inputs = inputs._replace(reward=tf.expand_dims(inputs.reward, axis=-1))\n\n    features = self._torso(inputs.observation)  # [T?, B, D]\n    action = tf.one_hot(inputs.action, depth=self._num_actions)  # [T?, B, A]\n    reward = tf.nn.tanh(inputs.reward)  # [T?, B, 1]\n\n    embedding = tf.concat([features, action, reward], axis=-1)  # [T?, B, D+A+1]\n\n    return embedding",
  "def __init__(self, torso: base.Module, num_actions: int):\n    super().__init__(name='oar_embedding')\n    self._num_actions = num_actions\n    self._torso = torso",
  "def __call__(self, inputs: observation_action_reward.OAR) -> tf.Tensor:\n    \"\"\"Embed each of the (observation, action, reward) inputs & concatenate.\"\"\"\n\n    # Add dummy trailing dimension to rewards if necessary.\n    if len(inputs.reward.shape.dims) == 1:\n      inputs = inputs._replace(reward=tf.expand_dims(inputs.reward, axis=-1))\n\n    features = self._torso(inputs.observation)  # [T?, B, D]\n    action = tf.one_hot(inputs.action, depth=self._num_actions)  # [T?, B, A]\n    reward = tf.nn.tanh(inputs.reward)  # [T?, B, 1]\n\n    embedding = tf.concat([features, action, reward], axis=-1)  # [T?, B, D+A+1]\n\n    return embedding",
  "class DiscreteFilteredQNetwork(snt.Module):\n  \"\"\"Discrete filtered Q-network.\n\n  This produces filtered Q values according to the method used in the discrete\n  BCQ algorithm (https://arxiv.org/pdf/1910.01708.pdf - section 4).\n  \"\"\"\n\n  def __init__(self,\n               g_network: snt.Module,\n               q_network: snt.Module,\n               threshold: float):\n    super().__init__(name='discrete_filtered_qnet')\n    assert threshold >= 0 and threshold <= 1\n    self.g_network = g_network\n    self.q_network = q_network\n    self._threshold = threshold\n\n  def __call__(self, o_t: tf.Tensor) -> tf.Tensor:\n    q_t = self.q_network(o_t)\n    g_t = tf.nn.softmax(self.g_network(o_t))\n    normalized_g_t = g_t / tf.reduce_max(g_t, axis=-1, keepdims=True)\n\n    # Filter actions based on g_network outputs.\n    min_q = tf.reduce_min(q_t, axis=-1, keepdims=True)\n    return tf.where(normalized_g_t >= self._threshold, q_t, min_q)",
  "def __init__(self,\n               g_network: snt.Module,\n               q_network: snt.Module,\n               threshold: float):\n    super().__init__(name='discrete_filtered_qnet')\n    assert threshold >= 0 and threshold <= 1\n    self.g_network = g_network\n    self.q_network = q_network\n    self._threshold = threshold",
  "def __call__(self, o_t: tf.Tensor) -> tf.Tensor:\n    q_t = self.q_network(o_t)\n    g_t = tf.nn.softmax(self.g_network(o_t))\n    normalized_g_t = g_t / tf.reduce_max(g_t, axis=-1, keepdims=True)\n\n    # Filter actions based on g_network outputs.\n    min_q = tf.reduce_min(q_t, axis=-1, keepdims=True)\n    return tf.where(normalized_g_t >= self._threshold, q_t, min_q)",
  "class CriticMultiplexer(snt.Module):\n  \"\"\"Module connecting a critic torso to (transformed) observations/actions.\n\n  This takes as input a `critic_network`, an `observation_network`, and an\n  `action_network` and returns another network whose outputs are given by\n  `critic_network(observation_network(o), action_network(a))`.\n\n  The observations and actions passed to this module are assumed to have a batch\n  dimension that match.\n\n  Notes:\n  - Either the `observation_` or `action_network` can be `None`, in which case\n    the observation or action, resp., are passed to the critic network as is.\n  - If all `critic_`, `observation_` and `action_network` are `None`, this\n    module reduces to a simple `tf2_utils.batch_concat()`.\n  \"\"\"\n\n  def __init__(self,\n               critic_network: Optional[TensorTransformation] = None,\n               observation_network: Optional[TensorTransformation] = None,\n               action_network: Optional[TensorTransformation] = None):\n    self._critic_network = critic_network\n    self._observation_network = observation_network\n    self._action_network = action_network\n    super().__init__(name='critic_multiplexer')\n\n  def __call__(self,\n               observation: types.NestedTensor,\n               action: types.NestedTensor) -> tf.Tensor:\n\n    # Maybe transform observations and actions before feeding them on.\n    if self._observation_network:\n      observation = self._observation_network(observation)\n    if self._action_network:\n      action = self._action_network(action)\n\n    if hasattr(observation, 'dtype') and hasattr(action, 'dtype'):\n      if observation.dtype != action.dtype:\n        # Observation and action must be the same type for concat to work\n        action = tf.cast(action, observation.dtype)\n\n    # Concat observations and actions, with one batch dimension.\n    outputs = tf2_utils.batch_concat([observation, action])\n\n    # Maybe transform output before returning.\n    if self._critic_network:\n      outputs = self._critic_network(outputs)\n\n    return outputs",
  "def __init__(self,\n               critic_network: Optional[TensorTransformation] = None,\n               observation_network: Optional[TensorTransformation] = None,\n               action_network: Optional[TensorTransformation] = None):\n    self._critic_network = critic_network\n    self._observation_network = observation_network\n    self._action_network = action_network\n    super().__init__(name='critic_multiplexer')",
  "def __call__(self,\n               observation: types.NestedTensor,\n               action: types.NestedTensor) -> tf.Tensor:\n\n    # Maybe transform observations and actions before feeding them on.\n    if self._observation_network:\n      observation = self._observation_network(observation)\n    if self._action_network:\n      action = self._action_network(action)\n\n    if hasattr(observation, 'dtype') and hasattr(action, 'dtype'):\n      if observation.dtype != action.dtype:\n        # Observation and action must be the same type for concat to work\n        action = tf.cast(action, observation.dtype)\n\n    # Concat observations and actions, with one batch dimension.\n    outputs = tf2_utils.batch_concat([observation, action])\n\n    # Maybe transform output before returning.\n    if self._critic_network:\n      outputs = self._critic_network(outputs)\n\n    return outputs",
  "class Critic(snt.Module):\n\n  def __call__(self, o, a):\n    return o * a",
  "class RNNCritic(snt.RNNCore):\n\n  def __call__(self, o, a, prev_state):\n    return o * a, prev_state\n\n  def initial_state(self, batch_size):\n    return ()",
  "class NetsTest(tf.test.TestCase):\n\n  def test_criticdeeprnn_snapshot(self):\n    \"\"\"Test that CriticDeepRNN works correctly with snapshotting.\"\"\"\n    # Create a test network.\n    critic = Critic()\n    rnn_critic = RNNCritic()\n\n    for base_net in [critic, rnn_critic]:\n      net = recurrence.CriticDeepRNN([base_net, snt.LSTM(10)])\n      obs = specs.Array([10], dtype=np.float32)\n      actions = specs.Array([10], dtype=np.float32)\n      spec = [obs, actions]\n      tf2_utils.create_variables(net, spec)\n\n      # Test that if you add some postprocessing without rerunning\n      # create_variables, it still works.\n      wrapped_net = recurrence.CriticDeepRNN([net, lambda x: x])\n\n      for curr_net in [net, wrapped_net]:\n        # Save the test network.\n        directory = absltest.get_default_test_tmpdir()\n        objects_to_save = {'net': curr_net}\n        snapshotter = tf2_savers.Snapshotter(\n            objects_to_save, directory=directory)\n        snapshotter.save()\n\n        # Reload the test network.\n        net2 = tf.saved_model.load(os.path.join(snapshotter.directory, 'net'))\n\n        obs = tf.ones((2, 10))\n        actions = tf.ones((2, 10))\n        state = curr_net.initial_state(2)\n        outputs1, next_state1 = curr_net(obs, actions, state)\n        outputs2, next_state2 = net2(obs, actions, state)\n\n        assert np.allclose(outputs1, outputs2)\n        assert np.allclose(tree.flatten(next_state1), tree.flatten(next_state2))",
  "def __call__(self, o, a):\n    return o * a",
  "def __call__(self, o, a, prev_state):\n    return o * a, prev_state",
  "def initial_state(self, batch_size):\n    return ()",
  "def test_criticdeeprnn_snapshot(self):\n    \"\"\"Test that CriticDeepRNN works correctly with snapshotting.\"\"\"\n    # Create a test network.\n    critic = Critic()\n    rnn_critic = RNNCritic()\n\n    for base_net in [critic, rnn_critic]:\n      net = recurrence.CriticDeepRNN([base_net, snt.LSTM(10)])\n      obs = specs.Array([10], dtype=np.float32)\n      actions = specs.Array([10], dtype=np.float32)\n      spec = [obs, actions]\n      tf2_utils.create_variables(net, spec)\n\n      # Test that if you add some postprocessing without rerunning\n      # create_variables, it still works.\n      wrapped_net = recurrence.CriticDeepRNN([net, lambda x: x])\n\n      for curr_net in [net, wrapped_net]:\n        # Save the test network.\n        directory = absltest.get_default_test_tmpdir()\n        objects_to_save = {'net': curr_net}\n        snapshotter = tf2_savers.Snapshotter(\n            objects_to_save, directory=directory)\n        snapshotter.save()\n\n        # Reload the test network.\n        net2 = tf.saved_model.load(os.path.join(snapshotter.directory, 'net'))\n\n        obs = tf.ones((2, 10))\n        actions = tf.ones((2, 10))\n        state = curr_net.initial_state(2)\n        outputs1, next_state1 = curr_net(obs, actions, state)\n        outputs2, next_state2 = net2(obs, actions, state)\n\n        assert np.allclose(outputs1, outputs2)\n        assert np.allclose(tree.flatten(next_state1), tree.flatten(next_state2))",
  "def _uniform_initializer():\n  return tf.initializers.VarianceScaling(\n      distribution='uniform', mode='fan_out', scale=0.333)",
  "class NearZeroInitializedLinear(snt.Linear):\n  \"\"\"Simple linear layer, initialized at near zero weights and zero biases.\"\"\"\n\n  def __init__(self, output_size: int, scale: float = 1e-4):\n    super().__init__(output_size, w_init=tf.initializers.VarianceScaling(scale))",
  "class LayerNormMLP(snt.Module):\n  \"\"\"Simple feedforward MLP torso with initial layer-norm.\n\n  This module is an MLP which uses LayerNorm (with a tanh normalizer) on the\n  first layer and non-linearities (elu) on all but the last remaining layers.\n  \"\"\"\n\n  def __init__(self,\n               layer_sizes: Sequence[int],\n               w_init: Optional[snt.initializers.Initializer] = None,\n               activation: Callable[[tf.Tensor], tf.Tensor] = tf.nn.elu,\n               activate_final: bool = False):\n    \"\"\"Construct the MLP.\n\n    Args:\n      layer_sizes: a sequence of ints specifying the size of each layer.\n      w_init: initializer for Linear weights.\n      activation: activation function to apply between linear layers. Defaults\n        to ELU. Note! This is different from snt.nets.MLP's default.\n      activate_final: whether or not to use the activation function on the final\n        layer of the neural network.\n    \"\"\"\n    super().__init__(name='feedforward_mlp_torso')\n\n    self._network = snt.Sequential([\n        snt.Linear(layer_sizes[0], w_init=w_init or _uniform_initializer()),\n        snt.LayerNorm(\n            axis=slice(1, None), create_scale=True, create_offset=True),\n        tf.nn.tanh,\n        snt.nets.MLP(\n            layer_sizes[1:],\n            w_init=w_init or _uniform_initializer(),\n            activation=activation,\n            activate_final=activate_final),\n    ])\n\n  def __call__(self, observations: types.Nest) -> tf.Tensor:\n    \"\"\"Forwards the policy network.\"\"\"\n    return self._network(tf2_utils.batch_concat(observations))",
  "class ResidualLayernormWrapper(snt.Module):\n  \"\"\"Wrapper that applies residual connections and layer norm.\"\"\"\n\n  def __init__(self, layer: base.Module):\n    \"\"\"Creates the Wrapper Class.\n\n    Args:\n      layer: module to wrap.\n    \"\"\"\n\n    super().__init__(name='ResidualLayernormWrapper')\n    self._layer = layer\n\n    self._layer_norm = snt.LayerNorm(\n        axis=-1, create_scale=True, create_offset=True)\n\n  def __call__(self, inputs: tf.Tensor):\n    \"\"\"Returns the result of the residual and layernorm computation.\n\n    Args:\n      inputs: inputs to the main module.\n    \"\"\"\n\n    # Apply main module.\n    outputs = self._layer(inputs)\n    outputs = self._layer_norm(outputs + inputs)\n\n    return outputs",
  "class LayerNormAndResidualMLP(snt.Module):\n  \"\"\"MLP with residual connections and layer norm.\n\n  An MLP which applies residual connection and layer normalisation every two\n  linear layers. Similar to Resnet, but with FC layers instead of convolutions.\n  \"\"\"\n\n  def __init__(self, hidden_size: int, num_blocks: int):\n    \"\"\"Create the model.\n\n    Args:\n      hidden_size: width of each hidden layer.\n      num_blocks: number of blocks, each block being MLP([hidden_size,\n        hidden_size]) + layer norm + residual connection.\n    \"\"\"\n    super().__init__(name='LayerNormAndResidualMLP')\n\n    # Create initial MLP layer.\n    layers = [snt.nets.MLP([hidden_size], w_init=_uniform_initializer())]\n\n    # Follow it up with num_blocks MLPs with layernorm and residual connections.\n    for _ in range(num_blocks):\n      mlp = snt.nets.MLP([hidden_size, hidden_size],\n                         w_init=_uniform_initializer())\n      layers.append(ResidualLayernormWrapper(mlp))\n\n    self._module = snt.Sequential(layers)\n\n  def __call__(self, inputs: tf.Tensor):\n    return self._module(inputs)",
  "def __init__(self, output_size: int, scale: float = 1e-4):\n    super().__init__(output_size, w_init=tf.initializers.VarianceScaling(scale))",
  "def __init__(self,\n               layer_sizes: Sequence[int],\n               w_init: Optional[snt.initializers.Initializer] = None,\n               activation: Callable[[tf.Tensor], tf.Tensor] = tf.nn.elu,\n               activate_final: bool = False):\n    \"\"\"Construct the MLP.\n\n    Args:\n      layer_sizes: a sequence of ints specifying the size of each layer.\n      w_init: initializer for Linear weights.\n      activation: activation function to apply between linear layers. Defaults\n        to ELU. Note! This is different from snt.nets.MLP's default.\n      activate_final: whether or not to use the activation function on the final\n        layer of the neural network.\n    \"\"\"\n    super().__init__(name='feedforward_mlp_torso')\n\n    self._network = snt.Sequential([\n        snt.Linear(layer_sizes[0], w_init=w_init or _uniform_initializer()),\n        snt.LayerNorm(\n            axis=slice(1, None), create_scale=True, create_offset=True),\n        tf.nn.tanh,\n        snt.nets.MLP(\n            layer_sizes[1:],\n            w_init=w_init or _uniform_initializer(),\n            activation=activation,\n            activate_final=activate_final),\n    ])",
  "def __call__(self, observations: types.Nest) -> tf.Tensor:\n    \"\"\"Forwards the policy network.\"\"\"\n    return self._network(tf2_utils.batch_concat(observations))",
  "def __init__(self, layer: base.Module):\n    \"\"\"Creates the Wrapper Class.\n\n    Args:\n      layer: module to wrap.\n    \"\"\"\n\n    super().__init__(name='ResidualLayernormWrapper')\n    self._layer = layer\n\n    self._layer_norm = snt.LayerNorm(\n        axis=-1, create_scale=True, create_offset=True)",
  "def __call__(self, inputs: tf.Tensor):\n    \"\"\"Returns the result of the residual and layernorm computation.\n\n    Args:\n      inputs: inputs to the main module.\n    \"\"\"\n\n    # Apply main module.\n    outputs = self._layer(inputs)\n    outputs = self._layer_norm(outputs + inputs)\n\n    return outputs",
  "def __init__(self, hidden_size: int, num_blocks: int):\n    \"\"\"Create the model.\n\n    Args:\n      hidden_size: width of each hidden layer.\n      num_blocks: number of blocks, each block being MLP([hidden_size,\n        hidden_size]) + layer norm + residual connection.\n    \"\"\"\n    super().__init__(name='LayerNormAndResidualMLP')\n\n    # Create initial MLP layer.\n    layers = [snt.nets.MLP([hidden_size], w_init=_uniform_initializer())]\n\n    # Follow it up with num_blocks MLPs with layernorm and residual connections.\n    for _ in range(num_blocks):\n      mlp = snt.nets.MLP([hidden_size, hidden_size],\n                         w_init=_uniform_initializer())\n      layers.append(ResidualLayernormWrapper(mlp))\n\n    self._module = snt.Sequential(layers)",
  "def __call__(self, inputs: tf.Tensor):\n    return self._module(inputs)",
  "class ClippedGaussian(snt.Module):\n  \"\"\"Sonnet module for adding clipped Gaussian noise to each output.\"\"\"\n\n  def __init__(self, stddev: float, name: str = 'clipped_gaussian'):\n    super().__init__(name=name)\n    self._noise = tfd.Normal(loc=0., scale=stddev)\n\n  def __call__(self, inputs: types.NestedTensor) -> types.NestedTensor:\n    def add_noise(tensor: tf.Tensor):\n      output = tensor + tf.cast(self._noise.sample(tensor.shape),\n                                dtype=tensor.dtype)\n      output = tf.clip_by_value(output, -1.0, 1.0)\n      return output\n\n    return tree.map_structure(add_noise, inputs)",
  "def __init__(self, stddev: float, name: str = 'clipped_gaussian'):\n    super().__init__(name=name)\n    self._noise = tfd.Normal(loc=0., scale=stddev)",
  "def __call__(self, inputs: types.NestedTensor) -> types.NestedTensor:\n    def add_noise(tensor: tf.Tensor):\n      output = tensor + tf.cast(self._noise.sample(tensor.shape),\n                                dtype=tensor.dtype)\n      output = tf.clip_by_value(output, -1.0, 1.0)\n      return output\n\n    return tree.map_structure(add_noise, inputs)",
  "def add_noise(tensor: tf.Tensor):\n      output = tensor + tf.cast(self._noise.sample(tensor.shape),\n                                dtype=tensor.dtype)\n      output = tf.clip_by_value(output, -1.0, 1.0)\n      return output",
  "class Module(snt.Module, abc.ABC):\n  \"\"\"A base class for module with abstract __call__ method.\"\"\"\n\n  @abc.abstractmethod\n  def __call__(self, *args, **kwargs) -> types.NestedTensor:\n    \"\"\"Forward pass of the module.\"\"\"",
  "class DistributionalModule(snt.Module, abc.ABC):\n  \"\"\"A base class for modules that output distributions.\"\"\"\n\n  @abc.abstractmethod\n  def __call__(self, *args, **kwargs) -> tfp.distributions.Distribution:\n    \"\"\"Forward pass of the module.\"\"\"",
  "class RNNCore(snt.RNNCore, abc.ABC):\n  \"\"\"An RNN core with a custom `unroll` function.\"\"\"\n\n  @abc.abstractmethod\n  def unroll(self,\n             inputs: types.NestedTensor,\n             state: State,\n             sequence_length: int,\n             ) -> Tuple[types.NestedTensor, State]:\n    \"\"\"A custom function for doing static unrolls over sequences.\n\n    This has the same API as `snt.static_unroll`, but allows the user to specify\n    their own implementation to take advantage of the structure of the network\n    for better performance, e.g. by batching the feed-forward pass over the\n    whole sequence.\n\n    Args:\n      inputs: A nest of `tf.Tensor` in time-major format.\n      state: The RNN core state.\n      sequence_length: How long the static_unroll should go for.\n\n    Returns:\n      Nested sequence output of RNN, and final state.\n    \"\"\"",
  "def __call__(self, *args, **kwargs) -> types.NestedTensor:\n    \"\"\"Forward pass of the module.\"\"\"",
  "def __call__(self, *args, **kwargs) -> tfp.distributions.Distribution:\n    \"\"\"Forward pass of the module.\"\"\"",
  "def unroll(self,\n             inputs: types.NestedTensor,\n             state: State,\n             sequence_length: int,\n             ) -> Tuple[types.NestedTensor, State]:\n    \"\"\"A custom function for doing static unrolls over sequences.\n\n    This has the same API as `snt.static_unroll`, but allows the user to specify\n    their own implementation to take advantage of the structure of the network\n    for better performance, e.g. by batching the feed-forward pass over the\n    whole sequence.\n\n    Args:\n      inputs: A nest of `tf.Tensor` in time-major format.\n      state: The RNN core state.\n      sequence_length: How long the static_unroll should go for.\n\n    Returns:\n      Nested sequence output of RNN, and final state.\n    \"\"\"",
  "class ClipToSpec(snt.Module):\n  \"\"\"Sonnet module clipping inputs to within a BoundedArraySpec.\"\"\"\n\n  def __init__(self, spec: specs.BoundedArray, name: str = 'clip_to_spec'):\n    super().__init__(name=name)\n    self._min = spec.minimum\n    self._max = spec.maximum\n\n  def __call__(self, inputs: tf.Tensor) -> tf.Tensor:\n    return tf.clip_by_value(inputs, self._min, self._max)",
  "class RescaleToSpec(snt.Module):\n  \"\"\"Sonnet module rescaling inputs in [-1, 1] to match a BoundedArraySpec.\"\"\"\n\n  def __init__(self, spec: specs.BoundedArray, name: str = 'rescale_to_spec'):\n    super().__init__(name=name)\n    self._scale = spec.maximum - spec.minimum\n    self._offset = spec.minimum\n\n  def __call__(self, inputs: tf.Tensor) -> tf.Tensor:\n    inputs = 0.5 * (inputs + 1.0)  # [0, 1]\n    output = inputs * self._scale + self._offset  # [minimum, maximum]\n\n    return output",
  "class TanhToSpec(snt.Module):\n  \"\"\"Sonnet module squashing real-valued inputs to match a BoundedArraySpec.\"\"\"\n\n  def __init__(self, spec: specs.BoundedArray, name: str = 'tanh_to_spec'):\n    super().__init__(name=name)\n    self._scale = spec.maximum - spec.minimum\n    self._offset = spec.minimum\n\n  def __call__(\n      self, inputs: Union[tf.Tensor, tfd.Distribution]\n      ) -> Union[tf.Tensor, tfd.Distribution]:\n    if isinstance(inputs, tfd.Distribution):\n      inputs = tfb.Tanh()(inputs)\n      inputs = tfb.ScaleMatvecDiag(0.5 * self._scale)(inputs)\n      output = tfb.Shift(self._offset + 0.5 * self._scale)(inputs)\n    else:\n      inputs = tf.tanh(inputs)  # [-1, 1]\n      inputs = 0.5 * (inputs + 1.0)  # [0, 1]\n      output = inputs * self._scale + self._offset  # [minimum, maximum]\n    return output",
  "def __init__(self, spec: specs.BoundedArray, name: str = 'clip_to_spec'):\n    super().__init__(name=name)\n    self._min = spec.minimum\n    self._max = spec.maximum",
  "def __call__(self, inputs: tf.Tensor) -> tf.Tensor:\n    return tf.clip_by_value(inputs, self._min, self._max)",
  "def __init__(self, spec: specs.BoundedArray, name: str = 'rescale_to_spec'):\n    super().__init__(name=name)\n    self._scale = spec.maximum - spec.minimum\n    self._offset = spec.minimum",
  "def __call__(self, inputs: tf.Tensor) -> tf.Tensor:\n    inputs = 0.5 * (inputs + 1.0)  # [0, 1]\n    output = inputs * self._scale + self._offset  # [minimum, maximum]\n\n    return output",
  "def __init__(self, spec: specs.BoundedArray, name: str = 'tanh_to_spec'):\n    super().__init__(name=name)\n    self._scale = spec.maximum - spec.minimum\n    self._offset = spec.minimum",
  "def __call__(\n      self, inputs: Union[tf.Tensor, tfd.Distribution]\n      ) -> Union[tf.Tensor, tfd.Distribution]:\n    if isinstance(inputs, tfd.Distribution):\n      inputs = tfb.Tanh()(inputs)\n      inputs = tfb.ScaleMatvecDiag(0.5 * self._scale)(inputs)\n      output = tfb.Shift(self._offset + 0.5 * self._scale)(inputs)\n    else:\n      inputs = tf.tanh(inputs)  # [-1, 1]\n      inputs = 0.5 * (inputs + 1.0)  # [0, 1]\n      output = inputs * self._scale + self._offset  # [minimum, maximum]\n    return output",
  "class IQNNetwork(snt.Module):\n  \"\"\"A feedforward network for use with IQN.\n\n  IQN extends the Q-network of regular DQN which consists of torso and head\n  networks. IQN embeds sampled quantile thresholds into the output space of the\n  torso network and merges them with the torso output.\n\n  Outputs a tuple consisting of (mean) Q-values, Q-value quantiles, and sampled\n  quantile thresholds.\n  \"\"\"\n\n  def __init__(self,\n               torso: snt.Module,\n               head: snt.Module,\n               latent_dim: int,\n               num_quantile_samples: int,\n               name: str = 'iqn_network'):\n    \"\"\"Initializes the network.\n\n    Args:\n      torso: Network producing an intermediate representation, typically a\n        convolutional network.\n      head: Network producing Q-value quantiles, typically an MLP.\n      latent_dim: Dimension of latent variables.\n      num_quantile_samples: Number of quantile thresholds to sample.\n      name: Module name.\n    \"\"\"\n    super().__init__(name)\n    self._torso = torso\n    self._head = head\n    self._latent_dim = latent_dim\n    self._num_quantile_samples = num_quantile_samples\n\n  @snt.once\n  def _create_embedding(self, size):\n    self._embedding = snt.Linear(size)\n\n  def __call__(self, observations):\n    # Transform observations to intermediate representations (typically a\n    # convolutional network).\n    torso_output = self._torso(observations)\n\n    # Now that dimension of intermediate representation is known initialize\n    # embedding of sample quantile thresholds (only done once).\n    self._create_embedding(torso_output.shape[-1])\n\n    # Sample quantile thresholds.\n    batch_size = tf.shape(observations)[0]\n    tau_shape = tf.stack([batch_size, self._num_quantile_samples])\n    tau = tf.random.uniform(tau_shape)\n    indices = tf.range(1, self._latent_dim+1, dtype=tf.float32)\n\n    # Embed sampled quantile thresholds in intermediate representation space.\n    tau_tiled = tf.tile(tau[:, :, None], (1, 1, self._latent_dim))\n    indices_tiled = tf.tile(indices[None, None, :],\n                            tf.concat([tau_shape, [1]], 0))\n    tau_embedding = tf.cos(tau_tiled * indices_tiled * np.pi)\n    tau_embedding = snt.BatchApply(self._embedding)(tau_embedding)\n    tau_embedding = tf.nn.relu(tau_embedding)\n\n    # Merge intermediate representations with embeddings, and apply head\n    # network (typically an MLP).\n    torso_output = tf.tile(torso_output[:, None, :],\n                           (1, self._num_quantile_samples, 1))\n    q_value_quantiles = snt.BatchApply(self._head)(tau_embedding * torso_output)\n    q_dist = tf.transpose(q_value_quantiles, (0, 2, 1))\n    q_values = tf.reduce_mean(q_value_quantiles, axis=1)\n    q_values = tf.stop_gradient(q_values)\n\n    return q_values, q_dist, tau",
  "def __init__(self,\n               torso: snt.Module,\n               head: snt.Module,\n               latent_dim: int,\n               num_quantile_samples: int,\n               name: str = 'iqn_network'):\n    \"\"\"Initializes the network.\n\n    Args:\n      torso: Network producing an intermediate representation, typically a\n        convolutional network.\n      head: Network producing Q-value quantiles, typically an MLP.\n      latent_dim: Dimension of latent variables.\n      num_quantile_samples: Number of quantile thresholds to sample.\n      name: Module name.\n    \"\"\"\n    super().__init__(name)\n    self._torso = torso\n    self._head = head\n    self._latent_dim = latent_dim\n    self._num_quantile_samples = num_quantile_samples",
  "def _create_embedding(self, size):\n    self._embedding = snt.Linear(size)",
  "def __call__(self, observations):\n    # Transform observations to intermediate representations (typically a\n    # convolutional network).\n    torso_output = self._torso(observations)\n\n    # Now that dimension of intermediate representation is known initialize\n    # embedding of sample quantile thresholds (only done once).\n    self._create_embedding(torso_output.shape[-1])\n\n    # Sample quantile thresholds.\n    batch_size = tf.shape(observations)[0]\n    tau_shape = tf.stack([batch_size, self._num_quantile_samples])\n    tau = tf.random.uniform(tau_shape)\n    indices = tf.range(1, self._latent_dim+1, dtype=tf.float32)\n\n    # Embed sampled quantile thresholds in intermediate representation space.\n    tau_tiled = tf.tile(tau[:, :, None], (1, 1, self._latent_dim))\n    indices_tiled = tf.tile(indices[None, None, :],\n                            tf.concat([tau_shape, [1]], 0))\n    tau_embedding = tf.cos(tau_tiled * indices_tiled * np.pi)\n    tau_embedding = snt.BatchApply(self._embedding)(tau_embedding)\n    tau_embedding = tf.nn.relu(tau_embedding)\n\n    # Merge intermediate representations with embeddings, and apply head\n    # network (typically an MLP).\n    torso_output = tf.tile(torso_output[:, None, :],\n                           (1, self._num_quantile_samples, 1))\n    q_value_quantiles = snt.BatchApply(self._head)(tau_embedding * torso_output)\n    q_dist = tf.transpose(q_value_quantiles, (0, 2, 1))\n    q_values = tf.reduce_mean(q_value_quantiles, axis=1)\n    q_values = tf.stop_gradient(q_values)\n\n    return q_values, q_dist, tau",
  "class DistributionalTest(parameterized.TestCase):\n\n  @parameterized.parameters(\n      ((2, 3), (), (), 5, (2, 5)),\n      ((2, 3), (4, 1), (1, 5), 6, (2, 4, 5, 6)),\n      )\n  def test_discrete_valued_head(\n      self,\n      input_shape,\n      vmin_shape,\n      vmax_shape,\n      num_atoms,\n      expected_logits_shape):\n\n    vmin = np.zeros(vmin_shape, float)\n    vmax = np.ones(vmax_shape, float)\n    head = distributional.DiscreteValuedHead(\n        vmin=vmin,\n        vmax=vmax,\n        num_atoms=num_atoms)\n    input_array = np.zeros(input_shape, dtype=float)\n    output_distribution = head(input_array)\n    self.assertEqual(output_distribution.logits_parameter().shape,\n                     expected_logits_shape)\n\n    values = output_distribution._values\n\n    # Can't do assert_allclose(values[..., 0], vmin), because the args may\n    # have broadcast-compatible but unequal shapes. Do the following instead:\n    npt.assert_allclose(values[..., 0] - vmin, np.zeros_like(values[..., 0]))\n    npt.assert_allclose(values[..., -1] - vmax, np.zeros_like(values[..., -1]))\n\n    # Check that values are monotonically increasing.\n    intervals = values[..., 1:] - values[..., :-1]\n    npt.assert_array_less(np.zeros_like(intervals), intervals)\n\n    # Check that the values are equally spaced.\n    npt.assert_allclose(intervals[..., 1:] - intervals[..., :1],\n                        np.zeros_like(intervals[..., 1:]),\n                        atol=1e-7)",
  "def test_discrete_valued_head(\n      self,\n      input_shape,\n      vmin_shape,\n      vmax_shape,\n      num_atoms,\n      expected_logits_shape):\n\n    vmin = np.zeros(vmin_shape, float)\n    vmax = np.ones(vmax_shape, float)\n    head = distributional.DiscreteValuedHead(\n        vmin=vmin,\n        vmax=vmax,\n        num_atoms=num_atoms)\n    input_array = np.zeros(input_shape, dtype=float)\n    output_distribution = head(input_array)\n    self.assertEqual(output_distribution.logits_parameter().shape,\n                     expected_logits_shape)\n\n    values = output_distribution._values\n\n    # Can't do assert_allclose(values[..., 0], vmin), because the args may\n    # have broadcast-compatible but unequal shapes. Do the following instead:\n    npt.assert_allclose(values[..., 0] - vmin, np.zeros_like(values[..., 0]))\n    npt.assert_allclose(values[..., -1] - vmax, np.zeros_like(values[..., -1]))\n\n    # Check that values are monotonically increasing.\n    intervals = values[..., 1:] - values[..., :-1]\n    npt.assert_array_less(np.zeros_like(intervals), intervals)\n\n    # Check that the values are equally spaced.\n    npt.assert_allclose(intervals[..., 1:] - intervals[..., :1],\n                        np.zeros_like(intervals[..., 1:]),\n                        atol=1e-7)",
  "class DuellingMLP(snt.Module):\n  \"\"\"A Duelling MLP Q-network.\"\"\"\n\n  def __init__(\n      self,\n      num_actions: int,\n      hidden_sizes: Sequence[int],\n  ):\n    super().__init__(name='duelling_q_network')\n\n    self._value_mlp = snt.nets.MLP([*hidden_sizes, 1])\n    self._advantage_mlp = snt.nets.MLP([*hidden_sizes, num_actions])\n\n  def __call__(self, inputs: tf.Tensor) -> tf.Tensor:\n    \"\"\"Forward pass of the duelling network.\n\n    Args:\n      inputs: 2-D tensor of shape [batch_size, embedding_size].\n\n    Returns:\n      q_values: 2-D tensor of action values of shape [batch_size, num_actions]\n    \"\"\"\n\n    # Compute value & advantage for duelling.\n    value = self._value_mlp(inputs)  # [B, 1]\n    advantages = self._advantage_mlp(inputs)  # [B, A]\n\n    # Advantages have zero mean.\n    advantages -= tf.reduce_mean(advantages, axis=-1, keepdims=True)  # [B, A]\n\n    q_values = value + advantages  # [B, A]\n\n    return q_values",
  "def __init__(\n      self,\n      num_actions: int,\n      hidden_sizes: Sequence[int],\n  ):\n    super().__init__(name='duelling_q_network')\n\n    self._value_mlp = snt.nets.MLP([*hidden_sizes, 1])\n    self._advantage_mlp = snt.nets.MLP([*hidden_sizes, num_actions])",
  "def __call__(self, inputs: tf.Tensor) -> tf.Tensor:\n    \"\"\"Forward pass of the duelling network.\n\n    Args:\n      inputs: 2-D tensor of shape [batch_size, embedding_size].\n\n    Returns:\n      q_values: 2-D tensor of action values of shape [batch_size, num_actions]\n    \"\"\"\n\n    # Compute value & advantage for duelling.\n    value = self._value_mlp(inputs)  # [B, 1]\n    advantages = self._advantage_mlp(inputs)  # [B, A]\n\n    # Advantages have zero mean.\n    advantages -= tf.reduce_mean(advantages, axis=-1, keepdims=True)  # [B, A]\n\n    q_values = value + advantages  # [B, A]\n\n    return q_values",
  "class ResNetTorso(snt.Module):\n  \"\"\"ResNet architecture used in IMPALA paper.\"\"\"\n\n  def __init__(\n      self,\n      num_channels: Sequence[int] = (16, 32, 32),  # default to IMPALA resnet.\n      num_blocks: Sequence[int] = (2, 2, 2),  # default to IMPALA resnet.\n      num_output_hidden: Sequence[int] = (256,),  # default to IMPALA resnet.\n      conv_shape: Union[int, Sequence[int]] = 3,\n      conv_stride: Union[int, Sequence[int]] = 1,\n      pool_size: Union[int, Sequence[int]] = 3,\n      pool_stride: Union[int, Sequence[int], Sequence[Sequence[int]]] = 2,\n      data_format: str = 'NHWC',\n      activation: Callable[[tf.Tensor], tf.Tensor] = tf.nn.relu,\n      output_dtype: tf.DType = tf.float32,\n      name: str = 'resnet_torso'):\n    \"\"\"Builds an IMPALA-style ResNet.\n\n    The arguments' default values construct the IMPALA resnet.\n\n    Args:\n      num_channels: The number of convolutional channels for each layer.\n      num_blocks: The number of resnet blocks in each \"layer\".\n      num_output_hidden: The output size(s) of the MLP layer(s) on top.\n      conv_shape: The convolution filter size (int), or size dimensions (H, W).\n      conv_stride: the convolution stride (int), or strides (row, column).\n      pool_size: The pooling footprint size (int), or size dimensions (H, W).\n      pool_stride: The pooling stride (int) or strides (row, column), or\n        strides for each of the N layers ((r1, c1), (r2, c2), ..., (rN, cN)).\n      data_format: The axis order of the input.\n      activation: The activation function.\n      output_dtype: the output dtype.\n      name: The Sonnet module name.\n    \"\"\"\n    super().__init__(name=name)\n\n    self._output_dtype = output_dtype\n    self._num_layers = len(num_blocks)\n\n    if isinstance(pool_stride, int):\n      pool_stride = (pool_stride, pool_stride)\n\n    if isinstance(pool_stride[0], int):\n      pool_stride = self._num_layers * (pool_stride,)\n\n    # Create sequence of residual blocks.\n    blocks = []\n    for i in range(self._num_layers):\n      blocks.append(\n          ResidualBlockGroup(\n              num_blocks[i],\n              num_channels[i],\n              conv_shape,\n              conv_stride,\n              pool_size,\n              pool_stride[i],\n              data_format=data_format,\n              activation=activation))\n\n    # Create output layer.\n    out_layer = snt.nets.MLP(num_output_hidden, activation=activation)\n\n    # Compose blocks and final layer.\n    self._resnet = snt.Sequential(\n        blocks + [activation, snt.Flatten(), out_layer])\n\n  def __call__(self, inputs: tf.Tensor) -> tf.Tensor:\n    \"\"\"Evaluates the ResidualPixelCore.\"\"\"\n\n    # Convert to floats.\n    preprocessed_inputs = _preprocess_inputs(inputs, self._output_dtype)\n    torso_output = self._resnet(preprocessed_inputs)\n\n    return torso_output",
  "class ResidualBlockGroup(snt.Module):\n  \"\"\"Higher level block for ResNet implementation.\"\"\"\n\n  def __init__(self,\n               num_blocks: int,\n               num_output_channels: int,\n               conv_shape: Union[int, Sequence[int]],\n               conv_stride: Union[int, Sequence[int]],\n               pool_shape: Union[int, Sequence[int]],\n               pool_stride: Union[int, Sequence[int]],\n               data_format: str = 'NHWC',\n               activation: Callable[[tf.Tensor], tf.Tensor] = tf.nn.relu,\n               name: Optional[str] = None):\n    super().__init__(name=name)\n\n    self._num_blocks = num_blocks\n    self._data_format = data_format\n    self._activation = activation\n\n    # The pooling operation expects a 2-rank shape/stride (height and width).\n    if isinstance(pool_shape, int):\n      pool_shape = 2 * [pool_shape]\n    if isinstance(pool_stride, int):\n      pool_stride = 2 * [pool_stride]\n\n    # Create a Conv2D factory since we'll be making quite a few.\n    def build_conv_layer(name: str):\n      return snt.Conv2D(\n          num_output_channels,\n          conv_shape,\n          stride=conv_stride,\n          padding='SAME',\n          data_format=data_format,\n          name=name)\n\n    # Create a pooling layer.\n    def pooling_layer(inputs: tf.Tensor) -> tf.Tensor:\n      return tf.nn.pool(\n          inputs,\n          pool_shape,\n          pooling_type='MAX',\n          strides=pool_stride,\n          padding='SAME',\n          data_format=data_format)\n\n    # Create an initial conv layer and pooling to scale the image down.\n    self._downscale = snt.Sequential(\n        [build_conv_layer('downscale'), pooling_layer])\n\n    # Residual block(s).\n    self._convs = []\n    for i in range(self._num_blocks):\n      name = 'residual_block_%d' % i\n      self._convs.append(\n          [build_conv_layer(name + '_0'),\n           build_conv_layer(name + '_1')])\n\n  def __call__(self, inputs: tf.Tensor) -> tf.Tensor:\n    # Downscale the inputs.\n    conv_out = self._downscale(inputs)\n\n    # Apply (sequence of) residual block(s).\n    for i in range(self._num_blocks):\n      block_input = conv_out\n      conv_out = self._activation(conv_out)\n      conv_out = self._convs[i][0](conv_out)\n      conv_out = self._activation(conv_out)\n      conv_out = self._convs[i][1](conv_out)\n      conv_out += block_input\n    return conv_out",
  "def _preprocess_inputs(inputs: tf.Tensor, output_dtype: tf.DType) -> tf.Tensor:\n  \"\"\"Returns the `Tensor` corresponding to the preprocessed inputs.\"\"\"\n  rank = inputs.shape.rank\n  if rank < 4:\n    raise ValueError(\n        'Input Tensor must have at least 4 dimensions (for '\n        'batch size, height, width, and channels), but it only has '\n        '{}'.format(rank))\n\n  flattened_inputs = snt.Flatten(preserve_dims=3)(inputs)\n  processed_inputs = tf.image.convert_image_dtype(\n      flattened_inputs, dtype=output_dtype)\n  return processed_inputs",
  "class DrQTorso(snt.Module):\n  \"\"\"DrQ Torso inspired by the second DrQ paper [Yarats et al., 2021].\n\n  [Yarats et al., 2021] https://arxiv.org/abs/2107.09645\n  \"\"\"\n\n  def __init__(\n      self,\n      data_format: str = 'NHWC',\n      activation: Callable[[tf.Tensor], tf.Tensor] = tf.nn.relu,\n      output_dtype: tf.DType = tf.float32,\n      name: str = 'resnet_torso'):\n    super().__init__(name=name)\n\n    self._output_dtype = output_dtype\n\n    # Create a Conv2D factory since we'll be making quite a few.\n    gain = 2**0.5 if activation == tf.nn.relu else 1.\n    def build_conv_layer(name: str,\n                         output_channels: int = 32,\n                         kernel_shape: Sequence[int] = (3, 3),\n                         stride: int = 1):\n      return snt.Conv2D(\n          output_channels=output_channels,\n          kernel_shape=kernel_shape,\n          stride=stride,\n          padding='SAME',\n          data_format=data_format,\n          w_init=snt.initializers.Orthogonal(gain=gain, seed=None),\n          b_init=snt.initializers.Zeros(),\n          name=name)\n\n    self._network = snt.Sequential(\n        [build_conv_layer('conv_0', stride=2),\n         activation,\n         build_conv_layer('conv_1', stride=1),\n         activation,\n         build_conv_layer('conv_2', stride=1),\n         activation,\n         build_conv_layer('conv_3', stride=1),\n         activation,\n         snt.Flatten()])\n\n  def __call__(self, inputs: tf.Tensor) -> tf.Tensor:\n    \"\"\"Evaluates the ResidualPixelCore.\"\"\"\n\n    # Normalize to -0.5 to 0.5\n    preprocessed_inputs = _preprocess_inputs(inputs, self._output_dtype) - 0.5\n\n    torso_output = self._network(preprocessed_inputs)\n\n    return torso_output",
  "def __init__(\n      self,\n      num_channels: Sequence[int] = (16, 32, 32),  # default to IMPALA resnet.\n      num_blocks: Sequence[int] = (2, 2, 2),  # default to IMPALA resnet.\n      num_output_hidden: Sequence[int] = (256,),  # default to IMPALA resnet.\n      conv_shape: Union[int, Sequence[int]] = 3,\n      conv_stride: Union[int, Sequence[int]] = 1,\n      pool_size: Union[int, Sequence[int]] = 3,\n      pool_stride: Union[int, Sequence[int], Sequence[Sequence[int]]] = 2,\n      data_format: str = 'NHWC',\n      activation: Callable[[tf.Tensor], tf.Tensor] = tf.nn.relu,\n      output_dtype: tf.DType = tf.float32,\n      name: str = 'resnet_torso'):\n    \"\"\"Builds an IMPALA-style ResNet.\n\n    The arguments' default values construct the IMPALA resnet.\n\n    Args:\n      num_channels: The number of convolutional channels for each layer.\n      num_blocks: The number of resnet blocks in each \"layer\".\n      num_output_hidden: The output size(s) of the MLP layer(s) on top.\n      conv_shape: The convolution filter size (int), or size dimensions (H, W).\n      conv_stride: the convolution stride (int), or strides (row, column).\n      pool_size: The pooling footprint size (int), or size dimensions (H, W).\n      pool_stride: The pooling stride (int) or strides (row, column), or\n        strides for each of the N layers ((r1, c1), (r2, c2), ..., (rN, cN)).\n      data_format: The axis order of the input.\n      activation: The activation function.\n      output_dtype: the output dtype.\n      name: The Sonnet module name.\n    \"\"\"\n    super().__init__(name=name)\n\n    self._output_dtype = output_dtype\n    self._num_layers = len(num_blocks)\n\n    if isinstance(pool_stride, int):\n      pool_stride = (pool_stride, pool_stride)\n\n    if isinstance(pool_stride[0], int):\n      pool_stride = self._num_layers * (pool_stride,)\n\n    # Create sequence of residual blocks.\n    blocks = []\n    for i in range(self._num_layers):\n      blocks.append(\n          ResidualBlockGroup(\n              num_blocks[i],\n              num_channels[i],\n              conv_shape,\n              conv_stride,\n              pool_size,\n              pool_stride[i],\n              data_format=data_format,\n              activation=activation))\n\n    # Create output layer.\n    out_layer = snt.nets.MLP(num_output_hidden, activation=activation)\n\n    # Compose blocks and final layer.\n    self._resnet = snt.Sequential(\n        blocks + [activation, snt.Flatten(), out_layer])",
  "def __call__(self, inputs: tf.Tensor) -> tf.Tensor:\n    \"\"\"Evaluates the ResidualPixelCore.\"\"\"\n\n    # Convert to floats.\n    preprocessed_inputs = _preprocess_inputs(inputs, self._output_dtype)\n    torso_output = self._resnet(preprocessed_inputs)\n\n    return torso_output",
  "def __init__(self,\n               num_blocks: int,\n               num_output_channels: int,\n               conv_shape: Union[int, Sequence[int]],\n               conv_stride: Union[int, Sequence[int]],\n               pool_shape: Union[int, Sequence[int]],\n               pool_stride: Union[int, Sequence[int]],\n               data_format: str = 'NHWC',\n               activation: Callable[[tf.Tensor], tf.Tensor] = tf.nn.relu,\n               name: Optional[str] = None):\n    super().__init__(name=name)\n\n    self._num_blocks = num_blocks\n    self._data_format = data_format\n    self._activation = activation\n\n    # The pooling operation expects a 2-rank shape/stride (height and width).\n    if isinstance(pool_shape, int):\n      pool_shape = 2 * [pool_shape]\n    if isinstance(pool_stride, int):\n      pool_stride = 2 * [pool_stride]\n\n    # Create a Conv2D factory since we'll be making quite a few.\n    def build_conv_layer(name: str):\n      return snt.Conv2D(\n          num_output_channels,\n          conv_shape,\n          stride=conv_stride,\n          padding='SAME',\n          data_format=data_format,\n          name=name)\n\n    # Create a pooling layer.\n    def pooling_layer(inputs: tf.Tensor) -> tf.Tensor:\n      return tf.nn.pool(\n          inputs,\n          pool_shape,\n          pooling_type='MAX',\n          strides=pool_stride,\n          padding='SAME',\n          data_format=data_format)\n\n    # Create an initial conv layer and pooling to scale the image down.\n    self._downscale = snt.Sequential(\n        [build_conv_layer('downscale'), pooling_layer])\n\n    # Residual block(s).\n    self._convs = []\n    for i in range(self._num_blocks):\n      name = 'residual_block_%d' % i\n      self._convs.append(\n          [build_conv_layer(name + '_0'),\n           build_conv_layer(name + '_1')])",
  "def __call__(self, inputs: tf.Tensor) -> tf.Tensor:\n    # Downscale the inputs.\n    conv_out = self._downscale(inputs)\n\n    # Apply (sequence of) residual block(s).\n    for i in range(self._num_blocks):\n      block_input = conv_out\n      conv_out = self._activation(conv_out)\n      conv_out = self._convs[i][0](conv_out)\n      conv_out = self._activation(conv_out)\n      conv_out = self._convs[i][1](conv_out)\n      conv_out += block_input\n    return conv_out",
  "def __init__(\n      self,\n      data_format: str = 'NHWC',\n      activation: Callable[[tf.Tensor], tf.Tensor] = tf.nn.relu,\n      output_dtype: tf.DType = tf.float32,\n      name: str = 'resnet_torso'):\n    super().__init__(name=name)\n\n    self._output_dtype = output_dtype\n\n    # Create a Conv2D factory since we'll be making quite a few.\n    gain = 2**0.5 if activation == tf.nn.relu else 1.\n    def build_conv_layer(name: str,\n                         output_channels: int = 32,\n                         kernel_shape: Sequence[int] = (3, 3),\n                         stride: int = 1):\n      return snt.Conv2D(\n          output_channels=output_channels,\n          kernel_shape=kernel_shape,\n          stride=stride,\n          padding='SAME',\n          data_format=data_format,\n          w_init=snt.initializers.Orthogonal(gain=gain, seed=None),\n          b_init=snt.initializers.Zeros(),\n          name=name)\n\n    self._network = snt.Sequential(\n        [build_conv_layer('conv_0', stride=2),\n         activation,\n         build_conv_layer('conv_1', stride=1),\n         activation,\n         build_conv_layer('conv_2', stride=1),\n         activation,\n         build_conv_layer('conv_3', stride=1),\n         activation,\n         snt.Flatten()])",
  "def __call__(self, inputs: tf.Tensor) -> tf.Tensor:\n    \"\"\"Evaluates the ResidualPixelCore.\"\"\"\n\n    # Normalize to -0.5 to 0.5\n    preprocessed_inputs = _preprocess_inputs(inputs, self._output_dtype) - 0.5\n\n    torso_output = self._network(preprocessed_inputs)\n\n    return torso_output",
  "def build_conv_layer(name: str):\n      return snt.Conv2D(\n          num_output_channels,\n          conv_shape,\n          stride=conv_stride,\n          padding='SAME',\n          data_format=data_format,\n          name=name)",
  "def pooling_layer(inputs: tf.Tensor) -> tf.Tensor:\n      return tf.nn.pool(\n          inputs,\n          pool_shape,\n          pooling_type='MAX',\n          strides=pool_stride,\n          padding='SAME',\n          data_format=data_format)",
  "def build_conv_layer(name: str,\n                         output_channels: int = 32,\n                         kernel_shape: Sequence[int] = (3, 3),\n                         stride: int = 1):\n      return snt.Conv2D(\n          output_channels=output_channels,\n          kernel_shape=kernel_shape,\n          stride=stride,\n          padding='SAME',\n          data_format=data_format,\n          w_init=snt.initializers.Orthogonal(gain=gain, seed=None),\n          b_init=snt.initializers.Zeros(),\n          name=name)",
  "class AtariTorso(base.Module):\n  \"\"\"Simple convolutional stack commonly used for Atari.\"\"\"\n\n  def __init__(self):\n    super().__init__(name='atari_torso')\n    self._network = snt.Sequential([\n        snt.Conv2D(32, [8, 8], [4, 4]),\n        tf.nn.relu,\n        snt.Conv2D(64, [4, 4], [2, 2]),\n        tf.nn.relu,\n        snt.Conv2D(64, [3, 3], [1, 1]),\n        tf.nn.relu,\n        snt.Flatten(),\n    ])\n\n  def __call__(self, inputs: Images) -> tf.Tensor:\n    return self._network(inputs)",
  "class DQNAtariNetwork(base.Module):\n  \"\"\"A feed-forward network for use with Ape-X DQN.\n\n  See https://arxiv.org/pdf/1803.00933.pdf for more information.\n  \"\"\"\n\n  def __init__(self, num_actions: int):\n    super().__init__(name='dqn_atari_network')\n    self._network = snt.Sequential([\n        AtariTorso(),\n        duelling.DuellingMLP(num_actions, hidden_sizes=[512]),\n    ])\n\n  def __call__(self, inputs: Images) -> QValues:\n    return self._network(inputs)",
  "class R2D2AtariNetwork(base.RNNCore):\n  \"\"\"A recurrent network for use with R2D2.\n\n  See https://openreview.net/forum?id=r1lyTjAqYX for more information.\n  \"\"\"\n\n  def __init__(self, num_actions: int, core: Optional[base.RNNCore] = None):\n    super().__init__(name='r2d2_atari_network')\n    self._embed = embedding.OAREmbedding(\n        torso=AtariTorso(), num_actions=num_actions)\n    self._core = core if core is not None else recurrence.LSTM(512)\n    self._head = duelling.DuellingMLP(num_actions, hidden_sizes=[512])\n\n  def __call__(\n      self,\n      inputs: observation_action_reward.OAR,\n      state: base.State,\n  ) -> Tuple[QValues, base.State]:\n\n    embeddings = self._embed(inputs)\n    embeddings, new_state = self._core(embeddings, state)\n    action_values = self._head(embeddings)  # [B, A]\n\n    return action_values, new_state\n\n  # TODO(b/171287329): Figure out why return type annotation causes error.\n  def initial_state(self, batch_size: int, **unused_kwargs) -> base.State:  # pytype: disable=invalid-annotation\n    return self._core.initial_state(batch_size)\n\n  def unroll(\n      self,\n      inputs: observation_action_reward.OAR,\n      state: base.State,\n      sequence_length: int,\n  ) -> Tuple[QValues, base.State]:\n    \"\"\"Efficient unroll that applies embeddings, MLP, & convnet in one pass.\"\"\"\n    embeddings = snt.BatchApply(self._embed)(inputs)  # [T, B, D+A+1]\n    embeddings, new_state = self._core.unroll(embeddings, state,\n                                              sequence_length)\n    action_values = snt.BatchApply(self._head)(embeddings)\n\n    return action_values, new_state",
  "class IMPALAAtariNetwork(snt.RNNCore):\n  \"\"\"A recurrent network for use with IMPALA.\n\n  See https://arxiv.org/pdf/1802.01561.pdf for more information.\n  \"\"\"\n\n  def __init__(self, num_actions: int):\n    super().__init__(name='impala_atari_network')\n    self._embed = embedding.OAREmbedding(\n        torso=AtariTorso(), num_actions=num_actions)\n    self._core = snt.LSTM(256)\n    self._head = snt.Sequential([\n        snt.Linear(256),\n        tf.nn.relu,\n        policy_value.PolicyValueHead(num_actions),\n    ])\n    self._num_actions = num_actions\n\n  def __call__(\n      self, inputs: observation_action_reward.OAR,\n      state: snt.LSTMState) -> Tuple[Tuple[Logits, Value], snt.LSTMState]:\n\n    embeddings = self._embed(inputs)\n    embeddings, new_state = self._core(embeddings, state)\n    logits, value = self._head(embeddings)  # [B, A]\n\n    return (logits, value), new_state\n\n  def initial_state(self, batch_size: int, **unused_kwargs) -> snt.LSTMState:\n    return self._core.initial_state(batch_size)",
  "class DeepIMPALAAtariNetwork(base.RNNCore):\n  \"\"\"A recurrent network for use with IMPALA.\n\n  See https://arxiv.org/pdf/1802.01561.pdf for more information.\n  \"\"\"\n\n  def __init__(self, num_actions: int):\n    super().__init__(name='deep_impala_atari_network')\n    self._embed = embedding.OAREmbedding(\n        torso=vision.ResNetTorso(), num_actions=num_actions)\n    self._core = snt.LSTM(256)\n    self._head = snt.Sequential([\n        snt.Linear(256),\n        tf.nn.relu,\n        policy_value.PolicyValueHead(num_actions),\n    ])\n    self._num_actions = num_actions\n\n  def __call__(\n      self, inputs: observation_action_reward.OAR,\n      state: snt.LSTMState) -> Tuple[Tuple[Logits, Value], snt.LSTMState]:\n\n    embeddings = self._embed(inputs)\n    embeddings, new_state = self._core(embeddings, state)\n    logits, value = self._head(embeddings)  # [B, A]\n\n    return (logits, value), new_state\n\n  def initial_state(self, batch_size: int, **unused_kwargs) -> snt.LSTMState:\n    return self._core.initial_state(batch_size)\n\n  def unroll(\n      self,\n      inputs: observation_action_reward.OAR,\n      states: snt.LSTMState,\n      sequence_length: int,\n  ) -> Tuple[Tuple[Logits, Value], snt.LSTMState]:\n    \"\"\"Efficient unroll that applies embeddings, MLP, & convnet in one pass.\"\"\"\n    embeddings = snt.BatchApply(self._embed)(inputs)  # [T, B, D+A+1]\n    embeddings, new_states = snt.static_unroll(self._core, embeddings, states,\n                                               sequence_length)\n    logits, values = snt.BatchApply(self._head)(embeddings)\n\n    return (logits, values), new_states",
  "def __init__(self):\n    super().__init__(name='atari_torso')\n    self._network = snt.Sequential([\n        snt.Conv2D(32, [8, 8], [4, 4]),\n        tf.nn.relu,\n        snt.Conv2D(64, [4, 4], [2, 2]),\n        tf.nn.relu,\n        snt.Conv2D(64, [3, 3], [1, 1]),\n        tf.nn.relu,\n        snt.Flatten(),\n    ])",
  "def __call__(self, inputs: Images) -> tf.Tensor:\n    return self._network(inputs)",
  "def __init__(self, num_actions: int):\n    super().__init__(name='dqn_atari_network')\n    self._network = snt.Sequential([\n        AtariTorso(),\n        duelling.DuellingMLP(num_actions, hidden_sizes=[512]),\n    ])",
  "def __call__(self, inputs: Images) -> QValues:\n    return self._network(inputs)",
  "def __init__(self, num_actions: int, core: Optional[base.RNNCore] = None):\n    super().__init__(name='r2d2_atari_network')\n    self._embed = embedding.OAREmbedding(\n        torso=AtariTorso(), num_actions=num_actions)\n    self._core = core if core is not None else recurrence.LSTM(512)\n    self._head = duelling.DuellingMLP(num_actions, hidden_sizes=[512])",
  "def __call__(\n      self,\n      inputs: observation_action_reward.OAR,\n      state: base.State,\n  ) -> Tuple[QValues, base.State]:\n\n    embeddings = self._embed(inputs)\n    embeddings, new_state = self._core(embeddings, state)\n    action_values = self._head(embeddings)  # [B, A]\n\n    return action_values, new_state",
  "def initial_state(self, batch_size: int, **unused_kwargs) -> base.State:  # pytype: disable=invalid-annotation\n    return self._core.initial_state(batch_size)",
  "def unroll(\n      self,\n      inputs: observation_action_reward.OAR,\n      state: base.State,\n      sequence_length: int,\n  ) -> Tuple[QValues, base.State]:\n    \"\"\"Efficient unroll that applies embeddings, MLP, & convnet in one pass.\"\"\"\n    embeddings = snt.BatchApply(self._embed)(inputs)  # [T, B, D+A+1]\n    embeddings, new_state = self._core.unroll(embeddings, state,\n                                              sequence_length)\n    action_values = snt.BatchApply(self._head)(embeddings)\n\n    return action_values, new_state",
  "def __init__(self, num_actions: int):\n    super().__init__(name='impala_atari_network')\n    self._embed = embedding.OAREmbedding(\n        torso=AtariTorso(), num_actions=num_actions)\n    self._core = snt.LSTM(256)\n    self._head = snt.Sequential([\n        snt.Linear(256),\n        tf.nn.relu,\n        policy_value.PolicyValueHead(num_actions),\n    ])\n    self._num_actions = num_actions",
  "def __call__(\n      self, inputs: observation_action_reward.OAR,\n      state: snt.LSTMState) -> Tuple[Tuple[Logits, Value], snt.LSTMState]:\n\n    embeddings = self._embed(inputs)\n    embeddings, new_state = self._core(embeddings, state)\n    logits, value = self._head(embeddings)  # [B, A]\n\n    return (logits, value), new_state",
  "def initial_state(self, batch_size: int, **unused_kwargs) -> snt.LSTMState:\n    return self._core.initial_state(batch_size)",
  "def __init__(self, num_actions: int):\n    super().__init__(name='deep_impala_atari_network')\n    self._embed = embedding.OAREmbedding(\n        torso=vision.ResNetTorso(), num_actions=num_actions)\n    self._core = snt.LSTM(256)\n    self._head = snt.Sequential([\n        snt.Linear(256),\n        tf.nn.relu,\n        policy_value.PolicyValueHead(num_actions),\n    ])\n    self._num_actions = num_actions",
  "def __call__(\n      self, inputs: observation_action_reward.OAR,\n      state: snt.LSTMState) -> Tuple[Tuple[Logits, Value], snt.LSTMState]:\n\n    embeddings = self._embed(inputs)\n    embeddings, new_state = self._core(embeddings, state)\n    logits, value = self._head(embeddings)  # [B, A]\n\n    return (logits, value), new_state",
  "def initial_state(self, batch_size: int, **unused_kwargs) -> snt.LSTMState:\n    return self._core.initial_state(batch_size)",
  "def unroll(\n      self,\n      inputs: observation_action_reward.OAR,\n      states: snt.LSTMState,\n      sequence_length: int,\n  ) -> Tuple[Tuple[Logits, Value], snt.LSTMState]:\n    \"\"\"Efficient unroll that applies embeddings, MLP, & convnet in one pass.\"\"\"\n    embeddings = snt.BatchApply(self._embed)(inputs)  # [T, B, D+A+1]\n    embeddings, new_states = snt.static_unroll(self._core, embeddings, states,\n                                               sequence_length)\n    logits, values = snt.BatchApply(self._head)(embeddings)\n\n    return (logits, values), new_states",
  "class DiscreteValuedDistribution(tfd.Categorical):\n  \"\"\"This is a generalization of a categorical distribution.\n\n  The support for the DiscreteValued distribution can be any real valued range,\n  whereas the categorical distribution has support [0, n_categories - 1] or\n  [1, n_categories]. This generalization allows us to take the mean of the\n  distribution over its support.\n  \"\"\"\n\n  def __init__(self,\n               values: tf.Tensor,\n               logits: Optional[tf.Tensor] = None,\n               probs: Optional[tf.Tensor] = None,\n               name: str = 'DiscreteValuedDistribution'):\n    \"\"\"Initialization.\n\n    Args:\n      values: Values making up support of the distribution. Should have a shape\n        compatible with logits.\n      logits: An N-D Tensor, N >= 1, representing the log probabilities of a set\n        of Categorical distributions. The first N - 1 dimensions index into a\n        batch of independent distributions and the last dimension indexes into\n        the classes.\n      probs: An N-D Tensor, N >= 1, representing the probabilities of a set of\n        Categorical distributions. The first N - 1 dimensions index into a batch\n        of independent distributions and the last dimension represents a vector\n        of probabilities for each class. Only one of logits or probs should be\n        passed in.\n      name: Name of the distribution object.\n    \"\"\"\n    self._values = tf.convert_to_tensor(values)\n    shape_strings = [f'D{i}' for i, _ in enumerate(values.shape)]\n\n    if logits is not None:\n      logits = tf.convert_to_tensor(logits)\n      tf.debugging.assert_shapes([(values, shape_strings),\n                                  (logits, [..., *shape_strings])])\n    if probs is not None:\n      probs = tf.convert_to_tensor(probs)\n      tf.debugging.assert_shapes([(values, shape_strings),\n                                  (probs, [..., *shape_strings])])\n\n    super().__init__(logits=logits, probs=probs, name=name)\n\n    self._parameters = dict(values=values,\n                            logits=logits,\n                            probs=probs,\n                            name=name)\n\n  @property\n  def values(self) -> tf.Tensor:\n    return self._values\n\n  @classmethod\n  def _parameter_properties(cls, dtype, num_classes=None):\n    return dict(\n        values=tfp.util.ParameterProperties(event_ndims=None),\n        logits=tfp.util.ParameterProperties(\n            event_ndims=lambda self: self.values.shape.rank),\n        probs=tfp.util.ParameterProperties(\n            event_ndims=lambda self: self.values.shape.rank,\n            is_preferred=False))\n\n  def _sample_n(self, n, seed=None) -> tf.Tensor:\n    indices = super()._sample_n(n, seed=seed)\n    return tf.gather(self.values, indices, axis=-1)\n\n  def _mean(self) -> tf.Tensor:\n    \"\"\"Overrides the Categorical mean by incorporating category values.\"\"\"\n    return tf.reduce_sum(self.probs_parameter() * self.values, axis=-1)\n\n  def _variance(self) -> tf.Tensor:\n    \"\"\"Overrides the Categorical variance by incorporating category values.\"\"\"\n    dist_squared = tf.square(tf.expand_dims(self.mean(), -1) - self.values)\n    return tf.reduce_sum(self.probs_parameter() * dist_squared, axis=-1)\n\n  def _event_shape(self):\n    # Omit the atoms axis, to return just the shape of a single (i.e. unbatched)\n    # sample value.\n    return self._values.shape[:-1]\n\n  def _event_shape_tensor(self):\n    return tf.shape(self._values)[:-1]",
  "def __init__(self,\n               values: tf.Tensor,\n               logits: Optional[tf.Tensor] = None,\n               probs: Optional[tf.Tensor] = None,\n               name: str = 'DiscreteValuedDistribution'):\n    \"\"\"Initialization.\n\n    Args:\n      values: Values making up support of the distribution. Should have a shape\n        compatible with logits.\n      logits: An N-D Tensor, N >= 1, representing the log probabilities of a set\n        of Categorical distributions. The first N - 1 dimensions index into a\n        batch of independent distributions and the last dimension indexes into\n        the classes.\n      probs: An N-D Tensor, N >= 1, representing the probabilities of a set of\n        Categorical distributions. The first N - 1 dimensions index into a batch\n        of independent distributions and the last dimension represents a vector\n        of probabilities for each class. Only one of logits or probs should be\n        passed in.\n      name: Name of the distribution object.\n    \"\"\"\n    self._values = tf.convert_to_tensor(values)\n    shape_strings = [f'D{i}' for i, _ in enumerate(values.shape)]\n\n    if logits is not None:\n      logits = tf.convert_to_tensor(logits)\n      tf.debugging.assert_shapes([(values, shape_strings),\n                                  (logits, [..., *shape_strings])])\n    if probs is not None:\n      probs = tf.convert_to_tensor(probs)\n      tf.debugging.assert_shapes([(values, shape_strings),\n                                  (probs, [..., *shape_strings])])\n\n    super().__init__(logits=logits, probs=probs, name=name)\n\n    self._parameters = dict(values=values,\n                            logits=logits,\n                            probs=probs,\n                            name=name)",
  "def values(self) -> tf.Tensor:\n    return self._values",
  "def _parameter_properties(cls, dtype, num_classes=None):\n    return dict(\n        values=tfp.util.ParameterProperties(event_ndims=None),\n        logits=tfp.util.ParameterProperties(\n            event_ndims=lambda self: self.values.shape.rank),\n        probs=tfp.util.ParameterProperties(\n            event_ndims=lambda self: self.values.shape.rank,\n            is_preferred=False))",
  "def _sample_n(self, n, seed=None) -> tf.Tensor:\n    indices = super()._sample_n(n, seed=seed)\n    return tf.gather(self.values, indices, axis=-1)",
  "def _mean(self) -> tf.Tensor:\n    \"\"\"Overrides the Categorical mean by incorporating category values.\"\"\"\n    return tf.reduce_sum(self.probs_parameter() * self.values, axis=-1)",
  "def _variance(self) -> tf.Tensor:\n    \"\"\"Overrides the Categorical variance by incorporating category values.\"\"\"\n    dist_squared = tf.square(tf.expand_dims(self.mean(), -1) - self.values)\n    return tf.reduce_sum(self.probs_parameter() * dist_squared, axis=-1)",
  "def _event_shape(self):\n    # Omit the atoms axis, to return just the shape of a single (i.e. unbatched)\n    # sample value.\n    return self._values.shape[:-1]",
  "def _event_shape_tensor(self):\n    return tf.shape(self._values)[:-1]",
  "class PolicyValueHead(snt.Module):\n  \"\"\"A network with two linear layers, for policy and value respectively.\"\"\"\n\n  def __init__(self, num_actions: int):\n    super().__init__(name='policy_value_network')\n    self._policy_layer = snt.Linear(num_actions)\n    self._value_layer = snt.Linear(1)\n\n  def __call__(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n    \"\"\"Returns a (Logits, Value) tuple.\"\"\"\n    logits = self._policy_layer(inputs)  # [B, A]\n    value = tf.squeeze(self._value_layer(inputs), axis=-1)  # [B]\n\n    return logits, value",
  "def __init__(self, num_actions: int):\n    super().__init__(name='policy_value_network')\n    self._policy_layer = snt.Linear(num_actions)\n    self._value_layer = snt.Linear(1)",
  "def __call__(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n    \"\"\"Returns a (Logits, Value) tuple.\"\"\"\n    logits = self._policy_layer(inputs)  # [B, A]\n    value = tf.squeeze(self._value_layer(inputs), axis=-1)  # [B]\n\n    return logits, value",
  "class Multihead(snt.Module):\n  \"\"\"Multi-head network module.\n\n  This takes as input a list of N `network_heads`, and returns another network\n  whose output is the stacked outputs of each of these network heads separately\n  applied to the module input. The dimension of the output is [..., N].\n  \"\"\"\n\n  def __init__(self,\n               network_heads: Sequence[TensorTransformation]):\n    if not network_heads:\n      raise ValueError('Must specify non-empty, non-None critic_network_heads.')\n    self._network_heads = network_heads\n    super().__init__(name='multihead')\n\n  def __call__(self,\n               inputs: tf.Tensor) -> Union[tf.Tensor, Sequence[tf.Tensor]]:\n    outputs = [network_head(inputs) for network_head in self._network_heads]\n    if isinstance(outputs[0], tfd.Distribution):\n      # Cannot stack distributions\n      return outputs\n    outputs = tf.stack(outputs, axis=-1)\n    return outputs",
  "def __init__(self,\n               network_heads: Sequence[TensorTransformation]):\n    if not network_heads:\n      raise ValueError('Must specify non-empty, non-None critic_network_heads.')\n    self._network_heads = network_heads\n    super().__init__(name='multihead')",
  "def __call__(self,\n               inputs: tf.Tensor) -> Union[tf.Tensor, Sequence[tf.Tensor]]:\n    outputs = [network_head(inputs) for network_head in self._network_heads]\n    if isinstance(outputs[0], tfd.Distribution):\n      # Cannot stack distributions\n      return outputs\n    outputs = tf.stack(outputs, axis=-1)\n    return outputs",
  "class DiscreteValuedHead(snt.Module):\n  \"\"\"Represents a parameterized discrete valued distribution.\n\n  The returned distribution is essentially a `tfd.Categorical`, but one which\n  knows its support and so can compute the mean value.\n  \"\"\"\n\n  def __init__(self,\n               vmin: Union[float, np.ndarray, tf.Tensor],\n               vmax: Union[float, np.ndarray, tf.Tensor],\n               num_atoms: int,\n               w_init: Optional[snt.initializers.Initializer] = None,\n               b_init: Optional[snt.initializers.Initializer] = None):\n    \"\"\"Initialization.\n\n    If vmin and vmax have shape S, this will store the category values as a\n    Tensor of shape (S*, num_atoms).\n\n    Args:\n      vmin: Minimum of the value range\n      vmax: Maximum of the value range\n      num_atoms: The atom values associated with each bin.\n      w_init: Initialization for linear layer weights.\n      b_init: Initialization for linear layer biases.\n    \"\"\"\n    super().__init__(name='DiscreteValuedHead')\n    vmin = tf.convert_to_tensor(vmin)\n    vmax = tf.convert_to_tensor(vmax)\n    self._values = tf.linspace(vmin, vmax, num_atoms, axis=-1)\n    self._distributional_layer = snt.Linear(tf.size(self._values),\n                                            w_init=w_init,\n                                            b_init=b_init)\n\n  def __call__(self, inputs: tf.Tensor) -> tfd.Distribution:\n    logits = self._distributional_layer(inputs)\n    logits = tf.reshape(logits,\n                        tf.concat([tf.shape(logits)[:1],  # batch size\n                                   tf.shape(self._values)],\n                                  axis=0))\n    values = tf.cast(self._values, logits.dtype)\n\n    return ad.DiscreteValuedDistribution(values=values, logits=logits)",
  "class MultivariateNormalDiagHead(snt.Module):\n  \"\"\"Module that produces a multivariate normal distribution using tfd.Independent or tfd.MultivariateNormalDiag.\"\"\"\n\n  def __init__(\n      self,\n      num_dimensions: int,\n      init_scale: float = 0.3,\n      min_scale: float = 1e-6,\n      tanh_mean: bool = False,\n      fixed_scale: bool = False,\n      use_tfd_independent: bool = False,\n      w_init: snt_init.Initializer = tf.initializers.VarianceScaling(1e-4),\n      b_init: snt_init.Initializer = tf.initializers.Zeros()):\n    \"\"\"Initialization.\n\n    Args:\n      num_dimensions: Number of dimensions of MVN distribution.\n      init_scale: Initial standard deviation.\n      min_scale: Minimum standard deviation.\n      tanh_mean: Whether to transform the mean (via tanh) before passing it to\n        the distribution.\n      fixed_scale: Whether to use a fixed variance.\n      use_tfd_independent: Whether to use tfd.Independent or\n        tfd.MultivariateNormalDiag class\n      w_init: Initialization for linear layer weights.\n      b_init: Initialization for linear layer biases.\n    \"\"\"\n    super().__init__(name='MultivariateNormalDiagHead')\n    self._init_scale = init_scale\n    self._min_scale = min_scale\n    self._tanh_mean = tanh_mean\n    self._mean_layer = snt.Linear(num_dimensions, w_init=w_init, b_init=b_init)\n    self._fixed_scale = fixed_scale\n\n    if not fixed_scale:\n      self._scale_layer = snt.Linear(\n          num_dimensions, w_init=w_init, b_init=b_init)\n    self._use_tfd_independent = use_tfd_independent\n\n  def __call__(self, inputs: tf.Tensor) -> tfd.Distribution:\n    zero = tf.constant(0, dtype=inputs.dtype)\n    mean = self._mean_layer(inputs)\n\n    if self._fixed_scale:\n      scale = tf.ones_like(mean) * self._init_scale\n    else:\n      scale = tf.nn.softplus(self._scale_layer(inputs))\n      scale *= self._init_scale / tf.nn.softplus(zero)\n      scale += self._min_scale\n\n    # Maybe transform the mean.\n    if self._tanh_mean:\n      mean = tf.tanh(mean)\n\n    if self._use_tfd_independent:\n      dist = tfd.Independent(tfd.Normal(loc=mean, scale=scale))\n    else:\n      dist = tfd.MultivariateNormalDiag(loc=mean, scale_diag=scale)\n\n    return dist",
  "class GaussianMixture(snt.Module):\n  \"\"\"Module that outputs a Gaussian Mixture Distribution.\"\"\"\n\n  def __init__(self,\n               num_dimensions: int,\n               num_components: int,\n               multivariate: bool,\n               init_scale: Optional[float] = None,\n               name: str = 'GaussianMixture'):\n    \"\"\"Initialization.\n\n    Args:\n      num_dimensions: dimensionality of the output distribution\n      num_components: number of mixture components.\n      multivariate: whether the resulting distribution is multivariate or not.\n      init_scale: the initial scale for the Gaussian mixture components.\n      name: name of the module passed to snt.Module parent class.\n    \"\"\"\n    super().__init__(name=name)\n\n    self._num_dimensions = num_dimensions\n    self._num_components = num_components\n    self._multivariate = multivariate\n\n    if init_scale is not None:\n      self._scale_factor = init_scale / tf.nn.softplus(0.)\n    else:\n      self._scale_factor = 1.0  # Corresponds to init_scale = softplus(0).\n\n    # Define the weight initializer.\n    w_init = tf.initializers.VarianceScaling(1e-5)\n\n    # Create a layer that outputs the unnormalized log-weights.\n    if self._multivariate:\n      logits_size = self._num_components\n    else:\n      logits_size = self._num_dimensions * self._num_components\n    self._logit_layer = snt.Linear(logits_size, w_init=w_init)\n\n    # Create two layers that outputs a location and a scale, respectively, for\n    # each dimension and each component.\n    self._loc_layer = snt.Linear(\n        self._num_dimensions * self._num_components, w_init=w_init)\n    self._scale_layer = snt.Linear(\n        self._num_dimensions * self._num_components, w_init=w_init)\n\n  def __call__(self,\n               inputs: tf.Tensor,\n               low_noise_policy: bool = False) -> tfd.Distribution:\n    \"\"\"Run the networks through inputs.\n\n    Args:\n      inputs: hidden activations of the policy network body.\n      low_noise_policy: whether to set vanishingly small scales for each\n        component. If this flag is set to True, the policy is effectively run\n        without Gaussian noise.\n\n    Returns:\n      Mixture Gaussian distribution.\n    \"\"\"\n\n    # Compute logits, locs, and scales if necessary.\n    logits = self._logit_layer(inputs)\n    locs = self._loc_layer(inputs)\n\n    # When a low_noise_policy is requested, set the scales to its minimum value.\n    if low_noise_policy:\n      scales = tf.fill(locs.shape, _MIN_SCALE)\n    else:\n      scales = self._scale_layer(inputs)\n      scales = self._scale_factor * tf.nn.softplus(scales) + _MIN_SCALE\n\n    if self._multivariate:\n      shape = [-1, self._num_components, self._num_dimensions]\n      # Reshape the mixture's location and scale parameters appropriately.\n      locs = tf.reshape(locs, shape)\n      scales = tf.reshape(scales, shape)\n      # In this case, no need to reshape logits as they are in the correct shape\n      # already, namely [batch_size, num_components].\n      components_distribution = tfd.MultivariateNormalDiag(\n          loc=locs, scale_diag=scales)\n    else:\n      shape = [-1, self._num_dimensions, self._num_components]\n      # Reshape the mixture's location and scale parameters appropriately.\n      locs = tf.reshape(locs, shape)\n      scales = tf.reshape(scales, shape)\n      components_distribution = tfd.Normal(loc=locs, scale=scales)\n      logits = tf.reshape(logits, shape)\n\n    # Create the mixture distribution.\n    distribution = tfd.MixtureSameFamily(\n        mixture_distribution=tfd.Categorical(logits=logits),\n        components_distribution=components_distribution)\n\n    if not self._multivariate:\n      distribution = tfd.Independent(distribution)\n\n    return distribution",
  "class UnivariateGaussianMixture(GaussianMixture):\n  \"\"\"Head which outputs a Mixture of Gaussians Distribution.\"\"\"\n\n  def __init__(self,\n               num_dimensions: int,\n               num_components: int = 5,\n               init_scale: Optional[float] = None,\n               num_mixtures: Optional[int] = None):\n    \"\"\"Create an mixture of Gaussian actor head.\n\n    Args:\n      num_dimensions: dimensionality of the output distribution. Each dimension\n        is going to be an independent 1d GMM model.\n      num_components: number of mixture components.\n      init_scale: the initial scale for the Gaussian mixture components.\n      num_mixtures: deprecated argument which overwrites num_components.\n    \"\"\"\n    if num_mixtures is not None:\n      logging.warning(\"\"\"the num_mixtures parameter has been deprecated; use\n                    num_components instead; the value of num_components is being\n                    ignored\"\"\")\n      num_components = num_mixtures\n    super().__init__(num_dimensions=num_dimensions,\n                     num_components=num_components,\n                     multivariate=False,\n                     init_scale=init_scale,\n                     name='UnivariateGaussianMixture')",
  "class MultivariateGaussianMixture(GaussianMixture):\n  \"\"\"Head which outputs a mixture of multivariate Gaussians distribution.\"\"\"\n\n  def __init__(self,\n               num_dimensions: int,\n               num_components: int = 5,\n               init_scale: Optional[float] = None):\n    \"\"\"Initialization.\n\n    Args:\n      num_dimensions: dimensionality of the output distribution\n        (also the dimensionality of the multivariate Gaussian model).\n      num_components: number of mixture components.\n      init_scale: the initial scale for the Gaussian mixture components.\n    \"\"\"\n    super().__init__(num_dimensions=num_dimensions,\n                     num_components=num_components,\n                     multivariate=True,\n                     init_scale=init_scale,\n                     name='MultivariateGaussianMixture')",
  "class ApproximateMode(snt.Module):\n  \"\"\"Override the mode function of the distribution.\n\n  For non-constant Jacobian transformed distributions, the mode is non-trivial\n  to compute, so for these distributions the mode function is not supported in\n  TFP. A frequently used approximation is to forward transform the mode of the\n  untransformed distribution.\n\n  Otherwise (an untransformed distribution or a transformed distribution with a\n  constant Jacobian), this is a no-op.\n  \"\"\"\n\n  def __call__(self, inputs: tfd.Distribution) -> tfd.Distribution:\n    if isinstance(inputs, tfd.TransformedDistribution):\n      if not inputs.bijector.is_constant_jacobian:\n        def _mode(self, **kwargs):\n          distribution_kwargs, bijector_kwargs = self._kwargs_split_fn(kwargs)\n          x = self.distribution.mode(**distribution_kwargs)\n          y = self.bijector.forward(x, **bijector_kwargs)\n          return y\n        inputs._mode = types.MethodType(_mode, inputs)\n    return inputs",
  "def __init__(self,\n               vmin: Union[float, np.ndarray, tf.Tensor],\n               vmax: Union[float, np.ndarray, tf.Tensor],\n               num_atoms: int,\n               w_init: Optional[snt.initializers.Initializer] = None,\n               b_init: Optional[snt.initializers.Initializer] = None):\n    \"\"\"Initialization.\n\n    If vmin and vmax have shape S, this will store the category values as a\n    Tensor of shape (S*, num_atoms).\n\n    Args:\n      vmin: Minimum of the value range\n      vmax: Maximum of the value range\n      num_atoms: The atom values associated with each bin.\n      w_init: Initialization for linear layer weights.\n      b_init: Initialization for linear layer biases.\n    \"\"\"\n    super().__init__(name='DiscreteValuedHead')\n    vmin = tf.convert_to_tensor(vmin)\n    vmax = tf.convert_to_tensor(vmax)\n    self._values = tf.linspace(vmin, vmax, num_atoms, axis=-1)\n    self._distributional_layer = snt.Linear(tf.size(self._values),\n                                            w_init=w_init,\n                                            b_init=b_init)",
  "def __call__(self, inputs: tf.Tensor) -> tfd.Distribution:\n    logits = self._distributional_layer(inputs)\n    logits = tf.reshape(logits,\n                        tf.concat([tf.shape(logits)[:1],  # batch size\n                                   tf.shape(self._values)],\n                                  axis=0))\n    values = tf.cast(self._values, logits.dtype)\n\n    return ad.DiscreteValuedDistribution(values=values, logits=logits)",
  "def __init__(\n      self,\n      num_dimensions: int,\n      init_scale: float = 0.3,\n      min_scale: float = 1e-6,\n      tanh_mean: bool = False,\n      fixed_scale: bool = False,\n      use_tfd_independent: bool = False,\n      w_init: snt_init.Initializer = tf.initializers.VarianceScaling(1e-4),\n      b_init: snt_init.Initializer = tf.initializers.Zeros()):\n    \"\"\"Initialization.\n\n    Args:\n      num_dimensions: Number of dimensions of MVN distribution.\n      init_scale: Initial standard deviation.\n      min_scale: Minimum standard deviation.\n      tanh_mean: Whether to transform the mean (via tanh) before passing it to\n        the distribution.\n      fixed_scale: Whether to use a fixed variance.\n      use_tfd_independent: Whether to use tfd.Independent or\n        tfd.MultivariateNormalDiag class\n      w_init: Initialization for linear layer weights.\n      b_init: Initialization for linear layer biases.\n    \"\"\"\n    super().__init__(name='MultivariateNormalDiagHead')\n    self._init_scale = init_scale\n    self._min_scale = min_scale\n    self._tanh_mean = tanh_mean\n    self._mean_layer = snt.Linear(num_dimensions, w_init=w_init, b_init=b_init)\n    self._fixed_scale = fixed_scale\n\n    if not fixed_scale:\n      self._scale_layer = snt.Linear(\n          num_dimensions, w_init=w_init, b_init=b_init)\n    self._use_tfd_independent = use_tfd_independent",
  "def __call__(self, inputs: tf.Tensor) -> tfd.Distribution:\n    zero = tf.constant(0, dtype=inputs.dtype)\n    mean = self._mean_layer(inputs)\n\n    if self._fixed_scale:\n      scale = tf.ones_like(mean) * self._init_scale\n    else:\n      scale = tf.nn.softplus(self._scale_layer(inputs))\n      scale *= self._init_scale / tf.nn.softplus(zero)\n      scale += self._min_scale\n\n    # Maybe transform the mean.\n    if self._tanh_mean:\n      mean = tf.tanh(mean)\n\n    if self._use_tfd_independent:\n      dist = tfd.Independent(tfd.Normal(loc=mean, scale=scale))\n    else:\n      dist = tfd.MultivariateNormalDiag(loc=mean, scale_diag=scale)\n\n    return dist",
  "def __init__(self,\n               num_dimensions: int,\n               num_components: int,\n               multivariate: bool,\n               init_scale: Optional[float] = None,\n               name: str = 'GaussianMixture'):\n    \"\"\"Initialization.\n\n    Args:\n      num_dimensions: dimensionality of the output distribution\n      num_components: number of mixture components.\n      multivariate: whether the resulting distribution is multivariate or not.\n      init_scale: the initial scale for the Gaussian mixture components.\n      name: name of the module passed to snt.Module parent class.\n    \"\"\"\n    super().__init__(name=name)\n\n    self._num_dimensions = num_dimensions\n    self._num_components = num_components\n    self._multivariate = multivariate\n\n    if init_scale is not None:\n      self._scale_factor = init_scale / tf.nn.softplus(0.)\n    else:\n      self._scale_factor = 1.0  # Corresponds to init_scale = softplus(0).\n\n    # Define the weight initializer.\n    w_init = tf.initializers.VarianceScaling(1e-5)\n\n    # Create a layer that outputs the unnormalized log-weights.\n    if self._multivariate:\n      logits_size = self._num_components\n    else:\n      logits_size = self._num_dimensions * self._num_components\n    self._logit_layer = snt.Linear(logits_size, w_init=w_init)\n\n    # Create two layers that outputs a location and a scale, respectively, for\n    # each dimension and each component.\n    self._loc_layer = snt.Linear(\n        self._num_dimensions * self._num_components, w_init=w_init)\n    self._scale_layer = snt.Linear(\n        self._num_dimensions * self._num_components, w_init=w_init)",
  "def __call__(self,\n               inputs: tf.Tensor,\n               low_noise_policy: bool = False) -> tfd.Distribution:\n    \"\"\"Run the networks through inputs.\n\n    Args:\n      inputs: hidden activations of the policy network body.\n      low_noise_policy: whether to set vanishingly small scales for each\n        component. If this flag is set to True, the policy is effectively run\n        without Gaussian noise.\n\n    Returns:\n      Mixture Gaussian distribution.\n    \"\"\"\n\n    # Compute logits, locs, and scales if necessary.\n    logits = self._logit_layer(inputs)\n    locs = self._loc_layer(inputs)\n\n    # When a low_noise_policy is requested, set the scales to its minimum value.\n    if low_noise_policy:\n      scales = tf.fill(locs.shape, _MIN_SCALE)\n    else:\n      scales = self._scale_layer(inputs)\n      scales = self._scale_factor * tf.nn.softplus(scales) + _MIN_SCALE\n\n    if self._multivariate:\n      shape = [-1, self._num_components, self._num_dimensions]\n      # Reshape the mixture's location and scale parameters appropriately.\n      locs = tf.reshape(locs, shape)\n      scales = tf.reshape(scales, shape)\n      # In this case, no need to reshape logits as they are in the correct shape\n      # already, namely [batch_size, num_components].\n      components_distribution = tfd.MultivariateNormalDiag(\n          loc=locs, scale_diag=scales)\n    else:\n      shape = [-1, self._num_dimensions, self._num_components]\n      # Reshape the mixture's location and scale parameters appropriately.\n      locs = tf.reshape(locs, shape)\n      scales = tf.reshape(scales, shape)\n      components_distribution = tfd.Normal(loc=locs, scale=scales)\n      logits = tf.reshape(logits, shape)\n\n    # Create the mixture distribution.\n    distribution = tfd.MixtureSameFamily(\n        mixture_distribution=tfd.Categorical(logits=logits),\n        components_distribution=components_distribution)\n\n    if not self._multivariate:\n      distribution = tfd.Independent(distribution)\n\n    return distribution",
  "def __init__(self,\n               num_dimensions: int,\n               num_components: int = 5,\n               init_scale: Optional[float] = None,\n               num_mixtures: Optional[int] = None):\n    \"\"\"Create an mixture of Gaussian actor head.\n\n    Args:\n      num_dimensions: dimensionality of the output distribution. Each dimension\n        is going to be an independent 1d GMM model.\n      num_components: number of mixture components.\n      init_scale: the initial scale for the Gaussian mixture components.\n      num_mixtures: deprecated argument which overwrites num_components.\n    \"\"\"\n    if num_mixtures is not None:\n      logging.warning(\"\"\"the num_mixtures parameter has been deprecated; use\n                    num_components instead; the value of num_components is being\n                    ignored\"\"\")\n      num_components = num_mixtures\n    super().__init__(num_dimensions=num_dimensions,\n                     num_components=num_components,\n                     multivariate=False,\n                     init_scale=init_scale,\n                     name='UnivariateGaussianMixture')",
  "def __init__(self,\n               num_dimensions: int,\n               num_components: int = 5,\n               init_scale: Optional[float] = None):\n    \"\"\"Initialization.\n\n    Args:\n      num_dimensions: dimensionality of the output distribution\n        (also the dimensionality of the multivariate Gaussian model).\n      num_components: number of mixture components.\n      init_scale: the initial scale for the Gaussian mixture components.\n    \"\"\"\n    super().__init__(num_dimensions=num_dimensions,\n                     num_components=num_components,\n                     multivariate=True,\n                     init_scale=init_scale,\n                     name='MultivariateGaussianMixture')",
  "def __call__(self, inputs: tfd.Distribution) -> tfd.Distribution:\n    if isinstance(inputs, tfd.TransformedDistribution):\n      if not inputs.bijector.is_constant_jacobian:\n        def _mode(self, **kwargs):\n          distribution_kwargs, bijector_kwargs = self._kwargs_split_fn(kwargs)\n          x = self.distribution.mode(**distribution_kwargs)\n          y = self.bijector.forward(x, **bijector_kwargs)\n          return y\n        inputs._mode = types.MethodType(_mode, inputs)\n    return inputs",
  "def _mode(self, **kwargs):\n          distribution_kwargs, bijector_kwargs = self._kwargs_split_fn(kwargs)\n          x = self.distribution.mode(**distribution_kwargs)\n          y = self.bijector.forward(x, **bijector_kwargs)\n          return y",
  "class StochasticModeHead(snt.Module):\n  \"\"\"Simple sonnet module to produce the mode of a tfp.Distribution.\"\"\"\n\n  def __call__(self, distribution: tfd.Distribution):\n    return distribution.mode()",
  "class StochasticMeanHead(snt.Module):\n  \"\"\"Simple sonnet module to produce the mean of a tfp.Distribution.\"\"\"\n\n  def __call__(self, distribution: tfd.Distribution):\n    return distribution.mean()",
  "class StochasticSamplingHead(snt.Module):\n  \"\"\"Simple sonnet module to sample from a tfp.Distribution.\"\"\"\n\n  def __call__(self, distribution: tfd.Distribution):\n    return distribution.sample()",
  "class ExpQWeightedPolicy(snt.Module):\n  \"\"\"Exponentially Q-weighted policy.\n\n  Given a stochastic policy and a critic, returns a (stochastic) policy which\n  samples multiple actions from the underlying policy, computes the Q-values for\n  each action, and chooses the final action among the sampled ones with\n  probability proportional to the exponentiated Q values, tempered by\n  a parameter beta.\n  \"\"\"\n\n  def __init__(self,\n               actor_network: snt.Module,\n               critic_network: snt.Module,\n               beta: float = 1.0,\n               num_action_samples: int = 16):\n    super().__init__(name='ExpQWeightedPolicy')\n    self._actor_network = actor_network\n    self._critic_network = critic_network\n    self._num_action_samples = num_action_samples\n    self._beta = beta\n\n  def __call__(self, inputs: types.NestedTensor) -> tf.Tensor:\n    # Inputs are of size [B, ...]. Here we tile them to be of shape [N, B, ...].\n    tiled_inputs = tf2_utils.tile_nested(inputs, self._num_action_samples)\n    shape = tf.shape(tree.flatten(tiled_inputs)[0])\n    n, b = shape[0], shape[1]\n    tf.debugging.assert_equal(n, self._num_action_samples,\n                              'Internal Error. Unexpected tiled_inputs shape.')\n    dummy_zeros_n_b = tf.zeros((n, b))\n    # Reshape to [N * B, ...].\n    merge = lambda x: snt.merge_leading_dims(x, 2)\n    tiled_inputs = tree.map_structure(merge, tiled_inputs)\n\n    tiled_actions = self._actor_network(tiled_inputs)\n\n    # Compute Q-values and the resulting tempered probabilities.\n    q = self._critic_network(tiled_inputs, tiled_actions)\n    boltzmann_logits = q / self._beta\n\n    boltzmann_logits = snt.split_leading_dim(boltzmann_logits, dummy_zeros_n_b,\n                                             2)\n    # [B, N]\n    boltzmann_logits = tf.transpose(boltzmann_logits, perm=(1, 0))\n    # Resample one action per batch according to the Boltzmann distribution.\n    action_idx = tfp.distributions.Categorical(logits=boltzmann_logits).sample()\n    # [B, 2], where the first column is 0, 1, 2,... corresponding to indices to\n    # the batch dimension.\n    action_idx = tf.stack((tf.range(b), action_idx), axis=1)\n\n    tiled_actions = snt.split_leading_dim(tiled_actions, dummy_zeros_n_b, 2)\n    action_dim = len(tiled_actions.get_shape().as_list())\n    tiled_actions = tf.transpose(tiled_actions,\n                                 perm=[1, 0] + list(range(2, action_dim)))\n    # [B, ...]\n    action_sample = tf.gather_nd(tiled_actions, action_idx)\n\n    return action_sample",
  "def __call__(self, distribution: tfd.Distribution):\n    return distribution.mode()",
  "def __call__(self, distribution: tfd.Distribution):\n    return distribution.mean()",
  "def __call__(self, distribution: tfd.Distribution):\n    return distribution.sample()",
  "def __init__(self,\n               actor_network: snt.Module,\n               critic_network: snt.Module,\n               beta: float = 1.0,\n               num_action_samples: int = 16):\n    super().__init__(name='ExpQWeightedPolicy')\n    self._actor_network = actor_network\n    self._critic_network = critic_network\n    self._num_action_samples = num_action_samples\n    self._beta = beta",
  "def __call__(self, inputs: types.NestedTensor) -> tf.Tensor:\n    # Inputs are of size [B, ...]. Here we tile them to be of shape [N, B, ...].\n    tiled_inputs = tf2_utils.tile_nested(inputs, self._num_action_samples)\n    shape = tf.shape(tree.flatten(tiled_inputs)[0])\n    n, b = shape[0], shape[1]\n    tf.debugging.assert_equal(n, self._num_action_samples,\n                              'Internal Error. Unexpected tiled_inputs shape.')\n    dummy_zeros_n_b = tf.zeros((n, b))\n    # Reshape to [N * B, ...].\n    merge = lambda x: snt.merge_leading_dims(x, 2)\n    tiled_inputs = tree.map_structure(merge, tiled_inputs)\n\n    tiled_actions = self._actor_network(tiled_inputs)\n\n    # Compute Q-values and the resulting tempered probabilities.\n    q = self._critic_network(tiled_inputs, tiled_actions)\n    boltzmann_logits = q / self._beta\n\n    boltzmann_logits = snt.split_leading_dim(boltzmann_logits, dummy_zeros_n_b,\n                                             2)\n    # [B, N]\n    boltzmann_logits = tf.transpose(boltzmann_logits, perm=(1, 0))\n    # Resample one action per batch according to the Boltzmann distribution.\n    action_idx = tfp.distributions.Categorical(logits=boltzmann_logits).sample()\n    # [B, 2], where the first column is 0, 1, 2,... corresponding to indices to\n    # the batch dimension.\n    action_idx = tf.stack((tf.range(b), action_idx), axis=1)\n\n    tiled_actions = snt.split_leading_dim(tiled_actions, dummy_zeros_n_b, 2)\n    action_dim = len(tiled_actions.get_shape().as_list())\n    tiled_actions = tf.transpose(tiled_actions,\n                                 perm=[1, 0] + list(range(2, action_dim)))\n    # [B, ...]\n    action_sample = tf.gather_nd(tiled_actions, action_idx)\n\n    return action_sample",
  "class DiscreteValuedDistributionTest(parameterized.TestCase):\n\n  @parameterized.parameters(\n      ((), (), 5),\n      ((2,), (), 5),\n      ((), (3, 4), 5),\n      ((2,), (3, 4), 5),\n      ((2, 6), (3, 4), 5),\n      )\n  def test_constructor(self, batch_shape, event_shape, num_values):\n    logits_shape = batch_shape + event_shape + (num_values,)\n    logits_size = np.prod(logits_shape)\n    logits = np.arange(logits_size, dtype=float).reshape(logits_shape)\n    values = np.linspace(start=-np.ones(event_shape, dtype=float),\n                         stop=np.ones(event_shape, dtype=float),\n                         num=num_values,\n                         axis=-1)\n    distribution = distributions.DiscreteValuedDistribution(values=values,\n                                                            logits=logits)\n\n    # Check batch and event shapes.\n    self.assertEqual(distribution.batch_shape, batch_shape)\n    self.assertEqual(distribution.event_shape, event_shape)\n    self.assertEqual(distribution.logits_parameter().shape.as_list(),\n                     list(logits.shape))\n    self.assertEqual(distribution.logits_parameter().shape.as_list()[-1],\n                     logits.shape[-1])\n\n    # Test slicing\n    if len(batch_shape) == 1:\n      slice_0_logits = distribution[1:3].logits_parameter().numpy()\n      expected_slice_0_logits = distribution.logits_parameter().numpy()[1:3]\n      npt.assert_allclose(slice_0_logits, expected_slice_0_logits)\n    elif len(batch_shape) == 2:\n      slice_logits = distribution[0, 1:3].logits_parameter().numpy()\n      expected_slice_logits = distribution.logits_parameter().numpy()[0, 1:3]\n      npt.assert_allclose(slice_logits, expected_slice_logits)\n    else:\n      assert not batch_shape",
  "def test_constructor(self, batch_shape, event_shape, num_values):\n    logits_shape = batch_shape + event_shape + (num_values,)\n    logits_size = np.prod(logits_shape)\n    logits = np.arange(logits_size, dtype=float).reshape(logits_shape)\n    values = np.linspace(start=-np.ones(event_shape, dtype=float),\n                         stop=np.ones(event_shape, dtype=float),\n                         num=num_values,\n                         axis=-1)\n    distribution = distributions.DiscreteValuedDistribution(values=values,\n                                                            logits=logits)\n\n    # Check batch and event shapes.\n    self.assertEqual(distribution.batch_shape, batch_shape)\n    self.assertEqual(distribution.event_shape, event_shape)\n    self.assertEqual(distribution.logits_parameter().shape.as_list(),\n                     list(logits.shape))\n    self.assertEqual(distribution.logits_parameter().shape.as_list()[-1],\n                     logits.shape[-1])\n\n    # Test slicing\n    if len(batch_shape) == 1:\n      slice_0_logits = distribution[1:3].logits_parameter().numpy()\n      expected_slice_0_logits = distribution.logits_parameter().numpy()[1:3]\n      npt.assert_allclose(slice_0_logits, expected_slice_0_logits)\n    elif len(batch_shape) == 2:\n      slice_logits = distribution[0, 1:3].logits_parameter().numpy()\n      expected_slice_logits = distribution.logits_parameter().numpy()[0, 1:3]\n      npt.assert_allclose(slice_logits, expected_slice_logits)\n    else:\n      assert not batch_shape",
  "class NetworkWithMaskedEpsilonGreedy(snt.Module):\n  \"\"\"Epsilon greedy sampling with action masking on network outputs.\"\"\"\n\n  def __init__(self,\n               network: snt.Module,\n               epsilon: Optional[tf.Tensor] = None):\n    \"\"\"Initialize the network and epsilon.\n\n    Usage:\n      Wrap an observation in a dictionary in your environment as follows:\n\n        observation <-- {\"your_key_for_observation\": observation,\n                         \"legal_actions_mask\": your_action_mask_tensor}\n\n    and update your network to use 'observation[\"your_key_for_observation\"]'\n    rather than 'observation'.\n\n    Args:\n      network: the online Q network (the one being optimized)\n      epsilon: probability of taking a random action.\n    \"\"\"\n    super().__init__()\n    self._network = network\n    self._epsilon = epsilon\n\n  def __call__(\n      self, observation: Union[Mapping[str, tf.Tensor],\n                               tf.Tensor]) -> tf.Tensor:\n    q = self._network(observation)\n    return trfl.epsilon_greedy(\n        q, epsilon=self._epsilon,\n        legal_actions_mask=observation['legal_actions_mask']).sample()",
  "def __init__(self,\n               network: snt.Module,\n               epsilon: Optional[tf.Tensor] = None):\n    \"\"\"Initialize the network and epsilon.\n\n    Usage:\n      Wrap an observation in a dictionary in your environment as follows:\n\n        observation <-- {\"your_key_for_observation\": observation,\n                         \"legal_actions_mask\": your_action_mask_tensor}\n\n    and update your network to use 'observation[\"your_key_for_observation\"]'\n    rather than 'observation'.\n\n    Args:\n      network: the online Q network (the one being optimized)\n      epsilon: probability of taking a random action.\n    \"\"\"\n    super().__init__()\n    self._network = network\n    self._epsilon = epsilon",
  "def __call__(\n      self, observation: Union[Mapping[str, tf.Tensor],\n                               tf.Tensor]) -> tf.Tensor:\n    q = self._network(observation)\n    return trfl.epsilon_greedy(\n        q, epsilon=self._epsilon,\n        legal_actions_mask=observation['legal_actions_mask']).sample()",
  "class PolicyCriticRNNState(NamedTuple):\n  \"\"\"Consists of two RNNStates called 'policy' and 'critic'.\"\"\"\n  policy: RNNState\n  critic: RNNState",
  "class UnpackWrapper(snt.Module):\n  \"\"\"Gets a list of arguments and pass them as separate arguments.\n\n  Example\n  ```\n    class Critic(snt.Module):\n      def __call__(self, o, a):\n        pass\n\n    critic = Critic()\n    UnpackWrapper(critic)((o, a))\n  ```\n  calls critic(o, a)\n  \"\"\"\n\n  def __init__(self, module: snt.Module, name: str = 'UnpackWrapper'):\n    super().__init__(name=name)\n    self._module = module\n\n  def __call__(self,\n               inputs: Sequence[types.NestedTensor]) -> types.NestedTensor:\n    # Unpack the inputs before passing to the underlying module.\n    return self._module(*inputs)",
  "class RNNUnpackWrapper(snt.RNNCore):\n  \"\"\"Gets a list of arguments and pass them as separate arguments.\n\n  Example\n  ```\n    class Critic(snt.RNNCore):\n      def __call__(self, o, a, prev_state):\n        pass\n\n    critic = Critic()\n    RNNUnpackWrapper(critic)((o, a), prev_state)\n  ```\n  calls m(o, a, prev_state)\n  \"\"\"\n\n  def __init__(self, module: snt.RNNCore, name: str = 'RNNUnpackWrapper'):\n    super().__init__(name=name)\n    self._module = module\n\n  def __call__(self, inputs: Sequence[types.NestedTensor],\n               prev_state: RNNState) -> Tuple[types.NestedTensor, RNNState]:\n    # Unpack the inputs before passing to the underlying module.\n    return self._module(*inputs, prev_state)\n\n  def initial_state(self, batch_size):\n    return self._module.initial_state(batch_size)",
  "class CriticDeepRNN(snt.DeepRNN):\n  \"\"\"Same as snt.DeepRNN, but takes three inputs (obs, act, prev_state).\n  \"\"\"\n\n  def __init__(self, layers: Sequence[snt.Module]):\n    # Make the first layer take a single input instead of a list of arguments.\n    if isinstance(layers[0], snt.RNNCore):\n      first_layer = RNNUnpackWrapper(layers[0])\n    else:\n      first_layer = UnpackWrapper(layers[0])\n    super().__init__([first_layer] + list(layers[1:]))\n\n    self._unwrapped_first_layer = layers[0]\n    self.__input_signature = None\n\n  def __call__(self, inputs: types.NestedTensor, action: tf.Tensor,\n               prev_state: RNNState) -> Tuple[types.NestedTensor, RNNState]:\n    # Pack the inputs into a tuple and then using inherited DeepRNN logic to\n    # pass them through the layers.\n    # This in turn will pass the packed inputs into the first layer\n    # (UnpackWrapper) which will unpack them back.\n    return super().__call__((inputs, action), prev_state)\n\n  @property\n  def _input_signature(self) -> Optional[tf.TensorSpec]:\n    \"\"\"Return input signature for Acme snapshotting.\n\n    The Acme way of snapshotting works as follows: you first create your network\n    variables via the utility function `acme.tf.utils.create_variables()`, which\n    adds an `_input_signature` attribute to your module. This attribute is\n    critical for proper snapshot saving and loading.\n\n    If a module with such an attribute is wrapped into e.g. DeepRNN, Acme\n    descends into the `_layers[0]` of that DeepRNN to find the input\n    signature.\n\n    This implementation allows CriticDeepRNN to work seamlessly like DeepRNN for\n    the following two use-cases:\n\n        1) Creating variables *before* wrapping:\n          ```\n          unwrapped_critic = Critic()\n          acme.tf.utils.create_variables(unwrapped_critic, specs)\n          critic = CriticDeepRNN([unwrapped_critic])\n          ```\n\n        2) Create variables *after* wrapping:\n          ```\n          unwrapped_critic = Critic()\n          critic = CriticDeepRNN([unwrapped_critic])\n          acme.tf.utils.create_variables(critic, specs)\n          ```\n\n    Returns:\n      input_signature of the module or None of it is not known (i.e. the\n      variables were not created by acme.tf.utils.create_variables nor for this\n      module nor for any of its descendants).\n    \"\"\"\n\n    if self.__input_signature is not None:\n      # To make case (2) (see above) work, we need to allow create_variables to\n      # assign an _input_signature attribute to this module, which is why we\n      # create additional __input_signature attribute with a setter (see below).\n      return self.__input_signature\n\n    # To make case (1) work, we descend into self._unwrapped_first_layer\n    # and try to get its input signature (if it exists) by calling\n    # savers.get_input_signature.\n\n    # Ideally, savers.get_input_signature should automatically descend into\n    # DeepRNN. But in this case it breaks on CriticDeepRNN because\n    # CriticDeepRNN._layers[0] is an UnpackWrapper around the underlying module\n    # and not the module itself.\n    input_signature = savers._get_input_signature(self._unwrapped_first_layer)  # pylint: disable=protected-access\n    if input_signature is None:\n      return None\n    # Since adding recurrent modules via CriticDeepRNN changes the recurrent\n    # state, we need to update its spec here.\n    state = self.initial_state(1)\n    input_signature[-1] = tree.map_structure(\n        lambda t: tf.TensorSpec((None,) + t.shape[1:], t.dtype), state)\n    self.__input_signature = input_signature\n    return input_signature\n\n  @_input_signature.setter\n  def _input_signature(self, new_spec: tf.TensorSpec):\n    self.__input_signature = new_spec",
  "class RecurrentExpQWeightedPolicy(snt.RNNCore):\n  \"\"\"Recurrent exponentially Q-weighted policy.\"\"\"\n\n  def __init__(self,\n               policy_network: snt.Module,\n               critic_network: snt.Module,\n               temperature_beta: float = 1.0,\n               num_action_samples: int = 16):\n    super().__init__(name='RecurrentExpQWeightedPolicy')\n    self._policy_network = policy_network\n    self._critic_network = critic_network\n    self._num_action_samples = num_action_samples\n    self._temperature_beta = temperature_beta\n\n  def __call__(self,\n               observation: types.NestedTensor,\n               prev_state: PolicyCriticRNNState\n               ) -> Tuple[types.NestedTensor, PolicyCriticRNNState]:\n\n    return tf.vectorized_map(self._call, (observation, prev_state))\n\n  def _call(\n      self, observation_and_state: Tuple[types.NestedTensor,\n                                         PolicyCriticRNNState]\n  ) -> Tuple[types.NestedTensor, PolicyCriticRNNState]:\n    \"\"\"Computes a forward step for a single element.\n\n    The observation and state are packed together in order to use\n    `tf.vectorized_map` to handle batches of observations.\n    See this module's __call__() function.\n\n    Args:\n      observation_and_state: the observation and state packed in a tuple.\n\n    Returns:\n      The selected action and the corresponding state.\n    \"\"\"\n    observation, prev_state = observation_and_state\n\n    # Tile input observations and states to allow multiple policy predictions.\n    tiled_observation, tiled_prev_state = utils.tile_nested(\n        (observation, prev_state), self._num_action_samples)\n    actions, policy_states = self._policy_network(\n        tiled_observation, tiled_prev_state.policy)\n\n    # Evaluate multiple critic predictions with the sampled actions.\n    value_distribution, critic_states = self._critic_network(\n        tiled_observation, actions, tiled_prev_state.critic)\n    value_estimate = value_distribution.mean()\n\n    # Resample a single action of the sampled actions according to logits given\n    # by the tempered Q-values.\n    selected_action_idx = tfp.distributions.Categorical(\n        probs=tf.nn.softmax(value_estimate / self._temperature_beta)).sample()\n    selected_action = actions[selected_action_idx]\n\n    # Select and return the RNN state that corresponds to the selected action.\n    states = PolicyCriticRNNState(\n        policy=policy_states, critic=critic_states)\n    selected_state = tree.map_structure(\n        lambda x: x[selected_action_idx], states)\n\n    return selected_action, selected_state\n\n  def initial_state(self, batch_size: int) -> PolicyCriticRNNState:\n    return PolicyCriticRNNState(\n        policy=self._policy_network.initial_state(batch_size),\n        critic=self._critic_network.initial_state(batch_size)\n        )",
  "class DeepRNN(snt.DeepRNN, base.RNNCore):\n  \"\"\"Unroll-aware deep RNN module.\n\n  Sonnet's DeepRNN steps through RNNCores sequentially which can result in a\n  performance hit, in particular when using Transformers. This module adds an\n  unroll() method which unrolls each module in the DeepRNN individually,\n  allowing efficient implementation of the unroll operation. For example, a\n  Transformer can 'unroll' by evaluating the whole sequence at once (this being\n  one of the advantages of Transformers over e.g. LSTMs).\n\n  Any RNNCore passed to this module should implement unroll(). Failure to so\n  may cause the RNNCore not to be called properly. For example, passing a\n  partial function application of a snt.RNNCore to this module will fail (this\n  is also true for snt.DeepRNN). However, the special case of passing in a\n  RNNCore object that does not implement unroll() is supported and will be\n  dynamically unrolled. Implement unroll() to override this behavior with\n  static unrolling.\n\n  Stateless modules (i.e. anything other than an RNNCore) which do not\n  implement unroll() are batch applied over the time and batch axes\n  simultaneously. Effectively, this means that such modules may be applied to\n  fairly large batches, potentially leading to out-of-memory issues.\n  \"\"\"\n\n  def __init__(self, layers, name: Optional[str] = None):\n    \"\"\"Initializes the module.\"\"\"\n    super().__init__(layers, name=name)\n\n    self.__input_signature = None\n    self._num_unrollable = 0\n\n    # As a convenience, check for snt.RNNCore modules and dynamically unroll\n    # them if they don't already support unrolling. This check can fail, e.g.\n    # if a partially applied RNNCore is passed in. Sonnet's implementation of\n    # DeepRNN suffers from the same problem.\n    for layer in self._layers:\n      if hasattr(layer, 'unroll'):\n        self._num_unrollable += 1\n      elif isinstance(layer, snt.RNNCore):\n        self._num_unrollable += 1\n        layer.unroll = functools.partial(snt.dynamic_unroll, layer)\n        logging.warning(\n            'Acme DeepRNN detected a Sonnet RNNCore. '\n            'This will be dynamically unrolled. Please implement unroll() '\n            'to suppress this warning.')\n\n  def unroll(self,\n             inputs: types.NestedTensor,\n             state: base.State,\n             sequence_length: int,\n             ) -> Tuple[types.NestedTensor, base.State]:\n    \"\"\"Unroll each layer individually.\n\n    Calls unroll() on layers which support it, all other layers are\n    batch-applied over the first two axes (assumed to be the time and batch\n    axes).\n\n    Args:\n      inputs: A nest of `tf.Tensor` in time-major format.\n      state: The RNN core state.\n      sequence_length: How long the static_unroll should go for.\n\n    Returns:\n      Nested sequence output of RNN, and final state.\n\n    Raises:\n      ValueError if the length of `state` does not match the number of\n      unrollable layers.\n    \"\"\"\n    if len(state) != self._num_unrollable:\n      raise ValueError(\n          'DeepRNN was called with the wrong number of states. The length of '\n          '`state` does not match the number of unrollable layers.')\n\n    states = iter(state)\n    outputs = inputs\n    next_states = []\n    for layer in self._layers:\n      if hasattr(layer, 'unroll'):\n        # The length of the `states` list was checked above.\n        outputs, next_state = layer.unroll(outputs, next(states),\n                                           sequence_length)\n        next_states.append(next_state)\n      else:\n        # Couldn't unroll(); assume that this is a stateless module.\n        outputs = snt.BatchApply(layer, num_dims=2)(outputs)\n\n    return outputs, tuple(next_states)\n\n  @property\n  def _input_signature(self) -> Optional[tf.TensorSpec]:\n    \"\"\"Return input signature for Acme snapshotting, see CriticDeepRNN.\"\"\"\n\n    if self.__input_signature is not None:\n      return self.__input_signature\n\n    input_signature = savers._get_input_signature(self._layers[0])  # pylint: disable=protected-access\n    if input_signature is None:\n      return None\n\n    state = self.initial_state(1)\n    input_signature[-1] = tree.map_structure(\n        lambda t: tf.TensorSpec((None,) + t.shape[1:], t.dtype), state)\n    self.__input_signature = input_signature\n    return input_signature\n\n  @_input_signature.setter\n  def _input_signature(self, new_spec: tf.TensorSpec):\n    self.__input_signature = new_spec",
  "class LSTM(snt.LSTM, base.RNNCore):\n  \"\"\"Unrollable interface to LSTM.\n\n  This module is supposed to be used with the DeepRNN class above, and more\n  generally in networks which support unroll().\n  \"\"\"\n\n  def unroll(self,\n             inputs: types.NestedTensor,\n             state: base.State,\n             sequence_length: int,\n             ) -> Tuple[types.NestedTensor, base.State]:\n    return snt.static_unroll(self, inputs, state, sequence_length)",
  "def __init__(self, module: snt.Module, name: str = 'UnpackWrapper'):\n    super().__init__(name=name)\n    self._module = module",
  "def __call__(self,\n               inputs: Sequence[types.NestedTensor]) -> types.NestedTensor:\n    # Unpack the inputs before passing to the underlying module.\n    return self._module(*inputs)",
  "def __init__(self, module: snt.RNNCore, name: str = 'RNNUnpackWrapper'):\n    super().__init__(name=name)\n    self._module = module",
  "def __call__(self, inputs: Sequence[types.NestedTensor],\n               prev_state: RNNState) -> Tuple[types.NestedTensor, RNNState]:\n    # Unpack the inputs before passing to the underlying module.\n    return self._module(*inputs, prev_state)",
  "def initial_state(self, batch_size):\n    return self._module.initial_state(batch_size)",
  "def __init__(self, layers: Sequence[snt.Module]):\n    # Make the first layer take a single input instead of a list of arguments.\n    if isinstance(layers[0], snt.RNNCore):\n      first_layer = RNNUnpackWrapper(layers[0])\n    else:\n      first_layer = UnpackWrapper(layers[0])\n    super().__init__([first_layer] + list(layers[1:]))\n\n    self._unwrapped_first_layer = layers[0]\n    self.__input_signature = None",
  "def __call__(self, inputs: types.NestedTensor, action: tf.Tensor,\n               prev_state: RNNState) -> Tuple[types.NestedTensor, RNNState]:\n    # Pack the inputs into a tuple and then using inherited DeepRNN logic to\n    # pass them through the layers.\n    # This in turn will pass the packed inputs into the first layer\n    # (UnpackWrapper) which will unpack them back.\n    return super().__call__((inputs, action), prev_state)",
  "def _input_signature(self) -> Optional[tf.TensorSpec]:\n    \"\"\"Return input signature for Acme snapshotting.\n\n    The Acme way of snapshotting works as follows: you first create your network\n    variables via the utility function `acme.tf.utils.create_variables()`, which\n    adds an `_input_signature` attribute to your module. This attribute is\n    critical for proper snapshot saving and loading.\n\n    If a module with such an attribute is wrapped into e.g. DeepRNN, Acme\n    descends into the `_layers[0]` of that DeepRNN to find the input\n    signature.\n\n    This implementation allows CriticDeepRNN to work seamlessly like DeepRNN for\n    the following two use-cases:\n\n        1) Creating variables *before* wrapping:\n          ```\n          unwrapped_critic = Critic()\n          acme.tf.utils.create_variables(unwrapped_critic, specs)\n          critic = CriticDeepRNN([unwrapped_critic])\n          ```\n\n        2) Create variables *after* wrapping:\n          ```\n          unwrapped_critic = Critic()\n          critic = CriticDeepRNN([unwrapped_critic])\n          acme.tf.utils.create_variables(critic, specs)\n          ```\n\n    Returns:\n      input_signature of the module or None of it is not known (i.e. the\n      variables were not created by acme.tf.utils.create_variables nor for this\n      module nor for any of its descendants).\n    \"\"\"\n\n    if self.__input_signature is not None:\n      # To make case (2) (see above) work, we need to allow create_variables to\n      # assign an _input_signature attribute to this module, which is why we\n      # create additional __input_signature attribute with a setter (see below).\n      return self.__input_signature\n\n    # To make case (1) work, we descend into self._unwrapped_first_layer\n    # and try to get its input signature (if it exists) by calling\n    # savers.get_input_signature.\n\n    # Ideally, savers.get_input_signature should automatically descend into\n    # DeepRNN. But in this case it breaks on CriticDeepRNN because\n    # CriticDeepRNN._layers[0] is an UnpackWrapper around the underlying module\n    # and not the module itself.\n    input_signature = savers._get_input_signature(self._unwrapped_first_layer)  # pylint: disable=protected-access\n    if input_signature is None:\n      return None\n    # Since adding recurrent modules via CriticDeepRNN changes the recurrent\n    # state, we need to update its spec here.\n    state = self.initial_state(1)\n    input_signature[-1] = tree.map_structure(\n        lambda t: tf.TensorSpec((None,) + t.shape[1:], t.dtype), state)\n    self.__input_signature = input_signature\n    return input_signature",
  "def _input_signature(self, new_spec: tf.TensorSpec):\n    self.__input_signature = new_spec",
  "def __init__(self,\n               policy_network: snt.Module,\n               critic_network: snt.Module,\n               temperature_beta: float = 1.0,\n               num_action_samples: int = 16):\n    super().__init__(name='RecurrentExpQWeightedPolicy')\n    self._policy_network = policy_network\n    self._critic_network = critic_network\n    self._num_action_samples = num_action_samples\n    self._temperature_beta = temperature_beta",
  "def __call__(self,\n               observation: types.NestedTensor,\n               prev_state: PolicyCriticRNNState\n               ) -> Tuple[types.NestedTensor, PolicyCriticRNNState]:\n\n    return tf.vectorized_map(self._call, (observation, prev_state))",
  "def _call(\n      self, observation_and_state: Tuple[types.NestedTensor,\n                                         PolicyCriticRNNState]\n  ) -> Tuple[types.NestedTensor, PolicyCriticRNNState]:\n    \"\"\"Computes a forward step for a single element.\n\n    The observation and state are packed together in order to use\n    `tf.vectorized_map` to handle batches of observations.\n    See this module's __call__() function.\n\n    Args:\n      observation_and_state: the observation and state packed in a tuple.\n\n    Returns:\n      The selected action and the corresponding state.\n    \"\"\"\n    observation, prev_state = observation_and_state\n\n    # Tile input observations and states to allow multiple policy predictions.\n    tiled_observation, tiled_prev_state = utils.tile_nested(\n        (observation, prev_state), self._num_action_samples)\n    actions, policy_states = self._policy_network(\n        tiled_observation, tiled_prev_state.policy)\n\n    # Evaluate multiple critic predictions with the sampled actions.\n    value_distribution, critic_states = self._critic_network(\n        tiled_observation, actions, tiled_prev_state.critic)\n    value_estimate = value_distribution.mean()\n\n    # Resample a single action of the sampled actions according to logits given\n    # by the tempered Q-values.\n    selected_action_idx = tfp.distributions.Categorical(\n        probs=tf.nn.softmax(value_estimate / self._temperature_beta)).sample()\n    selected_action = actions[selected_action_idx]\n\n    # Select and return the RNN state that corresponds to the selected action.\n    states = PolicyCriticRNNState(\n        policy=policy_states, critic=critic_states)\n    selected_state = tree.map_structure(\n        lambda x: x[selected_action_idx], states)\n\n    return selected_action, selected_state",
  "def initial_state(self, batch_size: int) -> PolicyCriticRNNState:\n    return PolicyCriticRNNState(\n        policy=self._policy_network.initial_state(batch_size),\n        critic=self._critic_network.initial_state(batch_size)\n        )",
  "def __init__(self, layers, name: Optional[str] = None):\n    \"\"\"Initializes the module.\"\"\"\n    super().__init__(layers, name=name)\n\n    self.__input_signature = None\n    self._num_unrollable = 0\n\n    # As a convenience, check for snt.RNNCore modules and dynamically unroll\n    # them if they don't already support unrolling. This check can fail, e.g.\n    # if a partially applied RNNCore is passed in. Sonnet's implementation of\n    # DeepRNN suffers from the same problem.\n    for layer in self._layers:\n      if hasattr(layer, 'unroll'):\n        self._num_unrollable += 1\n      elif isinstance(layer, snt.RNNCore):\n        self._num_unrollable += 1\n        layer.unroll = functools.partial(snt.dynamic_unroll, layer)\n        logging.warning(\n            'Acme DeepRNN detected a Sonnet RNNCore. '\n            'This will be dynamically unrolled. Please implement unroll() '\n            'to suppress this warning.')",
  "def unroll(self,\n             inputs: types.NestedTensor,\n             state: base.State,\n             sequence_length: int,\n             ) -> Tuple[types.NestedTensor, base.State]:\n    \"\"\"Unroll each layer individually.\n\n    Calls unroll() on layers which support it, all other layers are\n    batch-applied over the first two axes (assumed to be the time and batch\n    axes).\n\n    Args:\n      inputs: A nest of `tf.Tensor` in time-major format.\n      state: The RNN core state.\n      sequence_length: How long the static_unroll should go for.\n\n    Returns:\n      Nested sequence output of RNN, and final state.\n\n    Raises:\n      ValueError if the length of `state` does not match the number of\n      unrollable layers.\n    \"\"\"\n    if len(state) != self._num_unrollable:\n      raise ValueError(\n          'DeepRNN was called with the wrong number of states. The length of '\n          '`state` does not match the number of unrollable layers.')\n\n    states = iter(state)\n    outputs = inputs\n    next_states = []\n    for layer in self._layers:\n      if hasattr(layer, 'unroll'):\n        # The length of the `states` list was checked above.\n        outputs, next_state = layer.unroll(outputs, next(states),\n                                           sequence_length)\n        next_states.append(next_state)\n      else:\n        # Couldn't unroll(); assume that this is a stateless module.\n        outputs = snt.BatchApply(layer, num_dims=2)(outputs)\n\n    return outputs, tuple(next_states)",
  "def _input_signature(self) -> Optional[tf.TensorSpec]:\n    \"\"\"Return input signature for Acme snapshotting, see CriticDeepRNN.\"\"\"\n\n    if self.__input_signature is not None:\n      return self.__input_signature\n\n    input_signature = savers._get_input_signature(self._layers[0])  # pylint: disable=protected-access\n    if input_signature is None:\n      return None\n\n    state = self.initial_state(1)\n    input_signature[-1] = tree.map_structure(\n        lambda t: tf.TensorSpec((None,) + t.shape[1:], t.dtype), state)\n    self.__input_signature = input_signature\n    return input_signature",
  "def _input_signature(self, new_spec: tf.TensorSpec):\n    self.__input_signature = new_spec",
  "def unroll(self,\n             inputs: types.NestedTensor,\n             state: base.State,\n             sequence_length: int,\n             ) -> Tuple[types.NestedTensor, base.State]:\n    return snt.static_unroll(self, inputs, state, sequence_length)",
  "def _calculate_num_learner_steps(num_observations: int,\n                                 min_observations: int,\n                                 observations_per_step: float) -> int:\n  \"\"\"Calculates the number of learner steps to do at step=num_observations.\"\"\"\n  n = num_observations - min_observations\n  if n < 0:\n    # Do not do any learner steps until you have seen min_observations.\n    return 0\n  if observations_per_step > 1:\n    # One batch every 1/obs_per_step observations, otherwise zero.\n    return int(n % int(observations_per_step) == 0)\n  else:\n    # Always return 1/obs_per_step batches every observation.\n    return int(1 / observations_per_step)",
  "class Agent(core.Actor, core.VariableSource):\n  \"\"\"Agent class which combines acting and learning.\n\n  This provides an implementation of the `Actor` interface which acts and\n  learns. It takes as input instances of both `acme.Actor` and `acme.Learner`\n  classes, and implements the policy, observation, and update methods which\n  defer to the underlying actor and learner.\n\n  The only real logic implemented by this class is that it controls the number\n  of observations to make before running a learner step. This is done by\n  passing the number of `min_observations` to use and a ratio of\n  `observations_per_step` := num_actor_actions / num_learner_steps.\n\n  Note that the number of `observations_per_step` can also be in the range[0, 1]\n  in order to allow the agent to take more than 1 learner step per action.\n  \"\"\"\n\n  def __init__(self, actor: core.Actor, learner: core.Learner,\n               min_observations: Optional[int] = None,\n               observations_per_step: Optional[float] = None,\n               iterator: Optional[core.PrefetchingIterator] = None,\n               replay_tables: Optional[List[reverb.Table]] = None):\n    self._actor = actor\n    self._learner = learner\n    self._min_observations = min_observations\n    self._observations_per_step = observations_per_step\n    self._num_observations = 0\n    self._iterator = iterator\n    self._replay_tables = replay_tables\n    self._batch_size_upper_bounds = [1_000_000_000] * len(\n        replay_tables) if replay_tables else None\n\n  def select_action(self, observation: types.NestedArray) -> types.NestedArray:\n    return self._actor.select_action(observation)\n\n  def observe_first(self, timestep: dm_env.TimeStep):\n    self._actor.observe_first(timestep)\n\n  def observe(self, action: types.NestedArray, next_timestep: dm_env.TimeStep):\n    self._num_observations += 1\n    self._actor.observe(action, next_timestep)\n\n  def _has_data_for_training(self):\n    if self._iterator.ready():\n      return True\n    for (table, batch_size) in zip(self._replay_tables,\n                                   self._batch_size_upper_bounds):\n      if not table.can_sample(batch_size):\n        return False\n    return True\n\n  def update(self):  # pytype: disable=signature-mismatch  # overriding-parameter-count-checks\n    if self._iterator:\n      # Perform learner steps as long as iterator has data.\n      update_actor = False\n      while self._has_data_for_training():\n        # Run learner steps (usually means gradient steps).\n        total_batches = self._iterator.retrieved_elements()\n        self._learner.step()\n        current_batches = self._iterator.retrieved_elements() - total_batches\n        assert current_batches == 1, (\n            'Learner step must retrieve exactly one element from the iterator'\n            f' (retrieved {current_batches}). Otherwise agent can deadlock. '\n            'Example cause is that your chosen agent'\n            's Builder has a '\n            '`make_learner` factory that prefetches the data but it '\n            'shouldn'\n            't.')\n        self._batch_size_upper_bounds = [\n            math.ceil(t.info.rate_limiter_info.sample_stats.completed /\n                      (total_batches + 1)) for t in self._replay_tables\n        ]\n        update_actor = True\n      if update_actor:\n        # Update the actor weights only when learner was updated.\n        self._actor.update()\n      return\n\n    # If dataset is not provided, follback to the old logic.\n    # TODO(stanczyk): Remove when not used.\n    num_steps = _calculate_num_learner_steps(\n        num_observations=self._num_observations,\n        min_observations=self._min_observations,\n        observations_per_step=self._observations_per_step,\n    )\n    for _ in range(num_steps):\n      # Run learner steps (usually means gradient steps).\n      self._learner.step()\n    if num_steps > 0:\n      # Update the actor weights when learner updates.\n      self._actor.update()\n\n  def get_variables(self, names: Sequence[str]) -> List[List[np.ndarray]]:\n    return self._learner.get_variables(names)",
  "def __init__(self, actor: core.Actor, learner: core.Learner,\n               min_observations: Optional[int] = None,\n               observations_per_step: Optional[float] = None,\n               iterator: Optional[core.PrefetchingIterator] = None,\n               replay_tables: Optional[List[reverb.Table]] = None):\n    self._actor = actor\n    self._learner = learner\n    self._min_observations = min_observations\n    self._observations_per_step = observations_per_step\n    self._num_observations = 0\n    self._iterator = iterator\n    self._replay_tables = replay_tables\n    self._batch_size_upper_bounds = [1_000_000_000] * len(\n        replay_tables) if replay_tables else None",
  "def select_action(self, observation: types.NestedArray) -> types.NestedArray:\n    return self._actor.select_action(observation)",
  "def observe_first(self, timestep: dm_env.TimeStep):\n    self._actor.observe_first(timestep)",
  "def observe(self, action: types.NestedArray, next_timestep: dm_env.TimeStep):\n    self._num_observations += 1\n    self._actor.observe(action, next_timestep)",
  "def _has_data_for_training(self):\n    if self._iterator.ready():\n      return True\n    for (table, batch_size) in zip(self._replay_tables,\n                                   self._batch_size_upper_bounds):\n      if not table.can_sample(batch_size):\n        return False\n    return True",
  "def update(self):  # pytype: disable=signature-mismatch  # overriding-parameter-count-checks\n    if self._iterator:\n      # Perform learner steps as long as iterator has data.\n      update_actor = False\n      while self._has_data_for_training():\n        # Run learner steps (usually means gradient steps).\n        total_batches = self._iterator.retrieved_elements()\n        self._learner.step()\n        current_batches = self._iterator.retrieved_elements() - total_batches\n        assert current_batches == 1, (\n            'Learner step must retrieve exactly one element from the iterator'\n            f' (retrieved {current_batches}). Otherwise agent can deadlock. '\n            'Example cause is that your chosen agent'\n            's Builder has a '\n            '`make_learner` factory that prefetches the data but it '\n            'shouldn'\n            't.')\n        self._batch_size_upper_bounds = [\n            math.ceil(t.info.rate_limiter_info.sample_stats.completed /\n                      (total_batches + 1)) for t in self._replay_tables\n        ]\n        update_actor = True\n      if update_actor:\n        # Update the actor weights only when learner was updated.\n        self._actor.update()\n      return\n\n    # If dataset is not provided, follback to the old logic.\n    # TODO(stanczyk): Remove when not used.\n    num_steps = _calculate_num_learner_steps(\n        num_observations=self._num_observations,\n        min_observations=self._min_observations,\n        observations_per_step=self._observations_per_step,\n    )\n    for _ in range(num_steps):\n      # Run learner steps (usually means gradient steps).\n      self._learner.step()\n    if num_steps > 0:\n      # Update the actor weights when learner updates.\n      self._actor.update()",
  "def get_variables(self, names: Sequence[str]) -> List[List[np.ndarray]]:\n    return self._learner.get_variables(names)",
  "class ReverbReplay:\n  server: reverb.Server\n  adder: adders_lib.Adder\n  data_iterator: Iterator[reverb.ReplaySample]\n  client: Optional[reverb.Client] = None\n  can_sample: Callable[[], bool] = lambda: True",
  "def make_reverb_prioritized_nstep_replay(\n    environment_spec: specs.EnvironmentSpec,\n    extra_spec: types.NestedSpec = (),\n    n_step: int = 1,\n    batch_size: int = 32,\n    max_replay_size: int = 100_000,\n    min_replay_size: int = 1,\n    discount: float = 1.,\n    prefetch_size: int = 4,  # TODO(iosband): rationalize prefetch size.\n    replay_table_name: str = adders.DEFAULT_PRIORITY_TABLE,\n    priority_exponent: Optional[float] = None,  # If None, default to uniform.\n) -> ReverbReplay:\n  \"\"\"Creates a single-process replay infrastructure from an environment spec.\"\"\"\n  # Parsing priority exponent to determine uniform vs prioritized replay\n  if priority_exponent is None:\n    sampler = reverb.selectors.Uniform()\n    priority_fns = {replay_table_name: lambda x: 1.}\n  else:\n    sampler = reverb.selectors.Prioritized(priority_exponent)\n    priority_fns = None\n\n  # Create a replay server to add data to. This uses no limiter behavior in\n  # order to allow the Agent interface to handle it.\n  replay_table = reverb.Table(\n      name=replay_table_name,\n      sampler=sampler,\n      remover=reverb.selectors.Fifo(),\n      max_size=max_replay_size,\n      rate_limiter=reverb.rate_limiters.MinSize(min_replay_size),\n      signature=adders.NStepTransitionAdder.signature(environment_spec,\n                                                      extra_spec),\n  )\n  server = reverb.Server([replay_table], port=None)\n\n  # The adder is used to insert observations into replay.\n  address = f'localhost:{server.port}'\n  client = reverb.Client(address)\n  adder = adders.NStepTransitionAdder(\n      client, n_step, discount, priority_fns=priority_fns)\n\n  # The dataset provides an interface to sample from replay.\n  data_iterator = datasets.make_reverb_dataset(\n      table=replay_table_name,\n      server_address=address,\n      batch_size=batch_size,\n      prefetch_size=prefetch_size,\n  ).as_numpy_iterator()\n  return ReverbReplay(server, adder, data_iterator, client=client)",
  "def make_reverb_online_queue(\n    environment_spec: specs.EnvironmentSpec,\n    extra_spec: Dict[str, Any],\n    max_queue_size: int,\n    sequence_length: int,\n    sequence_period: int,\n    batch_size: int,\n    replay_table_name: str = adders.DEFAULT_PRIORITY_TABLE,\n) -> ReverbReplay:\n  \"\"\"Creates a single process queue from an environment spec and extra_spec.\"\"\"\n  signature = adders.SequenceAdder.signature(environment_spec, extra_spec)\n  queue = reverb.Table.queue(\n      name=replay_table_name, max_size=max_queue_size, signature=signature)\n  server = reverb.Server([queue], port=None)\n  can_sample = lambda: queue.can_sample(batch_size)\n\n  # Component to add things into replay.\n  address = f'localhost:{server.port}'\n  adder = adders.SequenceAdder(\n      client=reverb.Client(address),\n      period=sequence_period,\n      sequence_length=sequence_length,\n  )\n\n  # The dataset object to learn from.\n  # We don't use datasets.make_reverb_dataset() here to avoid interleaving\n  # and prefetching, that doesn't work well with can_sample() check on update.\n  dataset = reverb.TrajectoryDataset.from_table_signature(\n      server_address=address,\n      table=replay_table_name,\n      max_in_flight_samples_per_worker=1,\n  )\n  dataset = dataset.batch(batch_size, drop_remainder=True)\n  data_iterator = dataset.as_numpy_iterator()\n  return ReverbReplay(server, adder, data_iterator, can_sample=can_sample)",
  "def make_reverb_prioritized_sequence_replay(\n    environment_spec: specs.EnvironmentSpec,\n    extra_spec: types.NestedSpec = (),\n    batch_size: int = 32,\n    max_replay_size: int = 100_000,\n    min_replay_size: int = 1,\n    priority_exponent: float = 0.,\n    burn_in_length: int = 40,\n    sequence_length: int = 80,\n    sequence_period: int = 40,\n    replay_table_name: str = adders.DEFAULT_PRIORITY_TABLE,\n    prefetch_size: int = 4,\n) -> ReverbReplay:\n  \"\"\"Single-process replay for sequence data from an environment spec.\"\"\"\n  # Create a replay server to add data to. This uses no limiter behavior in\n  # order to allow the Agent interface to handle it.\n  replay_table = reverb.Table(\n      name=replay_table_name,\n      sampler=reverb.selectors.Prioritized(priority_exponent),\n      remover=reverb.selectors.Fifo(),\n      max_size=max_replay_size,\n      rate_limiter=reverb.rate_limiters.MinSize(min_replay_size),\n      signature=adders.SequenceAdder.signature(environment_spec, extra_spec),\n  )\n  server = reverb.Server([replay_table], port=None)\n\n  # The adder is used to insert observations into replay.\n  address = f'localhost:{server.port}'\n  client = reverb.Client(address)\n  sequence_length = burn_in_length + sequence_length + 1\n  adder = adders.SequenceAdder(\n      client=client,\n      period=sequence_period,\n      sequence_length=sequence_length,\n      delta_encoded=True,\n  )\n\n  # The dataset provides an interface to sample from replay.\n  data_iterator = datasets.make_reverb_dataset(\n      table=replay_table_name,\n      server_address=address,\n      batch_size=batch_size,\n      prefetch_size=prefetch_size,\n  ).as_numpy_iterator()\n  return ReverbReplay(server, adder, data_iterator, client)",
  "class FeedForwardActor(core.Actor):\n  \"\"\"A feed-forward actor.\n\n  An actor based on a feed-forward policy which takes non-batched observations\n  and outputs non-batched actions. It also allows adding experiences to replay\n  and updating the weights from the policy on the learner.\n  \"\"\"\n\n  def __init__(\n      self,\n      policy_network: snt.Module,\n      adder: Optional[adders.Adder] = None,\n      variable_client: Optional[tf2_variable_utils.VariableClient] = None,\n  ):\n    \"\"\"Initializes the actor.\n\n    Args:\n      policy_network: the policy to run.\n      adder: the adder object to which allows to add experiences to a\n        dataset/replay buffer.\n      variable_client: object which allows to copy weights from the learner copy\n        of the policy to the actor copy (in case they are separate).\n    \"\"\"\n\n    # Store these for later use.\n    self._adder = adder\n    self._variable_client = variable_client\n    self._policy_network = policy_network\n\n  @tf.function\n  def _policy(self, observation: types.NestedTensor) -> types.NestedTensor:\n    # Add a dummy batch dimension and as a side effect convert numpy to TF.\n    batched_observation = tf2_utils.add_batch_dim(observation)\n\n    # Compute the policy, conditioned on the observation.\n    policy = self._policy_network(batched_observation)\n\n    # Sample from the policy if it is stochastic.\n    action = policy.sample() if isinstance(policy, tfd.Distribution) else policy\n\n    return action\n\n  def select_action(self, observation: types.NestedArray) -> types.NestedArray:\n    # Pass the observation through the policy network.\n    action = self._policy(observation)\n\n    # Return a numpy array with squeezed out batch dimension.\n    return tf2_utils.to_numpy_squeeze(action)\n\n  def observe_first(self, timestep: dm_env.TimeStep):\n    if self._adder:\n      self._adder.add_first(timestep)\n\n  def observe(self, action: types.NestedArray, next_timestep: dm_env.TimeStep):\n    if self._adder:\n      self._adder.add(action, next_timestep)\n\n  def update(self, wait: bool = False):\n    if self._variable_client:\n      self._variable_client.update(wait)",
  "class RecurrentActor(core.Actor):\n  \"\"\"A recurrent actor.\n\n  An actor based on a recurrent policy which takes non-batched observations and\n  outputs non-batched actions, and keeps track of the recurrent state inside. It\n  also allows adding experiences to replay and updating the weights from the\n  policy on the learner.\n  \"\"\"\n\n  def __init__(\n      self,\n      policy_network: snt.RNNCore,\n      adder: Optional[adders.Adder] = None,\n      variable_client: Optional[tf2_variable_utils.VariableClient] = None,\n      store_recurrent_state: bool = True,\n  ):\n    \"\"\"Initializes the actor.\n\n    Args:\n      policy_network: the (recurrent) policy to run.\n      adder: the adder object to which allows to add experiences to a\n        dataset/replay buffer.\n      variable_client: object which allows to copy weights from the learner copy\n        of the policy to the actor copy (in case they are separate).\n      store_recurrent_state: Whether to pass the recurrent state to the adder.\n    \"\"\"\n    # Store these for later use.\n    self._adder = adder\n    self._variable_client = variable_client\n    self._network = policy_network\n    self._state = None\n    self._prev_state = None\n    self._store_recurrent_state = store_recurrent_state\n\n  @tf.function\n  def _policy(\n      self,\n      observation: types.NestedTensor,\n      state: types.NestedTensor,\n  ) -> Tuple[types.NestedTensor, types.NestedTensor]:\n\n    # Add a dummy batch dimension and as a side effect convert numpy to TF.\n    batched_observation = tf2_utils.add_batch_dim(observation)\n\n    # Compute the policy, conditioned on the observation.\n    policy, new_state = self._network(batched_observation, state)\n\n    # Sample from the policy if it is stochastic.\n    action = policy.sample() if isinstance(policy, tfd.Distribution) else policy\n\n    return action, new_state\n\n  def select_action(self, observation: types.NestedArray) -> types.NestedArray:\n    # Initialize the RNN state if necessary.\n    if self._state is None:\n      self._state = self._network.initial_state(1)\n\n    # Step the recurrent policy forward given the current observation and state.\n    policy_output, new_state = self._policy(observation, self._state)\n\n    # Bookkeeping of recurrent states for the observe method.\n    self._prev_state = self._state\n    self._state = new_state\n\n    # Return a numpy array with squeezed out batch dimension.\n    return tf2_utils.to_numpy_squeeze(policy_output)\n\n  def observe_first(self, timestep: dm_env.TimeStep):\n    if self._adder:\n      self._adder.add_first(timestep)\n\n    # Set the state to None so that we re-initialize at the next policy call.\n    self._state = None\n\n  def observe(self, action: types.NestedArray, next_timestep: dm_env.TimeStep):\n    if not self._adder:\n      return\n\n    if not self._store_recurrent_state:\n      self._adder.add(action, next_timestep)\n      return\n\n    numpy_state = tf2_utils.to_numpy_squeeze(self._prev_state)\n    self._adder.add(action, next_timestep, extras=(numpy_state,))\n\n  def update(self, wait: bool = False):\n    if self._variable_client:\n      self._variable_client.update(wait)",
  "def __init__(\n      self,\n      policy_network: snt.Module,\n      adder: Optional[adders.Adder] = None,\n      variable_client: Optional[tf2_variable_utils.VariableClient] = None,\n  ):\n    \"\"\"Initializes the actor.\n\n    Args:\n      policy_network: the policy to run.\n      adder: the adder object to which allows to add experiences to a\n        dataset/replay buffer.\n      variable_client: object which allows to copy weights from the learner copy\n        of the policy to the actor copy (in case they are separate).\n    \"\"\"\n\n    # Store these for later use.\n    self._adder = adder\n    self._variable_client = variable_client\n    self._policy_network = policy_network",
  "def _policy(self, observation: types.NestedTensor) -> types.NestedTensor:\n    # Add a dummy batch dimension and as a side effect convert numpy to TF.\n    batched_observation = tf2_utils.add_batch_dim(observation)\n\n    # Compute the policy, conditioned on the observation.\n    policy = self._policy_network(batched_observation)\n\n    # Sample from the policy if it is stochastic.\n    action = policy.sample() if isinstance(policy, tfd.Distribution) else policy\n\n    return action",
  "def select_action(self, observation: types.NestedArray) -> types.NestedArray:\n    # Pass the observation through the policy network.\n    action = self._policy(observation)\n\n    # Return a numpy array with squeezed out batch dimension.\n    return tf2_utils.to_numpy_squeeze(action)",
  "def observe_first(self, timestep: dm_env.TimeStep):\n    if self._adder:\n      self._adder.add_first(timestep)",
  "def observe(self, action: types.NestedArray, next_timestep: dm_env.TimeStep):\n    if self._adder:\n      self._adder.add(action, next_timestep)",
  "def update(self, wait: bool = False):\n    if self._variable_client:\n      self._variable_client.update(wait)",
  "def __init__(\n      self,\n      policy_network: snt.RNNCore,\n      adder: Optional[adders.Adder] = None,\n      variable_client: Optional[tf2_variable_utils.VariableClient] = None,\n      store_recurrent_state: bool = True,\n  ):\n    \"\"\"Initializes the actor.\n\n    Args:\n      policy_network: the (recurrent) policy to run.\n      adder: the adder object to which allows to add experiences to a\n        dataset/replay buffer.\n      variable_client: object which allows to copy weights from the learner copy\n        of the policy to the actor copy (in case they are separate).\n      store_recurrent_state: Whether to pass the recurrent state to the adder.\n    \"\"\"\n    # Store these for later use.\n    self._adder = adder\n    self._variable_client = variable_client\n    self._network = policy_network\n    self._state = None\n    self._prev_state = None\n    self._store_recurrent_state = store_recurrent_state",
  "def _policy(\n      self,\n      observation: types.NestedTensor,\n      state: types.NestedTensor,\n  ) -> Tuple[types.NestedTensor, types.NestedTensor]:\n\n    # Add a dummy batch dimension and as a side effect convert numpy to TF.\n    batched_observation = tf2_utils.add_batch_dim(observation)\n\n    # Compute the policy, conditioned on the observation.\n    policy, new_state = self._network(batched_observation, state)\n\n    # Sample from the policy if it is stochastic.\n    action = policy.sample() if isinstance(policy, tfd.Distribution) else policy\n\n    return action, new_state",
  "def select_action(self, observation: types.NestedArray) -> types.NestedArray:\n    # Initialize the RNN state if necessary.\n    if self._state is None:\n      self._state = self._network.initial_state(1)\n\n    # Step the recurrent policy forward given the current observation and state.\n    policy_output, new_state = self._policy(observation, self._state)\n\n    # Bookkeeping of recurrent states for the observe method.\n    self._prev_state = self._state\n    self._state = new_state\n\n    # Return a numpy array with squeezed out batch dimension.\n    return tf2_utils.to_numpy_squeeze(policy_output)",
  "def observe_first(self, timestep: dm_env.TimeStep):\n    if self._adder:\n      self._adder.add_first(timestep)\n\n    # Set the state to None so that we re-initialize at the next policy call.\n    self._state = None",
  "def observe(self, action: types.NestedArray, next_timestep: dm_env.TimeStep):\n    if not self._adder:\n      return\n\n    if not self._store_recurrent_state:\n      self._adder.add(action, next_timestep)\n      return\n\n    numpy_state = tf2_utils.to_numpy_squeeze(self._prev_state)\n    self._adder.add(action, next_timestep, extras=(numpy_state,))",
  "def update(self, wait: bool = False):\n    if self._variable_client:\n      self._variable_client.update(wait)",
  "def _make_fake_env() -> dm_env.Environment:\n  env_spec = specs.EnvironmentSpec(\n      observations=specs.Array(shape=(10, 5), dtype=np.float32),\n      actions=specs.DiscreteArray(num_values=3),\n      rewards=specs.Array(shape=(), dtype=np.float32),\n      discounts=specs.BoundedArray(\n          shape=(), dtype=np.float32, minimum=0., maximum=1.),\n  )\n  return fakes.Environment(env_spec, episode_length=10)",
  "class ActorTest(absltest.TestCase):\n\n  def test_feedforward(self):\n    environment = _make_fake_env()\n    env_spec = specs.make_environment_spec(environment)\n\n    network = snt.Sequential([\n        snt.Flatten(),\n        snt.Linear(env_spec.actions.num_values),\n        lambda x: tf.argmax(x, axis=-1, output_type=env_spec.actions.dtype),\n    ])\n\n    actor = actors.FeedForwardActor(network)\n    loop = environment_loop.EnvironmentLoop(environment, actor)\n    loop.run(20)\n\n  def test_recurrent(self):\n    environment = _make_fake_env()\n    env_spec = specs.make_environment_spec(environment)\n\n    network = snt.DeepRNN([\n        snt.Flatten(),\n        snt.Linear(env_spec.actions.num_values),\n        lambda x: tf.argmax(x, axis=-1, output_type=env_spec.actions.dtype),\n    ])\n\n    actor = actors.RecurrentActor(network)\n    loop = environment_loop.EnvironmentLoop(environment, actor)\n    loop.run(20)",
  "def test_feedforward(self):\n    environment = _make_fake_env()\n    env_spec = specs.make_environment_spec(environment)\n\n    network = snt.Sequential([\n        snt.Flatten(),\n        snt.Linear(env_spec.actions.num_values),\n        lambda x: tf.argmax(x, axis=-1, output_type=env_spec.actions.dtype),\n    ])\n\n    actor = actors.FeedForwardActor(network)\n    loop = environment_loop.EnvironmentLoop(environment, actor)\n    loop.run(20)",
  "def test_recurrent(self):\n    environment = _make_fake_env()\n    env_spec = specs.make_environment_spec(environment)\n\n    network = snt.DeepRNN([\n        snt.Flatten(),\n        snt.Linear(env_spec.actions.num_values),\n        lambda x: tf.argmax(x, axis=-1, output_type=env_spec.actions.dtype),\n    ])\n\n    actor = actors.RecurrentActor(network)\n    loop = environment_loop.EnvironmentLoop(environment, actor)\n    loop.run(20)",
  "class SVG0Config:\n  \"\"\"Configuration options for the agent.\"\"\"\n\n  discount: float = 0.99\n  batch_size: int = 256\n  prefetch_size: int = 4\n  target_update_period: int = 100\n  policy_optimizer: Optional[snt.Optimizer] = None\n  critic_optimizer: Optional[snt.Optimizer] = None\n  prior_optimizer: Optional[snt.Optimizer] = None\n  min_replay_size: int = 1000\n  max_replay_size: int = 1000000\n  samples_per_insert: Optional[float] = 32.0\n  sequence_length: int = 10\n  sigma: float = 0.3\n  replay_table_name: str = reverb_adders.DEFAULT_PRIORITY_TABLE\n  distillation_cost: Optional[float] = 1e-3\n  entropy_regularizer_cost: Optional[float] = 1e-3",
  "class SVG0Networks:\n  \"\"\"Structure containing the networks for SVG0.\"\"\"\n\n  policy_network: snt.Module\n  critic_network: snt.Module\n  prior_network: Optional[snt.Module]\n\n  def __init__(\n      self,\n      policy_network: snt.Module,\n      critic_network: snt.Module,\n      prior_network: Optional[snt.Module] = None\n  ):\n    # This method is implemented (rather than added by the dataclass decorator)\n    # in order to allow observation network to be passed as an arbitrary tensor\n    # transformation rather than as a snt Module.\n    # TODO(mwhoffman): use Protocol rather than Module/TensorTransformation.\n    self.policy_network = policy_network\n    self.critic_network = critic_network\n    self.prior_network = prior_network\n\n  def init(self, environment_spec: specs.EnvironmentSpec):\n    \"\"\"Initialize the networks given an environment spec.\"\"\"\n    # Get observation and action specs.\n    act_spec = environment_spec.actions\n    obs_spec = environment_spec.observations\n\n    # Create variables for the policy and critic nets.\n    _ = utils.create_variables(self.policy_network, [obs_spec])\n    _ = utils.create_variables(self.critic_network, [obs_spec, act_spec])\n    if self.prior_network is not None:\n      _ = utils.create_variables(self.prior_network, [obs_spec])\n\n  def make_policy(\n      self,\n  ) -> snt.Module:\n    \"\"\"Create a single network which evaluates the policy.\"\"\"\n    return self.policy_network\n\n  def make_prior(\n      self,\n  ) -> snt.Module:\n    \"\"\"Create a single network which evaluates the prior.\"\"\"\n    behavior_prior = self.prior_network\n    return behavior_prior",
  "class SVG0Builder:\n  \"\"\"Builder for SVG0 which constructs individual components of the agent.\"\"\"\n\n  def __init__(self, config: SVG0Config):\n    self._config = config\n\n  def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      sequence_length: int,\n  ) -> List[reverb.Table]:\n    \"\"\"Create tables to insert data into.\"\"\"\n    if self._config.samples_per_insert is None:\n      # We will take a samples_per_insert ratio of None to mean that there is\n      # no limit, i.e. this only implies a min size limit.\n      limiter = reverb.rate_limiters.MinSize(self._config.min_replay_size)\n\n    else:\n      error_buffer = max(1, self._config.samples_per_insert)\n      limiter = reverb.rate_limiters.SampleToInsertRatio(\n          min_size_to_sample=self._config.min_replay_size,\n          samples_per_insert=self._config.samples_per_insert,\n          error_buffer=error_buffer)\n\n    extras_spec = {\n        'log_prob': tf.ones(\n            shape=(), dtype=tf.float32)\n    }\n    replay_table = reverb.Table(\n        name=self._config.replay_table_name,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=self._config.max_replay_size,\n        rate_limiter=limiter,\n        signature=reverb_adders.SequenceAdder.signature(\n            environment_spec,\n            extras_spec=extras_spec,\n            sequence_length=sequence_length + 1))\n\n    return [replay_table]\n\n  def make_dataset_iterator(\n      self,\n      reverb_client: reverb.Client,\n  ) -> Iterator[reverb.ReplaySample]:\n    \"\"\"Create a dataset iterator to use for learning/updating the agent.\"\"\"\n    # The dataset provides an interface to sample from replay.\n    dataset = datasets.make_reverb_dataset(\n        table=self._config.replay_table_name,\n        server_address=reverb_client.server_address,\n        batch_size=self._config.batch_size,\n        prefetch_size=self._config.prefetch_size)\n\n    # TODO(b/155086959): Fix type stubs and remove.\n    return iter(dataset)  # pytype: disable=wrong-arg-types\n\n  def make_adder(\n      self,\n      replay_client: reverb.Client,\n  ) -> adders.Adder:\n    \"\"\"Create an adder which records data generated by the actor/environment.\"\"\"\n    return reverb_adders.SequenceAdder(\n        client=replay_client,\n        sequence_length=self._config.sequence_length+1,\n        priority_fns={self._config.replay_table_name: lambda x: 1.},\n        period=self._config.sequence_length,\n        end_of_episode_behavior=reverb_adders.EndBehavior.CONTINUE,\n        )\n\n  def make_actor(\n      self,\n      policy_network: snt.Module,\n      adder: Optional[adders.Adder] = None,\n      variable_source: Optional[core.VariableSource] = None,\n      deterministic_policy: Optional[bool] = False,\n  ):\n    \"\"\"Create an actor instance.\"\"\"\n    if variable_source:\n      # Create the variable client responsible for keeping the actor up-to-date.\n      variable_client = variable_utils.VariableClient(\n          client=variable_source,\n          variables={'policy': policy_network.variables},\n          update_period=1000,\n      )\n\n      # Make sure not to use a random policy after checkpoint restoration by\n      # assigning variables before running the environment loop.\n      variable_client.update_and_wait()\n\n    else:\n      variable_client = None\n\n    # Create the actor which defines how we take actions.\n    return acting.SVG0Actor(\n        policy_network=policy_network,\n        adder=adder,\n        variable_client=variable_client,\n        deterministic_policy=deterministic_policy\n    )\n\n  def make_learner(\n      self,\n      networks: Tuple[SVG0Networks, SVG0Networks],\n      dataset: Iterator[reverb.ReplaySample],\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = False,\n  ):\n    \"\"\"Creates an instance of the learner.\"\"\"\n    online_networks, target_networks = networks\n\n    # The learner updates the parameters (and initializes them).\n    return learning.SVG0Learner(\n        policy_network=online_networks.policy_network,\n        critic_network=online_networks.critic_network,\n        target_policy_network=target_networks.policy_network,\n        target_critic_network=target_networks.critic_network,\n        prior_network=online_networks.prior_network,\n        target_prior_network=target_networks.prior_network,\n        policy_optimizer=self._config.policy_optimizer,\n        critic_optimizer=self._config.critic_optimizer,\n        prior_optimizer=self._config.prior_optimizer,\n        distillation_cost=self._config.distillation_cost,\n        entropy_regularizer_cost=self._config.entropy_regularizer_cost,\n        discount=self._config.discount,\n        target_update_period=self._config.target_update_period,\n        dataset_iterator=dataset,\n        counter=counter,\n        logger=logger,\n        checkpoint=checkpoint,\n    )",
  "class SVG0(agent.Agent):\n  \"\"\"SVG0 Agent with prior.\n\n  This implements a single-process SVG0 agent. This is an actor-critic algorithm\n  that generates data via a behavior policy, inserts N-step transitions into\n  a replay buffer, and periodically updates the policy (and as a result the\n  behavior) by sampling uniformly from this buffer.\n  \"\"\"\n\n  def __init__(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy_network: snt.Module,\n      critic_network: snt.Module,\n      discount: float = 0.99,\n      batch_size: int = 256,\n      prefetch_size: int = 4,\n      target_update_period: int = 100,\n      prior_network: Optional[snt.Module] = None,\n      policy_optimizer: Optional[snt.Optimizer] = None,\n      critic_optimizer: Optional[snt.Optimizer] = None,\n      prior_optimizer: Optional[snt.Optimizer] = None,\n      distillation_cost: Optional[float] = 1e-3,\n      entropy_regularizer_cost: Optional[float] = 1e-3,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1000000,\n      samples_per_insert: float = 32.0,\n      sequence_length: int = 10,\n      sigma: float = 0.3,\n      replay_table_name: str = reverb_adders.DEFAULT_PRIORITY_TABLE,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = True,\n  ):\n    \"\"\"Initialize the agent.\n\n    Args:\n      environment_spec: description of the actions, observations, etc.\n      policy_network: the online (optimized) policy.\n      critic_network: the online critic.\n      discount: discount to use for TD updates.\n      batch_size: batch size for updates.\n      prefetch_size: size to prefetch from replay.\n      target_update_period: number of learner steps to perform before updating\n        the target networks.\n      prior_network: an optional `behavior prior` to regularize against.\n      policy_optimizer: optimizer for the policy network updates.\n      critic_optimizer: optimizer for the critic network updates.\n      prior_optimizer: optimizer for the prior network updates.\n      distillation_cost: a multiplier to be used when adding distillation\n        against the prior to the losses.\n      entropy_regularizer_cost: a multiplier used for per state sample based\n        entropy added to the actor loss.\n      min_replay_size: minimum replay size before updating.\n      max_replay_size: maximum replay size.\n      samples_per_insert: number of samples to take from replay for every insert\n        that is made.\n      sequence_length: number of timesteps to store for each trajectory.\n      sigma: standard deviation of zero-mean, Gaussian exploration noise.\n      replay_table_name: string indicating what name to give the replay table.\n      counter: counter object used to keep track of steps.\n      logger: logger object to be used by learner.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n    \"\"\"\n    # Create the Builder object which will internally create agent components.\n    builder = SVG0Builder(\n        # TODO(mwhoffman): pass the config dataclass in directly.\n        # TODO(mwhoffman): use the limiter rather than the workaround below.\n        # Right now this modifies min_replay_size and samples_per_insert so that\n        # they are not controlled by a limiter and are instead handled by the\n        # Agent base class (the above TODO directly references this behavior).\n        SVG0Config(\n            discount=discount,\n            batch_size=batch_size,\n            prefetch_size=prefetch_size,\n            target_update_period=target_update_period,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            prior_optimizer=prior_optimizer,\n            distillation_cost=distillation_cost,\n            entropy_regularizer_cost=entropy_regularizer_cost,\n            min_replay_size=1,  # Let the Agent class handle this.\n            max_replay_size=max_replay_size,\n            samples_per_insert=None,  # Let the Agent class handle this.\n            sequence_length=sequence_length,\n            sigma=sigma,\n            replay_table_name=replay_table_name,\n        ))\n\n    # TODO(mwhoffman): pass the network dataclass in directly.\n    online_networks = SVG0Networks(policy_network=policy_network,\n                                   critic_network=critic_network,\n                                   prior_network=prior_network,)\n\n    # Target networks are just a copy of the online networks.\n    target_networks = copy.deepcopy(online_networks)\n\n    # Initialize the networks.\n    online_networks.init(environment_spec)\n    target_networks.init(environment_spec)\n\n    # TODO(mwhoffman): either make this Dataclass or pass only one struct.\n    # The network struct passed to make_learner is just a tuple for the\n    # time-being (for backwards compatibility).\n    networks = (online_networks, target_networks)\n\n    # Create the behavior policy.\n    policy_network = online_networks.make_policy()\n\n    # Create the replay server and grab its address.\n    replay_tables = builder.make_replay_tables(environment_spec,\n                                               sequence_length)\n    replay_server = reverb.Server(replay_tables, port=None)\n    replay_client = reverb.Client(f'localhost:{replay_server.port}')\n\n    # Create actor, dataset, and learner for generating, storing, and consuming\n    # data respectively.\n    adder = builder.make_adder(replay_client)\n    actor = builder.make_actor(policy_network, adder)\n    dataset = builder.make_dataset_iterator(replay_client)\n    learner = builder.make_learner(networks, dataset, counter, logger,\n                                   checkpoint)\n\n    super().__init__(\n        actor=actor,\n        learner=learner,\n        min_observations=max(batch_size, min_replay_size),\n        observations_per_step=float(batch_size) / samples_per_insert)\n\n    # Save the replay so we don't garbage collect it.\n    self._replay_server = replay_server",
  "def __init__(\n      self,\n      policy_network: snt.Module,\n      critic_network: snt.Module,\n      prior_network: Optional[snt.Module] = None\n  ):\n    # This method is implemented (rather than added by the dataclass decorator)\n    # in order to allow observation network to be passed as an arbitrary tensor\n    # transformation rather than as a snt Module.\n    # TODO(mwhoffman): use Protocol rather than Module/TensorTransformation.\n    self.policy_network = policy_network\n    self.critic_network = critic_network\n    self.prior_network = prior_network",
  "def init(self, environment_spec: specs.EnvironmentSpec):\n    \"\"\"Initialize the networks given an environment spec.\"\"\"\n    # Get observation and action specs.\n    act_spec = environment_spec.actions\n    obs_spec = environment_spec.observations\n\n    # Create variables for the policy and critic nets.\n    _ = utils.create_variables(self.policy_network, [obs_spec])\n    _ = utils.create_variables(self.critic_network, [obs_spec, act_spec])\n    if self.prior_network is not None:\n      _ = utils.create_variables(self.prior_network, [obs_spec])",
  "def make_policy(\n      self,\n  ) -> snt.Module:\n    \"\"\"Create a single network which evaluates the policy.\"\"\"\n    return self.policy_network",
  "def make_prior(\n      self,\n  ) -> snt.Module:\n    \"\"\"Create a single network which evaluates the prior.\"\"\"\n    behavior_prior = self.prior_network\n    return behavior_prior",
  "def __init__(self, config: SVG0Config):\n    self._config = config",
  "def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      sequence_length: int,\n  ) -> List[reverb.Table]:\n    \"\"\"Create tables to insert data into.\"\"\"\n    if self._config.samples_per_insert is None:\n      # We will take a samples_per_insert ratio of None to mean that there is\n      # no limit, i.e. this only implies a min size limit.\n      limiter = reverb.rate_limiters.MinSize(self._config.min_replay_size)\n\n    else:\n      error_buffer = max(1, self._config.samples_per_insert)\n      limiter = reverb.rate_limiters.SampleToInsertRatio(\n          min_size_to_sample=self._config.min_replay_size,\n          samples_per_insert=self._config.samples_per_insert,\n          error_buffer=error_buffer)\n\n    extras_spec = {\n        'log_prob': tf.ones(\n            shape=(), dtype=tf.float32)\n    }\n    replay_table = reverb.Table(\n        name=self._config.replay_table_name,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=self._config.max_replay_size,\n        rate_limiter=limiter,\n        signature=reverb_adders.SequenceAdder.signature(\n            environment_spec,\n            extras_spec=extras_spec,\n            sequence_length=sequence_length + 1))\n\n    return [replay_table]",
  "def make_dataset_iterator(\n      self,\n      reverb_client: reverb.Client,\n  ) -> Iterator[reverb.ReplaySample]:\n    \"\"\"Create a dataset iterator to use for learning/updating the agent.\"\"\"\n    # The dataset provides an interface to sample from replay.\n    dataset = datasets.make_reverb_dataset(\n        table=self._config.replay_table_name,\n        server_address=reverb_client.server_address,\n        batch_size=self._config.batch_size,\n        prefetch_size=self._config.prefetch_size)\n\n    # TODO(b/155086959): Fix type stubs and remove.\n    return iter(dataset)",
  "def make_adder(\n      self,\n      replay_client: reverb.Client,\n  ) -> adders.Adder:\n    \"\"\"Create an adder which records data generated by the actor/environment.\"\"\"\n    return reverb_adders.SequenceAdder(\n        client=replay_client,\n        sequence_length=self._config.sequence_length+1,\n        priority_fns={self._config.replay_table_name: lambda x: 1.},\n        period=self._config.sequence_length,\n        end_of_episode_behavior=reverb_adders.EndBehavior.CONTINUE,\n        )",
  "def make_actor(\n      self,\n      policy_network: snt.Module,\n      adder: Optional[adders.Adder] = None,\n      variable_source: Optional[core.VariableSource] = None,\n      deterministic_policy: Optional[bool] = False,\n  ):\n    \"\"\"Create an actor instance.\"\"\"\n    if variable_source:\n      # Create the variable client responsible for keeping the actor up-to-date.\n      variable_client = variable_utils.VariableClient(\n          client=variable_source,\n          variables={'policy': policy_network.variables},\n          update_period=1000,\n      )\n\n      # Make sure not to use a random policy after checkpoint restoration by\n      # assigning variables before running the environment loop.\n      variable_client.update_and_wait()\n\n    else:\n      variable_client = None\n\n    # Create the actor which defines how we take actions.\n    return acting.SVG0Actor(\n        policy_network=policy_network,\n        adder=adder,\n        variable_client=variable_client,\n        deterministic_policy=deterministic_policy\n    )",
  "def make_learner(\n      self,\n      networks: Tuple[SVG0Networks, SVG0Networks],\n      dataset: Iterator[reverb.ReplaySample],\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = False,\n  ):\n    \"\"\"Creates an instance of the learner.\"\"\"\n    online_networks, target_networks = networks\n\n    # The learner updates the parameters (and initializes them).\n    return learning.SVG0Learner(\n        policy_network=online_networks.policy_network,\n        critic_network=online_networks.critic_network,\n        target_policy_network=target_networks.policy_network,\n        target_critic_network=target_networks.critic_network,\n        prior_network=online_networks.prior_network,\n        target_prior_network=target_networks.prior_network,\n        policy_optimizer=self._config.policy_optimizer,\n        critic_optimizer=self._config.critic_optimizer,\n        prior_optimizer=self._config.prior_optimizer,\n        distillation_cost=self._config.distillation_cost,\n        entropy_regularizer_cost=self._config.entropy_regularizer_cost,\n        discount=self._config.discount,\n        target_update_period=self._config.target_update_period,\n        dataset_iterator=dataset,\n        counter=counter,\n        logger=logger,\n        checkpoint=checkpoint,\n    )",
  "def __init__(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy_network: snt.Module,\n      critic_network: snt.Module,\n      discount: float = 0.99,\n      batch_size: int = 256,\n      prefetch_size: int = 4,\n      target_update_period: int = 100,\n      prior_network: Optional[snt.Module] = None,\n      policy_optimizer: Optional[snt.Optimizer] = None,\n      critic_optimizer: Optional[snt.Optimizer] = None,\n      prior_optimizer: Optional[snt.Optimizer] = None,\n      distillation_cost: Optional[float] = 1e-3,\n      entropy_regularizer_cost: Optional[float] = 1e-3,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1000000,\n      samples_per_insert: float = 32.0,\n      sequence_length: int = 10,\n      sigma: float = 0.3,\n      replay_table_name: str = reverb_adders.DEFAULT_PRIORITY_TABLE,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = True,\n  ):\n    \"\"\"Initialize the agent.\n\n    Args:\n      environment_spec: description of the actions, observations, etc.\n      policy_network: the online (optimized) policy.\n      critic_network: the online critic.\n      discount: discount to use for TD updates.\n      batch_size: batch size for updates.\n      prefetch_size: size to prefetch from replay.\n      target_update_period: number of learner steps to perform before updating\n        the target networks.\n      prior_network: an optional `behavior prior` to regularize against.\n      policy_optimizer: optimizer for the policy network updates.\n      critic_optimizer: optimizer for the critic network updates.\n      prior_optimizer: optimizer for the prior network updates.\n      distillation_cost: a multiplier to be used when adding distillation\n        against the prior to the losses.\n      entropy_regularizer_cost: a multiplier used for per state sample based\n        entropy added to the actor loss.\n      min_replay_size: minimum replay size before updating.\n      max_replay_size: maximum replay size.\n      samples_per_insert: number of samples to take from replay for every insert\n        that is made.\n      sequence_length: number of timesteps to store for each trajectory.\n      sigma: standard deviation of zero-mean, Gaussian exploration noise.\n      replay_table_name: string indicating what name to give the replay table.\n      counter: counter object used to keep track of steps.\n      logger: logger object to be used by learner.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n    \"\"\"\n    # Create the Builder object which will internally create agent components.\n    builder = SVG0Builder(\n        # TODO(mwhoffman): pass the config dataclass in directly.\n        # TODO(mwhoffman): use the limiter rather than the workaround below.\n        # Right now this modifies min_replay_size and samples_per_insert so that\n        # they are not controlled by a limiter and are instead handled by the\n        # Agent base class (the above TODO directly references this behavior).\n        SVG0Config(\n            discount=discount,\n            batch_size=batch_size,\n            prefetch_size=prefetch_size,\n            target_update_period=target_update_period,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            prior_optimizer=prior_optimizer,\n            distillation_cost=distillation_cost,\n            entropy_regularizer_cost=entropy_regularizer_cost,\n            min_replay_size=1,  # Let the Agent class handle this.\n            max_replay_size=max_replay_size,\n            samples_per_insert=None,  # Let the Agent class handle this.\n            sequence_length=sequence_length,\n            sigma=sigma,\n            replay_table_name=replay_table_name,\n        ))\n\n    # TODO(mwhoffman): pass the network dataclass in directly.\n    online_networks = SVG0Networks(policy_network=policy_network,\n                                   critic_network=critic_network,\n                                   prior_network=prior_network,)\n\n    # Target networks are just a copy of the online networks.\n    target_networks = copy.deepcopy(online_networks)\n\n    # Initialize the networks.\n    online_networks.init(environment_spec)\n    target_networks.init(environment_spec)\n\n    # TODO(mwhoffman): either make this Dataclass or pass only one struct.\n    # The network struct passed to make_learner is just a tuple for the\n    # time-being (for backwards compatibility).\n    networks = (online_networks, target_networks)\n\n    # Create the behavior policy.\n    policy_network = online_networks.make_policy()\n\n    # Create the replay server and grab its address.\n    replay_tables = builder.make_replay_tables(environment_spec,\n                                               sequence_length)\n    replay_server = reverb.Server(replay_tables, port=None)\n    replay_client = reverb.Client(f'localhost:{replay_server.port}')\n\n    # Create actor, dataset, and learner for generating, storing, and consuming\n    # data respectively.\n    adder = builder.make_adder(replay_client)\n    actor = builder.make_actor(policy_network, adder)\n    dataset = builder.make_dataset_iterator(replay_client)\n    learner = builder.make_learner(networks, dataset, counter, logger,\n                                   checkpoint)\n\n    super().__init__(\n        actor=actor,\n        learner=learner,\n        min_observations=max(batch_size, min_replay_size),\n        observations_per_step=float(batch_size) / samples_per_insert)\n\n    # Save the replay so we don't garbage collect it.\n    self._replay_server = replay_server",
  "class SVG0Learner(acme.Learner):\n  \"\"\"SVG0 learner with optional prior.\n\n  This is the learning component of an SVG0 agent. IE it takes a dataset as\n  input and implements update functionality to learn from this dataset.\n  \"\"\"\n\n  def __init__(\n      self,\n      policy_network: snt.Module,\n      critic_network: snt.Module,\n      target_policy_network: snt.Module,\n      target_critic_network: snt.Module,\n      discount: float,\n      target_update_period: int,\n      dataset_iterator: Iterator[reverb.ReplaySample],\n      prior_network: Optional[snt.Module] = None,\n      target_prior_network: Optional[snt.Module] = None,\n      policy_optimizer: Optional[snt.Optimizer] = None,\n      critic_optimizer: Optional[snt.Optimizer] = None,\n      prior_optimizer: Optional[snt.Optimizer] = None,\n      distillation_cost: Optional[float] = 1e-3,\n      entropy_regularizer_cost: Optional[float] = 1e-3,\n      num_action_samples: int = 10,\n      lambda_: float = 1.0,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = True,\n  ):\n    \"\"\"Initializes the learner.\n\n    Args:\n      policy_network: the online (optimized) policy.\n      critic_network: the online critic.\n      target_policy_network: the target policy (which lags behind the online\n        policy).\n      target_critic_network: the target critic.\n      discount: discount to use for TD updates.\n      target_update_period: number of learner steps to perform before updating\n        the target networks.\n      dataset_iterator: dataset to learn from, whether fixed or from a replay\n        buffer (see `acme.datasets.reverb.make_reverb_dataset` documentation).\n      prior_network: the online (optimized) prior.\n      target_prior_network: the target prior (which lags behind the online\n        prior).\n      policy_optimizer: the optimizer to be applied to the SVG-0 (policy) loss.\n      critic_optimizer: the optimizer to be applied to the distributional\n        Bellman loss.\n      prior_optimizer: the optimizer to be applied to the prior (distillation)\n        loss.\n      distillation_cost: a multiplier to be used when adding distillation\n        against the prior to the losses.\n      entropy_regularizer_cost: a multiplier used for per state sample based\n        entropy added to the actor loss.\n      num_action_samples: the number of action samples to use for estimating the\n        value function and sample based entropy.\n      lambda_: the `lambda` value to be used with retrace.\n      counter: counter object used to keep track of steps.\n      logger: logger object to be used by learner.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n    \"\"\"\n\n    # Store online and target networks.\n    self._policy_network = policy_network\n    self._critic_network = critic_network\n    self._target_policy_network = target_policy_network\n    self._target_critic_network = target_critic_network\n\n    self._prior_network = prior_network\n    self._target_prior_network = target_prior_network\n\n    self._lambda = lambda_\n    self._num_action_samples = num_action_samples\n    self._distillation_cost = distillation_cost\n    self._entropy_regularizer_cost = entropy_regularizer_cost\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger('learner')\n\n    # Other learner parameters.\n    self._discount = discount\n\n    # Necessary to track when to update target networks.\n    self._num_steps = tf.Variable(0, dtype=tf.int32)\n    self._target_update_period = target_update_period\n\n    # Batch dataset and create iterator.\n    self._iterator = dataset_iterator\n\n    # Create optimizers if they aren't given.\n    self._critic_optimizer = critic_optimizer or snt.optimizers.Adam(1e-4)\n    self._policy_optimizer = policy_optimizer or snt.optimizers.Adam(1e-4)\n    self._prior_optimizer = prior_optimizer or snt.optimizers.Adam(1e-4)\n\n    # Expose the variables.\n    self._variables = {\n        'critic': self._critic_network.variables,\n        'policy': self._policy_network.variables,\n    }\n    if self._prior_network is not None:\n      self._variables['prior'] = self._prior_network.variables\n\n    # Create a checkpointer and snapshotter objects.\n    self._checkpointer = None\n    self._snapshotter = None\n\n    if checkpoint:\n      objects_to_save = {\n          'counter': self._counter,\n          'policy': self._policy_network,\n          'critic': self._critic_network,\n          'target_policy': self._target_policy_network,\n          'target_critic': self._target_critic_network,\n          'policy_optimizer': self._policy_optimizer,\n          'critic_optimizer': self._critic_optimizer,\n          'num_steps': self._num_steps,\n      }\n      if self._prior_network is not None:\n        objects_to_save['prior'] = self._prior_network\n        objects_to_save['target_prior'] = self._target_prior_network\n        objects_to_save['prior_optimizer'] = self._prior_optimizer\n\n      self._checkpointer = tf2_savers.Checkpointer(\n          subdirectory='svg0_learner',\n          objects_to_save=objects_to_save)\n      objects_to_snapshot = {\n          'policy': self._policy_network,\n          'critic': self._critic_network,\n      }\n      if self._prior_network is not None:\n        objects_to_snapshot['prior'] = self._prior_network\n\n      self._snapshotter = tf2_savers.Snapshotter(\n          objects_to_save=objects_to_snapshot)\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None\n\n  @tf.function\n  def _step(self) -> Dict[str, tf.Tensor]:\n    # Update target network\n    online_variables = [\n        *self._critic_network.variables,\n        *self._policy_network.variables,\n    ]\n    if self._prior_network is not None:\n      online_variables += [*self._prior_network.variables]\n    online_variables = tuple(online_variables)\n\n    target_variables = [\n        *self._target_critic_network.variables,\n        *self._target_policy_network.variables,\n    ]\n    if self._prior_network is not None:\n      target_variables += [*self._target_prior_network.variables]\n    target_variables = tuple(target_variables)\n\n    # Make online -> target network update ops.\n    if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n      for src, dest in zip(online_variables, target_variables):\n        dest.assign(src)\n    self._num_steps.assign_add(1)\n\n    # Get data from replay (dropping extras if any) and flip to `[T, B, ...]`.\n    sample: reverb.ReplaySample = next(self._iterator)\n    data = tf2_utils.batch_to_sequence(sample.data)\n    observations, actions, rewards, discounts, extra = (data.observation,\n                                                        data.action,\n                                                        data.reward,\n                                                        data.discount,\n                                                        data.extras)\n    online_target_pi_q = svg0_utils.OnlineTargetPiQ(\n        online_pi=self._policy_network,\n        online_q=self._critic_network,\n        target_pi=self._target_policy_network,\n        target_q=self._target_critic_network,\n        num_samples=self._num_action_samples,\n        online_prior=self._prior_network,\n        target_prior=self._target_prior_network,\n    )\n    with tf.GradientTape(persistent=True) as tape:\n      step_outputs = svg0_utils.static_rnn(\n          core=online_target_pi_q,\n          inputs=(observations, actions),\n          unroll_length=rewards.shape[0])\n\n      # Flip target samples to have shape [S, T+1, B, ...] where 'S' is the\n      # number of action samples taken.\n      target_pi_samples = tf2_utils.batch_to_sequence(\n          step_outputs.target_samples)\n      # Tile observations to have shape [S, T+1, B,..].\n      tiled_observations = tf2_utils.tile_nested(observations,\n                                                 self._num_action_samples)\n\n      # Finally compute target Q values on the new action samples.\n      # Shape: [S, T+1, B, 1]\n      target_q_target_pi_samples = snt.BatchApply(self._target_critic_network,\n                                                  3)(tiled_observations,\n                                                     target_pi_samples)\n      # Compute the value estimate by averaging over the action dimension.\n      # Shape: [T+1, B, 1].\n      target_v_target_pi = tf.reduce_mean(target_q_target_pi_samples, axis=0)\n\n      # Split the target V's into the target for learning\n      # `value_function_target` and the bootstrap value. Shape: [T, B].\n      value_function_target = tf.squeeze(target_v_target_pi[:-1], axis=-1)\n      # Shape: [B].\n      bootstrap_value = tf.squeeze(target_v_target_pi[-1], axis=-1)\n\n      # When learning with a prior, add entropy terms to value targets.\n      if self._prior_network is not None:\n        value_function_target -= self._distillation_cost * tf.stop_gradient(\n            step_outputs.analytic_kl_to_target[:-1]\n            )\n        bootstrap_value -= self._distillation_cost * tf.stop_gradient(\n            step_outputs.analytic_kl_to_target[-1])\n\n      # Get target log probs and behavior log probs from rollout.\n      # Shape: [T+1, B].\n      target_log_probs_behavior_actions = (\n          step_outputs.target_log_probs_behavior_actions)\n      behavior_log_probs = extra['log_prob']\n      # Calculate importance weights. Shape: [T+1, B].\n      rhos = tf.exp(target_log_probs_behavior_actions - behavior_log_probs)\n\n      # Filter the importance weights to mask out episode restarts. Ignore the\n      # last action and consider the step type of the next step for masking.\n      # Shape: [T, B].\n      episode_start_mask = tf2_utils.batch_to_sequence(\n          sample.data.start_of_episode)[1:]\n\n      rhos = svg0_utils.mask_out_restarting(rhos[:-1], episode_start_mask)\n\n      # rhos = rhos[:-1]\n      # Compute the log importance weights with a small value added for\n      # stability.\n      # Shape: [T, B]\n      log_rhos = tf.math.log(rhos + _MIN_LOG_VAL)\n\n      # Retrieve the target and online Q values and throw away the last action.\n      # Shape: [T, B].\n      target_q_values = tf.squeeze(step_outputs.target_q[:-1], -1)\n      online_q_values = tf.squeeze(step_outputs.online_q[:-1], -1)\n\n      # Flip target samples to have shape [S, T+1, B, ...] where 'S' is the\n      # number of action samples taken.\n      online_pi_samples = tf2_utils.batch_to_sequence(\n          step_outputs.online_samples)\n      target_q_online_pi_samples = snt.BatchApply(self._target_critic_network,\n                                                  3)(tiled_observations,\n                                                     online_pi_samples)\n      expected_q = tf.reduce_mean(\n          tf.squeeze(target_q_online_pi_samples, -1), axis=0)\n\n      # Flip online_log_probs to be of shape [S, T+1, B] and then compute\n      # entropy by averaging over num samples. Final shape: [T+1, B].\n      online_log_probs = tf2_utils.batch_to_sequence(\n          step_outputs.online_log_probs)\n      sample_based_entropy = tf.reduce_mean(-online_log_probs, axis=0)\n      retrace_outputs = continuous_retrace_ops.retrace_from_importance_weights(\n          log_rhos=log_rhos,\n          discounts=self._discount * discounts[:-1],\n          rewards=rewards[:-1],\n          q_values=target_q_values,\n          values=value_function_target,\n          bootstrap_value=bootstrap_value,\n          lambda_=self._lambda,\n      )\n\n      # Critic loss. Shape: [T, B].\n      critic_loss = 0.5 * tf.math.squared_difference(\n          tf.stop_gradient(retrace_outputs.qs), online_q_values)\n\n      # Policy loss- SVG0 with sample based entropy. Shape: [T, B]\n      policy_loss = -(\n          expected_q + self._entropy_regularizer_cost * sample_based_entropy)\n      policy_loss = policy_loss[:-1]\n\n      if self._prior_network is not None:\n        # When training the prior, also add the per-timestep KL cost.\n        policy_loss += (\n            self._distillation_cost * step_outputs.analytic_kl_to_target[:-1])\n\n      # Ensure episode restarts are masked out when computing the losses.\n      critic_loss = svg0_utils.mask_out_restarting(critic_loss,\n                                                   episode_start_mask)\n      critic_loss = tf.reduce_mean(critic_loss)\n\n      policy_loss = svg0_utils.mask_out_restarting(policy_loss,\n                                                   episode_start_mask)\n      policy_loss = tf.reduce_mean(policy_loss)\n\n      if self._prior_network is not None:\n        prior_loss = step_outputs.analytic_kl_divergence[:-1]\n        prior_loss = svg0_utils.mask_out_restarting(prior_loss,\n                                                    episode_start_mask)\n        prior_loss = tf.reduce_mean(prior_loss)\n\n    # Get trainable variables.\n    policy_variables = self._policy_network.trainable_variables\n    critic_variables = self._critic_network.trainable_variables\n\n    # Compute gradients.\n    policy_gradients = tape.gradient(policy_loss, policy_variables)\n    critic_gradients = tape.gradient(critic_loss, critic_variables)\n    if self._prior_network is not None:\n      prior_variables = self._prior_network.trainable_variables\n      prior_gradients = tape.gradient(prior_loss, prior_variables)\n\n    # Delete the tape manually because of the persistent=True flag.\n    del tape\n\n    # Apply gradients.\n    self._policy_optimizer.apply(policy_gradients, policy_variables)\n    self._critic_optimizer.apply(critic_gradients, critic_variables)\n    losses = {\n        'critic_loss': critic_loss,\n        'policy_loss': policy_loss,\n    }\n\n    if self._prior_network is not None:\n      self._prior_optimizer.apply(prior_gradients, prior_variables)\n      losses['prior_loss'] = prior_loss\n\n    # Losses to track.\n    return losses\n\n  def step(self):\n    # Run the learning step.\n    fetches = self._step()\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n    fetches.update(counts)\n\n    # Checkpoint and attempt to write the logs.\n    if self._checkpointer is not None:\n      self._checkpointer.save()\n    if self._snapshotter is not None:\n      self._snapshotter.save()\n    self._logger.write(fetches)\n\n  def get_variables(self, names: List[str]) -> List[List[np.ndarray]]:\n    return [tf2_utils.to_numpy(self._variables[name]) for name in names]",
  "def __init__(\n      self,\n      policy_network: snt.Module,\n      critic_network: snt.Module,\n      target_policy_network: snt.Module,\n      target_critic_network: snt.Module,\n      discount: float,\n      target_update_period: int,\n      dataset_iterator: Iterator[reverb.ReplaySample],\n      prior_network: Optional[snt.Module] = None,\n      target_prior_network: Optional[snt.Module] = None,\n      policy_optimizer: Optional[snt.Optimizer] = None,\n      critic_optimizer: Optional[snt.Optimizer] = None,\n      prior_optimizer: Optional[snt.Optimizer] = None,\n      distillation_cost: Optional[float] = 1e-3,\n      entropy_regularizer_cost: Optional[float] = 1e-3,\n      num_action_samples: int = 10,\n      lambda_: float = 1.0,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = True,\n  ):\n    \"\"\"Initializes the learner.\n\n    Args:\n      policy_network: the online (optimized) policy.\n      critic_network: the online critic.\n      target_policy_network: the target policy (which lags behind the online\n        policy).\n      target_critic_network: the target critic.\n      discount: discount to use for TD updates.\n      target_update_period: number of learner steps to perform before updating\n        the target networks.\n      dataset_iterator: dataset to learn from, whether fixed or from a replay\n        buffer (see `acme.datasets.reverb.make_reverb_dataset` documentation).\n      prior_network: the online (optimized) prior.\n      target_prior_network: the target prior (which lags behind the online\n        prior).\n      policy_optimizer: the optimizer to be applied to the SVG-0 (policy) loss.\n      critic_optimizer: the optimizer to be applied to the distributional\n        Bellman loss.\n      prior_optimizer: the optimizer to be applied to the prior (distillation)\n        loss.\n      distillation_cost: a multiplier to be used when adding distillation\n        against the prior to the losses.\n      entropy_regularizer_cost: a multiplier used for per state sample based\n        entropy added to the actor loss.\n      num_action_samples: the number of action samples to use for estimating the\n        value function and sample based entropy.\n      lambda_: the `lambda` value to be used with retrace.\n      counter: counter object used to keep track of steps.\n      logger: logger object to be used by learner.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n    \"\"\"\n\n    # Store online and target networks.\n    self._policy_network = policy_network\n    self._critic_network = critic_network\n    self._target_policy_network = target_policy_network\n    self._target_critic_network = target_critic_network\n\n    self._prior_network = prior_network\n    self._target_prior_network = target_prior_network\n\n    self._lambda = lambda_\n    self._num_action_samples = num_action_samples\n    self._distillation_cost = distillation_cost\n    self._entropy_regularizer_cost = entropy_regularizer_cost\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger('learner')\n\n    # Other learner parameters.\n    self._discount = discount\n\n    # Necessary to track when to update target networks.\n    self._num_steps = tf.Variable(0, dtype=tf.int32)\n    self._target_update_period = target_update_period\n\n    # Batch dataset and create iterator.\n    self._iterator = dataset_iterator\n\n    # Create optimizers if they aren't given.\n    self._critic_optimizer = critic_optimizer or snt.optimizers.Adam(1e-4)\n    self._policy_optimizer = policy_optimizer or snt.optimizers.Adam(1e-4)\n    self._prior_optimizer = prior_optimizer or snt.optimizers.Adam(1e-4)\n\n    # Expose the variables.\n    self._variables = {\n        'critic': self._critic_network.variables,\n        'policy': self._policy_network.variables,\n    }\n    if self._prior_network is not None:\n      self._variables['prior'] = self._prior_network.variables\n\n    # Create a checkpointer and snapshotter objects.\n    self._checkpointer = None\n    self._snapshotter = None\n\n    if checkpoint:\n      objects_to_save = {\n          'counter': self._counter,\n          'policy': self._policy_network,\n          'critic': self._critic_network,\n          'target_policy': self._target_policy_network,\n          'target_critic': self._target_critic_network,\n          'policy_optimizer': self._policy_optimizer,\n          'critic_optimizer': self._critic_optimizer,\n          'num_steps': self._num_steps,\n      }\n      if self._prior_network is not None:\n        objects_to_save['prior'] = self._prior_network\n        objects_to_save['target_prior'] = self._target_prior_network\n        objects_to_save['prior_optimizer'] = self._prior_optimizer\n\n      self._checkpointer = tf2_savers.Checkpointer(\n          subdirectory='svg0_learner',\n          objects_to_save=objects_to_save)\n      objects_to_snapshot = {\n          'policy': self._policy_network,\n          'critic': self._critic_network,\n      }\n      if self._prior_network is not None:\n        objects_to_snapshot['prior'] = self._prior_network\n\n      self._snapshotter = tf2_savers.Snapshotter(\n          objects_to_save=objects_to_snapshot)\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None",
  "def _step(self) -> Dict[str, tf.Tensor]:\n    # Update target network\n    online_variables = [\n        *self._critic_network.variables,\n        *self._policy_network.variables,\n    ]\n    if self._prior_network is not None:\n      online_variables += [*self._prior_network.variables]\n    online_variables = tuple(online_variables)\n\n    target_variables = [\n        *self._target_critic_network.variables,\n        *self._target_policy_network.variables,\n    ]\n    if self._prior_network is not None:\n      target_variables += [*self._target_prior_network.variables]\n    target_variables = tuple(target_variables)\n\n    # Make online -> target network update ops.\n    if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n      for src, dest in zip(online_variables, target_variables):\n        dest.assign(src)\n    self._num_steps.assign_add(1)\n\n    # Get data from replay (dropping extras if any) and flip to `[T, B, ...]`.\n    sample: reverb.ReplaySample = next(self._iterator)\n    data = tf2_utils.batch_to_sequence(sample.data)\n    observations, actions, rewards, discounts, extra = (data.observation,\n                                                        data.action,\n                                                        data.reward,\n                                                        data.discount,\n                                                        data.extras)\n    online_target_pi_q = svg0_utils.OnlineTargetPiQ(\n        online_pi=self._policy_network,\n        online_q=self._critic_network,\n        target_pi=self._target_policy_network,\n        target_q=self._target_critic_network,\n        num_samples=self._num_action_samples,\n        online_prior=self._prior_network,\n        target_prior=self._target_prior_network,\n    )\n    with tf.GradientTape(persistent=True) as tape:\n      step_outputs = svg0_utils.static_rnn(\n          core=online_target_pi_q,\n          inputs=(observations, actions),\n          unroll_length=rewards.shape[0])\n\n      # Flip target samples to have shape [S, T+1, B, ...] where 'S' is the\n      # number of action samples taken.\n      target_pi_samples = tf2_utils.batch_to_sequence(\n          step_outputs.target_samples)\n      # Tile observations to have shape [S, T+1, B,..].\n      tiled_observations = tf2_utils.tile_nested(observations,\n                                                 self._num_action_samples)\n\n      # Finally compute target Q values on the new action samples.\n      # Shape: [S, T+1, B, 1]\n      target_q_target_pi_samples = snt.BatchApply(self._target_critic_network,\n                                                  3)(tiled_observations,\n                                                     target_pi_samples)\n      # Compute the value estimate by averaging over the action dimension.\n      # Shape: [T+1, B, 1].\n      target_v_target_pi = tf.reduce_mean(target_q_target_pi_samples, axis=0)\n\n      # Split the target V's into the target for learning\n      # `value_function_target` and the bootstrap value. Shape: [T, B].\n      value_function_target = tf.squeeze(target_v_target_pi[:-1], axis=-1)\n      # Shape: [B].\n      bootstrap_value = tf.squeeze(target_v_target_pi[-1], axis=-1)\n\n      # When learning with a prior, add entropy terms to value targets.\n      if self._prior_network is not None:\n        value_function_target -= self._distillation_cost * tf.stop_gradient(\n            step_outputs.analytic_kl_to_target[:-1]\n            )\n        bootstrap_value -= self._distillation_cost * tf.stop_gradient(\n            step_outputs.analytic_kl_to_target[-1])\n\n      # Get target log probs and behavior log probs from rollout.\n      # Shape: [T+1, B].\n      target_log_probs_behavior_actions = (\n          step_outputs.target_log_probs_behavior_actions)\n      behavior_log_probs = extra['log_prob']\n      # Calculate importance weights. Shape: [T+1, B].\n      rhos = tf.exp(target_log_probs_behavior_actions - behavior_log_probs)\n\n      # Filter the importance weights to mask out episode restarts. Ignore the\n      # last action and consider the step type of the next step for masking.\n      # Shape: [T, B].\n      episode_start_mask = tf2_utils.batch_to_sequence(\n          sample.data.start_of_episode)[1:]\n\n      rhos = svg0_utils.mask_out_restarting(rhos[:-1], episode_start_mask)\n\n      # rhos = rhos[:-1]\n      # Compute the log importance weights with a small value added for\n      # stability.\n      # Shape: [T, B]\n      log_rhos = tf.math.log(rhos + _MIN_LOG_VAL)\n\n      # Retrieve the target and online Q values and throw away the last action.\n      # Shape: [T, B].\n      target_q_values = tf.squeeze(step_outputs.target_q[:-1], -1)\n      online_q_values = tf.squeeze(step_outputs.online_q[:-1], -1)\n\n      # Flip target samples to have shape [S, T+1, B, ...] where 'S' is the\n      # number of action samples taken.\n      online_pi_samples = tf2_utils.batch_to_sequence(\n          step_outputs.online_samples)\n      target_q_online_pi_samples = snt.BatchApply(self._target_critic_network,\n                                                  3)(tiled_observations,\n                                                     online_pi_samples)\n      expected_q = tf.reduce_mean(\n          tf.squeeze(target_q_online_pi_samples, -1), axis=0)\n\n      # Flip online_log_probs to be of shape [S, T+1, B] and then compute\n      # entropy by averaging over num samples. Final shape: [T+1, B].\n      online_log_probs = tf2_utils.batch_to_sequence(\n          step_outputs.online_log_probs)\n      sample_based_entropy = tf.reduce_mean(-online_log_probs, axis=0)\n      retrace_outputs = continuous_retrace_ops.retrace_from_importance_weights(\n          log_rhos=log_rhos,\n          discounts=self._discount * discounts[:-1],\n          rewards=rewards[:-1],\n          q_values=target_q_values,\n          values=value_function_target,\n          bootstrap_value=bootstrap_value,\n          lambda_=self._lambda,\n      )\n\n      # Critic loss. Shape: [T, B].\n      critic_loss = 0.5 * tf.math.squared_difference(\n          tf.stop_gradient(retrace_outputs.qs), online_q_values)\n\n      # Policy loss- SVG0 with sample based entropy. Shape: [T, B]\n      policy_loss = -(\n          expected_q + self._entropy_regularizer_cost * sample_based_entropy)\n      policy_loss = policy_loss[:-1]\n\n      if self._prior_network is not None:\n        # When training the prior, also add the per-timestep KL cost.\n        policy_loss += (\n            self._distillation_cost * step_outputs.analytic_kl_to_target[:-1])\n\n      # Ensure episode restarts are masked out when computing the losses.\n      critic_loss = svg0_utils.mask_out_restarting(critic_loss,\n                                                   episode_start_mask)\n      critic_loss = tf.reduce_mean(critic_loss)\n\n      policy_loss = svg0_utils.mask_out_restarting(policy_loss,\n                                                   episode_start_mask)\n      policy_loss = tf.reduce_mean(policy_loss)\n\n      if self._prior_network is not None:\n        prior_loss = step_outputs.analytic_kl_divergence[:-1]\n        prior_loss = svg0_utils.mask_out_restarting(prior_loss,\n                                                    episode_start_mask)\n        prior_loss = tf.reduce_mean(prior_loss)\n\n    # Get trainable variables.\n    policy_variables = self._policy_network.trainable_variables\n    critic_variables = self._critic_network.trainable_variables\n\n    # Compute gradients.\n    policy_gradients = tape.gradient(policy_loss, policy_variables)\n    critic_gradients = tape.gradient(critic_loss, critic_variables)\n    if self._prior_network is not None:\n      prior_variables = self._prior_network.trainable_variables\n      prior_gradients = tape.gradient(prior_loss, prior_variables)\n\n    # Delete the tape manually because of the persistent=True flag.\n    del tape\n\n    # Apply gradients.\n    self._policy_optimizer.apply(policy_gradients, policy_variables)\n    self._critic_optimizer.apply(critic_gradients, critic_variables)\n    losses = {\n        'critic_loss': critic_loss,\n        'policy_loss': policy_loss,\n    }\n\n    if self._prior_network is not None:\n      self._prior_optimizer.apply(prior_gradients, prior_variables)\n      losses['prior_loss'] = prior_loss\n\n    # Losses to track.\n    return losses",
  "def step(self):\n    # Run the learning step.\n    fetches = self._step()\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n    fetches.update(counts)\n\n    # Checkpoint and attempt to write the logs.\n    if self._checkpointer is not None:\n      self._checkpointer.save()\n    if self._snapshotter is not None:\n      self._snapshotter.save()\n    self._logger.write(fetches)",
  "def get_variables(self, names: List[str]) -> List[List[np.ndarray]]:\n    return [tf2_utils.to_numpy(self._variables[name]) for name in names]",
  "class OnlineTargetPiQ(snt.Module):\n  \"\"\"Core to unroll online and target policies and Q functions at once.\n\n  A core that runs online and target policies and Q functions. This can be more\n  efficient if the core needs to be unrolled across time and called many times.\n  \"\"\"\n\n  def __init__(self,\n               online_pi: snt.Module,\n               online_q: snt.Module,\n               target_pi: snt.Module,\n               target_q: snt.Module,\n               num_samples: int,\n               online_prior: Optional[snt.Module] = None,\n               target_prior: Optional[snt.Module] = None,\n               name='OnlineTargetPiQ'):\n    super().__init__(name)\n\n    self._online_pi = online_pi\n    self._target_pi = target_pi\n    self._online_q = online_q\n    self._target_q = target_q\n    self._online_prior = online_prior\n    self._target_prior = target_prior\n\n    self._num_samples = num_samples\n    output_list = [\n        'online_samples', 'target_samples', 'target_log_probs_behavior_actions',\n        'online_log_probs', 'online_q', 'target_q'\n    ]\n    if online_prior is not None:\n      output_list += ['analytic_kl_divergence', 'analytic_kl_to_target']\n    self._output_tuple = collections.namedtuple(\n        'OnlineTargetPiQ', output_list)\n\n  def __call__(self, input_obs_and_action: Tuple[tf.Tensor, tf.Tensor]):\n    (obs, action) = input_obs_and_action\n    online_pi_dist = self._online_pi(obs)\n    target_pi_dist = self._target_pi(obs)\n\n    online_samples = online_pi_dist.sample(self._num_samples)\n    target_samples = target_pi_dist.sample(self._num_samples)\n    target_log_probs_behavior_actions = target_pi_dist.log_prob(action)\n\n    online_log_probs = online_pi_dist.log_prob(tf.stop_gradient(online_samples))\n\n    online_q_out = self._online_q(obs, action)\n    target_q_out = self._target_q(obs, action)\n\n    output_list = [\n        online_samples, target_samples, target_log_probs_behavior_actions,\n        online_log_probs, online_q_out, target_q_out\n    ]\n\n    if self._online_prior is not None:\n      prior_dist = self._online_prior(obs)\n      target_prior_dist = self._target_prior(obs)\n      analytic_kl_divergence = online_pi_dist.kl_divergence(prior_dist)\n      analytic_kl_to_target = online_pi_dist.kl_divergence(target_prior_dist)\n\n      output_list += [analytic_kl_divergence, analytic_kl_to_target]\n    output = self._output_tuple(*output_list)\n    return output",
  "def static_rnn(core: snt.Module, inputs: types.NestedTensor,\n               unroll_length: int):\n  \"\"\"Unroll core along inputs for unroll_length steps.\n\n  Note: for time-major input tensors whose leading dimension is less than\n  unroll_length, `None` would be provided instead.\n\n  Args:\n    core: an instance of snt.Module.\n    inputs: a `nest` of time-major input tensors.\n    unroll_length: number of time steps to unroll.\n\n  Returns:\n    step_outputs: a `nest` of time-major stacked output tensors of length\n      `unroll_length`.\n  \"\"\"\n  step_outputs = []\n  for time_dim in range(unroll_length):\n    inputs_t = tree.map_structure(\n        lambda t, i_=time_dim: t[i_] if i_ < t.shape[0] else None, inputs)\n    step_output = core(inputs_t)\n    step_outputs.append(step_output)\n\n  step_outputs = _nest_stack(step_outputs)\n  return step_outputs",
  "def mask_out_restarting(tensor: tf.Tensor, start_of_episode: tf.Tensor):\n  \"\"\"Mask out `tensor` taken on the step that resets the environment.\n\n  Args:\n    tensor: a time-major 2-D `Tensor` of shape [T, B].\n    start_of_episode: a 2-D `Tensor` of shape [T, B] that contains the points\n      where the episode restarts.\n\n  Returns:\n    tensor of shape [T, B] with elements are masked out according to step_types,\n    restarting weights of shape [T, B]\n  \"\"\"\n  tensor.get_shape().assert_has_rank(2)\n  start_of_episode.get_shape().assert_has_rank(2)\n  weights = tf.cast(~start_of_episode, dtype=tf.float32)\n  masked_tensor = tensor * weights\n  return masked_tensor",
  "def batch_concat_selection(observation_dict: Dict[str, types.NestedTensor],\n                           concat_keys: Optional[Iterable[str]] = None,\n                           output_dtype=tf.float32) -> tf.Tensor:\n  \"\"\"Concatenate a dict of observations into 2-D tensors.\"\"\"\n  concat_keys = concat_keys or sorted(observation_dict.keys())\n  to_concat = []\n  for obs in concat_keys:\n    if obs not in observation_dict:\n      raise KeyError(\n          'Missing observation. Requested: {} (available: {})'.format(\n              obs, list(observation_dict.keys())))\n    to_concat.append(tf.cast(observation_dict[obs], output_dtype))\n\n  return tf2_utils.batch_concat(to_concat)",
  "def _nest_stack(list_of_nests, axis=0):\n  \"\"\"Convert a list of nests to a nest of stacked lists.\"\"\"\n  return tree.map_structure(lambda *ts: tf.stack(ts, axis=axis), *list_of_nests)",
  "def __init__(self,\n               online_pi: snt.Module,\n               online_q: snt.Module,\n               target_pi: snt.Module,\n               target_q: snt.Module,\n               num_samples: int,\n               online_prior: Optional[snt.Module] = None,\n               target_prior: Optional[snt.Module] = None,\n               name='OnlineTargetPiQ'):\n    super().__init__(name)\n\n    self._online_pi = online_pi\n    self._target_pi = target_pi\n    self._online_q = online_q\n    self._target_q = target_q\n    self._online_prior = online_prior\n    self._target_prior = target_prior\n\n    self._num_samples = num_samples\n    output_list = [\n        'online_samples', 'target_samples', 'target_log_probs_behavior_actions',\n        'online_log_probs', 'online_q', 'target_q'\n    ]\n    if online_prior is not None:\n      output_list += ['analytic_kl_divergence', 'analytic_kl_to_target']\n    self._output_tuple = collections.namedtuple(\n        'OnlineTargetPiQ', output_list)",
  "def __call__(self, input_obs_and_action: Tuple[tf.Tensor, tf.Tensor]):\n    (obs, action) = input_obs_and_action\n    online_pi_dist = self._online_pi(obs)\n    target_pi_dist = self._target_pi(obs)\n\n    online_samples = online_pi_dist.sample(self._num_samples)\n    target_samples = target_pi_dist.sample(self._num_samples)\n    target_log_probs_behavior_actions = target_pi_dist.log_prob(action)\n\n    online_log_probs = online_pi_dist.log_prob(tf.stop_gradient(online_samples))\n\n    online_q_out = self._online_q(obs, action)\n    target_q_out = self._target_q(obs, action)\n\n    output_list = [\n        online_samples, target_samples, target_log_probs_behavior_actions,\n        online_log_probs, online_q_out, target_q_out\n    ]\n\n    if self._online_prior is not None:\n      prior_dist = self._online_prior(obs)\n      target_prior_dist = self._target_prior(obs)\n      analytic_kl_divergence = online_pi_dist.kl_divergence(prior_dist)\n      analytic_kl_to_target = online_pi_dist.kl_divergence(target_prior_dist)\n\n      output_list += [analytic_kl_divergence, analytic_kl_to_target]\n    output = self._output_tuple(*output_list)\n    return output",
  "def make_default_networks(\n    action_spec: specs.BoundedArray,\n    policy_layer_sizes: Sequence[int] = (256, 256, 256),\n    critic_layer_sizes: Sequence[int] = (512, 512, 256),\n) -> Mapping[str, types.TensorTransformation]:\n  \"\"\"Creates networks used by the agent.\"\"\"\n\n  # Get total number of action dimensions from action spec.\n  num_dimensions = np.prod(action_spec.shape, dtype=int)\n\n  policy_network = snt.Sequential([\n      tf2_utils.batch_concat,\n      networks.LayerNormMLP(policy_layer_sizes, activate_final=True),\n      networks.MultivariateNormalDiagHead(\n          num_dimensions,\n          tanh_mean=True,\n          min_scale=0.3,\n          init_scale=0.7,\n          fixed_scale=False,\n          use_tfd_independent=False)\n  ])\n  # The multiplexer concatenates the (maybe transformed) observations/actions.\n  multiplexer = networks.CriticMultiplexer(\n      action_network=networks.ClipToSpec(action_spec))\n  critic_network = snt.Sequential([\n      multiplexer,\n      networks.LayerNormMLP(critic_layer_sizes, activate_final=True),\n      networks.NearZeroInitializedLinear(1),\n  ])\n\n  return {\n      \"policy\": policy_network,\n      \"critic\": critic_network,\n  }",
  "def make_network_with_prior(\n    action_spec: specs.BoundedArray,\n    policy_layer_sizes: Sequence[int] = (200, 100),\n    critic_layer_sizes: Sequence[int] = (400, 300),\n    prior_layer_sizes: Sequence[int] = (200, 100),\n    policy_keys: Optional[Sequence[str]] = None,\n    prior_keys: Optional[Sequence[str]] = None,\n) -> Mapping[str, types.TensorTransformation]:\n  \"\"\"Creates networks used by the agent.\"\"\"\n\n  # Get total number of action dimensions from action spec.\n  num_dimensions = np.prod(action_spec.shape, dtype=int)\n  flatten_concat_policy = functools.partial(\n      svg0_utils.batch_concat_selection, concat_keys=policy_keys)\n  flatten_concat_prior = functools.partial(\n      svg0_utils.batch_concat_selection, concat_keys=prior_keys)\n\n  policy_network = snt.Sequential([\n      flatten_concat_policy,\n      networks.LayerNormMLP(policy_layer_sizes, activate_final=True),\n      networks.MultivariateNormalDiagHead(\n          num_dimensions,\n          tanh_mean=True,\n          min_scale=0.1,\n          init_scale=0.7,\n          fixed_scale=False,\n          use_tfd_independent=False)\n  ])\n  # The multiplexer concatenates the (maybe transformed) observations/actions.\n  multiplexer = networks.CriticMultiplexer(\n      observation_network=flatten_concat_policy,\n      action_network=networks.ClipToSpec(action_spec))\n  critic_network = snt.Sequential([\n      multiplexer,\n      networks.LayerNormMLP(critic_layer_sizes, activate_final=True),\n      networks.NearZeroInitializedLinear(1),\n  ])\n  prior_network = snt.Sequential([\n      flatten_concat_prior,\n      networks.LayerNormMLP(prior_layer_sizes, activate_final=True),\n      networks.MultivariateNormalDiagHead(\n          num_dimensions,\n          tanh_mean=True,\n          min_scale=0.1,\n          init_scale=0.7,\n          fixed_scale=False,\n          use_tfd_independent=False)\n  ])\n  return {\n      \"policy\": policy_network,\n      \"critic\": critic_network,\n      \"prior\": prior_network,\n  }",
  "class SVG0Actor(actors.FeedForwardActor):\n  \"\"\"An actor that also returns `log_prob`.\"\"\"\n\n  def __init__(\n      self,\n      policy_network: snt.Module,\n      adder: Optional[adders.Adder] = None,\n      variable_client: Optional[tf2_variable_utils.VariableClient] = None,\n      deterministic_policy: Optional[bool] = False,\n  ):\n    super().__init__(policy_network, adder, variable_client)\n    self._log_prob = None\n    self._deterministic_policy = deterministic_policy\n\n  def select_action(self, observation: types.NestedArray) -> types.NestedArray:\n    # Add a dummy batch dimension and as a side effect convert numpy to TF.\n    batched_observation = tf2_utils.add_batch_dim(observation)\n\n    # Compute the policy, conditioned on the observation.\n    policy = self._policy_network(batched_observation)\n    if self._deterministic_policy:\n      action = policy.mean()\n    else:\n      action = policy.sample()\n    self._log_prob = policy.log_prob(action)\n    return tf2_utils.to_numpy_squeeze(action)\n\n  def observe(\n      self,\n      action: types.NestedArray,\n      next_timestep: dm_env.TimeStep,\n  ):\n    if not self._adder:\n      return\n\n    extras = {'log_prob': self._log_prob}\n    extras = tf2_utils.to_numpy_squeeze(extras)\n    self._adder.add(action, next_timestep, extras)",
  "def __init__(\n      self,\n      policy_network: snt.Module,\n      adder: Optional[adders.Adder] = None,\n      variable_client: Optional[tf2_variable_utils.VariableClient] = None,\n      deterministic_policy: Optional[bool] = False,\n  ):\n    super().__init__(policy_network, adder, variable_client)\n    self._log_prob = None\n    self._deterministic_policy = deterministic_policy",
  "def select_action(self, observation: types.NestedArray) -> types.NestedArray:\n    # Add a dummy batch dimension and as a side effect convert numpy to TF.\n    batched_observation = tf2_utils.add_batch_dim(observation)\n\n    # Compute the policy, conditioned on the observation.\n    policy = self._policy_network(batched_observation)\n    if self._deterministic_policy:\n      action = policy.mean()\n    else:\n      action = policy.sample()\n    self._log_prob = policy.log_prob(action)\n    return tf2_utils.to_numpy_squeeze(action)",
  "def observe(\n      self,\n      action: types.NestedArray,\n      next_timestep: dm_env.TimeStep,\n  ):\n    if not self._adder:\n      return\n\n    extras = {'log_prob': self._log_prob}\n    extras = tf2_utils.to_numpy_squeeze(extras)\n    self._adder.add(action, next_timestep, extras)",
  "def make_networks(\n    action_spec: specs.BoundedArray,\n    policy_layer_sizes: Sequence[int] = (10, 10),\n    critic_layer_sizes: Sequence[int] = (10, 10),\n):\n  \"\"\"Simple networks for testing..\"\"\"\n\n  # Get total number of action dimensions from action spec.\n  num_dimensions = np.prod(action_spec.shape, dtype=int)\n\n  policy_network = snt.Sequential([\n      tf2_utils.batch_concat,\n      networks.LayerNormMLP(policy_layer_sizes, activate_final=True),\n      networks.MultivariateNormalDiagHead(\n          num_dimensions,\n          tanh_mean=True,\n          min_scale=0.3,\n          init_scale=0.7,\n          fixed_scale=False,\n          use_tfd_independent=False)\n  ])\n  # The multiplexer concatenates the (maybe transformed) observations/actions.\n  multiplexer = networks.CriticMultiplexer()\n  critic_network = snt.Sequential([\n      multiplexer,\n      networks.LayerNormMLP(critic_layer_sizes, activate_final=True),\n      networks.NearZeroInitializedLinear(1),\n  ])\n\n  return {\n      'policy': policy_network,\n      'critic': critic_network,\n  }",
  "class DistributedAgentTest(absltest.TestCase):\n  \"\"\"Simple integration/smoke test for the distributed agent.\"\"\"\n\n  def test_control_suite(self):\n    \"\"\"Tests that the agent can run on the control suite without crashing.\"\"\"\n\n    agent = svg0_prior.DistributedSVG0(\n        environment_factory=lambda x: fakes.ContinuousEnvironment(),\n        network_factory=make_networks,\n        num_actors=2,\n        batch_size=32,\n        min_replay_size=32,\n        max_replay_size=1000,\n    )\n    program = agent.build()\n\n    (learner_node,) = program.groups['learner']\n    learner_node.disable_run()\n\n    lp.launch(program, launch_type='test_mt')\n\n    learner: acme.Learner = learner_node.create_handle().dereference()\n\n    for _ in range(5):\n      learner.step()",
  "def test_control_suite(self):\n    \"\"\"Tests that the agent can run on the control suite without crashing.\"\"\"\n\n    agent = svg0_prior.DistributedSVG0(\n        environment_factory=lambda x: fakes.ContinuousEnvironment(),\n        network_factory=make_networks,\n        num_actors=2,\n        batch_size=32,\n        min_replay_size=32,\n        max_replay_size=1000,\n    )\n    program = agent.build()\n\n    (learner_node,) = program.groups['learner']\n    learner_node.disable_run()\n\n    lp.launch(program, launch_type='test_mt')\n\n    learner: acme.Learner = learner_node.create_handle().dereference()\n\n    for _ in range(5):\n      learner.step()",
  "def make_networks(\n    action_spec: types.NestedSpec,\n    policy_layer_sizes: Sequence[int] = (10, 10),\n    critic_layer_sizes: Sequence[int] = (10, 10),\n) -> Dict[str, snt.Module]:\n  \"\"\"Creates networks used by the agent.\"\"\"\n  # Get total number of action dimensions from action spec.\n  num_dimensions = np.prod(action_spec.shape, dtype=int)\n\n  policy_network = snt.Sequential([\n      tf2_utils.batch_concat,\n      networks.LayerNormMLP(policy_layer_sizes, activate_final=True),\n      networks.MultivariateNormalDiagHead(\n          num_dimensions,\n          tanh_mean=True,\n          min_scale=0.3,\n          init_scale=0.7,\n          fixed_scale=False,\n          use_tfd_independent=False)\n  ])\n  # The multiplexer concatenates the (maybe transformed) observations/actions.\n  multiplexer = networks.CriticMultiplexer()\n  critic_network = snt.Sequential([\n      multiplexer,\n      networks.LayerNormMLP(critic_layer_sizes, activate_final=True),\n      networks.NearZeroInitializedLinear(1),\n  ])\n\n  return {\n      'policy': policy_network,\n      'critic': critic_network,\n  }",
  "class SVG0Test(absltest.TestCase):\n\n  def test_svg0(self):\n    # Create a fake environment to test with.\n    environment = fakes.ContinuousEnvironment(episode_length=10)\n    spec = specs.make_environment_spec(environment)\n\n    # Create the networks.\n    agent_networks = make_networks(spec.actions)\n\n    # Construct the agent.\n    agent = svg0_prior.SVG0(\n        environment_spec=spec,\n        policy_network=agent_networks['policy'],\n        critic_network=agent_networks['critic'],\n        batch_size=10,\n        samples_per_insert=2,\n        min_replay_size=10,\n    )\n\n    # Try running the environment loop. We have no assertions here because all\n    # we care about is that the agent runs without raising any errors.\n    loop = acme.EnvironmentLoop(environment, agent)\n    loop.run(num_episodes=2)",
  "def test_svg0(self):\n    # Create a fake environment to test with.\n    environment = fakes.ContinuousEnvironment(episode_length=10)\n    spec = specs.make_environment_spec(environment)\n\n    # Create the networks.\n    agent_networks = make_networks(spec.actions)\n\n    # Construct the agent.\n    agent = svg0_prior.SVG0(\n        environment_spec=spec,\n        policy_network=agent_networks['policy'],\n        critic_network=agent_networks['critic'],\n        batch_size=10,\n        samples_per_insert=2,\n        min_replay_size=10,\n    )\n\n    # Try running the environment loop. We have no assertions here because all\n    # we care about is that the agent runs without raising any errors.\n    loop = acme.EnvironmentLoop(environment, agent)\n    loop.run(num_episodes=2)",
  "class DistributedSVG0:\n  \"\"\"Program definition for SVG0.\"\"\"\n\n  def __init__(\n      self,\n      environment_factory: Callable[[bool], dm_env.Environment],\n      network_factory: Callable[[specs.BoundedArray], Dict[str, snt.Module]],\n      num_actors: int = 1,\n      num_caches: int = 0,\n      environment_spec: Optional[specs.EnvironmentSpec] = None,\n      batch_size: int = 256,\n      prefetch_size: int = 4,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1000000,\n      samples_per_insert: Optional[float] = 32.0,\n      sequence_length: int = 10,\n      sigma: float = 0.3,\n      discount: float = 0.99,\n      policy_optimizer: Optional[snt.Optimizer] = None,\n      critic_optimizer: Optional[snt.Optimizer] = None,\n      prior_optimizer: Optional[snt.Optimizer] = None,\n      distillation_cost: Optional[float] = 1e-3,\n      entropy_regularizer_cost: Optional[float] = 1e-3,\n      target_update_period: int = 100,\n      max_actor_steps: Optional[int] = None,\n      log_every: float = 10.0,\n  ):\n\n    if not environment_spec:\n      environment_spec = specs.make_environment_spec(environment_factory(False))\n\n    # TODO(mwhoffman): Make network_factory directly return the struct.\n    # TODO(mwhoffman): Make the factory take the entire spec.\n    def wrapped_network_factory(action_spec):\n      networks_dict = network_factory(action_spec)\n      networks = agent.SVG0Networks(\n          policy_network=networks_dict.get('policy'),\n          critic_network=networks_dict.get('critic'),\n          prior_network=networks_dict.get('prior', None),)\n      return networks\n\n    self._environment_factory = environment_factory\n    self._network_factory = wrapped_network_factory\n    self._environment_spec = environment_spec\n    self._sigma = sigma\n    self._num_actors = num_actors\n    self._num_caches = num_caches\n    self._max_actor_steps = max_actor_steps\n    self._log_every = log_every\n    self._sequence_length = sequence_length\n\n    self._builder = agent.SVG0Builder(\n        # TODO(mwhoffman): pass the config dataclass in directly.\n        # TODO(mwhoffman): use the limiter rather than the workaround below.\n        agent.SVG0Config(\n            discount=discount,\n            batch_size=batch_size,\n            prefetch_size=prefetch_size,\n            target_update_period=target_update_period,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            prior_optimizer=prior_optimizer,\n            min_replay_size=min_replay_size,\n            max_replay_size=max_replay_size,\n            samples_per_insert=samples_per_insert,\n            sequence_length=sequence_length,\n            sigma=sigma,\n            distillation_cost=distillation_cost,\n            entropy_regularizer_cost=entropy_regularizer_cost,\n        ))\n\n  def replay(self):\n    \"\"\"The replay storage.\"\"\"\n    return self._builder.make_replay_tables(self._environment_spec,\n                                            self._sequence_length)\n\n  def counter(self):\n    return tf2_savers.CheckpointingRunner(counting.Counter(),\n                                          time_delta_minutes=1,\n                                          subdirectory='counter')\n\n  def coordinator(self, counter: counting.Counter):\n    return lp_utils.StepsLimiter(counter, self._max_actor_steps)\n\n  def learner(\n      self,\n      replay: reverb.Client,\n      counter: counting.Counter,\n  ):\n    \"\"\"The Learning part of the agent.\"\"\"\n\n    # Create the networks to optimize (online) and target networks.\n    online_networks = self._network_factory(self._environment_spec.actions)\n    target_networks = copy.deepcopy(online_networks)\n\n    # Initialize the networks.\n    online_networks.init(self._environment_spec)\n    target_networks.init(self._environment_spec)\n\n    dataset = self._builder.make_dataset_iterator(replay)\n    counter = counting.Counter(counter, 'learner')\n    logger = loggers.make_default_logger(\n        'learner', time_delta=self._log_every, steps_key='learner_steps')\n\n    return self._builder.make_learner(\n        networks=(online_networks, target_networks),\n        dataset=dataset,\n        counter=counter,\n        logger=logger,\n    )\n\n  def actor(\n      self,\n      replay: reverb.Client,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ) -> acme.EnvironmentLoop:\n    \"\"\"The actor process.\"\"\"\n\n    # Create the behavior policy.\n    networks = self._network_factory(self._environment_spec.actions)\n    networks.init(self._environment_spec)\n    policy_network = networks.make_policy()\n\n    # Create the agent.\n    actor = self._builder.make_actor(\n        policy_network=policy_network,\n        adder=self._builder.make_adder(replay),\n        variable_source=variable_source,\n    )\n\n    # Create the environment.\n    environment = self._environment_factory(False)\n\n    # Create logger and counter; actors will not spam bigtable.\n    counter = counting.Counter(counter, 'actor')\n    logger = loggers.make_default_logger(\n        'actor',\n        save_data=False,\n        time_delta=self._log_every,\n        steps_key='actor_steps')\n\n    # Create the loop to connect environment and agent.\n    return acme.EnvironmentLoop(environment, actor, counter, logger)\n\n  def evaluator(\n      self,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n      logger: Optional[loggers.Logger] = None,\n  ):\n    \"\"\"The evaluation process.\"\"\"\n\n    # Create the behavior policy.\n    networks = self._network_factory(self._environment_spec.actions)\n    networks.init(self._environment_spec)\n    policy_network = networks.make_policy()\n\n    # Create the agent.\n    actor = self._builder.make_actor(\n        policy_network=policy_network,\n        variable_source=variable_source,\n        deterministic_policy=True,\n    )\n\n    # Make the environment.\n    environment = self._environment_factory(True)\n\n    # Create logger and counter.\n    counter = counting.Counter(counter, 'evaluator')\n    logger = logger or loggers.make_default_logger(\n        'evaluator',\n        time_delta=self._log_every,\n        steps_key='evaluator_steps',\n    )\n\n    # Create the run loop and return it.\n    return acme.EnvironmentLoop(environment, actor, counter, logger)\n\n  def build(self, name='svg0'):\n    \"\"\"Build the distributed agent topology.\"\"\"\n    program = lp.Program(name=name)\n\n    with program.group('replay'):\n      replay = program.add_node(lp.ReverbNode(self.replay))\n\n    with program.group('counter'):\n      counter = program.add_node(lp.CourierNode(self.counter))\n\n    if self._max_actor_steps:\n      with program.group('coordinator'):\n        _ = program.add_node(lp.CourierNode(self.coordinator, counter))\n\n    with program.group('learner'):\n      learner = program.add_node(lp.CourierNode(self.learner, replay, counter))\n\n    with program.group('evaluator'):\n      program.add_node(lp.CourierNode(self.evaluator, learner, counter))\n\n    if not self._num_caches:\n      # Use our learner as a single variable source.\n      sources = [learner]\n    else:\n      with program.group('cacher'):\n        # Create a set of learner caches.\n        sources = []\n        for _ in range(self._num_caches):\n          cacher = program.add_node(\n              lp.CacherNode(\n                  learner, refresh_interval_ms=2000, stale_after_ms=4000))\n          sources.append(cacher)\n\n    with program.group('actor'):\n      # Add actors which pull round-robin from our variable sources.\n      for actor_id in range(self._num_actors):\n        source = sources[actor_id % len(sources)]\n        program.add_node(lp.CourierNode(self.actor, replay, source, counter))\n\n    return program",
  "def __init__(\n      self,\n      environment_factory: Callable[[bool], dm_env.Environment],\n      network_factory: Callable[[specs.BoundedArray], Dict[str, snt.Module]],\n      num_actors: int = 1,\n      num_caches: int = 0,\n      environment_spec: Optional[specs.EnvironmentSpec] = None,\n      batch_size: int = 256,\n      prefetch_size: int = 4,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1000000,\n      samples_per_insert: Optional[float] = 32.0,\n      sequence_length: int = 10,\n      sigma: float = 0.3,\n      discount: float = 0.99,\n      policy_optimizer: Optional[snt.Optimizer] = None,\n      critic_optimizer: Optional[snt.Optimizer] = None,\n      prior_optimizer: Optional[snt.Optimizer] = None,\n      distillation_cost: Optional[float] = 1e-3,\n      entropy_regularizer_cost: Optional[float] = 1e-3,\n      target_update_period: int = 100,\n      max_actor_steps: Optional[int] = None,\n      log_every: float = 10.0,\n  ):\n\n    if not environment_spec:\n      environment_spec = specs.make_environment_spec(environment_factory(False))\n\n    # TODO(mwhoffman): Make network_factory directly return the struct.\n    # TODO(mwhoffman): Make the factory take the entire spec.\n    def wrapped_network_factory(action_spec):\n      networks_dict = network_factory(action_spec)\n      networks = agent.SVG0Networks(\n          policy_network=networks_dict.get('policy'),\n          critic_network=networks_dict.get('critic'),\n          prior_network=networks_dict.get('prior', None),)\n      return networks\n\n    self._environment_factory = environment_factory\n    self._network_factory = wrapped_network_factory\n    self._environment_spec = environment_spec\n    self._sigma = sigma\n    self._num_actors = num_actors\n    self._num_caches = num_caches\n    self._max_actor_steps = max_actor_steps\n    self._log_every = log_every\n    self._sequence_length = sequence_length\n\n    self._builder = agent.SVG0Builder(\n        # TODO(mwhoffman): pass the config dataclass in directly.\n        # TODO(mwhoffman): use the limiter rather than the workaround below.\n        agent.SVG0Config(\n            discount=discount,\n            batch_size=batch_size,\n            prefetch_size=prefetch_size,\n            target_update_period=target_update_period,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            prior_optimizer=prior_optimizer,\n            min_replay_size=min_replay_size,\n            max_replay_size=max_replay_size,\n            samples_per_insert=samples_per_insert,\n            sequence_length=sequence_length,\n            sigma=sigma,\n            distillation_cost=distillation_cost,\n            entropy_regularizer_cost=entropy_regularizer_cost,\n        ))",
  "def replay(self):\n    \"\"\"The replay storage.\"\"\"\n    return self._builder.make_replay_tables(self._environment_spec,\n                                            self._sequence_length)",
  "def counter(self):\n    return tf2_savers.CheckpointingRunner(counting.Counter(),\n                                          time_delta_minutes=1,\n                                          subdirectory='counter')",
  "def coordinator(self, counter: counting.Counter):\n    return lp_utils.StepsLimiter(counter, self._max_actor_steps)",
  "def learner(\n      self,\n      replay: reverb.Client,\n      counter: counting.Counter,\n  ):\n    \"\"\"The Learning part of the agent.\"\"\"\n\n    # Create the networks to optimize (online) and target networks.\n    online_networks = self._network_factory(self._environment_spec.actions)\n    target_networks = copy.deepcopy(online_networks)\n\n    # Initialize the networks.\n    online_networks.init(self._environment_spec)\n    target_networks.init(self._environment_spec)\n\n    dataset = self._builder.make_dataset_iterator(replay)\n    counter = counting.Counter(counter, 'learner')\n    logger = loggers.make_default_logger(\n        'learner', time_delta=self._log_every, steps_key='learner_steps')\n\n    return self._builder.make_learner(\n        networks=(online_networks, target_networks),\n        dataset=dataset,\n        counter=counter,\n        logger=logger,\n    )",
  "def actor(\n      self,\n      replay: reverb.Client,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ) -> acme.EnvironmentLoop:\n    \"\"\"The actor process.\"\"\"\n\n    # Create the behavior policy.\n    networks = self._network_factory(self._environment_spec.actions)\n    networks.init(self._environment_spec)\n    policy_network = networks.make_policy()\n\n    # Create the agent.\n    actor = self._builder.make_actor(\n        policy_network=policy_network,\n        adder=self._builder.make_adder(replay),\n        variable_source=variable_source,\n    )\n\n    # Create the environment.\n    environment = self._environment_factory(False)\n\n    # Create logger and counter; actors will not spam bigtable.\n    counter = counting.Counter(counter, 'actor')\n    logger = loggers.make_default_logger(\n        'actor',\n        save_data=False,\n        time_delta=self._log_every,\n        steps_key='actor_steps')\n\n    # Create the loop to connect environment and agent.\n    return acme.EnvironmentLoop(environment, actor, counter, logger)",
  "def evaluator(\n      self,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n      logger: Optional[loggers.Logger] = None,\n  ):\n    \"\"\"The evaluation process.\"\"\"\n\n    # Create the behavior policy.\n    networks = self._network_factory(self._environment_spec.actions)\n    networks.init(self._environment_spec)\n    policy_network = networks.make_policy()\n\n    # Create the agent.\n    actor = self._builder.make_actor(\n        policy_network=policy_network,\n        variable_source=variable_source,\n        deterministic_policy=True,\n    )\n\n    # Make the environment.\n    environment = self._environment_factory(True)\n\n    # Create logger and counter.\n    counter = counting.Counter(counter, 'evaluator')\n    logger = logger or loggers.make_default_logger(\n        'evaluator',\n        time_delta=self._log_every,\n        steps_key='evaluator_steps',\n    )\n\n    # Create the run loop and return it.\n    return acme.EnvironmentLoop(environment, actor, counter, logger)",
  "def build(self, name='svg0'):\n    \"\"\"Build the distributed agent topology.\"\"\"\n    program = lp.Program(name=name)\n\n    with program.group('replay'):\n      replay = program.add_node(lp.ReverbNode(self.replay))\n\n    with program.group('counter'):\n      counter = program.add_node(lp.CourierNode(self.counter))\n\n    if self._max_actor_steps:\n      with program.group('coordinator'):\n        _ = program.add_node(lp.CourierNode(self.coordinator, counter))\n\n    with program.group('learner'):\n      learner = program.add_node(lp.CourierNode(self.learner, replay, counter))\n\n    with program.group('evaluator'):\n      program.add_node(lp.CourierNode(self.evaluator, learner, counter))\n\n    if not self._num_caches:\n      # Use our learner as a single variable source.\n      sources = [learner]\n    else:\n      with program.group('cacher'):\n        # Create a set of learner caches.\n        sources = []\n        for _ in range(self._num_caches):\n          cacher = program.add_node(\n              lp.CacherNode(\n                  learner, refresh_interval_ms=2000, stale_after_ms=4000))\n          sources.append(cacher)\n\n    with program.group('actor'):\n      # Add actors which pull round-robin from our variable sources.\n      for actor_id in range(self._num_actors):\n        source = sources[actor_id % len(sources)]\n        program.add_node(lp.CourierNode(self.actor, replay, source, counter))\n\n    return program",
  "def wrapped_network_factory(action_spec):\n      networks_dict = network_factory(action_spec)\n      networks = agent.SVG0Networks(\n          policy_network=networks_dict.get('policy'),\n          critic_network=networks_dict.get('critic'),\n          prior_network=networks_dict.get('prior', None),)\n      return networks",
  "class RCRRLearner(core.Learner):\n  \"\"\"Recurrent CRR learner.\n\n  This is the learning component of a RCRR agent. It takes a dataset as\n  input and implements update functionality to learn from this dataset.\n  \"\"\"\n\n  def __init__(self,\n               policy_network: snt.RNNCore,\n               critic_network: networks.CriticDeepRNN,\n               target_policy_network: snt.RNNCore,\n               target_critic_network: networks.CriticDeepRNN,\n               dataset: tf.data.Dataset,\n               accelerator_strategy: Optional[tf.distribute.Strategy] = None,\n               behavior_network: Optional[snt.Module] = None,\n               cwp_network: Optional[snt.Module] = None,\n               policy_optimizer: Optional[snt.Optimizer] = None,\n               critic_optimizer: Optional[snt.Optimizer] = None,\n               discount: float = 0.99,\n               target_update_period: int = 100,\n               num_action_samples_td_learning: int = 1,\n               num_action_samples_policy_weight: int = 4,\n               baseline_reduce_function: str = 'mean',\n               clipping: bool = True,\n               policy_improvement_modes: str = 'exp',\n               ratio_upper_bound: float = 20.,\n               beta: float = 1.0,\n               counter: Optional[counting.Counter] = None,\n               logger: Optional[loggers.Logger] = None,\n               checkpoint: bool = False):\n    \"\"\"Initializes the learner.\n\n    Args:\n      policy_network: the online (optimized) policy.\n      critic_network: the online critic.\n      target_policy_network: the target policy (which lags behind the online\n        policy).\n      target_critic_network: the target critic.\n      dataset: dataset to learn from, whether fixed or from a replay buffer\n        (see `acme.datasets.reverb.make_reverb_dataset` documentation).\n      accelerator_strategy: the strategy used to distribute computation,\n        whether on a single, or multiple, GPU or TPU; as supported by\n        tf.distribute.\n      behavior_network: The network to snapshot under `policy` name. If None,\n        snapshots `policy_network` instead.\n      cwp_network: CWP network to snapshot: samples actions\n        from the policy and weighs them with the critic, then returns the action\n        by sampling from the softmax distribution using critic values as logits.\n        Used only for snapshotting, not training.\n      policy_optimizer: the optimizer to be applied to the policy loss.\n      critic_optimizer: the optimizer to be applied to the distributional\n        Bellman loss.\n      discount: discount to use for TD updates.\n      target_update_period: number of learner steps to perform before updating\n        the target networks.\n      num_action_samples_td_learning: number of action samples to use to\n        estimate expected value of the critic loss w.r.t. stochastic policy.\n      num_action_samples_policy_weight: number of action samples to use to\n        estimate the advantage function for the CRR weighting of the policy\n        loss.\n      baseline_reduce_function: one of 'mean', 'max', 'min'. Way of aggregating\n        values from `num_action_samples` estimates of the value function.\n      clipping: whether to clip gradients by global norm.\n      policy_improvement_modes: one of 'exp', 'binary', 'all'. CRR mode which\n        determines how the advantage function is processed before being\n        multiplied by the policy loss.\n      ratio_upper_bound: if policy_improvement_modes is 'exp', determines\n        the upper bound of the weight (i.e. the weight is\n          min(exp(advantage / beta), upper_bound)\n        ).\n      beta: if policy_improvement_modes is 'exp', determines the beta (see\n        above).\n      counter: counter object used to keep track of steps.\n      logger: logger object to be used by learner.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n    \"\"\"\n\n    if accelerator_strategy is None:\n      accelerator_strategy = snt.distribute.Replicator()\n    self._accelerator_strategy = accelerator_strategy\n    self._policy_improvement_modes = policy_improvement_modes\n    self._ratio_upper_bound = ratio_upper_bound\n    self._num_action_samples_td_learning = num_action_samples_td_learning\n    self._num_action_samples_policy_weight = num_action_samples_policy_weight\n    self._baseline_reduce_function = baseline_reduce_function\n    self._beta = beta\n\n    # When running on TPUs we have to know the amount of memory required (and\n    # thus the sequence length) at the graph compilation stage. At the moment,\n    # the only way to get it is to sample from the dataset, since the dataset\n    # does not have any metadata, see b/160672927 to track this upcoming\n    # feature.\n    sample = next(dataset.as_numpy_iterator())\n    self._sequence_length = sample.action.shape[1]\n\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.TerminalLogger('learner', time_delta=1.)\n    self._discount = discount\n    self._clipping = clipping\n\n    self._target_update_period = target_update_period\n\n    with self._accelerator_strategy.scope():\n      # Necessary to track when to update target networks.\n      self._num_steps = tf.Variable(0, dtype=tf.int32)\n\n      # (Maybe) distributing the dataset across multiple accelerators.\n      distributed_dataset = self._accelerator_strategy.experimental_distribute_dataset(\n          dataset)\n      self._iterator = iter(distributed_dataset)\n\n      # Create the optimizers.\n      self._critic_optimizer = critic_optimizer or snt.optimizers.Adam(1e-4)\n      self._policy_optimizer = policy_optimizer or snt.optimizers.Adam(1e-4)\n\n    # Store online and target networks.\n    self._policy_network = policy_network\n    self._critic_network = critic_network\n    self._target_policy_network = target_policy_network\n    self._target_critic_network = target_critic_network\n\n    # Expose the variables.\n    self._variables = {\n        'critic': self._target_critic_network.variables,\n        'policy': self._target_policy_network.variables,\n    }\n\n    # Create a checkpointer object.\n    self._checkpointer = None\n    self._snapshotter = None\n    if checkpoint:\n      self._checkpointer = tf2_savers.Checkpointer(\n          objects_to_save={\n              'counter': self._counter,\n              'policy': self._policy_network,\n              'critic': self._critic_network,\n              'target_policy': self._target_policy_network,\n              'target_critic': self._target_critic_network,\n              'policy_optimizer': self._policy_optimizer,\n              'critic_optimizer': self._critic_optimizer,\n              'num_steps': self._num_steps,\n          },\n          time_delta_minutes=30.)\n\n      raw_policy = snt.DeepRNN(\n          [policy_network, networks.StochasticSamplingHead()])\n      critic_mean = networks.CriticDeepRNN(\n          [critic_network, networks.StochasticMeanHead()])\n      objects_to_save = {\n          'raw_policy': raw_policy,\n          'critic': critic_mean,\n      }\n      if behavior_network is not None:\n        objects_to_save['policy'] = behavior_network\n      if cwp_network is not None:\n        objects_to_save['cwp_policy'] = cwp_network\n      self._snapshotter = tf2_savers.Snapshotter(\n          objects_to_save=objects_to_save, time_delta_minutes=30)\n    # Timestamp to keep track of the wall time.\n    self._walltime_timestamp = time.time()\n\n  def _step(self, sample: reverb.ReplaySample) -> Dict[str, tf.Tensor]:\n    # Transpose batch and sequence axes, i.e. [B, T, ...] to [T, B, ...].\n    sample = tf2_utils.batch_to_sequence(sample)\n    observations = sample.observation\n    actions = sample.action\n    rewards = sample.reward\n    discounts = sample.discount\n\n    dtype = rewards.dtype\n\n    # Cast the additional discount to match the environment discount dtype.\n    discount = tf.cast(self._discount, dtype=discounts.dtype)\n\n    # Loss cumulants across time. These cannot be python mutable objects.\n    critic_loss = 0.\n    policy_loss = 0.\n\n    # Each transition induces a policy loss, which we then weight using\n    # the `policy_loss_coef_t`; shape [B], see https://arxiv.org/abs/2006.15134.\n    # `policy_loss_coef` is a scalar average of these coefficients across\n    # the batch and sequence length dimensions.\n    policy_loss_coef = 0.\n\n    per_device_batch_size = actions.shape[1]\n\n    # Initialize recurrent states.\n    critic_state = self._critic_network.initial_state(per_device_batch_size)\n    target_critic_state = critic_state\n    policy_state = self._policy_network.initial_state(per_device_batch_size)\n    target_policy_state = policy_state\n\n    with tf.GradientTape(persistent=True) as tape:\n      for t in range(1, self._sequence_length):\n        o_tm1 = tree.map_structure(operator.itemgetter(t - 1), observations)\n        a_tm1 = tree.map_structure(operator.itemgetter(t - 1), actions)\n        r_t = tree.map_structure(operator.itemgetter(t - 1), rewards)\n        d_t = tree.map_structure(operator.itemgetter(t - 1), discounts)\n        o_t = tree.map_structure(operator.itemgetter(t), observations)\n\n        if t != 1:\n          # By only updating the target critic state here we are forcing\n          # the target critic to ignore observations[0]. Otherwise, the\n          # target_critic will be unrolled for one more timestep than critic.\n          # The smaller the sequence length, the more problematic this is: if\n          # you use RNN on sequences of length 2, you would expect the code to\n          # never use recurrent connections. But if you don't skip updating the\n          # target_critic_state on observation[0] here, it won't be the case.\n          _, target_critic_state = self._target_critic_network(\n              o_tm1, a_tm1, target_critic_state)\n\n        # ========================= Critic learning ============================\n        q_tm1, next_critic_state = self._critic_network(o_tm1, a_tm1,\n                                                        critic_state)\n        target_action_distribution, target_policy_state = self._target_policy_network(\n            o_t, target_policy_state)\n\n        sampled_actions_t = target_action_distribution.sample(\n            self._num_action_samples_td_learning)\n        # [N, B, ...]\n        tiled_o_t = tf2_utils.tile_nested(\n            o_t, self._num_action_samples_td_learning)\n        tiled_target_critic_state = tf2_utils.tile_nested(\n            target_critic_state, self._num_action_samples_td_learning)\n\n        # Compute the target critic's Q-value of the sampled actions.\n        sampled_q_t, _ = snt.BatchApply(self._target_critic_network)(\n            tiled_o_t, sampled_actions_t, tiled_target_critic_state)\n\n        # Compute average logits by first reshaping them to [N, B, A] and then\n        # normalizing them across atoms.\n        new_shape = [self._num_action_samples_td_learning, r_t.shape[0], -1]\n        sampled_logits = tf.reshape(sampled_q_t.logits, new_shape)\n        sampled_logprobs = tf.math.log_softmax(sampled_logits, axis=-1)\n        averaged_logits = tf.reduce_logsumexp(sampled_logprobs, axis=0)\n\n        # Construct the expected distributional value for bootstrapping.\n        q_t = networks.DiscreteValuedDistribution(\n            values=sampled_q_t.values, logits=averaged_logits)\n        critic_loss_t = losses.categorical(q_tm1, r_t, discount * d_t, q_t)\n        critic_loss_t = tf.reduce_mean(critic_loss_t)\n\n        # ========================= Actor learning =============================\n        action_distribution_tm1, policy_state = self._policy_network(\n            o_tm1, policy_state)\n        q_tm1_mean = q_tm1.mean()\n\n        # Compute the estimate of the value function based on\n        # self._num_action_samples_policy_weight samples from the policy.\n        tiled_o_tm1 = tf2_utils.tile_nested(\n            o_tm1, self._num_action_samples_policy_weight)\n        tiled_critic_state = tf2_utils.tile_nested(\n            critic_state, self._num_action_samples_policy_weight)\n        action_tm1 = action_distribution_tm1.sample(\n            self._num_action_samples_policy_weight)\n        tiled_z_tm1, _ = snt.BatchApply(self._critic_network)(\n            tiled_o_tm1, action_tm1, tiled_critic_state)\n        tiled_v_tm1 = tf.reshape(tiled_z_tm1.mean(),\n                                 [self._num_action_samples_policy_weight, -1])\n\n        # Use mean, min, or max to aggregate Q(s, a_i), a_i ~ pi(s) into the\n        # final estimate of the value function.\n        if self._baseline_reduce_function == 'mean':\n          v_tm1_estimate = tf.reduce_mean(tiled_v_tm1, axis=0)\n        elif self._baseline_reduce_function == 'max':\n          v_tm1_estimate = tf.reduce_max(tiled_v_tm1, axis=0)\n        elif self._baseline_reduce_function == 'min':\n          v_tm1_estimate = tf.reduce_min(tiled_v_tm1, axis=0)\n\n        # Assert that action_distribution_tm1 is a batch of multivariate\n        # distributions (in contrast to e.g. a [batch, action_size] collection\n        # of 1d distributions).\n        assert len(action_distribution_tm1.batch_shape) == 1\n        policy_loss_batch = -action_distribution_tm1.log_prob(a_tm1)\n\n        advantage = q_tm1_mean - v_tm1_estimate\n        if self._policy_improvement_modes == 'exp':\n          policy_loss_coef_t = tf.math.minimum(\n              tf.math.exp(advantage / self._beta), self._ratio_upper_bound)\n        elif self._policy_improvement_modes == 'binary':\n          policy_loss_coef_t = tf.cast(advantage > 0, dtype=dtype)\n        elif self._policy_improvement_modes == 'all':\n          # Regress against all actions (effectively pure BC).\n          policy_loss_coef_t = 1.\n        policy_loss_coef_t = tf.stop_gradient(policy_loss_coef_t)\n\n        policy_loss_batch *= policy_loss_coef_t\n        policy_loss_t = tf.reduce_mean(policy_loss_batch)\n\n        critic_state = next_critic_state\n\n        critic_loss += critic_loss_t\n        policy_loss += policy_loss_t\n        policy_loss_coef += tf.reduce_mean(policy_loss_coef_t)  # For logging.\n\n      # Divide by sequence length to get mean losses.\n      critic_loss /= tf.cast(self._sequence_length, dtype=dtype)\n      policy_loss /= tf.cast(self._sequence_length, dtype=dtype)\n      policy_loss_coef /= tf.cast(self._sequence_length, dtype=dtype)\n\n    # Compute gradients.\n    critic_gradients = tape.gradient(critic_loss,\n                                     self._critic_network.trainable_variables)\n    policy_gradients = tape.gradient(policy_loss,\n                                     self._policy_network.trainable_variables)\n\n    # Delete the tape manually because of the persistent=True flag.\n    del tape\n\n    # Sync gradients across GPUs or TPUs.\n    ctx = tf.distribute.get_replica_context()\n    critic_gradients = ctx.all_reduce('mean', critic_gradients)\n    policy_gradients = ctx.all_reduce('mean', policy_gradients)\n\n    # Maybe clip gradients.\n    if self._clipping:\n      policy_gradients = tf.clip_by_global_norm(policy_gradients, 40.)[0]\n      critic_gradients = tf.clip_by_global_norm(critic_gradients, 40.)[0]\n\n    # Apply gradients.\n    self._critic_optimizer.apply(critic_gradients,\n                                 self._critic_network.trainable_variables)\n    self._policy_optimizer.apply(policy_gradients,\n                                 self._policy_network.trainable_variables)\n\n    source_variables = (\n        self._critic_network.variables + self._policy_network.variables)\n    target_variables = (\n        self._target_critic_network.variables +\n        self._target_policy_network.variables)\n\n    # Make online -> target network update ops.\n    if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n      for src, dest in zip(source_variables, target_variables):\n        dest.assign(src)\n    self._num_steps.assign_add(1)\n\n    return {\n        'critic_loss': critic_loss,\n        'policy_loss': policy_loss,\n        'policy_loss_coef': policy_loss_coef,\n    }\n\n  @tf.function\n  def _replicated_step(self) -> Dict[str, tf.Tensor]:\n    sample = next(self._iterator)\n    fetches = self._accelerator_strategy.run(self._step, args=(sample,))\n    mean = tf.distribute.ReduceOp.MEAN\n    return {\n        k: self._accelerator_strategy.reduce(mean, fetches[k], axis=None)\n        for k in fetches\n    }\n\n  def step(self):\n    # Run the learning step.\n    with self._accelerator_strategy.scope():\n      fetches = self._replicated_step()\n\n    # Update our counts and record it.\n    new_timestamp = time.time()\n    time_passed = new_timestamp - self._walltime_timestamp\n    self._walltime_timestamp = new_timestamp\n    counts = self._counter.increment(steps=1, wall_time=time_passed)\n    fetches.update(counts)\n\n    # Checkpoint and attempt to write the logs.\n    if self._checkpointer is not None:\n      self._checkpointer.save()\n      self._snapshotter.save()\n    self._logger.write(fetches)\n\n  def get_variables(self, names: List[str]) -> List[List[np.ndarray]]:\n    return [tf2_utils.to_numpy(self._variables[name]) for name in names]",
  "def __init__(self,\n               policy_network: snt.RNNCore,\n               critic_network: networks.CriticDeepRNN,\n               target_policy_network: snt.RNNCore,\n               target_critic_network: networks.CriticDeepRNN,\n               dataset: tf.data.Dataset,\n               accelerator_strategy: Optional[tf.distribute.Strategy] = None,\n               behavior_network: Optional[snt.Module] = None,\n               cwp_network: Optional[snt.Module] = None,\n               policy_optimizer: Optional[snt.Optimizer] = None,\n               critic_optimizer: Optional[snt.Optimizer] = None,\n               discount: float = 0.99,\n               target_update_period: int = 100,\n               num_action_samples_td_learning: int = 1,\n               num_action_samples_policy_weight: int = 4,\n               baseline_reduce_function: str = 'mean',\n               clipping: bool = True,\n               policy_improvement_modes: str = 'exp',\n               ratio_upper_bound: float = 20.,\n               beta: float = 1.0,\n               counter: Optional[counting.Counter] = None,\n               logger: Optional[loggers.Logger] = None,\n               checkpoint: bool = False):\n    \"\"\"Initializes the learner.\n\n    Args:\n      policy_network: the online (optimized) policy.\n      critic_network: the online critic.\n      target_policy_network: the target policy (which lags behind the online\n        policy).\n      target_critic_network: the target critic.\n      dataset: dataset to learn from, whether fixed or from a replay buffer\n        (see `acme.datasets.reverb.make_reverb_dataset` documentation).\n      accelerator_strategy: the strategy used to distribute computation,\n        whether on a single, or multiple, GPU or TPU; as supported by\n        tf.distribute.\n      behavior_network: The network to snapshot under `policy` name. If None,\n        snapshots `policy_network` instead.\n      cwp_network: CWP network to snapshot: samples actions\n        from the policy and weighs them with the critic, then returns the action\n        by sampling from the softmax distribution using critic values as logits.\n        Used only for snapshotting, not training.\n      policy_optimizer: the optimizer to be applied to the policy loss.\n      critic_optimizer: the optimizer to be applied to the distributional\n        Bellman loss.\n      discount: discount to use for TD updates.\n      target_update_period: number of learner steps to perform before updating\n        the target networks.\n      num_action_samples_td_learning: number of action samples to use to\n        estimate expected value of the critic loss w.r.t. stochastic policy.\n      num_action_samples_policy_weight: number of action samples to use to\n        estimate the advantage function for the CRR weighting of the policy\n        loss.\n      baseline_reduce_function: one of 'mean', 'max', 'min'. Way of aggregating\n        values from `num_action_samples` estimates of the value function.\n      clipping: whether to clip gradients by global norm.\n      policy_improvement_modes: one of 'exp', 'binary', 'all'. CRR mode which\n        determines how the advantage function is processed before being\n        multiplied by the policy loss.\n      ratio_upper_bound: if policy_improvement_modes is 'exp', determines\n        the upper bound of the weight (i.e. the weight is\n          min(exp(advantage / beta), upper_bound)\n        ).\n      beta: if policy_improvement_modes is 'exp', determines the beta (see\n        above).\n      counter: counter object used to keep track of steps.\n      logger: logger object to be used by learner.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n    \"\"\"\n\n    if accelerator_strategy is None:\n      accelerator_strategy = snt.distribute.Replicator()\n    self._accelerator_strategy = accelerator_strategy\n    self._policy_improvement_modes = policy_improvement_modes\n    self._ratio_upper_bound = ratio_upper_bound\n    self._num_action_samples_td_learning = num_action_samples_td_learning\n    self._num_action_samples_policy_weight = num_action_samples_policy_weight\n    self._baseline_reduce_function = baseline_reduce_function\n    self._beta = beta\n\n    # When running on TPUs we have to know the amount of memory required (and\n    # thus the sequence length) at the graph compilation stage. At the moment,\n    # the only way to get it is to sample from the dataset, since the dataset\n    # does not have any metadata, see b/160672927 to track this upcoming\n    # feature.\n    sample = next(dataset.as_numpy_iterator())\n    self._sequence_length = sample.action.shape[1]\n\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.TerminalLogger('learner', time_delta=1.)\n    self._discount = discount\n    self._clipping = clipping\n\n    self._target_update_period = target_update_period\n\n    with self._accelerator_strategy.scope():\n      # Necessary to track when to update target networks.\n      self._num_steps = tf.Variable(0, dtype=tf.int32)\n\n      # (Maybe) distributing the dataset across multiple accelerators.\n      distributed_dataset = self._accelerator_strategy.experimental_distribute_dataset(\n          dataset)\n      self._iterator = iter(distributed_dataset)\n\n      # Create the optimizers.\n      self._critic_optimizer = critic_optimizer or snt.optimizers.Adam(1e-4)\n      self._policy_optimizer = policy_optimizer or snt.optimizers.Adam(1e-4)\n\n    # Store online and target networks.\n    self._policy_network = policy_network\n    self._critic_network = critic_network\n    self._target_policy_network = target_policy_network\n    self._target_critic_network = target_critic_network\n\n    # Expose the variables.\n    self._variables = {\n        'critic': self._target_critic_network.variables,\n        'policy': self._target_policy_network.variables,\n    }\n\n    # Create a checkpointer object.\n    self._checkpointer = None\n    self._snapshotter = None\n    if checkpoint:\n      self._checkpointer = tf2_savers.Checkpointer(\n          objects_to_save={\n              'counter': self._counter,\n              'policy': self._policy_network,\n              'critic': self._critic_network,\n              'target_policy': self._target_policy_network,\n              'target_critic': self._target_critic_network,\n              'policy_optimizer': self._policy_optimizer,\n              'critic_optimizer': self._critic_optimizer,\n              'num_steps': self._num_steps,\n          },\n          time_delta_minutes=30.)\n\n      raw_policy = snt.DeepRNN(\n          [policy_network, networks.StochasticSamplingHead()])\n      critic_mean = networks.CriticDeepRNN(\n          [critic_network, networks.StochasticMeanHead()])\n      objects_to_save = {\n          'raw_policy': raw_policy,\n          'critic': critic_mean,\n      }\n      if behavior_network is not None:\n        objects_to_save['policy'] = behavior_network\n      if cwp_network is not None:\n        objects_to_save['cwp_policy'] = cwp_network\n      self._snapshotter = tf2_savers.Snapshotter(\n          objects_to_save=objects_to_save, time_delta_minutes=30)\n    # Timestamp to keep track of the wall time.\n    self._walltime_timestamp = time.time()",
  "def _step(self, sample: reverb.ReplaySample) -> Dict[str, tf.Tensor]:\n    # Transpose batch and sequence axes, i.e. [B, T, ...] to [T, B, ...].\n    sample = tf2_utils.batch_to_sequence(sample)\n    observations = sample.observation\n    actions = sample.action\n    rewards = sample.reward\n    discounts = sample.discount\n\n    dtype = rewards.dtype\n\n    # Cast the additional discount to match the environment discount dtype.\n    discount = tf.cast(self._discount, dtype=discounts.dtype)\n\n    # Loss cumulants across time. These cannot be python mutable objects.\n    critic_loss = 0.\n    policy_loss = 0.\n\n    # Each transition induces a policy loss, which we then weight using\n    # the `policy_loss_coef_t`; shape [B], see https://arxiv.org/abs/2006.15134.\n    # `policy_loss_coef` is a scalar average of these coefficients across\n    # the batch and sequence length dimensions.\n    policy_loss_coef = 0.\n\n    per_device_batch_size = actions.shape[1]\n\n    # Initialize recurrent states.\n    critic_state = self._critic_network.initial_state(per_device_batch_size)\n    target_critic_state = critic_state\n    policy_state = self._policy_network.initial_state(per_device_batch_size)\n    target_policy_state = policy_state\n\n    with tf.GradientTape(persistent=True) as tape:\n      for t in range(1, self._sequence_length):\n        o_tm1 = tree.map_structure(operator.itemgetter(t - 1), observations)\n        a_tm1 = tree.map_structure(operator.itemgetter(t - 1), actions)\n        r_t = tree.map_structure(operator.itemgetter(t - 1), rewards)\n        d_t = tree.map_structure(operator.itemgetter(t - 1), discounts)\n        o_t = tree.map_structure(operator.itemgetter(t), observations)\n\n        if t != 1:\n          # By only updating the target critic state here we are forcing\n          # the target critic to ignore observations[0]. Otherwise, the\n          # target_critic will be unrolled for one more timestep than critic.\n          # The smaller the sequence length, the more problematic this is: if\n          # you use RNN on sequences of length 2, you would expect the code to\n          # never use recurrent connections. But if you don't skip updating the\n          # target_critic_state on observation[0] here, it won't be the case.\n          _, target_critic_state = self._target_critic_network(\n              o_tm1, a_tm1, target_critic_state)\n\n        # ========================= Critic learning ============================\n        q_tm1, next_critic_state = self._critic_network(o_tm1, a_tm1,\n                                                        critic_state)\n        target_action_distribution, target_policy_state = self._target_policy_network(\n            o_t, target_policy_state)\n\n        sampled_actions_t = target_action_distribution.sample(\n            self._num_action_samples_td_learning)\n        # [N, B, ...]\n        tiled_o_t = tf2_utils.tile_nested(\n            o_t, self._num_action_samples_td_learning)\n        tiled_target_critic_state = tf2_utils.tile_nested(\n            target_critic_state, self._num_action_samples_td_learning)\n\n        # Compute the target critic's Q-value of the sampled actions.\n        sampled_q_t, _ = snt.BatchApply(self._target_critic_network)(\n            tiled_o_t, sampled_actions_t, tiled_target_critic_state)\n\n        # Compute average logits by first reshaping them to [N, B, A] and then\n        # normalizing them across atoms.\n        new_shape = [self._num_action_samples_td_learning, r_t.shape[0], -1]\n        sampled_logits = tf.reshape(sampled_q_t.logits, new_shape)\n        sampled_logprobs = tf.math.log_softmax(sampled_logits, axis=-1)\n        averaged_logits = tf.reduce_logsumexp(sampled_logprobs, axis=0)\n\n        # Construct the expected distributional value for bootstrapping.\n        q_t = networks.DiscreteValuedDistribution(\n            values=sampled_q_t.values, logits=averaged_logits)\n        critic_loss_t = losses.categorical(q_tm1, r_t, discount * d_t, q_t)\n        critic_loss_t = tf.reduce_mean(critic_loss_t)\n\n        # ========================= Actor learning =============================\n        action_distribution_tm1, policy_state = self._policy_network(\n            o_tm1, policy_state)\n        q_tm1_mean = q_tm1.mean()\n\n        # Compute the estimate of the value function based on\n        # self._num_action_samples_policy_weight samples from the policy.\n        tiled_o_tm1 = tf2_utils.tile_nested(\n            o_tm1, self._num_action_samples_policy_weight)\n        tiled_critic_state = tf2_utils.tile_nested(\n            critic_state, self._num_action_samples_policy_weight)\n        action_tm1 = action_distribution_tm1.sample(\n            self._num_action_samples_policy_weight)\n        tiled_z_tm1, _ = snt.BatchApply(self._critic_network)(\n            tiled_o_tm1, action_tm1, tiled_critic_state)\n        tiled_v_tm1 = tf.reshape(tiled_z_tm1.mean(),\n                                 [self._num_action_samples_policy_weight, -1])\n\n        # Use mean, min, or max to aggregate Q(s, a_i), a_i ~ pi(s) into the\n        # final estimate of the value function.\n        if self._baseline_reduce_function == 'mean':\n          v_tm1_estimate = tf.reduce_mean(tiled_v_tm1, axis=0)\n        elif self._baseline_reduce_function == 'max':\n          v_tm1_estimate = tf.reduce_max(tiled_v_tm1, axis=0)\n        elif self._baseline_reduce_function == 'min':\n          v_tm1_estimate = tf.reduce_min(tiled_v_tm1, axis=0)\n\n        # Assert that action_distribution_tm1 is a batch of multivariate\n        # distributions (in contrast to e.g. a [batch, action_size] collection\n        # of 1d distributions).\n        assert len(action_distribution_tm1.batch_shape) == 1\n        policy_loss_batch = -action_distribution_tm1.log_prob(a_tm1)\n\n        advantage = q_tm1_mean - v_tm1_estimate\n        if self._policy_improvement_modes == 'exp':\n          policy_loss_coef_t = tf.math.minimum(\n              tf.math.exp(advantage / self._beta), self._ratio_upper_bound)\n        elif self._policy_improvement_modes == 'binary':\n          policy_loss_coef_t = tf.cast(advantage > 0, dtype=dtype)\n        elif self._policy_improvement_modes == 'all':\n          # Regress against all actions (effectively pure BC).\n          policy_loss_coef_t = 1.\n        policy_loss_coef_t = tf.stop_gradient(policy_loss_coef_t)\n\n        policy_loss_batch *= policy_loss_coef_t\n        policy_loss_t = tf.reduce_mean(policy_loss_batch)\n\n        critic_state = next_critic_state\n\n        critic_loss += critic_loss_t\n        policy_loss += policy_loss_t\n        policy_loss_coef += tf.reduce_mean(policy_loss_coef_t)  # For logging.\n\n      # Divide by sequence length to get mean losses.\n      critic_loss /= tf.cast(self._sequence_length, dtype=dtype)\n      policy_loss /= tf.cast(self._sequence_length, dtype=dtype)\n      policy_loss_coef /= tf.cast(self._sequence_length, dtype=dtype)\n\n    # Compute gradients.\n    critic_gradients = tape.gradient(critic_loss,\n                                     self._critic_network.trainable_variables)\n    policy_gradients = tape.gradient(policy_loss,\n                                     self._policy_network.trainable_variables)\n\n    # Delete the tape manually because of the persistent=True flag.\n    del tape\n\n    # Sync gradients across GPUs or TPUs.\n    ctx = tf.distribute.get_replica_context()\n    critic_gradients = ctx.all_reduce('mean', critic_gradients)\n    policy_gradients = ctx.all_reduce('mean', policy_gradients)\n\n    # Maybe clip gradients.\n    if self._clipping:\n      policy_gradients = tf.clip_by_global_norm(policy_gradients, 40.)[0]\n      critic_gradients = tf.clip_by_global_norm(critic_gradients, 40.)[0]\n\n    # Apply gradients.\n    self._critic_optimizer.apply(critic_gradients,\n                                 self._critic_network.trainable_variables)\n    self._policy_optimizer.apply(policy_gradients,\n                                 self._policy_network.trainable_variables)\n\n    source_variables = (\n        self._critic_network.variables + self._policy_network.variables)\n    target_variables = (\n        self._target_critic_network.variables +\n        self._target_policy_network.variables)\n\n    # Make online -> target network update ops.\n    if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n      for src, dest in zip(source_variables, target_variables):\n        dest.assign(src)\n    self._num_steps.assign_add(1)\n\n    return {\n        'critic_loss': critic_loss,\n        'policy_loss': policy_loss,\n        'policy_loss_coef': policy_loss_coef,\n    }",
  "def _replicated_step(self) -> Dict[str, tf.Tensor]:\n    sample = next(self._iterator)\n    fetches = self._accelerator_strategy.run(self._step, args=(sample,))\n    mean = tf.distribute.ReduceOp.MEAN\n    return {\n        k: self._accelerator_strategy.reduce(mean, fetches[k], axis=None)\n        for k in fetches\n    }",
  "def step(self):\n    # Run the learning step.\n    with self._accelerator_strategy.scope():\n      fetches = self._replicated_step()\n\n    # Update our counts and record it.\n    new_timestamp = time.time()\n    time_passed = new_timestamp - self._walltime_timestamp\n    self._walltime_timestamp = new_timestamp\n    counts = self._counter.increment(steps=1, wall_time=time_passed)\n    fetches.update(counts)\n\n    # Checkpoint and attempt to write the logs.\n    if self._checkpointer is not None:\n      self._checkpointer.save()\n      self._snapshotter.save()\n    self._logger.write(fetches)",
  "def get_variables(self, names: List[str]) -> List[List[np.ndarray]]:\n    return [tf2_utils.to_numpy(self._variables[name]) for name in names]",
  "class D4PGConfig:\n  \"\"\"Configuration options for the D4PG agent.\"\"\"\n\n  accelerator: Optional[str] = None\n  discount: float = 0.99\n  batch_size: int = 256\n  prefetch_size: int = 4\n  target_update_period: int = 100\n  variable_update_period: int = 1000\n  policy_optimizer: Optional[snt.Optimizer] = None\n  critic_optimizer: Optional[snt.Optimizer] = None\n  min_replay_size: int = 1000\n  max_replay_size: int = 1000000\n  samples_per_insert: Optional[float] = 32.0\n  n_step: int = 5\n  sigma: float = 0.3\n  clipping: bool = True\n  replay_table_name: str = reverb_adders.DEFAULT_PRIORITY_TABLE",
  "class D4PGNetworks:\n  \"\"\"Structure containing the networks for D4PG.\"\"\"\n\n  policy_network: snt.Module\n  critic_network: snt.Module\n  observation_network: snt.Module\n\n  def __init__(\n      self,\n      policy_network: snt.Module,\n      critic_network: snt.Module,\n      observation_network: types.TensorTransformation,\n  ):\n    # This method is implemented (rather than added by the dataclass decorator)\n    # in order to allow observation network to be passed as an arbitrary tensor\n    # transformation rather than as a snt Module.\n    # TODO(mwhoffman): use Protocol rather than Module/TensorTransformation.\n    self.policy_network = policy_network\n    self.critic_network = critic_network\n    self.observation_network = utils.to_sonnet_module(observation_network)\n\n  def init(self, environment_spec: specs.EnvironmentSpec):\n    \"\"\"Initialize the networks given an environment spec.\"\"\"\n    # Get observation and action specs.\n    act_spec = environment_spec.actions\n    obs_spec = environment_spec.observations\n\n    # Create variables for the observation net and, as a side-effect, get a\n    # spec describing the embedding space.\n    emb_spec = utils.create_variables(self.observation_network, [obs_spec])\n\n    # Create variables for the policy and critic nets.\n    _ = utils.create_variables(self.policy_network, [emb_spec])\n    _ = utils.create_variables(self.critic_network, [emb_spec, act_spec])\n\n  def make_policy(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      sigma: float = 0.0,\n  ) -> snt.Module:\n    \"\"\"Create a single network which evaluates the policy.\"\"\"\n    # Stack the observation and policy networks.\n    stack = [\n        self.observation_network,\n        self.policy_network,\n    ]\n\n    # If a stochastic/non-greedy policy is requested, add Gaussian noise on\n    # top to enable a simple form of exploration.\n    # TODO(mwhoffman): Refactor this to remove it from the class.\n    if sigma > 0.0:\n      stack += [\n          network_utils.ClippedGaussian(sigma),\n          network_utils.ClipToSpec(environment_spec.actions),\n      ]\n\n    # Return a network which sequentially evaluates everything in the stack.\n    return snt.Sequential(stack)",
  "class D4PGBuilder:\n  \"\"\"Builder for D4PG which constructs individual components of the agent.\"\"\"\n\n  def __init__(self, config: D4PGConfig):\n    self._config = config\n\n  def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n  ) -> List[reverb.Table]:\n    \"\"\"Create tables to insert data into.\"\"\"\n    if self._config.samples_per_insert is None:\n      # We will take a samples_per_insert ratio of None to mean that there is\n      # no limit, i.e. this only implies a min size limit.\n      limiter = reverb.rate_limiters.MinSize(self._config.min_replay_size)\n\n    else:\n      # Create enough of an error buffer to give a 10% tolerance in rate.\n      samples_per_insert_tolerance = 0.1 * self._config.samples_per_insert\n      error_buffer = self._config.min_replay_size * samples_per_insert_tolerance\n      limiter = reverb.rate_limiters.SampleToInsertRatio(\n          min_size_to_sample=self._config.min_replay_size,\n          samples_per_insert=self._config.samples_per_insert,\n          error_buffer=error_buffer)\n\n    replay_table = reverb.Table(\n        name=self._config.replay_table_name,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=self._config.max_replay_size,\n        rate_limiter=limiter,\n        signature=reverb_adders.NStepTransitionAdder.signature(\n            environment_spec))\n\n    return [replay_table]\n\n  def make_dataset_iterator(\n      self,\n      reverb_client: reverb.Client,\n  ) -> Iterator[reverb.ReplaySample]:\n    \"\"\"Create a dataset iterator to use for learning/updating the agent.\"\"\"\n    # The dataset provides an interface to sample from replay.\n    dataset = datasets.make_reverb_dataset(\n        table=self._config.replay_table_name,\n        server_address=reverb_client.server_address,\n        batch_size=self._config.batch_size,\n        prefetch_size=self._config.prefetch_size)\n\n    replicator = get_replicator(self._config.accelerator)\n    dataset = replicator.experimental_distribute_dataset(dataset)\n\n    # TODO(b/155086959): Fix type stubs and remove.\n    return iter(dataset)  # pytype: disable=wrong-arg-types\n\n  def make_adder(\n      self,\n      replay_client: reverb.Client,\n  ) -> adders.Adder:\n    \"\"\"Create an adder which records data generated by the actor/environment.\"\"\"\n    return reverb_adders.NStepTransitionAdder(\n        priority_fns={self._config.replay_table_name: lambda x: 1.},\n        client=replay_client,\n        n_step=self._config.n_step,\n        discount=self._config.discount)\n\n  def make_actor(\n      self,\n      policy_network: snt.Module,\n      adder: Optional[adders.Adder] = None,\n      variable_source: Optional[core.VariableSource] = None,\n  ):\n    \"\"\"Create an actor instance.\"\"\"\n    if variable_source:\n      # Create the variable client responsible for keeping the actor up-to-date.\n      variable_client = variable_utils.VariableClient(\n          client=variable_source,\n          variables={'policy': policy_network.variables},\n          update_period=self._config.variable_update_period,\n      )\n\n      # Make sure not to use a random policy after checkpoint restoration by\n      # assigning variables before running the environment loop.\n      variable_client.update_and_wait()\n\n    else:\n      variable_client = None\n\n    # Create the actor which defines how we take actions.\n    return actors.FeedForwardActor(\n        policy_network=policy_network,\n        adder=adder,\n        variable_client=variable_client,\n    )\n\n  def make_learner(\n      self,\n      networks: Tuple[D4PGNetworks, D4PGNetworks],\n      dataset: Iterator[reverb.ReplaySample],\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = False,\n  ):\n    \"\"\"Creates an instance of the learner.\"\"\"\n    online_networks, target_networks = networks\n\n    # The learner updates the parameters (and initializes them).\n    return learning.D4PGLearner(\n        policy_network=online_networks.policy_network,\n        critic_network=online_networks.critic_network,\n        observation_network=online_networks.observation_network,\n        target_policy_network=target_networks.policy_network,\n        target_critic_network=target_networks.critic_network,\n        target_observation_network=target_networks.observation_network,\n        policy_optimizer=self._config.policy_optimizer,\n        critic_optimizer=self._config.critic_optimizer,\n        clipping=self._config.clipping,\n        discount=self._config.discount,\n        target_update_period=self._config.target_update_period,\n        dataset_iterator=dataset,\n        replicator=get_replicator(self._config.accelerator),\n        counter=counter,\n        logger=logger,\n        checkpoint=checkpoint,\n    )",
  "class D4PG(agent.Agent):\n  \"\"\"D4PG Agent.\n\n  This implements a single-process D4PG agent. This is an actor-critic algorithm\n  that generates data via a behavior policy, inserts N-step transitions into\n  a replay buffer, and periodically updates the policy (and as a result the\n  behavior) by sampling uniformly from this buffer.\n  \"\"\"\n\n  def __init__(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy_network: snt.Module,\n      critic_network: snt.Module,\n      observation_network: types.TensorTransformation = tf.identity,\n      accelerator: Optional[str] = None,\n      discount: float = 0.99,\n      batch_size: int = 256,\n      prefetch_size: int = 4,\n      target_update_period: int = 100,\n      policy_optimizer: Optional[snt.Optimizer] = None,\n      critic_optimizer: Optional[snt.Optimizer] = None,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1000000,\n      samples_per_insert: float = 32.0,\n      n_step: int = 5,\n      sigma: float = 0.3,\n      clipping: bool = True,\n      replay_table_name: str = reverb_adders.DEFAULT_PRIORITY_TABLE,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = True,\n  ):\n    \"\"\"Initialize the agent.\n\n    Args:\n      environment_spec: description of the actions, observations, etc.\n      policy_network: the online (optimized) policy.\n      critic_network: the online critic.\n      observation_network: optional network to transform the observations before\n        they are fed into any network.\n      accelerator: 'TPU', 'GPU', or 'CPU'. If omitted, the first available\n        accelerator type from ['TPU', 'GPU', 'CPU'] will be selected.\n      discount: discount to use for TD updates.\n      batch_size: batch size for updates.\n      prefetch_size: size to prefetch from replay.\n      target_update_period: number of learner steps to perform before updating\n        the target networks.\n      policy_optimizer: optimizer for the policy network updates.\n      critic_optimizer: optimizer for the critic network updates.\n      min_replay_size: minimum replay size before updating.\n      max_replay_size: maximum replay size.\n      samples_per_insert: number of samples to take from replay for every insert\n        that is made.\n      n_step: number of steps to squash into a single transition.\n      sigma: standard deviation of zero-mean, Gaussian exploration noise.\n      clipping: whether to clip gradients by global norm.\n      replay_table_name: string indicating what name to give the replay table.\n      counter: counter object used to keep track of steps.\n      logger: logger object to be used by learner.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n    \"\"\"\n    if not accelerator:\n      accelerator = _get_first_available_accelerator_type(['TPU', 'GPU', 'CPU'])\n\n    # Create the Builder object which will internally create agent components.\n    builder = D4PGBuilder(\n        # TODO(mwhoffman): pass the config dataclass in directly.\n        # TODO(mwhoffman): use the limiter rather than the workaround below.\n        # Right now this modifies min_replay_size and samples_per_insert so that\n        # they are not controlled by a limiter and are instead handled by the\n        # Agent base class (the above TODO directly references this behavior).\n        D4PGConfig(\n            accelerator=accelerator,\n            discount=discount,\n            batch_size=batch_size,\n            prefetch_size=prefetch_size,\n            target_update_period=target_update_period,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            min_replay_size=1,  # Let the Agent class handle this.\n            max_replay_size=max_replay_size,\n            samples_per_insert=None,  # Let the Agent class handle this.\n            n_step=n_step,\n            sigma=sigma,\n            clipping=clipping,\n            replay_table_name=replay_table_name,\n        ))\n\n    replicator = get_replicator(accelerator)\n\n    with replicator.scope():\n      # TODO(mwhoffman): pass the network dataclass in directly.\n      online_networks = D4PGNetworks(policy_network=policy_network,\n                                     critic_network=critic_network,\n                                     observation_network=observation_network)\n\n      # Target networks are just a copy of the online networks.\n      target_networks = copy.deepcopy(online_networks)\n\n      # Initialize the networks.\n      online_networks.init(environment_spec)\n      target_networks.init(environment_spec)\n\n    # TODO(mwhoffman): either make this Dataclass or pass only one struct.\n    # The network struct passed to make_learner is just a tuple for the\n    # time-being (for backwards compatibility).\n    networks = (online_networks, target_networks)\n\n    # Create the behavior policy.\n    policy_network = online_networks.make_policy(environment_spec, sigma)\n\n    # Create the replay server and grab its address.\n    replay_tables = builder.make_replay_tables(environment_spec)\n    replay_server = reverb.Server(replay_tables, port=None)\n    replay_client = reverb.Client(f'localhost:{replay_server.port}')\n\n    # Create actor, dataset, and learner for generating, storing, and consuming\n    # data respectively.\n    adder = builder.make_adder(replay_client)\n    actor = builder.make_actor(policy_network, adder)\n    dataset = builder.make_dataset_iterator(replay_client)\n    learner = builder.make_learner(networks, dataset, counter, logger,\n                                   checkpoint)\n\n    super().__init__(\n        actor=actor,\n        learner=learner,\n        min_observations=max(batch_size, min_replay_size),\n        observations_per_step=float(batch_size) / samples_per_insert)\n\n    # Save the replay so we don't garbage collect it.\n    self._replay_server = replay_server",
  "def _ensure_accelerator(accelerator: str) -> str:\n  \"\"\"Checks for the existence of the expected accelerator type.\n\n  Args:\n    accelerator: 'CPU', 'GPU' or 'TPU'.\n\n  Returns:\n    The validated `accelerator` argument.\n\n  Raises:\n    RuntimeError: Thrown if the expected accelerator isn't found.\n  \"\"\"\n  devices = tf.config.get_visible_devices(device_type=accelerator)\n\n  if devices:\n    return accelerator\n  else:\n    error_messages = [f'Couldn\\'t find any {accelerator} devices.',\n                      'tf.config.get_visible_devices() returned:']\n    error_messages.extend([str(d) for d in devices])\n    raise RuntimeError('\\n'.join(error_messages))",
  "def _get_first_available_accelerator_type(\n    wishlist: Sequence[str] = ('TPU', 'GPU', 'CPU')) -> str:\n  \"\"\"Returns the first available accelerator type listed in a wishlist.\n\n  Args:\n    wishlist: A sequence of elements from {'CPU', 'GPU', 'TPU'}, listed in\n      order of descending preference.\n\n  Returns:\n    The first available accelerator type from `wishlist`.\n\n  Raises:\n    RuntimeError: Thrown if no accelerators from the `wishlist` are found.\n  \"\"\"\n  get_visible_devices = tf.config.get_visible_devices\n\n  for wishlist_device in wishlist:\n    devices = get_visible_devices(device_type=wishlist_device)\n    if devices:\n      return wishlist_device\n\n  available = ', '.join(\n      sorted(frozenset([d.type for d in get_visible_devices()])))\n  raise RuntimeError(\n      'Couldn\\'t find any devices from {wishlist}.' +\n      f'Only the following types are available: {available}.')",
  "def get_replicator(accelerator: Optional[str]) -> Replicator:\n  \"\"\"Returns a replicator instance appropriate for the given accelerator.\n\n  This caches the instance using functools.cache, so that only one replicator\n  is instantiated per process and argument value.\n\n  Args:\n    accelerator: None, 'TPU', 'GPU', or 'CPU'. If None, the first available\n      accelerator type will be chosen from ('TPU', 'GPU', 'CPU').\n\n  Returns:\n    A replicator, for replciating weights, datasets, and updates across\n    one or more accelerators.\n  \"\"\"\n  if accelerator:\n    accelerator = _ensure_accelerator(accelerator)\n  else:\n    accelerator = _get_first_available_accelerator_type()\n\n  if accelerator == 'TPU':\n    tf.tpu.experimental.initialize_tpu_system()\n    return snt.distribute.TpuReplicator()\n  else:\n    return snt.distribute.Replicator()",
  "def __init__(\n      self,\n      policy_network: snt.Module,\n      critic_network: snt.Module,\n      observation_network: types.TensorTransformation,\n  ):\n    # This method is implemented (rather than added by the dataclass decorator)\n    # in order to allow observation network to be passed as an arbitrary tensor\n    # transformation rather than as a snt Module.\n    # TODO(mwhoffman): use Protocol rather than Module/TensorTransformation.\n    self.policy_network = policy_network\n    self.critic_network = critic_network\n    self.observation_network = utils.to_sonnet_module(observation_network)",
  "def init(self, environment_spec: specs.EnvironmentSpec):\n    \"\"\"Initialize the networks given an environment spec.\"\"\"\n    # Get observation and action specs.\n    act_spec = environment_spec.actions\n    obs_spec = environment_spec.observations\n\n    # Create variables for the observation net and, as a side-effect, get a\n    # spec describing the embedding space.\n    emb_spec = utils.create_variables(self.observation_network, [obs_spec])\n\n    # Create variables for the policy and critic nets.\n    _ = utils.create_variables(self.policy_network, [emb_spec])\n    _ = utils.create_variables(self.critic_network, [emb_spec, act_spec])",
  "def make_policy(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      sigma: float = 0.0,\n  ) -> snt.Module:\n    \"\"\"Create a single network which evaluates the policy.\"\"\"\n    # Stack the observation and policy networks.\n    stack = [\n        self.observation_network,\n        self.policy_network,\n    ]\n\n    # If a stochastic/non-greedy policy is requested, add Gaussian noise on\n    # top to enable a simple form of exploration.\n    # TODO(mwhoffman): Refactor this to remove it from the class.\n    if sigma > 0.0:\n      stack += [\n          network_utils.ClippedGaussian(sigma),\n          network_utils.ClipToSpec(environment_spec.actions),\n      ]\n\n    # Return a network which sequentially evaluates everything in the stack.\n    return snt.Sequential(stack)",
  "def __init__(self, config: D4PGConfig):\n    self._config = config",
  "def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n  ) -> List[reverb.Table]:\n    \"\"\"Create tables to insert data into.\"\"\"\n    if self._config.samples_per_insert is None:\n      # We will take a samples_per_insert ratio of None to mean that there is\n      # no limit, i.e. this only implies a min size limit.\n      limiter = reverb.rate_limiters.MinSize(self._config.min_replay_size)\n\n    else:\n      # Create enough of an error buffer to give a 10% tolerance in rate.\n      samples_per_insert_tolerance = 0.1 * self._config.samples_per_insert\n      error_buffer = self._config.min_replay_size * samples_per_insert_tolerance\n      limiter = reverb.rate_limiters.SampleToInsertRatio(\n          min_size_to_sample=self._config.min_replay_size,\n          samples_per_insert=self._config.samples_per_insert,\n          error_buffer=error_buffer)\n\n    replay_table = reverb.Table(\n        name=self._config.replay_table_name,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=self._config.max_replay_size,\n        rate_limiter=limiter,\n        signature=reverb_adders.NStepTransitionAdder.signature(\n            environment_spec))\n\n    return [replay_table]",
  "def make_dataset_iterator(\n      self,\n      reverb_client: reverb.Client,\n  ) -> Iterator[reverb.ReplaySample]:\n    \"\"\"Create a dataset iterator to use for learning/updating the agent.\"\"\"\n    # The dataset provides an interface to sample from replay.\n    dataset = datasets.make_reverb_dataset(\n        table=self._config.replay_table_name,\n        server_address=reverb_client.server_address,\n        batch_size=self._config.batch_size,\n        prefetch_size=self._config.prefetch_size)\n\n    replicator = get_replicator(self._config.accelerator)\n    dataset = replicator.experimental_distribute_dataset(dataset)\n\n    # TODO(b/155086959): Fix type stubs and remove.\n    return iter(dataset)",
  "def make_adder(\n      self,\n      replay_client: reverb.Client,\n  ) -> adders.Adder:\n    \"\"\"Create an adder which records data generated by the actor/environment.\"\"\"\n    return reverb_adders.NStepTransitionAdder(\n        priority_fns={self._config.replay_table_name: lambda x: 1.},\n        client=replay_client,\n        n_step=self._config.n_step,\n        discount=self._config.discount)",
  "def make_actor(\n      self,\n      policy_network: snt.Module,\n      adder: Optional[adders.Adder] = None,\n      variable_source: Optional[core.VariableSource] = None,\n  ):\n    \"\"\"Create an actor instance.\"\"\"\n    if variable_source:\n      # Create the variable client responsible for keeping the actor up-to-date.\n      variable_client = variable_utils.VariableClient(\n          client=variable_source,\n          variables={'policy': policy_network.variables},\n          update_period=self._config.variable_update_period,\n      )\n\n      # Make sure not to use a random policy after checkpoint restoration by\n      # assigning variables before running the environment loop.\n      variable_client.update_and_wait()\n\n    else:\n      variable_client = None\n\n    # Create the actor which defines how we take actions.\n    return actors.FeedForwardActor(\n        policy_network=policy_network,\n        adder=adder,\n        variable_client=variable_client,\n    )",
  "def make_learner(\n      self,\n      networks: Tuple[D4PGNetworks, D4PGNetworks],\n      dataset: Iterator[reverb.ReplaySample],\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = False,\n  ):\n    \"\"\"Creates an instance of the learner.\"\"\"\n    online_networks, target_networks = networks\n\n    # The learner updates the parameters (and initializes them).\n    return learning.D4PGLearner(\n        policy_network=online_networks.policy_network,\n        critic_network=online_networks.critic_network,\n        observation_network=online_networks.observation_network,\n        target_policy_network=target_networks.policy_network,\n        target_critic_network=target_networks.critic_network,\n        target_observation_network=target_networks.observation_network,\n        policy_optimizer=self._config.policy_optimizer,\n        critic_optimizer=self._config.critic_optimizer,\n        clipping=self._config.clipping,\n        discount=self._config.discount,\n        target_update_period=self._config.target_update_period,\n        dataset_iterator=dataset,\n        replicator=get_replicator(self._config.accelerator),\n        counter=counter,\n        logger=logger,\n        checkpoint=checkpoint,\n    )",
  "def __init__(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy_network: snt.Module,\n      critic_network: snt.Module,\n      observation_network: types.TensorTransformation = tf.identity,\n      accelerator: Optional[str] = None,\n      discount: float = 0.99,\n      batch_size: int = 256,\n      prefetch_size: int = 4,\n      target_update_period: int = 100,\n      policy_optimizer: Optional[snt.Optimizer] = None,\n      critic_optimizer: Optional[snt.Optimizer] = None,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1000000,\n      samples_per_insert: float = 32.0,\n      n_step: int = 5,\n      sigma: float = 0.3,\n      clipping: bool = True,\n      replay_table_name: str = reverb_adders.DEFAULT_PRIORITY_TABLE,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = True,\n  ):\n    \"\"\"Initialize the agent.\n\n    Args:\n      environment_spec: description of the actions, observations, etc.\n      policy_network: the online (optimized) policy.\n      critic_network: the online critic.\n      observation_network: optional network to transform the observations before\n        they are fed into any network.\n      accelerator: 'TPU', 'GPU', or 'CPU'. If omitted, the first available\n        accelerator type from ['TPU', 'GPU', 'CPU'] will be selected.\n      discount: discount to use for TD updates.\n      batch_size: batch size for updates.\n      prefetch_size: size to prefetch from replay.\n      target_update_period: number of learner steps to perform before updating\n        the target networks.\n      policy_optimizer: optimizer for the policy network updates.\n      critic_optimizer: optimizer for the critic network updates.\n      min_replay_size: minimum replay size before updating.\n      max_replay_size: maximum replay size.\n      samples_per_insert: number of samples to take from replay for every insert\n        that is made.\n      n_step: number of steps to squash into a single transition.\n      sigma: standard deviation of zero-mean, Gaussian exploration noise.\n      clipping: whether to clip gradients by global norm.\n      replay_table_name: string indicating what name to give the replay table.\n      counter: counter object used to keep track of steps.\n      logger: logger object to be used by learner.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n    \"\"\"\n    if not accelerator:\n      accelerator = _get_first_available_accelerator_type(['TPU', 'GPU', 'CPU'])\n\n    # Create the Builder object which will internally create agent components.\n    builder = D4PGBuilder(\n        # TODO(mwhoffman): pass the config dataclass in directly.\n        # TODO(mwhoffman): use the limiter rather than the workaround below.\n        # Right now this modifies min_replay_size and samples_per_insert so that\n        # they are not controlled by a limiter and are instead handled by the\n        # Agent base class (the above TODO directly references this behavior).\n        D4PGConfig(\n            accelerator=accelerator,\n            discount=discount,\n            batch_size=batch_size,\n            prefetch_size=prefetch_size,\n            target_update_period=target_update_period,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            min_replay_size=1,  # Let the Agent class handle this.\n            max_replay_size=max_replay_size,\n            samples_per_insert=None,  # Let the Agent class handle this.\n            n_step=n_step,\n            sigma=sigma,\n            clipping=clipping,\n            replay_table_name=replay_table_name,\n        ))\n\n    replicator = get_replicator(accelerator)\n\n    with replicator.scope():\n      # TODO(mwhoffman): pass the network dataclass in directly.\n      online_networks = D4PGNetworks(policy_network=policy_network,\n                                     critic_network=critic_network,\n                                     observation_network=observation_network)\n\n      # Target networks are just a copy of the online networks.\n      target_networks = copy.deepcopy(online_networks)\n\n      # Initialize the networks.\n      online_networks.init(environment_spec)\n      target_networks.init(environment_spec)\n\n    # TODO(mwhoffman): either make this Dataclass or pass only one struct.\n    # The network struct passed to make_learner is just a tuple for the\n    # time-being (for backwards compatibility).\n    networks = (online_networks, target_networks)\n\n    # Create the behavior policy.\n    policy_network = online_networks.make_policy(environment_spec, sigma)\n\n    # Create the replay server and grab its address.\n    replay_tables = builder.make_replay_tables(environment_spec)\n    replay_server = reverb.Server(replay_tables, port=None)\n    replay_client = reverb.Client(f'localhost:{replay_server.port}')\n\n    # Create actor, dataset, and learner for generating, storing, and consuming\n    # data respectively.\n    adder = builder.make_adder(replay_client)\n    actor = builder.make_actor(policy_network, adder)\n    dataset = builder.make_dataset_iterator(replay_client)\n    learner = builder.make_learner(networks, dataset, counter, logger,\n                                   checkpoint)\n\n    super().__init__(\n        actor=actor,\n        learner=learner,\n        min_observations=max(batch_size, min_replay_size),\n        observations_per_step=float(batch_size) / samples_per_insert)\n\n    # Save the replay so we don't garbage collect it.\n    self._replay_server = replay_server",
  "class D4PGLearner(acme.Learner):\n  \"\"\"D4PG learner.\n\n  This is the learning component of a D4PG agent. IE it takes a dataset as input\n  and implements update functionality to learn from this dataset.\n  \"\"\"\n\n  def __init__(\n      self,\n      policy_network: snt.Module,\n      critic_network: snt.Module,\n      target_policy_network: snt.Module,\n      target_critic_network: snt.Module,\n      discount: float,\n      target_update_period: int,\n      dataset_iterator: Iterator[reverb.ReplaySample],\n      replicator: Optional[Replicator] = None,\n      observation_network: types.TensorTransformation = lambda x: x,\n      target_observation_network: types.TensorTransformation = lambda x: x,\n      policy_optimizer: Optional[snt.Optimizer] = None,\n      critic_optimizer: Optional[snt.Optimizer] = None,\n      clipping: bool = True,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = True,\n  ):\n    \"\"\"Initializes the learner.\n\n    Args:\n      policy_network: the online (optimized) policy.\n      critic_network: the online critic.\n      target_policy_network: the target policy (which lags behind the online\n        policy).\n      target_critic_network: the target critic.\n      discount: discount to use for TD updates.\n      target_update_period: number of learner steps to perform before updating\n        the target networks.\n      dataset_iterator: dataset to learn from, whether fixed or from a replay\n        buffer (see `acme.datasets.reverb.make_reverb_dataset` documentation).\n      replicator: Replicates variables and their update methods over multiple\n        accelerators, such as the multiple chips in a TPU.\n      observation_network: an optional online network to process observations\n        before the policy and the critic.\n      target_observation_network: the target observation network.\n      policy_optimizer: the optimizer to be applied to the DPG (policy) loss.\n      critic_optimizer: the optimizer to be applied to the distributional\n        Bellman loss.\n      clipping: whether to clip gradients by global norm.\n      counter: counter object used to keep track of steps.\n      logger: logger object to be used by learner.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n    \"\"\"\n\n    # Store online and target networks.\n    self._policy_network = policy_network\n    self._critic_network = critic_network\n    self._target_policy_network = target_policy_network\n    self._target_critic_network = target_critic_network\n\n    # Make sure observation networks are snt.Module's so they have variables.\n    self._observation_network = tf2_utils.to_sonnet_module(observation_network)\n    self._target_observation_network = tf2_utils.to_sonnet_module(\n        target_observation_network)\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger('learner')\n\n    # Other learner parameters.\n    self._discount = discount\n    self._clipping = clipping\n\n    # Replicates Variables across multiple accelerators\n    if not replicator:\n      accelerator = _get_first_available_accelerator_type()\n      if accelerator == 'TPU':\n        replicator = snt.distribute.TpuReplicator()\n      else:\n        replicator = snt.distribute.Replicator()\n\n    self._replicator = replicator\n\n    with replicator.scope():\n      # Necessary to track when to update target networks.\n      self._num_steps = tf.Variable(0, dtype=tf.int32)\n      self._target_update_period = target_update_period\n\n      # Create optimizers if they aren't given.\n      self._critic_optimizer = critic_optimizer or snt.optimizers.Adam(1e-4)\n      self._policy_optimizer = policy_optimizer or snt.optimizers.Adam(1e-4)\n\n    # Batch dataset and create iterator.\n    self._iterator = dataset_iterator\n\n    # Expose the variables.\n    policy_network_to_expose = snt.Sequential(\n        [self._target_observation_network, self._target_policy_network])\n    self._variables = {\n        'critic': self._target_critic_network.variables,\n        'policy': policy_network_to_expose.variables,\n    }\n\n    # Create a checkpointer and snapshotter objects.\n    self._checkpointer = None\n    self._snapshotter = None\n\n    if checkpoint:\n      self._checkpointer = tf2_savers.Checkpointer(\n          subdirectory='d4pg_learner',\n          objects_to_save={\n              'counter': self._counter,\n              'policy': self._policy_network,\n              'critic': self._critic_network,\n              'observation': self._observation_network,\n              'target_policy': self._target_policy_network,\n              'target_critic': self._target_critic_network,\n              'target_observation': self._target_observation_network,\n              'policy_optimizer': self._policy_optimizer,\n              'critic_optimizer': self._critic_optimizer,\n              'num_steps': self._num_steps,\n          })\n      critic_mean = snt.Sequential(\n          [self._critic_network, acme_nets.StochasticMeanHead()])\n      self._snapshotter = tf2_savers.Snapshotter(\n          objects_to_save={\n              'policy': self._policy_network,\n              'critic': critic_mean,\n          })\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None\n\n  @tf.function\n  def _step(self, sample) -> Dict[str, tf.Tensor]:\n    transitions: types.Transition = sample.data  # Assuming ReverbSample.\n\n    # Cast the additional discount to match the environment discount dtype.\n    discount = tf.cast(self._discount, dtype=transitions.discount.dtype)\n\n    with tf.GradientTape(persistent=True) as tape:\n      # Maybe transform the observation before feeding into policy and critic.\n      # Transforming the observations this way at the start of the learning\n      # step effectively means that the policy and critic share observation\n      # network weights.\n      o_tm1 = self._observation_network(transitions.observation)\n      o_t = self._target_observation_network(transitions.next_observation)\n      # This stop_gradient prevents gradients to propagate into the target\n      # observation network. In addition, since the online policy network is\n      # evaluated at o_t, this also means the policy loss does not influence\n      # the observation network training.\n      o_t = tree.map_structure(tf.stop_gradient, o_t)\n\n      # Critic learning.\n      q_tm1 = self._critic_network(o_tm1, transitions.action)\n      q_t = self._target_critic_network(o_t, self._target_policy_network(o_t))\n\n      # Critic loss.\n      critic_loss = losses.categorical(q_tm1, transitions.reward,\n                                       discount * transitions.discount, q_t)\n      critic_loss = tf.reduce_mean(critic_loss, axis=[0])\n\n      # Actor learning.\n      dpg_a_t = self._policy_network(o_t)\n      dpg_z_t = self._critic_network(o_t, dpg_a_t)\n      dpg_q_t = dpg_z_t.mean()\n\n      # Actor loss. If clipping is true use dqda clipping and clip the norm.\n      dqda_clipping = 1.0 if self._clipping else None\n      policy_loss = losses.dpg(\n          dpg_q_t,\n          dpg_a_t,\n          tape=tape,\n          dqda_clipping=dqda_clipping,\n          clip_norm=self._clipping)\n      policy_loss = tf.reduce_mean(policy_loss, axis=[0])\n\n    # Get trainable variables.\n    policy_variables = self._policy_network.trainable_variables\n    critic_variables = (\n        # In this agent, the critic loss trains the observation network.\n        self._observation_network.trainable_variables +\n        self._critic_network.trainable_variables)\n\n    # Compute gradients.\n    replica_context = tf.distribute.get_replica_context()\n    policy_gradients = _average_gradients_across_replicas(\n        replica_context,\n        tape.gradient(policy_loss, policy_variables))\n    critic_gradients = _average_gradients_across_replicas(\n        replica_context,\n        tape.gradient(critic_loss, critic_variables))\n\n    # Delete the tape manually because of the persistent=True flag.\n    del tape\n\n    # Maybe clip gradients.\n    if self._clipping:\n      policy_gradients = tf.clip_by_global_norm(policy_gradients, 40.)[0]\n      critic_gradients = tf.clip_by_global_norm(critic_gradients, 40.)[0]\n\n    # Apply gradients.\n    self._policy_optimizer.apply(policy_gradients, policy_variables)\n    self._critic_optimizer.apply(critic_gradients, critic_variables)\n\n    # Losses to track.\n    return {\n        'critic_loss': critic_loss,\n        'policy_loss': policy_loss,\n    }\n\n  @tf.function\n  def _replicated_step(self):\n    # Update target network\n    online_variables = (\n        *self._observation_network.variables,\n        *self._critic_network.variables,\n        *self._policy_network.variables,\n    )\n    target_variables = (\n        *self._target_observation_network.variables,\n        *self._target_critic_network.variables,\n        *self._target_policy_network.variables,\n    )\n\n    # Make online -> target network update ops.\n    if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n      for src, dest in zip(online_variables, target_variables):\n        dest.assign(src)\n    self._num_steps.assign_add(1)\n\n    # Get data from replay (dropping extras if any). Note there is no\n    # extra data here because we do not insert any into Reverb.\n    sample = next(self._iterator)\n\n    # This mirrors the structure of the fetches returned by self._step(),\n    # but the Tensors are replaced with replicated Tensors, one per accelerator.\n    replicated_fetches = self._replicator.run(self._step, args=(sample,))\n\n    def reduce_mean_over_replicas(replicated_value):\n      \"\"\"Averages a replicated_value across replicas.\"\"\"\n      # The \"axis=None\" arg means reduce across replicas, not internal axes.\n      return self._replicator.reduce(\n          reduce_op=tf.distribute.ReduceOp.MEAN,\n          value=replicated_value,\n          axis=None)\n\n    fetches = tree.map_structure(reduce_mean_over_replicas, replicated_fetches)\n\n    return fetches\n\n  def step(self):\n    # Run the learning step.\n    fetches = self._replicated_step()\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n    fetches.update(counts)\n\n    # Checkpoint and attempt to write the logs.\n    if self._checkpointer is not None:\n      self._checkpointer.save()\n    if self._snapshotter is not None:\n      self._snapshotter.save()\n    self._logger.write(fetches)\n\n  def get_variables(self, names: List[str]) -> List[List[np.ndarray]]:\n    return [tf2_utils.to_numpy(self._variables[name]) for name in names]",
  "def _get_first_available_accelerator_type(\n    wishlist: Sequence[str] = ('TPU', 'GPU', 'CPU')) -> str:\n  \"\"\"Returns the first available accelerator type listed in a wishlist.\n\n  Args:\n    wishlist: A sequence of elements from {'CPU', 'GPU', 'TPU'}, listed in\n      order of descending preference.\n\n  Returns:\n    The first available accelerator type from `wishlist`.\n\n  Raises:\n    RuntimeError: Thrown if no accelerators from the `wishlist` are found.\n  \"\"\"\n  get_visible_devices = tf.config.get_visible_devices\n\n  for wishlist_device in wishlist:\n    devices = get_visible_devices(device_type=wishlist_device)\n    if devices:\n      return wishlist_device\n\n  available = ', '.join(\n      sorted(frozenset([d.type for d in get_visible_devices()])))\n  raise RuntimeError(\n      'Couldn\\'t find any devices from {wishlist}.' +\n      f'Only the following types are available: {available}.')",
  "def _average_gradients_across_replicas(replica_context, gradients):\n  \"\"\"Computes the average gradient across replicas.\n\n  This computes the gradient locally on this device, then copies over the\n  gradients computed on the other replicas, and takes the average across\n  replicas.\n\n  This is faster than copying the gradients from TPU to CPU, and averaging\n  them on the CPU (which is what we do for the losses/fetches).\n\n  Args:\n    replica_context: the return value of `tf.distribute.get_replica_context()`.\n    gradients: The output of tape.gradients(loss, variables)\n\n  Returns:\n    A list of (d_loss/d_varabiable)s.\n  \"\"\"\n\n  # We must remove any Nones from gradients before passing them to all_reduce.\n  # Nones occur when you call tape.gradient(loss, variables) with some\n  # variables that don't affect the loss.\n  # See: https://github.com/tensorflow/tensorflow/issues/783\n  gradients_without_nones = [g for g in gradients if g is not None]\n  original_indices = [i for i, g in enumerate(gradients) if g is not None]\n\n  results_without_nones = replica_context.all_reduce('mean',\n                                                     gradients_without_nones)\n  results = [None] * len(gradients)\n  for ii, result in zip(original_indices, results_without_nones):\n    results[ii] = result\n\n  return results",
  "def __init__(\n      self,\n      policy_network: snt.Module,\n      critic_network: snt.Module,\n      target_policy_network: snt.Module,\n      target_critic_network: snt.Module,\n      discount: float,\n      target_update_period: int,\n      dataset_iterator: Iterator[reverb.ReplaySample],\n      replicator: Optional[Replicator] = None,\n      observation_network: types.TensorTransformation = lambda x: x,\n      target_observation_network: types.TensorTransformation = lambda x: x,\n      policy_optimizer: Optional[snt.Optimizer] = None,\n      critic_optimizer: Optional[snt.Optimizer] = None,\n      clipping: bool = True,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = True,\n  ):\n    \"\"\"Initializes the learner.\n\n    Args:\n      policy_network: the online (optimized) policy.\n      critic_network: the online critic.\n      target_policy_network: the target policy (which lags behind the online\n        policy).\n      target_critic_network: the target critic.\n      discount: discount to use for TD updates.\n      target_update_period: number of learner steps to perform before updating\n        the target networks.\n      dataset_iterator: dataset to learn from, whether fixed or from a replay\n        buffer (see `acme.datasets.reverb.make_reverb_dataset` documentation).\n      replicator: Replicates variables and their update methods over multiple\n        accelerators, such as the multiple chips in a TPU.\n      observation_network: an optional online network to process observations\n        before the policy and the critic.\n      target_observation_network: the target observation network.\n      policy_optimizer: the optimizer to be applied to the DPG (policy) loss.\n      critic_optimizer: the optimizer to be applied to the distributional\n        Bellman loss.\n      clipping: whether to clip gradients by global norm.\n      counter: counter object used to keep track of steps.\n      logger: logger object to be used by learner.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n    \"\"\"\n\n    # Store online and target networks.\n    self._policy_network = policy_network\n    self._critic_network = critic_network\n    self._target_policy_network = target_policy_network\n    self._target_critic_network = target_critic_network\n\n    # Make sure observation networks are snt.Module's so they have variables.\n    self._observation_network = tf2_utils.to_sonnet_module(observation_network)\n    self._target_observation_network = tf2_utils.to_sonnet_module(\n        target_observation_network)\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger('learner')\n\n    # Other learner parameters.\n    self._discount = discount\n    self._clipping = clipping\n\n    # Replicates Variables across multiple accelerators\n    if not replicator:\n      accelerator = _get_first_available_accelerator_type()\n      if accelerator == 'TPU':\n        replicator = snt.distribute.TpuReplicator()\n      else:\n        replicator = snt.distribute.Replicator()\n\n    self._replicator = replicator\n\n    with replicator.scope():\n      # Necessary to track when to update target networks.\n      self._num_steps = tf.Variable(0, dtype=tf.int32)\n      self._target_update_period = target_update_period\n\n      # Create optimizers if they aren't given.\n      self._critic_optimizer = critic_optimizer or snt.optimizers.Adam(1e-4)\n      self._policy_optimizer = policy_optimizer or snt.optimizers.Adam(1e-4)\n\n    # Batch dataset and create iterator.\n    self._iterator = dataset_iterator\n\n    # Expose the variables.\n    policy_network_to_expose = snt.Sequential(\n        [self._target_observation_network, self._target_policy_network])\n    self._variables = {\n        'critic': self._target_critic_network.variables,\n        'policy': policy_network_to_expose.variables,\n    }\n\n    # Create a checkpointer and snapshotter objects.\n    self._checkpointer = None\n    self._snapshotter = None\n\n    if checkpoint:\n      self._checkpointer = tf2_savers.Checkpointer(\n          subdirectory='d4pg_learner',\n          objects_to_save={\n              'counter': self._counter,\n              'policy': self._policy_network,\n              'critic': self._critic_network,\n              'observation': self._observation_network,\n              'target_policy': self._target_policy_network,\n              'target_critic': self._target_critic_network,\n              'target_observation': self._target_observation_network,\n              'policy_optimizer': self._policy_optimizer,\n              'critic_optimizer': self._critic_optimizer,\n              'num_steps': self._num_steps,\n          })\n      critic_mean = snt.Sequential(\n          [self._critic_network, acme_nets.StochasticMeanHead()])\n      self._snapshotter = tf2_savers.Snapshotter(\n          objects_to_save={\n              'policy': self._policy_network,\n              'critic': critic_mean,\n          })\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None",
  "def _step(self, sample) -> Dict[str, tf.Tensor]:\n    transitions: types.Transition = sample.data  # Assuming ReverbSample.\n\n    # Cast the additional discount to match the environment discount dtype.\n    discount = tf.cast(self._discount, dtype=transitions.discount.dtype)\n\n    with tf.GradientTape(persistent=True) as tape:\n      # Maybe transform the observation before feeding into policy and critic.\n      # Transforming the observations this way at the start of the learning\n      # step effectively means that the policy and critic share observation\n      # network weights.\n      o_tm1 = self._observation_network(transitions.observation)\n      o_t = self._target_observation_network(transitions.next_observation)\n      # This stop_gradient prevents gradients to propagate into the target\n      # observation network. In addition, since the online policy network is\n      # evaluated at o_t, this also means the policy loss does not influence\n      # the observation network training.\n      o_t = tree.map_structure(tf.stop_gradient, o_t)\n\n      # Critic learning.\n      q_tm1 = self._critic_network(o_tm1, transitions.action)\n      q_t = self._target_critic_network(o_t, self._target_policy_network(o_t))\n\n      # Critic loss.\n      critic_loss = losses.categorical(q_tm1, transitions.reward,\n                                       discount * transitions.discount, q_t)\n      critic_loss = tf.reduce_mean(critic_loss, axis=[0])\n\n      # Actor learning.\n      dpg_a_t = self._policy_network(o_t)\n      dpg_z_t = self._critic_network(o_t, dpg_a_t)\n      dpg_q_t = dpg_z_t.mean()\n\n      # Actor loss. If clipping is true use dqda clipping and clip the norm.\n      dqda_clipping = 1.0 if self._clipping else None\n      policy_loss = losses.dpg(\n          dpg_q_t,\n          dpg_a_t,\n          tape=tape,\n          dqda_clipping=dqda_clipping,\n          clip_norm=self._clipping)\n      policy_loss = tf.reduce_mean(policy_loss, axis=[0])\n\n    # Get trainable variables.\n    policy_variables = self._policy_network.trainable_variables\n    critic_variables = (\n        # In this agent, the critic loss trains the observation network.\n        self._observation_network.trainable_variables +\n        self._critic_network.trainable_variables)\n\n    # Compute gradients.\n    replica_context = tf.distribute.get_replica_context()\n    policy_gradients = _average_gradients_across_replicas(\n        replica_context,\n        tape.gradient(policy_loss, policy_variables))\n    critic_gradients = _average_gradients_across_replicas(\n        replica_context,\n        tape.gradient(critic_loss, critic_variables))\n\n    # Delete the tape manually because of the persistent=True flag.\n    del tape\n\n    # Maybe clip gradients.\n    if self._clipping:\n      policy_gradients = tf.clip_by_global_norm(policy_gradients, 40.)[0]\n      critic_gradients = tf.clip_by_global_norm(critic_gradients, 40.)[0]\n\n    # Apply gradients.\n    self._policy_optimizer.apply(policy_gradients, policy_variables)\n    self._critic_optimizer.apply(critic_gradients, critic_variables)\n\n    # Losses to track.\n    return {\n        'critic_loss': critic_loss,\n        'policy_loss': policy_loss,\n    }",
  "def _replicated_step(self):\n    # Update target network\n    online_variables = (\n        *self._observation_network.variables,\n        *self._critic_network.variables,\n        *self._policy_network.variables,\n    )\n    target_variables = (\n        *self._target_observation_network.variables,\n        *self._target_critic_network.variables,\n        *self._target_policy_network.variables,\n    )\n\n    # Make online -> target network update ops.\n    if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n      for src, dest in zip(online_variables, target_variables):\n        dest.assign(src)\n    self._num_steps.assign_add(1)\n\n    # Get data from replay (dropping extras if any). Note there is no\n    # extra data here because we do not insert any into Reverb.\n    sample = next(self._iterator)\n\n    # This mirrors the structure of the fetches returned by self._step(),\n    # but the Tensors are replaced with replicated Tensors, one per accelerator.\n    replicated_fetches = self._replicator.run(self._step, args=(sample,))\n\n    def reduce_mean_over_replicas(replicated_value):\n      \"\"\"Averages a replicated_value across replicas.\"\"\"\n      # The \"axis=None\" arg means reduce across replicas, not internal axes.\n      return self._replicator.reduce(\n          reduce_op=tf.distribute.ReduceOp.MEAN,\n          value=replicated_value,\n          axis=None)\n\n    fetches = tree.map_structure(reduce_mean_over_replicas, replicated_fetches)\n\n    return fetches",
  "def step(self):\n    # Run the learning step.\n    fetches = self._replicated_step()\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n    fetches.update(counts)\n\n    # Checkpoint and attempt to write the logs.\n    if self._checkpointer is not None:\n      self._checkpointer.save()\n    if self._snapshotter is not None:\n      self._snapshotter.save()\n    self._logger.write(fetches)",
  "def get_variables(self, names: List[str]) -> List[List[np.ndarray]]:\n    return [tf2_utils.to_numpy(self._variables[name]) for name in names]",
  "def reduce_mean_over_replicas(replicated_value):\n      \"\"\"Averages a replicated_value across replicas.\"\"\"\n      # The \"axis=None\" arg means reduce across replicas, not internal axes.\n      return self._replicator.reduce(\n          reduce_op=tf.distribute.ReduceOp.MEAN,\n          value=replicated_value,\n          axis=None)",
  "def make_default_networks(\n    action_spec: specs.BoundedArray,\n    policy_layer_sizes: Sequence[int] = (256, 256, 256),\n    critic_layer_sizes: Sequence[int] = (512, 512, 256),\n    vmin: float = -150.,\n    vmax: float = 150.,\n    num_atoms: int = 51,\n) -> Mapping[str, types.TensorTransformation]:\n  \"\"\"Creates networks used by the agent.\"\"\"\n\n  # Get total number of action dimensions from action spec.\n  num_dimensions = np.prod(action_spec.shape, dtype=int)\n\n  # Create the shared observation network; here simply a state-less operation.\n  observation_network = tf2_utils.batch_concat\n\n  # Create the policy network.\n  policy_network = snt.Sequential([\n      networks.LayerNormMLP(policy_layer_sizes, activate_final=True),\n      networks.NearZeroInitializedLinear(num_dimensions),\n      networks.TanhToSpec(action_spec),\n  ])\n\n  # Create the critic network.\n  critic_network = snt.Sequential([\n      # The multiplexer concatenates the observations/actions.\n      networks.CriticMultiplexer(),\n      networks.LayerNormMLP(critic_layer_sizes, activate_final=True),\n      networks.DiscreteValuedHead(vmin, vmax, num_atoms),\n  ])\n\n  return {\n      'policy': policy_network,\n      'critic': critic_network,\n      'observation': observation_network,\n  }",
  "def make_networks(action_spec: specs.BoundedArray):\n  \"\"\"Simple networks for testing..\"\"\"\n\n  num_dimensions = np.prod(action_spec.shape, dtype=int)\n\n  policy_network = snt.Sequential([\n      networks.LayerNormMLP([50], activate_final=True),\n      networks.NearZeroInitializedLinear(num_dimensions),\n      networks.TanhToSpec(action_spec)\n  ])\n  # The multiplexer concatenates the (maybe transformed) observations/actions.\n  critic_network = snt.Sequential([\n      networks.CriticMultiplexer(\n          critic_network=networks.LayerNormMLP(\n              [50], activate_final=True)),\n      networks.DiscreteValuedHead(-1., 1., 10)\n  ])\n\n  return {\n      'policy': policy_network,\n      'critic': critic_network,\n      'observation': tf2_utils.batch_concat,\n  }",
  "class DistributedAgentTest(absltest.TestCase):\n  \"\"\"Simple integration/smoke test for the distributed agent.\"\"\"\n\n  def test_control_suite(self):\n    \"\"\"Tests that the agent can run on the control suite without crashing.\"\"\"\n\n    agent = d4pg.DistributedD4PG(\n        environment_factory=lambda x: fakes.ContinuousEnvironment(bounded=True),\n        network_factory=make_networks,\n        accelerator='CPU',\n        num_actors=2,\n        batch_size=32,\n        min_replay_size=32,\n        max_replay_size=1000,\n    )\n    program = agent.build()\n\n    (learner_node,) = program.groups['learner']\n    learner_node.disable_run()\n\n    lp.launch(program, launch_type='test_mt')\n\n    learner: acme.Learner = learner_node.create_handle().dereference()\n\n    for _ in range(5):\n      learner.step()",
  "def test_control_suite(self):\n    \"\"\"Tests that the agent can run on the control suite without crashing.\"\"\"\n\n    agent = d4pg.DistributedD4PG(\n        environment_factory=lambda x: fakes.ContinuousEnvironment(bounded=True),\n        network_factory=make_networks,\n        accelerator='CPU',\n        num_actors=2,\n        batch_size=32,\n        min_replay_size=32,\n        max_replay_size=1000,\n    )\n    program = agent.build()\n\n    (learner_node,) = program.groups['learner']\n    learner_node.disable_run()\n\n    lp.launch(program, launch_type='test_mt')\n\n    learner: acme.Learner = learner_node.create_handle().dereference()\n\n    for _ in range(5):\n      learner.step()",
  "def make_networks(\n    action_spec: types.NestedSpec,\n    policy_layer_sizes: Sequence[int] = (10, 10),\n    critic_layer_sizes: Sequence[int] = (10, 10),\n    vmin: float = -150.,\n    vmax: float = 150.,\n    num_atoms: int = 51,\n) -> Dict[str, snt.Module]:\n  \"\"\"Creates networks used by the agent.\"\"\"\n\n  num_dimensions = np.prod(action_spec.shape, dtype=int)\n  policy_layer_sizes = list(policy_layer_sizes) + [num_dimensions]\n\n  policy_network = snt.Sequential(\n      [networks.LayerNormMLP(policy_layer_sizes), tf.tanh])\n  critic_network = snt.Sequential([\n      networks.CriticMultiplexer(\n          critic_network=networks.LayerNormMLP(\n              critic_layer_sizes, activate_final=True)),\n      networks.DiscreteValuedHead(vmin, vmax, num_atoms)\n  ])\n\n  return {\n      'policy': policy_network,\n      'critic': critic_network,\n  }",
  "class D4PGTest(absltest.TestCase):\n\n  def test_d4pg(self):\n    # Create a fake environment to test with.\n    environment = fakes.ContinuousEnvironment(episode_length=10, bounded=True)\n    spec = specs.make_environment_spec(environment)\n\n    # Create the networks.\n    agent_networks = make_networks(spec.actions)\n\n    # Construct the agent.\n    agent = d4pg.D4PG(\n        environment_spec=spec,\n        accelerator='CPU',\n        policy_network=agent_networks['policy'],\n        critic_network=agent_networks['critic'],\n        batch_size=10,\n        samples_per_insert=2,\n        min_replay_size=10,\n    )\n\n    # Try running the environment loop. We have no assertions here because all\n    # we care about is that the agent runs without raising any errors.\n    loop = acme.EnvironmentLoop(environment, agent)\n    loop.run(num_episodes=2)",
  "def test_d4pg(self):\n    # Create a fake environment to test with.\n    environment = fakes.ContinuousEnvironment(episode_length=10, bounded=True)\n    spec = specs.make_environment_spec(environment)\n\n    # Create the networks.\n    agent_networks = make_networks(spec.actions)\n\n    # Construct the agent.\n    agent = d4pg.D4PG(\n        environment_spec=spec,\n        accelerator='CPU',\n        policy_network=agent_networks['policy'],\n        critic_network=agent_networks['critic'],\n        batch_size=10,\n        samples_per_insert=2,\n        min_replay_size=10,\n    )\n\n    # Try running the environment loop. We have no assertions here because all\n    # we care about is that the agent runs without raising any errors.\n    loop = acme.EnvironmentLoop(environment, agent)\n    loop.run(num_episodes=2)",
  "class DistributedD4PG:\n  \"\"\"Program definition for D4PG.\"\"\"\n\n  def __init__(\n      self,\n      environment_factory: Callable[[bool], dm_env.Environment],\n      network_factory: Callable[[specs.BoundedArray], Dict[str, snt.Module]],\n      accelerator: Optional[str] = None,\n      num_actors: int = 1,\n      num_caches: int = 0,\n      environment_spec: Optional[specs.EnvironmentSpec] = None,\n      batch_size: int = 256,\n      prefetch_size: int = 4,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1000000,\n      samples_per_insert: Optional[float] = 32.0,\n      n_step: int = 5,\n      sigma: float = 0.3,\n      clipping: bool = True,\n      discount: float = 0.99,\n      policy_optimizer: Optional[snt.Optimizer] = None,\n      critic_optimizer: Optional[snt.Optimizer] = None,\n      target_update_period: int = 100,\n      variable_update_period: int = 1000,\n      max_actor_steps: Optional[int] = None,\n      log_every: float = 10.0,\n  ):\n\n    if accelerator is not None and accelerator not in _ACCELERATORS:\n      raise ValueError(f'Accelerator must be one of {_ACCELERATORS}, '\n                       f'not \"{accelerator}\".')\n\n    if not environment_spec:\n      environment_spec = specs.make_environment_spec(environment_factory(False))\n\n    # TODO(mwhoffman): Make network_factory directly return the struct.\n    # TODO(mwhoffman): Make the factory take the entire spec.\n    def wrapped_network_factory(action_spec):\n      networks_dict = network_factory(action_spec)\n      networks = agent.D4PGNetworks(\n          policy_network=networks_dict.get('policy'),\n          critic_network=networks_dict.get('critic'),\n          observation_network=networks_dict.get('observation', tf.identity))\n      return networks\n\n    self._environment_factory = environment_factory\n    self._network_factory = wrapped_network_factory\n    self._environment_spec = environment_spec\n    self._sigma = sigma\n    self._num_actors = num_actors\n    self._num_caches = num_caches\n    self._max_actor_steps = max_actor_steps\n    self._log_every = log_every\n    self._accelerator = accelerator\n    self._variable_update_period = variable_update_period\n\n    self._builder = agent.D4PGBuilder(\n        # TODO(mwhoffman): pass the config dataclass in directly.\n        # TODO(mwhoffman): use the limiter rather than the workaround below.\n        agent.D4PGConfig(\n            accelerator=accelerator,\n            discount=discount,\n            batch_size=batch_size,\n            prefetch_size=prefetch_size,\n            target_update_period=target_update_period,\n            variable_update_period=variable_update_period,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            min_replay_size=min_replay_size,\n            max_replay_size=max_replay_size,\n            samples_per_insert=samples_per_insert,\n            n_step=n_step,\n            sigma=sigma,\n            clipping=clipping,\n        ))\n\n  def replay(self):\n    \"\"\"The replay storage.\"\"\"\n    return self._builder.make_replay_tables(self._environment_spec)\n\n  def counter(self):\n    return tf2_savers.CheckpointingRunner(counting.Counter(),\n                                          time_delta_minutes=1,\n                                          subdirectory='counter')\n\n  def coordinator(self, counter: counting.Counter):\n    return lp_utils.StepsLimiter(counter, self._max_actor_steps)\n\n  def learner(\n      self,\n      replay: reverb.Client,\n      counter: counting.Counter,\n  ):\n    \"\"\"The Learning part of the agent.\"\"\"\n\n    # If we are running on multiple accelerator devices, this replicates\n    # weights and updates across devices.\n    replicator = agent.get_replicator(self._accelerator)\n\n    with replicator.scope():\n      # Create the networks to optimize (online) and target networks.\n      online_networks = self._network_factory(self._environment_spec.actions)\n      target_networks = copy.deepcopy(online_networks)\n\n      # Initialize the networks.\n      online_networks.init(self._environment_spec)\n      target_networks.init(self._environment_spec)\n\n    dataset = self._builder.make_dataset_iterator(replay)\n\n    counter = counting.Counter(counter, 'learner')\n    logger = loggers.make_default_logger(\n        'learner', time_delta=self._log_every, steps_key='learner_steps')\n\n    return self._builder.make_learner(\n        networks=(online_networks, target_networks),\n        dataset=dataset,\n        counter=counter,\n        logger=logger,\n        checkpoint=True,\n    )\n\n  def actor(\n      self,\n      replay: reverb.Client,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ) -> acme.EnvironmentLoop:\n    \"\"\"The actor process.\"\"\"\n\n    # Create the behavior policy.\n    networks = self._network_factory(self._environment_spec.actions)\n    networks.init(self._environment_spec)\n    policy_network = networks.make_policy(\n        environment_spec=self._environment_spec,\n        sigma=self._sigma,\n    )\n\n    # Create the agent.\n    actor = self._builder.make_actor(\n        policy_network=policy_network,\n        adder=self._builder.make_adder(replay),\n        variable_source=variable_source,\n    )\n\n    # Create the environment.\n    environment = self._environment_factory(False)\n\n    # Create logger and counter; actors will not spam bigtable.\n    counter = counting.Counter(counter, 'actor')\n    logger = loggers.make_default_logger(\n        'actor',\n        save_data=False,\n        time_delta=self._log_every,\n        steps_key='actor_steps')\n\n    # Create the loop to connect environment and agent.\n    return acme.EnvironmentLoop(environment, actor, counter, logger)\n\n  def evaluator(\n      self,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n      logger: Optional[loggers.Logger] = None,\n  ):\n    \"\"\"The evaluation process.\"\"\"\n\n    # Create the behavior policy.\n    networks = self._network_factory(self._environment_spec.actions)\n    networks.init(self._environment_spec)\n    policy_network = networks.make_policy(self._environment_spec)\n\n    # Create the agent.\n    actor = self._builder.make_actor(\n        policy_network=policy_network,\n        variable_source=variable_source,\n    )\n\n    # Make the environment.\n    environment = self._environment_factory(True)\n\n    # Create logger and counter.\n    counter = counting.Counter(counter, 'evaluator')\n    logger = logger or loggers.make_default_logger(\n        'evaluator',\n        time_delta=self._log_every,\n        steps_key='evaluator_steps',\n    )\n\n    # Create the run loop and return it.\n    return acme.EnvironmentLoop(environment, actor, counter, logger)\n\n  def build(self, name='d4pg'):\n    \"\"\"Build the distributed agent topology.\"\"\"\n    program = lp.Program(name=name)\n\n    with program.group('replay'):\n      replay = program.add_node(lp.ReverbNode(self.replay))\n\n    with program.group('counter'):\n      counter = program.add_node(lp.CourierNode(self.counter))\n\n    if self._max_actor_steps:\n      with program.group('coordinator'):\n        _ = program.add_node(lp.CourierNode(self.coordinator, counter))\n\n    with program.group('learner'):\n      learner = program.add_node(lp.CourierNode(self.learner, replay, counter))\n\n    with program.group('evaluator'):\n      program.add_node(lp.CourierNode(self.evaluator, learner, counter))\n\n    if not self._num_caches:\n      # Use our learner as a single variable source.\n      sources = [learner]\n    else:\n      with program.group('cacher'):\n        # Create a set of learner caches.\n        sources = []\n        for _ in range(self._num_caches):\n          cacher = program.add_node(\n              lp.CacherNode(\n                  learner, refresh_interval_ms=2000, stale_after_ms=4000))\n          sources.append(cacher)\n\n    with program.group('actor'):\n      # Add actors which pull round-robin from our variable sources.\n      for actor_id in range(self._num_actors):\n        source = sources[actor_id % len(sources)]\n        program.add_node(lp.CourierNode(self.actor, replay, source, counter))\n\n    return program",
  "def __init__(\n      self,\n      environment_factory: Callable[[bool], dm_env.Environment],\n      network_factory: Callable[[specs.BoundedArray], Dict[str, snt.Module]],\n      accelerator: Optional[str] = None,\n      num_actors: int = 1,\n      num_caches: int = 0,\n      environment_spec: Optional[specs.EnvironmentSpec] = None,\n      batch_size: int = 256,\n      prefetch_size: int = 4,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1000000,\n      samples_per_insert: Optional[float] = 32.0,\n      n_step: int = 5,\n      sigma: float = 0.3,\n      clipping: bool = True,\n      discount: float = 0.99,\n      policy_optimizer: Optional[snt.Optimizer] = None,\n      critic_optimizer: Optional[snt.Optimizer] = None,\n      target_update_period: int = 100,\n      variable_update_period: int = 1000,\n      max_actor_steps: Optional[int] = None,\n      log_every: float = 10.0,\n  ):\n\n    if accelerator is not None and accelerator not in _ACCELERATORS:\n      raise ValueError(f'Accelerator must be one of {_ACCELERATORS}, '\n                       f'not \"{accelerator}\".')\n\n    if not environment_spec:\n      environment_spec = specs.make_environment_spec(environment_factory(False))\n\n    # TODO(mwhoffman): Make network_factory directly return the struct.\n    # TODO(mwhoffman): Make the factory take the entire spec.\n    def wrapped_network_factory(action_spec):\n      networks_dict = network_factory(action_spec)\n      networks = agent.D4PGNetworks(\n          policy_network=networks_dict.get('policy'),\n          critic_network=networks_dict.get('critic'),\n          observation_network=networks_dict.get('observation', tf.identity))\n      return networks\n\n    self._environment_factory = environment_factory\n    self._network_factory = wrapped_network_factory\n    self._environment_spec = environment_spec\n    self._sigma = sigma\n    self._num_actors = num_actors\n    self._num_caches = num_caches\n    self._max_actor_steps = max_actor_steps\n    self._log_every = log_every\n    self._accelerator = accelerator\n    self._variable_update_period = variable_update_period\n\n    self._builder = agent.D4PGBuilder(\n        # TODO(mwhoffman): pass the config dataclass in directly.\n        # TODO(mwhoffman): use the limiter rather than the workaround below.\n        agent.D4PGConfig(\n            accelerator=accelerator,\n            discount=discount,\n            batch_size=batch_size,\n            prefetch_size=prefetch_size,\n            target_update_period=target_update_period,\n            variable_update_period=variable_update_period,\n            policy_optimizer=policy_optimizer,\n            critic_optimizer=critic_optimizer,\n            min_replay_size=min_replay_size,\n            max_replay_size=max_replay_size,\n            samples_per_insert=samples_per_insert,\n            n_step=n_step,\n            sigma=sigma,\n            clipping=clipping,\n        ))",
  "def replay(self):\n    \"\"\"The replay storage.\"\"\"\n    return self._builder.make_replay_tables(self._environment_spec)",
  "def counter(self):\n    return tf2_savers.CheckpointingRunner(counting.Counter(),\n                                          time_delta_minutes=1,\n                                          subdirectory='counter')",
  "def coordinator(self, counter: counting.Counter):\n    return lp_utils.StepsLimiter(counter, self._max_actor_steps)",
  "def learner(\n      self,\n      replay: reverb.Client,\n      counter: counting.Counter,\n  ):\n    \"\"\"The Learning part of the agent.\"\"\"\n\n    # If we are running on multiple accelerator devices, this replicates\n    # weights and updates across devices.\n    replicator = agent.get_replicator(self._accelerator)\n\n    with replicator.scope():\n      # Create the networks to optimize (online) and target networks.\n      online_networks = self._network_factory(self._environment_spec.actions)\n      target_networks = copy.deepcopy(online_networks)\n\n      # Initialize the networks.\n      online_networks.init(self._environment_spec)\n      target_networks.init(self._environment_spec)\n\n    dataset = self._builder.make_dataset_iterator(replay)\n\n    counter = counting.Counter(counter, 'learner')\n    logger = loggers.make_default_logger(\n        'learner', time_delta=self._log_every, steps_key='learner_steps')\n\n    return self._builder.make_learner(\n        networks=(online_networks, target_networks),\n        dataset=dataset,\n        counter=counter,\n        logger=logger,\n        checkpoint=True,\n    )",
  "def actor(\n      self,\n      replay: reverb.Client,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ) -> acme.EnvironmentLoop:\n    \"\"\"The actor process.\"\"\"\n\n    # Create the behavior policy.\n    networks = self._network_factory(self._environment_spec.actions)\n    networks.init(self._environment_spec)\n    policy_network = networks.make_policy(\n        environment_spec=self._environment_spec,\n        sigma=self._sigma,\n    )\n\n    # Create the agent.\n    actor = self._builder.make_actor(\n        policy_network=policy_network,\n        adder=self._builder.make_adder(replay),\n        variable_source=variable_source,\n    )\n\n    # Create the environment.\n    environment = self._environment_factory(False)\n\n    # Create logger and counter; actors will not spam bigtable.\n    counter = counting.Counter(counter, 'actor')\n    logger = loggers.make_default_logger(\n        'actor',\n        save_data=False,\n        time_delta=self._log_every,\n        steps_key='actor_steps')\n\n    # Create the loop to connect environment and agent.\n    return acme.EnvironmentLoop(environment, actor, counter, logger)",
  "def evaluator(\n      self,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n      logger: Optional[loggers.Logger] = None,\n  ):\n    \"\"\"The evaluation process.\"\"\"\n\n    # Create the behavior policy.\n    networks = self._network_factory(self._environment_spec.actions)\n    networks.init(self._environment_spec)\n    policy_network = networks.make_policy(self._environment_spec)\n\n    # Create the agent.\n    actor = self._builder.make_actor(\n        policy_network=policy_network,\n        variable_source=variable_source,\n    )\n\n    # Make the environment.\n    environment = self._environment_factory(True)\n\n    # Create logger and counter.\n    counter = counting.Counter(counter, 'evaluator')\n    logger = logger or loggers.make_default_logger(\n        'evaluator',\n        time_delta=self._log_every,\n        steps_key='evaluator_steps',\n    )\n\n    # Create the run loop and return it.\n    return acme.EnvironmentLoop(environment, actor, counter, logger)",
  "def build(self, name='d4pg'):\n    \"\"\"Build the distributed agent topology.\"\"\"\n    program = lp.Program(name=name)\n\n    with program.group('replay'):\n      replay = program.add_node(lp.ReverbNode(self.replay))\n\n    with program.group('counter'):\n      counter = program.add_node(lp.CourierNode(self.counter))\n\n    if self._max_actor_steps:\n      with program.group('coordinator'):\n        _ = program.add_node(lp.CourierNode(self.coordinator, counter))\n\n    with program.group('learner'):\n      learner = program.add_node(lp.CourierNode(self.learner, replay, counter))\n\n    with program.group('evaluator'):\n      program.add_node(lp.CourierNode(self.evaluator, learner, counter))\n\n    if not self._num_caches:\n      # Use our learner as a single variable source.\n      sources = [learner]\n    else:\n      with program.group('cacher'):\n        # Create a set of learner caches.\n        sources = []\n        for _ in range(self._num_caches):\n          cacher = program.add_node(\n              lp.CacherNode(\n                  learner, refresh_interval_ms=2000, stale_after_ms=4000))\n          sources.append(cacher)\n\n    with program.group('actor'):\n      # Add actors which pull round-robin from our variable sources.\n      for actor_id in range(self._num_actors):\n        source = sources[actor_id % len(sources)]\n        program.add_node(lp.CourierNode(self.actor, replay, source, counter))\n\n    return program",
  "def wrapped_network_factory(action_spec):\n      networks_dict = network_factory(action_spec)\n      networks = agent.D4PGNetworks(\n          policy_network=networks_dict.get('policy'),\n          critic_network=networks_dict.get('critic'),\n          observation_network=networks_dict.get('observation', tf.identity))\n      return networks",
  "class DistributionalMPO(agent.Agent):\n  \"\"\"Distributional MPO Agent.\n\n  This implements a single-process distributional MPO agent. This is an\n  actor-critic algorithm that generates data via a behavior policy, inserts\n  N-step transitions into a replay buffer, and periodically updates the policy\n  (and as a result the behavior) by sampling uniformly from this buffer.\n  This agent distinguishes itself from the MPO agent by using a distributional\n  critic (state-action value approximator).\n  \"\"\"\n\n  def __init__(self,\n               environment_spec: specs.EnvironmentSpec,\n               policy_network: snt.Module,\n               critic_network: snt.Module,\n               observation_network: types.TensorTransformation = tf.identity,\n               discount: float = 0.99,\n               batch_size: int = 256,\n               prefetch_size: int = 4,\n               target_policy_update_period: int = 100,\n               target_critic_update_period: int = 100,\n               min_replay_size: int = 1000,\n               max_replay_size: int = 1000000,\n               samples_per_insert: float = 32.0,\n               policy_loss_module: Optional[snt.Module] = None,\n               policy_optimizer: Optional[snt.Optimizer] = None,\n               critic_optimizer: Optional[snt.Optimizer] = None,\n               n_step: int = 5,\n               num_samples: int = 20,\n               clipping: bool = True,\n               logger: Optional[loggers.Logger] = None,\n               counter: Optional[counting.Counter] = None,\n               checkpoint: bool = True,\n               replay_table_name: str = adders.DEFAULT_PRIORITY_TABLE):\n    \"\"\"Initialize the agent.\n\n    Args:\n      environment_spec: description of the actions, observations, etc.\n      policy_network: the online (optimized) policy.\n      critic_network: the online critic.\n      observation_network: optional network to transform the observations before\n        they are fed into any network.\n      discount: discount to use for TD updates.\n      batch_size: batch size for updates.\n      prefetch_size: size to prefetch from replay.\n      target_policy_update_period: number of updates to perform before updating\n        the target policy network.\n      target_critic_update_period: number of updates to perform before updating\n        the target critic network.\n      min_replay_size: minimum replay size before updating.\n      max_replay_size: maximum replay size.\n      samples_per_insert: number of samples to take from replay for every insert\n        that is made.\n      policy_loss_module: configured MPO loss function for the policy\n        optimization; defaults to sensible values on the control suite.\n        See `acme/tf/losses/mpo.py` for more details.\n      policy_optimizer: optimizer to be used on the policy.\n      critic_optimizer: optimizer to be used on the critic.\n      n_step: number of steps to squash into a single transition.\n      num_samples: number of actions to sample when doing a Monte Carlo\n        integration with respect to the policy.\n      clipping: whether to clip gradients by global norm.\n      logger: logging object used to write to logs.\n      counter: counter object used to keep track of steps.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n      replay_table_name: string indicating what name to give the replay table.\n    \"\"\"\n\n    # Create a replay server to add data to.\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=max_replay_size,\n        rate_limiter=reverb.rate_limiters.MinSize(min_size_to_sample=1),\n        signature=adders.NStepTransitionAdder.signature(environment_spec))\n    self._server = reverb.Server([replay_table], port=None)\n\n    # The adder is used to insert observations into replay.\n    address = f'localhost:{self._server.port}'\n    adder = adders.NStepTransitionAdder(\n        client=reverb.Client(address),\n        n_step=n_step,\n        discount=discount)\n\n    # The dataset object to learn from.\n    dataset = datasets.make_reverb_dataset(\n        table=replay_table_name,\n        server_address=address,\n        batch_size=batch_size,\n        prefetch_size=prefetch_size)\n\n    # Make sure observation network is a Sonnet Module.\n    observation_network = tf2_utils.to_sonnet_module(observation_network)\n\n    # Create target networks before creating online/target network variables.\n    target_policy_network = copy.deepcopy(policy_network)\n    target_critic_network = copy.deepcopy(critic_network)\n    target_observation_network = copy.deepcopy(observation_network)\n\n    # Get observation and action specs.\n    act_spec = environment_spec.actions\n    obs_spec = environment_spec.observations\n    emb_spec = tf2_utils.create_variables(observation_network, [obs_spec])\n\n    # Create the behavior policy.\n    behavior_network = snt.Sequential([\n        observation_network,\n        policy_network,\n        networks.StochasticSamplingHead(),\n    ])\n\n    # Create variables.\n    tf2_utils.create_variables(policy_network, [emb_spec])\n    tf2_utils.create_variables(critic_network, [emb_spec, act_spec])\n    tf2_utils.create_variables(target_policy_network, [emb_spec])\n    tf2_utils.create_variables(target_critic_network, [emb_spec, act_spec])\n    tf2_utils.create_variables(target_observation_network, [obs_spec])\n\n    # Create the actor which defines how we take actions.\n    actor = actors.FeedForwardActor(\n        policy_network=behavior_network, adder=adder)\n\n    # Create optimizers.\n    policy_optimizer = policy_optimizer or snt.optimizers.Adam(1e-4)\n    critic_optimizer = critic_optimizer or snt.optimizers.Adam(1e-4)\n\n    # The learner updates the parameters (and initializes them).\n    learner = learning.DistributionalMPOLearner(\n        policy_network=policy_network,\n        critic_network=critic_network,\n        observation_network=observation_network,\n        target_policy_network=target_policy_network,\n        target_critic_network=target_critic_network,\n        target_observation_network=target_observation_network,\n        policy_loss_module=policy_loss_module,\n        policy_optimizer=policy_optimizer,\n        critic_optimizer=critic_optimizer,\n        clipping=clipping,\n        discount=discount,\n        num_samples=num_samples,\n        target_policy_update_period=target_policy_update_period,\n        target_critic_update_period=target_critic_update_period,\n        dataset=dataset,\n        logger=logger,\n        counter=counter,\n        checkpoint=checkpoint)\n\n    super().__init__(\n        actor=actor,\n        learner=learner,\n        min_observations=max(batch_size, min_replay_size),\n        observations_per_step=float(batch_size) / samples_per_insert)",
  "def __init__(self,\n               environment_spec: specs.EnvironmentSpec,\n               policy_network: snt.Module,\n               critic_network: snt.Module,\n               observation_network: types.TensorTransformation = tf.identity,\n               discount: float = 0.99,\n               batch_size: int = 256,\n               prefetch_size: int = 4,\n               target_policy_update_period: int = 100,\n               target_critic_update_period: int = 100,\n               min_replay_size: int = 1000,\n               max_replay_size: int = 1000000,\n               samples_per_insert: float = 32.0,\n               policy_loss_module: Optional[snt.Module] = None,\n               policy_optimizer: Optional[snt.Optimizer] = None,\n               critic_optimizer: Optional[snt.Optimizer] = None,\n               n_step: int = 5,\n               num_samples: int = 20,\n               clipping: bool = True,\n               logger: Optional[loggers.Logger] = None,\n               counter: Optional[counting.Counter] = None,\n               checkpoint: bool = True,\n               replay_table_name: str = adders.DEFAULT_PRIORITY_TABLE):\n    \"\"\"Initialize the agent.\n\n    Args:\n      environment_spec: description of the actions, observations, etc.\n      policy_network: the online (optimized) policy.\n      critic_network: the online critic.\n      observation_network: optional network to transform the observations before\n        they are fed into any network.\n      discount: discount to use for TD updates.\n      batch_size: batch size for updates.\n      prefetch_size: size to prefetch from replay.\n      target_policy_update_period: number of updates to perform before updating\n        the target policy network.\n      target_critic_update_period: number of updates to perform before updating\n        the target critic network.\n      min_replay_size: minimum replay size before updating.\n      max_replay_size: maximum replay size.\n      samples_per_insert: number of samples to take from replay for every insert\n        that is made.\n      policy_loss_module: configured MPO loss function for the policy\n        optimization; defaults to sensible values on the control suite.\n        See `acme/tf/losses/mpo.py` for more details.\n      policy_optimizer: optimizer to be used on the policy.\n      critic_optimizer: optimizer to be used on the critic.\n      n_step: number of steps to squash into a single transition.\n      num_samples: number of actions to sample when doing a Monte Carlo\n        integration with respect to the policy.\n      clipping: whether to clip gradients by global norm.\n      logger: logging object used to write to logs.\n      counter: counter object used to keep track of steps.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n      replay_table_name: string indicating what name to give the replay table.\n    \"\"\"\n\n    # Create a replay server to add data to.\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=max_replay_size,\n        rate_limiter=reverb.rate_limiters.MinSize(min_size_to_sample=1),\n        signature=adders.NStepTransitionAdder.signature(environment_spec))\n    self._server = reverb.Server([replay_table], port=None)\n\n    # The adder is used to insert observations into replay.\n    address = f'localhost:{self._server.port}'\n    adder = adders.NStepTransitionAdder(\n        client=reverb.Client(address),\n        n_step=n_step,\n        discount=discount)\n\n    # The dataset object to learn from.\n    dataset = datasets.make_reverb_dataset(\n        table=replay_table_name,\n        server_address=address,\n        batch_size=batch_size,\n        prefetch_size=prefetch_size)\n\n    # Make sure observation network is a Sonnet Module.\n    observation_network = tf2_utils.to_sonnet_module(observation_network)\n\n    # Create target networks before creating online/target network variables.\n    target_policy_network = copy.deepcopy(policy_network)\n    target_critic_network = copy.deepcopy(critic_network)\n    target_observation_network = copy.deepcopy(observation_network)\n\n    # Get observation and action specs.\n    act_spec = environment_spec.actions\n    obs_spec = environment_spec.observations\n    emb_spec = tf2_utils.create_variables(observation_network, [obs_spec])\n\n    # Create the behavior policy.\n    behavior_network = snt.Sequential([\n        observation_network,\n        policy_network,\n        networks.StochasticSamplingHead(),\n    ])\n\n    # Create variables.\n    tf2_utils.create_variables(policy_network, [emb_spec])\n    tf2_utils.create_variables(critic_network, [emb_spec, act_spec])\n    tf2_utils.create_variables(target_policy_network, [emb_spec])\n    tf2_utils.create_variables(target_critic_network, [emb_spec, act_spec])\n    tf2_utils.create_variables(target_observation_network, [obs_spec])\n\n    # Create the actor which defines how we take actions.\n    actor = actors.FeedForwardActor(\n        policy_network=behavior_network, adder=adder)\n\n    # Create optimizers.\n    policy_optimizer = policy_optimizer or snt.optimizers.Adam(1e-4)\n    critic_optimizer = critic_optimizer or snt.optimizers.Adam(1e-4)\n\n    # The learner updates the parameters (and initializes them).\n    learner = learning.DistributionalMPOLearner(\n        policy_network=policy_network,\n        critic_network=critic_network,\n        observation_network=observation_network,\n        target_policy_network=target_policy_network,\n        target_critic_network=target_critic_network,\n        target_observation_network=target_observation_network,\n        policy_loss_module=policy_loss_module,\n        policy_optimizer=policy_optimizer,\n        critic_optimizer=critic_optimizer,\n        clipping=clipping,\n        discount=discount,\n        num_samples=num_samples,\n        target_policy_update_period=target_policy_update_period,\n        target_critic_update_period=target_critic_update_period,\n        dataset=dataset,\n        logger=logger,\n        counter=counter,\n        checkpoint=checkpoint)\n\n    super().__init__(\n        actor=actor,\n        learner=learner,\n        min_observations=max(batch_size, min_replay_size),\n        observations_per_step=float(batch_size) / samples_per_insert)",
  "class DistributionalMPOLearner(acme.Learner):\n  \"\"\"Distributional MPO learner.\"\"\"\n\n  def __init__(\n      self,\n      policy_network: snt.Module,\n      critic_network: snt.Module,\n      target_policy_network: snt.Module,\n      target_critic_network: snt.Module,\n      discount: float,\n      num_samples: int,\n      target_policy_update_period: int,\n      target_critic_update_period: int,\n      dataset: tf.data.Dataset,\n      observation_network: types.TensorTransformation = tf.identity,\n      target_observation_network: types.TensorTransformation = tf.identity,\n      policy_loss_module: Optional[snt.Module] = None,\n      policy_optimizer: Optional[snt.Optimizer] = None,\n      critic_optimizer: Optional[snt.Optimizer] = None,\n      dual_optimizer: Optional[snt.Optimizer] = None,\n      clipping: bool = True,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = True,\n  ):\n\n    # Store online and target networks.\n    self._policy_network = policy_network\n    self._critic_network = critic_network\n    self._target_policy_network = target_policy_network\n    self._target_critic_network = target_critic_network\n\n    # Make sure observation networks are snt.Module's so they have variables.\n    self._observation_network = tf2_utils.to_sonnet_module(observation_network)\n    self._target_observation_network = tf2_utils.to_sonnet_module(\n        target_observation_network)\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger('learner')\n\n    # Other learner parameters.\n    self._discount = discount\n    self._num_samples = num_samples\n    self._clipping = clipping\n\n    # Necessary to track when to update target networks.\n    self._num_steps = tf.Variable(0, dtype=tf.int32)\n    self._target_policy_update_period = target_policy_update_period\n    self._target_critic_update_period = target_critic_update_period\n\n    # Batch dataset and create iterator.\n    # TODO(b/155086959): Fix type stubs and remove.\n    self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n    self._policy_loss_module = policy_loss_module or losses.MPO(\n        epsilon=1e-1,\n        epsilon_penalty=1e-3,\n        epsilon_mean=2.5e-3,\n        epsilon_stddev=1e-6,\n        init_log_temperature=10.,\n        init_log_alpha_mean=10.,\n        init_log_alpha_stddev=1000.)\n\n    # Create the optimizers.\n    self._critic_optimizer = critic_optimizer or snt.optimizers.Adam(1e-4)\n    self._policy_optimizer = policy_optimizer or snt.optimizers.Adam(1e-4)\n    self._dual_optimizer = dual_optimizer or snt.optimizers.Adam(1e-2)\n\n    # Expose the variables.\n    policy_network_to_expose = snt.Sequential(\n        [self._target_observation_network, self._target_policy_network])\n    self._variables = {\n        'critic': self._target_critic_network.variables,\n        'policy': policy_network_to_expose.variables,\n    }\n\n    # Create a checkpointer and snapshotter object.\n    self._checkpointer = None\n    self._snapshotter = None\n\n    if checkpoint:\n      self._checkpointer = tf2_savers.Checkpointer(\n          subdirectory='dmpo_learner',\n          objects_to_save={\n              'counter': self._counter,\n              'policy': self._policy_network,\n              'critic': self._critic_network,\n              'observation': self._observation_network,\n              'target_policy': self._target_policy_network,\n              'target_critic': self._target_critic_network,\n              'target_observation': self._target_observation_network,\n              'policy_optimizer': self._policy_optimizer,\n              'critic_optimizer': self._critic_optimizer,\n              'dual_optimizer': self._dual_optimizer,\n              'policy_loss_module': self._policy_loss_module,\n              'num_steps': self._num_steps,\n          })\n\n      self._snapshotter = tf2_savers.Snapshotter(\n          objects_to_save={\n              'policy':\n                  snt.Sequential([\n                      self._target_observation_network,\n                      self._target_policy_network\n                  ]),\n          })\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None\n\n  @tf.function\n  def _step(self) -> types.NestedTensor:\n    # Update target network.\n    online_policy_variables = self._policy_network.variables\n    target_policy_variables = self._target_policy_network.variables\n    online_critic_variables = (\n        *self._observation_network.variables,\n        *self._critic_network.variables,\n    )\n    target_critic_variables = (\n        *self._target_observation_network.variables,\n        *self._target_critic_network.variables,\n    )\n\n    # Make online policy -> target policy network update ops.\n    if tf.math.mod(self._num_steps, self._target_policy_update_period) == 0:\n      for src, dest in zip(online_policy_variables, target_policy_variables):\n        dest.assign(src)\n    # Make online critic -> target critic network update ops.\n    if tf.math.mod(self._num_steps, self._target_critic_update_period) == 0:\n      for src, dest in zip(online_critic_variables, target_critic_variables):\n        dest.assign(src)\n\n    self._num_steps.assign_add(1)\n\n    # Get data from replay (dropping extras if any). Note there is no\n    # extra data here because we do not insert any into Reverb.\n    inputs = next(self._iterator)\n    transitions: types.Transition = inputs.data\n\n    # Get batch size and scalar dtype.\n    batch_size = transitions.reward.shape[0]\n\n    # Cast the additional discount to match the environment discount dtype.\n    discount = tf.cast(self._discount, dtype=transitions.discount.dtype)\n\n    with tf.GradientTape(persistent=True) as tape:\n      # Maybe transform the observation before feeding into policy and critic.\n      # Transforming the observations this way at the start of the learning\n      # step effectively means that the policy and critic share observation\n      # network weights.\n      o_tm1 = self._observation_network(transitions.observation)\n      # This stop_gradient prevents gradients to propagate into the target\n      # observation network. In addition, since the online policy network is\n      # evaluated at o_t, this also means the policy loss does not influence\n      # the observation network training.\n      o_t = tf.stop_gradient(\n          self._target_observation_network(transitions.next_observation))\n\n      # Get online and target action distributions from policy networks.\n      online_action_distribution = self._policy_network(o_t)\n      target_action_distribution = self._target_policy_network(o_t)\n\n      # Sample actions to evaluate policy; of size [N, B, ...].\n      sampled_actions = target_action_distribution.sample(self._num_samples)\n\n      # Tile embedded observations to feed into the target critic network.\n      # Note: this is more efficient than tiling before the embedding layer.\n      tiled_o_t = tf2_utils.tile_tensor(o_t, self._num_samples)  # [N, B, ...]\n\n      # Compute target-estimated distributional value of sampled actions at o_t.\n      sampled_q_t_distributions = self._target_critic_network(\n          # Merge batch dimensions; to shape [N*B, ...].\n          snt.merge_leading_dims(tiled_o_t, num_dims=2),\n          snt.merge_leading_dims(sampled_actions, num_dims=2))\n\n      # Compute average logits by first reshaping them and normalizing them\n      # across atoms.\n      new_shape = [self._num_samples, batch_size, -1]  # [N, B, A]\n      sampled_logits = tf.reshape(sampled_q_t_distributions.logits, new_shape)\n      sampled_logprobs = tf.math.log_softmax(sampled_logits, axis=-1)\n      averaged_logits = tf.reduce_logsumexp(sampled_logprobs, axis=0)\n\n      # Construct the expected distributional value for bootstrapping.\n      q_t_distribution = networks.DiscreteValuedDistribution(\n          values=sampled_q_t_distributions.values, logits=averaged_logits)\n\n      # Compute online critic value distribution of a_tm1 in state o_tm1.\n      q_tm1_distribution = self._critic_network(o_tm1, transitions.action)\n\n      # Compute critic distributional loss.\n      critic_loss = losses.categorical(q_tm1_distribution, transitions.reward,\n                                       discount * transitions.discount,\n                                       q_t_distribution)\n      critic_loss = tf.reduce_mean(critic_loss)\n\n      # Compute Q-values of sampled actions and reshape to [N, B].\n      sampled_q_values = sampled_q_t_distributions.mean()\n      sampled_q_values = tf.reshape(sampled_q_values, (self._num_samples, -1))\n\n      # Compute MPO policy loss.\n      policy_loss, policy_stats = self._policy_loss_module(\n          online_action_distribution=online_action_distribution,\n          target_action_distribution=target_action_distribution,\n          actions=sampled_actions,\n          q_values=sampled_q_values)\n\n    # For clarity, explicitly define which variables are trained by which loss.\n    critic_trainable_variables = (\n        # In this agent, the critic loss trains the observation network.\n        self._observation_network.trainable_variables +\n        self._critic_network.trainable_variables)\n    policy_trainable_variables = self._policy_network.trainable_variables\n    # The following are the MPO dual variables, stored in the loss module.\n    dual_trainable_variables = self._policy_loss_module.trainable_variables\n\n    # Compute gradients.\n    critic_gradients = tape.gradient(critic_loss, critic_trainable_variables)\n    policy_gradients, dual_gradients = tape.gradient(\n        policy_loss, (policy_trainable_variables, dual_trainable_variables))\n\n    # Delete the tape manually because of the persistent=True flag.\n    del tape\n\n    # Maybe clip gradients.\n    if self._clipping:\n      policy_gradients = tuple(tf.clip_by_global_norm(policy_gradients, 40.)[0])\n      critic_gradients = tuple(tf.clip_by_global_norm(critic_gradients, 40.)[0])\n\n    # Apply gradients.\n    self._critic_optimizer.apply(critic_gradients, critic_trainable_variables)\n    self._policy_optimizer.apply(policy_gradients, policy_trainable_variables)\n    self._dual_optimizer.apply(dual_gradients, dual_trainable_variables)\n\n    # Losses to track.\n    fetches = {\n        'critic_loss': critic_loss,\n        'policy_loss': policy_loss,\n    }\n    fetches.update(policy_stats)  # Log MPO stats.\n\n    return fetches\n\n  def step(self):\n    # Run the learning step.\n    fetches = self._step()\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n    fetches.update(counts)\n\n    # Checkpoint and attempt to write the logs.\n    if self._checkpointer is not None:\n      self._checkpointer.save()\n    if self._snapshotter is not None:\n      self._snapshotter.save()\n    self._logger.write(fetches)\n\n  def get_variables(self, names: List[str]) -> List[List[np.ndarray]]:\n    return [tf2_utils.to_numpy(self._variables[name]) for name in names]",
  "def __init__(\n      self,\n      policy_network: snt.Module,\n      critic_network: snt.Module,\n      target_policy_network: snt.Module,\n      target_critic_network: snt.Module,\n      discount: float,\n      num_samples: int,\n      target_policy_update_period: int,\n      target_critic_update_period: int,\n      dataset: tf.data.Dataset,\n      observation_network: types.TensorTransformation = tf.identity,\n      target_observation_network: types.TensorTransformation = tf.identity,\n      policy_loss_module: Optional[snt.Module] = None,\n      policy_optimizer: Optional[snt.Optimizer] = None,\n      critic_optimizer: Optional[snt.Optimizer] = None,\n      dual_optimizer: Optional[snt.Optimizer] = None,\n      clipping: bool = True,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = True,\n  ):\n\n    # Store online and target networks.\n    self._policy_network = policy_network\n    self._critic_network = critic_network\n    self._target_policy_network = target_policy_network\n    self._target_critic_network = target_critic_network\n\n    # Make sure observation networks are snt.Module's so they have variables.\n    self._observation_network = tf2_utils.to_sonnet_module(observation_network)\n    self._target_observation_network = tf2_utils.to_sonnet_module(\n        target_observation_network)\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger('learner')\n\n    # Other learner parameters.\n    self._discount = discount\n    self._num_samples = num_samples\n    self._clipping = clipping\n\n    # Necessary to track when to update target networks.\n    self._num_steps = tf.Variable(0, dtype=tf.int32)\n    self._target_policy_update_period = target_policy_update_period\n    self._target_critic_update_period = target_critic_update_period\n\n    # Batch dataset and create iterator.\n    # TODO(b/155086959): Fix type stubs and remove.\n    self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n    self._policy_loss_module = policy_loss_module or losses.MPO(\n        epsilon=1e-1,\n        epsilon_penalty=1e-3,\n        epsilon_mean=2.5e-3,\n        epsilon_stddev=1e-6,\n        init_log_temperature=10.,\n        init_log_alpha_mean=10.,\n        init_log_alpha_stddev=1000.)\n\n    # Create the optimizers.\n    self._critic_optimizer = critic_optimizer or snt.optimizers.Adam(1e-4)\n    self._policy_optimizer = policy_optimizer or snt.optimizers.Adam(1e-4)\n    self._dual_optimizer = dual_optimizer or snt.optimizers.Adam(1e-2)\n\n    # Expose the variables.\n    policy_network_to_expose = snt.Sequential(\n        [self._target_observation_network, self._target_policy_network])\n    self._variables = {\n        'critic': self._target_critic_network.variables,\n        'policy': policy_network_to_expose.variables,\n    }\n\n    # Create a checkpointer and snapshotter object.\n    self._checkpointer = None\n    self._snapshotter = None\n\n    if checkpoint:\n      self._checkpointer = tf2_savers.Checkpointer(\n          subdirectory='dmpo_learner',\n          objects_to_save={\n              'counter': self._counter,\n              'policy': self._policy_network,\n              'critic': self._critic_network,\n              'observation': self._observation_network,\n              'target_policy': self._target_policy_network,\n              'target_critic': self._target_critic_network,\n              'target_observation': self._target_observation_network,\n              'policy_optimizer': self._policy_optimizer,\n              'critic_optimizer': self._critic_optimizer,\n              'dual_optimizer': self._dual_optimizer,\n              'policy_loss_module': self._policy_loss_module,\n              'num_steps': self._num_steps,\n          })\n\n      self._snapshotter = tf2_savers.Snapshotter(\n          objects_to_save={\n              'policy':\n                  snt.Sequential([\n                      self._target_observation_network,\n                      self._target_policy_network\n                  ]),\n          })\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None",
  "def _step(self) -> types.NestedTensor:\n    # Update target network.\n    online_policy_variables = self._policy_network.variables\n    target_policy_variables = self._target_policy_network.variables\n    online_critic_variables = (\n        *self._observation_network.variables,\n        *self._critic_network.variables,\n    )\n    target_critic_variables = (\n        *self._target_observation_network.variables,\n        *self._target_critic_network.variables,\n    )\n\n    # Make online policy -> target policy network update ops.\n    if tf.math.mod(self._num_steps, self._target_policy_update_period) == 0:\n      for src, dest in zip(online_policy_variables, target_policy_variables):\n        dest.assign(src)\n    # Make online critic -> target critic network update ops.\n    if tf.math.mod(self._num_steps, self._target_critic_update_period) == 0:\n      for src, dest in zip(online_critic_variables, target_critic_variables):\n        dest.assign(src)\n\n    self._num_steps.assign_add(1)\n\n    # Get data from replay (dropping extras if any). Note there is no\n    # extra data here because we do not insert any into Reverb.\n    inputs = next(self._iterator)\n    transitions: types.Transition = inputs.data\n\n    # Get batch size and scalar dtype.\n    batch_size = transitions.reward.shape[0]\n\n    # Cast the additional discount to match the environment discount dtype.\n    discount = tf.cast(self._discount, dtype=transitions.discount.dtype)\n\n    with tf.GradientTape(persistent=True) as tape:\n      # Maybe transform the observation before feeding into policy and critic.\n      # Transforming the observations this way at the start of the learning\n      # step effectively means that the policy and critic share observation\n      # network weights.\n      o_tm1 = self._observation_network(transitions.observation)\n      # This stop_gradient prevents gradients to propagate into the target\n      # observation network. In addition, since the online policy network is\n      # evaluated at o_t, this also means the policy loss does not influence\n      # the observation network training.\n      o_t = tf.stop_gradient(\n          self._target_observation_network(transitions.next_observation))\n\n      # Get online and target action distributions from policy networks.\n      online_action_distribution = self._policy_network(o_t)\n      target_action_distribution = self._target_policy_network(o_t)\n\n      # Sample actions to evaluate policy; of size [N, B, ...].\n      sampled_actions = target_action_distribution.sample(self._num_samples)\n\n      # Tile embedded observations to feed into the target critic network.\n      # Note: this is more efficient than tiling before the embedding layer.\n      tiled_o_t = tf2_utils.tile_tensor(o_t, self._num_samples)  # [N, B, ...]\n\n      # Compute target-estimated distributional value of sampled actions at o_t.\n      sampled_q_t_distributions = self._target_critic_network(\n          # Merge batch dimensions; to shape [N*B, ...].\n          snt.merge_leading_dims(tiled_o_t, num_dims=2),\n          snt.merge_leading_dims(sampled_actions, num_dims=2))\n\n      # Compute average logits by first reshaping them and normalizing them\n      # across atoms.\n      new_shape = [self._num_samples, batch_size, -1]  # [N, B, A]\n      sampled_logits = tf.reshape(sampled_q_t_distributions.logits, new_shape)\n      sampled_logprobs = tf.math.log_softmax(sampled_logits, axis=-1)\n      averaged_logits = tf.reduce_logsumexp(sampled_logprobs, axis=0)\n\n      # Construct the expected distributional value for bootstrapping.\n      q_t_distribution = networks.DiscreteValuedDistribution(\n          values=sampled_q_t_distributions.values, logits=averaged_logits)\n\n      # Compute online critic value distribution of a_tm1 in state o_tm1.\n      q_tm1_distribution = self._critic_network(o_tm1, transitions.action)\n\n      # Compute critic distributional loss.\n      critic_loss = losses.categorical(q_tm1_distribution, transitions.reward,\n                                       discount * transitions.discount,\n                                       q_t_distribution)\n      critic_loss = tf.reduce_mean(critic_loss)\n\n      # Compute Q-values of sampled actions and reshape to [N, B].\n      sampled_q_values = sampled_q_t_distributions.mean()\n      sampled_q_values = tf.reshape(sampled_q_values, (self._num_samples, -1))\n\n      # Compute MPO policy loss.\n      policy_loss, policy_stats = self._policy_loss_module(\n          online_action_distribution=online_action_distribution,\n          target_action_distribution=target_action_distribution,\n          actions=sampled_actions,\n          q_values=sampled_q_values)\n\n    # For clarity, explicitly define which variables are trained by which loss.\n    critic_trainable_variables = (\n        # In this agent, the critic loss trains the observation network.\n        self._observation_network.trainable_variables +\n        self._critic_network.trainable_variables)\n    policy_trainable_variables = self._policy_network.trainable_variables\n    # The following are the MPO dual variables, stored in the loss module.\n    dual_trainable_variables = self._policy_loss_module.trainable_variables\n\n    # Compute gradients.\n    critic_gradients = tape.gradient(critic_loss, critic_trainable_variables)\n    policy_gradients, dual_gradients = tape.gradient(\n        policy_loss, (policy_trainable_variables, dual_trainable_variables))\n\n    # Delete the tape manually because of the persistent=True flag.\n    del tape\n\n    # Maybe clip gradients.\n    if self._clipping:\n      policy_gradients = tuple(tf.clip_by_global_norm(policy_gradients, 40.)[0])\n      critic_gradients = tuple(tf.clip_by_global_norm(critic_gradients, 40.)[0])\n\n    # Apply gradients.\n    self._critic_optimizer.apply(critic_gradients, critic_trainable_variables)\n    self._policy_optimizer.apply(policy_gradients, policy_trainable_variables)\n    self._dual_optimizer.apply(dual_gradients, dual_trainable_variables)\n\n    # Losses to track.\n    fetches = {\n        'critic_loss': critic_loss,\n        'policy_loss': policy_loss,\n    }\n    fetches.update(policy_stats)  # Log MPO stats.\n\n    return fetches",
  "def step(self):\n    # Run the learning step.\n    fetches = self._step()\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n    fetches.update(counts)\n\n    # Checkpoint and attempt to write the logs.\n    if self._checkpointer is not None:\n      self._checkpointer.save()\n    if self._snapshotter is not None:\n      self._snapshotter.save()\n    self._logger.write(fetches)",
  "def get_variables(self, names: List[str]) -> List[List[np.ndarray]]:\n    return [tf2_utils.to_numpy(self._variables[name]) for name in names]",
  "def make_networks(\n    action_spec: specs.BoundedArray,\n    policy_layer_sizes: Sequence[int] = (50,),\n    critic_layer_sizes: Sequence[int] = (50,),\n    vmin: float = -150.,\n    vmax: float = 150.,\n    num_atoms: int = 51,\n):\n  \"\"\"Creates networks used by the agent.\"\"\"\n\n  num_dimensions = np.prod(action_spec.shape, dtype=int)\n\n  policy_network = snt.Sequential([\n      networks.LayerNormMLP(policy_layer_sizes, activate_final=True),\n      networks.MultivariateNormalDiagHead(\n          num_dimensions,\n          tanh_mean=True,\n          init_scale=0.3,\n          fixed_scale=True,\n          use_tfd_independent=False)\n  ])\n\n  # The multiplexer concatenates the (maybe transformed) observations/actions.\n  critic_network = networks.CriticMultiplexer(\n      critic_network=networks.LayerNormMLP(\n          critic_layer_sizes, activate_final=True),\n      action_network=networks.ClipToSpec(action_spec))\n  critic_network = snt.Sequential(\n      [critic_network,\n       networks.DiscreteValuedHead(vmin, vmax, num_atoms)])\n\n  return {\n      'policy': policy_network,\n      'critic': critic_network,\n      'observation': tf2_utils.batch_concat,\n  }",
  "class DistributedAgentTest(absltest.TestCase):\n  \"\"\"Simple integration/smoke test for the distributed agent.\"\"\"\n\n  def test_agent(self):\n\n    agent = dmpo.DistributedDistributionalMPO(\n        environment_factory=lambda x: fakes.ContinuousEnvironment(bounded=True),\n        network_factory=make_networks,\n        num_actors=2,\n        batch_size=32,\n        min_replay_size=32,\n        max_replay_size=1000,\n    )\n    program = agent.build()\n\n    (learner_node,) = program.groups['learner']\n    learner_node.disable_run()\n\n    lp.launch(program, launch_type='test_mt')\n\n    learner: acme.Learner = learner_node.create_handle().dereference()\n\n    for _ in range(5):\n      learner.step()",
  "def test_agent(self):\n\n    agent = dmpo.DistributedDistributionalMPO(\n        environment_factory=lambda x: fakes.ContinuousEnvironment(bounded=True),\n        network_factory=make_networks,\n        num_actors=2,\n        batch_size=32,\n        min_replay_size=32,\n        max_replay_size=1000,\n    )\n    program = agent.build()\n\n    (learner_node,) = program.groups['learner']\n    learner_node.disable_run()\n\n    lp.launch(program, launch_type='test_mt')\n\n    learner: acme.Learner = learner_node.create_handle().dereference()\n\n    for _ in range(5):\n      learner.step()",
  "def make_networks(\n    action_spec: specs.Array,\n    policy_layer_sizes: Sequence[int] = (300, 200),\n    critic_layer_sizes: Sequence[int] = (400, 300),\n) -> Dict[str, snt.Module]:\n  \"\"\"Creates networks used by the agent.\"\"\"\n\n  num_dimensions = np.prod(action_spec.shape, dtype=int)\n  critic_layer_sizes = list(critic_layer_sizes)\n\n  policy_network = snt.Sequential([\n      networks.LayerNormMLP(policy_layer_sizes),\n      networks.MultivariateNormalDiagHead(num_dimensions),\n  ])\n  # The multiplexer concatenates the (maybe transformed) observations/actions.\n  critic_network = snt.Sequential([\n      networks.CriticMultiplexer(\n          critic_network=networks.LayerNormMLP(critic_layer_sizes)),\n      networks.DiscreteValuedHead(0., 1., 10),\n  ])\n\n  return {\n      'policy': policy_network,\n      'critic': critic_network,\n  }",
  "class DMPOTest(absltest.TestCase):\n\n  def test_dmpo(self):\n    # Create a fake environment to test with.\n    environment = fakes.ContinuousEnvironment(episode_length=10)\n    spec = specs.make_environment_spec(environment)\n\n    # Create networks.\n    agent_networks = make_networks(spec.actions)\n\n    # Construct the agent.\n    agent = dmpo.DistributionalMPO(\n        spec,\n        policy_network=agent_networks['policy'],\n        critic_network=agent_networks['critic'],\n        batch_size=10,\n        samples_per_insert=2,\n        min_replay_size=10)\n\n    # Try running the environment loop. We have no assertions here because all\n    # we care about is that the agent runs without raising any errors.\n    loop = acme.EnvironmentLoop(environment, agent)\n    loop.run(num_episodes=2)",
  "def test_dmpo(self):\n    # Create a fake environment to test with.\n    environment = fakes.ContinuousEnvironment(episode_length=10)\n    spec = specs.make_environment_spec(environment)\n\n    # Create networks.\n    agent_networks = make_networks(spec.actions)\n\n    # Construct the agent.\n    agent = dmpo.DistributionalMPO(\n        spec,\n        policy_network=agent_networks['policy'],\n        critic_network=agent_networks['critic'],\n        batch_size=10,\n        samples_per_insert=2,\n        min_replay_size=10)\n\n    # Try running the environment loop. We have no assertions here because all\n    # we care about is that the agent runs without raising any errors.\n    loop = acme.EnvironmentLoop(environment, agent)\n    loop.run(num_episodes=2)",
  "class DistributedDistributionalMPO:\n  \"\"\"Program definition for distributional MPO.\"\"\"\n\n  def __init__(\n      self,\n      environment_factory: Callable[[bool], dm_env.Environment],\n      network_factory: Callable[[specs.BoundedArray], Dict[str, snt.Module]],\n      num_actors: int = 1,\n      num_caches: int = 0,\n      environment_spec: Optional[specs.EnvironmentSpec] = None,\n      batch_size: int = 256,\n      prefetch_size: int = 4,\n      observation_augmentation: Optional[types.TensorTransformation] = None,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1000000,\n      samples_per_insert: Optional[float] = 32.0,\n      n_step: int = 5,\n      num_samples: int = 20,\n      additional_discount: float = 0.99,\n      target_policy_update_period: int = 100,\n      target_critic_update_period: int = 100,\n      variable_update_period: int = 1000,\n      policy_loss_factory: Optional[Callable[[], snt.Module]] = None,\n      max_actor_steps: Optional[int] = None,\n      log_every: float = 10.0,\n      make_observers: Optional[Callable[\n          [], Sequence[observers_lib.EnvLoopObserver]]] = None):\n\n    if environment_spec is None:\n      environment_spec = specs.make_environment_spec(environment_factory(False))\n\n    self._environment_factory = environment_factory\n    self._network_factory = network_factory\n    self._policy_loss_factory = policy_loss_factory\n    self._environment_spec = environment_spec\n    self._num_actors = num_actors\n    self._num_caches = num_caches\n    self._batch_size = batch_size\n    self._prefetch_size = prefetch_size\n    self._observation_augmentation = observation_augmentation\n    self._min_replay_size = min_replay_size\n    self._max_replay_size = max_replay_size\n    self._samples_per_insert = samples_per_insert\n    self._n_step = n_step\n    self._additional_discount = additional_discount\n    self._num_samples = num_samples\n    self._target_policy_update_period = target_policy_update_period\n    self._target_critic_update_period = target_critic_update_period\n    self._variable_update_period = variable_update_period\n    self._max_actor_steps = max_actor_steps\n    self._log_every = log_every\n    self._make_observers = make_observers\n\n  def replay(self):\n    \"\"\"The replay storage.\"\"\"\n    if self._samples_per_insert is not None:\n      # Create enough of an error buffer to give a 10% tolerance in rate.\n      samples_per_insert_tolerance = 0.1 * self._samples_per_insert\n      error_buffer = self._min_replay_size * samples_per_insert_tolerance\n\n      limiter = reverb.rate_limiters.SampleToInsertRatio(\n          min_size_to_sample=self._min_replay_size,\n          samples_per_insert=self._samples_per_insert,\n          error_buffer=error_buffer)\n    else:\n      limiter = reverb.rate_limiters.MinSize(self._min_replay_size)\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=self._max_replay_size,\n        rate_limiter=limiter,\n        signature=adders.NStepTransitionAdder.signature(\n            self._environment_spec))\n    return [replay_table]\n\n  def counter(self):\n    return tf2_savers.CheckpointingRunner(counting.Counter(),\n                                          time_delta_minutes=1,\n                                          subdirectory='counter')\n\n  def coordinator(self, counter: counting.Counter, max_actor_steps: int):\n    return lp_utils.StepsLimiter(counter, max_actor_steps)\n\n  def learner(\n      self,\n      replay: reverb.Client,\n      counter: counting.Counter,\n  ):\n    \"\"\"The Learning part of the agent.\"\"\"\n\n    act_spec = self._environment_spec.actions\n    obs_spec = self._environment_spec.observations\n\n    # Create online and target networks.\n    online_networks = self._network_factory(act_spec)\n    target_networks = self._network_factory(act_spec)\n\n    # Make sure observation network is a Sonnet Module.\n    observation_network = online_networks.get('observation', tf.identity)\n    target_observation_network = target_networks.get('observation', tf.identity)\n    observation_network = tf2_utils.to_sonnet_module(observation_network)\n    target_observation_network = tf2_utils.to_sonnet_module(\n        target_observation_network)\n\n    # Get embedding spec and create observation network variables.\n    emb_spec = tf2_utils.create_variables(observation_network, [obs_spec])\n\n    # Create variables.\n    tf2_utils.create_variables(online_networks['policy'], [emb_spec])\n    tf2_utils.create_variables(online_networks['critic'], [emb_spec, act_spec])\n    tf2_utils.create_variables(target_networks['policy'], [emb_spec])\n    tf2_utils.create_variables(target_networks['critic'], [emb_spec, act_spec])\n    tf2_utils.create_variables(target_observation_network, [obs_spec])\n\n    # The dataset object to learn from.\n    dataset = datasets.make_reverb_dataset(server_address=replay.server_address)\n    dataset = dataset.batch(self._batch_size, drop_remainder=True)\n    if self._observation_augmentation:\n      transform = image_augmentation.make_transform(\n          observation_transform=self._observation_augmentation)\n      dataset = dataset.map(\n          transform, num_parallel_calls=16, deterministic=False)\n    dataset = dataset.prefetch(self._prefetch_size)\n\n    counter = counting.Counter(counter, 'learner')\n    logger = loggers.make_default_logger(\n        'learner', time_delta=self._log_every, steps_key='learner_steps')\n\n    # Create policy loss module if a factory is passed.\n    if self._policy_loss_factory:\n      policy_loss_module = self._policy_loss_factory()\n    else:\n      policy_loss_module = None\n\n    # Return the learning agent.\n    return learning.DistributionalMPOLearner(\n        policy_network=online_networks['policy'],\n        critic_network=online_networks['critic'],\n        observation_network=observation_network,\n        target_policy_network=target_networks['policy'],\n        target_critic_network=target_networks['critic'],\n        target_observation_network=target_observation_network,\n        discount=self._additional_discount,\n        num_samples=self._num_samples,\n        target_policy_update_period=self._target_policy_update_period,\n        target_critic_update_period=self._target_critic_update_period,\n        policy_loss_module=policy_loss_module,\n        dataset=dataset,\n        counter=counter,\n        logger=logger)\n\n  def actor(\n      self,\n      replay: reverb.Client,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n      actor_id: int,\n  ) -> acme.EnvironmentLoop:\n    \"\"\"The actor process.\"\"\"\n\n    action_spec = self._environment_spec.actions\n    observation_spec = self._environment_spec.observations\n\n    # Create environment and target networks to act with.\n    environment = self._environment_factory(False)\n    agent_networks = self._network_factory(action_spec)\n\n    # Make sure observation network is defined.\n    observation_network = agent_networks.get('observation', tf.identity)\n\n    # Create a stochastic behavior policy.\n    behavior_network = snt.Sequential([\n        observation_network,\n        agent_networks['policy'],\n        networks.StochasticSamplingHead(),\n    ])\n\n    # Ensure network variables are created.\n    tf2_utils.create_variables(behavior_network, [observation_spec])\n    policy_variables = {'policy': behavior_network.variables}\n\n    # Create the variable client responsible for keeping the actor up-to-date.\n    variable_client = tf2_variable_utils.VariableClient(\n        variable_source,\n        policy_variables,\n        update_period=self._variable_update_period)\n\n    # Make sure not to use a random policy after checkpoint restoration by\n    # assigning variables before running the environment loop.\n    variable_client.update_and_wait()\n\n    # Component to add things into replay.\n    adder = adders.NStepTransitionAdder(\n        client=replay,\n        n_step=self._n_step,\n        discount=self._additional_discount)\n\n    # Create the agent.\n    actor = actors.FeedForwardActor(\n        policy_network=behavior_network,\n        adder=adder,\n        variable_client=variable_client)\n\n    # Create logger and counter; only the first actor stores logs to bigtable.\n    save_data = actor_id == 0\n    counter = counting.Counter(counter, 'actor')\n    logger = loggers.make_default_logger(\n        'actor',\n        save_data=save_data,\n        time_delta=self._log_every,\n        steps_key='actor_steps')\n    observers = self._make_observers() if self._make_observers else ()\n\n    # Create the run loop and return it.\n    return acme.EnvironmentLoop(\n        environment, actor, counter, logger, observers=observers)\n\n  def evaluator(\n      self,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ):\n    \"\"\"The evaluation process.\"\"\"\n\n    action_spec = self._environment_spec.actions\n    observation_spec = self._environment_spec.observations\n\n    # Create environment and target networks to act with.\n    environment = self._environment_factory(True)\n    agent_networks = self._network_factory(action_spec)\n\n    # Make sure observation network is defined.\n    observation_network = agent_networks.get('observation', tf.identity)\n\n    # Create a stochastic behavior policy.\n    evaluator_network = snt.Sequential([\n        observation_network,\n        agent_networks['policy'],\n        networks.StochasticMeanHead(),\n    ])\n\n    # Ensure network variables are created.\n    tf2_utils.create_variables(evaluator_network, [observation_spec])\n    policy_variables = {'policy': evaluator_network.variables}\n\n    # Create the variable client responsible for keeping the actor up-to-date.\n    variable_client = tf2_variable_utils.VariableClient(\n        variable_source,\n        policy_variables,\n        update_period=self._variable_update_period)\n\n    # Make sure not to evaluate a random actor by assigning variables before\n    # running the environment loop.\n    variable_client.update_and_wait()\n\n    # Create the agent.\n    evaluator = actors.FeedForwardActor(\n        policy_network=evaluator_network, variable_client=variable_client)\n\n    # Create logger and counter.\n    counter = counting.Counter(counter, 'evaluator')\n    logger = loggers.make_default_logger(\n        'evaluator', time_delta=self._log_every, steps_key='evaluator_steps')\n    observers = self._make_observers() if self._make_observers else ()\n\n    # Create the run loop and return it.\n    return acme.EnvironmentLoop(\n        environment,\n        evaluator,\n        counter,\n        logger,\n        observers=observers)\n\n  def build(self, name='dmpo'):\n    \"\"\"Build the distributed agent topology.\"\"\"\n    program = lp.Program(name=name)\n\n    with program.group('replay'):\n      replay = program.add_node(lp.ReverbNode(self.replay))\n\n    with program.group('counter'):\n      counter = program.add_node(lp.CourierNode(self.counter))\n\n      if self._max_actor_steps:\n        _ = program.add_node(\n            lp.CourierNode(self.coordinator, counter, self._max_actor_steps))\n\n    with program.group('learner'):\n      learner = program.add_node(\n          lp.CourierNode(self.learner, replay, counter))\n\n    with program.group('evaluator'):\n      program.add_node(\n          lp.CourierNode(self.evaluator, learner, counter))\n\n    if not self._num_caches:\n      # Use our learner as a single variable source.\n      sources = [learner]\n    else:\n      with program.group('cacher'):\n        # Create a set of learner caches.\n        sources = []\n        for _ in range(self._num_caches):\n          cacher = program.add_node(\n              lp.CacherNode(\n                  learner, refresh_interval_ms=2000, stale_after_ms=4000))\n          sources.append(cacher)\n\n    with program.group('actor'):\n      # Add actors which pull round-robin from our variable sources.\n      for actor_id in range(self._num_actors):\n        source = sources[actor_id % len(sources)]\n        program.add_node(\n            lp.CourierNode(self.actor, replay, source, counter, actor_id))\n\n    return program",
  "def __init__(\n      self,\n      environment_factory: Callable[[bool], dm_env.Environment],\n      network_factory: Callable[[specs.BoundedArray], Dict[str, snt.Module]],\n      num_actors: int = 1,\n      num_caches: int = 0,\n      environment_spec: Optional[specs.EnvironmentSpec] = None,\n      batch_size: int = 256,\n      prefetch_size: int = 4,\n      observation_augmentation: Optional[types.TensorTransformation] = None,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1000000,\n      samples_per_insert: Optional[float] = 32.0,\n      n_step: int = 5,\n      num_samples: int = 20,\n      additional_discount: float = 0.99,\n      target_policy_update_period: int = 100,\n      target_critic_update_period: int = 100,\n      variable_update_period: int = 1000,\n      policy_loss_factory: Optional[Callable[[], snt.Module]] = None,\n      max_actor_steps: Optional[int] = None,\n      log_every: float = 10.0,\n      make_observers: Optional[Callable[\n          [], Sequence[observers_lib.EnvLoopObserver]]] = None):\n\n    if environment_spec is None:\n      environment_spec = specs.make_environment_spec(environment_factory(False))\n\n    self._environment_factory = environment_factory\n    self._network_factory = network_factory\n    self._policy_loss_factory = policy_loss_factory\n    self._environment_spec = environment_spec\n    self._num_actors = num_actors\n    self._num_caches = num_caches\n    self._batch_size = batch_size\n    self._prefetch_size = prefetch_size\n    self._observation_augmentation = observation_augmentation\n    self._min_replay_size = min_replay_size\n    self._max_replay_size = max_replay_size\n    self._samples_per_insert = samples_per_insert\n    self._n_step = n_step\n    self._additional_discount = additional_discount\n    self._num_samples = num_samples\n    self._target_policy_update_period = target_policy_update_period\n    self._target_critic_update_period = target_critic_update_period\n    self._variable_update_period = variable_update_period\n    self._max_actor_steps = max_actor_steps\n    self._log_every = log_every\n    self._make_observers = make_observers",
  "def replay(self):\n    \"\"\"The replay storage.\"\"\"\n    if self._samples_per_insert is not None:\n      # Create enough of an error buffer to give a 10% tolerance in rate.\n      samples_per_insert_tolerance = 0.1 * self._samples_per_insert\n      error_buffer = self._min_replay_size * samples_per_insert_tolerance\n\n      limiter = reverb.rate_limiters.SampleToInsertRatio(\n          min_size_to_sample=self._min_replay_size,\n          samples_per_insert=self._samples_per_insert,\n          error_buffer=error_buffer)\n    else:\n      limiter = reverb.rate_limiters.MinSize(self._min_replay_size)\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=self._max_replay_size,\n        rate_limiter=limiter,\n        signature=adders.NStepTransitionAdder.signature(\n            self._environment_spec))\n    return [replay_table]",
  "def counter(self):\n    return tf2_savers.CheckpointingRunner(counting.Counter(),\n                                          time_delta_minutes=1,\n                                          subdirectory='counter')",
  "def coordinator(self, counter: counting.Counter, max_actor_steps: int):\n    return lp_utils.StepsLimiter(counter, max_actor_steps)",
  "def learner(\n      self,\n      replay: reverb.Client,\n      counter: counting.Counter,\n  ):\n    \"\"\"The Learning part of the agent.\"\"\"\n\n    act_spec = self._environment_spec.actions\n    obs_spec = self._environment_spec.observations\n\n    # Create online and target networks.\n    online_networks = self._network_factory(act_spec)\n    target_networks = self._network_factory(act_spec)\n\n    # Make sure observation network is a Sonnet Module.\n    observation_network = online_networks.get('observation', tf.identity)\n    target_observation_network = target_networks.get('observation', tf.identity)\n    observation_network = tf2_utils.to_sonnet_module(observation_network)\n    target_observation_network = tf2_utils.to_sonnet_module(\n        target_observation_network)\n\n    # Get embedding spec and create observation network variables.\n    emb_spec = tf2_utils.create_variables(observation_network, [obs_spec])\n\n    # Create variables.\n    tf2_utils.create_variables(online_networks['policy'], [emb_spec])\n    tf2_utils.create_variables(online_networks['critic'], [emb_spec, act_spec])\n    tf2_utils.create_variables(target_networks['policy'], [emb_spec])\n    tf2_utils.create_variables(target_networks['critic'], [emb_spec, act_spec])\n    tf2_utils.create_variables(target_observation_network, [obs_spec])\n\n    # The dataset object to learn from.\n    dataset = datasets.make_reverb_dataset(server_address=replay.server_address)\n    dataset = dataset.batch(self._batch_size, drop_remainder=True)\n    if self._observation_augmentation:\n      transform = image_augmentation.make_transform(\n          observation_transform=self._observation_augmentation)\n      dataset = dataset.map(\n          transform, num_parallel_calls=16, deterministic=False)\n    dataset = dataset.prefetch(self._prefetch_size)\n\n    counter = counting.Counter(counter, 'learner')\n    logger = loggers.make_default_logger(\n        'learner', time_delta=self._log_every, steps_key='learner_steps')\n\n    # Create policy loss module if a factory is passed.\n    if self._policy_loss_factory:\n      policy_loss_module = self._policy_loss_factory()\n    else:\n      policy_loss_module = None\n\n    # Return the learning agent.\n    return learning.DistributionalMPOLearner(\n        policy_network=online_networks['policy'],\n        critic_network=online_networks['critic'],\n        observation_network=observation_network,\n        target_policy_network=target_networks['policy'],\n        target_critic_network=target_networks['critic'],\n        target_observation_network=target_observation_network,\n        discount=self._additional_discount,\n        num_samples=self._num_samples,\n        target_policy_update_period=self._target_policy_update_period,\n        target_critic_update_period=self._target_critic_update_period,\n        policy_loss_module=policy_loss_module,\n        dataset=dataset,\n        counter=counter,\n        logger=logger)",
  "def actor(\n      self,\n      replay: reverb.Client,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n      actor_id: int,\n  ) -> acme.EnvironmentLoop:\n    \"\"\"The actor process.\"\"\"\n\n    action_spec = self._environment_spec.actions\n    observation_spec = self._environment_spec.observations\n\n    # Create environment and target networks to act with.\n    environment = self._environment_factory(False)\n    agent_networks = self._network_factory(action_spec)\n\n    # Make sure observation network is defined.\n    observation_network = agent_networks.get('observation', tf.identity)\n\n    # Create a stochastic behavior policy.\n    behavior_network = snt.Sequential([\n        observation_network,\n        agent_networks['policy'],\n        networks.StochasticSamplingHead(),\n    ])\n\n    # Ensure network variables are created.\n    tf2_utils.create_variables(behavior_network, [observation_spec])\n    policy_variables = {'policy': behavior_network.variables}\n\n    # Create the variable client responsible for keeping the actor up-to-date.\n    variable_client = tf2_variable_utils.VariableClient(\n        variable_source,\n        policy_variables,\n        update_period=self._variable_update_period)\n\n    # Make sure not to use a random policy after checkpoint restoration by\n    # assigning variables before running the environment loop.\n    variable_client.update_and_wait()\n\n    # Component to add things into replay.\n    adder = adders.NStepTransitionAdder(\n        client=replay,\n        n_step=self._n_step,\n        discount=self._additional_discount)\n\n    # Create the agent.\n    actor = actors.FeedForwardActor(\n        policy_network=behavior_network,\n        adder=adder,\n        variable_client=variable_client)\n\n    # Create logger and counter; only the first actor stores logs to bigtable.\n    save_data = actor_id == 0\n    counter = counting.Counter(counter, 'actor')\n    logger = loggers.make_default_logger(\n        'actor',\n        save_data=save_data,\n        time_delta=self._log_every,\n        steps_key='actor_steps')\n    observers = self._make_observers() if self._make_observers else ()\n\n    # Create the run loop and return it.\n    return acme.EnvironmentLoop(\n        environment, actor, counter, logger, observers=observers)",
  "def evaluator(\n      self,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ):\n    \"\"\"The evaluation process.\"\"\"\n\n    action_spec = self._environment_spec.actions\n    observation_spec = self._environment_spec.observations\n\n    # Create environment and target networks to act with.\n    environment = self._environment_factory(True)\n    agent_networks = self._network_factory(action_spec)\n\n    # Make sure observation network is defined.\n    observation_network = agent_networks.get('observation', tf.identity)\n\n    # Create a stochastic behavior policy.\n    evaluator_network = snt.Sequential([\n        observation_network,\n        agent_networks['policy'],\n        networks.StochasticMeanHead(),\n    ])\n\n    # Ensure network variables are created.\n    tf2_utils.create_variables(evaluator_network, [observation_spec])\n    policy_variables = {'policy': evaluator_network.variables}\n\n    # Create the variable client responsible for keeping the actor up-to-date.\n    variable_client = tf2_variable_utils.VariableClient(\n        variable_source,\n        policy_variables,\n        update_period=self._variable_update_period)\n\n    # Make sure not to evaluate a random actor by assigning variables before\n    # running the environment loop.\n    variable_client.update_and_wait()\n\n    # Create the agent.\n    evaluator = actors.FeedForwardActor(\n        policy_network=evaluator_network, variable_client=variable_client)\n\n    # Create logger and counter.\n    counter = counting.Counter(counter, 'evaluator')\n    logger = loggers.make_default_logger(\n        'evaluator', time_delta=self._log_every, steps_key='evaluator_steps')\n    observers = self._make_observers() if self._make_observers else ()\n\n    # Create the run loop and return it.\n    return acme.EnvironmentLoop(\n        environment,\n        evaluator,\n        counter,\n        logger,\n        observers=observers)",
  "def build(self, name='dmpo'):\n    \"\"\"Build the distributed agent topology.\"\"\"\n    program = lp.Program(name=name)\n\n    with program.group('replay'):\n      replay = program.add_node(lp.ReverbNode(self.replay))\n\n    with program.group('counter'):\n      counter = program.add_node(lp.CourierNode(self.counter))\n\n      if self._max_actor_steps:\n        _ = program.add_node(\n            lp.CourierNode(self.coordinator, counter, self._max_actor_steps))\n\n    with program.group('learner'):\n      learner = program.add_node(\n          lp.CourierNode(self.learner, replay, counter))\n\n    with program.group('evaluator'):\n      program.add_node(\n          lp.CourierNode(self.evaluator, learner, counter))\n\n    if not self._num_caches:\n      # Use our learner as a single variable source.\n      sources = [learner]\n    else:\n      with program.group('cacher'):\n        # Create a set of learner caches.\n        sources = []\n        for _ in range(self._num_caches):\n          cacher = program.add_node(\n              lp.CacherNode(\n                  learner, refresh_interval_ms=2000, stale_after_ms=4000))\n          sources.append(cacher)\n\n    with program.group('actor'):\n      # Add actors which pull round-robin from our variable sources.\n      for actor_id in range(self._num_actors):\n        source = sources[actor_id % len(sources)]\n        program.add_node(\n            lp.CourierNode(self.actor, replay, source, counter, actor_id))\n\n    return program",
  "class PolicyEvaluationConfig:\n  evaluate_stochastic_policy: bool = True\n  num_value_samples: int = 128",
  "class MoGMPOLearner(acme.Learner):\n  \"\"\"Distributional (MoG) MPO learner.\"\"\"\n\n  def __init__(\n      self,\n      policy_network: snt.Module,\n      critic_network: snt.Module,\n      target_policy_network: snt.Module,\n      target_critic_network: snt.Module,\n      discount: float,\n      num_samples: int,\n      target_policy_update_period: int,\n      target_critic_update_period: int,\n      dataset: tf.data.Dataset,\n      observation_network: snt.Module,\n      target_observation_network: snt.Module,\n      policy_evaluation_config: Optional[PolicyEvaluationConfig] = None,\n      policy_loss_module: Optional[snt.Module] = None,\n      policy_optimizer: Optional[snt.Optimizer] = None,\n      critic_optimizer: Optional[snt.Optimizer] = None,\n      dual_optimizer: Optional[snt.Optimizer] = None,\n      clipping: bool = True,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = True,\n  ):\n\n    # Store online and target networks.\n    self._policy_network = policy_network\n    self._critic_network = critic_network\n    self._observation_network = observation_network\n    self._target_policy_network = target_policy_network\n    self._target_critic_network = target_critic_network\n    self._target_observation_network = target_observation_network\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger('learner')\n\n    # Other learner parameters.\n    self._discount = discount\n    self._num_samples = num_samples\n    if policy_evaluation_config is None:\n      policy_evaluation_config = PolicyEvaluationConfig()\n    self._policy_evaluation_config = policy_evaluation_config\n    self._clipping = clipping\n\n    # Necessary to track when to update target networks.\n    self._num_steps = tf.Variable(0, dtype=tf.int32)\n    self._target_policy_update_period = target_policy_update_period\n    self._target_critic_update_period = target_critic_update_period\n\n    # Batch dataset and create iterator.\n    self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n    self._policy_loss_module = policy_loss_module or losses.MPO(\n        epsilon=1e-1,\n        epsilon_mean=3e-3,\n        epsilon_stddev=1e-6,\n        epsilon_penalty=1e-3,\n        init_log_temperature=10.,\n        init_log_alpha_mean=10.,\n        init_log_alpha_stddev=1000.)\n\n    # Create the optimizers.\n    self._critic_optimizer = critic_optimizer or snt.optimizers.Adam(1e-4)\n    self._policy_optimizer = policy_optimizer or snt.optimizers.Adam(1e-4)\n    self._dual_optimizer = dual_optimizer or snt.optimizers.Adam(1e-2)\n\n    # Expose the variables.\n    policy_network_to_expose = snt.Sequential(\n        [self._target_observation_network, self._target_policy_network])\n    self._variables = {\n        'critic': self._target_critic_network.variables,\n        'policy': policy_network_to_expose.variables,\n    }\n\n    # Create a checkpointer and snapshotter object.\n    self._checkpointer = None\n    self._snapshotter = None\n\n    if checkpoint:\n      self._checkpointer = tf2_savers.Checkpointer(\n          subdirectory='mog_mpo_learner',\n          objects_to_save={\n              'counter': self._counter,\n              'policy': self._policy_network,\n              'critic': self._critic_network,\n              'observation': self._observation_network,\n              'target_policy': self._target_policy_network,\n              'target_critic': self._target_critic_network,\n              'target_observation': self._target_observation_network,\n              'policy_optimizer': self._policy_optimizer,\n              'critic_optimizer': self._critic_optimizer,\n              'dual_optimizer': self._dual_optimizer,\n              'policy_loss_module': self._policy_loss_module,\n              'num_steps': self._num_steps,\n          })\n\n      self._snapshotter = tf2_savers.Snapshotter(\n          objects_to_save={\n              'policy':\n                  snt.Sequential([\n                      self._target_observation_network,\n                      self._target_policy_network\n                  ]),\n          })\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None\n\n  @tf.function\n  def _step(self, inputs: reverb.ReplaySample) -> types.NestedTensor:\n\n    # Get data from replay (dropping extras if any). Note there is no\n    # extra data here because we do not insert any into Reverb.\n    o_tm1, a_tm1, r_t, d_t, o_t = (inputs.data.observation, inputs.data.action,\n                                   inputs.data.reward, inputs.data.discount,\n                                   inputs.data.next_observation)\n\n    # Cast the additional discount to match the environment discount dtype.\n    discount = tf.cast(self._discount, dtype=d_t.dtype)\n\n    with tf.GradientTape(persistent=True) as tape:\n      # Maybe transform the observation before feeding into policy and critic.\n      # Transforming the observations this way at the start of the learning\n      # step effectively means that the policy and critic share observation\n      # network weights.\n      o_tm1 = self._observation_network(o_tm1)\n      # This stop_gradient prevents gradients to propagate into the target\n      # observation network. In addition, since the online policy network is\n      # evaluated at o_t, this also means the policy loss does not influence\n      # the observation network training.\n      o_t = tf.stop_gradient(self._target_observation_network(o_t))\n\n      # Get online and target action distributions from policy networks.\n      online_action_distribution = self._policy_network(o_t)\n      target_action_distribution = self._target_policy_network(o_t)\n\n      # Sample actions to evaluate policy; of size [N, B, ...].\n      sampled_actions = target_action_distribution.sample(self._num_samples)\n\n      # Tile embedded observations to feed into the target critic network.\n      # Note: this is more efficient than tiling before the embedding layer.\n      tiled_o_t = tf2_utils.tile_tensor(o_t, self._num_samples)  # [N, B, ...]\n\n      # Compute target-estimated distributional value of sampled actions at o_t.\n      sampled_q_t_distributions = self._target_critic_network(\n          # Merge batch dimensions; to shape [N*B, ...].\n          snt.merge_leading_dims(tiled_o_t, num_dims=2),\n          snt.merge_leading_dims(sampled_actions, num_dims=2))\n\n      # Compute online critic value distribution of a_tm1 in state o_tm1.\n      q_tm1_distribution = self._critic_network(o_tm1, a_tm1)  # [B, ...]\n\n      # Get the return distributions used in the policy evaluation bootstrap.\n      if self._policy_evaluation_config.evaluate_stochastic_policy:\n        z_distributions = sampled_q_t_distributions\n        num_joint_samples = self._num_samples\n      else:\n        z_distributions = self._target_critic_network(\n            o_t, target_action_distribution.mean())\n        num_joint_samples = 1\n\n      num_value_samples = self._policy_evaluation_config.num_value_samples\n      num_joint_samples *= num_value_samples\n      z_samples = z_distributions.sample(num_value_samples)\n      z_samples = tf.reshape(z_samples, (num_joint_samples, -1, 1))\n\n      # Expand dims of reward and discount tensors.\n      reward = r_t[..., tf.newaxis]  # [B, 1]\n      full_discount = discount * d_t[..., tf.newaxis]\n      target_q = reward + full_discount * z_samples  # [N, B, 1]\n      target_q = tf.stop_gradient(target_q)\n\n      # Compute sample-based cross-entropy.\n      log_probs_q = q_tm1_distribution.log_prob(target_q)  # [N, B, 1]\n      critic_loss = -tf.reduce_mean(log_probs_q, axis=0)  # [B, 1]\n      critic_loss = tf.reduce_mean(critic_loss)\n\n      # Compute Q-values of sampled actions and reshape to [N, B].\n      sampled_q_values = sampled_q_t_distributions.mean()\n      sampled_q_values = tf.reshape(sampled_q_values, (self._num_samples, -1))\n\n      # Compute MPO policy loss.\n      policy_loss, policy_stats = self._policy_loss_module(\n          online_action_distribution=online_action_distribution,\n          target_action_distribution=target_action_distribution,\n          actions=sampled_actions,\n          q_values=sampled_q_values)\n      policy_loss = tf.reduce_mean(policy_loss)\n\n    # For clarity, explicitly define which variables are trained by which loss.\n    critic_trainable_variables = (\n        # In this agent, the critic loss trains the observation network.\n        self._observation_network.trainable_variables +\n        self._critic_network.trainable_variables)\n    policy_trainable_variables = self._policy_network.trainable_variables\n    # The following are the MPO dual variables, stored in the loss module.\n    dual_trainable_variables = self._policy_loss_module.trainable_variables\n\n    # Compute gradients.\n    critic_gradients = tape.gradient(critic_loss, critic_trainable_variables)\n    policy_gradients, dual_gradients = tape.gradient(\n        policy_loss, (policy_trainable_variables, dual_trainable_variables))\n\n    # Delete the tape manually because of the persistent=True flag.\n    del tape\n\n    # Maybe clip gradients.\n    if self._clipping:\n      policy_gradients = tuple(tf.clip_by_global_norm(policy_gradients, 40.)[0])\n      critic_gradients = tuple(tf.clip_by_global_norm(critic_gradients, 40.)[0])\n\n    # Apply gradients.\n    self._critic_optimizer.apply(critic_gradients, critic_trainable_variables)\n    self._policy_optimizer.apply(policy_gradients, policy_trainable_variables)\n    self._dual_optimizer.apply(dual_gradients, dual_trainable_variables)\n\n    # Losses to track.\n    fetches = {\n        'critic_loss': critic_loss,\n        'policy_loss': policy_loss,\n    }\n    # Log MPO stats.\n    fetches.update(policy_stats)\n\n    return fetches\n\n  def step(self):\n    self._maybe_update_target_networks()\n    self._num_steps.assign_add(1)\n\n    # Run the learning step.\n    fetches = self._step(next(self._iterator))\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n    fetches.update(counts)\n\n    # Checkpoint and attempt to write the logs.\n    if self._checkpointer is not None:\n      self._checkpointer.save()\n    if self._snapshotter is not None:\n      self._snapshotter.save()\n    self._logger.write(fetches)\n\n  def get_variables(self, names: List[str]) -> List[List[np.ndarray]]:\n    return [tf2_utils.to_numpy(self._variables[name]) for name in names]\n\n  def _maybe_update_target_networks(self):\n    # Update target network.\n    online_policy_variables = self._policy_network.variables\n    target_policy_variables = self._target_policy_network.variables\n    online_critic_variables = (*self._observation_network.variables,\n                               *self._critic_network.variables)\n    target_critic_variables = (*self._target_observation_network.variables,\n                               *self._target_critic_network.variables)\n\n    # Make online policy -> target policy network update ops.\n    if tf.math.mod(self._num_steps, self._target_policy_update_period) == 0:\n      for src, dest in zip(online_policy_variables, target_policy_variables):\n        dest.assign(src)\n\n    # Make online critic -> target critic network update ops.\n    if tf.math.mod(self._num_steps, self._target_critic_update_period) == 0:\n      for src, dest in zip(online_critic_variables, target_critic_variables):\n        dest.assign(src)",
  "def __init__(\n      self,\n      policy_network: snt.Module,\n      critic_network: snt.Module,\n      target_policy_network: snt.Module,\n      target_critic_network: snt.Module,\n      discount: float,\n      num_samples: int,\n      target_policy_update_period: int,\n      target_critic_update_period: int,\n      dataset: tf.data.Dataset,\n      observation_network: snt.Module,\n      target_observation_network: snt.Module,\n      policy_evaluation_config: Optional[PolicyEvaluationConfig] = None,\n      policy_loss_module: Optional[snt.Module] = None,\n      policy_optimizer: Optional[snt.Optimizer] = None,\n      critic_optimizer: Optional[snt.Optimizer] = None,\n      dual_optimizer: Optional[snt.Optimizer] = None,\n      clipping: bool = True,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = True,\n  ):\n\n    # Store online and target networks.\n    self._policy_network = policy_network\n    self._critic_network = critic_network\n    self._observation_network = observation_network\n    self._target_policy_network = target_policy_network\n    self._target_critic_network = target_critic_network\n    self._target_observation_network = target_observation_network\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger('learner')\n\n    # Other learner parameters.\n    self._discount = discount\n    self._num_samples = num_samples\n    if policy_evaluation_config is None:\n      policy_evaluation_config = PolicyEvaluationConfig()\n    self._policy_evaluation_config = policy_evaluation_config\n    self._clipping = clipping\n\n    # Necessary to track when to update target networks.\n    self._num_steps = tf.Variable(0, dtype=tf.int32)\n    self._target_policy_update_period = target_policy_update_period\n    self._target_critic_update_period = target_critic_update_period\n\n    # Batch dataset and create iterator.\n    self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n    self._policy_loss_module = policy_loss_module or losses.MPO(\n        epsilon=1e-1,\n        epsilon_mean=3e-3,\n        epsilon_stddev=1e-6,\n        epsilon_penalty=1e-3,\n        init_log_temperature=10.,\n        init_log_alpha_mean=10.,\n        init_log_alpha_stddev=1000.)\n\n    # Create the optimizers.\n    self._critic_optimizer = critic_optimizer or snt.optimizers.Adam(1e-4)\n    self._policy_optimizer = policy_optimizer or snt.optimizers.Adam(1e-4)\n    self._dual_optimizer = dual_optimizer or snt.optimizers.Adam(1e-2)\n\n    # Expose the variables.\n    policy_network_to_expose = snt.Sequential(\n        [self._target_observation_network, self._target_policy_network])\n    self._variables = {\n        'critic': self._target_critic_network.variables,\n        'policy': policy_network_to_expose.variables,\n    }\n\n    # Create a checkpointer and snapshotter object.\n    self._checkpointer = None\n    self._snapshotter = None\n\n    if checkpoint:\n      self._checkpointer = tf2_savers.Checkpointer(\n          subdirectory='mog_mpo_learner',\n          objects_to_save={\n              'counter': self._counter,\n              'policy': self._policy_network,\n              'critic': self._critic_network,\n              'observation': self._observation_network,\n              'target_policy': self._target_policy_network,\n              'target_critic': self._target_critic_network,\n              'target_observation': self._target_observation_network,\n              'policy_optimizer': self._policy_optimizer,\n              'critic_optimizer': self._critic_optimizer,\n              'dual_optimizer': self._dual_optimizer,\n              'policy_loss_module': self._policy_loss_module,\n              'num_steps': self._num_steps,\n          })\n\n      self._snapshotter = tf2_savers.Snapshotter(\n          objects_to_save={\n              'policy':\n                  snt.Sequential([\n                      self._target_observation_network,\n                      self._target_policy_network\n                  ]),\n          })\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None",
  "def _step(self, inputs: reverb.ReplaySample) -> types.NestedTensor:\n\n    # Get data from replay (dropping extras if any). Note there is no\n    # extra data here because we do not insert any into Reverb.\n    o_tm1, a_tm1, r_t, d_t, o_t = (inputs.data.observation, inputs.data.action,\n                                   inputs.data.reward, inputs.data.discount,\n                                   inputs.data.next_observation)\n\n    # Cast the additional discount to match the environment discount dtype.\n    discount = tf.cast(self._discount, dtype=d_t.dtype)\n\n    with tf.GradientTape(persistent=True) as tape:\n      # Maybe transform the observation before feeding into policy and critic.\n      # Transforming the observations this way at the start of the learning\n      # step effectively means that the policy and critic share observation\n      # network weights.\n      o_tm1 = self._observation_network(o_tm1)\n      # This stop_gradient prevents gradients to propagate into the target\n      # observation network. In addition, since the online policy network is\n      # evaluated at o_t, this also means the policy loss does not influence\n      # the observation network training.\n      o_t = tf.stop_gradient(self._target_observation_network(o_t))\n\n      # Get online and target action distributions from policy networks.\n      online_action_distribution = self._policy_network(o_t)\n      target_action_distribution = self._target_policy_network(o_t)\n\n      # Sample actions to evaluate policy; of size [N, B, ...].\n      sampled_actions = target_action_distribution.sample(self._num_samples)\n\n      # Tile embedded observations to feed into the target critic network.\n      # Note: this is more efficient than tiling before the embedding layer.\n      tiled_o_t = tf2_utils.tile_tensor(o_t, self._num_samples)  # [N, B, ...]\n\n      # Compute target-estimated distributional value of sampled actions at o_t.\n      sampled_q_t_distributions = self._target_critic_network(\n          # Merge batch dimensions; to shape [N*B, ...].\n          snt.merge_leading_dims(tiled_o_t, num_dims=2),\n          snt.merge_leading_dims(sampled_actions, num_dims=2))\n\n      # Compute online critic value distribution of a_tm1 in state o_tm1.\n      q_tm1_distribution = self._critic_network(o_tm1, a_tm1)  # [B, ...]\n\n      # Get the return distributions used in the policy evaluation bootstrap.\n      if self._policy_evaluation_config.evaluate_stochastic_policy:\n        z_distributions = sampled_q_t_distributions\n        num_joint_samples = self._num_samples\n      else:\n        z_distributions = self._target_critic_network(\n            o_t, target_action_distribution.mean())\n        num_joint_samples = 1\n\n      num_value_samples = self._policy_evaluation_config.num_value_samples\n      num_joint_samples *= num_value_samples\n      z_samples = z_distributions.sample(num_value_samples)\n      z_samples = tf.reshape(z_samples, (num_joint_samples, -1, 1))\n\n      # Expand dims of reward and discount tensors.\n      reward = r_t[..., tf.newaxis]  # [B, 1]\n      full_discount = discount * d_t[..., tf.newaxis]\n      target_q = reward + full_discount * z_samples  # [N, B, 1]\n      target_q = tf.stop_gradient(target_q)\n\n      # Compute sample-based cross-entropy.\n      log_probs_q = q_tm1_distribution.log_prob(target_q)  # [N, B, 1]\n      critic_loss = -tf.reduce_mean(log_probs_q, axis=0)  # [B, 1]\n      critic_loss = tf.reduce_mean(critic_loss)\n\n      # Compute Q-values of sampled actions and reshape to [N, B].\n      sampled_q_values = sampled_q_t_distributions.mean()\n      sampled_q_values = tf.reshape(sampled_q_values, (self._num_samples, -1))\n\n      # Compute MPO policy loss.\n      policy_loss, policy_stats = self._policy_loss_module(\n          online_action_distribution=online_action_distribution,\n          target_action_distribution=target_action_distribution,\n          actions=sampled_actions,\n          q_values=sampled_q_values)\n      policy_loss = tf.reduce_mean(policy_loss)\n\n    # For clarity, explicitly define which variables are trained by which loss.\n    critic_trainable_variables = (\n        # In this agent, the critic loss trains the observation network.\n        self._observation_network.trainable_variables +\n        self._critic_network.trainable_variables)\n    policy_trainable_variables = self._policy_network.trainable_variables\n    # The following are the MPO dual variables, stored in the loss module.\n    dual_trainable_variables = self._policy_loss_module.trainable_variables\n\n    # Compute gradients.\n    critic_gradients = tape.gradient(critic_loss, critic_trainable_variables)\n    policy_gradients, dual_gradients = tape.gradient(\n        policy_loss, (policy_trainable_variables, dual_trainable_variables))\n\n    # Delete the tape manually because of the persistent=True flag.\n    del tape\n\n    # Maybe clip gradients.\n    if self._clipping:\n      policy_gradients = tuple(tf.clip_by_global_norm(policy_gradients, 40.)[0])\n      critic_gradients = tuple(tf.clip_by_global_norm(critic_gradients, 40.)[0])\n\n    # Apply gradients.\n    self._critic_optimizer.apply(critic_gradients, critic_trainable_variables)\n    self._policy_optimizer.apply(policy_gradients, policy_trainable_variables)\n    self._dual_optimizer.apply(dual_gradients, dual_trainable_variables)\n\n    # Losses to track.\n    fetches = {\n        'critic_loss': critic_loss,\n        'policy_loss': policy_loss,\n    }\n    # Log MPO stats.\n    fetches.update(policy_stats)\n\n    return fetches",
  "def step(self):\n    self._maybe_update_target_networks()\n    self._num_steps.assign_add(1)\n\n    # Run the learning step.\n    fetches = self._step(next(self._iterator))\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n    fetches.update(counts)\n\n    # Checkpoint and attempt to write the logs.\n    if self._checkpointer is not None:\n      self._checkpointer.save()\n    if self._snapshotter is not None:\n      self._snapshotter.save()\n    self._logger.write(fetches)",
  "def get_variables(self, names: List[str]) -> List[List[np.ndarray]]:\n    return [tf2_utils.to_numpy(self._variables[name]) for name in names]",
  "def _maybe_update_target_networks(self):\n    # Update target network.\n    online_policy_variables = self._policy_network.variables\n    target_policy_variables = self._target_policy_network.variables\n    online_critic_variables = (*self._observation_network.variables,\n                               *self._critic_network.variables)\n    target_critic_variables = (*self._target_observation_network.variables,\n                               *self._target_critic_network.variables)\n\n    # Make online policy -> target policy network update ops.\n    if tf.math.mod(self._num_steps, self._target_policy_update_period) == 0:\n      for src, dest in zip(online_policy_variables, target_policy_variables):\n        dest.assign(src)\n\n    # Make online critic -> target critic network update ops.\n    if tf.math.mod(self._num_steps, self._target_critic_update_period) == 0:\n      for src, dest in zip(online_critic_variables, target_critic_variables):\n        dest.assign(src)",
  "def make_default_networks(\n    environment_spec: specs.EnvironmentSpec,\n    *,\n    policy_layer_sizes: Sequence[int] = (256, 256, 256),\n    critic_layer_sizes: Sequence[int] = (512, 512, 256),\n    policy_init_scale: float = 0.7,\n    critic_init_scale: float = 1e-3,\n    critic_num_components: int = 5,\n) -> Mapping[str, snt.Module]:\n  \"\"\"Creates networks used by the agent.\"\"\"\n\n  # Unpack the environment spec to get appropriate shapes, dtypes, etc.\n  act_spec = environment_spec.actions\n  obs_spec = environment_spec.observations\n  num_dimensions = np.prod(act_spec.shape, dtype=int)\n\n  # Create the observation network and make sure it's a Sonnet module.\n  observation_network = tf2_utils.batch_concat\n  observation_network = tf2_utils.to_sonnet_module(observation_network)\n\n  # Create the policy network.\n  policy_network = snt.Sequential([\n      networks.LayerNormMLP(policy_layer_sizes, activate_final=True),\n      networks.MultivariateNormalDiagHead(\n          num_dimensions,\n          init_scale=policy_init_scale,\n          use_tfd_independent=True)\n  ])\n\n  # The multiplexer concatenates the (maybe transformed) observations/actions.\n  critic_network = snt.Sequential([\n      networks.CriticMultiplexer(action_network=networks.ClipToSpec(act_spec)),\n      networks.LayerNormMLP(critic_layer_sizes, activate_final=True),\n      networks.GaussianMixtureHead(\n          num_dimensions=1,\n          num_components=critic_num_components,\n          init_scale=critic_init_scale)\n  ])\n\n  # Create network variables.\n  # Get embedding spec by creating observation network variables.\n  emb_spec = tf2_utils.create_variables(observation_network, [obs_spec])\n  tf2_utils.create_variables(policy_network, [emb_spec])\n  tf2_utils.create_variables(critic_network, [emb_spec, act_spec])\n\n  return {\n      'policy': policy_network,\n      'critic': critic_network,\n      'observation': observation_network,\n  }",
  "class DistributedMoGMPO:\n  \"\"\"Program definition for distributional (MoG) MPO.\"\"\"\n\n  def __init__(\n      self,\n      environment_factory: Callable[[bool], dm_env.Environment],\n      network_factory: Callable[[specs.EnvironmentSpec], Dict[str, snt.Module]],\n      num_actors: int = 1,\n      num_caches: int = 0,\n      environment_spec: Optional[specs.EnvironmentSpec] = None,\n      batch_size: int = 256,\n      prefetch_size: int = 4,\n      min_replay_size: int = 1_000,\n      max_replay_size: int = 1_000_000,\n      samples_per_insert: Optional[float] = 32.0,\n      n_step: int = 5,\n      num_samples: int = 20,\n      policy_evaluation_config: Optional[\n          learning.PolicyEvaluationConfig] = None,\n      additional_discount: float = 0.99,\n      target_policy_update_period: int = 100,\n      target_critic_update_period: int = 100,\n      policy_loss_factory: Optional[Callable[[], snt.Module]] = None,\n      max_actor_steps: Optional[int] = None,\n      log_every: float = 10.0,\n  ):\n\n    if environment_spec is None:\n      environment_spec = specs.make_environment_spec(environment_factory(False))\n\n    self._environment_factory = environment_factory\n    self._network_factory = network_factory\n    self._policy_loss_factory = policy_loss_factory\n    self._environment_spec = environment_spec\n    self._num_actors = num_actors\n    self._num_caches = num_caches\n    self._batch_size = batch_size\n    self._prefetch_size = prefetch_size\n    self._min_replay_size = min_replay_size\n    self._max_replay_size = max_replay_size\n    self._samples_per_insert = samples_per_insert\n    self._n_step = n_step\n    self._additional_discount = additional_discount\n    self._num_samples = num_samples\n    self._policy_evaluation_config = policy_evaluation_config\n    self._target_policy_update_period = target_policy_update_period\n    self._target_critic_update_period = target_critic_update_period\n    self._max_actor_steps = max_actor_steps\n    self._log_every = log_every\n\n  def replay(self):\n    \"\"\"The replay storage.\"\"\"\n    if self._samples_per_insert is not None:\n      # Create enough of an error buffer to give a 10% tolerance in rate.\n      samples_per_insert_tolerance = 0.1 * self._samples_per_insert\n      error_buffer = self._min_replay_size * samples_per_insert_tolerance\n\n      limiter = reverb.rate_limiters.SampleToInsertRatio(\n          min_size_to_sample=self._min_replay_size,\n          samples_per_insert=self._samples_per_insert,\n          error_buffer=error_buffer)\n    else:\n      limiter = reverb.rate_limiters.MinSize(self._min_replay_size)\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=self._max_replay_size,\n        rate_limiter=limiter,\n        signature=adders.NStepTransitionAdder.signature(\n            self._environment_spec))\n    return [replay_table]\n\n  def counter(self):\n    return tf2_savers.CheckpointingRunner(\n        counting.Counter(), time_delta_minutes=1, subdirectory='counter')\n\n  def coordinator(self, counter: counting.Counter, max_actor_steps: int):\n    return lp_utils.StepsLimiter(counter, max_actor_steps)\n\n  def learner(\n      self,\n      replay: reverb.Client,\n      counter: counting.Counter,\n  ):\n    \"\"\"The Learning part of the agent.\"\"\"\n\n    # Create online and target networks.\n    online_networks = self._network_factory(self._environment_spec)\n    target_networks = self._network_factory(self._environment_spec)\n\n    # The dataset object to learn from.\n    dataset = datasets.make_reverb_dataset(\n        server_address=replay.server_address,\n        batch_size=self._batch_size,\n        prefetch_size=self._prefetch_size,\n    )\n\n    counter = counting.Counter(counter, 'learner')\n    logger = loggers.make_default_logger('learner', time_delta=self._log_every)\n\n    # Create policy loss module if a factory is passed.\n    if self._policy_loss_factory:\n      policy_loss_module = self._policy_loss_factory()\n    else:\n      policy_loss_module = None\n\n    # Return the learning agent.\n    return learning.MoGMPOLearner(\n        policy_network=online_networks['policy'],\n        critic_network=online_networks['critic'],\n        observation_network=online_networks['observation'],\n        target_policy_network=target_networks['policy'],\n        target_critic_network=target_networks['critic'],\n        target_observation_network=target_networks['observation'],\n        discount=self._additional_discount,\n        num_samples=self._num_samples,\n        policy_evaluation_config=self._policy_evaluation_config,\n        target_policy_update_period=self._target_policy_update_period,\n        target_critic_update_period=self._target_critic_update_period,\n        policy_loss_module=policy_loss_module,\n        dataset=dataset,\n        counter=counter,\n        logger=logger)\n\n  def actor(\n      self,\n      replay: reverb.Client,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n      actor_id: int,\n  ) -> acme.EnvironmentLoop:\n    \"\"\"The actor process.\"\"\"\n\n    # Create environment and target networks to act with.\n    environment = self._environment_factory(False)\n    agent_networks = self._network_factory(self._environment_spec)\n\n    # Create a stochastic behavior policy.\n    behavior_network = snt.Sequential([\n        agent_networks['observation'],\n        agent_networks['policy'],\n        networks.StochasticSamplingHead(),\n    ])\n\n    # Ensure network variables are created.\n    policy_variables = {'policy': behavior_network.variables}\n\n    # Create the variable client responsible for keeping the actor up-to-date.\n    variable_client = tf2_variable_utils.VariableClient(\n        variable_source, policy_variables, update_period=1000)\n\n    # Make sure not to use a random policy after checkpoint restoration by\n    # assigning variables before running the environment loop.\n    variable_client.update_and_wait()\n\n    # Component to add things into replay.\n    adder = adders.NStepTransitionAdder(\n        client=replay, n_step=self._n_step, discount=self._additional_discount)\n\n    # Create the agent.\n    actor = actors.FeedForwardActor(\n        policy_network=behavior_network,\n        adder=adder,\n        variable_client=variable_client)\n\n    # Create logger and counter; actors will not spam bigtable.\n    save_data = actor_id == 0\n    counter = counting.Counter(counter, 'actor')\n    logger = loggers.make_default_logger(\n        'actor', save_data=save_data, time_delta=self._log_every)\n\n    # Create the run loop and return it.\n    return acme.EnvironmentLoop(environment, actor, counter, logger)\n\n  def evaluator(\n      self,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ):\n    \"\"\"The evaluation process.\"\"\"\n\n    # Create environment and target networks to act with.\n    environment = self._environment_factory(True)\n    agent_networks = self._network_factory(self._environment_spec)\n\n    # Create a stochastic behavior policy.\n    evaluator_network = snt.Sequential([\n        agent_networks['observation'],\n        agent_networks['policy'],\n        networks.StochasticMeanHead(),\n    ])\n\n    # Create the variable client responsible for keeping the actor up-to-date.\n    variable_client = tf2_variable_utils.VariableClient(\n        variable_source,\n        variables={'policy': evaluator_network.variables},\n        update_period=1000)\n\n    # Make sure not to evaluate a random actor by assigning variables before\n    # running the environment loop.\n    variable_client.update_and_wait()\n\n    # Create the agent.\n    evaluator = actors.FeedForwardActor(\n        policy_network=evaluator_network, variable_client=variable_client)\n\n    # Create logger and counter.\n    counter = counting.Counter(counter, 'evaluator')\n    logger = loggers.make_default_logger(\n        'evaluator', time_delta=self._log_every)\n\n    # Create the run loop and return it.\n    return acme.EnvironmentLoop(environment, evaluator, counter, logger)\n\n  def build(self, name='dmpo'):\n    \"\"\"Build the distributed agent topology.\"\"\"\n    program = lp.Program(name=name)\n\n    with program.group('replay'):\n      replay = program.add_node(lp.ReverbNode(self.replay))\n\n    with program.group('counter'):\n      counter = program.add_node(lp.CourierNode(self.counter))\n\n      if self._max_actor_steps:\n        _ = program.add_node(\n            lp.CourierNode(self.coordinator, counter, self._max_actor_steps))\n\n    with program.group('learner'):\n      learner = program.add_node(lp.CourierNode(self.learner, replay, counter))\n\n    with program.group('evaluator'):\n      program.add_node(lp.CourierNode(self.evaluator, learner, counter))\n\n    if not self._num_caches:\n      # Use our learner as a single variable source.\n      sources = [learner]\n    else:\n      with program.group('cacher'):\n        # Create a set of learner caches.\n        sources = []\n        for _ in range(self._num_caches):\n          cacher = program.add_node(\n              lp.CacherNode(\n                  learner, refresh_interval_ms=2000, stale_after_ms=4000))\n          sources.append(cacher)\n\n    with program.group('actor'):\n      # Add actors which pull round-robin from our variable sources.\n      for actor_id in range(self._num_actors):\n        source = sources[actor_id % len(sources)]\n        program.add_node(\n            lp.CourierNode(self.actor, replay, source, counter, actor_id))\n\n    return program",
  "def __init__(\n      self,\n      environment_factory: Callable[[bool], dm_env.Environment],\n      network_factory: Callable[[specs.EnvironmentSpec], Dict[str, snt.Module]],\n      num_actors: int = 1,\n      num_caches: int = 0,\n      environment_spec: Optional[specs.EnvironmentSpec] = None,\n      batch_size: int = 256,\n      prefetch_size: int = 4,\n      min_replay_size: int = 1_000,\n      max_replay_size: int = 1_000_000,\n      samples_per_insert: Optional[float] = 32.0,\n      n_step: int = 5,\n      num_samples: int = 20,\n      policy_evaluation_config: Optional[\n          learning.PolicyEvaluationConfig] = None,\n      additional_discount: float = 0.99,\n      target_policy_update_period: int = 100,\n      target_critic_update_period: int = 100,\n      policy_loss_factory: Optional[Callable[[], snt.Module]] = None,\n      max_actor_steps: Optional[int] = None,\n      log_every: float = 10.0,\n  ):\n\n    if environment_spec is None:\n      environment_spec = specs.make_environment_spec(environment_factory(False))\n\n    self._environment_factory = environment_factory\n    self._network_factory = network_factory\n    self._policy_loss_factory = policy_loss_factory\n    self._environment_spec = environment_spec\n    self._num_actors = num_actors\n    self._num_caches = num_caches\n    self._batch_size = batch_size\n    self._prefetch_size = prefetch_size\n    self._min_replay_size = min_replay_size\n    self._max_replay_size = max_replay_size\n    self._samples_per_insert = samples_per_insert\n    self._n_step = n_step\n    self._additional_discount = additional_discount\n    self._num_samples = num_samples\n    self._policy_evaluation_config = policy_evaluation_config\n    self._target_policy_update_period = target_policy_update_period\n    self._target_critic_update_period = target_critic_update_period\n    self._max_actor_steps = max_actor_steps\n    self._log_every = log_every",
  "def replay(self):\n    \"\"\"The replay storage.\"\"\"\n    if self._samples_per_insert is not None:\n      # Create enough of an error buffer to give a 10% tolerance in rate.\n      samples_per_insert_tolerance = 0.1 * self._samples_per_insert\n      error_buffer = self._min_replay_size * samples_per_insert_tolerance\n\n      limiter = reverb.rate_limiters.SampleToInsertRatio(\n          min_size_to_sample=self._min_replay_size,\n          samples_per_insert=self._samples_per_insert,\n          error_buffer=error_buffer)\n    else:\n      limiter = reverb.rate_limiters.MinSize(self._min_replay_size)\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=self._max_replay_size,\n        rate_limiter=limiter,\n        signature=adders.NStepTransitionAdder.signature(\n            self._environment_spec))\n    return [replay_table]",
  "def counter(self):\n    return tf2_savers.CheckpointingRunner(\n        counting.Counter(), time_delta_minutes=1, subdirectory='counter')",
  "def coordinator(self, counter: counting.Counter, max_actor_steps: int):\n    return lp_utils.StepsLimiter(counter, max_actor_steps)",
  "def learner(\n      self,\n      replay: reverb.Client,\n      counter: counting.Counter,\n  ):\n    \"\"\"The Learning part of the agent.\"\"\"\n\n    # Create online and target networks.\n    online_networks = self._network_factory(self._environment_spec)\n    target_networks = self._network_factory(self._environment_spec)\n\n    # The dataset object to learn from.\n    dataset = datasets.make_reverb_dataset(\n        server_address=replay.server_address,\n        batch_size=self._batch_size,\n        prefetch_size=self._prefetch_size,\n    )\n\n    counter = counting.Counter(counter, 'learner')\n    logger = loggers.make_default_logger('learner', time_delta=self._log_every)\n\n    # Create policy loss module if a factory is passed.\n    if self._policy_loss_factory:\n      policy_loss_module = self._policy_loss_factory()\n    else:\n      policy_loss_module = None\n\n    # Return the learning agent.\n    return learning.MoGMPOLearner(\n        policy_network=online_networks['policy'],\n        critic_network=online_networks['critic'],\n        observation_network=online_networks['observation'],\n        target_policy_network=target_networks['policy'],\n        target_critic_network=target_networks['critic'],\n        target_observation_network=target_networks['observation'],\n        discount=self._additional_discount,\n        num_samples=self._num_samples,\n        policy_evaluation_config=self._policy_evaluation_config,\n        target_policy_update_period=self._target_policy_update_period,\n        target_critic_update_period=self._target_critic_update_period,\n        policy_loss_module=policy_loss_module,\n        dataset=dataset,\n        counter=counter,\n        logger=logger)",
  "def actor(\n      self,\n      replay: reverb.Client,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n      actor_id: int,\n  ) -> acme.EnvironmentLoop:\n    \"\"\"The actor process.\"\"\"\n\n    # Create environment and target networks to act with.\n    environment = self._environment_factory(False)\n    agent_networks = self._network_factory(self._environment_spec)\n\n    # Create a stochastic behavior policy.\n    behavior_network = snt.Sequential([\n        agent_networks['observation'],\n        agent_networks['policy'],\n        networks.StochasticSamplingHead(),\n    ])\n\n    # Ensure network variables are created.\n    policy_variables = {'policy': behavior_network.variables}\n\n    # Create the variable client responsible for keeping the actor up-to-date.\n    variable_client = tf2_variable_utils.VariableClient(\n        variable_source, policy_variables, update_period=1000)\n\n    # Make sure not to use a random policy after checkpoint restoration by\n    # assigning variables before running the environment loop.\n    variable_client.update_and_wait()\n\n    # Component to add things into replay.\n    adder = adders.NStepTransitionAdder(\n        client=replay, n_step=self._n_step, discount=self._additional_discount)\n\n    # Create the agent.\n    actor = actors.FeedForwardActor(\n        policy_network=behavior_network,\n        adder=adder,\n        variable_client=variable_client)\n\n    # Create logger and counter; actors will not spam bigtable.\n    save_data = actor_id == 0\n    counter = counting.Counter(counter, 'actor')\n    logger = loggers.make_default_logger(\n        'actor', save_data=save_data, time_delta=self._log_every)\n\n    # Create the run loop and return it.\n    return acme.EnvironmentLoop(environment, actor, counter, logger)",
  "def evaluator(\n      self,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ):\n    \"\"\"The evaluation process.\"\"\"\n\n    # Create environment and target networks to act with.\n    environment = self._environment_factory(True)\n    agent_networks = self._network_factory(self._environment_spec)\n\n    # Create a stochastic behavior policy.\n    evaluator_network = snt.Sequential([\n        agent_networks['observation'],\n        agent_networks['policy'],\n        networks.StochasticMeanHead(),\n    ])\n\n    # Create the variable client responsible for keeping the actor up-to-date.\n    variable_client = tf2_variable_utils.VariableClient(\n        variable_source,\n        variables={'policy': evaluator_network.variables},\n        update_period=1000)\n\n    # Make sure not to evaluate a random actor by assigning variables before\n    # running the environment loop.\n    variable_client.update_and_wait()\n\n    # Create the agent.\n    evaluator = actors.FeedForwardActor(\n        policy_network=evaluator_network, variable_client=variable_client)\n\n    # Create logger and counter.\n    counter = counting.Counter(counter, 'evaluator')\n    logger = loggers.make_default_logger(\n        'evaluator', time_delta=self._log_every)\n\n    # Create the run loop and return it.\n    return acme.EnvironmentLoop(environment, evaluator, counter, logger)",
  "def build(self, name='dmpo'):\n    \"\"\"Build the distributed agent topology.\"\"\"\n    program = lp.Program(name=name)\n\n    with program.group('replay'):\n      replay = program.add_node(lp.ReverbNode(self.replay))\n\n    with program.group('counter'):\n      counter = program.add_node(lp.CourierNode(self.counter))\n\n      if self._max_actor_steps:\n        _ = program.add_node(\n            lp.CourierNode(self.coordinator, counter, self._max_actor_steps))\n\n    with program.group('learner'):\n      learner = program.add_node(lp.CourierNode(self.learner, replay, counter))\n\n    with program.group('evaluator'):\n      program.add_node(lp.CourierNode(self.evaluator, learner, counter))\n\n    if not self._num_caches:\n      # Use our learner as a single variable source.\n      sources = [learner]\n    else:\n      with program.group('cacher'):\n        # Create a set of learner caches.\n        sources = []\n        for _ in range(self._num_caches):\n          cacher = program.add_node(\n              lp.CacherNode(\n                  learner, refresh_interval_ms=2000, stale_after_ms=4000))\n          sources.append(cacher)\n\n    with program.group('actor'):\n      # Add actors which pull round-robin from our variable sources.\n      for actor_id in range(self._num_actors):\n        source = sources[actor_id % len(sources)]\n        program.add_node(\n            lp.CourierNode(self.actor, replay, source, counter, actor_id))\n\n    return program",
  "class DQN(agent.Agent):\n  \"\"\"DQN agent.\n\n  This implements a single-process DQN agent. This is a simple Q-learning\n  algorithm that inserts N-step transitions into a replay buffer, and\n  periodically updates its policy by sampling these transitions using\n  prioritization.\n  \"\"\"\n\n  def __init__(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      network: snt.Module,\n      batch_size: int = 256,\n      prefetch_size: int = 4,\n      target_update_period: int = 100,\n      samples_per_insert: float = 32.0,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1000000,\n      importance_sampling_exponent: float = 0.2,\n      priority_exponent: float = 0.6,\n      n_step: int = 5,\n      epsilon: Optional[tf.Variable] = None,\n      learning_rate: float = 1e-3,\n      discount: float = 0.99,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = True,\n      checkpoint_subpath: str = '~/acme',\n      policy_network: Optional[snt.Module] = None,\n      max_gradient_norm: Optional[float] = None,\n  ):\n    \"\"\"Initialize the agent.\n\n    Args:\n      environment_spec: description of the actions, observations, etc.\n      network: the online Q network (the one being optimized)\n      batch_size: batch size for updates.\n      prefetch_size: size to prefetch from replay.\n      target_update_period: number of learner steps to perform before updating\n        the target networks.\n      samples_per_insert: number of samples to take from replay for every insert\n        that is made.\n      min_replay_size: minimum replay size before updating. This and all\n        following arguments are related to dataset construction and will be\n        ignored if a dataset argument is passed.\n      max_replay_size: maximum replay size.\n      importance_sampling_exponent: power to which importance weights are raised\n        before normalizing.\n      priority_exponent: exponent used in prioritized sampling.\n      n_step: number of steps to squash into a single transition.\n      epsilon: probability of taking a random action; ignored if a policy\n        network is given.\n      learning_rate: learning rate for the q-network update.\n      discount: discount to use for TD updates.\n      logger: logger object to be used by learner.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n      checkpoint_subpath: string indicating where the agent should save\n        checkpoints and snapshots.\n      policy_network: if given, this will be used as the policy network.\n        Otherwise, an epsilon greedy policy using the online Q network will be\n        created. Policy network is used in the actor to sample actions.\n      max_gradient_norm: used for gradient clipping.\n    \"\"\"\n\n    # Create a replay server to add data to. This uses no limiter behavior in\n    # order to allow the Agent interface to handle it.\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Prioritized(priority_exponent),\n        remover=reverb.selectors.Fifo(),\n        max_size=max_replay_size,\n        rate_limiter=reverb.rate_limiters.MinSize(1),\n        signature=adders.NStepTransitionAdder.signature(environment_spec))\n    self._server = reverb.Server([replay_table], port=None)\n\n    # The adder is used to insert observations into replay.\n    address = f'localhost:{self._server.port}'\n    adder = adders.NStepTransitionAdder(\n        client=reverb.Client(address),\n        n_step=n_step,\n        discount=discount)\n\n    # The dataset provides an interface to sample from replay.\n    replay_client = reverb.Client(address)\n    dataset = datasets.make_reverb_dataset(\n        server_address=address,\n        batch_size=batch_size,\n        prefetch_size=prefetch_size)\n\n    # Create epsilon greedy policy network by default.\n    if policy_network is None:\n      # Use constant 0.05 epsilon greedy policy by default.\n      if epsilon is None:\n        epsilon = tf.Variable(0.05, trainable=False)\n      policy_network = snt.Sequential([\n          network,\n          lambda q: trfl.epsilon_greedy(q, epsilon=epsilon).sample(),\n      ])\n\n    # Create a target network.\n    target_network = copy.deepcopy(network)\n\n    # Ensure that we create the variables before proceeding (maybe not needed).\n    tf2_utils.create_variables(network, [environment_spec.observations])\n    tf2_utils.create_variables(target_network, [environment_spec.observations])\n\n    # Create the actor which defines how we take actions.\n    actor = actors.FeedForwardActor(policy_network, adder)\n\n    # The learner updates the parameters (and initializes them).\n    learner = learning.DQNLearner(\n        network=network,\n        target_network=target_network,\n        discount=discount,\n        importance_sampling_exponent=importance_sampling_exponent,\n        learning_rate=learning_rate,\n        target_update_period=target_update_period,\n        dataset=dataset,\n        replay_client=replay_client,\n        max_gradient_norm=max_gradient_norm,\n        logger=logger,\n        checkpoint=checkpoint,\n        save_directory=checkpoint_subpath)\n\n    if checkpoint:\n      self._checkpointer = tf2_savers.Checkpointer(\n          directory=checkpoint_subpath,\n          objects_to_save=learner.state,\n          subdirectory='dqn_learner',\n          time_delta_minutes=60.)\n    else:\n      self._checkpointer = None\n\n    super().__init__(\n        actor=actor,\n        learner=learner,\n        min_observations=max(batch_size, min_replay_size),\n        observations_per_step=float(batch_size) / samples_per_insert)\n\n  def update(self):\n    super().update()\n    if self._checkpointer is not None:\n      self._checkpointer.save()",
  "def __init__(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      network: snt.Module,\n      batch_size: int = 256,\n      prefetch_size: int = 4,\n      target_update_period: int = 100,\n      samples_per_insert: float = 32.0,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1000000,\n      importance_sampling_exponent: float = 0.2,\n      priority_exponent: float = 0.6,\n      n_step: int = 5,\n      epsilon: Optional[tf.Variable] = None,\n      learning_rate: float = 1e-3,\n      discount: float = 0.99,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = True,\n      checkpoint_subpath: str = '~/acme',\n      policy_network: Optional[snt.Module] = None,\n      max_gradient_norm: Optional[float] = None,\n  ):\n    \"\"\"Initialize the agent.\n\n    Args:\n      environment_spec: description of the actions, observations, etc.\n      network: the online Q network (the one being optimized)\n      batch_size: batch size for updates.\n      prefetch_size: size to prefetch from replay.\n      target_update_period: number of learner steps to perform before updating\n        the target networks.\n      samples_per_insert: number of samples to take from replay for every insert\n        that is made.\n      min_replay_size: minimum replay size before updating. This and all\n        following arguments are related to dataset construction and will be\n        ignored if a dataset argument is passed.\n      max_replay_size: maximum replay size.\n      importance_sampling_exponent: power to which importance weights are raised\n        before normalizing.\n      priority_exponent: exponent used in prioritized sampling.\n      n_step: number of steps to squash into a single transition.\n      epsilon: probability of taking a random action; ignored if a policy\n        network is given.\n      learning_rate: learning rate for the q-network update.\n      discount: discount to use for TD updates.\n      logger: logger object to be used by learner.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n      checkpoint_subpath: string indicating where the agent should save\n        checkpoints and snapshots.\n      policy_network: if given, this will be used as the policy network.\n        Otherwise, an epsilon greedy policy using the online Q network will be\n        created. Policy network is used in the actor to sample actions.\n      max_gradient_norm: used for gradient clipping.\n    \"\"\"\n\n    # Create a replay server to add data to. This uses no limiter behavior in\n    # order to allow the Agent interface to handle it.\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Prioritized(priority_exponent),\n        remover=reverb.selectors.Fifo(),\n        max_size=max_replay_size,\n        rate_limiter=reverb.rate_limiters.MinSize(1),\n        signature=adders.NStepTransitionAdder.signature(environment_spec))\n    self._server = reverb.Server([replay_table], port=None)\n\n    # The adder is used to insert observations into replay.\n    address = f'localhost:{self._server.port}'\n    adder = adders.NStepTransitionAdder(\n        client=reverb.Client(address),\n        n_step=n_step,\n        discount=discount)\n\n    # The dataset provides an interface to sample from replay.\n    replay_client = reverb.Client(address)\n    dataset = datasets.make_reverb_dataset(\n        server_address=address,\n        batch_size=batch_size,\n        prefetch_size=prefetch_size)\n\n    # Create epsilon greedy policy network by default.\n    if policy_network is None:\n      # Use constant 0.05 epsilon greedy policy by default.\n      if epsilon is None:\n        epsilon = tf.Variable(0.05, trainable=False)\n      policy_network = snt.Sequential([\n          network,\n          lambda q: trfl.epsilon_greedy(q, epsilon=epsilon).sample(),\n      ])\n\n    # Create a target network.\n    target_network = copy.deepcopy(network)\n\n    # Ensure that we create the variables before proceeding (maybe not needed).\n    tf2_utils.create_variables(network, [environment_spec.observations])\n    tf2_utils.create_variables(target_network, [environment_spec.observations])\n\n    # Create the actor which defines how we take actions.\n    actor = actors.FeedForwardActor(policy_network, adder)\n\n    # The learner updates the parameters (and initializes them).\n    learner = learning.DQNLearner(\n        network=network,\n        target_network=target_network,\n        discount=discount,\n        importance_sampling_exponent=importance_sampling_exponent,\n        learning_rate=learning_rate,\n        target_update_period=target_update_period,\n        dataset=dataset,\n        replay_client=replay_client,\n        max_gradient_norm=max_gradient_norm,\n        logger=logger,\n        checkpoint=checkpoint,\n        save_directory=checkpoint_subpath)\n\n    if checkpoint:\n      self._checkpointer = tf2_savers.Checkpointer(\n          directory=checkpoint_subpath,\n          objects_to_save=learner.state,\n          subdirectory='dqn_learner',\n          time_delta_minutes=60.)\n    else:\n      self._checkpointer = None\n\n    super().__init__(\n        actor=actor,\n        learner=learner,\n        min_observations=max(batch_size, min_replay_size),\n        observations_per_step=float(batch_size) / samples_per_insert)",
  "def update(self):\n    super().update()\n    if self._checkpointer is not None:\n      self._checkpointer.save()",
  "class DQNLearner(acme.Learner, tf2_savers.TFSaveable):\n  \"\"\"DQN learner.\n\n  This is the learning component of a DQN agent. It takes a dataset as input\n  and implements update functionality to learn from this dataset. Optionally\n  it takes a replay client as well to allow for updating of priorities.\n  \"\"\"\n\n  def __init__(\n      self,\n      network: snt.Module,\n      target_network: snt.Module,\n      discount: float,\n      importance_sampling_exponent: float,\n      learning_rate: float,\n      target_update_period: int,\n      dataset: tf.data.Dataset,\n      max_abs_reward: Optional[float] = 1.,\n      huber_loss_parameter: float = 1.,\n      replay_client: Optional[Union[reverb.Client, reverb.TFClient]] = None,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = True,\n      save_directory: str = '~/acme',\n      max_gradient_norm: Optional[float] = None,\n  ):\n    \"\"\"Initializes the learner.\n\n    Args:\n      network: the online Q network (the one being optimized)\n      target_network: the target Q critic (which lags behind the online net).\n      discount: discount to use for TD updates.\n      importance_sampling_exponent: power to which importance weights are raised\n        before normalizing.\n      learning_rate: learning rate for the q-network update.\n      target_update_period: number of learner steps to perform before updating\n        the target networks.\n      dataset: dataset to learn from, whether fixed or from a replay buffer (see\n        `acme.datasets.reverb.make_reverb_dataset` documentation).\n      max_abs_reward: Optional maximum absolute value for the reward.\n      huber_loss_parameter: Quadratic-linear boundary for Huber loss.\n      replay_client: client to replay to allow for updating priorities.\n      counter: Counter object for (potentially distributed) counting.\n      logger: Logger object for writing logs to.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n      save_directory: string indicating where the learner should save\n        checkpoints and snapshots.\n      max_gradient_norm: used for gradient clipping.\n    \"\"\"\n\n    # TODO(mwhoffman): stop allowing replay_client to be passed as a TFClient.\n    # This is just here for backwards compatability for agents which reuse this\n    # Learner and still pass a TFClient instance.\n    if isinstance(replay_client, reverb.TFClient):\n      # TODO(b/170419518): open source pytype does not understand this\n      # isinstance() check because it does not have a way of getting precise\n      # type information for pip-installed packages.\n      replay_client = reverb.Client(replay_client._server_address)  # pytype: disable=attribute-error\n\n    # Internalise agent components (replay buffer, networks, optimizer).\n    # TODO(b/155086959): Fix type stubs and remove.\n    self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n    self._network = network\n    self._target_network = target_network\n    self._optimizer = snt.optimizers.Adam(learning_rate)\n    self._replay_client = replay_client\n\n    # Make sure to initialize the optimizer so that its variables (e.g. the Adam\n    # moments) are included in the state returned by the learner (which can then\n    # be checkpointed and restored).\n    self._optimizer._initialize(network.trainable_variables)  # pylint: disable= protected-access\n\n    # Internalise the hyperparameters.\n    self._discount = discount\n    self._target_update_period = target_update_period\n    self._importance_sampling_exponent = importance_sampling_exponent\n    self._max_abs_reward = max_abs_reward\n    self._huber_loss_parameter = huber_loss_parameter\n    if max_gradient_norm is None:\n      max_gradient_norm = 1e10  # A very large number. Infinity results in NaNs.\n    self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n\n    # Learner state.\n    self._variables: List[List[tf.Tensor]] = [network.trainable_variables]\n    self._num_steps = tf.Variable(0, dtype=tf.int32)\n\n    # Internalise logging/counting objects.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.TerminalLogger('learner', time_delta=1.)\n\n    # Create a snapshotter object.\n    if checkpoint:\n      self._snapshotter = tf2_savers.Snapshotter(\n          objects_to_save={'network': network},\n          directory=save_directory,\n          time_delta_minutes=60.)\n    else:\n      self._snapshotter = None\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None\n\n  @tf.function\n  def _step(self) -> Dict[str, tf.Tensor]:\n    \"\"\"Do a step of SGD and update the priorities.\"\"\"\n\n    # Pull out the data needed for updates/priorities.\n    inputs = next(self._iterator)\n    transitions: types.Transition = inputs.data\n    keys, probs = inputs.info[:2]\n\n    with tf.GradientTape() as tape:\n      # Evaluate our networks.\n      q_tm1 = self._network(transitions.observation)\n      q_t_value = self._target_network(transitions.next_observation)\n      q_t_selector = self._network(transitions.next_observation)\n\n      # The rewards and discounts have to have the same type as network values.\n      r_t = tf.cast(transitions.reward, q_tm1.dtype)\n      if self._max_abs_reward:\n        r_t = tf.clip_by_value(r_t, -self._max_abs_reward, self._max_abs_reward)\n      d_t = tf.cast(transitions.discount, q_tm1.dtype) * tf.cast(\n          self._discount, q_tm1.dtype)\n\n      # Compute the loss.\n      _, extra = trfl.double_qlearning(q_tm1, transitions.action, r_t, d_t,\n                                       q_t_value, q_t_selector)\n      loss = losses.huber(extra.td_error, self._huber_loss_parameter)\n\n      # Get the importance weights.\n      importance_weights = 1. / probs  # [B]\n      importance_weights **= self._importance_sampling_exponent\n      importance_weights /= tf.reduce_max(importance_weights)\n\n      # Reweight.\n      loss *= tf.cast(importance_weights, loss.dtype)  # [B]\n      loss = tf.reduce_mean(loss, axis=[0])  # []\n\n    # Do a step of SGD.\n    gradients = tape.gradient(loss, self._network.trainable_variables)\n    gradients, _ = tf.clip_by_global_norm(gradients, self._max_gradient_norm)\n    self._optimizer.apply(gradients, self._network.trainable_variables)\n\n    # Get the priorities that we'll use to update.\n    priorities = tf.abs(extra.td_error)\n\n    # Periodically update the target network.\n    if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n      for src, dest in zip(self._network.variables,\n                           self._target_network.variables):\n        dest.assign(src)\n    self._num_steps.assign_add(1)\n\n    # Report loss & statistics for logging.\n    fetches = {\n        'loss': loss,\n        'keys': keys,\n        'priorities': priorities,\n    }\n\n    return fetches\n\n  def step(self):\n    # Do a batch of SGD.\n    result = self._step()\n\n    # Get the keys and priorities.\n    keys = result.pop('keys')\n    priorities = result.pop('priorities')\n\n    # Update the priorities in the replay buffer.\n    if self._replay_client:\n      self._replay_client.mutate_priorities(\n          table=adders.DEFAULT_PRIORITY_TABLE,\n          updates=dict(zip(keys.numpy(), priorities.numpy())))\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n    result.update(counts)\n\n    # Snapshot and attempt to write logs.\n    if self._snapshotter is not None:\n      self._snapshotter.save()\n    self._logger.write(result)\n\n  def get_variables(self, names: List[str]) -> List[np.ndarray]:\n    return tf2_utils.to_numpy(self._variables)\n\n  @property\n  def state(self):\n    \"\"\"Returns the stateful parts of the learner for checkpointing.\"\"\"\n    return {\n        'network': self._network,\n        'target_network': self._target_network,\n        'optimizer': self._optimizer,\n        'num_steps': self._num_steps\n    }",
  "def __init__(\n      self,\n      network: snt.Module,\n      target_network: snt.Module,\n      discount: float,\n      importance_sampling_exponent: float,\n      learning_rate: float,\n      target_update_period: int,\n      dataset: tf.data.Dataset,\n      max_abs_reward: Optional[float] = 1.,\n      huber_loss_parameter: float = 1.,\n      replay_client: Optional[Union[reverb.Client, reverb.TFClient]] = None,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = True,\n      save_directory: str = '~/acme',\n      max_gradient_norm: Optional[float] = None,\n  ):\n    \"\"\"Initializes the learner.\n\n    Args:\n      network: the online Q network (the one being optimized)\n      target_network: the target Q critic (which lags behind the online net).\n      discount: discount to use for TD updates.\n      importance_sampling_exponent: power to which importance weights are raised\n        before normalizing.\n      learning_rate: learning rate for the q-network update.\n      target_update_period: number of learner steps to perform before updating\n        the target networks.\n      dataset: dataset to learn from, whether fixed or from a replay buffer (see\n        `acme.datasets.reverb.make_reverb_dataset` documentation).\n      max_abs_reward: Optional maximum absolute value for the reward.\n      huber_loss_parameter: Quadratic-linear boundary for Huber loss.\n      replay_client: client to replay to allow for updating priorities.\n      counter: Counter object for (potentially distributed) counting.\n      logger: Logger object for writing logs to.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n      save_directory: string indicating where the learner should save\n        checkpoints and snapshots.\n      max_gradient_norm: used for gradient clipping.\n    \"\"\"\n\n    # TODO(mwhoffman): stop allowing replay_client to be passed as a TFClient.\n    # This is just here for backwards compatability for agents which reuse this\n    # Learner and still pass a TFClient instance.\n    if isinstance(replay_client, reverb.TFClient):\n      # TODO(b/170419518): open source pytype does not understand this\n      # isinstance() check because it does not have a way of getting precise\n      # type information for pip-installed packages.\n      replay_client = reverb.Client(replay_client._server_address)  # pytype: disable=attribute-error\n\n    # Internalise agent components (replay buffer, networks, optimizer).\n    # TODO(b/155086959): Fix type stubs and remove.\n    self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n    self._network = network\n    self._target_network = target_network\n    self._optimizer = snt.optimizers.Adam(learning_rate)\n    self._replay_client = replay_client\n\n    # Make sure to initialize the optimizer so that its variables (e.g. the Adam\n    # moments) are included in the state returned by the learner (which can then\n    # be checkpointed and restored).\n    self._optimizer._initialize(network.trainable_variables)  # pylint: disable= protected-access\n\n    # Internalise the hyperparameters.\n    self._discount = discount\n    self._target_update_period = target_update_period\n    self._importance_sampling_exponent = importance_sampling_exponent\n    self._max_abs_reward = max_abs_reward\n    self._huber_loss_parameter = huber_loss_parameter\n    if max_gradient_norm is None:\n      max_gradient_norm = 1e10  # A very large number. Infinity results in NaNs.\n    self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n\n    # Learner state.\n    self._variables: List[List[tf.Tensor]] = [network.trainable_variables]\n    self._num_steps = tf.Variable(0, dtype=tf.int32)\n\n    # Internalise logging/counting objects.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.TerminalLogger('learner', time_delta=1.)\n\n    # Create a snapshotter object.\n    if checkpoint:\n      self._snapshotter = tf2_savers.Snapshotter(\n          objects_to_save={'network': network},\n          directory=save_directory,\n          time_delta_minutes=60.)\n    else:\n      self._snapshotter = None\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None",
  "def _step(self) -> Dict[str, tf.Tensor]:\n    \"\"\"Do a step of SGD and update the priorities.\"\"\"\n\n    # Pull out the data needed for updates/priorities.\n    inputs = next(self._iterator)\n    transitions: types.Transition = inputs.data\n    keys, probs = inputs.info[:2]\n\n    with tf.GradientTape() as tape:\n      # Evaluate our networks.\n      q_tm1 = self._network(transitions.observation)\n      q_t_value = self._target_network(transitions.next_observation)\n      q_t_selector = self._network(transitions.next_observation)\n\n      # The rewards and discounts have to have the same type as network values.\n      r_t = tf.cast(transitions.reward, q_tm1.dtype)\n      if self._max_abs_reward:\n        r_t = tf.clip_by_value(r_t, -self._max_abs_reward, self._max_abs_reward)\n      d_t = tf.cast(transitions.discount, q_tm1.dtype) * tf.cast(\n          self._discount, q_tm1.dtype)\n\n      # Compute the loss.\n      _, extra = trfl.double_qlearning(q_tm1, transitions.action, r_t, d_t,\n                                       q_t_value, q_t_selector)\n      loss = losses.huber(extra.td_error, self._huber_loss_parameter)\n\n      # Get the importance weights.\n      importance_weights = 1. / probs  # [B]\n      importance_weights **= self._importance_sampling_exponent\n      importance_weights /= tf.reduce_max(importance_weights)\n\n      # Reweight.\n      loss *= tf.cast(importance_weights, loss.dtype)  # [B]\n      loss = tf.reduce_mean(loss, axis=[0])  # []\n\n    # Do a step of SGD.\n    gradients = tape.gradient(loss, self._network.trainable_variables)\n    gradients, _ = tf.clip_by_global_norm(gradients, self._max_gradient_norm)\n    self._optimizer.apply(gradients, self._network.trainable_variables)\n\n    # Get the priorities that we'll use to update.\n    priorities = tf.abs(extra.td_error)\n\n    # Periodically update the target network.\n    if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n      for src, dest in zip(self._network.variables,\n                           self._target_network.variables):\n        dest.assign(src)\n    self._num_steps.assign_add(1)\n\n    # Report loss & statistics for logging.\n    fetches = {\n        'loss': loss,\n        'keys': keys,\n        'priorities': priorities,\n    }\n\n    return fetches",
  "def step(self):\n    # Do a batch of SGD.\n    result = self._step()\n\n    # Get the keys and priorities.\n    keys = result.pop('keys')\n    priorities = result.pop('priorities')\n\n    # Update the priorities in the replay buffer.\n    if self._replay_client:\n      self._replay_client.mutate_priorities(\n          table=adders.DEFAULT_PRIORITY_TABLE,\n          updates=dict(zip(keys.numpy(), priorities.numpy())))\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n    result.update(counts)\n\n    # Snapshot and attempt to write logs.\n    if self._snapshotter is not None:\n      self._snapshotter.save()\n    self._logger.write(result)",
  "def get_variables(self, names: List[str]) -> List[np.ndarray]:\n    return tf2_utils.to_numpy(self._variables)",
  "def state(self):\n    \"\"\"Returns the stateful parts of the learner for checkpointing.\"\"\"\n    return {\n        'network': self._network,\n        'target_network': self._target_network,\n        'optimizer': self._optimizer,\n        'num_steps': self._num_steps\n    }",
  "class DistributedAgentTest(absltest.TestCase):\n  \"\"\"Simple integration/smoke test for the distributed agent.\"\"\"\n\n  def test_atari(self):\n    \"\"\"Tests that the agent can run for some steps without crashing.\"\"\"\n    env_factory = lambda x: fakes.fake_atari_wrapped()\n    net_factory = lambda spec: networks.DQNAtariNetwork(spec.num_values)\n\n    agent = dqn.DistributedDQN(\n        environment_factory=env_factory,\n        network_factory=net_factory,\n        num_actors=2,\n        batch_size=32,\n        min_replay_size=32,\n        max_replay_size=1000,\n    )\n    program = agent.build()\n\n    (learner_node,) = program.groups['learner']\n    learner_node.disable_run()\n\n    lp.launch(program, launch_type='test_mt')\n\n    learner: acme.Learner = learner_node.create_handle().dereference()\n\n    for _ in range(5):\n      learner.step()",
  "def test_atari(self):\n    \"\"\"Tests that the agent can run for some steps without crashing.\"\"\"\n    env_factory = lambda x: fakes.fake_atari_wrapped()\n    net_factory = lambda spec: networks.DQNAtariNetwork(spec.num_values)\n\n    agent = dqn.DistributedDQN(\n        environment_factory=env_factory,\n        network_factory=net_factory,\n        num_actors=2,\n        batch_size=32,\n        min_replay_size=32,\n        max_replay_size=1000,\n    )\n    program = agent.build()\n\n    (learner_node,) = program.groups['learner']\n    learner_node.disable_run()\n\n    lp.launch(program, launch_type='test_mt')\n\n    learner: acme.Learner = learner_node.create_handle().dereference()\n\n    for _ in range(5):\n      learner.step()",
  "def _make_network(action_spec: specs.DiscreteArray) -> snt.Module:\n  return snt.Sequential([\n      snt.Flatten(),\n      snt.nets.MLP([50, 50, action_spec.num_values]),\n  ])",
  "class DQNTest(absltest.TestCase):\n\n  def test_dqn(self):\n    # Create a fake environment to test with.\n    environment = fakes.DiscreteEnvironment(\n        num_actions=5,\n        num_observations=10,\n        obs_dtype=np.float32,\n        episode_length=10)\n    spec = specs.make_environment_spec(environment)\n\n    # Construct the agent.\n    agent = dqn.DQN(\n        environment_spec=spec,\n        network=_make_network(spec.actions),\n        batch_size=10,\n        samples_per_insert=2,\n        min_replay_size=10)\n\n    # Try running the environment loop. We have no assertions here because all\n    # we care about is that the agent runs without raising any errors.\n    loop = acme.EnvironmentLoop(environment, agent)\n    loop.run(num_episodes=2)",
  "def test_dqn(self):\n    # Create a fake environment to test with.\n    environment = fakes.DiscreteEnvironment(\n        num_actions=5,\n        num_observations=10,\n        obs_dtype=np.float32,\n        episode_length=10)\n    spec = specs.make_environment_spec(environment)\n\n    # Construct the agent.\n    agent = dqn.DQN(\n        environment_spec=spec,\n        network=_make_network(spec.actions),\n        batch_size=10,\n        samples_per_insert=2,\n        min_replay_size=10)\n\n    # Try running the environment loop. We have no assertions here because all\n    # we care about is that the agent runs without raising any errors.\n    loop = acme.EnvironmentLoop(environment, agent)\n    loop.run(num_episodes=2)",
  "class DistributedDQN:\n  \"\"\"Distributed DQN agent.\"\"\"\n\n  def __init__(\n      self,\n      environment_factory: Callable[[bool], dm_env.Environment],\n      network_factory: Callable[[specs.DiscreteArray], snt.Module],\n      num_actors: int,\n      num_caches: int = 1,\n      batch_size: int = 256,\n      prefetch_size: int = 4,\n      target_update_period: int = 100,\n      samples_per_insert: float = 32.0,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1_000_000,\n      importance_sampling_exponent: float = 0.2,\n      priority_exponent: float = 0.6,\n      n_step: int = 5,\n      learning_rate: float = 1e-3,\n      evaluator_epsilon: float = 0.,\n      max_actor_steps: Optional[int] = None,\n      discount: float = 0.99,\n      environment_spec: Optional[specs.EnvironmentSpec] = None,\n      variable_update_period: int = 1000,\n  ):\n\n    assert num_caches >= 1\n\n    if environment_spec is None:\n      environment_spec = specs.make_environment_spec(environment_factory(False))\n\n    self._environment_factory = environment_factory\n    self._network_factory = network_factory\n    self._num_actors = num_actors\n    self._num_caches = num_caches\n    self._env_spec = environment_spec\n    self._batch_size = batch_size\n    self._prefetch_size = prefetch_size\n    self._target_update_period = target_update_period\n    self._samples_per_insert = samples_per_insert\n    self._min_replay_size = min_replay_size\n    self._max_replay_size = max_replay_size\n    self._importance_sampling_exponent = importance_sampling_exponent\n    self._priority_exponent = priority_exponent\n    self._n_step = n_step\n    self._learning_rate = learning_rate\n    self._evaluator_epsilon = evaluator_epsilon\n    self._max_actor_steps = max_actor_steps\n    self._discount = discount\n    self._variable_update_period = variable_update_period\n\n  def replay(self):\n    \"\"\"The replay storage.\"\"\"\n    if self._samples_per_insert:\n      limiter = reverb.rate_limiters.SampleToInsertRatio(\n          min_size_to_sample=self._min_replay_size,\n          samples_per_insert=self._samples_per_insert,\n          error_buffer=self._batch_size)\n    else:\n      limiter = reverb.rate_limiters.MinSize(self._min_replay_size)\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Prioritized(self._priority_exponent),\n        remover=reverb.selectors.Fifo(),\n        max_size=self._max_replay_size,\n        rate_limiter=limiter,\n        signature=adders.NStepTransitionAdder.signature(self._env_spec))\n    return [replay_table]\n\n  def counter(self):\n    \"\"\"Creates the master counter process.\"\"\"\n    return tf2_savers.CheckpointingRunner(\n        counting.Counter(), time_delta_minutes=1, subdirectory='counter')\n\n  def coordinator(self, counter: counting.Counter, max_actor_steps: int):\n    return lp_utils.StepsLimiter(counter, max_actor_steps)\n\n  def learner(self, replay: reverb.Client, counter: counting.Counter):\n    \"\"\"The Learning part of the agent.\"\"\"\n\n    # Create the networks.\n    network = self._network_factory(self._env_spec.actions)\n    target_network = copy.deepcopy(network)\n\n    tf2_utils.create_variables(network, [self._env_spec.observations])\n    tf2_utils.create_variables(target_network, [self._env_spec.observations])\n\n    # The dataset object to learn from.\n    replay_client = reverb.Client(replay.server_address)\n    dataset = datasets.make_reverb_dataset(\n        server_address=replay.server_address,\n        batch_size=self._batch_size,\n        prefetch_size=self._prefetch_size)\n\n    logger = loggers.make_default_logger('learner', steps_key='learner_steps')\n\n    # Return the learning agent.\n    counter = counting.Counter(counter, 'learner')\n\n    learner = learning.DQNLearner(\n        network=network,\n        target_network=target_network,\n        discount=self._discount,\n        importance_sampling_exponent=self._importance_sampling_exponent,\n        learning_rate=self._learning_rate,\n        target_update_period=self._target_update_period,\n        dataset=dataset,\n        replay_client=replay_client,\n        counter=counter,\n        logger=logger)\n    return tf2_savers.CheckpointingRunner(\n        learner, subdirectory='dqn_learner', time_delta_minutes=60)\n\n  def actor(\n      self,\n      replay: reverb.Client,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n      epsilon: float,\n  ) -> acme.EnvironmentLoop:\n    \"\"\"The actor process.\"\"\"\n    environment = self._environment_factory(False)\n    network = self._network_factory(self._env_spec.actions)\n\n    # Just inline the policy network here.\n    policy_network = snt.Sequential([\n        network,\n        lambda q: trfl.epsilon_greedy(q, epsilon=epsilon).sample(),\n    ])\n\n    tf2_utils.create_variables(policy_network, [self._env_spec.observations])\n    variable_client = tf2_variable_utils.VariableClient(\n        client=variable_source,\n        variables={'policy': policy_network.trainable_variables},\n        update_period=self._variable_update_period)\n\n    # Make sure not to use a random policy after checkpoint restoration by\n    # assigning variables before running the environment loop.\n    variable_client.update_and_wait()\n\n    # Component to add things into replay.\n    adder = adders.NStepTransitionAdder(\n        client=replay,\n        n_step=self._n_step,\n        discount=self._discount,\n    )\n\n    # Create the agent.\n    actor = actors.FeedForwardActor(policy_network, adder, variable_client)\n\n    # Create the loop to connect environment and agent.\n    counter = counting.Counter(counter, 'actor')\n    logger = loggers.make_default_logger(\n        'actor', save_data=False, steps_key='actor_steps')\n    return acme.EnvironmentLoop(environment, actor, counter, logger)\n\n  def evaluator(\n      self,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ):\n    \"\"\"The evaluation process.\"\"\"\n    environment = self._environment_factory(True)\n    network = self._network_factory(self._env_spec.actions)\n\n    # Just inline the policy network here.\n    policy_network = snt.Sequential([\n        network,\n        lambda q: trfl.epsilon_greedy(q, self._evaluator_epsilon).sample(),\n    ])\n\n    tf2_utils.create_variables(policy_network, [self._env_spec.observations])\n\n    variable_client = tf2_variable_utils.VariableClient(\n        client=variable_source,\n        variables={'policy': policy_network.trainable_variables},\n        update_period=self._variable_update_period)\n\n    # Make sure not to use a random policy after checkpoint restoration by\n    # assigning variables before running the environment loop.\n    variable_client.update_and_wait()\n\n    # Create the agent.\n    actor = actors.FeedForwardActor(\n        policy_network, variable_client=variable_client)\n\n    # Create the run loop and return it.\n    logger = loggers.make_default_logger(\n        'evaluator', steps_key='evaluator_steps')\n    counter = counting.Counter(counter, 'evaluator')\n    return acme.EnvironmentLoop(\n        environment, actor, counter=counter, logger=logger)\n\n  def build(self, name='dqn'):\n    \"\"\"Build the distributed agent topology.\"\"\"\n    program = lp.Program(name=name)\n\n    with program.group('replay'):\n      replay = program.add_node(lp.ReverbNode(self.replay))\n\n    with program.group('counter'):\n      counter = program.add_node(lp.CourierNode(self.counter))\n\n      if self._max_actor_steps:\n        program.add_node(\n            lp.CourierNode(self.coordinator, counter, self._max_actor_steps))\n\n    with program.group('learner'):\n      learner = program.add_node(lp.CourierNode(self.learner, replay, counter))\n\n    with program.group('evaluator'):\n      program.add_node(lp.CourierNode(self.evaluator, learner, counter))\n\n    # Generate an epsilon for each actor.\n    epsilons = np.flip(np.logspace(1, 8, self._num_actors, base=0.4), axis=0)\n\n    with program.group('cacher'):\n      # Create a set of learner caches.\n      sources = []\n      for _ in range(self._num_caches):\n        cacher = program.add_node(\n            lp.CacherNode(\n                learner, refresh_interval_ms=2000, stale_after_ms=4000))\n        sources.append(cacher)\n\n    with program.group('actor'):\n      # Add actors which pull round-robin from our variable sources.\n      for actor_id, epsilon in enumerate(epsilons):\n        source = sources[actor_id % len(sources)]\n        program.add_node(\n            lp.CourierNode(self.actor, replay, source, counter, epsilon))\n\n    return program",
  "def __init__(\n      self,\n      environment_factory: Callable[[bool], dm_env.Environment],\n      network_factory: Callable[[specs.DiscreteArray], snt.Module],\n      num_actors: int,\n      num_caches: int = 1,\n      batch_size: int = 256,\n      prefetch_size: int = 4,\n      target_update_period: int = 100,\n      samples_per_insert: float = 32.0,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1_000_000,\n      importance_sampling_exponent: float = 0.2,\n      priority_exponent: float = 0.6,\n      n_step: int = 5,\n      learning_rate: float = 1e-3,\n      evaluator_epsilon: float = 0.,\n      max_actor_steps: Optional[int] = None,\n      discount: float = 0.99,\n      environment_spec: Optional[specs.EnvironmentSpec] = None,\n      variable_update_period: int = 1000,\n  ):\n\n    assert num_caches >= 1\n\n    if environment_spec is None:\n      environment_spec = specs.make_environment_spec(environment_factory(False))\n\n    self._environment_factory = environment_factory\n    self._network_factory = network_factory\n    self._num_actors = num_actors\n    self._num_caches = num_caches\n    self._env_spec = environment_spec\n    self._batch_size = batch_size\n    self._prefetch_size = prefetch_size\n    self._target_update_period = target_update_period\n    self._samples_per_insert = samples_per_insert\n    self._min_replay_size = min_replay_size\n    self._max_replay_size = max_replay_size\n    self._importance_sampling_exponent = importance_sampling_exponent\n    self._priority_exponent = priority_exponent\n    self._n_step = n_step\n    self._learning_rate = learning_rate\n    self._evaluator_epsilon = evaluator_epsilon\n    self._max_actor_steps = max_actor_steps\n    self._discount = discount\n    self._variable_update_period = variable_update_period",
  "def replay(self):\n    \"\"\"The replay storage.\"\"\"\n    if self._samples_per_insert:\n      limiter = reverb.rate_limiters.SampleToInsertRatio(\n          min_size_to_sample=self._min_replay_size,\n          samples_per_insert=self._samples_per_insert,\n          error_buffer=self._batch_size)\n    else:\n      limiter = reverb.rate_limiters.MinSize(self._min_replay_size)\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Prioritized(self._priority_exponent),\n        remover=reverb.selectors.Fifo(),\n        max_size=self._max_replay_size,\n        rate_limiter=limiter,\n        signature=adders.NStepTransitionAdder.signature(self._env_spec))\n    return [replay_table]",
  "def counter(self):\n    \"\"\"Creates the master counter process.\"\"\"\n    return tf2_savers.CheckpointingRunner(\n        counting.Counter(), time_delta_minutes=1, subdirectory='counter')",
  "def coordinator(self, counter: counting.Counter, max_actor_steps: int):\n    return lp_utils.StepsLimiter(counter, max_actor_steps)",
  "def learner(self, replay: reverb.Client, counter: counting.Counter):\n    \"\"\"The Learning part of the agent.\"\"\"\n\n    # Create the networks.\n    network = self._network_factory(self._env_spec.actions)\n    target_network = copy.deepcopy(network)\n\n    tf2_utils.create_variables(network, [self._env_spec.observations])\n    tf2_utils.create_variables(target_network, [self._env_spec.observations])\n\n    # The dataset object to learn from.\n    replay_client = reverb.Client(replay.server_address)\n    dataset = datasets.make_reverb_dataset(\n        server_address=replay.server_address,\n        batch_size=self._batch_size,\n        prefetch_size=self._prefetch_size)\n\n    logger = loggers.make_default_logger('learner', steps_key='learner_steps')\n\n    # Return the learning agent.\n    counter = counting.Counter(counter, 'learner')\n\n    learner = learning.DQNLearner(\n        network=network,\n        target_network=target_network,\n        discount=self._discount,\n        importance_sampling_exponent=self._importance_sampling_exponent,\n        learning_rate=self._learning_rate,\n        target_update_period=self._target_update_period,\n        dataset=dataset,\n        replay_client=replay_client,\n        counter=counter,\n        logger=logger)\n    return tf2_savers.CheckpointingRunner(\n        learner, subdirectory='dqn_learner', time_delta_minutes=60)",
  "def actor(\n      self,\n      replay: reverb.Client,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n      epsilon: float,\n  ) -> acme.EnvironmentLoop:\n    \"\"\"The actor process.\"\"\"\n    environment = self._environment_factory(False)\n    network = self._network_factory(self._env_spec.actions)\n\n    # Just inline the policy network here.\n    policy_network = snt.Sequential([\n        network,\n        lambda q: trfl.epsilon_greedy(q, epsilon=epsilon).sample(),\n    ])\n\n    tf2_utils.create_variables(policy_network, [self._env_spec.observations])\n    variable_client = tf2_variable_utils.VariableClient(\n        client=variable_source,\n        variables={'policy': policy_network.trainable_variables},\n        update_period=self._variable_update_period)\n\n    # Make sure not to use a random policy after checkpoint restoration by\n    # assigning variables before running the environment loop.\n    variable_client.update_and_wait()\n\n    # Component to add things into replay.\n    adder = adders.NStepTransitionAdder(\n        client=replay,\n        n_step=self._n_step,\n        discount=self._discount,\n    )\n\n    # Create the agent.\n    actor = actors.FeedForwardActor(policy_network, adder, variable_client)\n\n    # Create the loop to connect environment and agent.\n    counter = counting.Counter(counter, 'actor')\n    logger = loggers.make_default_logger(\n        'actor', save_data=False, steps_key='actor_steps')\n    return acme.EnvironmentLoop(environment, actor, counter, logger)",
  "def evaluator(\n      self,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ):\n    \"\"\"The evaluation process.\"\"\"\n    environment = self._environment_factory(True)\n    network = self._network_factory(self._env_spec.actions)\n\n    # Just inline the policy network here.\n    policy_network = snt.Sequential([\n        network,\n        lambda q: trfl.epsilon_greedy(q, self._evaluator_epsilon).sample(),\n    ])\n\n    tf2_utils.create_variables(policy_network, [self._env_spec.observations])\n\n    variable_client = tf2_variable_utils.VariableClient(\n        client=variable_source,\n        variables={'policy': policy_network.trainable_variables},\n        update_period=self._variable_update_period)\n\n    # Make sure not to use a random policy after checkpoint restoration by\n    # assigning variables before running the environment loop.\n    variable_client.update_and_wait()\n\n    # Create the agent.\n    actor = actors.FeedForwardActor(\n        policy_network, variable_client=variable_client)\n\n    # Create the run loop and return it.\n    logger = loggers.make_default_logger(\n        'evaluator', steps_key='evaluator_steps')\n    counter = counting.Counter(counter, 'evaluator')\n    return acme.EnvironmentLoop(\n        environment, actor, counter=counter, logger=logger)",
  "def build(self, name='dqn'):\n    \"\"\"Build the distributed agent topology.\"\"\"\n    program = lp.Program(name=name)\n\n    with program.group('replay'):\n      replay = program.add_node(lp.ReverbNode(self.replay))\n\n    with program.group('counter'):\n      counter = program.add_node(lp.CourierNode(self.counter))\n\n      if self._max_actor_steps:\n        program.add_node(\n            lp.CourierNode(self.coordinator, counter, self._max_actor_steps))\n\n    with program.group('learner'):\n      learner = program.add_node(lp.CourierNode(self.learner, replay, counter))\n\n    with program.group('evaluator'):\n      program.add_node(lp.CourierNode(self.evaluator, learner, counter))\n\n    # Generate an epsilon for each actor.\n    epsilons = np.flip(np.logspace(1, 8, self._num_actors, base=0.4), axis=0)\n\n    with program.group('cacher'):\n      # Create a set of learner caches.\n      sources = []\n      for _ in range(self._num_caches):\n        cacher = program.add_node(\n            lp.CacherNode(\n                learner, refresh_interval_ms=2000, stale_after_ms=4000))\n        sources.append(cacher)\n\n    with program.group('actor'):\n      # Add actors which pull round-robin from our variable sources.\n      for actor_id, epsilon in enumerate(epsilons):\n        source = sources[actor_id % len(sources)]\n        program.add_node(\n            lp.CourierNode(self.actor, replay, source, counter, epsilon))\n\n    return program",
  "def _make_network(action_spec: specs.DiscreteArray) -> snt.Module:\n  return snt.Sequential([\n      snt.Flatten(),\n      snt.nets.MLP([50, 50, action_spec.num_values]),\n  ])",
  "class DiscreteBCQLearnerTest(absltest.TestCase):\n\n  def test_full_learner(self):\n    # Create dataset.\n    environment = fakes.DiscreteEnvironment(\n        num_actions=5,\n        num_observations=10,\n        obs_dtype=np.float32,\n        episode_length=10)\n    spec = specs.make_environment_spec(environment)\n    dataset = fakes.transition_dataset(environment).batch(2)\n\n    # Build network.\n    g_network = _make_network(spec.actions)\n    q_network = _make_network(spec.actions)\n    network = discrete_networks.DiscreteFilteredQNetwork(g_network=g_network,\n                                                         q_network=q_network,\n                                                         threshold=0.5)\n    tf2_utils.create_variables(network, [spec.observations])\n\n    # Build learner.\n    counter = counting.Counter()\n    learner = bcq.DiscreteBCQLearner(\n        network=network,\n        dataset=dataset,\n        learning_rate=1e-4,\n        discount=0.99,\n        importance_sampling_exponent=0.2,\n        target_update_period=100,\n        counter=counter)\n\n    # Run a learner step.\n    learner.step()\n\n    # Check counts from BC and BCQ learners.\n    counts = counter.get_counts()\n    self.assertEqual(1, counts['bc_steps'])\n    self.assertEqual(1, counts['bcq_steps'])\n\n    # Check learner state.\n    self.assertEqual(1, learner.state['bc_num_steps'].numpy())\n    self.assertEqual(1, learner.state['bcq_num_steps'].numpy())",
  "def test_full_learner(self):\n    # Create dataset.\n    environment = fakes.DiscreteEnvironment(\n        num_actions=5,\n        num_observations=10,\n        obs_dtype=np.float32,\n        episode_length=10)\n    spec = specs.make_environment_spec(environment)\n    dataset = fakes.transition_dataset(environment).batch(2)\n\n    # Build network.\n    g_network = _make_network(spec.actions)\n    q_network = _make_network(spec.actions)\n    network = discrete_networks.DiscreteFilteredQNetwork(g_network=g_network,\n                                                         q_network=q_network,\n                                                         threshold=0.5)\n    tf2_utils.create_variables(network, [spec.observations])\n\n    # Build learner.\n    counter = counting.Counter()\n    learner = bcq.DiscreteBCQLearner(\n        network=network,\n        dataset=dataset,\n        learning_rate=1e-4,\n        discount=0.99,\n        importance_sampling_exponent=0.2,\n        target_update_period=100,\n        counter=counter)\n\n    # Run a learner step.\n    learner.step()\n\n    # Check counts from BC and BCQ learners.\n    counts = counter.get_counts()\n    self.assertEqual(1, counts['bc_steps'])\n    self.assertEqual(1, counts['bcq_steps'])\n\n    # Check learner state.\n    self.assertEqual(1, learner.state['bc_num_steps'].numpy())\n    self.assertEqual(1, learner.state['bcq_num_steps'].numpy())",
  "class _InternalBCQLearner(core.Learner, tf2_savers.TFSaveable):\n  \"\"\"Internal BCQ learner.\n\n  This implements the Q-learning component in the discrete BCQ algorithm.\n  \"\"\"\n\n  def __init__(\n      self,\n      network: discrete_networks.DiscreteFilteredQNetwork,\n      discount: float,\n      importance_sampling_exponent: float,\n      learning_rate: float,\n      target_update_period: int,\n      dataset: tf.data.Dataset,\n      huber_loss_parameter: float = 1.,\n      replay_client: Optional[reverb.TFClient] = None,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = False,\n  ):\n    \"\"\"Initializes the learner.\n\n    Args:\n      network: BCQ network\n      discount: discount to use for TD updates.\n      importance_sampling_exponent: power to which importance weights are raised\n        before normalizing.\n      learning_rate: learning rate for the q-network update.\n      target_update_period: number of learner steps to perform before updating\n        the target networks.\n      dataset: dataset to learn from, whether fixed or from a replay buffer (see\n        `acme.datasets.reverb.make_reverb_dataset` documentation).\n      huber_loss_parameter: Quadratic-linear boundary for Huber loss.\n      replay_client: client to replay to allow for updating priorities.\n      counter: Counter object for (potentially distributed) counting.\n      logger: Logger object for writing logs to.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n    \"\"\"\n\n    # Internalise agent components (replay buffer, networks, optimizer).\n    # TODO(b/155086959): Fix type stubs and remove.\n    self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n    self._network = network\n    self._q_network = network.q_network\n    self._target_q_network = copy.deepcopy(network.q_network)\n    self._optimizer = snt.optimizers.Adam(learning_rate)\n    self._replay_client = replay_client\n\n    # Internalise the hyperparameters.\n    self._discount = discount\n    self._target_update_period = target_update_period\n    self._importance_sampling_exponent = importance_sampling_exponent\n    self._huber_loss_parameter = huber_loss_parameter\n\n    # Learner state.\n    self._variables = [self._network.trainable_variables]\n    self._num_steps = tf.Variable(0, dtype=tf.int32)\n\n    # Internalise logging/counting objects.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger('learner',\n                                                         save_data=False)\n\n    # Create a snapshotter object.\n    if checkpoint:\n      self._snapshotter = tf2_savers.Snapshotter(\n          objects_to_save={'network': network}, time_delta_minutes=60.)\n    else:\n      self._snapshotter = None\n\n  @tf.function\n  def _step(self) -> Dict[str, tf.Tensor]:\n    \"\"\"Do a step of SGD and update the priorities.\"\"\"\n\n    # Pull out the data needed for updates/priorities.\n    inputs = next(self._iterator)\n    transitions: types.Transition = inputs.data\n    keys, probs = inputs.info[:2]\n\n    with tf.GradientTape() as tape:\n      # Evaluate our networks.\n      q_tm1 = self._q_network(transitions.observation)\n      q_t_value = self._target_q_network(transitions.next_observation)\n      q_t_selector = self._network(transitions.next_observation)\n\n      # The rewards and discounts have to have the same type as network values.\n      r_t = tf.cast(transitions.reward, q_tm1.dtype)\n      r_t = tf.clip_by_value(r_t, -1., 1.)\n      d_t = tf.cast(transitions.discount, q_tm1.dtype) * tf.cast(\n          self._discount, q_tm1.dtype)\n\n      # Compute the loss.\n      _, extra = trfl.double_qlearning(q_tm1, transitions.action, r_t, d_t,\n                                       q_t_value, q_t_selector)\n      loss = losses.huber(extra.td_error, self._huber_loss_parameter)\n\n      # Get the importance weights.\n      importance_weights = 1. / probs  # [B]\n      importance_weights **= self._importance_sampling_exponent\n      importance_weights /= tf.reduce_max(importance_weights)\n\n      # Reweight.\n      loss *= tf.cast(importance_weights, loss.dtype)  # [B]\n      loss = tf.reduce_mean(loss, axis=[0])  # []\n\n    # Do a step of SGD.\n    gradients = tape.gradient(loss, self._network.trainable_variables)\n    self._optimizer.apply(gradients, self._network.trainable_variables)\n\n    # Update the priorities in the replay buffer.\n    if self._replay_client:\n      priorities = tf.cast(tf.abs(extra.td_error), tf.float64)\n      self._replay_client.update_priorities(\n          table=adders.DEFAULT_PRIORITY_TABLE, keys=keys, priorities=priorities)\n\n    # Periodically update the target network.\n    if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n      for src, dest in zip(self._q_network.variables,\n                           self._target_q_network.variables):\n        dest.assign(src)\n    self._num_steps.assign_add(1)\n\n    # Compute the global norm of the gradients for logging.\n    global_gradient_norm = tf.linalg.global_norm(gradients)\n\n    # Compute statistics of the Q-values for logging.\n    max_q = tf.reduce_max(q_t_value)\n    min_q = tf.reduce_min(q_t_value)\n    mean_q, var_q = tf.nn.moments(q_t_value, [0, 1])\n\n    # Report loss & statistics for logging.\n    fetches = {\n        'gradient_norm': global_gradient_norm,\n        'loss': loss,\n        'max_q': max_q,\n        'mean_q': mean_q,\n        'min_q': min_q,\n        'var_q': var_q,\n    }\n\n    return fetches\n\n  def step(self):\n    # Do a batch of SGD.\n    result = self._step()\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1)\n    result.update(counts)\n\n    # Snapshot and attempt to write logs.\n    if self._snapshotter is not None:\n      self._snapshotter.save()\n    self._logger.write(result)\n\n  def get_variables(self, names: List[str]) -> List[np.ndarray]:\n    return tf2_utils.to_numpy(self._variables)\n\n  @property\n  def state(self):\n    \"\"\"Returns the stateful parts of the learner for checkpointing.\"\"\"\n    return {\n        'network': self._network,\n        'target_q_network': self._target_q_network,\n        'optimizer': self._optimizer,\n        'num_steps': self._num_steps\n    }",
  "class DiscreteBCQLearner(core.Learner, tf2_savers.TFSaveable):\n  \"\"\"Discrete BCQ learner.\n\n  This learner combines supervised BC learning and Q learning to implement the\n  discrete BCQ algorithm as described in https://arxiv.org/pdf/1910.01708.pdf.\n  \"\"\"\n\n  def __init__(self,\n               network: discrete_networks.DiscreteFilteredQNetwork,\n               dataset: tf.data.Dataset,\n               learning_rate: float,\n               counter: Optional[counting.Counter] = None,\n               bc_logger: Optional[loggers.Logger] = None,\n               bcq_logger: Optional[loggers.Logger] = None,\n               **bcq_learner_kwargs):\n    counter = counter or counting.Counter()\n    self._bc_logger = bc_logger or loggers.TerminalLogger('bc_learner',\n                                                          time_delta=1.)\n    self._bcq_logger = bcq_logger or loggers.TerminalLogger('bcq_learner',\n                                                            time_delta=1.)\n\n    self._bc_learner = bc.BCLearner(\n        network=network.g_network,\n        learning_rate=learning_rate,\n        dataset=dataset,\n        counter=counting.Counter(counter, 'bc'),\n        logger=self._bc_logger,\n        checkpoint=False)\n    self._bcq_learner = _InternalBCQLearner(\n        network=network,\n        learning_rate=learning_rate,\n        dataset=dataset,\n        counter=counting.Counter(counter, 'bcq'),\n        logger=self._bcq_logger,\n        **bcq_learner_kwargs)\n\n  def get_variables(self, names):\n    return self._bcq_learner.get_variables(names)\n\n  @property\n  def state(self):\n    bc_state = self._bc_learner.state\n    bc_state.pop('network')  # No need to checkpoint the BC network.\n    bcq_state = self._bcq_learner.state\n    state = dict()\n    state.update({f'bc_{k}': v for k, v in bc_state.items()})\n    state.update({f'bcq_{k}': v for k, v in bcq_state.items()})\n    return state\n\n  def step(self):\n    self._bc_learner.step()\n    self._bcq_learner.step()",
  "def __init__(\n      self,\n      network: discrete_networks.DiscreteFilteredQNetwork,\n      discount: float,\n      importance_sampling_exponent: float,\n      learning_rate: float,\n      target_update_period: int,\n      dataset: tf.data.Dataset,\n      huber_loss_parameter: float = 1.,\n      replay_client: Optional[reverb.TFClient] = None,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = False,\n  ):\n    \"\"\"Initializes the learner.\n\n    Args:\n      network: BCQ network\n      discount: discount to use for TD updates.\n      importance_sampling_exponent: power to which importance weights are raised\n        before normalizing.\n      learning_rate: learning rate for the q-network update.\n      target_update_period: number of learner steps to perform before updating\n        the target networks.\n      dataset: dataset to learn from, whether fixed or from a replay buffer (see\n        `acme.datasets.reverb.make_reverb_dataset` documentation).\n      huber_loss_parameter: Quadratic-linear boundary for Huber loss.\n      replay_client: client to replay to allow for updating priorities.\n      counter: Counter object for (potentially distributed) counting.\n      logger: Logger object for writing logs to.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n    \"\"\"\n\n    # Internalise agent components (replay buffer, networks, optimizer).\n    # TODO(b/155086959): Fix type stubs and remove.\n    self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n    self._network = network\n    self._q_network = network.q_network\n    self._target_q_network = copy.deepcopy(network.q_network)\n    self._optimizer = snt.optimizers.Adam(learning_rate)\n    self._replay_client = replay_client\n\n    # Internalise the hyperparameters.\n    self._discount = discount\n    self._target_update_period = target_update_period\n    self._importance_sampling_exponent = importance_sampling_exponent\n    self._huber_loss_parameter = huber_loss_parameter\n\n    # Learner state.\n    self._variables = [self._network.trainable_variables]\n    self._num_steps = tf.Variable(0, dtype=tf.int32)\n\n    # Internalise logging/counting objects.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger('learner',\n                                                         save_data=False)\n\n    # Create a snapshotter object.\n    if checkpoint:\n      self._snapshotter = tf2_savers.Snapshotter(\n          objects_to_save={'network': network}, time_delta_minutes=60.)\n    else:\n      self._snapshotter = None",
  "def _step(self) -> Dict[str, tf.Tensor]:\n    \"\"\"Do a step of SGD and update the priorities.\"\"\"\n\n    # Pull out the data needed for updates/priorities.\n    inputs = next(self._iterator)\n    transitions: types.Transition = inputs.data\n    keys, probs = inputs.info[:2]\n\n    with tf.GradientTape() as tape:\n      # Evaluate our networks.\n      q_tm1 = self._q_network(transitions.observation)\n      q_t_value = self._target_q_network(transitions.next_observation)\n      q_t_selector = self._network(transitions.next_observation)\n\n      # The rewards and discounts have to have the same type as network values.\n      r_t = tf.cast(transitions.reward, q_tm1.dtype)\n      r_t = tf.clip_by_value(r_t, -1., 1.)\n      d_t = tf.cast(transitions.discount, q_tm1.dtype) * tf.cast(\n          self._discount, q_tm1.dtype)\n\n      # Compute the loss.\n      _, extra = trfl.double_qlearning(q_tm1, transitions.action, r_t, d_t,\n                                       q_t_value, q_t_selector)\n      loss = losses.huber(extra.td_error, self._huber_loss_parameter)\n\n      # Get the importance weights.\n      importance_weights = 1. / probs  # [B]\n      importance_weights **= self._importance_sampling_exponent\n      importance_weights /= tf.reduce_max(importance_weights)\n\n      # Reweight.\n      loss *= tf.cast(importance_weights, loss.dtype)  # [B]\n      loss = tf.reduce_mean(loss, axis=[0])  # []\n\n    # Do a step of SGD.\n    gradients = tape.gradient(loss, self._network.trainable_variables)\n    self._optimizer.apply(gradients, self._network.trainable_variables)\n\n    # Update the priorities in the replay buffer.\n    if self._replay_client:\n      priorities = tf.cast(tf.abs(extra.td_error), tf.float64)\n      self._replay_client.update_priorities(\n          table=adders.DEFAULT_PRIORITY_TABLE, keys=keys, priorities=priorities)\n\n    # Periodically update the target network.\n    if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n      for src, dest in zip(self._q_network.variables,\n                           self._target_q_network.variables):\n        dest.assign(src)\n    self._num_steps.assign_add(1)\n\n    # Compute the global norm of the gradients for logging.\n    global_gradient_norm = tf.linalg.global_norm(gradients)\n\n    # Compute statistics of the Q-values for logging.\n    max_q = tf.reduce_max(q_t_value)\n    min_q = tf.reduce_min(q_t_value)\n    mean_q, var_q = tf.nn.moments(q_t_value, [0, 1])\n\n    # Report loss & statistics for logging.\n    fetches = {\n        'gradient_norm': global_gradient_norm,\n        'loss': loss,\n        'max_q': max_q,\n        'mean_q': mean_q,\n        'min_q': min_q,\n        'var_q': var_q,\n    }\n\n    return fetches",
  "def step(self):\n    # Do a batch of SGD.\n    result = self._step()\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1)\n    result.update(counts)\n\n    # Snapshot and attempt to write logs.\n    if self._snapshotter is not None:\n      self._snapshotter.save()\n    self._logger.write(result)",
  "def get_variables(self, names: List[str]) -> List[np.ndarray]:\n    return tf2_utils.to_numpy(self._variables)",
  "def state(self):\n    \"\"\"Returns the stateful parts of the learner for checkpointing.\"\"\"\n    return {\n        'network': self._network,\n        'target_q_network': self._target_q_network,\n        'optimizer': self._optimizer,\n        'num_steps': self._num_steps\n    }",
  "def __init__(self,\n               network: discrete_networks.DiscreteFilteredQNetwork,\n               dataset: tf.data.Dataset,\n               learning_rate: float,\n               counter: Optional[counting.Counter] = None,\n               bc_logger: Optional[loggers.Logger] = None,\n               bcq_logger: Optional[loggers.Logger] = None,\n               **bcq_learner_kwargs):\n    counter = counter or counting.Counter()\n    self._bc_logger = bc_logger or loggers.TerminalLogger('bc_learner',\n                                                          time_delta=1.)\n    self._bcq_logger = bcq_logger or loggers.TerminalLogger('bcq_learner',\n                                                            time_delta=1.)\n\n    self._bc_learner = bc.BCLearner(\n        network=network.g_network,\n        learning_rate=learning_rate,\n        dataset=dataset,\n        counter=counting.Counter(counter, 'bc'),\n        logger=self._bc_logger,\n        checkpoint=False)\n    self._bcq_learner = _InternalBCQLearner(\n        network=network,\n        learning_rate=learning_rate,\n        dataset=dataset,\n        counter=counting.Counter(counter, 'bcq'),\n        logger=self._bcq_logger,\n        **bcq_learner_kwargs)",
  "def get_variables(self, names):\n    return self._bcq_learner.get_variables(names)",
  "def state(self):\n    bc_state = self._bc_learner.state\n    bc_state.pop('network')  # No need to checkpoint the BC network.\n    bcq_state = self._bcq_learner.state\n    state = dict()\n    state.update({f'bc_{k}': v for k, v in bc_state.items()})\n    state.update({f'bcq_{k}': v for k, v in bcq_state.items()})\n    return state",
  "def step(self):\n    self._bc_learner.step()\n    self._bcq_learner.step()",
  "class DQfD(agent.Agent):\n  \"\"\"DQfD agent.\n\n  This implements a single-process DQN agent that mixes demonstrations with\n  actor experience.\n  \"\"\"\n\n  def __init__(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      network: snt.Module,\n      demonstration_dataset: tf.data.Dataset,\n      demonstration_ratio: float,\n      batch_size: int = 256,\n      prefetch_size: int = 4,\n      target_update_period: int = 100,\n      samples_per_insert: float = 32.0,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1000000,\n      importance_sampling_exponent: float = 0.2,\n      n_step: int = 5,\n      epsilon: Optional[tf.Tensor] = None,\n      learning_rate: float = 1e-3,\n      discount: float = 0.99,\n  ):\n    \"\"\"Initialize the agent.\n\n    Args:\n      environment_spec: description of the actions, observations, etc.\n      network: the online Q network (the one being optimized)\n      demonstration_dataset: tf.data.Dataset producing (timestep, action)\n        tuples containing full episodes.\n      demonstration_ratio: Ratio of transitions coming from demonstrations.\n      batch_size: batch size for updates.\n      prefetch_size: size to prefetch from replay.\n      target_update_period: number of learner steps to perform before updating\n        the target networks.\n      samples_per_insert: number of samples to take from replay for every insert\n        that is made.\n      min_replay_size: minimum replay size before updating. This and all\n        following arguments are related to dataset construction and will be\n        ignored if a dataset argument is passed.\n      max_replay_size: maximum replay size.\n      importance_sampling_exponent: power to which importance weights are raised\n        before normalizing.\n      n_step: number of steps to squash into a single transition.\n      epsilon: probability of taking a random action; ignored if a policy\n        network is given.\n      learning_rate: learning rate for the q-network update.\n      discount: discount to use for TD updates.\n    \"\"\"\n\n    # Create a replay server to add data to. This uses no limiter behavior in\n    # order to allow the Agent interface to handle it.\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=max_replay_size,\n        rate_limiter=reverb.rate_limiters.MinSize(1),\n        signature=adders.NStepTransitionAdder.signature(environment_spec))\n    self._server = reverb.Server([replay_table], port=None)\n\n    # The adder is used to insert observations into replay.\n    address = f'localhost:{self._server.port}'\n    adder = adders.NStepTransitionAdder(\n        client=reverb.Client(address),\n        n_step=n_step,\n        discount=discount)\n\n    # The dataset provides an interface to sample from replay.\n    replay_client = reverb.TFClient(address)\n    dataset = datasets.make_reverb_dataset(server_address=address)\n\n    # Combine with demonstration dataset.\n    transition = functools.partial(_n_step_transition_from_episode,\n                                   n_step=n_step,\n                                   discount=discount)\n    dataset_demos = demonstration_dataset.map(transition)\n    dataset = tf.data.experimental.sample_from_datasets(\n        [dataset, dataset_demos],\n        [1 - demonstration_ratio, demonstration_ratio])\n\n    # Batch and prefetch.\n    dataset = dataset.batch(batch_size, drop_remainder=True)\n    dataset = dataset.prefetch(prefetch_size)\n\n    # Use constant 0.05 epsilon greedy policy by default.\n    if epsilon is None:\n      epsilon = tf.Variable(0.05, trainable=False)\n    policy_network = snt.Sequential([\n        network,\n        lambda q: trfl.epsilon_greedy(q, epsilon=epsilon).sample(),\n    ])\n\n    # Create a target network.\n    target_network = copy.deepcopy(network)\n\n    # Ensure that we create the variables before proceeding (maybe not needed).\n    tf2_utils.create_variables(network, [environment_spec.observations])\n    tf2_utils.create_variables(target_network, [environment_spec.observations])\n\n    # Create the actor which defines how we take actions.\n    actor = actors.FeedForwardActor(policy_network, adder)\n\n    # The learner updates the parameters (and initializes them).\n    learner = dqn.DQNLearner(\n        network=network,\n        target_network=target_network,\n        discount=discount,\n        importance_sampling_exponent=importance_sampling_exponent,\n        learning_rate=learning_rate,\n        target_update_period=target_update_period,\n        dataset=dataset,\n        replay_client=replay_client)\n\n    super().__init__(\n        actor=actor,\n        learner=learner,\n        min_observations=max(batch_size, min_replay_size),\n        observations_per_step=float(batch_size) / samples_per_insert)",
  "def _n_step_transition_from_episode(observations: acme_types.NestedTensor,\n                                    actions: tf.Tensor,\n                                    rewards: tf.Tensor,\n                                    discounts: tf.Tensor,\n                                    n_step: int,\n                                    discount: float):\n  \"\"\"Produce Reverb-like N-step transition from a full episode.\n\n  Observations, actions, rewards and discounts have the same length. This\n  function will ignore the first reward and discount and the last action.\n\n  Args:\n    observations: [L, ...] Tensor.\n    actions: [L, ...] Tensor.\n    rewards: [L] Tensor.\n    discounts: [L] Tensor.\n    n_step: number of steps to squash into a single transition.\n    discount: discount to use for TD updates.\n\n  Returns:\n    (o_t, a_t, r_t, d_t, o_tp1) tuple.\n  \"\"\"\n\n  max_index = tf.shape(rewards)[0] - 1\n  first = tf.random.uniform(shape=(), minval=0, maxval=max_index - 1,\n                            dtype=tf.int32)\n  last = tf.minimum(first + n_step, max_index)\n\n  o_t = tree.map_structure(operator.itemgetter(first), observations)\n  a_t = tree.map_structure(operator.itemgetter(first), actions)\n  o_tp1 = tree.map_structure(operator.itemgetter(last), observations)\n\n  # 0, 1, ..., n-1.\n  discount_range = tf.cast(tf.range(last - first), tf.float32)\n  # 1, g, ..., g^{n-1}.\n  additional_discounts = tf.pow(discount, discount_range)\n  # 1, d_t, d_t * d_{t+1}, ..., d_t * ... * d_{t+n-2}.\n  discounts = tf.concat([[1.], tf.math.cumprod(discounts[first:last-1])], 0)\n  # 1, g * d_t, ..., g^{n-1} * d_t * ... * d_{t+n-2}.\n  discounts *= additional_discounts\n  #\u00a0r_t + g * d_t * r_{t+1} + ... + g^{n-1} * d_t * ... * d_{t+n-2} * r_{t+n-1}\n  # We have to shift rewards by one so last=max_index corresponds to transitions\n  # that include the last reward.\n  r_t = tf.reduce_sum(rewards[first+1:last+1] * discounts)\n\n  # g^{n-1} * d_{t} * ... * d_{t+n-1}.\n  d_t = discounts[-1]\n\n  info = tree.map_structure(lambda dtype: tf.ones([], dtype),\n                            reverb.SampleInfo.tf_dtypes())\n  return reverb.ReplaySample(\n      info=info, data=acme_types.Transition(o_t, a_t, r_t, d_t, o_tp1))",
  "def __init__(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      network: snt.Module,\n      demonstration_dataset: tf.data.Dataset,\n      demonstration_ratio: float,\n      batch_size: int = 256,\n      prefetch_size: int = 4,\n      target_update_period: int = 100,\n      samples_per_insert: float = 32.0,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1000000,\n      importance_sampling_exponent: float = 0.2,\n      n_step: int = 5,\n      epsilon: Optional[tf.Tensor] = None,\n      learning_rate: float = 1e-3,\n      discount: float = 0.99,\n  ):\n    \"\"\"Initialize the agent.\n\n    Args:\n      environment_spec: description of the actions, observations, etc.\n      network: the online Q network (the one being optimized)\n      demonstration_dataset: tf.data.Dataset producing (timestep, action)\n        tuples containing full episodes.\n      demonstration_ratio: Ratio of transitions coming from demonstrations.\n      batch_size: batch size for updates.\n      prefetch_size: size to prefetch from replay.\n      target_update_period: number of learner steps to perform before updating\n        the target networks.\n      samples_per_insert: number of samples to take from replay for every insert\n        that is made.\n      min_replay_size: minimum replay size before updating. This and all\n        following arguments are related to dataset construction and will be\n        ignored if a dataset argument is passed.\n      max_replay_size: maximum replay size.\n      importance_sampling_exponent: power to which importance weights are raised\n        before normalizing.\n      n_step: number of steps to squash into a single transition.\n      epsilon: probability of taking a random action; ignored if a policy\n        network is given.\n      learning_rate: learning rate for the q-network update.\n      discount: discount to use for TD updates.\n    \"\"\"\n\n    # Create a replay server to add data to. This uses no limiter behavior in\n    # order to allow the Agent interface to handle it.\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=max_replay_size,\n        rate_limiter=reverb.rate_limiters.MinSize(1),\n        signature=adders.NStepTransitionAdder.signature(environment_spec))\n    self._server = reverb.Server([replay_table], port=None)\n\n    # The adder is used to insert observations into replay.\n    address = f'localhost:{self._server.port}'\n    adder = adders.NStepTransitionAdder(\n        client=reverb.Client(address),\n        n_step=n_step,\n        discount=discount)\n\n    # The dataset provides an interface to sample from replay.\n    replay_client = reverb.TFClient(address)\n    dataset = datasets.make_reverb_dataset(server_address=address)\n\n    # Combine with demonstration dataset.\n    transition = functools.partial(_n_step_transition_from_episode,\n                                   n_step=n_step,\n                                   discount=discount)\n    dataset_demos = demonstration_dataset.map(transition)\n    dataset = tf.data.experimental.sample_from_datasets(\n        [dataset, dataset_demos],\n        [1 - demonstration_ratio, demonstration_ratio])\n\n    # Batch and prefetch.\n    dataset = dataset.batch(batch_size, drop_remainder=True)\n    dataset = dataset.prefetch(prefetch_size)\n\n    # Use constant 0.05 epsilon greedy policy by default.\n    if epsilon is None:\n      epsilon = tf.Variable(0.05, trainable=False)\n    policy_network = snt.Sequential([\n        network,\n        lambda q: trfl.epsilon_greedy(q, epsilon=epsilon).sample(),\n    ])\n\n    # Create a target network.\n    target_network = copy.deepcopy(network)\n\n    # Ensure that we create the variables before proceeding (maybe not needed).\n    tf2_utils.create_variables(network, [environment_spec.observations])\n    tf2_utils.create_variables(target_network, [environment_spec.observations])\n\n    # Create the actor which defines how we take actions.\n    actor = actors.FeedForwardActor(policy_network, adder)\n\n    # The learner updates the parameters (and initializes them).\n    learner = dqn.DQNLearner(\n        network=network,\n        target_network=target_network,\n        discount=discount,\n        importance_sampling_exponent=importance_sampling_exponent,\n        learning_rate=learning_rate,\n        target_update_period=target_update_period,\n        dataset=dataset,\n        replay_client=replay_client)\n\n    super().__init__(\n        actor=actor,\n        learner=learner,\n        min_observations=max(batch_size, min_replay_size),\n        observations_per_step=float(batch_size) / samples_per_insert)",
  "def _nested_stack(sequence: List[Any]):\n  \"\"\"Stack nested elements in a sequence.\"\"\"\n  return tree.map_structure(lambda *x: np.stack(x), *sequence)",
  "class DemonstrationRecorder:\n  \"\"\"Records demonstrations.\n\n  A demonstration is a (observation, action, reward, discount) tuple where\n  every element is a numpy array corresponding to a full episode.\n  \"\"\"\n\n  def __init__(self):\n    self._demos = []\n    self._reset_episode()\n\n  def step(self, timestep: dm_env.TimeStep, action: np.ndarray):\n    reward = np.array(timestep.reward or 0, np.float32)\n    self._episode_reward += reward\n    self._episode.append((timestep.observation, action, reward,\n                          np.array(timestep.discount or 0, np.float32)))\n\n  def record_episode(self):\n    self._demos.append(_nested_stack(self._episode))\n    self._reset_episode()\n\n  def discard_episode(self):\n    self._reset_episode()\n\n  def _reset_episode(self):\n    self._episode = []\n    self._episode_reward = 0\n\n  @property\n  def episode_reward(self):\n    return self._episode_reward\n\n  def make_tf_dataset(self):\n    types = tree.map_structure(lambda x: x.dtype, self._demos[0])\n    shapes = tree.map_structure(lambda x: x.shape, self._demos[0])\n    ds = tf.data.Dataset.from_generator(lambda: self._demos, types, shapes)\n    return ds.repeat().shuffle(len(self._demos))",
  "def _optimal_deep_sea_policy(environment: deep_sea.DeepSea,\n                             timestep: dm_env.TimeStep):\n  action = environment._action_mapping[np.where(timestep.observation)]  # pylint: disable=protected-access\n  return action[0].astype(np.int32)",
  "def _run_optimal_deep_sea_episode(environment: deep_sea.DeepSea,\n                                  recorder: DemonstrationRecorder):\n  timestep = environment.reset()\n  while timestep.step_type is not dm_env.StepType.LAST:\n    action = _optimal_deep_sea_policy(environment, timestep)\n    recorder.step(timestep, action)\n    timestep = environment.step(action)\n  recorder.step(timestep, np.zeros_like(action))",
  "def _make_deep_sea_dataset(environment: deep_sea.DeepSea):\n  \"\"\"Make DeepSea demonstration dataset.\"\"\"\n\n  recorder = DemonstrationRecorder()\n\n  _run_optimal_deep_sea_episode(environment, recorder)\n  assert recorder.episode_reward > 0\n  recorder.record_episode()\n  return recorder.make_tf_dataset()",
  "def _make_deep_sea_stochastic_dataset(environment: deep_sea.DeepSea):\n  \"\"\"Make stochastic DeepSea demonstration dataset.\"\"\"\n\n  recorder = DemonstrationRecorder()\n\n  # Use 10*size demos, 80% success, 20% failure.\n  num_demos = environment._size * 10  # pylint: disable=protected-access\n  num_failures = num_demos // 5\n  num_successes = num_demos - num_failures\n\n  successes_saved = 0\n  failures_saved = 0\n  while (successes_saved < num_successes) or (failures_saved < num_failures):\n    _run_optimal_deep_sea_episode(environment, recorder)\n\n    if recorder.episode_reward > 0 and successes_saved < num_successes:\n      recorder.record_episode()\n      successes_saved += 1\n    elif recorder.episode_reward <= 0 and failures_saved < num_failures:\n      recorder.record_episode()\n      failures_saved += 1\n    else:\n      recorder.discard_episode()\n\n  return recorder.make_tf_dataset()",
  "def make_dataset(environment: dm_env.Environment, stochastic: bool):\n  \"\"\"Make bsuite demos for the current task.\"\"\"\n\n  if not stochastic:\n    assert isinstance(environment, deep_sea.DeepSea)\n    return _make_deep_sea_dataset(environment)\n  else:\n    assert isinstance(environment, deep_sea.DeepSea)\n    return _make_deep_sea_stochastic_dataset(environment)",
  "def __init__(self):\n    self._demos = []\n    self._reset_episode()",
  "def step(self, timestep: dm_env.TimeStep, action: np.ndarray):\n    reward = np.array(timestep.reward or 0, np.float32)\n    self._episode_reward += reward\n    self._episode.append((timestep.observation, action, reward,\n                          np.array(timestep.discount or 0, np.float32)))",
  "def record_episode(self):\n    self._demos.append(_nested_stack(self._episode))\n    self._reset_episode()",
  "def discard_episode(self):\n    self._reset_episode()",
  "def _reset_episode(self):\n    self._episode = []\n    self._episode_reward = 0",
  "def episode_reward(self):\n    return self._episode_reward",
  "def make_tf_dataset(self):\n    types = tree.map_structure(lambda x: x.dtype, self._demos[0])\n    shapes = tree.map_structure(lambda x: x.shape, self._demos[0])\n    ds = tf.data.Dataset.from_generator(lambda: self._demos, types, shapes)\n    return ds.repeat().shuffle(len(self._demos))",
  "def _make_network(action_spec: specs.DiscreteArray) -> snt.Module:\n  return snt.Sequential([\n      snt.Flatten(),\n      snt.nets.MLP([50, 50, action_spec.num_values]),\n  ])",
  "class DQfDTest(absltest.TestCase):\n\n  def test_dqfd(self):\n    # Create a fake environment to test with.\n    # TODO(b/152596848): Allow DQN to deal with integer observations.\n    environment = fakes.DiscreteEnvironment(\n        num_actions=5,\n        num_observations=10,\n        obs_dtype=np.float32,\n        episode_length=10)\n    spec = specs.make_environment_spec(environment)\n\n    # Build demonstrations.\n    dummy_action = np.zeros((), dtype=np.int32)\n    recorder = bsuite_demonstrations.DemonstrationRecorder()\n    timestep = environment.reset()\n    while timestep.step_type is not dm_env.StepType.LAST:\n      recorder.step(timestep, dummy_action)\n      timestep = environment.step(dummy_action)\n    recorder.step(timestep, dummy_action)\n    recorder.record_episode()\n\n    # Construct the agent.\n    agent = dqfd.DQfD(\n        environment_spec=spec,\n        network=_make_network(spec.actions),\n        demonstration_dataset=recorder.make_tf_dataset(),\n        demonstration_ratio=0.5,\n        batch_size=10,\n        samples_per_insert=2,\n        min_replay_size=10)\n\n    # Try running the environment loop. We have no assertions here because all\n    # we care about is that the agent runs without raising any errors.\n    loop = acme.EnvironmentLoop(environment, agent)\n    loop.run(num_episodes=10)",
  "def test_dqfd(self):\n    # Create a fake environment to test with.\n    # TODO(b/152596848): Allow DQN to deal with integer observations.\n    environment = fakes.DiscreteEnvironment(\n        num_actions=5,\n        num_observations=10,\n        obs_dtype=np.float32,\n        episode_length=10)\n    spec = specs.make_environment_spec(environment)\n\n    # Build demonstrations.\n    dummy_action = np.zeros((), dtype=np.int32)\n    recorder = bsuite_demonstrations.DemonstrationRecorder()\n    timestep = environment.reset()\n    while timestep.step_type is not dm_env.StepType.LAST:\n      recorder.step(timestep, dummy_action)\n      timestep = environment.step(dummy_action)\n    recorder.step(timestep, dummy_action)\n    recorder.record_episode()\n\n    # Construct the agent.\n    agent = dqfd.DQfD(\n        environment_spec=spec,\n        network=_make_network(spec.actions),\n        demonstration_dataset=recorder.make_tf_dataset(),\n        demonstration_ratio=0.5,\n        batch_size=10,\n        samples_per_insert=2,\n        min_replay_size=10)\n\n    # Try running the environment loop. We have no assertions here because all\n    # we care about is that the agent runs without raising any errors.\n    loop = acme.EnvironmentLoop(environment, agent)\n    loop.run(num_episodes=10)",
  "class MPO(agent.Agent):\n  \"\"\"MPO Agent.\n\n  This implements a single-process MPO agent. This is an actor-critic algorithm\n  that generates data via a behavior policy, inserts N-step transitions into\n  a replay buffer, and periodically updates the policy (and as a result the\n  behavior) by sampling uniformly from this buffer. This agent distinguishes\n  itself from the DPG agent by using MPO to learn a stochastic policy.\n  \"\"\"\n\n  def __init__(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy_network: snt.Module,\n      critic_network: snt.Module,\n      observation_network: types.TensorTransformation = tf.identity,\n      discount: float = 0.99,\n      batch_size: int = 256,\n      prefetch_size: int = 4,\n      target_policy_update_period: int = 100,\n      target_critic_update_period: int = 100,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1000000,\n      samples_per_insert: float = 32.0,\n      policy_loss_module: Optional[snt.Module] = None,\n      policy_optimizer: Optional[snt.Optimizer] = None,\n      critic_optimizer: Optional[snt.Optimizer] = None,\n      n_step: int = 5,\n      num_samples: int = 20,\n      clipping: bool = True,\n      logger: Optional[loggers.Logger] = None,\n      counter: Optional[counting.Counter] = None,\n      checkpoint: bool = True,\n      save_directory: str = '~/acme',\n      replay_table_name: str = adders.DEFAULT_PRIORITY_TABLE,\n  ):\n    \"\"\"Initialize the agent.\n\n    Args:\n      environment_spec: description of the actions, observations, etc.\n      policy_network: the online (optimized) policy.\n      critic_network: the online critic.\n      observation_network: optional network to transform the observations before\n        they are fed into any network.\n      discount: discount to use for TD updates.\n      batch_size: batch size for updates.\n      prefetch_size: size to prefetch from replay.\n      target_policy_update_period: number of updates to perform before updating\n        the target policy network.\n      target_critic_update_period: number of updates to perform before updating\n        the target critic network.\n      min_replay_size: minimum replay size before updating.\n      max_replay_size: maximum replay size.\n      samples_per_insert: number of samples to take from replay for every insert\n        that is made.\n      policy_loss_module: configured MPO loss function for the policy\n        optimization; defaults to sensible values on the control suite. See\n        `acme/tf/losses/mpo.py` for more details.\n      policy_optimizer: optimizer to be used on the policy.\n      critic_optimizer: optimizer to be used on the critic.\n      n_step: number of steps to squash into a single transition.\n      num_samples: number of actions to sample when doing a Monte Carlo\n        integration with respect to the policy.\n      clipping: whether to clip gradients by global norm.\n      logger: logging object used to write to logs.\n      counter: counter object used to keep track of steps.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n      save_directory: string indicating where the learner should save\n        checkpoints and snapshots.\n      replay_table_name: string indicating what name to give the replay table.\n    \"\"\"\n\n    # Create a replay server to add data to.\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=max_replay_size,\n        rate_limiter=reverb.rate_limiters.MinSize(min_size_to_sample=1),\n        signature=adders.NStepTransitionAdder.signature(environment_spec))\n    self._server = reverb.Server([replay_table], port=None)\n\n    # The adder is used to insert observations into replay.\n    address = f'localhost:{self._server.port}'\n    adder = adders.NStepTransitionAdder(\n        client=reverb.Client(address), n_step=n_step, discount=discount)\n\n    # The dataset object to learn from.\n    dataset = datasets.make_reverb_dataset(\n        table=replay_table_name,\n        server_address=address,\n        batch_size=batch_size,\n        prefetch_size=prefetch_size)\n\n    # Make sure observation network is a Sonnet Module.\n    observation_network = tf2_utils.to_sonnet_module(observation_network)\n\n    # Create target networks before creating online/target network variables.\n    target_policy_network = copy.deepcopy(policy_network)\n    target_critic_network = copy.deepcopy(critic_network)\n    target_observation_network = copy.deepcopy(observation_network)\n\n    # Get observation and action specs.\n    act_spec = environment_spec.actions\n    obs_spec = environment_spec.observations\n    emb_spec = tf2_utils.create_variables(observation_network, [obs_spec])\n\n    # Create the behavior policy.\n    behavior_network = snt.Sequential([\n        observation_network,\n        policy_network,\n        networks.StochasticSamplingHead(),\n    ])\n\n    # Create variables.\n    tf2_utils.create_variables(policy_network, [emb_spec])\n    tf2_utils.create_variables(critic_network, [emb_spec, act_spec])\n    tf2_utils.create_variables(target_policy_network, [emb_spec])\n    tf2_utils.create_variables(target_critic_network, [emb_spec, act_spec])\n    tf2_utils.create_variables(target_observation_network, [obs_spec])\n\n    # Create the actor which defines how we take actions.\n    actor = actors.FeedForwardActor(\n        policy_network=behavior_network, adder=adder)\n\n    # Create optimizers.\n    policy_optimizer = policy_optimizer or snt.optimizers.Adam(1e-4)\n    critic_optimizer = critic_optimizer or snt.optimizers.Adam(1e-4)\n\n    # The learner updates the parameters (and initializes them).\n    learner = learning.MPOLearner(\n        policy_network=policy_network,\n        critic_network=critic_network,\n        observation_network=observation_network,\n        target_policy_network=target_policy_network,\n        target_critic_network=target_critic_network,\n        target_observation_network=target_observation_network,\n        policy_loss_module=policy_loss_module,\n        policy_optimizer=policy_optimizer,\n        critic_optimizer=critic_optimizer,\n        clipping=clipping,\n        discount=discount,\n        num_samples=num_samples,\n        target_policy_update_period=target_policy_update_period,\n        target_critic_update_period=target_critic_update_period,\n        dataset=dataset,\n        logger=logger,\n        counter=counter,\n        checkpoint=checkpoint,\n        save_directory=save_directory)\n\n    super().__init__(\n        actor=actor,\n        learner=learner,\n        min_observations=max(batch_size, min_replay_size),\n        observations_per_step=float(batch_size) / samples_per_insert)",
  "def __init__(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy_network: snt.Module,\n      critic_network: snt.Module,\n      observation_network: types.TensorTransformation = tf.identity,\n      discount: float = 0.99,\n      batch_size: int = 256,\n      prefetch_size: int = 4,\n      target_policy_update_period: int = 100,\n      target_critic_update_period: int = 100,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1000000,\n      samples_per_insert: float = 32.0,\n      policy_loss_module: Optional[snt.Module] = None,\n      policy_optimizer: Optional[snt.Optimizer] = None,\n      critic_optimizer: Optional[snt.Optimizer] = None,\n      n_step: int = 5,\n      num_samples: int = 20,\n      clipping: bool = True,\n      logger: Optional[loggers.Logger] = None,\n      counter: Optional[counting.Counter] = None,\n      checkpoint: bool = True,\n      save_directory: str = '~/acme',\n      replay_table_name: str = adders.DEFAULT_PRIORITY_TABLE,\n  ):\n    \"\"\"Initialize the agent.\n\n    Args:\n      environment_spec: description of the actions, observations, etc.\n      policy_network: the online (optimized) policy.\n      critic_network: the online critic.\n      observation_network: optional network to transform the observations before\n        they are fed into any network.\n      discount: discount to use for TD updates.\n      batch_size: batch size for updates.\n      prefetch_size: size to prefetch from replay.\n      target_policy_update_period: number of updates to perform before updating\n        the target policy network.\n      target_critic_update_period: number of updates to perform before updating\n        the target critic network.\n      min_replay_size: minimum replay size before updating.\n      max_replay_size: maximum replay size.\n      samples_per_insert: number of samples to take from replay for every insert\n        that is made.\n      policy_loss_module: configured MPO loss function for the policy\n        optimization; defaults to sensible values on the control suite. See\n        `acme/tf/losses/mpo.py` for more details.\n      policy_optimizer: optimizer to be used on the policy.\n      critic_optimizer: optimizer to be used on the critic.\n      n_step: number of steps to squash into a single transition.\n      num_samples: number of actions to sample when doing a Monte Carlo\n        integration with respect to the policy.\n      clipping: whether to clip gradients by global norm.\n      logger: logging object used to write to logs.\n      counter: counter object used to keep track of steps.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n      save_directory: string indicating where the learner should save\n        checkpoints and snapshots.\n      replay_table_name: string indicating what name to give the replay table.\n    \"\"\"\n\n    # Create a replay server to add data to.\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=max_replay_size,\n        rate_limiter=reverb.rate_limiters.MinSize(min_size_to_sample=1),\n        signature=adders.NStepTransitionAdder.signature(environment_spec))\n    self._server = reverb.Server([replay_table], port=None)\n\n    # The adder is used to insert observations into replay.\n    address = f'localhost:{self._server.port}'\n    adder = adders.NStepTransitionAdder(\n        client=reverb.Client(address), n_step=n_step, discount=discount)\n\n    # The dataset object to learn from.\n    dataset = datasets.make_reverb_dataset(\n        table=replay_table_name,\n        server_address=address,\n        batch_size=batch_size,\n        prefetch_size=prefetch_size)\n\n    # Make sure observation network is a Sonnet Module.\n    observation_network = tf2_utils.to_sonnet_module(observation_network)\n\n    # Create target networks before creating online/target network variables.\n    target_policy_network = copy.deepcopy(policy_network)\n    target_critic_network = copy.deepcopy(critic_network)\n    target_observation_network = copy.deepcopy(observation_network)\n\n    # Get observation and action specs.\n    act_spec = environment_spec.actions\n    obs_spec = environment_spec.observations\n    emb_spec = tf2_utils.create_variables(observation_network, [obs_spec])\n\n    # Create the behavior policy.\n    behavior_network = snt.Sequential([\n        observation_network,\n        policy_network,\n        networks.StochasticSamplingHead(),\n    ])\n\n    # Create variables.\n    tf2_utils.create_variables(policy_network, [emb_spec])\n    tf2_utils.create_variables(critic_network, [emb_spec, act_spec])\n    tf2_utils.create_variables(target_policy_network, [emb_spec])\n    tf2_utils.create_variables(target_critic_network, [emb_spec, act_spec])\n    tf2_utils.create_variables(target_observation_network, [obs_spec])\n\n    # Create the actor which defines how we take actions.\n    actor = actors.FeedForwardActor(\n        policy_network=behavior_network, adder=adder)\n\n    # Create optimizers.\n    policy_optimizer = policy_optimizer or snt.optimizers.Adam(1e-4)\n    critic_optimizer = critic_optimizer or snt.optimizers.Adam(1e-4)\n\n    # The learner updates the parameters (and initializes them).\n    learner = learning.MPOLearner(\n        policy_network=policy_network,\n        critic_network=critic_network,\n        observation_network=observation_network,\n        target_policy_network=target_policy_network,\n        target_critic_network=target_critic_network,\n        target_observation_network=target_observation_network,\n        policy_loss_module=policy_loss_module,\n        policy_optimizer=policy_optimizer,\n        critic_optimizer=critic_optimizer,\n        clipping=clipping,\n        discount=discount,\n        num_samples=num_samples,\n        target_policy_update_period=target_policy_update_period,\n        target_critic_update_period=target_critic_update_period,\n        dataset=dataset,\n        logger=logger,\n        counter=counter,\n        checkpoint=checkpoint,\n        save_directory=save_directory)\n\n    super().__init__(\n        actor=actor,\n        learner=learner,\n        min_observations=max(batch_size, min_replay_size),\n        observations_per_step=float(batch_size) / samples_per_insert)",
  "class MPOLearner(acme.Learner):\n  \"\"\"MPO learner.\"\"\"\n\n  def __init__(\n      self,\n      policy_network: snt.Module,\n      critic_network: snt.Module,\n      target_policy_network: snt.Module,\n      target_critic_network: snt.Module,\n      discount: float,\n      num_samples: int,\n      target_policy_update_period: int,\n      target_critic_update_period: int,\n      dataset: tf.data.Dataset,\n      observation_network: types.TensorTransformation = tf.identity,\n      target_observation_network: types.TensorTransformation = tf.identity,\n      policy_loss_module: Optional[snt.Module] = None,\n      policy_optimizer: Optional[snt.Optimizer] = None,\n      critic_optimizer: Optional[snt.Optimizer] = None,\n      dual_optimizer: Optional[snt.Optimizer] = None,\n      clipping: bool = True,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = True,\n      save_directory: str = '~/acme',\n  ):\n\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger('learner')\n    self._discount = discount\n    self._num_samples = num_samples\n    self._clipping = clipping\n\n    # Necessary to track when to update target networks.\n    self._num_steps = tf.Variable(0, dtype=tf.int32)\n    self._target_policy_update_period = target_policy_update_period\n    self._target_critic_update_period = target_critic_update_period\n\n    # Batch dataset and create iterator.\n    # TODO(b/155086959): Fix type stubs and remove.\n    self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n    # Store online and target networks.\n    self._policy_network = policy_network\n    self._critic_network = critic_network\n    self._target_policy_network = target_policy_network\n    self._target_critic_network = target_critic_network\n\n    # Make sure observation networks are snt.Module's so they have variables.\n    self._observation_network = tf2_utils.to_sonnet_module(observation_network)\n    self._target_observation_network = tf2_utils.to_sonnet_module(\n        target_observation_network)\n\n    self._policy_loss_module = policy_loss_module or losses.MPO(\n        epsilon=1e-1,\n        epsilon_penalty=1e-3,\n        epsilon_mean=2.5e-3,\n        epsilon_stddev=1e-6,\n        init_log_temperature=10.,\n        init_log_alpha_mean=10.,\n        init_log_alpha_stddev=1000.)\n\n    # Create the optimizers.\n    self._critic_optimizer = critic_optimizer or snt.optimizers.Adam(1e-4)\n    self._policy_optimizer = policy_optimizer or snt.optimizers.Adam(1e-4)\n    self._dual_optimizer = dual_optimizer or snt.optimizers.Adam(1e-2)\n\n    # Expose the variables.\n    policy_network_to_expose = snt.Sequential(\n        [self._target_observation_network, self._target_policy_network])\n    self._variables = {\n        'critic': self._target_critic_network.variables,\n        'policy': policy_network_to_expose.variables,\n    }\n\n    # Create a checkpointer and snapshotter object.\n    self._checkpointer = None\n    self._snapshotter = None\n\n    if checkpoint:\n      self._checkpointer = tf2_savers.Checkpointer(\n          directory=save_directory,\n          subdirectory='mpo_learner',\n          objects_to_save={\n              'counter': self._counter,\n              'policy': self._policy_network,\n              'critic': self._critic_network,\n              'observation_network': self._observation_network,\n              'target_policy': self._target_policy_network,\n              'target_critic': self._target_critic_network,\n              'target_observation_network': self._target_observation_network,\n              'policy_optimizer': self._policy_optimizer,\n              'critic_optimizer': self._critic_optimizer,\n              'dual_optimizer': self._dual_optimizer,\n              'policy_loss_module': self._policy_loss_module,\n              'num_steps': self._num_steps,\n          })\n\n      self._snapshotter = tf2_savers.Snapshotter(\n          directory=save_directory,\n          objects_to_save={\n              'policy':\n                  snt.Sequential([\n                      self._target_observation_network,\n                      self._target_policy_network\n                  ]),\n          })\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None\n\n  @tf.function\n  def _step(self) -> types.Nest:\n    # Update target network.\n    online_policy_variables = self._policy_network.variables\n    target_policy_variables = self._target_policy_network.variables\n    online_critic_variables = (\n        *self._observation_network.variables,\n        *self._critic_network.variables,\n    )\n    target_critic_variables = (\n        *self._target_observation_network.variables,\n        *self._target_critic_network.variables,\n    )\n\n    # Make online policy -> target policy network update ops.\n    if tf.math.mod(self._num_steps, self._target_policy_update_period) == 0:\n      for src, dest in zip(online_policy_variables, target_policy_variables):\n        dest.assign(src)\n    # Make online critic -> target critic network update ops.\n    if tf.math.mod(self._num_steps, self._target_critic_update_period) == 0:\n      for src, dest in zip(online_critic_variables, target_critic_variables):\n        dest.assign(src)\n\n    # Increment number of learner steps for periodic update bookkeeping.\n    self._num_steps.assign_add(1)\n\n    # Get next batch of data.\n    inputs = next(self._iterator)\n\n    # Get data from replay (dropping extras if any). Note there is no\n    # extra data here because we do not insert any into Reverb.\n    transitions: types.Transition = inputs.data\n\n    # Cast the additional discount to match the environment discount dtype.\n    discount = tf.cast(self._discount, dtype=transitions.discount.dtype)\n\n    with tf.GradientTape(persistent=True) as tape:\n      # Maybe transform the observation before feeding into policy and critic.\n      # Transforming the observations this way at the start of the learning\n      # step effectively means that the policy and critic share observation\n      # network weights.\n      o_tm1 = self._observation_network(transitions.observation)\n      # This stop_gradient prevents gradients to propagate into the target\n      # observation network. In addition, since the online policy network is\n      # evaluated at o_t, this also means the policy loss does not influence\n      # the observation network training.\n      o_t = tf.stop_gradient(\n          self._target_observation_network(transitions.next_observation))\n\n      # Get action distributions from policy networks.\n      online_action_distribution = self._policy_network(o_t)\n      target_action_distribution = self._target_policy_network(o_t)\n\n      # Get sampled actions to evaluate policy; of size [N, B, ...].\n      sampled_actions = target_action_distribution.sample(self._num_samples)\n      tiled_o_t = tf2_utils.tile_tensor(o_t, self._num_samples)  # [N, B, ...]\n\n      # Compute the target critic's Q-value of the sampled actions in state o_t.\n      sampled_q_t = self._target_critic_network(\n          # Merge batch dimensions; to shape [N*B, ...].\n          snt.merge_leading_dims(tiled_o_t, num_dims=2),\n          snt.merge_leading_dims(sampled_actions, num_dims=2))\n\n      # Reshape Q-value samples back to original batch dimensions and average\n      # them to compute the TD-learning bootstrap target.\n      sampled_q_t = tf.reshape(sampled_q_t, (self._num_samples, -1))  # [N, B]\n      q_t = tf.reduce_mean(sampled_q_t, axis=0)  # [B]\n\n      # Compute online critic value of a_tm1 in state o_tm1.\n      q_tm1 = self._critic_network(o_tm1, transitions.action)  # [B, 1]\n      q_tm1 = tf.squeeze(q_tm1, axis=-1)  # [B]; necessary for trfl.td_learning.\n\n      # Critic loss.\n      critic_loss = trfl.td_learning(q_tm1, transitions.reward,\n                                     discount * transitions.discount, q_t).loss\n      critic_loss = tf.reduce_mean(critic_loss)\n\n      # Actor learning.\n      policy_loss, policy_stats = self._policy_loss_module(\n          online_action_distribution=online_action_distribution,\n          target_action_distribution=target_action_distribution,\n          actions=sampled_actions,\n          q_values=sampled_q_t)\n\n    # For clarity, explicitly define which variables are trained by which loss.\n    critic_trainable_variables = (\n        # In this agent, the critic loss trains the observation network.\n        self._observation_network.trainable_variables +\n        self._critic_network.trainable_variables)\n    policy_trainable_variables = self._policy_network.trainable_variables\n    # The following are the MPO dual variables, stored in the loss module.\n    dual_trainable_variables = self._policy_loss_module.trainable_variables\n\n    # Compute gradients.\n    critic_gradients = tape.gradient(critic_loss, critic_trainable_variables)\n    policy_gradients, dual_gradients = tape.gradient(\n        policy_loss, (policy_trainable_variables, dual_trainable_variables))\n\n    # Delete the tape manually because of the persistent=True flag.\n    del tape\n\n    # Maybe clip gradients.\n    if self._clipping:\n      policy_gradients = tuple(tf.clip_by_global_norm(policy_gradients, 40.)[0])\n      critic_gradients = tuple(tf.clip_by_global_norm(critic_gradients, 40.)[0])\n\n    # Apply gradients.\n    self._critic_optimizer.apply(critic_gradients, critic_trainable_variables)\n    self._policy_optimizer.apply(policy_gradients, policy_trainable_variables)\n    self._dual_optimizer.apply(dual_gradients, dual_trainable_variables)\n\n    # Losses to track.\n    fetches = {\n        'critic_loss': critic_loss,\n        'policy_loss': policy_loss,\n    }\n    fetches.update(policy_stats)  # Log MPO stats.\n\n    return fetches\n\n  def step(self):\n    # Run the learning step.\n    fetches = self._step()\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n    fetches.update(counts)\n\n    # Checkpoint and attempt to write the logs.\n    if self._checkpointer is not None:\n      self._checkpointer.save()\n    if self._snapshotter is not None:\n      self._snapshotter.save()\n    self._logger.write(fetches)\n\n  def get_variables(self, names: List[str]) -> List[List[np.ndarray]]:\n    return [tf2_utils.to_numpy(self._variables[name]) for name in names]",
  "def __init__(\n      self,\n      policy_network: snt.Module,\n      critic_network: snt.Module,\n      target_policy_network: snt.Module,\n      target_critic_network: snt.Module,\n      discount: float,\n      num_samples: int,\n      target_policy_update_period: int,\n      target_critic_update_period: int,\n      dataset: tf.data.Dataset,\n      observation_network: types.TensorTransformation = tf.identity,\n      target_observation_network: types.TensorTransformation = tf.identity,\n      policy_loss_module: Optional[snt.Module] = None,\n      policy_optimizer: Optional[snt.Optimizer] = None,\n      critic_optimizer: Optional[snt.Optimizer] = None,\n      dual_optimizer: Optional[snt.Optimizer] = None,\n      clipping: bool = True,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = True,\n      save_directory: str = '~/acme',\n  ):\n\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger('learner')\n    self._discount = discount\n    self._num_samples = num_samples\n    self._clipping = clipping\n\n    # Necessary to track when to update target networks.\n    self._num_steps = tf.Variable(0, dtype=tf.int32)\n    self._target_policy_update_period = target_policy_update_period\n    self._target_critic_update_period = target_critic_update_period\n\n    # Batch dataset and create iterator.\n    # TODO(b/155086959): Fix type stubs and remove.\n    self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n    # Store online and target networks.\n    self._policy_network = policy_network\n    self._critic_network = critic_network\n    self._target_policy_network = target_policy_network\n    self._target_critic_network = target_critic_network\n\n    # Make sure observation networks are snt.Module's so they have variables.\n    self._observation_network = tf2_utils.to_sonnet_module(observation_network)\n    self._target_observation_network = tf2_utils.to_sonnet_module(\n        target_observation_network)\n\n    self._policy_loss_module = policy_loss_module or losses.MPO(\n        epsilon=1e-1,\n        epsilon_penalty=1e-3,\n        epsilon_mean=2.5e-3,\n        epsilon_stddev=1e-6,\n        init_log_temperature=10.,\n        init_log_alpha_mean=10.,\n        init_log_alpha_stddev=1000.)\n\n    # Create the optimizers.\n    self._critic_optimizer = critic_optimizer or snt.optimizers.Adam(1e-4)\n    self._policy_optimizer = policy_optimizer or snt.optimizers.Adam(1e-4)\n    self._dual_optimizer = dual_optimizer or snt.optimizers.Adam(1e-2)\n\n    # Expose the variables.\n    policy_network_to_expose = snt.Sequential(\n        [self._target_observation_network, self._target_policy_network])\n    self._variables = {\n        'critic': self._target_critic_network.variables,\n        'policy': policy_network_to_expose.variables,\n    }\n\n    # Create a checkpointer and snapshotter object.\n    self._checkpointer = None\n    self._snapshotter = None\n\n    if checkpoint:\n      self._checkpointer = tf2_savers.Checkpointer(\n          directory=save_directory,\n          subdirectory='mpo_learner',\n          objects_to_save={\n              'counter': self._counter,\n              'policy': self._policy_network,\n              'critic': self._critic_network,\n              'observation_network': self._observation_network,\n              'target_policy': self._target_policy_network,\n              'target_critic': self._target_critic_network,\n              'target_observation_network': self._target_observation_network,\n              'policy_optimizer': self._policy_optimizer,\n              'critic_optimizer': self._critic_optimizer,\n              'dual_optimizer': self._dual_optimizer,\n              'policy_loss_module': self._policy_loss_module,\n              'num_steps': self._num_steps,\n          })\n\n      self._snapshotter = tf2_savers.Snapshotter(\n          directory=save_directory,\n          objects_to_save={\n              'policy':\n                  snt.Sequential([\n                      self._target_observation_network,\n                      self._target_policy_network\n                  ]),\n          })\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None",
  "def _step(self) -> types.Nest:\n    # Update target network.\n    online_policy_variables = self._policy_network.variables\n    target_policy_variables = self._target_policy_network.variables\n    online_critic_variables = (\n        *self._observation_network.variables,\n        *self._critic_network.variables,\n    )\n    target_critic_variables = (\n        *self._target_observation_network.variables,\n        *self._target_critic_network.variables,\n    )\n\n    # Make online policy -> target policy network update ops.\n    if tf.math.mod(self._num_steps, self._target_policy_update_period) == 0:\n      for src, dest in zip(online_policy_variables, target_policy_variables):\n        dest.assign(src)\n    # Make online critic -> target critic network update ops.\n    if tf.math.mod(self._num_steps, self._target_critic_update_period) == 0:\n      for src, dest in zip(online_critic_variables, target_critic_variables):\n        dest.assign(src)\n\n    # Increment number of learner steps for periodic update bookkeeping.\n    self._num_steps.assign_add(1)\n\n    # Get next batch of data.\n    inputs = next(self._iterator)\n\n    # Get data from replay (dropping extras if any). Note there is no\n    # extra data here because we do not insert any into Reverb.\n    transitions: types.Transition = inputs.data\n\n    # Cast the additional discount to match the environment discount dtype.\n    discount = tf.cast(self._discount, dtype=transitions.discount.dtype)\n\n    with tf.GradientTape(persistent=True) as tape:\n      # Maybe transform the observation before feeding into policy and critic.\n      # Transforming the observations this way at the start of the learning\n      # step effectively means that the policy and critic share observation\n      # network weights.\n      o_tm1 = self._observation_network(transitions.observation)\n      # This stop_gradient prevents gradients to propagate into the target\n      # observation network. In addition, since the online policy network is\n      # evaluated at o_t, this also means the policy loss does not influence\n      # the observation network training.\n      o_t = tf.stop_gradient(\n          self._target_observation_network(transitions.next_observation))\n\n      # Get action distributions from policy networks.\n      online_action_distribution = self._policy_network(o_t)\n      target_action_distribution = self._target_policy_network(o_t)\n\n      # Get sampled actions to evaluate policy; of size [N, B, ...].\n      sampled_actions = target_action_distribution.sample(self._num_samples)\n      tiled_o_t = tf2_utils.tile_tensor(o_t, self._num_samples)  # [N, B, ...]\n\n      # Compute the target critic's Q-value of the sampled actions in state o_t.\n      sampled_q_t = self._target_critic_network(\n          # Merge batch dimensions; to shape [N*B, ...].\n          snt.merge_leading_dims(tiled_o_t, num_dims=2),\n          snt.merge_leading_dims(sampled_actions, num_dims=2))\n\n      # Reshape Q-value samples back to original batch dimensions and average\n      # them to compute the TD-learning bootstrap target.\n      sampled_q_t = tf.reshape(sampled_q_t, (self._num_samples, -1))  # [N, B]\n      q_t = tf.reduce_mean(sampled_q_t, axis=0)  # [B]\n\n      # Compute online critic value of a_tm1 in state o_tm1.\n      q_tm1 = self._critic_network(o_tm1, transitions.action)  # [B, 1]\n      q_tm1 = tf.squeeze(q_tm1, axis=-1)  # [B]; necessary for trfl.td_learning.\n\n      # Critic loss.\n      critic_loss = trfl.td_learning(q_tm1, transitions.reward,\n                                     discount * transitions.discount, q_t).loss\n      critic_loss = tf.reduce_mean(critic_loss)\n\n      # Actor learning.\n      policy_loss, policy_stats = self._policy_loss_module(\n          online_action_distribution=online_action_distribution,\n          target_action_distribution=target_action_distribution,\n          actions=sampled_actions,\n          q_values=sampled_q_t)\n\n    # For clarity, explicitly define which variables are trained by which loss.\n    critic_trainable_variables = (\n        # In this agent, the critic loss trains the observation network.\n        self._observation_network.trainable_variables +\n        self._critic_network.trainable_variables)\n    policy_trainable_variables = self._policy_network.trainable_variables\n    # The following are the MPO dual variables, stored in the loss module.\n    dual_trainable_variables = self._policy_loss_module.trainable_variables\n\n    # Compute gradients.\n    critic_gradients = tape.gradient(critic_loss, critic_trainable_variables)\n    policy_gradients, dual_gradients = tape.gradient(\n        policy_loss, (policy_trainable_variables, dual_trainable_variables))\n\n    # Delete the tape manually because of the persistent=True flag.\n    del tape\n\n    # Maybe clip gradients.\n    if self._clipping:\n      policy_gradients = tuple(tf.clip_by_global_norm(policy_gradients, 40.)[0])\n      critic_gradients = tuple(tf.clip_by_global_norm(critic_gradients, 40.)[0])\n\n    # Apply gradients.\n    self._critic_optimizer.apply(critic_gradients, critic_trainable_variables)\n    self._policy_optimizer.apply(policy_gradients, policy_trainable_variables)\n    self._dual_optimizer.apply(dual_gradients, dual_trainable_variables)\n\n    # Losses to track.\n    fetches = {\n        'critic_loss': critic_loss,\n        'policy_loss': policy_loss,\n    }\n    fetches.update(policy_stats)  # Log MPO stats.\n\n    return fetches",
  "def step(self):\n    # Run the learning step.\n    fetches = self._step()\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n    fetches.update(counts)\n\n    # Checkpoint and attempt to write the logs.\n    if self._checkpointer is not None:\n      self._checkpointer.save()\n    if self._snapshotter is not None:\n      self._snapshotter.save()\n    self._logger.write(fetches)",
  "def get_variables(self, names: List[str]) -> List[List[np.ndarray]]:\n    return [tf2_utils.to_numpy(self._variables[name]) for name in names]",
  "def make_networks(\n    action_spec: specs.BoundedArray,\n    policy_layer_sizes: Sequence[int] = (50, 50),\n    critic_layer_sizes: Sequence[int] = (50, 50),\n):\n  \"\"\"Creates networks used by the agent.\"\"\"\n\n  num_dimensions = np.prod(action_spec.shape, dtype=int)\n\n  observation_network = tf2_utils.batch_concat\n  policy_network = snt.Sequential([\n      networks.LayerNormMLP(policy_layer_sizes, activate_final=True),\n      networks.MultivariateNormalDiagHead(\n          num_dimensions,\n          tanh_mean=True,\n          init_scale=0.3,\n          fixed_scale=True,\n          use_tfd_independent=False)\n  ])\n  evaluator_network = snt.Sequential([\n      observation_network,\n      policy_network,\n      networks.StochasticMeanHead(),\n  ])\n  # The multiplexer concatenates the (maybe transformed) observations/actions.\n  multiplexer = networks.CriticMultiplexer(\n      action_network=networks.ClipToSpec(action_spec))\n  critic_network = snt.Sequential([\n      multiplexer,\n      networks.LayerNormMLP(critic_layer_sizes, activate_final=True),\n      networks.NearZeroInitializedLinear(1),\n  ])\n\n  return {\n      'policy': policy_network,\n      'critic': critic_network,\n      'observation': observation_network,\n      'evaluator': evaluator_network,\n  }",
  "class DistributedAgentTest(absltest.TestCase):\n  \"\"\"Simple integration/smoke test for the distributed agent.\"\"\"\n\n  def test_agent(self):\n\n    agent = mpo.DistributedMPO(\n        environment_factory=lambda x: fakes.ContinuousEnvironment(bounded=True),\n        network_factory=make_networks,\n        num_actors=2,\n        batch_size=32,\n        min_replay_size=32,\n        max_replay_size=1000,\n    )\n    program = agent.build()\n\n    (learner_node,) = program.groups['learner']\n    learner_node.disable_run()\n\n    lp.launch(program, launch_type='test_mt')\n\n    learner: acme.Learner = learner_node.create_handle().dereference()\n\n    for _ in range(5):\n      learner.step()",
  "def test_agent(self):\n\n    agent = mpo.DistributedMPO(\n        environment_factory=lambda x: fakes.ContinuousEnvironment(bounded=True),\n        network_factory=make_networks,\n        num_actors=2,\n        batch_size=32,\n        min_replay_size=32,\n        max_replay_size=1000,\n    )\n    program = agent.build()\n\n    (learner_node,) = program.groups['learner']\n    learner_node.disable_run()\n\n    lp.launch(program, launch_type='test_mt')\n\n    learner: acme.Learner = learner_node.create_handle().dereference()\n\n    for _ in range(5):\n      learner.step()",
  "def make_networks(\n    action_spec,\n    policy_layer_sizes=(10, 10),\n    critic_layer_sizes=(10, 10),\n):\n  \"\"\"Creates networks used by the agent.\"\"\"\n\n  num_dimensions = np.prod(action_spec.shape, dtype=int)\n  critic_layer_sizes = list(critic_layer_sizes) + [1]\n\n  policy_network = snt.Sequential([\n      networks.LayerNormMLP(policy_layer_sizes),\n      networks.MultivariateNormalDiagHead(num_dimensions)\n  ])\n  critic_network = networks.CriticMultiplexer(\n      critic_network=networks.LayerNormMLP(critic_layer_sizes))\n\n  return {\n      'policy': policy_network,\n      'critic': critic_network,\n  }",
  "class MPOTest(absltest.TestCase):\n\n  def test_mpo(self):\n    # Create a fake environment to test with.\n    environment = fakes.ContinuousEnvironment(episode_length=10, bounded=False)\n    spec = specs.make_environment_spec(environment)\n\n    # Create networks.\n    agent_networks = make_networks(spec.actions)\n\n    # Construct the agent.\n    agent = mpo.MPO(\n        spec,\n        policy_network=agent_networks['policy'],\n        critic_network=agent_networks['critic'],\n        batch_size=10,\n        samples_per_insert=2,\n        min_replay_size=10)\n\n    # Try running the environment loop. We have no assertions here because all\n    # we care about is that the agent runs without raising any errors.\n    loop = acme.EnvironmentLoop(environment, agent)\n    loop.run(num_episodes=2)",
  "def test_mpo(self):\n    # Create a fake environment to test with.\n    environment = fakes.ContinuousEnvironment(episode_length=10, bounded=False)\n    spec = specs.make_environment_spec(environment)\n\n    # Create networks.\n    agent_networks = make_networks(spec.actions)\n\n    # Construct the agent.\n    agent = mpo.MPO(\n        spec,\n        policy_network=agent_networks['policy'],\n        critic_network=agent_networks['critic'],\n        batch_size=10,\n        samples_per_insert=2,\n        min_replay_size=10)\n\n    # Try running the environment loop. We have no assertions here because all\n    # we care about is that the agent runs without raising any errors.\n    loop = acme.EnvironmentLoop(environment, agent)\n    loop.run(num_episodes=2)",
  "class DistributedMPO:\n  \"\"\"Program definition for MPO.\"\"\"\n\n  def __init__(\n      self,\n      environment_factory: Callable[[bool], dm_env.Environment],\n      network_factory: Callable[[specs.BoundedArray], Dict[str, snt.Module]],\n      num_actors: int = 1,\n      num_caches: int = 0,\n      environment_spec: Optional[specs.EnvironmentSpec] = None,\n      batch_size: int = 256,\n      prefetch_size: int = 4,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1000000,\n      samples_per_insert: Optional[float] = 32.0,\n      n_step: int = 5,\n      num_samples: int = 20,\n      additional_discount: float = 0.99,\n      target_policy_update_period: int = 100,\n      target_critic_update_period: int = 100,\n      variable_update_period: int = 1000,\n      policy_loss_factory: Optional[Callable[[], snt.Module]] = None,\n      max_actor_steps: Optional[int] = None,\n      log_every: float = 10.0,\n  ):\n\n    if environment_spec is None:\n      environment_spec = specs.make_environment_spec(environment_factory(False))\n\n    self._environment_factory = environment_factory\n    self._network_factory = network_factory\n    self._policy_loss_factory = policy_loss_factory\n    self._environment_spec = environment_spec\n    self._num_actors = num_actors\n    self._num_caches = num_caches\n    self._batch_size = batch_size\n    self._prefetch_size = prefetch_size\n    self._min_replay_size = min_replay_size\n    self._max_replay_size = max_replay_size\n    self._samples_per_insert = samples_per_insert\n    self._n_step = n_step\n    self._additional_discount = additional_discount\n    self._num_samples = num_samples\n    self._target_policy_update_period = target_policy_update_period\n    self._target_critic_update_period = target_critic_update_period\n    self._variable_update_period = variable_update_period\n    self._max_actor_steps = max_actor_steps\n    self._log_every = log_every\n\n  def replay(self):\n    \"\"\"The replay storage.\"\"\"\n    if self._samples_per_insert is not None:\n      # Create enough of an error buffer to give a 10% tolerance in rate.\n      samples_per_insert_tolerance = 0.1 * self._samples_per_insert\n      error_buffer = self._min_replay_size * samples_per_insert_tolerance\n\n      limiter = reverb.rate_limiters.SampleToInsertRatio(\n          min_size_to_sample=self._min_replay_size,\n          samples_per_insert=self._samples_per_insert,\n          error_buffer=error_buffer)\n    else:\n      limiter = reverb.rate_limiters.MinSize(\n          min_size_to_sample=self._min_replay_size)\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=self._max_replay_size,\n        rate_limiter=limiter,\n        signature=adders.NStepTransitionAdder.signature(\n            self._environment_spec))\n    return [replay_table]\n\n  def counter(self):\n    return tf2_savers.CheckpointingRunner(counting.Counter(),\n                                          time_delta_minutes=1,\n                                          subdirectory='counter')\n\n  def coordinator(self, counter: counting.Counter, max_actor_steps: int):\n    return lp_utils.StepsLimiter(counter, max_actor_steps)\n\n  def learner(\n      self,\n      replay: reverb.Client,\n      counter: counting.Counter,\n  ):\n    \"\"\"The Learning part of the agent.\"\"\"\n\n    act_spec = self._environment_spec.actions\n    obs_spec = self._environment_spec.observations\n\n    # Create online and target networks.\n    online_networks = self._network_factory(act_spec)\n    target_networks = self._network_factory(act_spec)\n\n    # Make sure observation networks are Sonnet Modules.\n    observation_network = online_networks.get('observation', tf.identity)\n    observation_network = tf2_utils.to_sonnet_module(observation_network)\n    online_networks['observation'] = observation_network\n    target_observation_network = target_networks.get('observation', tf.identity)\n    target_observation_network = tf2_utils.to_sonnet_module(\n        target_observation_network)\n    target_networks['observation'] = target_observation_network\n\n    # Get embedding spec and create observation network variables.\n    emb_spec = tf2_utils.create_variables(observation_network, [obs_spec])\n\n    tf2_utils.create_variables(online_networks['policy'], [emb_spec])\n    tf2_utils.create_variables(online_networks['critic'], [emb_spec, act_spec])\n    tf2_utils.create_variables(target_networks['observation'], [obs_spec])\n    tf2_utils.create_variables(target_networks['policy'], [emb_spec])\n    tf2_utils.create_variables(target_networks['critic'], [emb_spec, act_spec])\n\n    # The dataset object to learn from.\n    dataset = datasets.make_reverb_dataset(\n        server_address=replay.server_address)\n    dataset = dataset.batch(self._batch_size, drop_remainder=True)\n    dataset = dataset.prefetch(self._prefetch_size)\n\n    # Create a counter and logger for bookkeeping steps and performance.\n    counter = counting.Counter(counter, 'learner')\n    logger = loggers.make_default_logger(\n        'learner', time_delta=self._log_every, steps_key='learner_steps')\n\n    # Create policy loss module if a factory is passed.\n    if self._policy_loss_factory:\n      policy_loss_module = self._policy_loss_factory()\n    else:\n      policy_loss_module = None\n\n    # Return the learning agent.\n    return learning.MPOLearner(\n        policy_network=online_networks['policy'],\n        critic_network=online_networks['critic'],\n        observation_network=observation_network,\n        target_policy_network=target_networks['policy'],\n        target_critic_network=target_networks['critic'],\n        target_observation_network=target_observation_network,\n        discount=self._additional_discount,\n        num_samples=self._num_samples,\n        target_policy_update_period=self._target_policy_update_period,\n        target_critic_update_period=self._target_critic_update_period,\n        policy_loss_module=policy_loss_module,\n        dataset=dataset,\n        counter=counter,\n        logger=logger)\n\n  def actor(\n      self,\n      replay: reverb.Client,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ) -> acme.EnvironmentLoop:\n    \"\"\"The actor process.\"\"\"\n\n    action_spec = self._environment_spec.actions\n    observation_spec = self._environment_spec.observations\n\n    # Create environment and target networks to act with.\n    environment = self._environment_factory(False)\n    agent_networks = self._network_factory(action_spec)\n\n    # Create a stochastic behavior policy.\n    behavior_modules = [\n        agent_networks.get('observation', tf.identity),\n        agent_networks.get('policy'),\n        networks.StochasticSamplingHead()\n    ]\n    behavior_network = snt.Sequential(behavior_modules)\n\n    # Ensure network variables are created.\n    tf2_utils.create_variables(behavior_network, [observation_spec])\n    policy_variables = {'policy': behavior_network.variables}\n\n    # Create the variable client responsible for keeping the actor up-to-date.\n    variable_client = tf2_variable_utils.VariableClient(\n        variable_source,\n        policy_variables,\n        update_period=self._variable_update_period)\n\n    # Make sure not to use a random policy after checkpoint restoration by\n    # assigning variables before running the environment loop.\n    variable_client.update_and_wait()\n\n    # Component to add things into replay.\n    adder = adders.NStepTransitionAdder(\n        client=replay,\n        n_step=self._n_step,\n        discount=self._additional_discount)\n\n    # Create the agent.\n    actor = actors.FeedForwardActor(\n        policy_network=behavior_network,\n        adder=adder,\n        variable_client=variable_client)\n\n    # Create logger and counter; actors will not spam bigtable.\n    counter = counting.Counter(counter, 'actor')\n    logger = loggers.make_default_logger(\n        'actor',\n        save_data=False,\n        time_delta=self._log_every,\n        steps_key='actor_steps')\n\n    # Create the run loop and return it.\n    return acme.EnvironmentLoop(environment, actor, counter, logger)\n\n  def evaluator(\n      self,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ):\n    \"\"\"The evaluation process.\"\"\"\n\n    action_spec = self._environment_spec.actions\n    observation_spec = self._environment_spec.observations\n\n    # Create environment and target networks to act with.\n    environment = self._environment_factory(True)\n    agent_networks = self._network_factory(action_spec)\n\n    # Create a stochastic behavior policy.\n    evaluator_modules = [\n        agent_networks.get('observation', tf.identity),\n        agent_networks.get('policy'),\n        networks.StochasticMeanHead(),\n    ]\n\n    if isinstance(action_spec, specs.BoundedArray):\n      evaluator_modules += [networks.ClipToSpec(action_spec)]\n    evaluator_network = snt.Sequential(evaluator_modules)\n\n    # Ensure network variables are created.\n    tf2_utils.create_variables(evaluator_network, [observation_spec])\n    policy_variables = {'policy': evaluator_network.variables}\n\n    # Create the variable client responsible for keeping the actor up-to-date.\n    variable_client = tf2_variable_utils.VariableClient(\n        variable_source,\n        policy_variables,\n        update_period=self._variable_update_period)\n\n    # Make sure not to evaluate a random actor by assigning variables before\n    # running the environment loop.\n    variable_client.update_and_wait()\n\n    # Create the agent.\n    evaluator = actors.FeedForwardActor(\n        policy_network=evaluator_network, variable_client=variable_client)\n\n    # Create logger and counter.\n    counter = counting.Counter(counter, 'evaluator')\n    logger = loggers.make_default_logger(\n        'evaluator', time_delta=self._log_every, steps_key='evaluator_steps')\n\n    # Create the run loop and return it.\n    return acme.EnvironmentLoop(environment, evaluator, counter, logger)\n\n  def build(self, name='mpo'):\n    \"\"\"Build the distributed agent topology.\"\"\"\n    program = lp.Program(name=name)\n\n    with program.group('replay'):\n      replay = program.add_node(lp.ReverbNode(self.replay))\n\n    with program.group('counter'):\n      counter = program.add_node(lp.CourierNode(self.counter))\n\n      if self._max_actor_steps:\n        _ = program.add_node(\n            lp.CourierNode(self.coordinator, counter, self._max_actor_steps))\n\n    with program.group('learner'):\n      learner = program.add_node(\n          lp.CourierNode(self.learner, replay, counter))\n\n    with program.group('evaluator'):\n      program.add_node(\n          lp.CourierNode(self.evaluator, learner, counter))\n\n    if not self._num_caches:\n      # Use our learner as a single variable source.\n      sources = [learner]\n    else:\n      with program.group('cacher'):\n        # Create a set of learner caches.\n        sources = []\n        for _ in range(self._num_caches):\n          cacher = program.add_node(\n              lp.CacherNode(\n                  learner, refresh_interval_ms=2000, stale_after_ms=4000))\n          sources.append(cacher)\n\n    with program.group('actor'):\n      # Add actors which pull round-robin from our variable sources.\n      for actor_id in range(self._num_actors):\n        source = sources[actor_id % len(sources)]\n        program.add_node(lp.CourierNode(self.actor, replay, source, counter))\n\n    return program",
  "def __init__(\n      self,\n      environment_factory: Callable[[bool], dm_env.Environment],\n      network_factory: Callable[[specs.BoundedArray], Dict[str, snt.Module]],\n      num_actors: int = 1,\n      num_caches: int = 0,\n      environment_spec: Optional[specs.EnvironmentSpec] = None,\n      batch_size: int = 256,\n      prefetch_size: int = 4,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1000000,\n      samples_per_insert: Optional[float] = 32.0,\n      n_step: int = 5,\n      num_samples: int = 20,\n      additional_discount: float = 0.99,\n      target_policy_update_period: int = 100,\n      target_critic_update_period: int = 100,\n      variable_update_period: int = 1000,\n      policy_loss_factory: Optional[Callable[[], snt.Module]] = None,\n      max_actor_steps: Optional[int] = None,\n      log_every: float = 10.0,\n  ):\n\n    if environment_spec is None:\n      environment_spec = specs.make_environment_spec(environment_factory(False))\n\n    self._environment_factory = environment_factory\n    self._network_factory = network_factory\n    self._policy_loss_factory = policy_loss_factory\n    self._environment_spec = environment_spec\n    self._num_actors = num_actors\n    self._num_caches = num_caches\n    self._batch_size = batch_size\n    self._prefetch_size = prefetch_size\n    self._min_replay_size = min_replay_size\n    self._max_replay_size = max_replay_size\n    self._samples_per_insert = samples_per_insert\n    self._n_step = n_step\n    self._additional_discount = additional_discount\n    self._num_samples = num_samples\n    self._target_policy_update_period = target_policy_update_period\n    self._target_critic_update_period = target_critic_update_period\n    self._variable_update_period = variable_update_period\n    self._max_actor_steps = max_actor_steps\n    self._log_every = log_every",
  "def replay(self):\n    \"\"\"The replay storage.\"\"\"\n    if self._samples_per_insert is not None:\n      # Create enough of an error buffer to give a 10% tolerance in rate.\n      samples_per_insert_tolerance = 0.1 * self._samples_per_insert\n      error_buffer = self._min_replay_size * samples_per_insert_tolerance\n\n      limiter = reverb.rate_limiters.SampleToInsertRatio(\n          min_size_to_sample=self._min_replay_size,\n          samples_per_insert=self._samples_per_insert,\n          error_buffer=error_buffer)\n    else:\n      limiter = reverb.rate_limiters.MinSize(\n          min_size_to_sample=self._min_replay_size)\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=self._max_replay_size,\n        rate_limiter=limiter,\n        signature=adders.NStepTransitionAdder.signature(\n            self._environment_spec))\n    return [replay_table]",
  "def counter(self):\n    return tf2_savers.CheckpointingRunner(counting.Counter(),\n                                          time_delta_minutes=1,\n                                          subdirectory='counter')",
  "def coordinator(self, counter: counting.Counter, max_actor_steps: int):\n    return lp_utils.StepsLimiter(counter, max_actor_steps)",
  "def learner(\n      self,\n      replay: reverb.Client,\n      counter: counting.Counter,\n  ):\n    \"\"\"The Learning part of the agent.\"\"\"\n\n    act_spec = self._environment_spec.actions\n    obs_spec = self._environment_spec.observations\n\n    # Create online and target networks.\n    online_networks = self._network_factory(act_spec)\n    target_networks = self._network_factory(act_spec)\n\n    # Make sure observation networks are Sonnet Modules.\n    observation_network = online_networks.get('observation', tf.identity)\n    observation_network = tf2_utils.to_sonnet_module(observation_network)\n    online_networks['observation'] = observation_network\n    target_observation_network = target_networks.get('observation', tf.identity)\n    target_observation_network = tf2_utils.to_sonnet_module(\n        target_observation_network)\n    target_networks['observation'] = target_observation_network\n\n    # Get embedding spec and create observation network variables.\n    emb_spec = tf2_utils.create_variables(observation_network, [obs_spec])\n\n    tf2_utils.create_variables(online_networks['policy'], [emb_spec])\n    tf2_utils.create_variables(online_networks['critic'], [emb_spec, act_spec])\n    tf2_utils.create_variables(target_networks['observation'], [obs_spec])\n    tf2_utils.create_variables(target_networks['policy'], [emb_spec])\n    tf2_utils.create_variables(target_networks['critic'], [emb_spec, act_spec])\n\n    # The dataset object to learn from.\n    dataset = datasets.make_reverb_dataset(\n        server_address=replay.server_address)\n    dataset = dataset.batch(self._batch_size, drop_remainder=True)\n    dataset = dataset.prefetch(self._prefetch_size)\n\n    # Create a counter and logger for bookkeeping steps and performance.\n    counter = counting.Counter(counter, 'learner')\n    logger = loggers.make_default_logger(\n        'learner', time_delta=self._log_every, steps_key='learner_steps')\n\n    # Create policy loss module if a factory is passed.\n    if self._policy_loss_factory:\n      policy_loss_module = self._policy_loss_factory()\n    else:\n      policy_loss_module = None\n\n    # Return the learning agent.\n    return learning.MPOLearner(\n        policy_network=online_networks['policy'],\n        critic_network=online_networks['critic'],\n        observation_network=observation_network,\n        target_policy_network=target_networks['policy'],\n        target_critic_network=target_networks['critic'],\n        target_observation_network=target_observation_network,\n        discount=self._additional_discount,\n        num_samples=self._num_samples,\n        target_policy_update_period=self._target_policy_update_period,\n        target_critic_update_period=self._target_critic_update_period,\n        policy_loss_module=policy_loss_module,\n        dataset=dataset,\n        counter=counter,\n        logger=logger)",
  "def actor(\n      self,\n      replay: reverb.Client,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ) -> acme.EnvironmentLoop:\n    \"\"\"The actor process.\"\"\"\n\n    action_spec = self._environment_spec.actions\n    observation_spec = self._environment_spec.observations\n\n    # Create environment and target networks to act with.\n    environment = self._environment_factory(False)\n    agent_networks = self._network_factory(action_spec)\n\n    # Create a stochastic behavior policy.\n    behavior_modules = [\n        agent_networks.get('observation', tf.identity),\n        agent_networks.get('policy'),\n        networks.StochasticSamplingHead()\n    ]\n    behavior_network = snt.Sequential(behavior_modules)\n\n    # Ensure network variables are created.\n    tf2_utils.create_variables(behavior_network, [observation_spec])\n    policy_variables = {'policy': behavior_network.variables}\n\n    # Create the variable client responsible for keeping the actor up-to-date.\n    variable_client = tf2_variable_utils.VariableClient(\n        variable_source,\n        policy_variables,\n        update_period=self._variable_update_period)\n\n    # Make sure not to use a random policy after checkpoint restoration by\n    # assigning variables before running the environment loop.\n    variable_client.update_and_wait()\n\n    # Component to add things into replay.\n    adder = adders.NStepTransitionAdder(\n        client=replay,\n        n_step=self._n_step,\n        discount=self._additional_discount)\n\n    # Create the agent.\n    actor = actors.FeedForwardActor(\n        policy_network=behavior_network,\n        adder=adder,\n        variable_client=variable_client)\n\n    # Create logger and counter; actors will not spam bigtable.\n    counter = counting.Counter(counter, 'actor')\n    logger = loggers.make_default_logger(\n        'actor',\n        save_data=False,\n        time_delta=self._log_every,\n        steps_key='actor_steps')\n\n    # Create the run loop and return it.\n    return acme.EnvironmentLoop(environment, actor, counter, logger)",
  "def evaluator(\n      self,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ):\n    \"\"\"The evaluation process.\"\"\"\n\n    action_spec = self._environment_spec.actions\n    observation_spec = self._environment_spec.observations\n\n    # Create environment and target networks to act with.\n    environment = self._environment_factory(True)\n    agent_networks = self._network_factory(action_spec)\n\n    # Create a stochastic behavior policy.\n    evaluator_modules = [\n        agent_networks.get('observation', tf.identity),\n        agent_networks.get('policy'),\n        networks.StochasticMeanHead(),\n    ]\n\n    if isinstance(action_spec, specs.BoundedArray):\n      evaluator_modules += [networks.ClipToSpec(action_spec)]\n    evaluator_network = snt.Sequential(evaluator_modules)\n\n    # Ensure network variables are created.\n    tf2_utils.create_variables(evaluator_network, [observation_spec])\n    policy_variables = {'policy': evaluator_network.variables}\n\n    # Create the variable client responsible for keeping the actor up-to-date.\n    variable_client = tf2_variable_utils.VariableClient(\n        variable_source,\n        policy_variables,\n        update_period=self._variable_update_period)\n\n    # Make sure not to evaluate a random actor by assigning variables before\n    # running the environment loop.\n    variable_client.update_and_wait()\n\n    # Create the agent.\n    evaluator = actors.FeedForwardActor(\n        policy_network=evaluator_network, variable_client=variable_client)\n\n    # Create logger and counter.\n    counter = counting.Counter(counter, 'evaluator')\n    logger = loggers.make_default_logger(\n        'evaluator', time_delta=self._log_every, steps_key='evaluator_steps')\n\n    # Create the run loop and return it.\n    return acme.EnvironmentLoop(environment, evaluator, counter, logger)",
  "def build(self, name='mpo'):\n    \"\"\"Build the distributed agent topology.\"\"\"\n    program = lp.Program(name=name)\n\n    with program.group('replay'):\n      replay = program.add_node(lp.ReverbNode(self.replay))\n\n    with program.group('counter'):\n      counter = program.add_node(lp.CourierNode(self.counter))\n\n      if self._max_actor_steps:\n        _ = program.add_node(\n            lp.CourierNode(self.coordinator, counter, self._max_actor_steps))\n\n    with program.group('learner'):\n      learner = program.add_node(\n          lp.CourierNode(self.learner, replay, counter))\n\n    with program.group('evaluator'):\n      program.add_node(\n          lp.CourierNode(self.evaluator, learner, counter))\n\n    if not self._num_caches:\n      # Use our learner as a single variable source.\n      sources = [learner]\n    else:\n      with program.group('cacher'):\n        # Create a set of learner caches.\n        sources = []\n        for _ in range(self._num_caches):\n          cacher = program.add_node(\n              lp.CacherNode(\n                  learner, refresh_interval_ms=2000, stale_after_ms=4000))\n          sources.append(cacher)\n\n    with program.group('actor'):\n      # Add actors which pull round-robin from our variable sources.\n      for actor_id in range(self._num_actors):\n        source = sources[actor_id % len(sources)]\n        program.add_node(lp.CourierNode(self.actor, replay, source, counter))\n\n    return program",
  "class MCTS(agent.Agent):\n  \"\"\"A single-process MCTS agent.\"\"\"\n\n  def __init__(\n      self,\n      network: snt.Module,\n      model: models.Model,\n      optimizer: snt.Optimizer,\n      n_step: int,\n      discount: float,\n      replay_capacity: int,\n      num_simulations: int,\n      environment_spec: specs.EnvironmentSpec,\n      batch_size: int,\n  ):\n\n    extra_spec = {\n        'pi':\n            specs.Array(\n                shape=(environment_spec.actions.num_values,), dtype=np.float32)\n    }\n    # Create a replay server for storing transitions.\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=replay_capacity,\n        rate_limiter=reverb.rate_limiters.MinSize(1),\n        signature=adders.NStepTransitionAdder.signature(\n            environment_spec, extra_spec))\n    self._server = reverb.Server([replay_table], port=None)\n\n    # The adder is used to insert observations into replay.\n    address = f'localhost:{self._server.port}'\n    adder = adders.NStepTransitionAdder(\n        client=reverb.Client(address),\n        n_step=n_step,\n        discount=discount)\n\n    # The dataset provides an interface to sample from replay.\n    dataset = datasets.make_reverb_dataset(server_address=address)\n    dataset = dataset.batch(batch_size, drop_remainder=True)\n\n    tf2_utils.create_variables(network, [environment_spec.observations])\n\n    # Now create the agent components: actor & learner.\n    actor = acting.MCTSActor(\n        environment_spec=environment_spec,\n        model=model,\n        network=network,\n        discount=discount,\n        adder=adder,\n        num_simulations=num_simulations,\n    )\n\n    learner = learning.AZLearner(\n        network=network,\n        optimizer=optimizer,\n        dataset=dataset,\n        discount=discount,\n    )\n\n    # The parent class combines these together into one 'agent'.\n    super().__init__(\n        actor=actor,\n        learner=learner,\n        min_observations=10,\n        observations_per_step=1,\n    )",
  "def __init__(\n      self,\n      network: snt.Module,\n      model: models.Model,\n      optimizer: snt.Optimizer,\n      n_step: int,\n      discount: float,\n      replay_capacity: int,\n      num_simulations: int,\n      environment_spec: specs.EnvironmentSpec,\n      batch_size: int,\n  ):\n\n    extra_spec = {\n        'pi':\n            specs.Array(\n                shape=(environment_spec.actions.num_values,), dtype=np.float32)\n    }\n    # Create a replay server for storing transitions.\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=replay_capacity,\n        rate_limiter=reverb.rate_limiters.MinSize(1),\n        signature=adders.NStepTransitionAdder.signature(\n            environment_spec, extra_spec))\n    self._server = reverb.Server([replay_table], port=None)\n\n    # The adder is used to insert observations into replay.\n    address = f'localhost:{self._server.port}'\n    adder = adders.NStepTransitionAdder(\n        client=reverb.Client(address),\n        n_step=n_step,\n        discount=discount)\n\n    # The dataset provides an interface to sample from replay.\n    dataset = datasets.make_reverb_dataset(server_address=address)\n    dataset = dataset.batch(batch_size, drop_remainder=True)\n\n    tf2_utils.create_variables(network, [environment_spec.observations])\n\n    # Now create the agent components: actor & learner.\n    actor = acting.MCTSActor(\n        environment_spec=environment_spec,\n        model=model,\n        network=network,\n        discount=discount,\n        adder=adder,\n        num_simulations=num_simulations,\n    )\n\n    learner = learning.AZLearner(\n        network=network,\n        optimizer=optimizer,\n        dataset=dataset,\n        discount=discount,\n    )\n\n    # The parent class combines these together into one 'agent'.\n    super().__init__(\n        actor=actor,\n        learner=learner,\n        min_observations=10,\n        observations_per_step=1,\n    )",
  "class AZLearner(acme.Learner):\n  \"\"\"AlphaZero-style learning.\"\"\"\n\n  def __init__(\n      self,\n      network: snt.Module,\n      optimizer: snt.Optimizer,\n      dataset: tf.data.Dataset,\n      discount: float,\n      logger: Optional[loggers.Logger] = None,\n      counter: Optional[counting.Counter] = None,\n  ):\n\n    # Logger and counter for tracking statistics / writing out to terminal.\n    self._counter = counting.Counter(counter, 'learner')\n    self._logger = logger or loggers.TerminalLogger('learner', time_delta=30.)\n\n    # Internalize components.\n    # TODO(b/155086959): Fix type stubs and remove.\n    self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n    self._optimizer = optimizer\n    self._network = network\n    self._variables = network.trainable_variables\n    self._discount = np.float32(discount)\n\n  @tf.function\n  def _step(self) -> tf.Tensor:\n    \"\"\"Do a step of SGD on the loss.\"\"\"\n\n    inputs = next(self._iterator)\n    o_t, _, r_t, d_t, o_tp1, extras = inputs.data\n    pi_t = extras['pi']\n\n    with tf.GradientTape() as tape:\n      # Forward the network on the two states in the transition.\n      logits, value = self._network(o_t)\n      _, target_value = self._network(o_tp1)\n      target_value = tf.stop_gradient(target_value)\n\n      # Value loss is simply on-policy TD learning.\n      value_loss = tf.square(r_t + self._discount * d_t * target_value - value)\n\n      # Policy loss distills MCTS policy into the policy network.\n      policy_loss = tf.nn.softmax_cross_entropy_with_logits(\n          logits=logits, labels=pi_t)\n\n      # Compute gradients.\n      loss = tf.reduce_mean(value_loss + policy_loss)\n      gradients = tape.gradient(loss, self._network.trainable_variables)\n\n    self._optimizer.apply(gradients, self._network.trainable_variables)\n\n    return loss\n\n  def step(self):\n    \"\"\"Does a step of SGD and logs the results.\"\"\"\n    loss = self._step()\n    self._logger.write({'loss': loss})\n\n  def get_variables(self, names: List[str]) -> List[List[np.ndarray]]:\n    \"\"\"Exposes the variables for actors to update from.\"\"\"\n    return tf2_utils.to_numpy(self._variables)",
  "def __init__(\n      self,\n      network: snt.Module,\n      optimizer: snt.Optimizer,\n      dataset: tf.data.Dataset,\n      discount: float,\n      logger: Optional[loggers.Logger] = None,\n      counter: Optional[counting.Counter] = None,\n  ):\n\n    # Logger and counter for tracking statistics / writing out to terminal.\n    self._counter = counting.Counter(counter, 'learner')\n    self._logger = logger or loggers.TerminalLogger('learner', time_delta=30.)\n\n    # Internalize components.\n    # TODO(b/155086959): Fix type stubs and remove.\n    self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n    self._optimizer = optimizer\n    self._network = network\n    self._variables = network.trainable_variables\n    self._discount = np.float32(discount)",
  "def _step(self) -> tf.Tensor:\n    \"\"\"Do a step of SGD on the loss.\"\"\"\n\n    inputs = next(self._iterator)\n    o_t, _, r_t, d_t, o_tp1, extras = inputs.data\n    pi_t = extras['pi']\n\n    with tf.GradientTape() as tape:\n      # Forward the network on the two states in the transition.\n      logits, value = self._network(o_t)\n      _, target_value = self._network(o_tp1)\n      target_value = tf.stop_gradient(target_value)\n\n      # Value loss is simply on-policy TD learning.\n      value_loss = tf.square(r_t + self._discount * d_t * target_value - value)\n\n      # Policy loss distills MCTS policy into the policy network.\n      policy_loss = tf.nn.softmax_cross_entropy_with_logits(\n          logits=logits, labels=pi_t)\n\n      # Compute gradients.\n      loss = tf.reduce_mean(value_loss + policy_loss)\n      gradients = tape.gradient(loss, self._network.trainable_variables)\n\n    self._optimizer.apply(gradients, self._network.trainable_variables)\n\n    return loss",
  "def step(self):\n    \"\"\"Does a step of SGD and logs the results.\"\"\"\n    loss = self._step()\n    self._logger.write({'loss': loss})",
  "def get_variables(self, names: List[str]) -> List[List[np.ndarray]]:\n    \"\"\"Exposes the variables for actors to update from.\"\"\"\n    return tf2_utils.to_numpy(self._variables)",
  "class TestSearch(parameterized.TestCase):\n\n  @parameterized.parameters([\n      'puct',\n      'bfs',\n  ])\n  def test_catch(self, policy_type: Text):\n    env = catch.Catch(rows=2, seed=1)\n    num_actions = env.action_spec().num_values\n    model = simulator.Simulator(env)\n    eval_fn = lambda _: (np.ones(num_actions) / num_actions, 0.)\n\n    timestep = env.reset()\n    model.reset()\n\n    search_policy = search.bfs if policy_type == 'bfs' else search.puct\n\n    root = search.mcts(\n        observation=timestep.observation,\n        model=model,\n        search_policy=search_policy,\n        evaluation=eval_fn,\n        num_simulations=100,\n        num_actions=num_actions)\n\n    values = np.array([c.value for c in root.children.values()])\n    best_action = search.argmax(values)\n\n    if env._paddle_x > env._ball_x:\n      self.assertEqual(best_action, 0)\n    if env._paddle_x == env._ball_x:\n      self.assertEqual(best_action, 1)\n    if env._paddle_x < env._ball_x:\n      self.assertEqual(best_action, 2)",
  "def test_catch(self, policy_type: Text):\n    env = catch.Catch(rows=2, seed=1)\n    num_actions = env.action_spec().num_values\n    model = simulator.Simulator(env)\n    eval_fn = lambda _: (np.ones(num_actions) / num_actions, 0.)\n\n    timestep = env.reset()\n    model.reset()\n\n    search_policy = search.bfs if policy_type == 'bfs' else search.puct\n\n    root = search.mcts(\n        observation=timestep.observation,\n        model=model,\n        search_policy=search_policy,\n        evaluation=eval_fn,\n        num_simulations=100,\n        num_actions=num_actions)\n\n    values = np.array([c.value for c in root.children.values()])\n    best_action = search.argmax(values)\n\n    if env._paddle_x > env._ball_x:\n      self.assertEqual(best_action, 0)\n    if env._paddle_x == env._ball_x:\n      self.assertEqual(best_action, 1)\n    if env._paddle_x < env._ball_x:\n      self.assertEqual(best_action, 2)",
  "class Node:\n  \"\"\"A MCTS node.\"\"\"\n\n  reward: float = 0.\n  visit_count: int = 0\n  terminal: bool = False\n  prior: float = 1.\n  total_value: float = 0.\n  children: Dict[types.Action, 'Node'] = dataclasses.field(default_factory=dict)\n\n  def expand(self, prior: np.ndarray):\n    \"\"\"Expands this node, adding child nodes.\"\"\"\n    assert prior.ndim == 1  # Prior should be a flat vector.\n    for a, p in enumerate(prior):\n      self.children[a] = Node(prior=p)\n\n  @property\n  def value(self) -> types.Value:  # Q(s, a)\n    \"\"\"Returns the value from this node.\"\"\"\n    if self.visit_count:\n      return self.total_value / self.visit_count\n    return 0.\n\n  @property\n  def children_visits(self) -> np.ndarray:\n    \"\"\"Return array of visit counts of visited children.\"\"\"\n    return np.array([c.visit_count for c in self.children.values()])\n\n  @property\n  def children_values(self) -> np.ndarray:\n    \"\"\"Return array of values of visited children.\"\"\"\n    return np.array([c.value for c in self.children.values()])",
  "def mcts(\n    observation: types.Observation,\n    model: models.Model,\n    search_policy: SearchPolicy,\n    evaluation: types.EvaluationFn,\n    num_simulations: int,\n    num_actions: int,\n    discount: float = 1.,\n    dirichlet_alpha: float = 1,\n    exploration_fraction: float = 0.,\n) -> Node:\n  \"\"\"Does Monte Carlo tree search (MCTS), AlphaZero style.\"\"\"\n\n  # Evaluate the prior policy for this state.\n  prior, value = evaluation(observation)\n  assert prior.shape == (num_actions,)\n\n  # Add exploration noise to the prior.\n  noise = np.random.dirichlet(alpha=[dirichlet_alpha] * num_actions)\n  prior = prior * (1 - exploration_fraction) + noise * exploration_fraction\n\n  # Create a fresh tree search.\n  root = Node()\n  root.expand(prior)\n\n  # Save the model state so that we can reset it for each simulation.\n  model.save_checkpoint()\n  for _ in range(num_simulations):\n    # Start a new simulation from the top.\n    trajectory = [root]\n    node = root\n\n    # Generate a trajectory.\n    timestep = None\n    while node.children:\n      # Select an action according to the search policy.\n      action = search_policy(node)\n\n      # Point the node at the corresponding child.\n      node = node.children[action]\n\n      # Step the simulator and add this timestep to the node.\n      timestep = model.step(action)\n      node.reward = timestep.reward or 0.\n      node.terminal = timestep.last()\n      trajectory.append(node)\n\n    if timestep is None:\n      raise ValueError('Generated an empty rollout; this should not happen.')\n\n    # Calculate the bootstrap for leaf nodes.\n    if node.terminal:\n      # If terminal, there is no bootstrap value.\n      value = 0.\n    else:\n      # Otherwise, bootstrap from this node with our value function.\n      prior, value = evaluation(timestep.observation)\n\n      # We also want to expand this node for next time.\n      node.expand(prior)\n\n    # Load the saved model state.\n    model.load_checkpoint()\n\n    # Monte Carlo back-up with bootstrap from value function.\n    ret = value\n    while trajectory:\n      # Pop off the latest node in the trajectory.\n      node = trajectory.pop()\n\n      # Accumulate the discounted return\n      ret *= discount\n      ret += node.reward\n\n      # Update the node.\n      node.total_value += ret\n      node.visit_count += 1\n\n  return root",
  "def bfs(node: Node) -> types.Action:\n  \"\"\"Breadth-first search policy.\"\"\"\n  visit_counts = np.array([c.visit_count for c in node.children.values()])\n  return argmax(-visit_counts)",
  "def puct(node: Node, ucb_scaling: float = 1.) -> types.Action:\n  \"\"\"PUCT search policy, i.e. UCT with 'prior' policy.\"\"\"\n  # Action values Q(s,a).\n  value_scores = np.array([child.value for child in node.children.values()])\n  check_numerics(value_scores)\n\n  # Policy prior P(s,a).\n  priors = np.array([child.prior for child in node.children.values()])\n  check_numerics(priors)\n\n  # Visit ratios.\n  visit_ratios = np.array([\n      np.sqrt(node.visit_count) / (child.visit_count + 1)\n      for child in node.children.values()\n  ])\n  check_numerics(visit_ratios)\n\n  # Combine.\n  puct_scores = value_scores + ucb_scaling * priors * visit_ratios\n  return argmax(puct_scores)",
  "def visit_count_policy(root: Node, temperature: float = 1.) -> types.Probs:\n  \"\"\"Probability weighted by visit^{1/temp} of children nodes.\"\"\"\n  visits = root.children_visits\n  if np.sum(visits) == 0:  # uniform policy for zero visits\n    visits += 1\n  rescaled_visits = visits**(1 / temperature)\n  probs = rescaled_visits / np.sum(rescaled_visits)\n  check_numerics(probs)\n\n  return probs",
  "def argmax(values: np.ndarray) -> types.Action:\n  \"\"\"Argmax with random tie-breaking.\"\"\"\n  check_numerics(values)\n  max_value = np.max(values)\n  return np.int32(np.random.choice(np.flatnonzero(values == max_value)))",
  "def check_numerics(values: np.ndarray):\n  \"\"\"Raises a ValueError if any of the inputs are NaN or Inf.\"\"\"\n  if not np.isfinite(values).all():\n    raise ValueError('check_numerics failed. Inputs: {}. '.format(values))",
  "def expand(self, prior: np.ndarray):\n    \"\"\"Expands this node, adding child nodes.\"\"\"\n    assert prior.ndim == 1  # Prior should be a flat vector.\n    for a, p in enumerate(prior):\n      self.children[a] = Node(prior=p)",
  "def value(self) -> types.Value:  # Q(s, a)\n    \"\"\"Returns the value from this node.\"\"\"\n    if self.visit_count:\n      return self.total_value / self.visit_count\n    return 0.",
  "def children_visits(self) -> np.ndarray:\n    \"\"\"Return array of visit counts of visited children.\"\"\"\n    return np.array([c.visit_count for c in self.children.values()])",
  "def children_values(self) -> np.ndarray:\n    \"\"\"Return array of values of visited children.\"\"\"\n    return np.array([c.value for c in self.children.values()])",
  "class MCTSActor(acme.Actor):\n  \"\"\"Executes a policy- and value-network guided MCTS search.\"\"\"\n\n  _prev_timestep: dm_env.TimeStep\n\n  def __init__(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      model: models.Model,\n      network: snt.Module,\n      discount: float,\n      num_simulations: int,\n      adder: Optional[adders.Adder] = None,\n      variable_client: Optional[tf2_variable_utils.VariableClient] = None,\n  ):\n\n    # Internalize components: model, network, data sink and variable source.\n    self._model = model\n    self._network = tf.function(network)\n    self._variable_client = variable_client\n    self._adder = adder\n\n    # Internalize hyperparameters.\n    self._num_actions = environment_spec.actions.num_values\n    self._num_simulations = num_simulations\n    self._actions = list(range(self._num_actions))\n    self._discount = discount\n\n    # We need to save the policy so as to add it to replay on the next step.\n    self._probs = np.ones(\n        shape=(self._num_actions,), dtype=np.float32) / self._num_actions\n\n  def _forward(\n      self, observation: types.Observation) -> Tuple[types.Probs, types.Value]:\n    \"\"\"Performs a forward pass of the policy-value network.\"\"\"\n    logits, value = self._network(tf.expand_dims(observation, axis=0))\n\n    # Convert to numpy & take softmax.\n    logits = logits.numpy().squeeze(axis=0)\n    value = value.numpy().item()\n    probs = special.softmax(logits)\n\n    return probs, value\n\n  def select_action(self, observation: types.Observation) -> types.Action:\n    \"\"\"Computes the agent's policy via MCTS.\"\"\"\n    if self._model.needs_reset:\n      self._model.reset(observation)\n\n    # Compute a fresh MCTS plan.\n    root = search.mcts(\n        observation,\n        model=self._model,\n        search_policy=search.puct,\n        evaluation=self._forward,\n        num_simulations=self._num_simulations,\n        num_actions=self._num_actions,\n        discount=self._discount,\n    )\n\n    # The agent's policy is softmax w.r.t. the *visit counts* as in AlphaZero.\n    probs = search.visit_count_policy(root)\n    action = np.int32(np.random.choice(self._actions, p=probs))\n\n    # Save the policy probs so that we can add them to replay in `observe()`.\n    self._probs = probs.astype(np.float32)\n\n    return action\n\n  def update(self, wait: bool = False):\n    \"\"\"Fetches the latest variables from the variable source, if needed.\"\"\"\n    if self._variable_client:\n      self._variable_client.update(wait)\n\n  def observe_first(self, timestep: dm_env.TimeStep):\n    self._prev_timestep = timestep\n    if self._adder:\n      self._adder.add_first(timestep)\n\n  def observe(self, action: types.Action, next_timestep: dm_env.TimeStep):\n    \"\"\"Updates the agent's internal model and adds the transition to replay.\"\"\"\n    self._model.update(self._prev_timestep, action, next_timestep)\n\n    self._prev_timestep = next_timestep\n\n    if self._adder:\n      self._adder.add(action, next_timestep, extras={'pi': self._probs})",
  "def __init__(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      model: models.Model,\n      network: snt.Module,\n      discount: float,\n      num_simulations: int,\n      adder: Optional[adders.Adder] = None,\n      variable_client: Optional[tf2_variable_utils.VariableClient] = None,\n  ):\n\n    # Internalize components: model, network, data sink and variable source.\n    self._model = model\n    self._network = tf.function(network)\n    self._variable_client = variable_client\n    self._adder = adder\n\n    # Internalize hyperparameters.\n    self._num_actions = environment_spec.actions.num_values\n    self._num_simulations = num_simulations\n    self._actions = list(range(self._num_actions))\n    self._discount = discount\n\n    # We need to save the policy so as to add it to replay on the next step.\n    self._probs = np.ones(\n        shape=(self._num_actions,), dtype=np.float32) / self._num_actions",
  "def _forward(\n      self, observation: types.Observation) -> Tuple[types.Probs, types.Value]:\n    \"\"\"Performs a forward pass of the policy-value network.\"\"\"\n    logits, value = self._network(tf.expand_dims(observation, axis=0))\n\n    # Convert to numpy & take softmax.\n    logits = logits.numpy().squeeze(axis=0)\n    value = value.numpy().item()\n    probs = special.softmax(logits)\n\n    return probs, value",
  "def select_action(self, observation: types.Observation) -> types.Action:\n    \"\"\"Computes the agent's policy via MCTS.\"\"\"\n    if self._model.needs_reset:\n      self._model.reset(observation)\n\n    # Compute a fresh MCTS plan.\n    root = search.mcts(\n        observation,\n        model=self._model,\n        search_policy=search.puct,\n        evaluation=self._forward,\n        num_simulations=self._num_simulations,\n        num_actions=self._num_actions,\n        discount=self._discount,\n    )\n\n    # The agent's policy is softmax w.r.t. the *visit counts* as in AlphaZero.\n    probs = search.visit_count_policy(root)\n    action = np.int32(np.random.choice(self._actions, p=probs))\n\n    # Save the policy probs so that we can add them to replay in `observe()`.\n    self._probs = probs.astype(np.float32)\n\n    return action",
  "def update(self, wait: bool = False):\n    \"\"\"Fetches the latest variables from the variable source, if needed.\"\"\"\n    if self._variable_client:\n      self._variable_client.update(wait)",
  "def observe_first(self, timestep: dm_env.TimeStep):\n    self._prev_timestep = timestep\n    if self._adder:\n      self._adder.add_first(timestep)",
  "def observe(self, action: types.Action, next_timestep: dm_env.TimeStep):\n    \"\"\"Updates the agent's internal model and adds the transition to replay.\"\"\"\n    self._model.update(self._prev_timestep, action, next_timestep)\n\n    self._prev_timestep = next_timestep\n\n    if self._adder:\n      self._adder.add(action, next_timestep, extras={'pi': self._probs})",
  "class MCTSTest(absltest.TestCase):\n\n  def test_mcts(self):\n    # Create a fake environment to test with.\n    num_actions = 5\n    environment = fakes.DiscreteEnvironment(\n        num_actions=num_actions,\n        num_observations=10,\n        obs_dtype=np.float32,\n        episode_length=10)\n    spec = specs.make_environment_spec(environment)\n\n    network = snt.Sequential([\n        snt.Flatten(),\n        snt.nets.MLP([50, 50]),\n        networks.PolicyValueHead(spec.actions.num_values),\n    ])\n    model = simulator.Simulator(environment)\n    optimizer = snt.optimizers.Adam(1e-3)\n\n    # Construct the agent.\n    agent = mcts.MCTS(\n        environment_spec=spec,\n        network=network,\n        model=model,\n        optimizer=optimizer,\n        n_step=1,\n        discount=1.,\n        replay_capacity=100,\n        num_simulations=10,\n        batch_size=10)\n\n    # Try running the environment loop. We have no assertions here because all\n    # we care about is that the agent runs without raising any errors.\n    loop = acme.EnvironmentLoop(environment, agent)\n    loop.run(num_episodes=2)",
  "def test_mcts(self):\n    # Create a fake environment to test with.\n    num_actions = 5\n    environment = fakes.DiscreteEnvironment(\n        num_actions=num_actions,\n        num_observations=10,\n        obs_dtype=np.float32,\n        episode_length=10)\n    spec = specs.make_environment_spec(environment)\n\n    network = snt.Sequential([\n        snt.Flatten(),\n        snt.nets.MLP([50, 50]),\n        networks.PolicyValueHead(spec.actions.num_values),\n    ])\n    model = simulator.Simulator(environment)\n    optimizer = snt.optimizers.Adam(1e-3)\n\n    # Construct the agent.\n    agent = mcts.MCTS(\n        environment_spec=spec,\n        network=network,\n        model=model,\n        optimizer=optimizer,\n        n_step=1,\n        discount=1.,\n        replay_capacity=100,\n        num_simulations=10,\n        batch_size=10)\n\n    # Try running the environment loop. We have no assertions here because all\n    # we care about is that the agent runs without raising any errors.\n    loop = acme.EnvironmentLoop(environment, agent)\n    loop.run(num_episodes=2)",
  "class DistributedMCTS:\n  \"\"\"Distributed MCTS agent.\"\"\"\n\n  def __init__(\n      self,\n      environment_factory: Callable[[], dm_env.Environment],\n      network_factory: Callable[[specs.DiscreteArray], snt.Module],\n      model_factory: Callable[[specs.EnvironmentSpec], models.Model],\n      num_actors: int,\n      num_simulations: int = 50,\n      batch_size: int = 256,\n      prefetch_size: int = 4,\n      target_update_period: int = 100,\n      samples_per_insert: float = 32.0,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1000000,\n      importance_sampling_exponent: float = 0.2,\n      priority_exponent: float = 0.6,\n      n_step: int = 5,\n      learning_rate: float = 1e-3,\n      discount: float = 0.99,\n      environment_spec: Optional[specs.EnvironmentSpec] = None,\n      save_logs: bool = False,\n      variable_update_period: int = 1000,\n  ):\n\n    if environment_spec is None:\n      environment_spec = specs.make_environment_spec(environment_factory())\n\n    # These 'factories' create the relevant components on the workers.\n    self._environment_factory = environment_factory\n    self._network_factory = network_factory\n    self._model_factory = model_factory\n\n    # Internalize hyperparameters.\n    self._num_actors = num_actors\n    self._num_simulations = num_simulations\n    self._env_spec = environment_spec\n    self._batch_size = batch_size\n    self._prefetch_size = prefetch_size\n    self._target_update_period = target_update_period\n    self._samples_per_insert = samples_per_insert\n    self._min_replay_size = min_replay_size\n    self._max_replay_size = max_replay_size\n    self._importance_sampling_exponent = importance_sampling_exponent\n    self._priority_exponent = priority_exponent\n    self._n_step = n_step\n    self._learning_rate = learning_rate\n    self._discount = discount\n    self._save_logs = save_logs\n    self._variable_update_period = variable_update_period\n\n  def replay(self):\n    \"\"\"The replay storage worker.\"\"\"\n    limiter = reverb.rate_limiters.SampleToInsertRatio(\n        min_size_to_sample=self._min_replay_size,\n        samples_per_insert=self._samples_per_insert,\n        error_buffer=self._batch_size)\n    extra_spec = {\n        'pi':\n            specs.Array(\n                shape=(self._env_spec.actions.num_values,), dtype='float32')\n    }\n    signature = adders.NStepTransitionAdder.signature(self._env_spec,\n                                                      extra_spec)\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=self._max_replay_size,\n        rate_limiter=limiter,\n        signature=signature)\n    return [replay_table]\n\n  def learner(self, replay: reverb.Client, counter: counting.Counter):\n    \"\"\"The learning part of the agent.\"\"\"\n    # Create the networks.\n    network = self._network_factory(self._env_spec.actions)\n\n    tf2_utils.create_variables(network, [self._env_spec.observations])\n\n    # The dataset object to learn from.\n    dataset = datasets.make_reverb_dataset(\n        server_address=replay.server_address,\n        batch_size=self._batch_size,\n        prefetch_size=self._prefetch_size)\n\n    # Create the optimizer.\n    optimizer = snt.optimizers.Adam(self._learning_rate)\n\n    # Return the learning agent.\n    return learning.AZLearner(\n        network=network,\n        discount=self._discount,\n        dataset=dataset,\n        optimizer=optimizer,\n        counter=counter,\n    )\n\n  def actor(\n      self,\n      replay: reverb.Client,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ) -> acme.EnvironmentLoop:\n    \"\"\"The actor process.\"\"\"\n\n    # Build environment, model, network.\n    environment = self._environment_factory()\n    network = self._network_factory(self._env_spec.actions)\n    model = self._model_factory(self._env_spec)\n\n    # Create variable client for communicating with the learner.\n    tf2_utils.create_variables(network, [self._env_spec.observations])\n    variable_client = tf2_variable_utils.VariableClient(\n        client=variable_source,\n        variables={'network': network.trainable_variables},\n        update_period=self._variable_update_period)\n\n    # Component to add things into replay.\n    adder = adders.NStepTransitionAdder(\n        client=replay,\n        n_step=self._n_step,\n        discount=self._discount,\n    )\n\n    # Create the agent.\n    actor = acting.MCTSActor(\n        environment_spec=self._env_spec,\n        model=model,\n        network=network,\n        discount=self._discount,\n        adder=adder,\n        variable_client=variable_client,\n        num_simulations=self._num_simulations,\n    )\n\n    # Create the loop to connect environment and agent.\n    return acme.EnvironmentLoop(environment, actor, counter)\n\n  def evaluator(\n      self,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ):\n    \"\"\"The evaluation process.\"\"\"\n\n    # Build environment, model, network.\n    environment = self._environment_factory()\n    network = self._network_factory(self._env_spec.actions)\n    model = self._model_factory(self._env_spec)\n\n    # Create variable client for communicating with the learner.\n    tf2_utils.create_variables(network, [self._env_spec.observations])\n    variable_client = tf2_variable_utils.VariableClient(\n        client=variable_source,\n        variables={'policy': network.trainable_variables},\n        update_period=self._variable_update_period)\n\n    # Create the agent.\n    actor = acting.MCTSActor(\n        environment_spec=self._env_spec,\n        model=model,\n        network=network,\n        discount=self._discount,\n        variable_client=variable_client,\n        num_simulations=self._num_simulations,\n    )\n\n    # Create the run loop and return it.\n    logger = loggers.make_default_logger('evaluator')\n    return acme.EnvironmentLoop(\n        environment, actor, counter=counter, logger=logger)\n\n  def build(self, name='MCTS'):\n    \"\"\"Builds the distributed agent topology.\"\"\"\n    program = lp.Program(name=name)\n\n    with program.group('replay'):\n      replay = program.add_node(lp.ReverbNode(self.replay), label='replay')\n\n    with program.group('counter'):\n      counter = program.add_node(\n          lp.CourierNode(counting.Counter), label='counter')\n\n    with program.group('learner'):\n      learner = program.add_node(\n          lp.CourierNode(self.learner, replay, counter), label='learner')\n\n    with program.group('evaluator'):\n      program.add_node(\n          lp.CourierNode(self.evaluator, learner, counter), label='evaluator')\n\n    with program.group('actor'):\n      program.add_node(\n          lp.CourierNode(self.actor, replay, learner, counter), label='actor')\n\n    return program",
  "def __init__(\n      self,\n      environment_factory: Callable[[], dm_env.Environment],\n      network_factory: Callable[[specs.DiscreteArray], snt.Module],\n      model_factory: Callable[[specs.EnvironmentSpec], models.Model],\n      num_actors: int,\n      num_simulations: int = 50,\n      batch_size: int = 256,\n      prefetch_size: int = 4,\n      target_update_period: int = 100,\n      samples_per_insert: float = 32.0,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1000000,\n      importance_sampling_exponent: float = 0.2,\n      priority_exponent: float = 0.6,\n      n_step: int = 5,\n      learning_rate: float = 1e-3,\n      discount: float = 0.99,\n      environment_spec: Optional[specs.EnvironmentSpec] = None,\n      save_logs: bool = False,\n      variable_update_period: int = 1000,\n  ):\n\n    if environment_spec is None:\n      environment_spec = specs.make_environment_spec(environment_factory())\n\n    # These 'factories' create the relevant components on the workers.\n    self._environment_factory = environment_factory\n    self._network_factory = network_factory\n    self._model_factory = model_factory\n\n    # Internalize hyperparameters.\n    self._num_actors = num_actors\n    self._num_simulations = num_simulations\n    self._env_spec = environment_spec\n    self._batch_size = batch_size\n    self._prefetch_size = prefetch_size\n    self._target_update_period = target_update_period\n    self._samples_per_insert = samples_per_insert\n    self._min_replay_size = min_replay_size\n    self._max_replay_size = max_replay_size\n    self._importance_sampling_exponent = importance_sampling_exponent\n    self._priority_exponent = priority_exponent\n    self._n_step = n_step\n    self._learning_rate = learning_rate\n    self._discount = discount\n    self._save_logs = save_logs\n    self._variable_update_period = variable_update_period",
  "def replay(self):\n    \"\"\"The replay storage worker.\"\"\"\n    limiter = reverb.rate_limiters.SampleToInsertRatio(\n        min_size_to_sample=self._min_replay_size,\n        samples_per_insert=self._samples_per_insert,\n        error_buffer=self._batch_size)\n    extra_spec = {\n        'pi':\n            specs.Array(\n                shape=(self._env_spec.actions.num_values,), dtype='float32')\n    }\n    signature = adders.NStepTransitionAdder.signature(self._env_spec,\n                                                      extra_spec)\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=self._max_replay_size,\n        rate_limiter=limiter,\n        signature=signature)\n    return [replay_table]",
  "def learner(self, replay: reverb.Client, counter: counting.Counter):\n    \"\"\"The learning part of the agent.\"\"\"\n    # Create the networks.\n    network = self._network_factory(self._env_spec.actions)\n\n    tf2_utils.create_variables(network, [self._env_spec.observations])\n\n    # The dataset object to learn from.\n    dataset = datasets.make_reverb_dataset(\n        server_address=replay.server_address,\n        batch_size=self._batch_size,\n        prefetch_size=self._prefetch_size)\n\n    # Create the optimizer.\n    optimizer = snt.optimizers.Adam(self._learning_rate)\n\n    # Return the learning agent.\n    return learning.AZLearner(\n        network=network,\n        discount=self._discount,\n        dataset=dataset,\n        optimizer=optimizer,\n        counter=counter,\n    )",
  "def actor(\n      self,\n      replay: reverb.Client,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ) -> acme.EnvironmentLoop:\n    \"\"\"The actor process.\"\"\"\n\n    # Build environment, model, network.\n    environment = self._environment_factory()\n    network = self._network_factory(self._env_spec.actions)\n    model = self._model_factory(self._env_spec)\n\n    # Create variable client for communicating with the learner.\n    tf2_utils.create_variables(network, [self._env_spec.observations])\n    variable_client = tf2_variable_utils.VariableClient(\n        client=variable_source,\n        variables={'network': network.trainable_variables},\n        update_period=self._variable_update_period)\n\n    # Component to add things into replay.\n    adder = adders.NStepTransitionAdder(\n        client=replay,\n        n_step=self._n_step,\n        discount=self._discount,\n    )\n\n    # Create the agent.\n    actor = acting.MCTSActor(\n        environment_spec=self._env_spec,\n        model=model,\n        network=network,\n        discount=self._discount,\n        adder=adder,\n        variable_client=variable_client,\n        num_simulations=self._num_simulations,\n    )\n\n    # Create the loop to connect environment and agent.\n    return acme.EnvironmentLoop(environment, actor, counter)",
  "def evaluator(\n      self,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ):\n    \"\"\"The evaluation process.\"\"\"\n\n    # Build environment, model, network.\n    environment = self._environment_factory()\n    network = self._network_factory(self._env_spec.actions)\n    model = self._model_factory(self._env_spec)\n\n    # Create variable client for communicating with the learner.\n    tf2_utils.create_variables(network, [self._env_spec.observations])\n    variable_client = tf2_variable_utils.VariableClient(\n        client=variable_source,\n        variables={'policy': network.trainable_variables},\n        update_period=self._variable_update_period)\n\n    # Create the agent.\n    actor = acting.MCTSActor(\n        environment_spec=self._env_spec,\n        model=model,\n        network=network,\n        discount=self._discount,\n        variable_client=variable_client,\n        num_simulations=self._num_simulations,\n    )\n\n    # Create the run loop and return it.\n    logger = loggers.make_default_logger('evaluator')\n    return acme.EnvironmentLoop(\n        environment, actor, counter=counter, logger=logger)",
  "def build(self, name='MCTS'):\n    \"\"\"Builds the distributed agent topology.\"\"\"\n    program = lp.Program(name=name)\n\n    with program.group('replay'):\n      replay = program.add_node(lp.ReverbNode(self.replay), label='replay')\n\n    with program.group('counter'):\n      counter = program.add_node(\n          lp.CourierNode(counting.Counter), label='counter')\n\n    with program.group('learner'):\n      learner = program.add_node(\n          lp.CourierNode(self.learner, replay, counter), label='learner')\n\n    with program.group('evaluator'):\n      program.add_node(\n          lp.CourierNode(self.evaluator, learner, counter), label='evaluator')\n\n    with program.group('actor'):\n      program.add_node(\n          lp.CourierNode(self.actor, replay, learner, counter), label='actor')\n\n    return program",
  "class Model(dm_env.Environment, abc.ABC):\n  \"\"\"Base (abstract) class for models used for planning via MCTS.\"\"\"\n\n  @abc.abstractmethod\n  def load_checkpoint(self):\n    \"\"\"Loads a saved model state, if it exists.\"\"\"\n\n  @abc.abstractmethod\n  def save_checkpoint(self):\n    \"\"\"Saves the model state so that we can reset it after a rollout.\"\"\"\n\n  @abc.abstractmethod\n  def update(\n      self,\n      timestep: dm_env.TimeStep,\n      action: types.Action,\n      next_timestep: dm_env.TimeStep,\n  ) -> dm_env.TimeStep:\n    \"\"\"Updates the model given an observation, action, reward, and discount.\"\"\"\n\n  @abc.abstractmethod\n  def reset(self, initial_state: Optional[types.Observation] = None):\n    \"\"\"Resets the model, optionally to an initial state.\"\"\"\n\n  @property\n  @abc.abstractmethod\n  def needs_reset(self) -> bool:\n    \"\"\"Returns whether or not the model needs to be reset.\"\"\"",
  "def load_checkpoint(self):\n    \"\"\"Loads a saved model state, if it exists.\"\"\"",
  "def save_checkpoint(self):\n    \"\"\"Saves the model state so that we can reset it after a rollout.\"\"\"",
  "def update(\n      self,\n      timestep: dm_env.TimeStep,\n      action: types.Action,\n      next_timestep: dm_env.TimeStep,\n  ) -> dm_env.TimeStep:\n    \"\"\"Updates the model given an observation, action, reward, and discount.\"\"\"",
  "def reset(self, initial_state: Optional[types.Observation] = None):\n    \"\"\"Resets the model, optionally to an initial state.\"\"\"",
  "def needs_reset(self) -> bool:\n    \"\"\"Returns whether or not the model needs to be reset.\"\"\"",
  "class Checkpoint:\n  \"\"\"Holds the checkpoint state for the environment simulator.\"\"\"\n  needs_reset: bool\n  environment: dm_env.Environment",
  "class Simulator(base.Model):\n  \"\"\"A simulator model, which wraps a copy of the true environment.\n\n  Assumptions:\n    - The environment (including RNG) is fully copyable via `deepcopy`.\n    - Environment dynamics (modulo episode resets) are deterministic.\n  \"\"\"\n\n  _checkpoint: Checkpoint\n  _env: dm_env.Environment\n\n  def __init__(self, env: dm_env.Environment):\n    # Make a 'checkpoint' copy env to save/load from when doing rollouts.\n    self._env = copy.deepcopy(env)\n    self._needs_reset = True\n    self.save_checkpoint()\n\n  def update(\n      self,\n      timestep: dm_env.TimeStep,\n      action: types.Action,\n      next_timestep: dm_env.TimeStep,\n  ) -> dm_env.TimeStep:\n    # Call update() once per 'real' experience to keep this env in sync.\n    return self.step(action)\n\n  def save_checkpoint(self):\n    self._checkpoint = Checkpoint(\n        needs_reset=self._needs_reset,\n        environment=copy.deepcopy(self._env),\n    )\n\n  def load_checkpoint(self):\n    self._env = copy.deepcopy(self._checkpoint.environment)\n    self._needs_reset = self._checkpoint.needs_reset\n\n  def step(self, action: types.Action) -> dm_env.TimeStep:\n    if self._needs_reset:\n      raise ValueError('This model needs to be explicitly reset.')\n    timestep = self._env.step(action)\n    self._needs_reset = timestep.last()\n    return timestep\n\n  def reset(self, *unused_args, **unused_kwargs):\n    self._needs_reset = False\n    return self._env.reset()\n\n  def observation_spec(self):\n    return self._env.observation_spec()\n\n  def action_spec(self):\n    return self._env.action_spec()\n\n  @property\n  def needs_reset(self) -> bool:\n    return self._needs_reset",
  "def __init__(self, env: dm_env.Environment):\n    # Make a 'checkpoint' copy env to save/load from when doing rollouts.\n    self._env = copy.deepcopy(env)\n    self._needs_reset = True\n    self.save_checkpoint()",
  "def update(\n      self,\n      timestep: dm_env.TimeStep,\n      action: types.Action,\n      next_timestep: dm_env.TimeStep,\n  ) -> dm_env.TimeStep:\n    # Call update() once per 'real' experience to keep this env in sync.\n    return self.step(action)",
  "def save_checkpoint(self):\n    self._checkpoint = Checkpoint(\n        needs_reset=self._needs_reset,\n        environment=copy.deepcopy(self._env),\n    )",
  "def load_checkpoint(self):\n    self._env = copy.deepcopy(self._checkpoint.environment)\n    self._needs_reset = self._checkpoint.needs_reset",
  "def step(self, action: types.Action) -> dm_env.TimeStep:\n    if self._needs_reset:\n      raise ValueError('This model needs to be explicitly reset.')\n    timestep = self._env.step(action)\n    self._needs_reset = timestep.last()\n    return timestep",
  "def reset(self, *unused_args, **unused_kwargs):\n    self._needs_reset = False\n    return self._env.reset()",
  "def observation_spec(self):\n    return self._env.observation_spec()",
  "def action_spec(self):\n    return self._env.action_spec()",
  "def needs_reset(self) -> bool:\n    return self._needs_reset",
  "class MLPTransitionModel(snt.Module):\n  \"\"\"This uses MLPs to model (s, a) -> (r, d, s').\"\"\"\n\n  def __init__(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      hidden_sizes: Tuple[int, ...],\n  ):\n    super(MLPTransitionModel, self).__init__(name='mlp_transition_model')\n\n    # Get num actions/observation shape.\n    self._num_actions = environment_spec.actions.num_values\n    self._input_shape = environment_spec.observations.shape\n    self._flat_shape = int(np.prod(self._input_shape))\n\n    # Prediction networks.\n    self._state_network = snt.Sequential([\n        snt.nets.MLP(hidden_sizes + (self._flat_shape,)),\n        snt.Reshape(self._input_shape)\n    ])\n    self._reward_network = snt.Sequential([\n        snt.nets.MLP(hidden_sizes + (1,)),\n        lambda r: tf.squeeze(r, axis=-1),\n    ])\n    self._discount_network = snt.Sequential([\n        snt.nets.MLP(hidden_sizes + (1,)),\n        lambda d: tf.squeeze(d, axis=-1),\n    ])\n\n  def __call__(self, state: tf.Tensor,\n               action: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n\n    embedded_state = snt.Flatten()(state)\n    embedded_action = tf.one_hot(action, depth=self._num_actions)\n\n    embedding = tf.concat([embedded_state, embedded_action], axis=-1)\n\n    # Predict the next state, reward, and termination.\n    next_state = self._state_network(embedding)\n    reward = self._reward_network(embedding)\n    discount_logits = self._discount_network(embedding)\n\n    return next_state, reward, discount_logits",
  "class MLPModel(base.Model):\n  \"\"\"A simple environment model.\"\"\"\n\n  _checkpoint: types.Observation\n  _state: types.Observation\n\n  def __init__(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      replay_capacity: int,\n      batch_size: int,\n      hidden_sizes: Tuple[int, ...],\n      learning_rate: float = 1e-3,\n      terminal_tol: float = 1e-3,\n  ):\n    self._obs_spec = environment_spec.observations\n    self._action_spec = environment_spec.actions\n    # Hyperparameters.\n    self._batch_size = batch_size\n    self._terminal_tol = terminal_tol\n\n    # Modelling\n    self._replay = replay.Replay(replay_capacity)\n    self._transition_model = MLPTransitionModel(environment_spec, hidden_sizes)\n    self._optimizer = snt.optimizers.Adam(learning_rate)\n    self._forward = tf.function(self._transition_model)\n    tf2_utils.create_variables(\n        self._transition_model, [self._obs_spec, self._action_spec])\n    self._variables = self._transition_model.trainable_variables\n\n    # Model state.\n    self._needs_reset = True\n\n  @tf.function\n  def _step(\n      self,\n      o_t: tf.Tensor,\n      a_t: tf.Tensor,\n      r_t: tf.Tensor,\n      d_t: tf.Tensor,\n      o_tp1: tf.Tensor,\n  ) -> tf.Tensor:\n\n    with tf.GradientTape() as tape:\n      next_state, reward, discount = self._transition_model(o_t, a_t)\n\n      state_loss = tf.square(next_state - o_tp1)\n      reward_loss = tf.square(reward - r_t)\n      discount_loss = tf.nn.sigmoid_cross_entropy_with_logits(d_t, discount)\n\n      loss = sum([\n          tf.reduce_mean(state_loss),\n          tf.reduce_mean(reward_loss),\n          tf.reduce_mean(discount_loss),\n      ])\n\n    gradients = tape.gradient(loss, self._variables)\n    self._optimizer.apply(gradients, self._variables)\n\n    return loss\n\n  def step(self, action: types.Action):\n    # Reset if required.\n    if self._needs_reset:\n      raise ValueError('Model must be reset with an initial timestep.')\n\n    # Step the model.\n    state, action = tf2_utils.add_batch_dim([self._state, action])\n    new_state, reward, discount_logits = [\n        x.numpy().squeeze(axis=0) for x in self._forward(state, action)\n    ]\n    discount = special.softmax(discount_logits)\n\n    # Save the resulting state for the next step.\n    self._state = new_state\n\n    # We threshold discount on a given tolerance.\n    if discount < self._terminal_tol:\n      self._needs_reset = True\n      return dm_env.termination(reward=reward, observation=self._state.copy())\n    return dm_env.transition(reward=reward, observation=self._state.copy())\n\n  def reset(self, initial_state: Optional[types.Observation] = None):\n    if initial_state is None:\n      raise ValueError('Model must be reset with an initial state.')\n    # We reset to an initial state that we are explicitly given.\n    # This allows us to handle environments with stochastic resets (e.g. Catch).\n    self._state = initial_state.copy()\n    self._needs_reset = False\n    return dm_env.restart(self._state)\n\n  def update(\n      self,\n      timestep: dm_env.TimeStep,\n      action: types.Action,\n      next_timestep: dm_env.TimeStep,\n  ) -> dm_env.TimeStep:\n    # Add the true transition to replay.\n    transition = [\n        timestep.observation,\n        action,\n        next_timestep.reward,\n        next_timestep.discount,\n        next_timestep.observation,\n    ]\n    self._replay.add(transition)\n\n    # Step the model to generate a synthetic transition.\n    ts = self.step(action)\n\n    # Copy the *true* state on update.\n    self._state = next_timestep.observation.copy()\n\n    if ts.last() or next_timestep.last():\n      # Model believes that a termination has happened.\n      # This will result in a crash during planning if the true environment\n      # didn't terminate here as well. So, we indicate that we need a reset.\n      self._needs_reset = True\n\n    # Sample from replay and do SGD.\n    if self._replay.size >= self._batch_size:\n      batch = self._replay.sample(self._batch_size)\n      self._step(*batch)\n\n    return ts\n\n  def save_checkpoint(self):\n    if self._needs_reset:\n      raise ValueError('Cannot save checkpoint: model must be reset first.')\n    self._checkpoint = self._state.copy()\n\n  def load_checkpoint(self):\n    self._needs_reset = False\n    self._state = self._checkpoint.copy()\n\n  def action_spec(self):\n    return self._action_spec\n\n  def observation_spec(self):\n    return self._obs_spec\n\n  @property\n  def needs_reset(self) -> bool:\n    return self._needs_reset",
  "def __init__(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      hidden_sizes: Tuple[int, ...],\n  ):\n    super(MLPTransitionModel, self).__init__(name='mlp_transition_model')\n\n    # Get num actions/observation shape.\n    self._num_actions = environment_spec.actions.num_values\n    self._input_shape = environment_spec.observations.shape\n    self._flat_shape = int(np.prod(self._input_shape))\n\n    # Prediction networks.\n    self._state_network = snt.Sequential([\n        snt.nets.MLP(hidden_sizes + (self._flat_shape,)),\n        snt.Reshape(self._input_shape)\n    ])\n    self._reward_network = snt.Sequential([\n        snt.nets.MLP(hidden_sizes + (1,)),\n        lambda r: tf.squeeze(r, axis=-1),\n    ])\n    self._discount_network = snt.Sequential([\n        snt.nets.MLP(hidden_sizes + (1,)),\n        lambda d: tf.squeeze(d, axis=-1),\n    ])",
  "def __call__(self, state: tf.Tensor,\n               action: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n\n    embedded_state = snt.Flatten()(state)\n    embedded_action = tf.one_hot(action, depth=self._num_actions)\n\n    embedding = tf.concat([embedded_state, embedded_action], axis=-1)\n\n    # Predict the next state, reward, and termination.\n    next_state = self._state_network(embedding)\n    reward = self._reward_network(embedding)\n    discount_logits = self._discount_network(embedding)\n\n    return next_state, reward, discount_logits",
  "def __init__(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      replay_capacity: int,\n      batch_size: int,\n      hidden_sizes: Tuple[int, ...],\n      learning_rate: float = 1e-3,\n      terminal_tol: float = 1e-3,\n  ):\n    self._obs_spec = environment_spec.observations\n    self._action_spec = environment_spec.actions\n    # Hyperparameters.\n    self._batch_size = batch_size\n    self._terminal_tol = terminal_tol\n\n    # Modelling\n    self._replay = replay.Replay(replay_capacity)\n    self._transition_model = MLPTransitionModel(environment_spec, hidden_sizes)\n    self._optimizer = snt.optimizers.Adam(learning_rate)\n    self._forward = tf.function(self._transition_model)\n    tf2_utils.create_variables(\n        self._transition_model, [self._obs_spec, self._action_spec])\n    self._variables = self._transition_model.trainable_variables\n\n    # Model state.\n    self._needs_reset = True",
  "def _step(\n      self,\n      o_t: tf.Tensor,\n      a_t: tf.Tensor,\n      r_t: tf.Tensor,\n      d_t: tf.Tensor,\n      o_tp1: tf.Tensor,\n  ) -> tf.Tensor:\n\n    with tf.GradientTape() as tape:\n      next_state, reward, discount = self._transition_model(o_t, a_t)\n\n      state_loss = tf.square(next_state - o_tp1)\n      reward_loss = tf.square(reward - r_t)\n      discount_loss = tf.nn.sigmoid_cross_entropy_with_logits(d_t, discount)\n\n      loss = sum([\n          tf.reduce_mean(state_loss),\n          tf.reduce_mean(reward_loss),\n          tf.reduce_mean(discount_loss),\n      ])\n\n    gradients = tape.gradient(loss, self._variables)\n    self._optimizer.apply(gradients, self._variables)\n\n    return loss",
  "def step(self, action: types.Action):\n    # Reset if required.\n    if self._needs_reset:\n      raise ValueError('Model must be reset with an initial timestep.')\n\n    # Step the model.\n    state, action = tf2_utils.add_batch_dim([self._state, action])\n    new_state, reward, discount_logits = [\n        x.numpy().squeeze(axis=0) for x in self._forward(state, action)\n    ]\n    discount = special.softmax(discount_logits)\n\n    # Save the resulting state for the next step.\n    self._state = new_state\n\n    # We threshold discount on a given tolerance.\n    if discount < self._terminal_tol:\n      self._needs_reset = True\n      return dm_env.termination(reward=reward, observation=self._state.copy())\n    return dm_env.transition(reward=reward, observation=self._state.copy())",
  "def reset(self, initial_state: Optional[types.Observation] = None):\n    if initial_state is None:\n      raise ValueError('Model must be reset with an initial state.')\n    # We reset to an initial state that we are explicitly given.\n    # This allows us to handle environments with stochastic resets (e.g. Catch).\n    self._state = initial_state.copy()\n    self._needs_reset = False\n    return dm_env.restart(self._state)",
  "def update(\n      self,\n      timestep: dm_env.TimeStep,\n      action: types.Action,\n      next_timestep: dm_env.TimeStep,\n  ) -> dm_env.TimeStep:\n    # Add the true transition to replay.\n    transition = [\n        timestep.observation,\n        action,\n        next_timestep.reward,\n        next_timestep.discount,\n        next_timestep.observation,\n    ]\n    self._replay.add(transition)\n\n    # Step the model to generate a synthetic transition.\n    ts = self.step(action)\n\n    # Copy the *true* state on update.\n    self._state = next_timestep.observation.copy()\n\n    if ts.last() or next_timestep.last():\n      # Model believes that a termination has happened.\n      # This will result in a crash during planning if the true environment\n      # didn't terminate here as well. So, we indicate that we need a reset.\n      self._needs_reset = True\n\n    # Sample from replay and do SGD.\n    if self._replay.size >= self._batch_size:\n      batch = self._replay.sample(self._batch_size)\n      self._step(*batch)\n\n    return ts",
  "def save_checkpoint(self):\n    if self._needs_reset:\n      raise ValueError('Cannot save checkpoint: model must be reset first.')\n    self._checkpoint = self._state.copy()",
  "def load_checkpoint(self):\n    self._needs_reset = False\n    self._state = self._checkpoint.copy()",
  "def action_spec(self):\n    return self._action_spec",
  "def observation_spec(self):\n    return self._obs_spec",
  "def needs_reset(self) -> bool:\n    return self._needs_reset",
  "class SimulatorTest(absltest.TestCase):\n\n  def _check_equal(self, a: dm_env.TimeStep, b: dm_env.TimeStep):\n    self.assertEqual(a.reward, b.reward)\n    self.assertEqual(a.discount, b.discount)\n    self.assertEqual(a.step_type, b.step_type)\n    np.testing.assert_array_equal(a.observation, b.observation)\n\n  def test_simulator_fidelity(self):\n    \"\"\"Tests whether the simulator match the ground truth.\"\"\"\n\n    # Given an environment.\n    env = catch.Catch()\n\n    # If we instantiate a simulator 'model' of this environment.\n    model = simulator.Simulator(env)\n\n    # Then the model and environment should always agree as we step them.\n    num_actions = env.action_spec().num_values\n    for _ in range(10):\n      true_timestep = env.reset()\n      self.assertTrue(model.needs_reset)\n      model_timestep = model.reset()\n      self.assertFalse(model.needs_reset)\n      self._check_equal(true_timestep, model_timestep)\n\n      while not true_timestep.last():\n        action = np.random.randint(num_actions)\n        true_timestep = env.step(action)\n        model_timestep = model.step(action)\n        self._check_equal(true_timestep, model_timestep)\n\n  def test_checkpointing(self):\n    \"\"\"Tests whether checkpointing restores the state correctly.\"\"\"\n    # Given an environment, and a model based on this environment.\n    model = simulator.Simulator(catch.Catch())\n    num_actions = model.action_spec().num_values\n\n    model.reset()\n\n    # Now, we save a checkpoint.\n    model.save_checkpoint()\n\n    ts = model.step(1)\n\n    # Step the model once and load the checkpoint.\n    timestep = model.step(np.random.randint(num_actions))\n    model.load_checkpoint()\n    self._check_equal(ts, model.step(1))\n\n    while not timestep.last():\n      timestep = model.step(np.random.randint(num_actions))\n\n    # The model should require a reset.\n    self.assertTrue(model.needs_reset)\n\n    # Once we load checkpoint, the model should no longer require reset.\n    model.load_checkpoint()\n    self.assertFalse(model.needs_reset)\n\n    # Further steps should agree with the original environment state.\n    self._check_equal(ts, model.step(1))",
  "def _check_equal(self, a: dm_env.TimeStep, b: dm_env.TimeStep):\n    self.assertEqual(a.reward, b.reward)\n    self.assertEqual(a.discount, b.discount)\n    self.assertEqual(a.step_type, b.step_type)\n    np.testing.assert_array_equal(a.observation, b.observation)",
  "def test_simulator_fidelity(self):\n    \"\"\"Tests whether the simulator match the ground truth.\"\"\"\n\n    # Given an environment.\n    env = catch.Catch()\n\n    # If we instantiate a simulator 'model' of this environment.\n    model = simulator.Simulator(env)\n\n    # Then the model and environment should always agree as we step them.\n    num_actions = env.action_spec().num_values\n    for _ in range(10):\n      true_timestep = env.reset()\n      self.assertTrue(model.needs_reset)\n      model_timestep = model.reset()\n      self.assertFalse(model.needs_reset)\n      self._check_equal(true_timestep, model_timestep)\n\n      while not true_timestep.last():\n        action = np.random.randint(num_actions)\n        true_timestep = env.step(action)\n        model_timestep = model.step(action)\n        self._check_equal(true_timestep, model_timestep)",
  "def test_checkpointing(self):\n    \"\"\"Tests whether checkpointing restores the state correctly.\"\"\"\n    # Given an environment, and a model based on this environment.\n    model = simulator.Simulator(catch.Catch())\n    num_actions = model.action_spec().num_values\n\n    model.reset()\n\n    # Now, we save a checkpoint.\n    model.save_checkpoint()\n\n    ts = model.step(1)\n\n    # Step the model once and load the checkpoint.\n    timestep = model.step(np.random.randint(num_actions))\n    model.load_checkpoint()\n    self._check_equal(ts, model.step(1))\n\n    while not timestep.last():\n      timestep = model.step(np.random.randint(num_actions))\n\n    # The model should require a reset.\n    self.assertTrue(model.needs_reset)\n\n    # Once we load checkpoint, the model should no longer require reset.\n    model.load_checkpoint()\n    self.assertFalse(model.needs_reset)\n\n    # Further steps should agree with the original environment state.\n    self._check_equal(ts, model.step(1))",
  "class R2D2(agent.Agent):\n  \"\"\"R2D2 Agent.\n\n  This implements a single-process R2D2 agent. This is a Q-learning algorithm\n  that generates data via a (epislon-greedy) behavior policy, inserts\n  trajectories into a replay buffer, and periodically updates the policy (and\n  as a result the behavior) by sampling from this buffer.\n  \"\"\"\n\n  def __init__(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      network: snt.RNNCore,\n      burn_in_length: int,\n      trace_length: int,\n      replay_period: int,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      discount: float = 0.99,\n      batch_size: int = 32,\n      prefetch_size: int = tf.data.experimental.AUTOTUNE,\n      target_update_period: int = 100,\n      importance_sampling_exponent: float = 0.2,\n      priority_exponent: float = 0.6,\n      epsilon: float = 0.01,\n      learning_rate: float = 1e-3,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1000000,\n      samples_per_insert: float = 32.0,\n      store_lstm_state: bool = True,\n      max_priority_weight: float = 0.9,\n      checkpoint: bool = True,\n  ):\n\n    if store_lstm_state:\n      extra_spec = {\n          'core_state': tf2_utils.squeeze_batch_dim(network.initial_state(1)),\n      }\n    else:\n      extra_spec = ()\n\n    sequence_length = burn_in_length + trace_length + 1\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Prioritized(priority_exponent),\n        remover=reverb.selectors.Fifo(),\n        max_size=max_replay_size,\n        rate_limiter=reverb.rate_limiters.MinSize(min_size_to_sample=1),\n        signature=adders.SequenceAdder.signature(\n            environment_spec, extra_spec, sequence_length=sequence_length))\n    self._server = reverb.Server([replay_table], port=None)\n    address = f'localhost:{self._server.port}'\n\n    # Component to add things into replay.\n    adder = adders.SequenceAdder(\n        client=reverb.Client(address),\n        period=replay_period,\n        sequence_length=sequence_length,\n    )\n\n    # The dataset object to learn from.\n    dataset = datasets.make_reverb_dataset(\n        server_address=address,\n        batch_size=batch_size,\n        prefetch_size=prefetch_size)\n\n    target_network = copy.deepcopy(network)\n    tf2_utils.create_variables(network, [environment_spec.observations])\n    tf2_utils.create_variables(target_network, [environment_spec.observations])\n\n    learner = learning.R2D2Learner(\n        environment_spec=environment_spec,\n        network=network,\n        target_network=target_network,\n        burn_in_length=burn_in_length,\n        sequence_length=sequence_length,\n        dataset=dataset,\n        reverb_client=reverb.TFClient(address),\n        counter=counter,\n        logger=logger,\n        discount=discount,\n        target_update_period=target_update_period,\n        importance_sampling_exponent=importance_sampling_exponent,\n        max_replay_size=max_replay_size,\n        learning_rate=learning_rate,\n        store_lstm_state=store_lstm_state,\n        max_priority_weight=max_priority_weight,\n    )\n\n    self._checkpointer = tf2_savers.Checkpointer(\n        subdirectory='r2d2_learner',\n        time_delta_minutes=60,\n        objects_to_save=learner.state,\n        enable_checkpointing=checkpoint,\n    )\n    self._snapshotter = tf2_savers.Snapshotter(\n        objects_to_save={'network': network}, time_delta_minutes=60.)\n\n    policy_network = snt.DeepRNN([\n        network,\n        lambda qs: trfl.epsilon_greedy(qs, epsilon=epsilon).sample(),\n    ])\n\n    actor = actors.RecurrentActor(\n        policy_network, adder, store_recurrent_state=store_lstm_state)\n    observations_per_step = (\n        float(replay_period * batch_size) / samples_per_insert)\n    super().__init__(\n        actor=actor,\n        learner=learner,\n        min_observations=replay_period * max(batch_size, min_replay_size),\n        observations_per_step=observations_per_step)\n\n  def update(self):\n    super().update()\n    self._snapshotter.save()\n    self._checkpointer.save()",
  "def __init__(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      network: snt.RNNCore,\n      burn_in_length: int,\n      trace_length: int,\n      replay_period: int,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      discount: float = 0.99,\n      batch_size: int = 32,\n      prefetch_size: int = tf.data.experimental.AUTOTUNE,\n      target_update_period: int = 100,\n      importance_sampling_exponent: float = 0.2,\n      priority_exponent: float = 0.6,\n      epsilon: float = 0.01,\n      learning_rate: float = 1e-3,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1000000,\n      samples_per_insert: float = 32.0,\n      store_lstm_state: bool = True,\n      max_priority_weight: float = 0.9,\n      checkpoint: bool = True,\n  ):\n\n    if store_lstm_state:\n      extra_spec = {\n          'core_state': tf2_utils.squeeze_batch_dim(network.initial_state(1)),\n      }\n    else:\n      extra_spec = ()\n\n    sequence_length = burn_in_length + trace_length + 1\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Prioritized(priority_exponent),\n        remover=reverb.selectors.Fifo(),\n        max_size=max_replay_size,\n        rate_limiter=reverb.rate_limiters.MinSize(min_size_to_sample=1),\n        signature=adders.SequenceAdder.signature(\n            environment_spec, extra_spec, sequence_length=sequence_length))\n    self._server = reverb.Server([replay_table], port=None)\n    address = f'localhost:{self._server.port}'\n\n    # Component to add things into replay.\n    adder = adders.SequenceAdder(\n        client=reverb.Client(address),\n        period=replay_period,\n        sequence_length=sequence_length,\n    )\n\n    # The dataset object to learn from.\n    dataset = datasets.make_reverb_dataset(\n        server_address=address,\n        batch_size=batch_size,\n        prefetch_size=prefetch_size)\n\n    target_network = copy.deepcopy(network)\n    tf2_utils.create_variables(network, [environment_spec.observations])\n    tf2_utils.create_variables(target_network, [environment_spec.observations])\n\n    learner = learning.R2D2Learner(\n        environment_spec=environment_spec,\n        network=network,\n        target_network=target_network,\n        burn_in_length=burn_in_length,\n        sequence_length=sequence_length,\n        dataset=dataset,\n        reverb_client=reverb.TFClient(address),\n        counter=counter,\n        logger=logger,\n        discount=discount,\n        target_update_period=target_update_period,\n        importance_sampling_exponent=importance_sampling_exponent,\n        max_replay_size=max_replay_size,\n        learning_rate=learning_rate,\n        store_lstm_state=store_lstm_state,\n        max_priority_weight=max_priority_weight,\n    )\n\n    self._checkpointer = tf2_savers.Checkpointer(\n        subdirectory='r2d2_learner',\n        time_delta_minutes=60,\n        objects_to_save=learner.state,\n        enable_checkpointing=checkpoint,\n    )\n    self._snapshotter = tf2_savers.Snapshotter(\n        objects_to_save={'network': network}, time_delta_minutes=60.)\n\n    policy_network = snt.DeepRNN([\n        network,\n        lambda qs: trfl.epsilon_greedy(qs, epsilon=epsilon).sample(),\n    ])\n\n    actor = actors.RecurrentActor(\n        policy_network, adder, store_recurrent_state=store_lstm_state)\n    observations_per_step = (\n        float(replay_period * batch_size) / samples_per_insert)\n    super().__init__(\n        actor=actor,\n        learner=learner,\n        min_observations=replay_period * max(batch_size, min_replay_size),\n        observations_per_step=observations_per_step)",
  "def update(self):\n    super().update()\n    self._snapshotter.save()\n    self._checkpointer.save()",
  "class R2D2Learner(acme.Learner, tf2_savers.TFSaveable):\n  \"\"\"R2D2 learner.\n\n  This is the learning component of the R2D2 agent. It takes a dataset as input\n  and implements update functionality to learn from this dataset.\n  \"\"\"\n\n  def __init__(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      network: Union[networks.RNNCore, snt.RNNCore],\n      target_network: Union[networks.RNNCore, snt.RNNCore],\n      burn_in_length: int,\n      sequence_length: int,\n      dataset: tf.data.Dataset,\n      reverb_client: Optional[reverb.TFClient] = None,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      discount: float = 0.99,\n      target_update_period: int = 100,\n      importance_sampling_exponent: float = 0.2,\n      max_replay_size: int = 1_000_000,\n      learning_rate: float = 1e-3,\n      # TODO(sergomez): rename to use_core_state for consistency with JAX agent.\n      store_lstm_state: bool = True,\n      max_priority_weight: float = 0.9,\n      n_step: int = 5,\n      clip_grad_norm: Optional[float] = None,\n  ):\n\n    if not isinstance(network, networks.RNNCore):\n      network.unroll = functools.partial(snt.static_unroll, network)\n      target_network.unroll = functools.partial(snt.static_unroll,\n                                                target_network)\n\n    # Internalise agent components (replay buffer, networks, optimizer).\n    # TODO(b/155086959): Fix type stubs and remove.\n    self._iterator: Iterator[reverb.ReplaySample] = iter(dataset)  # pytype: disable=wrong-arg-types\n    self._network = network\n    self._target_network = target_network\n    self._optimizer = snt.optimizers.Adam(learning_rate, epsilon=1e-3)\n    self._reverb_client = reverb_client\n\n    # Internalise the hyperparameters.\n    self._store_lstm_state = store_lstm_state\n    self._burn_in_length = burn_in_length\n    self._discount = discount\n    self._max_replay_size = max_replay_size\n    self._importance_sampling_exponent = importance_sampling_exponent\n    self._max_priority_weight = max_priority_weight\n    self._target_update_period = target_update_period\n    self._num_actions = environment_spec.actions.num_values\n    self._sequence_length = sequence_length\n    self._n_step = n_step\n    self._clip_grad_norm = clip_grad_norm\n\n    if burn_in_length:\n      self._burn_in = lambda o, s: self._network.unroll(o, s, burn_in_length)\n    else:\n      self._burn_in = lambda o, s: (o, s)  # pylint: disable=unnecessary-lambda\n\n    # Learner state.\n    self._variables = network.variables\n    self._num_steps = tf.Variable(\n        0., dtype=tf.float32, trainable=False, name='step')\n\n    # Internalise logging/counting objects.\n    self._counter = counting.Counter(counter, 'learner')\n    self._logger = logger or loggers.TerminalLogger('learner', time_delta=100.)\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None\n\n  @tf.function\n  def _step(self) -> Dict[str, tf.Tensor]:\n\n    # Draw a batch of data from replay.\n    sample: reverb.ReplaySample = next(self._iterator)\n\n    data = tf2_utils.batch_to_sequence(sample.data)\n    observations, actions, rewards, discounts, extra = (data.observation,\n                                                        data.action,\n                                                        data.reward,\n                                                        data.discount,\n                                                        data.extras)\n    unused_sequence_length, batch_size = actions.shape\n\n    # Get initial state for the LSTM, either from replay or simply use zeros.\n    if self._store_lstm_state:\n      core_state = tree.map_structure(lambda x: x[0], extra['core_state'])\n    else:\n      core_state = self._network.initial_state(batch_size)\n    target_core_state = tree.map_structure(tf.identity, core_state)\n\n    # Before training, optionally unroll the LSTM for a fixed warmup period.\n    burn_in_obs = tree.map_structure(lambda x: x[:self._burn_in_length],\n                                     observations)\n    _, core_state = self._burn_in(burn_in_obs, core_state)\n    _, target_core_state = self._burn_in(burn_in_obs, target_core_state)\n\n    # Don't train on the warmup period.\n    observations, actions, rewards, discounts, extra = tree.map_structure(\n        lambda x: x[self._burn_in_length:],\n        (observations, actions, rewards, discounts, extra))\n\n    with tf.GradientTape() as tape:\n      # Unroll the online and target Q-networks on the sequences.\n      q_values, _ = self._network.unroll(observations, core_state,\n                                         self._sequence_length)\n      target_q_values, _ = self._target_network.unroll(observations,\n                                                       target_core_state,\n                                                       self._sequence_length)\n\n      # Compute the target policy distribution (greedy).\n      greedy_actions = tf.argmax(q_values, output_type=tf.int32, axis=-1)\n      target_policy_probs = tf.one_hot(\n          greedy_actions, depth=self._num_actions, dtype=q_values.dtype)\n\n      # Compute the transformed n-step loss.\n      rewards = tree.map_structure(lambda x: x[:-1], rewards)\n      discounts = tree.map_structure(lambda x: x[:-1], discounts)\n      loss, extra = losses.transformed_n_step_loss(\n          qs=q_values,\n          targnet_qs=target_q_values,\n          actions=actions,\n          rewards=rewards,\n          pcontinues=discounts * self._discount,\n          target_policy_probs=target_policy_probs,\n          bootstrap_n=self._n_step,\n      )\n\n      # Calculate importance weights and use them to scale the loss.\n      sample_info = sample.info\n      keys, probs = sample_info.key, sample_info.probability\n      importance_weights = 1. / (self._max_replay_size * probs)  # [T, B]\n      importance_weights **= self._importance_sampling_exponent\n      importance_weights /= tf.reduce_max(importance_weights)\n      loss *= tf.cast(importance_weights, tf.float32)  # [T, B]\n      loss = tf.reduce_mean(loss)  # []\n\n    # Apply gradients via optimizer.\n    gradients = tape.gradient(loss, self._network.trainable_variables)\n    #\u00a0Clip and apply gradients.\n    if self._clip_grad_norm is not None:\n      gradients, _ = tf.clip_by_global_norm(gradients, self._clip_grad_norm)\n\n    self._optimizer.apply(gradients, self._network.trainable_variables)\n\n    # Periodically update the target network.\n    if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n      for src, dest in zip(self._network.variables,\n                           self._target_network.variables):\n        dest.assign(src)\n    self._num_steps.assign_add(1)\n\n    if self._reverb_client:\n      # Compute updated priorities.\n      priorities = compute_priority(extra.errors, self._max_priority_weight)\n      # Compute priorities and add an op to update them on the reverb side.\n      self._reverb_client.update_priorities(\n          table=adders.DEFAULT_PRIORITY_TABLE,\n          keys=keys,\n          priorities=tf.cast(priorities, tf.float64))\n\n    return {'loss': loss}\n\n  def step(self):\n    # Run the learning step.\n    results = self._step()\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n    results.update(counts)\n    self._logger.write(results)\n\n  def get_variables(self, names: List[str]) -> List[Variables]:\n    return [tf2_utils.to_numpy(self._variables)]\n\n  @property\n  def state(self) -> Mapping[str, tf2_savers.Checkpointable]:\n    \"\"\"Returns the stateful parts of the learner for checkpointing.\"\"\"\n    return {\n        'network': self._network,\n        'target_network': self._target_network,\n        'optimizer': self._optimizer,\n        'num_steps': self._num_steps,\n    }",
  "def compute_priority(errors: tf.Tensor, alpha: float):\n  \"\"\"Compute priority as mixture of max and mean sequence errors.\"\"\"\n  abs_errors = tf.abs(errors)\n  mean_priority = tf.reduce_mean(abs_errors, axis=0)\n  max_priority = tf.reduce_max(abs_errors, axis=0)\n\n  return alpha * max_priority + (1 - alpha) * mean_priority",
  "def __init__(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      network: Union[networks.RNNCore, snt.RNNCore],\n      target_network: Union[networks.RNNCore, snt.RNNCore],\n      burn_in_length: int,\n      sequence_length: int,\n      dataset: tf.data.Dataset,\n      reverb_client: Optional[reverb.TFClient] = None,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      discount: float = 0.99,\n      target_update_period: int = 100,\n      importance_sampling_exponent: float = 0.2,\n      max_replay_size: int = 1_000_000,\n      learning_rate: float = 1e-3,\n      # TODO(sergomez): rename to use_core_state for consistency with JAX agent.\n      store_lstm_state: bool = True,\n      max_priority_weight: float = 0.9,\n      n_step: int = 5,\n      clip_grad_norm: Optional[float] = None,\n  ):\n\n    if not isinstance(network, networks.RNNCore):\n      network.unroll = functools.partial(snt.static_unroll, network)\n      target_network.unroll = functools.partial(snt.static_unroll,\n                                                target_network)\n\n    # Internalise agent components (replay buffer, networks, optimizer).\n    # TODO(b/155086959): Fix type stubs and remove.\n    self._iterator: Iterator[reverb.ReplaySample] = iter(dataset)  # pytype: disable=wrong-arg-types\n    self._network = network\n    self._target_network = target_network\n    self._optimizer = snt.optimizers.Adam(learning_rate, epsilon=1e-3)\n    self._reverb_client = reverb_client\n\n    # Internalise the hyperparameters.\n    self._store_lstm_state = store_lstm_state\n    self._burn_in_length = burn_in_length\n    self._discount = discount\n    self._max_replay_size = max_replay_size\n    self._importance_sampling_exponent = importance_sampling_exponent\n    self._max_priority_weight = max_priority_weight\n    self._target_update_period = target_update_period\n    self._num_actions = environment_spec.actions.num_values\n    self._sequence_length = sequence_length\n    self._n_step = n_step\n    self._clip_grad_norm = clip_grad_norm\n\n    if burn_in_length:\n      self._burn_in = lambda o, s: self._network.unroll(o, s, burn_in_length)\n    else:\n      self._burn_in = lambda o, s: (o, s)  # pylint: disable=unnecessary-lambda\n\n    # Learner state.\n    self._variables = network.variables\n    self._num_steps = tf.Variable(\n        0., dtype=tf.float32, trainable=False, name='step')\n\n    # Internalise logging/counting objects.\n    self._counter = counting.Counter(counter, 'learner')\n    self._logger = logger or loggers.TerminalLogger('learner', time_delta=100.)\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None",
  "def _step(self) -> Dict[str, tf.Tensor]:\n\n    # Draw a batch of data from replay.\n    sample: reverb.ReplaySample = next(self._iterator)\n\n    data = tf2_utils.batch_to_sequence(sample.data)\n    observations, actions, rewards, discounts, extra = (data.observation,\n                                                        data.action,\n                                                        data.reward,\n                                                        data.discount,\n                                                        data.extras)\n    unused_sequence_length, batch_size = actions.shape\n\n    # Get initial state for the LSTM, either from replay or simply use zeros.\n    if self._store_lstm_state:\n      core_state = tree.map_structure(lambda x: x[0], extra['core_state'])\n    else:\n      core_state = self._network.initial_state(batch_size)\n    target_core_state = tree.map_structure(tf.identity, core_state)\n\n    # Before training, optionally unroll the LSTM for a fixed warmup period.\n    burn_in_obs = tree.map_structure(lambda x: x[:self._burn_in_length],\n                                     observations)\n    _, core_state = self._burn_in(burn_in_obs, core_state)\n    _, target_core_state = self._burn_in(burn_in_obs, target_core_state)\n\n    # Don't train on the warmup period.\n    observations, actions, rewards, discounts, extra = tree.map_structure(\n        lambda x: x[self._burn_in_length:],\n        (observations, actions, rewards, discounts, extra))\n\n    with tf.GradientTape() as tape:\n      # Unroll the online and target Q-networks on the sequences.\n      q_values, _ = self._network.unroll(observations, core_state,\n                                         self._sequence_length)\n      target_q_values, _ = self._target_network.unroll(observations,\n                                                       target_core_state,\n                                                       self._sequence_length)\n\n      # Compute the target policy distribution (greedy).\n      greedy_actions = tf.argmax(q_values, output_type=tf.int32, axis=-1)\n      target_policy_probs = tf.one_hot(\n          greedy_actions, depth=self._num_actions, dtype=q_values.dtype)\n\n      # Compute the transformed n-step loss.\n      rewards = tree.map_structure(lambda x: x[:-1], rewards)\n      discounts = tree.map_structure(lambda x: x[:-1], discounts)\n      loss, extra = losses.transformed_n_step_loss(\n          qs=q_values,\n          targnet_qs=target_q_values,\n          actions=actions,\n          rewards=rewards,\n          pcontinues=discounts * self._discount,\n          target_policy_probs=target_policy_probs,\n          bootstrap_n=self._n_step,\n      )\n\n      # Calculate importance weights and use them to scale the loss.\n      sample_info = sample.info\n      keys, probs = sample_info.key, sample_info.probability\n      importance_weights = 1. / (self._max_replay_size * probs)  # [T, B]\n      importance_weights **= self._importance_sampling_exponent\n      importance_weights /= tf.reduce_max(importance_weights)\n      loss *= tf.cast(importance_weights, tf.float32)  # [T, B]\n      loss = tf.reduce_mean(loss)  # []\n\n    # Apply gradients via optimizer.\n    gradients = tape.gradient(loss, self._network.trainable_variables)\n    #\u00a0Clip and apply gradients.\n    if self._clip_grad_norm is not None:\n      gradients, _ = tf.clip_by_global_norm(gradients, self._clip_grad_norm)\n\n    self._optimizer.apply(gradients, self._network.trainable_variables)\n\n    # Periodically update the target network.\n    if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n      for src, dest in zip(self._network.variables,\n                           self._target_network.variables):\n        dest.assign(src)\n    self._num_steps.assign_add(1)\n\n    if self._reverb_client:\n      # Compute updated priorities.\n      priorities = compute_priority(extra.errors, self._max_priority_weight)\n      # Compute priorities and add an op to update them on the reverb side.\n      self._reverb_client.update_priorities(\n          table=adders.DEFAULT_PRIORITY_TABLE,\n          keys=keys,\n          priorities=tf.cast(priorities, tf.float64))\n\n    return {'loss': loss}",
  "def step(self):\n    # Run the learning step.\n    results = self._step()\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n    results.update(counts)\n    self._logger.write(results)",
  "def get_variables(self, names: List[str]) -> List[Variables]:\n    return [tf2_utils.to_numpy(self._variables)]",
  "def state(self) -> Mapping[str, tf2_savers.Checkpointable]:\n    \"\"\"Returns the stateful parts of the learner for checkpointing.\"\"\"\n    return {\n        'network': self._network,\n        'target_network': self._target_network,\n        'optimizer': self._optimizer,\n        'num_steps': self._num_steps,\n    }",
  "class DistributedAgentTest(absltest.TestCase):\n  \"\"\"Simple integration/smoke test for the distributed agent.\"\"\"\n\n  def test_agent(self):\n    env_factory = lambda x: fakes.fake_atari_wrapped(oar_wrapper=True)\n    net_factory = lambda spec: networks.R2D2AtariNetwork(spec.num_values)\n\n    agent = r2d2.DistributedR2D2(\n        environment_factory=env_factory,\n        network_factory=net_factory,\n        num_actors=2,\n        batch_size=32,\n        min_replay_size=32,\n        max_replay_size=1000,\n        replay_period=1,\n        burn_in_length=1,\n        trace_length=10,\n    )\n    program = agent.build()\n\n    (learner_node,) = program.groups['learner']\n    learner_node.disable_run()\n\n    lp.launch(program, launch_type='test_mt')\n\n    learner: acme.Learner = learner_node.create_handle().dereference()\n\n    for _ in range(5):\n      learner.step()",
  "def test_agent(self):\n    env_factory = lambda x: fakes.fake_atari_wrapped(oar_wrapper=True)\n    net_factory = lambda spec: networks.R2D2AtariNetwork(spec.num_values)\n\n    agent = r2d2.DistributedR2D2(\n        environment_factory=env_factory,\n        network_factory=net_factory,\n        num_actors=2,\n        batch_size=32,\n        min_replay_size=32,\n        max_replay_size=1000,\n        replay_period=1,\n        burn_in_length=1,\n        trace_length=10,\n    )\n    program = agent.build()\n\n    (learner_node,) = program.groups['learner']\n    learner_node.disable_run()\n\n    lp.launch(program, launch_type='test_mt')\n\n    learner: acme.Learner = learner_node.create_handle().dereference()\n\n    for _ in range(5):\n      learner.step()",
  "class SimpleNetwork(networks.RNNCore):\n\n  def __init__(self, action_spec: specs.DiscreteArray):\n    super().__init__(name='r2d2_test_network')\n    self._net = snt.DeepRNN([\n        snt.Flatten(),\n        snt.LSTM(20),\n        snt.nets.MLP([50, 50, action_spec.num_values])\n    ])\n\n  def __call__(self, inputs, state):\n    return self._net(inputs, state)\n\n  def initial_state(self, batch_size: int, **kwargs):\n    return self._net.initial_state(batch_size)\n\n  def unroll(self, inputs, state, sequence_length):\n    return snt.static_unroll(self._net, inputs, state, sequence_length)",
  "class R2D2Test(parameterized.TestCase):\n\n  @parameterized.parameters(True, False)\n  def test_r2d2(self, store_lstm_state: bool):\n    # Create a fake environment to test with.\n    # TODO(b/152596848): Allow R2D2 to deal with integer observations.\n    environment = fakes.DiscreteEnvironment(\n        num_actions=5,\n        num_observations=10,\n        obs_shape=(10, 4),\n        obs_dtype=np.float32,\n        episode_length=10)\n    spec = specs.make_environment_spec(environment)\n\n    # Construct the agent.\n    agent = r2d2.R2D2(\n        environment_spec=spec,\n        network=SimpleNetwork(spec.actions),\n        batch_size=10,\n        samples_per_insert=2,\n        min_replay_size=10,\n        store_lstm_state=store_lstm_state,\n        burn_in_length=2,\n        trace_length=6,\n        replay_period=4,\n        checkpoint=False,\n    )\n\n    # Try running the environment loop. We have no assertions here because all\n    # we care about is that the agent runs without raising any errors.\n    loop = acme.EnvironmentLoop(environment, agent)\n    loop.run(num_episodes=5)",
  "def __init__(self, action_spec: specs.DiscreteArray):\n    super().__init__(name='r2d2_test_network')\n    self._net = snt.DeepRNN([\n        snt.Flatten(),\n        snt.LSTM(20),\n        snt.nets.MLP([50, 50, action_spec.num_values])\n    ])",
  "def __call__(self, inputs, state):\n    return self._net(inputs, state)",
  "def initial_state(self, batch_size: int, **kwargs):\n    return self._net.initial_state(batch_size)",
  "def unroll(self, inputs, state, sequence_length):\n    return snt.static_unroll(self._net, inputs, state, sequence_length)",
  "def test_r2d2(self, store_lstm_state: bool):\n    # Create a fake environment to test with.\n    # TODO(b/152596848): Allow R2D2 to deal with integer observations.\n    environment = fakes.DiscreteEnvironment(\n        num_actions=5,\n        num_observations=10,\n        obs_shape=(10, 4),\n        obs_dtype=np.float32,\n        episode_length=10)\n    spec = specs.make_environment_spec(environment)\n\n    # Construct the agent.\n    agent = r2d2.R2D2(\n        environment_spec=spec,\n        network=SimpleNetwork(spec.actions),\n        batch_size=10,\n        samples_per_insert=2,\n        min_replay_size=10,\n        store_lstm_state=store_lstm_state,\n        burn_in_length=2,\n        trace_length=6,\n        replay_period=4,\n        checkpoint=False,\n    )\n\n    # Try running the environment loop. We have no assertions here because all\n    # we care about is that the agent runs without raising any errors.\n    loop = acme.EnvironmentLoop(environment, agent)\n    loop.run(num_episodes=5)",
  "class DistributedR2D2:\n  \"\"\"Program definition for Recurrent Replay Distributed DQN (R2D2).\"\"\"\n\n  def __init__(self,\n               environment_factory: Callable[[bool], dm_env.Environment],\n               network_factory: Callable[[specs.DiscreteArray], snt.RNNCore],\n               num_actors: int,\n               burn_in_length: int,\n               trace_length: int,\n               replay_period: int,\n               environment_spec: Optional[specs.EnvironmentSpec] = None,\n               batch_size: int = 256,\n               prefetch_size: int = tf.data.experimental.AUTOTUNE,\n               min_replay_size: int = 1000,\n               max_replay_size: int = 100_000,\n               samples_per_insert: float = 32.0,\n               discount: float = 0.99,\n               priority_exponent: float = 0.6,\n               importance_sampling_exponent: float = 0.2,\n               variable_update_period: int = 1000,\n               learning_rate: float = 1e-3,\n               evaluator_epsilon: float = 0.,\n               target_update_period: int = 100,\n               save_logs: bool = False):\n\n    if environment_spec is None:\n      environment_spec = specs.make_environment_spec(environment_factory(False))\n\n    self._environment_factory = environment_factory\n    self._network_factory = network_factory\n    self._environment_spec = environment_spec\n    self._num_actors = num_actors\n    self._batch_size = batch_size\n    self._prefetch_size = prefetch_size\n    self._min_replay_size = min_replay_size\n    self._max_replay_size = max_replay_size\n    self._samples_per_insert = samples_per_insert\n    self._burn_in_length = burn_in_length\n    self._trace_length = trace_length\n    self._replay_period = replay_period\n    self._discount = discount\n    self._target_update_period = target_update_period\n    self._variable_update_period = variable_update_period\n    self._save_logs = save_logs\n    self._priority_exponent = priority_exponent\n    self._learning_rate = learning_rate\n    self._evaluator_epsilon = evaluator_epsilon\n    self._importance_sampling_exponent = importance_sampling_exponent\n\n    self._obs_spec = environment_spec.observations\n\n  def replay(self) -> List[reverb.Table]:\n    \"\"\"The replay storage.\"\"\"\n    network = self._network_factory(self._environment_spec.actions)\n    extra_spec = {\n        'core_state': network.initial_state(1),\n    }\n    # Remove batch dimensions.\n    extra_spec = tf2_utils.squeeze_batch_dim(extra_spec)\n    if self._samples_per_insert:\n      limiter = reverb.rate_limiters.SampleToInsertRatio(\n          min_size_to_sample=self._min_replay_size,\n          samples_per_insert=self._samples_per_insert,\n          error_buffer=self._batch_size)\n    else:\n      limiter = reverb.rate_limiters.MinSize(self._min_replay_size)\n    table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Prioritized(self._priority_exponent),\n        remover=reverb.selectors.Fifo(),\n        max_size=self._max_replay_size,\n        rate_limiter=limiter,\n        signature=adders.SequenceAdder.signature(\n            self._environment_spec,\n            extra_spec,\n            sequence_length=self._burn_in_length + self._trace_length + 1))\n\n    return [table]\n\n  def counter(self):\n    \"\"\"Creates the master counter process.\"\"\"\n    return tf2_savers.CheckpointingRunner(\n        counting.Counter(), time_delta_minutes=1, subdirectory='counter')\n\n  def learner(self, replay: reverb.Client, counter: counting.Counter):\n    \"\"\"The Learning part of the agent.\"\"\"\n    # Use architect and create the environment.\n    # Create the networks.\n    network = self._network_factory(self._environment_spec.actions)\n    target_network = copy.deepcopy(network)\n\n    tf2_utils.create_variables(network, [self._obs_spec])\n    tf2_utils.create_variables(target_network, [self._obs_spec])\n\n    # The dataset object to learn from.\n    reverb_client = reverb.TFClient(replay.server_address)\n    sequence_length = self._burn_in_length + self._trace_length + 1\n    dataset = datasets.make_reverb_dataset(\n        server_address=replay.server_address,\n        batch_size=self._batch_size,\n        prefetch_size=self._prefetch_size)\n\n    counter = counting.Counter(counter, 'learner')\n    logger = loggers.make_default_logger(\n        'learner', save_data=True, steps_key='learner_steps')\n    # Return the learning agent.\n    learner = learning.R2D2Learner(\n        environment_spec=self._environment_spec,\n        network=network,\n        target_network=target_network,\n        burn_in_length=self._burn_in_length,\n        sequence_length=sequence_length,\n        dataset=dataset,\n        reverb_client=reverb_client,\n        counter=counter,\n        logger=logger,\n        discount=self._discount,\n        target_update_period=self._target_update_period,\n        importance_sampling_exponent=self._importance_sampling_exponent,\n        learning_rate=self._learning_rate,\n        max_replay_size=self._max_replay_size)\n    return tf2_savers.CheckpointingRunner(\n        wrapped=learner, time_delta_minutes=60, subdirectory='r2d2_learner')\n\n  def actor(\n      self,\n      replay: reverb.Client,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n      epsilon: float,\n  ) -> acme.EnvironmentLoop:\n    \"\"\"The actor process.\"\"\"\n    environment = self._environment_factory(False)\n    network = self._network_factory(self._environment_spec.actions)\n\n    tf2_utils.create_variables(network, [self._obs_spec])\n\n    policy_network = snt.DeepRNN([\n        network,\n        lambda qs: tf.cast(trfl.epsilon_greedy(qs, epsilon).sample(), tf.int32),\n    ])\n\n    # Component to add things into replay.\n    sequence_length = self._burn_in_length + self._trace_length + 1\n    adder = adders.SequenceAdder(\n        client=replay,\n        period=self._replay_period,\n        sequence_length=sequence_length,\n        delta_encoded=True,\n    )\n\n    variable_client = tf2_variable_utils.VariableClient(\n        client=variable_source,\n        variables={'policy': policy_network.variables},\n        update_period=self._variable_update_period)\n\n    # Make sure not to use a random policy after checkpoint restoration by\n    # assigning variables before running the environment loop.\n    variable_client.update_and_wait()\n\n    # Create the agent.\n    actor = actors.RecurrentActor(\n        policy_network=policy_network,\n        variable_client=variable_client,\n        adder=adder)\n\n    counter = counting.Counter(counter, 'actor')\n    logger = loggers.make_default_logger(\n        'actor', save_data=False, steps_key='actor_steps')\n\n    # Create the loop to connect environment and agent.\n    return acme.EnvironmentLoop(environment, actor, counter, logger)\n\n  def evaluator(\n      self,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ):\n    \"\"\"The evaluation process.\"\"\"\n    environment = self._environment_factory(True)\n    network = self._network_factory(self._environment_spec.actions)\n\n    tf2_utils.create_variables(network, [self._obs_spec])\n    policy_network = snt.DeepRNN([\n        network,\n        lambda qs: tf.cast(tf.argmax(qs, axis=-1), tf.int32),\n    ])\n\n    variable_client = tf2_variable_utils.VariableClient(\n        client=variable_source,\n        variables={'policy': policy_network.variables},\n        update_period=self._variable_update_period)\n\n    # Make sure not to use a random policy after checkpoint restoration by\n    # assigning variables before running the environment loop.\n    variable_client.update_and_wait()\n\n    # Create the agent.\n    actor = actors.RecurrentActor(\n        policy_network=policy_network, variable_client=variable_client)\n\n    # Create the run loop and return it.\n    logger = loggers.make_default_logger(\n        'evaluator', save_data=True, steps_key='evaluator_steps')\n    counter = counting.Counter(counter, 'evaluator')\n\n    return acme.EnvironmentLoop(environment, actor, counter, logger)\n\n  def build(self, name='r2d2'):\n    \"\"\"Build the distributed agent topology.\"\"\"\n    program = lp.Program(name=name)\n\n    with program.group('replay'):\n      replay = program.add_node(lp.ReverbNode(self.replay))\n\n    with program.group('counter'):\n      counter = program.add_node(lp.CourierNode(self.counter))\n\n    with program.group('learner'):\n      learner = program.add_node(lp.CourierNode(self.learner, replay, counter))\n\n    with program.group('cacher'):\n      cacher = program.add_node(\n          lp.CacherNode(learner, refresh_interval_ms=2000, stale_after_ms=4000))\n\n    with program.group('evaluator'):\n      program.add_node(lp.CourierNode(self.evaluator, cacher, counter))\n\n    # Generate an epsilon for each actor.\n    epsilons = np.flip(np.logspace(1, 8, self._num_actors, base=0.4), axis=0)\n\n    with program.group('actor'):\n      for epsilon in epsilons:\n        program.add_node(\n            lp.CourierNode(self.actor, replay, cacher, counter, epsilon))\n\n    return program",
  "def __init__(self,\n               environment_factory: Callable[[bool], dm_env.Environment],\n               network_factory: Callable[[specs.DiscreteArray], snt.RNNCore],\n               num_actors: int,\n               burn_in_length: int,\n               trace_length: int,\n               replay_period: int,\n               environment_spec: Optional[specs.EnvironmentSpec] = None,\n               batch_size: int = 256,\n               prefetch_size: int = tf.data.experimental.AUTOTUNE,\n               min_replay_size: int = 1000,\n               max_replay_size: int = 100_000,\n               samples_per_insert: float = 32.0,\n               discount: float = 0.99,\n               priority_exponent: float = 0.6,\n               importance_sampling_exponent: float = 0.2,\n               variable_update_period: int = 1000,\n               learning_rate: float = 1e-3,\n               evaluator_epsilon: float = 0.,\n               target_update_period: int = 100,\n               save_logs: bool = False):\n\n    if environment_spec is None:\n      environment_spec = specs.make_environment_spec(environment_factory(False))\n\n    self._environment_factory = environment_factory\n    self._network_factory = network_factory\n    self._environment_spec = environment_spec\n    self._num_actors = num_actors\n    self._batch_size = batch_size\n    self._prefetch_size = prefetch_size\n    self._min_replay_size = min_replay_size\n    self._max_replay_size = max_replay_size\n    self._samples_per_insert = samples_per_insert\n    self._burn_in_length = burn_in_length\n    self._trace_length = trace_length\n    self._replay_period = replay_period\n    self._discount = discount\n    self._target_update_period = target_update_period\n    self._variable_update_period = variable_update_period\n    self._save_logs = save_logs\n    self._priority_exponent = priority_exponent\n    self._learning_rate = learning_rate\n    self._evaluator_epsilon = evaluator_epsilon\n    self._importance_sampling_exponent = importance_sampling_exponent\n\n    self._obs_spec = environment_spec.observations",
  "def replay(self) -> List[reverb.Table]:\n    \"\"\"The replay storage.\"\"\"\n    network = self._network_factory(self._environment_spec.actions)\n    extra_spec = {\n        'core_state': network.initial_state(1),\n    }\n    # Remove batch dimensions.\n    extra_spec = tf2_utils.squeeze_batch_dim(extra_spec)\n    if self._samples_per_insert:\n      limiter = reverb.rate_limiters.SampleToInsertRatio(\n          min_size_to_sample=self._min_replay_size,\n          samples_per_insert=self._samples_per_insert,\n          error_buffer=self._batch_size)\n    else:\n      limiter = reverb.rate_limiters.MinSize(self._min_replay_size)\n    table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Prioritized(self._priority_exponent),\n        remover=reverb.selectors.Fifo(),\n        max_size=self._max_replay_size,\n        rate_limiter=limiter,\n        signature=adders.SequenceAdder.signature(\n            self._environment_spec,\n            extra_spec,\n            sequence_length=self._burn_in_length + self._trace_length + 1))\n\n    return [table]",
  "def counter(self):\n    \"\"\"Creates the master counter process.\"\"\"\n    return tf2_savers.CheckpointingRunner(\n        counting.Counter(), time_delta_minutes=1, subdirectory='counter')",
  "def learner(self, replay: reverb.Client, counter: counting.Counter):\n    \"\"\"The Learning part of the agent.\"\"\"\n    # Use architect and create the environment.\n    # Create the networks.\n    network = self._network_factory(self._environment_spec.actions)\n    target_network = copy.deepcopy(network)\n\n    tf2_utils.create_variables(network, [self._obs_spec])\n    tf2_utils.create_variables(target_network, [self._obs_spec])\n\n    # The dataset object to learn from.\n    reverb_client = reverb.TFClient(replay.server_address)\n    sequence_length = self._burn_in_length + self._trace_length + 1\n    dataset = datasets.make_reverb_dataset(\n        server_address=replay.server_address,\n        batch_size=self._batch_size,\n        prefetch_size=self._prefetch_size)\n\n    counter = counting.Counter(counter, 'learner')\n    logger = loggers.make_default_logger(\n        'learner', save_data=True, steps_key='learner_steps')\n    # Return the learning agent.\n    learner = learning.R2D2Learner(\n        environment_spec=self._environment_spec,\n        network=network,\n        target_network=target_network,\n        burn_in_length=self._burn_in_length,\n        sequence_length=sequence_length,\n        dataset=dataset,\n        reverb_client=reverb_client,\n        counter=counter,\n        logger=logger,\n        discount=self._discount,\n        target_update_period=self._target_update_period,\n        importance_sampling_exponent=self._importance_sampling_exponent,\n        learning_rate=self._learning_rate,\n        max_replay_size=self._max_replay_size)\n    return tf2_savers.CheckpointingRunner(\n        wrapped=learner, time_delta_minutes=60, subdirectory='r2d2_learner')",
  "def actor(\n      self,\n      replay: reverb.Client,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n      epsilon: float,\n  ) -> acme.EnvironmentLoop:\n    \"\"\"The actor process.\"\"\"\n    environment = self._environment_factory(False)\n    network = self._network_factory(self._environment_spec.actions)\n\n    tf2_utils.create_variables(network, [self._obs_spec])\n\n    policy_network = snt.DeepRNN([\n        network,\n        lambda qs: tf.cast(trfl.epsilon_greedy(qs, epsilon).sample(), tf.int32),\n    ])\n\n    # Component to add things into replay.\n    sequence_length = self._burn_in_length + self._trace_length + 1\n    adder = adders.SequenceAdder(\n        client=replay,\n        period=self._replay_period,\n        sequence_length=sequence_length,\n        delta_encoded=True,\n    )\n\n    variable_client = tf2_variable_utils.VariableClient(\n        client=variable_source,\n        variables={'policy': policy_network.variables},\n        update_period=self._variable_update_period)\n\n    # Make sure not to use a random policy after checkpoint restoration by\n    # assigning variables before running the environment loop.\n    variable_client.update_and_wait()\n\n    # Create the agent.\n    actor = actors.RecurrentActor(\n        policy_network=policy_network,\n        variable_client=variable_client,\n        adder=adder)\n\n    counter = counting.Counter(counter, 'actor')\n    logger = loggers.make_default_logger(\n        'actor', save_data=False, steps_key='actor_steps')\n\n    # Create the loop to connect environment and agent.\n    return acme.EnvironmentLoop(environment, actor, counter, logger)",
  "def evaluator(\n      self,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ):\n    \"\"\"The evaluation process.\"\"\"\n    environment = self._environment_factory(True)\n    network = self._network_factory(self._environment_spec.actions)\n\n    tf2_utils.create_variables(network, [self._obs_spec])\n    policy_network = snt.DeepRNN([\n        network,\n        lambda qs: tf.cast(tf.argmax(qs, axis=-1), tf.int32),\n    ])\n\n    variable_client = tf2_variable_utils.VariableClient(\n        client=variable_source,\n        variables={'policy': policy_network.variables},\n        update_period=self._variable_update_period)\n\n    # Make sure not to use a random policy after checkpoint restoration by\n    # assigning variables before running the environment loop.\n    variable_client.update_and_wait()\n\n    # Create the agent.\n    actor = actors.RecurrentActor(\n        policy_network=policy_network, variable_client=variable_client)\n\n    # Create the run loop and return it.\n    logger = loggers.make_default_logger(\n        'evaluator', save_data=True, steps_key='evaluator_steps')\n    counter = counting.Counter(counter, 'evaluator')\n\n    return acme.EnvironmentLoop(environment, actor, counter, logger)",
  "def build(self, name='r2d2'):\n    \"\"\"Build the distributed agent topology.\"\"\"\n    program = lp.Program(name=name)\n\n    with program.group('replay'):\n      replay = program.add_node(lp.ReverbNode(self.replay))\n\n    with program.group('counter'):\n      counter = program.add_node(lp.CourierNode(self.counter))\n\n    with program.group('learner'):\n      learner = program.add_node(lp.CourierNode(self.learner, replay, counter))\n\n    with program.group('cacher'):\n      cacher = program.add_node(\n          lp.CacherNode(learner, refresh_interval_ms=2000, stale_after_ms=4000))\n\n    with program.group('evaluator'):\n      program.add_node(lp.CourierNode(self.evaluator, cacher, counter))\n\n    # Generate an epsilon for each actor.\n    epsilons = np.flip(np.logspace(1, 8, self._num_actors, base=0.4), axis=0)\n\n    with program.group('actor'):\n      for epsilon in epsilons:\n        program.add_node(\n            lp.CourierNode(self.actor, replay, cacher, counter, epsilon))\n\n    return program",
  "class MultiObjectiveMPO(agent.Agent):\n  \"\"\"Multi-objective MPO Agent.\n\n  This implements a single-process multi-objective MPO agent. This is an\n  actor-critic algorithm that generates data via a behavior policy, inserts\n  N-step transitions into a replay buffer, and periodically updates the policy\n  (and as a result the behavior) by sampling uniformly from this buffer.\n  This agent distinguishes itself from the MPO agent in two ways:\n  - Allowing for one or more objectives (see `acme/agents/tf/mompo/learning.py`\n      for details on what form this sequence of objectives should take)\n  - Optionally using a distributional critic (state-action value approximator)\n      as in DMPO. In other words, the critic network can output either scalar\n      Q-values or a DiscreteValuedDistribution.\n  \"\"\"\n\n  def __init__(self,\n               reward_objectives: Sequence[learning.RewardObjective],\n               qvalue_objectives: Sequence[learning.QValueObjective],\n               environment_spec: specs.EnvironmentSpec,\n               policy_network: snt.Module,\n               critic_network: snt.Module,\n               observation_network: types.TensorTransformation = tf.identity,\n               discount: float = 0.99,\n               batch_size: int = 512,\n               prefetch_size: int = 4,\n               target_policy_update_period: int = 200,\n               target_critic_update_period: int = 200,\n               min_replay_size: int = 1000,\n               max_replay_size: int = 1000000,\n               samples_per_insert: float = 16.,\n               policy_loss_module: Optional[losses.MultiObjectiveMPO] = None,\n               policy_optimizer: Optional[snt.Optimizer] = None,\n               critic_optimizer: Optional[snt.Optimizer] = None,\n               n_step: int = 5,\n               num_samples: int = 20,\n               clipping: bool = True,\n               logger: Optional[loggers.Logger] = None,\n               counter: Optional[counting.Counter] = None,\n               checkpoint: bool = True,\n               replay_table_name: str = adders.DEFAULT_PRIORITY_TABLE):\n    \"\"\"Initialize the agent.\n\n    Args:\n      reward_objectives: list of the objectives that the policy should optimize;\n        each objective is defined by its reward function\n      qvalue_objectives: list of the objectives that the policy should optimize;\n        each objective is defined by its Q-value function\n      environment_spec: description of the actions, observations, etc.\n      policy_network: the online (optimized) policy.\n      critic_network: the online critic.\n      observation_network: optional network to transform the observations before\n        they are fed into any network.\n      discount: discount to use for TD updates.\n      batch_size: batch size for updates.\n      prefetch_size: size to prefetch from replay.\n      target_policy_update_period: number of updates to perform before updating\n        the target policy network.\n      target_critic_update_period: number of updates to perform before updating\n        the target critic network.\n      min_replay_size: minimum replay size before updating.\n      max_replay_size: maximum replay size.\n      samples_per_insert: number of samples to take from replay for every insert\n        that is made.\n      policy_loss_module: configured MO-MPO loss function for the policy\n        optimization; defaults to sensible values on the control suite.\n        See `acme/tf/losses/mompo.py` for more details.\n      policy_optimizer: optimizer to be used on the policy.\n      critic_optimizer: optimizer to be used on the critic.\n      n_step: number of steps to squash into a single transition.\n      num_samples: number of actions to sample when doing a Monte Carlo\n        integration with respect to the policy.\n      clipping: whether to clip gradients by global norm.\n      logger: logging object used to write to logs.\n      counter: counter object used to keep track of steps.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n      replay_table_name: string indicating what name to give the replay table.\n    \"\"\"\n    # Check that at least one objective's reward function is specified.\n    if not reward_objectives:\n      raise ValueError('Must specify at least one reward objective.')\n\n    # Create a replay server to add data to.\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=max_replay_size,\n        rate_limiter=reverb.rate_limiters.MinSize(min_size_to_sample=1),\n        signature=adders.NStepTransitionAdder.signature(environment_spec))\n    self._server = reverb.Server([replay_table], port=None)\n\n    # The adder is used to insert observations into replay.\n    address = f'localhost:{self._server.port}'\n    adder = adders.NStepTransitionAdder(\n        client=reverb.Client(address),\n        n_step=n_step,\n        discount=discount)\n\n    # The dataset object to learn from.\n    dataset = datasets.make_reverb_dataset(\n        table=replay_table_name,\n        server_address=address,\n        batch_size=batch_size,\n        prefetch_size=prefetch_size)\n\n    # Make sure observation network is a Sonnet Module.\n    observation_network = tf2_utils.to_sonnet_module(observation_network)\n\n    # Create target networks before creating online/target network variables.\n    target_policy_network = copy.deepcopy(policy_network)\n    target_critic_network = copy.deepcopy(critic_network)\n    target_observation_network = copy.deepcopy(observation_network)\n\n    # Get observation and action specs.\n    act_spec = environment_spec.actions\n    obs_spec = environment_spec.observations\n    emb_spec = tf2_utils.create_variables(observation_network, [obs_spec])\n\n    # Create the behavior policy.\n    behavior_network = snt.Sequential([\n        observation_network,\n        policy_network,\n        networks.StochasticSamplingHead(),\n    ])\n\n    # Create variables.\n    tf2_utils.create_variables(policy_network, [emb_spec])\n    tf2_utils.create_variables(critic_network, [emb_spec, act_spec])\n    tf2_utils.create_variables(target_policy_network, [emb_spec])\n    tf2_utils.create_variables(target_critic_network, [emb_spec, act_spec])\n    tf2_utils.create_variables(target_observation_network, [obs_spec])\n\n    # Create the actor which defines how we take actions.\n    actor = actors.FeedForwardActor(\n        policy_network=behavior_network, adder=adder)\n\n    # Create optimizers.\n    policy_optimizer = policy_optimizer or snt.optimizers.Adam(1e-4)\n    critic_optimizer = critic_optimizer or snt.optimizers.Adam(1e-4)\n\n    # The learner updates the parameters (and initializes them).\n    learner = learning.MultiObjectiveMPOLearner(\n        reward_objectives=reward_objectives,\n        qvalue_objectives=qvalue_objectives,\n        policy_network=policy_network,\n        critic_network=critic_network,\n        observation_network=observation_network,\n        target_policy_network=target_policy_network,\n        target_critic_network=target_critic_network,\n        target_observation_network=target_observation_network,\n        policy_loss_module=policy_loss_module,\n        policy_optimizer=policy_optimizer,\n        critic_optimizer=critic_optimizer,\n        clipping=clipping,\n        discount=discount,\n        num_samples=num_samples,\n        target_policy_update_period=target_policy_update_period,\n        target_critic_update_period=target_critic_update_period,\n        dataset=dataset,\n        logger=logger,\n        counter=counter,\n        checkpoint=checkpoint)\n\n    super().__init__(\n        actor=actor,\n        learner=learner,\n        min_observations=max(batch_size, min_replay_size),\n        observations_per_step=float(batch_size) / samples_per_insert)",
  "def __init__(self,\n               reward_objectives: Sequence[learning.RewardObjective],\n               qvalue_objectives: Sequence[learning.QValueObjective],\n               environment_spec: specs.EnvironmentSpec,\n               policy_network: snt.Module,\n               critic_network: snt.Module,\n               observation_network: types.TensorTransformation = tf.identity,\n               discount: float = 0.99,\n               batch_size: int = 512,\n               prefetch_size: int = 4,\n               target_policy_update_period: int = 200,\n               target_critic_update_period: int = 200,\n               min_replay_size: int = 1000,\n               max_replay_size: int = 1000000,\n               samples_per_insert: float = 16.,\n               policy_loss_module: Optional[losses.MultiObjectiveMPO] = None,\n               policy_optimizer: Optional[snt.Optimizer] = None,\n               critic_optimizer: Optional[snt.Optimizer] = None,\n               n_step: int = 5,\n               num_samples: int = 20,\n               clipping: bool = True,\n               logger: Optional[loggers.Logger] = None,\n               counter: Optional[counting.Counter] = None,\n               checkpoint: bool = True,\n               replay_table_name: str = adders.DEFAULT_PRIORITY_TABLE):\n    \"\"\"Initialize the agent.\n\n    Args:\n      reward_objectives: list of the objectives that the policy should optimize;\n        each objective is defined by its reward function\n      qvalue_objectives: list of the objectives that the policy should optimize;\n        each objective is defined by its Q-value function\n      environment_spec: description of the actions, observations, etc.\n      policy_network: the online (optimized) policy.\n      critic_network: the online critic.\n      observation_network: optional network to transform the observations before\n        they are fed into any network.\n      discount: discount to use for TD updates.\n      batch_size: batch size for updates.\n      prefetch_size: size to prefetch from replay.\n      target_policy_update_period: number of updates to perform before updating\n        the target policy network.\n      target_critic_update_period: number of updates to perform before updating\n        the target critic network.\n      min_replay_size: minimum replay size before updating.\n      max_replay_size: maximum replay size.\n      samples_per_insert: number of samples to take from replay for every insert\n        that is made.\n      policy_loss_module: configured MO-MPO loss function for the policy\n        optimization; defaults to sensible values on the control suite.\n        See `acme/tf/losses/mompo.py` for more details.\n      policy_optimizer: optimizer to be used on the policy.\n      critic_optimizer: optimizer to be used on the critic.\n      n_step: number of steps to squash into a single transition.\n      num_samples: number of actions to sample when doing a Monte Carlo\n        integration with respect to the policy.\n      clipping: whether to clip gradients by global norm.\n      logger: logging object used to write to logs.\n      counter: counter object used to keep track of steps.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n      replay_table_name: string indicating what name to give the replay table.\n    \"\"\"\n    # Check that at least one objective's reward function is specified.\n    if not reward_objectives:\n      raise ValueError('Must specify at least one reward objective.')\n\n    # Create a replay server to add data to.\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=max_replay_size,\n        rate_limiter=reverb.rate_limiters.MinSize(min_size_to_sample=1),\n        signature=adders.NStepTransitionAdder.signature(environment_spec))\n    self._server = reverb.Server([replay_table], port=None)\n\n    # The adder is used to insert observations into replay.\n    address = f'localhost:{self._server.port}'\n    adder = adders.NStepTransitionAdder(\n        client=reverb.Client(address),\n        n_step=n_step,\n        discount=discount)\n\n    # The dataset object to learn from.\n    dataset = datasets.make_reverb_dataset(\n        table=replay_table_name,\n        server_address=address,\n        batch_size=batch_size,\n        prefetch_size=prefetch_size)\n\n    # Make sure observation network is a Sonnet Module.\n    observation_network = tf2_utils.to_sonnet_module(observation_network)\n\n    # Create target networks before creating online/target network variables.\n    target_policy_network = copy.deepcopy(policy_network)\n    target_critic_network = copy.deepcopy(critic_network)\n    target_observation_network = copy.deepcopy(observation_network)\n\n    # Get observation and action specs.\n    act_spec = environment_spec.actions\n    obs_spec = environment_spec.observations\n    emb_spec = tf2_utils.create_variables(observation_network, [obs_spec])\n\n    # Create the behavior policy.\n    behavior_network = snt.Sequential([\n        observation_network,\n        policy_network,\n        networks.StochasticSamplingHead(),\n    ])\n\n    # Create variables.\n    tf2_utils.create_variables(policy_network, [emb_spec])\n    tf2_utils.create_variables(critic_network, [emb_spec, act_spec])\n    tf2_utils.create_variables(target_policy_network, [emb_spec])\n    tf2_utils.create_variables(target_critic_network, [emb_spec, act_spec])\n    tf2_utils.create_variables(target_observation_network, [obs_spec])\n\n    # Create the actor which defines how we take actions.\n    actor = actors.FeedForwardActor(\n        policy_network=behavior_network, adder=adder)\n\n    # Create optimizers.\n    policy_optimizer = policy_optimizer or snt.optimizers.Adam(1e-4)\n    critic_optimizer = critic_optimizer or snt.optimizers.Adam(1e-4)\n\n    # The learner updates the parameters (and initializes them).\n    learner = learning.MultiObjectiveMPOLearner(\n        reward_objectives=reward_objectives,\n        qvalue_objectives=qvalue_objectives,\n        policy_network=policy_network,\n        critic_network=critic_network,\n        observation_network=observation_network,\n        target_policy_network=target_policy_network,\n        target_critic_network=target_critic_network,\n        target_observation_network=target_observation_network,\n        policy_loss_module=policy_loss_module,\n        policy_optimizer=policy_optimizer,\n        critic_optimizer=critic_optimizer,\n        clipping=clipping,\n        discount=discount,\n        num_samples=num_samples,\n        target_policy_update_period=target_policy_update_period,\n        target_critic_update_period=target_critic_update_period,\n        dataset=dataset,\n        logger=logger,\n        counter=counter,\n        checkpoint=checkpoint)\n\n    super().__init__(\n        actor=actor,\n        learner=learner,\n        min_observations=max(batch_size, min_replay_size),\n        observations_per_step=float(batch_size) / samples_per_insert)",
  "class QValueObjective:\n  \"\"\"Defines an objective by specifying its 'Q-values' directly.\"\"\"\n\n  name: str\n  # This computes \"Q-values\" directly from the sampled actions and other Q's.\n  qvalue_fn: QValueFunctionSpec",
  "class RewardObjective:\n  \"\"\"Defines an objective by specifying its reward function.\"\"\"\n\n  name: str\n  # This computes the reward from observations, actions, and environment task\n  # reward. In the learner, a head will automatically be added to the critic\n  # network, to learn Q-values for this objective.\n  reward_fn: RewardFunctionSpec",
  "class MultiObjectiveMPOLearner(acme.Learner):\n  \"\"\"Distributional MPO learner.\n\n  This is the learning component of a multi-objective MPO (MO-MPO) agent. Two\n  sequences of objectives must be specified. Otherwise, the inputs are identical\n  to those of the MPO / DMPO learners.\n\n  Each objective must be defined as either a RewardObjective or an\n  QValueObjective. These objectives are provided by the reward_objectives and\n  qvalue_objectives parameters, respectively. For each RewardObjective, a critic\n  will be trained to estimate Q-values for that objective. Whereas for each\n  QValueObjective, the Q-values are computed directly by its qvalue_fn.\n\n  A RewardObjective's reward_fn takes the observation, action, and environment\n  reward as input, and returns the reward for that objective. For example, if\n  the environment reward is a scalar, then an objective corresponding to the =\n  task would simply return the environment reward.\n\n  A QValueObjective's qvalue_fn takes the actions and reward-based objectives'\n  Q-values as input, and outputs the \"Q-values\" for that objective. For\n  instance, in the MO-MPO paper ([Abdolmaleki, Huang et al., 2020]), the action\n  norm objective in the Humanoid run task is defined by setting the qvalue_fn\n  to be the l2-norm of the actions.\n\n  Note: If there is only one objective and that is the task reward, then this\n  algorithm becomes exactly the same as (D)MPO.\n\n  (Abdolmaleki, Huang et al., 2020): https://arxiv.org/pdf/2005.07513.pdf\n  \"\"\"\n\n  def __init__(\n      self,\n      reward_objectives: Sequence[RewardObjective],\n      qvalue_objectives: Sequence[QValueObjective],\n      policy_network: snt.Module,\n      critic_network: snt.Module,\n      target_policy_network: snt.Module,\n      target_critic_network: snt.Module,\n      discount: float,\n      num_samples: int,\n      target_policy_update_period: int,\n      target_critic_update_period: int,\n      dataset: tf.data.Dataset,\n      observation_network: types.TensorTransformation = tf.identity,\n      target_observation_network: types.TensorTransformation = tf.identity,\n      policy_loss_module: Optional[losses.MultiObjectiveMPO] = None,\n      policy_optimizer: Optional[snt.Optimizer] = None,\n      critic_optimizer: Optional[snt.Optimizer] = None,\n      dual_optimizer: Optional[snt.Optimizer] = None,\n      clipping: bool = True,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = True,\n  ):\n\n    # Store online and target networks.\n    self._policy_network = policy_network\n    self._critic_network = critic_network\n    self._target_policy_network = target_policy_network\n    self._target_critic_network = target_critic_network\n\n    # Make sure observation networks are snt.Module's so they have variables.\n    self._observation_network = tf2_utils.to_sonnet_module(observation_network)\n    self._target_observation_network = tf2_utils.to_sonnet_module(\n        target_observation_network)\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger('learner')\n\n    # Other learner parameters.\n    self._discount = discount\n    self._num_samples = num_samples\n    self._clipping = clipping\n\n    # Necessary to track when to update target networks.\n    self._num_steps = tf.Variable(0, dtype=tf.int32)\n    self._target_policy_update_period = target_policy_update_period\n    self._target_critic_update_period = target_critic_update_period\n\n    # Batch dataset and create iterator.\n    # TODO(b/155086959): Fix type stubs and remove.\n    self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n    # Store objectives\n    self._reward_objectives = reward_objectives\n    self._qvalue_objectives = qvalue_objectives\n    if self._qvalue_objectives is None:\n      self._qvalue_objectives = []\n    self._num_critic_heads = len(self._reward_objectives)  # C\n    self._objective_names = (\n        [x.name for x in self._reward_objectives] +\n        [x.name for x in self._qvalue_objectives])\n\n    self._policy_loss_module = policy_loss_module or losses.MultiObjectiveMPO(\n        epsilons=[losses.KLConstraint(name, _DEFAULT_EPSILON)\n                  for name in self._objective_names],\n        epsilon_mean=_DEFAULT_EPSILON_MEAN,\n        epsilon_stddev=_DEFAULT_EPSILON_STDDEV,\n        init_log_temperature=_DEFAULT_INIT_LOG_TEMPERATURE,\n        init_log_alpha_mean=_DEFAULT_INIT_LOG_ALPHA_MEAN,\n        init_log_alpha_stddev=_DEFAULT_INIT_LOG_ALPHA_STDDEV)\n\n    # Check that ordering of objectives matches the policy_loss_module's\n    if self._objective_names != list(self._policy_loss_module.objective_names):\n      raise ValueError(\"Agent's ordering of objectives doesn't match \"\n                       \"the policy loss module's ordering of epsilons.\")\n\n    # Create the optimizers.\n    self._critic_optimizer = critic_optimizer or snt.optimizers.Adam(1e-4)\n    self._policy_optimizer = policy_optimizer or snt.optimizers.Adam(1e-4)\n    self._dual_optimizer = dual_optimizer or snt.optimizers.Adam(1e-2)\n\n    # Expose the variables.\n    policy_network_to_expose = snt.Sequential(\n        [self._target_observation_network, self._target_policy_network])\n    self._variables = {\n        'critic': self._target_critic_network.variables,\n        'policy': policy_network_to_expose.variables,\n    }\n\n    # Create a checkpointer and snapshotter object.\n    self._checkpointer = None\n    self._snapshotter = None\n\n    if checkpoint:\n      self._checkpointer = tf2_savers.Checkpointer(\n          subdirectory='mompo_learner',\n          objects_to_save={\n              'counter': self._counter,\n              'policy': self._policy_network,\n              'critic': self._critic_network,\n              'observation': self._observation_network,\n              'target_policy': self._target_policy_network,\n              'target_critic': self._target_critic_network,\n              'target_observation': self._target_observation_network,\n              'policy_optimizer': self._policy_optimizer,\n              'critic_optimizer': self._critic_optimizer,\n              'dual_optimizer': self._dual_optimizer,\n              'policy_loss_module': self._policy_loss_module,\n              'num_steps': self._num_steps,\n          })\n\n      self._snapshotter = tf2_savers.Snapshotter(\n          objects_to_save={\n              'policy':\n                  snt.Sequential([\n                      self._target_observation_network,\n                      self._target_policy_network\n                  ]),\n          })\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp: float = None\n\n  @tf.function\n  def _step(self) -> types.NestedTensor:\n    # Update target network.\n    online_policy_variables = self._policy_network.variables\n    target_policy_variables = self._target_policy_network.variables\n    online_critic_variables = (\n        *self._observation_network.variables,\n        *self._critic_network.variables,\n    )\n    target_critic_variables = (\n        *self._target_observation_network.variables,\n        *self._target_critic_network.variables,\n    )\n\n    # Make online policy -> target policy network update ops.\n    if tf.math.mod(self._num_steps, self._target_policy_update_period) == 0:\n      for src, dest in zip(online_policy_variables, target_policy_variables):\n        dest.assign(src)\n    # Make online critic -> target critic network update ops.\n    if tf.math.mod(self._num_steps, self._target_critic_update_period) == 0:\n      for src, dest in zip(online_critic_variables, target_critic_variables):\n        dest.assign(src)\n\n    self._num_steps.assign_add(1)\n\n    # Get data from replay (dropping extras if any). Note there is no\n    # extra data here because we do not insert any into Reverb.\n    inputs = next(self._iterator)\n    transitions: types.Transition = inputs.data\n\n    with tf.GradientTape(persistent=True) as tape:\n      # Maybe transform the observation before feeding into policy and critic.\n      # Transforming the observations this way at the start of the learning\n      # step effectively means that the policy and critic share observation\n      # network weights.\n      o_tm1 = self._observation_network(transitions.observation)\n      # This stop_gradient prevents gradients to propagate into the target\n      # observation network. In addition, since the online policy network is\n      # evaluated at o_t, this also means the policy loss does not influence\n      # the observation network training.\n      o_t = tf.stop_gradient(\n          self._target_observation_network(transitions.next_observation))\n\n      # Get online and target action distributions from policy networks.\n      online_action_distribution = self._policy_network(o_t)\n      target_action_distribution = self._target_policy_network(o_t)\n\n      # Sample actions to evaluate policy; of size [N, B, ...].\n      sampled_actions = target_action_distribution.sample(self._num_samples)\n\n      # Tile embedded observations to feed into the target critic network.\n      # Note: this is more efficient than tiling before the embedding layer.\n      tiled_o_t = tf2_utils.tile_tensor(o_t, self._num_samples)  # [N, B, ...]\n\n      # Compute target-estimated distributional value of sampled actions at o_t.\n      sampled_q_t_all = self._target_critic_network(\n          # Merge batch dimensions; to shape [N*B, ...].\n          snt.merge_leading_dims(tiled_o_t, num_dims=2),\n          snt.merge_leading_dims(sampled_actions, num_dims=2))\n\n      # Compute online critic value distribution of a_tm1 in state o_tm1.\n      q_tm1_all = self._critic_network(o_tm1, transitions.action)\n\n      # Compute rewards for objectives with defined reward_fn\n      reward_stats = {}\n      r_t_all = []\n      for objective in self._reward_objectives:\n        r = objective.reward_fn(o_tm1, transitions.action, transitions.reward)\n        reward_stats['{}_reward'.format(objective.name)] = tf.reduce_mean(r)\n        r_t_all.append(r)\n      r_t_all = tf.stack(r_t_all, axis=-1)\n      r_t_all.get_shape().assert_has_rank(2)  # [B, C]\n\n      if isinstance(sampled_q_t_all, list):  # Distributional critics\n        critic_loss, sampled_q_t = _compute_distributional_critic_loss(\n            sampled_q_t_all, q_tm1_all, r_t_all, transitions.discount,\n            self._discount, self._num_samples)\n      else:\n        critic_loss, sampled_q_t = _compute_critic_loss(\n            sampled_q_t_all, q_tm1_all, r_t_all, transitions.discount,\n            self._discount, self._num_samples, self._num_critic_heads)\n\n      # Add sampled Q-values for objectives with defined qvalue_fn\n      sampled_q_t_k = [sampled_q_t]\n      for objective in self._qvalue_objectives:\n        sampled_q_t_k.append(tf.expand_dims(tf.stop_gradient(\n            objective.qvalue_fn(sampled_actions, sampled_q_t)), axis=-1))\n      sampled_q_t_k = tf.concat(sampled_q_t_k, axis=-1)  # [N, B, K]\n\n      # Compute MPO policy loss.\n      policy_loss, policy_stats = self._policy_loss_module(\n          online_action_distribution=online_action_distribution,\n          target_action_distribution=target_action_distribution,\n          actions=sampled_actions,\n          q_values=sampled_q_t_k)\n\n    # For clarity, explicitly define which variables are trained by which loss.\n    critic_trainable_variables = (\n        # In this agent, the critic loss trains the observation network.\n        self._observation_network.trainable_variables +\n        self._critic_network.trainable_variables)\n    policy_trainable_variables = self._policy_network.trainable_variables\n    # The following are the MPO dual variables, stored in the loss module.\n    dual_trainable_variables = self._policy_loss_module.trainable_variables\n\n    # Compute gradients.\n    critic_gradients = tape.gradient(critic_loss, critic_trainable_variables)\n    policy_gradients, dual_gradients = tape.gradient(\n        policy_loss, (policy_trainable_variables, dual_trainable_variables))\n\n    # Delete the tape manually because of the persistent=True flag.\n    del tape\n\n    # Maybe clip gradients.\n    if self._clipping:\n      policy_gradients = tuple(tf.clip_by_global_norm(policy_gradients, 40.)[0])\n      critic_gradients = tuple(tf.clip_by_global_norm(critic_gradients, 40.)[0])\n\n    # Apply gradients.\n    self._critic_optimizer.apply(critic_gradients, critic_trainable_variables)\n    self._policy_optimizer.apply(policy_gradients, policy_trainable_variables)\n    self._dual_optimizer.apply(dual_gradients, dual_trainable_variables)\n\n    # Losses to track.\n    fetches = {\n        'critic_loss': critic_loss,\n        'policy_loss': policy_loss,\n    }\n    fetches.update(policy_stats)  # Log MPO stats.\n    fetches.update(reward_stats)  # Log reward stats.\n\n    return fetches\n\n  def step(self):\n    # Run the learning step.\n    fetches = self._step()\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n    fetches.update(counts)\n\n    # Checkpoint and attempt to write the logs.\n    if self._checkpointer is not None:\n      self._checkpointer.save()\n    if self._snapshotter is not None:\n      self._snapshotter.save()\n    self._logger.write(fetches)\n\n  def get_variables(self, names: List[str]) -> List[List[np.ndarray]]:\n    return [tf2_utils.to_numpy(self._variables[name]) for name in names]",
  "def _compute_distributional_critic_loss(\n    sampled_q_t_all: List[tf.Tensor],\n    q_tm1_all: List[tf.Tensor],\n    r_t_all: tf.Tensor,\n    d_t: tf.Tensor,\n    discount: float,\n    num_samples: int):\n  \"\"\"Compute loss and sampled Q-values for distributional critics.\"\"\"\n  # Compute average logits by first reshaping them and normalizing them\n  # across atoms.\n  batch_size = r_t_all.get_shape()[0]\n  # Cast the additional discount to match the environment discount dtype.\n  discount = tf.cast(discount, dtype=d_t.dtype)\n  critic_losses = []\n  sampled_q_ts = []\n  for idx, (sampled_q_t_distributions, q_tm1_distribution) in enumerate(\n      zip(sampled_q_t_all, q_tm1_all)):\n    # Compute loss for distributional critic for objective c\n    sampled_logits = tf.reshape(\n        sampled_q_t_distributions.logits,\n        [num_samples, batch_size, -1])  # [N, B, A]\n    sampled_logprobs = tf.math.log_softmax(sampled_logits, axis=-1)\n    averaged_logits = tf.reduce_logsumexp(sampled_logprobs, axis=0)\n\n    # Construct the expected distributional value for bootstrapping.\n    q_t_distribution = networks.DiscreteValuedDistribution(\n        values=sampled_q_t_distributions.values, logits=averaged_logits)\n\n    # Compute critic distributional loss.\n    critic_loss = losses.categorical(\n        q_tm1_distribution, r_t_all[:, idx], discount * d_t,\n        q_t_distribution)\n    critic_losses.append(tf.reduce_mean(critic_loss))\n\n    # Compute Q-values of sampled actions and reshape to [N, B].\n    sampled_q_ts.append(tf.reshape(\n        sampled_q_t_distributions.mean(), (num_samples, -1)))\n\n  critic_loss = tf.reduce_mean(critic_losses)\n  sampled_q_t = tf.stack(sampled_q_ts, axis=-1)  # [N, B, C]\n  return critic_loss, sampled_q_t",
  "def _compute_critic_loss(\n    sampled_q_t_all: tf.Tensor,\n    q_tm1_all: tf.Tensor,\n    r_t_all: tf.Tensor,\n    d_t: tf.Tensor,\n    discount: float,\n    num_samples: int,\n    num_critic_heads: int):\n  \"\"\"Compute loss and sampled Q-values for (non-distributional) critics.\"\"\"\n  # Reshape Q-value samples back to original batch dimensions and average\n  # them to compute the TD-learning bootstrap target.\n  batch_size = r_t_all.get_shape()[0]\n  sampled_q_t = tf.reshape(\n      sampled_q_t_all,\n      (num_samples, batch_size, num_critic_heads))  # [N,B,C]\n  q_t = tf.reduce_mean(sampled_q_t, axis=0)  # [B, C]\n\n  # Flatten q_t and q_tm1; necessary for trfl.td_learning\n  q_t = tf.reshape(q_t, [-1])  # [B*C]\n  q_tm1 = tf.reshape(q_tm1_all, [-1])  # [B*C]\n\n  # Flatten r_t_all; necessary for trfl.td_learning\n  r_t_all = tf.reshape(r_t_all, [-1])  # [B*C]\n\n  # Broadcast and then flatten d_t, to match shape of q_t and q_tm1\n  d_t = tf.tile(d_t, [num_critic_heads])  # [B*C]\n  # Cast the additional discount to match the environment discount dtype.\n  discount = tf.cast(discount, dtype=d_t.dtype)\n\n  # Critic loss.\n  critic_loss = trfl.td_learning(q_tm1, r_t_all, discount * d_t, q_t).loss\n  critic_loss = tf.reduce_mean(critic_loss)\n  return critic_loss, sampled_q_t",
  "def __init__(\n      self,\n      reward_objectives: Sequence[RewardObjective],\n      qvalue_objectives: Sequence[QValueObjective],\n      policy_network: snt.Module,\n      critic_network: snt.Module,\n      target_policy_network: snt.Module,\n      target_critic_network: snt.Module,\n      discount: float,\n      num_samples: int,\n      target_policy_update_period: int,\n      target_critic_update_period: int,\n      dataset: tf.data.Dataset,\n      observation_network: types.TensorTransformation = tf.identity,\n      target_observation_network: types.TensorTransformation = tf.identity,\n      policy_loss_module: Optional[losses.MultiObjectiveMPO] = None,\n      policy_optimizer: Optional[snt.Optimizer] = None,\n      critic_optimizer: Optional[snt.Optimizer] = None,\n      dual_optimizer: Optional[snt.Optimizer] = None,\n      clipping: bool = True,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = True,\n  ):\n\n    # Store online and target networks.\n    self._policy_network = policy_network\n    self._critic_network = critic_network\n    self._target_policy_network = target_policy_network\n    self._target_critic_network = target_critic_network\n\n    # Make sure observation networks are snt.Module's so they have variables.\n    self._observation_network = tf2_utils.to_sonnet_module(observation_network)\n    self._target_observation_network = tf2_utils.to_sonnet_module(\n        target_observation_network)\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger('learner')\n\n    # Other learner parameters.\n    self._discount = discount\n    self._num_samples = num_samples\n    self._clipping = clipping\n\n    # Necessary to track when to update target networks.\n    self._num_steps = tf.Variable(0, dtype=tf.int32)\n    self._target_policy_update_period = target_policy_update_period\n    self._target_critic_update_period = target_critic_update_period\n\n    # Batch dataset and create iterator.\n    # TODO(b/155086959): Fix type stubs and remove.\n    self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n    # Store objectives\n    self._reward_objectives = reward_objectives\n    self._qvalue_objectives = qvalue_objectives\n    if self._qvalue_objectives is None:\n      self._qvalue_objectives = []\n    self._num_critic_heads = len(self._reward_objectives)  # C\n    self._objective_names = (\n        [x.name for x in self._reward_objectives] +\n        [x.name for x in self._qvalue_objectives])\n\n    self._policy_loss_module = policy_loss_module or losses.MultiObjectiveMPO(\n        epsilons=[losses.KLConstraint(name, _DEFAULT_EPSILON)\n                  for name in self._objective_names],\n        epsilon_mean=_DEFAULT_EPSILON_MEAN,\n        epsilon_stddev=_DEFAULT_EPSILON_STDDEV,\n        init_log_temperature=_DEFAULT_INIT_LOG_TEMPERATURE,\n        init_log_alpha_mean=_DEFAULT_INIT_LOG_ALPHA_MEAN,\n        init_log_alpha_stddev=_DEFAULT_INIT_LOG_ALPHA_STDDEV)\n\n    # Check that ordering of objectives matches the policy_loss_module's\n    if self._objective_names != list(self._policy_loss_module.objective_names):\n      raise ValueError(\"Agent's ordering of objectives doesn't match \"\n                       \"the policy loss module's ordering of epsilons.\")\n\n    # Create the optimizers.\n    self._critic_optimizer = critic_optimizer or snt.optimizers.Adam(1e-4)\n    self._policy_optimizer = policy_optimizer or snt.optimizers.Adam(1e-4)\n    self._dual_optimizer = dual_optimizer or snt.optimizers.Adam(1e-2)\n\n    # Expose the variables.\n    policy_network_to_expose = snt.Sequential(\n        [self._target_observation_network, self._target_policy_network])\n    self._variables = {\n        'critic': self._target_critic_network.variables,\n        'policy': policy_network_to_expose.variables,\n    }\n\n    # Create a checkpointer and snapshotter object.\n    self._checkpointer = None\n    self._snapshotter = None\n\n    if checkpoint:\n      self._checkpointer = tf2_savers.Checkpointer(\n          subdirectory='mompo_learner',\n          objects_to_save={\n              'counter': self._counter,\n              'policy': self._policy_network,\n              'critic': self._critic_network,\n              'observation': self._observation_network,\n              'target_policy': self._target_policy_network,\n              'target_critic': self._target_critic_network,\n              'target_observation': self._target_observation_network,\n              'policy_optimizer': self._policy_optimizer,\n              'critic_optimizer': self._critic_optimizer,\n              'dual_optimizer': self._dual_optimizer,\n              'policy_loss_module': self._policy_loss_module,\n              'num_steps': self._num_steps,\n          })\n\n      self._snapshotter = tf2_savers.Snapshotter(\n          objects_to_save={\n              'policy':\n                  snt.Sequential([\n                      self._target_observation_network,\n                      self._target_policy_network\n                  ]),\n          })\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp: float = None",
  "def _step(self) -> types.NestedTensor:\n    # Update target network.\n    online_policy_variables = self._policy_network.variables\n    target_policy_variables = self._target_policy_network.variables\n    online_critic_variables = (\n        *self._observation_network.variables,\n        *self._critic_network.variables,\n    )\n    target_critic_variables = (\n        *self._target_observation_network.variables,\n        *self._target_critic_network.variables,\n    )\n\n    # Make online policy -> target policy network update ops.\n    if tf.math.mod(self._num_steps, self._target_policy_update_period) == 0:\n      for src, dest in zip(online_policy_variables, target_policy_variables):\n        dest.assign(src)\n    # Make online critic -> target critic network update ops.\n    if tf.math.mod(self._num_steps, self._target_critic_update_period) == 0:\n      for src, dest in zip(online_critic_variables, target_critic_variables):\n        dest.assign(src)\n\n    self._num_steps.assign_add(1)\n\n    # Get data from replay (dropping extras if any). Note there is no\n    # extra data here because we do not insert any into Reverb.\n    inputs = next(self._iterator)\n    transitions: types.Transition = inputs.data\n\n    with tf.GradientTape(persistent=True) as tape:\n      # Maybe transform the observation before feeding into policy and critic.\n      # Transforming the observations this way at the start of the learning\n      # step effectively means that the policy and critic share observation\n      # network weights.\n      o_tm1 = self._observation_network(transitions.observation)\n      # This stop_gradient prevents gradients to propagate into the target\n      # observation network. In addition, since the online policy network is\n      # evaluated at o_t, this also means the policy loss does not influence\n      # the observation network training.\n      o_t = tf.stop_gradient(\n          self._target_observation_network(transitions.next_observation))\n\n      # Get online and target action distributions from policy networks.\n      online_action_distribution = self._policy_network(o_t)\n      target_action_distribution = self._target_policy_network(o_t)\n\n      # Sample actions to evaluate policy; of size [N, B, ...].\n      sampled_actions = target_action_distribution.sample(self._num_samples)\n\n      # Tile embedded observations to feed into the target critic network.\n      # Note: this is more efficient than tiling before the embedding layer.\n      tiled_o_t = tf2_utils.tile_tensor(o_t, self._num_samples)  # [N, B, ...]\n\n      # Compute target-estimated distributional value of sampled actions at o_t.\n      sampled_q_t_all = self._target_critic_network(\n          # Merge batch dimensions; to shape [N*B, ...].\n          snt.merge_leading_dims(tiled_o_t, num_dims=2),\n          snt.merge_leading_dims(sampled_actions, num_dims=2))\n\n      # Compute online critic value distribution of a_tm1 in state o_tm1.\n      q_tm1_all = self._critic_network(o_tm1, transitions.action)\n\n      # Compute rewards for objectives with defined reward_fn\n      reward_stats = {}\n      r_t_all = []\n      for objective in self._reward_objectives:\n        r = objective.reward_fn(o_tm1, transitions.action, transitions.reward)\n        reward_stats['{}_reward'.format(objective.name)] = tf.reduce_mean(r)\n        r_t_all.append(r)\n      r_t_all = tf.stack(r_t_all, axis=-1)\n      r_t_all.get_shape().assert_has_rank(2)  # [B, C]\n\n      if isinstance(sampled_q_t_all, list):  # Distributional critics\n        critic_loss, sampled_q_t = _compute_distributional_critic_loss(\n            sampled_q_t_all, q_tm1_all, r_t_all, transitions.discount,\n            self._discount, self._num_samples)\n      else:\n        critic_loss, sampled_q_t = _compute_critic_loss(\n            sampled_q_t_all, q_tm1_all, r_t_all, transitions.discount,\n            self._discount, self._num_samples, self._num_critic_heads)\n\n      # Add sampled Q-values for objectives with defined qvalue_fn\n      sampled_q_t_k = [sampled_q_t]\n      for objective in self._qvalue_objectives:\n        sampled_q_t_k.append(tf.expand_dims(tf.stop_gradient(\n            objective.qvalue_fn(sampled_actions, sampled_q_t)), axis=-1))\n      sampled_q_t_k = tf.concat(sampled_q_t_k, axis=-1)  # [N, B, K]\n\n      # Compute MPO policy loss.\n      policy_loss, policy_stats = self._policy_loss_module(\n          online_action_distribution=online_action_distribution,\n          target_action_distribution=target_action_distribution,\n          actions=sampled_actions,\n          q_values=sampled_q_t_k)\n\n    # For clarity, explicitly define which variables are trained by which loss.\n    critic_trainable_variables = (\n        # In this agent, the critic loss trains the observation network.\n        self._observation_network.trainable_variables +\n        self._critic_network.trainable_variables)\n    policy_trainable_variables = self._policy_network.trainable_variables\n    # The following are the MPO dual variables, stored in the loss module.\n    dual_trainable_variables = self._policy_loss_module.trainable_variables\n\n    # Compute gradients.\n    critic_gradients = tape.gradient(critic_loss, critic_trainable_variables)\n    policy_gradients, dual_gradients = tape.gradient(\n        policy_loss, (policy_trainable_variables, dual_trainable_variables))\n\n    # Delete the tape manually because of the persistent=True flag.\n    del tape\n\n    # Maybe clip gradients.\n    if self._clipping:\n      policy_gradients = tuple(tf.clip_by_global_norm(policy_gradients, 40.)[0])\n      critic_gradients = tuple(tf.clip_by_global_norm(critic_gradients, 40.)[0])\n\n    # Apply gradients.\n    self._critic_optimizer.apply(critic_gradients, critic_trainable_variables)\n    self._policy_optimizer.apply(policy_gradients, policy_trainable_variables)\n    self._dual_optimizer.apply(dual_gradients, dual_trainable_variables)\n\n    # Losses to track.\n    fetches = {\n        'critic_loss': critic_loss,\n        'policy_loss': policy_loss,\n    }\n    fetches.update(policy_stats)  # Log MPO stats.\n    fetches.update(reward_stats)  # Log reward stats.\n\n    return fetches",
  "def step(self):\n    # Run the learning step.\n    fetches = self._step()\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n    fetches.update(counts)\n\n    # Checkpoint and attempt to write the logs.\n    if self._checkpointer is not None:\n      self._checkpointer.save()\n    if self._snapshotter is not None:\n      self._snapshotter.save()\n    self._logger.write(fetches)",
  "def get_variables(self, names: List[str]) -> List[List[np.ndarray]]:\n    return [tf2_utils.to_numpy(self._variables[name]) for name in names]",
  "def make_networks(\n    action_spec: specs.BoundedArray,\n    num_critic_heads: int,\n    policy_layer_sizes: Sequence[int] = (50,),\n    critic_layer_sizes: Sequence[int] = (50,),\n    num_layers_shared: int = 1,\n    distributional_critic: bool = True,\n    vmin: float = -150.,\n    vmax: float = 150.,\n    num_atoms: int = 51,\n):\n  \"\"\"Creates networks used by the agent.\"\"\"\n\n  num_dimensions = np.prod(action_spec.shape, dtype=int)\n\n  policy_network = snt.Sequential([\n      networks.LayerNormMLP(policy_layer_sizes, activate_final=True),\n      networks.MultivariateNormalDiagHead(\n          num_dimensions,\n          tanh_mean=False,\n          init_scale=0.69)\n  ])\n\n  if not distributional_critic:\n    critic_layer_sizes = list(critic_layer_sizes) + [1]\n\n  if not num_layers_shared:\n    # No layers are shared\n    critic_network_base = None\n  else:\n    critic_network_base = networks.LayerNormMLP(\n        critic_layer_sizes[:num_layers_shared], activate_final=True)\n  critic_network_heads = [\n      snt.nets.MLP(critic_layer_sizes, activation=tf.nn.elu,\n                   activate_final=False)\n      for _ in range(num_critic_heads)]\n  if distributional_critic:\n    critic_network_heads = [\n        snt.Sequential([\n            c, networks.DiscreteValuedHead(vmin, vmax, num_atoms)\n        ]) for c in critic_network_heads]\n  # The multiplexer concatenates the (maybe transformed) observations/actions.\n  critic_network = snt.Sequential([\n      networks.CriticMultiplexer(\n          critic_network=critic_network_base,\n          action_network=networks.ClipToSpec(action_spec)),\n      networks.Multihead(network_heads=critic_network_heads),\n  ])\n\n  return {\n      'policy': policy_network,\n      'critic': critic_network,\n      'observation': tf2_utils.batch_concat,\n  }",
  "def make_environment(evaluation: bool = False):\n  del evaluation  # Unused.\n  environment = suite.load('cartpole', 'balance')\n  wrapped = wrappers.SinglePrecisionWrapper(environment)\n  return wrapped",
  "def compute_action_norm(target_pi_samples: tf.Tensor,\n                        target_q_target_pi_samples: tf.Tensor) -> tf.Tensor:\n  \"\"\"Compute Q-values for the action norm objective from action samples.\"\"\"\n  del target_q_target_pi_samples\n  action_norm = tf.norm(target_pi_samples, ord=2, axis=-1)\n  return tf.stop_gradient(-1 * action_norm)",
  "def task_reward_fn(observation: tf.Tensor,\n                   action: tf.Tensor,\n                   reward: tf.Tensor) -> tf.Tensor:\n  del observation, action\n  return tf.stop_gradient(reward)",
  "def make_objectives() -> Tuple[\n    Sequence[mompo.RewardObjective], Sequence[mompo.QValueObjective]]:\n  \"\"\"Define the multiple objectives for the policy to learn.\"\"\"\n  task_reward = mompo.RewardObjective(\n      name='task',\n      reward_fn=task_reward_fn)\n  action_norm = mompo.QValueObjective(\n      name='action_norm_q',\n      qvalue_fn=compute_action_norm)\n  return [task_reward], [action_norm]",
  "class DistributedAgentTest(parameterized.TestCase):\n  \"\"\"Simple integration/smoke test for the distributed agent.\"\"\"\n\n  @parameterized.named_parameters(\n      ('distributional_critic', True),\n      ('vanilla_critic', False))\n  def test_agent(self, distributional_critic):\n    # Create objectives.\n    reward_objectives, qvalue_objectives = make_objectives()\n\n    network_factory = lp_utils.partial_kwargs(\n        make_networks, distributional_critic=distributional_critic)\n\n    agent = mompo.DistributedMultiObjectiveMPO(\n        reward_objectives,\n        qvalue_objectives,\n        environment_factory=make_environment,\n        network_factory=network_factory,\n        num_actors=2,\n        batch_size=32,\n        min_replay_size=32,\n        max_replay_size=1000,\n    )\n    program = agent.build()\n\n    (learner_node,) = program.groups['learner']\n    learner_node.disable_run()\n\n    lp.launch(program, launch_type='test_mt')\n\n    learner: acme.Learner = learner_node.create_handle().dereference()\n\n    for _ in range(5):\n      learner.step()",
  "def test_agent(self, distributional_critic):\n    # Create objectives.\n    reward_objectives, qvalue_objectives = make_objectives()\n\n    network_factory = lp_utils.partial_kwargs(\n        make_networks, distributional_critic=distributional_critic)\n\n    agent = mompo.DistributedMultiObjectiveMPO(\n        reward_objectives,\n        qvalue_objectives,\n        environment_factory=make_environment,\n        network_factory=network_factory,\n        num_actors=2,\n        batch_size=32,\n        min_replay_size=32,\n        max_replay_size=1000,\n    )\n    program = agent.build()\n\n    (learner_node,) = program.groups['learner']\n    learner_node.disable_run()\n\n    lp.launch(program, launch_type='test_mt')\n\n    learner: acme.Learner = learner_node.create_handle().dereference()\n\n    for _ in range(5):\n      learner.step()",
  "def make_networks(\n    action_spec: specs.Array,\n    num_critic_heads: int,\n    policy_layer_sizes: Sequence[int] = (300, 200),\n    critic_layer_sizes: Sequence[int] = (400, 300),\n    num_layers_shared: int = 1,\n    distributional_critic: bool = True,\n    vmin: float = -150.,\n    vmax: float = 150.,\n    num_atoms: int = 51,\n) -> Dict[str, snt.Module]:\n  \"\"\"Creates networks used by the agent.\"\"\"\n\n  num_dimensions = np.prod(action_spec.shape, dtype=int)\n\n  policy_network = snt.Sequential([\n      networks.LayerNormMLP(policy_layer_sizes, activate_final=True),\n      networks.MultivariateNormalDiagHead(\n          num_dimensions,\n          tanh_mean=False,\n          init_scale=0.69)\n  ])\n\n  if not distributional_critic:\n    critic_layer_sizes = list(critic_layer_sizes) + [1]\n\n  if not num_layers_shared:\n    # No layers are shared\n    critic_network_base = None\n  else:\n    critic_network_base = networks.LayerNormMLP(\n        critic_layer_sizes[:num_layers_shared], activate_final=True)\n  critic_network_heads = [\n      snt.nets.MLP(critic_layer_sizes, activation=tf.nn.elu,\n                   activate_final=False)\n      for _ in range(num_critic_heads)]\n  if distributional_critic:\n    critic_network_heads = [\n        snt.Sequential([\n            c, networks.DiscreteValuedHead(vmin, vmax, num_atoms)\n        ]) for c in critic_network_heads]\n  # The multiplexer concatenates the (maybe transformed) observations/actions.\n  critic_network = snt.Sequential([\n      networks.CriticMultiplexer(\n          critic_network=critic_network_base),\n      networks.Multihead(network_heads=critic_network_heads),\n  ])\n  return {\n      'policy': policy_network,\n      'critic': critic_network,\n  }",
  "def compute_action_norm(target_pi_samples: tf.Tensor,\n                        target_q_target_pi_samples: tf.Tensor) -> tf.Tensor:\n  \"\"\"Compute Q-values for the action norm objective from action samples.\"\"\"\n  del target_q_target_pi_samples\n  action_norm = tf.norm(target_pi_samples, ord=2, axis=-1)\n  return tf.stop_gradient(-1 * action_norm)",
  "def task_reward_fn(observation: tf.Tensor,\n                   action: tf.Tensor,\n                   reward: tf.Tensor) -> tf.Tensor:\n  del observation, action\n  return tf.stop_gradient(reward)",
  "def make_objectives() -> Tuple[\n    Sequence[mompo.RewardObjective], Sequence[mompo.QValueObjective]]:\n  \"\"\"Define the multiple objectives for the policy to learn.\"\"\"\n  task_reward = mompo.RewardObjective(\n      name='task',\n      reward_fn=task_reward_fn)\n  action_norm = mompo.QValueObjective(\n      name='action_norm_q',\n      qvalue_fn=compute_action_norm)\n  return [task_reward], [action_norm]",
  "class MOMPOTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      ('distributional_critic', True),\n      ('vanilla_critic', False))\n  def test_mompo(self, distributional_critic):\n    # Create a fake environment to test with.\n    environment = fakes.ContinuousEnvironment(episode_length=10)\n    spec = specs.make_environment_spec(environment)\n\n    # Create objectives.\n    reward_objectives, qvalue_objectives = make_objectives()\n    num_critic_heads = len(reward_objectives)\n\n    # Create networks.\n    agent_networks = make_networks(\n        spec.actions, num_critic_heads=num_critic_heads,\n        distributional_critic=distributional_critic)\n\n    # Construct the agent.\n    agent = mompo.MultiObjectiveMPO(\n        reward_objectives,\n        qvalue_objectives,\n        spec,\n        policy_network=agent_networks['policy'],\n        critic_network=agent_networks['critic'],\n        batch_size=10,\n        samples_per_insert=2,\n        min_replay_size=10)\n\n    # Try running the environment loop. We have no assertions here because all\n    # we care about is that the agent runs without raising any errors.\n    loop = acme.EnvironmentLoop(environment, agent)\n    loop.run(num_episodes=2)",
  "def test_mompo(self, distributional_critic):\n    # Create a fake environment to test with.\n    environment = fakes.ContinuousEnvironment(episode_length=10)\n    spec = specs.make_environment_spec(environment)\n\n    # Create objectives.\n    reward_objectives, qvalue_objectives = make_objectives()\n    num_critic_heads = len(reward_objectives)\n\n    # Create networks.\n    agent_networks = make_networks(\n        spec.actions, num_critic_heads=num_critic_heads,\n        distributional_critic=distributional_critic)\n\n    # Construct the agent.\n    agent = mompo.MultiObjectiveMPO(\n        reward_objectives,\n        qvalue_objectives,\n        spec,\n        policy_network=agent_networks['policy'],\n        critic_network=agent_networks['critic'],\n        batch_size=10,\n        samples_per_insert=2,\n        min_replay_size=10)\n\n    # Try running the environment loop. We have no assertions here because all\n    # we care about is that the agent runs without raising any errors.\n    loop = acme.EnvironmentLoop(environment, agent)\n    loop.run(num_episodes=2)",
  "class DistributedMultiObjectiveMPO:\n  \"\"\"Program definition for multi-objective MPO.\n\n  This agent distinguishes itself from the distributed MPO agent in two ways:\n  - Allowing for one or more objectives (see `acme/agents/tf/mompo/learning.py`\n      for details on what form this sequence of objectives should take)\n  - Optionally using a distributional critic (state-action value approximator)\n      as in DMPO. In other words, the critic network can output either scalar\n      Q-values or a DiscreteValuedDistribution.\n  \"\"\"\n\n  def __init__(\n      self,\n      reward_objectives: Sequence[learning.RewardObjective],\n      qvalue_objectives: Sequence[learning.QValueObjective],\n      environment_factory: Callable[[bool], dm_env.Environment],\n      network_factory: MultiObjectiveNetworkFactorySpec,\n      num_actors: int = 1,\n      num_caches: int = 0,\n      environment_spec: Optional[specs.EnvironmentSpec] = None,\n      batch_size: int = 512,\n      prefetch_size: int = 4,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1000000,\n      samples_per_insert: Optional[float] = None,\n      n_step: int = 5,\n      max_in_flight_items: int = 5,\n      num_samples: int = 20,\n      additional_discount: float = 0.99,\n      target_policy_update_period: int = 200,\n      target_critic_update_period: int = 200,\n      policy_loss_factory: Optional[MultiObjectivePolicyLossFactorySpec] = None,\n      max_actor_steps: Optional[int] = None,\n      log_every: float = 10.0,\n  ):\n\n    if environment_spec is None:\n      environment_spec = specs.make_environment_spec(environment_factory(False))\n\n    self._environment_factory = environment_factory\n    self._network_factory = network_factory\n    self._policy_loss_factory = policy_loss_factory\n    self._environment_spec = environment_spec\n    self._num_actors = num_actors\n    self._num_caches = num_caches\n    self._batch_size = batch_size\n    self._prefetch_size = prefetch_size\n    self._min_replay_size = min_replay_size\n    self._max_replay_size = max_replay_size\n    self._samples_per_insert = samples_per_insert\n    self._n_step = n_step\n    self._max_in_flight_items = max_in_flight_items\n    self._additional_discount = additional_discount\n    self._num_samples = num_samples\n    self._target_policy_update_period = target_policy_update_period\n    self._target_critic_update_period = target_critic_update_period\n    self._max_actor_steps = max_actor_steps\n    self._log_every = log_every\n\n    self._reward_objectives = reward_objectives\n    self._qvalue_objectives = qvalue_objectives\n    self._num_critic_heads = len(self._reward_objectives)\n\n    if not self._reward_objectives:\n      raise ValueError('Must specify at least one reward objective.')\n\n  def replay(self):\n    \"\"\"The replay storage.\"\"\"\n    if self._samples_per_insert is not None:\n      # Create enough of an error buffer to give a 10% tolerance in rate.\n      samples_per_insert_tolerance = 0.1 * self._samples_per_insert\n      error_buffer = self._min_replay_size * samples_per_insert_tolerance\n\n      limiter = reverb.rate_limiters.SampleToInsertRatio(\n          min_size_to_sample=self._min_replay_size,\n          samples_per_insert=self._samples_per_insert,\n          error_buffer=error_buffer)\n    else:\n      limiter = reverb.rate_limiters.MinSize(self._min_replay_size)\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=self._max_replay_size,\n        rate_limiter=limiter,\n        signature=adders.NStepTransitionAdder.signature(\n            self._environment_spec))\n    return [replay_table]\n\n  def counter(self):\n    return tf2_savers.CheckpointingRunner(counting.Counter(),\n                                          time_delta_minutes=1,\n                                          subdirectory='counter')\n\n  def coordinator(self, counter: counting.Counter, max_actor_steps: int):\n    return lp_utils.StepsLimiter(counter, max_actor_steps)\n\n  def learner(\n      self,\n      replay: reverb.Client,\n      counter: counting.Counter,\n  ):\n    \"\"\"The Learning part of the agent.\"\"\"\n\n    act_spec = self._environment_spec.actions\n    obs_spec = self._environment_spec.observations\n\n    # Create online and target networks.\n    online_networks = self._network_factory(act_spec, self._num_critic_heads)\n    target_networks = self._network_factory(act_spec, self._num_critic_heads)\n\n    # Make sure observation network is a Sonnet Module.\n    observation_network = online_networks.get('observation', tf.identity)\n    target_observation_network = target_networks.get('observation', tf.identity)\n    observation_network = tf2_utils.to_sonnet_module(observation_network)\n    target_observation_network = tf2_utils.to_sonnet_module(\n        target_observation_network)\n\n    # Get embedding spec and create observation network variables.\n    emb_spec = tf2_utils.create_variables(observation_network, [obs_spec])\n\n    # Create variables.\n    tf2_utils.create_variables(online_networks['policy'], [emb_spec])\n    tf2_utils.create_variables(online_networks['critic'], [emb_spec, act_spec])\n    tf2_utils.create_variables(target_networks['policy'], [emb_spec])\n    tf2_utils.create_variables(target_networks['critic'], [emb_spec, act_spec])\n    tf2_utils.create_variables(target_observation_network, [obs_spec])\n\n    # The dataset object to learn from.\n    dataset = datasets.make_reverb_dataset(server_address=replay.server_address)\n    dataset = dataset.batch(self._batch_size, drop_remainder=True)\n    dataset = dataset.prefetch(self._prefetch_size)\n\n    counter = counting.Counter(counter, 'learner')\n    logger = loggers.make_default_logger(\n        'learner', time_delta=self._log_every, steps_key='learner_steps')\n\n    # Create policy loss module if a factory is passed.\n    if self._policy_loss_factory:\n      policy_loss_module = self._policy_loss_factory()\n    else:\n      policy_loss_module = None\n\n    # Return the learning agent.\n    return learning.MultiObjectiveMPOLearner(\n        reward_objectives=self._reward_objectives,\n        qvalue_objectives=self._qvalue_objectives,\n        policy_network=online_networks['policy'],\n        critic_network=online_networks['critic'],\n        observation_network=observation_network,\n        target_policy_network=target_networks['policy'],\n        target_critic_network=target_networks['critic'],\n        target_observation_network=target_observation_network,\n        discount=self._additional_discount,\n        num_samples=self._num_samples,\n        target_policy_update_period=self._target_policy_update_period,\n        target_critic_update_period=self._target_critic_update_period,\n        policy_loss_module=policy_loss_module,\n        dataset=dataset,\n        counter=counter,\n        logger=logger)\n\n  def actor(\n      self,\n      replay: reverb.Client,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ) -> acme.EnvironmentLoop:\n    \"\"\"The actor process.\"\"\"\n\n    action_spec = self._environment_spec.actions\n    observation_spec = self._environment_spec.observations\n\n    # Create environment and target networks to act with.\n    environment = self._environment_factory(False)\n    agent_networks = self._network_factory(action_spec, self._num_critic_heads)\n\n    # Make sure observation network is defined.\n    observation_network = agent_networks.get('observation', tf.identity)\n\n    # Create a stochastic behavior policy.\n    behavior_network = snt.Sequential([\n        observation_network,\n        agent_networks['policy'],\n        networks.StochasticSamplingHead(),\n    ])\n\n    # Ensure network variables are created.\n    tf2_utils.create_variables(behavior_network, [observation_spec])\n    policy_variables = {'policy': behavior_network.variables}\n\n    # Create the variable client responsible for keeping the actor up-to-date.\n    variable_client = tf2_variable_utils.VariableClient(\n        variable_source, policy_variables, update_period=1000)\n\n    # Make sure not to use a random policy after checkpoint restoration by\n    # assigning variables before running the environment loop.\n    variable_client.update_and_wait()\n\n    # Component to add things into replay.\n    adder = adders.NStepTransitionAdder(\n        client=replay,\n        n_step=self._n_step,\n        max_in_flight_items=self._max_in_flight_items,\n        discount=self._additional_discount)\n\n    # Create the agent.\n    actor = actors.FeedForwardActor(\n        policy_network=behavior_network,\n        adder=adder,\n        variable_client=variable_client)\n\n    # Create logger and counter; actors will not spam bigtable.\n    counter = counting.Counter(counter, 'actor')\n    logger = loggers.make_default_logger(\n        'actor',\n        save_data=False,\n        time_delta=self._log_every,\n        steps_key='actor_steps')\n\n    # Create the run loop and return it.\n    return acme.EnvironmentLoop(\n        environment, actor, counter, logger)\n\n  def evaluator(\n      self,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ):\n    \"\"\"The evaluation process.\"\"\"\n\n    action_spec = self._environment_spec.actions\n    observation_spec = self._environment_spec.observations\n\n    # Create environment and target networks to act with.\n    environment = self._environment_factory(True)\n    agent_networks = self._network_factory(action_spec, self._num_critic_heads)\n\n    # Make sure observation network is defined.\n    observation_network = agent_networks.get('observation', tf.identity)\n\n    # Create a deterministic behavior policy.\n    evaluator_modules = [\n        observation_network,\n        agent_networks['policy'],\n        networks.StochasticMeanHead(),\n    ]\n    if isinstance(action_spec, specs.BoundedArray):\n      evaluator_modules += [networks.ClipToSpec(action_spec)]\n    evaluator_network = snt.Sequential(evaluator_modules)\n\n    # Ensure network variables are created.\n    tf2_utils.create_variables(evaluator_network, [observation_spec])\n    policy_variables = {'policy': evaluator_network.variables}\n\n    # Create the variable client responsible for keeping the actor up-to-date.\n    variable_client = tf2_variable_utils.VariableClient(\n        variable_source, policy_variables, update_period=1000)\n\n    # Make sure not to evaluate a random actor by assigning variables before\n    # running the environment loop.\n    variable_client.update_and_wait()\n\n    # Create the agent.\n    evaluator = actors.FeedForwardActor(\n        policy_network=evaluator_network, variable_client=variable_client)\n\n    # Create logger and counter.\n    counter = counting.Counter(counter, 'evaluator')\n    logger = loggers.make_default_logger(\n        'evaluator', time_delta=self._log_every, steps_key='evaluator_steps')\n\n    # Create the run loop and return it.\n    return acme.EnvironmentLoop(\n        environment, evaluator, counter, logger)\n\n  def build(self, name='mompo'):\n    \"\"\"Build the distributed agent topology.\"\"\"\n    program = lp.Program(name=name)\n\n    with program.group('replay'):\n      replay = program.add_node(lp.ReverbNode(self.replay))\n\n    with program.group('counter'):\n      counter = program.add_node(lp.CourierNode(self.counter))\n\n      if self._max_actor_steps:\n        _ = program.add_node(\n            lp.CourierNode(self.coordinator, counter, self._max_actor_steps))\n\n    with program.group('learner'):\n      learner = program.add_node(\n          lp.CourierNode(self.learner, replay, counter))\n\n    with program.group('evaluator'):\n      program.add_node(\n          lp.CourierNode(self.evaluator, learner, counter))\n\n    if not self._num_caches:\n      # Use our learner as a single variable source.\n      sources = [learner]\n    else:\n      with program.group('cacher'):\n        # Create a set of learner caches.\n        sources = []\n        for _ in range(self._num_caches):\n          cacher = program.add_node(\n              lp.CacherNode(\n                  learner, refresh_interval_ms=2000, stale_after_ms=4000))\n          sources.append(cacher)\n\n    with program.group('actor'):\n      # Add actors which pull round-robin from our variable sources.\n      for actor_id in range(self._num_actors):\n        source = sources[actor_id % len(sources)]\n        program.add_node(lp.CourierNode(self.actor, replay, source, counter))\n\n    return program",
  "def __init__(\n      self,\n      reward_objectives: Sequence[learning.RewardObjective],\n      qvalue_objectives: Sequence[learning.QValueObjective],\n      environment_factory: Callable[[bool], dm_env.Environment],\n      network_factory: MultiObjectiveNetworkFactorySpec,\n      num_actors: int = 1,\n      num_caches: int = 0,\n      environment_spec: Optional[specs.EnvironmentSpec] = None,\n      batch_size: int = 512,\n      prefetch_size: int = 4,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1000000,\n      samples_per_insert: Optional[float] = None,\n      n_step: int = 5,\n      max_in_flight_items: int = 5,\n      num_samples: int = 20,\n      additional_discount: float = 0.99,\n      target_policy_update_period: int = 200,\n      target_critic_update_period: int = 200,\n      policy_loss_factory: Optional[MultiObjectivePolicyLossFactorySpec] = None,\n      max_actor_steps: Optional[int] = None,\n      log_every: float = 10.0,\n  ):\n\n    if environment_spec is None:\n      environment_spec = specs.make_environment_spec(environment_factory(False))\n\n    self._environment_factory = environment_factory\n    self._network_factory = network_factory\n    self._policy_loss_factory = policy_loss_factory\n    self._environment_spec = environment_spec\n    self._num_actors = num_actors\n    self._num_caches = num_caches\n    self._batch_size = batch_size\n    self._prefetch_size = prefetch_size\n    self._min_replay_size = min_replay_size\n    self._max_replay_size = max_replay_size\n    self._samples_per_insert = samples_per_insert\n    self._n_step = n_step\n    self._max_in_flight_items = max_in_flight_items\n    self._additional_discount = additional_discount\n    self._num_samples = num_samples\n    self._target_policy_update_period = target_policy_update_period\n    self._target_critic_update_period = target_critic_update_period\n    self._max_actor_steps = max_actor_steps\n    self._log_every = log_every\n\n    self._reward_objectives = reward_objectives\n    self._qvalue_objectives = qvalue_objectives\n    self._num_critic_heads = len(self._reward_objectives)\n\n    if not self._reward_objectives:\n      raise ValueError('Must specify at least one reward objective.')",
  "def replay(self):\n    \"\"\"The replay storage.\"\"\"\n    if self._samples_per_insert is not None:\n      # Create enough of an error buffer to give a 10% tolerance in rate.\n      samples_per_insert_tolerance = 0.1 * self._samples_per_insert\n      error_buffer = self._min_replay_size * samples_per_insert_tolerance\n\n      limiter = reverb.rate_limiters.SampleToInsertRatio(\n          min_size_to_sample=self._min_replay_size,\n          samples_per_insert=self._samples_per_insert,\n          error_buffer=error_buffer)\n    else:\n      limiter = reverb.rate_limiters.MinSize(self._min_replay_size)\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=self._max_replay_size,\n        rate_limiter=limiter,\n        signature=adders.NStepTransitionAdder.signature(\n            self._environment_spec))\n    return [replay_table]",
  "def counter(self):\n    return tf2_savers.CheckpointingRunner(counting.Counter(),\n                                          time_delta_minutes=1,\n                                          subdirectory='counter')",
  "def coordinator(self, counter: counting.Counter, max_actor_steps: int):\n    return lp_utils.StepsLimiter(counter, max_actor_steps)",
  "def learner(\n      self,\n      replay: reverb.Client,\n      counter: counting.Counter,\n  ):\n    \"\"\"The Learning part of the agent.\"\"\"\n\n    act_spec = self._environment_spec.actions\n    obs_spec = self._environment_spec.observations\n\n    # Create online and target networks.\n    online_networks = self._network_factory(act_spec, self._num_critic_heads)\n    target_networks = self._network_factory(act_spec, self._num_critic_heads)\n\n    # Make sure observation network is a Sonnet Module.\n    observation_network = online_networks.get('observation', tf.identity)\n    target_observation_network = target_networks.get('observation', tf.identity)\n    observation_network = tf2_utils.to_sonnet_module(observation_network)\n    target_observation_network = tf2_utils.to_sonnet_module(\n        target_observation_network)\n\n    # Get embedding spec and create observation network variables.\n    emb_spec = tf2_utils.create_variables(observation_network, [obs_spec])\n\n    # Create variables.\n    tf2_utils.create_variables(online_networks['policy'], [emb_spec])\n    tf2_utils.create_variables(online_networks['critic'], [emb_spec, act_spec])\n    tf2_utils.create_variables(target_networks['policy'], [emb_spec])\n    tf2_utils.create_variables(target_networks['critic'], [emb_spec, act_spec])\n    tf2_utils.create_variables(target_observation_network, [obs_spec])\n\n    # The dataset object to learn from.\n    dataset = datasets.make_reverb_dataset(server_address=replay.server_address)\n    dataset = dataset.batch(self._batch_size, drop_remainder=True)\n    dataset = dataset.prefetch(self._prefetch_size)\n\n    counter = counting.Counter(counter, 'learner')\n    logger = loggers.make_default_logger(\n        'learner', time_delta=self._log_every, steps_key='learner_steps')\n\n    # Create policy loss module if a factory is passed.\n    if self._policy_loss_factory:\n      policy_loss_module = self._policy_loss_factory()\n    else:\n      policy_loss_module = None\n\n    # Return the learning agent.\n    return learning.MultiObjectiveMPOLearner(\n        reward_objectives=self._reward_objectives,\n        qvalue_objectives=self._qvalue_objectives,\n        policy_network=online_networks['policy'],\n        critic_network=online_networks['critic'],\n        observation_network=observation_network,\n        target_policy_network=target_networks['policy'],\n        target_critic_network=target_networks['critic'],\n        target_observation_network=target_observation_network,\n        discount=self._additional_discount,\n        num_samples=self._num_samples,\n        target_policy_update_period=self._target_policy_update_period,\n        target_critic_update_period=self._target_critic_update_period,\n        policy_loss_module=policy_loss_module,\n        dataset=dataset,\n        counter=counter,\n        logger=logger)",
  "def actor(\n      self,\n      replay: reverb.Client,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ) -> acme.EnvironmentLoop:\n    \"\"\"The actor process.\"\"\"\n\n    action_spec = self._environment_spec.actions\n    observation_spec = self._environment_spec.observations\n\n    # Create environment and target networks to act with.\n    environment = self._environment_factory(False)\n    agent_networks = self._network_factory(action_spec, self._num_critic_heads)\n\n    # Make sure observation network is defined.\n    observation_network = agent_networks.get('observation', tf.identity)\n\n    # Create a stochastic behavior policy.\n    behavior_network = snt.Sequential([\n        observation_network,\n        agent_networks['policy'],\n        networks.StochasticSamplingHead(),\n    ])\n\n    # Ensure network variables are created.\n    tf2_utils.create_variables(behavior_network, [observation_spec])\n    policy_variables = {'policy': behavior_network.variables}\n\n    # Create the variable client responsible for keeping the actor up-to-date.\n    variable_client = tf2_variable_utils.VariableClient(\n        variable_source, policy_variables, update_period=1000)\n\n    # Make sure not to use a random policy after checkpoint restoration by\n    # assigning variables before running the environment loop.\n    variable_client.update_and_wait()\n\n    # Component to add things into replay.\n    adder = adders.NStepTransitionAdder(\n        client=replay,\n        n_step=self._n_step,\n        max_in_flight_items=self._max_in_flight_items,\n        discount=self._additional_discount)\n\n    # Create the agent.\n    actor = actors.FeedForwardActor(\n        policy_network=behavior_network,\n        adder=adder,\n        variable_client=variable_client)\n\n    # Create logger and counter; actors will not spam bigtable.\n    counter = counting.Counter(counter, 'actor')\n    logger = loggers.make_default_logger(\n        'actor',\n        save_data=False,\n        time_delta=self._log_every,\n        steps_key='actor_steps')\n\n    # Create the run loop and return it.\n    return acme.EnvironmentLoop(\n        environment, actor, counter, logger)",
  "def evaluator(\n      self,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ):\n    \"\"\"The evaluation process.\"\"\"\n\n    action_spec = self._environment_spec.actions\n    observation_spec = self._environment_spec.observations\n\n    # Create environment and target networks to act with.\n    environment = self._environment_factory(True)\n    agent_networks = self._network_factory(action_spec, self._num_critic_heads)\n\n    # Make sure observation network is defined.\n    observation_network = agent_networks.get('observation', tf.identity)\n\n    # Create a deterministic behavior policy.\n    evaluator_modules = [\n        observation_network,\n        agent_networks['policy'],\n        networks.StochasticMeanHead(),\n    ]\n    if isinstance(action_spec, specs.BoundedArray):\n      evaluator_modules += [networks.ClipToSpec(action_spec)]\n    evaluator_network = snt.Sequential(evaluator_modules)\n\n    # Ensure network variables are created.\n    tf2_utils.create_variables(evaluator_network, [observation_spec])\n    policy_variables = {'policy': evaluator_network.variables}\n\n    # Create the variable client responsible for keeping the actor up-to-date.\n    variable_client = tf2_variable_utils.VariableClient(\n        variable_source, policy_variables, update_period=1000)\n\n    # Make sure not to evaluate a random actor by assigning variables before\n    # running the environment loop.\n    variable_client.update_and_wait()\n\n    # Create the agent.\n    evaluator = actors.FeedForwardActor(\n        policy_network=evaluator_network, variable_client=variable_client)\n\n    # Create logger and counter.\n    counter = counting.Counter(counter, 'evaluator')\n    logger = loggers.make_default_logger(\n        'evaluator', time_delta=self._log_every, steps_key='evaluator_steps')\n\n    # Create the run loop and return it.\n    return acme.EnvironmentLoop(\n        environment, evaluator, counter, logger)",
  "def build(self, name='mompo'):\n    \"\"\"Build the distributed agent topology.\"\"\"\n    program = lp.Program(name=name)\n\n    with program.group('replay'):\n      replay = program.add_node(lp.ReverbNode(self.replay))\n\n    with program.group('counter'):\n      counter = program.add_node(lp.CourierNode(self.counter))\n\n      if self._max_actor_steps:\n        _ = program.add_node(\n            lp.CourierNode(self.coordinator, counter, self._max_actor_steps))\n\n    with program.group('learner'):\n      learner = program.add_node(\n          lp.CourierNode(self.learner, replay, counter))\n\n    with program.group('evaluator'):\n      program.add_node(\n          lp.CourierNode(self.evaluator, learner, counter))\n\n    if not self._num_caches:\n      # Use our learner as a single variable source.\n      sources = [learner]\n    else:\n      with program.group('cacher'):\n        # Create a set of learner caches.\n        sources = []\n        for _ in range(self._num_caches):\n          cacher = program.add_node(\n              lp.CacherNode(\n                  learner, refresh_interval_ms=2000, stale_after_ms=4000))\n          sources.append(cacher)\n\n    with program.group('actor'):\n      # Add actors which pull round-robin from our variable sources.\n      for actor_id in range(self._num_actors):\n        source = sources[actor_id % len(sources)]\n        program.add_node(lp.CourierNode(self.actor, replay, source, counter))\n\n    return program",
  "class BCLearner(acme.Learner, tf2_savers.TFSaveable):\n  \"\"\"BC learner.\n\n  This is the learning component of a BC agent. IE it takes a dataset as input\n  and implements update functionality to learn from this dataset.\n  \"\"\"\n\n  def __init__(self,\n               network: snt.Module,\n               learning_rate: float,\n               dataset: tf.data.Dataset,\n               counter: Optional[counting.Counter] = None,\n               logger: Optional[loggers.Logger] = None,\n               checkpoint: bool = True):\n    \"\"\"Initializes the learner.\n\n    Args:\n      network: the BC network (the one being optimized)\n      learning_rate: learning rate for the cross-entropy update.\n      dataset: dataset to learn from.\n      counter: Counter object for (potentially distributed) counting.\n      logger: Logger object for writing logs to.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n    \"\"\"\n\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.TerminalLogger('learner', time_delta=1.)\n\n    # Get an iterator over the dataset.\n    self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n    # TODO(b/155086959): Fix type stubs and remove.\n\n    self._network = network\n    self._optimizer = snt.optimizers.Adam(learning_rate)\n\n    self._variables: List[List[tf.Tensor]] = [network.trainable_variables]\n    self._num_steps = tf.Variable(0, dtype=tf.int32)\n\n    # Create a snapshotter object.\n    if checkpoint:\n      self._snapshotter = tf2_savers.Snapshotter(\n          objects_to_save={'network': network}, time_delta_minutes=60.)\n    else:\n      self._snapshotter = None\n\n  @tf.function\n  def _step(self) -> Dict[str, tf.Tensor]:\n    \"\"\"Do a step of SGD and update the priorities.\"\"\"\n\n    # Pull out the data needed for updates/priorities.\n    inputs = next(self._iterator)\n    transitions: types.Transition = inputs.data\n\n    with tf.GradientTape() as tape:\n      # Evaluate our networks.\n      logits = self._network(transitions.observation)\n      cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n      loss = cce(transitions.action, logits)\n\n    gradients = tape.gradient(loss, self._network.trainable_variables)\n    self._optimizer.apply(gradients, self._network.trainable_variables)\n\n    self._num_steps.assign_add(1)\n\n    # Compute the global norm of the gradients for logging.\n    global_gradient_norm = tf.linalg.global_norm(gradients)\n    fetches = {'loss': loss, 'gradient_norm': global_gradient_norm}\n\n    return fetches\n\n  def step(self):\n    # Do a batch of SGD.\n    result = self._step()\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1)\n    result.update(counts)\n\n    # Snapshot and attempt to write logs.\n    if self._snapshotter is not None:\n      self._snapshotter.save()\n    self._logger.write(result)\n\n  def get_variables(self, names: List[str]) -> List[np.ndarray]:\n    return tf2_utils.to_numpy(self._variables)\n\n  @property\n  def state(self):\n    \"\"\"Returns the stateful parts of the learner for checkpointing.\"\"\"\n    return {\n        'network': self._network,\n        'optimizer': self._optimizer,\n        'num_steps': self._num_steps\n    }",
  "def __init__(self,\n               network: snt.Module,\n               learning_rate: float,\n               dataset: tf.data.Dataset,\n               counter: Optional[counting.Counter] = None,\n               logger: Optional[loggers.Logger] = None,\n               checkpoint: bool = True):\n    \"\"\"Initializes the learner.\n\n    Args:\n      network: the BC network (the one being optimized)\n      learning_rate: learning rate for the cross-entropy update.\n      dataset: dataset to learn from.\n      counter: Counter object for (potentially distributed) counting.\n      logger: Logger object for writing logs to.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n    \"\"\"\n\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.TerminalLogger('learner', time_delta=1.)\n\n    # Get an iterator over the dataset.\n    self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n    # TODO(b/155086959): Fix type stubs and remove.\n\n    self._network = network\n    self._optimizer = snt.optimizers.Adam(learning_rate)\n\n    self._variables: List[List[tf.Tensor]] = [network.trainable_variables]\n    self._num_steps = tf.Variable(0, dtype=tf.int32)\n\n    # Create a snapshotter object.\n    if checkpoint:\n      self._snapshotter = tf2_savers.Snapshotter(\n          objects_to_save={'network': network}, time_delta_minutes=60.)\n    else:\n      self._snapshotter = None",
  "def _step(self) -> Dict[str, tf.Tensor]:\n    \"\"\"Do a step of SGD and update the priorities.\"\"\"\n\n    # Pull out the data needed for updates/priorities.\n    inputs = next(self._iterator)\n    transitions: types.Transition = inputs.data\n\n    with tf.GradientTape() as tape:\n      # Evaluate our networks.\n      logits = self._network(transitions.observation)\n      cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n      loss = cce(transitions.action, logits)\n\n    gradients = tape.gradient(loss, self._network.trainable_variables)\n    self._optimizer.apply(gradients, self._network.trainable_variables)\n\n    self._num_steps.assign_add(1)\n\n    # Compute the global norm of the gradients for logging.\n    global_gradient_norm = tf.linalg.global_norm(gradients)\n    fetches = {'loss': loss, 'gradient_norm': global_gradient_norm}\n\n    return fetches",
  "def step(self):\n    # Do a batch of SGD.\n    result = self._step()\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1)\n    result.update(counts)\n\n    # Snapshot and attempt to write logs.\n    if self._snapshotter is not None:\n      self._snapshotter.save()\n    self._logger.write(result)",
  "def get_variables(self, names: List[str]) -> List[np.ndarray]:\n    return tf2_utils.to_numpy(self._variables)",
  "def state(self):\n    \"\"\"Returns the stateful parts of the learner for checkpointing.\"\"\"\n    return {\n        'network': self._network,\n        'optimizer': self._optimizer,\n        'num_steps': self._num_steps\n    }",
  "class IMPALA(acme.Actor):\n  \"\"\"IMPALA Agent.\"\"\"\n\n  def __init__(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      network: snt.RNNCore,\n      sequence_length: int,\n      sequence_period: int,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      discount: float = 0.99,\n      max_queue_size: int = 100000,\n      batch_size: int = 16,\n      learning_rate: float = 1e-3,\n      entropy_cost: float = 0.01,\n      baseline_cost: float = 0.5,\n      max_abs_reward: Optional[float] = None,\n      max_gradient_norm: Optional[float] = None,\n  ):\n\n    num_actions = environment_spec.actions.num_values\n    self._logger = logger or loggers.TerminalLogger('agent')\n\n    extra_spec = {\n        'core_state': network.initial_state(1),\n        'logits': tf.ones(shape=(1, num_actions), dtype=tf.float32)\n    }\n    # Remove batch dimensions.\n    extra_spec = tf2_utils.squeeze_batch_dim(extra_spec)\n\n    queue = reverb.Table.queue(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        max_size=max_queue_size,\n        signature=adders.SequenceAdder.signature(\n            environment_spec,\n            extras_spec=extra_spec,\n            sequence_length=sequence_length))\n    self._server = reverb.Server([queue], port=None)\n    self._can_sample = lambda: queue.can_sample(batch_size)\n    address = f'localhost:{self._server.port}'\n\n    # Component to add things into replay.\n    adder = adders.SequenceAdder(\n        client=reverb.Client(address),\n        period=sequence_period,\n        sequence_length=sequence_length,\n    )\n\n    # The dataset object to learn from.\n    dataset = datasets.make_reverb_dataset(\n        server_address=address,\n        batch_size=batch_size)\n\n    tf2_utils.create_variables(network, [environment_spec.observations])\n\n    self._actor = acting.IMPALAActor(network, adder)\n    self._learner = learning.IMPALALearner(\n        environment_spec=environment_spec,\n        network=network,\n        dataset=dataset,\n        counter=counter,\n        logger=logger,\n        discount=discount,\n        learning_rate=learning_rate,\n        entropy_cost=entropy_cost,\n        baseline_cost=baseline_cost,\n        max_gradient_norm=max_gradient_norm,\n        max_abs_reward=max_abs_reward,\n    )\n\n  def observe_first(self, timestep: dm_env.TimeStep):\n    self._actor.observe_first(timestep)\n\n  def observe(\n      self,\n      action: types.NestedArray,\n      next_timestep: dm_env.TimeStep,\n  ):\n    self._actor.observe(action, next_timestep)\n\n  def update(self, wait: bool = False):\n    # Run a number of learner steps (usually gradient steps).\n    while self._can_sample():\n      self._learner.step()\n\n  def select_action(self, observation: np.ndarray) -> int:\n    return self._actor.select_action(observation)",
  "def __init__(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      network: snt.RNNCore,\n      sequence_length: int,\n      sequence_period: int,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      discount: float = 0.99,\n      max_queue_size: int = 100000,\n      batch_size: int = 16,\n      learning_rate: float = 1e-3,\n      entropy_cost: float = 0.01,\n      baseline_cost: float = 0.5,\n      max_abs_reward: Optional[float] = None,\n      max_gradient_norm: Optional[float] = None,\n  ):\n\n    num_actions = environment_spec.actions.num_values\n    self._logger = logger or loggers.TerminalLogger('agent')\n\n    extra_spec = {\n        'core_state': network.initial_state(1),\n        'logits': tf.ones(shape=(1, num_actions), dtype=tf.float32)\n    }\n    # Remove batch dimensions.\n    extra_spec = tf2_utils.squeeze_batch_dim(extra_spec)\n\n    queue = reverb.Table.queue(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        max_size=max_queue_size,\n        signature=adders.SequenceAdder.signature(\n            environment_spec,\n            extras_spec=extra_spec,\n            sequence_length=sequence_length))\n    self._server = reverb.Server([queue], port=None)\n    self._can_sample = lambda: queue.can_sample(batch_size)\n    address = f'localhost:{self._server.port}'\n\n    # Component to add things into replay.\n    adder = adders.SequenceAdder(\n        client=reverb.Client(address),\n        period=sequence_period,\n        sequence_length=sequence_length,\n    )\n\n    # The dataset object to learn from.\n    dataset = datasets.make_reverb_dataset(\n        server_address=address,\n        batch_size=batch_size)\n\n    tf2_utils.create_variables(network, [environment_spec.observations])\n\n    self._actor = acting.IMPALAActor(network, adder)\n    self._learner = learning.IMPALALearner(\n        environment_spec=environment_spec,\n        network=network,\n        dataset=dataset,\n        counter=counter,\n        logger=logger,\n        discount=discount,\n        learning_rate=learning_rate,\n        entropy_cost=entropy_cost,\n        baseline_cost=baseline_cost,\n        max_gradient_norm=max_gradient_norm,\n        max_abs_reward=max_abs_reward,\n    )",
  "def observe_first(self, timestep: dm_env.TimeStep):\n    self._actor.observe_first(timestep)",
  "def observe(\n      self,\n      action: types.NestedArray,\n      next_timestep: dm_env.TimeStep,\n  ):\n    self._actor.observe(action, next_timestep)",
  "def update(self, wait: bool = False):\n    # Run a number of learner steps (usually gradient steps).\n    while self._can_sample():\n      self._learner.step()",
  "def select_action(self, observation: np.ndarray) -> int:\n    return self._actor.select_action(observation)",
  "class IMPALALearner(acme.Learner, tf2_savers.TFSaveable):\n  \"\"\"Learner for an importanced-weighted advantage actor-critic.\"\"\"\n\n  def __init__(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      network: snt.RNNCore,\n      dataset: tf.data.Dataset,\n      learning_rate: float,\n      discount: float = 0.99,\n      entropy_cost: float = 0.,\n      baseline_cost: float = 1.,\n      max_abs_reward: Optional[float] = None,\n      max_gradient_norm: Optional[float] = None,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n  ):\n\n    # Internalise, optimizer, and dataset.\n    self._env_spec = environment_spec\n    self._optimizer = snt.optimizers.Adam(learning_rate=learning_rate)\n    self._network = network\n    self._variables = network.variables\n    # TODO(b/155086959): Fix type stubs and remove.\n    self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n    # Hyperparameters.\n    self._discount = discount\n    self._entropy_cost = entropy_cost\n    self._baseline_cost = baseline_cost\n\n    # Set up reward/gradient clipping.\n    if max_abs_reward is None:\n      max_abs_reward = np.inf\n    if max_gradient_norm is None:\n      max_gradient_norm = 1e10  # A very large number. Infinity results in NaNs.\n    self._max_abs_reward = tf.convert_to_tensor(max_abs_reward)\n    self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n\n    # Set up logging/counting.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.TerminalLogger('learner', time_delta=1.)\n\n    self._snapshotter = tf2_savers.Snapshotter(\n        objects_to_save={'network': network}, time_delta_minutes=60.)\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None\n\n  @property\n  def state(self) -> Mapping[str, tf2_savers.Checkpointable]:\n    \"\"\"Returns the stateful objects for checkpointing.\"\"\"\n    return {\n        'network': self._network,\n        'optimizer': self._optimizer,\n    }\n\n  @tf.function\n  def _step(self) -> Dict[str, tf.Tensor]:\n    \"\"\"Does an SGD step on a batch of sequences.\"\"\"\n\n    # Retrieve a batch of data from replay.\n    inputs: reverb.ReplaySample = next(self._iterator)\n    data = tf2_utils.batch_to_sequence(inputs.data)\n    observations, actions, rewards, discounts, extra = (data.observation,\n                                                        data.action,\n                                                        data.reward,\n                                                        data.discount,\n                                                        data.extras)\n    core_state = tree.map_structure(lambda s: s[0], extra['core_state'])\n\n    #\n    actions = actions[:-1]  # [T-1]\n    rewards = rewards[:-1]  # [T-1]\n    discounts = discounts[:-1]  # [T-1]\n\n    with tf.GradientTape() as tape:\n      # Unroll current policy over observations.\n      (logits, values), _ = snt.static_unroll(self._network, observations,\n                                              core_state)\n\n      # Compute importance sampling weights: current policy / behavior policy.\n      behaviour_logits = extra['logits']\n      pi_behaviour = tfd.Categorical(logits=behaviour_logits[:-1])\n      pi_target = tfd.Categorical(logits=logits[:-1])\n      log_rhos = pi_target.log_prob(actions) - pi_behaviour.log_prob(actions)\n\n      # Optionally clip rewards.\n      rewards = tf.clip_by_value(rewards,\n                                 tf.cast(-self._max_abs_reward, rewards.dtype),\n                                 tf.cast(self._max_abs_reward, rewards.dtype))\n\n      # Critic loss.\n      vtrace_returns = trfl.vtrace_from_importance_weights(\n          log_rhos=tf.cast(log_rhos, tf.float32),\n          discounts=tf.cast(self._discount * discounts, tf.float32),\n          rewards=tf.cast(rewards, tf.float32),\n          values=tf.cast(values[:-1], tf.float32),\n          bootstrap_value=values[-1],\n      )\n      critic_loss = tf.square(vtrace_returns.vs - values[:-1])\n\n      # Policy-gradient loss.\n      policy_gradient_loss = trfl.policy_gradient(\n          policies=pi_target,\n          actions=actions,\n          action_values=vtrace_returns.pg_advantages,\n      )\n\n      # Entropy regulariser.\n      entropy_loss = trfl.policy_entropy_loss(pi_target).loss\n\n      # Combine weighted sum of actor & critic losses.\n      loss = tf.reduce_mean(policy_gradient_loss +\n                            self._baseline_cost * critic_loss +\n                            self._entropy_cost * entropy_loss)\n\n    # Compute gradients and optionally apply clipping.\n    gradients = tape.gradient(loss, self._network.trainable_variables)\n    gradients, _ = tf.clip_by_global_norm(gradients, self._max_gradient_norm)\n    self._optimizer.apply(gradients, self._network.trainable_variables)\n\n    metrics = {\n        'loss': loss,\n        'critic_loss': tf.reduce_mean(critic_loss),\n        'entropy_loss': tf.reduce_mean(entropy_loss),\n        'policy_gradient_loss': tf.reduce_mean(policy_gradient_loss),\n    }\n\n    return metrics\n\n  def step(self):\n    \"\"\"Does a step of SGD and logs the results.\"\"\"\n\n    # Do a batch of SGD.\n    results = self._step()\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n    results.update(counts)\n\n    # Snapshot and attempt to write logs.\n    self._snapshotter.save()\n    self._logger.write(results)\n\n  def get_variables(self, names: List[str]) -> List[List[np.ndarray]]:\n    return [tf2_utils.to_numpy(self._variables)]",
  "def __init__(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      network: snt.RNNCore,\n      dataset: tf.data.Dataset,\n      learning_rate: float,\n      discount: float = 0.99,\n      entropy_cost: float = 0.,\n      baseline_cost: float = 1.,\n      max_abs_reward: Optional[float] = None,\n      max_gradient_norm: Optional[float] = None,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n  ):\n\n    # Internalise, optimizer, and dataset.\n    self._env_spec = environment_spec\n    self._optimizer = snt.optimizers.Adam(learning_rate=learning_rate)\n    self._network = network\n    self._variables = network.variables\n    # TODO(b/155086959): Fix type stubs and remove.\n    self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n    # Hyperparameters.\n    self._discount = discount\n    self._entropy_cost = entropy_cost\n    self._baseline_cost = baseline_cost\n\n    # Set up reward/gradient clipping.\n    if max_abs_reward is None:\n      max_abs_reward = np.inf\n    if max_gradient_norm is None:\n      max_gradient_norm = 1e10  # A very large number. Infinity results in NaNs.\n    self._max_abs_reward = tf.convert_to_tensor(max_abs_reward)\n    self._max_gradient_norm = tf.convert_to_tensor(max_gradient_norm)\n\n    # Set up logging/counting.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.TerminalLogger('learner', time_delta=1.)\n\n    self._snapshotter = tf2_savers.Snapshotter(\n        objects_to_save={'network': network}, time_delta_minutes=60.)\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None",
  "def state(self) -> Mapping[str, tf2_savers.Checkpointable]:\n    \"\"\"Returns the stateful objects for checkpointing.\"\"\"\n    return {\n        'network': self._network,\n        'optimizer': self._optimizer,\n    }",
  "def _step(self) -> Dict[str, tf.Tensor]:\n    \"\"\"Does an SGD step on a batch of sequences.\"\"\"\n\n    # Retrieve a batch of data from replay.\n    inputs: reverb.ReplaySample = next(self._iterator)\n    data = tf2_utils.batch_to_sequence(inputs.data)\n    observations, actions, rewards, discounts, extra = (data.observation,\n                                                        data.action,\n                                                        data.reward,\n                                                        data.discount,\n                                                        data.extras)\n    core_state = tree.map_structure(lambda s: s[0], extra['core_state'])\n\n    #\n    actions = actions[:-1]  # [T-1]\n    rewards = rewards[:-1]  # [T-1]\n    discounts = discounts[:-1]  # [T-1]\n\n    with tf.GradientTape() as tape:\n      # Unroll current policy over observations.\n      (logits, values), _ = snt.static_unroll(self._network, observations,\n                                              core_state)\n\n      # Compute importance sampling weights: current policy / behavior policy.\n      behaviour_logits = extra['logits']\n      pi_behaviour = tfd.Categorical(logits=behaviour_logits[:-1])\n      pi_target = tfd.Categorical(logits=logits[:-1])\n      log_rhos = pi_target.log_prob(actions) - pi_behaviour.log_prob(actions)\n\n      # Optionally clip rewards.\n      rewards = tf.clip_by_value(rewards,\n                                 tf.cast(-self._max_abs_reward, rewards.dtype),\n                                 tf.cast(self._max_abs_reward, rewards.dtype))\n\n      # Critic loss.\n      vtrace_returns = trfl.vtrace_from_importance_weights(\n          log_rhos=tf.cast(log_rhos, tf.float32),\n          discounts=tf.cast(self._discount * discounts, tf.float32),\n          rewards=tf.cast(rewards, tf.float32),\n          values=tf.cast(values[:-1], tf.float32),\n          bootstrap_value=values[-1],\n      )\n      critic_loss = tf.square(vtrace_returns.vs - values[:-1])\n\n      # Policy-gradient loss.\n      policy_gradient_loss = trfl.policy_gradient(\n          policies=pi_target,\n          actions=actions,\n          action_values=vtrace_returns.pg_advantages,\n      )\n\n      # Entropy regulariser.\n      entropy_loss = trfl.policy_entropy_loss(pi_target).loss\n\n      # Combine weighted sum of actor & critic losses.\n      loss = tf.reduce_mean(policy_gradient_loss +\n                            self._baseline_cost * critic_loss +\n                            self._entropy_cost * entropy_loss)\n\n    # Compute gradients and optionally apply clipping.\n    gradients = tape.gradient(loss, self._network.trainable_variables)\n    gradients, _ = tf.clip_by_global_norm(gradients, self._max_gradient_norm)\n    self._optimizer.apply(gradients, self._network.trainable_variables)\n\n    metrics = {\n        'loss': loss,\n        'critic_loss': tf.reduce_mean(critic_loss),\n        'entropy_loss': tf.reduce_mean(entropy_loss),\n        'policy_gradient_loss': tf.reduce_mean(policy_gradient_loss),\n    }\n\n    return metrics",
  "def step(self):\n    \"\"\"Does a step of SGD and logs the results.\"\"\"\n\n    # Do a batch of SGD.\n    results = self._step()\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n    results.update(counts)\n\n    # Snapshot and attempt to write logs.\n    self._snapshotter.save()\n    self._logger.write(results)",
  "def get_variables(self, names: List[str]) -> List[List[np.ndarray]]:\n    return [tf2_utils.to_numpy(self._variables)]",
  "class IMPALAActor(core.Actor):\n  \"\"\"A recurrent actor.\"\"\"\n\n  def __init__(\n      self,\n      network: snt.RNNCore,\n      adder: Optional[adders.Adder] = None,\n      variable_client: Optional[tf2_variable_utils.VariableClient] = None,\n  ):\n\n    # Store these for later use.\n    self._adder = adder\n    self._variable_client = variable_client\n    self._network = network\n\n    # TODO(b/152382420): Ideally we would call tf.function(network) instead but\n    # this results in an error when using acme RNN snapshots.\n    self._policy = tf.function(network.__call__)\n\n    self._state = None\n    self._prev_state = None\n    self._prev_logits = None\n\n  def select_action(self, observation: types.NestedArray) -> types.NestedArray:\n    # Add a dummy batch dimension and as a side effect convert numpy to TF.\n    batched_obs = tf2_utils.add_batch_dim(observation)\n\n    if self._state is None:\n      self._state = self._network.initial_state(1)\n\n    # Forward.\n    (logits, _), new_state = self._policy(batched_obs, self._state)\n\n    self._prev_logits = logits\n    self._prev_state = self._state\n    self._state = new_state\n\n    action = tfd.Categorical(logits).sample()\n    action = tf2_utils.to_numpy_squeeze(action)\n\n    return action\n\n  def observe_first(self, timestep: dm_env.TimeStep):\n    if self._adder:\n      self._adder.add_first(timestep)\n\n    # Set the state to None so that we re-initialize at the next policy call.\n    self._state = None\n\n  def observe(\n      self,\n      action: types.NestedArray,\n      next_timestep: dm_env.TimeStep,\n  ):\n    if not self._adder:\n      return\n\n    extras = {'logits': self._prev_logits, 'core_state': self._prev_state}\n    extras = tf2_utils.to_numpy_squeeze(extras)\n    self._adder.add(action, next_timestep, extras)\n\n  def update(self, wait: bool = False):\n    if self._variable_client:\n      self._variable_client.update(wait)",
  "def __init__(\n      self,\n      network: snt.RNNCore,\n      adder: Optional[adders.Adder] = None,\n      variable_client: Optional[tf2_variable_utils.VariableClient] = None,\n  ):\n\n    # Store these for later use.\n    self._adder = adder\n    self._variable_client = variable_client\n    self._network = network\n\n    # TODO(b/152382420): Ideally we would call tf.function(network) instead but\n    # this results in an error when using acme RNN snapshots.\n    self._policy = tf.function(network.__call__)\n\n    self._state = None\n    self._prev_state = None\n    self._prev_logits = None",
  "def select_action(self, observation: types.NestedArray) -> types.NestedArray:\n    # Add a dummy batch dimension and as a side effect convert numpy to TF.\n    batched_obs = tf2_utils.add_batch_dim(observation)\n\n    if self._state is None:\n      self._state = self._network.initial_state(1)\n\n    # Forward.\n    (logits, _), new_state = self._policy(batched_obs, self._state)\n\n    self._prev_logits = logits\n    self._prev_state = self._state\n    self._state = new_state\n\n    action = tfd.Categorical(logits).sample()\n    action = tf2_utils.to_numpy_squeeze(action)\n\n    return action",
  "def observe_first(self, timestep: dm_env.TimeStep):\n    if self._adder:\n      self._adder.add_first(timestep)\n\n    # Set the state to None so that we re-initialize at the next policy call.\n    self._state = None",
  "def observe(\n      self,\n      action: types.NestedArray,\n      next_timestep: dm_env.TimeStep,\n  ):\n    if not self._adder:\n      return\n\n    extras = {'logits': self._prev_logits, 'core_state': self._prev_state}\n    extras = tf2_utils.to_numpy_squeeze(extras)\n    self._adder.add(action, next_timestep, extras)",
  "def update(self, wait: bool = False):\n    if self._variable_client:\n      self._variable_client.update(wait)",
  "class DistributedAgentTest(absltest.TestCase):\n  \"\"\"Simple integration/smoke test for the distributed agent.\"\"\"\n\n  def test_atari(self):\n    \"\"\"Tests that the agent can run for some steps without crashing.\"\"\"\n    env_factory = lambda x: fakes.fake_atari_wrapped(oar_wrapper=True)\n    net_factory = lambda spec: networks.IMPALAAtariNetwork(spec.num_values)\n\n    agent = impala.DistributedIMPALA(\n        environment_factory=env_factory,\n        network_factory=net_factory,\n        num_actors=2,\n        batch_size=32,\n        sequence_length=5,\n        sequence_period=1,\n    )\n    program = agent.build()\n\n    (learner_node,) = program.groups['learner']\n    learner_node.disable_run()\n\n    lp.launch(program, launch_type='test_mt')\n\n    learner: acme.Learner = learner_node.create_handle().dereference()\n\n    for _ in range(5):\n      learner.step()",
  "def test_atari(self):\n    \"\"\"Tests that the agent can run for some steps without crashing.\"\"\"\n    env_factory = lambda x: fakes.fake_atari_wrapped(oar_wrapper=True)\n    net_factory = lambda spec: networks.IMPALAAtariNetwork(spec.num_values)\n\n    agent = impala.DistributedIMPALA(\n        environment_factory=env_factory,\n        network_factory=net_factory,\n        num_actors=2,\n        batch_size=32,\n        sequence_length=5,\n        sequence_period=1,\n    )\n    program = agent.build()\n\n    (learner_node,) = program.groups['learner']\n    learner_node.disable_run()\n\n    lp.launch(program, launch_type='test_mt')\n\n    learner: acme.Learner = learner_node.create_handle().dereference()\n\n    for _ in range(5):\n      learner.step()",
  "def _make_network(action_spec: specs.DiscreteArray) -> snt.RNNCore:\n  return snt.DeepRNN([\n      snt.Flatten(),\n      snt.LSTM(20),\n      snt.nets.MLP([50, 50]),\n      networks.PolicyValueHead(action_spec.num_values),\n  ])",
  "class IMPALATest(absltest.TestCase):\n\n  # TODO(b/200509080): This test case is timing out.\n  @absltest.SkipTest\n  def test_impala(self):\n    # Create a fake environment to test with.\n    environment = fakes.DiscreteEnvironment(\n        num_actions=5,\n        num_observations=10,\n        obs_dtype=np.float32,\n        episode_length=10)\n    spec = specs.make_environment_spec(environment)\n\n    # Construct the agent.\n    agent = impala.IMPALA(\n        environment_spec=spec,\n        network=_make_network(spec.actions),\n        sequence_length=3,\n        sequence_period=3,\n        batch_size=6,\n    )\n\n    # Try running the environment loop. We have no assertions here because all\n    # we care about is that the agent runs without raising any errors.\n    loop = acme.EnvironmentLoop(environment, agent)\n    loop.run(num_episodes=20)",
  "def test_impala(self):\n    # Create a fake environment to test with.\n    environment = fakes.DiscreteEnvironment(\n        num_actions=5,\n        num_observations=10,\n        obs_dtype=np.float32,\n        episode_length=10)\n    spec = specs.make_environment_spec(environment)\n\n    # Construct the agent.\n    agent = impala.IMPALA(\n        environment_spec=spec,\n        network=_make_network(spec.actions),\n        sequence_length=3,\n        sequence_period=3,\n        batch_size=6,\n    )\n\n    # Try running the environment loop. We have no assertions here because all\n    # we care about is that the agent runs without raising any errors.\n    loop = acme.EnvironmentLoop(environment, agent)\n    loop.run(num_episodes=20)",
  "class DistributedIMPALA:\n  \"\"\"Program definition for IMPALA.\"\"\"\n\n  def __init__(self,\n               environment_factory: Callable[[bool], dm_env.Environment],\n               network_factory: Callable[[specs.DiscreteArray], snt.RNNCore],\n               num_actors: int,\n               sequence_length: int,\n               sequence_period: int,\n               environment_spec: Optional[specs.EnvironmentSpec] = None,\n               batch_size: int = 256,\n               prefetch_size: int = 4,\n               max_queue_size: int = 10_000,\n               learning_rate: float = 1e-3,\n               discount: float = 0.99,\n               entropy_cost: float = 0.01,\n               baseline_cost: float = 0.5,\n               max_abs_reward: Optional[float] = None,\n               max_gradient_norm: Optional[float] = None,\n               variable_update_period: int = 1000,\n               save_logs: bool = False):\n\n    if environment_spec is None:\n      environment_spec = specs.make_environment_spec(environment_factory(False))\n\n    self._environment_factory = environment_factory\n    self._network_factory = network_factory\n    self._environment_spec = environment_spec\n    self._num_actors = num_actors\n    self._batch_size = batch_size\n    self._prefetch_size = prefetch_size\n    self._sequence_length = sequence_length\n    self._max_queue_size = max_queue_size\n    self._sequence_period = sequence_period\n    self._discount = discount\n    self._learning_rate = learning_rate\n    self._entropy_cost = entropy_cost\n    self._baseline_cost = baseline_cost\n    self._max_abs_reward = max_abs_reward\n    self._max_gradient_norm = max_gradient_norm\n    self._variable_update_period = variable_update_period\n    self._save_logs = save_logs\n\n  def queue(self):\n    \"\"\"The queue.\"\"\"\n    num_actions = self._environment_spec.actions.num_values\n    network = self._network_factory(self._environment_spec.actions)\n    extra_spec = {\n        'core_state': network.initial_state(1),\n        'logits': tf.ones(shape=(1, num_actions), dtype=tf.float32)\n    }\n    # Remove batch dimensions.\n    extra_spec = tf2_utils.squeeze_batch_dim(extra_spec)\n    signature = adders.SequenceAdder.signature(\n        self._environment_spec,\n        extra_spec,\n        sequence_length=self._sequence_length)\n    queue = reverb.Table.queue(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        max_size=self._max_queue_size,\n        signature=signature)\n    return [queue]\n\n  def counter(self):\n    \"\"\"Creates the master counter process.\"\"\"\n    return tf2_savers.CheckpointingRunner(\n        counting.Counter(), time_delta_minutes=1, subdirectory='counter')\n\n  def learner(self, queue: reverb.Client, counter: counting.Counter):\n    \"\"\"The Learning part of the agent.\"\"\"\n    # Use architect and create the environment.\n    # Create the networks.\n    network = self._network_factory(self._environment_spec.actions)\n    tf2_utils.create_variables(network, [self._environment_spec.observations])\n\n    # The dataset object to learn from.\n    dataset = datasets.make_reverb_dataset(\n        server_address=queue.server_address,\n        batch_size=self._batch_size,\n        prefetch_size=self._prefetch_size)\n\n    logger = loggers.make_default_logger('learner', steps_key='learner_steps')\n    counter = counting.Counter(counter, 'learner')\n\n    # Return the learning agent.\n    learner = learning.IMPALALearner(\n        environment_spec=self._environment_spec,\n        network=network,\n        dataset=dataset,\n        discount=self._discount,\n        learning_rate=self._learning_rate,\n        entropy_cost=self._entropy_cost,\n        baseline_cost=self._baseline_cost,\n        max_abs_reward=self._max_abs_reward,\n        max_gradient_norm=self._max_gradient_norm,\n        counter=counter,\n        logger=logger,\n    )\n\n    return tf2_savers.CheckpointingRunner(learner,\n                                          time_delta_minutes=5,\n                                          subdirectory='impala_learner')\n\n  def actor(\n      self,\n      replay: reverb.Client,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ) -> acme.EnvironmentLoop:\n    \"\"\"The actor process.\"\"\"\n    environment = self._environment_factory(False)\n    network = self._network_factory(self._environment_spec.actions)\n    tf2_utils.create_variables(network, [self._environment_spec.observations])\n\n    # Component to add things into the queue.\n    adder = adders.SequenceAdder(\n        client=replay,\n        period=self._sequence_period,\n        sequence_length=self._sequence_length)\n\n    variable_client = tf2_variable_utils.VariableClient(\n        client=variable_source,\n        variables={'policy': network.variables},\n        update_period=self._variable_update_period)\n\n    # Make sure not to use a random policy after checkpoint restoration by\n    # assigning variables before running the environment loop.\n    variable_client.update_and_wait()\n\n    # Create the agent.\n    actor = acting.IMPALAActor(\n        network=network,\n        variable_client=variable_client,\n        adder=adder)\n\n    counter = counting.Counter(counter, 'actor')\n    logger = loggers.make_default_logger(\n        'actor', save_data=False, steps_key='actor_steps')\n\n    # Create the loop to connect environment and agent.\n    return acme.EnvironmentLoop(environment, actor, counter, logger)\n\n  def evaluator(self, variable_source: acme.VariableSource,\n                counter: counting.Counter):\n    \"\"\"The evaluation process.\"\"\"\n    environment = self._environment_factory(True)\n    network = self._network_factory(self._environment_spec.actions)\n    tf2_utils.create_variables(network, [self._environment_spec.observations])\n\n    variable_client = tf2_variable_utils.VariableClient(\n        client=variable_source,\n        variables={'policy': network.variables},\n        update_period=self._variable_update_period)\n\n    # Make sure not to use a random policy after checkpoint restoration by\n    # assigning variables before running the environment loop.\n    variable_client.update_and_wait()\n\n    # Create the agent.\n    actor = acting.IMPALAActor(\n        network=network, variable_client=variable_client)\n\n    # Create the run loop and return it.\n    logger = loggers.make_default_logger(\n        'evaluator', steps_key='evaluator_steps')\n    counter = counting.Counter(counter, 'evaluator')\n    return acme.EnvironmentLoop(environment, actor, counter, logger)\n\n  def build(self, name='impala'):\n    \"\"\"Build the distributed agent topology.\"\"\"\n    program = lp.Program(name=name)\n\n    with program.group('replay'):\n      queue = program.add_node(lp.ReverbNode(self.queue))\n\n    with program.group('counter'):\n      counter = program.add_node(lp.CourierNode(self.counter))\n\n    with program.group('learner'):\n      learner = program.add_node(\n          lp.CourierNode(self.learner, queue, counter))\n\n    with program.group('evaluator'):\n      program.add_node(lp.CourierNode(self.evaluator, learner, counter))\n\n    with program.group('cacher'):\n      cacher = program.add_node(\n          lp.CacherNode(learner, refresh_interval_ms=2000, stale_after_ms=4000))\n\n    with program.group('actor'):\n      for _ in range(self._num_actors):\n        program.add_node(lp.CourierNode(self.actor, queue, cacher, counter))\n\n    return program",
  "def __init__(self,\n               environment_factory: Callable[[bool], dm_env.Environment],\n               network_factory: Callable[[specs.DiscreteArray], snt.RNNCore],\n               num_actors: int,\n               sequence_length: int,\n               sequence_period: int,\n               environment_spec: Optional[specs.EnvironmentSpec] = None,\n               batch_size: int = 256,\n               prefetch_size: int = 4,\n               max_queue_size: int = 10_000,\n               learning_rate: float = 1e-3,\n               discount: float = 0.99,\n               entropy_cost: float = 0.01,\n               baseline_cost: float = 0.5,\n               max_abs_reward: Optional[float] = None,\n               max_gradient_norm: Optional[float] = None,\n               variable_update_period: int = 1000,\n               save_logs: bool = False):\n\n    if environment_spec is None:\n      environment_spec = specs.make_environment_spec(environment_factory(False))\n\n    self._environment_factory = environment_factory\n    self._network_factory = network_factory\n    self._environment_spec = environment_spec\n    self._num_actors = num_actors\n    self._batch_size = batch_size\n    self._prefetch_size = prefetch_size\n    self._sequence_length = sequence_length\n    self._max_queue_size = max_queue_size\n    self._sequence_period = sequence_period\n    self._discount = discount\n    self._learning_rate = learning_rate\n    self._entropy_cost = entropy_cost\n    self._baseline_cost = baseline_cost\n    self._max_abs_reward = max_abs_reward\n    self._max_gradient_norm = max_gradient_norm\n    self._variable_update_period = variable_update_period\n    self._save_logs = save_logs",
  "def queue(self):\n    \"\"\"The queue.\"\"\"\n    num_actions = self._environment_spec.actions.num_values\n    network = self._network_factory(self._environment_spec.actions)\n    extra_spec = {\n        'core_state': network.initial_state(1),\n        'logits': tf.ones(shape=(1, num_actions), dtype=tf.float32)\n    }\n    # Remove batch dimensions.\n    extra_spec = tf2_utils.squeeze_batch_dim(extra_spec)\n    signature = adders.SequenceAdder.signature(\n        self._environment_spec,\n        extra_spec,\n        sequence_length=self._sequence_length)\n    queue = reverb.Table.queue(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        max_size=self._max_queue_size,\n        signature=signature)\n    return [queue]",
  "def counter(self):\n    \"\"\"Creates the master counter process.\"\"\"\n    return tf2_savers.CheckpointingRunner(\n        counting.Counter(), time_delta_minutes=1, subdirectory='counter')",
  "def learner(self, queue: reverb.Client, counter: counting.Counter):\n    \"\"\"The Learning part of the agent.\"\"\"\n    # Use architect and create the environment.\n    # Create the networks.\n    network = self._network_factory(self._environment_spec.actions)\n    tf2_utils.create_variables(network, [self._environment_spec.observations])\n\n    # The dataset object to learn from.\n    dataset = datasets.make_reverb_dataset(\n        server_address=queue.server_address,\n        batch_size=self._batch_size,\n        prefetch_size=self._prefetch_size)\n\n    logger = loggers.make_default_logger('learner', steps_key='learner_steps')\n    counter = counting.Counter(counter, 'learner')\n\n    # Return the learning agent.\n    learner = learning.IMPALALearner(\n        environment_spec=self._environment_spec,\n        network=network,\n        dataset=dataset,\n        discount=self._discount,\n        learning_rate=self._learning_rate,\n        entropy_cost=self._entropy_cost,\n        baseline_cost=self._baseline_cost,\n        max_abs_reward=self._max_abs_reward,\n        max_gradient_norm=self._max_gradient_norm,\n        counter=counter,\n        logger=logger,\n    )\n\n    return tf2_savers.CheckpointingRunner(learner,\n                                          time_delta_minutes=5,\n                                          subdirectory='impala_learner')",
  "def actor(\n      self,\n      replay: reverb.Client,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ) -> acme.EnvironmentLoop:\n    \"\"\"The actor process.\"\"\"\n    environment = self._environment_factory(False)\n    network = self._network_factory(self._environment_spec.actions)\n    tf2_utils.create_variables(network, [self._environment_spec.observations])\n\n    # Component to add things into the queue.\n    adder = adders.SequenceAdder(\n        client=replay,\n        period=self._sequence_period,\n        sequence_length=self._sequence_length)\n\n    variable_client = tf2_variable_utils.VariableClient(\n        client=variable_source,\n        variables={'policy': network.variables},\n        update_period=self._variable_update_period)\n\n    # Make sure not to use a random policy after checkpoint restoration by\n    # assigning variables before running the environment loop.\n    variable_client.update_and_wait()\n\n    # Create the agent.\n    actor = acting.IMPALAActor(\n        network=network,\n        variable_client=variable_client,\n        adder=adder)\n\n    counter = counting.Counter(counter, 'actor')\n    logger = loggers.make_default_logger(\n        'actor', save_data=False, steps_key='actor_steps')\n\n    # Create the loop to connect environment and agent.\n    return acme.EnvironmentLoop(environment, actor, counter, logger)",
  "def evaluator(self, variable_source: acme.VariableSource,\n                counter: counting.Counter):\n    \"\"\"The evaluation process.\"\"\"\n    environment = self._environment_factory(True)\n    network = self._network_factory(self._environment_spec.actions)\n    tf2_utils.create_variables(network, [self._environment_spec.observations])\n\n    variable_client = tf2_variable_utils.VariableClient(\n        client=variable_source,\n        variables={'policy': network.variables},\n        update_period=self._variable_update_period)\n\n    # Make sure not to use a random policy after checkpoint restoration by\n    # assigning variables before running the environment loop.\n    variable_client.update_and_wait()\n\n    # Create the agent.\n    actor = acting.IMPALAActor(\n        network=network, variable_client=variable_client)\n\n    # Create the run loop and return it.\n    logger = loggers.make_default_logger(\n        'evaluator', steps_key='evaluator_steps')\n    counter = counting.Counter(counter, 'evaluator')\n    return acme.EnvironmentLoop(environment, actor, counter, logger)",
  "def build(self, name='impala'):\n    \"\"\"Build the distributed agent topology.\"\"\"\n    program = lp.Program(name=name)\n\n    with program.group('replay'):\n      queue = program.add_node(lp.ReverbNode(self.queue))\n\n    with program.group('counter'):\n      counter = program.add_node(lp.CourierNode(self.counter))\n\n    with program.group('learner'):\n      learner = program.add_node(\n          lp.CourierNode(self.learner, queue, counter))\n\n    with program.group('evaluator'):\n      program.add_node(lp.CourierNode(self.evaluator, learner, counter))\n\n    with program.group('cacher'):\n      cacher = program.add_node(\n          lp.CacherNode(learner, refresh_interval_ms=2000, stale_after_ms=4000))\n\n    with program.group('actor'):\n      for _ in range(self._num_actors):\n        program.add_node(lp.CourierNode(self.actor, queue, cacher, counter))\n\n    return program",
  "class DDPG(agent.Agent):\n  \"\"\"DDPG Agent.\n\n  This implements a single-process DDPG agent. This is an actor-critic algorithm\n  that generates data via a behavior policy, inserts N-step transitions into\n  a replay buffer, and periodically updates the policy (and as a result the\n  behavior) by sampling uniformly from this buffer.\n  \"\"\"\n\n  def __init__(self,\n               environment_spec: specs.EnvironmentSpec,\n               policy_network: snt.Module,\n               critic_network: snt.Module,\n               observation_network: types.TensorTransformation = tf.identity,\n               discount: float = 0.99,\n               batch_size: int = 256,\n               prefetch_size: int = 4,\n               target_update_period: int = 100,\n               min_replay_size: int = 1000,\n               max_replay_size: int = 1000000,\n               samples_per_insert: float = 32.0,\n               n_step: int = 5,\n               sigma: float = 0.3,\n               clipping: bool = True,\n               logger: Optional[loggers.Logger] = None,\n               counter: Optional[counting.Counter] = None,\n               checkpoint: bool = True,\n               replay_table_name: str = adders.DEFAULT_PRIORITY_TABLE):\n    \"\"\"Initialize the agent.\n\n    Args:\n      environment_spec: description of the actions, observations, etc.\n      policy_network: the online (optimized) policy.\n      critic_network: the online critic.\n      observation_network: optional network to transform the observations before\n        they are fed into any network.\n      discount: discount to use for TD updates.\n      batch_size: batch size for updates.\n      prefetch_size: size to prefetch from replay.\n      target_update_period: number of learner steps to perform before updating\n        the target networks.\n      min_replay_size: minimum replay size before updating.\n      max_replay_size: maximum replay size.\n      samples_per_insert: number of samples to take from replay for every insert\n        that is made.\n      n_step: number of steps to squash into a single transition.\n      sigma: standard deviation of zero-mean, Gaussian exploration noise.\n      clipping: whether to clip gradients by global norm.\n      logger: logger object to be used by learner.\n      counter: counter object used to keep track of steps.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n      replay_table_name: string indicating what name to give the replay table.\n    \"\"\"\n    # Create a replay server to add data to. This uses no limiter behavior in\n    # order to allow the Agent interface to handle it.\n    replay_table = reverb.Table(\n        name=replay_table_name,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=max_replay_size,\n        rate_limiter=reverb.rate_limiters.MinSize(1),\n        signature=adders.NStepTransitionAdder.signature(environment_spec))\n    self._server = reverb.Server([replay_table], port=None)\n\n    # The adder is used to insert observations into replay.\n    address = f'localhost:{self._server.port}'\n    adder = adders.NStepTransitionAdder(\n        priority_fns={replay_table_name: lambda x: 1.},\n        client=reverb.Client(address),\n        n_step=n_step,\n        discount=discount)\n\n    # The dataset provides an interface to sample from replay.\n    dataset = datasets.make_reverb_dataset(\n        table=replay_table_name,\n        server_address=address,\n        batch_size=batch_size,\n        prefetch_size=prefetch_size)\n\n    # Make sure observation network is a Sonnet Module.\n    observation_network = tf2_utils.to_sonnet_module(observation_network)\n\n    # Get observation and action specs.\n    act_spec = environment_spec.actions\n    obs_spec = environment_spec.observations\n    emb_spec = tf2_utils.create_variables(observation_network, [obs_spec])\n\n    # Create target networks.\n    target_policy_network = copy.deepcopy(policy_network)\n    target_critic_network = copy.deepcopy(critic_network)\n    target_observation_network = copy.deepcopy(observation_network)\n\n    # Create the behavior policy.\n    behavior_network = snt.Sequential([\n        observation_network,\n        policy_network,\n        networks.ClippedGaussian(sigma),\n        networks.ClipToSpec(act_spec),\n    ])\n\n    # Create variables.\n    tf2_utils.create_variables(policy_network, [emb_spec])\n    tf2_utils.create_variables(critic_network, [emb_spec, act_spec])\n    tf2_utils.create_variables(target_policy_network, [emb_spec])\n    tf2_utils.create_variables(target_critic_network, [emb_spec, act_spec])\n    tf2_utils.create_variables(target_observation_network, [obs_spec])\n\n    # Create the actor which defines how we take actions.\n    actor = actors.FeedForwardActor(behavior_network, adder=adder)\n\n    # Create optimizers.\n    policy_optimizer = snt.optimizers.Adam(learning_rate=1e-4)\n    critic_optimizer = snt.optimizers.Adam(learning_rate=1e-4)\n\n    # The learner updates the parameters (and initializes them).\n    learner = learning.DDPGLearner(\n        policy_network=policy_network,\n        critic_network=critic_network,\n        observation_network=observation_network,\n        target_policy_network=target_policy_network,\n        target_critic_network=target_critic_network,\n        target_observation_network=target_observation_network,\n        policy_optimizer=policy_optimizer,\n        critic_optimizer=critic_optimizer,\n        clipping=clipping,\n        discount=discount,\n        target_update_period=target_update_period,\n        dataset=dataset,\n        counter=counter,\n        logger=logger,\n        checkpoint=checkpoint,\n    )\n\n    super().__init__(\n        actor=actor,\n        learner=learner,\n        min_observations=max(batch_size, min_replay_size),\n        observations_per_step=float(batch_size) / samples_per_insert)",
  "def __init__(self,\n               environment_spec: specs.EnvironmentSpec,\n               policy_network: snt.Module,\n               critic_network: snt.Module,\n               observation_network: types.TensorTransformation = tf.identity,\n               discount: float = 0.99,\n               batch_size: int = 256,\n               prefetch_size: int = 4,\n               target_update_period: int = 100,\n               min_replay_size: int = 1000,\n               max_replay_size: int = 1000000,\n               samples_per_insert: float = 32.0,\n               n_step: int = 5,\n               sigma: float = 0.3,\n               clipping: bool = True,\n               logger: Optional[loggers.Logger] = None,\n               counter: Optional[counting.Counter] = None,\n               checkpoint: bool = True,\n               replay_table_name: str = adders.DEFAULT_PRIORITY_TABLE):\n    \"\"\"Initialize the agent.\n\n    Args:\n      environment_spec: description of the actions, observations, etc.\n      policy_network: the online (optimized) policy.\n      critic_network: the online critic.\n      observation_network: optional network to transform the observations before\n        they are fed into any network.\n      discount: discount to use for TD updates.\n      batch_size: batch size for updates.\n      prefetch_size: size to prefetch from replay.\n      target_update_period: number of learner steps to perform before updating\n        the target networks.\n      min_replay_size: minimum replay size before updating.\n      max_replay_size: maximum replay size.\n      samples_per_insert: number of samples to take from replay for every insert\n        that is made.\n      n_step: number of steps to squash into a single transition.\n      sigma: standard deviation of zero-mean, Gaussian exploration noise.\n      clipping: whether to clip gradients by global norm.\n      logger: logger object to be used by learner.\n      counter: counter object used to keep track of steps.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n      replay_table_name: string indicating what name to give the replay table.\n    \"\"\"\n    # Create a replay server to add data to. This uses no limiter behavior in\n    # order to allow the Agent interface to handle it.\n    replay_table = reverb.Table(\n        name=replay_table_name,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=max_replay_size,\n        rate_limiter=reverb.rate_limiters.MinSize(1),\n        signature=adders.NStepTransitionAdder.signature(environment_spec))\n    self._server = reverb.Server([replay_table], port=None)\n\n    # The adder is used to insert observations into replay.\n    address = f'localhost:{self._server.port}'\n    adder = adders.NStepTransitionAdder(\n        priority_fns={replay_table_name: lambda x: 1.},\n        client=reverb.Client(address),\n        n_step=n_step,\n        discount=discount)\n\n    # The dataset provides an interface to sample from replay.\n    dataset = datasets.make_reverb_dataset(\n        table=replay_table_name,\n        server_address=address,\n        batch_size=batch_size,\n        prefetch_size=prefetch_size)\n\n    # Make sure observation network is a Sonnet Module.\n    observation_network = tf2_utils.to_sonnet_module(observation_network)\n\n    # Get observation and action specs.\n    act_spec = environment_spec.actions\n    obs_spec = environment_spec.observations\n    emb_spec = tf2_utils.create_variables(observation_network, [obs_spec])\n\n    # Create target networks.\n    target_policy_network = copy.deepcopy(policy_network)\n    target_critic_network = copy.deepcopy(critic_network)\n    target_observation_network = copy.deepcopy(observation_network)\n\n    # Create the behavior policy.\n    behavior_network = snt.Sequential([\n        observation_network,\n        policy_network,\n        networks.ClippedGaussian(sigma),\n        networks.ClipToSpec(act_spec),\n    ])\n\n    # Create variables.\n    tf2_utils.create_variables(policy_network, [emb_spec])\n    tf2_utils.create_variables(critic_network, [emb_spec, act_spec])\n    tf2_utils.create_variables(target_policy_network, [emb_spec])\n    tf2_utils.create_variables(target_critic_network, [emb_spec, act_spec])\n    tf2_utils.create_variables(target_observation_network, [obs_spec])\n\n    # Create the actor which defines how we take actions.\n    actor = actors.FeedForwardActor(behavior_network, adder=adder)\n\n    # Create optimizers.\n    policy_optimizer = snt.optimizers.Adam(learning_rate=1e-4)\n    critic_optimizer = snt.optimizers.Adam(learning_rate=1e-4)\n\n    # The learner updates the parameters (and initializes them).\n    learner = learning.DDPGLearner(\n        policy_network=policy_network,\n        critic_network=critic_network,\n        observation_network=observation_network,\n        target_policy_network=target_policy_network,\n        target_critic_network=target_critic_network,\n        target_observation_network=target_observation_network,\n        policy_optimizer=policy_optimizer,\n        critic_optimizer=critic_optimizer,\n        clipping=clipping,\n        discount=discount,\n        target_update_period=target_update_period,\n        dataset=dataset,\n        counter=counter,\n        logger=logger,\n        checkpoint=checkpoint,\n    )\n\n    super().__init__(\n        actor=actor,\n        learner=learner,\n        min_observations=max(batch_size, min_replay_size),\n        observations_per_step=float(batch_size) / samples_per_insert)",
  "class DDPGLearner(acme.Learner):\n  \"\"\"DDPG learner.\n\n  This is the learning component of a DDPG agent. IE it takes a dataset as input\n  and implements update functionality to learn from this dataset.\n  \"\"\"\n\n  def __init__(\n      self,\n      policy_network: snt.Module,\n      critic_network: snt.Module,\n      target_policy_network: snt.Module,\n      target_critic_network: snt.Module,\n      discount: float,\n      target_update_period: int,\n      dataset: tf.data.Dataset,\n      observation_network: types.TensorTransformation = lambda x: x,\n      target_observation_network: types.TensorTransformation = lambda x: x,\n      policy_optimizer: Optional[snt.Optimizer] = None,\n      critic_optimizer: Optional[snt.Optimizer] = None,\n      clipping: bool = True,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = True,\n  ):\n    \"\"\"Initializes the learner.\n\n    Args:\n      policy_network: the online (optimized) policy.\n      critic_network: the online critic.\n      target_policy_network: the target policy (which lags behind the online\n        policy).\n      target_critic_network: the target critic.\n      discount: discount to use for TD updates.\n      target_update_period: number of learner steps to perform before updating\n        the target networks.\n      dataset: dataset to learn from, whether fixed or from a replay buffer\n        (see `acme.datasets.reverb.make_reverb_dataset` documentation).\n      observation_network: an optional online network to process observations\n        before the policy and the critic.\n      target_observation_network: the target observation network.\n      policy_optimizer: the optimizer to be applied to the DPG (policy) loss.\n      critic_optimizer: the optimizer to be applied to the critic loss.\n      clipping: whether to clip gradients by global norm.\n      counter: counter object used to keep track of steps.\n      logger: logger object to be used by learner.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n    \"\"\"\n\n    # Store online and target networks.\n    self._policy_network = policy_network\n    self._critic_network = critic_network\n    self._target_policy_network = target_policy_network\n    self._target_critic_network = target_critic_network\n\n    # Make sure observation networks are snt.Module's so they have variables.\n    self._observation_network = tf2_utils.to_sonnet_module(observation_network)\n    self._target_observation_network = tf2_utils.to_sonnet_module(\n        target_observation_network)\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger('learner')\n\n    # Other learner parameters.\n    self._discount = discount\n    self._clipping = clipping\n\n    # Necessary to track when to update target networks.\n    self._num_steps = tf.Variable(0, dtype=tf.int32)\n    self._target_update_period = target_update_period\n\n    # Create an iterator to go through the dataset.\n    # TODO(b/155086959): Fix type stubs and remove.\n    self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n    # Create optimizers if they aren't given.\n    self._critic_optimizer = critic_optimizer or snt.optimizers.Adam(1e-4)\n    self._policy_optimizer = policy_optimizer or snt.optimizers.Adam(1e-4)\n\n    # Expose the variables.\n    policy_network_to_expose = snt.Sequential(\n        [self._target_observation_network, self._target_policy_network])\n    self._variables = {\n        'critic': target_critic_network.variables,\n        'policy': policy_network_to_expose.variables,\n    }\n\n    self._checkpointer = tf2_savers.Checkpointer(\n        time_delta_minutes=5,\n        objects_to_save={\n            'counter': self._counter,\n            'policy': self._policy_network,\n            'critic': self._critic_network,\n            'target_policy': self._target_policy_network,\n            'target_critic': self._target_critic_network,\n            'policy_optimizer': self._policy_optimizer,\n            'critic_optimizer': self._critic_optimizer,\n            'num_steps': self._num_steps,\n        },\n        enable_checkpointing=checkpoint,\n    )\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None\n\n  @tf.function\n  def _step(self):\n    # Update target network.\n    online_variables = (\n        *self._observation_network.variables,\n        *self._critic_network.variables,\n        *self._policy_network.variables,\n    )\n    target_variables = (\n        *self._target_observation_network.variables,\n        *self._target_critic_network.variables,\n        *self._target_policy_network.variables,\n    )\n\n    # Make online -> target network update ops.\n    if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n      for src, dest in zip(online_variables, target_variables):\n        dest.assign(src)\n    self._num_steps.assign_add(1)\n\n    # Get data from replay (dropping extras if any). Note there is no\n    # extra data here because we do not insert any into Reverb.\n    inputs = next(self._iterator)\n    transitions: types.Transition = inputs.data\n\n    # Cast the additional discount to match the environment discount dtype.\n    discount = tf.cast(self._discount, dtype=transitions.discount.dtype)\n\n    with tf.GradientTape(persistent=True) as tape:\n      # Maybe transform the observation before feeding into policy and critic.\n      # Transforming the observations this way at the start of the learning\n      # step effectively means that the policy and critic share observation\n      # network weights.\n      o_tm1 = self._observation_network(transitions.observation)\n      o_t = self._target_observation_network(transitions.next_observation)\n      # This stop_gradient prevents gradients to propagate into the target\n      # observation network. In addition, since the online policy network is\n      # evaluated at o_t, this also means the policy loss does not influence\n      # the observation network training.\n      o_t = tree.map_structure(tf.stop_gradient, o_t)\n\n      # Critic learning.\n      q_tm1 = self._critic_network(o_tm1, transitions.action)\n      q_t = self._target_critic_network(o_t, self._target_policy_network(o_t))\n\n      # Squeeze into the shape expected by the td_learning implementation.\n      q_tm1 = tf.squeeze(q_tm1, axis=-1)  # [B]\n      q_t = tf.squeeze(q_t, axis=-1)  # [B]\n\n      # Critic loss.\n      critic_loss = trfl.td_learning(q_tm1, transitions.reward,\n                                     discount * transitions.discount, q_t).loss\n      critic_loss = tf.reduce_mean(critic_loss, axis=0)\n\n      # Actor learning.\n      dpg_a_t = self._policy_network(o_t)\n      dpg_q_t = self._critic_network(o_t, dpg_a_t)\n\n      # Actor loss. If clipping is true use dqda clipping and clip the norm.\n      dqda_clipping = 1.0 if self._clipping else None\n      policy_loss = losses.dpg(\n          dpg_q_t,\n          dpg_a_t,\n          tape=tape,\n          dqda_clipping=dqda_clipping,\n          clip_norm=self._clipping)\n      policy_loss = tf.reduce_mean(policy_loss, axis=0)\n\n    # Get trainable variables.\n    policy_variables = self._policy_network.trainable_variables\n    critic_variables = (\n        # In this agent, the critic loss trains the observation network.\n        self._observation_network.trainable_variables +\n        self._critic_network.trainable_variables)\n\n    # Compute gradients.\n    policy_gradients = tape.gradient(policy_loss, policy_variables)\n    critic_gradients = tape.gradient(critic_loss, critic_variables)\n\n    # Delete the tape manually because of the persistent=True flag.\n    del tape\n\n    # Maybe clip gradients.\n    if self._clipping:\n      policy_gradients = tf.clip_by_global_norm(policy_gradients, 40.)[0]\n      critic_gradients = tf.clip_by_global_norm(critic_gradients, 40.)[0]\n\n    # Apply gradients.\n    self._policy_optimizer.apply(policy_gradients, policy_variables)\n    self._critic_optimizer.apply(critic_gradients, critic_variables)\n\n    # Losses to track.\n    return {\n        'critic_loss': critic_loss,\n        'policy_loss': policy_loss,\n    }\n\n  def step(self):\n    # Run the learning step.\n    fetches = self._step()\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n    fetches.update(counts)\n\n    # Checkpoint and attempt to write the logs.\n    self._checkpointer.save()\n    self._logger.write(fetches)\n\n  def get_variables(self, names: List[str]) -> List[List[np.ndarray]]:\n    return [tf2_utils.to_numpy(self._variables[name]) for name in names]",
  "def __init__(\n      self,\n      policy_network: snt.Module,\n      critic_network: snt.Module,\n      target_policy_network: snt.Module,\n      target_critic_network: snt.Module,\n      discount: float,\n      target_update_period: int,\n      dataset: tf.data.Dataset,\n      observation_network: types.TensorTransformation = lambda x: x,\n      target_observation_network: types.TensorTransformation = lambda x: x,\n      policy_optimizer: Optional[snt.Optimizer] = None,\n      critic_optimizer: Optional[snt.Optimizer] = None,\n      clipping: bool = True,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = True,\n  ):\n    \"\"\"Initializes the learner.\n\n    Args:\n      policy_network: the online (optimized) policy.\n      critic_network: the online critic.\n      target_policy_network: the target policy (which lags behind the online\n        policy).\n      target_critic_network: the target critic.\n      discount: discount to use for TD updates.\n      target_update_period: number of learner steps to perform before updating\n        the target networks.\n      dataset: dataset to learn from, whether fixed or from a replay buffer\n        (see `acme.datasets.reverb.make_reverb_dataset` documentation).\n      observation_network: an optional online network to process observations\n        before the policy and the critic.\n      target_observation_network: the target observation network.\n      policy_optimizer: the optimizer to be applied to the DPG (policy) loss.\n      critic_optimizer: the optimizer to be applied to the critic loss.\n      clipping: whether to clip gradients by global norm.\n      counter: counter object used to keep track of steps.\n      logger: logger object to be used by learner.\n      checkpoint: boolean indicating whether to checkpoint the learner.\n    \"\"\"\n\n    # Store online and target networks.\n    self._policy_network = policy_network\n    self._critic_network = critic_network\n    self._target_policy_network = target_policy_network\n    self._target_critic_network = target_critic_network\n\n    # Make sure observation networks are snt.Module's so they have variables.\n    self._observation_network = tf2_utils.to_sonnet_module(observation_network)\n    self._target_observation_network = tf2_utils.to_sonnet_module(\n        target_observation_network)\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger('learner')\n\n    # Other learner parameters.\n    self._discount = discount\n    self._clipping = clipping\n\n    # Necessary to track when to update target networks.\n    self._num_steps = tf.Variable(0, dtype=tf.int32)\n    self._target_update_period = target_update_period\n\n    # Create an iterator to go through the dataset.\n    # TODO(b/155086959): Fix type stubs and remove.\n    self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n\n    # Create optimizers if they aren't given.\n    self._critic_optimizer = critic_optimizer or snt.optimizers.Adam(1e-4)\n    self._policy_optimizer = policy_optimizer or snt.optimizers.Adam(1e-4)\n\n    # Expose the variables.\n    policy_network_to_expose = snt.Sequential(\n        [self._target_observation_network, self._target_policy_network])\n    self._variables = {\n        'critic': target_critic_network.variables,\n        'policy': policy_network_to_expose.variables,\n    }\n\n    self._checkpointer = tf2_savers.Checkpointer(\n        time_delta_minutes=5,\n        objects_to_save={\n            'counter': self._counter,\n            'policy': self._policy_network,\n            'critic': self._critic_network,\n            'target_policy': self._target_policy_network,\n            'target_critic': self._target_critic_network,\n            'policy_optimizer': self._policy_optimizer,\n            'critic_optimizer': self._critic_optimizer,\n            'num_steps': self._num_steps,\n        },\n        enable_checkpointing=checkpoint,\n    )\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None",
  "def _step(self):\n    # Update target network.\n    online_variables = (\n        *self._observation_network.variables,\n        *self._critic_network.variables,\n        *self._policy_network.variables,\n    )\n    target_variables = (\n        *self._target_observation_network.variables,\n        *self._target_critic_network.variables,\n        *self._target_policy_network.variables,\n    )\n\n    # Make online -> target network update ops.\n    if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n      for src, dest in zip(online_variables, target_variables):\n        dest.assign(src)\n    self._num_steps.assign_add(1)\n\n    # Get data from replay (dropping extras if any). Note there is no\n    # extra data here because we do not insert any into Reverb.\n    inputs = next(self._iterator)\n    transitions: types.Transition = inputs.data\n\n    # Cast the additional discount to match the environment discount dtype.\n    discount = tf.cast(self._discount, dtype=transitions.discount.dtype)\n\n    with tf.GradientTape(persistent=True) as tape:\n      # Maybe transform the observation before feeding into policy and critic.\n      # Transforming the observations this way at the start of the learning\n      # step effectively means that the policy and critic share observation\n      # network weights.\n      o_tm1 = self._observation_network(transitions.observation)\n      o_t = self._target_observation_network(transitions.next_observation)\n      # This stop_gradient prevents gradients to propagate into the target\n      # observation network. In addition, since the online policy network is\n      # evaluated at o_t, this also means the policy loss does not influence\n      # the observation network training.\n      o_t = tree.map_structure(tf.stop_gradient, o_t)\n\n      # Critic learning.\n      q_tm1 = self._critic_network(o_tm1, transitions.action)\n      q_t = self._target_critic_network(o_t, self._target_policy_network(o_t))\n\n      # Squeeze into the shape expected by the td_learning implementation.\n      q_tm1 = tf.squeeze(q_tm1, axis=-1)  # [B]\n      q_t = tf.squeeze(q_t, axis=-1)  # [B]\n\n      # Critic loss.\n      critic_loss = trfl.td_learning(q_tm1, transitions.reward,\n                                     discount * transitions.discount, q_t).loss\n      critic_loss = tf.reduce_mean(critic_loss, axis=0)\n\n      # Actor learning.\n      dpg_a_t = self._policy_network(o_t)\n      dpg_q_t = self._critic_network(o_t, dpg_a_t)\n\n      # Actor loss. If clipping is true use dqda clipping and clip the norm.\n      dqda_clipping = 1.0 if self._clipping else None\n      policy_loss = losses.dpg(\n          dpg_q_t,\n          dpg_a_t,\n          tape=tape,\n          dqda_clipping=dqda_clipping,\n          clip_norm=self._clipping)\n      policy_loss = tf.reduce_mean(policy_loss, axis=0)\n\n    # Get trainable variables.\n    policy_variables = self._policy_network.trainable_variables\n    critic_variables = (\n        # In this agent, the critic loss trains the observation network.\n        self._observation_network.trainable_variables +\n        self._critic_network.trainable_variables)\n\n    # Compute gradients.\n    policy_gradients = tape.gradient(policy_loss, policy_variables)\n    critic_gradients = tape.gradient(critic_loss, critic_variables)\n\n    # Delete the tape manually because of the persistent=True flag.\n    del tape\n\n    # Maybe clip gradients.\n    if self._clipping:\n      policy_gradients = tf.clip_by_global_norm(policy_gradients, 40.)[0]\n      critic_gradients = tf.clip_by_global_norm(critic_gradients, 40.)[0]\n\n    # Apply gradients.\n    self._policy_optimizer.apply(policy_gradients, policy_variables)\n    self._critic_optimizer.apply(critic_gradients, critic_variables)\n\n    # Losses to track.\n    return {\n        'critic_loss': critic_loss,\n        'policy_loss': policy_loss,\n    }",
  "def step(self):\n    # Run the learning step.\n    fetches = self._step()\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n    fetches.update(counts)\n\n    # Checkpoint and attempt to write the logs.\n    self._checkpointer.save()\n    self._logger.write(fetches)",
  "def get_variables(self, names: List[str]) -> List[List[np.ndarray]]:\n    return [tf2_utils.to_numpy(self._variables[name]) for name in names]",
  "def make_networks(action_spec: specs.BoundedArray):\n  \"\"\"Creates simple networks for testing..\"\"\"\n\n  num_dimensions = np.prod(action_spec.shape, dtype=int)\n\n  # Create the observation network shared between the policy and critic.\n  observation_network = tf2_utils.batch_concat\n\n  # Create the policy network (head) and the evaluation network.\n  policy_network = snt.Sequential([\n      networks.LayerNormMLP([50], activate_final=True),\n      networks.NearZeroInitializedLinear(num_dimensions),\n      networks.TanhToSpec(action_spec)\n  ])\n  evaluator_network = snt.Sequential([observation_network, policy_network])\n\n  # Create the critic network.\n  critic_network = snt.Sequential([\n      # The multiplexer concatenates the observations/actions.\n      networks.CriticMultiplexer(),\n      networks.LayerNormMLP([50], activate_final=True),\n      networks.NearZeroInitializedLinear(1),\n  ])\n\n  return {\n      'policy': policy_network,\n      'critic': critic_network,\n      'observation': observation_network,\n      'evaluator': evaluator_network,\n  }",
  "class DistributedAgentTest(absltest.TestCase):\n  \"\"\"Simple integration/smoke test for the distributed agent.\"\"\"\n\n  def test_agent(self):\n\n    agent = ddpg.DistributedDDPG(\n        environment_factory=lambda x: fakes.ContinuousEnvironment(bounded=True),\n        network_factory=make_networks,\n        num_actors=2,\n        batch_size=32,\n        min_replay_size=32,\n        max_replay_size=1000,\n    )\n    program = agent.build()\n\n    (learner_node,) = program.groups['learner']\n    learner_node.disable_run()\n\n    lp.launch(program, launch_type='test_mt')\n\n    learner: acme.Learner = learner_node.create_handle().dereference()\n\n    for _ in range(5):\n      learner.step()",
  "def test_agent(self):\n\n    agent = ddpg.DistributedDDPG(\n        environment_factory=lambda x: fakes.ContinuousEnvironment(bounded=True),\n        network_factory=make_networks,\n        num_actors=2,\n        batch_size=32,\n        min_replay_size=32,\n        max_replay_size=1000,\n    )\n    program = agent.build()\n\n    (learner_node,) = program.groups['learner']\n    learner_node.disable_run()\n\n    lp.launch(program, launch_type='test_mt')\n\n    learner: acme.Learner = learner_node.create_handle().dereference()\n\n    for _ in range(5):\n      learner.step()",
  "def make_networks(\n    action_spec: types.NestedSpec,\n    policy_layer_sizes: Sequence[int] = (10, 10),\n    critic_layer_sizes: Sequence[int] = (10, 10),\n) -> Dict[str, snt.Module]:\n  \"\"\"Creates networks used by the agent.\"\"\"\n\n  num_dimensions = np.prod(action_spec.shape, dtype=int)\n  policy_layer_sizes = list(policy_layer_sizes) + [num_dimensions]\n  critic_layer_sizes = list(critic_layer_sizes) + [1]\n\n  policy_network = snt.Sequential(\n      [networks.LayerNormMLP(policy_layer_sizes), tf.tanh])\n  # The multiplexer concatenates the (maybe transformed) observations/actions.\n  critic_network = networks.CriticMultiplexer(\n      critic_network=networks.LayerNormMLP(critic_layer_sizes))\n\n  return {\n      'policy': policy_network,\n      'critic': critic_network,\n  }",
  "class DDPGTest(absltest.TestCase):\n\n  def test_ddpg(self):\n    # Create a fake environment to test with.\n    environment = fakes.ContinuousEnvironment(episode_length=10, bounded=True)\n    spec = specs.make_environment_spec(environment)\n\n    # Create the networks to optimize (online) and target networks.\n    agent_networks = make_networks(spec.actions)\n\n    # Construct the agent.\n    agent = ddpg.DDPG(\n        environment_spec=spec,\n        policy_network=agent_networks['policy'],\n        critic_network=agent_networks['critic'],\n        batch_size=10,\n        samples_per_insert=2,\n        min_replay_size=10,\n    )\n\n    # Try running the environment loop. We have no assertions here because all\n    # we care about is that the agent runs without raising any errors.\n    loop = acme.EnvironmentLoop(environment, agent)\n    loop.run(num_episodes=2)",
  "def test_ddpg(self):\n    # Create a fake environment to test with.\n    environment = fakes.ContinuousEnvironment(episode_length=10, bounded=True)\n    spec = specs.make_environment_spec(environment)\n\n    # Create the networks to optimize (online) and target networks.\n    agent_networks = make_networks(spec.actions)\n\n    # Construct the agent.\n    agent = ddpg.DDPG(\n        environment_spec=spec,\n        policy_network=agent_networks['policy'],\n        critic_network=agent_networks['critic'],\n        batch_size=10,\n        samples_per_insert=2,\n        min_replay_size=10,\n    )\n\n    # Try running the environment loop. We have no assertions here because all\n    # we care about is that the agent runs without raising any errors.\n    loop = acme.EnvironmentLoop(environment, agent)\n    loop.run(num_episodes=2)",
  "class DistributedDDPG:\n  \"\"\"Program definition for distributed DDPG (D3PG).\"\"\"\n\n  def __init__(\n      self,\n      environment_factory: Callable[[bool], dm_env.Environment],\n      network_factory: Callable[[specs.BoundedArray], Dict[str, snt.Module]],\n      num_actors: int = 1,\n      num_caches: int = 0,\n      environment_spec: Optional[specs.EnvironmentSpec] = None,\n      batch_size: int = 256,\n      prefetch_size: int = 4,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1000000,\n      samples_per_insert: Optional[float] = 32.0,\n      n_step: int = 5,\n      sigma: float = 0.3,\n      clipping: bool = True,\n      discount: float = 0.99,\n      target_update_period: int = 100,\n      variable_update_period: int = 1000,\n      max_actor_steps: Optional[int] = None,\n      log_every: float = 10.0,\n  ):\n\n    if not environment_spec:\n      environment_spec = specs.make_environment_spec(environment_factory(False))\n\n    self._environment_factory = environment_factory\n    self._network_factory = network_factory\n    self._environment_spec = environment_spec\n    self._num_actors = num_actors\n    self._num_caches = num_caches\n    self._batch_size = batch_size\n    self._prefetch_size = prefetch_size\n    self._min_replay_size = min_replay_size\n    self._max_replay_size = max_replay_size\n    self._samples_per_insert = samples_per_insert\n    self._n_step = n_step\n    self._sigma = sigma\n    self._clipping = clipping\n    self._discount = discount\n    self._target_update_period = target_update_period\n    self._variable_update_period = variable_update_period\n    self._max_actor_steps = max_actor_steps\n    self._log_every = log_every\n\n  def replay(self):\n    \"\"\"The replay storage.\"\"\"\n    if self._samples_per_insert is not None:\n      # Create enough of an error buffer to give a 10% tolerance in rate.\n      samples_per_insert_tolerance = 0.1 * self._samples_per_insert\n      error_buffer = self._min_replay_size * samples_per_insert_tolerance\n\n      limiter = reverb.rate_limiters.SampleToInsertRatio(\n          min_size_to_sample=self._min_replay_size,\n          samples_per_insert=self._samples_per_insert,\n          error_buffer=error_buffer)\n    else:\n      limiter = reverb.rate_limiters.MinSize(self._min_replay_size)\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=self._max_replay_size,\n        rate_limiter=limiter,\n        signature=adders.NStepTransitionAdder.signature(\n            self._environment_spec))\n    return [replay_table]\n\n  def counter(self):\n    return tf2_savers.CheckpointingRunner(counting.Counter(),\n                                          time_delta_minutes=1,\n                                          subdirectory='counter')\n\n  def coordinator(self, counter: counting.Counter, max_actor_steps: int):\n    return lp_utils.StepsLimiter(counter, max_actor_steps)\n\n  def learner(\n      self,\n      replay: reverb.Client,\n      counter: counting.Counter,\n  ):\n    \"\"\"The Learning part of the agent.\"\"\"\n\n    act_spec = self._environment_spec.actions\n    obs_spec = self._environment_spec.observations\n\n    # Create the networks to optimize (online) and target networks.\n    online_networks = self._network_factory(act_spec)\n    target_networks = self._network_factory(act_spec)\n\n    # Make sure observation network is a Sonnet Module.\n    observation_network = online_networks.get('observation', tf.identity)\n    target_observation_network = target_networks.get('observation', tf.identity)\n    observation_network = tf2_utils.to_sonnet_module(observation_network)\n    target_observation_network = tf2_utils.to_sonnet_module(\n        target_observation_network)\n\n    # Get embedding spec and create observation network variables.\n    emb_spec = tf2_utils.create_variables(observation_network, [obs_spec])\n\n    # Create variables.\n    tf2_utils.create_variables(online_networks['policy'], [emb_spec])\n    tf2_utils.create_variables(online_networks['critic'], [emb_spec, act_spec])\n    tf2_utils.create_variables(target_networks['policy'], [emb_spec])\n    tf2_utils.create_variables(target_networks['critic'], [emb_spec, act_spec])\n    tf2_utils.create_variables(target_observation_network, [obs_spec])\n\n    # The dataset object to learn from.\n    dataset = datasets.make_reverb_dataset(\n        server_address=replay.server_address,\n        batch_size=self._batch_size,\n        prefetch_size=self._prefetch_size)\n\n    # Create optimizers.\n    policy_optimizer = snt.optimizers.Adam(learning_rate=1e-4)\n    critic_optimizer = snt.optimizers.Adam(learning_rate=1e-4)\n\n    counter = counting.Counter(counter, 'learner')\n    logger = loggers.make_default_logger(\n        'learner', time_delta=self._log_every, steps_key='learner_steps')\n\n    # Return the learning agent.\n    return learning.DDPGLearner(\n        policy_network=online_networks['policy'],\n        critic_network=online_networks['critic'],\n        observation_network=observation_network,\n        target_policy_network=target_networks['policy'],\n        target_critic_network=target_networks['critic'],\n        target_observation_network=target_observation_network,\n        discount=self._discount,\n        target_update_period=self._target_update_period,\n        dataset=dataset,\n        policy_optimizer=policy_optimizer,\n        critic_optimizer=critic_optimizer,\n        clipping=self._clipping,\n        counter=counter,\n        logger=logger,\n    )\n\n  def actor(\n      self,\n      replay: reverb.Client,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ):\n    \"\"\"The actor process.\"\"\"\n\n    action_spec = self._environment_spec.actions\n    observation_spec = self._environment_spec.observations\n\n    # Create environment and behavior networks\n    environment = self._environment_factory(False)\n    agent_networks = self._network_factory(action_spec)\n\n    # Create behavior network by adding some random dithering.\n    behavior_network = snt.Sequential([\n        agent_networks.get('observation', tf.identity),\n        agent_networks.get('policy'),\n        networks.ClippedGaussian(self._sigma),\n    ])\n\n    # Ensure network variables are created.\n    tf2_utils.create_variables(behavior_network, [observation_spec])\n    variables = {'policy': behavior_network.variables}\n\n    # Create the variable client responsible for keeping the actor up-to-date.\n    variable_client = tf2_variable_utils.VariableClient(\n        variable_source, variables, update_period=self._variable_update_period)\n\n    # Make sure not to use a random policy after checkpoint restoration by\n    # assigning variables before running the environment loop.\n    variable_client.update_and_wait()\n\n    # Component to add things into replay.\n    adder = adders.NStepTransitionAdder(\n        client=replay, n_step=self._n_step, discount=self._discount)\n\n    # Create the agent.\n    actor = actors.FeedForwardActor(\n        behavior_network, adder=adder, variable_client=variable_client)\n\n    # Create logger and counter; actors will not spam bigtable.\n    counter = counting.Counter(counter, 'actor')\n    logger = loggers.make_default_logger(\n        'actor',\n        save_data=False,\n        time_delta=self._log_every,\n        steps_key='actor_steps')\n\n    # Create the loop to connect environment and agent.\n    return acme.EnvironmentLoop(environment, actor, counter, logger)\n\n  def evaluator(\n      self,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ):\n    \"\"\"The evaluation process.\"\"\"\n\n    action_spec = self._environment_spec.actions\n    observation_spec = self._environment_spec.observations\n\n    # Create environment and evaluator networks\n    environment = self._environment_factory(True)\n    agent_networks = self._network_factory(action_spec)\n\n    # Create evaluator network.\n    evaluator_network = snt.Sequential([\n        agent_networks.get('observation', tf.identity),\n        agent_networks.get('policy'),\n    ])\n\n    # Ensure network variables are created.\n    tf2_utils.create_variables(evaluator_network, [observation_spec])\n    variables = {'policy': evaluator_network.variables}\n\n    # Create the variable client responsible for keeping the actor up-to-date.\n    variable_client = tf2_variable_utils.VariableClient(\n        variable_source, variables, update_period=self._variable_update_period)\n\n    # Make sure not to evaluate a random actor by assigning variables before\n    # running the environment loop.\n    variable_client.update_and_wait()\n\n    # Create the evaluator; note it will not add experience to replay.\n    evaluator = actors.FeedForwardActor(\n        evaluator_network, variable_client=variable_client)\n\n    # Create logger and counter.\n    counter = counting.Counter(counter, 'evaluator')\n    logger = loggers.make_default_logger(\n        'evaluator', time_delta=self._log_every, steps_key='evaluator_steps')\n\n    # Create the run loop and return it.\n    return acme.EnvironmentLoop(\n        environment, evaluator, counter, logger)\n\n  def build(self, name='ddpg'):\n    \"\"\"Build the distributed agent topology.\"\"\"\n    program = lp.Program(name=name)\n\n    with program.group('replay'):\n      replay = program.add_node(lp.ReverbNode(self.replay))\n\n    with program.group('counter'):\n      counter = program.add_node(lp.CourierNode(self.counter))\n\n      if self._max_actor_steps:\n        _ = program.add_node(\n            lp.CourierNode(self.coordinator, counter, self._max_actor_steps))\n\n    with program.group('learner'):\n      learner = program.add_node(\n          lp.CourierNode(self.learner, replay, counter))\n\n    with program.group('evaluator'):\n      program.add_node(\n          lp.CourierNode(self.evaluator, learner, counter))\n\n    if not self._num_caches:\n      # Use our learner as a single variable source.\n      sources = [learner]\n    else:\n      with program.group('cacher'):\n        # Create a set of learner caches.\n        sources = []\n        for _ in range(self._num_caches):\n          cacher = program.add_node(\n              lp.CacherNode(\n                  learner, refresh_interval_ms=2000, stale_after_ms=4000))\n          sources.append(cacher)\n\n    with program.group('actor'):\n      # Add actors which pull round-robin from our variable sources.\n      for actor_id in range(self._num_actors):\n        source = sources[actor_id % len(sources)]\n        program.add_node(lp.CourierNode(self.actor, replay, source, counter))\n\n    return program",
  "def __init__(\n      self,\n      environment_factory: Callable[[bool], dm_env.Environment],\n      network_factory: Callable[[specs.BoundedArray], Dict[str, snt.Module]],\n      num_actors: int = 1,\n      num_caches: int = 0,\n      environment_spec: Optional[specs.EnvironmentSpec] = None,\n      batch_size: int = 256,\n      prefetch_size: int = 4,\n      min_replay_size: int = 1000,\n      max_replay_size: int = 1000000,\n      samples_per_insert: Optional[float] = 32.0,\n      n_step: int = 5,\n      sigma: float = 0.3,\n      clipping: bool = True,\n      discount: float = 0.99,\n      target_update_period: int = 100,\n      variable_update_period: int = 1000,\n      max_actor_steps: Optional[int] = None,\n      log_every: float = 10.0,\n  ):\n\n    if not environment_spec:\n      environment_spec = specs.make_environment_spec(environment_factory(False))\n\n    self._environment_factory = environment_factory\n    self._network_factory = network_factory\n    self._environment_spec = environment_spec\n    self._num_actors = num_actors\n    self._num_caches = num_caches\n    self._batch_size = batch_size\n    self._prefetch_size = prefetch_size\n    self._min_replay_size = min_replay_size\n    self._max_replay_size = max_replay_size\n    self._samples_per_insert = samples_per_insert\n    self._n_step = n_step\n    self._sigma = sigma\n    self._clipping = clipping\n    self._discount = discount\n    self._target_update_period = target_update_period\n    self._variable_update_period = variable_update_period\n    self._max_actor_steps = max_actor_steps\n    self._log_every = log_every",
  "def replay(self):\n    \"\"\"The replay storage.\"\"\"\n    if self._samples_per_insert is not None:\n      # Create enough of an error buffer to give a 10% tolerance in rate.\n      samples_per_insert_tolerance = 0.1 * self._samples_per_insert\n      error_buffer = self._min_replay_size * samples_per_insert_tolerance\n\n      limiter = reverb.rate_limiters.SampleToInsertRatio(\n          min_size_to_sample=self._min_replay_size,\n          samples_per_insert=self._samples_per_insert,\n          error_buffer=error_buffer)\n    else:\n      limiter = reverb.rate_limiters.MinSize(self._min_replay_size)\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=self._max_replay_size,\n        rate_limiter=limiter,\n        signature=adders.NStepTransitionAdder.signature(\n            self._environment_spec))\n    return [replay_table]",
  "def counter(self):\n    return tf2_savers.CheckpointingRunner(counting.Counter(),\n                                          time_delta_minutes=1,\n                                          subdirectory='counter')",
  "def coordinator(self, counter: counting.Counter, max_actor_steps: int):\n    return lp_utils.StepsLimiter(counter, max_actor_steps)",
  "def learner(\n      self,\n      replay: reverb.Client,\n      counter: counting.Counter,\n  ):\n    \"\"\"The Learning part of the agent.\"\"\"\n\n    act_spec = self._environment_spec.actions\n    obs_spec = self._environment_spec.observations\n\n    # Create the networks to optimize (online) and target networks.\n    online_networks = self._network_factory(act_spec)\n    target_networks = self._network_factory(act_spec)\n\n    # Make sure observation network is a Sonnet Module.\n    observation_network = online_networks.get('observation', tf.identity)\n    target_observation_network = target_networks.get('observation', tf.identity)\n    observation_network = tf2_utils.to_sonnet_module(observation_network)\n    target_observation_network = tf2_utils.to_sonnet_module(\n        target_observation_network)\n\n    # Get embedding spec and create observation network variables.\n    emb_spec = tf2_utils.create_variables(observation_network, [obs_spec])\n\n    # Create variables.\n    tf2_utils.create_variables(online_networks['policy'], [emb_spec])\n    tf2_utils.create_variables(online_networks['critic'], [emb_spec, act_spec])\n    tf2_utils.create_variables(target_networks['policy'], [emb_spec])\n    tf2_utils.create_variables(target_networks['critic'], [emb_spec, act_spec])\n    tf2_utils.create_variables(target_observation_network, [obs_spec])\n\n    # The dataset object to learn from.\n    dataset = datasets.make_reverb_dataset(\n        server_address=replay.server_address,\n        batch_size=self._batch_size,\n        prefetch_size=self._prefetch_size)\n\n    # Create optimizers.\n    policy_optimizer = snt.optimizers.Adam(learning_rate=1e-4)\n    critic_optimizer = snt.optimizers.Adam(learning_rate=1e-4)\n\n    counter = counting.Counter(counter, 'learner')\n    logger = loggers.make_default_logger(\n        'learner', time_delta=self._log_every, steps_key='learner_steps')\n\n    # Return the learning agent.\n    return learning.DDPGLearner(\n        policy_network=online_networks['policy'],\n        critic_network=online_networks['critic'],\n        observation_network=observation_network,\n        target_policy_network=target_networks['policy'],\n        target_critic_network=target_networks['critic'],\n        target_observation_network=target_observation_network,\n        discount=self._discount,\n        target_update_period=self._target_update_period,\n        dataset=dataset,\n        policy_optimizer=policy_optimizer,\n        critic_optimizer=critic_optimizer,\n        clipping=self._clipping,\n        counter=counter,\n        logger=logger,\n    )",
  "def actor(\n      self,\n      replay: reverb.Client,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ):\n    \"\"\"The actor process.\"\"\"\n\n    action_spec = self._environment_spec.actions\n    observation_spec = self._environment_spec.observations\n\n    # Create environment and behavior networks\n    environment = self._environment_factory(False)\n    agent_networks = self._network_factory(action_spec)\n\n    # Create behavior network by adding some random dithering.\n    behavior_network = snt.Sequential([\n        agent_networks.get('observation', tf.identity),\n        agent_networks.get('policy'),\n        networks.ClippedGaussian(self._sigma),\n    ])\n\n    # Ensure network variables are created.\n    tf2_utils.create_variables(behavior_network, [observation_spec])\n    variables = {'policy': behavior_network.variables}\n\n    # Create the variable client responsible for keeping the actor up-to-date.\n    variable_client = tf2_variable_utils.VariableClient(\n        variable_source, variables, update_period=self._variable_update_period)\n\n    # Make sure not to use a random policy after checkpoint restoration by\n    # assigning variables before running the environment loop.\n    variable_client.update_and_wait()\n\n    # Component to add things into replay.\n    adder = adders.NStepTransitionAdder(\n        client=replay, n_step=self._n_step, discount=self._discount)\n\n    # Create the agent.\n    actor = actors.FeedForwardActor(\n        behavior_network, adder=adder, variable_client=variable_client)\n\n    # Create logger and counter; actors will not spam bigtable.\n    counter = counting.Counter(counter, 'actor')\n    logger = loggers.make_default_logger(\n        'actor',\n        save_data=False,\n        time_delta=self._log_every,\n        steps_key='actor_steps')\n\n    # Create the loop to connect environment and agent.\n    return acme.EnvironmentLoop(environment, actor, counter, logger)",
  "def evaluator(\n      self,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ):\n    \"\"\"The evaluation process.\"\"\"\n\n    action_spec = self._environment_spec.actions\n    observation_spec = self._environment_spec.observations\n\n    # Create environment and evaluator networks\n    environment = self._environment_factory(True)\n    agent_networks = self._network_factory(action_spec)\n\n    # Create evaluator network.\n    evaluator_network = snt.Sequential([\n        agent_networks.get('observation', tf.identity),\n        agent_networks.get('policy'),\n    ])\n\n    # Ensure network variables are created.\n    tf2_utils.create_variables(evaluator_network, [observation_spec])\n    variables = {'policy': evaluator_network.variables}\n\n    # Create the variable client responsible for keeping the actor up-to-date.\n    variable_client = tf2_variable_utils.VariableClient(\n        variable_source, variables, update_period=self._variable_update_period)\n\n    # Make sure not to evaluate a random actor by assigning variables before\n    # running the environment loop.\n    variable_client.update_and_wait()\n\n    # Create the evaluator; note it will not add experience to replay.\n    evaluator = actors.FeedForwardActor(\n        evaluator_network, variable_client=variable_client)\n\n    # Create logger and counter.\n    counter = counting.Counter(counter, 'evaluator')\n    logger = loggers.make_default_logger(\n        'evaluator', time_delta=self._log_every, steps_key='evaluator_steps')\n\n    # Create the run loop and return it.\n    return acme.EnvironmentLoop(\n        environment, evaluator, counter, logger)",
  "def build(self, name='ddpg'):\n    \"\"\"Build the distributed agent topology.\"\"\"\n    program = lp.Program(name=name)\n\n    with program.group('replay'):\n      replay = program.add_node(lp.ReverbNode(self.replay))\n\n    with program.group('counter'):\n      counter = program.add_node(lp.CourierNode(self.counter))\n\n      if self._max_actor_steps:\n        _ = program.add_node(\n            lp.CourierNode(self.coordinator, counter, self._max_actor_steps))\n\n    with program.group('learner'):\n      learner = program.add_node(\n          lp.CourierNode(self.learner, replay, counter))\n\n    with program.group('evaluator'):\n      program.add_node(\n          lp.CourierNode(self.evaluator, learner, counter))\n\n    if not self._num_caches:\n      # Use our learner as a single variable source.\n      sources = [learner]\n    else:\n      with program.group('cacher'):\n        # Create a set of learner caches.\n        sources = []\n        for _ in range(self._num_caches):\n          cacher = program.add_node(\n              lp.CacherNode(\n                  learner, refresh_interval_ms=2000, stale_after_ms=4000))\n          sources.append(cacher)\n\n    with program.group('actor'):\n      # Add actors which pull round-robin from our variable sources.\n      for actor_id in range(self._num_actors):\n        source = sources[actor_id % len(sources)]\n        program.add_node(lp.CourierNode(self.actor, replay, source, counter))\n\n    return program",
  "class R2D3(agent.Agent):\n  \"\"\"R2D3 Agent.\n\n  This implements a single-process R2D2 agent that mixes demonstrations with\n  actor experience.\n  \"\"\"\n\n  def __init__(self,\n               environment_spec: specs.EnvironmentSpec,\n               network: snt.RNNCore,\n               target_network: snt.RNNCore,\n               burn_in_length: int,\n               trace_length: int,\n               replay_period: int,\n               demonstration_dataset: tf.data.Dataset,\n               demonstration_ratio: float,\n               counter: Optional[counting.Counter] = None,\n               logger: Optional[loggers.Logger] = None,\n               discount: float = 0.99,\n               batch_size: int = 32,\n               target_update_period: int = 100,\n               importance_sampling_exponent: float = 0.2,\n               epsilon: float = 0.01,\n               learning_rate: float = 1e-3,\n               save_logs: bool = False,\n               log_name: str = 'agent',\n               checkpoint: bool = True,\n               min_replay_size: int = 1000,\n               max_replay_size: int = 1000000,\n               samples_per_insert: float = 32.0):\n\n    sequence_length = burn_in_length + trace_length + 1\n    extra_spec = {\n        'core_state': network.initial_state(1),\n    }\n    # Remove batch dimensions.\n    extra_spec = tf2_utils.squeeze_batch_dim(extra_spec)\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=max_replay_size,\n        rate_limiter=reverb.rate_limiters.MinSize(min_size_to_sample=1),\n        signature=adders.SequenceAdder.signature(\n            environment_spec, extra_spec, sequence_length=sequence_length))\n    self._server = reverb.Server([replay_table], port=None)\n    address = f'localhost:{self._server.port}'\n\n    # Component to add things into replay.\n    sequence_kwargs = dict(\n        period=replay_period,\n        sequence_length=sequence_length,\n    )\n    adder = adders.SequenceAdder(client=reverb.Client(address),\n                                 **sequence_kwargs)\n\n    # The dataset object to learn from.\n    dataset = datasets.make_reverb_dataset(\n        server_address=address)\n\n    # Combine with demonstration dataset.\n    transition = functools.partial(_sequence_from_episode,\n                                   extra_spec=extra_spec,\n                                   **sequence_kwargs)\n    dataset_demos = demonstration_dataset.map(transition)\n    dataset = tf.data.experimental.sample_from_datasets(\n        [dataset, dataset_demos],\n        [1 - demonstration_ratio, demonstration_ratio])\n\n    # Batch and prefetch.\n    dataset = dataset.batch(batch_size, drop_remainder=True)\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n    tf2_utils.create_variables(network, [environment_spec.observations])\n    tf2_utils.create_variables(target_network, [environment_spec.observations])\n\n    learner = learning.R2D2Learner(\n        environment_spec=environment_spec,\n        network=network,\n        target_network=target_network,\n        burn_in_length=burn_in_length,\n        dataset=dataset,\n        reverb_client=reverb.TFClient(address),\n        counter=counter,\n        logger=logger,\n        sequence_length=sequence_length,\n        discount=discount,\n        target_update_period=target_update_period,\n        importance_sampling_exponent=importance_sampling_exponent,\n        max_replay_size=max_replay_size,\n        learning_rate=learning_rate,\n        store_lstm_state=False,\n    )\n\n    self._checkpointer = tf2_savers.Checkpointer(\n        subdirectory='r2d2_learner',\n        time_delta_minutes=60,\n        objects_to_save=learner.state,\n        enable_checkpointing=checkpoint,\n    )\n\n    self._snapshotter = tf2_savers.Snapshotter(\n        objects_to_save={'network': network}, time_delta_minutes=60.)\n\n    policy_network = snt.DeepRNN([\n        network,\n        lambda qs: trfl.epsilon_greedy(qs, epsilon=epsilon).sample(),\n    ])\n\n    actor = actors.RecurrentActor(policy_network, adder)\n    observations_per_step = (float(replay_period * batch_size) /\n                             samples_per_insert)\n    super().__init__(\n        actor=actor,\n        learner=learner,\n        min_observations=replay_period * max(batch_size, min_replay_size),\n        observations_per_step=observations_per_step)\n\n  def update(self):\n    super().update()\n    self._snapshotter.save()\n    self._checkpointer.save()",
  "def _sequence_from_episode(observations: acme_types.NestedTensor,\n                           actions: tf.Tensor,\n                           rewards: tf.Tensor,\n                           discounts: tf.Tensor,\n                           extra_spec: acme_types.NestedSpec,\n                           period: int,\n                           sequence_length: int):\n  \"\"\"Produce Reverb-like sequence from a full episode.\n\n  Observations, actions, rewards and discounts have the same length. This\n  function will ignore the first reward and discount and the last action.\n\n  This function generates fake (all-zero) extras.\n\n  See docs for reverb.SequenceAdder() for more details.\n\n  Args:\n    observations: [L, ...] Tensor.\n    actions: [L, ...] Tensor.\n    rewards: [L] Tensor.\n    discounts: [L] Tensor.\n    extra_spec: A possibly nested structure of specs for extras. This function\n      will generate fake (all-zero) extras.\n    period: The period with which we add sequences.\n    sequence_length: The fixed length of sequences we wish to add.\n\n  Returns:\n    (o_t, a_t, r_t, d_t, e_t) Tuple.\n  \"\"\"\n\n  length = tf.shape(rewards)[0]\n  first = tf.random.uniform(shape=(), minval=0, maxval=length, dtype=tf.int32)\n  first = first // period * period  # Get a multiple of `period`.\n  to = tf.minimum(first + sequence_length, length)\n\n  def _slice_and_pad(x):\n    pad_length = sequence_length + first - to\n    padding_shape = tf.concat([[pad_length], tf.shape(x)[1:]], axis=0)\n    result = tf.concat([x[first:to], tf.zeros(padding_shape, x.dtype)], axis=0)\n    result.set_shape([sequence_length] + x.shape.as_list()[1:])\n    return result\n\n  o_t = tree.map_structure(_slice_and_pad, observations)\n  a_t = tree.map_structure(_slice_and_pad, actions)\n  r_t = _slice_and_pad(rewards)\n  d_t = _slice_and_pad(discounts)\n  start_of_episode = tf.equal(first, 0)\n  start_of_episode = tf.expand_dims(start_of_episode, axis=0)\n  start_of_episode = tf.tile(start_of_episode, [sequence_length])\n\n  def _sequence_zeros(spec):\n    return tf.zeros([sequence_length] + spec.shape, spec.dtype)\n\n  e_t = tree.map_structure(_sequence_zeros, extra_spec)\n  info = tree.map_structure(lambda dtype: tf.ones([], dtype),\n                            reverb.SampleInfo.tf_dtypes())\n  return reverb.ReplaySample(\n      info=info,\n      data=adders.Step(\n          observation=o_t,\n          action=a_t,\n          reward=r_t,\n          discount=d_t,\n          start_of_episode=start_of_episode,\n          extras=e_t))",
  "def __init__(self,\n               environment_spec: specs.EnvironmentSpec,\n               network: snt.RNNCore,\n               target_network: snt.RNNCore,\n               burn_in_length: int,\n               trace_length: int,\n               replay_period: int,\n               demonstration_dataset: tf.data.Dataset,\n               demonstration_ratio: float,\n               counter: Optional[counting.Counter] = None,\n               logger: Optional[loggers.Logger] = None,\n               discount: float = 0.99,\n               batch_size: int = 32,\n               target_update_period: int = 100,\n               importance_sampling_exponent: float = 0.2,\n               epsilon: float = 0.01,\n               learning_rate: float = 1e-3,\n               save_logs: bool = False,\n               log_name: str = 'agent',\n               checkpoint: bool = True,\n               min_replay_size: int = 1000,\n               max_replay_size: int = 1000000,\n               samples_per_insert: float = 32.0):\n\n    sequence_length = burn_in_length + trace_length + 1\n    extra_spec = {\n        'core_state': network.initial_state(1),\n    }\n    # Remove batch dimensions.\n    extra_spec = tf2_utils.squeeze_batch_dim(extra_spec)\n    replay_table = reverb.Table(\n        name=adders.DEFAULT_PRIORITY_TABLE,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=max_replay_size,\n        rate_limiter=reverb.rate_limiters.MinSize(min_size_to_sample=1),\n        signature=adders.SequenceAdder.signature(\n            environment_spec, extra_spec, sequence_length=sequence_length))\n    self._server = reverb.Server([replay_table], port=None)\n    address = f'localhost:{self._server.port}'\n\n    # Component to add things into replay.\n    sequence_kwargs = dict(\n        period=replay_period,\n        sequence_length=sequence_length,\n    )\n    adder = adders.SequenceAdder(client=reverb.Client(address),\n                                 **sequence_kwargs)\n\n    # The dataset object to learn from.\n    dataset = datasets.make_reverb_dataset(\n        server_address=address)\n\n    # Combine with demonstration dataset.\n    transition = functools.partial(_sequence_from_episode,\n                                   extra_spec=extra_spec,\n                                   **sequence_kwargs)\n    dataset_demos = demonstration_dataset.map(transition)\n    dataset = tf.data.experimental.sample_from_datasets(\n        [dataset, dataset_demos],\n        [1 - demonstration_ratio, demonstration_ratio])\n\n    # Batch and prefetch.\n    dataset = dataset.batch(batch_size, drop_remainder=True)\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n    tf2_utils.create_variables(network, [environment_spec.observations])\n    tf2_utils.create_variables(target_network, [environment_spec.observations])\n\n    learner = learning.R2D2Learner(\n        environment_spec=environment_spec,\n        network=network,\n        target_network=target_network,\n        burn_in_length=burn_in_length,\n        dataset=dataset,\n        reverb_client=reverb.TFClient(address),\n        counter=counter,\n        logger=logger,\n        sequence_length=sequence_length,\n        discount=discount,\n        target_update_period=target_update_period,\n        importance_sampling_exponent=importance_sampling_exponent,\n        max_replay_size=max_replay_size,\n        learning_rate=learning_rate,\n        store_lstm_state=False,\n    )\n\n    self._checkpointer = tf2_savers.Checkpointer(\n        subdirectory='r2d2_learner',\n        time_delta_minutes=60,\n        objects_to_save=learner.state,\n        enable_checkpointing=checkpoint,\n    )\n\n    self._snapshotter = tf2_savers.Snapshotter(\n        objects_to_save={'network': network}, time_delta_minutes=60.)\n\n    policy_network = snt.DeepRNN([\n        network,\n        lambda qs: trfl.epsilon_greedy(qs, epsilon=epsilon).sample(),\n    ])\n\n    actor = actors.RecurrentActor(policy_network, adder)\n    observations_per_step = (float(replay_period * batch_size) /\n                             samples_per_insert)\n    super().__init__(\n        actor=actor,\n        learner=learner,\n        min_observations=replay_period * max(batch_size, min_replay_size),\n        observations_per_step=observations_per_step)",
  "def update(self):\n    super().update()\n    self._snapshotter.save()\n    self._checkpointer.save()",
  "def _slice_and_pad(x):\n    pad_length = sequence_length + first - to\n    padding_shape = tf.concat([[pad_length], tf.shape(x)[1:]], axis=0)\n    result = tf.concat([x[first:to], tf.zeros(padding_shape, x.dtype)], axis=0)\n    result.set_shape([sequence_length] + x.shape.as_list()[1:])\n    return result",
  "def _sequence_zeros(spec):\n    return tf.zeros([sequence_length] + spec.shape, spec.dtype)",
  "class SimpleNetwork(networks.RNNCore):\n\n  def __init__(self, action_spec: specs.DiscreteArray):\n    super().__init__(name='r2d2_test_network')\n    self._net = snt.DeepRNN([\n        snt.Flatten(),\n        snt.LSTM(20),\n        snt.nets.MLP([50, 50, action_spec.num_values])\n    ])\n\n  def __call__(self, inputs, state):\n    return self._net(inputs, state)\n\n  def initial_state(self, batch_size: int, **kwargs):\n    return self._net.initial_state(batch_size)\n\n  def unroll(self, inputs, state, sequence_length):\n    return snt.static_unroll(self._net, inputs, state, sequence_length)",
  "class R2D3Test(absltest.TestCase):\n\n  def test_r2d3(self):\n    # Create a fake environment to test with.\n    environment = fakes.DiscreteEnvironment(\n        num_actions=5,\n        num_observations=10,\n        obs_dtype=np.float32,\n        episode_length=10)\n    spec = specs.make_environment_spec(environment)\n\n    # Build demonstrations.\n    dummy_action = np.zeros((), dtype=np.int32)\n    recorder = bsuite_demonstrations.DemonstrationRecorder()\n    timestep = environment.reset()\n    while timestep.step_type is not dm_env.StepType.LAST:\n      recorder.step(timestep, dummy_action)\n      timestep = environment.step(dummy_action)\n    recorder.step(timestep, dummy_action)\n    recorder.record_episode()\n\n    # Construct the agent.\n    agent = r2d3.R2D3(\n        environment_spec=spec,\n        network=SimpleNetwork(spec.actions),\n        target_network=SimpleNetwork(spec.actions),\n        demonstration_dataset=recorder.make_tf_dataset(),\n        demonstration_ratio=0.5,\n        batch_size=10,\n        samples_per_insert=2,\n        min_replay_size=10,\n        burn_in_length=2,\n        trace_length=6,\n        replay_period=4,\n        checkpoint=False,\n    )\n\n    # Try running the environment loop. We have no assertions here because all\n    # we care about is that the agent runs without raising any errors.\n    loop = acme.EnvironmentLoop(environment, agent)\n    loop.run(num_episodes=5)",
  "def __init__(self, action_spec: specs.DiscreteArray):\n    super().__init__(name='r2d2_test_network')\n    self._net = snt.DeepRNN([\n        snt.Flatten(),\n        snt.LSTM(20),\n        snt.nets.MLP([50, 50, action_spec.num_values])\n    ])",
  "def __call__(self, inputs, state):\n    return self._net(inputs, state)",
  "def initial_state(self, batch_size: int, **kwargs):\n    return self._net.initial_state(batch_size)",
  "def unroll(self, inputs, state, sequence_length):\n    return snt.static_unroll(self._net, inputs, state, sequence_length)",
  "def test_r2d3(self):\n    # Create a fake environment to test with.\n    environment = fakes.DiscreteEnvironment(\n        num_actions=5,\n        num_observations=10,\n        obs_dtype=np.float32,\n        episode_length=10)\n    spec = specs.make_environment_spec(environment)\n\n    # Build demonstrations.\n    dummy_action = np.zeros((), dtype=np.int32)\n    recorder = bsuite_demonstrations.DemonstrationRecorder()\n    timestep = environment.reset()\n    while timestep.step_type is not dm_env.StepType.LAST:\n      recorder.step(timestep, dummy_action)\n      timestep = environment.step(dummy_action)\n    recorder.step(timestep, dummy_action)\n    recorder.record_episode()\n\n    # Construct the agent.\n    agent = r2d3.R2D3(\n        environment_spec=spec,\n        network=SimpleNetwork(spec.actions),\n        target_network=SimpleNetwork(spec.actions),\n        demonstration_dataset=recorder.make_tf_dataset(),\n        demonstration_ratio=0.5,\n        batch_size=10,\n        samples_per_insert=2,\n        min_replay_size=10,\n        burn_in_length=2,\n        trace_length=6,\n        replay_period=4,\n        checkpoint=False,\n    )\n\n    # Try running the environment loop. We have no assertions here because all\n    # we care about is that the agent runs without raising any errors.\n    loop = acme.EnvironmentLoop(environment, agent)\n    loop.run(num_episodes=5)",
  "class IQNLearner(core.Learner, tf2_savers.TFSaveable):\n  \"\"\"Distributional DQN learner.\"\"\"\n\n  def __init__(\n      self,\n      network: networks.IQNNetwork,\n      target_network: snt.Module,\n      discount: float,\n      importance_sampling_exponent: float,\n      learning_rate: float,\n      target_update_period: int,\n      dataset: tf.data.Dataset,\n      huber_loss_parameter: float = 1.,\n      replay_client: Optional[reverb.TFClient] = None,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = True,\n    ):\n    \"\"\"Initializes the learner.\n\n    Args:\n      network: the online Q network (the one being optimized) that outputs\n        (q_values, q_logits, atoms).\n      target_network: the target Q critic (which lags behind the online net).\n      discount: discount to use for TD updates.\n      importance_sampling_exponent: power to which importance weights are raised\n        before normalizing.\n      learning_rate: learning rate for the q-network update.\n      target_update_period: number of learner steps to perform before updating\n        the target networks.\n      dataset: dataset to learn from, whether fixed or from a replay buffer\n        (see `acme.datasets.reverb.make_reverb_dataset` documentation).\n      huber_loss_parameter: Quadratic-linear boundary for Huber loss.\n      replay_client: client to replay to allow for updating priorities.\n      counter: Counter object for (potentially distributed) counting.\n      logger: Logger object for writing logs to.\n      checkpoint: boolean indicating whether to checkpoint the learner or not.\n    \"\"\"\n\n    # Internalise agent components (replay buffer, networks, optimizer).\n    self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n    self._network = network\n    self._target_network = target_network\n    self._optimizer = snt.optimizers.Adam(learning_rate)\n    self._replay_client = replay_client\n\n    # Internalise the hyperparameters.\n    self._discount = discount\n    self._target_update_period = target_update_period\n    self._importance_sampling_exponent = importance_sampling_exponent\n    self._huber_loss_parameter = huber_loss_parameter\n\n    # Learner state.\n    self._variables: List[List[tf.Tensor]] = [network.trainable_variables]\n    self._num_steps = tf.Variable(0, dtype=tf.int32)\n\n    # Internalise logging/counting objects.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.TerminalLogger('learner', time_delta=1.)\n\n    # Create a snapshotter object.\n    if checkpoint:\n      self._checkpointer = tf2_savers.Checkpointer(\n          time_delta_minutes=5,\n          objects_to_save={\n              'network': self._network,\n              'target_network': self._target_network,\n              'optimizer': self._optimizer,\n              'num_steps': self._num_steps\n          })\n      self._snapshotter = tf2_savers.Snapshotter(\n          objects_to_save={'network': network}, time_delta_minutes=60.)\n    else:\n      self._checkpointer = None\n      self._snapshotter = None\n\n  @tf.function\n  def _step(self) -> Dict[str, tf.Tensor]:\n    \"\"\"Do a step of SGD and update the priorities.\"\"\"\n\n    # Pull out the data needed for updates/priorities.\n    inputs = next(self._iterator)\n    transitions: types.Transition = inputs.data\n    keys, probs, *_ = inputs.info\n\n    with tf.GradientTape() as tape:\n      loss, fetches = self._loss_and_fetches(transitions.observation,\n                                             transitions.action,\n                                             transitions.reward,\n                                             transitions.discount,\n                                             transitions.next_observation)\n\n      # Get the importance weights.\n      importance_weights = 1. / probs  # [B]\n      importance_weights **= self._importance_sampling_exponent\n      importance_weights /= tf.reduce_max(importance_weights)\n\n      # Reweight.\n      loss *= tf.cast(importance_weights, loss.dtype)  # [B]\n      loss = tf.reduce_mean(loss, axis=[0])  # []\n\n    # Do a step of SGD.\n    gradients = tape.gradient(loss, self._network.trainable_variables)\n    self._optimizer.apply(gradients, self._network.trainable_variables)\n\n    # Update the priorities in the replay buffer.\n    if self._replay_client:\n      priorities = tf.clip_by_value(tf.abs(loss), -100, 100)\n      priorities = tf.cast(priorities, tf.float64)\n      self._replay_client.update_priorities(\n          table=adders.DEFAULT_PRIORITY_TABLE, keys=keys, priorities=priorities)\n\n    # Periodically update the target network.\n    if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n      for src, dest in zip(self._network.variables,\n                           self._target_network.variables):\n        dest.assign(src)\n    self._num_steps.assign_add(1)\n\n    # Report gradient norms.\n    fetches.update(\n        loss=loss,\n        gradient_norm=tf.linalg.global_norm(gradients))\n    return fetches\n\n  def step(self):\n    # Do a batch of SGD.\n    result = self._step()\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1)\n    result.update(counts)\n\n    # Checkpoint and attempt to write logs.\n    if self._checkpointer is not None:\n      self._checkpointer.save()\n    if self._snapshotter is not None:\n      self._snapshotter.save()\n    self._logger.write(result)\n\n  def get_variables(self, names: List[str]) -> List[np.ndarray]:\n    return tf2_utils.to_numpy(self._variables)\n\n  def _loss_and_fetches(\n      self,\n      o_tm1: tf.Tensor,\n      a_tm1: tf.Tensor,\n      r_t: tf.Tensor,\n      d_t: tf.Tensor,\n      o_t: tf.Tensor,\n  ) -> Tuple[tf.Tensor, Dict[str, tf.Tensor]]:\n    # Evaluate our networks.\n    _, dist_tm1, tau = self._network(o_tm1)\n    q_tm1 = _index_embs_with_actions(dist_tm1, a_tm1)\n\n    q_selector, _, _ = self._target_network(o_t)\n    a_t = tf.argmax(q_selector, axis=1)\n\n    _, dist_t, _ = self._target_network(o_t)\n    q_t = _index_embs_with_actions(dist_t, a_t)\n\n    q_tm1 = losses.QuantileDistribution(values=q_tm1,\n                                        logits=tf.zeros_like(q_tm1))\n    q_t = losses.QuantileDistribution(values=q_t, logits=tf.zeros_like(q_t))\n\n    # The rewards and discounts have to have the same type as network values.\n    r_t = tf.cast(r_t, tf.float32)\n    r_t = tf.clip_by_value(r_t, -1., 1.)\n    d_t = tf.cast(d_t, tf.float32) * tf.cast(self._discount, tf.float32)\n\n    # Compute the loss.\n    loss_module = losses.NonUniformQuantileRegression(\n        self._huber_loss_parameter)\n    loss = loss_module(q_tm1, r_t, d_t, q_t, tau)\n\n    # Compute statistics of the Q-values for logging.\n    max_q = tf.reduce_max(q_t.values)\n    min_q = tf.reduce_min(q_t.values)\n    mean_q, var_q = tf.nn.moments(q_t.values, [0, 1])\n    fetches = {\n        'max_q': max_q,\n        'mean_q': mean_q,\n        'min_q': min_q,\n        'var_q': var_q,\n    }\n\n    return loss, fetches\n\n  @property\n  def state(self):\n    \"\"\"Returns the stateful parts of the learner for checkpointing.\"\"\"\n    return {\n        'network': self._network,\n        'target_network': self._target_network,\n        'optimizer': self._optimizer,\n        'num_steps': self._num_steps\n    }",
  "def _index_embs_with_actions(\n    embeddings: tf.Tensor,\n    actions: tf.Tensor,\n) -> tf.Tensor:\n  \"\"\"Slice an embedding Tensor with action indices.\n\n  Take embeddings of the form [batch_size, num_actions, embed_dim]\n  and actions of the form [batch_size], and return the sliced embeddings\n  like embeddings[:, actions, :]. Doing this my way because the comments in\n  the official op are scary.\n\n  Args:\n    embeddings: Tensor of embeddings to index.\n    actions: int Tensor to use as index into embeddings\n\n  Returns:\n    Tensor of embeddings indexed by actions\n  \"\"\"\n  batch_size, num_actions, _ = embeddings.shape.as_list()\n\n  # Values are the 'values' in a sparse tensor we will be setting\n  act_indx = tf.cast(actions, tf.int64)[:, None]\n  values = tf.ones([tf.size(actions)], dtype=tf.bool)\n\n  # Create a range for each index into the batch\n  act_range = tf.range(0, batch_size, dtype=tf.int64)[:, None]\n  # Combine this into coordinates with the action indices\n  indices = tf.concat([act_range, act_indx], 1)\n\n  actions_mask = tf.SparseTensor(indices, values, [batch_size, num_actions])\n  actions_mask = tf.stop_gradient(\n      tf.sparse.to_dense(actions_mask, default_value=False))\n  sliced_emb = tf.boolean_mask(embeddings, actions_mask)\n  return sliced_emb",
  "def __init__(\n      self,\n      network: networks.IQNNetwork,\n      target_network: snt.Module,\n      discount: float,\n      importance_sampling_exponent: float,\n      learning_rate: float,\n      target_update_period: int,\n      dataset: tf.data.Dataset,\n      huber_loss_parameter: float = 1.,\n      replay_client: Optional[reverb.TFClient] = None,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      checkpoint: bool = True,\n    ):\n    \"\"\"Initializes the learner.\n\n    Args:\n      network: the online Q network (the one being optimized) that outputs\n        (q_values, q_logits, atoms).\n      target_network: the target Q critic (which lags behind the online net).\n      discount: discount to use for TD updates.\n      importance_sampling_exponent: power to which importance weights are raised\n        before normalizing.\n      learning_rate: learning rate for the q-network update.\n      target_update_period: number of learner steps to perform before updating\n        the target networks.\n      dataset: dataset to learn from, whether fixed or from a replay buffer\n        (see `acme.datasets.reverb.make_reverb_dataset` documentation).\n      huber_loss_parameter: Quadratic-linear boundary for Huber loss.\n      replay_client: client to replay to allow for updating priorities.\n      counter: Counter object for (potentially distributed) counting.\n      logger: Logger object for writing logs to.\n      checkpoint: boolean indicating whether to checkpoint the learner or not.\n    \"\"\"\n\n    # Internalise agent components (replay buffer, networks, optimizer).\n    self._iterator = iter(dataset)  # pytype: disable=wrong-arg-types\n    self._network = network\n    self._target_network = target_network\n    self._optimizer = snt.optimizers.Adam(learning_rate)\n    self._replay_client = replay_client\n\n    # Internalise the hyperparameters.\n    self._discount = discount\n    self._target_update_period = target_update_period\n    self._importance_sampling_exponent = importance_sampling_exponent\n    self._huber_loss_parameter = huber_loss_parameter\n\n    # Learner state.\n    self._variables: List[List[tf.Tensor]] = [network.trainable_variables]\n    self._num_steps = tf.Variable(0, dtype=tf.int32)\n\n    # Internalise logging/counting objects.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.TerminalLogger('learner', time_delta=1.)\n\n    # Create a snapshotter object.\n    if checkpoint:\n      self._checkpointer = tf2_savers.Checkpointer(\n          time_delta_minutes=5,\n          objects_to_save={\n              'network': self._network,\n              'target_network': self._target_network,\n              'optimizer': self._optimizer,\n              'num_steps': self._num_steps\n          })\n      self._snapshotter = tf2_savers.Snapshotter(\n          objects_to_save={'network': network}, time_delta_minutes=60.)\n    else:\n      self._checkpointer = None\n      self._snapshotter = None",
  "def _step(self) -> Dict[str, tf.Tensor]:\n    \"\"\"Do a step of SGD and update the priorities.\"\"\"\n\n    # Pull out the data needed for updates/priorities.\n    inputs = next(self._iterator)\n    transitions: types.Transition = inputs.data\n    keys, probs, *_ = inputs.info\n\n    with tf.GradientTape() as tape:\n      loss, fetches = self._loss_and_fetches(transitions.observation,\n                                             transitions.action,\n                                             transitions.reward,\n                                             transitions.discount,\n                                             transitions.next_observation)\n\n      # Get the importance weights.\n      importance_weights = 1. / probs  # [B]\n      importance_weights **= self._importance_sampling_exponent\n      importance_weights /= tf.reduce_max(importance_weights)\n\n      # Reweight.\n      loss *= tf.cast(importance_weights, loss.dtype)  # [B]\n      loss = tf.reduce_mean(loss, axis=[0])  # []\n\n    # Do a step of SGD.\n    gradients = tape.gradient(loss, self._network.trainable_variables)\n    self._optimizer.apply(gradients, self._network.trainable_variables)\n\n    # Update the priorities in the replay buffer.\n    if self._replay_client:\n      priorities = tf.clip_by_value(tf.abs(loss), -100, 100)\n      priorities = tf.cast(priorities, tf.float64)\n      self._replay_client.update_priorities(\n          table=adders.DEFAULT_PRIORITY_TABLE, keys=keys, priorities=priorities)\n\n    # Periodically update the target network.\n    if tf.math.mod(self._num_steps, self._target_update_period) == 0:\n      for src, dest in zip(self._network.variables,\n                           self._target_network.variables):\n        dest.assign(src)\n    self._num_steps.assign_add(1)\n\n    # Report gradient norms.\n    fetches.update(\n        loss=loss,\n        gradient_norm=tf.linalg.global_norm(gradients))\n    return fetches",
  "def step(self):\n    # Do a batch of SGD.\n    result = self._step()\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1)\n    result.update(counts)\n\n    # Checkpoint and attempt to write logs.\n    if self._checkpointer is not None:\n      self._checkpointer.save()\n    if self._snapshotter is not None:\n      self._snapshotter.save()\n    self._logger.write(result)",
  "def get_variables(self, names: List[str]) -> List[np.ndarray]:\n    return tf2_utils.to_numpy(self._variables)",
  "def _loss_and_fetches(\n      self,\n      o_tm1: tf.Tensor,\n      a_tm1: tf.Tensor,\n      r_t: tf.Tensor,\n      d_t: tf.Tensor,\n      o_t: tf.Tensor,\n  ) -> Tuple[tf.Tensor, Dict[str, tf.Tensor]]:\n    # Evaluate our networks.\n    _, dist_tm1, tau = self._network(o_tm1)\n    q_tm1 = _index_embs_with_actions(dist_tm1, a_tm1)\n\n    q_selector, _, _ = self._target_network(o_t)\n    a_t = tf.argmax(q_selector, axis=1)\n\n    _, dist_t, _ = self._target_network(o_t)\n    q_t = _index_embs_with_actions(dist_t, a_t)\n\n    q_tm1 = losses.QuantileDistribution(values=q_tm1,\n                                        logits=tf.zeros_like(q_tm1))\n    q_t = losses.QuantileDistribution(values=q_t, logits=tf.zeros_like(q_t))\n\n    # The rewards and discounts have to have the same type as network values.\n    r_t = tf.cast(r_t, tf.float32)\n    r_t = tf.clip_by_value(r_t, -1., 1.)\n    d_t = tf.cast(d_t, tf.float32) * tf.cast(self._discount, tf.float32)\n\n    # Compute the loss.\n    loss_module = losses.NonUniformQuantileRegression(\n        self._huber_loss_parameter)\n    loss = loss_module(q_tm1, r_t, d_t, q_t, tau)\n\n    # Compute statistics of the Q-values for logging.\n    max_q = tf.reduce_max(q_t.values)\n    min_q = tf.reduce_min(q_t.values)\n    mean_q, var_q = tf.nn.moments(q_t.values, [0, 1])\n    fetches = {\n        'max_q': max_q,\n        'mean_q': mean_q,\n        'min_q': min_q,\n        'var_q': var_q,\n    }\n\n    return loss, fetches",
  "def state(self):\n    \"\"\"Returns the stateful parts of the learner for checkpointing.\"\"\"\n    return {\n        'network': self._network,\n        'target_network': self._target_network,\n        'optimizer': self._optimizer,\n        'num_steps': self._num_steps\n    }",
  "def _make_torso_network(num_outputs: int) -> snt.Module:\n  \"\"\"Create torso network (outputs intermediate representation).\"\"\"\n  return snt.Sequential([\n      snt.Flatten(),\n      snt.nets.MLP([num_outputs])\n  ])",
  "def _make_head_network(num_outputs: int) -> snt.Module:\n  \"\"\"Create head network (outputs Q-values).\"\"\"\n  return snt.nets.MLP([num_outputs])",
  "class IQNLearnerTest(absltest.TestCase):\n\n  def test_full_learner(self):\n    # Create dataset.\n    environment = fakes.DiscreteEnvironment(\n        num_actions=5,\n        num_observations=10,\n        obs_dtype=np.float32,\n        episode_length=10)\n    spec = specs.make_environment_spec(environment)\n    dataset = fakes.transition_dataset(environment).batch(\n        2, drop_remainder=True)\n\n    # Build network.\n    network = networks.IQNNetwork(\n        torso=_make_torso_network(num_outputs=2),\n        head=_make_head_network(num_outputs=spec.actions.num_values),\n        latent_dim=2,\n        num_quantile_samples=1)\n    tf2_utils.create_variables(network, [spec.observations])\n\n    # Build learner.\n    counter = counting.Counter()\n    learner = iqn.IQNLearner(\n        network=network,\n        target_network=copy.deepcopy(network),\n        dataset=dataset,\n        learning_rate=1e-4,\n        discount=0.99,\n        importance_sampling_exponent=0.2,\n        target_update_period=1,\n        counter=counter)\n\n    # Run a learner step.\n    learner.step()\n\n    # Check counts from IQN learner.\n    counts = counter.get_counts()\n    self.assertEqual(1, counts['steps'])\n\n    # Check learner state.\n    self.assertEqual(1, learner.state['num_steps'].numpy())",
  "def test_full_learner(self):\n    # Create dataset.\n    environment = fakes.DiscreteEnvironment(\n        num_actions=5,\n        num_observations=10,\n        obs_dtype=np.float32,\n        episode_length=10)\n    spec = specs.make_environment_spec(environment)\n    dataset = fakes.transition_dataset(environment).batch(\n        2, drop_remainder=True)\n\n    # Build network.\n    network = networks.IQNNetwork(\n        torso=_make_torso_network(num_outputs=2),\n        head=_make_head_network(num_outputs=spec.actions.num_values),\n        latent_dim=2,\n        num_quantile_samples=1)\n    tf2_utils.create_variables(network, [spec.observations])\n\n    # Build learner.\n    counter = counting.Counter()\n    learner = iqn.IQNLearner(\n        network=network,\n        target_network=copy.deepcopy(network),\n        dataset=dataset,\n        learning_rate=1e-4,\n        discount=0.99,\n        importance_sampling_exponent=0.2,\n        target_update_period=1,\n        counter=counter)\n\n    # Run a learner step.\n    learner.step()\n\n    # Check counts from IQN learner.\n    counts = counter.get_counts()\n    self.assertEqual(1, counts['steps'])\n\n    # Check learner state.\n    self.assertEqual(1, learner.state['num_steps'].numpy())",
  "class GenericActor(core.Actor, Generic[actor_core.State, actor_core.Extras]):\n  \"\"\"A generic actor implemented on top of ActorCore.\n\n  An actor based on a policy which takes observations and outputs actions. It\n  also adds experiences to replay and updates the actor weights from the policy\n  on the learner.\n  \"\"\"\n\n  def __init__(\n      self,\n      actor: actor_core.ActorCore[actor_core.State, actor_core.Extras],\n      random_key: network_lib.PRNGKey,\n      variable_client: Optional[variable_utils.VariableClient],\n      adder: Optional[adders.Adder] = None,\n      jit: bool = True,\n      backend: Optional[str] = 'cpu',\n      per_episode_update: bool = False\n  ):\n    \"\"\"Initializes a feed forward actor.\n\n    Args:\n      actor: actor core.\n      random_key: Random key.\n      variable_client: The variable client to get policy parameters from.\n      adder: An adder to add experiences to.\n      jit: Whether or not to jit the passed ActorCore's pure functions.\n      backend: Which backend to use when jitting the policy.\n      per_episode_update: if True, updates variable client params once at the\n        beginning of each episode\n    \"\"\"\n    self._random_key = random_key\n    self._variable_client = variable_client\n    self._adder = adder\n    self._state = None\n\n    # Unpack ActorCore, jitting if requested.\n    if jit:\n      self._init = jax.jit(actor.init, backend=backend)\n      self._policy = jax.jit(actor.select_action, backend=backend)\n    else:\n      self._init = actor.init\n      self._policy = actor.select_action\n    self._get_extras = actor.get_extras\n    self._per_episode_update = per_episode_update\n\n  @property\n  def _params(self):\n    return self._variable_client.params if self._variable_client else []\n\n  def select_action(self,\n                    observation: network_lib.Observation) -> types.NestedArray:\n    action, self._state = self._policy(self._params, observation, self._state)\n    return utils.to_numpy(action)\n\n  def observe_first(self, timestep: dm_env.TimeStep):\n    self._random_key, key = jax.random.split(self._random_key)\n    self._state = self._init(key)\n    if self._adder:\n      self._adder.add_first(timestep)\n    if self._variable_client and self._per_episode_update:\n      self._variable_client.update_and_wait()\n\n  def observe(self, action: network_lib.Action, next_timestep: dm_env.TimeStep):\n    if self._adder:\n      self._adder.add(\n          action, next_timestep, extras=self._get_extras(self._state))\n\n  def update(self, wait: bool = False):\n    if self._variable_client and not self._per_episode_update:\n      self._variable_client.update(wait)",
  "def __init__(\n      self,\n      actor: actor_core.ActorCore[actor_core.State, actor_core.Extras],\n      random_key: network_lib.PRNGKey,\n      variable_client: Optional[variable_utils.VariableClient],\n      adder: Optional[adders.Adder] = None,\n      jit: bool = True,\n      backend: Optional[str] = 'cpu',\n      per_episode_update: bool = False\n  ):\n    \"\"\"Initializes a feed forward actor.\n\n    Args:\n      actor: actor core.\n      random_key: Random key.\n      variable_client: The variable client to get policy parameters from.\n      adder: An adder to add experiences to.\n      jit: Whether or not to jit the passed ActorCore's pure functions.\n      backend: Which backend to use when jitting the policy.\n      per_episode_update: if True, updates variable client params once at the\n        beginning of each episode\n    \"\"\"\n    self._random_key = random_key\n    self._variable_client = variable_client\n    self._adder = adder\n    self._state = None\n\n    # Unpack ActorCore, jitting if requested.\n    if jit:\n      self._init = jax.jit(actor.init, backend=backend)\n      self._policy = jax.jit(actor.select_action, backend=backend)\n    else:\n      self._init = actor.init\n      self._policy = actor.select_action\n    self._get_extras = actor.get_extras\n    self._per_episode_update = per_episode_update",
  "def _params(self):\n    return self._variable_client.params if self._variable_client else []",
  "def select_action(self,\n                    observation: network_lib.Observation) -> types.NestedArray:\n    action, self._state = self._policy(self._params, observation, self._state)\n    return utils.to_numpy(action)",
  "def observe_first(self, timestep: dm_env.TimeStep):\n    self._random_key, key = jax.random.split(self._random_key)\n    self._state = self._init(key)\n    if self._adder:\n      self._adder.add_first(timestep)\n    if self._variable_client and self._per_episode_update:\n      self._variable_client.update_and_wait()",
  "def observe(self, action: network_lib.Action, next_timestep: dm_env.TimeStep):\n    if self._adder:\n      self._adder.add(\n          action, next_timestep, extras=self._get_extras(self._state))",
  "def update(self, wait: bool = False):\n    if self._variable_client and not self._per_episode_update:\n      self._variable_client.update(wait)",
  "class OfflineBuilder(abc.ABC, Generic[Networks, Policy, Sample]):\n  \"\"\"Interface for defining the components of an offline RL agent.\n\n  Implementations of this interface contain a complete specification of a\n  concrete offline RL agent. An instance of this class can be used to build an\n  offline RL agent that operates either locally or in a distributed setup.\n  \"\"\"\n\n  @abc.abstractmethod\n  def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: Networks,\n      dataset: Iterator[Sample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      *,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    \"\"\"Creates an instance of the learner.\n\n    Args:\n      random_key: A key for random number generation.\n      networks: struct describing the networks needed by the learner; this is\n        specific to the learner in question.\n      dataset: iterator over demonstration samples.\n      logger_fn: factory providing loggers used for logging progress.\n      environment_spec: A container for all relevant environment specs.\n      counter: a Counter which allows for recording of counts (learner steps,\n        evaluator steps, etc.) distributed throughout the agent.\n    \"\"\"\n\n  @abc.abstractmethod\n  def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: Policy,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n  ) -> core.Actor:\n    \"\"\"Create an actor instance to be used for evaluation.\n\n    Args:\n      random_key: A key for random number generation.\n      policy: Instance of a policy expected by the algorithm corresponding to\n        this builder.\n      environment_spec: A container for all relevant environment specs.\n      variable_source: A source providing the necessary actor parameters.\n    \"\"\"\n\n  @abc.abstractmethod\n  def make_policy(self, networks: Networks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool) -> Policy:\n    \"\"\"Creates the agent policy to be used for evaluation.\n\n    Args:\n      networks: struct describing the networks needed to generate the policy.\n      environment_spec: struct describing the specs of the environment.\n      evaluation: This flag is present for consistency with the\n        ActorLearnerBuilder, in which case data-generating actors and evaluation\n        actors can behave differently. For OfflineBuilders, this should be set\n        to True.\n\n    Returns:\n      Policy to be used for evaluation. The exact form of this object may differ\n      from one agent to the next; it could be a simple callable, a nest of\n      callables, or an ActorCore for instance.\n    \"\"\"",
  "class ActorLearnerBuilder(OfflineBuilder[Networks, Policy, Sample],\n                          Generic[Networks, Policy, Sample]):\n  \"\"\"Defines an interface for defining the components of an RL agent.\n\n  Implementations of this interface contain a complete specification of a\n  concrete RL agent. An instance of this class can be used to build an\n  RL agent which interacts with the environment either locally or in a\n  distributed setup.\n  \"\"\"\n\n  @abc.abstractmethod\n  def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: Policy,\n  ) -> List[reverb.Table]:\n    \"\"\"Create tables to insert data into.\n\n    Args:\n      environment_spec: A container for all relevant environment specs.\n      policy: Agent's policy which can be used to extract the extras_spec.\n\n    Returns:\n      The replay tables used to store the experience the agent uses to train.\n    \"\"\"\n\n  @abc.abstractmethod\n  def make_dataset_iterator(\n      self,\n      replay_client: reverb.Client,\n  ) -> Iterator[Sample]:\n    \"\"\"Create a dataset iterator to use for learning/updating the agent.\"\"\"\n\n  @abc.abstractmethod\n  def make_adder(\n      self,\n      replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[Policy],\n  ) -> Optional[adders.Adder]:\n    \"\"\"Create an adder which records data generated by the actor/environment.\n\n    Args:\n      replay_client: Reverb Client which points to the replay server.\n      environment_spec: specs of the environment.\n      policy: Agent's policy which can be used to extract the extras_spec.\n    \"\"\"\n    # TODO(sabela): make the parameters non-optional.\n\n  @abc.abstractmethod\n  def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: Policy,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> core.Actor:\n    \"\"\"Create an actor instance.\n\n    Args:\n      random_key: A key for random number generation.\n      policy: Instance of a policy expected by the algorithm corresponding to\n        this builder.\n      environment_spec: A container for all relevant environment specs.\n      variable_source: A source providing the necessary actor parameters.\n      adder: How data is recorded (e.g. added to replay).\n    \"\"\"\n\n  @abc.abstractmethod\n  def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: Networks,\n      dataset: Iterator[Sample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    \"\"\"Creates an instance of the learner.\n\n    Args:\n      random_key: A key for random number generation.\n      networks: struct describing the networks needed by the learner; this can\n        be specific to the learner in question.\n      dataset: iterator over samples from replay.\n      logger_fn: factory providing loggers used for logging progress.\n      environment_spec: A container for all relevant environment specs.\n      replay_client: client which allows communication with replay. Note that\n        this is only intended to be used for updating priorities. Samples should\n        be obtained from `dataset`.\n      counter: a Counter which allows for recording of counts (learner steps,\n        actor steps, etc.) distributed throughout the agent.\n    \"\"\"\n\n  def make_policy(self,\n                  networks: Networks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> Policy:\n    \"\"\"Creates the agent policy.\n\n       Creates the agent policy given the collection of network components and\n       environment spec. An optional boolean can be given to indicate if the\n       policy will be used for evaluation.\n\n    Args:\n      networks: struct describing the networks needed to generate the policy.\n      environment_spec: struct describing the specs of the environment.\n      evaluation: when true, a version of the policy to use for evaluation\n        should be returned. This is algorithm-specific so if an algorithm makes\n        no distinction between behavior and evaluation policies this boolean may\n        be ignored.\n\n    Returns:\n      Behavior policy or evaluation policy for the agent.\n    \"\"\"\n    # TODO(sabela): make abstract once all agents implement it.\n    del networks, environment_spec, evaluation\n    raise NotImplementedError",
  "class ActorLearnerBuilderWrapper(ActorLearnerBuilder[Networks, Policy, Sample],\n                                 Generic[Networks, Policy, Sample]):\n  \"\"\"An empty wrapper for ActorLearnerBuilder.\"\"\"\n\n  wrapped: ActorLearnerBuilder[Networks, Policy, Sample]\n\n  def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: Policy,\n  ) -> List[reverb.Table]:\n    return self.wrapped.make_replay_tables(environment_spec, policy)\n\n  def make_dataset_iterator(\n      self,\n      replay_client: reverb.Client,\n  ) -> Iterator[Sample]:\n    return self.wrapped.make_dataset_iterator(replay_client)\n\n  def make_adder(\n      self,\n      replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[Policy],\n  ) -> Optional[adders.Adder]:\n    return self.wrapped.make_adder(replay_client, environment_spec, policy)\n\n  def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: Policy,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> core.Actor:\n    return self.wrapped.make_actor(random_key, policy, environment_spec,\n                                   variable_source, adder)\n\n  def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: Networks,\n      dataset: Iterator[Sample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    return self.wrapped.make_learner(random_key, networks, dataset, logger_fn,\n                                     environment_spec, replay_client, counter)\n\n  def make_policy(self,\n                  networks: Networks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> Policy:\n    return self.wrapped.make_policy(networks, environment_spec, evaluation)",
  "def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: Networks,\n      dataset: Iterator[Sample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      *,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    \"\"\"Creates an instance of the learner.\n\n    Args:\n      random_key: A key for random number generation.\n      networks: struct describing the networks needed by the learner; this is\n        specific to the learner in question.\n      dataset: iterator over demonstration samples.\n      logger_fn: factory providing loggers used for logging progress.\n      environment_spec: A container for all relevant environment specs.\n      counter: a Counter which allows for recording of counts (learner steps,\n        evaluator steps, etc.) distributed throughout the agent.\n    \"\"\"",
  "def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: Policy,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n  ) -> core.Actor:\n    \"\"\"Create an actor instance to be used for evaluation.\n\n    Args:\n      random_key: A key for random number generation.\n      policy: Instance of a policy expected by the algorithm corresponding to\n        this builder.\n      environment_spec: A container for all relevant environment specs.\n      variable_source: A source providing the necessary actor parameters.\n    \"\"\"",
  "def make_policy(self, networks: Networks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool) -> Policy:\n    \"\"\"Creates the agent policy to be used for evaluation.\n\n    Args:\n      networks: struct describing the networks needed to generate the policy.\n      environment_spec: struct describing the specs of the environment.\n      evaluation: This flag is present for consistency with the\n        ActorLearnerBuilder, in which case data-generating actors and evaluation\n        actors can behave differently. For OfflineBuilders, this should be set\n        to True.\n\n    Returns:\n      Policy to be used for evaluation. The exact form of this object may differ\n      from one agent to the next; it could be a simple callable, a nest of\n      callables, or an ActorCore for instance.\n    \"\"\"",
  "def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: Policy,\n  ) -> List[reverb.Table]:\n    \"\"\"Create tables to insert data into.\n\n    Args:\n      environment_spec: A container for all relevant environment specs.\n      policy: Agent's policy which can be used to extract the extras_spec.\n\n    Returns:\n      The replay tables used to store the experience the agent uses to train.\n    \"\"\"",
  "def make_dataset_iterator(\n      self,\n      replay_client: reverb.Client,\n  ) -> Iterator[Sample]:\n    \"\"\"Create a dataset iterator to use for learning/updating the agent.\"\"\"",
  "def make_adder(\n      self,\n      replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[Policy],\n  ) -> Optional[adders.Adder]:\n    \"\"\"Create an adder which records data generated by the actor/environment.\n\n    Args:\n      replay_client: Reverb Client which points to the replay server.\n      environment_spec: specs of the environment.\n      policy: Agent's policy which can be used to extract the extras_spec.\n    \"\"\"",
  "def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: Policy,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> core.Actor:\n    \"\"\"Create an actor instance.\n\n    Args:\n      random_key: A key for random number generation.\n      policy: Instance of a policy expected by the algorithm corresponding to\n        this builder.\n      environment_spec: A container for all relevant environment specs.\n      variable_source: A source providing the necessary actor parameters.\n      adder: How data is recorded (e.g. added to replay).\n    \"\"\"",
  "def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: Networks,\n      dataset: Iterator[Sample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    \"\"\"Creates an instance of the learner.\n\n    Args:\n      random_key: A key for random number generation.\n      networks: struct describing the networks needed by the learner; this can\n        be specific to the learner in question.\n      dataset: iterator over samples from replay.\n      logger_fn: factory providing loggers used for logging progress.\n      environment_spec: A container for all relevant environment specs.\n      replay_client: client which allows communication with replay. Note that\n        this is only intended to be used for updating priorities. Samples should\n        be obtained from `dataset`.\n      counter: a Counter which allows for recording of counts (learner steps,\n        actor steps, etc.) distributed throughout the agent.\n    \"\"\"",
  "def make_policy(self,\n                  networks: Networks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> Policy:\n    \"\"\"Creates the agent policy.\n\n       Creates the agent policy given the collection of network components and\n       environment spec. An optional boolean can be given to indicate if the\n       policy will be used for evaluation.\n\n    Args:\n      networks: struct describing the networks needed to generate the policy.\n      environment_spec: struct describing the specs of the environment.\n      evaluation: when true, a version of the policy to use for evaluation\n        should be returned. This is algorithm-specific so if an algorithm makes\n        no distinction between behavior and evaluation policies this boolean may\n        be ignored.\n\n    Returns:\n      Behavior policy or evaluation policy for the agent.\n    \"\"\"\n    # TODO(sabela): make abstract once all agents implement it.\n    del networks, environment_spec, evaluation\n    raise NotImplementedError",
  "def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: Policy,\n  ) -> List[reverb.Table]:\n    return self.wrapped.make_replay_tables(environment_spec, policy)",
  "def make_dataset_iterator(\n      self,\n      replay_client: reverb.Client,\n  ) -> Iterator[Sample]:\n    return self.wrapped.make_dataset_iterator(replay_client)",
  "def make_adder(\n      self,\n      replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[Policy],\n  ) -> Optional[adders.Adder]:\n    return self.wrapped.make_adder(replay_client, environment_spec, policy)",
  "def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: Policy,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> core.Actor:\n    return self.wrapped.make_actor(random_key, policy, environment_spec,\n                                   variable_source, adder)",
  "def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: Networks,\n      dataset: Iterator[Sample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    return self.wrapped.make_learner(random_key, networks, dataset, logger_fn,\n                                     environment_spec, replay_client, counter)",
  "def make_policy(self,\n                  networks: Networks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> Policy:\n    return self.wrapped.make_policy(networks, environment_spec, evaluation)",
  "def _make_fake_env() -> dm_env.Environment:\n  env_spec = specs.EnvironmentSpec(\n      observations=specs.Array(shape=(10, 5), dtype=np.float32),\n      actions=specs.DiscreteArray(num_values=3),\n      rewards=specs.Array(shape=(), dtype=np.float32),\n      discounts=specs.BoundedArray(\n          shape=(), dtype=np.float32, minimum=0., maximum=1.),\n  )\n  return fakes.Environment(env_spec, episode_length=10)",
  "class ActorTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      ('policy', False),\n      ('policy_with_extras', True))\n  def test_feedforward(self, has_extras):\n    environment = _make_fake_env()\n    env_spec = specs.make_environment_spec(environment)\n\n    def policy(inputs: jnp.ndarray):\n      action_values = hk.Sequential([\n          hk.Flatten(),\n          hk.Linear(env_spec.actions.num_values),\n      ])(\n          inputs)\n      action = jnp.argmax(action_values, axis=-1)\n      if has_extras:\n        return action, (action_values,)\n      else:\n        return action\n\n    policy = hk.transform(policy)\n\n    rng = hk.PRNGSequence(1)\n    dummy_obs = utils.add_batch_dim(utils.zeros_like(env_spec.observations))\n    params = policy.init(next(rng), dummy_obs)\n\n    variable_source = fakes.VariableSource(params)\n    variable_client = variable_utils.VariableClient(variable_source, 'policy')\n\n    if has_extras:\n      actor_core = actor_core_lib.batched_feed_forward_with_extras_to_actor_core(\n          policy.apply)\n    else:\n      actor_core = actor_core_lib.batched_feed_forward_to_actor_core(\n          policy.apply)\n    actor = actors.GenericActor(\n        actor_core,\n        random_key=jax.random.PRNGKey(1),\n        variable_client=variable_client)\n\n    loop = environment_loop.EnvironmentLoop(environment, actor)\n    loop.run(20)",
  "def _transform_without_rng(f):\n  return hk.without_apply_rng(hk.transform(f))",
  "class RecurrentActorTest(absltest.TestCase):\n\n  def test_recurrent(self):\n    environment = _make_fake_env()\n    env_spec = specs.make_environment_spec(environment)\n    output_size = env_spec.actions.num_values\n    obs = utils.add_batch_dim(utils.zeros_like(env_spec.observations))\n    rng = hk.PRNGSequence(1)\n\n    @_transform_without_rng\n    def network(inputs: jnp.ndarray, state: hk.LSTMState):\n      return hk.DeepRNN([hk.Reshape([-1], preserve_dims=1),\n                         hk.LSTM(output_size)])(inputs, state)\n\n    @_transform_without_rng\n    def initial_state(batch_size: Optional[int] = None):\n      network = hk.DeepRNN([hk.Reshape([-1], preserve_dims=1),\n                            hk.LSTM(output_size)])\n      return network.initial_state(batch_size)\n\n    initial_state = initial_state.apply(initial_state.init(next(rng)), 1)\n    params = network.init(next(rng), obs, initial_state)\n\n    def policy(\n        params: jnp.ndarray,\n        key: jnp.ndarray,\n        observation: jnp.ndarray,\n        core_state: hk.LSTMState\n    ) -> Tuple[jnp.ndarray, hk.LSTMState]:\n      del key  # Unused for test-case deterministic policy.\n      action_values, core_state = network.apply(params, observation, core_state)\n      actions = jnp.argmax(action_values, axis=-1)\n      return actions, core_state\n\n    variable_source = fakes.VariableSource(params)\n    variable_client = variable_utils.VariableClient(variable_source, 'policy')\n\n    actor_core = actor_core_lib.batched_recurrent_to_actor_core(\n        policy, initial_state)\n    actor = actors.GenericActor(actor_core, jax.random.PRNGKey(1),\n                                variable_client)\n\n    loop = environment_loop.EnvironmentLoop(environment, actor)\n    loop.run(20)",
  "def test_feedforward(self, has_extras):\n    environment = _make_fake_env()\n    env_spec = specs.make_environment_spec(environment)\n\n    def policy(inputs: jnp.ndarray):\n      action_values = hk.Sequential([\n          hk.Flatten(),\n          hk.Linear(env_spec.actions.num_values),\n      ])(\n          inputs)\n      action = jnp.argmax(action_values, axis=-1)\n      if has_extras:\n        return action, (action_values,)\n      else:\n        return action\n\n    policy = hk.transform(policy)\n\n    rng = hk.PRNGSequence(1)\n    dummy_obs = utils.add_batch_dim(utils.zeros_like(env_spec.observations))\n    params = policy.init(next(rng), dummy_obs)\n\n    variable_source = fakes.VariableSource(params)\n    variable_client = variable_utils.VariableClient(variable_source, 'policy')\n\n    if has_extras:\n      actor_core = actor_core_lib.batched_feed_forward_with_extras_to_actor_core(\n          policy.apply)\n    else:\n      actor_core = actor_core_lib.batched_feed_forward_to_actor_core(\n          policy.apply)\n    actor = actors.GenericActor(\n        actor_core,\n        random_key=jax.random.PRNGKey(1),\n        variable_client=variable_client)\n\n    loop = environment_loop.EnvironmentLoop(environment, actor)\n    loop.run(20)",
  "def test_recurrent(self):\n    environment = _make_fake_env()\n    env_spec = specs.make_environment_spec(environment)\n    output_size = env_spec.actions.num_values\n    obs = utils.add_batch_dim(utils.zeros_like(env_spec.observations))\n    rng = hk.PRNGSequence(1)\n\n    @_transform_without_rng\n    def network(inputs: jnp.ndarray, state: hk.LSTMState):\n      return hk.DeepRNN([hk.Reshape([-1], preserve_dims=1),\n                         hk.LSTM(output_size)])(inputs, state)\n\n    @_transform_without_rng\n    def initial_state(batch_size: Optional[int] = None):\n      network = hk.DeepRNN([hk.Reshape([-1], preserve_dims=1),\n                            hk.LSTM(output_size)])\n      return network.initial_state(batch_size)\n\n    initial_state = initial_state.apply(initial_state.init(next(rng)), 1)\n    params = network.init(next(rng), obs, initial_state)\n\n    def policy(\n        params: jnp.ndarray,\n        key: jnp.ndarray,\n        observation: jnp.ndarray,\n        core_state: hk.LSTMState\n    ) -> Tuple[jnp.ndarray, hk.LSTMState]:\n      del key  # Unused for test-case deterministic policy.\n      action_values, core_state = network.apply(params, observation, core_state)\n      actions = jnp.argmax(action_values, axis=-1)\n      return actions, core_state\n\n    variable_source = fakes.VariableSource(params)\n    variable_client = variable_utils.VariableClient(variable_source, 'policy')\n\n    actor_core = actor_core_lib.batched_recurrent_to_actor_core(\n        policy, initial_state)\n    actor = actors.GenericActor(actor_core, jax.random.PRNGKey(1),\n                                variable_client)\n\n    loop = environment_loop.EnvironmentLoop(environment, actor)\n    loop.run(20)",
  "def policy(inputs: jnp.ndarray):\n      action_values = hk.Sequential([\n          hk.Flatten(),\n          hk.Linear(env_spec.actions.num_values),\n      ])(\n          inputs)\n      action = jnp.argmax(action_values, axis=-1)\n      if has_extras:\n        return action, (action_values,)\n      else:\n        return action",
  "def network(inputs: jnp.ndarray, state: hk.LSTMState):\n      return hk.DeepRNN([hk.Reshape([-1], preserve_dims=1),\n                         hk.LSTM(output_size)])(inputs, state)",
  "def initial_state(batch_size: Optional[int] = None):\n      network = hk.DeepRNN([hk.Reshape([-1], preserve_dims=1),\n                            hk.LSTM(output_size)])\n      return network.initial_state(batch_size)",
  "def policy(\n        params: jnp.ndarray,\n        key: jnp.ndarray,\n        observation: jnp.ndarray,\n        core_state: hk.LSTMState\n    ) -> Tuple[jnp.ndarray, hk.LSTMState]:\n      del key  # Unused for test-case deterministic policy.\n      action_values, core_state = network.apply(params, observation, core_state)\n      actions = jnp.argmax(action_values, axis=-1)\n      return actions, core_state",
  "class ActorCore(Generic[State, Extras]):\n  \"\"\"Pure functions that define the algorithm-specific actor functionality.\"\"\"\n  init: Callable[[PRNGKey], State]\n  select_action: SelectActionFn\n  get_extras: Callable[[State], Extras]",
  "def batched_feed_forward_to_actor_core(\n    policy: FeedForwardPolicy) -> ActorCore[PRNGKey, Tuple[()]]:\n  \"\"\"A convenience adaptor from FeedForwardPolicy to ActorCore.\"\"\"\n\n  def select_action(params: networks_lib.Params,\n                    observation: networks_lib.Observation,\n                    state: PRNGKey):\n    rng = state\n    rng1, rng2 = jax.random.split(rng)\n    observation = utils.add_batch_dim(observation)\n    action = utils.squeeze_batch_dim(policy(params, rng1, observation))\n    return action, rng2\n\n  def init(rng: PRNGKey) -> PRNGKey:\n    return rng\n\n  def get_extras(unused_rng: PRNGKey) -> Tuple[()]:\n    return ()\n  return ActorCore(init=init, select_action=select_action,\n                   get_extras=get_extras)",
  "class SimpleActorCoreStateWithExtras:\n  rng: PRNGKey\n  extras: Mapping[str, jnp.ndarray]",
  "def unvectorize_select_action(actor_core: ActorCore) -> ActorCore:\n  \"\"\"Makes an actor core's select_action method expect unbatched arguments.\"\"\"\n\n  def unvectorized_select_action(\n      params: networks_lib.Params,\n      observations: networks_lib.Observation,\n      state: State,\n  ) -> Tuple[networks_lib.Action, State]:\n    observations, state = utils.add_batch_dim((observations, state))\n    actions, state = actor_core.select_action(params, observations, state)\n    return utils.squeeze_batch_dim((actions, state))\n\n  return ActorCore(\n      init=actor_core.init,\n      select_action=unvectorized_select_action,\n      get_extras=actor_core.get_extras)",
  "def batched_feed_forward_with_extras_to_actor_core(\n    policy: FeedForwardPolicyWithExtra\n) -> ActorCore[SimpleActorCoreStateWithExtras, Mapping[str, jnp.ndarray]]:\n  \"\"\"A convenience adaptor from FeedForwardPolicy to ActorCore.\"\"\"\n\n  def select_action(params: networks_lib.Params,\n                    observation: networks_lib.Observation,\n                    state: SimpleActorCoreStateWithExtras):\n    rng = state.rng\n    rng1, rng2 = jax.random.split(rng)\n    observation = utils.add_batch_dim(observation)\n    action, extras = utils.squeeze_batch_dim(policy(params, rng1, observation))\n    return action, SimpleActorCoreStateWithExtras(rng2, extras)\n\n  def init(rng: PRNGKey) -> SimpleActorCoreStateWithExtras:\n    return SimpleActorCoreStateWithExtras(rng, {})\n\n  def get_extras(\n      state: SimpleActorCoreStateWithExtras) -> Mapping[str, jnp.ndarray]:\n    return state.extras\n  return ActorCore(init=init, select_action=select_action,\n                   get_extras=get_extras)",
  "class SimpleActorCoreRecurrentState(Generic[RecurrentState]):\n  rng: PRNGKey\n  recurrent_state: RecurrentState",
  "def batched_recurrent_to_actor_core(\n    recurrent_policy: RecurrentPolicy, initial_core_state: RecurrentState\n) -> ActorCore[SimpleActorCoreRecurrentState[RecurrentState], Mapping[\n    str, jnp.ndarray]]:\n  \"\"\"Returns ActorCore for a recurrent policy.\"\"\"\n  def select_action(params: networks_lib.Params,\n                    observation: networks_lib.Observation,\n                    state: SimpleActorCoreRecurrentState[RecurrentState]):\n    # TODO(b/161332815): Make JAX Actor work with batched or unbatched inputs.\n    rng = state.rng\n    rng, policy_rng = jax.random.split(rng)\n    observation = utils.add_batch_dim(observation)\n    recurrent_state = utils.add_batch_dim(state.recurrent_state)\n    action, new_recurrent_state = utils.squeeze_batch_dim(recurrent_policy(\n        params, policy_rng, observation, recurrent_state))\n    return action, SimpleActorCoreRecurrentState(rng, new_recurrent_state)\n\n  initial_core_state = utils.squeeze_batch_dim(initial_core_state)\n  def init(rng: PRNGKey) -> SimpleActorCoreRecurrentState[RecurrentState]:\n    return SimpleActorCoreRecurrentState(rng, initial_core_state)\n\n  def get_extras(\n      state: SimpleActorCoreRecurrentState[RecurrentState]\n  ) -> Mapping[str, jnp.ndarray]:\n    return {'core_state': state.recurrent_state}\n\n  return ActorCore(init=init, select_action=select_action,\n                   get_extras=get_extras)",
  "def select_action(params: networks_lib.Params,\n                    observation: networks_lib.Observation,\n                    state: PRNGKey):\n    rng = state\n    rng1, rng2 = jax.random.split(rng)\n    observation = utils.add_batch_dim(observation)\n    action = utils.squeeze_batch_dim(policy(params, rng1, observation))\n    return action, rng2",
  "def init(rng: PRNGKey) -> PRNGKey:\n    return rng",
  "def get_extras(unused_rng: PRNGKey) -> Tuple[()]:\n    return ()",
  "def unvectorized_select_action(\n      params: networks_lib.Params,\n      observations: networks_lib.Observation,\n      state: State,\n  ) -> Tuple[networks_lib.Action, State]:\n    observations, state = utils.add_batch_dim((observations, state))\n    actions, state = actor_core.select_action(params, observations, state)\n    return utils.squeeze_batch_dim((actions, state))",
  "def select_action(params: networks_lib.Params,\n                    observation: networks_lib.Observation,\n                    state: SimpleActorCoreStateWithExtras):\n    rng = state.rng\n    rng1, rng2 = jax.random.split(rng)\n    observation = utils.add_batch_dim(observation)\n    action, extras = utils.squeeze_batch_dim(policy(params, rng1, observation))\n    return action, SimpleActorCoreStateWithExtras(rng2, extras)",
  "def init(rng: PRNGKey) -> SimpleActorCoreStateWithExtras:\n    return SimpleActorCoreStateWithExtras(rng, {})",
  "def get_extras(\n      state: SimpleActorCoreStateWithExtras) -> Mapping[str, jnp.ndarray]:\n    return state.extras",
  "def select_action(params: networks_lib.Params,\n                    observation: networks_lib.Observation,\n                    state: SimpleActorCoreRecurrentState[RecurrentState]):\n    # TODO(b/161332815): Make JAX Actor work with batched or unbatched inputs.\n    rng = state.rng\n    rng, policy_rng = jax.random.split(rng)\n    observation = utils.add_batch_dim(observation)\n    recurrent_state = utils.add_batch_dim(state.recurrent_state)\n    action, new_recurrent_state = utils.squeeze_batch_dim(recurrent_policy(\n        params, policy_rng, observation, recurrent_state))\n    return action, SimpleActorCoreRecurrentState(rng, new_recurrent_state)",
  "def init(rng: PRNGKey) -> SimpleActorCoreRecurrentState[RecurrentState]:\n    return SimpleActorCoreRecurrentState(rng, initial_core_state)",
  "def get_extras(\n      state: SimpleActorCoreRecurrentState[RecurrentState]\n  ) -> Mapping[str, jnp.ndarray]:\n    return {'core_state': state.recurrent_state}",
  "class NormalizationActorWrapper(core.Actor):\n  \"\"\"An actor wrapper that normalizes observations before applying policy.\"\"\"\n\n  def __init__(self,\n               wrapped_actor: core.Actor,\n               variable_source: core.VariableSource,\n               max_abs_observation: Optional[float],\n               update_period: int = 1,\n               backend: Optional[str] = None):\n    self._wrapped_actor = wrapped_actor\n    self._variable_client = variable_utils.VariableClient(\n        variable_source,\n        key=_NORMALIZATION_VARIABLES,\n        update_period=update_period,\n        device=backend)\n    self._apply_normalization = jax.jit(\n        functools.partial(\n            running_statistics.normalize, max_abs_value=max_abs_observation),\n        backend=backend)\n\n  def select_action(self, observation: types.NestedArray) -> types.NestedArray:\n    self._variable_client.update()\n    observation_stats = self._variable_client.params\n    observation = self._apply_normalization(observation, observation_stats)\n    return self._wrapped_actor.select_action(observation)\n\n  def observe_first(self, timestep: dm_env.TimeStep):\n    return self._wrapped_actor.observe_first(timestep)\n\n  def observe(\n      self,\n      action: types.NestedArray,\n      next_timestep: dm_env.TimeStep,\n  ):\n    return self._wrapped_actor.observe(action, next_timestep)\n\n  def update(self, wait: bool = False):\n    return self._wrapped_actor.update(wait)",
  "class NormalizationLearnerWrapperState:\n  wrapped_learner_state: Any\n  observation_running_statistics: running_statistics.RunningStatisticsState",
  "class NormalizationLearnerWrapper(core.Learner, core.Saveable):\n  \"\"\"A learner wrapper that normalizes observations using running statistics.\"\"\"\n\n  def __init__(self, learner_factory: Callable[[Iterator[reverb.ReplaySample]],\n                                               acme.Learner],\n               iterator: Iterator[reverb.ReplaySample],\n               environment_spec: specs.EnvironmentSpec,\n               max_abs_observation: Optional[float]):\n\n    def normalize_sample(\n        observation_statistics: running_statistics.RunningStatisticsState,\n        sample: reverb.ReplaySample\n    ) -> Tuple[running_statistics.RunningStatisticsState, reverb.ReplaySample]:\n      observation = sample.data.observation\n      observation_statistics = running_statistics.update(\n          observation_statistics, observation)\n      observation = running_statistics.normalize(\n          observation,\n          observation_statistics,\n          max_abs_value=max_abs_observation)\n      sample = reverb.ReplaySample(\n          sample.info, sample.data._replace(observation=observation))\n      if hasattr(sample.data, 'next_observation'):\n        next_observation = running_statistics.normalize(\n            sample.data.next_observation,\n            observation_statistics,\n            max_abs_value=max_abs_observation)\n        sample = reverb.ReplaySample(\n            sample.info,\n            sample.data._replace(next_observation=next_observation))\n\n      return observation_statistics, sample\n\n    self._observation_running_statistics = running_statistics.init_state(\n        environment_spec.observations)\n    self._normalize_sample = jax.jit(normalize_sample)\n\n    normalizing_iterator = (\n        self._normalize_sample_and_update(sample) for sample in iterator)\n    self._wrapped_learner = learner_factory(normalizing_iterator)\n\n  def _normalize_sample_and_update(\n      self, sample: reverb.ReplaySample) -> reverb.ReplaySample:\n    self._observation_running_statistics, sample = self._normalize_sample(\n        self._observation_running_statistics, sample)\n    return sample\n\n  def step(self):\n    self._wrapped_learner.step()\n\n  def get_variables(self, names: List[str]) -> List[types.NestedArray]:\n    stats = self._observation_running_statistics\n    # Make sure to only pass mean and std to minimize trafic.\n    mean_std = running_statistics.NestedMeanStd(mean=stats.mean, std=stats.std)\n    normalization_variables = {_NORMALIZATION_VARIABLES: mean_std}\n\n    learner_names = [\n        name for name in names if name not in normalization_variables\n    ]\n    learner_variables = dict(\n        zip(learner_names, self._wrapped_learner.get_variables(\n            learner_names))) if learner_names else {}\n\n    return [\n        normalization_variables.get(name, learner_variables.get(name, None))\n        for name in names\n    ]\n\n  def save(self) -> NormalizationLearnerWrapperState:\n    return NormalizationLearnerWrapperState(\n        wrapped_learner_state=self._wrapped_learner.save(),\n        observation_running_statistics=self._observation_running_statistics)\n\n  def restore(self, state: NormalizationLearnerWrapperState):\n    self._wrapped_learner.restore(state.wrapped_learner_state)\n    self._observation_running_statistics = state.observation_running_statistics",
  "class NormalizationBuilder(Generic[Networks, Policy],\n                           builders.ActorLearnerBuilder[Networks, Policy,\n                                                        reverb.ReplaySample]):\n  \"\"\"Builder wrapper that normalizes observations using running mean/std.\"\"\"\n  builder: builders.ActorLearnerBuilder[Networks, Policy, reverb.ReplaySample]\n  max_abs_observation: Optional[float] = 10.0\n  statistics_update_period: int = 100\n\n  def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: Policy,\n  ) -> List[reverb.Table]:\n    return self.builder.make_replay_tables(environment_spec, policy)\n\n  def make_dataset_iterator(\n      self, replay_client: reverb.Client) -> Iterator[reverb.ReplaySample]:\n    return self.builder.make_dataset_iterator(replay_client)\n\n  def make_adder(self, replay_client: reverb.Client,\n                 environment_spec: Optional[specs.EnvironmentSpec],\n                 policy: Optional[Policy]) -> Optional[adders.Adder]:\n    return self.builder.make_adder(replay_client, environment_spec, policy)\n\n  def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: Networks,\n      dataset: Iterator[reverb.ReplaySample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n\n    learner_factory = functools.partial(\n        self.builder.make_learner,\n        random_key,\n        networks,\n        logger_fn=logger_fn,\n        environment_spec=environment_spec,\n        replay_client=replay_client,\n        counter=counter)\n\n    return NormalizationLearnerWrapper(\n        learner_factory=learner_factory,\n        iterator=dataset,\n        environment_spec=environment_spec,\n        max_abs_observation=self.max_abs_observation)\n\n  def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: Policy,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> core.Actor:\n    actor = self.builder.make_actor(random_key, policy, environment_spec,\n                                    variable_source, adder)\n    return NormalizationActorWrapper(\n        actor,\n        variable_source,\n        max_abs_observation=self.max_abs_observation,\n        update_period=self.statistics_update_period,\n        backend='cpu')\n\n  def make_policy(self,\n                  networks: Networks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> Policy:\n    return self.builder.make_policy(\n        networks=networks,\n        environment_spec=environment_spec,\n        evaluation=evaluation)",
  "class NormalizationConfig:\n  \"\"\"Configuration for normalization based on running statistics.\n\n  Attributes:\n    max_abs: Maximum value for clipping.\n    statistics_update_period: How often to update running statistics used for\n      normalization.\n  \"\"\"\n  max_abs: int = 10\n  statistics_update_period: int = 100",
  "class InputNormalizerConfig(Protocol):\n  \"\"\"Protocol for the config of the agent that uses the normalization decorator.\n\n  If the agent builder is decorated with the `input_normalization_builder`\n  the agent config class must implement this protocol.\n  \"\"\"\n\n  @property\n  def input_normalization(self) -> Optional[NormalizationConfig]:\n    ...",
  "def input_normalization_builder(\n    actor_learner_builder_class: Callable[[InputNormalizerConfig],\n                                          builders.ActorLearnerBuilder]):\n  \"\"\"Builder class decorator that adds support for input normalization.\"\"\"\n\n  # TODO(b/247075349): find a way to use ActorLearnerBuilderWrapper here.\n  class InputNormalizationBuilder(\n      Generic[builders.Networks, builders.Policy, builders.Sample],\n      builders.ActorLearnerBuilder[builders.Networks, builders.Policy,\n                                   builders.Sample]):\n    \"\"\"Builder wrapper that adds input normalization based on the config.\"\"\"\n\n    def __init__(self, config: InputNormalizerConfig):\n      builder = actor_learner_builder_class(config)\n      if config.input_normalization:\n        builder = NormalizationBuilder(\n            builder,\n            max_abs_observation=config.input_normalization.max_abs,\n            statistics_update_period=config.input_normalization\n            .statistics_update_period)\n      self.wrapped = builder\n\n    def make_replay_tables(\n        self,\n        environment_spec: specs.EnvironmentSpec,\n        policy: builders.Policy,\n    ) -> List[reverb.Table]:\n      return self.wrapped.make_replay_tables(environment_spec, policy)\n\n    def make_dataset_iterator(\n        self,\n        replay_client: reverb.Client,\n    ) -> Iterator[builders.Sample]:\n      return self.wrapped.make_dataset_iterator(replay_client)\n\n    def make_adder(\n        self,\n        replay_client: reverb.Client,\n        environment_spec: Optional[specs.EnvironmentSpec],\n        policy: Optional[builders.Policy],\n    ) -> Optional[adders.Adder]:\n      return self.wrapped.make_adder(replay_client, environment_spec, policy)\n\n    def make_actor(\n        self,\n        random_key: networks_lib.PRNGKey,\n        policy: builders.Policy,\n        environment_spec: specs.EnvironmentSpec,\n        variable_source: Optional[core.VariableSource] = None,\n        adder: Optional[adders.Adder] = None,\n    ) -> core.Actor:\n      return self.wrapped.make_actor(random_key, policy, environment_spec,\n                                     variable_source, adder)\n\n    def make_learner(\n        self,\n        random_key: networks_lib.PRNGKey,\n        networks: Networks,\n        dataset: Iterator[builders.Sample],\n        logger_fn: loggers.LoggerFactory,\n        environment_spec: specs.EnvironmentSpec,\n        replay_client: Optional[reverb.Client] = None,\n        counter: Optional[counting.Counter] = None,\n    ) -> core.Learner:\n      return self.wrapped.make_learner(random_key, networks, dataset, logger_fn,\n                                       environment_spec, replay_client, counter)\n\n    def make_policy(self,\n                    networks: builders.Networks,\n                    environment_spec: specs.EnvironmentSpec,\n                    evaluation: bool = False) -> builders.Policy:\n      return self.wrapped.make_policy(networks, environment_spec, evaluation)\n\n  return InputNormalizationBuilder",
  "def __init__(self,\n               wrapped_actor: core.Actor,\n               variable_source: core.VariableSource,\n               max_abs_observation: Optional[float],\n               update_period: int = 1,\n               backend: Optional[str] = None):\n    self._wrapped_actor = wrapped_actor\n    self._variable_client = variable_utils.VariableClient(\n        variable_source,\n        key=_NORMALIZATION_VARIABLES,\n        update_period=update_period,\n        device=backend)\n    self._apply_normalization = jax.jit(\n        functools.partial(\n            running_statistics.normalize, max_abs_value=max_abs_observation),\n        backend=backend)",
  "def select_action(self, observation: types.NestedArray) -> types.NestedArray:\n    self._variable_client.update()\n    observation_stats = self._variable_client.params\n    observation = self._apply_normalization(observation, observation_stats)\n    return self._wrapped_actor.select_action(observation)",
  "def observe_first(self, timestep: dm_env.TimeStep):\n    return self._wrapped_actor.observe_first(timestep)",
  "def observe(\n      self,\n      action: types.NestedArray,\n      next_timestep: dm_env.TimeStep,\n  ):\n    return self._wrapped_actor.observe(action, next_timestep)",
  "def update(self, wait: bool = False):\n    return self._wrapped_actor.update(wait)",
  "def __init__(self, learner_factory: Callable[[Iterator[reverb.ReplaySample]],\n                                               acme.Learner],\n               iterator: Iterator[reverb.ReplaySample],\n               environment_spec: specs.EnvironmentSpec,\n               max_abs_observation: Optional[float]):\n\n    def normalize_sample(\n        observation_statistics: running_statistics.RunningStatisticsState,\n        sample: reverb.ReplaySample\n    ) -> Tuple[running_statistics.RunningStatisticsState, reverb.ReplaySample]:\n      observation = sample.data.observation\n      observation_statistics = running_statistics.update(\n          observation_statistics, observation)\n      observation = running_statistics.normalize(\n          observation,\n          observation_statistics,\n          max_abs_value=max_abs_observation)\n      sample = reverb.ReplaySample(\n          sample.info, sample.data._replace(observation=observation))\n      if hasattr(sample.data, 'next_observation'):\n        next_observation = running_statistics.normalize(\n            sample.data.next_observation,\n            observation_statistics,\n            max_abs_value=max_abs_observation)\n        sample = reverb.ReplaySample(\n            sample.info,\n            sample.data._replace(next_observation=next_observation))\n\n      return observation_statistics, sample\n\n    self._observation_running_statistics = running_statistics.init_state(\n        environment_spec.observations)\n    self._normalize_sample = jax.jit(normalize_sample)\n\n    normalizing_iterator = (\n        self._normalize_sample_and_update(sample) for sample in iterator)\n    self._wrapped_learner = learner_factory(normalizing_iterator)",
  "def _normalize_sample_and_update(\n      self, sample: reverb.ReplaySample) -> reverb.ReplaySample:\n    self._observation_running_statistics, sample = self._normalize_sample(\n        self._observation_running_statistics, sample)\n    return sample",
  "def step(self):\n    self._wrapped_learner.step()",
  "def get_variables(self, names: List[str]) -> List[types.NestedArray]:\n    stats = self._observation_running_statistics\n    # Make sure to only pass mean and std to minimize trafic.\n    mean_std = running_statistics.NestedMeanStd(mean=stats.mean, std=stats.std)\n    normalization_variables = {_NORMALIZATION_VARIABLES: mean_std}\n\n    learner_names = [\n        name for name in names if name not in normalization_variables\n    ]\n    learner_variables = dict(\n        zip(learner_names, self._wrapped_learner.get_variables(\n            learner_names))) if learner_names else {}\n\n    return [\n        normalization_variables.get(name, learner_variables.get(name, None))\n        for name in names\n    ]",
  "def save(self) -> NormalizationLearnerWrapperState:\n    return NormalizationLearnerWrapperState(\n        wrapped_learner_state=self._wrapped_learner.save(),\n        observation_running_statistics=self._observation_running_statistics)",
  "def restore(self, state: NormalizationLearnerWrapperState):\n    self._wrapped_learner.restore(state.wrapped_learner_state)\n    self._observation_running_statistics = state.observation_running_statistics",
  "def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: Policy,\n  ) -> List[reverb.Table]:\n    return self.builder.make_replay_tables(environment_spec, policy)",
  "def make_dataset_iterator(\n      self, replay_client: reverb.Client) -> Iterator[reverb.ReplaySample]:\n    return self.builder.make_dataset_iterator(replay_client)",
  "def make_adder(self, replay_client: reverb.Client,\n                 environment_spec: Optional[specs.EnvironmentSpec],\n                 policy: Optional[Policy]) -> Optional[adders.Adder]:\n    return self.builder.make_adder(replay_client, environment_spec, policy)",
  "def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: Networks,\n      dataset: Iterator[reverb.ReplaySample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n\n    learner_factory = functools.partial(\n        self.builder.make_learner,\n        random_key,\n        networks,\n        logger_fn=logger_fn,\n        environment_spec=environment_spec,\n        replay_client=replay_client,\n        counter=counter)\n\n    return NormalizationLearnerWrapper(\n        learner_factory=learner_factory,\n        iterator=dataset,\n        environment_spec=environment_spec,\n        max_abs_observation=self.max_abs_observation)",
  "def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: Policy,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> core.Actor:\n    actor = self.builder.make_actor(random_key, policy, environment_spec,\n                                    variable_source, adder)\n    return NormalizationActorWrapper(\n        actor,\n        variable_source,\n        max_abs_observation=self.max_abs_observation,\n        update_period=self.statistics_update_period,\n        backend='cpu')",
  "def make_policy(self,\n                  networks: Networks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> Policy:\n    return self.builder.make_policy(\n        networks=networks,\n        environment_spec=environment_spec,\n        evaluation=evaluation)",
  "def input_normalization(self) -> Optional[NormalizationConfig]:\n    ...",
  "class InputNormalizationBuilder(\n      Generic[builders.Networks, builders.Policy, builders.Sample],\n      builders.ActorLearnerBuilder[builders.Networks, builders.Policy,\n                                   builders.Sample]):\n    \"\"\"Builder wrapper that adds input normalization based on the config.\"\"\"\n\n    def __init__(self, config: InputNormalizerConfig):\n      builder = actor_learner_builder_class(config)\n      if config.input_normalization:\n        builder = NormalizationBuilder(\n            builder,\n            max_abs_observation=config.input_normalization.max_abs,\n            statistics_update_period=config.input_normalization\n            .statistics_update_period)\n      self.wrapped = builder\n\n    def make_replay_tables(\n        self,\n        environment_spec: specs.EnvironmentSpec,\n        policy: builders.Policy,\n    ) -> List[reverb.Table]:\n      return self.wrapped.make_replay_tables(environment_spec, policy)\n\n    def make_dataset_iterator(\n        self,\n        replay_client: reverb.Client,\n    ) -> Iterator[builders.Sample]:\n      return self.wrapped.make_dataset_iterator(replay_client)\n\n    def make_adder(\n        self,\n        replay_client: reverb.Client,\n        environment_spec: Optional[specs.EnvironmentSpec],\n        policy: Optional[builders.Policy],\n    ) -> Optional[adders.Adder]:\n      return self.wrapped.make_adder(replay_client, environment_spec, policy)\n\n    def make_actor(\n        self,\n        random_key: networks_lib.PRNGKey,\n        policy: builders.Policy,\n        environment_spec: specs.EnvironmentSpec,\n        variable_source: Optional[core.VariableSource] = None,\n        adder: Optional[adders.Adder] = None,\n    ) -> core.Actor:\n      return self.wrapped.make_actor(random_key, policy, environment_spec,\n                                     variable_source, adder)\n\n    def make_learner(\n        self,\n        random_key: networks_lib.PRNGKey,\n        networks: Networks,\n        dataset: Iterator[builders.Sample],\n        logger_fn: loggers.LoggerFactory,\n        environment_spec: specs.EnvironmentSpec,\n        replay_client: Optional[reverb.Client] = None,\n        counter: Optional[counting.Counter] = None,\n    ) -> core.Learner:\n      return self.wrapped.make_learner(random_key, networks, dataset, logger_fn,\n                                       environment_spec, replay_client, counter)\n\n    def make_policy(self,\n                    networks: builders.Networks,\n                    environment_spec: specs.EnvironmentSpec,\n                    evaluation: bool = False) -> builders.Policy:\n      return self.wrapped.make_policy(networks, environment_spec, evaluation)",
  "def normalize_sample(\n        observation_statistics: running_statistics.RunningStatisticsState,\n        sample: reverb.ReplaySample\n    ) -> Tuple[running_statistics.RunningStatisticsState, reverb.ReplaySample]:\n      observation = sample.data.observation\n      observation_statistics = running_statistics.update(\n          observation_statistics, observation)\n      observation = running_statistics.normalize(\n          observation,\n          observation_statistics,\n          max_abs_value=max_abs_observation)\n      sample = reverb.ReplaySample(\n          sample.info, sample.data._replace(observation=observation))\n      if hasattr(sample.data, 'next_observation'):\n        next_observation = running_statistics.normalize(\n            sample.data.next_observation,\n            observation_statistics,\n            max_abs_value=max_abs_observation)\n        sample = reverb.ReplaySample(\n            sample.info,\n            sample.data._replace(next_observation=next_observation))\n\n      return observation_statistics, sample",
  "def __init__(self, config: InputNormalizerConfig):\n      builder = actor_learner_builder_class(config)\n      if config.input_normalization:\n        builder = NormalizationBuilder(\n            builder,\n            max_abs_observation=config.input_normalization.max_abs,\n            statistics_update_period=config.input_normalization\n            .statistics_update_period)\n      self.wrapped = builder",
  "def make_replay_tables(\n        self,\n        environment_spec: specs.EnvironmentSpec,\n        policy: builders.Policy,\n    ) -> List[reverb.Table]:\n      return self.wrapped.make_replay_tables(environment_spec, policy)",
  "def make_dataset_iterator(\n        self,\n        replay_client: reverb.Client,\n    ) -> Iterator[builders.Sample]:\n      return self.wrapped.make_dataset_iterator(replay_client)",
  "def make_adder(\n        self,\n        replay_client: reverb.Client,\n        environment_spec: Optional[specs.EnvironmentSpec],\n        policy: Optional[builders.Policy],\n    ) -> Optional[adders.Adder]:\n      return self.wrapped.make_adder(replay_client, environment_spec, policy)",
  "def make_actor(\n        self,\n        random_key: networks_lib.PRNGKey,\n        policy: builders.Policy,\n        environment_spec: specs.EnvironmentSpec,\n        variable_source: Optional[core.VariableSource] = None,\n        adder: Optional[adders.Adder] = None,\n    ) -> core.Actor:\n      return self.wrapped.make_actor(random_key, policy, environment_spec,\n                                     variable_source, adder)",
  "def make_learner(\n        self,\n        random_key: networks_lib.PRNGKey,\n        networks: Networks,\n        dataset: Iterator[builders.Sample],\n        logger_fn: loggers.LoggerFactory,\n        environment_spec: specs.EnvironmentSpec,\n        replay_client: Optional[reverb.Client] = None,\n        counter: Optional[counting.Counter] = None,\n    ) -> core.Learner:\n      return self.wrapped.make_learner(random_key, networks, dataset, logger_fn,\n                                       environment_spec, replay_client, counter)",
  "def make_policy(self,\n                    networks: builders.Networks,\n                    environment_spec: specs.EnvironmentSpec,\n                    evaluation: bool = False) -> builders.Policy:\n      return self.wrapped.make_policy(networks, environment_spec, evaluation)",
  "class TrainingState(NamedTuple):\n  \"\"\"Contains training state for the learner.\"\"\"\n  policy_optimizer_state: optax.OptState\n  critic_optimizer_state: optax.OptState\n  policy_params: networks_lib.Params\n  critic_params: networks_lib.Params\n  target_critic_params: networks_lib.Params\n  key: networks_lib.PRNGKey\n\n  # Optimizer and value of the alpha parameter from SAC (entropy temperature).\n  # These fields are only used with an adaptive coefficient (when\n  # fixed_entropy_coefficeint is None in the CQLLearner)\n  alpha_optimizer_state: Optional[optax.OptState] = None\n  log_sac_alpha: Optional[networks_lib.Params] = None\n\n  # Optimizer and value of the alpha parameter from CQL (regularization\n  # coefficient).\n  # These fields are only used with an adaptive coefficient (when\n  # fixed_cql_coefficiennt is None in the CQLLearner)\n  cql_optimizer_state: Optional[optax.OptState] = None\n  log_cql_alpha: Optional[networks_lib.Params] = None\n\n  steps: int = 0",
  "class CQLLearner(acme.Learner):\n  \"\"\"CQL learner.\n\n  Learning component of the Conservative Q-Learning algorithm from\n  [Kumar et al., 2020] https://arxiv.org/abs/2006.04779.\n  \"\"\"\n\n  _state: TrainingState\n\n  def __init__(self,\n               batch_size: int,\n               networks: CQLNetworks,\n               random_key: networks_lib.PRNGKey,\n               demonstrations: Iterator[types.Transition],\n               policy_optimizer: optax.GradientTransformation,\n               critic_optimizer: optax.GradientTransformation,\n               tau: float = 0.005,\n               fixed_cql_coefficient: Optional[float] = None,\n               cql_lagrange_threshold: Optional[float] = None,\n               cql_num_samples: int = 10,\n               num_sgd_steps_per_step: int = 1,\n               reward_scale: float = 1.0,\n               discount: float = 0.99,\n               fixed_entropy_coefficient: Optional[float] = None,\n               target_entropy: Optional[float] = 0,\n               num_bc_iters: int = 50_000,\n               counter: Optional[counting.Counter] = None,\n               logger: Optional[loggers.Logger] = None):\n    \"\"\"Initializes the CQL learner.\n\n    Args:\n      batch_size: batch size.\n      networks: CQL networks.\n      random_key: a key for random number generation.\n      demonstrations: an iterator over training data.\n      policy_optimizer: the policy optimizer.\n      critic_optimizer: the Q-function optimizer.\n      tau: target smoothing coefficient.\n      fixed_cql_coefficient: the value for cql coefficient. If None, an adaptive\n        coefficient will be used.\n      cql_lagrange_threshold: a threshold that controls the adaptive loss for\n        the cql coefficient.\n      cql_num_samples: number of samples used to compute logsumexp(Q) via\n        importance sampling.\n      num_sgd_steps_per_step: how many gradient updated to perform per batch.\n        batch is split into this many smaller batches, thus should be a multiple\n        of num_sgd_steps_per_step\n      reward_scale: reward scale.\n      discount: discount to use for TD updates.\n      fixed_entropy_coefficient: coefficient applied to the entropy bonus. If\n        None, an adaptative coefficient will be used.\n      target_entropy: Target entropy when using adapdative entropy bonus.\n      num_bc_iters: Number of BC steps for actor initialization.\n      counter: counter object used to keep track of steps.\n      logger: logger object to be used by learner.\n    \"\"\"\n    self._num_bc_iters = num_bc_iters\n    adaptive_entropy_coefficient = fixed_entropy_coefficient is None\n    action_spec = networks.environment_specs.actions\n    if adaptive_entropy_coefficient:\n      # sac_alpha is the temperature parameter that determines the relative\n      # importance of the entropy term versus the reward.\n      log_sac_alpha = jnp.asarray(0., dtype=jnp.float32)\n      alpha_optimizer = optax.adam(learning_rate=3e-4)\n      alpha_optimizer_state = alpha_optimizer.init(log_sac_alpha)\n    else:\n      if target_entropy:\n        raise ValueError('target_entropy should not be set when '\n                         'fixed_entropy_coefficient is provided')\n\n    adaptive_cql_coefficient = fixed_cql_coefficient is None\n    if adaptive_cql_coefficient:\n      log_cql_alpha = jnp.asarray(0., dtype=jnp.float32)\n      cql_optimizer = optax.adam(learning_rate=3e-4)\n      cql_optimizer_state = cql_optimizer.init(log_cql_alpha)\n    else:\n      if cql_lagrange_threshold:\n        raise ValueError('cql_lagrange_threshold should not be set when '\n                         'fixed_cql_coefficient is provided')\n\n    def alpha_loss(log_sac_alpha: jnp.ndarray,\n                   policy_params: networks_lib.Params,\n                   transitions: types.Transition,\n                   key: jnp.ndarray) -> jnp.ndarray:\n      \"\"\"Eq 18 from https://arxiv.org/pdf/1812.05905.pdf.\"\"\"\n      dist_params = networks.policy_network.apply(policy_params,\n                                                  transitions.observation)\n      action = networks.sample(dist_params, key)\n      log_prob = networks.log_prob(dist_params, action)\n      sac_alpha = jnp.exp(log_sac_alpha)\n      sac_alpha_loss = sac_alpha * jax.lax.stop_gradient(-log_prob -\n                                                         target_entropy)\n      return jnp.mean(sac_alpha_loss)\n\n    def sac_critic_loss(q_old_action: jnp.ndarray,\n                        policy_params: networks_lib.Params,\n                        target_critic_params: networks_lib.Params,\n                        transitions: types.Transition,\n                        key: networks_lib.PRNGKey) -> jnp.ndarray:\n      \"\"\"Computes the SAC part of the loss.\"\"\"\n      next_dist_params = networks.policy_network.apply(\n          policy_params, transitions.next_observation)\n      next_action = networks.sample(next_dist_params, key)\n      next_q = networks.critic_network.apply(target_critic_params,\n                                             transitions.next_observation,\n                                             next_action)\n      next_v = jnp.min(next_q, axis=-1)\n      target_q = jax.lax.stop_gradient(transitions.reward * reward_scale +\n                                       transitions.discount * discount * next_v)\n      return jnp.mean(jnp.square(q_old_action - jnp.expand_dims(target_q, -1)))\n\n    def batched_critic(actions: jnp.ndarray, critic_params: networks_lib.Params,\n                       observation: jnp.ndarray) -> jnp.ndarray:\n      \"\"\"Applies the critic network to a batch of sampled actions.\"\"\"\n      actions = jax.lax.stop_gradient(actions)\n      tiled_actions = jnp.reshape(actions, (batch_size * cql_num_samples, -1))\n      tiled_states = jnp.tile(observation, [cql_num_samples, 1])\n      tiled_q = networks.critic_network.apply(critic_params, tiled_states,\n                                              tiled_actions)\n      return jnp.reshape(tiled_q, (cql_num_samples, batch_size, -1))\n\n    def cql_critic_loss(q_old_action: jnp.ndarray,\n                        critic_params: networks_lib.Params,\n                        policy_params: networks_lib.Params,\n                        transitions: types.Transition,\n                        key: networks_lib.PRNGKey) -> jnp.ndarray:\n      \"\"\"Computes the CQL part of the loss.\"\"\"\n      # The CQL part of the loss is\n      #     logsumexp(Q(s,\u00b7)) - Q(s,a),\n      # where s is the currrent state, and a the action in the dataset (so\n      # Q(s,a) is simply q_old_action.\n      # We need to estimate logsumexp(Q). This is done with importance sampling\n      # (IS). This function implements the unlabeled equation page 29, Appx. F,\n      # in https://arxiv.org/abs/2006.04779.\n      # Here, IS is done with the uniform distribution and the policy in the\n      # current state s. In their implementation, the authors also add the\n      # policy in the transiting state s':\n      # https://github.com/aviralkumar2907/CQL/blob/master/d4rl/rlkit/torch/sac/cql.py,\n      # (l. 233-236).\n\n      key_policy, key_policy_next, key_uniform = jax.random.split(key, 3)\n\n      def sampled_q(obs, key):\n        actions, log_probs = apply_and_sample_n(\n            key, networks, policy_params, obs, cql_num_samples)\n        return batched_critic(actions, critic_params,\n                              transitions.observation) - jax.lax.stop_gradient(\n                                  jnp.expand_dims(log_probs, -1))\n\n      # Sample wrt policy in s\n      sampled_q_from_policy = sampled_q(transitions.observation, key_policy)\n\n      # Sample wrt policy in s'\n      sampled_q_from_policy_next = sampled_q(transitions.next_observation,\n                                             key_policy_next)\n\n      # Sample wrt uniform\n      actions_uniform = jax.random.uniform(\n          key_uniform, (cql_num_samples, batch_size) + action_spec.shape,\n          minval=action_spec.minimum, maxval=action_spec.maximum)\n      log_prob_uniform = -jnp.sum(\n          jnp.log(action_spec.maximum - action_spec.minimum))\n      sampled_q_from_uniform = (\n          batched_critic(actions_uniform, critic_params,\n                         transitions.observation) - log_prob_uniform)\n\n      # Combine the samplings\n      combined = jnp.concatenate(\n          (sampled_q_from_uniform, sampled_q_from_policy,\n           sampled_q_from_policy_next),\n          axis=0)\n      lse_q = jax.nn.logsumexp(combined, axis=0, b=1. / (3 * cql_num_samples))\n\n      return jnp.mean(lse_q - q_old_action)\n\n    def critic_loss(critic_params: networks_lib.Params,\n                    policy_params: networks_lib.Params,\n                    target_critic_params: networks_lib.Params,\n                    cql_alpha: jnp.ndarray, transitions: types.Transition,\n                    key: networks_lib.PRNGKey) -> jnp.ndarray:\n      \"\"\"Computes the full critic loss.\"\"\"\n      key_cql, key_sac = jax.random.split(key, 2)\n      q_old_action = networks.critic_network.apply(critic_params,\n                                                   transitions.observation,\n                                                   transitions.action)\n      cql_loss = cql_critic_loss(q_old_action, critic_params, policy_params,\n                                 transitions, key_cql)\n      sac_loss = sac_critic_loss(q_old_action, policy_params,\n                                 target_critic_params, transitions, key_sac)\n      return cql_alpha * cql_loss + sac_loss\n\n    def cql_lagrange_loss(log_cql_alpha: jnp.ndarray,\n                          critic_params: networks_lib.Params,\n                          policy_params: networks_lib.Params,\n                          transitions: types.Transition,\n                          key: jnp.ndarray) -> jnp.ndarray:\n      \"\"\"Computes the loss that optimizes the cql coefficient.\"\"\"\n      cql_alpha = jnp.exp(log_cql_alpha)\n      q_old_action = networks.critic_network.apply(critic_params,\n                                                   transitions.observation,\n                                                   transitions.action)\n      return -cql_alpha * (\n          cql_critic_loss(q_old_action, critic_params, policy_params,\n                          transitions, key) - cql_lagrange_threshold)\n\n    def actor_loss(policy_params: networks_lib.Params,\n                   critic_params: networks_lib.Params, sac_alpha: jnp.ndarray,\n                   transitions: types.Transition, key: jnp.ndarray,\n                   in_initial_bc_iters: bool) -> jnp.ndarray:\n      \"\"\"Computes the loss for the policy.\"\"\"\n      dist_params = networks.policy_network.apply(policy_params,\n                                                  transitions.observation)\n      if in_initial_bc_iters:\n        log_prob = networks.log_prob(dist_params, transitions.action)\n        actor_loss = -jnp.mean(log_prob)\n      else:\n        action = networks.sample(dist_params, key)\n        log_prob = networks.log_prob(dist_params, action)\n        q_action = networks.critic_network.apply(critic_params,\n                                                 transitions.observation,\n                                                 action)\n        min_q = jnp.min(q_action, axis=-1)\n        actor_loss = jnp.mean(sac_alpha * log_prob - min_q)\n      return actor_loss\n\n    alpha_grad = jax.value_and_grad(alpha_loss)\n    cql_lagrange_grad = jax.value_and_grad(cql_lagrange_loss)\n    critic_grad = jax.value_and_grad(critic_loss)\n    actor_grad = jax.value_and_grad(actor_loss)\n\n    def update_step(\n        state: TrainingState,\n        rb_transitions: types.Transition,\n        in_initial_bc_iters: bool,\n    ) -> Tuple[TrainingState, Dict[str, jnp.ndarray]]:\n\n      key, key_alpha, key_critic, key_actor = jax.random.split(state.key, 4)\n\n      if adaptive_entropy_coefficient:\n        alpha_loss, alpha_grads = alpha_grad(state.log_sac_alpha,\n                                             state.policy_params,\n                                             rb_transitions, key_alpha)\n        sac_alpha = jnp.exp(state.log_sac_alpha)\n      else:\n        sac_alpha = fixed_entropy_coefficient\n\n      if adaptive_cql_coefficient:\n        cql_lagrange_loss, cql_lagrange_grads = cql_lagrange_grad(\n            state.log_cql_alpha, state.critic_params, state.policy_params,\n            rb_transitions, key_critic)\n        cql_lagrange_grads = jnp.clip(cql_lagrange_grads,\n                                      -_CQL_GRAD_CLIPPING_VALUE,\n                                      _CQL_GRAD_CLIPPING_VALUE)\n        cql_alpha = jnp.exp(state.log_cql_alpha)\n        cql_alpha = jnp.clip(\n            cql_alpha, a_min=0., a_max=_CQL_COEFFICIENT_MAX_VALUE)\n      else:\n        cql_alpha = fixed_cql_coefficient\n\n      critic_loss, critic_grads = critic_grad(state.critic_params,\n                                              state.policy_params,\n                                              state.target_critic_params,\n                                              cql_alpha, rb_transitions,\n                                              key_critic)\n      actor_loss, actor_grads = actor_grad(state.policy_params,\n                                           state.critic_params, sac_alpha,\n                                           rb_transitions, key_actor,\n                                           in_initial_bc_iters)\n\n      # Apply policy gradients\n      actor_update, policy_optimizer_state = policy_optimizer.update(\n          actor_grads, state.policy_optimizer_state)\n      policy_params = optax.apply_updates(state.policy_params, actor_update)\n\n      # Apply critic gradients\n      critic_update, critic_optimizer_state = critic_optimizer.update(\n          critic_grads, state.critic_optimizer_state)\n      critic_params = optax.apply_updates(state.critic_params, critic_update)\n\n      new_target_critic_params = jax.tree_map(\n          lambda x, y: x * (1 - tau) + y * tau, state.target_critic_params,\n          critic_params)\n\n      metrics = {\n          'critic_loss': critic_loss,\n          'actor_loss': actor_loss,\n      }\n\n      new_state = TrainingState(\n          policy_optimizer_state=policy_optimizer_state,\n          critic_optimizer_state=critic_optimizer_state,\n          policy_params=policy_params,\n          critic_params=critic_params,\n          target_critic_params=new_target_critic_params,\n          key=key,\n          alpha_optimizer_state=state.alpha_optimizer_state,\n          log_sac_alpha=state.log_sac_alpha,\n          steps=state.steps + 1,\n      )\n      if adaptive_entropy_coefficient and (not in_initial_bc_iters):\n        # Apply sac_alpha gradients\n        alpha_update, alpha_optimizer_state = alpha_optimizer.update(\n            alpha_grads, state.alpha_optimizer_state)\n        log_sac_alpha = optax.apply_updates(state.log_sac_alpha, alpha_update)\n        metrics.update({\n            'alpha_loss': alpha_loss,\n            'sac_alpha': jnp.exp(log_sac_alpha),\n        })\n        new_state = new_state._replace(\n            alpha_optimizer_state=alpha_optimizer_state,\n            log_sac_alpha=log_sac_alpha)\n      else:\n        metrics['alpha_loss'] = 0.\n        metrics['sac_alpha'] = fixed_cql_coefficient\n\n      if adaptive_cql_coefficient:\n        # Apply cql coeff gradients\n        cql_update, cql_optimizer_state = cql_optimizer.update(\n            cql_lagrange_grads, state.cql_optimizer_state)\n        log_cql_alpha = optax.apply_updates(state.log_cql_alpha, cql_update)\n        metrics.update({\n            'cql_lagrange_loss': cql_lagrange_loss,\n            'cql_alpha': jnp.exp(log_cql_alpha),\n        })\n        new_state = new_state._replace(\n            cql_optimizer_state=cql_optimizer_state,\n            log_cql_alpha=log_cql_alpha)\n\n      return new_state, metrics\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger(\n        'learner',\n        asynchronous=True,\n        serialize_fn=utils.fetch_devicearray,\n        steps_key=self._counter.get_steps_key())\n\n    # Iterator on demonstration transitions.\n    self._demonstrations = demonstrations\n\n    # Use the JIT compiler.\n    update_step_in_initial_bc_iters = utils.process_multiple_batches(\n        lambda x, y: update_step(x, y, True), num_sgd_steps_per_step)\n    update_step_rest = utils.process_multiple_batches(\n        lambda x, y: update_step(x, y, False), num_sgd_steps_per_step)\n\n    self._update_step_in_initial_bc_iters = jax.jit(\n        update_step_in_initial_bc_iters)\n    self._update_step_rest = jax.jit(update_step_rest)\n\n    # Create initial state.\n    key_policy, key_q, training_state_key = jax.random.split(random_key, 3)\n    del random_key\n    policy_params = networks.policy_network.init(key_policy)\n    policy_optimizer_state = policy_optimizer.init(policy_params)\n    critic_params = networks.critic_network.init(key_q)\n    critic_optimizer_state = critic_optimizer.init(critic_params)\n\n    self._state = TrainingState(\n        policy_optimizer_state=policy_optimizer_state,\n        critic_optimizer_state=critic_optimizer_state,\n        policy_params=policy_params,\n        critic_params=critic_params,\n        target_critic_params=critic_params,\n        key=training_state_key,\n        steps=0)\n\n    if adaptive_entropy_coefficient:\n      self._state = self._state._replace(\n          alpha_optimizer_state=alpha_optimizer_state,\n          log_sac_alpha=log_sac_alpha)\n    if adaptive_cql_coefficient:\n      self._state = self._state._replace(\n          cql_optimizer_state=cql_optimizer_state, log_cql_alpha=log_cql_alpha)\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None\n\n  def step(self):\n    # Get data from replay (dropping extras if any). Note there is no\n    # extra data here because we do not insert any into Reverb.\n    transitions = next(self._demonstrations)\n\n    counts = self._counter.get_counts()\n    if 'learner_steps' not in counts:\n      cur_step = 0\n    else:\n      cur_step = counts['learner_steps']\n    in_initial_bc_iters = cur_step < self._num_bc_iters\n\n    if in_initial_bc_iters:\n      self._state, metrics = self._update_step_in_initial_bc_iters(\n          self._state, transitions)\n    else:\n      self._state, metrics = self._update_step_rest(self._state, transitions)\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Increment counts and record the current time\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n\n    # Attempts to write the logs.\n    self._logger.write({**metrics, **counts})\n\n  def get_variables(self, names: List[str]) -> List[Any]:\n    variables = {\n        'policy': self._state.policy_params,\n    }\n    return [variables[name] for name in names]\n\n  def save(self) -> TrainingState:\n    return self._state\n\n  def restore(self, state: TrainingState):\n    self._state = state",
  "def __init__(self,\n               batch_size: int,\n               networks: CQLNetworks,\n               random_key: networks_lib.PRNGKey,\n               demonstrations: Iterator[types.Transition],\n               policy_optimizer: optax.GradientTransformation,\n               critic_optimizer: optax.GradientTransformation,\n               tau: float = 0.005,\n               fixed_cql_coefficient: Optional[float] = None,\n               cql_lagrange_threshold: Optional[float] = None,\n               cql_num_samples: int = 10,\n               num_sgd_steps_per_step: int = 1,\n               reward_scale: float = 1.0,\n               discount: float = 0.99,\n               fixed_entropy_coefficient: Optional[float] = None,\n               target_entropy: Optional[float] = 0,\n               num_bc_iters: int = 50_000,\n               counter: Optional[counting.Counter] = None,\n               logger: Optional[loggers.Logger] = None):\n    \"\"\"Initializes the CQL learner.\n\n    Args:\n      batch_size: batch size.\n      networks: CQL networks.\n      random_key: a key for random number generation.\n      demonstrations: an iterator over training data.\n      policy_optimizer: the policy optimizer.\n      critic_optimizer: the Q-function optimizer.\n      tau: target smoothing coefficient.\n      fixed_cql_coefficient: the value for cql coefficient. If None, an adaptive\n        coefficient will be used.\n      cql_lagrange_threshold: a threshold that controls the adaptive loss for\n        the cql coefficient.\n      cql_num_samples: number of samples used to compute logsumexp(Q) via\n        importance sampling.\n      num_sgd_steps_per_step: how many gradient updated to perform per batch.\n        batch is split into this many smaller batches, thus should be a multiple\n        of num_sgd_steps_per_step\n      reward_scale: reward scale.\n      discount: discount to use for TD updates.\n      fixed_entropy_coefficient: coefficient applied to the entropy bonus. If\n        None, an adaptative coefficient will be used.\n      target_entropy: Target entropy when using adapdative entropy bonus.\n      num_bc_iters: Number of BC steps for actor initialization.\n      counter: counter object used to keep track of steps.\n      logger: logger object to be used by learner.\n    \"\"\"\n    self._num_bc_iters = num_bc_iters\n    adaptive_entropy_coefficient = fixed_entropy_coefficient is None\n    action_spec = networks.environment_specs.actions\n    if adaptive_entropy_coefficient:\n      # sac_alpha is the temperature parameter that determines the relative\n      # importance of the entropy term versus the reward.\n      log_sac_alpha = jnp.asarray(0., dtype=jnp.float32)\n      alpha_optimizer = optax.adam(learning_rate=3e-4)\n      alpha_optimizer_state = alpha_optimizer.init(log_sac_alpha)\n    else:\n      if target_entropy:\n        raise ValueError('target_entropy should not be set when '\n                         'fixed_entropy_coefficient is provided')\n\n    adaptive_cql_coefficient = fixed_cql_coefficient is None\n    if adaptive_cql_coefficient:\n      log_cql_alpha = jnp.asarray(0., dtype=jnp.float32)\n      cql_optimizer = optax.adam(learning_rate=3e-4)\n      cql_optimizer_state = cql_optimizer.init(log_cql_alpha)\n    else:\n      if cql_lagrange_threshold:\n        raise ValueError('cql_lagrange_threshold should not be set when '\n                         'fixed_cql_coefficient is provided')\n\n    def alpha_loss(log_sac_alpha: jnp.ndarray,\n                   policy_params: networks_lib.Params,\n                   transitions: types.Transition,\n                   key: jnp.ndarray) -> jnp.ndarray:\n      \"\"\"Eq 18 from https://arxiv.org/pdf/1812.05905.pdf.\"\"\"\n      dist_params = networks.policy_network.apply(policy_params,\n                                                  transitions.observation)\n      action = networks.sample(dist_params, key)\n      log_prob = networks.log_prob(dist_params, action)\n      sac_alpha = jnp.exp(log_sac_alpha)\n      sac_alpha_loss = sac_alpha * jax.lax.stop_gradient(-log_prob -\n                                                         target_entropy)\n      return jnp.mean(sac_alpha_loss)\n\n    def sac_critic_loss(q_old_action: jnp.ndarray,\n                        policy_params: networks_lib.Params,\n                        target_critic_params: networks_lib.Params,\n                        transitions: types.Transition,\n                        key: networks_lib.PRNGKey) -> jnp.ndarray:\n      \"\"\"Computes the SAC part of the loss.\"\"\"\n      next_dist_params = networks.policy_network.apply(\n          policy_params, transitions.next_observation)\n      next_action = networks.sample(next_dist_params, key)\n      next_q = networks.critic_network.apply(target_critic_params,\n                                             transitions.next_observation,\n                                             next_action)\n      next_v = jnp.min(next_q, axis=-1)\n      target_q = jax.lax.stop_gradient(transitions.reward * reward_scale +\n                                       transitions.discount * discount * next_v)\n      return jnp.mean(jnp.square(q_old_action - jnp.expand_dims(target_q, -1)))\n\n    def batched_critic(actions: jnp.ndarray, critic_params: networks_lib.Params,\n                       observation: jnp.ndarray) -> jnp.ndarray:\n      \"\"\"Applies the critic network to a batch of sampled actions.\"\"\"\n      actions = jax.lax.stop_gradient(actions)\n      tiled_actions = jnp.reshape(actions, (batch_size * cql_num_samples, -1))\n      tiled_states = jnp.tile(observation, [cql_num_samples, 1])\n      tiled_q = networks.critic_network.apply(critic_params, tiled_states,\n                                              tiled_actions)\n      return jnp.reshape(tiled_q, (cql_num_samples, batch_size, -1))\n\n    def cql_critic_loss(q_old_action: jnp.ndarray,\n                        critic_params: networks_lib.Params,\n                        policy_params: networks_lib.Params,\n                        transitions: types.Transition,\n                        key: networks_lib.PRNGKey) -> jnp.ndarray:\n      \"\"\"Computes the CQL part of the loss.\"\"\"\n      # The CQL part of the loss is\n      #     logsumexp(Q(s,\u00b7)) - Q(s,a),\n      # where s is the currrent state, and a the action in the dataset (so\n      # Q(s,a) is simply q_old_action.\n      # We need to estimate logsumexp(Q). This is done with importance sampling\n      # (IS). This function implements the unlabeled equation page 29, Appx. F,\n      # in https://arxiv.org/abs/2006.04779.\n      # Here, IS is done with the uniform distribution and the policy in the\n      # current state s. In their implementation, the authors also add the\n      # policy in the transiting state s':\n      # https://github.com/aviralkumar2907/CQL/blob/master/d4rl/rlkit/torch/sac/cql.py,\n      # (l. 233-236).\n\n      key_policy, key_policy_next, key_uniform = jax.random.split(key, 3)\n\n      def sampled_q(obs, key):\n        actions, log_probs = apply_and_sample_n(\n            key, networks, policy_params, obs, cql_num_samples)\n        return batched_critic(actions, critic_params,\n                              transitions.observation) - jax.lax.stop_gradient(\n                                  jnp.expand_dims(log_probs, -1))\n\n      # Sample wrt policy in s\n      sampled_q_from_policy = sampled_q(transitions.observation, key_policy)\n\n      # Sample wrt policy in s'\n      sampled_q_from_policy_next = sampled_q(transitions.next_observation,\n                                             key_policy_next)\n\n      # Sample wrt uniform\n      actions_uniform = jax.random.uniform(\n          key_uniform, (cql_num_samples, batch_size) + action_spec.shape,\n          minval=action_spec.minimum, maxval=action_spec.maximum)\n      log_prob_uniform = -jnp.sum(\n          jnp.log(action_spec.maximum - action_spec.minimum))\n      sampled_q_from_uniform = (\n          batched_critic(actions_uniform, critic_params,\n                         transitions.observation) - log_prob_uniform)\n\n      # Combine the samplings\n      combined = jnp.concatenate(\n          (sampled_q_from_uniform, sampled_q_from_policy,\n           sampled_q_from_policy_next),\n          axis=0)\n      lse_q = jax.nn.logsumexp(combined, axis=0, b=1. / (3 * cql_num_samples))\n\n      return jnp.mean(lse_q - q_old_action)\n\n    def critic_loss(critic_params: networks_lib.Params,\n                    policy_params: networks_lib.Params,\n                    target_critic_params: networks_lib.Params,\n                    cql_alpha: jnp.ndarray, transitions: types.Transition,\n                    key: networks_lib.PRNGKey) -> jnp.ndarray:\n      \"\"\"Computes the full critic loss.\"\"\"\n      key_cql, key_sac = jax.random.split(key, 2)\n      q_old_action = networks.critic_network.apply(critic_params,\n                                                   transitions.observation,\n                                                   transitions.action)\n      cql_loss = cql_critic_loss(q_old_action, critic_params, policy_params,\n                                 transitions, key_cql)\n      sac_loss = sac_critic_loss(q_old_action, policy_params,\n                                 target_critic_params, transitions, key_sac)\n      return cql_alpha * cql_loss + sac_loss\n\n    def cql_lagrange_loss(log_cql_alpha: jnp.ndarray,\n                          critic_params: networks_lib.Params,\n                          policy_params: networks_lib.Params,\n                          transitions: types.Transition,\n                          key: jnp.ndarray) -> jnp.ndarray:\n      \"\"\"Computes the loss that optimizes the cql coefficient.\"\"\"\n      cql_alpha = jnp.exp(log_cql_alpha)\n      q_old_action = networks.critic_network.apply(critic_params,\n                                                   transitions.observation,\n                                                   transitions.action)\n      return -cql_alpha * (\n          cql_critic_loss(q_old_action, critic_params, policy_params,\n                          transitions, key) - cql_lagrange_threshold)\n\n    def actor_loss(policy_params: networks_lib.Params,\n                   critic_params: networks_lib.Params, sac_alpha: jnp.ndarray,\n                   transitions: types.Transition, key: jnp.ndarray,\n                   in_initial_bc_iters: bool) -> jnp.ndarray:\n      \"\"\"Computes the loss for the policy.\"\"\"\n      dist_params = networks.policy_network.apply(policy_params,\n                                                  transitions.observation)\n      if in_initial_bc_iters:\n        log_prob = networks.log_prob(dist_params, transitions.action)\n        actor_loss = -jnp.mean(log_prob)\n      else:\n        action = networks.sample(dist_params, key)\n        log_prob = networks.log_prob(dist_params, action)\n        q_action = networks.critic_network.apply(critic_params,\n                                                 transitions.observation,\n                                                 action)\n        min_q = jnp.min(q_action, axis=-1)\n        actor_loss = jnp.mean(sac_alpha * log_prob - min_q)\n      return actor_loss\n\n    alpha_grad = jax.value_and_grad(alpha_loss)\n    cql_lagrange_grad = jax.value_and_grad(cql_lagrange_loss)\n    critic_grad = jax.value_and_grad(critic_loss)\n    actor_grad = jax.value_and_grad(actor_loss)\n\n    def update_step(\n        state: TrainingState,\n        rb_transitions: types.Transition,\n        in_initial_bc_iters: bool,\n    ) -> Tuple[TrainingState, Dict[str, jnp.ndarray]]:\n\n      key, key_alpha, key_critic, key_actor = jax.random.split(state.key, 4)\n\n      if adaptive_entropy_coefficient:\n        alpha_loss, alpha_grads = alpha_grad(state.log_sac_alpha,\n                                             state.policy_params,\n                                             rb_transitions, key_alpha)\n        sac_alpha = jnp.exp(state.log_sac_alpha)\n      else:\n        sac_alpha = fixed_entropy_coefficient\n\n      if adaptive_cql_coefficient:\n        cql_lagrange_loss, cql_lagrange_grads = cql_lagrange_grad(\n            state.log_cql_alpha, state.critic_params, state.policy_params,\n            rb_transitions, key_critic)\n        cql_lagrange_grads = jnp.clip(cql_lagrange_grads,\n                                      -_CQL_GRAD_CLIPPING_VALUE,\n                                      _CQL_GRAD_CLIPPING_VALUE)\n        cql_alpha = jnp.exp(state.log_cql_alpha)\n        cql_alpha = jnp.clip(\n            cql_alpha, a_min=0., a_max=_CQL_COEFFICIENT_MAX_VALUE)\n      else:\n        cql_alpha = fixed_cql_coefficient\n\n      critic_loss, critic_grads = critic_grad(state.critic_params,\n                                              state.policy_params,\n                                              state.target_critic_params,\n                                              cql_alpha, rb_transitions,\n                                              key_critic)\n      actor_loss, actor_grads = actor_grad(state.policy_params,\n                                           state.critic_params, sac_alpha,\n                                           rb_transitions, key_actor,\n                                           in_initial_bc_iters)\n\n      # Apply policy gradients\n      actor_update, policy_optimizer_state = policy_optimizer.update(\n          actor_grads, state.policy_optimizer_state)\n      policy_params = optax.apply_updates(state.policy_params, actor_update)\n\n      # Apply critic gradients\n      critic_update, critic_optimizer_state = critic_optimizer.update(\n          critic_grads, state.critic_optimizer_state)\n      critic_params = optax.apply_updates(state.critic_params, critic_update)\n\n      new_target_critic_params = jax.tree_map(\n          lambda x, y: x * (1 - tau) + y * tau, state.target_critic_params,\n          critic_params)\n\n      metrics = {\n          'critic_loss': critic_loss,\n          'actor_loss': actor_loss,\n      }\n\n      new_state = TrainingState(\n          policy_optimizer_state=policy_optimizer_state,\n          critic_optimizer_state=critic_optimizer_state,\n          policy_params=policy_params,\n          critic_params=critic_params,\n          target_critic_params=new_target_critic_params,\n          key=key,\n          alpha_optimizer_state=state.alpha_optimizer_state,\n          log_sac_alpha=state.log_sac_alpha,\n          steps=state.steps + 1,\n      )\n      if adaptive_entropy_coefficient and (not in_initial_bc_iters):\n        # Apply sac_alpha gradients\n        alpha_update, alpha_optimizer_state = alpha_optimizer.update(\n            alpha_grads, state.alpha_optimizer_state)\n        log_sac_alpha = optax.apply_updates(state.log_sac_alpha, alpha_update)\n        metrics.update({\n            'alpha_loss': alpha_loss,\n            'sac_alpha': jnp.exp(log_sac_alpha),\n        })\n        new_state = new_state._replace(\n            alpha_optimizer_state=alpha_optimizer_state,\n            log_sac_alpha=log_sac_alpha)\n      else:\n        metrics['alpha_loss'] = 0.\n        metrics['sac_alpha'] = fixed_cql_coefficient\n\n      if adaptive_cql_coefficient:\n        # Apply cql coeff gradients\n        cql_update, cql_optimizer_state = cql_optimizer.update(\n            cql_lagrange_grads, state.cql_optimizer_state)\n        log_cql_alpha = optax.apply_updates(state.log_cql_alpha, cql_update)\n        metrics.update({\n            'cql_lagrange_loss': cql_lagrange_loss,\n            'cql_alpha': jnp.exp(log_cql_alpha),\n        })\n        new_state = new_state._replace(\n            cql_optimizer_state=cql_optimizer_state,\n            log_cql_alpha=log_cql_alpha)\n\n      return new_state, metrics\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger(\n        'learner',\n        asynchronous=True,\n        serialize_fn=utils.fetch_devicearray,\n        steps_key=self._counter.get_steps_key())\n\n    # Iterator on demonstration transitions.\n    self._demonstrations = demonstrations\n\n    # Use the JIT compiler.\n    update_step_in_initial_bc_iters = utils.process_multiple_batches(\n        lambda x, y: update_step(x, y, True), num_sgd_steps_per_step)\n    update_step_rest = utils.process_multiple_batches(\n        lambda x, y: update_step(x, y, False), num_sgd_steps_per_step)\n\n    self._update_step_in_initial_bc_iters = jax.jit(\n        update_step_in_initial_bc_iters)\n    self._update_step_rest = jax.jit(update_step_rest)\n\n    # Create initial state.\n    key_policy, key_q, training_state_key = jax.random.split(random_key, 3)\n    del random_key\n    policy_params = networks.policy_network.init(key_policy)\n    policy_optimizer_state = policy_optimizer.init(policy_params)\n    critic_params = networks.critic_network.init(key_q)\n    critic_optimizer_state = critic_optimizer.init(critic_params)\n\n    self._state = TrainingState(\n        policy_optimizer_state=policy_optimizer_state,\n        critic_optimizer_state=critic_optimizer_state,\n        policy_params=policy_params,\n        critic_params=critic_params,\n        target_critic_params=critic_params,\n        key=training_state_key,\n        steps=0)\n\n    if adaptive_entropy_coefficient:\n      self._state = self._state._replace(\n          alpha_optimizer_state=alpha_optimizer_state,\n          log_sac_alpha=log_sac_alpha)\n    if adaptive_cql_coefficient:\n      self._state = self._state._replace(\n          cql_optimizer_state=cql_optimizer_state, log_cql_alpha=log_cql_alpha)\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None",
  "def step(self):\n    # Get data from replay (dropping extras if any). Note there is no\n    # extra data here because we do not insert any into Reverb.\n    transitions = next(self._demonstrations)\n\n    counts = self._counter.get_counts()\n    if 'learner_steps' not in counts:\n      cur_step = 0\n    else:\n      cur_step = counts['learner_steps']\n    in_initial_bc_iters = cur_step < self._num_bc_iters\n\n    if in_initial_bc_iters:\n      self._state, metrics = self._update_step_in_initial_bc_iters(\n          self._state, transitions)\n    else:\n      self._state, metrics = self._update_step_rest(self._state, transitions)\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Increment counts and record the current time\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n\n    # Attempts to write the logs.\n    self._logger.write({**metrics, **counts})",
  "def get_variables(self, names: List[str]) -> List[Any]:\n    variables = {\n        'policy': self._state.policy_params,\n    }\n    return [variables[name] for name in names]",
  "def save(self) -> TrainingState:\n    return self._state",
  "def restore(self, state: TrainingState):\n    self._state = state",
  "def alpha_loss(log_sac_alpha: jnp.ndarray,\n                   policy_params: networks_lib.Params,\n                   transitions: types.Transition,\n                   key: jnp.ndarray) -> jnp.ndarray:\n      \"\"\"Eq 18 from https://arxiv.org/pdf/1812.05905.pdf.\"\"\"\n      dist_params = networks.policy_network.apply(policy_params,\n                                                  transitions.observation)\n      action = networks.sample(dist_params, key)\n      log_prob = networks.log_prob(dist_params, action)\n      sac_alpha = jnp.exp(log_sac_alpha)\n      sac_alpha_loss = sac_alpha * jax.lax.stop_gradient(-log_prob -\n                                                         target_entropy)\n      return jnp.mean(sac_alpha_loss)",
  "def sac_critic_loss(q_old_action: jnp.ndarray,\n                        policy_params: networks_lib.Params,\n                        target_critic_params: networks_lib.Params,\n                        transitions: types.Transition,\n                        key: networks_lib.PRNGKey) -> jnp.ndarray:\n      \"\"\"Computes the SAC part of the loss.\"\"\"\n      next_dist_params = networks.policy_network.apply(\n          policy_params, transitions.next_observation)\n      next_action = networks.sample(next_dist_params, key)\n      next_q = networks.critic_network.apply(target_critic_params,\n                                             transitions.next_observation,\n                                             next_action)\n      next_v = jnp.min(next_q, axis=-1)\n      target_q = jax.lax.stop_gradient(transitions.reward * reward_scale +\n                                       transitions.discount * discount * next_v)\n      return jnp.mean(jnp.square(q_old_action - jnp.expand_dims(target_q, -1)))",
  "def batched_critic(actions: jnp.ndarray, critic_params: networks_lib.Params,\n                       observation: jnp.ndarray) -> jnp.ndarray:\n      \"\"\"Applies the critic network to a batch of sampled actions.\"\"\"\n      actions = jax.lax.stop_gradient(actions)\n      tiled_actions = jnp.reshape(actions, (batch_size * cql_num_samples, -1))\n      tiled_states = jnp.tile(observation, [cql_num_samples, 1])\n      tiled_q = networks.critic_network.apply(critic_params, tiled_states,\n                                              tiled_actions)\n      return jnp.reshape(tiled_q, (cql_num_samples, batch_size, -1))",
  "def cql_critic_loss(q_old_action: jnp.ndarray,\n                        critic_params: networks_lib.Params,\n                        policy_params: networks_lib.Params,\n                        transitions: types.Transition,\n                        key: networks_lib.PRNGKey) -> jnp.ndarray:\n      \"\"\"Computes the CQL part of the loss.\"\"\"\n      # The CQL part of the loss is\n      #     logsumexp(Q(s,\u00b7)) - Q(s,a),\n      # where s is the currrent state, and a the action in the dataset (so\n      # Q(s,a) is simply q_old_action.\n      # We need to estimate logsumexp(Q). This is done with importance sampling\n      # (IS). This function implements the unlabeled equation page 29, Appx. F,\n      # in https://arxiv.org/abs/2006.04779.\n      # Here, IS is done with the uniform distribution and the policy in the\n      # current state s. In their implementation, the authors also add the\n      # policy in the transiting state s':\n      # https://github.com/aviralkumar2907/CQL/blob/master/d4rl/rlkit/torch/sac/cql.py,\n      # (l. 233-236).\n\n      key_policy, key_policy_next, key_uniform = jax.random.split(key, 3)\n\n      def sampled_q(obs, key):\n        actions, log_probs = apply_and_sample_n(\n            key, networks, policy_params, obs, cql_num_samples)\n        return batched_critic(actions, critic_params,\n                              transitions.observation) - jax.lax.stop_gradient(\n                                  jnp.expand_dims(log_probs, -1))\n\n      # Sample wrt policy in s\n      sampled_q_from_policy = sampled_q(transitions.observation, key_policy)\n\n      # Sample wrt policy in s'\n      sampled_q_from_policy_next = sampled_q(transitions.next_observation,\n                                             key_policy_next)\n\n      # Sample wrt uniform\n      actions_uniform = jax.random.uniform(\n          key_uniform, (cql_num_samples, batch_size) + action_spec.shape,\n          minval=action_spec.minimum, maxval=action_spec.maximum)\n      log_prob_uniform = -jnp.sum(\n          jnp.log(action_spec.maximum - action_spec.minimum))\n      sampled_q_from_uniform = (\n          batched_critic(actions_uniform, critic_params,\n                         transitions.observation) - log_prob_uniform)\n\n      # Combine the samplings\n      combined = jnp.concatenate(\n          (sampled_q_from_uniform, sampled_q_from_policy,\n           sampled_q_from_policy_next),\n          axis=0)\n      lse_q = jax.nn.logsumexp(combined, axis=0, b=1. / (3 * cql_num_samples))\n\n      return jnp.mean(lse_q - q_old_action)",
  "def critic_loss(critic_params: networks_lib.Params,\n                    policy_params: networks_lib.Params,\n                    target_critic_params: networks_lib.Params,\n                    cql_alpha: jnp.ndarray, transitions: types.Transition,\n                    key: networks_lib.PRNGKey) -> jnp.ndarray:\n      \"\"\"Computes the full critic loss.\"\"\"\n      key_cql, key_sac = jax.random.split(key, 2)\n      q_old_action = networks.critic_network.apply(critic_params,\n                                                   transitions.observation,\n                                                   transitions.action)\n      cql_loss = cql_critic_loss(q_old_action, critic_params, policy_params,\n                                 transitions, key_cql)\n      sac_loss = sac_critic_loss(q_old_action, policy_params,\n                                 target_critic_params, transitions, key_sac)\n      return cql_alpha * cql_loss + sac_loss",
  "def cql_lagrange_loss(log_cql_alpha: jnp.ndarray,\n                          critic_params: networks_lib.Params,\n                          policy_params: networks_lib.Params,\n                          transitions: types.Transition,\n                          key: jnp.ndarray) -> jnp.ndarray:\n      \"\"\"Computes the loss that optimizes the cql coefficient.\"\"\"\n      cql_alpha = jnp.exp(log_cql_alpha)\n      q_old_action = networks.critic_network.apply(critic_params,\n                                                   transitions.observation,\n                                                   transitions.action)\n      return -cql_alpha * (\n          cql_critic_loss(q_old_action, critic_params, policy_params,\n                          transitions, key) - cql_lagrange_threshold)",
  "def actor_loss(policy_params: networks_lib.Params,\n                   critic_params: networks_lib.Params, sac_alpha: jnp.ndarray,\n                   transitions: types.Transition, key: jnp.ndarray,\n                   in_initial_bc_iters: bool) -> jnp.ndarray:\n      \"\"\"Computes the loss for the policy.\"\"\"\n      dist_params = networks.policy_network.apply(policy_params,\n                                                  transitions.observation)\n      if in_initial_bc_iters:\n        log_prob = networks.log_prob(dist_params, transitions.action)\n        actor_loss = -jnp.mean(log_prob)\n      else:\n        action = networks.sample(dist_params, key)\n        log_prob = networks.log_prob(dist_params, action)\n        q_action = networks.critic_network.apply(critic_params,\n                                                 transitions.observation,\n                                                 action)\n        min_q = jnp.min(q_action, axis=-1)\n        actor_loss = jnp.mean(sac_alpha * log_prob - min_q)\n      return actor_loss",
  "def update_step(\n        state: TrainingState,\n        rb_transitions: types.Transition,\n        in_initial_bc_iters: bool,\n    ) -> Tuple[TrainingState, Dict[str, jnp.ndarray]]:\n\n      key, key_alpha, key_critic, key_actor = jax.random.split(state.key, 4)\n\n      if adaptive_entropy_coefficient:\n        alpha_loss, alpha_grads = alpha_grad(state.log_sac_alpha,\n                                             state.policy_params,\n                                             rb_transitions, key_alpha)\n        sac_alpha = jnp.exp(state.log_sac_alpha)\n      else:\n        sac_alpha = fixed_entropy_coefficient\n\n      if adaptive_cql_coefficient:\n        cql_lagrange_loss, cql_lagrange_grads = cql_lagrange_grad(\n            state.log_cql_alpha, state.critic_params, state.policy_params,\n            rb_transitions, key_critic)\n        cql_lagrange_grads = jnp.clip(cql_lagrange_grads,\n                                      -_CQL_GRAD_CLIPPING_VALUE,\n                                      _CQL_GRAD_CLIPPING_VALUE)\n        cql_alpha = jnp.exp(state.log_cql_alpha)\n        cql_alpha = jnp.clip(\n            cql_alpha, a_min=0., a_max=_CQL_COEFFICIENT_MAX_VALUE)\n      else:\n        cql_alpha = fixed_cql_coefficient\n\n      critic_loss, critic_grads = critic_grad(state.critic_params,\n                                              state.policy_params,\n                                              state.target_critic_params,\n                                              cql_alpha, rb_transitions,\n                                              key_critic)\n      actor_loss, actor_grads = actor_grad(state.policy_params,\n                                           state.critic_params, sac_alpha,\n                                           rb_transitions, key_actor,\n                                           in_initial_bc_iters)\n\n      # Apply policy gradients\n      actor_update, policy_optimizer_state = policy_optimizer.update(\n          actor_grads, state.policy_optimizer_state)\n      policy_params = optax.apply_updates(state.policy_params, actor_update)\n\n      # Apply critic gradients\n      critic_update, critic_optimizer_state = critic_optimizer.update(\n          critic_grads, state.critic_optimizer_state)\n      critic_params = optax.apply_updates(state.critic_params, critic_update)\n\n      new_target_critic_params = jax.tree_map(\n          lambda x, y: x * (1 - tau) + y * tau, state.target_critic_params,\n          critic_params)\n\n      metrics = {\n          'critic_loss': critic_loss,\n          'actor_loss': actor_loss,\n      }\n\n      new_state = TrainingState(\n          policy_optimizer_state=policy_optimizer_state,\n          critic_optimizer_state=critic_optimizer_state,\n          policy_params=policy_params,\n          critic_params=critic_params,\n          target_critic_params=new_target_critic_params,\n          key=key,\n          alpha_optimizer_state=state.alpha_optimizer_state,\n          log_sac_alpha=state.log_sac_alpha,\n          steps=state.steps + 1,\n      )\n      if adaptive_entropy_coefficient and (not in_initial_bc_iters):\n        # Apply sac_alpha gradients\n        alpha_update, alpha_optimizer_state = alpha_optimizer.update(\n            alpha_grads, state.alpha_optimizer_state)\n        log_sac_alpha = optax.apply_updates(state.log_sac_alpha, alpha_update)\n        metrics.update({\n            'alpha_loss': alpha_loss,\n            'sac_alpha': jnp.exp(log_sac_alpha),\n        })\n        new_state = new_state._replace(\n            alpha_optimizer_state=alpha_optimizer_state,\n            log_sac_alpha=log_sac_alpha)\n      else:\n        metrics['alpha_loss'] = 0.\n        metrics['sac_alpha'] = fixed_cql_coefficient\n\n      if adaptive_cql_coefficient:\n        # Apply cql coeff gradients\n        cql_update, cql_optimizer_state = cql_optimizer.update(\n            cql_lagrange_grads, state.cql_optimizer_state)\n        log_cql_alpha = optax.apply_updates(state.log_cql_alpha, cql_update)\n        metrics.update({\n            'cql_lagrange_loss': cql_lagrange_loss,\n            'cql_alpha': jnp.exp(log_cql_alpha),\n        })\n        new_state = new_state._replace(\n            cql_optimizer_state=cql_optimizer_state,\n            log_cql_alpha=log_cql_alpha)\n\n      return new_state, metrics",
  "def sampled_q(obs, key):\n        actions, log_probs = apply_and_sample_n(\n            key, networks, policy_params, obs, cql_num_samples)\n        return batched_critic(actions, critic_params,\n                              transitions.observation) - jax.lax.stop_gradient(\n                                  jnp.expand_dims(log_probs, -1))",
  "class CQLNetworks:\n  \"\"\"Network and pure functions for the CQL agent.\"\"\"\n  policy_network: networks_lib.FeedForwardNetwork\n  critic_network: networks_lib.FeedForwardNetwork\n  log_prob: networks_lib.LogProbFn\n  sample: Optional[networks_lib.SampleFn]\n  sample_eval: Optional[networks_lib.SampleFn]\n  environment_specs: specs.EnvironmentSpec",
  "def apply_and_sample_n(key: networks_lib.PRNGKey,\n                       networks: CQLNetworks,\n                       params: networks_lib.Params, obs: jnp.ndarray,\n                       num_samples: int) -> Tuple[jnp.ndarray, jnp.ndarray]:\n  \"\"\"Applies the policy and samples num_samples actions.\"\"\"\n  dist_params = networks.policy_network.apply(params, obs)\n  sampled_actions = jnp.array([\n      networks.sample(dist_params, key_n)\n      for key_n in jax.random.split(key, num_samples)\n  ])\n  sampled_log_probs = networks.log_prob(dist_params, sampled_actions)\n  return sampled_actions, sampled_log_probs",
  "def make_networks(\n    spec: specs.EnvironmentSpec, **kwargs) -> CQLNetworks:\n  sac_networks = sac.make_networks(spec, **kwargs)\n  return CQLNetworks(\n      policy_network=sac_networks.policy_network,\n      critic_network=sac_networks.q_network,\n      log_prob=sac_networks.log_prob,\n      sample=sac_networks.sample,\n      sample_eval=sac_networks.sample_eval,\n      environment_specs=spec)",
  "class CQLConfig:\n  \"\"\"Configuration options for CQL.\n\n  Attributes:\n    batch_size: batch size.\n    policy_learning_rate: learning rate for the policy optimizer.\n    critic_learning_rate: learning rate for the Q-function optimizer.\n    tau: Target smoothing coefficient.\n    fixed_cql_coefficient: the value for cql coefficient. If None an adaptive\n      coefficient will be used.\n    cql_lagrange_threshold: a threshold that controls the adaptive loss for the\n      cql coefficient.\n    cql_num_samples: number of samples used to compute logsumexp(Q) via\n      importance sampling.\n    num_sgd_steps_per_step: how many gradient updates to perform per batch.\n      Batch is split into this many smaller batches thus should be a multiple of\n      num_sgd_steps_per_step\n    reward_scale: reward scale.\n    discount: discount to use for TD updates.\n    fixed_entropy_coefficient: coefficient applied to the entropy bonus. If None\n      an adaptative coefficient will be used.\n    target_entropy: target entropy when using adapdative entropy bonus.\n    num_bc_iters: number of BC steps for actor initialization.\n  \"\"\"\n  batch_size: int = 256\n  policy_learning_rate: float = 3e-5\n  critic_learning_rate: float = 3e-4\n  fixed_cql_coefficient: float = 5.\n  tau: float = 0.005\n  fixed_cql_coefficient: Optional[float] = 5.\n  cql_lagrange_threshold: Optional[float] = None\n  cql_num_samples: int = 10\n  num_sgd_steps_per_step: int = 1\n  reward_scale: float = 1.0\n  discount: float = 0.99\n  fixed_entropy_coefficient: Optional[float] = 0.\n  target_entropy: Optional[float] = 0\n  num_bc_iters: int = 50_000",
  "class CQLBuilder(builders.OfflineBuilder[cql_networks.CQLNetworks,\n                                         actor_core_lib.FeedForwardPolicy,\n                                         types.Transition]):\n  \"\"\"CQL Builder.\"\"\"\n\n  def __init__(\n      self,\n      config: cql_config.CQLConfig,\n  ):\n    \"\"\"Creates a CQL learner, an evaluation policy and an eval actor.\n\n    Args:\n      config: a config with CQL hps.\n    \"\"\"\n    self._config = config\n\n  def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: cql_networks.CQLNetworks,\n      dataset: Iterator[types.Transition],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      *,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    del environment_spec\n\n    return learning.CQLLearner(\n        batch_size=self._config.batch_size,\n        networks=networks,\n        random_key=random_key,\n        demonstrations=dataset,\n        policy_optimizer=optax.adam(self._config.policy_learning_rate),\n        critic_optimizer=optax.adam(self._config.critic_learning_rate),\n        tau=self._config.tau,\n        fixed_cql_coefficient=self._config.fixed_cql_coefficient,\n        cql_lagrange_threshold=self._config.cql_lagrange_threshold,\n        cql_num_samples=self._config.cql_num_samples,\n        num_sgd_steps_per_step=self._config.num_sgd_steps_per_step,\n        reward_scale=self._config.reward_scale,\n        discount=self._config.discount,\n        fixed_entropy_coefficient=self._config.fixed_entropy_coefficient,\n        target_entropy=self._config.target_entropy,\n        num_bc_iters=self._config.num_bc_iters,\n        logger=logger_fn('learner'),\n        counter=counter)\n\n  def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: actor_core_lib.FeedForwardPolicy,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n  ) -> core.Actor:\n    del environment_spec\n    assert variable_source is not None\n    actor_core = actor_core_lib.batched_feed_forward_to_actor_core(policy)\n    variable_client = variable_utils.VariableClient(\n        variable_source, 'policy', device='cpu')\n    return actors.GenericActor(\n        actor_core, random_key, variable_client, backend='cpu')\n\n  def make_policy(self, networks: cql_networks.CQLNetworks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool) -> actor_core_lib.FeedForwardPolicy:\n    \"\"\"Construct the policy.\"\"\"\n    del environment_spec, evaluation\n\n    def evaluation_policy(\n        params: networks_lib.Params, key: networks_lib.PRNGKey,\n        observation: networks_lib.Observation) -> networks_lib.Action:\n      dist_params = networks.policy_network.apply(params, observation)\n      return networks.sample_eval(dist_params, key)\n\n    return evaluation_policy",
  "def __init__(\n      self,\n      config: cql_config.CQLConfig,\n  ):\n    \"\"\"Creates a CQL learner, an evaluation policy and an eval actor.\n\n    Args:\n      config: a config with CQL hps.\n    \"\"\"\n    self._config = config",
  "def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: cql_networks.CQLNetworks,\n      dataset: Iterator[types.Transition],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      *,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    del environment_spec\n\n    return learning.CQLLearner(\n        batch_size=self._config.batch_size,\n        networks=networks,\n        random_key=random_key,\n        demonstrations=dataset,\n        policy_optimizer=optax.adam(self._config.policy_learning_rate),\n        critic_optimizer=optax.adam(self._config.critic_learning_rate),\n        tau=self._config.tau,\n        fixed_cql_coefficient=self._config.fixed_cql_coefficient,\n        cql_lagrange_threshold=self._config.cql_lagrange_threshold,\n        cql_num_samples=self._config.cql_num_samples,\n        num_sgd_steps_per_step=self._config.num_sgd_steps_per_step,\n        reward_scale=self._config.reward_scale,\n        discount=self._config.discount,\n        fixed_entropy_coefficient=self._config.fixed_entropy_coefficient,\n        target_entropy=self._config.target_entropy,\n        num_bc_iters=self._config.num_bc_iters,\n        logger=logger_fn('learner'),\n        counter=counter)",
  "def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: actor_core_lib.FeedForwardPolicy,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n  ) -> core.Actor:\n    del environment_spec\n    assert variable_source is not None\n    actor_core = actor_core_lib.batched_feed_forward_to_actor_core(policy)\n    variable_client = variable_utils.VariableClient(\n        variable_source, 'policy', device='cpu')\n    return actors.GenericActor(\n        actor_core, random_key, variable_client, backend='cpu')",
  "def make_policy(self, networks: cql_networks.CQLNetworks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool) -> actor_core_lib.FeedForwardPolicy:\n    \"\"\"Construct the policy.\"\"\"\n    del environment_spec, evaluation\n\n    def evaluation_policy(\n        params: networks_lib.Params, key: networks_lib.PRNGKey,\n        observation: networks_lib.Observation) -> networks_lib.Action:\n      dist_params = networks.policy_network.apply(params, observation)\n      return networks.sample_eval(dist_params, key)\n\n    return evaluation_policy",
  "def evaluation_policy(\n        params: networks_lib.Params, key: networks_lib.PRNGKey,\n        observation: networks_lib.Observation) -> networks_lib.Action:\n      dist_params = networks.policy_network.apply(params, observation)\n      return networks.sample_eval(dist_params, key)",
  "class CQLTest(absltest.TestCase):\n\n  def test_train(self):\n    seed = 0\n    num_iterations = 6\n    batch_size = 64\n\n    # Create a fake environment to test with.\n    environment = fakes.ContinuousEnvironment(\n        episode_length=10, bounded=True, action_dim=6)\n    spec = specs.make_environment_spec(environment)\n\n    # Construct the agent.\n    networks = cql.make_networks(\n        spec, hidden_layer_sizes=(8, 8))\n    dataset = fakes.transition_iterator(environment)\n    key = jax.random.PRNGKey(seed)\n    learner = cql.CQLLearner(\n        batch_size,\n        networks,\n        key,\n        demonstrations=dataset(batch_size),\n        policy_optimizer=optax.adam(3e-5),\n        critic_optimizer=optax.adam(3e-4),\n        fixed_cql_coefficient=5.,\n        cql_lagrange_threshold=None,\n        target_entropy=0.1,\n        num_bc_iters=2,\n        num_sgd_steps_per_step=1)\n\n    # Train the agent\n    for _ in range(num_iterations):\n      learner.step()",
  "def test_train(self):\n    seed = 0\n    num_iterations = 6\n    batch_size = 64\n\n    # Create a fake environment to test with.\n    environment = fakes.ContinuousEnvironment(\n        episode_length=10, bounded=True, action_dim=6)\n    spec = specs.make_environment_spec(environment)\n\n    # Construct the agent.\n    networks = cql.make_networks(\n        spec, hidden_layer_sizes=(8, 8))\n    dataset = fakes.transition_iterator(environment)\n    key = jax.random.PRNGKey(seed)\n    learner = cql.CQLLearner(\n        batch_size,\n        networks,\n        key,\n        demonstrations=dataset(batch_size),\n        policy_optimizer=optax.adam(3e-5),\n        critic_optimizer=optax.adam(3e-4),\n        fixed_cql_coefficient=5.,\n        cql_lagrange_threshold=None,\n        target_entropy=0.1,\n        num_bc_iters=2,\n        num_sgd_steps_per_step=1)\n\n    # Train the agent\n    for _ in range(num_iterations):\n      learner.step()",
  "def _compute_advantage(networks: CRRNetworks,\n                       policy_params: networks_lib.Params,\n                       critic_params: networks_lib.Params,\n                       transition: types.Transition,\n                       key: networks_lib.PRNGKey,\n                       num_action_samples: int = 4) -> jnp.ndarray:\n  \"\"\"Returns the advantage for the transition.\"\"\"\n  # Sample count actions.\n  replicated_observation = jnp.broadcast_to(transition.observation,\n                                            (num_action_samples,) +\n                                            transition.observation.shape)\n  dist_params = networks.policy_network.apply(policy_params,\n                                              replicated_observation)\n  actions = networks.sample(dist_params, key)\n  # Compute the state-action values for the sampled actions.\n  q_actions = networks.critic_network.apply(critic_params,\n                                            replicated_observation, actions)\n  # Take the mean as the state-value estimate. It is also possible to take the\n  # maximum, aka CRR(max); see table 1 in CRR paper.\n  q_estimate = jnp.mean(q_actions, axis=0)\n  # Compute the advantage.\n  q = networks.critic_network.apply(critic_params, transition.observation,\n                                    transition.action)\n  return q - q_estimate",
  "def policy_loss_coeff_advantage_exp(\n    networks: CRRNetworks,\n    policy_params: networks_lib.Params,\n    critic_params: networks_lib.Params,\n    transition: types.Transition,\n    key: networks_lib.PRNGKey,\n    num_action_samples: int = 4,\n    beta: float = 1.0,\n    ratio_upper_bound: float = 20.0) -> jnp.ndarray:\n  \"\"\"Exponential advantage weigting; see equation (4) in CRR paper.\"\"\"\n  advantage = _compute_advantage(networks, policy_params, critic_params,\n                                 transition, key, num_action_samples)\n  return jnp.minimum(jnp.exp(advantage / beta), ratio_upper_bound)",
  "def policy_loss_coeff_advantage_indicator(\n    networks: CRRNetworks,\n    policy_params: networks_lib.Params,\n    critic_params: networks_lib.Params,\n    transition: types.Transition,\n    key: networks_lib.PRNGKey,\n    num_action_samples: int = 4) -> jnp.ndarray:\n  \"\"\"Indicator advantage weighting; see equation (3) in CRR paper.\"\"\"\n  advantage = _compute_advantage(networks, policy_params, critic_params,\n                                 transition, key, num_action_samples)\n  return jnp.heaviside(advantage, 0.)",
  "def policy_loss_coeff_constant(networks: CRRNetworks,\n                               policy_params: networks_lib.Params,\n                               critic_params: networks_lib.Params,\n                               transition: types.Transition,\n                               key: networks_lib.PRNGKey,\n                               value: float = 1.0) -> jnp.ndarray:\n  \"\"\"Constant weights.\"\"\"\n  del networks\n  del policy_params\n  del critic_params\n  del transition\n  del key\n  return value",
  "class TrainingState(NamedTuple):\n  \"\"\"Contains training state for the learner.\"\"\"\n  policy_params: networks_lib.Params\n  target_policy_params: networks_lib.Params\n  critic_params: networks_lib.Params\n  target_critic_params: networks_lib.Params\n  policy_opt_state: optax.OptState\n  critic_opt_state: optax.OptState\n  steps: int\n  key: networks_lib.PRNGKey",
  "class CRRLearner(acme.Learner):\n  \"\"\"Critic Regularized Regression (CRR) learner.\n\n  This is the learning component of a CRR agent as described in\n  https://arxiv.org/abs/2006.15134.\n  \"\"\"\n\n  _state: TrainingState\n\n  def __init__(self,\n               networks: CRRNetworks,\n               random_key: networks_lib.PRNGKey,\n               discount: float,\n               target_update_period: int,\n               policy_loss_coeff_fn: PolicyLossCoeff,\n               iterator: Iterator[types.Transition],\n               policy_optimizer: optax.GradientTransformation,\n               critic_optimizer: optax.GradientTransformation,\n               counter: Optional[counting.Counter] = None,\n               logger: Optional[loggers.Logger] = None,\n               grad_updates_per_batch: int = 1,\n               use_sarsa_target: bool = False):\n    \"\"\"Initializes the CRR learner.\n\n    Args:\n      networks: CRR networks.\n      random_key: a key for random number generation.\n      discount: discount to use for TD updates.\n      target_update_period: period to update target's parameters.\n      policy_loss_coeff_fn: set the loss function for the policy.\n      iterator: an iterator over training data.\n      policy_optimizer: the policy optimizer.\n      critic_optimizer: the Q-function optimizer.\n      counter: counter object used to keep track of steps.\n      logger: logger object to be used by learner.\n      grad_updates_per_batch: how many gradient updates given a sampled batch.\n      use_sarsa_target: compute on-policy target using iterator's actions rather\n        than sampled actions.\n        Useful for 1-step offline RL (https://arxiv.org/pdf/2106.08909.pdf).\n        When set to `True`, `target_policy_params` are unused.\n    \"\"\"\n\n    critic_network = networks.critic_network\n    policy_network = networks.policy_network\n\n    def policy_loss(\n        policy_params: networks_lib.Params,\n        critic_params: networks_lib.Params,\n        transition: types.Transition,\n        key: networks_lib.PRNGKey,\n    ) -> jnp.ndarray:\n      # Compute the loss coefficients.\n      coeff = policy_loss_coeff_fn(networks, policy_params, critic_params,\n                                   transition, key)\n      coeff = jax.lax.stop_gradient(coeff)\n      # Return the weighted loss.\n      dist_params = policy_network.apply(policy_params, transition.observation)\n      logp_action = networks.log_prob(dist_params, transition.action)\n      # Make sure there is no broadcasting.\n      logp_action *= coeff.flatten()\n      assert len(logp_action.shape) == 1\n      return -jnp.mean(logp_action)\n\n    def critic_loss(\n        critic_params: networks_lib.Params,\n        target_policy_params: networks_lib.Params,\n        target_critic_params: networks_lib.Params,\n        transition: types.Transition,\n        key: networks_lib.PRNGKey,\n    ):\n      # Sample the next action.\n      if use_sarsa_target:\n        # TODO(b/222674779): use N-steps Trajectories to get the next actions.\n        assert 'next_action' in transition.extras, (\n            'next actions should be given as extras for one step RL.')\n        next_action = transition.extras['next_action']\n      else:\n        next_dist_params = policy_network.apply(target_policy_params,\n                                                transition.next_observation)\n        next_action = networks.sample(next_dist_params, key)\n      # Calculate the value of the next state and action.\n      next_q = critic_network.apply(target_critic_params,\n                                    transition.next_observation, next_action)\n      target_q = transition.reward + transition.discount * discount * next_q\n      target_q = jax.lax.stop_gradient(target_q)\n\n      q = critic_network.apply(critic_params, transition.observation,\n                               transition.action)\n      q_error = q - target_q\n      # Loss is MSE scaled by 0.5, so the gradient is equal to the TD error.\n      # TODO(sertan): Replace with a distributional critic. CRR paper states\n      # that this may perform better.\n      return 0.5 * jnp.mean(jnp.square(q_error))\n\n    policy_loss_and_grad = jax.value_and_grad(policy_loss)\n    critic_loss_and_grad = jax.value_and_grad(critic_loss)\n\n    def sgd_step(\n        state: TrainingState,\n        transitions: types.Transition,\n    ) -> Tuple[TrainingState, Dict[str, jnp.ndarray]]:\n\n      key, key_policy, key_critic = jax.random.split(state.key, 3)\n\n      # Compute losses and their gradients.\n      policy_loss_value, policy_gradients = policy_loss_and_grad(\n          state.policy_params, state.critic_params, transitions, key_policy)\n      critic_loss_value, critic_gradients = critic_loss_and_grad(\n          state.critic_params, state.target_policy_params,\n          state.target_critic_params, transitions, key_critic)\n\n      # Get optimizer updates and state.\n      policy_updates, policy_opt_state = policy_optimizer.update(\n          policy_gradients, state.policy_opt_state)\n      critic_updates, critic_opt_state = critic_optimizer.update(\n          critic_gradients, state.critic_opt_state)\n\n      # Apply optimizer updates to parameters.\n      policy_params = optax.apply_updates(state.policy_params, policy_updates)\n      critic_params = optax.apply_updates(state.critic_params, critic_updates)\n\n      steps = state.steps + 1\n\n      # Periodically update target networks.\n      target_policy_params, target_critic_params = optax.periodic_update(  # pytype: disable=wrong-arg-types  # numpy-scalars\n          (policy_params, critic_params),\n          (state.target_policy_params, state.target_critic_params), steps,\n          target_update_period)\n\n      new_state = TrainingState(\n          policy_params=policy_params,\n          target_policy_params=target_policy_params,\n          critic_params=critic_params,\n          target_critic_params=target_critic_params,\n          policy_opt_state=policy_opt_state,\n          critic_opt_state=critic_opt_state,\n          steps=steps,\n          key=key,\n      )\n\n      metrics = {\n          'policy_loss': policy_loss_value,\n          'critic_loss': critic_loss_value,\n      }\n\n      return new_state, metrics\n\n    sgd_step = utils.process_multiple_batches(sgd_step, grad_updates_per_batch)\n    self._sgd_step = jax.jit(sgd_step)\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger(\n        'learner',\n        asynchronous=True,\n        serialize_fn=utils.fetch_devicearray,\n        steps_key=self._counter.get_steps_key())\n\n    # Create prefetching dataset iterator.\n    self._iterator = iterator\n\n    # Create the network parameters and copy into the target network parameters.\n    key, key_policy, key_critic = jax.random.split(random_key, 3)\n    initial_policy_params = policy_network.init(key_policy)\n    initial_critic_params = critic_network.init(key_critic)\n    initial_target_policy_params = initial_policy_params\n    initial_target_critic_params = initial_critic_params\n\n    # Initialize optimizers.\n    initial_policy_opt_state = policy_optimizer.init(initial_policy_params)\n    initial_critic_opt_state = critic_optimizer.init(initial_critic_params)\n\n    # Create initial state.\n    self._state = TrainingState(\n        policy_params=initial_policy_params,\n        target_policy_params=initial_target_policy_params,\n        critic_params=initial_critic_params,\n        target_critic_params=initial_target_critic_params,\n        policy_opt_state=initial_policy_opt_state,\n        critic_opt_state=initial_critic_opt_state,\n        steps=0,\n        key=key,\n    )\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None\n\n  def step(self):\n    transitions = next(self._iterator)\n\n    self._state, metrics = self._sgd_step(self._state, transitions)\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Increment counts and record the current time\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n\n    # Attempts to write the logs.\n    self._logger.write({**metrics, **counts})\n\n  def get_variables(self, names: List[str]) -> List[networks_lib.Params]:\n    # We only expose the variables for the learned policy and critic. The target\n    # policy and critic are internal details.\n    variables = {\n        'policy': self._state.target_policy_params,\n        'critic': self._state.target_critic_params,\n    }\n    return [variables[name] for name in names]\n\n  def save(self) -> TrainingState:\n    return self._state\n\n  def restore(self, state: TrainingState):\n    self._state = state",
  "def __init__(self,\n               networks: CRRNetworks,\n               random_key: networks_lib.PRNGKey,\n               discount: float,\n               target_update_period: int,\n               policy_loss_coeff_fn: PolicyLossCoeff,\n               iterator: Iterator[types.Transition],\n               policy_optimizer: optax.GradientTransformation,\n               critic_optimizer: optax.GradientTransformation,\n               counter: Optional[counting.Counter] = None,\n               logger: Optional[loggers.Logger] = None,\n               grad_updates_per_batch: int = 1,\n               use_sarsa_target: bool = False):\n    \"\"\"Initializes the CRR learner.\n\n    Args:\n      networks: CRR networks.\n      random_key: a key for random number generation.\n      discount: discount to use for TD updates.\n      target_update_period: period to update target's parameters.\n      policy_loss_coeff_fn: set the loss function for the policy.\n      iterator: an iterator over training data.\n      policy_optimizer: the policy optimizer.\n      critic_optimizer: the Q-function optimizer.\n      counter: counter object used to keep track of steps.\n      logger: logger object to be used by learner.\n      grad_updates_per_batch: how many gradient updates given a sampled batch.\n      use_sarsa_target: compute on-policy target using iterator's actions rather\n        than sampled actions.\n        Useful for 1-step offline RL (https://arxiv.org/pdf/2106.08909.pdf).\n        When set to `True`, `target_policy_params` are unused.\n    \"\"\"\n\n    critic_network = networks.critic_network\n    policy_network = networks.policy_network\n\n    def policy_loss(\n        policy_params: networks_lib.Params,\n        critic_params: networks_lib.Params,\n        transition: types.Transition,\n        key: networks_lib.PRNGKey,\n    ) -> jnp.ndarray:\n      # Compute the loss coefficients.\n      coeff = policy_loss_coeff_fn(networks, policy_params, critic_params,\n                                   transition, key)\n      coeff = jax.lax.stop_gradient(coeff)\n      # Return the weighted loss.\n      dist_params = policy_network.apply(policy_params, transition.observation)\n      logp_action = networks.log_prob(dist_params, transition.action)\n      # Make sure there is no broadcasting.\n      logp_action *= coeff.flatten()\n      assert len(logp_action.shape) == 1\n      return -jnp.mean(logp_action)\n\n    def critic_loss(\n        critic_params: networks_lib.Params,\n        target_policy_params: networks_lib.Params,\n        target_critic_params: networks_lib.Params,\n        transition: types.Transition,\n        key: networks_lib.PRNGKey,\n    ):\n      # Sample the next action.\n      if use_sarsa_target:\n        # TODO(b/222674779): use N-steps Trajectories to get the next actions.\n        assert 'next_action' in transition.extras, (\n            'next actions should be given as extras for one step RL.')\n        next_action = transition.extras['next_action']\n      else:\n        next_dist_params = policy_network.apply(target_policy_params,\n                                                transition.next_observation)\n        next_action = networks.sample(next_dist_params, key)\n      # Calculate the value of the next state and action.\n      next_q = critic_network.apply(target_critic_params,\n                                    transition.next_observation, next_action)\n      target_q = transition.reward + transition.discount * discount * next_q\n      target_q = jax.lax.stop_gradient(target_q)\n\n      q = critic_network.apply(critic_params, transition.observation,\n                               transition.action)\n      q_error = q - target_q\n      # Loss is MSE scaled by 0.5, so the gradient is equal to the TD error.\n      # TODO(sertan): Replace with a distributional critic. CRR paper states\n      # that this may perform better.\n      return 0.5 * jnp.mean(jnp.square(q_error))\n\n    policy_loss_and_grad = jax.value_and_grad(policy_loss)\n    critic_loss_and_grad = jax.value_and_grad(critic_loss)\n\n    def sgd_step(\n        state: TrainingState,\n        transitions: types.Transition,\n    ) -> Tuple[TrainingState, Dict[str, jnp.ndarray]]:\n\n      key, key_policy, key_critic = jax.random.split(state.key, 3)\n\n      # Compute losses and their gradients.\n      policy_loss_value, policy_gradients = policy_loss_and_grad(\n          state.policy_params, state.critic_params, transitions, key_policy)\n      critic_loss_value, critic_gradients = critic_loss_and_grad(\n          state.critic_params, state.target_policy_params,\n          state.target_critic_params, transitions, key_critic)\n\n      # Get optimizer updates and state.\n      policy_updates, policy_opt_state = policy_optimizer.update(\n          policy_gradients, state.policy_opt_state)\n      critic_updates, critic_opt_state = critic_optimizer.update(\n          critic_gradients, state.critic_opt_state)\n\n      # Apply optimizer updates to parameters.\n      policy_params = optax.apply_updates(state.policy_params, policy_updates)\n      critic_params = optax.apply_updates(state.critic_params, critic_updates)\n\n      steps = state.steps + 1\n\n      # Periodically update target networks.\n      target_policy_params, target_critic_params = optax.periodic_update(  # pytype: disable=wrong-arg-types  # numpy-scalars\n          (policy_params, critic_params),\n          (state.target_policy_params, state.target_critic_params), steps,\n          target_update_period)\n\n      new_state = TrainingState(\n          policy_params=policy_params,\n          target_policy_params=target_policy_params,\n          critic_params=critic_params,\n          target_critic_params=target_critic_params,\n          policy_opt_state=policy_opt_state,\n          critic_opt_state=critic_opt_state,\n          steps=steps,\n          key=key,\n      )\n\n      metrics = {\n          'policy_loss': policy_loss_value,\n          'critic_loss': critic_loss_value,\n      }\n\n      return new_state, metrics\n\n    sgd_step = utils.process_multiple_batches(sgd_step, grad_updates_per_batch)\n    self._sgd_step = jax.jit(sgd_step)\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger(\n        'learner',\n        asynchronous=True,\n        serialize_fn=utils.fetch_devicearray,\n        steps_key=self._counter.get_steps_key())\n\n    # Create prefetching dataset iterator.\n    self._iterator = iterator\n\n    # Create the network parameters and copy into the target network parameters.\n    key, key_policy, key_critic = jax.random.split(random_key, 3)\n    initial_policy_params = policy_network.init(key_policy)\n    initial_critic_params = critic_network.init(key_critic)\n    initial_target_policy_params = initial_policy_params\n    initial_target_critic_params = initial_critic_params\n\n    # Initialize optimizers.\n    initial_policy_opt_state = policy_optimizer.init(initial_policy_params)\n    initial_critic_opt_state = critic_optimizer.init(initial_critic_params)\n\n    # Create initial state.\n    self._state = TrainingState(\n        policy_params=initial_policy_params,\n        target_policy_params=initial_target_policy_params,\n        critic_params=initial_critic_params,\n        target_critic_params=initial_target_critic_params,\n        policy_opt_state=initial_policy_opt_state,\n        critic_opt_state=initial_critic_opt_state,\n        steps=0,\n        key=key,\n    )\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None",
  "def step(self):\n    transitions = next(self._iterator)\n\n    self._state, metrics = self._sgd_step(self._state, transitions)\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Increment counts and record the current time\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n\n    # Attempts to write the logs.\n    self._logger.write({**metrics, **counts})",
  "def get_variables(self, names: List[str]) -> List[networks_lib.Params]:\n    # We only expose the variables for the learned policy and critic. The target\n    # policy and critic are internal details.\n    variables = {\n        'policy': self._state.target_policy_params,\n        'critic': self._state.target_critic_params,\n    }\n    return [variables[name] for name in names]",
  "def save(self) -> TrainingState:\n    return self._state",
  "def restore(self, state: TrainingState):\n    self._state = state",
  "def policy_loss(\n        policy_params: networks_lib.Params,\n        critic_params: networks_lib.Params,\n        transition: types.Transition,\n        key: networks_lib.PRNGKey,\n    ) -> jnp.ndarray:\n      # Compute the loss coefficients.\n      coeff = policy_loss_coeff_fn(networks, policy_params, critic_params,\n                                   transition, key)\n      coeff = jax.lax.stop_gradient(coeff)\n      # Return the weighted loss.\n      dist_params = policy_network.apply(policy_params, transition.observation)\n      logp_action = networks.log_prob(dist_params, transition.action)\n      # Make sure there is no broadcasting.\n      logp_action *= coeff.flatten()\n      assert len(logp_action.shape) == 1\n      return -jnp.mean(logp_action)",
  "def critic_loss(\n        critic_params: networks_lib.Params,\n        target_policy_params: networks_lib.Params,\n        target_critic_params: networks_lib.Params,\n        transition: types.Transition,\n        key: networks_lib.PRNGKey,\n    ):\n      # Sample the next action.\n      if use_sarsa_target:\n        # TODO(b/222674779): use N-steps Trajectories to get the next actions.\n        assert 'next_action' in transition.extras, (\n            'next actions should be given as extras for one step RL.')\n        next_action = transition.extras['next_action']\n      else:\n        next_dist_params = policy_network.apply(target_policy_params,\n                                                transition.next_observation)\n        next_action = networks.sample(next_dist_params, key)\n      # Calculate the value of the next state and action.\n      next_q = critic_network.apply(target_critic_params,\n                                    transition.next_observation, next_action)\n      target_q = transition.reward + transition.discount * discount * next_q\n      target_q = jax.lax.stop_gradient(target_q)\n\n      q = critic_network.apply(critic_params, transition.observation,\n                               transition.action)\n      q_error = q - target_q\n      # Loss is MSE scaled by 0.5, so the gradient is equal to the TD error.\n      # TODO(sertan): Replace with a distributional critic. CRR paper states\n      # that this may perform better.\n      return 0.5 * jnp.mean(jnp.square(q_error))",
  "def sgd_step(\n        state: TrainingState,\n        transitions: types.Transition,\n    ) -> Tuple[TrainingState, Dict[str, jnp.ndarray]]:\n\n      key, key_policy, key_critic = jax.random.split(state.key, 3)\n\n      # Compute losses and their gradients.\n      policy_loss_value, policy_gradients = policy_loss_and_grad(\n          state.policy_params, state.critic_params, transitions, key_policy)\n      critic_loss_value, critic_gradients = critic_loss_and_grad(\n          state.critic_params, state.target_policy_params,\n          state.target_critic_params, transitions, key_critic)\n\n      # Get optimizer updates and state.\n      policy_updates, policy_opt_state = policy_optimizer.update(\n          policy_gradients, state.policy_opt_state)\n      critic_updates, critic_opt_state = critic_optimizer.update(\n          critic_gradients, state.critic_opt_state)\n\n      # Apply optimizer updates to parameters.\n      policy_params = optax.apply_updates(state.policy_params, policy_updates)\n      critic_params = optax.apply_updates(state.critic_params, critic_updates)\n\n      steps = state.steps + 1\n\n      # Periodically update target networks.\n      target_policy_params, target_critic_params = optax.periodic_update(  # pytype: disable=wrong-arg-types  # numpy-scalars\n          (policy_params, critic_params),\n          (state.target_policy_params, state.target_critic_params), steps,\n          target_update_period)\n\n      new_state = TrainingState(\n          policy_params=policy_params,\n          target_policy_params=target_policy_params,\n          critic_params=critic_params,\n          target_critic_params=target_critic_params,\n          policy_opt_state=policy_opt_state,\n          critic_opt_state=critic_opt_state,\n          steps=steps,\n          key=key,\n      )\n\n      metrics = {\n          'policy_loss': policy_loss_value,\n          'critic_loss': critic_loss_value,\n      }\n\n      return new_state, metrics",
  "class CRRNetworks:\n  \"\"\"Network and pure functions for the CRR agent..\"\"\"\n  policy_network: networks_lib.FeedForwardNetwork\n  critic_network: networks_lib.FeedForwardNetwork\n  log_prob: networks_lib.LogProbFn\n  sample: networks_lib.SampleFn\n  sample_eval: networks_lib.SampleFn",
  "def make_networks(\n    spec: specs.EnvironmentSpec,\n    policy_layer_sizes: Tuple[int, ...] = (256, 256),\n    critic_layer_sizes: Tuple[int, ...] = (256, 256),\n    activation: Callable[[jnp.ndarray], jnp.ndarray] = jax.nn.relu,\n) -> CRRNetworks:\n  \"\"\"Creates networks used by the agent.\"\"\"\n  num_actions = np.prod(spec.actions.shape, dtype=int)\n\n  # Create dummy observations and actions to create network parameters.\n  dummy_action = utils.add_batch_dim(utils.zeros_like(spec.actions))\n  dummy_obs = utils.add_batch_dim(utils.zeros_like(spec.observations))\n\n  def _policy_fn(obs: jnp.ndarray) -> jnp.ndarray:\n    network = hk.Sequential([\n        hk.nets.MLP(\n            list(policy_layer_sizes),\n            w_init=hk.initializers.VarianceScaling(1.0, 'fan_in', 'uniform'),\n            activation=activation,\n            activate_final=True),\n        networks_lib.NormalTanhDistribution(num_actions),\n    ])\n    return network(obs)\n\n  policy = hk.without_apply_rng(hk.transform(_policy_fn))\n  policy_network = networks_lib.FeedForwardNetwork(\n      lambda key: policy.init(key, dummy_obs), policy.apply)\n\n  def _critic_fn(obs, action):\n    network = hk.Sequential([\n        hk.nets.MLP(\n            list(critic_layer_sizes) + [1],\n            w_init=hk.initializers.VarianceScaling(1.0, 'fan_in', 'uniform'),\n            activation=activation),\n    ])\n    data = jnp.concatenate([obs, action], axis=-1)\n    return network(data)\n\n  critic = hk.without_apply_rng(hk.transform(_critic_fn))\n  critic_network = networks_lib.FeedForwardNetwork(\n      lambda key: critic.init(key, dummy_obs, dummy_action), critic.apply)\n\n  return CRRNetworks(\n      policy_network=policy_network,\n      critic_network=critic_network,\n      log_prob=lambda params, actions: params.log_prob(actions),\n      sample=lambda params, key: params.sample(seed=key),\n      sample_eval=lambda params, key: params.mode())",
  "def _policy_fn(obs: jnp.ndarray) -> jnp.ndarray:\n    network = hk.Sequential([\n        hk.nets.MLP(\n            list(policy_layer_sizes),\n            w_init=hk.initializers.VarianceScaling(1.0, 'fan_in', 'uniform'),\n            activation=activation,\n            activate_final=True),\n        networks_lib.NormalTanhDistribution(num_actions),\n    ])\n    return network(obs)",
  "def _critic_fn(obs, action):\n    network = hk.Sequential([\n        hk.nets.MLP(\n            list(critic_layer_sizes) + [1],\n            w_init=hk.initializers.VarianceScaling(1.0, 'fan_in', 'uniform'),\n            activation=activation),\n    ])\n    data = jnp.concatenate([obs, action], axis=-1)\n    return network(data)",
  "class CRRConfig:\n  \"\"\"Configuration options for CRR.\n\n  Attributes:\n    learning_rate: Learning rate.\n    discount: discount to use for TD updates.\n    target_update_period: period to update target's parameters.\n    use_sarsa_target: compute on-policy target using iterator's actions rather\n      than sampled actions.\n      Useful for 1-step offline RL (https://arxiv.org/pdf/2106.08909.pdf).\n  \"\"\"\n  learning_rate: float = 3e-4\n  discount: float = 0.99\n  target_update_period: int = 100\n  use_sarsa_target: bool = False",
  "class CRRBuilder(builders.OfflineBuilder[crr_networks.CRRNetworks,\n                                         actor_core_lib.FeedForwardPolicy,\n                                         types.Transition]):\n  \"\"\"CRR Builder.\"\"\"\n\n  def __init__(\n      self,\n      config: crr_config.CRRConfig,\n      policy_loss_coeff_fn: losses.PolicyLossCoeff,\n  ):\n    \"\"\"Creates a CRR learner, an evaluation policy and an eval actor.\n\n    Args:\n      config: a config with CRR hps.\n      policy_loss_coeff_fn: set the loss function for the policy.\n    \"\"\"\n    self._config = config\n    self._policy_loss_coeff_fn = policy_loss_coeff_fn\n\n  def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: crr_networks.CRRNetworks,\n      dataset: Iterator[types.Transition],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      *,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    del environment_spec\n\n    return learning.CRRLearner(\n        networks=networks,\n        random_key=random_key,\n        discount=self._config.discount,\n        target_update_period=self._config.target_update_period,\n        iterator=dataset,\n        policy_loss_coeff_fn=self._policy_loss_coeff_fn,\n        policy_optimizer=optax.adam(self._config.learning_rate),\n        critic_optimizer=optax.adam(self._config.learning_rate),\n        use_sarsa_target=self._config.use_sarsa_target,\n        logger=logger_fn('learner'),\n        counter=counter)\n\n  def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: actor_core_lib.FeedForwardPolicy,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n  ) -> core.Actor:\n    del environment_spec\n    assert variable_source is not None\n    actor_core = actor_core_lib.batched_feed_forward_to_actor_core(policy)\n    variable_client = variable_utils.VariableClient(\n        variable_source, 'policy', device='cpu')\n    return actors.GenericActor(\n        actor_core, random_key, variable_client, backend='cpu')\n\n  def make_policy(self, networks: crr_networks.CRRNetworks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool) -> actor_core_lib.FeedForwardPolicy:\n    \"\"\"Construct the policy.\"\"\"\n    del environment_spec, evaluation\n\n    def evaluation_policy(\n        params: networks_lib.Params, key: networks_lib.PRNGKey,\n        observation: networks_lib.Observation) -> networks_lib.Action:\n      dist_params = networks.policy_network.apply(params, observation)\n      return networks.sample_eval(dist_params, key)\n\n    return evaluation_policy",
  "def __init__(\n      self,\n      config: crr_config.CRRConfig,\n      policy_loss_coeff_fn: losses.PolicyLossCoeff,\n  ):\n    \"\"\"Creates a CRR learner, an evaluation policy and an eval actor.\n\n    Args:\n      config: a config with CRR hps.\n      policy_loss_coeff_fn: set the loss function for the policy.\n    \"\"\"\n    self._config = config\n    self._policy_loss_coeff_fn = policy_loss_coeff_fn",
  "def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: crr_networks.CRRNetworks,\n      dataset: Iterator[types.Transition],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      *,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    del environment_spec\n\n    return learning.CRRLearner(\n        networks=networks,\n        random_key=random_key,\n        discount=self._config.discount,\n        target_update_period=self._config.target_update_period,\n        iterator=dataset,\n        policy_loss_coeff_fn=self._policy_loss_coeff_fn,\n        policy_optimizer=optax.adam(self._config.learning_rate),\n        critic_optimizer=optax.adam(self._config.learning_rate),\n        use_sarsa_target=self._config.use_sarsa_target,\n        logger=logger_fn('learner'),\n        counter=counter)",
  "def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: actor_core_lib.FeedForwardPolicy,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n  ) -> core.Actor:\n    del environment_spec\n    assert variable_source is not None\n    actor_core = actor_core_lib.batched_feed_forward_to_actor_core(policy)\n    variable_client = variable_utils.VariableClient(\n        variable_source, 'policy', device='cpu')\n    return actors.GenericActor(\n        actor_core, random_key, variable_client, backend='cpu')",
  "def make_policy(self, networks: crr_networks.CRRNetworks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool) -> actor_core_lib.FeedForwardPolicy:\n    \"\"\"Construct the policy.\"\"\"\n    del environment_spec, evaluation\n\n    def evaluation_policy(\n        params: networks_lib.Params, key: networks_lib.PRNGKey,\n        observation: networks_lib.Observation) -> networks_lib.Action:\n      dist_params = networks.policy_network.apply(params, observation)\n      return networks.sample_eval(dist_params, key)\n\n    return evaluation_policy",
  "def evaluation_policy(\n        params: networks_lib.Params, key: networks_lib.PRNGKey,\n        observation: networks_lib.Observation) -> networks_lib.Action:\n      dist_params = networks.policy_network.apply(params, observation)\n      return networks.sample_eval(dist_params, key)",
  "class CRRTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      ('exp', crr.policy_loss_coeff_advantage_exp),\n      ('indicator', crr.policy_loss_coeff_advantage_indicator),\n      ('all', crr.policy_loss_coeff_constant))\n  def test_train(self, policy_loss_coeff_fn):\n    seed = 0\n    num_iterations = 5\n    batch_size = 64\n    grad_updates_per_batch = 1\n\n    # Create a fake environment to test with.\n    environment = fakes.ContinuousEnvironment(\n        episode_length=10, bounded=True, action_dim=6)\n    spec = specs.make_environment_spec(environment)\n\n    # Construct the learner.\n    networks = crr.make_networks(\n        spec, policy_layer_sizes=(8, 8), critic_layer_sizes=(8, 8))\n    key = jax.random.PRNGKey(seed)\n    dataset = fakes.transition_iterator(environment)\n    learner = crr.CRRLearner(\n        networks,\n        key,\n        discount=0.95,\n        target_update_period=2,\n        policy_loss_coeff_fn=policy_loss_coeff_fn,\n        iterator=dataset(batch_size * grad_updates_per_batch),\n        policy_optimizer=optax.adam(1e-4),\n        critic_optimizer=optax.adam(1e-4),\n        grad_updates_per_batch=grad_updates_per_batch)\n\n    # Train the learner.\n    for _ in range(num_iterations):\n      learner.step()",
  "def test_train(self, policy_loss_coeff_fn):\n    seed = 0\n    num_iterations = 5\n    batch_size = 64\n    grad_updates_per_batch = 1\n\n    # Create a fake environment to test with.\n    environment = fakes.ContinuousEnvironment(\n        episode_length=10, bounded=True, action_dim=6)\n    spec = specs.make_environment_spec(environment)\n\n    # Construct the learner.\n    networks = crr.make_networks(\n        spec, policy_layer_sizes=(8, 8), critic_layer_sizes=(8, 8))\n    key = jax.random.PRNGKey(seed)\n    dataset = fakes.transition_iterator(environment)\n    learner = crr.CRRLearner(\n        networks,\n        key,\n        discount=0.95,\n        target_update_period=2,\n        policy_loss_coeff_fn=policy_loss_coeff_fn,\n        iterator=dataset(batch_size * grad_updates_per_batch),\n        policy_optimizer=optax.adam(1e-4),\n        critic_optimizer=optax.adam(1e-4),\n        grad_updates_per_batch=grad_updates_per_batch)\n\n    # Train the learner.\n    for _ in range(num_iterations):\n      learner.step()",
  "class TrainingState(NamedTuple):\n  \"\"\"Contains training state for the learner.\"\"\"\n  policy_params: networks_lib.Params\n  target_policy_params: networks_lib.Params\n  critic_params: networks_lib.Params\n  target_critic_params: networks_lib.Params\n  policy_opt_state: optax.OptState\n  critic_opt_state: optax.OptState\n  steps: int",
  "class D4PGLearner(acme.Learner):\n  \"\"\"D4PG learner.\n\n  This is the learning component of a D4PG agent. IE it takes a dataset as input\n  and implements update functionality to learn from this dataset.\n  \"\"\"\n\n  _state: TrainingState\n\n  def __init__(self,\n               policy_network: networks_lib.FeedForwardNetwork,\n               critic_network: networks_lib.FeedForwardNetwork,\n               random_key: networks_lib.PRNGKey,\n               discount: float,\n               target_update_period: int,\n               iterator: Iterator[reverb.ReplaySample],\n               policy_optimizer: Optional[optax.GradientTransformation] = None,\n               critic_optimizer: Optional[optax.GradientTransformation] = None,\n               clipping: bool = True,\n               counter: Optional[counting.Counter] = None,\n               logger: Optional[loggers.Logger] = None,\n               jit: bool = True,\n               num_sgd_steps_per_step: int = 1):\n\n    def critic_mean(\n        critic_params: networks_lib.Params,\n        observation: types.NestedArray,\n        action: types.NestedArray,\n    ) -> jnp.ndarray:\n      # We add batch dimension to make sure batch concat in critic_network\n      # works correctly.\n      observation = utils.add_batch_dim(observation)\n      action = utils.add_batch_dim(action)\n      # Computes the mean action-value estimate.\n      logits, atoms = critic_network.apply(critic_params, observation, action)\n      logits = utils.squeeze_batch_dim(logits)\n      probabilities = jax.nn.softmax(logits)\n      return jnp.sum(probabilities * atoms, axis=-1)\n\n    def policy_loss(\n        policy_params: networks_lib.Params,\n        critic_params: networks_lib.Params,\n        o_t: types.NestedArray,\n    ) -> jnp.ndarray:\n      # Computes the discrete policy gradient loss.\n      dpg_a_t = policy_network.apply(policy_params, o_t)\n      grad_critic = jax.vmap(\n          jax.grad(critic_mean, argnums=2), in_axes=(None, 0, 0))\n      dq_da = grad_critic(critic_params, o_t, dpg_a_t)\n      dqda_clipping = 1. if clipping else None\n      batch_dpg_learning = jax.vmap(rlax.dpg_loss, in_axes=(0, 0, None))\n      loss = batch_dpg_learning(dpg_a_t, dq_da, dqda_clipping)\n      return jnp.mean(loss)\n\n    def critic_loss(\n        critic_params: networks_lib.Params,\n        state: TrainingState,\n        transition: types.Transition,\n    ):\n      # Computes the distributional critic loss.\n      q_tm1, atoms_tm1 = critic_network.apply(critic_params,\n                                              transition.observation,\n                                              transition.action)\n      a = policy_network.apply(state.target_policy_params,\n                               transition.next_observation)\n      q_t, atoms_t = critic_network.apply(state.target_critic_params,\n                                          transition.next_observation, a)\n      batch_td_learning = jax.vmap(\n          rlax.categorical_td_learning, in_axes=(None, 0, 0, 0, None, 0))\n      loss = batch_td_learning(atoms_tm1, q_tm1, transition.reward,\n                               discount * transition.discount, atoms_t, q_t)\n      return jnp.mean(loss)\n\n    def sgd_step(\n        state: TrainingState,\n        transitions: types.Transition,\n    ) -> Tuple[TrainingState, Dict[str, jnp.ndarray]]:\n\n      # TODO(jaslanides): Use a shared forward pass for efficiency.\n      policy_loss_and_grad = jax.value_and_grad(policy_loss)\n      critic_loss_and_grad = jax.value_and_grad(critic_loss)\n\n      # Compute losses and their gradients.\n      policy_loss_value, policy_gradients = policy_loss_and_grad(\n          state.policy_params, state.critic_params,\n          transitions.next_observation)\n      critic_loss_value, critic_gradients = critic_loss_and_grad(\n          state.critic_params, state, transitions)\n\n      # Average over all devices.\n      policy_loss_value, policy_gradients = jax.lax.pmean(\n          (policy_loss_value, policy_gradients), _PMAP_AXIS_NAME)\n      critic_loss_value, critic_gradients = jax.lax.pmean(\n          (critic_loss_value, critic_gradients), _PMAP_AXIS_NAME)\n\n      # Get optimizer updates and state.\n      policy_updates, policy_opt_state = policy_optimizer.update(  # pytype: disable=attribute-error\n          policy_gradients, state.policy_opt_state)\n      critic_updates, critic_opt_state = critic_optimizer.update(  # pytype: disable=attribute-error\n          critic_gradients, state.critic_opt_state)\n\n      # Apply optimizer updates to parameters.\n      policy_params = optax.apply_updates(state.policy_params, policy_updates)\n      critic_params = optax.apply_updates(state.critic_params, critic_updates)\n\n      steps = state.steps + 1\n\n      # Periodically update target networks.\n      target_policy_params, target_critic_params = optax.periodic_update(  # pytype: disable=wrong-arg-types  # numpy-scalars\n          (policy_params, critic_params),\n          (state.target_policy_params, state.target_critic_params), steps,\n          self._target_update_period)\n\n      new_state = TrainingState(\n          policy_params=policy_params,\n          critic_params=critic_params,\n          target_policy_params=target_policy_params,\n          target_critic_params=target_critic_params,\n          policy_opt_state=policy_opt_state,\n          critic_opt_state=critic_opt_state,\n          steps=steps,\n      )\n\n      metrics = {\n          'policy_loss': policy_loss_value,\n          'critic_loss': critic_loss_value,\n      }\n\n      return new_state, metrics\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger(\n        'learner',\n        asynchronous=True,\n        serialize_fn=utils.fetch_devicearray,\n        steps_key=self._counter.get_steps_key())\n\n    # Necessary to track when to update target networks.\n    self._target_update_period = target_update_period\n\n    # Create prefetching dataset iterator.\n    self._iterator = iterator\n\n    # Maybe use the JIT compiler.\n    sgd_step = utils.process_multiple_batches(sgd_step, num_sgd_steps_per_step)\n    self._sgd_step = (\n        jax.pmap(sgd_step, _PMAP_AXIS_NAME, devices=jax.devices())\n        if jit else sgd_step)\n\n    # Create the network parameters and copy into the target network parameters.\n    key_policy, key_critic = jax.random.split(random_key)\n    initial_policy_params = policy_network.init(key_policy)\n    initial_critic_params = critic_network.init(key_critic)\n    initial_target_policy_params = initial_policy_params\n    initial_target_critic_params = initial_critic_params\n\n    # Create optimizers if they aren't given.\n    critic_optimizer = critic_optimizer or optax.adam(1e-4)\n    policy_optimizer = policy_optimizer or optax.adam(1e-4)\n\n    # Initialize optimizers.\n    initial_policy_opt_state = policy_optimizer.init(initial_policy_params)  # pytype: disable=attribute-error\n    initial_critic_opt_state = critic_optimizer.init(initial_critic_params)  # pytype: disable=attribute-error\n\n    # Create the initial state and replicate it in all devices.\n    self._state = utils.replicate_in_all_devices(\n        TrainingState(\n            policy_params=initial_policy_params,\n            target_policy_params=initial_target_policy_params,\n            critic_params=initial_critic_params,\n            target_critic_params=initial_target_critic_params,\n            policy_opt_state=initial_policy_opt_state,\n            critic_opt_state=initial_critic_opt_state,\n            steps=0,\n        ))\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None\n\n  def step(self):\n    # Sample from replay and pack the data in a Transition.\n    sample = next(self._iterator)\n    transitions = types.Transition(*sample.data)\n\n    self._state, metrics = self._sgd_step(self._state, transitions)\n\n    # Take the metrics from the first device, since they've been pmeaned over\n    # all devices and are therefore identical.\n    metrics = utils.get_from_first_device(metrics)\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Increment counts and record the current time\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n\n    # Attempts to write the logs.\n    self._logger.write({**metrics, **counts})\n\n  def get_variables(self, names: List[str]) -> List[networks_lib.Params]:\n    variables = {\n        'policy': self._state.target_policy_params,\n        'critic': self._state.target_critic_params,\n    }\n    return utils.get_from_first_device([variables[name] for name in names])\n\n  def save(self) -> TrainingState:\n    return utils.get_from_first_device(self._state)\n\n  def restore(self, state: TrainingState):\n    self._state = utils.replicate_in_all_devices(state)",
  "def __init__(self,\n               policy_network: networks_lib.FeedForwardNetwork,\n               critic_network: networks_lib.FeedForwardNetwork,\n               random_key: networks_lib.PRNGKey,\n               discount: float,\n               target_update_period: int,\n               iterator: Iterator[reverb.ReplaySample],\n               policy_optimizer: Optional[optax.GradientTransformation] = None,\n               critic_optimizer: Optional[optax.GradientTransformation] = None,\n               clipping: bool = True,\n               counter: Optional[counting.Counter] = None,\n               logger: Optional[loggers.Logger] = None,\n               jit: bool = True,\n               num_sgd_steps_per_step: int = 1):\n\n    def critic_mean(\n        critic_params: networks_lib.Params,\n        observation: types.NestedArray,\n        action: types.NestedArray,\n    ) -> jnp.ndarray:\n      # We add batch dimension to make sure batch concat in critic_network\n      # works correctly.\n      observation = utils.add_batch_dim(observation)\n      action = utils.add_batch_dim(action)\n      # Computes the mean action-value estimate.\n      logits, atoms = critic_network.apply(critic_params, observation, action)\n      logits = utils.squeeze_batch_dim(logits)\n      probabilities = jax.nn.softmax(logits)\n      return jnp.sum(probabilities * atoms, axis=-1)\n\n    def policy_loss(\n        policy_params: networks_lib.Params,\n        critic_params: networks_lib.Params,\n        o_t: types.NestedArray,\n    ) -> jnp.ndarray:\n      # Computes the discrete policy gradient loss.\n      dpg_a_t = policy_network.apply(policy_params, o_t)\n      grad_critic = jax.vmap(\n          jax.grad(critic_mean, argnums=2), in_axes=(None, 0, 0))\n      dq_da = grad_critic(critic_params, o_t, dpg_a_t)\n      dqda_clipping = 1. if clipping else None\n      batch_dpg_learning = jax.vmap(rlax.dpg_loss, in_axes=(0, 0, None))\n      loss = batch_dpg_learning(dpg_a_t, dq_da, dqda_clipping)\n      return jnp.mean(loss)\n\n    def critic_loss(\n        critic_params: networks_lib.Params,\n        state: TrainingState,\n        transition: types.Transition,\n    ):\n      # Computes the distributional critic loss.\n      q_tm1, atoms_tm1 = critic_network.apply(critic_params,\n                                              transition.observation,\n                                              transition.action)\n      a = policy_network.apply(state.target_policy_params,\n                               transition.next_observation)\n      q_t, atoms_t = critic_network.apply(state.target_critic_params,\n                                          transition.next_observation, a)\n      batch_td_learning = jax.vmap(\n          rlax.categorical_td_learning, in_axes=(None, 0, 0, 0, None, 0))\n      loss = batch_td_learning(atoms_tm1, q_tm1, transition.reward,\n                               discount * transition.discount, atoms_t, q_t)\n      return jnp.mean(loss)\n\n    def sgd_step(\n        state: TrainingState,\n        transitions: types.Transition,\n    ) -> Tuple[TrainingState, Dict[str, jnp.ndarray]]:\n\n      # TODO(jaslanides): Use a shared forward pass for efficiency.\n      policy_loss_and_grad = jax.value_and_grad(policy_loss)\n      critic_loss_and_grad = jax.value_and_grad(critic_loss)\n\n      # Compute losses and their gradients.\n      policy_loss_value, policy_gradients = policy_loss_and_grad(\n          state.policy_params, state.critic_params,\n          transitions.next_observation)\n      critic_loss_value, critic_gradients = critic_loss_and_grad(\n          state.critic_params, state, transitions)\n\n      # Average over all devices.\n      policy_loss_value, policy_gradients = jax.lax.pmean(\n          (policy_loss_value, policy_gradients), _PMAP_AXIS_NAME)\n      critic_loss_value, critic_gradients = jax.lax.pmean(\n          (critic_loss_value, critic_gradients), _PMAP_AXIS_NAME)\n\n      # Get optimizer updates and state.\n      policy_updates, policy_opt_state = policy_optimizer.update(  # pytype: disable=attribute-error\n          policy_gradients, state.policy_opt_state)\n      critic_updates, critic_opt_state = critic_optimizer.update(  # pytype: disable=attribute-error\n          critic_gradients, state.critic_opt_state)\n\n      # Apply optimizer updates to parameters.\n      policy_params = optax.apply_updates(state.policy_params, policy_updates)\n      critic_params = optax.apply_updates(state.critic_params, critic_updates)\n\n      steps = state.steps + 1\n\n      # Periodically update target networks.\n      target_policy_params, target_critic_params = optax.periodic_update(  # pytype: disable=wrong-arg-types  # numpy-scalars\n          (policy_params, critic_params),\n          (state.target_policy_params, state.target_critic_params), steps,\n          self._target_update_period)\n\n      new_state = TrainingState(\n          policy_params=policy_params,\n          critic_params=critic_params,\n          target_policy_params=target_policy_params,\n          target_critic_params=target_critic_params,\n          policy_opt_state=policy_opt_state,\n          critic_opt_state=critic_opt_state,\n          steps=steps,\n      )\n\n      metrics = {\n          'policy_loss': policy_loss_value,\n          'critic_loss': critic_loss_value,\n      }\n\n      return new_state, metrics\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger(\n        'learner',\n        asynchronous=True,\n        serialize_fn=utils.fetch_devicearray,\n        steps_key=self._counter.get_steps_key())\n\n    # Necessary to track when to update target networks.\n    self._target_update_period = target_update_period\n\n    # Create prefetching dataset iterator.\n    self._iterator = iterator\n\n    # Maybe use the JIT compiler.\n    sgd_step = utils.process_multiple_batches(sgd_step, num_sgd_steps_per_step)\n    self._sgd_step = (\n        jax.pmap(sgd_step, _PMAP_AXIS_NAME, devices=jax.devices())\n        if jit else sgd_step)\n\n    # Create the network parameters and copy into the target network parameters.\n    key_policy, key_critic = jax.random.split(random_key)\n    initial_policy_params = policy_network.init(key_policy)\n    initial_critic_params = critic_network.init(key_critic)\n    initial_target_policy_params = initial_policy_params\n    initial_target_critic_params = initial_critic_params\n\n    # Create optimizers if they aren't given.\n    critic_optimizer = critic_optimizer or optax.adam(1e-4)\n    policy_optimizer = policy_optimizer or optax.adam(1e-4)\n\n    # Initialize optimizers.\n    initial_policy_opt_state = policy_optimizer.init(initial_policy_params)  # pytype: disable=attribute-error\n    initial_critic_opt_state = critic_optimizer.init(initial_critic_params)  # pytype: disable=attribute-error\n\n    # Create the initial state and replicate it in all devices.\n    self._state = utils.replicate_in_all_devices(\n        TrainingState(\n            policy_params=initial_policy_params,\n            target_policy_params=initial_target_policy_params,\n            critic_params=initial_critic_params,\n            target_critic_params=initial_target_critic_params,\n            policy_opt_state=initial_policy_opt_state,\n            critic_opt_state=initial_critic_opt_state,\n            steps=0,\n        ))\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None",
  "def step(self):\n    # Sample from replay and pack the data in a Transition.\n    sample = next(self._iterator)\n    transitions = types.Transition(*sample.data)\n\n    self._state, metrics = self._sgd_step(self._state, transitions)\n\n    # Take the metrics from the first device, since they've been pmeaned over\n    # all devices and are therefore identical.\n    metrics = utils.get_from_first_device(metrics)\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Increment counts and record the current time\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n\n    # Attempts to write the logs.\n    self._logger.write({**metrics, **counts})",
  "def get_variables(self, names: List[str]) -> List[networks_lib.Params]:\n    variables = {\n        'policy': self._state.target_policy_params,\n        'critic': self._state.target_critic_params,\n    }\n    return utils.get_from_first_device([variables[name] for name in names])",
  "def save(self) -> TrainingState:\n    return utils.get_from_first_device(self._state)",
  "def restore(self, state: TrainingState):\n    self._state = utils.replicate_in_all_devices(state)",
  "def critic_mean(\n        critic_params: networks_lib.Params,\n        observation: types.NestedArray,\n        action: types.NestedArray,\n    ) -> jnp.ndarray:\n      # We add batch dimension to make sure batch concat in critic_network\n      # works correctly.\n      observation = utils.add_batch_dim(observation)\n      action = utils.add_batch_dim(action)\n      # Computes the mean action-value estimate.\n      logits, atoms = critic_network.apply(critic_params, observation, action)\n      logits = utils.squeeze_batch_dim(logits)\n      probabilities = jax.nn.softmax(logits)\n      return jnp.sum(probabilities * atoms, axis=-1)",
  "def policy_loss(\n        policy_params: networks_lib.Params,\n        critic_params: networks_lib.Params,\n        o_t: types.NestedArray,\n    ) -> jnp.ndarray:\n      # Computes the discrete policy gradient loss.\n      dpg_a_t = policy_network.apply(policy_params, o_t)\n      grad_critic = jax.vmap(\n          jax.grad(critic_mean, argnums=2), in_axes=(None, 0, 0))\n      dq_da = grad_critic(critic_params, o_t, dpg_a_t)\n      dqda_clipping = 1. if clipping else None\n      batch_dpg_learning = jax.vmap(rlax.dpg_loss, in_axes=(0, 0, None))\n      loss = batch_dpg_learning(dpg_a_t, dq_da, dqda_clipping)\n      return jnp.mean(loss)",
  "def critic_loss(\n        critic_params: networks_lib.Params,\n        state: TrainingState,\n        transition: types.Transition,\n    ):\n      # Computes the distributional critic loss.\n      q_tm1, atoms_tm1 = critic_network.apply(critic_params,\n                                              transition.observation,\n                                              transition.action)\n      a = policy_network.apply(state.target_policy_params,\n                               transition.next_observation)\n      q_t, atoms_t = critic_network.apply(state.target_critic_params,\n                                          transition.next_observation, a)\n      batch_td_learning = jax.vmap(\n          rlax.categorical_td_learning, in_axes=(None, 0, 0, 0, None, 0))\n      loss = batch_td_learning(atoms_tm1, q_tm1, transition.reward,\n                               discount * transition.discount, atoms_t, q_t)\n      return jnp.mean(loss)",
  "def sgd_step(\n        state: TrainingState,\n        transitions: types.Transition,\n    ) -> Tuple[TrainingState, Dict[str, jnp.ndarray]]:\n\n      # TODO(jaslanides): Use a shared forward pass for efficiency.\n      policy_loss_and_grad = jax.value_and_grad(policy_loss)\n      critic_loss_and_grad = jax.value_and_grad(critic_loss)\n\n      # Compute losses and their gradients.\n      policy_loss_value, policy_gradients = policy_loss_and_grad(\n          state.policy_params, state.critic_params,\n          transitions.next_observation)\n      critic_loss_value, critic_gradients = critic_loss_and_grad(\n          state.critic_params, state, transitions)\n\n      # Average over all devices.\n      policy_loss_value, policy_gradients = jax.lax.pmean(\n          (policy_loss_value, policy_gradients), _PMAP_AXIS_NAME)\n      critic_loss_value, critic_gradients = jax.lax.pmean(\n          (critic_loss_value, critic_gradients), _PMAP_AXIS_NAME)\n\n      # Get optimizer updates and state.\n      policy_updates, policy_opt_state = policy_optimizer.update(  # pytype: disable=attribute-error\n          policy_gradients, state.policy_opt_state)\n      critic_updates, critic_opt_state = critic_optimizer.update(  # pytype: disable=attribute-error\n          critic_gradients, state.critic_opt_state)\n\n      # Apply optimizer updates to parameters.\n      policy_params = optax.apply_updates(state.policy_params, policy_updates)\n      critic_params = optax.apply_updates(state.critic_params, critic_updates)\n\n      steps = state.steps + 1\n\n      # Periodically update target networks.\n      target_policy_params, target_critic_params = optax.periodic_update(  # pytype: disable=wrong-arg-types  # numpy-scalars\n          (policy_params, critic_params),\n          (state.target_policy_params, state.target_critic_params), steps,\n          self._target_update_period)\n\n      new_state = TrainingState(\n          policy_params=policy_params,\n          critic_params=critic_params,\n          target_policy_params=target_policy_params,\n          target_critic_params=target_critic_params,\n          policy_opt_state=policy_opt_state,\n          critic_opt_state=critic_opt_state,\n          steps=steps,\n      )\n\n      metrics = {\n          'policy_loss': policy_loss_value,\n          'critic_loss': critic_loss_value,\n      }\n\n      return new_state, metrics",
  "class D4PGNetworks:\n  \"\"\"Network and pure functions for the D4PG agent..\"\"\"\n  policy_network: networks_lib.FeedForwardNetwork\n  critic_network: networks_lib.FeedForwardNetwork",
  "def get_default_behavior_policy(\n    networks: D4PGNetworks,\n    config: d4pg_config.D4PGConfig) -> actor_core_lib.FeedForwardPolicy:\n  \"\"\"Selects action according to the training policy.\"\"\"\n  def behavior_policy(params: networks_lib.Params, key: networks_lib.PRNGKey,\n                      observation: types.NestedArray):\n    action = networks.policy_network.apply(params, observation)\n    if config.sigma != 0:\n      action = rlax.add_gaussian_noise(key, action, config.sigma)\n    return action\n\n  return behavior_policy",
  "def get_default_eval_policy(\n    networks: D4PGNetworks) -> actor_core_lib.FeedForwardPolicy:\n  \"\"\"Selects action according to the training policy.\"\"\"\n  def behavior_policy(params: networks_lib.Params, key: networks_lib.PRNGKey,\n                      observation: types.NestedArray):\n    del key\n    action = networks.policy_network.apply(params, observation)\n    return action\n  return behavior_policy",
  "def make_networks(\n    spec: specs.EnvironmentSpec,\n    policy_layer_sizes: Sequence[int] = (300, 200),\n    critic_layer_sizes: Sequence[int] = (400, 300),\n    vmin: float = -150.,\n    vmax: float = 150.,\n    num_atoms: int = 51,\n) -> D4PGNetworks:\n  \"\"\"Creates networks used by the agent.\"\"\"\n\n  action_spec = spec.actions\n\n  num_dimensions = np.prod(action_spec.shape, dtype=int)\n  critic_atoms = jnp.linspace(vmin, vmax, num_atoms)\n\n  def _actor_fn(obs):\n    network = hk.Sequential([\n        utils.batch_concat,\n        networks_lib.LayerNormMLP(policy_layer_sizes, activate_final=True),\n        networks_lib.NearZeroInitializedLinear(num_dimensions),\n        networks_lib.TanhToSpec(action_spec),\n    ])\n    return network(obs)\n\n  def _critic_fn(obs, action):\n    network = hk.Sequential([\n        utils.batch_concat,\n        networks_lib.LayerNormMLP(layer_sizes=[*critic_layer_sizes, num_atoms]),\n    ])\n    value = network([obs, action])\n    return value, critic_atoms\n\n  policy = hk.without_apply_rng(hk.transform(_actor_fn))\n  critic = hk.without_apply_rng(hk.transform(_critic_fn))\n\n  # Create dummy observations and actions to create network parameters.\n  dummy_action = utils.zeros_like(spec.actions)\n  dummy_obs = utils.zeros_like(spec.observations)\n  dummy_action = utils.add_batch_dim(dummy_action)\n  dummy_obs = utils.add_batch_dim(dummy_obs)\n\n  return D4PGNetworks(\n      policy_network=networks_lib.FeedForwardNetwork(\n          lambda rng: policy.init(rng, dummy_obs), policy.apply),\n      critic_network=networks_lib.FeedForwardNetwork(\n          lambda rng: critic.init(rng, dummy_obs, dummy_action), critic.apply))",
  "def behavior_policy(params: networks_lib.Params, key: networks_lib.PRNGKey,\n                      observation: types.NestedArray):\n    action = networks.policy_network.apply(params, observation)\n    if config.sigma != 0:\n      action = rlax.add_gaussian_noise(key, action, config.sigma)\n    return action",
  "def behavior_policy(params: networks_lib.Params, key: networks_lib.PRNGKey,\n                      observation: types.NestedArray):\n    del key\n    action = networks.policy_network.apply(params, observation)\n    return action",
  "def _actor_fn(obs):\n    network = hk.Sequential([\n        utils.batch_concat,\n        networks_lib.LayerNormMLP(policy_layer_sizes, activate_final=True),\n        networks_lib.NearZeroInitializedLinear(num_dimensions),\n        networks_lib.TanhToSpec(action_spec),\n    ])\n    return network(obs)",
  "def _critic_fn(obs, action):\n    network = hk.Sequential([\n        utils.batch_concat,\n        networks_lib.LayerNormMLP(layer_sizes=[*critic_layer_sizes, num_atoms]),\n    ])\n    value = network([obs, action])\n    return value, critic_atoms",
  "class D4PGConfig:\n  \"\"\"Configuration options for D4PG.\"\"\"\n  sigma: float = 0.3\n  target_update_period: int = 100\n  samples_per_insert: Optional[float] = 32.0\n\n  # Loss options\n  n_step: int = 5\n  discount: float = 0.99\n  batch_size: int = 256\n  learning_rate: float = 1e-4\n  clipping: bool = True\n\n  # Replay options\n  min_replay_size: int = 1000\n  max_replay_size: int = 1000000\n  replay_table_name: str = adders_reverb.DEFAULT_PRIORITY_TABLE\n  prefetch_size: int = 4\n  # Rate to be used for the SampleToInsertRatio rate limitter tolerance.\n  # See a formula in make_replay_tables for more details.\n  samples_per_insert_tolerance_rate: float = 0.1\n\n  # How many gradient updates to perform per step.\n  num_sgd_steps_per_step: int = 1",
  "def _make_adder_config(step_spec: reverb_base.Step, n_step: int,\n                       table: str) -> List[sw.Config]:\n  return adders_reverb.create_n_step_transition_config(\n      step_spec=step_spec, n_step=n_step, table=table)",
  "def _as_n_step_transition(flat_trajectory: reverb.ReplaySample,\n                          agent_discount: float) -> reverb.ReplaySample:\n  \"\"\"Compute discounted return and total discount for N-step transitions.\n\n  For N greater than 1, transitions are of the form:\n\n          (s_t, a_t, r_{t:t+n}, r_{t:t+n}, s_{t+N}, e_t),\n\n  where:\n\n      s_t = State (observation) at time t.\n      a_t = Action taken from state s_t.\n      g = the additional discount, used by the agent to discount future returns.\n      r_{t:t+n} = A vector of N-step rewards: [r_t r_{t+1} ... r_{t+n}]\n      d_{t:t+n} = A vector of N-step environment: [d_t d_{t+1} ... d_{t+n}]\n        For most environments d_i is 1 for all steps except the last,\n        i.e. it is the episode termination signal.\n      s_{t+n}: The \"arrival\" state, i.e. the state at time t+n.\n      e_t [Optional]: A nested structure of any 'extras' the user wishes to add.\n\n  As such postprocessing is necessary to calculate the N-Step discounted return\n  and the total discount as follows:\n\n          (s_t, a_t, R_{t:t+n}, D_{t:t+n}, s_{t+N}, e_t),\n\n    where:\n\n      R_{t:t+n} = N-step discounted return, i.e. accumulated over N rewards:\n            R_{t:t+n} := r_t + g * d_t * r_{t+1} + ...\n                            + g^{n-1} * d_t * ... * d_{t+n-2} * r_{t+n-1}.\n      D_{t:t+n}: N-step product of agent discounts g_i and environment\n        \"discounts\" d_i.\n            D_{t:t+n} := g^{n-1} * d_{t} * ... * d_{t+n-1},\n\n  Args:\n    flat_trajectory: An trajectory with n-step rewards and discounts to be\n      process.\n    agent_discount: An additional discount factor used by the agent to discount\n      futrue returns.\n\n  Returns:\n    A reverb.ReplaySample with computed discounted return and total discount.\n  \"\"\"\n  trajectory = flat_trajectory.data\n\n  def compute_discount_and_reward(\n      state: types.NestedTensor,\n      discount_and_reward: types.NestedTensor) -> types.NestedTensor:\n    compounded_discount, discounted_reward = state\n    return (agent_discount * discount_and_reward[0] * compounded_discount,\n            discounted_reward + discount_and_reward[1] * compounded_discount)\n\n  initializer = (tf.constant(1, dtype=tf.float32),\n                 tf.constant(0, dtype=tf.float32))\n  elems = tf.stack((trajectory.discount, trajectory.reward), axis=-1)\n  total_discount, n_step_return = tf.scan(\n      compute_discount_and_reward, elems, initializer, reverse=True)\n  return reverb.ReplaySample(\n      info=flat_trajectory.info,\n      data=types.Transition(\n          observation=tree.map_structure(lambda x: x[0],\n                                         trajectory.observation),\n          action=tree.map_structure(lambda x: x[0], trajectory.action),\n          reward=n_step_return[0],\n          discount=total_discount[0],\n          next_observation=tree.map_structure(lambda x: x[-1],\n                                              trajectory.observation),\n          extras=tree.map_structure(lambda x: x[0], trajectory.extras)))",
  "class D4PGBuilder(builders.ActorLearnerBuilder[d4pg_networks.D4PGNetworks,\n                                               actor_core_lib.ActorCore,\n                                               reverb.ReplaySample]):\n  \"\"\"D4PG Builder.\"\"\"\n\n  def __init__(\n      self,\n      config: d4pg_config.D4PGConfig,\n  ):\n    \"\"\"Creates a D4PG learner, a behavior policy and an eval actor.\n\n    Args:\n      config: a config with D4PG hps\n    \"\"\"\n    self._config = config\n\n  def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: d4pg_networks.D4PGNetworks,\n      dataset: Iterator[reverb.ReplaySample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    del environment_spec, replay_client\n\n    policy_optimizer = optax.adam(self._config.learning_rate)\n    critic_optimizer = optax.adam(self._config.learning_rate)\n\n    if self._config.clipping:\n      policy_optimizer = optax.chain(\n          optax.clip_by_global_norm(40.), policy_optimizer)\n      critic_optimizer = optax.chain(\n          optax.clip_by_global_norm(40.), critic_optimizer)\n\n    # The learner updates the parameters (and initializes them).\n    return learning.D4PGLearner(\n        policy_network=networks.policy_network,\n        critic_network=networks.critic_network,\n        random_key=random_key,\n        policy_optimizer=policy_optimizer,\n        critic_optimizer=critic_optimizer,\n        clipping=self._config.clipping,\n        discount=self._config.discount,\n        target_update_period=self._config.target_update_period,\n        iterator=dataset,\n        counter=counter,\n        logger=logger_fn('learner'),\n        num_sgd_steps_per_step=self._config.num_sgd_steps_per_step)\n\n  def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: actor_core_lib.ActorCore,\n  ) -> List[reverb.Table]:\n    \"\"\"Create tables to insert data into.\"\"\"\n    dummy_actor_state = policy.init(jax.random.PRNGKey(0))\n    extras_spec = policy.get_extras(dummy_actor_state)\n    step_spec = adders_reverb.create_step_spec(\n        environment_spec=environment_spec, extras_spec=extras_spec)\n\n    # Create the rate limiter.\n    if self._config.samples_per_insert:\n      samples_per_insert_tolerance = (\n          self._config.samples_per_insert_tolerance_rate *\n          self._config.samples_per_insert)\n      error_buffer = self._config.min_replay_size * samples_per_insert_tolerance\n      limiter = rate_limiters.SampleToInsertRatio(\n          min_size_to_sample=self._config.min_replay_size,\n          samples_per_insert=self._config.samples_per_insert,\n          error_buffer=error_buffer)\n    else:\n      limiter = rate_limiters.MinSize(self._config.min_replay_size)\n    return [\n        reverb.Table(\n            name=self._config.replay_table_name,\n            sampler=reverb.selectors.Uniform(),\n            remover=reverb.selectors.Fifo(),\n            max_size=self._config.max_replay_size,\n            rate_limiter=limiter,\n            signature=sw.infer_signature(\n                configs=_make_adder_config(step_spec, self._config.n_step,\n                                           self._config.replay_table_name),\n                step_spec=step_spec))\n    ]\n\n  def make_dataset_iterator(\n      self,\n      replay_client: reverb.Client,\n  ) -> Iterator[reverb.ReplaySample]:\n    \"\"\"Create a dataset iterator to use for learning/updating the agent.\"\"\"\n\n    def postprocess(\n        flat_trajectory: reverb.ReplaySample) -> reverb.ReplaySample:\n      return _as_n_step_transition(flat_trajectory, self._config.discount)\n\n    batch_size_per_device = self._config.batch_size // jax.device_count()\n\n    dataset = datasets.make_reverb_dataset(\n        table=self._config.replay_table_name,\n        server_address=replay_client.server_address,\n        batch_size=batch_size_per_device * self._config.num_sgd_steps_per_step,\n        prefetch_size=self._config.prefetch_size,\n        postprocess=postprocess,\n    )\n    return utils.multi_device_put(dataset.as_numpy_iterator(),\n                                  jax.local_devices())\n\n  def make_adder(\n      self,\n      replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[actor_core_lib.ActorCore],\n  ) -> Optional[adders.Adder]:\n    \"\"\"Create an adder which records data generated by the actor/environment.\"\"\"\n    if environment_spec is None or policy is None:\n      raise ValueError('`environment_spec` and `policy` cannot be None.')\n    dummy_actor_state = policy.init(jax.random.PRNGKey(0))\n    extras_spec = policy.get_extras(dummy_actor_state)\n    step_spec = adders_reverb.create_step_spec(\n        environment_spec=environment_spec, extras_spec=extras_spec)\n    return adders_reverb.StructuredAdder(\n        client=replay_client,\n        max_in_flight_items=5,\n        configs=_make_adder_config(step_spec, self._config.n_step,\n                                   self._config.replay_table_name),\n        step_spec=step_spec)\n\n  def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: actor_core_lib.ActorCore,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> acme.Actor:\n    del environment_spec\n    assert variable_source is not None\n    # Inference happens on CPU, so it's better to move variables there too.\n    variable_client = variable_utils.VariableClient(\n        variable_source, 'policy', device='cpu')\n    return actors.GenericActor(\n        policy, random_key, variable_client, adder, backend='cpu')\n\n  def make_policy(self,\n                  networks: d4pg_networks.D4PGNetworks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> actor_core_lib.ActorCore:\n    \"\"\"Create the policy.\"\"\"\n    del environment_spec\n    if evaluation:\n      policy = d4pg_networks.get_default_eval_policy(networks)\n    else:\n      policy = d4pg_networks.get_default_behavior_policy(networks, self._config)\n\n    return actor_core_lib.batched_feed_forward_to_actor_core(policy)",
  "def compute_discount_and_reward(\n      state: types.NestedTensor,\n      discount_and_reward: types.NestedTensor) -> types.NestedTensor:\n    compounded_discount, discounted_reward = state\n    return (agent_discount * discount_and_reward[0] * compounded_discount,\n            discounted_reward + discount_and_reward[1] * compounded_discount)",
  "def __init__(\n      self,\n      config: d4pg_config.D4PGConfig,\n  ):\n    \"\"\"Creates a D4PG learner, a behavior policy and an eval actor.\n\n    Args:\n      config: a config with D4PG hps\n    \"\"\"\n    self._config = config",
  "def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: d4pg_networks.D4PGNetworks,\n      dataset: Iterator[reverb.ReplaySample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    del environment_spec, replay_client\n\n    policy_optimizer = optax.adam(self._config.learning_rate)\n    critic_optimizer = optax.adam(self._config.learning_rate)\n\n    if self._config.clipping:\n      policy_optimizer = optax.chain(\n          optax.clip_by_global_norm(40.), policy_optimizer)\n      critic_optimizer = optax.chain(\n          optax.clip_by_global_norm(40.), critic_optimizer)\n\n    # The learner updates the parameters (and initializes them).\n    return learning.D4PGLearner(\n        policy_network=networks.policy_network,\n        critic_network=networks.critic_network,\n        random_key=random_key,\n        policy_optimizer=policy_optimizer,\n        critic_optimizer=critic_optimizer,\n        clipping=self._config.clipping,\n        discount=self._config.discount,\n        target_update_period=self._config.target_update_period,\n        iterator=dataset,\n        counter=counter,\n        logger=logger_fn('learner'),\n        num_sgd_steps_per_step=self._config.num_sgd_steps_per_step)",
  "def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: actor_core_lib.ActorCore,\n  ) -> List[reverb.Table]:\n    \"\"\"Create tables to insert data into.\"\"\"\n    dummy_actor_state = policy.init(jax.random.PRNGKey(0))\n    extras_spec = policy.get_extras(dummy_actor_state)\n    step_spec = adders_reverb.create_step_spec(\n        environment_spec=environment_spec, extras_spec=extras_spec)\n\n    # Create the rate limiter.\n    if self._config.samples_per_insert:\n      samples_per_insert_tolerance = (\n          self._config.samples_per_insert_tolerance_rate *\n          self._config.samples_per_insert)\n      error_buffer = self._config.min_replay_size * samples_per_insert_tolerance\n      limiter = rate_limiters.SampleToInsertRatio(\n          min_size_to_sample=self._config.min_replay_size,\n          samples_per_insert=self._config.samples_per_insert,\n          error_buffer=error_buffer)\n    else:\n      limiter = rate_limiters.MinSize(self._config.min_replay_size)\n    return [\n        reverb.Table(\n            name=self._config.replay_table_name,\n            sampler=reverb.selectors.Uniform(),\n            remover=reverb.selectors.Fifo(),\n            max_size=self._config.max_replay_size,\n            rate_limiter=limiter,\n            signature=sw.infer_signature(\n                configs=_make_adder_config(step_spec, self._config.n_step,\n                                           self._config.replay_table_name),\n                step_spec=step_spec))\n    ]",
  "def make_dataset_iterator(\n      self,\n      replay_client: reverb.Client,\n  ) -> Iterator[reverb.ReplaySample]:\n    \"\"\"Create a dataset iterator to use for learning/updating the agent.\"\"\"\n\n    def postprocess(\n        flat_trajectory: reverb.ReplaySample) -> reverb.ReplaySample:\n      return _as_n_step_transition(flat_trajectory, self._config.discount)\n\n    batch_size_per_device = self._config.batch_size // jax.device_count()\n\n    dataset = datasets.make_reverb_dataset(\n        table=self._config.replay_table_name,\n        server_address=replay_client.server_address,\n        batch_size=batch_size_per_device * self._config.num_sgd_steps_per_step,\n        prefetch_size=self._config.prefetch_size,\n        postprocess=postprocess,\n    )\n    return utils.multi_device_put(dataset.as_numpy_iterator(),\n                                  jax.local_devices())",
  "def make_adder(\n      self,\n      replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[actor_core_lib.ActorCore],\n  ) -> Optional[adders.Adder]:\n    \"\"\"Create an adder which records data generated by the actor/environment.\"\"\"\n    if environment_spec is None or policy is None:\n      raise ValueError('`environment_spec` and `policy` cannot be None.')\n    dummy_actor_state = policy.init(jax.random.PRNGKey(0))\n    extras_spec = policy.get_extras(dummy_actor_state)\n    step_spec = adders_reverb.create_step_spec(\n        environment_spec=environment_spec, extras_spec=extras_spec)\n    return adders_reverb.StructuredAdder(\n        client=replay_client,\n        max_in_flight_items=5,\n        configs=_make_adder_config(step_spec, self._config.n_step,\n                                   self._config.replay_table_name),\n        step_spec=step_spec)",
  "def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: actor_core_lib.ActorCore,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> acme.Actor:\n    del environment_spec\n    assert variable_source is not None\n    # Inference happens on CPU, so it's better to move variables there too.\n    variable_client = variable_utils.VariableClient(\n        variable_source, 'policy', device='cpu')\n    return actors.GenericActor(\n        policy, random_key, variable_client, adder, backend='cpu')",
  "def make_policy(self,\n                  networks: d4pg_networks.D4PGNetworks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> actor_core_lib.ActorCore:\n    \"\"\"Create the policy.\"\"\"\n    del environment_spec\n    if evaluation:\n      policy = d4pg_networks.get_default_eval_policy(networks)\n    else:\n      policy = d4pg_networks.get_default_behavior_policy(networks, self._config)\n\n    return actor_core_lib.batched_feed_forward_to_actor_core(policy)",
  "def postprocess(\n        flat_trajectory: reverb.ReplaySample) -> reverb.ReplaySample:\n      return _as_n_step_transition(flat_trajectory, self._config.discount)",
  "class DefaultSupportedAgent(enum.Enum):\n  \"\"\"Agents which have default initializers supported below.\"\"\"\n  TD3 = 'TD3'\n  SAC = 'SAC'\n  PPO = 'PPO'",
  "def init_default_network(\n    agent_type: DefaultSupportedAgent,\n    agent_spec: specs.EnvironmentSpec) -> ma_types.Networks:\n  \"\"\"Returns default networks for a single agent.\"\"\"\n  if agent_type == DefaultSupportedAgent.TD3:\n    return td3.make_networks(agent_spec)\n  elif agent_type == DefaultSupportedAgent.SAC:\n    return sac.make_networks(agent_spec)\n  elif agent_type == DefaultSupportedAgent.PPO:\n    return ppo.make_networks(agent_spec)\n  else:\n    raise ValueError(f'Unsupported agent type: {agent_type}.')",
  "def init_default_policy_network(\n    agent_type: DefaultSupportedAgent,\n    network: ma_types.Networks,\n    agent_spec: specs.EnvironmentSpec,\n    config: ma_types.AgentConfig,\n    eval_mode: ma_types.EvalMode = False) -> ma_types.PolicyNetwork:\n  \"\"\"Returns default policy network for a single agent.\"\"\"\n  if agent_type == DefaultSupportedAgent.TD3:\n    sigma = 0. if eval_mode else config.sigma\n    return td3.get_default_behavior_policy(\n        network, agent_spec.actions, sigma=sigma)\n  elif agent_type == DefaultSupportedAgent.SAC:\n    return sac.apply_policy_and_sample(network, eval_mode=eval_mode)\n  elif agent_type == DefaultSupportedAgent.PPO:\n    return ppo.make_inference_fn(network, evaluation=eval_mode)\n  else:\n    raise ValueError(f'Unsupported agent type: {agent_type}.')",
  "def init_default_builder(\n    agent_type: DefaultSupportedAgent,\n    agent_config: ma_types.AgentConfig,\n) -> jax_builders.GenericActorLearnerBuilder:\n  \"\"\"Returns default builder for a single agent.\"\"\"\n  if agent_type == DefaultSupportedAgent.TD3:\n    assert isinstance(agent_config, td3.TD3Config)\n    return td3.TD3Builder(agent_config)\n  elif agent_type == DefaultSupportedAgent.SAC:\n    assert isinstance(agent_config, sac.SACConfig)\n    return sac.SACBuilder(agent_config)\n  elif agent_type == DefaultSupportedAgent.PPO:\n    assert isinstance(agent_config, ppo.PPOConfig)\n    return ppo.PPOBuilder(agent_config)\n  else:\n    raise ValueError(f'Unsupported agent type: {agent_type}.')",
  "def init_default_config(\n    agent_type: DefaultSupportedAgent,\n    config_overrides: Dict[str, Any]) -> ma_types.AgentConfig:\n  \"\"\"Returns default config for a single agent.\"\"\"\n  if agent_type == DefaultSupportedAgent.TD3:\n    return td3.TD3Config(**config_overrides)\n  elif agent_type == DefaultSupportedAgent.SAC:\n    return sac.SACConfig(**config_overrides)\n  elif agent_type == DefaultSupportedAgent.PPO:\n    return ppo.PPOConfig(**config_overrides)\n  else:\n    raise ValueError(f'Unsupported agent type: {agent_type}.')",
  "def default_config_factory(\n    agent_types: Dict[ma_types.AgentID, DefaultSupportedAgent],\n    batch_size: int,\n    config_overrides: Optional[Dict[ma_types.AgentID, Dict[str, Any]]] = None\n) -> Dict[ma_types.AgentID, ma_types.AgentConfig]:\n  \"\"\"Returns default configs for all agents.\n\n  Args:\n    agent_types: dict mapping agent IDs to their type.\n    batch_size: shared batch size for all agents.\n    config_overrides: dict mapping (potentially a subset of) agent IDs to their\n      config overrides. This should include any mandatory config parameters for\n      the agents that do not have default values.\n  \"\"\"\n  configs = {}\n  for agent_id, agent_type in agent_types.items():\n    agent_config_overrides = dict(\n        # batch_size is required by LocalLayout, which is shared amongst\n        # the agents. Hence, we enforce a shared batch_size in builders.\n        batch_size=batch_size,\n        # Unique replay_table_name per agent.\n        replay_table_name=f'{adders_reverb.DEFAULT_PRIORITY_TABLE}_agent{agent_id}'\n    )\n    if config_overrides is not None and agent_id in config_overrides:\n      agent_config_overrides = {\n          **config_overrides[agent_id],\n          **agent_config_overrides  # Comes second to ensure batch_size override\n      }\n    configs[agent_id] = init_default_config(agent_type, agent_config_overrides)\n  return configs",
  "def network_factory(\n    environment_spec: specs.EnvironmentSpec,\n    agent_types: Dict[ma_types.AgentID, ma_types.GenericAgent],\n    init_network_fn: Optional[ma_types.InitNetworkFn] = None\n) -> ma_types.MultiAgentNetworks:\n  \"\"\"Returns networks for all agents.\n\n  Args:\n    environment_spec: environment spec.\n    agent_types: dict mapping agent IDs to their type.\n    init_network_fn: optional callable that handles the network initialization\n      for all sub-agents. If this is not supplied, a default network initializer\n      is used (if it is supported for the designated agent type).\n  \"\"\"\n  init_fn = init_network_fn or init_default_network\n  networks = {}\n  for agent_id, agent_type in agent_types.items():\n    single_agent_spec = ma_utils.get_agent_spec(environment_spec, agent_id)\n    networks[agent_id] = init_fn(agent_type, single_agent_spec)\n  return networks",
  "def policy_network_factory(\n    networks: ma_types.MultiAgentNetworks,\n    environment_spec: specs.EnvironmentSpec,\n    agent_types: Dict[ma_types.AgentID, ma_types.GenericAgent],\n    agent_configs: Dict[ma_types.AgentID, ma_types.AgentConfig],\n    eval_mode: ma_types.EvalMode,\n    init_policy_network_fn: Optional[ma_types.InitPolicyNetworkFn] = None\n) -> ma_types.MultiAgentPolicyNetworks:\n  \"\"\"Returns default policy networks for all agents.\n\n  Args:\n    networks: dict mapping agent IDs to their networks.\n    environment_spec: environment spec.\n    agent_types: dict mapping agent IDs to their type.\n    agent_configs: dict mapping agent IDs to their config.\n    eval_mode: whether the policy should be initialized in evaluation mode (only\n      used if an init_policy_network_fn is not explicitly supplied).\n    init_policy_network_fn: optional callable that handles the policy network\n      initialization for all sub-agents. If this is not supplied, a default\n      policy network initializer is used (if it is supported for the designated\n      agent type).\n  \"\"\"\n  init_fn = init_policy_network_fn or init_default_policy_network\n  policy_networks = {}\n  for agent_id, agent_type in agent_types.items():\n    single_agent_spec = ma_utils.get_agent_spec(environment_spec, agent_id)\n    policy_networks[agent_id] = init_fn(agent_type, networks[agent_id],\n                                        single_agent_spec,\n                                        agent_configs[agent_id], eval_mode)\n  return policy_networks",
  "def builder_factory(\n    agent_types: Dict[ma_types.AgentID, ma_types.GenericAgent],\n    agent_configs: Dict[ma_types.AgentID, ma_types.AgentConfig],\n    init_builder_fn: Optional[ma_types.InitBuilderFn] = None\n) -> Dict[ma_types.AgentID, jax_builders.GenericActorLearnerBuilder]:\n  \"\"\"Returns default policy networks for all agents.\"\"\"\n  init_fn = init_builder_fn or init_default_builder\n  builders = {}\n  for agent_id, agent_type in agent_types.items():\n    builders[agent_id] = init_fn(agent_type, agent_configs[agent_id])\n  return builders",
  "class SynchronousDecentralizedLearnerSetState:\n  \"\"\"State of a SynchronousDecentralizedLearnerSet.\"\"\"\n  # States of the learners keyed by their names.\n  learner_states: Dict[ma_types.AgentID, LearnerState]",
  "class SynchronousDecentralizedLearnerSet(core.Learner):\n  \"\"\"Creates a composed learner which wraps a set of local agent learners.\"\"\"\n\n  def __init__(self,\n               learners: Dict[ma_types.AgentID, core.Learner],\n               separator: str = '-'):\n    \"\"\"Initializer.\n\n    Args:\n      learners: a dict specifying the learners for all sub-agents.\n      separator: separator character used to disambiguate sub-learner variables.\n    \"\"\"\n    self._learners = learners\n    self._separator = separator\n\n  def step(self):\n    for learner in self._learners.values():\n      learner.step()\n\n  def get_variables(self, names: List[str]) -> List[types.NestedArray]:\n    \"\"\"Return the named variables as a collection of (nested) numpy arrays.\n\n    The variable names should be prefixed with the name of the child learners\n    using the separator specified in the constructor, e.g. learner1/var.\n\n    Args:\n      names: args where each name is a string identifying a predefined subset of\n        the variables. The variables names should be prefixed with the name of\n        the learners using the separator specified in the constructor, e.g.\n        learner-var if the separator is -.\n\n    Returns:\n      A list of (nested) numpy arrays `variables` such that `variables[i]`\n      corresponds to the collection named by `names[i]`.\n    \"\"\"\n    variables = []\n    for name in names:\n      # Note: if separator is not found, learner_name=name, which is OK.\n      learner_id, _, variable_name = name.partition(self._separator)\n      learner = self._learners[learner_id]\n      variables.extend(learner.get_variables([variable_name]))\n    return variables\n\n  def save(self) -> SynchronousDecentralizedLearnerSetState:\n    return SynchronousDecentralizedLearnerSetState(learner_states={\n        name: learner.save() for name, learner in self._learners.items()\n    })\n\n  def restore(self, state: SynchronousDecentralizedLearnerSetState):\n    for name, learner in self._learners.items():\n      learner.restore(state.learner_states[name])",
  "def __init__(self,\n               learners: Dict[ma_types.AgentID, core.Learner],\n               separator: str = '-'):\n    \"\"\"Initializer.\n\n    Args:\n      learners: a dict specifying the learners for all sub-agents.\n      separator: separator character used to disambiguate sub-learner variables.\n    \"\"\"\n    self._learners = learners\n    self._separator = separator",
  "def step(self):\n    for learner in self._learners.values():\n      learner.step()",
  "def get_variables(self, names: List[str]) -> List[types.NestedArray]:\n    \"\"\"Return the named variables as a collection of (nested) numpy arrays.\n\n    The variable names should be prefixed with the name of the child learners\n    using the separator specified in the constructor, e.g. learner1/var.\n\n    Args:\n      names: args where each name is a string identifying a predefined subset of\n        the variables. The variables names should be prefixed with the name of\n        the learners using the separator specified in the constructor, e.g.\n        learner-var if the separator is -.\n\n    Returns:\n      A list of (nested) numpy arrays `variables` such that `variables[i]`\n      corresponds to the collection named by `names[i]`.\n    \"\"\"\n    variables = []\n    for name in names:\n      # Note: if separator is not found, learner_name=name, which is OK.\n      learner_id, _, variable_name = name.partition(self._separator)\n      learner = self._learners[learner_id]\n      variables.extend(learner.get_variables([variable_name]))\n    return variables",
  "def save(self) -> SynchronousDecentralizedLearnerSetState:\n    return SynchronousDecentralizedLearnerSetState(learner_states={\n        name: learner.save() for name, learner in self._learners.items()\n    })",
  "def restore(self, state: SynchronousDecentralizedLearnerSetState):\n    for name, learner in self._learners.items():\n      learner.restore(state.learner_states[name])",
  "class DecentralizedMultiagentConfig:\n  \"\"\"Configuration options for decentralized multiagent.\"\"\"\n  sub_agent_configs: Dict[types.AgentID, types.AgentConfig]\n  batch_size: int = 256\n  prefetch_size: int = 2",
  "class PrefixedVariableSource(core.VariableSource):\n  \"\"\"Wraps a variable source to add a pre-defined prefix to all names.\"\"\"\n\n  def __init__(self, source: core.VariableSource, prefix: str):\n    self._source = source\n    self._prefix = prefix\n\n  def get_variables(self, names: Sequence[str]) -> List[types.NestedArray]:\n    return self._source.get_variables([self._prefix + name for name in names])",
  "class DecentralizedMultiAgentBuilder(\n    acme_builders.GenericActorLearnerBuilder[\n        ma_types.MultiAgentNetworks,\n        ma_types.MultiAgentPolicyNetworks,\n        ma_types.MultiAgentSample]):\n  \"\"\"Builder for decentralized multiagent setup.\"\"\"\n\n  def __init__(\n      self,\n      agent_types: Dict[ma_types.AgentID, ma_types.GenericAgent],\n      agent_configs: Dict[ma_types.AgentID, ma_types.AgentConfig],\n      init_policy_network_fn: Optional[ma_types.InitPolicyNetworkFn] = None):\n    \"\"\"Initializer.\n\n    Args:\n      agent_types: Dict mapping agent IDs to their types.\n      agent_configs: Dict mapping agent IDs to their configs.\n      init_policy_network_fn: Optional custom policy network initializer\n        function.\n    \"\"\"\n\n    self._agent_types = agent_types\n    self._agent_configs = agent_configs\n    self._builders = decentralized_factories.builder_factory(\n        agent_types, agent_configs)\n    self._num_agents = len(self._builders)\n    self._init_policy_network_fn = init_policy_network_fn\n\n  def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: ma_types.MultiAgentPolicyNetworks,\n  ) -> List[reverb.Table]:\n    \"\"\"Returns replay tables for all agents.\n\n    Args:\n      environment_spec: the (multiagent) environment spec, which will be\n        factorized into single-agent specs for replay table initialization.\n      policy: the (multiagent) mapping from agent ID to the corresponding\n        agent's policy, used to get the correct extras_spec.\n    \"\"\"\n    replay_tables = []\n    for agent_id, builder in self._builders.items():\n      single_agent_spec = ma_utils.get_agent_spec(environment_spec, agent_id)\n      replay_tables += builder.make_replay_tables(single_agent_spec,\n                                                  policy[agent_id])\n    return replay_tables\n\n  def make_dataset_iterator(\n      self,\n      replay_client: reverb.Client) -> Iterator[ma_types.MultiAgentSample]:\n    # Zipping stores sub-iterators in the order dictated by\n    # self._builders.values(), which are insertion-ordered in Python3.7+.\n    # Hence, later unzipping (in make_learner()) and accessing the iterators\n    # via the same self._builders.items() dict ordering should be safe.\n    return zip(*[\n        b.make_dataset_iterator(replay_client) for b in self._builders.values()\n    ])\n\n  def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: ma_types.MultiAgentNetworks,\n      dataset: Iterator[ma_types.MultiAgentSample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: Optional[specs.EnvironmentSpec] = None,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None\n  ) -> learner_set.SynchronousDecentralizedLearnerSet:\n    \"\"\"Returns multiagent learner set.\n\n    Args:\n      random_key: random key.\n      networks: dict of networks, one per learner. Networks can be heterogeneous\n        (i.e., distinct in architecture) across learners.\n      dataset: list of iterators over samples from replay, one per learner.\n      logger_fn: factory providing loggers used for logging progress.\n      environment_spec: the (multiagent) environment spec, which will be\n        factorized into single-agent specs for replay table initialization.\n      replay_client: replay client that is shared amongst the sub-learners.\n      counter: a Counter which allows for recording of counts (learner steps,\n        actor steps, etc.) distributed throughout the agent.\n    \"\"\"\n    parent_counter = counter or counting.Counter()\n    sub_learners = {}\n    unzipped_dataset = iterator_utils.unzip_iterators(\n        dataset, num_sub_iterators=self._num_agents)\n\n    def make_logger_fn(agent_id: str) -> loggers.LoggerFactory:\n      \"\"\"Returns a logger factory for the subagent with the given id.\"\"\"\n\n      def logger_factory(\n          label: loggers.LoggerLabel,\n          steps_key: Optional[loggers.LoggerStepsKey] = None,\n          instance: Optional[loggers.TaskInstance] = None) -> loggers.Logger:\n        return logger_fn(f'{label}{agent_id}', steps_key, instance)\n\n      return logger_factory\n\n    for i_dataset, (agent_id, builder) in enumerate(self._builders.items()):\n      counter = counting.Counter(parent_counter, prefix=f'{agent_id}')\n      single_agent_spec = ma_utils.get_agent_spec(environment_spec, agent_id)\n      random_key, learner_key = jax.random.split(random_key)\n      sub_learners[agent_id] = builder.make_learner(\n          learner_key,\n          networks[agent_id],\n          unzipped_dataset[i_dataset],\n          logger_fn=make_logger_fn(agent_id),\n          environment_spec=single_agent_spec,\n          replay_client=replay_client,\n          counter=counter)\n    return learner_set.SynchronousDecentralizedLearnerSet(\n        sub_learners, separator=VARIABLE_SEPARATOR)\n\n  def make_adder(  # Internal pytype check.\n      self,\n      replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec] = None,\n      policy: Optional[ma_types.MultiAgentPolicyNetworks] = None,\n  ) -> Mapping[ma_types.AgentID, Optional[adders.Adder]]:\n    del environment_spec, policy  # Unused.\n    return {\n        agent_id:\n        b.make_adder(replay_client, environment_spec=None, policy=None)\n        for agent_id, b in self._builders.items()\n    }\n\n  def make_actor(  # Internal pytype check.\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: ma_types.MultiAgentPolicyNetworks,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[Mapping[ma_types.AgentID, adders.Adder]] = None,\n  ) -> core.Actor:\n    \"\"\"Returns simultaneous-acting multiagent actor instance.\n\n    Args:\n      random_key: random key.\n      policy: dict of policies, one for each actor. Policies can\n        be heterogeneous (i.e., distinct in architecture) across actors.\n      environment_spec: the (multiagent) environment spec, which will be\n        factorized into single-agent specs for replay table initialization.\n      variable_source: an optional LearnerSet. Each sub_actor pulls its local\n        variables from variable_source.\n      adder: how data is recorded (e.g., added to replay) for each actor.\n    \"\"\"\n    if adder is None:\n      adder = {agent_id: None for agent_id in policy.keys()}\n\n    sub_actors = {}\n    for agent_id, builder in self._builders.items():\n      single_agent_spec = ma_utils.get_agent_spec(environment_spec, agent_id)\n      random_key, actor_key = jax.random.split(random_key)\n      # Adds a prefix to each sub-actor's variable names to ensure the correct\n      # sub-learner is queried for variables.\n      sub_variable_source = PrefixedVariableSource(\n          variable_source, f'{agent_id}{VARIABLE_SEPARATOR}')\n      sub_actors[agent_id] = builder.make_actor(actor_key, policy[agent_id],\n                                                single_agent_spec,\n                                                sub_variable_source,\n                                                adder[agent_id])\n    return actor.SimultaneousActingMultiAgentActor(sub_actors)\n\n  def make_policy(\n      self,\n      networks: ma_types.MultiAgentNetworks,\n      environment_spec: specs.EnvironmentSpec,\n      evaluation: bool = False) -> ma_types.MultiAgentPolicyNetworks:\n    return decentralized_factories.policy_network_factory(\n        networks,\n        environment_spec,\n        self._agent_types,\n        self._agent_configs,\n        eval_mode=evaluation,\n        init_policy_network_fn=self._init_policy_network_fn)",
  "def __init__(self, source: core.VariableSource, prefix: str):\n    self._source = source\n    self._prefix = prefix",
  "def get_variables(self, names: Sequence[str]) -> List[types.NestedArray]:\n    return self._source.get_variables([self._prefix + name for name in names])",
  "def __init__(\n      self,\n      agent_types: Dict[ma_types.AgentID, ma_types.GenericAgent],\n      agent_configs: Dict[ma_types.AgentID, ma_types.AgentConfig],\n      init_policy_network_fn: Optional[ma_types.InitPolicyNetworkFn] = None):\n    \"\"\"Initializer.\n\n    Args:\n      agent_types: Dict mapping agent IDs to their types.\n      agent_configs: Dict mapping agent IDs to their configs.\n      init_policy_network_fn: Optional custom policy network initializer\n        function.\n    \"\"\"\n\n    self._agent_types = agent_types\n    self._agent_configs = agent_configs\n    self._builders = decentralized_factories.builder_factory(\n        agent_types, agent_configs)\n    self._num_agents = len(self._builders)\n    self._init_policy_network_fn = init_policy_network_fn",
  "def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: ma_types.MultiAgentPolicyNetworks,\n  ) -> List[reverb.Table]:\n    \"\"\"Returns replay tables for all agents.\n\n    Args:\n      environment_spec: the (multiagent) environment spec, which will be\n        factorized into single-agent specs for replay table initialization.\n      policy: the (multiagent) mapping from agent ID to the corresponding\n        agent's policy, used to get the correct extras_spec.\n    \"\"\"\n    replay_tables = []\n    for agent_id, builder in self._builders.items():\n      single_agent_spec = ma_utils.get_agent_spec(environment_spec, agent_id)\n      replay_tables += builder.make_replay_tables(single_agent_spec,\n                                                  policy[agent_id])\n    return replay_tables",
  "def make_dataset_iterator(\n      self,\n      replay_client: reverb.Client) -> Iterator[ma_types.MultiAgentSample]:\n    # Zipping stores sub-iterators in the order dictated by\n    # self._builders.values(), which are insertion-ordered in Python3.7+.\n    # Hence, later unzipping (in make_learner()) and accessing the iterators\n    # via the same self._builders.items() dict ordering should be safe.\n    return zip(*[\n        b.make_dataset_iterator(replay_client) for b in self._builders.values()\n    ])",
  "def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: ma_types.MultiAgentNetworks,\n      dataset: Iterator[ma_types.MultiAgentSample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: Optional[specs.EnvironmentSpec] = None,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None\n  ) -> learner_set.SynchronousDecentralizedLearnerSet:\n    \"\"\"Returns multiagent learner set.\n\n    Args:\n      random_key: random key.\n      networks: dict of networks, one per learner. Networks can be heterogeneous\n        (i.e., distinct in architecture) across learners.\n      dataset: list of iterators over samples from replay, one per learner.\n      logger_fn: factory providing loggers used for logging progress.\n      environment_spec: the (multiagent) environment spec, which will be\n        factorized into single-agent specs for replay table initialization.\n      replay_client: replay client that is shared amongst the sub-learners.\n      counter: a Counter which allows for recording of counts (learner steps,\n        actor steps, etc.) distributed throughout the agent.\n    \"\"\"\n    parent_counter = counter or counting.Counter()\n    sub_learners = {}\n    unzipped_dataset = iterator_utils.unzip_iterators(\n        dataset, num_sub_iterators=self._num_agents)\n\n    def make_logger_fn(agent_id: str) -> loggers.LoggerFactory:\n      \"\"\"Returns a logger factory for the subagent with the given id.\"\"\"\n\n      def logger_factory(\n          label: loggers.LoggerLabel,\n          steps_key: Optional[loggers.LoggerStepsKey] = None,\n          instance: Optional[loggers.TaskInstance] = None) -> loggers.Logger:\n        return logger_fn(f'{label}{agent_id}', steps_key, instance)\n\n      return logger_factory\n\n    for i_dataset, (agent_id, builder) in enumerate(self._builders.items()):\n      counter = counting.Counter(parent_counter, prefix=f'{agent_id}')\n      single_agent_spec = ma_utils.get_agent_spec(environment_spec, agent_id)\n      random_key, learner_key = jax.random.split(random_key)\n      sub_learners[agent_id] = builder.make_learner(\n          learner_key,\n          networks[agent_id],\n          unzipped_dataset[i_dataset],\n          logger_fn=make_logger_fn(agent_id),\n          environment_spec=single_agent_spec,\n          replay_client=replay_client,\n          counter=counter)\n    return learner_set.SynchronousDecentralizedLearnerSet(\n        sub_learners, separator=VARIABLE_SEPARATOR)",
  "def make_adder(  # Internal pytype check.\n      self,\n      replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec] = None,\n      policy: Optional[ma_types.MultiAgentPolicyNetworks] = None,\n  ) -> Mapping[ma_types.AgentID, Optional[adders.Adder]]:\n    del environment_spec, policy  # Unused.\n    return {\n        agent_id:\n        b.make_adder(replay_client, environment_spec=None, policy=None)\n        for agent_id, b in self._builders.items()\n    }",
  "def make_actor(  # Internal pytype check.\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: ma_types.MultiAgentPolicyNetworks,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[Mapping[ma_types.AgentID, adders.Adder]] = None,\n  ) -> core.Actor:\n    \"\"\"Returns simultaneous-acting multiagent actor instance.\n\n    Args:\n      random_key: random key.\n      policy: dict of policies, one for each actor. Policies can\n        be heterogeneous (i.e., distinct in architecture) across actors.\n      environment_spec: the (multiagent) environment spec, which will be\n        factorized into single-agent specs for replay table initialization.\n      variable_source: an optional LearnerSet. Each sub_actor pulls its local\n        variables from variable_source.\n      adder: how data is recorded (e.g., added to replay) for each actor.\n    \"\"\"\n    if adder is None:\n      adder = {agent_id: None for agent_id in policy.keys()}\n\n    sub_actors = {}\n    for agent_id, builder in self._builders.items():\n      single_agent_spec = ma_utils.get_agent_spec(environment_spec, agent_id)\n      random_key, actor_key = jax.random.split(random_key)\n      # Adds a prefix to each sub-actor's variable names to ensure the correct\n      # sub-learner is queried for variables.\n      sub_variable_source = PrefixedVariableSource(\n          variable_source, f'{agent_id}{VARIABLE_SEPARATOR}')\n      sub_actors[agent_id] = builder.make_actor(actor_key, policy[agent_id],\n                                                single_agent_spec,\n                                                sub_variable_source,\n                                                adder[agent_id])\n    return actor.SimultaneousActingMultiAgentActor(sub_actors)",
  "def make_policy(\n      self,\n      networks: ma_types.MultiAgentNetworks,\n      environment_spec: specs.EnvironmentSpec,\n      evaluation: bool = False) -> ma_types.MultiAgentPolicyNetworks:\n    return decentralized_factories.policy_network_factory(\n        networks,\n        environment_spec,\n        self._agent_types,\n        self._agent_configs,\n        eval_mode=evaluation,\n        init_policy_network_fn=self._init_policy_network_fn)",
  "def make_logger_fn(agent_id: str) -> loggers.LoggerFactory:\n      \"\"\"Returns a logger factory for the subagent with the given id.\"\"\"\n\n      def logger_factory(\n          label: loggers.LoggerLabel,\n          steps_key: Optional[loggers.LoggerStepsKey] = None,\n          instance: Optional[loggers.TaskInstance] = None) -> loggers.Logger:\n        return logger_fn(f'{label}{agent_id}', steps_key, instance)\n\n      return logger_factory",
  "def logger_factory(\n          label: loggers.LoggerLabel,\n          steps_key: Optional[loggers.LoggerStepsKey] = None,\n          instance: Optional[loggers.TaskInstance] = None) -> loggers.Logger:\n        return logger_fn(f'{label}{agent_id}', steps_key, instance)",
  "class SimultaneousActingMultiAgentActor(core.Actor):\n  \"\"\"Simultaneous-move actor (see README.md for expected environment interface).\"\"\"\n\n  def __init__(self, actors: Dict[ma_types.AgentID, core.Actor]):\n    \"\"\"Initializer.\n\n    Args:\n      actors: a dict specifying sub-actors.\n    \"\"\"\n    self._actors = actors\n\n  def select_action(\n      self, observation: Dict[ma_types.AgentID, networks.Observation]\n  ) -> Dict[ma_types.AgentID, networks.Action]:\n    return {\n        actor_id: actor.select_action(observation[actor_id])\n        for actor_id, actor in self._actors.items()\n    }\n\n  def observe_first(self, timestep: dm_env.TimeStep):\n    for actor_id, actor in self._actors.items():\n      sub_timestep = ma_utils.get_agent_timestep(timestep, actor_id)\n      actor.observe_first(sub_timestep)\n\n  def observe(self, actions: Dict[ma_types.AgentID, networks.Action],\n              next_timestep: dm_env.TimeStep):\n    for actor_id, actor in self._actors.items():\n      sub_next_timestep = ma_utils.get_agent_timestep(next_timestep, actor_id)\n      actor.observe(actions[actor_id], sub_next_timestep)\n\n  def update(self, wait: bool = False):\n    for actor in self._actors.values():\n      actor.update(wait=wait)",
  "def __init__(self, actors: Dict[ma_types.AgentID, core.Actor]):\n    \"\"\"Initializer.\n\n    Args:\n      actors: a dict specifying sub-actors.\n    \"\"\"\n    self._actors = actors",
  "def select_action(\n      self, observation: Dict[ma_types.AgentID, networks.Observation]\n  ) -> Dict[ma_types.AgentID, networks.Action]:\n    return {\n        actor_id: actor.select_action(observation[actor_id])\n        for actor_id, actor in self._actors.items()\n    }",
  "def observe_first(self, timestep: dm_env.TimeStep):\n    for actor_id, actor in self._actors.items():\n      sub_timestep = ma_utils.get_agent_timestep(timestep, actor_id)\n      actor.observe_first(sub_timestep)",
  "def observe(self, actions: Dict[ma_types.AgentID, networks.Action],\n              next_timestep: dm_env.TimeStep):\n    for actor_id, actor in self._actors.items():\n      sub_next_timestep = ma_utils.get_agent_timestep(next_timestep, actor_id)\n      actor.observe(actions[actor_id], sub_next_timestep)",
  "def update(self, wait: bool = False):\n    for actor in self._actors.values():\n      actor.update(wait=wait)",
  "class TrainingState(NamedTuple):\n  \"\"\"Contains training state for the learner.\"\"\"\n  policy_optimizer_state: optax.OptState\n  q_optimizer_state: optax.OptState\n  policy_params: networks_lib.Params\n  q_params: networks_lib.Params\n  target_q_params: networks_lib.Params\n  key: networks_lib.PRNGKey\n  alpha_optimizer_state: Optional[optax.OptState] = None\n  alpha_params: Optional[networks_lib.Params] = None",
  "class SACLearner(acme.Learner):\n  \"\"\"SAC learner.\"\"\"\n\n  _state: TrainingState\n\n  def __init__(\n      self,\n      networks: sac_networks.SACNetworks,\n      rng: jnp.ndarray,\n      iterator: Iterator[reverb.ReplaySample],\n      policy_optimizer: optax.GradientTransformation,\n      q_optimizer: optax.GradientTransformation,\n      tau: float = 0.005,\n      reward_scale: float = 1.0,\n      discount: float = 0.99,\n      entropy_coefficient: Optional[float] = None,\n      target_entropy: float = 0,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      num_sgd_steps_per_step: int = 1):\n    \"\"\"Initialize the SAC learner.\n\n    Args:\n      networks: SAC networks\n      rng: a key for random number generation.\n      iterator: an iterator over training data.\n      policy_optimizer: the policy optimizer.\n      q_optimizer: the Q-function optimizer.\n      tau: target smoothing coefficient.\n      reward_scale: reward scale.\n      discount: discount to use for TD updates.\n      entropy_coefficient: coefficient applied to the entropy bonus. If None, an\n        adaptative coefficient will be used.\n      target_entropy: Used to normalize entropy. Only used when\n        entropy_coefficient is None.\n      counter: counter object used to keep track of steps.\n      logger: logger object to be used by learner.\n      num_sgd_steps_per_step: number of sgd steps to perform per learner 'step'.\n    \"\"\"\n    adaptive_entropy_coefficient = entropy_coefficient is None\n    if adaptive_entropy_coefficient:\n      # alpha is the temperature parameter that determines the relative\n      # importance of the entropy term versus the reward.\n      log_alpha = jnp.asarray(0., dtype=jnp.float32)\n      alpha_optimizer = optax.adam(learning_rate=3e-4)\n      alpha_optimizer_state = alpha_optimizer.init(log_alpha)\n    else:\n      if target_entropy:\n        raise ValueError('target_entropy should not be set when '\n                         'entropy_coefficient is provided')\n\n    def alpha_loss(log_alpha: jnp.ndarray,\n                   policy_params: networks_lib.Params,\n                   transitions: types.Transition,\n                   key: networks_lib.PRNGKey) -> jnp.ndarray:\n      \"\"\"Eq 18 from https://arxiv.org/pdf/1812.05905.pdf.\"\"\"\n      dist_params = networks.policy_network.apply(\n          policy_params, transitions.observation)\n      action = networks.sample(dist_params, key)\n      log_prob = networks.log_prob(dist_params, action)\n      alpha = jnp.exp(log_alpha)\n      alpha_loss = alpha * jax.lax.stop_gradient(-log_prob - target_entropy)\n      return jnp.mean(alpha_loss)\n\n    def critic_loss(q_params: networks_lib.Params,\n                    policy_params: networks_lib.Params,\n                    target_q_params: networks_lib.Params,\n                    alpha: jnp.ndarray,\n                    transitions: types.Transition,\n                    key: networks_lib.PRNGKey) -> jnp.ndarray:\n      q_old_action = networks.q_network.apply(\n          q_params, transitions.observation, transitions.action)\n      next_dist_params = networks.policy_network.apply(\n          policy_params, transitions.next_observation)\n      next_action = networks.sample(next_dist_params, key)\n      next_log_prob = networks.log_prob(next_dist_params, next_action)\n      next_q = networks.q_network.apply(\n          target_q_params, transitions.next_observation, next_action)\n      next_v = jnp.min(next_q, axis=-1) - alpha * next_log_prob\n      target_q = jax.lax.stop_gradient(transitions.reward * reward_scale +\n                                       transitions.discount * discount * next_v)\n      q_error = q_old_action - jnp.expand_dims(target_q, -1)\n      q_loss = 0.5 * jnp.mean(jnp.square(q_error))\n      return q_loss\n\n    def actor_loss(policy_params: networks_lib.Params,\n                   q_params: networks_lib.Params,\n                   alpha: jnp.ndarray,\n                   transitions: types.Transition,\n                   key: networks_lib.PRNGKey) -> jnp.ndarray:\n      dist_params = networks.policy_network.apply(\n          policy_params, transitions.observation)\n      action = networks.sample(dist_params, key)\n      log_prob = networks.log_prob(dist_params, action)\n      q_action = networks.q_network.apply(\n          q_params, transitions.observation, action)\n      min_q = jnp.min(q_action, axis=-1)\n      actor_loss = alpha * log_prob - min_q\n      return jnp.mean(actor_loss)\n\n    alpha_grad = jax.value_and_grad(alpha_loss)\n    critic_grad = jax.value_and_grad(critic_loss)\n    actor_grad = jax.value_and_grad(actor_loss)\n\n    def update_step(\n        state: TrainingState,\n        transitions: types.Transition,\n    ) -> Tuple[TrainingState, Dict[str, jnp.ndarray]]:\n\n      key, key_alpha, key_critic, key_actor = jax.random.split(state.key, 4)\n      if adaptive_entropy_coefficient:\n        alpha_loss, alpha_grads = alpha_grad(state.alpha_params,\n                                             state.policy_params, transitions,\n                                             key_alpha)\n        alpha = jnp.exp(state.alpha_params)\n      else:\n        alpha = entropy_coefficient\n      critic_loss, critic_grads = critic_grad(state.q_params,\n                                              state.policy_params,\n                                              state.target_q_params, alpha,\n                                              transitions, key_critic)\n      actor_loss, actor_grads = actor_grad(state.policy_params, state.q_params,\n                                           alpha, transitions, key_actor)\n\n      # Apply policy gradients\n      actor_update, policy_optimizer_state = policy_optimizer.update(\n          actor_grads, state.policy_optimizer_state)\n      policy_params = optax.apply_updates(state.policy_params, actor_update)\n\n      # Apply critic gradients\n      critic_update, q_optimizer_state = q_optimizer.update(\n          critic_grads, state.q_optimizer_state)\n      q_params = optax.apply_updates(state.q_params, critic_update)\n\n      new_target_q_params = jax.tree_map(lambda x, y: x * (1 - tau) + y * tau,\n                                         state.target_q_params, q_params)\n\n      metrics = {\n          'critic_loss': critic_loss,\n          'actor_loss': actor_loss,\n      }\n\n      new_state = TrainingState(\n          policy_optimizer_state=policy_optimizer_state,\n          q_optimizer_state=q_optimizer_state,\n          policy_params=policy_params,\n          q_params=q_params,\n          target_q_params=new_target_q_params,\n          key=key,\n      )\n      if adaptive_entropy_coefficient:\n        # Apply alpha gradients\n        alpha_update, alpha_optimizer_state = alpha_optimizer.update(\n            alpha_grads, state.alpha_optimizer_state)\n        alpha_params = optax.apply_updates(state.alpha_params, alpha_update)\n        metrics.update({\n            'alpha_loss': alpha_loss,\n            'alpha': jnp.exp(alpha_params),\n        })\n        new_state = new_state._replace(\n            alpha_optimizer_state=alpha_optimizer_state,\n            alpha_params=alpha_params)\n\n      metrics['rewards_mean'] = jnp.mean(\n          jnp.abs(jnp.mean(transitions.reward, axis=0)))\n      metrics['rewards_std'] = jnp.std(transitions.reward, axis=0)\n\n      return new_state, metrics\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger(\n        'learner',\n        asynchronous=True,\n        serialize_fn=utils.fetch_devicearray,\n        steps_key=self._counter.get_steps_key())\n\n    # Iterator on demonstration transitions.\n    self._iterator = iterator\n\n    update_step = utils.process_multiple_batches(update_step,\n                                                 num_sgd_steps_per_step)\n    # Use the JIT compiler.\n    self._update_step = jax.jit(update_step)\n\n    def make_initial_state(key: networks_lib.PRNGKey) -> TrainingState:\n      \"\"\"Initialises the training state (parameters and optimiser state).\"\"\"\n      key_policy, key_q, key = jax.random.split(key, 3)\n\n      policy_params = networks.policy_network.init(key_policy)\n      policy_optimizer_state = policy_optimizer.init(policy_params)\n\n      q_params = networks.q_network.init(key_q)\n      q_optimizer_state = q_optimizer.init(q_params)\n\n      state = TrainingState(\n          policy_optimizer_state=policy_optimizer_state,\n          q_optimizer_state=q_optimizer_state,\n          policy_params=policy_params,\n          q_params=q_params,\n          target_q_params=q_params,\n          key=key)\n\n      if adaptive_entropy_coefficient:\n        state = state._replace(alpha_optimizer_state=alpha_optimizer_state,\n                               alpha_params=log_alpha)\n      return state\n\n    # Create initial state.\n    self._state = make_initial_state(rng)\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None\n\n  def step(self):\n    sample = next(self._iterator)\n    transitions = types.Transition(*sample.data)\n\n    self._state, metrics = self._update_step(self._state, transitions)\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Increment counts and record the current time\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n\n    # Attempts to write the logs.\n    self._logger.write({**metrics, **counts})\n\n  def get_variables(self, names: List[str]) -> List[Any]:\n    variables = {\n        'policy': self._state.policy_params,\n        'critic': self._state.q_params,\n    }\n    return [variables[name] for name in names]\n\n  def save(self) -> TrainingState:\n    return self._state\n\n  def restore(self, state: TrainingState):\n    self._state = state",
  "def __init__(\n      self,\n      networks: sac_networks.SACNetworks,\n      rng: jnp.ndarray,\n      iterator: Iterator[reverb.ReplaySample],\n      policy_optimizer: optax.GradientTransformation,\n      q_optimizer: optax.GradientTransformation,\n      tau: float = 0.005,\n      reward_scale: float = 1.0,\n      discount: float = 0.99,\n      entropy_coefficient: Optional[float] = None,\n      target_entropy: float = 0,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      num_sgd_steps_per_step: int = 1):\n    \"\"\"Initialize the SAC learner.\n\n    Args:\n      networks: SAC networks\n      rng: a key for random number generation.\n      iterator: an iterator over training data.\n      policy_optimizer: the policy optimizer.\n      q_optimizer: the Q-function optimizer.\n      tau: target smoothing coefficient.\n      reward_scale: reward scale.\n      discount: discount to use for TD updates.\n      entropy_coefficient: coefficient applied to the entropy bonus. If None, an\n        adaptative coefficient will be used.\n      target_entropy: Used to normalize entropy. Only used when\n        entropy_coefficient is None.\n      counter: counter object used to keep track of steps.\n      logger: logger object to be used by learner.\n      num_sgd_steps_per_step: number of sgd steps to perform per learner 'step'.\n    \"\"\"\n    adaptive_entropy_coefficient = entropy_coefficient is None\n    if adaptive_entropy_coefficient:\n      # alpha is the temperature parameter that determines the relative\n      # importance of the entropy term versus the reward.\n      log_alpha = jnp.asarray(0., dtype=jnp.float32)\n      alpha_optimizer = optax.adam(learning_rate=3e-4)\n      alpha_optimizer_state = alpha_optimizer.init(log_alpha)\n    else:\n      if target_entropy:\n        raise ValueError('target_entropy should not be set when '\n                         'entropy_coefficient is provided')\n\n    def alpha_loss(log_alpha: jnp.ndarray,\n                   policy_params: networks_lib.Params,\n                   transitions: types.Transition,\n                   key: networks_lib.PRNGKey) -> jnp.ndarray:\n      \"\"\"Eq 18 from https://arxiv.org/pdf/1812.05905.pdf.\"\"\"\n      dist_params = networks.policy_network.apply(\n          policy_params, transitions.observation)\n      action = networks.sample(dist_params, key)\n      log_prob = networks.log_prob(dist_params, action)\n      alpha = jnp.exp(log_alpha)\n      alpha_loss = alpha * jax.lax.stop_gradient(-log_prob - target_entropy)\n      return jnp.mean(alpha_loss)\n\n    def critic_loss(q_params: networks_lib.Params,\n                    policy_params: networks_lib.Params,\n                    target_q_params: networks_lib.Params,\n                    alpha: jnp.ndarray,\n                    transitions: types.Transition,\n                    key: networks_lib.PRNGKey) -> jnp.ndarray:\n      q_old_action = networks.q_network.apply(\n          q_params, transitions.observation, transitions.action)\n      next_dist_params = networks.policy_network.apply(\n          policy_params, transitions.next_observation)\n      next_action = networks.sample(next_dist_params, key)\n      next_log_prob = networks.log_prob(next_dist_params, next_action)\n      next_q = networks.q_network.apply(\n          target_q_params, transitions.next_observation, next_action)\n      next_v = jnp.min(next_q, axis=-1) - alpha * next_log_prob\n      target_q = jax.lax.stop_gradient(transitions.reward * reward_scale +\n                                       transitions.discount * discount * next_v)\n      q_error = q_old_action - jnp.expand_dims(target_q, -1)\n      q_loss = 0.5 * jnp.mean(jnp.square(q_error))\n      return q_loss\n\n    def actor_loss(policy_params: networks_lib.Params,\n                   q_params: networks_lib.Params,\n                   alpha: jnp.ndarray,\n                   transitions: types.Transition,\n                   key: networks_lib.PRNGKey) -> jnp.ndarray:\n      dist_params = networks.policy_network.apply(\n          policy_params, transitions.observation)\n      action = networks.sample(dist_params, key)\n      log_prob = networks.log_prob(dist_params, action)\n      q_action = networks.q_network.apply(\n          q_params, transitions.observation, action)\n      min_q = jnp.min(q_action, axis=-1)\n      actor_loss = alpha * log_prob - min_q\n      return jnp.mean(actor_loss)\n\n    alpha_grad = jax.value_and_grad(alpha_loss)\n    critic_grad = jax.value_and_grad(critic_loss)\n    actor_grad = jax.value_and_grad(actor_loss)\n\n    def update_step(\n        state: TrainingState,\n        transitions: types.Transition,\n    ) -> Tuple[TrainingState, Dict[str, jnp.ndarray]]:\n\n      key, key_alpha, key_critic, key_actor = jax.random.split(state.key, 4)\n      if adaptive_entropy_coefficient:\n        alpha_loss, alpha_grads = alpha_grad(state.alpha_params,\n                                             state.policy_params, transitions,\n                                             key_alpha)\n        alpha = jnp.exp(state.alpha_params)\n      else:\n        alpha = entropy_coefficient\n      critic_loss, critic_grads = critic_grad(state.q_params,\n                                              state.policy_params,\n                                              state.target_q_params, alpha,\n                                              transitions, key_critic)\n      actor_loss, actor_grads = actor_grad(state.policy_params, state.q_params,\n                                           alpha, transitions, key_actor)\n\n      # Apply policy gradients\n      actor_update, policy_optimizer_state = policy_optimizer.update(\n          actor_grads, state.policy_optimizer_state)\n      policy_params = optax.apply_updates(state.policy_params, actor_update)\n\n      # Apply critic gradients\n      critic_update, q_optimizer_state = q_optimizer.update(\n          critic_grads, state.q_optimizer_state)\n      q_params = optax.apply_updates(state.q_params, critic_update)\n\n      new_target_q_params = jax.tree_map(lambda x, y: x * (1 - tau) + y * tau,\n                                         state.target_q_params, q_params)\n\n      metrics = {\n          'critic_loss': critic_loss,\n          'actor_loss': actor_loss,\n      }\n\n      new_state = TrainingState(\n          policy_optimizer_state=policy_optimizer_state,\n          q_optimizer_state=q_optimizer_state,\n          policy_params=policy_params,\n          q_params=q_params,\n          target_q_params=new_target_q_params,\n          key=key,\n      )\n      if adaptive_entropy_coefficient:\n        # Apply alpha gradients\n        alpha_update, alpha_optimizer_state = alpha_optimizer.update(\n            alpha_grads, state.alpha_optimizer_state)\n        alpha_params = optax.apply_updates(state.alpha_params, alpha_update)\n        metrics.update({\n            'alpha_loss': alpha_loss,\n            'alpha': jnp.exp(alpha_params),\n        })\n        new_state = new_state._replace(\n            alpha_optimizer_state=alpha_optimizer_state,\n            alpha_params=alpha_params)\n\n      metrics['rewards_mean'] = jnp.mean(\n          jnp.abs(jnp.mean(transitions.reward, axis=0)))\n      metrics['rewards_std'] = jnp.std(transitions.reward, axis=0)\n\n      return new_state, metrics\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger(\n        'learner',\n        asynchronous=True,\n        serialize_fn=utils.fetch_devicearray,\n        steps_key=self._counter.get_steps_key())\n\n    # Iterator on demonstration transitions.\n    self._iterator = iterator\n\n    update_step = utils.process_multiple_batches(update_step,\n                                                 num_sgd_steps_per_step)\n    # Use the JIT compiler.\n    self._update_step = jax.jit(update_step)\n\n    def make_initial_state(key: networks_lib.PRNGKey) -> TrainingState:\n      \"\"\"Initialises the training state (parameters and optimiser state).\"\"\"\n      key_policy, key_q, key = jax.random.split(key, 3)\n\n      policy_params = networks.policy_network.init(key_policy)\n      policy_optimizer_state = policy_optimizer.init(policy_params)\n\n      q_params = networks.q_network.init(key_q)\n      q_optimizer_state = q_optimizer.init(q_params)\n\n      state = TrainingState(\n          policy_optimizer_state=policy_optimizer_state,\n          q_optimizer_state=q_optimizer_state,\n          policy_params=policy_params,\n          q_params=q_params,\n          target_q_params=q_params,\n          key=key)\n\n      if adaptive_entropy_coefficient:\n        state = state._replace(alpha_optimizer_state=alpha_optimizer_state,\n                               alpha_params=log_alpha)\n      return state\n\n    # Create initial state.\n    self._state = make_initial_state(rng)\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None",
  "def step(self):\n    sample = next(self._iterator)\n    transitions = types.Transition(*sample.data)\n\n    self._state, metrics = self._update_step(self._state, transitions)\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Increment counts and record the current time\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n\n    # Attempts to write the logs.\n    self._logger.write({**metrics, **counts})",
  "def get_variables(self, names: List[str]) -> List[Any]:\n    variables = {\n        'policy': self._state.policy_params,\n        'critic': self._state.q_params,\n    }\n    return [variables[name] for name in names]",
  "def save(self) -> TrainingState:\n    return self._state",
  "def restore(self, state: TrainingState):\n    self._state = state",
  "def alpha_loss(log_alpha: jnp.ndarray,\n                   policy_params: networks_lib.Params,\n                   transitions: types.Transition,\n                   key: networks_lib.PRNGKey) -> jnp.ndarray:\n      \"\"\"Eq 18 from https://arxiv.org/pdf/1812.05905.pdf.\"\"\"\n      dist_params = networks.policy_network.apply(\n          policy_params, transitions.observation)\n      action = networks.sample(dist_params, key)\n      log_prob = networks.log_prob(dist_params, action)\n      alpha = jnp.exp(log_alpha)\n      alpha_loss = alpha * jax.lax.stop_gradient(-log_prob - target_entropy)\n      return jnp.mean(alpha_loss)",
  "def critic_loss(q_params: networks_lib.Params,\n                    policy_params: networks_lib.Params,\n                    target_q_params: networks_lib.Params,\n                    alpha: jnp.ndarray,\n                    transitions: types.Transition,\n                    key: networks_lib.PRNGKey) -> jnp.ndarray:\n      q_old_action = networks.q_network.apply(\n          q_params, transitions.observation, transitions.action)\n      next_dist_params = networks.policy_network.apply(\n          policy_params, transitions.next_observation)\n      next_action = networks.sample(next_dist_params, key)\n      next_log_prob = networks.log_prob(next_dist_params, next_action)\n      next_q = networks.q_network.apply(\n          target_q_params, transitions.next_observation, next_action)\n      next_v = jnp.min(next_q, axis=-1) - alpha * next_log_prob\n      target_q = jax.lax.stop_gradient(transitions.reward * reward_scale +\n                                       transitions.discount * discount * next_v)\n      q_error = q_old_action - jnp.expand_dims(target_q, -1)\n      q_loss = 0.5 * jnp.mean(jnp.square(q_error))\n      return q_loss",
  "def actor_loss(policy_params: networks_lib.Params,\n                   q_params: networks_lib.Params,\n                   alpha: jnp.ndarray,\n                   transitions: types.Transition,\n                   key: networks_lib.PRNGKey) -> jnp.ndarray:\n      dist_params = networks.policy_network.apply(\n          policy_params, transitions.observation)\n      action = networks.sample(dist_params, key)\n      log_prob = networks.log_prob(dist_params, action)\n      q_action = networks.q_network.apply(\n          q_params, transitions.observation, action)\n      min_q = jnp.min(q_action, axis=-1)\n      actor_loss = alpha * log_prob - min_q\n      return jnp.mean(actor_loss)",
  "def update_step(\n        state: TrainingState,\n        transitions: types.Transition,\n    ) -> Tuple[TrainingState, Dict[str, jnp.ndarray]]:\n\n      key, key_alpha, key_critic, key_actor = jax.random.split(state.key, 4)\n      if adaptive_entropy_coefficient:\n        alpha_loss, alpha_grads = alpha_grad(state.alpha_params,\n                                             state.policy_params, transitions,\n                                             key_alpha)\n        alpha = jnp.exp(state.alpha_params)\n      else:\n        alpha = entropy_coefficient\n      critic_loss, critic_grads = critic_grad(state.q_params,\n                                              state.policy_params,\n                                              state.target_q_params, alpha,\n                                              transitions, key_critic)\n      actor_loss, actor_grads = actor_grad(state.policy_params, state.q_params,\n                                           alpha, transitions, key_actor)\n\n      # Apply policy gradients\n      actor_update, policy_optimizer_state = policy_optimizer.update(\n          actor_grads, state.policy_optimizer_state)\n      policy_params = optax.apply_updates(state.policy_params, actor_update)\n\n      # Apply critic gradients\n      critic_update, q_optimizer_state = q_optimizer.update(\n          critic_grads, state.q_optimizer_state)\n      q_params = optax.apply_updates(state.q_params, critic_update)\n\n      new_target_q_params = jax.tree_map(lambda x, y: x * (1 - tau) + y * tau,\n                                         state.target_q_params, q_params)\n\n      metrics = {\n          'critic_loss': critic_loss,\n          'actor_loss': actor_loss,\n      }\n\n      new_state = TrainingState(\n          policy_optimizer_state=policy_optimizer_state,\n          q_optimizer_state=q_optimizer_state,\n          policy_params=policy_params,\n          q_params=q_params,\n          target_q_params=new_target_q_params,\n          key=key,\n      )\n      if adaptive_entropy_coefficient:\n        # Apply alpha gradients\n        alpha_update, alpha_optimizer_state = alpha_optimizer.update(\n            alpha_grads, state.alpha_optimizer_state)\n        alpha_params = optax.apply_updates(state.alpha_params, alpha_update)\n        metrics.update({\n            'alpha_loss': alpha_loss,\n            'alpha': jnp.exp(alpha_params),\n        })\n        new_state = new_state._replace(\n            alpha_optimizer_state=alpha_optimizer_state,\n            alpha_params=alpha_params)\n\n      metrics['rewards_mean'] = jnp.mean(\n          jnp.abs(jnp.mean(transitions.reward, axis=0)))\n      metrics['rewards_std'] = jnp.std(transitions.reward, axis=0)\n\n      return new_state, metrics",
  "def make_initial_state(key: networks_lib.PRNGKey) -> TrainingState:\n      \"\"\"Initialises the training state (parameters and optimiser state).\"\"\"\n      key_policy, key_q, key = jax.random.split(key, 3)\n\n      policy_params = networks.policy_network.init(key_policy)\n      policy_optimizer_state = policy_optimizer.init(policy_params)\n\n      q_params = networks.q_network.init(key_q)\n      q_optimizer_state = q_optimizer.init(q_params)\n\n      state = TrainingState(\n          policy_optimizer_state=policy_optimizer_state,\n          q_optimizer_state=q_optimizer_state,\n          policy_params=policy_params,\n          q_params=q_params,\n          target_q_params=q_params,\n          key=key)\n\n      if adaptive_entropy_coefficient:\n        state = state._replace(alpha_optimizer_state=alpha_optimizer_state,\n                               alpha_params=log_alpha)\n      return state",
  "class SACNetworks:\n  \"\"\"Network and pure functions for the SAC agent..\"\"\"\n  policy_network: networks_lib.FeedForwardNetwork\n  q_network: networks_lib.FeedForwardNetwork\n  log_prob: networks_lib.LogProbFn\n  sample: networks_lib.SampleFn\n  sample_eval: Optional[networks_lib.SampleFn] = None",
  "def default_models_to_snapshot(\n    networks: SACNetworks,\n    spec: specs.EnvironmentSpec):\n  \"\"\"Defines default models to be snapshotted.\"\"\"\n  dummy_obs = utils.zeros_like(spec.observations)\n  dummy_action = utils.zeros_like(spec.actions)\n  dummy_key = jax.random.PRNGKey(0)\n\n  def q_network(\n      source: core.VariableSource) -> types.ModelToSnapshot:\n    params = source.get_variables(['critic'])[0]\n    return types.ModelToSnapshot(\n        networks.q_network.apply, params,\n        {'obs': dummy_obs, 'action': dummy_action})\n\n  def default_training_actor(\n      source: core.VariableSource) -> types.ModelToSnapshot:\n    params = source.get_variables(['policy'])[0]\n    return types.ModelToSnapshot(apply_policy_and_sample(networks, False),\n                                 params,\n                                 {'key': dummy_key, 'obs': dummy_obs})\n\n  def default_eval_actor(\n      source: core.VariableSource) -> types.ModelToSnapshot:\n    params = source.get_variables(['policy'])[0]\n    return types.ModelToSnapshot(\n        apply_policy_and_sample(networks, True), params,\n        {'key': dummy_key, 'obs': dummy_obs})\n\n  return {\n      'q_network': q_network,\n      'default_training_actor': default_training_actor,\n      'default_eval_actor': default_eval_actor,\n  }",
  "def apply_policy_and_sample(\n    networks: SACNetworks,\n    eval_mode: bool = False) -> actor_core_lib.FeedForwardPolicy:\n  \"\"\"Returns a function that computes actions.\"\"\"\n  sample_fn = networks.sample if not eval_mode else networks.sample_eval\n  if not sample_fn:\n    raise ValueError('sample function is not provided')\n\n  def apply_and_sample(params, key, obs):\n    return sample_fn(networks.policy_network.apply(params, obs), key)\n  return apply_and_sample",
  "def make_networks(\n    spec: specs.EnvironmentSpec,\n    hidden_layer_sizes: Tuple[int, ...] = (256, 256)) -> SACNetworks:\n  \"\"\"Creates networks used by the agent.\"\"\"\n\n  num_dimensions = np.prod(spec.actions.shape, dtype=int)\n\n  def _actor_fn(obs):\n    network = hk.Sequential([\n        hk.nets.MLP(\n            list(hidden_layer_sizes),\n            w_init=hk.initializers.VarianceScaling(1.0, 'fan_in', 'uniform'),\n            activation=jax.nn.relu,\n            activate_final=True),\n        networks_lib.NormalTanhDistribution(num_dimensions),\n    ])\n    return network(obs)\n\n  def _critic_fn(obs, action):\n    network1 = hk.Sequential([\n        hk.nets.MLP(\n            list(hidden_layer_sizes) + [1],\n            w_init=hk.initializers.VarianceScaling(1.0, 'fan_in', 'uniform'),\n            activation=jax.nn.relu),\n    ])\n    network2 = hk.Sequential([\n        hk.nets.MLP(\n            list(hidden_layer_sizes) + [1],\n            w_init=hk.initializers.VarianceScaling(1.0, 'fan_in', 'uniform'),\n            activation=jax.nn.relu),\n    ])\n    input_ = jnp.concatenate([obs, action], axis=-1)\n    value1 = network1(input_)\n    value2 = network2(input_)\n    return jnp.concatenate([value1, value2], axis=-1)\n\n  policy = hk.without_apply_rng(hk.transform(_actor_fn))\n  critic = hk.without_apply_rng(hk.transform(_critic_fn))\n\n  # Create dummy observations and actions to create network parameters.\n  dummy_action = utils.zeros_like(spec.actions)\n  dummy_obs = utils.zeros_like(spec.observations)\n  dummy_action = utils.add_batch_dim(dummy_action)\n  dummy_obs = utils.add_batch_dim(dummy_obs)\n\n  return SACNetworks(\n      policy_network=networks_lib.FeedForwardNetwork(\n          lambda key: policy.init(key, dummy_obs), policy.apply),\n      q_network=networks_lib.FeedForwardNetwork(\n          lambda key: critic.init(key, dummy_obs, dummy_action), critic.apply),\n      log_prob=lambda params, actions: params.log_prob(actions),\n      sample=lambda params, key: params.sample(seed=key),\n      sample_eval=lambda params, key: params.mode())",
  "def q_network(\n      source: core.VariableSource) -> types.ModelToSnapshot:\n    params = source.get_variables(['critic'])[0]\n    return types.ModelToSnapshot(\n        networks.q_network.apply, params,\n        {'obs': dummy_obs, 'action': dummy_action})",
  "def default_training_actor(\n      source: core.VariableSource) -> types.ModelToSnapshot:\n    params = source.get_variables(['policy'])[0]\n    return types.ModelToSnapshot(apply_policy_and_sample(networks, False),\n                                 params,\n                                 {'key': dummy_key, 'obs': dummy_obs})",
  "def default_eval_actor(\n      source: core.VariableSource) -> types.ModelToSnapshot:\n    params = source.get_variables(['policy'])[0]\n    return types.ModelToSnapshot(\n        apply_policy_and_sample(networks, True), params,\n        {'key': dummy_key, 'obs': dummy_obs})",
  "def apply_and_sample(params, key, obs):\n    return sample_fn(networks.policy_network.apply(params, obs), key)",
  "def _actor_fn(obs):\n    network = hk.Sequential([\n        hk.nets.MLP(\n            list(hidden_layer_sizes),\n            w_init=hk.initializers.VarianceScaling(1.0, 'fan_in', 'uniform'),\n            activation=jax.nn.relu,\n            activate_final=True),\n        networks_lib.NormalTanhDistribution(num_dimensions),\n    ])\n    return network(obs)",
  "def _critic_fn(obs, action):\n    network1 = hk.Sequential([\n        hk.nets.MLP(\n            list(hidden_layer_sizes) + [1],\n            w_init=hk.initializers.VarianceScaling(1.0, 'fan_in', 'uniform'),\n            activation=jax.nn.relu),\n    ])\n    network2 = hk.Sequential([\n        hk.nets.MLP(\n            list(hidden_layer_sizes) + [1],\n            w_init=hk.initializers.VarianceScaling(1.0, 'fan_in', 'uniform'),\n            activation=jax.nn.relu),\n    ])\n    input_ = jnp.concatenate([obs, action], axis=-1)\n    value1 = network1(input_)\n    value2 = network2(input_)\n    return jnp.concatenate([value1, value2], axis=-1)",
  "class SACConfig(normalization.InputNormalizerConfig):\n  \"\"\"Configuration options for SAC.\"\"\"\n  # Loss options\n  batch_size: int = 256\n  learning_rate: float = 3e-4\n  reward_scale: float = 1\n  discount: float = 0.99\n  n_step: int = 1\n  # Coefficient applied to the entropy bonus. If None, an adaptative\n  # coefficient will be used.\n  entropy_coefficient: Optional[float] = None\n  target_entropy: float = 0.0\n  # Target smoothing coefficient.\n  tau: float = 0.005\n\n  # Replay options\n  min_replay_size: int = 10000\n  max_replay_size: int = 1000000\n  replay_table_name: str = adders_reverb.DEFAULT_PRIORITY_TABLE\n  prefetch_size: int = 4\n  samples_per_insert: float = 256\n  # Rate to be used for the SampleToInsertRatio rate limitter tolerance.\n  # See a formula in make_replay_tables for more details.\n  samples_per_insert_tolerance_rate: float = 0.1\n\n  # How many gradient updates to perform per step.\n  num_sgd_steps_per_step: int = 1\n\n  input_normalization: Optional[normalization.NormalizationConfig] = None",
  "def target_entropy_from_env_spec(\n    spec: specs.EnvironmentSpec,\n    target_entropy_per_dimension: Optional[float] = None,\n) -> float:\n  \"\"\"A heuristic to determine a target entropy.\n\n  If target_entropy_per_dimension is not specified, the target entropy is\n  computed as \"-num_actions\", otherwise it is\n  \"target_entropy_per_dimension * num_actions\".\n\n  Args:\n    spec: environment spec\n    target_entropy_per_dimension: None or target entropy per action dimension\n\n  Returns:\n    target entropy\n  \"\"\"\n\n  def get_num_actions(action_spec: Any) -> float:\n    \"\"\"Returns a number of actions in the spec.\"\"\"\n    if isinstance(action_spec, specs.BoundedArray):\n      return onp.prod(action_spec.shape, dtype=int)\n    elif isinstance(action_spec, tuple):\n      return sum(get_num_actions(subspace) for subspace in action_spec)\n    else:\n      raise ValueError('Unknown action space type.')\n\n  num_actions = get_num_actions(spec.actions)\n  if target_entropy_per_dimension is None:\n    if not isinstance(spec.actions, specs.BoundedArray) or isinstance(\n        spec.actions, specs.DiscreteArray):\n      raise ValueError('Only accept BoundedArrays for automatic '\n                       f'target_entropy, got: {spec.actions}')\n    if not onp.all(spec.actions.minimum == -1.):\n      raise ValueError(\n          f'Minimum expected to be -1, got: {spec.actions.minimum}')\n    if not onp.all(spec.actions.maximum == 1.):\n      raise ValueError(\n          f'Maximum expected to be 1, got: {spec.actions.maximum}')\n\n    return -num_actions\n  else:\n    return target_entropy_per_dimension * num_actions",
  "def get_num_actions(action_spec: Any) -> float:\n    \"\"\"Returns a number of actions in the spec.\"\"\"\n    if isinstance(action_spec, specs.BoundedArray):\n      return onp.prod(action_spec.shape, dtype=int)\n    elif isinstance(action_spec, tuple):\n      return sum(get_num_actions(subspace) for subspace in action_spec)\n    else:\n      raise ValueError('Unknown action space type.')",
  "class SACBuilder(builders.ActorLearnerBuilder[sac_networks.SACNetworks,\n                                              actor_core_lib.FeedForwardPolicy,\n                                              reverb.ReplaySample]):\n  \"\"\"SAC Builder.\"\"\"\n\n  def __init__(\n      self,\n      config: sac_config.SACConfig,\n  ):\n    \"\"\"Creates a SAC learner, a behavior policy and an eval actor.\n\n    Args:\n      config: a config with SAC hps\n    \"\"\"\n    self._config = config\n\n  def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: sac_networks.SACNetworks,\n      dataset: Iterator[reverb.ReplaySample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    del environment_spec, replay_client\n\n    # Create optimizers\n    policy_optimizer = optax.adam(learning_rate=self._config.learning_rate)\n    q_optimizer = optax.adam(learning_rate=self._config.learning_rate)\n\n    return learning.SACLearner(\n        networks=networks,\n        tau=self._config.tau,\n        discount=self._config.discount,\n        entropy_coefficient=self._config.entropy_coefficient,\n        target_entropy=self._config.target_entropy,\n        rng=random_key,\n        reward_scale=self._config.reward_scale,\n        num_sgd_steps_per_step=self._config.num_sgd_steps_per_step,\n        policy_optimizer=policy_optimizer,\n        q_optimizer=q_optimizer,\n        iterator=dataset,\n        logger=logger_fn('learner'),\n        counter=counter)\n\n  def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: actor_core_lib.FeedForwardPolicy,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> acme.Actor:\n    del environment_spec\n    assert variable_source is not None\n    actor_core = actor_core_lib.batched_feed_forward_to_actor_core(policy)\n    variable_client = variable_utils.VariableClient(\n        variable_source, 'policy', device='cpu')\n    return actors.GenericActor(\n        actor_core, random_key, variable_client, adder, backend='cpu')\n\n  def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: actor_core_lib.FeedForwardPolicy,\n  ) -> List[reverb.Table]:\n    \"\"\"Create tables to insert data into.\"\"\"\n    del policy\n    samples_per_insert_tolerance = (\n        self._config.samples_per_insert_tolerance_rate *\n        self._config.samples_per_insert)\n    error_buffer = self._config.min_replay_size * samples_per_insert_tolerance\n    limiter = rate_limiters.SampleToInsertRatio(\n        min_size_to_sample=self._config.min_replay_size,\n        samples_per_insert=self._config.samples_per_insert,\n        error_buffer=error_buffer)\n    return [\n        reverb.Table(\n            name=self._config.replay_table_name,\n            sampler=reverb.selectors.Uniform(),\n            remover=reverb.selectors.Fifo(),\n            max_size=self._config.max_replay_size,\n            rate_limiter=limiter,\n            signature=adders_reverb.NStepTransitionAdder.signature(\n                environment_spec))\n    ]\n\n  def make_dataset_iterator(\n      self, replay_client: reverb.Client) -> Iterator[reverb.ReplaySample]:\n    \"\"\"Create a dataset iterator to use for learning/updating the agent.\"\"\"\n    dataset = datasets.make_reverb_dataset(\n        table=self._config.replay_table_name,\n        server_address=replay_client.server_address,\n        batch_size=(self._config.batch_size *\n                    self._config.num_sgd_steps_per_step),\n        prefetch_size=self._config.prefetch_size)\n    return utils.device_put(dataset.as_numpy_iterator(), jax.devices()[0])\n\n  def make_adder(\n      self, replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[actor_core_lib.FeedForwardPolicy]\n  ) -> Optional[adders.Adder]:\n    \"\"\"Create an adder which records data generated by the actor/environment.\"\"\"\n    del environment_spec, policy\n    return adders_reverb.NStepTransitionAdder(\n        priority_fns={self._config.replay_table_name: None},\n        client=replay_client,\n        n_step=self._config.n_step,\n        discount=self._config.discount)\n\n  def make_policy(self,\n                  networks: sac_networks.SACNetworks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> actor_core_lib.FeedForwardPolicy:\n    \"\"\"Construct the policy.\"\"\"\n    del environment_spec\n    return sac_networks.apply_policy_and_sample(networks, eval_mode=evaluation)",
  "def __init__(\n      self,\n      config: sac_config.SACConfig,\n  ):\n    \"\"\"Creates a SAC learner, a behavior policy and an eval actor.\n\n    Args:\n      config: a config with SAC hps\n    \"\"\"\n    self._config = config",
  "def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: sac_networks.SACNetworks,\n      dataset: Iterator[reverb.ReplaySample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    del environment_spec, replay_client\n\n    # Create optimizers\n    policy_optimizer = optax.adam(learning_rate=self._config.learning_rate)\n    q_optimizer = optax.adam(learning_rate=self._config.learning_rate)\n\n    return learning.SACLearner(\n        networks=networks,\n        tau=self._config.tau,\n        discount=self._config.discount,\n        entropy_coefficient=self._config.entropy_coefficient,\n        target_entropy=self._config.target_entropy,\n        rng=random_key,\n        reward_scale=self._config.reward_scale,\n        num_sgd_steps_per_step=self._config.num_sgd_steps_per_step,\n        policy_optimizer=policy_optimizer,\n        q_optimizer=q_optimizer,\n        iterator=dataset,\n        logger=logger_fn('learner'),\n        counter=counter)",
  "def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: actor_core_lib.FeedForwardPolicy,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> acme.Actor:\n    del environment_spec\n    assert variable_source is not None\n    actor_core = actor_core_lib.batched_feed_forward_to_actor_core(policy)\n    variable_client = variable_utils.VariableClient(\n        variable_source, 'policy', device='cpu')\n    return actors.GenericActor(\n        actor_core, random_key, variable_client, adder, backend='cpu')",
  "def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: actor_core_lib.FeedForwardPolicy,\n  ) -> List[reverb.Table]:\n    \"\"\"Create tables to insert data into.\"\"\"\n    del policy\n    samples_per_insert_tolerance = (\n        self._config.samples_per_insert_tolerance_rate *\n        self._config.samples_per_insert)\n    error_buffer = self._config.min_replay_size * samples_per_insert_tolerance\n    limiter = rate_limiters.SampleToInsertRatio(\n        min_size_to_sample=self._config.min_replay_size,\n        samples_per_insert=self._config.samples_per_insert,\n        error_buffer=error_buffer)\n    return [\n        reverb.Table(\n            name=self._config.replay_table_name,\n            sampler=reverb.selectors.Uniform(),\n            remover=reverb.selectors.Fifo(),\n            max_size=self._config.max_replay_size,\n            rate_limiter=limiter,\n            signature=adders_reverb.NStepTransitionAdder.signature(\n                environment_spec))\n    ]",
  "def make_dataset_iterator(\n      self, replay_client: reverb.Client) -> Iterator[reverb.ReplaySample]:\n    \"\"\"Create a dataset iterator to use for learning/updating the agent.\"\"\"\n    dataset = datasets.make_reverb_dataset(\n        table=self._config.replay_table_name,\n        server_address=replay_client.server_address,\n        batch_size=(self._config.batch_size *\n                    self._config.num_sgd_steps_per_step),\n        prefetch_size=self._config.prefetch_size)\n    return utils.device_put(dataset.as_numpy_iterator(), jax.devices()[0])",
  "def make_adder(\n      self, replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[actor_core_lib.FeedForwardPolicy]\n  ) -> Optional[adders.Adder]:\n    \"\"\"Create an adder which records data generated by the actor/environment.\"\"\"\n    del environment_spec, policy\n    return adders_reverb.NStepTransitionAdder(\n        priority_fns={self._config.replay_table_name: None},\n        client=replay_client,\n        n_step=self._config.n_step,\n        discount=self._config.discount)",
  "def make_policy(self,\n                  networks: sac_networks.SACNetworks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> actor_core_lib.FeedForwardPolicy:\n    \"\"\"Construct the policy.\"\"\"\n    del environment_spec\n    return sac_networks.apply_policy_and_sample(networks, eval_mode=evaluation)",
  "class PWILAdder(adders.Adder):\n  \"\"\"Adder wrapper substituting PWIL rewards.\"\"\"\n\n  def __init__(self, direct_rl_adder: adders.Adder,\n               pwil_rewarder: rewarder.WassersteinDistanceRewarder):\n    self._adder = direct_rl_adder\n    self._rewarder = pwil_rewarder\n    self._latest_observation = None\n\n  def add_first(self, timestep: dm_env.TimeStep):\n    self._rewarder.reset()\n    self._latest_observation = timestep.observation\n    self._adder.add_first(timestep)\n\n  def add(self,\n          action: types.NestedArray,\n          next_timestep: dm_env.TimeStep,\n          extras: types.NestedArray = ()):\n    updated_timestep = next_timestep._replace(\n        reward=self._rewarder.append_and_compute_reward(\n            observation=self._latest_observation, action=action))\n    self._latest_observation = next_timestep.observation\n    self._adder.add(action, updated_timestep, extras)\n\n  def reset(self):\n    self._latest_observation = None\n    self._adder.reset()",
  "def __init__(self, direct_rl_adder: adders.Adder,\n               pwil_rewarder: rewarder.WassersteinDistanceRewarder):\n    self._adder = direct_rl_adder\n    self._rewarder = pwil_rewarder\n    self._latest_observation = None",
  "def add_first(self, timestep: dm_env.TimeStep):\n    self._rewarder.reset()\n    self._latest_observation = timestep.observation\n    self._adder.add_first(timestep)",
  "def add(self,\n          action: types.NestedArray,\n          next_timestep: dm_env.TimeStep,\n          extras: types.NestedArray = ()):\n    updated_timestep = next_timestep._replace(\n        reward=self._rewarder.append_and_compute_reward(\n            observation=self._latest_observation, action=action))\n    self._latest_observation = next_timestep.observation\n    self._adder.add(action, updated_timestep, extras)",
  "def reset(self):\n    self._latest_observation = None\n    self._adder.reset()",
  "class PWILConfig:\n  \"\"\"Configuration options for PWIL.\n\n  The default values correspond to the experiment setup from the PWIL\n  publication http://arxiv.org/abs/2006.04678.\n  \"\"\"\n\n  # Number of transitions to fill the replay buffer with for pretraining.\n  num_transitions_rb: int = 50000\n\n  # If False, uses only observations for computing the distance; if True, also\n  # uses the actions.\n  use_actions_for_distance: bool = True\n\n  # Scaling for the reward function, see equation (6) in\n  # http://arxiv.org/abs/2006.04678.\n  alpha: float = 5.\n\n  # Controls the kernel size of the reward function, see equation (6)\n  # in http://arxiv.org/abs/2006.04678.\n  beta: float = 5.\n\n  # When False, uses the reward signal from the dataset during prefilling.\n  prefill_constant_reward: bool = True\n\n  num_sgd_steps_per_step: int = 1",
  "class PWILDemonstrations:\n  \"\"\"Unbatched, unshuffled transitions with approximate episode length.\"\"\"\n  demonstrations: Iterator[types.Transition]\n  episode_length: int",
  "def _prefill_with_demonstrations(adder: adders.Adder,\n                                 demonstrations: Sequence[types.Transition],\n                                 reward: Optional[float],\n                                 min_num_transitions: int = 0) -> None:\n  \"\"\"Fill the adder's replay buffer with expert transitions.\n\n  Assumes that the demonstrations dataset stores transitions in order.\n\n  Args:\n    adder: the agent which adds the demonstrations.\n    demonstrations: the expert demonstrations to iterate over.\n    reward: if non-None, populates the environment reward entry of transitions.\n    min_num_transitions: the lower bound on transitions processed, the dataset\n      will be iterated over multiple times if needed. Once at least\n      min_num_transitions are added, the processing is interrupted at the\n      nearest episode end.\n  \"\"\"\n  if not demonstrations:\n    return\n\n  reward = np.float32(reward) if reward is not None else reward\n  remaining_transitions = min_num_transitions\n  step_type = None\n  action = None\n  ts = dm_env.TimeStep(None, None, None, None)  # Unused.\n  while remaining_transitions > 0:\n    # In case we share the adder or demonstrations don't end with\n    # end-of-episode, reset the adder prior to add_first.\n    adder.reset()\n    for transition_num, transition in enumerate(demonstrations):\n      remaining_transitions -= 1\n      discount = np.float32(1.0)\n      ts_reward = reward if reward is not None else transition.reward\n      if step_type == dm_env.StepType.LAST or transition_num == 0:\n        ts = dm_env.TimeStep(dm_env.StepType.FIRST, ts_reward, discount,\n                             transition.observation)\n        adder.add_first(ts)\n\n      observation = transition.next_observation\n      action = transition.action\n      if transition.discount == 0. or transition_num == len(demonstrations) - 1:\n        step_type = dm_env.StepType.LAST\n        discount = np.float32(0.0)\n      else:\n        step_type = dm_env.StepType.MID\n      ts = dm_env.TimeStep(step_type, ts_reward, discount, observation)\n      adder.add(action, ts)\n      if remaining_transitions <= 0:\n        # Note: we could check `step_type == dm_env.StepType.LAST` to stop at an\n        # episode end if possible.\n        break\n\n  # Explicitly finalize the Reverb client writes.\n  adder.reset()",
  "class PWILBuilder(builders.ActorLearnerBuilder[DirectRLNetworks,\n                                               DirectPolicyNetwork,\n                                               reverb.ReplaySample],\n                  Generic[DirectRLNetworks, DirectPolicyNetwork]):\n  \"\"\"PWIL Agent builder.\"\"\"\n\n  def __init__(self,\n               rl_agent: builders.ActorLearnerBuilder[DirectRLNetworks,\n                                                      DirectPolicyNetwork,\n                                                      reverb.ReplaySample],\n               config: pwil_config.PWILConfig,\n               demonstrations_fn: Callable[[], pwil_config.PWILDemonstrations]):\n    \"\"\"Initialize the agent.\n\n    Args:\n      rl_agent: the standard RL algorithm.\n      config: PWIL-specific configuration.\n      demonstrations_fn: A function that returns an iterator over contiguous\n        demonstration transitions, and the average demonstration episode length.\n    \"\"\"\n    self._rl_agent = rl_agent\n    self._config = config\n    self._demonstrations_fn = demonstrations_fn\n    super().__init__()\n\n  def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: DirectRLNetworks,\n      dataset: Iterator[reverb.ReplaySample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    return self._rl_agent.make_learner(\n        random_key=random_key,\n        networks=networks,\n        dataset=dataset,\n        logger_fn=logger_fn,\n        environment_spec=environment_spec,\n        replay_client=replay_client,\n        counter=counter)\n\n  def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: DirectPolicyNetwork,\n  ) -> List[reverb.Table]:\n    return self._rl_agent.make_replay_tables(environment_spec, policy)\n\n  def make_dataset_iterator(  # pytype: disable=signature-mismatch  # overriding-return-type-checks\n      self,\n      replay_client: reverb.Client) -> Optional[Iterator[reverb.ReplaySample]]:\n    # make_dataset_iterator is only called once (per learner), to pass the\n    # iterator to make_learner. By using adders we ensure the transition types\n    # (e.g. n-step transitions) that the direct RL agent expects.\n    if self._config.num_transitions_rb > 0:\n\n      def prefill_thread():\n        # Populating the replay buffer with the direct RL agent guarantees that\n        # a constant reward will be used, not the imitation reward.\n        prefill_reward = (\n            self._config.alpha\n            if self._config.prefill_constant_reward else None)\n        _prefill_with_demonstrations(\n            adder=self._rl_agent.make_adder(replay_client, None, None),\n            demonstrations=list(self._demonstrations_fn().demonstrations),\n            min_num_transitions=self._config.num_transitions_rb,\n            reward=prefill_reward)\n      # Populate the replay buffer in a separate thread, so that the learner\n      # can sample from the buffer, to avoid blocking on the buffer being full.\n      threading.Thread(target=prefill_thread, daemon=True).start()\n\n    return self._rl_agent.make_dataset_iterator(replay_client)\n\n  def make_adder(\n      self,\n      replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[DirectPolicyNetwork],\n  ) -> Optional[adders.Adder]:\n    \"\"\"Creates the adder substituting imitation reward.\"\"\"\n    pwil_demonstrations = self._demonstrations_fn()\n    return pwil_adder.PWILAdder(\n        direct_rl_adder=self._rl_agent.make_adder(replay_client,\n                                                  environment_spec, policy),\n        pwil_rewarder=rewarder.WassersteinDistanceRewarder(\n            demonstrations_it=pwil_demonstrations.demonstrations,\n            episode_length=pwil_demonstrations.episode_length,\n            use_actions_for_distance=self._config.use_actions_for_distance,\n            alpha=self._config.alpha,\n            beta=self._config.beta))\n\n  def make_actor(\n      self,\n      random_key: PRNGKey,\n      policy: DirectPolicyNetwork,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> core.Actor:\n    return self._rl_agent.make_actor(random_key, policy, environment_spec,\n                                     variable_source, adder)\n\n  def make_policy(self,\n                  networks: DirectRLNetworks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> DirectPolicyNetwork:\n    return self._rl_agent.make_policy(networks, environment_spec, evaluation)",
  "def __init__(self,\n               rl_agent: builders.ActorLearnerBuilder[DirectRLNetworks,\n                                                      DirectPolicyNetwork,\n                                                      reverb.ReplaySample],\n               config: pwil_config.PWILConfig,\n               demonstrations_fn: Callable[[], pwil_config.PWILDemonstrations]):\n    \"\"\"Initialize the agent.\n\n    Args:\n      rl_agent: the standard RL algorithm.\n      config: PWIL-specific configuration.\n      demonstrations_fn: A function that returns an iterator over contiguous\n        demonstration transitions, and the average demonstration episode length.\n    \"\"\"\n    self._rl_agent = rl_agent\n    self._config = config\n    self._demonstrations_fn = demonstrations_fn\n    super().__init__()",
  "def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: DirectRLNetworks,\n      dataset: Iterator[reverb.ReplaySample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    return self._rl_agent.make_learner(\n        random_key=random_key,\n        networks=networks,\n        dataset=dataset,\n        logger_fn=logger_fn,\n        environment_spec=environment_spec,\n        replay_client=replay_client,\n        counter=counter)",
  "def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: DirectPolicyNetwork,\n  ) -> List[reverb.Table]:\n    return self._rl_agent.make_replay_tables(environment_spec, policy)",
  "def make_dataset_iterator(  # pytype: disable=signature-mismatch  # overriding-return-type-checks\n      self,\n      replay_client: reverb.Client) -> Optional[Iterator[reverb.ReplaySample]]:\n    # make_dataset_iterator is only called once (per learner), to pass the\n    # iterator to make_learner. By using adders we ensure the transition types\n    # (e.g. n-step transitions) that the direct RL agent expects.\n    if self._config.num_transitions_rb > 0:\n\n      def prefill_thread():\n        # Populating the replay buffer with the direct RL agent guarantees that\n        # a constant reward will be used, not the imitation reward.\n        prefill_reward = (\n            self._config.alpha\n            if self._config.prefill_constant_reward else None)\n        _prefill_with_demonstrations(\n            adder=self._rl_agent.make_adder(replay_client, None, None),\n            demonstrations=list(self._demonstrations_fn().demonstrations),\n            min_num_transitions=self._config.num_transitions_rb,\n            reward=prefill_reward)\n      # Populate the replay buffer in a separate thread, so that the learner\n      # can sample from the buffer, to avoid blocking on the buffer being full.\n      threading.Thread(target=prefill_thread, daemon=True).start()\n\n    return self._rl_agent.make_dataset_iterator(replay_client)",
  "def make_adder(\n      self,\n      replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[DirectPolicyNetwork],\n  ) -> Optional[adders.Adder]:\n    \"\"\"Creates the adder substituting imitation reward.\"\"\"\n    pwil_demonstrations = self._demonstrations_fn()\n    return pwil_adder.PWILAdder(\n        direct_rl_adder=self._rl_agent.make_adder(replay_client,\n                                                  environment_spec, policy),\n        pwil_rewarder=rewarder.WassersteinDistanceRewarder(\n            demonstrations_it=pwil_demonstrations.demonstrations,\n            episode_length=pwil_demonstrations.episode_length,\n            use_actions_for_distance=self._config.use_actions_for_distance,\n            alpha=self._config.alpha,\n            beta=self._config.beta))",
  "def make_actor(\n      self,\n      random_key: PRNGKey,\n      policy: DirectPolicyNetwork,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> core.Actor:\n    return self._rl_agent.make_actor(random_key, policy, environment_spec,\n                                     variable_source, adder)",
  "def make_policy(self,\n                  networks: DirectRLNetworks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> DirectPolicyNetwork:\n    return self._rl_agent.make_policy(networks, environment_spec, evaluation)",
  "def prefill_thread():\n        # Populating the replay buffer with the direct RL agent guarantees that\n        # a constant reward will be used, not the imitation reward.\n        prefill_reward = (\n            self._config.alpha\n            if self._config.prefill_constant_reward else None)\n        _prefill_with_demonstrations(\n            adder=self._rl_agent.make_adder(replay_client, None, None),\n            demonstrations=list(self._demonstrations_fn().demonstrations),\n            min_num_transitions=self._config.num_transitions_rb,\n            reward=prefill_reward)",
  "class WassersteinDistanceRewarder:\n  \"\"\"Computes PWIL rewards along a trajectory.\n\n  The rewards measure similarity to the demonstration transitions and are based\n  on a greedy approximation to the Wasserstein distance between trajectories.\n  \"\"\"\n\n  def __init__(self,\n               demonstrations_it: Iterator[types.Transition],\n               episode_length: int,\n               use_actions_for_distance: bool = False,\n               alpha: float = 5.,\n               beta: float = 5.):\n    \"\"\"Initializes the rewarder.\n\n    Args:\n      demonstrations_it: An iterator over acme.types.Transition.\n      episode_length: a target episode length (policies will be encouraged by\n        the imitation reward to have that length).\n      use_actions_for_distance: whether to use action to compute reward.\n      alpha: float scaling the reward function.\n      beta: float controling the kernel size of the reward function.\n    \"\"\"\n    self._episode_length = episode_length\n\n    self._use_actions_for_distance = use_actions_for_distance\n    self._vectorized_demonstrations = self._vectorize(demonstrations_it)\n\n    # Observations and actions are flat.\n    atom_dims = self._vectorized_demonstrations.shape[1]\n    self._reward_sigma = beta * self._episode_length / np.sqrt(atom_dims)\n    self._reward_scale = alpha\n\n    self._std = np.std(self._vectorized_demonstrations, axis=0, dtype='float64')\n    # The std is set to 1 if the observation values are below a threshold.\n    # This prevents normalizing observation values that are constant (which can\n    # be problematic with e.g. demonstrations coming from a different version\n    # of the environment and where the constant values are slightly different).\n    self._std = (self._std < 1e-6) + self._std\n\n    self.expert_atoms = self._vectorized_demonstrations / self._std\n    self._compute_norm = jax.jit(lambda a, b: jnp.linalg.norm(a - b, axis=1),\n                                 device=jax.devices('cpu')[0])\n\n  def _vectorize(self,\n                 demonstrations_it: Iterator[types.Transition]) -> np.ndarray:\n    \"\"\"Converts filtered expert demonstrations to numpy array.\n\n    Args:\n      demonstrations_it: list of expert demonstrations\n\n    Returns:\n      numpy array with dimension:\n      [num_expert_transitions, dim_observation] if not use_actions_for_distance\n      [num_expert_transitions, (dim_observation + dim_action)] otherwise\n    \"\"\"\n    if self._use_actions_for_distance:\n      demonstrations = [\n          np.concatenate([t.observation, t.action]) for t in demonstrations_it\n      ]\n    else:\n      demonstrations = [t.observation for t in demonstrations_it]\n    return np.array(demonstrations)\n\n  def reset(self) -> None:\n    \"\"\"Makes all expert transitions available and initialize weights.\"\"\"\n    num_expert_atoms = len(self.expert_atoms)\n    self._all_expert_weights_zero = False\n    self.expert_weights = np.ones(num_expert_atoms) / num_expert_atoms\n\n  def append_and_compute_reward(self, observation: jnp.ndarray,\n                                action: jnp.ndarray) -> np.float32:\n    \"\"\"Computes reward and updates state, advancing it along a trajectory.\n\n    Subsequent calls to append_and_compute_reward assume inputs are subsequent\n    trajectory points.\n\n    Args:\n      observation: observation on a trajectory, to compare with the expert\n        demonstration(s).\n      action: the action following the observation on the trajectory.\n\n    Returns:\n      the reward value: the return contribution from the trajectory point.\n\n    \"\"\"\n    # If we run out of demonstrations, penalize further action.\n    if self._all_expert_weights_zero:\n      return np.float32(0.)\n\n    # Scale observation and action.\n    if self._use_actions_for_distance:\n      agent_atom = np.concatenate([observation, action])\n    else:\n      agent_atom = observation\n    agent_atom /= self._std\n\n    cost = 0.\n    # A special marker for records with zero expert weight. Has to be large so\n    # that argmin will not return it.\n    DELETED = 1e10  # pylint: disable=invalid-name\n    # As we match the expert's weights with the agent's weights, we might\n    # raise an error due to float precision, we substract a small epsilon from\n    # the agent's weights to prevent that.\n    weight = 1. / self._episode_length - 1e-6\n    norms = np.array(self._compute_norm(self.expert_atoms, agent_atom))\n    # We need to mask out states with zero weight, so that 'argmin' would not\n    # return them.\n    adjusted_norms = (1 - np.sign(self.expert_weights)) * DELETED + norms\n    while weight > 0:\n      # Get closest expert state action to agent's state action.\n      argmin = adjusted_norms.argmin()\n      effective_weight = min(weight, self.expert_weights[argmin])\n\n      if adjusted_norms[argmin] >= DELETED:\n        self._all_expert_weights_zero = True\n        break\n\n      # Update cost and weights.\n      weight -= effective_weight\n      self.expert_weights[argmin] -= effective_weight\n      cost += effective_weight * norms[argmin]\n      adjusted_norms[argmin] = DELETED\n\n    if weight > 0:\n      # We have a 'partial' cost if we ran out of demonstrations in the reward\n      # computation loop. We assign a high cost (infinite) in this case which\n      # makes the reward equal to 0.\n      reward = np.array(0.)\n    else:\n      reward = self._reward_scale * np.exp(-self._reward_sigma * cost)\n\n    return reward.astype('float32')",
  "def __init__(self,\n               demonstrations_it: Iterator[types.Transition],\n               episode_length: int,\n               use_actions_for_distance: bool = False,\n               alpha: float = 5.,\n               beta: float = 5.):\n    \"\"\"Initializes the rewarder.\n\n    Args:\n      demonstrations_it: An iterator over acme.types.Transition.\n      episode_length: a target episode length (policies will be encouraged by\n        the imitation reward to have that length).\n      use_actions_for_distance: whether to use action to compute reward.\n      alpha: float scaling the reward function.\n      beta: float controling the kernel size of the reward function.\n    \"\"\"\n    self._episode_length = episode_length\n\n    self._use_actions_for_distance = use_actions_for_distance\n    self._vectorized_demonstrations = self._vectorize(demonstrations_it)\n\n    # Observations and actions are flat.\n    atom_dims = self._vectorized_demonstrations.shape[1]\n    self._reward_sigma = beta * self._episode_length / np.sqrt(atom_dims)\n    self._reward_scale = alpha\n\n    self._std = np.std(self._vectorized_demonstrations, axis=0, dtype='float64')\n    # The std is set to 1 if the observation values are below a threshold.\n    # This prevents normalizing observation values that are constant (which can\n    # be problematic with e.g. demonstrations coming from a different version\n    # of the environment and where the constant values are slightly different).\n    self._std = (self._std < 1e-6) + self._std\n\n    self.expert_atoms = self._vectorized_demonstrations / self._std\n    self._compute_norm = jax.jit(lambda a, b: jnp.linalg.norm(a - b, axis=1),\n                                 device=jax.devices('cpu')[0])",
  "def _vectorize(self,\n                 demonstrations_it: Iterator[types.Transition]) -> np.ndarray:\n    \"\"\"Converts filtered expert demonstrations to numpy array.\n\n    Args:\n      demonstrations_it: list of expert demonstrations\n\n    Returns:\n      numpy array with dimension:\n      [num_expert_transitions, dim_observation] if not use_actions_for_distance\n      [num_expert_transitions, (dim_observation + dim_action)] otherwise\n    \"\"\"\n    if self._use_actions_for_distance:\n      demonstrations = [\n          np.concatenate([t.observation, t.action]) for t in demonstrations_it\n      ]\n    else:\n      demonstrations = [t.observation for t in demonstrations_it]\n    return np.array(demonstrations)",
  "def reset(self) -> None:\n    \"\"\"Makes all expert transitions available and initialize weights.\"\"\"\n    num_expert_atoms = len(self.expert_atoms)\n    self._all_expert_weights_zero = False\n    self.expert_weights = np.ones(num_expert_atoms) / num_expert_atoms",
  "def append_and_compute_reward(self, observation: jnp.ndarray,\n                                action: jnp.ndarray) -> np.float32:\n    \"\"\"Computes reward and updates state, advancing it along a trajectory.\n\n    Subsequent calls to append_and_compute_reward assume inputs are subsequent\n    trajectory points.\n\n    Args:\n      observation: observation on a trajectory, to compare with the expert\n        demonstration(s).\n      action: the action following the observation on the trajectory.\n\n    Returns:\n      the reward value: the return contribution from the trajectory point.\n\n    \"\"\"\n    # If we run out of demonstrations, penalize further action.\n    if self._all_expert_weights_zero:\n      return np.float32(0.)\n\n    # Scale observation and action.\n    if self._use_actions_for_distance:\n      agent_atom = np.concatenate([observation, action])\n    else:\n      agent_atom = observation\n    agent_atom /= self._std\n\n    cost = 0.\n    # A special marker for records with zero expert weight. Has to be large so\n    # that argmin will not return it.\n    DELETED = 1e10  # pylint: disable=invalid-name\n    # As we match the expert's weights with the agent's weights, we might\n    # raise an error due to float precision, we substract a small epsilon from\n    # the agent's weights to prevent that.\n    weight = 1. / self._episode_length - 1e-6\n    norms = np.array(self._compute_norm(self.expert_atoms, agent_atom))\n    # We need to mask out states with zero weight, so that 'argmin' would not\n    # return them.\n    adjusted_norms = (1 - np.sign(self.expert_weights)) * DELETED + norms\n    while weight > 0:\n      # Get closest expert state action to agent's state action.\n      argmin = adjusted_norms.argmin()\n      effective_weight = min(weight, self.expert_weights[argmin])\n\n      if adjusted_norms[argmin] >= DELETED:\n        self._all_expert_weights_zero = True\n        break\n\n      # Update cost and weights.\n      weight -= effective_weight\n      self.expert_weights[argmin] -= effective_weight\n      cost += effective_weight * norms[argmin]\n      adjusted_norms[argmin] = DELETED\n\n    if weight > 0:\n      # We have a 'partial' cost if we ran out of demonstrations in the reward\n      # computation loop. We assign a high cost (infinite) in this case which\n      # makes the reward equal to 0.\n      reward = np.array(0.)\n    else:\n      reward = self._reward_scale * np.exp(-self._reward_sigma * cost)\n\n    return reward.astype('float32')",
  "class DACConfig:\n  \"\"\"Configuration options specific to DAC.\n\n  Attributes:\n    ail_config: AIL config.\n    td3_config: TD3 config.\n    entropy_coefficient: Entropy coefficient of the discriminator loss.\n    gradient_penalty_coefficient: Coefficient for the gradient penalty term in\n      the discriminator loss.\n  \"\"\"\n  ail_config: ail_config.AILConfig\n  td3_config: td3.TD3Config\n  entropy_coefficient: float = 1e-3\n  gradient_penalty_coefficient: float = 10.",
  "class DACBuilder(builder.AILBuilder[td3.TD3Networks,\n                                    actor_core_lib.FeedForwardPolicy]):\n  \"\"\"DAC Builder.\"\"\"\n\n  def __init__(self, config: DACConfig,\n               make_demonstrations: Callable[[int],\n                                             Iterator[types.Transition]]):\n\n    td3_builder = td3.TD3Builder(config.td3_config)\n    dac_loss = losses.add_gradient_penalty(\n        losses.gail_loss(entropy_coefficient=config.entropy_coefficient),\n        gradient_penalty_coefficient=config.gradient_penalty_coefficient,\n        gradient_penalty_target=1.)\n    super().__init__(\n        td3_builder,\n        config=config.ail_config,\n        discriminator_loss=dac_loss,\n        make_demonstrations=make_demonstrations)",
  "def __init__(self, config: DACConfig,\n               make_demonstrations: Callable[[int],\n                                             Iterator[types.Transition]]):\n\n    td3_builder = td3.TD3Builder(config.td3_config)\n    dac_loss = losses.add_gradient_penalty(\n        losses.gail_loss(entropy_coefficient=config.entropy_coefficient),\n        gradient_penalty_coefficient=config.gradient_penalty_coefficient,\n        gradient_penalty_target=1.)\n    super().__init__(\n        td3_builder,\n        config=config.ail_config,\n        discriminator_loss=dac_loss,\n        make_demonstrations=make_demonstrations)",
  "def _binary_cross_entropy_loss(logit: jnp.ndarray,\n                               label: jnp.ndarray) -> jnp.ndarray:\n  return label * jax.nn.softplus(-logit) + (1 - label) * jax.nn.softplus(logit)",
  "def _weighted_average(x: jnp.ndarray, y: jnp.ndarray,\n                      lambdas: jnp.ndarray) -> jnp.ndarray:\n  return lambdas * x + (1. - lambdas) * y",
  "def _label_data(\n    rb_transitions: types.Transition,\n    demonstration_transitions: types.Transition, mixup_alpha: Optional[float],\n    key: networks_lib.PRNGKey) -> Tuple[types.Transition, jnp.ndarray]:\n  \"\"\"Create a tuple data, labels by concatenating the rb and dem transitions.\"\"\"\n  data = tree.map_structure(lambda x, y: jnp.concatenate([x, y]),\n                            rb_transitions, demonstration_transitions)\n  labels = jnp.concatenate([\n      jnp.zeros(rb_transitions.reward.shape),\n      jnp.ones(demonstration_transitions.reward.shape)\n  ])\n\n  if mixup_alpha is not None:\n    lambda_key, mixup_key = jax.random.split(key)\n\n    lambdas = tfd.Beta(mixup_alpha, mixup_alpha).sample(\n        len(labels), seed=lambda_key)\n\n    shuffled_data = tree.map_structure(\n        lambda x: jax.random.permutation(key=mixup_key, x=x), data)\n    shuffled_labels = jax.random.permutation(key=mixup_key, x=labels)\n\n    data = tree.map_structure(lambda x, y: _weighted_average(x, y, lambdas),\n                              data, shuffled_data)\n    labels = _weighted_average(labels, shuffled_labels, lambdas)\n\n  return data, labels",
  "def _logit_bernoulli_entropy(logits: networks_lib.Logits) -> jnp.ndarray:\n  return (1. - jax.nn.sigmoid(logits)) * logits - jax.nn.log_sigmoid(logits)",
  "def gail_loss(entropy_coefficient: float = 0.,\n              mixup_alpha: Optional[float] = None) -> Loss:\n  \"\"\"Computes the standard GAIL loss.\"\"\"\n\n  def loss_fn(\n      discriminator_fn: DiscriminatorFn,\n      discriminator_state: State,\n      demo_transitions: types.Transition, rb_transitions: types.Transition,\n      rng_key: networks_lib.PRNGKey) -> LossOutput:\n\n    data, labels = _label_data(\n        rb_transitions=rb_transitions,\n        demonstration_transitions=demo_transitions,\n        mixup_alpha=mixup_alpha,\n        key=rng_key)\n    logits, discriminator_state = discriminator_fn(discriminator_state, data)\n\n    classification_loss = jnp.mean(_binary_cross_entropy_loss(logits, labels))\n\n    entropy = jnp.mean(_logit_bernoulli_entropy(logits))\n    entropy_loss = -entropy_coefficient * entropy\n\n    total_loss = classification_loss + entropy_loss\n\n    metrics = {\n        'total_loss': total_loss,\n        'entropy_loss': entropy_loss,\n        'classification_loss': classification_loss\n    }\n    return total_loss, (metrics, discriminator_state)\n\n  return loss_fn",
  "def pugail_loss(positive_class_prior: float,\n                entropy_coefficient: float,\n                pugail_beta: Optional[float] = None) -> Loss:\n  \"\"\"Computes the PUGAIL loss (https://arxiv.org/pdf/1911.00459.pdf).\"\"\"\n\n  def loss_fn(\n      discriminator_fn: DiscriminatorFn,\n      discriminator_state: State,\n      demo_transitions: types.Transition, rb_transitions: types.Transition,\n      rng_key: networks_lib.PRNGKey) -> LossOutput:\n    del rng_key\n\n    demo_logits, discriminator_state = discriminator_fn(discriminator_state,\n                                                        demo_transitions)\n    rb_logits, discriminator_state = discriminator_fn(discriminator_state,\n                                                      rb_transitions)\n\n    # Quick Maths:\n    # output = logit(D) = ln(D) - ln(1-D)\n    # -softplus(-output) = ln(D)\n    # softplus(output) = -ln(1-D)\n\n    # prior * -ln(D(expert))\n    positive_loss = positive_class_prior * -jax.nn.log_sigmoid(demo_logits)\n    # -ln(1 - D(policy)) - prior * -ln(1 - D(expert))\n    negative_loss = jax.nn.softplus(\n        rb_logits) - positive_class_prior * jax.nn.softplus(demo_logits)\n    if pugail_beta is not None:\n      negative_loss = jnp.clip(negative_loss, a_min=-1. * pugail_beta)\n\n    classification_loss = jnp.mean(positive_loss + negative_loss)\n\n    entropy = jnp.mean(\n        _logit_bernoulli_entropy(jnp.concatenate([demo_logits, rb_logits])))\n    entropy_loss = -entropy_coefficient * entropy\n\n    total_loss = classification_loss + entropy_loss\n\n    metrics = {\n        'total_loss': total_loss,\n        'positive_loss': jnp.mean(positive_loss),\n        'negative_loss': jnp.mean(negative_loss),\n        'demo_logits': jnp.mean(demo_logits),\n        'rb_logits': jnp.mean(rb_logits),\n        'entropy_loss': entropy_loss,\n        'classification_loss': classification_loss\n    }\n    return total_loss, (metrics, discriminator_state)\n\n  return loss_fn",
  "def _make_gradient_penalty_data(rb_transitions: types.Transition,\n                                demonstration_transitions: types.Transition,\n                                key: networks_lib.PRNGKey) -> types.Transition:\n  lambdas = tfd.Uniform().sample(len(rb_transitions.reward), seed=key)\n  return tree.map_structure(lambda x, y: _weighted_average(x, y, lambdas),\n                            rb_transitions, demonstration_transitions)",
  "def _compute_gradient_penalty(gradient_penalty_data: types.Transition,\n                              discriminator_fn: Callable[[types.Transition],\n                                                         float],\n                              gradient_penalty_target: float) -> float:\n  \"\"\"Computes a penalty based on the gradient norm on the data.\"\"\"\n  # The input should not be batched.\n  assert not gradient_penalty_data.reward.shape\n  discriminator_gradient_fn = jax.grad(discriminator_fn)\n  gradients = discriminator_gradient_fn(gradient_penalty_data)\n  gradients = tree.map_structure(lambda x: x.flatten(), gradients)\n  gradients = jnp.concatenate([gradients.observation, gradients.action,\n                               gradients.next_observation])\n  gradient_norms = jnp.linalg.norm(gradients + 1e-8)\n  k = gradient_penalty_target * jnp.ones_like(gradient_norms)\n  return jnp.mean(jnp.square(gradient_norms - k))",
  "def add_gradient_penalty(base_loss: Loss,\n                         gradient_penalty_coefficient: float,\n                         gradient_penalty_target: float) -> Loss:\n  \"\"\"Adds a gradient penalty to the base_loss.\"\"\"\n\n  if not gradient_penalty_coefficient:\n    return base_loss\n\n  def loss_fn(discriminator_fn: DiscriminatorFn,\n              discriminator_state: State,\n              demo_transitions: types.Transition,\n              rb_transitions: types.Transition,\n              rng_key: networks_lib.PRNGKey) -> LossOutput:\n    super_key, gradient_penalty_key = jax.random.split(rng_key)\n\n    partial_loss, (losses, discriminator_state) = base_loss(\n        discriminator_fn, discriminator_state, demo_transitions, rb_transitions,\n        super_key)\n\n    gradient_penalty_data = _make_gradient_penalty_data(\n        rb_transitions=rb_transitions,\n        demonstration_transitions=demo_transitions,\n        key=gradient_penalty_key)\n    def apply_discriminator_fn(transitions: types.Transition) -> float:\n      logits, _ = discriminator_fn(discriminator_state, transitions)\n      return logits  # pytype: disable=bad-return-type  # jax-ndarray\n    gradient_penalty = gradient_penalty_coefficient * jnp.mean(\n        _compute_gradient_penalty(gradient_penalty_data, apply_discriminator_fn,\n                                  gradient_penalty_target))\n\n    losses['gradient_penalty'] = gradient_penalty\n    total_loss = partial_loss + gradient_penalty\n    losses['total_loss'] = total_loss\n\n    return total_loss, (losses, discriminator_state)\n\n  return loss_fn",
  "def loss_fn(\n      discriminator_fn: DiscriminatorFn,\n      discriminator_state: State,\n      demo_transitions: types.Transition, rb_transitions: types.Transition,\n      rng_key: networks_lib.PRNGKey) -> LossOutput:\n\n    data, labels = _label_data(\n        rb_transitions=rb_transitions,\n        demonstration_transitions=demo_transitions,\n        mixup_alpha=mixup_alpha,\n        key=rng_key)\n    logits, discriminator_state = discriminator_fn(discriminator_state, data)\n\n    classification_loss = jnp.mean(_binary_cross_entropy_loss(logits, labels))\n\n    entropy = jnp.mean(_logit_bernoulli_entropy(logits))\n    entropy_loss = -entropy_coefficient * entropy\n\n    total_loss = classification_loss + entropy_loss\n\n    metrics = {\n        'total_loss': total_loss,\n        'entropy_loss': entropy_loss,\n        'classification_loss': classification_loss\n    }\n    return total_loss, (metrics, discriminator_state)",
  "def loss_fn(\n      discriminator_fn: DiscriminatorFn,\n      discriminator_state: State,\n      demo_transitions: types.Transition, rb_transitions: types.Transition,\n      rng_key: networks_lib.PRNGKey) -> LossOutput:\n    del rng_key\n\n    demo_logits, discriminator_state = discriminator_fn(discriminator_state,\n                                                        demo_transitions)\n    rb_logits, discriminator_state = discriminator_fn(discriminator_state,\n                                                      rb_transitions)\n\n    # Quick Maths:\n    # output = logit(D) = ln(D) - ln(1-D)\n    # -softplus(-output) = ln(D)\n    # softplus(output) = -ln(1-D)\n\n    # prior * -ln(D(expert))\n    positive_loss = positive_class_prior * -jax.nn.log_sigmoid(demo_logits)\n    # -ln(1 - D(policy)) - prior * -ln(1 - D(expert))\n    negative_loss = jax.nn.softplus(\n        rb_logits) - positive_class_prior * jax.nn.softplus(demo_logits)\n    if pugail_beta is not None:\n      negative_loss = jnp.clip(negative_loss, a_min=-1. * pugail_beta)\n\n    classification_loss = jnp.mean(positive_loss + negative_loss)\n\n    entropy = jnp.mean(\n        _logit_bernoulli_entropy(jnp.concatenate([demo_logits, rb_logits])))\n    entropy_loss = -entropy_coefficient * entropy\n\n    total_loss = classification_loss + entropy_loss\n\n    metrics = {\n        'total_loss': total_loss,\n        'positive_loss': jnp.mean(positive_loss),\n        'negative_loss': jnp.mean(negative_loss),\n        'demo_logits': jnp.mean(demo_logits),\n        'rb_logits': jnp.mean(rb_logits),\n        'entropy_loss': entropy_loss,\n        'classification_loss': classification_loss\n    }\n    return total_loss, (metrics, discriminator_state)",
  "def loss_fn(discriminator_fn: DiscriminatorFn,\n              discriminator_state: State,\n              demo_transitions: types.Transition,\n              rb_transitions: types.Transition,\n              rng_key: networks_lib.PRNGKey) -> LossOutput:\n    super_key, gradient_penalty_key = jax.random.split(rng_key)\n\n    partial_loss, (losses, discriminator_state) = base_loss(\n        discriminator_fn, discriminator_state, demo_transitions, rb_transitions,\n        super_key)\n\n    gradient_penalty_data = _make_gradient_penalty_data(\n        rb_transitions=rb_transitions,\n        demonstration_transitions=demo_transitions,\n        key=gradient_penalty_key)\n    def apply_discriminator_fn(transitions: types.Transition) -> float:\n      logits, _ = discriminator_fn(discriminator_state, transitions)\n      return logits  # pytype: disable=bad-return-type  # jax-ndarray\n    gradient_penalty = gradient_penalty_coefficient * jnp.mean(\n        _compute_gradient_penalty(gradient_penalty_data, apply_discriminator_fn,\n                                  gradient_penalty_target))\n\n    losses['gradient_penalty'] = gradient_penalty\n    total_loss = partial_loss + gradient_penalty\n    losses['total_loss'] = total_loss\n\n    return total_loss, (losses, discriminator_state)",
  "def apply_discriminator_fn(transitions: types.Transition) -> float:\n      logits, _ = discriminator_fn(discriminator_state, transitions)\n      return logits",
  "class DiscriminatorTrainingState(NamedTuple):\n  \"\"\"Contains training state for the discriminator.\"\"\"\n  # State of the optimizer used to optimize the discriminator parameters.\n  optimizer_state: optax.OptState\n\n  # Parameters of the discriminator.\n  discriminator_params: networks_lib.Params\n\n  # State of the discriminator\n  discriminator_state: losses.State\n\n  # For AIRL variants, we need the policy params to compute the loss.\n  policy_params: Optional[networks_lib.Params]\n\n  # Key for random number generation.\n  key: networks_lib.PRNGKey\n\n  # Training step of the discriminator.\n  steps: int",
  "class TrainingState(NamedTuple):\n  \"\"\"Contains training state of the AIL learner.\"\"\"\n  rewarder_state: DiscriminatorTrainingState\n  learner_state: Any",
  "def ail_update_step(\n    state: DiscriminatorTrainingState, data: Tuple[types.Transition,\n                                                   types.Transition],\n    optimizer: optax.GradientTransformation,\n    ail_network: ail_networks.AILNetworks,\n    loss_fn: losses.Loss) -> Tuple[DiscriminatorTrainingState, losses.Metrics]:\n  \"\"\"Run an update steps on the given transitions.\n\n  Args:\n    state: The learner state.\n    data: Demo and rb transitions.\n    optimizer: Discriminator optimizer.\n    ail_network: AIL networks.\n    loss_fn: Discriminator loss to minimize.\n\n  Returns:\n    A new state and metrics.\n  \"\"\"\n  demo_transitions, rb_transitions = data\n  key, discriminator_key, loss_key = jax.random.split(state.key, 3)\n\n  def compute_loss(\n      discriminator_params: networks_lib.Params) -> losses.LossOutput:\n    discriminator_fn = functools.partial(\n        ail_network.discriminator_network.apply,\n        discriminator_params,\n        state.policy_params,\n        is_training=True,\n        rng=discriminator_key)\n    return loss_fn(discriminator_fn, state.discriminator_state,\n                   demo_transitions, rb_transitions, loss_key)\n\n  loss_grad = jax.grad(compute_loss, has_aux=True)\n\n  grads, (loss, new_discriminator_state) = loss_grad(state.discriminator_params)\n\n  update, optimizer_state = optimizer.update(\n      grads,\n      state.optimizer_state,\n      params=state.discriminator_params)\n  discriminator_params = optax.apply_updates(state.discriminator_params, update)\n\n  new_state = DiscriminatorTrainingState(\n      optimizer_state=optimizer_state,\n      discriminator_params=discriminator_params,\n      discriminator_state=new_discriminator_state,\n      policy_params=state.policy_params,  # Not modified.\n      key=key,\n      steps=state.steps + 1,\n  )\n  return new_state, loss",
  "class AILSample(NamedTuple):\n  discriminator_sample: types.Transition\n  direct_sample: reverb.ReplaySample\n  demonstration_sample: types.Transition",
  "class AILLearner(acme.Learner):\n  \"\"\"AIL learner.\"\"\"\n\n  def __init__(\n      self,\n      counter: counting.Counter,\n      direct_rl_learner_factory: Callable[[Iterator[reverb.ReplaySample]],\n                                          acme.Learner],\n      loss_fn: losses.Loss,\n      iterator: Iterator[AILSample],\n      discriminator_optimizer: optax.GradientTransformation,\n      ail_network: ail_networks.AILNetworks,\n      discriminator_key: networks_lib.PRNGKey,\n      is_sequence_based: bool,\n      num_sgd_steps_per_step: int = 1,\n      policy_variable_name: Optional[str] = None,\n      logger: Optional[loggers.Logger] = None):\n    \"\"\"AIL Learner.\n\n    Args:\n      counter: Counter.\n      direct_rl_learner_factory: Function that creates the direct RL learner\n        when passed a replay sample iterator.\n      loss_fn: Discriminator loss.\n      iterator: Iterator that returns AILSamples.\n      discriminator_optimizer: Discriminator optax optimizer.\n      ail_network: AIL networks.\n      discriminator_key: RNG key.\n      is_sequence_based: If True, a direct rl algorithm is using SequenceAdder\n        data format. Otherwise the learner assumes that the direct rl algorithm\n        is using NStepTransitionAdder.\n      num_sgd_steps_per_step: Number of discriminator gradient updates per step.\n      policy_variable_name: The name of the policy variable to retrieve\n        direct_rl policy parameters.\n      logger: Logger.\n    \"\"\"\n    self._is_sequence_based = is_sequence_based\n\n    state_key, networks_key = jax.random.split(discriminator_key)\n\n    # Generator expression that works the same as an iterator.\n    # https://pymbook.readthedocs.io/en/latest/igd.html#generator-expressions\n    iterator, direct_rl_iterator = itertools.tee(iterator)\n    direct_rl_iterator = (\n        self._process_sample(sample.direct_sample)\n        for sample in direct_rl_iterator)\n    self._direct_rl_learner = direct_rl_learner_factory(direct_rl_iterator)\n\n    self._iterator = iterator\n\n    if policy_variable_name is not None:\n\n      def get_policy_params():\n        return self._direct_rl_learner.get_variables([policy_variable_name])[0]\n\n      self._get_policy_params = get_policy_params\n\n    else:\n      self._get_policy_params = lambda: None\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger(\n        'learner',\n        asynchronous=True,\n        serialize_fn=utils.fetch_devicearray,\n        steps_key=self._counter.get_steps_key())\n\n    # Use the JIT compiler.\n    self._update_step = functools.partial(\n        ail_update_step,\n        optimizer=discriminator_optimizer,\n        ail_network=ail_network,\n        loss_fn=loss_fn)\n    self._update_step = utils.process_multiple_batches(self._update_step,\n                                                       num_sgd_steps_per_step)\n    self._update_step = jax.jit(self._update_step)\n\n    discriminator_params, discriminator_state = (\n        ail_network.discriminator_network.init(networks_key))\n    self._state = DiscriminatorTrainingState(\n        optimizer_state=discriminator_optimizer.init(discriminator_params),\n        discriminator_params=discriminator_params,\n        discriminator_state=discriminator_state,\n        policy_params=self._get_policy_params(),\n        key=state_key,\n        steps=0,\n    )\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None\n\n    self._get_reward = jax.jit(\n        functools.partial(\n            ail_networks.compute_ail_reward, networks=ail_network))\n\n  def _process_sample(self, sample: reverb.ReplaySample) -> reverb.ReplaySample:\n    \"\"\"Updates the reward of the replay sample.\n\n    Args:\n      sample: Replay sample to update the reward to.\n\n    Returns:\n      The replay sample with an updated reward.\n    \"\"\"\n    transitions = reverb_utils.replay_sample_to_sars_transition(\n        sample, is_sequence=self._is_sequence_based)\n    rewards = self._get_reward(self._state.discriminator_params,\n                               self._state.discriminator_state,\n                               self._state.policy_params, transitions)\n\n    return sample._replace(data=sample.data._replace(reward=rewards))\n\n  def step(self):\n    sample = next(self._iterator)\n    rb_transitions = sample.discriminator_sample\n    demo_transitions = sample.demonstration_sample\n\n    if demo_transitions.reward.shape != rb_transitions.reward.shape:\n      raise ValueError(\n          'Different shapes for demo transitions and rb_transitions: '\n          f'{demo_transitions.reward.shape} != {rb_transitions.reward.shape}')\n\n    # Update the parameters of the policy before doing a gradient step.\n    state = self._state._replace(policy_params=self._get_policy_params())\n    self._state, metrics = self._update_step(state,\n                                             (demo_transitions, rb_transitions))\n\n    # The order is important for AIRL.\n    # In AIRL, the discriminator update depends on the logpi of the direct rl\n    # policy.\n    # When updating the discriminator, we want the logpi for which the\n    # transitions were made with and not an updated one.\n    # Get data from replay (dropping extras if any). Note there is no\n    # extra data here because we do not insert any into Reverb.\n    self._direct_rl_learner.step()\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Increment counts and record the current time.\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n\n    # Attempts to write the logs.\n    self._logger.write({**metrics, **counts})\n\n  def get_variables(self, names: List[str]) -> List[Any]:\n    rewarder_dict = {'discriminator': self._state.discriminator_params}\n\n    learner_names = [name for name in names if name not in rewarder_dict]\n    learner_dict = {}\n    if learner_names:\n      learner_dict = dict(\n          zip(learner_names,\n              self._direct_rl_learner.get_variables(learner_names)))\n\n    variables = [\n        rewarder_dict.get(name, learner_dict.get(name, None)) for name in names\n    ]\n    return variables\n\n  def save(self) -> TrainingState:\n    return TrainingState(\n        rewarder_state=self._state,\n        learner_state=self._direct_rl_learner.save())\n\n  def restore(self, state: TrainingState):\n    self._state = state.rewarder_state\n    self._direct_rl_learner.restore(state.learner_state)",
  "def compute_loss(\n      discriminator_params: networks_lib.Params) -> losses.LossOutput:\n    discriminator_fn = functools.partial(\n        ail_network.discriminator_network.apply,\n        discriminator_params,\n        state.policy_params,\n        is_training=True,\n        rng=discriminator_key)\n    return loss_fn(discriminator_fn, state.discriminator_state,\n                   demo_transitions, rb_transitions, loss_key)",
  "def __init__(\n      self,\n      counter: counting.Counter,\n      direct_rl_learner_factory: Callable[[Iterator[reverb.ReplaySample]],\n                                          acme.Learner],\n      loss_fn: losses.Loss,\n      iterator: Iterator[AILSample],\n      discriminator_optimizer: optax.GradientTransformation,\n      ail_network: ail_networks.AILNetworks,\n      discriminator_key: networks_lib.PRNGKey,\n      is_sequence_based: bool,\n      num_sgd_steps_per_step: int = 1,\n      policy_variable_name: Optional[str] = None,\n      logger: Optional[loggers.Logger] = None):\n    \"\"\"AIL Learner.\n\n    Args:\n      counter: Counter.\n      direct_rl_learner_factory: Function that creates the direct RL learner\n        when passed a replay sample iterator.\n      loss_fn: Discriminator loss.\n      iterator: Iterator that returns AILSamples.\n      discriminator_optimizer: Discriminator optax optimizer.\n      ail_network: AIL networks.\n      discriminator_key: RNG key.\n      is_sequence_based: If True, a direct rl algorithm is using SequenceAdder\n        data format. Otherwise the learner assumes that the direct rl algorithm\n        is using NStepTransitionAdder.\n      num_sgd_steps_per_step: Number of discriminator gradient updates per step.\n      policy_variable_name: The name of the policy variable to retrieve\n        direct_rl policy parameters.\n      logger: Logger.\n    \"\"\"\n    self._is_sequence_based = is_sequence_based\n\n    state_key, networks_key = jax.random.split(discriminator_key)\n\n    # Generator expression that works the same as an iterator.\n    # https://pymbook.readthedocs.io/en/latest/igd.html#generator-expressions\n    iterator, direct_rl_iterator = itertools.tee(iterator)\n    direct_rl_iterator = (\n        self._process_sample(sample.direct_sample)\n        for sample in direct_rl_iterator)\n    self._direct_rl_learner = direct_rl_learner_factory(direct_rl_iterator)\n\n    self._iterator = iterator\n\n    if policy_variable_name is not None:\n\n      def get_policy_params():\n        return self._direct_rl_learner.get_variables([policy_variable_name])[0]\n\n      self._get_policy_params = get_policy_params\n\n    else:\n      self._get_policy_params = lambda: None\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger(\n        'learner',\n        asynchronous=True,\n        serialize_fn=utils.fetch_devicearray,\n        steps_key=self._counter.get_steps_key())\n\n    # Use the JIT compiler.\n    self._update_step = functools.partial(\n        ail_update_step,\n        optimizer=discriminator_optimizer,\n        ail_network=ail_network,\n        loss_fn=loss_fn)\n    self._update_step = utils.process_multiple_batches(self._update_step,\n                                                       num_sgd_steps_per_step)\n    self._update_step = jax.jit(self._update_step)\n\n    discriminator_params, discriminator_state = (\n        ail_network.discriminator_network.init(networks_key))\n    self._state = DiscriminatorTrainingState(\n        optimizer_state=discriminator_optimizer.init(discriminator_params),\n        discriminator_params=discriminator_params,\n        discriminator_state=discriminator_state,\n        policy_params=self._get_policy_params(),\n        key=state_key,\n        steps=0,\n    )\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None\n\n    self._get_reward = jax.jit(\n        functools.partial(\n            ail_networks.compute_ail_reward, networks=ail_network))",
  "def _process_sample(self, sample: reverb.ReplaySample) -> reverb.ReplaySample:\n    \"\"\"Updates the reward of the replay sample.\n\n    Args:\n      sample: Replay sample to update the reward to.\n\n    Returns:\n      The replay sample with an updated reward.\n    \"\"\"\n    transitions = reverb_utils.replay_sample_to_sars_transition(\n        sample, is_sequence=self._is_sequence_based)\n    rewards = self._get_reward(self._state.discriminator_params,\n                               self._state.discriminator_state,\n                               self._state.policy_params, transitions)\n\n    return sample._replace(data=sample.data._replace(reward=rewards))",
  "def step(self):\n    sample = next(self._iterator)\n    rb_transitions = sample.discriminator_sample\n    demo_transitions = sample.demonstration_sample\n\n    if demo_transitions.reward.shape != rb_transitions.reward.shape:\n      raise ValueError(\n          'Different shapes for demo transitions and rb_transitions: '\n          f'{demo_transitions.reward.shape} != {rb_transitions.reward.shape}')\n\n    # Update the parameters of the policy before doing a gradient step.\n    state = self._state._replace(policy_params=self._get_policy_params())\n    self._state, metrics = self._update_step(state,\n                                             (demo_transitions, rb_transitions))\n\n    # The order is important for AIRL.\n    # In AIRL, the discriminator update depends on the logpi of the direct rl\n    # policy.\n    # When updating the discriminator, we want the logpi for which the\n    # transitions were made with and not an updated one.\n    # Get data from replay (dropping extras if any). Note there is no\n    # extra data here because we do not insert any into Reverb.\n    self._direct_rl_learner.step()\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Increment counts and record the current time.\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n\n    # Attempts to write the logs.\n    self._logger.write({**metrics, **counts})",
  "def get_variables(self, names: List[str]) -> List[Any]:\n    rewarder_dict = {'discriminator': self._state.discriminator_params}\n\n    learner_names = [name for name in names if name not in rewarder_dict]\n    learner_dict = {}\n    if learner_names:\n      learner_dict = dict(\n          zip(learner_names,\n              self._direct_rl_learner.get_variables(learner_names)))\n\n    variables = [\n        rewarder_dict.get(name, learner_dict.get(name, None)) for name in names\n    ]\n    return variables",
  "def save(self) -> TrainingState:\n    return TrainingState(\n        rewarder_state=self._state,\n        learner_state=self._direct_rl_learner.save())",
  "def restore(self, state: TrainingState):\n    self._state = state.rewarder_state\n    self._direct_rl_learner.restore(state.learner_state)",
  "def get_policy_params():\n        return self._direct_rl_learner.get_variables([policy_variable_name])[0]",
  "class GAILConfig:\n  \"\"\"Configuration options specific to GAIL.\"\"\"\n  ail_config: ail_config.AILConfig\n  ppo_config: ppo.PPOConfig",
  "class GAILBuilder(builder.AILBuilder[ppo.PPONetworks,\n                                     actor_core_lib.FeedForwardPolicyWithExtra]\n                 ):\n  \"\"\"GAIL Builder.\"\"\"\n\n  def __init__(self, config: GAILConfig,\n               make_demonstrations: Callable[[int],\n                                             Iterator[types.Transition]]):\n\n    ppo_builder = ppo.PPOBuilder(config.ppo_config)\n    super().__init__(\n        ppo_builder,\n        config=config.ail_config,\n        discriminator_loss=losses.gail_loss(),\n        make_demonstrations=make_demonstrations)",
  "def __init__(self, config: GAILConfig,\n               make_demonstrations: Callable[[int],\n                                             Iterator[types.Transition]]):\n\n    ppo_builder = ppo.PPOBuilder(config.ppo_config)\n    super().__init__(\n        ppo_builder,\n        config=config.ail_config,\n        discriminator_loss=losses.gail_loss(),\n        make_demonstrations=make_demonstrations)",
  "class AilLossTest(absltest.TestCase):\n\n  def test_gradient_penalty(self):\n\n    def dummy_discriminator(\n        transition: types.Transition) -> networks_lib.Logits:\n      return transition.observation + jnp.square(transition.action)\n\n    zero_transition = types.Transition(0., 0., 0., 0., 0.)\n    zero_transition = tree.map_structure(lambda x: jnp.expand_dims(x, axis=0),\n                                         zero_transition)\n    self.assertEqual(\n        losses._compute_gradient_penalty(zero_transition, dummy_discriminator,\n                                         0.), 1**2 + 0**2)\n\n    one_transition = types.Transition(1., 1., 0., 0., 0.)\n    one_transition = tree.map_structure(lambda x: jnp.expand_dims(x, axis=0),\n                                        one_transition)\n    self.assertEqual(\n        losses._compute_gradient_penalty(one_transition, dummy_discriminator,\n                                         0.), 1**2 + 2**2)\n\n  def test_pugail(self):\n\n    def dummy_discriminator(\n        state: losses.State,\n        transition: types.Transition) -> losses.DiscriminatorOutput:\n      return transition.observation, state\n\n    zero_transition = types.Transition(.1, 0., 0., 0., 0.)\n    zero_transition = tree.map_structure(lambda x: jnp.expand_dims(x, axis=0),\n                                         zero_transition)\n\n    one_transition = types.Transition(1., 0., 0., 0., 0.)\n    one_transition = tree.map_structure(lambda x: jnp.expand_dims(x, axis=0),\n                                        one_transition)\n\n    prior = .7\n    loss_fn = losses.pugail_loss(\n        positive_class_prior=prior, entropy_coefficient=0.)\n    loss, _ = loss_fn(dummy_discriminator, {}, one_transition,\n                      zero_transition, ())\n\n    d_one = jax.nn.sigmoid(dummy_discriminator({}, one_transition)[0])\n    d_zero = jax.nn.sigmoid(dummy_discriminator({}, zero_transition)[0])\n    expected_loss = -prior * jnp.log(\n        d_one) + -jnp.log(1. - d_zero) - prior * -jnp.log(1 - d_one)\n\n    self.assertAlmostEqual(loss, expected_loss, places=6)",
  "def test_gradient_penalty(self):\n\n    def dummy_discriminator(\n        transition: types.Transition) -> networks_lib.Logits:\n      return transition.observation + jnp.square(transition.action)\n\n    zero_transition = types.Transition(0., 0., 0., 0., 0.)\n    zero_transition = tree.map_structure(lambda x: jnp.expand_dims(x, axis=0),\n                                         zero_transition)\n    self.assertEqual(\n        losses._compute_gradient_penalty(zero_transition, dummy_discriminator,\n                                         0.), 1**2 + 0**2)\n\n    one_transition = types.Transition(1., 1., 0., 0., 0.)\n    one_transition = tree.map_structure(lambda x: jnp.expand_dims(x, axis=0),\n                                        one_transition)\n    self.assertEqual(\n        losses._compute_gradient_penalty(one_transition, dummy_discriminator,\n                                         0.), 1**2 + 2**2)",
  "def test_pugail(self):\n\n    def dummy_discriminator(\n        state: losses.State,\n        transition: types.Transition) -> losses.DiscriminatorOutput:\n      return transition.observation, state\n\n    zero_transition = types.Transition(.1, 0., 0., 0., 0.)\n    zero_transition = tree.map_structure(lambda x: jnp.expand_dims(x, axis=0),\n                                         zero_transition)\n\n    one_transition = types.Transition(1., 0., 0., 0., 0.)\n    one_transition = tree.map_structure(lambda x: jnp.expand_dims(x, axis=0),\n                                        one_transition)\n\n    prior = .7\n    loss_fn = losses.pugail_loss(\n        positive_class_prior=prior, entropy_coefficient=0.)\n    loss, _ = loss_fn(dummy_discriminator, {}, one_transition,\n                      zero_transition, ())\n\n    d_one = jax.nn.sigmoid(dummy_discriminator({}, one_transition)[0])\n    d_zero = jax.nn.sigmoid(dummy_discriminator({}, zero_transition)[0])\n    expected_loss = -prior * jnp.log(\n        d_one) + -jnp.log(1. - d_zero) - prior * -jnp.log(1 - d_one)\n\n    self.assertAlmostEqual(loss, expected_loss, places=6)",
  "def dummy_discriminator(\n        transition: types.Transition) -> networks_lib.Logits:\n      return transition.observation + jnp.square(transition.action)",
  "def dummy_discriminator(\n        state: losses.State,\n        transition: types.Transition) -> losses.DiscriminatorOutput:\n      return transition.observation, state",
  "class AILNetworks(Generic[DirectRLNetworks]):\n  \"\"\"AIL networks data class.\n\n  Attributes:\n    discriminator_network: Networks which takes as input:\n      (observations, actions, next_observations, direct_rl_params)\n      to return the logit of the discriminator.\n      If the discriminator does not need direct_rl_params you can pass ().\n    imitation_reward_fn: Function from logit of the discriminator to imitation\n      reward.\n    direct_rl_networks: Networks of the direct RL algorithm.\n  \"\"\"\n  discriminator_network: networks_lib.FeedForwardNetwork\n  imitation_reward_fn: ImitationRewardFn\n  direct_rl_networks: DirectRLNetworks",
  "def compute_ail_reward(discriminator_params: networks_lib.Params,\n                       discriminator_state: State,\n                       policy_params: Optional[networks_lib.Params],\n                       transitions: types.Transition,\n                       networks: AILNetworks) -> jnp.ndarray:\n  \"\"\"Computes the AIL reward for a given transition.\n\n  Args:\n    discriminator_params: Parameters of the discriminator network.\n    discriminator_state: State of the discriminator network.\n    policy_params: Parameters of the direct RL policy.\n    transitions: Transitions to compute the reward for.\n    networks: AIL networks.\n\n  Returns:\n    The rewards as an ndarray.\n  \"\"\"\n  logits, _ = networks.discriminator_network.apply(\n      discriminator_params,\n      policy_params,\n      discriminator_state,\n      transitions,\n      is_training=False,\n      rng=None)\n  return networks.imitation_reward_fn(logits)",
  "class SpectralNormalizedLinear(hk.Module):\n  \"\"\"SpectralNormalizedLinear module.\n\n  This is a Linear layer with a upper-bounded Lipschitz. It is used in iResNet.\n\n  Reference:\n    Behrmann et al. Invertible Residual Networks. ICML 2019.\n    https://arxiv.org/pdf/1811.00995.pdf\n  \"\"\"\n\n  def __init__(\n      self,\n      output_size: int,\n      lipschitz_coeff: float,\n      with_bias: bool = True,\n      w_init: Optional[hk.initializers.Initializer] = None,\n      b_init: Optional[hk.initializers.Initializer] = None,\n      name: Optional[str] = None,\n  ):\n    \"\"\"Constructs the SpectralNormalizedLinear module.\n\n    Args:\n      output_size: Output dimensionality.\n      lipschitz_coeff: Spectral normalization coefficient.\n      with_bias: Whether to add a bias to the output.\n      w_init: Optional initializer for weights. By default, uses random values\n        from truncated normal, with stddev ``1 / sqrt(fan_in)``. See\n        https://arxiv.org/abs/1502.03167v3.\n      b_init: Optional initializer for bias. By default, zero.\n      name: Name of the module.\n    \"\"\"\n    super().__init__(name=name)\n    self.input_size = None\n    self.output_size = output_size\n    self.with_bias = with_bias\n    self.w_init = w_init\n    self.b_init = b_init or jnp.zeros\n    self.lipschitz_coeff = lipschitz_coeff\n    self.num_iterations = 100\n    self.eps = 1e-6\n\n  def get_normalized_weights(self,\n                             weights: jnp.ndarray,\n                             renormalize: bool = False) -> jnp.ndarray:\n\n    def _l2_normalize(x, axis=None, eps=1e-12):\n      return x * jax.lax.rsqrt((x * x).sum(axis=axis, keepdims=True) + eps)\n\n    output_size = self.output_size\n    dtype = weights.dtype\n    assert output_size == weights.shape[-1]\n    sigma = hk.get_state('sigma', (), init=jnp.ones)\n    if renormalize:\n      # Power iterations to compute spectral norm V*W*U^T.\n      u = hk.get_state(\n          'u', (1, output_size), dtype, init=hk.initializers.RandomNormal())\n      for _ in range(self.num_iterations):\n        v = _l2_normalize(jnp.matmul(u, weights.transpose()), eps=self.eps)\n        u = _l2_normalize(jnp.matmul(v, weights), eps=self.eps)\n      u = jax.lax.stop_gradient(u)\n      v = jax.lax.stop_gradient(v)\n      sigma = jnp.matmul(jnp.matmul(v, weights), jnp.transpose(u))[0, 0]\n      hk.set_state('u', u)\n      hk.set_state('v', v)\n      hk.set_state('sigma', sigma)\n    factor = jnp.maximum(1, sigma / self.lipschitz_coeff)\n    return weights / factor\n\n  def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Computes a linear transform of the input.\"\"\"\n    if not inputs.shape:\n      raise ValueError('Input must not be scalar.')\n\n    input_size = self.input_size = inputs.shape[-1]\n    output_size = self.output_size\n    dtype = inputs.dtype\n\n    w_init = self.w_init\n    if w_init is None:\n      stddev = 1. / np.sqrt(self.input_size)\n      w_init = hk.initializers.TruncatedNormal(stddev=stddev)\n    w = hk.get_parameter('w', [input_size, output_size], dtype, init=w_init)\n    w = self.get_normalized_weights(w, renormalize=True)\n\n    out = jnp.dot(inputs, w)\n\n    if self.with_bias:\n      b = hk.get_parameter('b', [self.output_size], dtype, init=self.b_init)\n      b = jnp.broadcast_to(b, out.shape)\n      out = out + b\n\n    return out",
  "class DiscriminatorMLP(hk.Module):\n  \"\"\"A multi-layer perceptron module.\"\"\"\n\n  def __init__(\n      self,\n      hidden_layer_sizes: Iterable[int],\n      w_init: Optional[hk.initializers.Initializer] = None,\n      b_init: Optional[hk.initializers.Initializer] = None,\n      with_bias: bool = True,\n      activation: Callable[[jnp.ndarray], jnp.ndarray] = jax.nn.relu,\n      input_dropout_rate: float = 0.,\n      hidden_dropout_rate: float = 0.,\n      spectral_normalization_lipschitz_coeff: Optional[float] = None,\n      name: Optional[str] = None\n  ):\n    \"\"\"Constructs an MLP.\n\n    Args:\n      hidden_layer_sizes: Hiddent layer sizes.\n      w_init: Initializer for :class:`~haiku.Linear` weights.\n      b_init: Initializer for :class:`~haiku.Linear` bias. Must be ``None`` if\n        ``with_bias=False``.\n      with_bias: Whether or not to apply a bias in each layer.\n      activation: Activation function to apply between :class:`~haiku.Linear`\n        layers. Defaults to ReLU.\n      input_dropout_rate: Dropout on the input.\n      hidden_dropout_rate: Dropout on the hidden layer outputs.\n      spectral_normalization_lipschitz_coeff: If not None, the network will have\n        spectral normalization with the given constant.\n      name: Optional name for this module.\n\n    Raises:\n      ValueError: If ``with_bias`` is ``False`` and ``b_init`` is not ``None``.\n    \"\"\"\n    if not with_bias and b_init is not None:\n      raise ValueError('When with_bias=False b_init must not be set.')\n\n    super().__init__(name=name)\n    self._activation = activation\n    self._input_dropout_rate = input_dropout_rate\n    self._hidden_dropout_rate = hidden_dropout_rate\n    layer_sizes = list(hidden_layer_sizes) + [1]\n\n    if spectral_normalization_lipschitz_coeff is not None:\n      layer_lipschitz_coeff = np.power(spectral_normalization_lipschitz_coeff,\n                                       1. / len(layer_sizes))\n      layer_module = functools.partial(\n          SpectralNormalizedLinear,\n          lipschitz_coeff=layer_lipschitz_coeff,\n          w_init=w_init,\n          b_init=b_init,\n          with_bias=with_bias)\n    else:\n      layer_module = functools.partial(\n          hk.Linear,\n          w_init=w_init,\n          b_init=b_init,\n          with_bias=with_bias)\n\n    layers = []\n    for index, output_size in enumerate(layer_sizes):\n      layers.append(\n          layer_module(output_size=output_size, name=f'linear_{index}'))\n    self._layers = tuple(layers)\n\n  def __call__(\n      self,\n      inputs: jnp.ndarray,\n      is_training: bool,\n      rng: Optional[networks_lib.PRNGKey],\n  ) -> networks_lib.Logits:\n    rng = hk.PRNGSequence(rng) if rng is not None else None\n\n    out = inputs\n    for i, layer in enumerate(self._layers):\n      if is_training:\n        dropout_rate = (\n            self._input_dropout_rate if i == 0 else self._hidden_dropout_rate)\n        out = hk.dropout(next(rng), dropout_rate, out)\n      out = layer(out)\n      if i < len(self._layers) - 1:\n        out = self._activation(out)\n\n    return out",
  "class DiscriminatorModule(hk.Module):\n  \"\"\"Discriminator module that concatenates its inputs.\"\"\"\n\n  def __init__(self,\n               environment_spec: specs.EnvironmentSpec,\n               use_action: bool,\n               use_next_obs: bool,\n               network_core: Callable[..., Any],\n               observation_embedding: Callable[[networks_lib.Observation],\n                                               jnp.ndarray] = lambda x: x,\n               name='discriminator'):\n    super().__init__(name=name)\n    self._use_action = use_action\n    self._environment_spec = environment_spec\n    self._use_next_obs = use_next_obs\n    self._network_core = network_core\n    self._observation_embedding = observation_embedding\n\n  def __call__(self, observations: networks_lib.Observation,\n               actions: networks_lib.Action,\n               next_observations: networks_lib.Observation, is_training: bool,\n               rng: networks_lib.PRNGKey) -> networks_lib.Logits:\n    observations = self._observation_embedding(observations)\n    if self._use_next_obs:\n      next_observations = self._observation_embedding(next_observations)\n      data = jnp.concatenate([observations, next_observations], axis=-1)\n    else:\n      data = observations\n    if self._use_action:\n      action_spec = self._environment_spec.actions\n      if isinstance(action_spec, specs.DiscreteArray):\n        actions = jax.nn.one_hot(actions,\n                                 action_spec.num_values)\n      data = jnp.concatenate([data, actions], axis=-1)\n    output = self._network_core(data, is_training, rng)\n    output = jnp.squeeze(output, axis=-1)\n    return output",
  "class AIRLModule(hk.Module):\n  \"\"\"AIRL Module.\"\"\"\n\n  def __init__(self,\n               environment_spec: specs.EnvironmentSpec,\n               use_action: bool,\n               use_next_obs: bool,\n               discount: float,\n               g_core: Callable[..., Any],\n               h_core: Callable[..., Any],\n               observation_embedding: Callable[[networks_lib.Observation],\n                                               jnp.ndarray] = lambda x: x,\n               name='airl'):\n    super().__init__(name=name)\n    self._environment_spec = environment_spec\n    self._use_action = use_action\n    self._use_next_obs = use_next_obs\n    self._discount = discount\n    self._g_core = g_core\n    self._h_core = h_core\n    self._observation_embedding = observation_embedding\n\n  def __call__(self, observations: networks_lib.Observation,\n               actions: networks_lib.Action,\n               next_observations: networks_lib.Observation,\n               is_training: bool,\n               rng: networks_lib.PRNGKey) -> networks_lib.Logits:\n    g_output = DiscriminatorModule(\n        environment_spec=self._environment_spec,\n        use_action=self._use_action,\n        use_next_obs=self._use_next_obs,\n        network_core=self._g_core,\n        observation_embedding=self._observation_embedding,\n        name='airl_g')(observations, actions, next_observations, is_training,\n                       rng)\n    h_module = DiscriminatorModule(\n        environment_spec=self._environment_spec,\n        use_action=False,\n        use_next_obs=False,\n        network_core=self._h_core,\n        observation_embedding=self._observation_embedding,\n        name='airl_h')\n    return (g_output + self._discount * h_module(next_observations, (),\n                                                 (), is_training, rng) -\n            h_module(observations, (), (), is_training, rng))",
  "def make_discriminator(\n    environment_spec: specs.EnvironmentSpec,\n    discriminator_transformed: hk.TransformedWithState,\n    logpi_fn: Optional[Callable[\n        [networks_lib.Params, networks_lib.Observation, networks_lib.Action],\n        jnp.ndarray]] = None\n) -> networks_lib.FeedForwardNetwork:\n  \"\"\"Creates the discriminator network.\n\n  Args:\n    environment_spec: Environment spec\n    discriminator_transformed: Haiku transformed of the discriminator.\n    logpi_fn: If the policy logpi function is provided, its output will be\n      removed from the discriminator logit.\n\n  Returns:\n    The network.\n  \"\"\"\n\n  def apply_fn(params: hk.Params,\n               policy_params: networks_lib.Params,\n               state: hk.State,\n               transitions: types.Transition,\n               is_training: bool,\n               rng: networks_lib.PRNGKey) -> networks_lib.Logits:\n    output, state = discriminator_transformed.apply(\n        params, state, transitions.observation, transitions.action,\n        transitions.next_observation, is_training, rng)\n    if logpi_fn is not None:\n      logpi = logpi_fn(policy_params, transitions.observation,\n                       transitions.action)\n\n      # Quick Maths:\n      # D = exp(output)/(exp(output) + pi(a|s))\n      # logit(D) = log(D/(1-D)) = log(exp(output)/pi(a|s))\n      # logit(D) = output - logpi\n      return output - logpi, state  # pytype: disable=bad-return-type  # jax-ndarray\n    return output, state  # pytype: disable=bad-return-type  # jax-ndarray\n\n  dummy_obs = utils.zeros_like(environment_spec.observations)\n  dummy_obs = utils.add_batch_dim(dummy_obs)\n  dummy_actions = utils.zeros_like(environment_spec.actions)\n  dummy_actions = utils.add_batch_dim(dummy_actions)\n\n  return networks_lib.FeedForwardNetwork(\n      # pylint: disable=g-long-lambda\n      init=lambda rng: discriminator_transformed.init(\n          rng, dummy_obs, dummy_actions, dummy_obs, False, rng),\n      apply=apply_fn)",
  "def __init__(\n      self,\n      output_size: int,\n      lipschitz_coeff: float,\n      with_bias: bool = True,\n      w_init: Optional[hk.initializers.Initializer] = None,\n      b_init: Optional[hk.initializers.Initializer] = None,\n      name: Optional[str] = None,\n  ):\n    \"\"\"Constructs the SpectralNormalizedLinear module.\n\n    Args:\n      output_size: Output dimensionality.\n      lipschitz_coeff: Spectral normalization coefficient.\n      with_bias: Whether to add a bias to the output.\n      w_init: Optional initializer for weights. By default, uses random values\n        from truncated normal, with stddev ``1 / sqrt(fan_in)``. See\n        https://arxiv.org/abs/1502.03167v3.\n      b_init: Optional initializer for bias. By default, zero.\n      name: Name of the module.\n    \"\"\"\n    super().__init__(name=name)\n    self.input_size = None\n    self.output_size = output_size\n    self.with_bias = with_bias\n    self.w_init = w_init\n    self.b_init = b_init or jnp.zeros\n    self.lipschitz_coeff = lipschitz_coeff\n    self.num_iterations = 100\n    self.eps = 1e-6",
  "def get_normalized_weights(self,\n                             weights: jnp.ndarray,\n                             renormalize: bool = False) -> jnp.ndarray:\n\n    def _l2_normalize(x, axis=None, eps=1e-12):\n      return x * jax.lax.rsqrt((x * x).sum(axis=axis, keepdims=True) + eps)\n\n    output_size = self.output_size\n    dtype = weights.dtype\n    assert output_size == weights.shape[-1]\n    sigma = hk.get_state('sigma', (), init=jnp.ones)\n    if renormalize:\n      # Power iterations to compute spectral norm V*W*U^T.\n      u = hk.get_state(\n          'u', (1, output_size), dtype, init=hk.initializers.RandomNormal())\n      for _ in range(self.num_iterations):\n        v = _l2_normalize(jnp.matmul(u, weights.transpose()), eps=self.eps)\n        u = _l2_normalize(jnp.matmul(v, weights), eps=self.eps)\n      u = jax.lax.stop_gradient(u)\n      v = jax.lax.stop_gradient(v)\n      sigma = jnp.matmul(jnp.matmul(v, weights), jnp.transpose(u))[0, 0]\n      hk.set_state('u', u)\n      hk.set_state('v', v)\n      hk.set_state('sigma', sigma)\n    factor = jnp.maximum(1, sigma / self.lipschitz_coeff)\n    return weights / factor",
  "def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Computes a linear transform of the input.\"\"\"\n    if not inputs.shape:\n      raise ValueError('Input must not be scalar.')\n\n    input_size = self.input_size = inputs.shape[-1]\n    output_size = self.output_size\n    dtype = inputs.dtype\n\n    w_init = self.w_init\n    if w_init is None:\n      stddev = 1. / np.sqrt(self.input_size)\n      w_init = hk.initializers.TruncatedNormal(stddev=stddev)\n    w = hk.get_parameter('w', [input_size, output_size], dtype, init=w_init)\n    w = self.get_normalized_weights(w, renormalize=True)\n\n    out = jnp.dot(inputs, w)\n\n    if self.with_bias:\n      b = hk.get_parameter('b', [self.output_size], dtype, init=self.b_init)\n      b = jnp.broadcast_to(b, out.shape)\n      out = out + b\n\n    return out",
  "def __init__(\n      self,\n      hidden_layer_sizes: Iterable[int],\n      w_init: Optional[hk.initializers.Initializer] = None,\n      b_init: Optional[hk.initializers.Initializer] = None,\n      with_bias: bool = True,\n      activation: Callable[[jnp.ndarray], jnp.ndarray] = jax.nn.relu,\n      input_dropout_rate: float = 0.,\n      hidden_dropout_rate: float = 0.,\n      spectral_normalization_lipschitz_coeff: Optional[float] = None,\n      name: Optional[str] = None\n  ):\n    \"\"\"Constructs an MLP.\n\n    Args:\n      hidden_layer_sizes: Hiddent layer sizes.\n      w_init: Initializer for :class:`~haiku.Linear` weights.\n      b_init: Initializer for :class:`~haiku.Linear` bias. Must be ``None`` if\n        ``with_bias=False``.\n      with_bias: Whether or not to apply a bias in each layer.\n      activation: Activation function to apply between :class:`~haiku.Linear`\n        layers. Defaults to ReLU.\n      input_dropout_rate: Dropout on the input.\n      hidden_dropout_rate: Dropout on the hidden layer outputs.\n      spectral_normalization_lipschitz_coeff: If not None, the network will have\n        spectral normalization with the given constant.\n      name: Optional name for this module.\n\n    Raises:\n      ValueError: If ``with_bias`` is ``False`` and ``b_init`` is not ``None``.\n    \"\"\"\n    if not with_bias and b_init is not None:\n      raise ValueError('When with_bias=False b_init must not be set.')\n\n    super().__init__(name=name)\n    self._activation = activation\n    self._input_dropout_rate = input_dropout_rate\n    self._hidden_dropout_rate = hidden_dropout_rate\n    layer_sizes = list(hidden_layer_sizes) + [1]\n\n    if spectral_normalization_lipschitz_coeff is not None:\n      layer_lipschitz_coeff = np.power(spectral_normalization_lipschitz_coeff,\n                                       1. / len(layer_sizes))\n      layer_module = functools.partial(\n          SpectralNormalizedLinear,\n          lipschitz_coeff=layer_lipschitz_coeff,\n          w_init=w_init,\n          b_init=b_init,\n          with_bias=with_bias)\n    else:\n      layer_module = functools.partial(\n          hk.Linear,\n          w_init=w_init,\n          b_init=b_init,\n          with_bias=with_bias)\n\n    layers = []\n    for index, output_size in enumerate(layer_sizes):\n      layers.append(\n          layer_module(output_size=output_size, name=f'linear_{index}'))\n    self._layers = tuple(layers)",
  "def __call__(\n      self,\n      inputs: jnp.ndarray,\n      is_training: bool,\n      rng: Optional[networks_lib.PRNGKey],\n  ) -> networks_lib.Logits:\n    rng = hk.PRNGSequence(rng) if rng is not None else None\n\n    out = inputs\n    for i, layer in enumerate(self._layers):\n      if is_training:\n        dropout_rate = (\n            self._input_dropout_rate if i == 0 else self._hidden_dropout_rate)\n        out = hk.dropout(next(rng), dropout_rate, out)\n      out = layer(out)\n      if i < len(self._layers) - 1:\n        out = self._activation(out)\n\n    return out",
  "def __init__(self,\n               environment_spec: specs.EnvironmentSpec,\n               use_action: bool,\n               use_next_obs: bool,\n               network_core: Callable[..., Any],\n               observation_embedding: Callable[[networks_lib.Observation],\n                                               jnp.ndarray] = lambda x: x,\n               name='discriminator'):\n    super().__init__(name=name)\n    self._use_action = use_action\n    self._environment_spec = environment_spec\n    self._use_next_obs = use_next_obs\n    self._network_core = network_core\n    self._observation_embedding = observation_embedding",
  "def __call__(self, observations: networks_lib.Observation,\n               actions: networks_lib.Action,\n               next_observations: networks_lib.Observation, is_training: bool,\n               rng: networks_lib.PRNGKey) -> networks_lib.Logits:\n    observations = self._observation_embedding(observations)\n    if self._use_next_obs:\n      next_observations = self._observation_embedding(next_observations)\n      data = jnp.concatenate([observations, next_observations], axis=-1)\n    else:\n      data = observations\n    if self._use_action:\n      action_spec = self._environment_spec.actions\n      if isinstance(action_spec, specs.DiscreteArray):\n        actions = jax.nn.one_hot(actions,\n                                 action_spec.num_values)\n      data = jnp.concatenate([data, actions], axis=-1)\n    output = self._network_core(data, is_training, rng)\n    output = jnp.squeeze(output, axis=-1)\n    return output",
  "def __init__(self,\n               environment_spec: specs.EnvironmentSpec,\n               use_action: bool,\n               use_next_obs: bool,\n               discount: float,\n               g_core: Callable[..., Any],\n               h_core: Callable[..., Any],\n               observation_embedding: Callable[[networks_lib.Observation],\n                                               jnp.ndarray] = lambda x: x,\n               name='airl'):\n    super().__init__(name=name)\n    self._environment_spec = environment_spec\n    self._use_action = use_action\n    self._use_next_obs = use_next_obs\n    self._discount = discount\n    self._g_core = g_core\n    self._h_core = h_core\n    self._observation_embedding = observation_embedding",
  "def __call__(self, observations: networks_lib.Observation,\n               actions: networks_lib.Action,\n               next_observations: networks_lib.Observation,\n               is_training: bool,\n               rng: networks_lib.PRNGKey) -> networks_lib.Logits:\n    g_output = DiscriminatorModule(\n        environment_spec=self._environment_spec,\n        use_action=self._use_action,\n        use_next_obs=self._use_next_obs,\n        network_core=self._g_core,\n        observation_embedding=self._observation_embedding,\n        name='airl_g')(observations, actions, next_observations, is_training,\n                       rng)\n    h_module = DiscriminatorModule(\n        environment_spec=self._environment_spec,\n        use_action=False,\n        use_next_obs=False,\n        network_core=self._h_core,\n        observation_embedding=self._observation_embedding,\n        name='airl_h')\n    return (g_output + self._discount * h_module(next_observations, (),\n                                                 (), is_training, rng) -\n            h_module(observations, (), (), is_training, rng))",
  "def apply_fn(params: hk.Params,\n               policy_params: networks_lib.Params,\n               state: hk.State,\n               transitions: types.Transition,\n               is_training: bool,\n               rng: networks_lib.PRNGKey) -> networks_lib.Logits:\n    output, state = discriminator_transformed.apply(\n        params, state, transitions.observation, transitions.action,\n        transitions.next_observation, is_training, rng)\n    if logpi_fn is not None:\n      logpi = logpi_fn(policy_params, transitions.observation,\n                       transitions.action)\n\n      # Quick Maths:\n      # D = exp(output)/(exp(output) + pi(a|s))\n      # logit(D) = log(D/(1-D)) = log(exp(output)/pi(a|s))\n      # logit(D) = output - logpi\n      return output - logpi, state  # pytype: disable=bad-return-type  # jax-ndarray\n    return output, state",
  "def _l2_normalize(x, axis=None, eps=1e-12):\n      return x * jax.lax.rsqrt((x * x).sum(axis=axis, keepdims=True) + eps)",
  "class AILConfig:\n  \"\"\"Configuration options for AIL.\n\n  Attributes:\n    direct_rl_batch_size: Batch size of a direct rl algorithm (measured in\n      transitions).\n    is_sequence_based: If True, a direct rl algorithm is using SequenceAdder\n      data format. Otherwise the learner assumes that the direct rl algorithm is\n      using NStepTransitionAdder.\n    share_iterator: If True, AIL will use the same iterator for the\n      discriminator network training as the direct rl algorithm.\n    num_sgd_steps_per_step: Only used if 'share_iterator' is False. Denotes how\n      many gradient updates perform per one learner step.\n    discriminator_batch_size:  Batch size for training the discriminator.\n    policy_variable_name: The name of the policy variable to retrieve direct_rl\n      policy parameters.\n    discriminator_optimizer: Optimizer for the discriminator. If not specified\n      it is set to Adam with learning rate of 1e-5.\n    replay_table_name: The name of the reverb replay table to use.\n    prefetch_size: How many batches to prefetch\n    discount: Discount to use for TD updates\n    min_replay_size: Minimal size of replay buffer\n    max_replay_size: Maximal size of replay buffer\n    policy_to_expert_data_ratio: If not None, the direct RL learner will receive\n      expert transitions in the given proportions.\n      policy_to_expert_data_ratio + 1 must divide the direct RL batch size.\n  \"\"\"\n  direct_rl_batch_size: int\n  is_sequence_based: bool = False\n  share_iterator: bool = True\n  num_sgd_steps_per_step: int = 1\n  discriminator_batch_size: int = 256\n  policy_variable_name: Optional[str] = None\n  discriminator_optimizer: Optional[optax.GradientTransformation] = None\n  replay_table_name: str = 'ail_table'\n  prefetch_size: int = 4\n  discount: float = 0.99\n  min_replay_size: int = 1000\n  max_replay_size: int = int(1e6)\n  policy_to_expert_data_ratio: Optional[int] = None\n\n  def __post_init__(self):\n    assert self.direct_rl_batch_size % self.discriminator_batch_size == 0",
  "def get_per_learner_step_batch_size(config: AILConfig) -> int:\n  \"\"\"Returns how many transitions should be sampled per direct learner step.\"\"\"\n  # If the iterators are tied, the discriminator learning batch size has to\n  # match the direct RL one.\n  if config.share_iterator:\n    assert (config.direct_rl_batch_size % config.discriminator_batch_size) == 0\n    return config.direct_rl_batch_size\n  # Otherwise each iteration of the discriminator will sample a batch which will\n  # be split in num_sgd_steps_per_step batches, each of size\n  # discriminator_batch_size.\n  return config.discriminator_batch_size * config.num_sgd_steps_per_step",
  "def __post_init__(self):\n    assert self.direct_rl_batch_size % self.discriminator_batch_size == 0",
  "def _split_transitions(\n    transitions: types.Transition,\n    index: int) -> Tuple[types.Transition, types.Transition]:\n  \"\"\"Splits the given transition on the first axis at the given index.\n\n  Args:\n    transitions: Transitions to split.\n    index: Spliting index.\n\n  Returns:\n    A pair of transitions, the first containing elements before the index\n    (exclusive) and the second after the index (inclusive)\n  \"\"\"\n  return (tree.map_structure(lambda x: x[:index], transitions),\n          tree.map_structure(lambda x: x[index:], transitions))",
  "def _rebatch(iterator: Iterator[types.Transition],\n             batch_size: int) -> Iterator[types.Transition]:\n  \"\"\"Rebatch the itererator with the given batch size.\n\n  Args:\n    iterator: Iterator to rebatch.\n    batch_size: New batch size.\n\n  Yields:\n    Transitions with the new batch size.\n  \"\"\"\n  data = next(iterator)\n  while True:\n    while len(data.reward) < batch_size:\n      # Ensure we can get enough demonstrations.\n      next_data = next(iterator)\n      data = tree.map_structure(lambda *args: np.concatenate(list(args)), data,\n                                next_data)\n    output, data = _split_transitions(data, batch_size)\n    yield output",
  "def _mix_arrays(\n    replay: np.ndarray,\n    demo: np.ndarray,\n    index: int,\n    seed: int) -> np.ndarray:\n  \"\"\"Mixes `replay` and `demo`.\n\n  Args:\n    replay: Replay data to mix. Only index element will be selected.\n    demo: Demonstration data to mix.\n    index: Amount of replay data we should include.\n    seed: RNG seed.\n\n  Returns:\n    An array with replay elements up to 'index' and all the demos.\n  \"\"\"\n  # We're throwing away some replay data here. We have to if we want to make\n  # sure the output info field is correct.\n  output = np.concatenate((replay[:index], demo))\n  return np.random.default_rng(seed=seed).permutation(output)",
  "def _generate_samples_with_demonstrations(\n    demonstration_iterator: Iterator[types.Transition],\n    replay_iterator: Iterator[reverb.ReplaySample],\n    policy_to_expert_data_ratio: int,\n    batch_size) -> Iterator[reverb.ReplaySample]:\n  \"\"\"Generator which creates the sample having demonstrations in them.\n\n  It takes the demonstrations and replay iterators and generates batches with\n  same size as the replay iterator, such that each batches have the ratio of\n  policy and expert data specified in policy_to_expert_data_ratio on average.\n  There is no constraints on how the demonstrations and replay samples should be\n  batched.\n\n  Args:\n    demonstration_iterator: Iterator of demonstrations.\n    replay_iterator: Replay buffer sample iterator.\n    policy_to_expert_data_ratio: Amount of policy transitions for 1 expert\n      transitions.\n    batch_size: Output batch size, which should match the replay batch size.\n\n  Yields:\n    Samples having a mix of demonstrations and policy data. The info will match\n    the current replay sample info and the batch size will be the same as the\n    replay_iterator data batch size.\n  \"\"\"\n  count = 0\n  if batch_size % (policy_to_expert_data_ratio + 1) != 0:\n    raise ValueError(\n        'policy_to_expert_data_ratio + 1 must divide the batch size but '\n        f'{batch_size} % {policy_to_expert_data_ratio+1} !=0')\n  demo_insertion_size = batch_size // (policy_to_expert_data_ratio + 1)\n  policy_insertion_size = batch_size - demo_insertion_size\n\n  demonstration_iterator = _rebatch(demonstration_iterator, demo_insertion_size)\n  for sample, demos in zip(replay_iterator, demonstration_iterator):\n    output_transitions = tree.map_structure(\n        functools.partial(_mix_arrays,\n                          index=policy_insertion_size,\n                          seed=count),\n        sample.data, demos)\n    count += 1\n    yield reverb.ReplaySample(info=sample.info, data=output_transitions)",
  "class AILBuilder(builders.ActorLearnerBuilder[ail_networks.AILNetworks,\n                                              DirectPolicyNetwork,\n                                              learning.AILSample],\n                 Generic[ail_networks.DirectRLNetworks, DirectPolicyNetwork]):\n  \"\"\"AIL Builder.\"\"\"\n\n  def __init__(\n      self,\n      rl_agent: builders.ActorLearnerBuilder[ail_networks.DirectRLNetworks,\n                                             DirectPolicyNetwork,\n                                             reverb.ReplaySample],\n      config: ail_config.AILConfig, discriminator_loss: losses.Loss,\n      make_demonstrations: Callable[[int], Iterator[types.Transition]]):\n    \"\"\"Implements a builder for AIL using rl_agent as forward RL algorithm.\n\n    Args:\n      rl_agent: The standard RL agent used by AIL to optimize the generator.\n      config: a AIL config\n      discriminator_loss: The loss function for the discriminator to minimize.\n      make_demonstrations: A function that returns an iterator with\n        demonstrations to be imitated.\n    \"\"\"\n    self._rl_agent = rl_agent\n    self._config = config\n    self._discriminator_loss = discriminator_loss\n    self._make_demonstrations = make_demonstrations\n\n  def make_learner(self,\n                   random_key: jax_types.PRNGKey,\n                   networks: ail_networks.AILNetworks,\n                   dataset: Iterator[learning.AILSample],\n                   logger_fn: loggers.LoggerFactory,\n                   environment_spec: specs.EnvironmentSpec,\n                   replay_client: Optional[reverb.Client] = None,\n                   counter: Optional[counting.Counter] = None) -> core.Learner:\n    counter = counter or counting.Counter()\n    direct_rl_counter = counting.Counter(counter, 'direct_rl')\n    batch_size_per_learner_step = ail_config.get_per_learner_step_batch_size(\n        self._config)\n\n    direct_rl_learner_key, discriminator_key = jax.random.split(random_key)\n\n    direct_rl_learner = functools.partial(\n        self._rl_agent.make_learner,\n        direct_rl_learner_key,\n        networks.direct_rl_networks,\n        logger_fn=logger_fn,\n        environment_spec=environment_spec,\n        replay_client=replay_client,\n        counter=direct_rl_counter)\n\n    discriminator_optimizer = (\n        self._config.discriminator_optimizer or optax.adam(1e-5))\n\n    return learning.AILLearner(\n        counter,\n        direct_rl_learner_factory=direct_rl_learner,\n        loss_fn=self._discriminator_loss,\n        iterator=dataset,\n        discriminator_optimizer=discriminator_optimizer,\n        ail_network=networks,\n        discriminator_key=discriminator_key,\n        is_sequence_based=self._config.is_sequence_based,\n        num_sgd_steps_per_step=batch_size_per_learner_step //\n        self._config.discriminator_batch_size,\n        policy_variable_name=self._config.policy_variable_name,\n        logger=logger_fn('learner', steps_key=counter.get_steps_key()))\n\n  def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: DirectPolicyNetwork,\n  ) -> List[reverb.Table]:\n    replay_tables = self._rl_agent.make_replay_tables(environment_spec, policy)\n    if self._config.share_iterator:\n      return replay_tables\n    replay_tables.append(\n        reverb.Table(\n            name=self._config.replay_table_name,\n            sampler=reverb.selectors.Uniform(),\n            remover=reverb.selectors.Fifo(),\n            max_size=self._config.max_replay_size,\n            rate_limiter=rate_limiters.MinSize(self._config.min_replay_size),\n            signature=adders_reverb.NStepTransitionAdder.signature(\n                environment_spec)))\n    return replay_tables\n\n  # This function does not expose all the iterators used by the learner when\n  # share_iterator is False, making further wrapping impossible.\n  # TODO(eorsini): Expose all iterators.\n  # Currently GAIL uses 3 iterators, instead we can make it use a single\n  # iterator and return this one here. The way to achieve this would be:\n  # * Create the 3 iterators here.\n  # * zip them and return them here.\n  # * upzip them in the learner (this step will not be necessary once we move to\n  # stateless learners)\n  # This should work fine as the 3 iterators are always iterated in parallel\n  # (i.e. at every step we call next once on each of them).\n  def make_dataset_iterator(\n      self, replay_client: reverb.Client) -> Iterator[learning.AILSample]:\n    batch_size_per_learner_step = ail_config.get_per_learner_step_batch_size(\n        self._config)\n\n    iterator_demonstration = self._make_demonstrations(\n        batch_size_per_learner_step)\n\n    direct_iterator = self._rl_agent.make_dataset_iterator(replay_client)\n\n    if self._config.share_iterator:\n      # In order to reuse the iterator return values and not lose a 2x factor on\n      # sample efficiency, we need to use itertools.tee().\n      discriminator_iterator, direct_iterator = itertools.tee(direct_iterator)\n    else:\n      discriminator_iterator = datasets.make_reverb_dataset(\n          table=self._config.replay_table_name,\n          server_address=replay_client.server_address,\n          batch_size=ail_config.get_per_learner_step_batch_size(self._config),\n          prefetch_size=self._config.prefetch_size).as_numpy_iterator()\n\n    if self._config.policy_to_expert_data_ratio is not None:\n      iterator_demonstration, iterator_demonstration2 = itertools.tee(\n          iterator_demonstration)\n      direct_iterator = _generate_samples_with_demonstrations(\n          iterator_demonstration2, direct_iterator,\n          self._config.policy_to_expert_data_ratio,\n          self._config.direct_rl_batch_size)\n\n    is_sequence_based = self._config.is_sequence_based\n\n    # Don't flatten the discriminator batch if the iterator is not shared.\n    process_discriminator_sample = functools.partial(\n        reverb_utils.replay_sample_to_sars_transition,\n        is_sequence=is_sequence_based and self._config.share_iterator,\n        flatten_batch=is_sequence_based and self._config.share_iterator,\n        strip_last_transition=is_sequence_based and self._config.share_iterator)\n\n    discriminator_iterator = (\n        # Remove the extras to have the same nested structure as demonstrations.\n        process_discriminator_sample(sample)._replace(extras=())\n        for sample in discriminator_iterator)\n\n    return utils.device_put((learning.AILSample(*sample) for sample in zip(\n        discriminator_iterator, direct_iterator, iterator_demonstration)),\n                            jax.devices()[0])\n\n  def make_adder(\n      self, replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[DirectPolicyNetwork]) -> Optional[adders.Adder]:\n    direct_rl_adder = self._rl_agent.make_adder(replay_client, environment_spec,\n                                                policy)\n    if self._config.share_iterator:\n      return direct_rl_adder\n    ail_adder = adders_reverb.NStepTransitionAdder(\n        priority_fns={self._config.replay_table_name: None},\n        client=replay_client,\n        n_step=1,\n        discount=self._config.discount)\n\n    # Some direct rl algorithms (such as PPO), might be passing extra data\n    # which we won't be able to process here properly, so we need to ignore them\n    return adders.ForkingAdder(\n        [adders.IgnoreExtrasAdder(ail_adder), direct_rl_adder])\n\n  def make_actor(\n      self,\n      random_key: jax_types.PRNGKey,\n      policy: DirectPolicyNetwork,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> core.Actor:\n    return self._rl_agent.make_actor(random_key, policy, environment_spec,\n                                     variable_source, adder)\n\n  def make_policy(self,\n                  networks: ail_networks.AILNetworks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> DirectPolicyNetwork:\n    return self._rl_agent.make_policy(networks.direct_rl_networks,\n                                      environment_spec, evaluation)",
  "def __init__(\n      self,\n      rl_agent: builders.ActorLearnerBuilder[ail_networks.DirectRLNetworks,\n                                             DirectPolicyNetwork,\n                                             reverb.ReplaySample],\n      config: ail_config.AILConfig, discriminator_loss: losses.Loss,\n      make_demonstrations: Callable[[int], Iterator[types.Transition]]):\n    \"\"\"Implements a builder for AIL using rl_agent as forward RL algorithm.\n\n    Args:\n      rl_agent: The standard RL agent used by AIL to optimize the generator.\n      config: a AIL config\n      discriminator_loss: The loss function for the discriminator to minimize.\n      make_demonstrations: A function that returns an iterator with\n        demonstrations to be imitated.\n    \"\"\"\n    self._rl_agent = rl_agent\n    self._config = config\n    self._discriminator_loss = discriminator_loss\n    self._make_demonstrations = make_demonstrations",
  "def make_learner(self,\n                   random_key: jax_types.PRNGKey,\n                   networks: ail_networks.AILNetworks,\n                   dataset: Iterator[learning.AILSample],\n                   logger_fn: loggers.LoggerFactory,\n                   environment_spec: specs.EnvironmentSpec,\n                   replay_client: Optional[reverb.Client] = None,\n                   counter: Optional[counting.Counter] = None) -> core.Learner:\n    counter = counter or counting.Counter()\n    direct_rl_counter = counting.Counter(counter, 'direct_rl')\n    batch_size_per_learner_step = ail_config.get_per_learner_step_batch_size(\n        self._config)\n\n    direct_rl_learner_key, discriminator_key = jax.random.split(random_key)\n\n    direct_rl_learner = functools.partial(\n        self._rl_agent.make_learner,\n        direct_rl_learner_key,\n        networks.direct_rl_networks,\n        logger_fn=logger_fn,\n        environment_spec=environment_spec,\n        replay_client=replay_client,\n        counter=direct_rl_counter)\n\n    discriminator_optimizer = (\n        self._config.discriminator_optimizer or optax.adam(1e-5))\n\n    return learning.AILLearner(\n        counter,\n        direct_rl_learner_factory=direct_rl_learner,\n        loss_fn=self._discriminator_loss,\n        iterator=dataset,\n        discriminator_optimizer=discriminator_optimizer,\n        ail_network=networks,\n        discriminator_key=discriminator_key,\n        is_sequence_based=self._config.is_sequence_based,\n        num_sgd_steps_per_step=batch_size_per_learner_step //\n        self._config.discriminator_batch_size,\n        policy_variable_name=self._config.policy_variable_name,\n        logger=logger_fn('learner', steps_key=counter.get_steps_key()))",
  "def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: DirectPolicyNetwork,\n  ) -> List[reverb.Table]:\n    replay_tables = self._rl_agent.make_replay_tables(environment_spec, policy)\n    if self._config.share_iterator:\n      return replay_tables\n    replay_tables.append(\n        reverb.Table(\n            name=self._config.replay_table_name,\n            sampler=reverb.selectors.Uniform(),\n            remover=reverb.selectors.Fifo(),\n            max_size=self._config.max_replay_size,\n            rate_limiter=rate_limiters.MinSize(self._config.min_replay_size),\n            signature=adders_reverb.NStepTransitionAdder.signature(\n                environment_spec)))\n    return replay_tables",
  "def make_dataset_iterator(\n      self, replay_client: reverb.Client) -> Iterator[learning.AILSample]:\n    batch_size_per_learner_step = ail_config.get_per_learner_step_batch_size(\n        self._config)\n\n    iterator_demonstration = self._make_demonstrations(\n        batch_size_per_learner_step)\n\n    direct_iterator = self._rl_agent.make_dataset_iterator(replay_client)\n\n    if self._config.share_iterator:\n      # In order to reuse the iterator return values and not lose a 2x factor on\n      # sample efficiency, we need to use itertools.tee().\n      discriminator_iterator, direct_iterator = itertools.tee(direct_iterator)\n    else:\n      discriminator_iterator = datasets.make_reverb_dataset(\n          table=self._config.replay_table_name,\n          server_address=replay_client.server_address,\n          batch_size=ail_config.get_per_learner_step_batch_size(self._config),\n          prefetch_size=self._config.prefetch_size).as_numpy_iterator()\n\n    if self._config.policy_to_expert_data_ratio is not None:\n      iterator_demonstration, iterator_demonstration2 = itertools.tee(\n          iterator_demonstration)\n      direct_iterator = _generate_samples_with_demonstrations(\n          iterator_demonstration2, direct_iterator,\n          self._config.policy_to_expert_data_ratio,\n          self._config.direct_rl_batch_size)\n\n    is_sequence_based = self._config.is_sequence_based\n\n    # Don't flatten the discriminator batch if the iterator is not shared.\n    process_discriminator_sample = functools.partial(\n        reverb_utils.replay_sample_to_sars_transition,\n        is_sequence=is_sequence_based and self._config.share_iterator,\n        flatten_batch=is_sequence_based and self._config.share_iterator,\n        strip_last_transition=is_sequence_based and self._config.share_iterator)\n\n    discriminator_iterator = (\n        # Remove the extras to have the same nested structure as demonstrations.\n        process_discriminator_sample(sample)._replace(extras=())\n        for sample in discriminator_iterator)\n\n    return utils.device_put((learning.AILSample(*sample) for sample in zip(\n        discriminator_iterator, direct_iterator, iterator_demonstration)),\n                            jax.devices()[0])",
  "def make_adder(\n      self, replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[DirectPolicyNetwork]) -> Optional[adders.Adder]:\n    direct_rl_adder = self._rl_agent.make_adder(replay_client, environment_spec,\n                                                policy)\n    if self._config.share_iterator:\n      return direct_rl_adder\n    ail_adder = adders_reverb.NStepTransitionAdder(\n        priority_fns={self._config.replay_table_name: None},\n        client=replay_client,\n        n_step=1,\n        discount=self._config.discount)\n\n    # Some direct rl algorithms (such as PPO), might be passing extra data\n    # which we won't be able to process here properly, so we need to ignore them\n    return adders.ForkingAdder(\n        [adders.IgnoreExtrasAdder(ail_adder), direct_rl_adder])",
  "def make_actor(\n      self,\n      random_key: jax_types.PRNGKey,\n      policy: DirectPolicyNetwork,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> core.Actor:\n    return self._rl_agent.make_actor(random_key, policy, environment_spec,\n                                     variable_source, adder)",
  "def make_policy(self,\n                  networks: ail_networks.AILNetworks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> DirectPolicyNetwork:\n    return self._rl_agent.make_policy(networks.direct_rl_networks,\n                                      environment_spec, evaluation)",
  "def fairl_reward(\n    max_reward_magnitude: Optional[float] = None\n) -> ail_networks.ImitationRewardFn:\n  \"\"\"The FAIRL reward function (https://arxiv.org/pdf/1911.02256.pdf).\n\n  Args:\n    max_reward_magnitude: Clipping value for the reward.\n\n  Returns:\n    The function from logit to imitation reward.\n  \"\"\"\n\n  def imitation_reward(logits: networks_lib.Logits) -> float:\n    rewards = jnp.exp(jnp.clip(logits, a_max=20.)) * -logits\n    if max_reward_magnitude is not None:\n      # pylint: disable=invalid-unary-operand-type\n      rewards = jnp.clip(\n          rewards, a_min=-max_reward_magnitude, a_max=max_reward_magnitude)\n    return rewards  # pytype: disable=bad-return-type  # jax-types\n\n  return imitation_reward",
  "def gail_reward(\n    reward_balance: float = .5,\n    max_reward_magnitude: Optional[float] = None\n) -> ail_networks.ImitationRewardFn:\n  \"\"\"GAIL reward function (https://arxiv.org/pdf/1606.03476.pdf).\n\n  Args:\n    reward_balance: 1 means log(D) reward, 0 means -log(1-D) and other values\n      mean an average of the two.\n    max_reward_magnitude: Clipping value for the reward.\n\n  Returns:\n    The function from logit to imitation reward.\n  \"\"\"\n\n  def imitation_reward(logits: networks_lib.Logits) -> float:\n    # Quick Maths:\n    # logits = ln(D) - ln(1-D)\n    # -softplus(-logits) = ln(D)\n    # softplus(logits) = -ln(1-D)\n    rewards = (\n        reward_balance * -jax.nn.softplus(-logits) +\n        (1 - reward_balance) * jax.nn.softplus(logits))\n    if max_reward_magnitude is not None:\n      # pylint: disable=invalid-unary-operand-type\n      rewards = jnp.clip(\n          rewards, a_min=-max_reward_magnitude, a_max=max_reward_magnitude)\n    return rewards\n\n  return imitation_reward",
  "def imitation_reward(logits: networks_lib.Logits) -> float:\n    rewards = jnp.exp(jnp.clip(logits, a_max=20.)) * -logits\n    if max_reward_magnitude is not None:\n      # pylint: disable=invalid-unary-operand-type\n      rewards = jnp.clip(\n          rewards, a_min=-max_reward_magnitude, a_max=max_reward_magnitude)\n    return rewards",
  "def imitation_reward(logits: networks_lib.Logits) -> float:\n    # Quick Maths:\n    # logits = ln(D) - ln(1-D)\n    # -softplus(-logits) = ln(D)\n    # softplus(logits) = -ln(1-D)\n    rewards = (\n        reward_balance * -jax.nn.softplus(-logits) +\n        (1 - reward_balance) * jax.nn.softplus(logits))\n    if max_reward_magnitude is not None:\n      # pylint: disable=invalid-unary-operand-type\n      rewards = jnp.clip(\n          rewards, a_min=-max_reward_magnitude, a_max=max_reward_magnitude)\n    return rewards",
  "class BuilderTest(absltest.TestCase):\n\n  def test_weighted_generator(self):\n    data0 = types.Transition(np.array([[1], [2], [3]]), (), _REWARD, (), ())\n    it0 = iter([data0])\n\n    data1 = types.Transition(np.array([[4], [5], [6]]), (), _REWARD, (), ())\n    data2 = types.Transition(np.array([[7], [8], [9]]), (), _REWARD, (), ())\n    it1 = iter([\n        reverb.ReplaySample(\n            info=reverb.SampleInfo(\n                *[() for _ in reverb.SampleInfo.tf_dtypes()]),\n            data=data1),\n        reverb.ReplaySample(\n            info=reverb.SampleInfo(\n                *[() for _ in reverb.SampleInfo.tf_dtypes()]),\n            data=data2)\n    ])\n\n    weighted_it = builder._generate_samples_with_demonstrations(\n        it0, it1, policy_to_expert_data_ratio=2, batch_size=3)\n\n    np.testing.assert_array_equal(\n        next(weighted_it).data.observation, np.array([[1], [4], [5]]))\n    np.testing.assert_array_equal(\n        next(weighted_it).data.observation, np.array([[7], [8], [2]]))\n    self.assertRaises(StopIteration, lambda: next(weighted_it))",
  "def test_weighted_generator(self):\n    data0 = types.Transition(np.array([[1], [2], [3]]), (), _REWARD, (), ())\n    it0 = iter([data0])\n\n    data1 = types.Transition(np.array([[4], [5], [6]]), (), _REWARD, (), ())\n    data2 = types.Transition(np.array([[7], [8], [9]]), (), _REWARD, (), ())\n    it1 = iter([\n        reverb.ReplaySample(\n            info=reverb.SampleInfo(\n                *[() for _ in reverb.SampleInfo.tf_dtypes()]),\n            data=data1),\n        reverb.ReplaySample(\n            info=reverb.SampleInfo(\n                *[() for _ in reverb.SampleInfo.tf_dtypes()]),\n            data=data2)\n    ])\n\n    weighted_it = builder._generate_samples_with_demonstrations(\n        it0, it1, policy_to_expert_data_ratio=2, batch_size=3)\n\n    np.testing.assert_array_equal(\n        next(weighted_it).data.observation, np.array([[1], [4], [5]]))\n    np.testing.assert_array_equal(\n        next(weighted_it).data.observation, np.array([[7], [8], [2]]))\n    self.assertRaises(StopIteration, lambda: next(weighted_it))",
  "def _make_discriminator(spec):\n  def discriminator(*args, **kwargs) -> networks_lib.Logits:\n    return ail_networks.DiscriminatorModule(\n        environment_spec=spec,\n        use_action=False,\n        use_next_obs=False,\n        network_core=ail_networks.DiscriminatorMLP([]))(*args, **kwargs)\n\n  discriminator_transformed = hk.without_apply_rng(\n      hk.transform_with_state(discriminator))\n  return ail_networks.make_discriminator(\n      environment_spec=spec,\n      discriminator_transformed=discriminator_transformed)",
  "class AilLearnerTest(absltest.TestCase):\n\n  def test_step(self):\n    simple_spec = specs.Array(shape=(), dtype=float)\n\n    spec = specs.EnvironmentSpec(simple_spec, simple_spec, simple_spec,\n                                 simple_spec)\n\n    discriminator = _make_discriminator(spec)\n    ail_network = ail_networks.AILNetworks(\n        discriminator, imitation_reward_fn=lambda x: x, direct_rl_networks=None)\n\n    loss = losses.gail_loss()\n\n    optimizer = optax.adam(.01)\n\n    step = jax.jit(functools.partial(\n        ail_learning.ail_update_step,\n        optimizer=optimizer,\n        ail_network=ail_network,\n        loss_fn=loss))\n\n    zero_transition = types.Transition(\n        np.array([0.]), np.array([0.]), 0., 0., np.array([0.]))\n    zero_transition = utils.add_batch_dim(zero_transition)\n\n    one_transition = types.Transition(\n        np.array([1.]), np.array([0.]), 0., 0., np.array([0.]))\n    one_transition = utils.add_batch_dim(one_transition)\n\n    key = jax.random.PRNGKey(0)\n    discriminator_params, discriminator_state = discriminator.init(key)\n\n    state = ail_learning.DiscriminatorTrainingState(\n        optimizer_state=optimizer.init(discriminator_params),\n        discriminator_params=discriminator_params,\n        discriminator_state=discriminator_state,\n        policy_params=None,\n        key=key,\n        steps=0,\n    )\n\n    expected_loss = [1.062, 1.057, 1.052]\n\n    for i in range(3):\n      state, loss = step(state, (one_transition, zero_transition))\n      self.assertAlmostEqual(loss['total_loss'], expected_loss[i], places=3)",
  "def discriminator(*args, **kwargs) -> networks_lib.Logits:\n    return ail_networks.DiscriminatorModule(\n        environment_spec=spec,\n        use_action=False,\n        use_next_obs=False,\n        network_core=ail_networks.DiscriminatorMLP([]))(*args, **kwargs)",
  "def test_step(self):\n    simple_spec = specs.Array(shape=(), dtype=float)\n\n    spec = specs.EnvironmentSpec(simple_spec, simple_spec, simple_spec,\n                                 simple_spec)\n\n    discriminator = _make_discriminator(spec)\n    ail_network = ail_networks.AILNetworks(\n        discriminator, imitation_reward_fn=lambda x: x, direct_rl_networks=None)\n\n    loss = losses.gail_loss()\n\n    optimizer = optax.adam(.01)\n\n    step = jax.jit(functools.partial(\n        ail_learning.ail_update_step,\n        optimizer=optimizer,\n        ail_network=ail_network,\n        loss_fn=loss))\n\n    zero_transition = types.Transition(\n        np.array([0.]), np.array([0.]), 0., 0., np.array([0.]))\n    zero_transition = utils.add_batch_dim(zero_transition)\n\n    one_transition = types.Transition(\n        np.array([1.]), np.array([0.]), 0., 0., np.array([0.]))\n    one_transition = utils.add_batch_dim(one_transition)\n\n    key = jax.random.PRNGKey(0)\n    discriminator_params, discriminator_state = discriminator.init(key)\n\n    state = ail_learning.DiscriminatorTrainingState(\n        optimizer_state=optimizer.init(discriminator_params),\n        discriminator_params=discriminator_params,\n        discriminator_state=discriminator_state,\n        policy_params=None,\n        key=key,\n        steps=0,\n    )\n\n    expected_loss = [1.062, 1.057, 1.052]\n\n    for i in range(3):\n      state, loss = step(state, (one_transition, zero_transition))\n      self.assertAlmostEqual(loss['total_loss'], expected_loss[i], places=3)",
  "class PrioritizedDoubleQLearning(learning_lib.LossFn):\n  \"\"\"Clipped double q learning with prioritization on TD error.\"\"\"\n  discount: float = 0.99\n  importance_sampling_exponent: float = 0.2\n  max_abs_reward: float = 1.\n  huber_loss_parameter: float = 1.\n\n  def __call__(\n      self,\n      network: networks_lib.TypedFeedForwardNetwork,\n      params: networks_lib.Params,\n      target_params: networks_lib.Params,\n      batch: reverb.ReplaySample,\n      key: networks_lib.PRNGKey,\n  ) -> Tuple[jax.Array, learning_lib.LossExtra]:\n    \"\"\"Calculate a loss on a single batch of data.\"\"\"\n    transitions: types.Transition = batch.data\n    probs = batch.info.probability\n\n    # Forward pass.\n    key1, key2, key3 = jax.random.split(key, 3)\n    q_tm1 = network.apply(\n        params, transitions.observation, is_training=True, key=key1)\n    q_t_value = network.apply(\n        target_params, transitions.next_observation, is_training=True, key=key2)\n    q_t_selector = network.apply(\n        params, transitions.next_observation, is_training=True, key=key3)\n\n    # Cast and clip rewards.\n    d_t = (transitions.discount * self.discount).astype(jnp.float32)\n    r_t = jnp.clip(transitions.reward, -self.max_abs_reward,\n                   self.max_abs_reward).astype(jnp.float32)\n\n    # Compute double Q-learning n-step TD-error.\n    batch_error = jax.vmap(rlax.double_q_learning)\n    td_error = batch_error(q_tm1, transitions.action, r_t, d_t, q_t_value,\n                           q_t_selector)\n    batch_loss = rlax.huber_loss(td_error, self.huber_loss_parameter)\n\n    # Importance weighting.\n    importance_weights = (1. / probs).astype(jnp.float32)\n    importance_weights **= self.importance_sampling_exponent\n    importance_weights /= jnp.max(importance_weights)\n\n    # Reweight.\n    loss = jnp.mean(importance_weights * batch_loss)  # []\n    extra = learning_lib.LossExtra(\n        metrics={}, reverb_priorities=jnp.abs(td_error).astype(jnp.float64))\n    return loss, extra",
  "class QrDqn(learning_lib.LossFn):\n  \"\"\"Quantile Regression DQN.\n\n  https://arxiv.org/abs/1710.10044\n  \"\"\"\n  num_atoms: int = 51\n  huber_param: float = 1.0\n\n  def __call__(\n      self,\n      network: networks_lib.TypedFeedForwardNetwork,\n      params: networks_lib.Params,\n      target_params: networks_lib.Params,\n      batch: reverb.ReplaySample,\n      key: networks_lib.PRNGKey,\n  ) -> Tuple[jax.Array, learning_lib.LossExtra]:\n    \"\"\"Calculate a loss on a single batch of data.\"\"\"\n    transitions: types.Transition = batch.data\n    key1, key2 = jax.random.split(key)\n    _, dist_q_tm1 = network.apply(\n        params, transitions.observation, is_training=True, key=key1)\n    _, dist_q_target_t = network.apply(\n        target_params, transitions.next_observation, is_training=True, key=key2)\n    batch_size = len(transitions.observation)\n    chex.assert_shape(\n        dist_q_tm1, (\n            batch_size,\n            None,\n            self.num_atoms,\n        ),\n        custom_message=f'Expected (batch_size, num_actions, num_atoms), got: {dist_q_tm1.shape}',\n        include_default_message=True)\n    chex.assert_shape(\n        dist_q_target_t, (\n            batch_size,\n            None,\n            self.num_atoms,\n        ),\n        custom_message=f'Expected (batch_size, num_actions, num_atoms), got: {dist_q_target_t.shape}',\n        include_default_message=True)\n    # Swap distribution and action dimension, since\n    # rlax.quantile_q_learning expects it that way.\n    dist_q_tm1 = jnp.swapaxes(dist_q_tm1, 1, 2)\n    dist_q_target_t = jnp.swapaxes(dist_q_target_t, 1, 2)\n    quantiles = (\n        (jnp.arange(self.num_atoms, dtype=jnp.float32) + 0.5) / self.num_atoms)\n    batch_quantile_q_learning = jax.vmap(\n        rlax.quantile_q_learning, in_axes=(0, None, 0, 0, 0, 0, 0, None))\n    losses = batch_quantile_q_learning(\n        dist_q_tm1,\n        quantiles,\n        transitions.action,\n        transitions.reward,\n        transitions.discount,\n        dist_q_target_t,  # No double Q-learning here.\n        dist_q_target_t,\n        self.huber_param,\n    )\n    loss = jnp.mean(losses)\n    chex.assert_shape(losses, (batch_size,))\n    extra = learning_lib.LossExtra(metrics={'mean_loss': loss})\n    return loss, extra",
  "class PrioritizedCategoricalDoubleQLearning(learning_lib.LossFn):\n  \"\"\"Categorical double q learning with prioritization on TD error.\"\"\"\n  discount: float = 0.99\n  importance_sampling_exponent: float = 0.2\n  max_abs_reward: float = 1.\n\n  def __call__(\n      self,\n      network: networks_lib.TypedFeedForwardNetwork,\n      params: networks_lib.Params,\n      target_params: networks_lib.Params,\n      batch: reverb.ReplaySample,\n      key: networks_lib.PRNGKey,\n  ) -> Tuple[jax.Array, learning_lib.LossExtra]:\n    \"\"\"Calculate a loss on a single batch of data.\"\"\"\n    transitions: types.Transition = batch.data\n    probs = batch.info.probability\n\n    # Forward pass.\n    key1, key2, key3 = jax.random.split(key, 3)\n    _, logits_tm1, atoms_tm1 = network.apply(\n        params, transitions.observation, is_training=True, key=key1)\n    _, logits_t, atoms_t = network.apply(\n        target_params, transitions.next_observation, is_training=True, key=key2)\n    q_t_selector, _, _ = network.apply(\n        params, transitions.next_observation, is_training=True, key=key3)\n\n    # Cast and clip rewards.\n    d_t = (transitions.discount * self.discount).astype(jnp.float32)\n    r_t = jnp.clip(transitions.reward, -self.max_abs_reward,\n                   self.max_abs_reward).astype(jnp.float32)\n\n    # Compute categorical double Q-learning loss.\n    batch_loss_fn = jax.vmap(\n        rlax.categorical_double_q_learning,\n        in_axes=(None, 0, 0, 0, 0, None, 0, 0))\n    batch_loss = batch_loss_fn(atoms_tm1, logits_tm1, transitions.action, r_t,\n                               d_t, atoms_t, logits_t, q_t_selector)\n\n    # Importance weighting.\n    importance_weights = (1. / probs).astype(jnp.float32)\n    importance_weights **= self.importance_sampling_exponent\n    importance_weights /= jnp.max(importance_weights)\n\n    # Reweight.\n    loss = jnp.mean(importance_weights * batch_loss)  # []\n    extra = learning_lib.LossExtra(\n        metrics={}, reverb_priorities=jnp.abs(batch_loss).astype(jnp.float64))\n    return loss, extra",
  "class QLearning(learning_lib.LossFn):\n  \"\"\"Deep q learning.\n\n  This matches the original DQN loss: https://arxiv.org/abs/1312.5602.\n  It differs by two aspects that improve it on the optimization side\n    - it uses Adam instead of RMSProp as an optimizer\n    - it uses a square loss instead of the Huber one.\n  \"\"\"\n  discount: float = 0.99\n  max_abs_reward: float = 1.\n\n  def __call__(\n      self,\n      network: networks_lib.TypedFeedForwardNetwork,\n      params: networks_lib.Params,\n      target_params: networks_lib.Params,\n      batch: reverb.ReplaySample,\n      key: networks_lib.PRNGKey,\n  ) -> Tuple[jax.Array, learning_lib.LossExtra]:\n    \"\"\"Calculate a loss on a single batch of data.\"\"\"\n    transitions: types.Transition = batch.data\n\n    # Forward pass.\n    key1, key2 = jax.random.split(key)\n    q_tm1 = network.apply(\n        params, transitions.observation, is_training=True, key=key1)\n    q_t = network.apply(\n        target_params, transitions.next_observation, is_training=True, key=key2)\n\n    # Cast and clip rewards.\n    d_t = (transitions.discount * self.discount).astype(jnp.float32)\n    r_t = jnp.clip(transitions.reward, -self.max_abs_reward,\n                   self.max_abs_reward).astype(jnp.float32)\n\n    # Compute Q-learning TD-error.\n    batch_error = jax.vmap(rlax.q_learning)\n    td_error = batch_error(q_tm1, transitions.action, r_t, d_t, q_t)\n    batch_loss = jnp.square(td_error)\n\n    loss = jnp.mean(batch_loss)\n    extra = learning_lib.LossExtra(metrics={})\n    return loss, extra",
  "class RegularizedQLearning(learning_lib.LossFn):\n  \"\"\"Regularized Q-learning.\n\n  Implements DQNReg loss function: https://arxiv.org/abs/2101.03958.\n  This is almost identical to QLearning except: 1) Adds a regularization term;\n  2) Uses vanilla TD error without huber loss. 3) No reward clipping.\n  \"\"\"\n  discount: float = 0.99\n  regularizer_coeff = 0.1\n\n  def __call__(\n      self,\n      network: networks_lib.TypedFeedForwardNetwork,\n      params: networks_lib.Params,\n      target_params: networks_lib.Params,\n      batch: reverb.ReplaySample,\n      key: networks_lib.PRNGKey,\n  ) -> Tuple[jax.Array, learning_lib.LossExtra]:\n    \"\"\"Calculate a loss on a single batch of data.\"\"\"\n    transitions: types.Transition = batch.data\n\n    # Forward pass.\n    key1, key2 = jax.random.split(key)\n    q_tm1 = network.apply(\n        params, transitions.observation, is_training=True, key=key1)\n    q_t = network.apply(\n        target_params, transitions.next_observation, is_training=True, key=key2)\n\n    d_t = (transitions.discount * self.discount).astype(jnp.float32)\n\n    # Compute Q-learning TD-error.\n    batch_error = jax.vmap(rlax.q_learning)\n    td_error = batch_error(\n        q_tm1, transitions.action, transitions.reward, d_t, q_t)\n    td_error = 0.5 * jnp.square(td_error)\n\n    def select(qtm1, action):\n      return qtm1[action]\n    q_regularizer = jax.vmap(select)(q_tm1, transitions.action)\n\n    loss = self.regularizer_coeff * jnp.mean(q_regularizer) + jnp.mean(td_error)\n    extra = learning_lib.LossExtra(metrics={})\n    return loss, extra",
  "class MunchausenQLearning(learning_lib.LossFn):\n  \"\"\"Munchausen q learning.\n\n  Implements M-DQN: https://arxiv.org/abs/2007.14430.\n  \"\"\"\n  entropy_temperature: float = 0.03  # tau parameter\n  munchausen_coefficient: float = 0.9  # alpha parameter\n  clip_value_min: float = -1e3\n  discount: float = 0.99\n  max_abs_reward: float = 1.\n  huber_loss_parameter: float = 1.\n\n  def __call__(\n      self,\n      network: networks_lib.TypedFeedForwardNetwork,\n      params: networks_lib.Params,\n      target_params: networks_lib.Params,\n      batch: reverb.ReplaySample,\n      key: networks_lib.PRNGKey,\n  ) -> Tuple[jax.Array, learning_lib.LossExtra]:\n    \"\"\"Calculate a loss on a single batch of data.\"\"\"\n    transitions: types.Transition = batch.data\n\n    # Forward pass.\n    key1, key2, key3 = jax.random.split(key, 3)\n    q_online_s = network.apply(\n        params, transitions.observation, is_training=True, key=key1)\n    action_one_hot = jax.nn.one_hot(transitions.action, q_online_s.shape[-1])\n    q_online_sa = jnp.sum(action_one_hot * q_online_s, axis=-1)\n    q_target_s = network.apply(\n        target_params, transitions.observation, is_training=True, key=key2)\n    q_target_next = network.apply(\n        target_params, transitions.next_observation, is_training=True, key=key3)\n\n    # Cast and clip rewards.\n    d_t = (transitions.discount * self.discount).astype(jnp.float32)\n    r_t = jnp.clip(transitions.reward, -self.max_abs_reward,\n                   self.max_abs_reward).astype(jnp.float32)\n\n    # Munchausen term : tau * log_pi(a|s)\n    munchausen_term = self.entropy_temperature * jax.nn.log_softmax(\n        q_target_s / self.entropy_temperature, axis=-1)\n    munchausen_term_a = jnp.sum(action_one_hot * munchausen_term, axis=-1)\n    munchausen_term_a = jnp.clip(munchausen_term_a,\n                                 a_min=self.clip_value_min,\n                                 a_max=0.)\n\n    # Soft Bellman operator applied to q\n    next_v = self.entropy_temperature * jax.nn.logsumexp(\n        q_target_next / self.entropy_temperature, axis=-1)\n    target_q = jax.lax.stop_gradient(r_t + self.munchausen_coefficient *\n                                     munchausen_term_a + d_t * next_v)\n\n    batch_loss = rlax.huber_loss(target_q - q_online_sa,\n                                 self.huber_loss_parameter)\n    loss = jnp.mean(batch_loss)\n\n    extra = learning_lib.LossExtra(metrics={})\n    return loss, extra",
  "def __call__(\n      self,\n      network: networks_lib.TypedFeedForwardNetwork,\n      params: networks_lib.Params,\n      target_params: networks_lib.Params,\n      batch: reverb.ReplaySample,\n      key: networks_lib.PRNGKey,\n  ) -> Tuple[jax.Array, learning_lib.LossExtra]:\n    \"\"\"Calculate a loss on a single batch of data.\"\"\"\n    transitions: types.Transition = batch.data\n    probs = batch.info.probability\n\n    # Forward pass.\n    key1, key2, key3 = jax.random.split(key, 3)\n    q_tm1 = network.apply(\n        params, transitions.observation, is_training=True, key=key1)\n    q_t_value = network.apply(\n        target_params, transitions.next_observation, is_training=True, key=key2)\n    q_t_selector = network.apply(\n        params, transitions.next_observation, is_training=True, key=key3)\n\n    # Cast and clip rewards.\n    d_t = (transitions.discount * self.discount).astype(jnp.float32)\n    r_t = jnp.clip(transitions.reward, -self.max_abs_reward,\n                   self.max_abs_reward).astype(jnp.float32)\n\n    # Compute double Q-learning n-step TD-error.\n    batch_error = jax.vmap(rlax.double_q_learning)\n    td_error = batch_error(q_tm1, transitions.action, r_t, d_t, q_t_value,\n                           q_t_selector)\n    batch_loss = rlax.huber_loss(td_error, self.huber_loss_parameter)\n\n    # Importance weighting.\n    importance_weights = (1. / probs).astype(jnp.float32)\n    importance_weights **= self.importance_sampling_exponent\n    importance_weights /= jnp.max(importance_weights)\n\n    # Reweight.\n    loss = jnp.mean(importance_weights * batch_loss)  # []\n    extra = learning_lib.LossExtra(\n        metrics={}, reverb_priorities=jnp.abs(td_error).astype(jnp.float64))\n    return loss, extra",
  "def __call__(\n      self,\n      network: networks_lib.TypedFeedForwardNetwork,\n      params: networks_lib.Params,\n      target_params: networks_lib.Params,\n      batch: reverb.ReplaySample,\n      key: networks_lib.PRNGKey,\n  ) -> Tuple[jax.Array, learning_lib.LossExtra]:\n    \"\"\"Calculate a loss on a single batch of data.\"\"\"\n    transitions: types.Transition = batch.data\n    key1, key2 = jax.random.split(key)\n    _, dist_q_tm1 = network.apply(\n        params, transitions.observation, is_training=True, key=key1)\n    _, dist_q_target_t = network.apply(\n        target_params, transitions.next_observation, is_training=True, key=key2)\n    batch_size = len(transitions.observation)\n    chex.assert_shape(\n        dist_q_tm1, (\n            batch_size,\n            None,\n            self.num_atoms,\n        ),\n        custom_message=f'Expected (batch_size, num_actions, num_atoms), got: {dist_q_tm1.shape}',\n        include_default_message=True)\n    chex.assert_shape(\n        dist_q_target_t, (\n            batch_size,\n            None,\n            self.num_atoms,\n        ),\n        custom_message=f'Expected (batch_size, num_actions, num_atoms), got: {dist_q_target_t.shape}',\n        include_default_message=True)\n    # Swap distribution and action dimension, since\n    # rlax.quantile_q_learning expects it that way.\n    dist_q_tm1 = jnp.swapaxes(dist_q_tm1, 1, 2)\n    dist_q_target_t = jnp.swapaxes(dist_q_target_t, 1, 2)\n    quantiles = (\n        (jnp.arange(self.num_atoms, dtype=jnp.float32) + 0.5) / self.num_atoms)\n    batch_quantile_q_learning = jax.vmap(\n        rlax.quantile_q_learning, in_axes=(0, None, 0, 0, 0, 0, 0, None))\n    losses = batch_quantile_q_learning(\n        dist_q_tm1,\n        quantiles,\n        transitions.action,\n        transitions.reward,\n        transitions.discount,\n        dist_q_target_t,  # No double Q-learning here.\n        dist_q_target_t,\n        self.huber_param,\n    )\n    loss = jnp.mean(losses)\n    chex.assert_shape(losses, (batch_size,))\n    extra = learning_lib.LossExtra(metrics={'mean_loss': loss})\n    return loss, extra",
  "def __call__(\n      self,\n      network: networks_lib.TypedFeedForwardNetwork,\n      params: networks_lib.Params,\n      target_params: networks_lib.Params,\n      batch: reverb.ReplaySample,\n      key: networks_lib.PRNGKey,\n  ) -> Tuple[jax.Array, learning_lib.LossExtra]:\n    \"\"\"Calculate a loss on a single batch of data.\"\"\"\n    transitions: types.Transition = batch.data\n    probs = batch.info.probability\n\n    # Forward pass.\n    key1, key2, key3 = jax.random.split(key, 3)\n    _, logits_tm1, atoms_tm1 = network.apply(\n        params, transitions.observation, is_training=True, key=key1)\n    _, logits_t, atoms_t = network.apply(\n        target_params, transitions.next_observation, is_training=True, key=key2)\n    q_t_selector, _, _ = network.apply(\n        params, transitions.next_observation, is_training=True, key=key3)\n\n    # Cast and clip rewards.\n    d_t = (transitions.discount * self.discount).astype(jnp.float32)\n    r_t = jnp.clip(transitions.reward, -self.max_abs_reward,\n                   self.max_abs_reward).astype(jnp.float32)\n\n    # Compute categorical double Q-learning loss.\n    batch_loss_fn = jax.vmap(\n        rlax.categorical_double_q_learning,\n        in_axes=(None, 0, 0, 0, 0, None, 0, 0))\n    batch_loss = batch_loss_fn(atoms_tm1, logits_tm1, transitions.action, r_t,\n                               d_t, atoms_t, logits_t, q_t_selector)\n\n    # Importance weighting.\n    importance_weights = (1. / probs).astype(jnp.float32)\n    importance_weights **= self.importance_sampling_exponent\n    importance_weights /= jnp.max(importance_weights)\n\n    # Reweight.\n    loss = jnp.mean(importance_weights * batch_loss)  # []\n    extra = learning_lib.LossExtra(\n        metrics={}, reverb_priorities=jnp.abs(batch_loss).astype(jnp.float64))\n    return loss, extra",
  "def __call__(\n      self,\n      network: networks_lib.TypedFeedForwardNetwork,\n      params: networks_lib.Params,\n      target_params: networks_lib.Params,\n      batch: reverb.ReplaySample,\n      key: networks_lib.PRNGKey,\n  ) -> Tuple[jax.Array, learning_lib.LossExtra]:\n    \"\"\"Calculate a loss on a single batch of data.\"\"\"\n    transitions: types.Transition = batch.data\n\n    # Forward pass.\n    key1, key2 = jax.random.split(key)\n    q_tm1 = network.apply(\n        params, transitions.observation, is_training=True, key=key1)\n    q_t = network.apply(\n        target_params, transitions.next_observation, is_training=True, key=key2)\n\n    # Cast and clip rewards.\n    d_t = (transitions.discount * self.discount).astype(jnp.float32)\n    r_t = jnp.clip(transitions.reward, -self.max_abs_reward,\n                   self.max_abs_reward).astype(jnp.float32)\n\n    # Compute Q-learning TD-error.\n    batch_error = jax.vmap(rlax.q_learning)\n    td_error = batch_error(q_tm1, transitions.action, r_t, d_t, q_t)\n    batch_loss = jnp.square(td_error)\n\n    loss = jnp.mean(batch_loss)\n    extra = learning_lib.LossExtra(metrics={})\n    return loss, extra",
  "def __call__(\n      self,\n      network: networks_lib.TypedFeedForwardNetwork,\n      params: networks_lib.Params,\n      target_params: networks_lib.Params,\n      batch: reverb.ReplaySample,\n      key: networks_lib.PRNGKey,\n  ) -> Tuple[jax.Array, learning_lib.LossExtra]:\n    \"\"\"Calculate a loss on a single batch of data.\"\"\"\n    transitions: types.Transition = batch.data\n\n    # Forward pass.\n    key1, key2 = jax.random.split(key)\n    q_tm1 = network.apply(\n        params, transitions.observation, is_training=True, key=key1)\n    q_t = network.apply(\n        target_params, transitions.next_observation, is_training=True, key=key2)\n\n    d_t = (transitions.discount * self.discount).astype(jnp.float32)\n\n    # Compute Q-learning TD-error.\n    batch_error = jax.vmap(rlax.q_learning)\n    td_error = batch_error(\n        q_tm1, transitions.action, transitions.reward, d_t, q_t)\n    td_error = 0.5 * jnp.square(td_error)\n\n    def select(qtm1, action):\n      return qtm1[action]\n    q_regularizer = jax.vmap(select)(q_tm1, transitions.action)\n\n    loss = self.regularizer_coeff * jnp.mean(q_regularizer) + jnp.mean(td_error)\n    extra = learning_lib.LossExtra(metrics={})\n    return loss, extra",
  "def __call__(\n      self,\n      network: networks_lib.TypedFeedForwardNetwork,\n      params: networks_lib.Params,\n      target_params: networks_lib.Params,\n      batch: reverb.ReplaySample,\n      key: networks_lib.PRNGKey,\n  ) -> Tuple[jax.Array, learning_lib.LossExtra]:\n    \"\"\"Calculate a loss on a single batch of data.\"\"\"\n    transitions: types.Transition = batch.data\n\n    # Forward pass.\n    key1, key2, key3 = jax.random.split(key, 3)\n    q_online_s = network.apply(\n        params, transitions.observation, is_training=True, key=key1)\n    action_one_hot = jax.nn.one_hot(transitions.action, q_online_s.shape[-1])\n    q_online_sa = jnp.sum(action_one_hot * q_online_s, axis=-1)\n    q_target_s = network.apply(\n        target_params, transitions.observation, is_training=True, key=key2)\n    q_target_next = network.apply(\n        target_params, transitions.next_observation, is_training=True, key=key3)\n\n    # Cast and clip rewards.\n    d_t = (transitions.discount * self.discount).astype(jnp.float32)\n    r_t = jnp.clip(transitions.reward, -self.max_abs_reward,\n                   self.max_abs_reward).astype(jnp.float32)\n\n    # Munchausen term : tau * log_pi(a|s)\n    munchausen_term = self.entropy_temperature * jax.nn.log_softmax(\n        q_target_s / self.entropy_temperature, axis=-1)\n    munchausen_term_a = jnp.sum(action_one_hot * munchausen_term, axis=-1)\n    munchausen_term_a = jnp.clip(munchausen_term_a,\n                                 a_min=self.clip_value_min,\n                                 a_max=0.)\n\n    # Soft Bellman operator applied to q\n    next_v = self.entropy_temperature * jax.nn.logsumexp(\n        q_target_next / self.entropy_temperature, axis=-1)\n    target_q = jax.lax.stop_gradient(r_t + self.munchausen_coefficient *\n                                     munchausen_term_a + d_t * next_v)\n\n    batch_loss = rlax.huber_loss(target_q - q_online_sa,\n                                 self.huber_loss_parameter)\n    loss = jnp.mean(batch_loss)\n\n    extra = learning_lib.LossExtra(metrics={})\n    return loss, extra",
  "def select(qtm1, action):\n      return qtm1[action]",
  "class DQNLearner(learning_lib.SGDLearner):\n  \"\"\"DQN learner.\n\n  We are in the process of migrating towards a more general SGDLearner to allow\n  for easy configuration of the loss. This is maintained now for compatibility.\n  \"\"\"\n\n  def __init__(self,\n               network: networks_lib.TypedFeedForwardNetwork,\n               discount: float,\n               importance_sampling_exponent: float,\n               target_update_period: int,\n               iterator: Iterator[utils.PrefetchingSplit],\n               optimizer: optax.GradientTransformation,\n               random_key: networks_lib.PRNGKey,\n               max_abs_reward: float = 1.,\n               huber_loss_parameter: float = 1.,\n               replay_client: Optional[reverb.Client] = None,\n               replay_table_name: str = adders.DEFAULT_PRIORITY_TABLE,\n               counter: Optional[counting.Counter] = None,\n               logger: Optional[loggers.Logger] = None,\n               num_sgd_steps_per_step: int = 1):\n    \"\"\"Initializes the learner.\"\"\"\n    loss_fn = losses.PrioritizedDoubleQLearning(\n        discount=discount,\n        importance_sampling_exponent=importance_sampling_exponent,\n        max_abs_reward=max_abs_reward,\n        huber_loss_parameter=huber_loss_parameter,\n    )\n    super().__init__(\n        network=network,\n        loss_fn=loss_fn,\n        optimizer=optimizer,\n        data_iterator=iterator,\n        target_update_period=target_update_period,\n        random_key=random_key,\n        replay_client=replay_client,\n        replay_table_name=replay_table_name,\n        counter=counter,\n        logger=logger,\n        num_sgd_steps_per_step=num_sgd_steps_per_step,\n    )",
  "def __init__(self,\n               network: networks_lib.TypedFeedForwardNetwork,\n               discount: float,\n               importance_sampling_exponent: float,\n               target_update_period: int,\n               iterator: Iterator[utils.PrefetchingSplit],\n               optimizer: optax.GradientTransformation,\n               random_key: networks_lib.PRNGKey,\n               max_abs_reward: float = 1.,\n               huber_loss_parameter: float = 1.,\n               replay_client: Optional[reverb.Client] = None,\n               replay_table_name: str = adders.DEFAULT_PRIORITY_TABLE,\n               counter: Optional[counting.Counter] = None,\n               logger: Optional[loggers.Logger] = None,\n               num_sgd_steps_per_step: int = 1):\n    \"\"\"Initializes the learner.\"\"\"\n    loss_fn = losses.PrioritizedDoubleQLearning(\n        discount=discount,\n        importance_sampling_exponent=importance_sampling_exponent,\n        max_abs_reward=max_abs_reward,\n        huber_loss_parameter=huber_loss_parameter,\n    )\n    super().__init__(\n        network=network,\n        loss_fn=loss_fn,\n        optimizer=optimizer,\n        data_iterator=iterator,\n        target_update_period=target_update_period,\n        random_key=random_key,\n        replay_client=replay_client,\n        replay_table_name=replay_table_name,\n        counter=counter,\n        logger=logger,\n        num_sgd_steps_per_step=num_sgd_steps_per_step,\n    )",
  "def default_sample_fn(action_values: networks_lib.NetworkOutput,\n                      key: types.PRNGKey,\n                      epsilon: Epsilon) -> networks_lib.Action:\n  return rlax.epsilon_greedy(epsilon).sample(key, action_values)",
  "class DQNNetworks:\n  \"\"\"The network and pure functions for the DQN agent.\n\n  Attributes:\n    policy_network: The policy network.\n    sample_fn: A pure function. Samples an action based on the network output.\n    log_prob: A pure function. Computes log-probability for an action.\n  \"\"\"\n  policy_network: networks_lib.TypedFeedForwardNetwork\n  sample_fn: EpsilonSampleFn = default_sample_fn\n  log_prob: Optional[EpsilonLogProbFn] = None",
  "class ReverbUpdate(NamedTuple):\n  \"\"\"Tuple for updating reverb priority information.\"\"\"\n  keys: jnp.ndarray\n  priorities: jnp.ndarray",
  "class LossExtra(NamedTuple):\n  \"\"\"Extra information that is returned along with loss value.\"\"\"\n  metrics: Dict[str, jax.Array]\n  # New optional updated priorities for the samples.\n  reverb_priorities: Optional[jax.Array] = None",
  "class LossFn(typing_extensions.Protocol):\n  \"\"\"A LossFn calculates a loss on a single batch of data.\"\"\"\n\n  def __call__(\n      self,\n      network: networks_lib.TypedFeedForwardNetwork,\n      params: networks_lib.Params,\n      target_params: networks_lib.Params,\n      batch: reverb.ReplaySample,\n      key: networks_lib.PRNGKey,\n  ) -> Tuple[jax.Array, LossExtra]:\n    \"\"\"Calculates a loss on a single batch of data.\"\"\"",
  "class TrainingState(NamedTuple):\n  \"\"\"Holds the agent's training state.\"\"\"\n  params: networks_lib.Params\n  target_params: networks_lib.Params\n  opt_state: optax.OptState\n  steps: int\n  rng_key: networks_lib.PRNGKey",
  "class SGDLearner(acme.Learner):\n  \"\"\"An Acme learner based around SGD on batches.\n\n  This learner currently supports optional prioritized replay and assumes a\n  TrainingState as described above.\n  \"\"\"\n\n  def __init__(self,\n               network: networks_lib.TypedFeedForwardNetwork,\n               loss_fn: LossFn,\n               optimizer: optax.GradientTransformation,\n               data_iterator: Iterator[utils.PrefetchingSplit],\n               target_update_period: int,\n               random_key: networks_lib.PRNGKey,\n               replay_client: Optional[reverb.Client] = None,\n               replay_table_name: str = adders.DEFAULT_PRIORITY_TABLE,\n               counter: Optional[counting.Counter] = None,\n               logger: Optional[loggers.Logger] = None,\n               num_sgd_steps_per_step: int = 1):\n    \"\"\"Initialize the SGD learner.\"\"\"\n    self.network = network\n\n    # Internalize the loss_fn with network.\n    self._loss = jax.jit(functools.partial(loss_fn, self.network))\n\n    # SGD performs the loss, optimizer update and periodic target net update.\n    def sgd_step(state: TrainingState,\n                 batch: reverb.ReplaySample) -> Tuple[TrainingState, LossExtra]:\n      next_rng_key, rng_key = jax.random.split(state.rng_key)\n      # Implements one SGD step of the loss and updates training state\n      (loss, extra), grads = jax.value_and_grad(\n          self._loss, has_aux=True)(state.params, state.target_params, batch,\n                                    rng_key)\n\n      loss = jax.lax.pmean(loss, axis_name=PMAP_AXIS_NAME)\n      # Average gradients over pmap replicas before optimizer update.\n      grads = jax.lax.pmean(grads, axis_name=PMAP_AXIS_NAME)\n      # Apply the optimizer updates\n      updates, new_opt_state = optimizer.update(grads, state.opt_state)\n      new_params = optax.apply_updates(state.params, updates)\n\n      extra.metrics.update({'total_loss': loss})\n\n      # Periodically update target networks.\n      steps = state.steps + 1\n      target_params = optax.periodic_update(new_params, state.target_params,  # pytype: disable=wrong-arg-types  # numpy-scalars\n                                            steps, target_update_period)\n\n      new_training_state = TrainingState(\n          new_params, target_params, new_opt_state, steps, next_rng_key)\n      return new_training_state, extra\n\n    def postprocess_aux(extra: LossExtra) -> LossExtra:\n      reverb_priorities = jax.tree_util.tree_map(\n          lambda a: jnp.reshape(a, (-1, *a.shape[2:])), extra.reverb_priorities)\n      return extra._replace(\n          metrics=jax.tree_util.tree_map(jnp.mean, extra.metrics),\n          reverb_priorities=reverb_priorities)\n\n    self._num_sgd_steps_per_step = num_sgd_steps_per_step\n    sgd_step = utils.process_multiple_batches(sgd_step, num_sgd_steps_per_step,\n                                              postprocess_aux)\n    self._sgd_step = jax.pmap(\n        sgd_step, axis_name=PMAP_AXIS_NAME, devices=jax.devices())\n\n    # Internalise agent components\n    self._data_iterator = data_iterator\n    self._target_update_period = target_update_period\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.TerminalLogger('learner', time_delta=1.)\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None\n\n    # Initialize the network parameters\n    key_params, key_target, key_state = jax.random.split(random_key, 3)\n    initial_params = self.network.init(key_params)\n    initial_target_params = self.network.init(key_target)\n    state = TrainingState(\n        params=initial_params,\n        target_params=initial_target_params,\n        opt_state=optimizer.init(initial_params),\n        steps=0,\n        rng_key=key_state,\n    )\n    self._state = utils.replicate_in_all_devices(state, jax.local_devices())\n\n    # Update replay priorities\n    def update_priorities(reverb_update: ReverbUpdate) -> None:\n      if replay_client is None:\n        return\n      keys, priorities = tree.map_structure(\n          # Fetch array and combine device and batch dimensions.\n          lambda x: utils.fetch_devicearray(x).reshape((-1,) + x.shape[2:]),\n          (reverb_update.keys, reverb_update.priorities))\n      replay_client.mutate_priorities(\n          table=replay_table_name,\n          updates=dict(zip(keys, priorities)))\n    self._replay_client = replay_client\n    self._async_priority_updater = async_utils.AsyncExecutor(update_priorities)\n\n    self._current_step = 0\n\n  def step(self):\n    \"\"\"Takes one SGD step on the learner.\"\"\"\n    with jax.profiler.StepTraceAnnotation('step', step_num=self._current_step):\n      prefetching_split = next(self._data_iterator)\n      # In this case the host property of the prefetching split contains only\n      # replay keys and the device property is the prefetched full original\n      # sample. Key is on host since it's uint64 type.\n      reverb_keys = prefetching_split.host\n      batch: reverb.ReplaySample = prefetching_split.device\n\n      self._state, extra = self._sgd_step(self._state, batch)\n      # Compute elapsed time.\n      timestamp = time.time()\n      elapsed = timestamp - self._timestamp if self._timestamp else 0\n      self._timestamp = timestamp\n\n      if self._replay_client and extra.reverb_priorities is not None:\n        reverb_update = ReverbUpdate(reverb_keys, extra.reverb_priorities)\n        self._async_priority_updater.put(reverb_update)\n\n      steps_per_sec = (self._num_sgd_steps_per_step / elapsed) if elapsed else 0\n      self._current_step, metrics = utils.get_from_first_device(\n          (self._state.steps, extra.metrics))\n      metrics['steps_per_second'] = steps_per_sec\n\n      # Update our counts and record it.\n      result = self._counter.increment(\n          steps=self._num_sgd_steps_per_step, walltime=elapsed)\n      result.update(metrics)\n      self._logger.write(result)\n\n  def get_variables(self, names: List[str]) -> List[networks_lib.Params]:\n    # Return first replica of parameters.\n    return utils.get_from_first_device([self._state.params])\n\n  def save(self) -> TrainingState:\n    # Serialize only the first replica of parameters and optimizer state.\n    return utils.get_from_first_device(self._state)\n\n  def restore(self, state: TrainingState):\n    self._state = utils.replicate_in_all_devices(state, jax.local_devices())",
  "def __call__(\n      self,\n      network: networks_lib.TypedFeedForwardNetwork,\n      params: networks_lib.Params,\n      target_params: networks_lib.Params,\n      batch: reverb.ReplaySample,\n      key: networks_lib.PRNGKey,\n  ) -> Tuple[jax.Array, LossExtra]:\n    \"\"\"Calculates a loss on a single batch of data.\"\"\"",
  "def __init__(self,\n               network: networks_lib.TypedFeedForwardNetwork,\n               loss_fn: LossFn,\n               optimizer: optax.GradientTransformation,\n               data_iterator: Iterator[utils.PrefetchingSplit],\n               target_update_period: int,\n               random_key: networks_lib.PRNGKey,\n               replay_client: Optional[reverb.Client] = None,\n               replay_table_name: str = adders.DEFAULT_PRIORITY_TABLE,\n               counter: Optional[counting.Counter] = None,\n               logger: Optional[loggers.Logger] = None,\n               num_sgd_steps_per_step: int = 1):\n    \"\"\"Initialize the SGD learner.\"\"\"\n    self.network = network\n\n    # Internalize the loss_fn with network.\n    self._loss = jax.jit(functools.partial(loss_fn, self.network))\n\n    # SGD performs the loss, optimizer update and periodic target net update.\n    def sgd_step(state: TrainingState,\n                 batch: reverb.ReplaySample) -> Tuple[TrainingState, LossExtra]:\n      next_rng_key, rng_key = jax.random.split(state.rng_key)\n      # Implements one SGD step of the loss and updates training state\n      (loss, extra), grads = jax.value_and_grad(\n          self._loss, has_aux=True)(state.params, state.target_params, batch,\n                                    rng_key)\n\n      loss = jax.lax.pmean(loss, axis_name=PMAP_AXIS_NAME)\n      # Average gradients over pmap replicas before optimizer update.\n      grads = jax.lax.pmean(grads, axis_name=PMAP_AXIS_NAME)\n      # Apply the optimizer updates\n      updates, new_opt_state = optimizer.update(grads, state.opt_state)\n      new_params = optax.apply_updates(state.params, updates)\n\n      extra.metrics.update({'total_loss': loss})\n\n      # Periodically update target networks.\n      steps = state.steps + 1\n      target_params = optax.periodic_update(new_params, state.target_params,  # pytype: disable=wrong-arg-types  # numpy-scalars\n                                            steps, target_update_period)\n\n      new_training_state = TrainingState(\n          new_params, target_params, new_opt_state, steps, next_rng_key)\n      return new_training_state, extra\n\n    def postprocess_aux(extra: LossExtra) -> LossExtra:\n      reverb_priorities = jax.tree_util.tree_map(\n          lambda a: jnp.reshape(a, (-1, *a.shape[2:])), extra.reverb_priorities)\n      return extra._replace(\n          metrics=jax.tree_util.tree_map(jnp.mean, extra.metrics),\n          reverb_priorities=reverb_priorities)\n\n    self._num_sgd_steps_per_step = num_sgd_steps_per_step\n    sgd_step = utils.process_multiple_batches(sgd_step, num_sgd_steps_per_step,\n                                              postprocess_aux)\n    self._sgd_step = jax.pmap(\n        sgd_step, axis_name=PMAP_AXIS_NAME, devices=jax.devices())\n\n    # Internalise agent components\n    self._data_iterator = data_iterator\n    self._target_update_period = target_update_period\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.TerminalLogger('learner', time_delta=1.)\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None\n\n    # Initialize the network parameters\n    key_params, key_target, key_state = jax.random.split(random_key, 3)\n    initial_params = self.network.init(key_params)\n    initial_target_params = self.network.init(key_target)\n    state = TrainingState(\n        params=initial_params,\n        target_params=initial_target_params,\n        opt_state=optimizer.init(initial_params),\n        steps=0,\n        rng_key=key_state,\n    )\n    self._state = utils.replicate_in_all_devices(state, jax.local_devices())\n\n    # Update replay priorities\n    def update_priorities(reverb_update: ReverbUpdate) -> None:\n      if replay_client is None:\n        return\n      keys, priorities = tree.map_structure(\n          # Fetch array and combine device and batch dimensions.\n          lambda x: utils.fetch_devicearray(x).reshape((-1,) + x.shape[2:]),\n          (reverb_update.keys, reverb_update.priorities))\n      replay_client.mutate_priorities(\n          table=replay_table_name,\n          updates=dict(zip(keys, priorities)))\n    self._replay_client = replay_client\n    self._async_priority_updater = async_utils.AsyncExecutor(update_priorities)\n\n    self._current_step = 0",
  "def step(self):\n    \"\"\"Takes one SGD step on the learner.\"\"\"\n    with jax.profiler.StepTraceAnnotation('step', step_num=self._current_step):\n      prefetching_split = next(self._data_iterator)\n      # In this case the host property of the prefetching split contains only\n      # replay keys and the device property is the prefetched full original\n      # sample. Key is on host since it's uint64 type.\n      reverb_keys = prefetching_split.host\n      batch: reverb.ReplaySample = prefetching_split.device\n\n      self._state, extra = self._sgd_step(self._state, batch)\n      # Compute elapsed time.\n      timestamp = time.time()\n      elapsed = timestamp - self._timestamp if self._timestamp else 0\n      self._timestamp = timestamp\n\n      if self._replay_client and extra.reverb_priorities is not None:\n        reverb_update = ReverbUpdate(reverb_keys, extra.reverb_priorities)\n        self._async_priority_updater.put(reverb_update)\n\n      steps_per_sec = (self._num_sgd_steps_per_step / elapsed) if elapsed else 0\n      self._current_step, metrics = utils.get_from_first_device(\n          (self._state.steps, extra.metrics))\n      metrics['steps_per_second'] = steps_per_sec\n\n      # Update our counts and record it.\n      result = self._counter.increment(\n          steps=self._num_sgd_steps_per_step, walltime=elapsed)\n      result.update(metrics)\n      self._logger.write(result)",
  "def get_variables(self, names: List[str]) -> List[networks_lib.Params]:\n    # Return first replica of parameters.\n    return utils.get_from_first_device([self._state.params])",
  "def save(self) -> TrainingState:\n    # Serialize only the first replica of parameters and optimizer state.\n    return utils.get_from_first_device(self._state)",
  "def restore(self, state: TrainingState):\n    self._state = utils.replicate_in_all_devices(state, jax.local_devices())",
  "def sgd_step(state: TrainingState,\n                 batch: reverb.ReplaySample) -> Tuple[TrainingState, LossExtra]:\n      next_rng_key, rng_key = jax.random.split(state.rng_key)\n      # Implements one SGD step of the loss and updates training state\n      (loss, extra), grads = jax.value_and_grad(\n          self._loss, has_aux=True)(state.params, state.target_params, batch,\n                                    rng_key)\n\n      loss = jax.lax.pmean(loss, axis_name=PMAP_AXIS_NAME)\n      # Average gradients over pmap replicas before optimizer update.\n      grads = jax.lax.pmean(grads, axis_name=PMAP_AXIS_NAME)\n      # Apply the optimizer updates\n      updates, new_opt_state = optimizer.update(grads, state.opt_state)\n      new_params = optax.apply_updates(state.params, updates)\n\n      extra.metrics.update({'total_loss': loss})\n\n      # Periodically update target networks.\n      steps = state.steps + 1\n      target_params = optax.periodic_update(new_params, state.target_params,  # pytype: disable=wrong-arg-types  # numpy-scalars\n                                            steps, target_update_period)\n\n      new_training_state = TrainingState(\n          new_params, target_params, new_opt_state, steps, next_rng_key)\n      return new_training_state, extra",
  "def postprocess_aux(extra: LossExtra) -> LossExtra:\n      reverb_priorities = jax.tree_util.tree_map(\n          lambda a: jnp.reshape(a, (-1, *a.shape[2:])), extra.reverb_priorities)\n      return extra._replace(\n          metrics=jax.tree_util.tree_map(jnp.mean, extra.metrics),\n          reverb_priorities=reverb_priorities)",
  "def update_priorities(reverb_update: ReverbUpdate) -> None:\n      if replay_client is None:\n        return\n      keys, priorities = tree.map_structure(\n          # Fetch array and combine device and batch dimensions.\n          lambda x: utils.fetch_devicearray(x).reshape((-1,) + x.shape[2:]),\n          (reverb_update.keys, reverb_update.priorities))\n      replay_client.mutate_priorities(\n          table=replay_table_name,\n          updates=dict(zip(keys, priorities)))",
  "class DQNConfig:\n  \"\"\"Configuration options for DQN agent.\n\n  Attributes:\n    epsilon: for use by epsilon-greedy policies. If multiple, the epsilons are\n      alternated randomly per-episode.\n    eval_epsilon: for use by evaluation epsilon-greedy policies.\n    seed: Random seed.\n    learning_rate: Learning rate for Adam optimizer. Could be a number or a\n      function defining a schedule.\n    adam_eps: Epsilon for Adam optimizer.\n    discount: Discount rate applied to value per timestep.\n    n_step: N-step TD learning.\n    target_update_period: Update target network every period.\n    max_gradient_norm: For gradient clipping.\n    batch_size: Number of transitions per batch.\n    min_replay_size: Minimum replay size.\n    max_replay_size: Maximum replay size.\n    replay_table_name: Reverb table, defaults to DEFAULT_PRIORITY_TABLE.\n    importance_sampling_exponent: Importance sampling for replay.\n    priority_exponent: Priority exponent for replay.\n    prefetch_size: Prefetch size for reverb replay performance.\n    samples_per_insert: Ratio of learning samples to insert.\n    samples_per_insert_tolerance_rate: Rate to be used for\n      the SampleToInsertRatio rate limitter tolerance.\n      See a formula in make_replay_tables for more details.\n    num_sgd_steps_per_step: How many gradient updates to perform per learner\n      step.\n  \"\"\"\n  epsilon: Union[float, Sequence[float]] = 0.05\n  eval_epsilon: Optional[float] = None\n  # TODO(b/191706065): update all clients and remove this field.\n  seed: int = 1\n\n  # Learning rule\n  learning_rate: Union[float, Callable[[int], float]] = 1e-3\n  adam_eps: float = 1e-8  # Eps for Adam optimizer.\n  discount: float = 0.99  # Discount rate applied to value per timestep.\n  n_step: int = 5  # N-step TD learning.\n  target_update_period: int = 100  # Update target network every period.\n  max_gradient_norm: float = np.inf  # For gradient clipping.\n\n  # Replay options\n  batch_size: int = 256\n  min_replay_size: int = 1_000\n  max_replay_size: int = 1_000_000\n  replay_table_name: str = adders_reverb.DEFAULT_PRIORITY_TABLE\n  importance_sampling_exponent: float = 0.2\n  priority_exponent: float = 0.6\n  prefetch_size: int = 4\n  samples_per_insert: float = 0.5\n  samples_per_insert_tolerance_rate: float = 0.1\n\n  num_sgd_steps_per_step: int = 1",
  "def logspace_epsilons(\n    num_epsilons: int, epsilon: float = 0.017\n) -> Union[Sequence[float], jnp.ndarray]:\n  \"\"\"`num_epsilons` of logspace-distributed values, with median `epsilon`.\"\"\"\n  if num_epsilons <= 1:\n    return (epsilon,)\n  return jnp.logspace(1, 8, num_epsilons, base=epsilon ** (2./9.))",
  "class RainbowConfig(dqn_config.DQNConfig):\n  \"\"\"(Additional) configuration options for RainbowDQN.\"\"\"\n  max_abs_reward: float = 1.0",
  "def apply_policy_and_sample(\n    network: networks_lib.FeedForwardNetwork,) -> dqn_actor.EpsilonPolicy:\n  \"\"\"Returns a function that computes actions.\n\n  Note that this differs from default_behavior_policy with that it\n  expects c51-style network head which returns a tuple with the first entry\n  representing q-values.\n\n  Args:\n    network: A c51-style feedforward network.\n\n  Returns:\n    A feedforward policy.\n  \"\"\"\n\n  def apply_and_sample(params, key, obs, epsilon):\n    # TODO(b/161332815): Make JAX Actor work with batched or unbatched inputs.\n    obs = utils.add_batch_dim(obs)\n    action_values = network.apply(params, obs)[0]\n    action_values = utils.squeeze_batch_dim(action_values)\n    return rlax.epsilon_greedy(epsilon).sample(key, action_values)\n\n  return apply_and_sample",
  "def eval_policy(network: networks_lib.FeedForwardNetwork,\n                eval_epsilon: float) -> dqn_actor.EpsilonPolicy:\n  \"\"\"Returns a function that computes actions.\n\n  Note that this differs from default_behavior_policy with that it\n  expects c51-style network head which returns a tuple with the first entry\n  representing q-values.\n\n  Args:\n    network: A c51-style feedforward network.\n    eval_epsilon: for epsilon-greedy exploration.\n\n  Returns:\n    A feedforward policy.\n  \"\"\"\n  policy = apply_policy_and_sample(network)\n\n  def apply_and_sample(params, key, obs, _):\n    return policy(params, key, obs, eval_epsilon)\n\n  return apply_and_sample",
  "def make_builder(config: RainbowConfig):\n  \"\"\"Returns a DQNBuilder with a pre-built loss function.\"\"\"\n  loss_fn = losses.PrioritizedCategoricalDoubleQLearning(\n      discount=config.discount,\n      importance_sampling_exponent=config.importance_sampling_exponent,\n      max_abs_reward=config.max_abs_reward,\n  )\n  return builder.DQNBuilder(config, loss_fn=loss_fn)",
  "def apply_and_sample(params, key, obs, epsilon):\n    # TODO(b/161332815): Make JAX Actor work with batched or unbatched inputs.\n    obs = utils.add_batch_dim(obs)\n    action_values = network.apply(params, obs)[0]\n    action_values = utils.squeeze_batch_dim(action_values)\n    return rlax.epsilon_greedy(epsilon).sample(key, action_values)",
  "def apply_and_sample(params, key, obs, _):\n    return policy(params, key, obs, eval_epsilon)",
  "class DQNBuilder(builders.ActorLearnerBuilder[dqn_networks.DQNNetworks,\n                                              dqn_actor.DQNPolicy,\n                                              utils.PrefetchingSplit]):\n  \"\"\"DQN Builder.\"\"\"\n\n  def __init__(self,\n               config: dqn_config.DQNConfig,\n               loss_fn: learning_lib.LossFn,\n               actor_backend: Optional[str] = 'cpu'):\n    \"\"\"Creates DQN learner and the behavior policies.\n\n    Args:\n      config: DQN config.\n      loss_fn: A loss function.\n      actor_backend: Which backend to use when jitting the policy.\n    \"\"\"\n    self._config = config\n    self._loss_fn = loss_fn\n    self._actor_backend = actor_backend\n\n  def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: dqn_networks.DQNNetworks,\n      dataset: Iterator[utils.PrefetchingSplit],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    del environment_spec\n\n    return learning_lib.SGDLearner(\n        network=networks.policy_network,\n        random_key=random_key,\n        optimizer=optax.adam(\n            self._config.learning_rate, eps=self._config.adam_eps),\n        target_update_period=self._config.target_update_period,\n        data_iterator=dataset,\n        loss_fn=self._loss_fn,\n        replay_client=replay_client,\n        replay_table_name=self._config.replay_table_name,\n        counter=counter,\n        num_sgd_steps_per_step=self._config.num_sgd_steps_per_step,\n        logger=logger_fn('learner'))\n\n  def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: dqn_actor.DQNPolicy,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> core.Actor:\n    del environment_spec\n    assert variable_source is not None\n    # Inference happens on CPU, so it's better to move variables there too.\n    variable_client = variable_utils.VariableClient(\n        variable_source, '', device='cpu')\n    return actors.GenericActor(\n        actor=policy,\n        random_key=random_key,\n        variable_client=variable_client,\n        adder=adder,\n        backend=self._actor_backend)\n\n  def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: dqn_actor.DQNPolicy,\n  ) -> List[reverb.Table]:\n    \"\"\"Creates reverb tables for the algorithm.\"\"\"\n    del policy\n    samples_per_insert_tolerance = (\n        self._config.samples_per_insert_tolerance_rate *\n        self._config.samples_per_insert)\n    error_buffer = self._config.min_replay_size * samples_per_insert_tolerance\n    limiter = rate_limiters.SampleToInsertRatio(\n        min_size_to_sample=self._config.min_replay_size,\n        samples_per_insert=self._config.samples_per_insert,\n        error_buffer=error_buffer)\n    return [\n        reverb.Table(\n            name=self._config.replay_table_name,\n            sampler=reverb.selectors.Prioritized(\n                self._config.priority_exponent),\n            remover=reverb.selectors.Fifo(),\n            max_size=self._config.max_replay_size,\n            rate_limiter=limiter,\n            signature=adders_reverb.NStepTransitionAdder.signature(\n                environment_spec))\n    ]\n\n  @property\n  def batch_size_per_device(self) -> int:\n    \"\"\"Splits the batch size across local devices.\"\"\"\n\n    # Account for the number of SGD steps per step.\n    batch_size = self._config.batch_size * self._config.num_sgd_steps_per_step\n\n    num_devices = jax.local_device_count()\n    # TODO(bshahr): Using jax.device_count will not be valid when colocating\n    # learning and inference.\n\n    if batch_size % num_devices != 0:\n      raise ValueError(\n          'The DQN learner received a batch size that is not divisible by the '\n          f'number of available learner devices. Got: batch_size={batch_size}, '\n          f'num_devices={num_devices}.')\n\n    return batch_size // num_devices\n\n  def make_dataset_iterator(\n      self,\n      replay_client: reverb.Client,\n  ) -> Iterator[utils.PrefetchingSplit]:\n    \"\"\"Creates a dataset iterator to use for learning.\"\"\"\n    dataset = datasets.make_reverb_dataset(\n        table=self._config.replay_table_name,\n        server_address=replay_client.server_address,\n        batch_size=self.batch_size_per_device,\n        prefetch_size=self._config.prefetch_size)\n\n    return utils.multi_device_put(\n        dataset.as_numpy_iterator(),\n        jax.local_devices(),\n        split_fn=utils.keep_key_on_host)\n\n  def make_adder(\n      self,\n      replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[dqn_actor.DQNPolicy],\n  ) -> Optional[adders.Adder]:\n    \"\"\"Creates an adder which handles observations.\"\"\"\n    del environment_spec, policy\n    return adders_reverb.NStepTransitionAdder(\n        priority_fns={self._config.replay_table_name: None},\n        client=replay_client,\n        n_step=self._config.n_step,\n        discount=self._config.discount)\n\n  def _policy_epsilons(self, evaluation: bool) -> Sequence[float]:\n    if evaluation and self._config.eval_epsilon:\n      epsilon = self._config.eval_epsilon\n    else:\n      epsilon = self._config.epsilon\n    epsilons = epsilon if isinstance(epsilon, Sequence) else (epsilon,)\n    return epsilons\n\n  def make_policy(self,\n                  networks: dqn_networks.DQNNetworks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> dqn_actor.DQNPolicy:\n    \"\"\"Creates the policy.\"\"\"\n    del environment_spec\n\n    return dqn_actor.alternating_epsilons_actor_core(\n        dqn_actor.behavior_policy(networks),\n        epsilons=self._policy_epsilons(evaluation))",
  "class DistributionalDQNBuilder(DQNBuilder):\n  \"\"\"Distributional DQN Builder.\"\"\"\n\n  def make_policy(self,\n                  networks: dqn_networks.DQNNetworks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> dqn_actor.DQNPolicy:\n    \"\"\"Creates the policy.\n\n      Expects network head which returns a tuple with the first entry\n      representing q-values.\n      Creates the agent policy given the collection of network components and\n      environment spec. An optional boolean can be given to indicate if the\n      policy will be used for evaluation.\n\n    Args:\n      networks: struct describing the networks needed to generate the policy.\n      environment_spec: struct describing the specs of the environment.\n      evaluation: when true, a version of the policy to use for evaluation\n        should be returned. This is algorithm-specific so if an algorithm makes\n        no distinction between behavior and evaluation policies this boolean may\n        be ignored.\n\n    Returns:\n      Behavior policy or evaluation policy for the agent.\n    \"\"\"\n    del environment_spec\n\n    def get_action_values(params: networks_lib.Params,\n                          observation: networks_lib.Observation, *args,\n                          **kwargs) -> networks_lib.NetworkOutput:\n      return networks.policy_network.apply(params, observation, *args,\n                                           **kwargs)[0]\n\n    typed_network = networks_lib.TypedFeedForwardNetwork(\n        init=networks.policy_network.init, apply=get_action_values)\n    behavior_policy = dqn_actor.behavior_policy(\n        dqn_networks.DQNNetworks(policy_network=typed_network))\n\n    return dqn_actor.alternating_epsilons_actor_core(\n        behavior_policy, epsilons=self._policy_epsilons(evaluation))",
  "def __init__(self,\n               config: dqn_config.DQNConfig,\n               loss_fn: learning_lib.LossFn,\n               actor_backend: Optional[str] = 'cpu'):\n    \"\"\"Creates DQN learner and the behavior policies.\n\n    Args:\n      config: DQN config.\n      loss_fn: A loss function.\n      actor_backend: Which backend to use when jitting the policy.\n    \"\"\"\n    self._config = config\n    self._loss_fn = loss_fn\n    self._actor_backend = actor_backend",
  "def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: dqn_networks.DQNNetworks,\n      dataset: Iterator[utils.PrefetchingSplit],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    del environment_spec\n\n    return learning_lib.SGDLearner(\n        network=networks.policy_network,\n        random_key=random_key,\n        optimizer=optax.adam(\n            self._config.learning_rate, eps=self._config.adam_eps),\n        target_update_period=self._config.target_update_period,\n        data_iterator=dataset,\n        loss_fn=self._loss_fn,\n        replay_client=replay_client,\n        replay_table_name=self._config.replay_table_name,\n        counter=counter,\n        num_sgd_steps_per_step=self._config.num_sgd_steps_per_step,\n        logger=logger_fn('learner'))",
  "def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: dqn_actor.DQNPolicy,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> core.Actor:\n    del environment_spec\n    assert variable_source is not None\n    # Inference happens on CPU, so it's better to move variables there too.\n    variable_client = variable_utils.VariableClient(\n        variable_source, '', device='cpu')\n    return actors.GenericActor(\n        actor=policy,\n        random_key=random_key,\n        variable_client=variable_client,\n        adder=adder,\n        backend=self._actor_backend)",
  "def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: dqn_actor.DQNPolicy,\n  ) -> List[reverb.Table]:\n    \"\"\"Creates reverb tables for the algorithm.\"\"\"\n    del policy\n    samples_per_insert_tolerance = (\n        self._config.samples_per_insert_tolerance_rate *\n        self._config.samples_per_insert)\n    error_buffer = self._config.min_replay_size * samples_per_insert_tolerance\n    limiter = rate_limiters.SampleToInsertRatio(\n        min_size_to_sample=self._config.min_replay_size,\n        samples_per_insert=self._config.samples_per_insert,\n        error_buffer=error_buffer)\n    return [\n        reverb.Table(\n            name=self._config.replay_table_name,\n            sampler=reverb.selectors.Prioritized(\n                self._config.priority_exponent),\n            remover=reverb.selectors.Fifo(),\n            max_size=self._config.max_replay_size,\n            rate_limiter=limiter,\n            signature=adders_reverb.NStepTransitionAdder.signature(\n                environment_spec))\n    ]",
  "def batch_size_per_device(self) -> int:\n    \"\"\"Splits the batch size across local devices.\"\"\"\n\n    # Account for the number of SGD steps per step.\n    batch_size = self._config.batch_size * self._config.num_sgd_steps_per_step\n\n    num_devices = jax.local_device_count()\n    # TODO(bshahr): Using jax.device_count will not be valid when colocating\n    # learning and inference.\n\n    if batch_size % num_devices != 0:\n      raise ValueError(\n          'The DQN learner received a batch size that is not divisible by the '\n          f'number of available learner devices. Got: batch_size={batch_size}, '\n          f'num_devices={num_devices}.')\n\n    return batch_size // num_devices",
  "def make_dataset_iterator(\n      self,\n      replay_client: reverb.Client,\n  ) -> Iterator[utils.PrefetchingSplit]:\n    \"\"\"Creates a dataset iterator to use for learning.\"\"\"\n    dataset = datasets.make_reverb_dataset(\n        table=self._config.replay_table_name,\n        server_address=replay_client.server_address,\n        batch_size=self.batch_size_per_device,\n        prefetch_size=self._config.prefetch_size)\n\n    return utils.multi_device_put(\n        dataset.as_numpy_iterator(),\n        jax.local_devices(),\n        split_fn=utils.keep_key_on_host)",
  "def make_adder(\n      self,\n      replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[dqn_actor.DQNPolicy],\n  ) -> Optional[adders.Adder]:\n    \"\"\"Creates an adder which handles observations.\"\"\"\n    del environment_spec, policy\n    return adders_reverb.NStepTransitionAdder(\n        priority_fns={self._config.replay_table_name: None},\n        client=replay_client,\n        n_step=self._config.n_step,\n        discount=self._config.discount)",
  "def _policy_epsilons(self, evaluation: bool) -> Sequence[float]:\n    if evaluation and self._config.eval_epsilon:\n      epsilon = self._config.eval_epsilon\n    else:\n      epsilon = self._config.epsilon\n    epsilons = epsilon if isinstance(epsilon, Sequence) else (epsilon,)\n    return epsilons",
  "def make_policy(self,\n                  networks: dqn_networks.DQNNetworks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> dqn_actor.DQNPolicy:\n    \"\"\"Creates the policy.\"\"\"\n    del environment_spec\n\n    return dqn_actor.alternating_epsilons_actor_core(\n        dqn_actor.behavior_policy(networks),\n        epsilons=self._policy_epsilons(evaluation))",
  "def make_policy(self,\n                  networks: dqn_networks.DQNNetworks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> dqn_actor.DQNPolicy:\n    \"\"\"Creates the policy.\n\n      Expects network head which returns a tuple with the first entry\n      representing q-values.\n      Creates the agent policy given the collection of network components and\n      environment spec. An optional boolean can be given to indicate if the\n      policy will be used for evaluation.\n\n    Args:\n      networks: struct describing the networks needed to generate the policy.\n      environment_spec: struct describing the specs of the environment.\n      evaluation: when true, a version of the policy to use for evaluation\n        should be returned. This is algorithm-specific so if an algorithm makes\n        no distinction between behavior and evaluation policies this boolean may\n        be ignored.\n\n    Returns:\n      Behavior policy or evaluation policy for the agent.\n    \"\"\"\n    del environment_spec\n\n    def get_action_values(params: networks_lib.Params,\n                          observation: networks_lib.Observation, *args,\n                          **kwargs) -> networks_lib.NetworkOutput:\n      return networks.policy_network.apply(params, observation, *args,\n                                           **kwargs)[0]\n\n    typed_network = networks_lib.TypedFeedForwardNetwork(\n        init=networks.policy_network.init, apply=get_action_values)\n    behavior_policy = dqn_actor.behavior_policy(\n        dqn_networks.DQNNetworks(policy_network=typed_network))\n\n    return dqn_actor.alternating_epsilons_actor_core(\n        behavior_policy, epsilons=self._policy_epsilons(evaluation))",
  "def get_action_values(params: networks_lib.Params,\n                          observation: networks_lib.Observation, *args,\n                          **kwargs) -> networks_lib.NetworkOutput:\n      return networks.policy_network.apply(params, observation, *args,\n                                           **kwargs)[0]",
  "class EpsilonActorState:\n  rng: networks_lib.PRNGKey\n  epsilon: jnp.ndarray",
  "def alternating_epsilons_actor_core(policy_network: EpsilonPolicy,\n                                    epsilons: Sequence[float]) -> DQNPolicy:\n  \"\"\"Returns actor components for alternating epsilon exploration.\n\n  Args:\n    policy_network: A feedforward action selecting function.\n    epsilons: epsilons to alternate per-episode for epsilon-greedy exploration.\n\n  Returns:\n    A feedforward policy.\n  \"\"\"\n  epsilons = jnp.array(epsilons)\n\n  def apply_and_sample(params: networks_lib.Params,\n                       observation: networks_lib.Observation,\n                       state: EpsilonActorState):\n    random_key, key = jax.random.split(state.rng)\n    actions = policy_network(params, key, observation, state.epsilon)  # pytype: disable=wrong-arg-types  # jax-ndarray\n    return (actions.astype(jnp.int32),\n            EpsilonActorState(rng=random_key, epsilon=state.epsilon))\n\n  def policy_init(random_key: networks_lib.PRNGKey):\n    random_key, key = jax.random.split(random_key)\n    epsilon = jax.random.choice(key, epsilons)\n    return EpsilonActorState(rng=random_key, epsilon=epsilon)\n\n  return actor_core_lib.ActorCore(\n      init=policy_init, select_action=apply_and_sample,\n      get_extras=lambda _: None)",
  "def behavior_policy(networks: dqn_networks.DQNNetworks) -> EpsilonPolicy:\n  \"\"\"A policy with parameterized epsilon-greedy exploration.\"\"\"\n\n  def apply_and_sample(params: networks_lib.Params, key: networks_lib.PRNGKey,\n                       observation: networks_lib.Observation, epsilon: Epsilon\n                       ) -> networks_lib.Action:\n    # TODO(b/161332815): Make JAX Actor work with batched or unbatched inputs.\n    observation = utils.add_batch_dim(observation)\n    action_values = networks.policy_network.apply(\n        params, observation, is_training=False)\n    action_values = utils.squeeze_batch_dim(action_values)\n    return networks.sample_fn(action_values, key, epsilon)\n\n  return apply_and_sample",
  "def default_behavior_policy(networks: dqn_networks.DQNNetworks,\n                            epsilon: Epsilon) -> EpsilonPolicy:\n  \"\"\"A policy with a fixed-epsilon epsilon-greedy exploration.\n\n  DEPRECATED: use behavior_policy instead.\n  Args:\n    networks: DQN networks\n    epsilon: sampling parameter that overrides the one in EpsilonPolicy\n  Returns:\n    epsilon-greedy behavior policy with fixed epsilon\n  \"\"\"\n  # TODO(lukstafi): remove this function and migrate its users.\n\n  def apply_and_sample(params: networks_lib.Params, key: networks_lib.PRNGKey,\n                       observation: networks_lib.Observation, _: Epsilon\n                       ) -> networks_lib.Action:\n    # TODO(b/161332815): Make JAX Actor work with batched or unbatched inputs.\n    observation = utils.add_batch_dim(observation)\n    action_values = networks.policy_network.apply(\n        params, observation, is_training=False)\n    action_values = utils.squeeze_batch_dim(action_values)\n    return networks.sample_fn(action_values, key, epsilon)\n\n  return apply_and_sample",
  "def apply_and_sample(params: networks_lib.Params,\n                       observation: networks_lib.Observation,\n                       state: EpsilonActorState):\n    random_key, key = jax.random.split(state.rng)\n    actions = policy_network(params, key, observation, state.epsilon)  # pytype: disable=wrong-arg-types  # jax-ndarray\n    return (actions.astype(jnp.int32),\n            EpsilonActorState(rng=random_key, epsilon=state.epsilon))",
  "def policy_init(random_key: networks_lib.PRNGKey):\n    random_key, key = jax.random.split(random_key)\n    epsilon = jax.random.choice(key, epsilons)\n    return EpsilonActorState(rng=random_key, epsilon=epsilon)",
  "def apply_and_sample(params: networks_lib.Params, key: networks_lib.PRNGKey,\n                       observation: networks_lib.Observation, epsilon: Epsilon\n                       ) -> networks_lib.Action:\n    # TODO(b/161332815): Make JAX Actor work with batched or unbatched inputs.\n    observation = utils.add_batch_dim(observation)\n    action_values = networks.policy_network.apply(\n        params, observation, is_training=False)\n    action_values = utils.squeeze_batch_dim(action_values)\n    return networks.sample_fn(action_values, key, epsilon)",
  "def apply_and_sample(params: networks_lib.Params, key: networks_lib.PRNGKey,\n                       observation: networks_lib.Observation, _: Epsilon\n                       ) -> networks_lib.Action:\n    # TODO(b/161332815): Make JAX Actor work with batched or unbatched inputs.\n    observation = utils.add_batch_dim(observation)\n    action_values = networks.policy_network.apply(\n        params, observation, is_training=False)\n    action_values = utils.squeeze_batch_dim(action_values)\n    return networks.sample_fn(action_values, key, epsilon)",
  "class TrainingState(NamedTuple):\n  \"\"\"Contains training state for the learner.\"\"\"\n  params: mpo_networks.MPONetworkParams\n  target_params: mpo_networks.MPONetworkParams\n  dual_params: mpo_types.DualParams\n  opt_state: optax.OptState\n  dual_opt_state: optax.OptState\n  steps: int\n  random_key: jax_types.PRNGKey",
  "def softmax_cross_entropy(\n    logits: chex.Array, target_probs: chex.Array) -> chex.Array:\n  \"\"\"Compute cross entropy loss between logits and target probabilities.\"\"\"\n  chex.assert_equal_shape([target_probs, logits])\n  return -jnp.sum(target_probs * jax.nn.log_softmax(logits), axis=-1)",
  "def top1_accuracy_tiebreak(logits: chex.Array,\n                           targets: chex.Array,\n                           *,\n                           rng: jax_types.PRNGKey,\n                           eps: float = 1e-6) -> chex.Array:\n  \"\"\"Compute the top-1 accuracy with an argmax of targets (random tie-break).\"\"\"\n  noise = jax.random.uniform(rng, shape=targets.shape,\n                             minval=-eps, maxval=eps)\n  acc = jnp.argmax(logits, axis=-1) == jnp.argmax(targets + noise, axis=-1)\n  return jnp.mean(acc)",
  "class MPOLearner(acme.Learner):\n  \"\"\"MPO learner (discrete or continuous, distributional or not).\"\"\"\n\n  _state: TrainingState\n\n  def __init__(  # pytype: disable=annotation-type-mismatch  # numpy-scalars\n      self,\n      critic_type: CriticType,\n      discrete_policy: bool,\n      environment_spec: specs.EnvironmentSpec,\n      networks: mpo_networks.MPONetworks,\n      random_key: jax_types.PRNGKey,\n      discount: float,\n      num_samples: int,\n      iterator: Iterator[reverb.ReplaySample],\n      experience_type: mpo_types.ExperienceType,\n      loss_scales: mpo_types.LossScalesConfig,\n      target_update_period: Optional[int] = 100,\n      target_update_rate: Optional[float] = None,\n      sgd_steps_per_learner_step: int = 20,\n      policy_eval_stochastic: bool = True,\n      policy_eval_num_val_samples: int = 128,\n      policy_loss_config: Optional[mpo_types.PolicyLossConfig] = None,\n      use_online_policy_to_bootstrap: bool = False,\n      use_stale_state: bool = False,\n      use_retrace: bool = False,\n      retrace_lambda: float = 0.95,\n      model_rollout_length: int = 0,\n      optimizer: Optional[optax.GradientTransformation] = None,\n      learning_rate: optax.ScalarOrSchedule = 1e-4,\n      dual_optimizer: Optional[optax.GradientTransformation] = None,\n      dual_learning_rate: optax.ScalarOrSchedule = 1e-2,\n      grad_norm_clip: float = 40.0,\n      reward_clip: float = np.float32('inf'),\n      value_tx_pair: rlax.TxPair = rlax.IDENTITY_PAIR,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      devices: Optional[Sequence[jax.Device]] = None,\n  ):\n    self._critic_type = critic_type\n    self._discrete_policy = discrete_policy\n\n    process_id = jax.process_index()\n    local_devices = jax.local_devices()\n    self._devices = devices or local_devices\n    logging.info('Learner process id: %s. Devices passed: %s', process_id,\n                 devices)\n    logging.info('Learner process id: %s. Local devices from JAX API: %s',\n                 process_id, local_devices)\n    self._local_devices = [d for d in self._devices if d in local_devices]\n\n    # Store networks.\n    self._networks = networks\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger\n\n    # Other learner parameters.\n    self._discount = discount\n    self._num_samples = num_samples\n    self._sgd_steps_per_learner_step = sgd_steps_per_learner_step\n\n    self._policy_eval_stochastic = policy_eval_stochastic\n    self._policy_eval_num_val_samples = policy_eval_num_val_samples\n\n    self._reward_clip_range = sorted([-reward_clip, reward_clip])\n    self._tx_pair = value_tx_pair\n    self._loss_scales = loss_scales\n    self._use_online_policy_to_bootstrap = use_online_policy_to_bootstrap\n    self._model_rollout_length = model_rollout_length\n\n    self._use_retrace = use_retrace\n    self._retrace_lambda = retrace_lambda\n    if use_retrace and critic_type == CriticType.MIXTURE_OF_GAUSSIANS:\n      logging.warning(\n          'Warning! Retrace has not been tested with the MoG critic.')\n    self._use_stale_state = use_stale_state\n\n    self._experience_type = experience_type\n    if isinstance(self._experience_type, mpo_types.FromTransitions):\n      # Each n=5-step transition will be converted to a length 2 sequence before\n      # being passed to the loss, so we do n=1 step bootstrapping on the\n      # resulting sequence to get n=5-step bootstrapping as intended.\n      self._n_step_for_sequence_bootstrap = 1\n      self._td_lambda = 1.0\n    elif isinstance(self._experience_type, mpo_types.FromSequences):\n      self._n_step_for_sequence_bootstrap = self._experience_type.n_step\n      self._td_lambda = self._experience_type.td_lambda\n\n    # Necessary to track when to update target networks.\n    self._target_update_period = target_update_period\n    self._target_update_rate = target_update_rate\n    # Assert one and only one of target update period or rate is defined.\n    if ((target_update_period and target_update_rate) or\n        (target_update_period is None and target_update_rate is None)):\n      raise ValueError(\n          'Exactly one of target_update_{period|rate} must be set.'\n          f' Received target_update_period={target_update_period} and'\n          f' target_update_rate={target_update_rate}.')\n\n    # Create policy loss.\n    if self._discrete_policy:\n      policy_loss_config = (\n          policy_loss_config or mpo_types.CategoricalPolicyLossConfig())\n      self._policy_loss_module = discrete_losses.CategoricalMPO(\n          **dataclasses.asdict(policy_loss_config))\n    else:\n      policy_loss_config = (\n          policy_loss_config or mpo_types.GaussianPolicyLossConfig())\n      self._policy_loss_module = continuous_losses.MPO(\n          **dataclasses.asdict(policy_loss_config))\n\n    self._policy_loss_module.__call__ = jax.named_call(\n        self._policy_loss_module.__call__, name='policy_loss')\n\n    # Create the dynamics model rollout loss.\n    if model_rollout_length > 0:\n      if not discrete_policy and (self._loss_scales.rollout.policy or\n                                  self._loss_scales.rollout.bc_policy):\n        raise ValueError('Policy rollout losses are only supported in the '\n                         'discrete policy case.')\n      self._model_rollout_loss_fn = rollout_loss.RolloutLoss(\n          dynamics_model=networks.dynamics_model,\n          model_rollout_length=model_rollout_length,\n          loss_scales=loss_scales,\n          distributional_loss_fn=self._distributional_loss)\n\n    # Create optimizers if they aren't given.\n    self._optimizer = optimizer or _get_default_optimizer(\n        learning_rate, grad_norm_clip\n    )\n    self._dual_optimizer = dual_optimizer or _get_default_optimizer(\n        dual_learning_rate, grad_norm_clip\n    )\n\n    self._action_spec = environment_spec.actions\n\n    # Initialize random key for the rest of training.\n    random_key, key = jax.random.split(random_key)\n\n    # Initialize network parameters, ignoring the dummy initial state.\n    network_params, _ = mpo_networks.init_params(\n        self._networks,\n        environment_spec,\n        key,\n        add_batch_dim=True,\n        dynamics_rollout_length=self._model_rollout_length)\n\n    # Get action dims (unused in the discrete case).\n    dummy_action = utils.zeros_like(environment_spec.actions)\n    dummy_action_concat = utils.batch_concat(dummy_action, num_batch_dims=0)\n\n    if isinstance(self._policy_loss_module, discrete_losses.CategoricalMPO):\n      self._dual_clip_fn = discrete_losses.clip_categorical_mpo_params\n    elif isinstance(self._policy_loss_module, continuous_losses.MPO):\n      is_constraining = self._policy_loss_module.per_dim_constraining\n      self._dual_clip_fn = lambda dp: continuous_losses.clip_mpo_params(  # pylint: disable=g-long-lambda  # pytype: disable=wrong-arg-types  # numpy-scalars\n          dp,\n          per_dim_constraining=is_constraining)\n\n    # Create dual parameters. In the discrete case, the action dim is unused.\n    dual_params = self._policy_loss_module.init_params(\n        action_dim=dummy_action_concat.shape[-1], dtype=jnp.float32)\n\n    # Initialize optimizers.\n    opt_state = self._optimizer.init(network_params)\n    dual_opt_state = self._dual_optimizer.init(dual_params)\n\n    # Initialise training state (parameters and optimiser state).\n    state = TrainingState(\n        params=network_params,\n        target_params=network_params,\n        dual_params=dual_params,\n        opt_state=opt_state,\n        dual_opt_state=dual_opt_state,\n        steps=0,\n        random_key=random_key,\n    )\n    self._state = utils.replicate_in_all_devices(state, self._local_devices)\n\n    # Log how many parameters the network has.\n    sizes = tree.map_structure(jnp.size, network_params)._asdict()\n    num_params_by_component_str = ' | '.join(\n        [f'{key}: {sum(tree.flatten(size))}' for key, size in sizes.items()])\n    logging.info('Number of params by network component: %s',\n                 num_params_by_component_str)\n    logging.info('Total number of params: %d',\n                 sum(tree.flatten(sizes.values())))\n\n    # Combine multiple SGD steps and pmap across devices.\n    sgd_steps = utils.process_multiple_batches(self._sgd_step,\n                                               self._sgd_steps_per_learner_step)\n    self._sgd_steps = jax.pmap(\n        sgd_steps, axis_name=_PMAP_AXIS_NAME, devices=self._devices)\n\n    self._iterator = iterator\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None\n    self._current_step = 0\n\n  def _distributional_loss(self, prediction: mpo_types.DistributionLike,\n                           target: chex.Array):\n    \"\"\"Compute the critic loss given the prediction and target.\"\"\"\n    # TODO(abef): break this function into separate functions for each critic.\n    chex.assert_rank(target, 3)  # [N, Z, T] except for Categorical is [1, T, L]\n    if self._critic_type == CriticType.MIXTURE_OF_GAUSSIANS:\n      # Sample-based cross-entropy loss.\n      loss = -prediction.log_prob(target[..., jnp.newaxis])\n      loss = jnp.mean(loss, axis=[0, 1])  # [T]\n    elif self._critic_type == CriticType.NONDISTRIBUTIONAL:\n      # TD error.\n      prediction = prediction.squeeze(axis=-1)  # [T]\n      loss = 0.5 * jnp.square(target - prediction)\n      chex.assert_equal_shape([target, loss])  # Check broadcasting.\n    elif self._critic_type == mpo_types.CriticType.CATEGORICAL_2HOT:\n      # Cross-entropy loss (two-hot categorical).\n      target = jnp.mean(target, axis=(0, 1))  # [N, Z, T] -> [T]\n      # TODO(abef): Compute target differently? (e.g., do mean cross ent.).\n      target_probs = rlax.transform_to_2hot(  # [T, L]\n          target,\n          min_value=prediction.values.min(),\n          max_value=prediction.values.max(),\n          num_bins=prediction.logits.shape[-1])\n      logits = jnp.squeeze(prediction.logits, axis=1)  # [T, L]\n      chex.assert_equal_shape([target_probs, logits])\n      loss = jax.vmap(rlax.categorical_cross_entropy)(target_probs, logits)\n    elif self._critic_type == mpo_types.CriticType.CATEGORICAL:\n      loss = jax.vmap(rlax.categorical_cross_entropy)(jnp.squeeze(\n          target, axis=0), jnp.squeeze(prediction.logits, axis=1))\n    return jnp.mean(loss)  # [T] -> []\n\n  def _compute_predictions(self, params: mpo_networks.MPONetworkParams,\n                           sequence: adders.Step) -> mpo_types.ModelOutputs:\n    \"\"\"Compute model predictions at observed and rolled out states.\"\"\"\n\n    # Initialize the core states, possibly to the recorded stale state.\n    if self._use_stale_state:\n      initial_state = utils.maybe_recover_lstm_type(\n          sequence.extras['core_state'])\n      initial_state = tree.map_structure(lambda x: x[0], initial_state)\n    else:\n      initial_state = self._networks.torso.initial_state_fn(\n          params.torso_initial_state, None)\n\n    # Unroll the online core network. Note that this may pass the embeddings\n    # unchanged if, say, the core is an hk.IdentityCore.\n    state_embedding, _ = self._networks.torso_unroll(   # [T, ...]\n        params, sequence.observation, initial_state)\n\n    # Compute the root policy and critic outputs; [T, ...] and [T-1, ...].\n    policy = self._networks.policy_head_apply(params, state_embedding)\n    q_value = self._networks.critic_head_apply(\n        params, state_embedding[:-1], sequence.action[:-1])\n\n    return mpo_types.ModelOutputs(\n        policy=policy,  # [T, ...]\n        q_value=q_value,  # [T-1, ...]\n        reward=None,\n        embedding=state_embedding)  # [T, ...]\n\n  def _compute_targets(\n      self,\n      target_params: mpo_networks.MPONetworkParams,\n      dual_params: mpo_types.DualParams,\n      sequence: adders.Step,\n      online_policy: types.NestedArray,  # TODO(abef): remove this.\n      key: jax_types.PRNGKey) -> mpo_types.LossTargets:\n    \"\"\"Compute the targets needed to train the agent.\"\"\"\n\n    # Initialize the core states, possibly to the recorded stale state.\n    if self._use_stale_state:\n      initial_state = utils.maybe_recover_lstm_type(\n          sequence.extras['core_state'])\n      initial_state = tree.map_structure(lambda x: x[0], initial_state)\n    else:\n      initial_state = self._networks.torso.initial_state_fn(\n          target_params.torso_initial_state, None)\n\n    # Unroll the target core network. Note that this may pass the embeddings\n    # unchanged if, say, the core is an hk.IdentityCore.\n    target_state_embedding, _ = self._networks.torso_unroll(\n        target_params, sequence.observation, initial_state)  # [T, ...]\n\n    # Compute the action distribution from target policy network.\n    target_policy = self._networks.policy_head_apply(\n        target_params, target_state_embedding)  # [T, ...]\n\n    # Maybe reward clip.\n    clipped_reward = jnp.clip(sequence.reward, *self._reward_clip_range)  # [T]\n    # TODO(abef): when to clip rewards, if at all, if learning dynamics model?\n\n    @jax.named_call\n    @jax.vmap\n    def critic_mean_fn(action_: jnp.ndarray) -> jnp.ndarray:\n      \"\"\"Compute mean of target critic distribution.\"\"\"\n      critic_output = self._networks.critic_head_apply(\n          target_params, target_state_embedding, action_)\n      if self._critic_type != CriticType.NONDISTRIBUTIONAL:\n        critic_output = critic_output.mean()\n      return critic_output\n\n    @jax.named_call\n    @jax.vmap\n    def critic_sample_fn(action_: jnp.ndarray,\n                         seed_: jnp.ndarray) -> jnp.ndarray:\n      \"\"\"Sample from the target critic distribution.\"\"\"\n      z_distribution = self._networks.critic_head_apply(\n          target_params, target_state_embedding, action_)\n      z_samples = z_distribution.sample(\n          self._policy_eval_num_val_samples, seed=seed_)\n      return z_samples  # [Z, T, 1]\n\n    if self._discrete_policy:\n      # Use all actions to improve policy (no sampling); N = num_actions.\n      a_improvement = jnp.arange(self._action_spec.num_values)  # [N]\n      seq_len = target_state_embedding.shape[0]  # T\n      a_improvement = jnp.tile(a_improvement[..., None], [1, seq_len])  # [N, T]\n    else:\n      # Sample actions to improve policy; [N=num_samples, T].\n      a_improvement = target_policy.sample(self._num_samples, seed=key)\n\n    # TODO(abef): use model to get q_improvement = r + gamma*V?\n\n    # Compute the mean Q-values used in policy improvement; [N, T].\n    q_improvement = critic_mean_fn(a_improvement).squeeze(axis=-1)\n\n    # Policy to use for policy evaluation and bootstrapping.\n    if self._use_online_policy_to_bootstrap:\n      policy_to_evaluate = online_policy\n      chex.assert_equal(online_policy.batch_shape, target_policy.batch_shape)\n    else:\n      policy_to_evaluate = target_policy\n\n    # Action(s) to use for policy evaluation; shape [N, T].\n    if self._policy_eval_stochastic:\n      a_evaluation = policy_to_evaluate.sample(self._num_samples, seed=key)\n    else:\n      a_evaluation = policy_to_evaluate.mode()\n      a_evaluation = jnp.expand_dims(a_evaluation, axis=0)  # [N=1, T]\n\n    # TODO(abef): policy_eval_stochastic=False makes our targets more \"greedy\"\n\n    # Add a stopgrad in case we use the online policy for evaluation.\n    a_evaluation = jax.lax.stop_gradient(a_evaluation)\n\n    if self._critic_type == CriticType.MIXTURE_OF_GAUSSIANS:\n      # Produce Z return samples for every N action sample; [N, Z, T, 1].\n      seeds = jax.random.split(key, num=a_evaluation.shape[0])\n      z_samples = critic_sample_fn(a_evaluation, seeds)\n    else:\n      normalized_weights = 1. / a_evaluation.shape[0]\n      z_samples = critic_mean_fn(a_evaluation)  # [N, T, 1]\n\n      # When policy_eval_stochastic == True, this corresponds to expected SARSA.\n      # Otherwise, normalized_weights = 1.0 and N = 1 so the sum is a no-op.\n      z_samples = jnp.sum(normalized_weights * z_samples, axis=0, keepdims=True)\n      z_samples = jnp.expand_dims(z_samples, axis=1)  # [N, Z=1, T, 1]\n\n    # Slice to t = 1...T and transform into raw reward space; [N, Z, T].\n    z_samples_itx = self._tx_pair.apply_inv(z_samples.squeeze(axis=-1))\n\n    # Compute the value estimate by averaging the sampled returns in the raw\n    # reward space; shape [N=1, Z=1, T].\n    value_target_itx = jnp.mean(z_samples_itx, axis=(0, 1), keepdims=True)\n\n    if self._use_retrace:\n      # Warning! Retrace has not been tested with the MoG critic.\n      log_rhos = (\n          target_policy.log_prob(sequence.action) - sequence.extras['log_prob'])\n\n      # Compute Q-values; expand and squeeze because critic_mean_fn is vmapped.\n      q_t = critic_mean_fn(jnp.expand_dims(sequence.action, axis=0)).squeeze(0)\n      q_t = q_t.squeeze(-1)  # Also squeeze trailing scalar dimension; [T].\n\n      # Compute retrace targets.\n      # These targets use the rewards and discounts as in normal TD-learning but\n      # they use a mix of bootstrapped values V(s') and Q(s', a'), weighing the\n      # latter based on how likely a' is under the current policy (s' and a' are\n      # samples from replay).\n      # See [Munos et al., 2016](https://arxiv.org/abs/1606.02647) for more.\n      q_value_target_itx = rlax.general_off_policy_returns_from_q_and_v(\n          q_t=self._tx_pair.apply_inv(q_t[1:-1]),\n          v_t=jnp.squeeze(value_target_itx, axis=(0, 1))[1:],\n          r_t=clipped_reward[:-1],\n          discount_t=self._discount * sequence.discount[:-1],\n          c_t=self._retrace_lambda * jnp.minimum(1.0, jnp.exp(log_rhos[1:-1])))\n\n      # Expand dims to the expected [N=1, Z=1, T-1].\n      q_value_target_itx = jnp.expand_dims(q_value_target_itx, axis=(0, 1))\n    else:\n      # Compute bootstrap target from sequences. vmap return computation across\n      # N action and Z return samples; shape [N, Z, T-1].\n      n_step_return_fn = functools.partial(\n          rlax.n_step_bootstrapped_returns,\n          r_t=clipped_reward[:-1],\n          discount_t=self._discount * sequence.discount[:-1],\n          n=self._n_step_for_sequence_bootstrap,\n          lambda_t=self._td_lambda)\n      n_step_return_vfn = jax.vmap(jax.vmap(n_step_return_fn))\n      q_value_target_itx = n_step_return_vfn(v_t=z_samples_itx[..., 1:])\n\n    # Transform back to the canonical space and stop gradients.\n    q_value_target = jax.lax.stop_gradient(\n        self._tx_pair.apply(q_value_target_itx))\n    reward_target = jax.lax.stop_gradient(self._tx_pair.apply(clipped_reward))\n    value_target = jax.lax.stop_gradient(self._tx_pair.apply(value_target_itx))\n\n    if self._critic_type == mpo_types.CriticType.CATEGORICAL:\n\n      @jax.vmap\n      def get_logits_and_values(\n          action: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:\n        critic_output = self._networks.critic_head_apply(\n            target_params, target_state_embedding[1:], action)\n        return critic_output.logits, critic_output.values\n\n      z_t_logits, z_t_values = get_logits_and_values(a_evaluation[:, 1:])\n      z_t_logits = jnp.squeeze(z_t_logits, axis=2)  # [N, T-1, L]\n      z_t_values = z_t_values[0]  # Values are identical at each N; [L].\n\n      gamma = self._discount * sequence.discount[:-1, None]  # [T-1, 1]\n      r_t = clipped_reward[:-1, None]  # [T-1, 1]\n      atoms_itx = self._tx_pair.apply_inv(z_t_values)[None, ...]  # [1, L]\n      z_target_atoms = self._tx_pair.apply(r_t + gamma * atoms_itx)  # [T-1, L]\n      # Note: this is n=1-step TD unless using experience=FromTransitions(n>1).\n      z_target_probs = jax.nn.softmax(z_t_logits)  # [N, T-1, L]\n      z_target_atoms = jax.lax.broadcast(\n          z_target_atoms, z_target_probs.shape[:1])  # [N, T-1, L]\n      project_fn = functools.partial(\n          rlax.categorical_l2_project, z_q=z_t_values)\n      z_target = jax.vmap(jax.vmap(project_fn))(z_target_atoms, z_target_probs)\n      z_target = jnp.mean(z_target, axis=0)\n      q_value_target = jax.lax.stop_gradient(z_target[None, ...])  # [1, T-1, L]\n      # TODO(abef): make q_v_target shape align with expected [N, Z, T-1] shape?\n\n    targets = mpo_types.LossTargets(\n        policy=target_policy,  # [T, ...]\n        a_improvement=a_improvement,  # [N, T]\n        q_improvement=q_improvement,  # [N, T]\n        q_value=q_value_target,  # [N, Z, T-1] ([1, T-1, L] for CATEGORICAL)\n        value=value_target[..., :-1],  # [N=1, Z=1, T-1]\n        reward=reward_target,  # [T]\n        embedding=target_state_embedding)  # [T, ...]\n\n    return targets\n\n  def _loss_fn(\n      self,\n      params: mpo_networks.MPONetworkParams,\n      dual_params: mpo_types.DualParams,\n      # TODO(bshahr): clean up types: Step is not a great type for sequences.\n      sequence: adders.Step,\n      target_params: mpo_networks.MPONetworkParams,\n      key: jax_types.PRNGKey) -> Tuple[jnp.ndarray, mpo_types.LogDict]:\n    # Compute the model predictions at the root and for the rollouts.\n    predictions = self._compute_predictions(params=params, sequence=sequence)\n\n    # Compute the targets to use for the losses.\n    targets = self._compute_targets(\n        target_params=target_params,\n        dual_params=dual_params,\n        sequence=sequence,\n        online_policy=predictions.policy,\n        key=key)\n\n    # TODO(abef): mask policy loss at terminal states or use uniform targets\n    # is_terminal = sequence.discount == 0.\n\n    # Compute MPO policy loss on each state in the sequence.\n    policy_loss, policy_stats = self._policy_loss_module(\n        params=dual_params,\n        online_action_distribution=predictions.policy,  # [T, ...].\n        target_action_distribution=targets.policy,  # [T, ...].\n        actions=targets.a_improvement,  # Unused in discrete case; [N, T].\n        q_values=targets.q_improvement)  # [N, T]\n\n    # Compute the critic loss on the states in the sequence.\n    critic_loss = self._distributional_loss(\n        prediction=predictions.q_value,  # [T-1, 1, ...]\n        target=targets.q_value)  # [N, Z, T-1]\n\n    loss = (self._loss_scales.policy * policy_loss +\n            self._loss_scales.critic * critic_loss)\n    loss_logging_dict = {\n        'loss': loss,\n        'root_policy_loss': policy_loss,\n        'root_critic_loss': critic_loss,\n        'policy_loss': policy_loss,\n        'critic_loss': critic_loss,\n    }\n\n    # Append MPO statistics.\n    loss_logging_dict.update(\n        {f'policy/root/{k}': v for k, v in policy_stats._asdict().items()})\n\n    # Compute rollout losses.\n    if self._model_rollout_length > 0:\n      model_rollout_loss, rollout_logs = self._model_rollout_loss_fn(\n          params, dual_params, sequence, predictions.embedding, targets, key)\n      loss += model_rollout_loss\n      loss_logging_dict.update(rollout_logs)\n      loss_logging_dict.update({\n          'policy_loss': policy_loss + rollout_logs['rollout_policy_loss'],\n          'critic_loss': critic_loss + rollout_logs['rollout_critic_loss'],\n          'loss': loss})\n\n    return loss, loss_logging_dict\n\n  def _sgd_step(\n      self,\n      state: TrainingState,\n      transitions: Union[types.Transition, adders.Step],\n  ) -> Tuple[TrainingState, Dict[str, Any]]:\n    \"\"\"Perform one parameter update step.\"\"\"\n\n    if isinstance(transitions, types.Transition):\n      sequences = mpo_utils.make_sequences_from_transitions(transitions)\n      if self._model_rollout_length > 0:\n        raise ValueError('model rollouts not yet supported from transitions')\n    else:\n      sequences = transitions\n\n    # Get next random_key and `batch_size` keys.\n    batch_size = sequences.reward.shape[0]\n    keys = jax.random.split(state.random_key, num=batch_size+1)\n    random_key, keys = keys[0], keys[1:]\n\n    # Vmap over the batch dimension when learning from sequences.\n    loss_vfn = jax.vmap(self._loss_fn, in_axes=(None, None, 0, None, 0))\n    safe_mean = lambda x: jnp.mean(x) if x is not None else x\n    # TODO(bshahr): Consider cleaning this up via acme.tree_utils.tree_map.\n    loss_fn = lambda *a, **k: tree.map_structure(safe_mean, loss_vfn(*a, **k))\n\n    loss_and_grad = jax.value_and_grad(loss_fn, argnums=(0, 1), has_aux=True)\n\n    # Compute the loss and gradient.\n    (_, loss_log_dict), all_gradients = loss_and_grad(\n        state.params, state.dual_params, sequences, state.target_params, keys)\n\n    # Average gradients across replicas.\n    gradients, dual_gradients = jax.lax.pmean(all_gradients, _PMAP_AXIS_NAME)\n\n    # Compute gradient norms before clipping.\n    gradients_norm = optax.global_norm(gradients)\n    dual_gradients_norm = optax.global_norm(dual_gradients)\n\n    # Get optimizer updates and state.\n    updates, opt_state = self._optimizer.update(\n        gradients, state.opt_state, state.params)\n    dual_updates, dual_opt_state = self._dual_optimizer.update(\n        dual_gradients, state.dual_opt_state, state.dual_params)\n\n    # Apply optimizer updates to parameters.\n    params = optax.apply_updates(state.params, updates)\n    dual_params = optax.apply_updates(state.dual_params, dual_updates)\n\n    # Clip dual params at some minimum value.\n    dual_params = self._dual_clip_fn(dual_params)\n\n    steps = state.steps + 1\n\n    # Periodically update target networks.\n    if self._target_update_period:\n      target_params = optax.periodic_update(params, state.target_params, steps,  # pytype: disable=wrong-arg-types  # numpy-scalars\n                                            self._target_update_period)\n    elif self._target_update_rate:\n      target_params = optax.incremental_update(params, state.target_params,\n                                               self._target_update_rate)\n\n    new_state = TrainingState(  # pytype: disable=wrong-arg-types  # numpy-scalars\n        params=params,\n        target_params=target_params,\n        dual_params=dual_params,\n        opt_state=opt_state,\n        dual_opt_state=dual_opt_state,\n        steps=steps,\n        random_key=random_key,\n    )\n\n    # Log the metrics from this learner step.\n    metrics = {f'loss/{k}': v for k, v in loss_log_dict.items()}\n\n    metrics.update({\n        'opt/grad_norm': gradients_norm,\n        'opt/param_norm': optax.global_norm(params)})\n\n    dual_metrics = {\n        'opt/dual_grad_norm': dual_gradients_norm,\n        'opt/dual_param_norm': optax.global_norm(dual_params),\n        'params/dual/log_temperature_avg': dual_params.log_temperature}\n    if isinstance(dual_params, continuous_losses.MPOParams):\n      dual_metrics.update({\n          'params/dual/log_alpha_mean_avg': dual_params.log_alpha_mean,\n          'params/dual/log_alpha_stddev_avg': dual_params.log_alpha_stddev})\n      if dual_params.log_penalty_temperature is not None:\n        dual_metrics['params/dual/log_penalty_temp_mean'] = (\n            dual_params.log_penalty_temperature)\n    elif isinstance(dual_params, discrete_losses.CategoricalMPOParams):\n      dual_metrics['params/dual/log_alpha_avg'] = dual_params.log_alpha\n    metrics.update(jax.tree_map(jnp.mean, dual_metrics))\n\n    return new_state, metrics\n\n  def step(self):\n    \"\"\"Perform one learner step, which in general does multiple SGD steps.\"\"\"\n    with jax.profiler.StepTraceAnnotation('step', step_num=self._current_step):\n      # Get data from replay (dropping extras if any). Note there is no\n      # extra data here because we do not insert any into Reverb.\n      sample = next(self._iterator)\n      if isinstance(self._experience_type, mpo_types.FromTransitions):\n        minibatch = types.Transition(*sample.data)\n      elif isinstance(self._experience_type, mpo_types.FromSequences):\n        minibatch = adders.Step(*sample.data)\n\n      self._state, metrics = self._sgd_steps(self._state, minibatch)\n      self._current_step, metrics = mpo_utils.get_from_first_device(\n          (self._state.steps, metrics))\n\n      # Compute elapsed time.\n      timestamp = time.time()\n      elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n      self._timestamp = timestamp\n\n      # Increment counts and record the current time\n      counts = self._counter.increment(\n          steps=self._sgd_steps_per_learner_step, walltime=elapsed_time)\n\n      if elapsed_time > 0:\n        metrics['steps_per_second'] = (\n            self._sgd_steps_per_learner_step / elapsed_time)\n      else:\n        metrics['steps_per_second'] = 0.\n\n      # Attempts to write the logs.\n      if self._logger:\n        self._logger.write({**metrics, **counts})\n\n  def get_variables(self, names: List[str]) -> network_lib.Params:\n    params = mpo_utils.get_from_first_device(self._state.target_params)\n\n    variables = {\n        'policy_head': params.policy_head,\n        'critic_head': params.critic_head,\n        'torso': params.torso,\n        'network': params,\n        'policy': params._replace(critic_head={}),\n        'critic': params._replace(policy_head={}),\n    }\n    return [variables[name] for name in names]\n\n  def save(self) -> TrainingState:\n    return jax.tree_map(mpo_utils.get_from_first_device, self._state)\n\n  def restore(self, state: TrainingState):\n    self._state = utils.replicate_in_all_devices(state, self._local_devices)",
  "def _get_default_optimizer(\n    learning_rate: optax.ScalarOrSchedule, max_grad_norm: Optional[float] = None\n) -> optax.GradientTransformation:\n  optimizer = optax.adam(learning_rate)\n  if max_grad_norm and max_grad_norm > 0:\n    optimizer = optax.chain(optax.clip_by_global_norm(max_grad_norm), optimizer)\n  return optimizer",
  "def __init__(  # pytype: disable=annotation-type-mismatch  # numpy-scalars\n      self,\n      critic_type: CriticType,\n      discrete_policy: bool,\n      environment_spec: specs.EnvironmentSpec,\n      networks: mpo_networks.MPONetworks,\n      random_key: jax_types.PRNGKey,\n      discount: float,\n      num_samples: int,\n      iterator: Iterator[reverb.ReplaySample],\n      experience_type: mpo_types.ExperienceType,\n      loss_scales: mpo_types.LossScalesConfig,\n      target_update_period: Optional[int] = 100,\n      target_update_rate: Optional[float] = None,\n      sgd_steps_per_learner_step: int = 20,\n      policy_eval_stochastic: bool = True,\n      policy_eval_num_val_samples: int = 128,\n      policy_loss_config: Optional[mpo_types.PolicyLossConfig] = None,\n      use_online_policy_to_bootstrap: bool = False,\n      use_stale_state: bool = False,\n      use_retrace: bool = False,\n      retrace_lambda: float = 0.95,\n      model_rollout_length: int = 0,\n      optimizer: Optional[optax.GradientTransformation] = None,\n      learning_rate: optax.ScalarOrSchedule = 1e-4,\n      dual_optimizer: Optional[optax.GradientTransformation] = None,\n      dual_learning_rate: optax.ScalarOrSchedule = 1e-2,\n      grad_norm_clip: float = 40.0,\n      reward_clip: float = np.float32('inf'),\n      value_tx_pair: rlax.TxPair = rlax.IDENTITY_PAIR,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      devices: Optional[Sequence[jax.Device]] = None,\n  ):\n    self._critic_type = critic_type\n    self._discrete_policy = discrete_policy\n\n    process_id = jax.process_index()\n    local_devices = jax.local_devices()\n    self._devices = devices or local_devices\n    logging.info('Learner process id: %s. Devices passed: %s', process_id,\n                 devices)\n    logging.info('Learner process id: %s. Local devices from JAX API: %s',\n                 process_id, local_devices)\n    self._local_devices = [d for d in self._devices if d in local_devices]\n\n    # Store networks.\n    self._networks = networks\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger\n\n    # Other learner parameters.\n    self._discount = discount\n    self._num_samples = num_samples\n    self._sgd_steps_per_learner_step = sgd_steps_per_learner_step\n\n    self._policy_eval_stochastic = policy_eval_stochastic\n    self._policy_eval_num_val_samples = policy_eval_num_val_samples\n\n    self._reward_clip_range = sorted([-reward_clip, reward_clip])\n    self._tx_pair = value_tx_pair\n    self._loss_scales = loss_scales\n    self._use_online_policy_to_bootstrap = use_online_policy_to_bootstrap\n    self._model_rollout_length = model_rollout_length\n\n    self._use_retrace = use_retrace\n    self._retrace_lambda = retrace_lambda\n    if use_retrace and critic_type == CriticType.MIXTURE_OF_GAUSSIANS:\n      logging.warning(\n          'Warning! Retrace has not been tested with the MoG critic.')\n    self._use_stale_state = use_stale_state\n\n    self._experience_type = experience_type\n    if isinstance(self._experience_type, mpo_types.FromTransitions):\n      # Each n=5-step transition will be converted to a length 2 sequence before\n      # being passed to the loss, so we do n=1 step bootstrapping on the\n      # resulting sequence to get n=5-step bootstrapping as intended.\n      self._n_step_for_sequence_bootstrap = 1\n      self._td_lambda = 1.0\n    elif isinstance(self._experience_type, mpo_types.FromSequences):\n      self._n_step_for_sequence_bootstrap = self._experience_type.n_step\n      self._td_lambda = self._experience_type.td_lambda\n\n    # Necessary to track when to update target networks.\n    self._target_update_period = target_update_period\n    self._target_update_rate = target_update_rate\n    # Assert one and only one of target update period or rate is defined.\n    if ((target_update_period and target_update_rate) or\n        (target_update_period is None and target_update_rate is None)):\n      raise ValueError(\n          'Exactly one of target_update_{period|rate} must be set.'\n          f' Received target_update_period={target_update_period} and'\n          f' target_update_rate={target_update_rate}.')\n\n    # Create policy loss.\n    if self._discrete_policy:\n      policy_loss_config = (\n          policy_loss_config or mpo_types.CategoricalPolicyLossConfig())\n      self._policy_loss_module = discrete_losses.CategoricalMPO(\n          **dataclasses.asdict(policy_loss_config))\n    else:\n      policy_loss_config = (\n          policy_loss_config or mpo_types.GaussianPolicyLossConfig())\n      self._policy_loss_module = continuous_losses.MPO(\n          **dataclasses.asdict(policy_loss_config))\n\n    self._policy_loss_module.__call__ = jax.named_call(\n        self._policy_loss_module.__call__, name='policy_loss')\n\n    # Create the dynamics model rollout loss.\n    if model_rollout_length > 0:\n      if not discrete_policy and (self._loss_scales.rollout.policy or\n                                  self._loss_scales.rollout.bc_policy):\n        raise ValueError('Policy rollout losses are only supported in the '\n                         'discrete policy case.')\n      self._model_rollout_loss_fn = rollout_loss.RolloutLoss(\n          dynamics_model=networks.dynamics_model,\n          model_rollout_length=model_rollout_length,\n          loss_scales=loss_scales,\n          distributional_loss_fn=self._distributional_loss)\n\n    # Create optimizers if they aren't given.\n    self._optimizer = optimizer or _get_default_optimizer(\n        learning_rate, grad_norm_clip\n    )\n    self._dual_optimizer = dual_optimizer or _get_default_optimizer(\n        dual_learning_rate, grad_norm_clip\n    )\n\n    self._action_spec = environment_spec.actions\n\n    # Initialize random key for the rest of training.\n    random_key, key = jax.random.split(random_key)\n\n    # Initialize network parameters, ignoring the dummy initial state.\n    network_params, _ = mpo_networks.init_params(\n        self._networks,\n        environment_spec,\n        key,\n        add_batch_dim=True,\n        dynamics_rollout_length=self._model_rollout_length)\n\n    # Get action dims (unused in the discrete case).\n    dummy_action = utils.zeros_like(environment_spec.actions)\n    dummy_action_concat = utils.batch_concat(dummy_action, num_batch_dims=0)\n\n    if isinstance(self._policy_loss_module, discrete_losses.CategoricalMPO):\n      self._dual_clip_fn = discrete_losses.clip_categorical_mpo_params\n    elif isinstance(self._policy_loss_module, continuous_losses.MPO):\n      is_constraining = self._policy_loss_module.per_dim_constraining\n      self._dual_clip_fn = lambda dp: continuous_losses.clip_mpo_params(  # pylint: disable=g-long-lambda  # pytype: disable=wrong-arg-types  # numpy-scalars\n          dp,\n          per_dim_constraining=is_constraining)\n\n    # Create dual parameters. In the discrete case, the action dim is unused.\n    dual_params = self._policy_loss_module.init_params(\n        action_dim=dummy_action_concat.shape[-1], dtype=jnp.float32)\n\n    # Initialize optimizers.\n    opt_state = self._optimizer.init(network_params)\n    dual_opt_state = self._dual_optimizer.init(dual_params)\n\n    # Initialise training state (parameters and optimiser state).\n    state = TrainingState(\n        params=network_params,\n        target_params=network_params,\n        dual_params=dual_params,\n        opt_state=opt_state,\n        dual_opt_state=dual_opt_state,\n        steps=0,\n        random_key=random_key,\n    )\n    self._state = utils.replicate_in_all_devices(state, self._local_devices)\n\n    # Log how many parameters the network has.\n    sizes = tree.map_structure(jnp.size, network_params)._asdict()\n    num_params_by_component_str = ' | '.join(\n        [f'{key}: {sum(tree.flatten(size))}' for key, size in sizes.items()])\n    logging.info('Number of params by network component: %s',\n                 num_params_by_component_str)\n    logging.info('Total number of params: %d',\n                 sum(tree.flatten(sizes.values())))\n\n    # Combine multiple SGD steps and pmap across devices.\n    sgd_steps = utils.process_multiple_batches(self._sgd_step,\n                                               self._sgd_steps_per_learner_step)\n    self._sgd_steps = jax.pmap(\n        sgd_steps, axis_name=_PMAP_AXIS_NAME, devices=self._devices)\n\n    self._iterator = iterator\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None\n    self._current_step = 0",
  "def _distributional_loss(self, prediction: mpo_types.DistributionLike,\n                           target: chex.Array):\n    \"\"\"Compute the critic loss given the prediction and target.\"\"\"\n    # TODO(abef): break this function into separate functions for each critic.\n    chex.assert_rank(target, 3)  # [N, Z, T] except for Categorical is [1, T, L]\n    if self._critic_type == CriticType.MIXTURE_OF_GAUSSIANS:\n      # Sample-based cross-entropy loss.\n      loss = -prediction.log_prob(target[..., jnp.newaxis])\n      loss = jnp.mean(loss, axis=[0, 1])  # [T]\n    elif self._critic_type == CriticType.NONDISTRIBUTIONAL:\n      # TD error.\n      prediction = prediction.squeeze(axis=-1)  # [T]\n      loss = 0.5 * jnp.square(target - prediction)\n      chex.assert_equal_shape([target, loss])  # Check broadcasting.\n    elif self._critic_type == mpo_types.CriticType.CATEGORICAL_2HOT:\n      # Cross-entropy loss (two-hot categorical).\n      target = jnp.mean(target, axis=(0, 1))  # [N, Z, T] -> [T]\n      # TODO(abef): Compute target differently? (e.g., do mean cross ent.).\n      target_probs = rlax.transform_to_2hot(  # [T, L]\n          target,\n          min_value=prediction.values.min(),\n          max_value=prediction.values.max(),\n          num_bins=prediction.logits.shape[-1])\n      logits = jnp.squeeze(prediction.logits, axis=1)  # [T, L]\n      chex.assert_equal_shape([target_probs, logits])\n      loss = jax.vmap(rlax.categorical_cross_entropy)(target_probs, logits)\n    elif self._critic_type == mpo_types.CriticType.CATEGORICAL:\n      loss = jax.vmap(rlax.categorical_cross_entropy)(jnp.squeeze(\n          target, axis=0), jnp.squeeze(prediction.logits, axis=1))\n    return jnp.mean(loss)",
  "def _compute_predictions(self, params: mpo_networks.MPONetworkParams,\n                           sequence: adders.Step) -> mpo_types.ModelOutputs:\n    \"\"\"Compute model predictions at observed and rolled out states.\"\"\"\n\n    # Initialize the core states, possibly to the recorded stale state.\n    if self._use_stale_state:\n      initial_state = utils.maybe_recover_lstm_type(\n          sequence.extras['core_state'])\n      initial_state = tree.map_structure(lambda x: x[0], initial_state)\n    else:\n      initial_state = self._networks.torso.initial_state_fn(\n          params.torso_initial_state, None)\n\n    # Unroll the online core network. Note that this may pass the embeddings\n    # unchanged if, say, the core is an hk.IdentityCore.\n    state_embedding, _ = self._networks.torso_unroll(   # [T, ...]\n        params, sequence.observation, initial_state)\n\n    # Compute the root policy and critic outputs; [T, ...] and [T-1, ...].\n    policy = self._networks.policy_head_apply(params, state_embedding)\n    q_value = self._networks.critic_head_apply(\n        params, state_embedding[:-1], sequence.action[:-1])\n\n    return mpo_types.ModelOutputs(\n        policy=policy,  # [T, ...]\n        q_value=q_value,  # [T-1, ...]\n        reward=None,\n        embedding=state_embedding)",
  "def _compute_targets(\n      self,\n      target_params: mpo_networks.MPONetworkParams,\n      dual_params: mpo_types.DualParams,\n      sequence: adders.Step,\n      online_policy: types.NestedArray,  # TODO(abef): remove this.\n      key: jax_types.PRNGKey) -> mpo_types.LossTargets:\n    \"\"\"Compute the targets needed to train the agent.\"\"\"\n\n    # Initialize the core states, possibly to the recorded stale state.\n    if self._use_stale_state:\n      initial_state = utils.maybe_recover_lstm_type(\n          sequence.extras['core_state'])\n      initial_state = tree.map_structure(lambda x: x[0], initial_state)\n    else:\n      initial_state = self._networks.torso.initial_state_fn(\n          target_params.torso_initial_state, None)\n\n    # Unroll the target core network. Note that this may pass the embeddings\n    # unchanged if, say, the core is an hk.IdentityCore.\n    target_state_embedding, _ = self._networks.torso_unroll(\n        target_params, sequence.observation, initial_state)  # [T, ...]\n\n    # Compute the action distribution from target policy network.\n    target_policy = self._networks.policy_head_apply(\n        target_params, target_state_embedding)  # [T, ...]\n\n    # Maybe reward clip.\n    clipped_reward = jnp.clip(sequence.reward, *self._reward_clip_range)  # [T]\n    # TODO(abef): when to clip rewards, if at all, if learning dynamics model?\n\n    @jax.named_call\n    @jax.vmap\n    def critic_mean_fn(action_: jnp.ndarray) -> jnp.ndarray:\n      \"\"\"Compute mean of target critic distribution.\"\"\"\n      critic_output = self._networks.critic_head_apply(\n          target_params, target_state_embedding, action_)\n      if self._critic_type != CriticType.NONDISTRIBUTIONAL:\n        critic_output = critic_output.mean()\n      return critic_output\n\n    @jax.named_call\n    @jax.vmap\n    def critic_sample_fn(action_: jnp.ndarray,\n                         seed_: jnp.ndarray) -> jnp.ndarray:\n      \"\"\"Sample from the target critic distribution.\"\"\"\n      z_distribution = self._networks.critic_head_apply(\n          target_params, target_state_embedding, action_)\n      z_samples = z_distribution.sample(\n          self._policy_eval_num_val_samples, seed=seed_)\n      return z_samples  # [Z, T, 1]\n\n    if self._discrete_policy:\n      # Use all actions to improve policy (no sampling); N = num_actions.\n      a_improvement = jnp.arange(self._action_spec.num_values)  # [N]\n      seq_len = target_state_embedding.shape[0]  # T\n      a_improvement = jnp.tile(a_improvement[..., None], [1, seq_len])  # [N, T]\n    else:\n      # Sample actions to improve policy; [N=num_samples, T].\n      a_improvement = target_policy.sample(self._num_samples, seed=key)\n\n    # TODO(abef): use model to get q_improvement = r + gamma*V?\n\n    # Compute the mean Q-values used in policy improvement; [N, T].\n    q_improvement = critic_mean_fn(a_improvement).squeeze(axis=-1)\n\n    # Policy to use for policy evaluation and bootstrapping.\n    if self._use_online_policy_to_bootstrap:\n      policy_to_evaluate = online_policy\n      chex.assert_equal(online_policy.batch_shape, target_policy.batch_shape)\n    else:\n      policy_to_evaluate = target_policy\n\n    # Action(s) to use for policy evaluation; shape [N, T].\n    if self._policy_eval_stochastic:\n      a_evaluation = policy_to_evaluate.sample(self._num_samples, seed=key)\n    else:\n      a_evaluation = policy_to_evaluate.mode()\n      a_evaluation = jnp.expand_dims(a_evaluation, axis=0)  # [N=1, T]\n\n    # TODO(abef): policy_eval_stochastic=False makes our targets more \"greedy\"\n\n    # Add a stopgrad in case we use the online policy for evaluation.\n    a_evaluation = jax.lax.stop_gradient(a_evaluation)\n\n    if self._critic_type == CriticType.MIXTURE_OF_GAUSSIANS:\n      # Produce Z return samples for every N action sample; [N, Z, T, 1].\n      seeds = jax.random.split(key, num=a_evaluation.shape[0])\n      z_samples = critic_sample_fn(a_evaluation, seeds)\n    else:\n      normalized_weights = 1. / a_evaluation.shape[0]\n      z_samples = critic_mean_fn(a_evaluation)  # [N, T, 1]\n\n      # When policy_eval_stochastic == True, this corresponds to expected SARSA.\n      # Otherwise, normalized_weights = 1.0 and N = 1 so the sum is a no-op.\n      z_samples = jnp.sum(normalized_weights * z_samples, axis=0, keepdims=True)\n      z_samples = jnp.expand_dims(z_samples, axis=1)  # [N, Z=1, T, 1]\n\n    # Slice to t = 1...T and transform into raw reward space; [N, Z, T].\n    z_samples_itx = self._tx_pair.apply_inv(z_samples.squeeze(axis=-1))\n\n    # Compute the value estimate by averaging the sampled returns in the raw\n    # reward space; shape [N=1, Z=1, T].\n    value_target_itx = jnp.mean(z_samples_itx, axis=(0, 1), keepdims=True)\n\n    if self._use_retrace:\n      # Warning! Retrace has not been tested with the MoG critic.\n      log_rhos = (\n          target_policy.log_prob(sequence.action) - sequence.extras['log_prob'])\n\n      # Compute Q-values; expand and squeeze because critic_mean_fn is vmapped.\n      q_t = critic_mean_fn(jnp.expand_dims(sequence.action, axis=0)).squeeze(0)\n      q_t = q_t.squeeze(-1)  # Also squeeze trailing scalar dimension; [T].\n\n      # Compute retrace targets.\n      # These targets use the rewards and discounts as in normal TD-learning but\n      # they use a mix of bootstrapped values V(s') and Q(s', a'), weighing the\n      # latter based on how likely a' is under the current policy (s' and a' are\n      # samples from replay).\n      # See [Munos et al., 2016](https://arxiv.org/abs/1606.02647) for more.\n      q_value_target_itx = rlax.general_off_policy_returns_from_q_and_v(\n          q_t=self._tx_pair.apply_inv(q_t[1:-1]),\n          v_t=jnp.squeeze(value_target_itx, axis=(0, 1))[1:],\n          r_t=clipped_reward[:-1],\n          discount_t=self._discount * sequence.discount[:-1],\n          c_t=self._retrace_lambda * jnp.minimum(1.0, jnp.exp(log_rhos[1:-1])))\n\n      # Expand dims to the expected [N=1, Z=1, T-1].\n      q_value_target_itx = jnp.expand_dims(q_value_target_itx, axis=(0, 1))\n    else:\n      # Compute bootstrap target from sequences. vmap return computation across\n      # N action and Z return samples; shape [N, Z, T-1].\n      n_step_return_fn = functools.partial(\n          rlax.n_step_bootstrapped_returns,\n          r_t=clipped_reward[:-1],\n          discount_t=self._discount * sequence.discount[:-1],\n          n=self._n_step_for_sequence_bootstrap,\n          lambda_t=self._td_lambda)\n      n_step_return_vfn = jax.vmap(jax.vmap(n_step_return_fn))\n      q_value_target_itx = n_step_return_vfn(v_t=z_samples_itx[..., 1:])\n\n    # Transform back to the canonical space and stop gradients.\n    q_value_target = jax.lax.stop_gradient(\n        self._tx_pair.apply(q_value_target_itx))\n    reward_target = jax.lax.stop_gradient(self._tx_pair.apply(clipped_reward))\n    value_target = jax.lax.stop_gradient(self._tx_pair.apply(value_target_itx))\n\n    if self._critic_type == mpo_types.CriticType.CATEGORICAL:\n\n      @jax.vmap\n      def get_logits_and_values(\n          action: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:\n        critic_output = self._networks.critic_head_apply(\n            target_params, target_state_embedding[1:], action)\n        return critic_output.logits, critic_output.values\n\n      z_t_logits, z_t_values = get_logits_and_values(a_evaluation[:, 1:])\n      z_t_logits = jnp.squeeze(z_t_logits, axis=2)  # [N, T-1, L]\n      z_t_values = z_t_values[0]  # Values are identical at each N; [L].\n\n      gamma = self._discount * sequence.discount[:-1, None]  # [T-1, 1]\n      r_t = clipped_reward[:-1, None]  # [T-1, 1]\n      atoms_itx = self._tx_pair.apply_inv(z_t_values)[None, ...]  # [1, L]\n      z_target_atoms = self._tx_pair.apply(r_t + gamma * atoms_itx)  # [T-1, L]\n      # Note: this is n=1-step TD unless using experience=FromTransitions(n>1).\n      z_target_probs = jax.nn.softmax(z_t_logits)  # [N, T-1, L]\n      z_target_atoms = jax.lax.broadcast(\n          z_target_atoms, z_target_probs.shape[:1])  # [N, T-1, L]\n      project_fn = functools.partial(\n          rlax.categorical_l2_project, z_q=z_t_values)\n      z_target = jax.vmap(jax.vmap(project_fn))(z_target_atoms, z_target_probs)\n      z_target = jnp.mean(z_target, axis=0)\n      q_value_target = jax.lax.stop_gradient(z_target[None, ...])  # [1, T-1, L]\n      # TODO(abef): make q_v_target shape align with expected [N, Z, T-1] shape?\n\n    targets = mpo_types.LossTargets(\n        policy=target_policy,  # [T, ...]\n        a_improvement=a_improvement,  # [N, T]\n        q_improvement=q_improvement,  # [N, T]\n        q_value=q_value_target,  # [N, Z, T-1] ([1, T-1, L] for CATEGORICAL)\n        value=value_target[..., :-1],  # [N=1, Z=1, T-1]\n        reward=reward_target,  # [T]\n        embedding=target_state_embedding)  # [T, ...]\n\n    return targets",
  "def _loss_fn(\n      self,\n      params: mpo_networks.MPONetworkParams,\n      dual_params: mpo_types.DualParams,\n      # TODO(bshahr): clean up types: Step is not a great type for sequences.\n      sequence: adders.Step,\n      target_params: mpo_networks.MPONetworkParams,\n      key: jax_types.PRNGKey) -> Tuple[jnp.ndarray, mpo_types.LogDict]:\n    # Compute the model predictions at the root and for the rollouts.\n    predictions = self._compute_predictions(params=params, sequence=sequence)\n\n    # Compute the targets to use for the losses.\n    targets = self._compute_targets(\n        target_params=target_params,\n        dual_params=dual_params,\n        sequence=sequence,\n        online_policy=predictions.policy,\n        key=key)\n\n    # TODO(abef): mask policy loss at terminal states or use uniform targets\n    # is_terminal = sequence.discount == 0.\n\n    # Compute MPO policy loss on each state in the sequence.\n    policy_loss, policy_stats = self._policy_loss_module(\n        params=dual_params,\n        online_action_distribution=predictions.policy,  # [T, ...].\n        target_action_distribution=targets.policy,  # [T, ...].\n        actions=targets.a_improvement,  # Unused in discrete case; [N, T].\n        q_values=targets.q_improvement)  # [N, T]\n\n    # Compute the critic loss on the states in the sequence.\n    critic_loss = self._distributional_loss(\n        prediction=predictions.q_value,  # [T-1, 1, ...]\n        target=targets.q_value)  # [N, Z, T-1]\n\n    loss = (self._loss_scales.policy * policy_loss +\n            self._loss_scales.critic * critic_loss)\n    loss_logging_dict = {\n        'loss': loss,\n        'root_policy_loss': policy_loss,\n        'root_critic_loss': critic_loss,\n        'policy_loss': policy_loss,\n        'critic_loss': critic_loss,\n    }\n\n    # Append MPO statistics.\n    loss_logging_dict.update(\n        {f'policy/root/{k}': v for k, v in policy_stats._asdict().items()})\n\n    # Compute rollout losses.\n    if self._model_rollout_length > 0:\n      model_rollout_loss, rollout_logs = self._model_rollout_loss_fn(\n          params, dual_params, sequence, predictions.embedding, targets, key)\n      loss += model_rollout_loss\n      loss_logging_dict.update(rollout_logs)\n      loss_logging_dict.update({\n          'policy_loss': policy_loss + rollout_logs['rollout_policy_loss'],\n          'critic_loss': critic_loss + rollout_logs['rollout_critic_loss'],\n          'loss': loss})\n\n    return loss, loss_logging_dict",
  "def _sgd_step(\n      self,\n      state: TrainingState,\n      transitions: Union[types.Transition, adders.Step],\n  ) -> Tuple[TrainingState, Dict[str, Any]]:\n    \"\"\"Perform one parameter update step.\"\"\"\n\n    if isinstance(transitions, types.Transition):\n      sequences = mpo_utils.make_sequences_from_transitions(transitions)\n      if self._model_rollout_length > 0:\n        raise ValueError('model rollouts not yet supported from transitions')\n    else:\n      sequences = transitions\n\n    # Get next random_key and `batch_size` keys.\n    batch_size = sequences.reward.shape[0]\n    keys = jax.random.split(state.random_key, num=batch_size+1)\n    random_key, keys = keys[0], keys[1:]\n\n    # Vmap over the batch dimension when learning from sequences.\n    loss_vfn = jax.vmap(self._loss_fn, in_axes=(None, None, 0, None, 0))\n    safe_mean = lambda x: jnp.mean(x) if x is not None else x\n    # TODO(bshahr): Consider cleaning this up via acme.tree_utils.tree_map.\n    loss_fn = lambda *a, **k: tree.map_structure(safe_mean, loss_vfn(*a, **k))\n\n    loss_and_grad = jax.value_and_grad(loss_fn, argnums=(0, 1), has_aux=True)\n\n    # Compute the loss and gradient.\n    (_, loss_log_dict), all_gradients = loss_and_grad(\n        state.params, state.dual_params, sequences, state.target_params, keys)\n\n    # Average gradients across replicas.\n    gradients, dual_gradients = jax.lax.pmean(all_gradients, _PMAP_AXIS_NAME)\n\n    # Compute gradient norms before clipping.\n    gradients_norm = optax.global_norm(gradients)\n    dual_gradients_norm = optax.global_norm(dual_gradients)\n\n    # Get optimizer updates and state.\n    updates, opt_state = self._optimizer.update(\n        gradients, state.opt_state, state.params)\n    dual_updates, dual_opt_state = self._dual_optimizer.update(\n        dual_gradients, state.dual_opt_state, state.dual_params)\n\n    # Apply optimizer updates to parameters.\n    params = optax.apply_updates(state.params, updates)\n    dual_params = optax.apply_updates(state.dual_params, dual_updates)\n\n    # Clip dual params at some minimum value.\n    dual_params = self._dual_clip_fn(dual_params)\n\n    steps = state.steps + 1\n\n    # Periodically update target networks.\n    if self._target_update_period:\n      target_params = optax.periodic_update(params, state.target_params, steps,  # pytype: disable=wrong-arg-types  # numpy-scalars\n                                            self._target_update_period)\n    elif self._target_update_rate:\n      target_params = optax.incremental_update(params, state.target_params,\n                                               self._target_update_rate)\n\n    new_state = TrainingState(  # pytype: disable=wrong-arg-types  # numpy-scalars\n        params=params,\n        target_params=target_params,\n        dual_params=dual_params,\n        opt_state=opt_state,\n        dual_opt_state=dual_opt_state,\n        steps=steps,\n        random_key=random_key,\n    )\n\n    # Log the metrics from this learner step.\n    metrics = {f'loss/{k}': v for k, v in loss_log_dict.items()}\n\n    metrics.update({\n        'opt/grad_norm': gradients_norm,\n        'opt/param_norm': optax.global_norm(params)})\n\n    dual_metrics = {\n        'opt/dual_grad_norm': dual_gradients_norm,\n        'opt/dual_param_norm': optax.global_norm(dual_params),\n        'params/dual/log_temperature_avg': dual_params.log_temperature}\n    if isinstance(dual_params, continuous_losses.MPOParams):\n      dual_metrics.update({\n          'params/dual/log_alpha_mean_avg': dual_params.log_alpha_mean,\n          'params/dual/log_alpha_stddev_avg': dual_params.log_alpha_stddev})\n      if dual_params.log_penalty_temperature is not None:\n        dual_metrics['params/dual/log_penalty_temp_mean'] = (\n            dual_params.log_penalty_temperature)\n    elif isinstance(dual_params, discrete_losses.CategoricalMPOParams):\n      dual_metrics['params/dual/log_alpha_avg'] = dual_params.log_alpha\n    metrics.update(jax.tree_map(jnp.mean, dual_metrics))\n\n    return new_state, metrics",
  "def step(self):\n    \"\"\"Perform one learner step, which in general does multiple SGD steps.\"\"\"\n    with jax.profiler.StepTraceAnnotation('step', step_num=self._current_step):\n      # Get data from replay (dropping extras if any). Note there is no\n      # extra data here because we do not insert any into Reverb.\n      sample = next(self._iterator)\n      if isinstance(self._experience_type, mpo_types.FromTransitions):\n        minibatch = types.Transition(*sample.data)\n      elif isinstance(self._experience_type, mpo_types.FromSequences):\n        minibatch = adders.Step(*sample.data)\n\n      self._state, metrics = self._sgd_steps(self._state, minibatch)\n      self._current_step, metrics = mpo_utils.get_from_first_device(\n          (self._state.steps, metrics))\n\n      # Compute elapsed time.\n      timestamp = time.time()\n      elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n      self._timestamp = timestamp\n\n      # Increment counts and record the current time\n      counts = self._counter.increment(\n          steps=self._sgd_steps_per_learner_step, walltime=elapsed_time)\n\n      if elapsed_time > 0:\n        metrics['steps_per_second'] = (\n            self._sgd_steps_per_learner_step / elapsed_time)\n      else:\n        metrics['steps_per_second'] = 0.\n\n      # Attempts to write the logs.\n      if self._logger:\n        self._logger.write({**metrics, **counts})",
  "def get_variables(self, names: List[str]) -> network_lib.Params:\n    params = mpo_utils.get_from_first_device(self._state.target_params)\n\n    variables = {\n        'policy_head': params.policy_head,\n        'critic_head': params.critic_head,\n        'torso': params.torso,\n        'network': params,\n        'policy': params._replace(critic_head={}),\n        'critic': params._replace(policy_head={}),\n    }\n    return [variables[name] for name in names]",
  "def save(self) -> TrainingState:\n    return jax.tree_map(mpo_utils.get_from_first_device, self._state)",
  "def restore(self, state: TrainingState):\n    self._state = utils.replicate_in_all_devices(state, self._local_devices)",
  "def critic_mean_fn(action_: jnp.ndarray) -> jnp.ndarray:\n      \"\"\"Compute mean of target critic distribution.\"\"\"\n      critic_output = self._networks.critic_head_apply(\n          target_params, target_state_embedding, action_)\n      if self._critic_type != CriticType.NONDISTRIBUTIONAL:\n        critic_output = critic_output.mean()\n      return critic_output",
  "def critic_sample_fn(action_: jnp.ndarray,\n                         seed_: jnp.ndarray) -> jnp.ndarray:\n      \"\"\"Sample from the target critic distribution.\"\"\"\n      z_distribution = self._networks.critic_head_apply(\n          target_params, target_state_embedding, action_)\n      z_samples = z_distribution.sample(\n          self._policy_eval_num_val_samples, seed=seed_)\n      return z_samples",
  "def get_logits_and_values(\n          action: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:\n        critic_output = self._networks.critic_head_apply(\n            target_params, target_state_embedding[1:], action)\n        return critic_output.logits, critic_output.values",
  "def _fetch_devicearray(x):\n  if isinstance(x, jax.Array):\n    return np.asarray(x)\n  return x",
  "def get_from_first_device(nest, as_numpy: bool = True):\n  \"\"\"Gets the first array of a nest of `jax.pxla.ShardedDeviceArray`s.\"\"\"\n  # TODO(abef): remove this when fake_pmap is fixed or acme error is removed.\n\n  def _slice_and_maybe_to_numpy(x):\n    x = x[0]\n    return _fetch_devicearray(x) if as_numpy else x\n\n  return jax.tree_map(_slice_and_maybe_to_numpy, nest)",
  "def rolling_window(x: jnp.ndarray,\n                   window: int,\n                   axis: int = 0,\n                   time_major: bool = True):\n  \"\"\"Stack the N=T-W+1 length W slices [0:W, 1:W+1, ..., T-W:T] from a tensor.\n\n  Args:\n    x: The tensor to select rolling slices from (along specified axis), with\n      shape [..., T, ...]; i.e., T = x.shape[axis].\n    window: The length (W) of the slices to select.\n    axis: The axis to slice from (defaults to 0).\n    time_major: If true, output will have shape [..., W, N, ...], otherwise\n      it will have shape [..., N, W, ...], where x.shape is [..., T, ...].\n\n  Returns:\n    A tensor containing the stacked slices [0:W, ... T-W:T] from an axis of x.\n  \"\"\"\n  sequence_length = x.shape[axis]\n  starts = jnp.arange(sequence_length - window + 1)\n  ends = jnp.arange(window)\n  if time_major:\n    idx = starts[None, :] + ends[:, None]  # Output will be [..., W, N, ...].\n  else:\n    idx = starts[:, None] + ends[None, :]  # Output will be [..., N, W, ...].\n  out = jnp.take(x, idx, axis=axis)\n  return out",
  "def tree_map_distribution(\n    f: Callable[[mpo_types.DistributionOrArray], mpo_types.DistributionOrArray],\n    x: mpo_types.DistributionOrArray) -> mpo_types.DistributionOrArray:\n  \"\"\"Apply a jax function to a distribution by treating it as tree.\"\"\"\n  if isinstance(x, distrax.Distribution):\n    safe_f = lambda y: f(y) if isinstance(y, jnp.ndarray) else y\n    nil, tree_data = x.tree_flatten()\n    new_tree_data = jax.tree_map(safe_f, tree_data)\n    new_x = x.tree_unflatten(new_tree_data, nil)\n    return new_x\n  elif isinstance(x, tfd.Distribution):\n    return jax.tree_map(f, x)\n  else:\n    return f(x)",
  "def make_sequences_from_transitions(\n    transitions: types.Transition,\n    num_batch_dims: int = 1) -> adders.Step:\n  \"\"\"Convert a batch of transitions into a batch of 1-step sequences.\"\"\"\n  stack = lambda x, y: jnp.stack((x, y), axis=num_batch_dims)\n  duplicate = lambda x: stack(x, x)\n  observation = jax.tree_map(stack, transitions.observation,\n                             transitions.next_observation)\n  reward = duplicate(transitions.reward)\n\n  return adders.Step(\n      observation=observation,\n      action=duplicate(transitions.action),\n      reward=reward,\n      discount=duplicate(transitions.discount),\n      start_of_episode=jnp.zeros_like(reward, dtype=jnp.bool_),\n      extras=jax.tree_map(duplicate, transitions.extras))",
  "def _slice_and_maybe_to_numpy(x):\n    x = x[0]\n    return _fetch_devicearray(x) if as_numpy else x",
  "class MPONetworkParams(NamedTuple):\n  policy_head: Optional[hk.Params] = None\n  critic_head: Optional[hk.Params] = None\n  torso: Optional[hk.Params] = None\n  torso_initial_state: Optional[hk.Params] = None\n  dynamics_model: Union[hk.Params, Tuple[()]] = ()\n  dynamics_model_initial_state: Union[hk.Params, Tuple[()]] = ()",
  "class UnrollableNetwork:\n  \"\"\"Network that can unroll over an input sequence.\"\"\"\n  init: Callable[[networks_lib.PRNGKey, types.Observation, hk.LSTMState],\n                 hk.Params]\n  apply: Callable[[hk.Params, types.Observation, hk.LSTMState],\n                  Tuple[jnp.ndarray, hk.LSTMState]]\n  unroll: Callable[[hk.Params, types.Observation, hk.LSTMState],\n                   Tuple[jnp.ndarray, hk.LSTMState]]\n  initial_state_fn_init: Callable[[networks_lib.PRNGKey, Optional[int]],\n                                  hk.Params]\n  initial_state_fn: Callable[[hk.Params, Optional[int]], hk.LSTMState]",
  "class MPONetworks:\n  \"\"\"Network for the MPO agent.\"\"\"\n  policy_head: Optional[hk.Transformed] = None\n  critic_head: Optional[hk.Transformed] = None\n  torso: Optional[UnrollableNetwork] = None\n  dynamics_model: Optional[UnrollableNetwork] = None\n\n  def policy_head_apply(self, params: MPONetworkParams,\n                        obs_embedding: types.ObservationEmbedding):\n    return self.policy_head.apply(params.policy_head, obs_embedding)\n\n  def critic_head_apply(self, params: MPONetworkParams,\n                        obs_embedding: types.ObservationEmbedding,\n                        actions: types.Action):\n    return self.critic_head.apply(params.critic_head, obs_embedding, actions)\n\n  def torso_unroll(self, params: MPONetworkParams,\n                   observations: types.Observation, state: hk.LSTMState):\n    return self.torso.unroll(params.torso, observations, state)\n\n  def dynamics_model_unroll(self, params: MPONetworkParams,\n                            actions: types.Action, state: hk.LSTMState):\n    return self.dynamics_model.unroll(params.dynamics_model, actions, state)",
  "def init_params(\n    networks: MPONetworks,\n    spec: specs.EnvironmentSpec,\n    random_key: types.RNGKey,\n    add_batch_dim: bool = False,\n    dynamics_rollout_length: int = 0,\n) -> Tuple[MPONetworkParams, hk.LSTMState]:\n  \"\"\"Initialize the parameters of a MPO network.\"\"\"\n\n  rng_keys = jax.random.split(random_key, 6)\n\n  # Create a dummy observation/action to initialize network parameters.\n  observations, actions = utils.zeros_like((spec.observations, spec.actions))\n\n  # Add batch dimensions if necessary by the scope that is calling this init.\n  if add_batch_dim:\n    observations, actions = utils.add_batch_dim((observations, actions))\n\n  # Initialize the state torso parameters and create a dummy core state.\n  batch_size = 1 if add_batch_dim else None\n  params_torso_initial_state = networks.torso.initial_state_fn_init(\n      rng_keys[0], batch_size)\n  state = networks.torso.initial_state_fn(\n      params_torso_initial_state, batch_size)\n\n  # Initialize the core and unroll one step to create a dummy core output.\n  # The input to the core is the current action and the next observation.\n  params_torso = networks.torso.init(rng_keys[1], observations, state)\n  embeddings, _ = networks.torso.apply(params_torso, observations, state)\n\n  # Initialize the policy and critic heads by passing in the dummy embedding.\n  params_policy_head, params_critic_head = {}, {}  # Cannot be None for BIT.\n  if networks.policy_head:\n    params_policy_head = networks.policy_head.init(rng_keys[2], embeddings)\n  if networks.critic_head:\n    params_critic_head = networks.critic_head.init(rng_keys[3], embeddings,\n                                                   actions)\n\n  # Initialize the recurrent dynamics model if it exists.\n  if networks.dynamics_model and dynamics_rollout_length > 0:\n    params_dynamics_initial_state = networks.dynamics_model.initial_state_fn_init(\n        rng_keys[4], embeddings)\n    dynamics_state = networks.dynamics_model.initial_state_fn(\n        params_dynamics_initial_state, embeddings)\n    params_dynamics = networks.dynamics_model.init(\n        rng_keys[5], actions, dynamics_state)\n  else:\n    params_dynamics_initial_state = ()\n    params_dynamics = ()\n\n  params = MPONetworkParams(\n      policy_head=params_policy_head,\n      critic_head=params_critic_head,\n      torso=params_torso,\n      torso_initial_state=params_torso_initial_state,\n      dynamics_model=params_dynamics,\n      dynamics_model_initial_state=params_dynamics_initial_state)\n\n  return params, state",
  "def make_unrollable_network(\n    make_core_module: Callable[[], hk.RNNCore] = hk.IdentityCore,\n    make_feedforward_module: Optional[Callable[[], hk.SupportsCall]] = None,\n    make_initial_state_fn: Optional[Callable[[], hk.SupportsCall]] = None,\n) -> UnrollableNetwork:\n  \"\"\"Produces an UnrollableNetwork and a state initializing hk.Transformed.\"\"\"\n\n  def default_initial_state_fn(batch_size: Optional[int] = None) -> jnp.ndarray:\n    return make_core_module().initial_state(batch_size)\n\n  def _apply_core_fn(observation: types.Observation,\n                     state: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:\n    if make_feedforward_module:\n      observation = make_feedforward_module()(observation)\n    return make_core_module()(observation, state)\n\n  def _unroll_core_fn(observation: types.Observation,\n                      state: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:\n    if make_feedforward_module:\n      observation = make_feedforward_module()(observation)\n    return hk.dynamic_unroll(make_core_module(), observation, state)\n\n  if make_initial_state_fn:\n    initial_state_fn = make_initial_state_fn()\n  else:\n    initial_state_fn = default_initial_state_fn\n\n  # Transform module functions into pure functions.\n  hk_initial_state_fn = hk.without_apply_rng(hk.transform(initial_state_fn))\n  apply_core = hk.without_apply_rng(hk.transform(_apply_core_fn))\n  unroll_core = hk.without_apply_rng(hk.transform(_unroll_core_fn))\n\n  # Pack all core network pure functions into a single convenient container.\n  return UnrollableNetwork(\n      init=apply_core.init,\n      apply=apply_core.apply,\n      unroll=unroll_core.apply,\n      initial_state_fn_init=hk_initial_state_fn.init,\n      initial_state_fn=hk_initial_state_fn.apply)",
  "def make_control_networks(\n    environment_spec: specs.EnvironmentSpec,\n    *,\n    with_recurrence: bool = False,\n    policy_layer_sizes: Sequence[int] = (256, 256, 256),\n    critic_layer_sizes: Sequence[int] = (512, 512, 256),\n    policy_init_scale: float = 0.7,\n    critic_type: types.CriticType = types.CriticType.MIXTURE_OF_GAUSSIANS,\n    mog_init_scale: float = 1e-3,  # Used by MoG critic.\n    mog_num_components: int = 5,  # Used by MoG critic.\n    categorical_num_bins: int = 51,  # Used by CATEGORICAL* critics.\n    vmin: float = -150.,  # Used by CATEGORICAL* critics.\n    vmax: float = 150.,  # Used by CATEGORICAL* critics.\n) -> MPONetworks:\n  \"\"\"Creates MPONetworks to be used DM Control suite tasks.\"\"\"\n\n  # Unpack the environment spec to get appropriate shapes, dtypes, etc.\n  num_dimensions = np.prod(environment_spec.actions.shape, dtype=int)\n\n  # Factory to create the core hk.Module. Must be a factory as the module must\n  # be initialized within a hk.transform scope.\n  if with_recurrence:\n    make_core_module = lambda: GRUWithSkip(16)\n  else:\n    make_core_module = hk.IdentityCore\n\n  def policy_fn(observation: types.NestedArray) -> tfd.Distribution:\n    embedding = networks_lib.LayerNormMLP(\n        policy_layer_sizes, activate_final=True)(\n            observation)\n    return networks_lib.MultivariateNormalDiagHead(\n        num_dimensions, init_scale=policy_init_scale)(\n            embedding)\n\n  def critic_fn(observation: types.NestedArray,\n                action: types.NestedArray) -> DistributionOrArray:\n    # Action is clipped to avoid critic extrapolations outside the spec range.\n    clipped_action = networks_lib.ClipToSpec(environment_spec.actions)(action)\n    inputs = jnp.concatenate([observation, clipped_action], axis=-1)\n    embedding = networks_lib.LayerNormMLP(\n        critic_layer_sizes, activate_final=True)(\n            inputs)\n\n    if critic_type == types.CriticType.MIXTURE_OF_GAUSSIANS:\n      return networks_lib.GaussianMixture(\n          num_dimensions=1,\n          num_components=mog_num_components,\n          multivariate=False,\n          init_scale=mog_init_scale,\n          append_singleton_event_dim=False,\n          reinterpreted_batch_ndims=0)(\n              embedding)\n    elif critic_type in (types.CriticType.CATEGORICAL,\n                         types.CriticType.CATEGORICAL_2HOT):\n      return networks_lib.CategoricalCriticHead(\n          num_bins=categorical_num_bins, vmin=vmin, vmax=vmax)(\n              embedding)\n    else:\n      return hk.Linear(\n          output_size=1, w_init=hk_init.TruncatedNormal(0.01))(\n              embedding)\n\n  # Create unrollable torso.\n  torso = make_unrollable_network(make_core_module=make_core_module)\n\n  # Create MPONetworks to add functionality required by the agent.\n  return MPONetworks(\n      policy_head=hk.without_apply_rng(hk.transform(policy_fn)),\n      critic_head=hk.without_apply_rng(hk.transform(critic_fn)),\n      torso=torso)",
  "def add_batch(nest, batch_size: Optional[int]):\n  \"\"\"Adds a batch dimension at axis 0 to the leaves of a nested structure.\"\"\"\n  broadcast = lambda x: jnp.broadcast_to(x, (batch_size,) + x.shape)\n  return jax.tree_map(broadcast, nest)",
  "def w_init_identity(shape: Sequence[int], dtype) -> jnp.ndarray:\n  chex.assert_equal(len(shape), 2)\n  chex.assert_equal(shape[0], shape[1])\n  return jnp.eye(shape[0], dtype=dtype)",
  "class IdentityRNN(hk.RNNCore):\n  r\"\"\"Basic fully-connected RNN core with identity initialization.\n\n  Given :math:`x_t` and the previous hidden state :math:`h_{t-1}` the\n  core computes\n  .. math::\n     h_t = \\operatorname{ReLU}(w_i x_t + b_i + w_h h_{t-1} + b_h)\n  The output is equal to the new state, :math:`h_t`.\n\n  Initialized using the strategy described in:\n    https://arxiv.org/pdf/1504.00941.pdf\n  \"\"\"\n\n  def __init__(self,\n               hidden_size: int,\n               hidden_scale: float = 1e-2,\n               name: Optional[str] = None):\n    \"\"\"Constructs a vanilla RNN core.\n\n    Args:\n      hidden_size: Hidden layer size.\n      hidden_scale: Scalar multiplying the hidden-to-hidden matmul.\n      name: Name of the module.\n    \"\"\"\n    super().__init__(name=name)\n    self._initial_state = jnp.zeros([hidden_size])\n    self._hidden_scale = hidden_scale\n    self._input_to_hidden = hk.Linear(hidden_size)\n    self._hidden_to_hidden = hk.Linear(\n        hidden_size, with_bias=True, w_init=w_init_identity)\n\n  def __call__(self, inputs: jnp.ndarray, prev_state: jnp.ndarray):\n    out = jax.nn.relu(\n        self._input_to_hidden(inputs) +\n        self._hidden_scale * self._hidden_to_hidden(prev_state))\n    return out, out\n\n  def initial_state(self, batch_size: Optional[int]):\n    state = self._initial_state\n    if batch_size is not None:\n      state = add_batch(state, batch_size)\n    return state",
  "class GRU(hk.GRU):\n  \"\"\"GRU with an identity initialization.\"\"\"\n\n  def __init__(self, hidden_size: int, name: Optional[str] = None):\n\n    def b_init(unused_size: Sequence[int], dtype) -> jnp.ndarray:\n      \"\"\"Initializes the biases so the GRU ignores the state and acts as a tanh.\"\"\"\n      return jnp.concatenate([\n          +2 * jnp.ones([hidden_size], dtype=dtype),\n          -2 * jnp.ones([hidden_size], dtype=dtype),\n          jnp.zeros([hidden_size], dtype=dtype)\n      ])\n\n    super().__init__(hidden_size=hidden_size, b_init=b_init, name=name)",
  "class GRUWithSkip(hk.GRU):\n  \"\"\"GRU with a skip-connection from input to output.\"\"\"\n\n  def __call__(self, inputs: jnp.ndarray, prev_state: jnp.ndarray):\n    outputs, state = super().__call__(inputs, prev_state)\n    outputs = jnp.concatenate([inputs, outputs], axis=-1)\n    return outputs, state",
  "class Conv2DLSTMWithSkip(hk.Conv2DLSTM):\n  \"\"\"Conv2DLSTM with a skip-connection from input to output.\"\"\"\n\n  def __call__(self, inputs: jnp.ndarray, state: jnp.ndarray):\n    outputs, state = super().__call__(inputs, state)  # pytype: disable=wrong-arg-types  # jax-ndarray\n    outputs = jnp.concatenate([inputs, outputs], axis=-1)\n    return outputs, state",
  "def policy_head_apply(self, params: MPONetworkParams,\n                        obs_embedding: types.ObservationEmbedding):\n    return self.policy_head.apply(params.policy_head, obs_embedding)",
  "def critic_head_apply(self, params: MPONetworkParams,\n                        obs_embedding: types.ObservationEmbedding,\n                        actions: types.Action):\n    return self.critic_head.apply(params.critic_head, obs_embedding, actions)",
  "def torso_unroll(self, params: MPONetworkParams,\n                   observations: types.Observation, state: hk.LSTMState):\n    return self.torso.unroll(params.torso, observations, state)",
  "def dynamics_model_unroll(self, params: MPONetworkParams,\n                            actions: types.Action, state: hk.LSTMState):\n    return self.dynamics_model.unroll(params.dynamics_model, actions, state)",
  "def default_initial_state_fn(batch_size: Optional[int] = None) -> jnp.ndarray:\n    return make_core_module().initial_state(batch_size)",
  "def _apply_core_fn(observation: types.Observation,\n                     state: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:\n    if make_feedforward_module:\n      observation = make_feedforward_module()(observation)\n    return make_core_module()(observation, state)",
  "def _unroll_core_fn(observation: types.Observation,\n                      state: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:\n    if make_feedforward_module:\n      observation = make_feedforward_module()(observation)\n    return hk.dynamic_unroll(make_core_module(), observation, state)",
  "def policy_fn(observation: types.NestedArray) -> tfd.Distribution:\n    embedding = networks_lib.LayerNormMLP(\n        policy_layer_sizes, activate_final=True)(\n            observation)\n    return networks_lib.MultivariateNormalDiagHead(\n        num_dimensions, init_scale=policy_init_scale)(\n            embedding)",
  "def critic_fn(observation: types.NestedArray,\n                action: types.NestedArray) -> DistributionOrArray:\n    # Action is clipped to avoid critic extrapolations outside the spec range.\n    clipped_action = networks_lib.ClipToSpec(environment_spec.actions)(action)\n    inputs = jnp.concatenate([observation, clipped_action], axis=-1)\n    embedding = networks_lib.LayerNormMLP(\n        critic_layer_sizes, activate_final=True)(\n            inputs)\n\n    if critic_type == types.CriticType.MIXTURE_OF_GAUSSIANS:\n      return networks_lib.GaussianMixture(\n          num_dimensions=1,\n          num_components=mog_num_components,\n          multivariate=False,\n          init_scale=mog_init_scale,\n          append_singleton_event_dim=False,\n          reinterpreted_batch_ndims=0)(\n              embedding)\n    elif critic_type in (types.CriticType.CATEGORICAL,\n                         types.CriticType.CATEGORICAL_2HOT):\n      return networks_lib.CategoricalCriticHead(\n          num_bins=categorical_num_bins, vmin=vmin, vmax=vmax)(\n              embedding)\n    else:\n      return hk.Linear(\n          output_size=1, w_init=hk_init.TruncatedNormal(0.01))(\n              embedding)",
  "def __init__(self,\n               hidden_size: int,\n               hidden_scale: float = 1e-2,\n               name: Optional[str] = None):\n    \"\"\"Constructs a vanilla RNN core.\n\n    Args:\n      hidden_size: Hidden layer size.\n      hidden_scale: Scalar multiplying the hidden-to-hidden matmul.\n      name: Name of the module.\n    \"\"\"\n    super().__init__(name=name)\n    self._initial_state = jnp.zeros([hidden_size])\n    self._hidden_scale = hidden_scale\n    self._input_to_hidden = hk.Linear(hidden_size)\n    self._hidden_to_hidden = hk.Linear(\n        hidden_size, with_bias=True, w_init=w_init_identity)",
  "def __call__(self, inputs: jnp.ndarray, prev_state: jnp.ndarray):\n    out = jax.nn.relu(\n        self._input_to_hidden(inputs) +\n        self._hidden_scale * self._hidden_to_hidden(prev_state))\n    return out, out",
  "def initial_state(self, batch_size: Optional[int]):\n    state = self._initial_state\n    if batch_size is not None:\n      state = add_batch(state, batch_size)\n    return state",
  "def __init__(self, hidden_size: int, name: Optional[str] = None):\n\n    def b_init(unused_size: Sequence[int], dtype) -> jnp.ndarray:\n      \"\"\"Initializes the biases so the GRU ignores the state and acts as a tanh.\"\"\"\n      return jnp.concatenate([\n          +2 * jnp.ones([hidden_size], dtype=dtype),\n          -2 * jnp.ones([hidden_size], dtype=dtype),\n          jnp.zeros([hidden_size], dtype=dtype)\n      ])\n\n    super().__init__(hidden_size=hidden_size, b_init=b_init, name=name)",
  "def __call__(self, inputs: jnp.ndarray, prev_state: jnp.ndarray):\n    outputs, state = super().__call__(inputs, prev_state)\n    outputs = jnp.concatenate([inputs, outputs], axis=-1)\n    return outputs, state",
  "def __call__(self, inputs: jnp.ndarray, state: jnp.ndarray):\n    outputs, state = super().__call__(inputs, state)  # pytype: disable=wrong-arg-types  # jax-ndarray\n    outputs = jnp.concatenate([inputs, outputs], axis=-1)\n    return outputs, state",
  "def b_init(unused_size: Sequence[int], dtype) -> jnp.ndarray:\n      \"\"\"Initializes the biases so the GRU ignores the state and acts as a tanh.\"\"\"\n      return jnp.concatenate([\n          +2 * jnp.ones([hidden_size], dtype=dtype),\n          -2 * jnp.ones([hidden_size], dtype=dtype),\n          jnp.zeros([hidden_size], dtype=dtype)\n      ])",
  "class FromTransitions:\n  \"\"\"Configuration for learning from n-step transitions.\"\"\"\n  n_step: int = 1",
  "class FromSequences:\n  \"\"\"Configuration for learning from sequences.\"\"\"\n  sequence_length: int = 2\n  sequence_period: int = 1\n  # Configuration of how to bootstrap from these sequences.\n  n_step: Optional[int] = 5\n  # Lambda used to discount future rewards as in TD(lambda), Retrace, etc.\n  td_lambda: Optional[float] = 1.0",
  "class CriticType(enum.Enum):\n  \"\"\"Types of critic that are supported.\"\"\"\n  NONDISTRIBUTIONAL = 'nondistributional'\n  MIXTURE_OF_GAUSSIANS = 'mixture_of_gaussians'\n  CATEGORICAL_2HOT = 'categorical_2hot'\n  CATEGORICAL = 'categorical'",
  "class RnnCoreType(enum.Enum):\n  \"\"\"Types of core that are supported for rnn.\"\"\"\n  IDENTITY = 'identity'\n  GRU = 'gru'",
  "class GaussianPolicyLossConfig:\n  \"\"\"Configuration for the continuous (Gaussian) policy loss.\"\"\"\n  epsilon: float = 0.1\n  epsilon_penalty: float = 0.001\n  epsilon_mean: float = 0.0025\n  epsilon_stddev: float = 1e-6\n  init_log_temperature: float = 10.\n  init_log_alpha_mean: float = 10.\n  init_log_alpha_stddev: float = 1000.\n  action_penalization: bool = True\n  per_dim_constraining: bool = True",
  "class CategoricalPolicyLossConfig:\n  \"\"\"Configuration for the discrete (categorical) policy loss.\"\"\"\n  epsilon: float = 0.1\n  epsilon_policy: float = 0.0025\n  init_log_temperature: float = 3.\n  init_log_alpha: float = 3.",
  "class RolloutLossScalesConfig:\n  \"\"\"Configuration for scaling the rollout losses used in the learner.\"\"\"\n  policy: float = 1.0\n  bc_policy: float = 1.0\n  critic: float = 1.0\n  reward: float = 1.0",
  "class LossScalesConfig:\n  \"\"\"Configuration for scaling the rollout losses used in the learner.\"\"\"\n  policy: float = 1.0\n  critic: float = 1.0\n  rollout: Optional[RolloutLossScalesConfig] = None",
  "class ModelOutputs:\n  \"\"\"Container for the outputs of the model.\"\"\"\n  policy: Optional[types.NestedArray] = None\n  q_value: Optional[types.NestedArray] = None\n  value: Optional[types.NestedArray] = None\n  reward: Optional[types.NestedArray] = None\n  embedding: Optional[types.NestedArray] = None\n  recurrent_state: Optional[types.NestedArray] = None",
  "class LossTargets:\n  \"\"\"Container for the targets used to compute the model loss.\"\"\"\n  # Policy targets.\n  policy: Optional[types.NestedArray] = None\n  a_improvement: Optional[types.NestedArray] = None\n  q_improvement: Optional[types.NestedArray] = None\n\n  # Value targets.\n  q_value: Optional[types.NestedArray] = None\n  value: Optional[types.NestedArray] = None\n  reward: Optional[types.NestedArray] = None\n\n  embedding: Optional[types.NestedArray] = None",
  "class MPOConfig:\n  \"\"\"MPO agent configuration.\"\"\"\n\n  batch_size: int = 256  # Total batch size across all learner devices.\n  discount: float = 0.99\n  discrete_policy: bool = False\n\n  # Specification of the type of experience the learner will consume.\n  experience_type: mpo_types.ExperienceType = dataclasses.field(\n      default_factory=lambda: mpo_types.FromTransitions(n_step=5)\n  )\n  num_stacked_observations: int = 1\n  # Optional data-augmentation transformation for observations.\n  observation_transform: Optional[Callable[[types.NestedTensor],\n                                           types.NestedTensor]] = None\n\n  # Specification of replay, e.g., min/max size, pure or mixed.\n  # NOTE: When replay_fraction = 1.0, this reverts to pure replay and the online\n  # queue is not created.\n  replay_fraction: float = 1.0  # Fraction of replay data (vs online) per batch.\n  samples_per_insert: Optional[float] = 32.0\n  min_replay_size: int = 1_000\n  max_replay_size: int = 1_000_000\n  online_queue_capacity: int = 0  # If not set, will use 4 * online_batch_size.\n\n  # Critic training configuration.\n  critic_type: mpo_types.CriticType = mpo_types.CriticType.MIXTURE_OF_GAUSSIANS\n  value_tx_pair: rlax.TxPair = rlax.IDENTITY_PAIR\n  use_retrace: bool = False\n  retrace_lambda: float = 0.95\n  reward_clip: float = np.float32('inf')  # pytype: disable=annotation-type-mismatch  # numpy-scalars\n  use_online_policy_to_bootstrap: bool = False\n  use_stale_state: bool = False\n\n  # Policy training configuration.\n  num_samples: int = 20  # Number of MPO action samples.\n  policy_loss_config: Optional[mpo_types.PolicyLossConfig] = None\n  policy_eval_stochastic: bool = True\n  policy_eval_num_val_samples: int = 128\n\n  # Optimizer configuration.\n  learning_rate: Union[float, Callable[[int], float]] = 1e-4\n  dual_learning_rate: Union[float, Callable[[int], float]] = 1e-2\n  grad_norm_clip: float = 40.\n  adam_b1: float = 0.9\n  adam_b2: float = 0.999\n  weight_decay: float = 0.0\n  use_cosine_lr_decay: bool = False\n  cosine_lr_decay_warmup_steps: int = 3000\n\n  # Set the target update period or rate depending on whether you want a\n  # periodic or incremental (exponential weighted average) target update.\n  # Exactly one must be specified (not None).\n  target_update_period: Optional[int] = 100\n  target_update_rate: Optional[float] = None\n  variable_update_period: int = 1000\n\n  # Configuring the mixture of policy and critic losses.\n  policy_loss_scale: float = 1.0\n  critic_loss_scale: float = 1.0\n\n  # Optional roll-out loss configuration (off by default).\n  model_rollout_length: int = 0\n  rollout_policy_loss_scale: float = 1.0\n  rollout_bc_policy_loss_scale: float = 1.0\n  rollout_critic_loss_scale: float = 1.0\n  rollout_reward_loss_scale: float = 1.0\n\n  jit_learner: bool = True\n\n  def __post_init__(self):\n    if ((self.target_update_period and self.target_update_rate) or\n        (self.target_update_period is None and\n         self.target_update_rate is None)):\n      raise ValueError(\n          'Exactly one of target_update_{period|rate} must be set.'\n          f' Received target_update_period={self.target_update_period} and'\n          f' target_update_rate={self.target_update_rate}.')\n\n    online_batch_size = int(self.batch_size * (1. - self.replay_fraction))\n    if not self.online_queue_capacity:\n      # Note: larger capacities mean the online data is more \"stale\". This seems\n      # a reasonable default for now.\n      self.online_queue_capacity = int(4 * online_batch_size)\n    self.online_queue_capacity = max(self.online_queue_capacity,\n                                     online_batch_size + 1)\n\n    if self.samples_per_insert is not None and self.replay_fraction < 1:\n      raise ValueError(\n          'Cannot set samples_per_insert when using a mixed replay (i.e when '\n          '0 < replay_fraction < 1). Received:\\n'\n          f'\\tsamples_per_insert={self.samples_per_insert} and\\n'\n          f'\\treplay_fraction={self.replay_fraction}.')\n\n    if (0 < self.replay_fraction < 1 and\n        self.min_replay_size > self.online_queue_capacity):\n      raise ValueError('When mixing replay with an online queue, min replay '\n                       'size must not be larger than the queue capacity.')\n\n    if (isinstance(self.experience_type, mpo_types.FromTransitions) and\n        self.num_stacked_observations > 1):\n      raise ValueError(\n          'Agent-side frame-stacking is currently only supported when learning '\n          'from sequences. Consider environment-side frame-stacking instead.')\n\n    if self.critic_type == mpo_types.CriticType.CATEGORICAL:\n      if self.model_rollout_length > 0:\n        raise ValueError(\n            'Model rollouts are not supported for the Categorical critic')\n      if not isinstance(self.experience_type, mpo_types.FromTransitions):\n        raise ValueError(\n            'Categorical critic only supports experience_type=FromTransitions')\n      if self.use_retrace:\n        raise ValueError('retrace is not supported for the Categorical critic')\n\n    if self.model_rollout_length > 0 and not self.discrete_policy:\n      if (self.rollout_policy_loss_scale or self.rollout_bc_policy_loss_scale):\n        raise ValueError('Policy rollout losses are only supported in the '\n                         'discrete policy case.')",
  "def _compute_spi_from_replay_fraction(replay_fraction: float) -> float:\n  \"\"\"Computes an estimated samples_per_insert from a replay_fraction.\n\n  Assumes actors simultaneously add to both the queue and replay in a mixed\n  replay setup. Since the online queue sets samples_per_insert = 1, then the\n  total SPI can be calculated as:\n\n    SPI = B / O = O / (1 - f) / O = 1 / (1 - f).\n\n  Key:\n    B: total batch size\n    O: online batch size\n    f: replay fraction.\n\n  Args:\n    replay_fraction: fraction of a batch size taken from replay (as opposed to\n      the queue of online experience) in a mixed replay setting.\n\n  Returns:\n    An estimate of the samples_per_insert value to produce comparable runs in\n    the pure replay setting.\n  \"\"\"\n  return 1 / (1 - replay_fraction)",
  "def _compute_num_inserts_per_actor_step(samples_per_insert: float,\n                                        batch_size: int,\n                                        sequence_period: int = 1) -> float:\n  \"\"\"Estimate the number inserts per actor steps.\"\"\"\n  return sequence_period * batch_size / samples_per_insert",
  "def __post_init__(self):\n    if ((self.target_update_period and self.target_update_rate) or\n        (self.target_update_period is None and\n         self.target_update_rate is None)):\n      raise ValueError(\n          'Exactly one of target_update_{period|rate} must be set.'\n          f' Received target_update_period={self.target_update_period} and'\n          f' target_update_rate={self.target_update_rate}.')\n\n    online_batch_size = int(self.batch_size * (1. - self.replay_fraction))\n    if not self.online_queue_capacity:\n      # Note: larger capacities mean the online data is more \"stale\". This seems\n      # a reasonable default for now.\n      self.online_queue_capacity = int(4 * online_batch_size)\n    self.online_queue_capacity = max(self.online_queue_capacity,\n                                     online_batch_size + 1)\n\n    if self.samples_per_insert is not None and self.replay_fraction < 1:\n      raise ValueError(\n          'Cannot set samples_per_insert when using a mixed replay (i.e when '\n          '0 < replay_fraction < 1). Received:\\n'\n          f'\\tsamples_per_insert={self.samples_per_insert} and\\n'\n          f'\\treplay_fraction={self.replay_fraction}.')\n\n    if (0 < self.replay_fraction < 1 and\n        self.min_replay_size > self.online_queue_capacity):\n      raise ValueError('When mixing replay with an online queue, min replay '\n                       'size must not be larger than the queue capacity.')\n\n    if (isinstance(self.experience_type, mpo_types.FromTransitions) and\n        self.num_stacked_observations > 1):\n      raise ValueError(\n          'Agent-side frame-stacking is currently only supported when learning '\n          'from sequences. Consider environment-side frame-stacking instead.')\n\n    if self.critic_type == mpo_types.CriticType.CATEGORICAL:\n      if self.model_rollout_length > 0:\n        raise ValueError(\n            'Model rollouts are not supported for the Categorical critic')\n      if not isinstance(self.experience_type, mpo_types.FromTransitions):\n        raise ValueError(\n            'Categorical critic only supports experience_type=FromTransitions')\n      if self.use_retrace:\n        raise ValueError('retrace is not supported for the Categorical critic')\n\n    if self.model_rollout_length > 0 and not self.discrete_policy:\n      if (self.rollout_policy_loss_scale or self.rollout_bc_policy_loss_scale):\n        raise ValueError('Policy rollout losses are only supported in the '\n                         'discrete policy case.')",
  "class ActorState(NamedTuple):\n  key: jax_types.PRNGKey\n  core_state: hk.LSTMState\n  prev_core_state: hk.LSTMState\n  log_prob: Union[jnp.ndarray, Tuple[()]] = ()",
  "def make_actor_core(mpo_networks: networks.MPONetworks,\n                    stochastic: bool = True,\n                    store_core_state: bool = False,\n                    store_log_prob: bool = True) -> actor_core_lib.ActorCore:\n  \"\"\"Returns a MPO ActorCore from the MPONetworks.\"\"\"\n\n  def init(key: jax_types.PRNGKey) -> ActorState:\n    next_key, key = jax.random.split(key, 2)\n    batch_size = None\n    params_initial_state = mpo_networks.torso.initial_state_fn_init(\n        key, batch_size)\n    core_state = mpo_networks.torso.initial_state_fn(params_initial_state,\n                                                     batch_size)\n    return ActorState(\n        key=next_key,\n        core_state=core_state,\n        prev_core_state=core_state,\n        log_prob=np.zeros(shape=(), dtype=np.float32) if store_log_prob else ())\n\n  def select_action(params: networks.MPONetworkParams,\n                    observations: types.Observation,\n                    state: ActorState) -> Tuple[types.Action, ActorState]:\n\n    next_key, key = jax.random.split(state.key, 2)\n\n    # Embed observations and apply stateful core (e.g. recurrent, transformer).\n    embeddings, core_state = mpo_networks.torso.apply(params.torso,\n                                                      observations,\n                                                      state.core_state)\n\n    # Get the action distribution for these observations.\n    policy = mpo_networks.policy_head_apply(params, embeddings)\n    actions = policy.sample(seed=key) if stochastic else policy.mode()\n\n    return actions, ActorState(\n        key=next_key,\n        core_state=core_state,\n        prev_core_state=state.core_state,\n        # Compute log-probabilities for use in off-policy correction schemes.\n        log_prob=policy.log_prob(actions) if store_log_prob else ())\n\n  def get_extras(state: ActorState) -> Mapping[str, jnp.ndarray]:\n    extras = {}\n\n    if store_core_state:\n      extras['core_state'] = state.prev_core_state\n\n    if store_log_prob:\n      extras['log_prob'] = state.log_prob\n\n    return extras  # pytype: disable=bad-return-type  # jax-ndarray\n\n  return actor_core_lib.ActorCore(\n      init=init, select_action=select_action, get_extras=get_extras)",
  "def init(key: jax_types.PRNGKey) -> ActorState:\n    next_key, key = jax.random.split(key, 2)\n    batch_size = None\n    params_initial_state = mpo_networks.torso.initial_state_fn_init(\n        key, batch_size)\n    core_state = mpo_networks.torso.initial_state_fn(params_initial_state,\n                                                     batch_size)\n    return ActorState(\n        key=next_key,\n        core_state=core_state,\n        prev_core_state=core_state,\n        log_prob=np.zeros(shape=(), dtype=np.float32) if store_log_prob else ())",
  "def select_action(params: networks.MPONetworkParams,\n                    observations: types.Observation,\n                    state: ActorState) -> Tuple[types.Action, ActorState]:\n\n    next_key, key = jax.random.split(state.key, 2)\n\n    # Embed observations and apply stateful core (e.g. recurrent, transformer).\n    embeddings, core_state = mpo_networks.torso.apply(params.torso,\n                                                      observations,\n                                                      state.core_state)\n\n    # Get the action distribution for these observations.\n    policy = mpo_networks.policy_head_apply(params, embeddings)\n    actions = policy.sample(seed=key) if stochastic else policy.mode()\n\n    return actions, ActorState(\n        key=next_key,\n        core_state=core_state,\n        prev_core_state=state.core_state,\n        # Compute log-probabilities for use in off-policy correction schemes.\n        log_prob=policy.log_prob(actions) if store_log_prob else ())",
  "def get_extras(state: ActorState) -> Mapping[str, jnp.ndarray]:\n    extras = {}\n\n    if store_core_state:\n      extras['core_state'] = state.prev_core_state\n\n    if store_log_prob:\n      extras['log_prob'] = state.log_prob\n\n    return extras",
  "class MPOBuilder(builders.ActorLearnerBuilder):\n  \"\"\"Builder class for MPO agent components.\"\"\"\n\n  def __init__(self,\n               config: mpo_config.MPOConfig,\n               *,\n               sgd_steps_per_learner_step: int = 8,\n               max_learner_steps: Optional[int] = None):\n    self.config = config\n    self.sgd_steps_per_learner_step = sgd_steps_per_learner_step\n    self._max_learner_steps = max_learner_steps\n\n  def make_policy(\n      self,\n      networks: mpo_networks.MPONetworks,\n      environment_spec: specs.EnvironmentSpec,\n      evaluation: bool = False,\n  ) -> actor_core_lib.ActorCore:\n    actor_core = acting.make_actor_core(\n        networks,\n        stochastic=not evaluation,\n        store_core_state=self.config.use_stale_state,\n        store_log_prob=self.config.use_retrace)\n\n    # Maybe wrap the actor core to perform actor-side observation stacking.\n    if self.config.num_stacked_observations > 1:\n      actor_core = obs_stacking.wrap_actor_core(\n          actor_core,\n          observation_spec=environment_spec.observations,\n          num_stacked_observations=self.config.num_stacked_observations)\n\n    return actor_core\n\n  def make_actor(\n      self,\n      random_key: jax_types.PRNGKey,\n      policy: actor_core_lib.ActorCore,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[base.Adder] = None,\n  ) -> core.Actor:\n\n    del environment_spec  # This actor doesn't need the spec beyond the policy.\n    variable_client = variable_utils.VariableClient(\n        client=variable_source,\n        key=_POLICY_KEY,\n        update_period=self.config.variable_update_period)\n\n    return actors.GenericActor(\n        actor=policy,\n        random_key=random_key,\n        variable_client=variable_client,\n        adder=adder,\n        backend='cpu')\n\n  def make_learner(self,\n                   random_key: jax_types.PRNGKey,\n                   networks: mpo_networks.MPONetworks,\n                   dataset: Iterator[reverb.ReplaySample],\n                   logger_fn: loggers.LoggerFactory,\n                   environment_spec: specs.EnvironmentSpec,\n                   replay_client: Optional[reverb.Client] = None,\n                   counter: Optional[counting.Counter] = None) -> core.Learner:\n    # Set defaults.\n    del replay_client  # Unused as we do not update priorities.\n    learning_rate = self.config.learning_rate\n\n    # Make sure we can split the batches evenly across all accelerator devices.\n    num_learner_devices = jax.device_count()\n    if self.config.batch_size % num_learner_devices > 0:\n      raise ValueError(\n          'Batch size must divide evenly by the number of learner devices.'\n          f' Passed a batch size of {self.config.batch_size} and the number of'\n          f' available learner devices is {num_learner_devices}. Specifically,'\n          f' devices: {jax.devices()}.')\n\n    agent_environment_spec = environment_spec\n    if self.config.num_stacked_observations > 1:\n      # Adjust the observation spec for the agent-side frame-stacking.\n      # Note: this is only for the ActorCore's benefit, the adders want the true\n      # environment spec.\n      agent_environment_spec = obs_stacking.get_adjusted_environment_spec(\n          agent_environment_spec, self.config.num_stacked_observations)\n\n    if self.config.use_cosine_lr_decay:\n      learning_rate = optax.warmup_cosine_decay_schedule(\n          init_value=0.,\n          peak_value=self.config.learning_rate,\n          warmup_steps=self.config.cosine_lr_decay_warmup_steps,\n          decay_steps=self._max_learner_steps)\n\n    optimizer = optax.adamw(\n        learning_rate,\n        b1=self.config.adam_b1,\n        b2=self.config.adam_b2,\n        weight_decay=self.config.weight_decay)\n    # TODO(abef): move LR scheduling and optimizer creation into launcher.\n\n    loss_scales_config = mpo_types.LossScalesConfig(\n        policy=self.config.policy_loss_scale,\n        critic=self.config.critic_loss_scale,\n        rollout=mpo_types.RolloutLossScalesConfig(\n            policy=self.config.rollout_policy_loss_scale,\n            bc_policy=self.config.rollout_bc_policy_loss_scale,\n            critic=self.config.rollout_critic_loss_scale,\n            reward=self.config.rollout_reward_loss_scale,\n        ))\n\n    logger = logger_fn(\n        'learner',\n        steps_key=counter.get_steps_key() if counter else 'learner_steps')\n\n    with chex.fake_pmap_and_jit(not self.config.jit_learner,\n                                not self.config.jit_learner):\n      learner = learning.MPOLearner(\n          iterator=dataset,\n          networks=networks,\n          environment_spec=agent_environment_spec,\n          critic_type=self.config.critic_type,\n          discrete_policy=self.config.discrete_policy,\n          random_key=random_key,\n          discount=self.config.discount,\n          num_samples=self.config.num_samples,\n          policy_eval_stochastic=self.config.policy_eval_stochastic,\n          policy_eval_num_val_samples=self.config.policy_eval_num_val_samples,\n          policy_loss_config=self.config.policy_loss_config,\n          loss_scales=loss_scales_config,\n          target_update_period=self.config.target_update_period,\n          target_update_rate=self.config.target_update_rate,\n          experience_type=self.config.experience_type,\n          use_online_policy_to_bootstrap=(\n              self.config.use_online_policy_to_bootstrap),\n          use_stale_state=self.config.use_stale_state,\n          use_retrace=self.config.use_retrace,\n          retrace_lambda=self.config.retrace_lambda,\n          model_rollout_length=self.config.model_rollout_length,\n          sgd_steps_per_learner_step=self.sgd_steps_per_learner_step,\n          optimizer=optimizer,\n          dual_optimizer=optax.adam(self.config.dual_learning_rate),\n          grad_norm_clip=self.config.grad_norm_clip,\n          reward_clip=self.config.reward_clip,\n          value_tx_pair=self.config.value_tx_pair,\n          counter=counter,\n          logger=logger,\n          devices=jax.devices(),\n      )\n    return learner\n\n  def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: actor_core_lib.ActorCore,  # Used to get accurate extras_spec.\n  ) -> List[reverb.Table]:\n    dummy_actor_state = policy.init(jax.random.PRNGKey(0))\n    extras_spec = policy.get_extras(dummy_actor_state)\n\n    if isinstance(self.config.experience_type, mpo_types.FromTransitions):\n      signature = adders.NStepTransitionAdder.signature(environment_spec,\n                                                        extras_spec)\n    elif isinstance(self.config.experience_type, mpo_types.FromSequences):\n      sequence_length = (\n          self.config.experience_type.sequence_length +\n          self.config.num_stacked_observations - 1)\n      signature = adders.SequenceAdder.signature(\n          environment_spec, extras_spec, sequence_length=sequence_length)\n    # TODO(bshahr): This way of obtaining the signature is error-prone. Find a\n    # programmatic way via make_adder.\n\n    # Create the rate limiter.\n    if self.config.samples_per_insert:\n      # Create enough of an error buffer to give a 10% tolerance in rate.\n      samples_per_insert_tolerance = 0.1 * self.config.samples_per_insert\n      error_buffer = self.config.min_replay_size * samples_per_insert_tolerance\n      limiter = reverb.rate_limiters.SampleToInsertRatio(\n          min_size_to_sample=self.config.min_replay_size,\n          samples_per_insert=self.config.samples_per_insert,\n          error_buffer=max(error_buffer, 2 * self.config.samples_per_insert))\n    else:\n      limiter = reverb.rate_limiters.MinSize(self.config.min_replay_size)\n\n    # Reverb loves Acme.\n    replay_extensions = []\n    queue_extensions = []\n\n\n    # Create replay tables.\n    tables = []\n    if self.config.replay_fraction > 0:\n      replay_table = reverb.Table(\n          name=adders.DEFAULT_PRIORITY_TABLE,\n          sampler=reverb.selectors.Uniform(),\n          remover=reverb.selectors.Fifo(),\n          max_size=self.config.max_replay_size,\n          rate_limiter=limiter,\n          extensions=replay_extensions,\n          signature=signature)\n      tables.append(replay_table)\n      logging.info(\n          'Creating off-policy replay buffer with replay fraction %g '\n          'of batch %d', self.config.replay_fraction, self.config.batch_size)\n\n    if self.config.replay_fraction < 1:\n      # Create a FIFO queue. This will provide the rate limitation if used.\n      queue = reverb.Table.queue(\n          name=_QUEUE_TABLE_NAME,\n          max_size=self.config.online_queue_capacity,\n          extensions=queue_extensions,\n          signature=signature)\n      tables.append(queue)\n      logging.info(\n          'Creating online replay queue with queue fraction %g '\n          'of batch %d', 1.0 - self.config.replay_fraction,\n          self.config.batch_size)\n\n    return tables\n\n  def make_adder(\n      self,\n      replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[actor_core_lib.ActorCore],\n  ) -> Optional[base.Adder]:\n    del environment_spec, policy\n    # Specify the tables to insert into but don't use prioritization.\n    priority_fns = {}\n    if self.config.replay_fraction > 0:\n      priority_fns[adders.DEFAULT_PRIORITY_TABLE] = None\n    if self.config.replay_fraction < 1:\n      priority_fns[_QUEUE_TABLE_NAME] = None\n\n    if isinstance(self.config.experience_type, mpo_types.FromTransitions):\n      return adders.NStepTransitionAdder(\n          client=replay_client,\n          n_step=self.config.experience_type.n_step,\n          discount=self.config.discount,\n          priority_fns=priority_fns)\n    elif isinstance(self.config.experience_type, mpo_types.FromSequences):\n      sequence_length = (\n          self.config.experience_type.sequence_length +\n          self.config.num_stacked_observations - 1)\n      return adders.SequenceAdder(\n          client=replay_client,\n          sequence_length=sequence_length,\n          period=self.config.experience_type.sequence_period,\n          end_of_episode_behavior=adders.EndBehavior.WRITE,\n          max_in_flight_items=1,\n          priority_fns=priority_fns)\n\n  def make_dataset_iterator(\n      self, replay_client: reverb.Client) -> Iterator[reverb.ReplaySample]:\n\n    if self.config.num_stacked_observations > 1:\n      maybe_stack_observations = functools.partial(\n          obs_stacking.stack_reverb_observation,\n          stack_size=self.config.num_stacked_observations)\n    else:\n      maybe_stack_observations = None\n\n    dataset = datasets.make_reverb_dataset(\n        server_address=replay_client.server_address,\n        batch_size=self.config.batch_size // jax.device_count(),\n        table={\n            adders.DEFAULT_PRIORITY_TABLE: self.config.replay_fraction,\n            _QUEUE_TABLE_NAME: 1. - self.config.replay_fraction,\n        },\n        num_parallel_calls=max(16, 4 * jax.local_device_count()),\n        max_in_flight_samples_per_worker=(2 * self.sgd_steps_per_learner_step *\n                                          self.config.batch_size //\n                                          jax.device_count()),\n        postprocess=maybe_stack_observations)\n\n    if self.config.observation_transform:\n      # Augment dataset with random translations, simulated by pad-and-crop.\n      transform = img_aug.make_transform(\n          observation_transform=self.config.observation_transform,\n          transform_next_observation=isinstance(self.config.experience_type,\n                                                mpo_types.FromTransitions))\n      dataset = dataset.map(\n          transform, num_parallel_calls=16, deterministic=False)\n\n    # Batch and then flatten to feed multiple SGD steps per learner step.\n    if self.sgd_steps_per_learner_step > 1:\n      dataset = dataset.batch(\n          self.sgd_steps_per_learner_step, drop_remainder=True)\n      batch_flatten = lambda t: tf.reshape(t, [-1] + t.shape[2:].as_list())\n      dataset = dataset.map(lambda x: tree.map_structure(batch_flatten, x))\n\n    return utils.multi_device_put(dataset.as_numpy_iterator(),\n                                  jax.local_devices())",
  "def __init__(self,\n               config: mpo_config.MPOConfig,\n               *,\n               sgd_steps_per_learner_step: int = 8,\n               max_learner_steps: Optional[int] = None):\n    self.config = config\n    self.sgd_steps_per_learner_step = sgd_steps_per_learner_step\n    self._max_learner_steps = max_learner_steps",
  "def make_policy(\n      self,\n      networks: mpo_networks.MPONetworks,\n      environment_spec: specs.EnvironmentSpec,\n      evaluation: bool = False,\n  ) -> actor_core_lib.ActorCore:\n    actor_core = acting.make_actor_core(\n        networks,\n        stochastic=not evaluation,\n        store_core_state=self.config.use_stale_state,\n        store_log_prob=self.config.use_retrace)\n\n    # Maybe wrap the actor core to perform actor-side observation stacking.\n    if self.config.num_stacked_observations > 1:\n      actor_core = obs_stacking.wrap_actor_core(\n          actor_core,\n          observation_spec=environment_spec.observations,\n          num_stacked_observations=self.config.num_stacked_observations)\n\n    return actor_core",
  "def make_actor(\n      self,\n      random_key: jax_types.PRNGKey,\n      policy: actor_core_lib.ActorCore,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[base.Adder] = None,\n  ) -> core.Actor:\n\n    del environment_spec  # This actor doesn't need the spec beyond the policy.\n    variable_client = variable_utils.VariableClient(\n        client=variable_source,\n        key=_POLICY_KEY,\n        update_period=self.config.variable_update_period)\n\n    return actors.GenericActor(\n        actor=policy,\n        random_key=random_key,\n        variable_client=variable_client,\n        adder=adder,\n        backend='cpu')",
  "def make_learner(self,\n                   random_key: jax_types.PRNGKey,\n                   networks: mpo_networks.MPONetworks,\n                   dataset: Iterator[reverb.ReplaySample],\n                   logger_fn: loggers.LoggerFactory,\n                   environment_spec: specs.EnvironmentSpec,\n                   replay_client: Optional[reverb.Client] = None,\n                   counter: Optional[counting.Counter] = None) -> core.Learner:\n    # Set defaults.\n    del replay_client  # Unused as we do not update priorities.\n    learning_rate = self.config.learning_rate\n\n    # Make sure we can split the batches evenly across all accelerator devices.\n    num_learner_devices = jax.device_count()\n    if self.config.batch_size % num_learner_devices > 0:\n      raise ValueError(\n          'Batch size must divide evenly by the number of learner devices.'\n          f' Passed a batch size of {self.config.batch_size} and the number of'\n          f' available learner devices is {num_learner_devices}. Specifically,'\n          f' devices: {jax.devices()}.')\n\n    agent_environment_spec = environment_spec\n    if self.config.num_stacked_observations > 1:\n      # Adjust the observation spec for the agent-side frame-stacking.\n      # Note: this is only for the ActorCore's benefit, the adders want the true\n      # environment spec.\n      agent_environment_spec = obs_stacking.get_adjusted_environment_spec(\n          agent_environment_spec, self.config.num_stacked_observations)\n\n    if self.config.use_cosine_lr_decay:\n      learning_rate = optax.warmup_cosine_decay_schedule(\n          init_value=0.,\n          peak_value=self.config.learning_rate,\n          warmup_steps=self.config.cosine_lr_decay_warmup_steps,\n          decay_steps=self._max_learner_steps)\n\n    optimizer = optax.adamw(\n        learning_rate,\n        b1=self.config.adam_b1,\n        b2=self.config.adam_b2,\n        weight_decay=self.config.weight_decay)\n    # TODO(abef): move LR scheduling and optimizer creation into launcher.\n\n    loss_scales_config = mpo_types.LossScalesConfig(\n        policy=self.config.policy_loss_scale,\n        critic=self.config.critic_loss_scale,\n        rollout=mpo_types.RolloutLossScalesConfig(\n            policy=self.config.rollout_policy_loss_scale,\n            bc_policy=self.config.rollout_bc_policy_loss_scale,\n            critic=self.config.rollout_critic_loss_scale,\n            reward=self.config.rollout_reward_loss_scale,\n        ))\n\n    logger = logger_fn(\n        'learner',\n        steps_key=counter.get_steps_key() if counter else 'learner_steps')\n\n    with chex.fake_pmap_and_jit(not self.config.jit_learner,\n                                not self.config.jit_learner):\n      learner = learning.MPOLearner(\n          iterator=dataset,\n          networks=networks,\n          environment_spec=agent_environment_spec,\n          critic_type=self.config.critic_type,\n          discrete_policy=self.config.discrete_policy,\n          random_key=random_key,\n          discount=self.config.discount,\n          num_samples=self.config.num_samples,\n          policy_eval_stochastic=self.config.policy_eval_stochastic,\n          policy_eval_num_val_samples=self.config.policy_eval_num_val_samples,\n          policy_loss_config=self.config.policy_loss_config,\n          loss_scales=loss_scales_config,\n          target_update_period=self.config.target_update_period,\n          target_update_rate=self.config.target_update_rate,\n          experience_type=self.config.experience_type,\n          use_online_policy_to_bootstrap=(\n              self.config.use_online_policy_to_bootstrap),\n          use_stale_state=self.config.use_stale_state,\n          use_retrace=self.config.use_retrace,\n          retrace_lambda=self.config.retrace_lambda,\n          model_rollout_length=self.config.model_rollout_length,\n          sgd_steps_per_learner_step=self.sgd_steps_per_learner_step,\n          optimizer=optimizer,\n          dual_optimizer=optax.adam(self.config.dual_learning_rate),\n          grad_norm_clip=self.config.grad_norm_clip,\n          reward_clip=self.config.reward_clip,\n          value_tx_pair=self.config.value_tx_pair,\n          counter=counter,\n          logger=logger,\n          devices=jax.devices(),\n      )\n    return learner",
  "def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: actor_core_lib.ActorCore,  # Used to get accurate extras_spec.\n  ) -> List[reverb.Table]:\n    dummy_actor_state = policy.init(jax.random.PRNGKey(0))\n    extras_spec = policy.get_extras(dummy_actor_state)\n\n    if isinstance(self.config.experience_type, mpo_types.FromTransitions):\n      signature = adders.NStepTransitionAdder.signature(environment_spec,\n                                                        extras_spec)\n    elif isinstance(self.config.experience_type, mpo_types.FromSequences):\n      sequence_length = (\n          self.config.experience_type.sequence_length +\n          self.config.num_stacked_observations - 1)\n      signature = adders.SequenceAdder.signature(\n          environment_spec, extras_spec, sequence_length=sequence_length)\n    # TODO(bshahr): This way of obtaining the signature is error-prone. Find a\n    # programmatic way via make_adder.\n\n    # Create the rate limiter.\n    if self.config.samples_per_insert:\n      # Create enough of an error buffer to give a 10% tolerance in rate.\n      samples_per_insert_tolerance = 0.1 * self.config.samples_per_insert\n      error_buffer = self.config.min_replay_size * samples_per_insert_tolerance\n      limiter = reverb.rate_limiters.SampleToInsertRatio(\n          min_size_to_sample=self.config.min_replay_size,\n          samples_per_insert=self.config.samples_per_insert,\n          error_buffer=max(error_buffer, 2 * self.config.samples_per_insert))\n    else:\n      limiter = reverb.rate_limiters.MinSize(self.config.min_replay_size)\n\n    # Reverb loves Acme.\n    replay_extensions = []\n    queue_extensions = []\n\n\n    # Create replay tables.\n    tables = []\n    if self.config.replay_fraction > 0:\n      replay_table = reverb.Table(\n          name=adders.DEFAULT_PRIORITY_TABLE,\n          sampler=reverb.selectors.Uniform(),\n          remover=reverb.selectors.Fifo(),\n          max_size=self.config.max_replay_size,\n          rate_limiter=limiter,\n          extensions=replay_extensions,\n          signature=signature)\n      tables.append(replay_table)\n      logging.info(\n          'Creating off-policy replay buffer with replay fraction %g '\n          'of batch %d', self.config.replay_fraction, self.config.batch_size)\n\n    if self.config.replay_fraction < 1:\n      # Create a FIFO queue. This will provide the rate limitation if used.\n      queue = reverb.Table.queue(\n          name=_QUEUE_TABLE_NAME,\n          max_size=self.config.online_queue_capacity,\n          extensions=queue_extensions,\n          signature=signature)\n      tables.append(queue)\n      logging.info(\n          'Creating online replay queue with queue fraction %g '\n          'of batch %d', 1.0 - self.config.replay_fraction,\n          self.config.batch_size)\n\n    return tables",
  "def make_adder(\n      self,\n      replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[actor_core_lib.ActorCore],\n  ) -> Optional[base.Adder]:\n    del environment_spec, policy\n    # Specify the tables to insert into but don't use prioritization.\n    priority_fns = {}\n    if self.config.replay_fraction > 0:\n      priority_fns[adders.DEFAULT_PRIORITY_TABLE] = None\n    if self.config.replay_fraction < 1:\n      priority_fns[_QUEUE_TABLE_NAME] = None\n\n    if isinstance(self.config.experience_type, mpo_types.FromTransitions):\n      return adders.NStepTransitionAdder(\n          client=replay_client,\n          n_step=self.config.experience_type.n_step,\n          discount=self.config.discount,\n          priority_fns=priority_fns)\n    elif isinstance(self.config.experience_type, mpo_types.FromSequences):\n      sequence_length = (\n          self.config.experience_type.sequence_length +\n          self.config.num_stacked_observations - 1)\n      return adders.SequenceAdder(\n          client=replay_client,\n          sequence_length=sequence_length,\n          period=self.config.experience_type.sequence_period,\n          end_of_episode_behavior=adders.EndBehavior.WRITE,\n          max_in_flight_items=1,\n          priority_fns=priority_fns)",
  "def make_dataset_iterator(\n      self, replay_client: reverb.Client) -> Iterator[reverb.ReplaySample]:\n\n    if self.config.num_stacked_observations > 1:\n      maybe_stack_observations = functools.partial(\n          obs_stacking.stack_reverb_observation,\n          stack_size=self.config.num_stacked_observations)\n    else:\n      maybe_stack_observations = None\n\n    dataset = datasets.make_reverb_dataset(\n        server_address=replay_client.server_address,\n        batch_size=self.config.batch_size // jax.device_count(),\n        table={\n            adders.DEFAULT_PRIORITY_TABLE: self.config.replay_fraction,\n            _QUEUE_TABLE_NAME: 1. - self.config.replay_fraction,\n        },\n        num_parallel_calls=max(16, 4 * jax.local_device_count()),\n        max_in_flight_samples_per_worker=(2 * self.sgd_steps_per_learner_step *\n                                          self.config.batch_size //\n                                          jax.device_count()),\n        postprocess=maybe_stack_observations)\n\n    if self.config.observation_transform:\n      # Augment dataset with random translations, simulated by pad-and-crop.\n      transform = img_aug.make_transform(\n          observation_transform=self.config.observation_transform,\n          transform_next_observation=isinstance(self.config.experience_type,\n                                                mpo_types.FromTransitions))\n      dataset = dataset.map(\n          transform, num_parallel_calls=16, deterministic=False)\n\n    # Batch and then flatten to feed multiple SGD steps per learner step.\n    if self.sgd_steps_per_learner_step > 1:\n      dataset = dataset.batch(\n          self.sgd_steps_per_learner_step, drop_remainder=True)\n      batch_flatten = lambda t: tf.reshape(t, [-1] + t.shape[2:].as_list())\n      dataset = dataset.map(lambda x: tree.map_structure(batch_flatten, x))\n\n    return utils.multi_device_put(dataset.as_numpy_iterator(),\n                                  jax.local_devices())",
  "class CategoricalMPOParams(NamedTuple):\n  \"\"\"NamedTuple to store trainable loss parameters.\"\"\"\n  log_temperature: jnp.ndarray\n  log_alpha: jnp.ndarray",
  "class CategoricalMPOStats(NamedTuple):\n  \"\"\"NamedTuple to store loss statistics.\"\"\"\n  dual_alpha: float\n  dual_temperature: float\n\n  loss_e_step: float\n  loss_m_step: float\n  loss_dual: float\n\n  loss_policy: float\n  loss_alpha: float\n  loss_temperature: float\n\n  kl_q_rel: float\n  kl_mean_rel: float\n\n  q_min: float\n  q_max: float\n\n  entropy_online: float\n  entropy_target: float",
  "class CategoricalMPO:\n  \"\"\"MPO loss for a categorical policy (Abdolmaleki et al., 2018).\n\n  (Abdolmaleki et al., 2018): https://arxiv.org/pdf/1812.02256.pdf\n  \"\"\"\n\n  def __init__(self,\n               epsilon: float,\n               epsilon_policy: float,\n               init_log_temperature: float,\n               init_log_alpha: float):\n    \"\"\"Initializes the MPO loss for discrete (categorical) policies.\n\n    Args:\n      epsilon: KL constraint on the non-parametric auxiliary policy, the one\n        associated with the dual variable called temperature.\n      epsilon_policy: KL constraint on the categorical policy, the one\n        associated with the dual variable called alpha.\n      init_log_temperature: initial value for the temperature in log-space, note\n        a softplus (rather than an exp) will be used to transform this.\n      init_log_alpha: initial value for alpha in log-space. Note that a softplus\n        (rather than an exp) will be used to transform this.\n    \"\"\"\n\n    # MPO constraint thresholds.\n    self._epsilon = epsilon\n    self._epsilon_policy = epsilon_policy\n\n    # Initial values for the constraints' dual variables.\n    self._init_log_temperature = init_log_temperature\n    self._init_log_alpha = init_log_alpha\n\n  def init_params(self, action_dim: int, dtype: DType = jnp.float32):\n    \"\"\"Creates an initial set of parameters.\"\"\"\n    del action_dim  # Unused.\n    return CategoricalMPOParams(\n        log_temperature=jnp.full([1], self._init_log_temperature, dtype=dtype),\n        log_alpha=jnp.full([1], self._init_log_alpha, dtype=dtype))\n\n  def __call__(\n      self,\n      params: CategoricalMPOParams,\n      online_action_distribution: distrax.Categorical,\n      target_action_distribution: distrax.Categorical,\n      actions: jnp.ndarray,  # Unused.\n      q_values: jnp.ndarray,  # Shape [D, B].\n  ) -> Tuple[jnp.ndarray, CategoricalMPOStats]:\n    \"\"\"Computes the MPO loss for a categorical policy.\n\n    Args:\n      params: parameters tracking the temperature and the dual variables.\n      online_action_distribution: online distribution returned by the online\n        policy network; expects batch_dims of [B] and event_dims of [D].\n      target_action_distribution: target distribution returned by the target\n        policy network; expects same shapes as online distribution.\n      actions: Unused.\n      q_values: Q-values associated with every action; expects shape [D, B].\n\n    Returns:\n      Loss, combining the policy loss, KL penalty, and dual losses required to\n        adapt the dual variables.\n      Stats, for diagnostics and tracking performance.\n    \"\"\"\n\n    q_values = jnp.transpose(q_values)  # [D, B] --> [B, D].\n\n    # Transform dual variables from log-space.\n    # Note: using softplus instead of exponential for numerical stability.\n    temperature = get_temperature_from_params(params)\n    alpha = jax.nn.softplus(params.log_alpha) + _MPO_FLOAT_EPSILON\n\n    # Compute the E-step logits and the temperature loss, used to adapt the\n    # tempering of Q-values.\n    logits_e_step, loss_temperature = compute_weights_and_temperature_loss(  # pytype: disable=wrong-arg-types  # jax-ndarray\n        q_values=q_values, logits=target_action_distribution.logits,\n        epsilon=self._epsilon, temperature=temperature)\n    action_distribution_e_step = distrax.Categorical(logits=logits_e_step)\n\n    # Only needed for diagnostics: Compute estimated actualized KL between the\n    # non-parametric and current target policies.\n    kl_nonparametric = action_distribution_e_step.kl_divergence(\n        target_action_distribution)\n\n    # Compute the policy loss.\n    loss_policy = action_distribution_e_step.cross_entropy(\n        online_action_distribution)\n    loss_policy = jnp.mean(loss_policy)\n\n    # Compute the regularization.\n    kl = target_action_distribution.kl_divergence(online_action_distribution)\n    mean_kl = jnp.mean(kl, axis=0)\n    loss_kl = jax.lax.stop_gradient(alpha) * mean_kl\n\n    # Compute the dual loss.\n    loss_alpha = alpha * (self._epsilon_policy - jax.lax.stop_gradient(mean_kl))\n\n    # Combine losses.\n    loss_dual = loss_alpha + loss_temperature\n    loss = loss_policy + loss_kl + loss_dual\n\n    # Create statistics.\n    stats = CategoricalMPOStats(\n        # Dual Variables.\n        dual_alpha=jnp.mean(alpha),\n        dual_temperature=jnp.mean(temperature),\n        # Losses.\n        loss_e_step=loss_policy,\n        loss_m_step=loss_kl,\n        loss_dual=loss_dual,\n        loss_policy=jnp.mean(loss),\n        loss_alpha=jnp.mean(loss_alpha),\n        loss_temperature=jnp.mean(loss_temperature),\n        # KL measurements.\n        kl_q_rel=jnp.mean(kl_nonparametric) / self._epsilon,\n        kl_mean_rel=mean_kl / self._epsilon_policy,\n        # Q measurements.\n        q_min=jnp.mean(jnp.min(q_values, axis=0)),\n        q_max=jnp.mean(jnp.max(q_values, axis=0)),\n        entropy_online=jnp.mean(online_action_distribution.entropy()),\n        entropy_target=jnp.mean(target_action_distribution.entropy())\n    )\n\n    return loss, stats",
  "def compute_weights_and_temperature_loss(\n    q_values: jnp.ndarray,\n    logits: jnp.ndarray,\n    epsilon: float,\n    temperature: jnp.ndarray,\n) -> Tuple[jnp.ndarray, jnp.ndarray]:\n  \"\"\"Computes normalized importance weights for the policy optimization.\n\n  Args:\n    q_values: Q-values associated with the actions sampled from the target\n      policy; expected shape [B, D].\n    logits: Parameters to the categorical distribution with respect to which the\n      expectations are going to be computed.\n    epsilon: Desired constraint on the KL between the target and non-parametric\n      policies.\n    temperature: Scalar used to temper the Q-values before computing normalized\n      importance weights from them. This is really the Lagrange dual variable in\n      the constrained optimization problem, the solution of which is the\n      non-parametric policy targeted by the policy loss.\n\n  Returns:\n    Normalized importance weights, used for policy optimization.\n    Temperature loss, used to adapt the temperature.\n  \"\"\"\n\n  # Temper the given Q-values using the current temperature.\n  tempered_q_values = jax.lax.stop_gradient(q_values) / temperature\n\n  # Compute the E-step normalized logits.\n  unnormalized_logits = tempered_q_values + jax.nn.log_softmax(logits, axis=-1)\n  logits_e_step = jax.nn.log_softmax(unnormalized_logits, axis=-1)\n\n  # Compute the temperature loss (dual of the E-step optimization problem).\n  # Note that the log normalizer will be the same for all actions, so we choose\n  # only the first one.\n  log_normalizer = unnormalized_logits[:, 0] - logits_e_step[:, 0]\n  loss_temperature = temperature * (epsilon + jnp.mean(log_normalizer))\n\n  return logits_e_step, loss_temperature",
  "def clip_categorical_mpo_params(\n    params: CategoricalMPOParams) -> CategoricalMPOParams:\n  return params._replace(\n      log_temperature=jnp.maximum(_MIN_LOG_TEMPERATURE, params.log_temperature),\n      log_alpha=jnp.maximum(_MIN_LOG_ALPHA, params.log_alpha))",
  "def get_temperature_from_params(params: CategoricalMPOParams) -> float:\n  return jax.nn.softplus(params.log_temperature) + _MPO_FLOAT_EPSILON",
  "def __init__(self,\n               epsilon: float,\n               epsilon_policy: float,\n               init_log_temperature: float,\n               init_log_alpha: float):\n    \"\"\"Initializes the MPO loss for discrete (categorical) policies.\n\n    Args:\n      epsilon: KL constraint on the non-parametric auxiliary policy, the one\n        associated with the dual variable called temperature.\n      epsilon_policy: KL constraint on the categorical policy, the one\n        associated with the dual variable called alpha.\n      init_log_temperature: initial value for the temperature in log-space, note\n        a softplus (rather than an exp) will be used to transform this.\n      init_log_alpha: initial value for alpha in log-space. Note that a softplus\n        (rather than an exp) will be used to transform this.\n    \"\"\"\n\n    # MPO constraint thresholds.\n    self._epsilon = epsilon\n    self._epsilon_policy = epsilon_policy\n\n    # Initial values for the constraints' dual variables.\n    self._init_log_temperature = init_log_temperature\n    self._init_log_alpha = init_log_alpha",
  "def init_params(self, action_dim: int, dtype: DType = jnp.float32):\n    \"\"\"Creates an initial set of parameters.\"\"\"\n    del action_dim  # Unused.\n    return CategoricalMPOParams(\n        log_temperature=jnp.full([1], self._init_log_temperature, dtype=dtype),\n        log_alpha=jnp.full([1], self._init_log_alpha, dtype=dtype))",
  "def __call__(\n      self,\n      params: CategoricalMPOParams,\n      online_action_distribution: distrax.Categorical,\n      target_action_distribution: distrax.Categorical,\n      actions: jnp.ndarray,  # Unused.\n      q_values: jnp.ndarray,  # Shape [D, B].\n  ) -> Tuple[jnp.ndarray, CategoricalMPOStats]:\n    \"\"\"Computes the MPO loss for a categorical policy.\n\n    Args:\n      params: parameters tracking the temperature and the dual variables.\n      online_action_distribution: online distribution returned by the online\n        policy network; expects batch_dims of [B] and event_dims of [D].\n      target_action_distribution: target distribution returned by the target\n        policy network; expects same shapes as online distribution.\n      actions: Unused.\n      q_values: Q-values associated with every action; expects shape [D, B].\n\n    Returns:\n      Loss, combining the policy loss, KL penalty, and dual losses required to\n        adapt the dual variables.\n      Stats, for diagnostics and tracking performance.\n    \"\"\"\n\n    q_values = jnp.transpose(q_values)  # [D, B] --> [B, D].\n\n    # Transform dual variables from log-space.\n    # Note: using softplus instead of exponential for numerical stability.\n    temperature = get_temperature_from_params(params)\n    alpha = jax.nn.softplus(params.log_alpha) + _MPO_FLOAT_EPSILON\n\n    # Compute the E-step logits and the temperature loss, used to adapt the\n    # tempering of Q-values.\n    logits_e_step, loss_temperature = compute_weights_and_temperature_loss(  # pytype: disable=wrong-arg-types  # jax-ndarray\n        q_values=q_values, logits=target_action_distribution.logits,\n        epsilon=self._epsilon, temperature=temperature)\n    action_distribution_e_step = distrax.Categorical(logits=logits_e_step)\n\n    # Only needed for diagnostics: Compute estimated actualized KL between the\n    # non-parametric and current target policies.\n    kl_nonparametric = action_distribution_e_step.kl_divergence(\n        target_action_distribution)\n\n    # Compute the policy loss.\n    loss_policy = action_distribution_e_step.cross_entropy(\n        online_action_distribution)\n    loss_policy = jnp.mean(loss_policy)\n\n    # Compute the regularization.\n    kl = target_action_distribution.kl_divergence(online_action_distribution)\n    mean_kl = jnp.mean(kl, axis=0)\n    loss_kl = jax.lax.stop_gradient(alpha) * mean_kl\n\n    # Compute the dual loss.\n    loss_alpha = alpha * (self._epsilon_policy - jax.lax.stop_gradient(mean_kl))\n\n    # Combine losses.\n    loss_dual = loss_alpha + loss_temperature\n    loss = loss_policy + loss_kl + loss_dual\n\n    # Create statistics.\n    stats = CategoricalMPOStats(\n        # Dual Variables.\n        dual_alpha=jnp.mean(alpha),\n        dual_temperature=jnp.mean(temperature),\n        # Losses.\n        loss_e_step=loss_policy,\n        loss_m_step=loss_kl,\n        loss_dual=loss_dual,\n        loss_policy=jnp.mean(loss),\n        loss_alpha=jnp.mean(loss_alpha),\n        loss_temperature=jnp.mean(loss_temperature),\n        # KL measurements.\n        kl_q_rel=jnp.mean(kl_nonparametric) / self._epsilon,\n        kl_mean_rel=mean_kl / self._epsilon_policy,\n        # Q measurements.\n        q_min=jnp.mean(jnp.min(q_values, axis=0)),\n        q_max=jnp.mean(jnp.max(q_values, axis=0)),\n        entropy_online=jnp.mean(online_action_distribution.entropy()),\n        entropy_target=jnp.mean(target_action_distribution.entropy())\n    )\n\n    return loss, stats",
  "def softmax_cross_entropy(\n    logits: chex.Array, target_probs: chex.Array) -> chex.Array:\n  \"\"\"Compute cross entropy loss between logits and target probabilities.\"\"\"\n  chex.assert_equal_shape([target_probs, logits])\n  return -jnp.sum(target_probs * jax.nn.log_softmax(logits), axis=-1)",
  "def top1_accuracy_tiebreak(\n    logits: chex.Array,\n    targets: chex.Array,\n    *,\n    rng: chex.PRNGKey,\n    eps: float = 1e-6) -> chex.Array:\n  \"\"\"Compute the top-1 accuracy with an argmax of targets (random tie-break).\"\"\"\n  noise = jax.random.uniform(rng, shape=targets.shape,\n                             minval=-eps, maxval=eps)\n  acc = jnp.argmax(logits, axis=-1) == jnp.argmax(targets + noise, axis=-1)\n  return jnp.mean(acc)",
  "class RolloutLoss:\n  \"\"\"A MuZero/Muesli-style loss on the rollouts of the dynamics model.\"\"\"\n\n  def __init__(\n      self,\n      dynamics_model: mpo_networks.UnrollableNetwork,\n      model_rollout_length: int,\n      loss_scales: mpo_types.LossScalesConfig,\n      distributional_loss_fn: mpo_types.DistributionalLossFn,\n  ):\n    self._dynamics_model = dynamics_model\n    self._model_rollout_length = model_rollout_length\n    self._loss_scales = loss_scales\n    self._distributional_loss_fn = distributional_loss_fn\n\n  def _rolling_window(self, x: chex.Array, axis: int = 0) -> chex.Array:\n    \"\"\"A convenient tree-mapped and configured call to rolling window.\n\n    Stacks R = T - K + 1 action slices of length K = model_rollout_length from\n    tensor x: [..., 0:K; ...; T-K:T, ...].\n\n    Args:\n      x: The tensor to select rolling slices from (along specified axis), with\n        shape [..., T, ...] such that T = x.shape[axis].\n      axis: The axis to slice from (defaults to 0).\n\n    Returns:\n      A tensor containing the stacked slices [0:K, ... T-K:T] from an axis of x\n      with shape [..., K, R, ...] for input shape [..., T, ...].\n    \"\"\"\n    def rw(y):\n      return mpo_utils.rolling_window(\n          y, window=self._model_rollout_length, axis=axis, time_major=True)\n\n    return mpo_utils.tree_map_distribution(rw, x)\n\n  def _compute_model_rollout_predictions(\n      self, params: mpo_networks.MPONetworkParams,\n      state_embeddings: types.NestedArray,\n      action_sequence: types.NestedArray) -> mpo_types.ModelOutputs:\n    \"\"\"Roll out the dynamics model for each embedding state.\"\"\"\n    assert self._model_rollout_length > 0\n    # Stack the R=T-K+1 action slices of length K: [0:K; ...; T-K:T]; [K, R].\n    rollout_actions = self._rolling_window(action_sequence)\n\n    # Create batch of root states (embeddings) s_t for t \\in {0, ..., R}.\n    num_rollouts = action_sequence.shape[0] - self._model_rollout_length + 1\n    root_state = self._dynamics_model.initial_state_fn(\n        params.dynamics_model_initial_state, state_embeddings[:num_rollouts])\n    # TODO(abef): randomly choose (fewer?) root unroll states, as in Muesli?\n\n    # Roll out K steps forward in time for each root embedding; [K, R, ...].\n    # For example, policy_rollout[k, t] is the step-k prediction starting from\n    # state s_t (and same for value_rollout and reward_rollout). Thus, for\n    # valid values of k, t, and i, policy_rollout[k, t] and\n    # policy_rollout[k-i, t+i] share the same target.\n    (policy_rollout, value_rollout, reward_rollout,\n     embedding_rollout), _ = self._dynamics_model.unroll(\n         params.dynamics_model, rollout_actions, root_state)\n    # TODO(abef): try using the same params for both the root & rollout heads.\n\n    chex.assert_shape([rollout_actions, embedding_rollout],\n                      (self._model_rollout_length, num_rollouts, ...))\n\n    # Create the outputs but drop the rollout that uses action a_{T-1} (and\n    # thus contains state s_T) for the policy, value, and embedding because we\n    # don't have targets for s_T (but we do know them for the final reward).\n    # Also drop the rollout with s_{T-1} for the value because we don't have\n    # targets for that either.\n    return mpo_types.ModelOutputs(\n        policy=policy_rollout[:, :-1],  # [K, R-1, ...]\n        value=value_rollout[:, :-2],  # [K, R-2, ...]\n        reward=reward_rollout,  # [K, R, ...]\n        embedding=embedding_rollout[:, :-1])  # [K, R-1, ...]\n\n  def __call__(\n      self,\n      params: mpo_networks.MPONetworkParams,\n      dual_params: mpo_types.DualParams,\n      sequence: adders.Step,\n      state_embeddings: types.NestedArray,\n      targets: mpo_types.LossTargets,\n      key: network_lib.PRNGKey,\n  ) -> Tuple[jnp.ndarray, mpo_types.LogDict]:\n\n    num_rollouts = sequence.reward.shape[0] - self._model_rollout_length + 1\n    indices = jnp.arange(num_rollouts)\n\n    # Create rollout predictions.\n    rollout = self._compute_model_rollout_predictions(\n        params=params, state_embeddings=state_embeddings,\n        action_sequence=sequence.action)\n\n    # Create rollout target tensors. The rollouts will not contain the policy\n    # and value at t=0 because they start after taking the first action in\n    # the sequence, so drop those when creating the targets. They will contain\n    # the reward at t=0, however, because of how the sequences are stored.\n    # Rollout target shapes:\n    #   - value: [N, Z, T-2] -> [N, Z, K, R-2],\n    #   - reward: [T] -> [K, R].\n    value_targets = self._rolling_window(targets.value[..., 1:], axis=-1)\n    reward_targets = self._rolling_window(targets.reward)[None, None, ...]\n\n    # Define the value and reward rollout loss functions.\n    def value_loss_fn(root_idx) -> jnp.ndarray:\n      return self._distributional_loss_fn(\n          rollout.value[:, root_idx],  # [K, R-2, ...]\n          value_targets[..., root_idx])  # [..., K, R-2]\n\n    def reward_loss_fn(root_idx) -> jnp.ndarray:\n      return self._distributional_loss_fn(\n          rollout.reward[:, root_idx],  # [K, R, ...]\n          reward_targets[..., root_idx])  # [..., K, R]\n\n    # Reward and value losses.\n    critic_loss = jnp.mean(jax.vmap(value_loss_fn)(indices[:-2]))\n    reward_loss = jnp.mean(jax.vmap(reward_loss_fn)(indices))\n\n    # Define the MPO policy rollout loss.\n    mpo_policy_loss = 0\n    if self._loss_scales.rollout.policy:\n      # Rollout target shapes:\n      #   - policy: [T-1, ...] -> [K, R-1, ...],\n      #   - q_improvement: [N, T-1] -> [N, K, R-1].\n      policy_targets = self._rolling_window(targets.policy[1:])\n      q_improvement = self._rolling_window(targets.q_improvement[:, 1:], axis=1)\n\n      def policy_loss_fn(root_idx) -> jnp.ndarray:\n        chex.assert_shape((rollout.policy.logits, policy_targets.logits),  # pytype: disable=attribute-error  # numpy-scalars\n                          (self._model_rollout_length, num_rollouts - 1, None))\n        chex.assert_shape(q_improvement,\n                          (None, self._model_rollout_length, num_rollouts - 1))\n        # Compute MPO's E-step unnormalized logits.\n        temperature = discrete_losses.get_temperature_from_params(dual_params)\n        policy_target_probs = jax.nn.softmax(\n            jnp.transpose(q_improvement[..., root_idx]) / temperature +\n            jax.nn.log_softmax(policy_targets[:, root_idx].logits, axis=-1))  # pytype: disable=attribute-error  # numpy-scalars\n        return softmax_cross_entropy(rollout.policy[:, root_idx].logits,  # pytype: disable=bad-return-type  # numpy-scalars\n                                     jax.lax.stop_gradient(policy_target_probs))\n\n      # Compute the MPO loss and add it to the overall rollout policy loss.\n      mpo_policy_loss = jax.vmap(policy_loss_fn)(indices[:-1])\n      mpo_policy_loss = jnp.mean(mpo_policy_loss)\n\n    # Define the BC policy rollout loss (only supported for discrete policies).\n    bc_policy_loss, bc_policy_acc = 0, 0\n    if self._loss_scales.rollout.bc_policy:\n      num_actions = rollout.policy.logits.shape[-1]  # A\n      bc_targets = self._rolling_window(  # [T-1, A] -> [K, R-1, A]\n          rlax.one_hot(sequence.action[1:], num_actions))\n\n      def bc_policy_loss_fn(root_idx) -> Tuple[jnp.ndarray, jnp.ndarray]:\n        \"\"\"Self-behavior-cloning loss (cross entropy on rollout actions).\"\"\"\n        chex.assert_shape(\n            (rollout.policy.logits, bc_targets),\n            (self._model_rollout_length, num_rollouts - 1, num_actions))\n        loss = softmax_cross_entropy(rollout.policy.logits[:, root_idx],\n                                     bc_targets[:, root_idx])\n        top1_accuracy = top1_accuracy_tiebreak(\n            rollout.policy.logits[:, root_idx],\n            bc_targets[:, root_idx],\n            rng=key)\n        return loss, top1_accuracy  # pytype: disable=bad-return-type  # numpy-scalars\n\n      # Compute each rollout loss by vmapping over the rollouts.\n      bc_policy_loss, bc_policy_acc = jax.vmap(bc_policy_loss_fn)(indices[:-1])\n      bc_policy_loss = jnp.mean(bc_policy_loss)\n      bc_policy_acc = jnp.mean(bc_policy_acc)\n\n    # Combine losses.\n    loss = (\n        self._loss_scales.rollout.policy * mpo_policy_loss +\n        self._loss_scales.rollout.bc_policy * bc_policy_loss +\n        self._loss_scales.critic * self._loss_scales.rollout.critic *\n        critic_loss + self._loss_scales.rollout.reward * reward_loss)\n\n    logging_dict = {\n        'rollout_critic_loss': critic_loss,\n        'rollout_reward_loss': reward_loss,\n        'rollout_policy_loss': mpo_policy_loss,\n        'rollout_bc_policy_loss': bc_policy_loss,\n        'rollout_bc_accuracy': bc_policy_acc,\n        'rollout_loss': loss,\n    }\n\n    return loss, logging_dict",
  "def __init__(\n      self,\n      dynamics_model: mpo_networks.UnrollableNetwork,\n      model_rollout_length: int,\n      loss_scales: mpo_types.LossScalesConfig,\n      distributional_loss_fn: mpo_types.DistributionalLossFn,\n  ):\n    self._dynamics_model = dynamics_model\n    self._model_rollout_length = model_rollout_length\n    self._loss_scales = loss_scales\n    self._distributional_loss_fn = distributional_loss_fn",
  "def _rolling_window(self, x: chex.Array, axis: int = 0) -> chex.Array:\n    \"\"\"A convenient tree-mapped and configured call to rolling window.\n\n    Stacks R = T - K + 1 action slices of length K = model_rollout_length from\n    tensor x: [..., 0:K; ...; T-K:T, ...].\n\n    Args:\n      x: The tensor to select rolling slices from (along specified axis), with\n        shape [..., T, ...] such that T = x.shape[axis].\n      axis: The axis to slice from (defaults to 0).\n\n    Returns:\n      A tensor containing the stacked slices [0:K, ... T-K:T] from an axis of x\n      with shape [..., K, R, ...] for input shape [..., T, ...].\n    \"\"\"\n    def rw(y):\n      return mpo_utils.rolling_window(\n          y, window=self._model_rollout_length, axis=axis, time_major=True)\n\n    return mpo_utils.tree_map_distribution(rw, x)",
  "def _compute_model_rollout_predictions(\n      self, params: mpo_networks.MPONetworkParams,\n      state_embeddings: types.NestedArray,\n      action_sequence: types.NestedArray) -> mpo_types.ModelOutputs:\n    \"\"\"Roll out the dynamics model for each embedding state.\"\"\"\n    assert self._model_rollout_length > 0\n    # Stack the R=T-K+1 action slices of length K: [0:K; ...; T-K:T]; [K, R].\n    rollout_actions = self._rolling_window(action_sequence)\n\n    # Create batch of root states (embeddings) s_t for t \\in {0, ..., R}.\n    num_rollouts = action_sequence.shape[0] - self._model_rollout_length + 1\n    root_state = self._dynamics_model.initial_state_fn(\n        params.dynamics_model_initial_state, state_embeddings[:num_rollouts])\n    # TODO(abef): randomly choose (fewer?) root unroll states, as in Muesli?\n\n    # Roll out K steps forward in time for each root embedding; [K, R, ...].\n    # For example, policy_rollout[k, t] is the step-k prediction starting from\n    # state s_t (and same for value_rollout and reward_rollout). Thus, for\n    # valid values of k, t, and i, policy_rollout[k, t] and\n    # policy_rollout[k-i, t+i] share the same target.\n    (policy_rollout, value_rollout, reward_rollout,\n     embedding_rollout), _ = self._dynamics_model.unroll(\n         params.dynamics_model, rollout_actions, root_state)\n    # TODO(abef): try using the same params for both the root & rollout heads.\n\n    chex.assert_shape([rollout_actions, embedding_rollout],\n                      (self._model_rollout_length, num_rollouts, ...))\n\n    # Create the outputs but drop the rollout that uses action a_{T-1} (and\n    # thus contains state s_T) for the policy, value, and embedding because we\n    # don't have targets for s_T (but we do know them for the final reward).\n    # Also drop the rollout with s_{T-1} for the value because we don't have\n    # targets for that either.\n    return mpo_types.ModelOutputs(\n        policy=policy_rollout[:, :-1],  # [K, R-1, ...]\n        value=value_rollout[:, :-2],  # [K, R-2, ...]\n        reward=reward_rollout,  # [K, R, ...]\n        embedding=embedding_rollout[:, :-1])",
  "def __call__(\n      self,\n      params: mpo_networks.MPONetworkParams,\n      dual_params: mpo_types.DualParams,\n      sequence: adders.Step,\n      state_embeddings: types.NestedArray,\n      targets: mpo_types.LossTargets,\n      key: network_lib.PRNGKey,\n  ) -> Tuple[jnp.ndarray, mpo_types.LogDict]:\n\n    num_rollouts = sequence.reward.shape[0] - self._model_rollout_length + 1\n    indices = jnp.arange(num_rollouts)\n\n    # Create rollout predictions.\n    rollout = self._compute_model_rollout_predictions(\n        params=params, state_embeddings=state_embeddings,\n        action_sequence=sequence.action)\n\n    # Create rollout target tensors. The rollouts will not contain the policy\n    # and value at t=0 because they start after taking the first action in\n    # the sequence, so drop those when creating the targets. They will contain\n    # the reward at t=0, however, because of how the sequences are stored.\n    # Rollout target shapes:\n    #   - value: [N, Z, T-2] -> [N, Z, K, R-2],\n    #   - reward: [T] -> [K, R].\n    value_targets = self._rolling_window(targets.value[..., 1:], axis=-1)\n    reward_targets = self._rolling_window(targets.reward)[None, None, ...]\n\n    # Define the value and reward rollout loss functions.\n    def value_loss_fn(root_idx) -> jnp.ndarray:\n      return self._distributional_loss_fn(\n          rollout.value[:, root_idx],  # [K, R-2, ...]\n          value_targets[..., root_idx])  # [..., K, R-2]\n\n    def reward_loss_fn(root_idx) -> jnp.ndarray:\n      return self._distributional_loss_fn(\n          rollout.reward[:, root_idx],  # [K, R, ...]\n          reward_targets[..., root_idx])  # [..., K, R]\n\n    # Reward and value losses.\n    critic_loss = jnp.mean(jax.vmap(value_loss_fn)(indices[:-2]))\n    reward_loss = jnp.mean(jax.vmap(reward_loss_fn)(indices))\n\n    # Define the MPO policy rollout loss.\n    mpo_policy_loss = 0\n    if self._loss_scales.rollout.policy:\n      # Rollout target shapes:\n      #   - policy: [T-1, ...] -> [K, R-1, ...],\n      #   - q_improvement: [N, T-1] -> [N, K, R-1].\n      policy_targets = self._rolling_window(targets.policy[1:])\n      q_improvement = self._rolling_window(targets.q_improvement[:, 1:], axis=1)\n\n      def policy_loss_fn(root_idx) -> jnp.ndarray:\n        chex.assert_shape((rollout.policy.logits, policy_targets.logits),  # pytype: disable=attribute-error  # numpy-scalars\n                          (self._model_rollout_length, num_rollouts - 1, None))\n        chex.assert_shape(q_improvement,\n                          (None, self._model_rollout_length, num_rollouts - 1))\n        # Compute MPO's E-step unnormalized logits.\n        temperature = discrete_losses.get_temperature_from_params(dual_params)\n        policy_target_probs = jax.nn.softmax(\n            jnp.transpose(q_improvement[..., root_idx]) / temperature +\n            jax.nn.log_softmax(policy_targets[:, root_idx].logits, axis=-1))  # pytype: disable=attribute-error  # numpy-scalars\n        return softmax_cross_entropy(rollout.policy[:, root_idx].logits,  # pytype: disable=bad-return-type  # numpy-scalars\n                                     jax.lax.stop_gradient(policy_target_probs))\n\n      # Compute the MPO loss and add it to the overall rollout policy loss.\n      mpo_policy_loss = jax.vmap(policy_loss_fn)(indices[:-1])\n      mpo_policy_loss = jnp.mean(mpo_policy_loss)\n\n    # Define the BC policy rollout loss (only supported for discrete policies).\n    bc_policy_loss, bc_policy_acc = 0, 0\n    if self._loss_scales.rollout.bc_policy:\n      num_actions = rollout.policy.logits.shape[-1]  # A\n      bc_targets = self._rolling_window(  # [T-1, A] -> [K, R-1, A]\n          rlax.one_hot(sequence.action[1:], num_actions))\n\n      def bc_policy_loss_fn(root_idx) -> Tuple[jnp.ndarray, jnp.ndarray]:\n        \"\"\"Self-behavior-cloning loss (cross entropy on rollout actions).\"\"\"\n        chex.assert_shape(\n            (rollout.policy.logits, bc_targets),\n            (self._model_rollout_length, num_rollouts - 1, num_actions))\n        loss = softmax_cross_entropy(rollout.policy.logits[:, root_idx],\n                                     bc_targets[:, root_idx])\n        top1_accuracy = top1_accuracy_tiebreak(\n            rollout.policy.logits[:, root_idx],\n            bc_targets[:, root_idx],\n            rng=key)\n        return loss, top1_accuracy  # pytype: disable=bad-return-type  # numpy-scalars\n\n      # Compute each rollout loss by vmapping over the rollouts.\n      bc_policy_loss, bc_policy_acc = jax.vmap(bc_policy_loss_fn)(indices[:-1])\n      bc_policy_loss = jnp.mean(bc_policy_loss)\n      bc_policy_acc = jnp.mean(bc_policy_acc)\n\n    # Combine losses.\n    loss = (\n        self._loss_scales.rollout.policy * mpo_policy_loss +\n        self._loss_scales.rollout.bc_policy * bc_policy_loss +\n        self._loss_scales.critic * self._loss_scales.rollout.critic *\n        critic_loss + self._loss_scales.rollout.reward * reward_loss)\n\n    logging_dict = {\n        'rollout_critic_loss': critic_loss,\n        'rollout_reward_loss': reward_loss,\n        'rollout_policy_loss': mpo_policy_loss,\n        'rollout_bc_policy_loss': bc_policy_loss,\n        'rollout_bc_accuracy': bc_policy_acc,\n        'rollout_loss': loss,\n    }\n\n    return loss, logging_dict",
  "def rw(y):\n      return mpo_utils.rolling_window(\n          y, window=self._model_rollout_length, axis=axis, time_major=True)",
  "def value_loss_fn(root_idx) -> jnp.ndarray:\n      return self._distributional_loss_fn(\n          rollout.value[:, root_idx],  # [K, R-2, ...]\n          value_targets[..., root_idx])",
  "def reward_loss_fn(root_idx) -> jnp.ndarray:\n      return self._distributional_loss_fn(\n          rollout.reward[:, root_idx],  # [K, R, ...]\n          reward_targets[..., root_idx])",
  "def policy_loss_fn(root_idx) -> jnp.ndarray:\n        chex.assert_shape((rollout.policy.logits, policy_targets.logits),  # pytype: disable=attribute-error  # numpy-scalars\n                          (self._model_rollout_length, num_rollouts - 1, None))\n        chex.assert_shape(q_improvement,\n                          (None, self._model_rollout_length, num_rollouts - 1))\n        # Compute MPO's E-step unnormalized logits.\n        temperature = discrete_losses.get_temperature_from_params(dual_params)\n        policy_target_probs = jax.nn.softmax(\n            jnp.transpose(q_improvement[..., root_idx]) / temperature +\n            jax.nn.log_softmax(policy_targets[:, root_idx].logits, axis=-1))  # pytype: disable=attribute-error  # numpy-scalars\n        return softmax_cross_entropy(rollout.policy[:, root_idx].logits,  # pytype: disable=bad-return-type  # numpy-scalars\n                                     jax.lax.stop_gradient(policy_target_probs))",
  "def bc_policy_loss_fn(root_idx) -> Tuple[jnp.ndarray, jnp.ndarray]:\n        \"\"\"Self-behavior-cloning loss (cross entropy on rollout actions).\"\"\"\n        chex.assert_shape(\n            (rollout.policy.logits, bc_targets),\n            (self._model_rollout_length, num_rollouts - 1, num_actions))\n        loss = softmax_cross_entropy(rollout.policy.logits[:, root_idx],\n                                     bc_targets[:, root_idx])\n        top1_accuracy = top1_accuracy_tiebreak(\n            rollout.policy.logits[:, root_idx],\n            bc_targets[:, root_idx],\n            rng=key)\n        return loss, top1_accuracy",
  "class BVELoss(dqn.LossFn):\n  \"\"\"This loss implements TD-loss to estimate behavior value.\n\n    This loss function uses the next action to learn with the SARSA tuples.\n    It is intended to be used with dqn.SGDLearner. The method was proposed\n    in \"Regularized Behavior Value Estimation\" by Gulcehre et al to overcome\n    the extrapolation error in offline RL setting:\n    https://arxiv.org/abs/2103.09575\n  \"\"\"\n  discount: float = 0.99\n  max_abs_reward: float = 1.\n  huber_loss_parameter: float = 1.\n\n  def __call__(\n      self,\n      network: networks_lib.TypedFeedForwardNetwork,\n      params: networks_lib.Params,\n      target_params: networks_lib.Params,\n      batch: reverb.ReplaySample,\n      key: networks_lib.PRNGKey,\n  ) -> Tuple[jax.Array, dqn.LossExtra]:\n    \"\"\"Calculate a loss on a single batch of data.\"\"\"\n    transitions: types.Transition = batch.data\n\n    # Forward pass.\n    key1, key2 = jax.random.split(key)\n    q_tm1 = network.apply(\n        params, transitions.observation, is_training=True, key=key1)\n    q_t_value = network.apply(\n        target_params, transitions.next_observation, is_training=True, key=key2)\n\n    # Cast and clip rewards.\n    d_t = (transitions.discount * self.discount).astype(jnp.float32)\n    r_t = jnp.clip(transitions.reward, -self.max_abs_reward,\n                   self.max_abs_reward).astype(jnp.float32)\n\n    # Compute double Q-learning n-step TD-error.\n    batch_error = jax.vmap(rlax.sarsa)\n    next_action = transitions.extras['next_action']\n    td_error = batch_error(q_tm1, transitions.action, r_t, d_t, q_t_value,\n                           next_action)\n    batch_loss = rlax.huber_loss(td_error, self.huber_loss_parameter)\n\n    # Average:\n    loss = jnp.mean(batch_loss)  # []\n    metrics = {'td_error': td_error, 'batch_loss': batch_loss}\n    return loss, dqn.LossExtra(\n        metrics=metrics,\n        reverb_priorities=jnp.abs(td_error).astype(jnp.float64))",
  "def __call__(\n      self,\n      network: networks_lib.TypedFeedForwardNetwork,\n      params: networks_lib.Params,\n      target_params: networks_lib.Params,\n      batch: reverb.ReplaySample,\n      key: networks_lib.PRNGKey,\n  ) -> Tuple[jax.Array, dqn.LossExtra]:\n    \"\"\"Calculate a loss on a single batch of data.\"\"\"\n    transitions: types.Transition = batch.data\n\n    # Forward pass.\n    key1, key2 = jax.random.split(key)\n    q_tm1 = network.apply(\n        params, transitions.observation, is_training=True, key=key1)\n    q_t_value = network.apply(\n        target_params, transitions.next_observation, is_training=True, key=key2)\n\n    # Cast and clip rewards.\n    d_t = (transitions.discount * self.discount).astype(jnp.float32)\n    r_t = jnp.clip(transitions.reward, -self.max_abs_reward,\n                   self.max_abs_reward).astype(jnp.float32)\n\n    # Compute double Q-learning n-step TD-error.\n    batch_error = jax.vmap(rlax.sarsa)\n    next_action = transitions.extras['next_action']\n    td_error = batch_error(q_tm1, transitions.action, r_t, d_t, q_t_value,\n                           next_action)\n    batch_loss = rlax.huber_loss(td_error, self.huber_loss_parameter)\n\n    # Average:\n    loss = jnp.mean(batch_loss)  # []\n    metrics = {'td_error': td_error, 'batch_loss': batch_loss}\n    return loss, dqn.LossExtra(\n        metrics=metrics,\n        reverb_priorities=jnp.abs(td_error).astype(jnp.float64))",
  "class BVENetworks:\n  \"\"\"The network and pure functions for the BVE agent.\n\n  Attributes:\n    policy_network: The policy network.\n    sample_fn: A pure function. Samples an action based on the network output.\n    log_prob: A pure function. Computes log-probability for an action.\n  \"\"\"\n  policy_network: networks_lib.TypedFeedForwardNetwork\n  sample_fn: networks_lib.SampleFn\n  log_prob: Optional[networks_lib.LogProbFn] = None",
  "class BVEConfig:\n  \"\"\"Configuration options for BVE agent.\n\n  Attributes:\n    epsilon: for use by epsilon-greedy policies. If multiple, the epsilons are\n      alternated randomly per-episode.\n    seed: Random seed.\n    learning_rate: Learning rate for Adam optimizer. Could be a number or a\n      function defining a schedule.\n    adam_eps: Epsilon for Adam optimizer.\n    discount: Discount rate applied to value per timestep.\n    target_update_period: Update target network every period.\n    max_gradient_norm: For gradient clipping.\n    max_abs_reward: Maximum absolute reward.\n    huber_loss_parameter: The delta parameter of the huber loss.\n    batch_size: Number of transitions per batch.\n    prefetch_size: Prefetch size for reverb replay performance.\n    num_sgd_steps_per_step: How many gradient updates to perform per learner\n      step.\n  \"\"\"\n  epsilon: Union[float, Sequence[float]] = 0.05\n  # TODO(b/191706065): update all clients and remove this field.\n  seed: int = 1\n\n  # Learning rule\n  learning_rate: Union[float, Callable[[int], float]] = 3e-4\n  adam_eps: float = 1e-8  # Eps for Adam optimizer.\n  discount: float = 0.99  # Discount rate applied to value per timestep.\n  target_update_period: int = 2500  # Update target network every period.\n  max_gradient_norm: float = np.inf  # For gradient clipping.\n  max_abs_reward: float = 1.  # Maximum absolute value to clip the rewards.\n  huber_loss_parameter: float = 1.  # Huber loss delta parameter.\n  batch_size: int = 256  # Minibatch size.\n  prefetch_size = 500  # The amount of data to prefetch into the memory.\n  num_sgd_steps_per_step: int = 1",
  "class BVEBuilder(builders.OfflineBuilder[bve_networks.BVENetworks,\n                                         actor_core_lib.ActorCore,\n                                         utils.PrefetchingSplit]):\n  \"\"\"BVE Builder.\"\"\"\n\n  def __init__(self, config):\n    \"\"\"Build a BVE agent.\n\n    Args:\n      config: The config of the BVE agent.\n    \"\"\"\n    self._config = config\n\n  def make_learner(self,\n                   random_key: jax_types.PRNGKey,\n                   networks: bve_networks.BVENetworks,\n                   dataset: Iterator[utils.PrefetchingSplit],\n                   logger_fn: loggers.LoggerFactory,\n                   environment_spec: specs.EnvironmentSpec,\n                   counter: Optional[counting.Counter] = None) -> core.Learner:\n    del environment_spec\n\n    loss_fn = losses.BVELoss(\n        discount=self._config.discount,\n        max_abs_reward=self._config.max_abs_reward,\n        huber_loss_parameter=self._config.huber_loss_parameter,\n    )\n\n    return learning_lib.SGDLearner(\n        network=networks.policy_network,\n        random_key=random_key,\n        optimizer=optax.adam(\n            self._config.learning_rate, eps=self._config.adam_eps),\n        target_update_period=self._config.target_update_period,\n        data_iterator=dataset,\n        loss_fn=loss_fn,\n        counter=counter,\n        num_sgd_steps_per_step=self._config.num_sgd_steps_per_step,\n        logger=logger_fn('learner'))\n\n  def make_actor(\n      self,\n      random_key: jax_types.PRNGKey,\n      policy: actor_core_lib.ActorCore,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None) -> core.Actor:\n    \"\"\"Create the actor for the BVE to perform online evals.\n\n    Args:\n      random_key: prng key.\n      policy: The DQN policy.\n      environment_spec: The environment spec.\n      variable_source: The source of where the variables are coming from.\n\n    Returns:\n      Return the actor for the evaluations.\n    \"\"\"\n    del environment_spec\n    variable_client = variable_utils.VariableClient(\n        variable_source, 'policy', device='cpu')\n    return actors.GenericActor(policy, random_key, variable_client)\n\n  def make_policy(\n      self,\n      networks: bve_networks.BVENetworks,\n      environment_spec: specs.EnvironmentSpec,\n      evaluation: Optional[bool] = False) -> actor_core_lib.ActorCore:\n    \"\"\"Creates a policy.\"\"\"\n    del environment_spec, evaluation\n\n    def behavior_policy(\n        params: hk.Params, key: jax_types.PRNGKey,\n        observation: networks_lib.Observation) -> networks_lib.Action:\n      network_output = networks.policy_network.apply(\n          params, observation, is_training=False)\n      return networks.sample_fn(network_output, key)\n\n    return actor_core_lib.batched_feed_forward_to_actor_core(behavior_policy)",
  "def __init__(self, config):\n    \"\"\"Build a BVE agent.\n\n    Args:\n      config: The config of the BVE agent.\n    \"\"\"\n    self._config = config",
  "def make_learner(self,\n                   random_key: jax_types.PRNGKey,\n                   networks: bve_networks.BVENetworks,\n                   dataset: Iterator[utils.PrefetchingSplit],\n                   logger_fn: loggers.LoggerFactory,\n                   environment_spec: specs.EnvironmentSpec,\n                   counter: Optional[counting.Counter] = None) -> core.Learner:\n    del environment_spec\n\n    loss_fn = losses.BVELoss(\n        discount=self._config.discount,\n        max_abs_reward=self._config.max_abs_reward,\n        huber_loss_parameter=self._config.huber_loss_parameter,\n    )\n\n    return learning_lib.SGDLearner(\n        network=networks.policy_network,\n        random_key=random_key,\n        optimizer=optax.adam(\n            self._config.learning_rate, eps=self._config.adam_eps),\n        target_update_period=self._config.target_update_period,\n        data_iterator=dataset,\n        loss_fn=loss_fn,\n        counter=counter,\n        num_sgd_steps_per_step=self._config.num_sgd_steps_per_step,\n        logger=logger_fn('learner'))",
  "def make_actor(\n      self,\n      random_key: jax_types.PRNGKey,\n      policy: actor_core_lib.ActorCore,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None) -> core.Actor:\n    \"\"\"Create the actor for the BVE to perform online evals.\n\n    Args:\n      random_key: prng key.\n      policy: The DQN policy.\n      environment_spec: The environment spec.\n      variable_source: The source of where the variables are coming from.\n\n    Returns:\n      Return the actor for the evaluations.\n    \"\"\"\n    del environment_spec\n    variable_client = variable_utils.VariableClient(\n        variable_source, 'policy', device='cpu')\n    return actors.GenericActor(policy, random_key, variable_client)",
  "def make_policy(\n      self,\n      networks: bve_networks.BVENetworks,\n      environment_spec: specs.EnvironmentSpec,\n      evaluation: Optional[bool] = False) -> actor_core_lib.ActorCore:\n    \"\"\"Creates a policy.\"\"\"\n    del environment_spec, evaluation\n\n    def behavior_policy(\n        params: hk.Params, key: jax_types.PRNGKey,\n        observation: networks_lib.Observation) -> networks_lib.Action:\n      network_output = networks.policy_network.apply(\n          params, observation, is_training=False)\n      return networks.sample_fn(network_output, key)\n\n    return actor_core_lib.batched_feed_forward_to_actor_core(behavior_policy)",
  "def behavior_policy(\n        params: hk.Params, key: jax_types.PRNGKey,\n        observation: networks_lib.Observation) -> networks_lib.Action:\n      network_output = networks.policy_network.apply(\n          params, observation, is_training=False)\n      return networks.sample_fn(network_output, key)",
  "def return_weighted_average(action_trajectories: jnp.ndarray,\n                            cum_reward: jnp.ndarray,\n                            kappa: float) -> jnp.ndarray:\n  r\"\"\"Calculates return-weighted average over all trajectories.\n\n  This will calculate the return-weighted average over a set of trajectories as\n  defined on l.17 of Alg. 2 in the MBOP paper:\n  [https://arxiv.org/abs/2008.05556].\n\n  Note: Clipping will be performed for `cum_reward` values > 80 to avoid NaNs.\n\n  Args:\n    action_trajectories: (n_trajectories, horizon, action_dim) tensor of action\n      trajectories, corresponds to `A` in Alg. 2.\n    cum_reward: (n_trajectories) vector of corresponding cumulative rewards\n      (returns) for each trajectory. Corresponds to `\\mathcal{R}` in Alg. 2.\n    kappa: `\\kappa` constant, changes the 'peakiness' of the exponential\n      averaging.\n\n  Returns:\n    Single action trajectory corresponding to the return-weighted average of the\n      trajectories.\n  \"\"\"\n  # Substract maximum reward to avoid NaNs:\n  cum_reward = cum_reward - cum_reward.max()\n  # Remove the batch dimension of cum_reward allows for an implicit broadcast in\n  # jnp.average:\n  exp_cum_reward = jnp.exp(kappa * jnp.squeeze(cum_reward))\n  return jnp.average(action_trajectories, weights=exp_cum_reward, axis=0)",
  "def return_top_k_average(action_trajectories: jnp.ndarray,\n                         cum_reward: jnp.ndarray,\n                         k: int = 10) -> jnp.ndarray:\n  r\"\"\"Calculates the top-k average over all trajectories.\n\n  This will calculate the top-k average over a set of trajectories as\n  defined in the POIR Paper:\n\n  Note: top-k average is more numerically stable than the weighted average.\n\n  Args:\n    action_trajectories: (n_trajectories, horizon, action_dim) tensor of action\n      trajectories.\n    cum_reward: (n_trajectories) vector of corresponding cumulative rewards\n      (returns) for each trajectory.\n    k: the number of trajectories to average.\n\n  Returns:\n    Single action trajectory corresponding to the average of the k best\n      trajectories.\n  \"\"\"\n  top_k_trajectories = action_trajectories[jnp.argsort(\n      jnp.squeeze(cum_reward))[-int(k):]]\n  return jnp.mean(top_k_trajectories, axis=0)",
  "class MPPIConfig:\n  \"\"\"Config dataclass for MPPI-style planning, used in mppi.py.\n\n  These variables correspond to different parameters of `MBOP-Trajopt` as\n  defined in MBOP [https://arxiv.org/abs/2008.05556] (Alg. 2).\n\n  Attributes:\n    sigma: Variance of action-additive noise.\n    beta: Mixture parameter between old trajectory and new action.\n    horizon: Planning horizon, corresponds to H in Alg. 2 line 8.\n    n_trajectories: Number of trajectories used in `mppi_planner` to sample the\n      best action. Corresponds to `N` in Alg. 2 line. 5.\n    previous_trajectory_clip: Value to clip the previous_trajectory's actions\n      to. Disabled if None.\n    action_aggregation_fn: Function that aggregates action trajectories and\n      returns a single action trajectory.\n  \"\"\"\n  sigma: float = 0.8\n  beta: float = 0.2\n  horizon: int = 15\n  n_trajectories: int = 1000\n  previous_trajectory_clip: Optional[float] = None\n  action_aggregation_fn: ActionAggregationFn = (\n      functools.partial(return_weighted_average, kappa=0.5))",
  "def get_initial_trajectory(config: MPPIConfig, env_spec: specs.EnvironmentSpec):\n  \"\"\"Returns the initial empty trajectory `T_0`.\"\"\"\n  return jnp.zeros((max(1, config.horizon),) + env_spec.actions.shape)",
  "def _repeat_n(new_batch: int, data: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Create new batch dimension of size `new_batch` by repeating `data`.\"\"\"\n  return jnp.broadcast_to(data, (new_batch,) + data.shape)",
  "def mppi_planner(\n    config: MPPIConfig,\n    world_model: models.WorldModel,\n    policy_prior: models.PolicyPrior,\n    n_step_return: models.NStepReturn,\n    world_model_params: networks.Params,\n    policy_prior_params: networks.Params,\n    n_step_return_params: networks.Params,\n    random_key: networks.PRNGKey,\n    observation: networks.Observation,\n    previous_trajectory: jnp.ndarray,\n) -> jnp.ndarray:\n  \"\"\"MPPI-extended trajectory optimizer.\n\n  This implements the trajectory optimizer described in MBOP\n  [https://arxiv.org/abs/2008.05556] (Alg. 2) which is an extended version of\n  MPPI that adds support for arbitrary sampling distributions and extends the\n  return horizon using an approximate model of returns.  There are a couple\n  notation changes for readability:\n  A -> action_trajectories\n  T -> action_trajectory\n\n  If the horizon is set to 0, the planner will simply call the policy_prior\n  and average the action over the ensemble heads.\n\n  Args:\n    config: Base configuration parameters of MPPI.\n    world_model: Corresponds to `f_m(s_t, a_t)_s` in Alg. 2.\n    policy_prior: Corresponds to `f_b(s_t, a_tm1)` in Alg. 2.\n    n_step_return: Corresponds to `f_R(s_t, a_t)` in Alg. 2.\n    world_model_params: Parameters for world model.\n    policy_prior_params: Parameters for policy prior.\n    n_step_return_params: Parameters for n_step return.\n    random_key: JAX random key seed.\n    observation: Normalized current observation from the environment, `s` in\n      Alg. 2.\n    previous_trajectory: Normalized previous action trajectory. `T` in Alg 2.\n      Shape is [n_trajectories, horizon, action_dims].\n\n  Returns:\n    jnp.ndarray: Average action trajectory of shape [horizon, action_dims].\n  \"\"\"\n  action_trajectory_tm1 = previous_trajectory\n  policy_prior_state = policy_prior.init(random_key)\n\n  # Broadcast so that we have n_trajectories copies of each:\n  observation_t = jax.tree_map(\n      functools.partial(_repeat_n, config.n_trajectories), observation)\n  action_tm1 = jnp.broadcast_to(action_trajectory_tm1[0],\n                                (config.n_trajectories,) +\n                                action_trajectory_tm1[0].shape)\n\n  if config.previous_trajectory_clip is not None:\n    action_tm1 = jnp.clip(\n        action_tm1,\n        a_min=-config.previous_trajectory_clip,\n        a_max=config.previous_trajectory_clip)\n\n  # First check if planning is unnecessary:\n  if config.horizon == 0:\n    if hasattr(policy_prior_state, 'action_tm1'):\n      policy_prior_state = policy_prior_state.replace(action_tm1=action_tm1)\n    action_set, _ = policy_prior.select_action(policy_prior_params,\n                                               observation_t,\n                                               policy_prior_state)\n    # Need to re-create an action trajectory from a single action.\n    return jnp.broadcast_to(\n        jnp.mean(action_set, axis=0), (1, action_set.shape[-1]))\n\n  # Accumulators for returns and trajectories:\n  cum_reward = jnp.zeros((config.n_trajectories, 1))\n\n  # Generate noise once:\n  random_key, noise_key = random.split(random_key)\n  action_noise = config.sigma * random.normal(noise_key, (\n      (config.horizon,) + action_tm1.shape))\n\n  # Initialize empty set of action trajectories for concatenation in loop:\n  action_trajectories = jnp.zeros((config.n_trajectories, 0) +\n                                  action_trajectory_tm1[0].shape)\n\n  for t in range(config.horizon):\n    # Query policy prior for proposed action:\n    if hasattr(policy_prior_state, 'action_tm1'):\n      policy_prior_state = policy_prior_state.replace(action_tm1=action_tm1)\n    action_t, policy_prior_state = policy_prior.select_action(\n        policy_prior_params, observation_t, policy_prior_state)\n    # Add action noise:\n    action_t = action_t + action_noise[t]\n    # Mix action with previous trajectory's corresponding action:\n    action_t = (1 -\n                config.beta) * action_t + config.beta * action_trajectory_tm1[t]\n\n    # Query world model to get next observation and reward:\n    observation_tp1, reward_t = world_model(world_model_params, observation_t,\n                                            action_t)\n    cum_reward += reward_t\n\n    # Insert actions into trajectory matrix:\n    action_trajectories = jnp.concatenate(\n        [action_trajectories,\n         jnp.expand_dims(action_t, axis=1)], axis=1)\n    # Bump variable timesteps for next loop:\n    observation_t = observation_tp1\n    action_tm1 = action_t\n\n  # De-normalize and append the final n_step return prediction:\n  n_step_return_t = n_step_return(n_step_return_params, observation_t, action_t)\n  cum_reward += n_step_return_t\n\n  # Average the set of `n_trajectories` trajectories into a single trajectory.\n  return config.action_aggregation_fn(action_trajectories, cum_reward)",
  "def mse(a: jnp.ndarray, b: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"MSE distance.\"\"\"\n  return jnp.mean(jnp.square(a - b))",
  "def world_model_loss(apply_fn: Callable[[networks.Observation, networks.Action],\n                                        Tuple[networks.Observation,\n                                              jnp.ndarray]],\n                     steps: types.Transition) -> jnp.ndarray:\n  \"\"\"Returns the loss for the world model.\n\n  Args:\n    apply_fn: applies a transition model (o_t, a_t) -> (o_t+1, r), expects the\n      leading axis to index the batch and the second axis to index the\n      transition triplet (t-1, t, t+1).\n    steps: RLDS dictionary of transition triplets as prepared by\n      `rlds_loader.episode_to_timestep_batch`.\n\n  Returns:\n    A scalar loss value as jnp.ndarray.\n  \"\"\"\n  observation_t = jax.tree_map(lambda obs: obs[:, dataset.CURRENT, ...],\n                               steps.observation)\n  action_t = steps.action[:, dataset.CURRENT, ...]\n  observation_tp1 = jax.tree_map(lambda obs: obs[:, dataset.NEXT, ...],\n                                 steps.observation)\n  reward_t = steps.reward[:, dataset.CURRENT, ...]\n  (predicted_observation_tp1,\n   predicted_reward_t) = apply_fn(observation_t, action_t)\n  # predicted_* variables may have an extra outer dimension due to ensembling,\n  # the mse loss still works due to broadcasting however.\n  if len(observation_tp1.shape) != len(reward_t.shape):\n    # The rewards from the transitions may not have the last singular dimension.\n    reward_t = jnp.expand_dims(reward_t, axis=-1)\n  return mse(\n      jnp.concatenate([predicted_observation_tp1, predicted_reward_t], axis=-1),\n      jnp.concatenate([observation_tp1, reward_t], axis=-1))",
  "def policy_prior_loss(\n    apply_fn: Callable[[networks.Observation, networks.Action],\n                       networks.Action], steps: types.Transition):\n  \"\"\"Returns the loss for the policy prior.\n\n  Args:\n    apply_fn: applies a policy prior (o_t, a_t) -> a_t+1, expects the leading\n      axis to index the batch and the second axis to index the transition\n      triplet (t-1, t, t+1).\n    steps: RLDS dictionary of transition triplets as prepared by\n      `rlds_loader.episode_to_timestep_batch`.\n\n  Returns:\n    A scalar loss value as jnp.ndarray.\n  \"\"\"\n  observation_t = jax.tree_map(lambda obs: obs[:, dataset.CURRENT, ...],\n                               steps.observation)\n  action_tm1 = steps.action[:, dataset.PREVIOUS, ...]\n  action_t = steps.action[:, dataset.CURRENT, ...]\n\n  predicted_action_t = apply_fn(observation_t, action_tm1)\n  return mse(predicted_action_t, action_t)",
  "def return_loss(apply_fn: Callable[[networks.Observation, networks.Action],\n                                   jnp.ndarray], steps: types.Transition):\n  \"\"\"Returns the loss for the n-step return model.\n\n  Args:\n    apply_fn: applies an n-step return model (o_t, a_t) -> r, expects the\n      leading axis to index the batch and the second axis to index the\n      transition triplet (t-1, t, t+1).\n    steps: RLDS dictionary of transition triplets as prepared by\n      `rlds_loader.episode_to_timestep_batch`.\n\n  Returns:\n    A scalar loss value as jnp.ndarray.\n  \"\"\"\n  observation_t = jax.tree_map(lambda obs: obs[:, dataset.CURRENT, ...],\n                               steps.observation)\n  action_t = steps.action[:, dataset.CURRENT, ...]\n  n_step_return_t = steps.extras[dataset.N_STEP_RETURN][:, dataset.CURRENT, ...]\n\n  predicted_n_step_return_t = apply_fn(observation_t, action_t)\n  return mse(predicted_n_step_return_t, n_step_return_t)",
  "class MBOPLosses:\n  \"\"\"Losses for the world model, policy prior and the n-step return.\"\"\"\n  world_model_loss: Optional[TransitionLoss] = world_model_loss\n  policy_prior_loss: Optional[TransitionLoss] = policy_prior_loss\n  n_step_return_loss: Optional[TransitionLoss] = return_loss",
  "class TrainingState:\n  \"\"\"States of the world model, policy prior and n-step return learners.\"\"\"\n  world_model: Any\n  policy_prior: Any\n  n_step_return: Any",
  "def make_ensemble_regressor_learner(\n    name: str,\n    num_networks: int,\n    logger_fn: loggers.LoggerFactory,\n    counter: counting.Counter,\n    rng_key: jnp.ndarray,\n    iterator: Iterator[types.Transition],\n    base_network: networks_lib.FeedForwardNetwork,\n    loss: mbop_losses.TransitionLoss,\n    optimizer: optax.GradientTransformation,\n    num_sgd_steps_per_step: int,\n):\n  \"\"\"Creates an ensemble regressor learner from the base network.\n\n  Args:\n    name: Name of the learner used for logging and counters.\n    num_networks: Number of networks in the ensemble.\n    logger_fn: Constructs a logger for a label.\n    counter: Parent counter object.\n    rng_key: Random key.\n    iterator: An iterator of time-batched transitions used to train the\n      networks.\n    base_network: Base network for the ensemble.\n    loss: Training loss to use.\n    optimizer: Optax optimizer.\n    num_sgd_steps_per_step: Number of gradient updates per step.\n\n  Returns:\n    An ensemble regressor learner.\n  \"\"\"\n  mbop_ensemble = ensemble.make_ensemble(base_network, ensemble.apply_all,\n                                         num_networks)\n  local_counter = counting.Counter(parent=counter, prefix=name)\n  local_logger = logger_fn(name,\n                           local_counter.get_steps_key()) if logger_fn else None\n\n  def loss_fn(networks: bc.BCNetworks, params: networks_lib.Params,\n              key: jax_types.PRNGKey,\n              transitions: types.Transition) -> jnp.ndarray:\n    del key\n    return loss(\n        functools.partial(networks.policy_network.apply, params), transitions)\n\n  bc_policy_network = bc.convert_to_bc_network(mbop_ensemble)\n  bc_networks = bc.BCNetworks(bc_policy_network)\n\n  # This is effectively a regressor learner.\n  return bc.BCLearner(\n      bc_networks,\n      rng_key,\n      loss_fn,\n      optimizer,\n      iterator,\n      num_sgd_steps_per_step,\n      logger=local_logger,\n      counter=local_counter)",
  "class MBOPLearner(core.Learner):\n  \"\"\"Model-Based Offline Planning (MBOP) learner.\n\n  See https://arxiv.org/abs/2008.05556 for more information.\n  \"\"\"\n\n  def __init__(self,\n               networks: mbop_networks.MBOPNetworks,\n               losses: mbop_losses.MBOPLosses,\n               iterator: Iterator[types.Transition],\n               rng_key: jax_types.PRNGKey,\n               logger_fn: LoggerFn,\n               make_world_model_learner: MakeWorldModelLearner,\n               make_policy_prior_learner: MakePolicyPriorLearner,\n               make_n_step_return_learner: MakeNStepReturnLearner,\n               counter: Optional[counting.Counter] = None):\n    \"\"\"Creates an MBOP learner.\n\n    Args:\n      networks: One network per model.\n      losses: One loss per model.\n      iterator: An iterator of time-batched transitions used to train the\n        networks.\n      rng_key: Random key.\n      logger_fn: Constructs a logger for a label.\n      make_world_model_learner: Function to create the world model learner.\n      make_policy_prior_learner: Function to create the policy prior learner.\n      make_n_step_return_learner: Function to create the n-step return learner.\n      counter: Parent counter object.\n    \"\"\"\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger_fn('mbop', 'steps')\n\n    # Prepare iterators for the learners, to not split the data (preserve sample\n    # efficiency).\n    sharded_prefetching_dataset = utils.sharded_prefetch(iterator)\n    world_model_iterator, policy_prior_iterator, n_step_return_iterator = (\n        itertools.tee(sharded_prefetching_dataset, 3))\n\n    world_model_key, policy_prior_key, n_step_return_key = jax.random.split(\n        rng_key, 3)\n\n    self._world_model = make_world_model_learner(logger_fn, self._counter,\n                                                 world_model_key,\n                                                 world_model_iterator,\n                                                 networks.world_model_network,\n                                                 losses.world_model_loss)\n    self._policy_prior = make_policy_prior_learner(\n        logger_fn, self._counter, policy_prior_key, policy_prior_iterator,\n        networks.policy_prior_network, losses.policy_prior_loss)\n    self._n_step_return = make_n_step_return_learner(\n        logger_fn, self._counter, n_step_return_key, n_step_return_iterator,\n        networks.n_step_return_network, losses.n_step_return_loss)\n    # Start recording timestamps after the first learning step to not report\n    # \"warmup\" time.\n    self._timestamp = None\n    self._learners = {\n        'world_model': self._world_model,\n        'policy_prior': self._policy_prior,\n        'n_step_return': self._n_step_return\n    }\n\n  def step(self):\n    # Step the world model, policy learner and n-step return learners.\n    self._world_model.step()\n    self._policy_prior.step()\n    self._n_step_return.step()\n\n    # Compute the elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n    # Increment counts and record the current time.\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n    # Attempt to write the logs.\n    self._logger.write({**counts})\n\n  def get_variables(self, names: List[str]) -> List[types.NestedArray]:\n    variables = []\n    for name in names:\n      # Variables will be prefixed by the learner names. If separator is not\n      # found, learner_name=name, which is OK.\n      learner_name, _, variable_name = name.partition('-')\n      learner = self._learners[learner_name]\n      variables.extend(learner.get_variables([variable_name]))\n    return variables\n\n  def save(self) -> TrainingState:\n    return TrainingState(\n        world_model=self._world_model.save(),\n        policy_prior=self._policy_prior.save(),\n        n_step_return=self._n_step_return.save())\n\n  def restore(self, state: TrainingState):\n    self._world_model.restore(state.world_model)\n    self._policy_prior.restore(state.policy_prior)\n    self._n_step_return.restore(state.n_step_return)",
  "def loss_fn(networks: bc.BCNetworks, params: networks_lib.Params,\n              key: jax_types.PRNGKey,\n              transitions: types.Transition) -> jnp.ndarray:\n    del key\n    return loss(\n        functools.partial(networks.policy_network.apply, params), transitions)",
  "def __init__(self,\n               networks: mbop_networks.MBOPNetworks,\n               losses: mbop_losses.MBOPLosses,\n               iterator: Iterator[types.Transition],\n               rng_key: jax_types.PRNGKey,\n               logger_fn: LoggerFn,\n               make_world_model_learner: MakeWorldModelLearner,\n               make_policy_prior_learner: MakePolicyPriorLearner,\n               make_n_step_return_learner: MakeNStepReturnLearner,\n               counter: Optional[counting.Counter] = None):\n    \"\"\"Creates an MBOP learner.\n\n    Args:\n      networks: One network per model.\n      losses: One loss per model.\n      iterator: An iterator of time-batched transitions used to train the\n        networks.\n      rng_key: Random key.\n      logger_fn: Constructs a logger for a label.\n      make_world_model_learner: Function to create the world model learner.\n      make_policy_prior_learner: Function to create the policy prior learner.\n      make_n_step_return_learner: Function to create the n-step return learner.\n      counter: Parent counter object.\n    \"\"\"\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger_fn('mbop', 'steps')\n\n    # Prepare iterators for the learners, to not split the data (preserve sample\n    # efficiency).\n    sharded_prefetching_dataset = utils.sharded_prefetch(iterator)\n    world_model_iterator, policy_prior_iterator, n_step_return_iterator = (\n        itertools.tee(sharded_prefetching_dataset, 3))\n\n    world_model_key, policy_prior_key, n_step_return_key = jax.random.split(\n        rng_key, 3)\n\n    self._world_model = make_world_model_learner(logger_fn, self._counter,\n                                                 world_model_key,\n                                                 world_model_iterator,\n                                                 networks.world_model_network,\n                                                 losses.world_model_loss)\n    self._policy_prior = make_policy_prior_learner(\n        logger_fn, self._counter, policy_prior_key, policy_prior_iterator,\n        networks.policy_prior_network, losses.policy_prior_loss)\n    self._n_step_return = make_n_step_return_learner(\n        logger_fn, self._counter, n_step_return_key, n_step_return_iterator,\n        networks.n_step_return_network, losses.n_step_return_loss)\n    # Start recording timestamps after the first learning step to not report\n    # \"warmup\" time.\n    self._timestamp = None\n    self._learners = {\n        'world_model': self._world_model,\n        'policy_prior': self._policy_prior,\n        'n_step_return': self._n_step_return\n    }",
  "def step(self):\n    # Step the world model, policy learner and n-step return learners.\n    self._world_model.step()\n    self._policy_prior.step()\n    self._n_step_return.step()\n\n    # Compute the elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n    # Increment counts and record the current time.\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n    # Attempt to write the logs.\n    self._logger.write({**counts})",
  "def get_variables(self, names: List[str]) -> List[types.NestedArray]:\n    variables = []\n    for name in names:\n      # Variables will be prefixed by the learner names. If separator is not\n      # found, learner_name=name, which is OK.\n      learner_name, _, variable_name = name.partition('-')\n      learner = self._learners[learner_name]\n      variables.extend(learner.get_variables([variable_name]))\n    return variables",
  "def save(self) -> TrainingState:\n    return TrainingState(\n        world_model=self._world_model.save(),\n        policy_prior=self._policy_prior.save(),\n        n_step_return=self._n_step_return.save())",
  "def restore(self, state: TrainingState):\n    self._world_model.restore(state.world_model)\n    self._policy_prior.restore(state.policy_prior)\n    self._n_step_return.restore(state.n_step_return)",
  "def _append_n_step_return(output, n_step_return):\n  \"\"\"Append n-step return to an output step.\"\"\"\n  output[N_STEP_RETURN] = n_step_return\n  return output",
  "def _append_episode_return(output, episode_return):\n  \"\"\"Append episode return to an output step.\"\"\"\n  output[EPISODE_RETURN] = episode_return\n  return output",
  "def _expand_scalars(output):\n  \"\"\"If rewards are scalar, expand them.\"\"\"\n  return tree.map_structure(tf.experimental.numpy.atleast_1d, output)",
  "def episode_to_timestep_batch(\n    episode: rlds.BatchedStep,\n    return_horizon: int = 0,\n    drop_return_horizon: bool = False,\n    calculate_episode_return: bool = False) -> tf.data.Dataset:\n  \"\"\"Converts an episode into multi-timestep batches.\n\n  Args:\n    episode: Batched steps as provided directly by RLDS.\n    return_horizon: int describing the horizon to which we should accumulate the\n      return.\n    drop_return_horizon: bool whether we should drop the last `return_horizon`\n      steps to avoid mis-calculated returns near the end of the episode.\n    calculate_episode_return: Whether to calculate episode return.  Can be an\n      expensive operation on datasets with many episodes.\n\n  Returns:\n    rl_dataset.DatasetType of 3-batched transitions, with scalar rewards\n      expanded to 1D rewards\n\n  This means that for every step, the corresponding elements will be a batch of\n  size 3, with the first batched element corresponding to *_t-1, the second to\n  *_t and the third to *_t+1,  e.g. you can access the previous observation as:\n  ```\n  o_tm1 = el[types.OBSERVATION][0]\n  ```\n  Two additional keys can be added: 'R_t' which corresponds to the undiscounted\n  return for horizon `return_horizon` from time t (always present), and\n  'R_total' which corresponds to the total return of the associated episode (if\n  `calculate_episode_return` is True). Rewards are converted to be (at least)\n  one-dimensional, prior to batching (to avoid ()-shaped elements).\n\n  In this example, 0-valued observations correspond to o_{t-1}, 1-valued\n  observations correspond to o_t, and 2-valued observations correspond to\n  s_{t+1}.  This same structure is true for all keys, except 'R_t' and 'R_total'\n  which are both scalars.\n  ```\n  ipdb> el[types.OBSERVATION]\n  <tf.Tensor: shape=(3, 11), dtype=float32, numpy=\n  array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]], dtype=float32)>\n  ```\n  \"\"\"\n  steps = episode[rlds.STEPS]\n\n  if drop_return_horizon:\n    episode_length = steps.cardinality()\n    steps = steps.take(episode_length - return_horizon)\n\n  # Calculate n-step return:\n  rewards = steps.map(lambda step: step[rlds.REWARD])\n  batched_rewards = rlds.transformations.batch(\n      rewards, size=return_horizon, shift=1, stride=1, drop_remainder=True)\n  returns = batched_rewards.map(tf.math.reduce_sum)\n  output = tf.data.Dataset.zip((steps, returns)).map(_append_n_step_return)\n\n  # Calculate total episode return for potential filtering, use total # of steps\n  # to calculate return.\n  if calculate_episode_return:\n    dtype = jnp.float64 if jax.config.jax_enable_x64 else jnp.float32\n    # Need to redefine this here to avoid a tf.data crash.\n    rewards = steps.map(lambda step: step[rlds.REWARD])\n    episode_return = rewards.reduce(dtype(0), lambda x, y: x + y)\n    output = output.map(\n        functools.partial(\n            _append_episode_return, episode_return=episode_return))\n\n  output = output.map(_expand_scalars)\n\n  output = rlds.transformations.batch(\n      output, size=3, shift=1, drop_remainder=True)\n  return output",
  "def _step_to_transition(rlds_step: rlds.BatchedStep) -> types.Transition:\n  \"\"\"Converts batched RLDS steps to batched transitions.\"\"\"\n  return types.Transition(\n      observation=rlds_step[rlds.OBSERVATION],\n      action=rlds_step[rlds.ACTION],\n      reward=rlds_step[rlds.REWARD],\n      discount=rlds_step[rlds.DISCOUNT],\n      #  We provide next_observation if an algorithm needs it, however note that\n      # it will only contain s_t and s_t+1, so will be one element short of all\n      # other attributes (which contain s_t-1, s_t, s_t+1).\n      next_observation=tree.map_structure(lambda x: x[1:],\n                                          rlds_step[rlds.OBSERVATION]),\n      extras={\n          N_STEP_RETURN: rlds_step[N_STEP_RETURN],\n      })",
  "def episodes_to_timestep_batched_transitions(\n    episode_dataset: tf.data.Dataset,\n    return_horizon: int = 10,\n    drop_return_horizon: bool = False,\n    min_return_filter: Optional[float] = None) -> tf.data.Dataset:\n  \"\"\"Process an existing dataset converting it to episode to 3-transitions.\n\n  A 3-transition is an Transition with each attribute having an extra dimension\n  of size 3, representing 3 consecutive timesteps. Each 3-step object will be\n  in random order relative to each other.  See `episode_to_timestep_batch` for\n  more information.\n\n  Args:\n    episode_dataset: An RLDS dataset to process.\n    return_horizon: The horizon we want calculate Monte-Carlo returns to.\n    drop_return_horizon: Whether we should drop the last `return_horizon` steps.\n    min_return_filter: Minimum episode return below which we drop an episode.\n\n  Returns:\n    A tf.data.Dataset of 3-transitions.\n  \"\"\"\n  dataset = episode_dataset.interleave(\n      functools.partial(\n          episode_to_timestep_batch,\n          return_horizon=return_horizon,\n          drop_return_horizon=drop_return_horizon,\n          calculate_episode_return=min_return_filter is not None),\n      num_parallel_calls=tf.data.experimental.AUTOTUNE,\n      deterministic=False)\n\n  if min_return_filter is not None:\n\n    def filter_on_return(step):\n      return step[EPISODE_RETURN][0][0] > min_return_filter\n\n    dataset = dataset.filter(filter_on_return)\n\n  dataset = dataset.map(\n      _step_to_transition, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n  return dataset",
  "def get_normalization_stats(\n    iterator: Iterator[types.Transition],\n    num_normalization_batches: int = 50\n) -> running_statistics.RunningStatisticsState:\n  \"\"\"Precomputes normalization statistics over a fixed number of batches.\n\n  The iterator should contain batches of 3-transitions, i.e. with two leading\n  dimensions, the first one denoting the batch dimension and the second one the\n  previous, current and next timesteps. The statistics are calculated using the\n  data of the previous timestep.\n\n  Args:\n    iterator: Iterator of batchs of 3-transitions.\n    num_normalization_batches: Number of batches to calculate the statistics.\n\n  Returns:\n    RunningStatisticsState containing the normalization statistics.\n  \"\"\"\n  # Set up normalization:\n  example = next(iterator)\n  unbatched_single_example = jax.tree_map(lambda x: x[0, PREVIOUS, :], example)\n  mean_std = running_statistics.init_state(unbatched_single_example)\n\n  for batch in itertools.islice(iterator, num_normalization_batches - 1):\n    example = jax.tree_map(lambda x: x[:, PREVIOUS, :], batch)\n    mean_std = running_statistics.update(mean_std, example)\n\n  return mean_std",
  "def filter_on_return(step):\n      return step[EPISODE_RETURN][0][0] > min_return_filter",
  "class MBOPNetworks:\n  \"\"\"Container class to hold MBOP networks.\"\"\"\n  world_model_network: WorldModelNetwork\n  policy_prior_network: PolicyPriorNetwork\n  n_step_return_network: NStepReturnNetwork",
  "def make_network_from_module(\n    module: hk.Transformed,\n    spec: specs.EnvironmentSpec) -> networks.FeedForwardNetwork:\n  \"\"\"Creates a network with dummy init arguments using the specified module.\n\n  Args:\n    module: Module that expects one batch axis and one features axis for its\n      inputs.\n    spec: EnvironmentSpec shapes to derive dummy inputs.\n\n  Returns:\n    FeedForwardNetwork whose `init` method only takes a random key, and `apply`\n    takes an observation and action and produces an output.\n  \"\"\"\n  dummy_obs = utils.add_batch_dim(utils.zeros_like(spec.observations))\n  dummy_action = utils.add_batch_dim(utils.zeros_like(spec.actions))\n  return networks.FeedForwardNetwork(\n      lambda key: module.init(key, dummy_obs, dummy_action), module.apply)",
  "def make_world_model_network(\n    spec: specs.EnvironmentSpec, hidden_layer_sizes: Tuple[int, ...] = (64, 64)\n) -> networks.FeedForwardNetwork:\n  \"\"\"Creates a world model network used by the agent.\"\"\"\n\n  observation_size = np.prod(spec.observations.shape, dtype=int)\n\n  def _world_model_fn(observation_t, action_t, is_training=False, key=None):\n    # is_training and key allows to defined train/test dependant modules\n    # like dropout.\n    del is_training\n    del key\n    network = hk.nets.MLP(hidden_layer_sizes + (observation_size + 1,))\n    # World model returns both an observation and a reward.\n    observation_tp1, reward_t = jnp.split(\n        network(jnp.concatenate([observation_t, action_t], axis=-1)),\n        [observation_size],\n        axis=-1)\n    return observation_tp1, reward_t\n\n  world_model = hk.without_apply_rng(hk.transform(_world_model_fn))\n  return make_network_from_module(world_model, spec)",
  "def make_policy_prior_network(\n    spec: specs.EnvironmentSpec, hidden_layer_sizes: Tuple[int, ...] = (64, 64)\n) -> networks.FeedForwardNetwork:\n  \"\"\"Creates a policy prior network used by the agent.\"\"\"\n\n  action_size = np.prod(spec.actions.shape, dtype=int)\n\n  def _policy_prior_fn(observation_t, action_tm1, is_training=False, key=None):\n    # is_training and key allows to defined train/test dependant modules\n    # like dropout.\n    del is_training\n    del key\n    network = hk.nets.MLP(hidden_layer_sizes + (action_size,))\n    # Policy prior returns an action.\n    return network(jnp.concatenate([observation_t, action_tm1], axis=-1))\n\n  policy_prior = hk.without_apply_rng(hk.transform(_policy_prior_fn))\n  return make_network_from_module(policy_prior, spec)",
  "def make_n_step_return_network(\n    spec: specs.EnvironmentSpec, hidden_layer_sizes: Tuple[int, ...] = (64, 64)\n) -> networks.FeedForwardNetwork:\n  \"\"\"Creates an N-step return network used by the agent.\"\"\"\n\n  def _n_step_return_fn(observation_t, action_t, is_training=False, key=None):\n    # is_training and key allows to defined train/test dependant modules\n    # like dropout.\n    del is_training\n    del key\n    network = hk.nets.MLP(hidden_layer_sizes + (1,))\n    return network(jnp.concatenate([observation_t, action_t], axis=-1))\n\n  n_step_return = hk.without_apply_rng(hk.transform(_n_step_return_fn))\n  return make_network_from_module(n_step_return, spec)",
  "def make_networks(\n    spec: specs.EnvironmentSpec,\n    hidden_layer_sizes: Tuple[int, ...] = (64, 64),\n) -> MBOPNetworks:\n  \"\"\"Creates networks used by the agent.\"\"\"\n  world_model_network = make_world_model_network(\n      spec, hidden_layer_sizes=hidden_layer_sizes)\n  policy_prior_network = make_policy_prior_network(\n      spec, hidden_layer_sizes=hidden_layer_sizes)\n  n_step_return_network = make_n_step_return_network(\n      spec, hidden_layer_sizes=hidden_layer_sizes)\n\n  return MBOPNetworks(\n      world_model_network=world_model_network,\n      policy_prior_network=policy_prior_network,\n      n_step_return_network=n_step_return_network)",
  "def _world_model_fn(observation_t, action_t, is_training=False, key=None):\n    # is_training and key allows to defined train/test dependant modules\n    # like dropout.\n    del is_training\n    del key\n    network = hk.nets.MLP(hidden_layer_sizes + (observation_size + 1,))\n    # World model returns both an observation and a reward.\n    observation_tp1, reward_t = jnp.split(\n        network(jnp.concatenate([observation_t, action_t], axis=-1)),\n        [observation_size],\n        axis=-1)\n    return observation_tp1, reward_t",
  "def _policy_prior_fn(observation_t, action_tm1, is_training=False, key=None):\n    # is_training and key allows to defined train/test dependant modules\n    # like dropout.\n    del is_training\n    del key\n    network = hk.nets.MLP(hidden_layer_sizes + (action_size,))\n    # Policy prior returns an action.\n    return network(jnp.concatenate([observation_t, action_tm1], axis=-1))",
  "def _n_step_return_fn(observation_t, action_t, is_training=False, key=None):\n    # is_training and key allows to defined train/test dependant modules\n    # like dropout.\n    del is_training\n    del key\n    network = hk.nets.MLP(hidden_layer_sizes + (1,))\n    return network(jnp.concatenate([observation_t, action_t], axis=-1))",
  "def _split_batch_dimension(new_batch: int, data: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Splits the batch dimension and introduces new one with size `new_batch`.\n\n  The result has two batch dimensions, first one of size `new_batch`, second one\n  of size `data.shape[0]/new_batch`. It expects that `data.shape[0]` is\n  divisible by `new_batch`.\n\n  Args:\n    new_batch: Dimension of outer batch dimension.\n    data: jnp.ndarray to be reshaped.\n\n  Returns:\n    jnp.ndarray with extra batch dimension at start and updated second\n    dimension.\n  \"\"\"\n  # The first dimension will be used for allocating to a specific ensemble\n  # member, and the second dimension is the parallelized batch dimension, and\n  # the remaining dimensions are passed as-is to the wrapped network.\n  # We use Fortan (F) order so that each input batch i is allocated to\n  # ensemble member k = i % new_batch.\n  return jnp.reshape(data, (new_batch, -1) + data.shape[1:], order='F')",
  "def _repeat_n(new_batch: int, data: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Create new batch dimension of size `new_batch` by repeating `data`.\"\"\"\n  return jnp.broadcast_to(data, (new_batch,) + data.shape)",
  "def ensemble_init(base_init: Callable[[networks.PRNGKey], networks.Params],\n                  num_networks: int, rnd: jnp.ndarray):\n  \"\"\"Initializes the ensemble parameters.\n\n  Args:\n    base_init: An init function that takes only a PRNGKey, if a network's init\n      function requires other parameters such as example inputs they need to\n      have been previously wrapped i.e. with functool.partial using kwargs.\n    num_networks: Number of networks to generate parameters for.\n    rnd: PRNGKey to split from when generating parameters.\n\n  Returns:\n    `params` for the set of ensemble networks.\n  \"\"\"\n  rnds = jax.random.split(rnd, num_networks)\n  return jax.vmap(base_init)(rnds)",
  "def apply_round_robin(base_apply: Callable[[networks.Params, Any], Any],\n                      params: networks.Params, *args, **kwargs) -> Any:\n  \"\"\"Passes the input in a round-robin manner.\n\n  The round-robin application means that each element of the input batch will\n  be passed through a single ensemble member in a deterministic round-robin\n  manner, i.e. element_i -> member_k where k = i % num_networks.\n\n  It expects that:\n  * `base_apply(member_params, *member_args, **member_kwargs)` is a valid call,\n     where:\n    * `member_params.shape = params.shape[1:]`\n    * `member_args` and `member_kwargs` have the same structure as `args` and\n      `kwargs`.\n  * `params[k]` contains the params of the k-th member of the ensemble.\n  * All jax arrays in `args` and `kwargs` have a batch dimension at axis 0 of\n    the same size, which is divisible by `params.shape[0]`.\n\n  Args:\n    base_apply: Base network `apply` function that will be used for round-robin.\n      NOTE -- This will not work with mutable/stateful apply functions. --\n    params: Model parameters.  Number of networks is deduced from this.\n    *args: Allows for arbitrary call signatures for `base_apply`.\n    **kwargs: Allows for arbitrary call signatures for `base_apply`.\n\n  Returns:\n    pytree of the round-robin application.\n    Output shape will be [initial_batch_size, <remaining dimensions>].\n  \"\"\"\n  # `num_networks` is the size of the batch dimension in `params`.\n  num_networks = jax.tree_util.tree_leaves(params)[0].shape[0]\n\n  # Reshape args and kwargs for the round-robin:\n  args = jax.tree_map(\n      functools.partial(_split_batch_dimension, num_networks), args)\n  kwargs = jax.tree_map(\n      functools.partial(_split_batch_dimension, num_networks), kwargs)\n  # `out.shape` is `(num_networks, initial_batch_size/num_networks, ...)\n  out = jax.vmap(base_apply)(params, *args, **kwargs)\n  # Reshape to [initial_batch_size, <remaining dimensions>]. Using the 'F' order\n  # forces the original values to the last dimension.\n  return jax.tree_map(lambda x: x.reshape((-1,) + x.shape[2:], order='F'), out)",
  "def apply_all(base_apply: Callable[[networks.Params, Any], Any],\n              params: networks.Params, *args, **kwargs) -> Any:\n  \"\"\"Pass the input to all ensemble members.\n\n  Inputs can either have a batch dimension which will get implicitly vmapped\n  over, or can be a single vector which will get sent to all ensemble members.\n  e.g. [<inputs_dims>] or [batch_size, <input_dims>].\n\n  Args:\n    base_apply: Base network `apply` function that will be used for averaging.\n      NOTE -- This will not work with mutable/stateful apply functions. --\n    params: Model parameters.  Number of networks is deduced from this.\n    *args: Allows for arbitrary call signatures for `base_apply`.\n    **kwargs: Allows for arbitrary call signatures for `base_apply`.\n\n  Returns:\n    pytree of the resulting output of passing input to all ensemble members.\n    Output shape will be [num_members, batch_size, <network output dims>].\n  \"\"\"\n  # `num_networks` is the size of the batch dimension in `params`.\n  num_networks = jax.tree_util.tree_leaves(params)[0].shape[0]\n\n  args = jax.tree_map(functools.partial(_repeat_n, num_networks), args)\n  kwargs = jax.tree_map(functools.partial(_repeat_n, num_networks), kwargs)\n  # `out` is of shape `(num_networks, batch_size, <remaining dimensions>)`.\n  return jax.vmap(base_apply)(params, *args, **kwargs)",
  "def apply_mean(base_apply: Callable[[networks.Params, Any], Any],\n               params: networks.Params, *args, **kwargs) -> Any:\n  \"\"\"Calculates the mean over all ensemble members for each batch element.\n\n  Args:\n    base_apply: Base network `apply` function that will be used for averaging.\n      NOTE -- This will not work with mutable/stateful apply functions. --\n    params: Model parameters.  Number of networks is deduced from this.\n    *args: Allows for arbitrary call signatures for `base_apply`.\n    **kwargs: Allows for arbitrary call signatures for `base_apply`.\n\n  Returns:\n    pytree of the average over all ensembles for each element.\n    Output shape will be [batch_size, <network output_dims>]\n  \"\"\"\n  out = apply_all(base_apply, params, *args, **kwargs)\n  return jax.tree_map(functools.partial(jnp.mean, axis=0), out)",
  "def make_ensemble(base_network: networks.FeedForwardNetwork,\n                  ensemble_apply: Callable[..., Any],\n                  num_networks: int) -> networks.FeedForwardNetwork:\n  return networks.FeedForwardNetwork(\n      init=functools.partial(ensemble_init, base_network.init, num_networks),\n      apply=functools.partial(ensemble_apply, base_network.apply))",
  "class MBOPConfig:\n  \"\"\"Configuration options for the MBOP agent.\n\n  Attributes:\n    mppi_config: Planner hyperparameters.\n    learning_rate: Learning rate.\n    num_networks: Number of networks in the ensembles.\n    num_sgd_steps_per_step: How many gradient updates to perform per learner\n      step.\n  \"\"\"\n  mppi_config: mppi.MPPIConfig = dataclasses.field(\n      default_factory=mppi.MPPIConfig\n  )\n  learning_rate: float = 3e-4\n  num_networks: int = 5\n  num_sgd_steps_per_step: int = 1",
  "def make_actor_core(\n    mppi_config: mppi.MPPIConfig,\n    world_model: models.WorldModel,\n    policy_prior: models.PolicyPrior,\n    n_step_return: models.NStepReturn,\n    environment_spec: specs.EnvironmentSpec,\n    mean_std: Optional[running_statistics.NestedMeanStd] = None,\n) -> ActorCore:\n  \"\"\"Creates an actor core wrapping the MBOP-configured MPPI planner.\n\n  Args:\n    mppi_config: Planner hyperparameters.\n    world_model: A world model.\n    policy_prior: A policy prior.\n    n_step_return: An n-step return.\n    environment_spec: Used to initialize the initial trajectory data structure.\n    mean_std: Used to undo normalization if the networks trained normalized.\n\n  Returns:\n    A recurrent actor core.\n  \"\"\"\n\n  if mean_std is not None:\n    mean_std_observation = running_statistics.NestedMeanStd(\n        mean=mean_std.mean.observation, std=mean_std.std.observation)\n    mean_std_action = running_statistics.NestedMeanStd(\n        mean=mean_std.mean.action, std=mean_std.std.action)\n    mean_std_reward = running_statistics.NestedMeanStd(\n        mean=mean_std.mean.reward, std=mean_std.std.reward)\n    mean_std_n_step_return = running_statistics.NestedMeanStd(\n        mean=mean_std.mean.extras['n_step_return'],\n        std=mean_std.std.extras['n_step_return'])\n\n    def denormalized_world_model(\n        params: networks_lib.Params, observation_t: networks_lib.Observation,\n        action_t: networks_lib.Action\n    ) -> Tuple[networks_lib.Observation, networks_lib.Value]:\n      \"\"\"Denormalizes the reward for proper weighting in the planner.\"\"\"\n      observation_tp1, normalized_reward_t = world_model(\n          params, observation_t, action_t)\n      reward_t = running_statistics.denormalize(normalized_reward_t,\n                                                mean_std_reward)\n      return observation_tp1, reward_t\n\n    planner_world_model = denormalized_world_model\n\n    def denormalized_n_step_return(\n        params: networks_lib.Params, observation_t: networks_lib.Observation,\n        action_t: networks_lib.Action) -> networks_lib.Value:\n      \"\"\"Denormalize the n-step return for proper weighting in the planner.\"\"\"\n      normalized_n_step_return_t = n_step_return(params, observation_t,\n                                                 action_t)\n      return running_statistics.denormalize(normalized_n_step_return_t,\n                                            mean_std_n_step_return)\n\n    planner_n_step_return = denormalized_n_step_return\n  else:\n    planner_world_model = world_model\n    planner_n_step_return = n_step_return\n\n  def recurrent_policy(\n      params_list: List[networks_lib.Params],\n      random_key: networks_lib.PRNGKey,\n      observation: networks_lib.Observation,\n      previous_trajectory: Trajectory,\n  ) -> Tuple[networks_lib.Action, Trajectory]:\n    # Note that splitting the random key is handled by GenericActor.\n    if mean_std is not None:\n      observation = running_statistics.normalize(\n          observation, mean_std=mean_std_observation)\n    trajectory = mppi.mppi_planner(\n        config=mppi_config,\n        world_model=planner_world_model,\n        policy_prior=policy_prior,\n        n_step_return=planner_n_step_return,\n        world_model_params=params_list[0],\n        policy_prior_params=params_list[1],\n        n_step_return_params=params_list[2],\n        random_key=random_key,\n        observation=observation,\n        previous_trajectory=previous_trajectory)\n    action = trajectory[0, ...]\n    if mean_std is not None:\n      action = running_statistics.denormalize(action, mean_std=mean_std_action)\n    return (action, trajectory)\n\n  batched_policy = jax.vmap(recurrent_policy, in_axes=(None, None, 0, 0))\n  batched_policy = jax.jit(batched_policy)\n\n  initial_trajectory = mppi.get_initial_trajectory(\n      config=mppi_config, env_spec=environment_spec)\n  initial_trajectory = jnp.expand_dims(initial_trajectory, axis=0)\n\n  return actor_core_lib.batched_recurrent_to_actor_core(batched_policy,\n                                                        initial_trajectory)",
  "def make_ensemble_actor_core(\n    networks: mbop_networks.MBOPNetworks,\n    mppi_config: mppi.MPPIConfig,\n    environment_spec: specs.EnvironmentSpec,\n    mean_std: Optional[running_statistics.NestedMeanStd] = None,\n    use_round_robin: bool = True,\n) -> ActorCore:\n  \"\"\"Creates an actor core that uses ensemble models.\n\n  Args:\n    networks: MBOP networks.\n    mppi_config: Planner hyperparameters.\n    environment_spec: Used to initialize the initial trajectory data structure.\n    mean_std: Used to undo normalization if the networks trained normalized.\n    use_round_robin: Whether to use round robin or mean to calculate the policy\n      prior over the ensemble members.\n\n  Returns:\n    A recurrent actor core.\n  \"\"\"\n  world_model = models.make_ensemble_world_model(networks.world_model_network)\n  policy_prior = models.make_ensemble_policy_prior(\n      networks.policy_prior_network,\n      environment_spec,\n      use_round_robin=use_round_robin)\n  n_step_return = models.make_ensemble_n_step_return(\n      networks.n_step_return_network)\n\n  return make_actor_core(mppi_config, world_model, policy_prior, n_step_return,\n                         environment_spec, mean_std)",
  "def make_actor(actor_core: ActorCore,\n               random_key: networks_lib.PRNGKey,\n               variable_source: core.VariableSource,\n               adder: Optional[adders.Adder] = None) -> core.Actor:\n  \"\"\"Creates an MBOP actor from an actor core.\n\n  Args:\n    actor_core: An MBOP actor core.\n    random_key: JAX Random key.\n    variable_source: The source to get networks parameters from.\n    adder: An adder to add experiences to. The `extras` of the adder holds the\n      state of the recurrent policy. If `has_extras=True` then the `extras` part\n      returned from the recurrent policy is appended to the state before added\n      to the adder.\n\n  Returns:\n    A recurrent actor.\n  \"\"\"\n  variable_client = variable_utils.VariableClient(\n      client=variable_source,\n      key=['world_model-policy', 'policy_prior-policy', 'n_step_return-policy'])\n\n  return actors.GenericActor(\n      actor_core, random_key, variable_client, adder, backend=None)",
  "def recurrent_policy(\n      params_list: List[networks_lib.Params],\n      random_key: networks_lib.PRNGKey,\n      observation: networks_lib.Observation,\n      previous_trajectory: Trajectory,\n  ) -> Tuple[networks_lib.Action, Trajectory]:\n    # Note that splitting the random key is handled by GenericActor.\n    if mean_std is not None:\n      observation = running_statistics.normalize(\n          observation, mean_std=mean_std_observation)\n    trajectory = mppi.mppi_planner(\n        config=mppi_config,\n        world_model=planner_world_model,\n        policy_prior=policy_prior,\n        n_step_return=planner_n_step_return,\n        world_model_params=params_list[0],\n        policy_prior_params=params_list[1],\n        n_step_return_params=params_list[2],\n        random_key=random_key,\n        observation=observation,\n        previous_trajectory=previous_trajectory)\n    action = trajectory[0, ...]\n    if mean_std is not None:\n      action = running_statistics.denormalize(action, mean_std=mean_std_action)\n    return (action, trajectory)",
  "def denormalized_world_model(\n        params: networks_lib.Params, observation_t: networks_lib.Observation,\n        action_t: networks_lib.Action\n    ) -> Tuple[networks_lib.Observation, networks_lib.Value]:\n      \"\"\"Denormalizes the reward for proper weighting in the planner.\"\"\"\n      observation_tp1, normalized_reward_t = world_model(\n          params, observation_t, action_t)\n      reward_t = running_statistics.denormalize(normalized_reward_t,\n                                                mean_std_reward)\n      return observation_tp1, reward_t",
  "def denormalized_n_step_return(\n        params: networks_lib.Params, observation_t: networks_lib.Observation,\n        action_t: networks_lib.Action) -> networks_lib.Value:\n      \"\"\"Denormalize the n-step return for proper weighting in the planner.\"\"\"\n      normalized_n_step_return_t = n_step_return(params, observation_t,\n                                                 action_t)\n      return running_statistics.denormalize(normalized_n_step_return_t,\n                                            mean_std_n_step_return)",
  "class PolicyPriorState(Generic[actor_core.RecurrentState]):\n  \"\"\"State of a policy prior.\n\n  Attributes:\n    rng: Random key.\n    action_tm1: Previous action.\n    recurrent_state: Recurrent state. It will be none for non-recurrent, e.g.\n      feed forward, policies.\n  \"\"\"\n  rng: networks.PRNGKey\n  action_tm1: networks.Action\n  recurrent_state: Optional[actor_core.RecurrentState] = None",
  "def feed_forward_policy_prior_to_actor_core(\n    policy: actor_core.RecurrentPolicy, initial_action_tm1: networks.Action\n) -> actor_core.ActorCore[PolicyPriorState, actor_core.NoneType]:\n  \"\"\"A convenience adaptor from a feed forward policy prior to ActorCore.\n\n  Args:\n    policy: A feed forward policy prior. In the planner and other components,\n      the previous action is explicitly passed as an argument to the policy\n      prior together with the observation to infer the next action. Therefore,\n      we model feed forward policy priors as recurrent ActorCore policies with\n      previous action being the recurrent state.\n    initial_action_tm1: Initial previous action. This will usually be a zero\n      tensor.\n\n  Returns:\n    an ActorCore representing the feed forward policy prior.\n  \"\"\"\n\n  def select_action(params: networks.Params, observation: networks.Observation,\n                    state: FeedForwardPolicyState):\n    rng, policy_rng = jax.random.split(state.rng)\n    action = policy(params, policy_rng, observation, state.action_tm1)\n    return action, PolicyPriorState(rng, action)\n\n  def init(rng: networks.PRNGKey) -> FeedForwardPolicyState:\n    return PolicyPriorState(rng, initial_action_tm1)\n\n  def get_extras(unused_state: FeedForwardPolicyState) -> actor_core.NoneType:\n    return None\n\n  return actor_core.ActorCore(\n      init=init, select_action=select_action, get_extras=get_extras)",
  "def make_ensemble_world_model(\n    world_model_network: mbop_networks.WorldModelNetwork) -> WorldModel:\n  \"\"\"Creates an ensemble world model from its network.\"\"\"\n  return functools.partial(ensemble.apply_round_robin,\n                           world_model_network.apply)",
  "def make_ensemble_policy_prior(\n    policy_prior_network: mbop_networks.PolicyPriorNetwork,\n    spec: specs.EnvironmentSpec,\n    use_round_robin: bool = True) -> PolicyPrior:\n  \"\"\"Creates an ensemble policy prior from its network.\n\n  Args:\n    policy_prior_network: The policy prior network.\n    spec: Environment specification.\n    use_round_robin: Whether to use round robin or mean to calculate the policy\n      prior over the ensemble members.\n\n  Returns:\n    A policy prior.\n  \"\"\"\n\n  def _policy_prior(params: networks.Params, key: networks.PRNGKey,\n                    observation_t: networks.Observation,\n                    action_tm1: networks.Action) -> networks.Action:\n    # Regressor policies are deterministic.\n    del key\n    apply_fn = (\n        ensemble.apply_round_robin if use_round_robin else ensemble.apply_mean)\n    return apply_fn(\n        policy_prior_network.apply,\n        params,\n        observation_t=observation_t,\n        action_tm1=action_tm1)\n\n  dummy_action = utils.zeros_like(spec.actions)\n  dummy_action = utils.add_batch_dim(dummy_action)\n\n  return feed_forward_policy_prior_to_actor_core(_policy_prior, dummy_action)",
  "def make_ensemble_n_step_return(\n    n_step_return_network: mbop_networks.NStepReturnNetwork) -> NStepReturn:\n  \"\"\"Creates an ensemble n-step return model from its network.\"\"\"\n  return functools.partial(ensemble.apply_mean, n_step_return_network.apply)",
  "def select_action(params: networks.Params, observation: networks.Observation,\n                    state: FeedForwardPolicyState):\n    rng, policy_rng = jax.random.split(state.rng)\n    action = policy(params, policy_rng, observation, state.action_tm1)\n    return action, PolicyPriorState(rng, action)",
  "def init(rng: networks.PRNGKey) -> FeedForwardPolicyState:\n    return PolicyPriorState(rng, initial_action_tm1)",
  "def get_extras(unused_state: FeedForwardPolicyState) -> actor_core.NoneType:\n    return None",
  "def _policy_prior(params: networks.Params, key: networks.PRNGKey,\n                    observation_t: networks.Observation,\n                    action_tm1: networks.Action) -> networks.Action:\n    # Regressor policies are deterministic.\n    del key\n    apply_fn = (\n        ensemble.apply_round_robin if use_round_robin else ensemble.apply_mean)\n    return apply_fn(\n        policy_prior_network.apply,\n        params,\n        observation_t=observation_t,\n        action_tm1=action_tm1)",
  "class RandomFFN(nn.Module):\n\n  @nn.compact\n  def __call__(self, x):\n    return nn.Dense(15)(x)",
  "def params_adding_ffn(x: jnp.ndarray) -> networks.FeedForwardNetwork:\n  \"\"\"Apply adds the parameters to the inputs.\"\"\"\n  return networks.FeedForwardNetwork(\n      init=lambda key, x=x: jax.random.uniform(key, x.shape),\n      apply=lambda params, x: params + x)",
  "def funny_args_ffn(x: jnp.ndarray) -> networks.FeedForwardNetwork:\n  \"\"\"Apply takes additional parameters, returns `params + x + foo - bar`.\"\"\"\n  return networks.FeedForwardNetwork(\n      init=lambda key, x=x: jax.random.uniform(key, x.shape),\n      apply=lambda params, x, foo, bar: params + x + foo - bar)",
  "def struct_params_adding_ffn(sx: Any) -> networks.FeedForwardNetwork:\n  \"\"\"Like params_adding_ffn, but with pytree inputs, preserves structure.\"\"\"\n\n  def init_fn(key, sx=sx):\n    return jax.tree_map(lambda x: jax.random.uniform(key, x.shape), sx)\n\n  def apply_fn(params, x):\n    return jax.tree_map(lambda p, v: p + v, params, x)\n\n  return networks.FeedForwardNetwork(init=init_fn, apply=apply_fn)",
  "class EnsembleTest(absltest.TestCase):\n\n  def test_ensemble_init(self):\n    x = jnp.ones(10)  # Base input\n\n    wrapped_ffn = params_adding_ffn(x)\n\n    rr_ensemble = ensemble.make_ensemble(\n        wrapped_ffn, ensemble.apply_round_robin, num_networks=3)\n    key = jax.random.PRNGKey(0)\n    params = rr_ensemble.init(key)\n\n    self.assertTupleEqual(params.shape, (3,) + x.shape)\n\n    # The ensemble dimension is the lead dimension.\n    self.assertFalse((params[0, ...] == params[1, ...]).all())\n\n  def test_apply_all(self):\n    x = jnp.ones(10)  # Base input\n    bx = jnp.ones((7, 10))  # Batched input\n\n    wrapped_ffn = params_adding_ffn(x)\n\n    rr_ensemble = ensemble.make_ensemble(\n        wrapped_ffn, ensemble.apply_all, num_networks=3)\n    key = jax.random.PRNGKey(0)\n    params = rr_ensemble.init(key)\n    self.assertTupleEqual(params.shape, (3,) + x.shape)\n\n    y = rr_ensemble.apply(params, x)\n    self.assertTupleEqual(y.shape, (3,) + x.shape)\n    np.testing.assert_allclose(params, y - jnp.broadcast_to(x, (3,) + x.shape))\n\n    by = rr_ensemble.apply(params, bx)\n    # Note: the batch dimension is no longer the leading dimension.\n    self.assertTupleEqual(by.shape, (3,) + bx.shape)\n\n  def test_apply_round_robin(self):\n    x = jnp.ones(10)  # Base input\n    bx = jnp.ones((7, 10))  # Batched input\n\n    wrapped_ffn = params_adding_ffn(x)\n\n    rr_ensemble = ensemble.make_ensemble(\n        wrapped_ffn, ensemble.apply_round_robin, num_networks=3)\n    key = jax.random.PRNGKey(0)\n    params = rr_ensemble.init(key)\n    self.assertTupleEqual(params.shape, (3,) + x.shape)\n\n    y = rr_ensemble.apply(params, jnp.broadcast_to(x, (3,) + x.shape))\n    self.assertTupleEqual(y.shape, (3,) + x.shape)\n    np.testing.assert_allclose(params, y - x)\n\n    # Note: the ensemble dimension must lead, the batch dimension is no longer\n    # the leading dimension.\n    by = rr_ensemble.apply(\n        params, jnp.broadcast_to(jnp.expand_dims(bx, axis=0), (3,) + bx.shape))\n    self.assertTupleEqual(by.shape, (3,) + bx.shape)\n\n    # If num_networks=3, then `round_robin(params, input)[4]` should be equal\n    # to `apply(params[1], input[4])`, etc.\n    yy = rr_ensemble.apply(params, jnp.broadcast_to(x, (6,) + x.shape))\n    self.assertTupleEqual(yy.shape, (6,) + x.shape)\n    np.testing.assert_allclose(\n        jnp.concatenate([params, params], axis=0),\n        yy - jnp.expand_dims(x, axis=0))\n\n  def test_apply_mean(self):\n    x = jnp.ones(10)  # Base input\n    bx = jnp.ones((7, 10))  # Batched input\n\n    wrapped_ffn = params_adding_ffn(x)\n\n    rr_ensemble = ensemble.make_ensemble(\n        wrapped_ffn, ensemble.apply_mean, num_networks=3)\n    key = jax.random.PRNGKey(0)\n    params = rr_ensemble.init(key)\n    self.assertTupleEqual(params.shape, (3,) + x.shape)\n    self.assertFalse((params[0, ...] == params[1, ...]).all())\n\n    y = rr_ensemble.apply(params, x)\n    self.assertTupleEqual(y.shape, x.shape)\n    np.testing.assert_allclose(\n        jnp.mean(params, axis=0), y - x, atol=1E-5, rtol=1E-5)\n\n    by = rr_ensemble.apply(params, bx)\n    self.assertTupleEqual(by.shape, bx.shape)\n\n  def test_apply_all_multiargs(self):\n    x = jnp.ones(10)  # Base input\n\n    wrapped_ffn = funny_args_ffn(x)\n\n    rr_ensemble = ensemble.make_ensemble(\n        wrapped_ffn, ensemble.apply_all, num_networks=3)\n    key = jax.random.PRNGKey(0)\n    params = rr_ensemble.init(key)\n    self.assertTupleEqual(params.shape, (3,) + x.shape)\n\n    y = rr_ensemble.apply(params, x, 2 * x, x)\n    self.assertTupleEqual(y.shape, (3,) + x.shape)\n    np.testing.assert_allclose(\n        params,\n        y - jnp.broadcast_to(2 * x, (3,) + x.shape),\n        atol=1E-5,\n        rtol=1E-5)\n\n    y = rr_ensemble.apply(params, x, bar=x, foo=2 * x)\n    self.assertTupleEqual(y.shape, (3,) + x.shape)\n    np.testing.assert_allclose(\n        params,\n        y - jnp.broadcast_to(2 * x, (3,) + x.shape),\n        atol=1E-5,\n        rtol=1E-5)\n\n  def test_apply_all_structured(self):\n    x = jnp.ones(10)\n    sx = [(3 * x, 2 * x), 5 * x]  # Base input\n\n    wrapped_ffn = struct_params_adding_ffn(sx)\n\n    rr_ensemble = ensemble.make_ensemble(\n        wrapped_ffn, ensemble.apply_all, num_networks=3)\n    key = jax.random.PRNGKey(0)\n    params = rr_ensemble.init(key)\n\n    y = rr_ensemble.apply(params, sx)\n    ex = jnp.broadcast_to(x, (3,) + x.shape)\n    np.testing.assert_allclose(y[0][0], params[0][0] + 3 * ex)\n\n  def test_apply_round_robin_multiargs(self):\n    x = jnp.ones(10)  # Base input\n\n    wrapped_ffn = funny_args_ffn(x)\n\n    rr_ensemble = ensemble.make_ensemble(\n        wrapped_ffn, ensemble.apply_round_robin, num_networks=3)\n    key = jax.random.PRNGKey(0)\n    params = rr_ensemble.init(key)\n    self.assertTupleEqual(params.shape, (3,) + x.shape)\n\n    ex = jnp.broadcast_to(x, (3,) + x.shape)\n    y = rr_ensemble.apply(params, ex, 2 * ex, ex)\n    self.assertTupleEqual(y.shape, (3,) + x.shape)\n    np.testing.assert_allclose(\n        params,\n        y - jnp.broadcast_to(2 * x, (3,) + x.shape),\n        atol=1E-5,\n        rtol=1E-5)\n\n    y = rr_ensemble.apply(params, ex, bar=ex, foo=2 * ex)\n    self.assertTupleEqual(y.shape, (3,) + x.shape)\n    np.testing.assert_allclose(\n        params,\n        y - jnp.broadcast_to(2 * x, (3,) + x.shape),\n        atol=1E-5,\n        rtol=1E-5)\n\n  def test_apply_round_robin_structured(self):\n    x = jnp.ones(10)\n    sx = [(3 * x, 2 * x), 5 * x]  # Base input\n\n    wrapped_ffn = struct_params_adding_ffn(sx)\n\n    rr_ensemble = ensemble.make_ensemble(\n        wrapped_ffn, ensemble.apply_round_robin, num_networks=3)\n    key = jax.random.PRNGKey(0)\n    params = rr_ensemble.init(key)\n\n    ex = jnp.broadcast_to(x, (3,) + x.shape)\n    esx = [(3 * ex, 2 * ex), 5 * ex]\n    y = rr_ensemble.apply(params, esx)\n    np.testing.assert_allclose(y[0][0], params[0][0] + 3 * ex)\n\n  def test_apply_mean_multiargs(self):\n    x = jnp.ones(10)  # Base input\n\n    wrapped_ffn = funny_args_ffn(x)\n\n    rr_ensemble = ensemble.make_ensemble(\n        wrapped_ffn, ensemble.apply_mean, num_networks=3)\n    key = jax.random.PRNGKey(0)\n    params = rr_ensemble.init(key)\n    self.assertTupleEqual(params.shape, (3,) + x.shape)\n\n    y = rr_ensemble.apply(params, x, 2 * x, x)\n    self.assertTupleEqual(y.shape, x.shape)\n    np.testing.assert_allclose(\n        jnp.mean(params, axis=0), y - 2 * x, atol=1E-5, rtol=1E-5)\n\n    y = rr_ensemble.apply(params, x, bar=x, foo=2 * x)\n    self.assertTupleEqual(y.shape, x.shape)\n    np.testing.assert_allclose(\n        jnp.mean(params, axis=0), y - 2 * x, atol=1E-5, rtol=1E-5)\n\n  def test_apply_mean_structured(self):\n    x = jnp.ones(10)\n    sx = [(3 * x, 2 * x), 5 * x]  # Base input\n\n    wrapped_ffn = struct_params_adding_ffn(sx)\n\n    rr_ensemble = ensemble.make_ensemble(\n        wrapped_ffn, ensemble.apply_mean, num_networks=3)\n    key = jax.random.PRNGKey(0)\n    params = rr_ensemble.init(key)\n\n    y = rr_ensemble.apply(params, sx)\n    np.testing.assert_allclose(\n        y[0][0], jnp.mean(params[0][0], axis=0) + 3 * x, atol=1E-5, rtol=1E-5)\n\n  def test_round_robin_random(self):\n    x = jnp.ones(10)  # Base input\n    bx = jnp.ones((9, 10))  # Batched input\n    ffn = RandomFFN()\n    wrapped_ffn = networks.FeedForwardNetwork(\n        init=functools.partial(ffn.init, x=x), apply=ffn.apply)\n    rr_ensemble = ensemble.make_ensemble(\n        wrapped_ffn, ensemble.apply_round_robin, num_networks=3)\n\n    key = jax.random.PRNGKey(0)\n    params = rr_ensemble.init(key)\n    out = rr_ensemble.apply(params, bx)\n    # The output should be the same every 3 rows:\n    blocks = jnp.split(out, 3, axis=0)\n    np.testing.assert_array_equal(blocks[0], blocks[1])\n    np.testing.assert_array_equal(blocks[0], blocks[2])\n    self.assertTrue((out[0] != out[1]).any())\n\n    for i in range(9):\n      np.testing.assert_allclose(\n          out[i],\n          ffn.apply(jax.tree_map(lambda p, i=i: p[i % 3], params), bx[i]),\n          atol=1E-5,\n          rtol=1E-5)\n\n  def test_mean_random(self):\n    x = jnp.ones(10)\n    bx = jnp.ones((9, 10))\n    ffn = RandomFFN()\n    wrapped_ffn = networks.FeedForwardNetwork(\n        init=functools.partial(ffn.init, x=x), apply=ffn.apply)\n    mean_ensemble = ensemble.make_ensemble(\n        wrapped_ffn, ensemble.apply_mean, num_networks=3)\n    key = jax.random.PRNGKey(0)\n    params = mean_ensemble.init(key)\n    single_output = mean_ensemble.apply(params, x)\n    self.assertEqual(single_output.shape, (15,))\n    batch_output = mean_ensemble.apply(params, bx)\n    # Make sure all rows are equal:\n    np.testing.assert_allclose(\n        jnp.broadcast_to(batch_output[0], batch_output.shape),\n        batch_output,\n        atol=1E-5,\n        rtol=1E-5)\n\n    # Check results explicitly:\n    all_members = jnp.concatenate([\n        jnp.expand_dims(\n            ffn.apply(jax.tree_map(lambda p, i=i: p[i], params), bx), axis=0)\n        for i in range(3)\n    ])\n    batch_means = jnp.mean(all_members, axis=0)\n    np.testing.assert_allclose(batch_output, batch_means, atol=1E-5, rtol=1E-5)",
  "def __call__(self, x):\n    return nn.Dense(15)(x)",
  "def init_fn(key, sx=sx):\n    return jax.tree_map(lambda x: jax.random.uniform(key, x.shape), sx)",
  "def apply_fn(params, x):\n    return jax.tree_map(lambda p, v: p + v, params, x)",
  "def test_ensemble_init(self):\n    x = jnp.ones(10)  # Base input\n\n    wrapped_ffn = params_adding_ffn(x)\n\n    rr_ensemble = ensemble.make_ensemble(\n        wrapped_ffn, ensemble.apply_round_robin, num_networks=3)\n    key = jax.random.PRNGKey(0)\n    params = rr_ensemble.init(key)\n\n    self.assertTupleEqual(params.shape, (3,) + x.shape)\n\n    # The ensemble dimension is the lead dimension.\n    self.assertFalse((params[0, ...] == params[1, ...]).all())",
  "def test_apply_all(self):\n    x = jnp.ones(10)  # Base input\n    bx = jnp.ones((7, 10))  # Batched input\n\n    wrapped_ffn = params_adding_ffn(x)\n\n    rr_ensemble = ensemble.make_ensemble(\n        wrapped_ffn, ensemble.apply_all, num_networks=3)\n    key = jax.random.PRNGKey(0)\n    params = rr_ensemble.init(key)\n    self.assertTupleEqual(params.shape, (3,) + x.shape)\n\n    y = rr_ensemble.apply(params, x)\n    self.assertTupleEqual(y.shape, (3,) + x.shape)\n    np.testing.assert_allclose(params, y - jnp.broadcast_to(x, (3,) + x.shape))\n\n    by = rr_ensemble.apply(params, bx)\n    # Note: the batch dimension is no longer the leading dimension.\n    self.assertTupleEqual(by.shape, (3,) + bx.shape)",
  "def test_apply_round_robin(self):\n    x = jnp.ones(10)  # Base input\n    bx = jnp.ones((7, 10))  # Batched input\n\n    wrapped_ffn = params_adding_ffn(x)\n\n    rr_ensemble = ensemble.make_ensemble(\n        wrapped_ffn, ensemble.apply_round_robin, num_networks=3)\n    key = jax.random.PRNGKey(0)\n    params = rr_ensemble.init(key)\n    self.assertTupleEqual(params.shape, (3,) + x.shape)\n\n    y = rr_ensemble.apply(params, jnp.broadcast_to(x, (3,) + x.shape))\n    self.assertTupleEqual(y.shape, (3,) + x.shape)\n    np.testing.assert_allclose(params, y - x)\n\n    # Note: the ensemble dimension must lead, the batch dimension is no longer\n    # the leading dimension.\n    by = rr_ensemble.apply(\n        params, jnp.broadcast_to(jnp.expand_dims(bx, axis=0), (3,) + bx.shape))\n    self.assertTupleEqual(by.shape, (3,) + bx.shape)\n\n    # If num_networks=3, then `round_robin(params, input)[4]` should be equal\n    # to `apply(params[1], input[4])`, etc.\n    yy = rr_ensemble.apply(params, jnp.broadcast_to(x, (6,) + x.shape))\n    self.assertTupleEqual(yy.shape, (6,) + x.shape)\n    np.testing.assert_allclose(\n        jnp.concatenate([params, params], axis=0),\n        yy - jnp.expand_dims(x, axis=0))",
  "def test_apply_mean(self):\n    x = jnp.ones(10)  # Base input\n    bx = jnp.ones((7, 10))  # Batched input\n\n    wrapped_ffn = params_adding_ffn(x)\n\n    rr_ensemble = ensemble.make_ensemble(\n        wrapped_ffn, ensemble.apply_mean, num_networks=3)\n    key = jax.random.PRNGKey(0)\n    params = rr_ensemble.init(key)\n    self.assertTupleEqual(params.shape, (3,) + x.shape)\n    self.assertFalse((params[0, ...] == params[1, ...]).all())\n\n    y = rr_ensemble.apply(params, x)\n    self.assertTupleEqual(y.shape, x.shape)\n    np.testing.assert_allclose(\n        jnp.mean(params, axis=0), y - x, atol=1E-5, rtol=1E-5)\n\n    by = rr_ensemble.apply(params, bx)\n    self.assertTupleEqual(by.shape, bx.shape)",
  "def test_apply_all_multiargs(self):\n    x = jnp.ones(10)  # Base input\n\n    wrapped_ffn = funny_args_ffn(x)\n\n    rr_ensemble = ensemble.make_ensemble(\n        wrapped_ffn, ensemble.apply_all, num_networks=3)\n    key = jax.random.PRNGKey(0)\n    params = rr_ensemble.init(key)\n    self.assertTupleEqual(params.shape, (3,) + x.shape)\n\n    y = rr_ensemble.apply(params, x, 2 * x, x)\n    self.assertTupleEqual(y.shape, (3,) + x.shape)\n    np.testing.assert_allclose(\n        params,\n        y - jnp.broadcast_to(2 * x, (3,) + x.shape),\n        atol=1E-5,\n        rtol=1E-5)\n\n    y = rr_ensemble.apply(params, x, bar=x, foo=2 * x)\n    self.assertTupleEqual(y.shape, (3,) + x.shape)\n    np.testing.assert_allclose(\n        params,\n        y - jnp.broadcast_to(2 * x, (3,) + x.shape),\n        atol=1E-5,\n        rtol=1E-5)",
  "def test_apply_all_structured(self):\n    x = jnp.ones(10)\n    sx = [(3 * x, 2 * x), 5 * x]  # Base input\n\n    wrapped_ffn = struct_params_adding_ffn(sx)\n\n    rr_ensemble = ensemble.make_ensemble(\n        wrapped_ffn, ensemble.apply_all, num_networks=3)\n    key = jax.random.PRNGKey(0)\n    params = rr_ensemble.init(key)\n\n    y = rr_ensemble.apply(params, sx)\n    ex = jnp.broadcast_to(x, (3,) + x.shape)\n    np.testing.assert_allclose(y[0][0], params[0][0] + 3 * ex)",
  "def test_apply_round_robin_multiargs(self):\n    x = jnp.ones(10)  # Base input\n\n    wrapped_ffn = funny_args_ffn(x)\n\n    rr_ensemble = ensemble.make_ensemble(\n        wrapped_ffn, ensemble.apply_round_robin, num_networks=3)\n    key = jax.random.PRNGKey(0)\n    params = rr_ensemble.init(key)\n    self.assertTupleEqual(params.shape, (3,) + x.shape)\n\n    ex = jnp.broadcast_to(x, (3,) + x.shape)\n    y = rr_ensemble.apply(params, ex, 2 * ex, ex)\n    self.assertTupleEqual(y.shape, (3,) + x.shape)\n    np.testing.assert_allclose(\n        params,\n        y - jnp.broadcast_to(2 * x, (3,) + x.shape),\n        atol=1E-5,\n        rtol=1E-5)\n\n    y = rr_ensemble.apply(params, ex, bar=ex, foo=2 * ex)\n    self.assertTupleEqual(y.shape, (3,) + x.shape)\n    np.testing.assert_allclose(\n        params,\n        y - jnp.broadcast_to(2 * x, (3,) + x.shape),\n        atol=1E-5,\n        rtol=1E-5)",
  "def test_apply_round_robin_structured(self):\n    x = jnp.ones(10)\n    sx = [(3 * x, 2 * x), 5 * x]  # Base input\n\n    wrapped_ffn = struct_params_adding_ffn(sx)\n\n    rr_ensemble = ensemble.make_ensemble(\n        wrapped_ffn, ensemble.apply_round_robin, num_networks=3)\n    key = jax.random.PRNGKey(0)\n    params = rr_ensemble.init(key)\n\n    ex = jnp.broadcast_to(x, (3,) + x.shape)\n    esx = [(3 * ex, 2 * ex), 5 * ex]\n    y = rr_ensemble.apply(params, esx)\n    np.testing.assert_allclose(y[0][0], params[0][0] + 3 * ex)",
  "def test_apply_mean_multiargs(self):\n    x = jnp.ones(10)  # Base input\n\n    wrapped_ffn = funny_args_ffn(x)\n\n    rr_ensemble = ensemble.make_ensemble(\n        wrapped_ffn, ensemble.apply_mean, num_networks=3)\n    key = jax.random.PRNGKey(0)\n    params = rr_ensemble.init(key)\n    self.assertTupleEqual(params.shape, (3,) + x.shape)\n\n    y = rr_ensemble.apply(params, x, 2 * x, x)\n    self.assertTupleEqual(y.shape, x.shape)\n    np.testing.assert_allclose(\n        jnp.mean(params, axis=0), y - 2 * x, atol=1E-5, rtol=1E-5)\n\n    y = rr_ensemble.apply(params, x, bar=x, foo=2 * x)\n    self.assertTupleEqual(y.shape, x.shape)\n    np.testing.assert_allclose(\n        jnp.mean(params, axis=0), y - 2 * x, atol=1E-5, rtol=1E-5)",
  "def test_apply_mean_structured(self):\n    x = jnp.ones(10)\n    sx = [(3 * x, 2 * x), 5 * x]  # Base input\n\n    wrapped_ffn = struct_params_adding_ffn(sx)\n\n    rr_ensemble = ensemble.make_ensemble(\n        wrapped_ffn, ensemble.apply_mean, num_networks=3)\n    key = jax.random.PRNGKey(0)\n    params = rr_ensemble.init(key)\n\n    y = rr_ensemble.apply(params, sx)\n    np.testing.assert_allclose(\n        y[0][0], jnp.mean(params[0][0], axis=0) + 3 * x, atol=1E-5, rtol=1E-5)",
  "def test_round_robin_random(self):\n    x = jnp.ones(10)  # Base input\n    bx = jnp.ones((9, 10))  # Batched input\n    ffn = RandomFFN()\n    wrapped_ffn = networks.FeedForwardNetwork(\n        init=functools.partial(ffn.init, x=x), apply=ffn.apply)\n    rr_ensemble = ensemble.make_ensemble(\n        wrapped_ffn, ensemble.apply_round_robin, num_networks=3)\n\n    key = jax.random.PRNGKey(0)\n    params = rr_ensemble.init(key)\n    out = rr_ensemble.apply(params, bx)\n    # The output should be the same every 3 rows:\n    blocks = jnp.split(out, 3, axis=0)\n    np.testing.assert_array_equal(blocks[0], blocks[1])\n    np.testing.assert_array_equal(blocks[0], blocks[2])\n    self.assertTrue((out[0] != out[1]).any())\n\n    for i in range(9):\n      np.testing.assert_allclose(\n          out[i],\n          ffn.apply(jax.tree_map(lambda p, i=i: p[i % 3], params), bx[i]),\n          atol=1E-5,\n          rtol=1E-5)",
  "def test_mean_random(self):\n    x = jnp.ones(10)\n    bx = jnp.ones((9, 10))\n    ffn = RandomFFN()\n    wrapped_ffn = networks.FeedForwardNetwork(\n        init=functools.partial(ffn.init, x=x), apply=ffn.apply)\n    mean_ensemble = ensemble.make_ensemble(\n        wrapped_ffn, ensemble.apply_mean, num_networks=3)\n    key = jax.random.PRNGKey(0)\n    params = mean_ensemble.init(key)\n    single_output = mean_ensemble.apply(params, x)\n    self.assertEqual(single_output.shape, (15,))\n    batch_output = mean_ensemble.apply(params, bx)\n    # Make sure all rows are equal:\n    np.testing.assert_allclose(\n        jnp.broadcast_to(batch_output[0], batch_output.shape),\n        batch_output,\n        atol=1E-5,\n        rtol=1E-5)\n\n    # Check results explicitly:\n    all_members = jnp.concatenate([\n        jnp.expand_dims(\n            ffn.apply(jax.tree_map(lambda p, i=i: p[i], params), bx), axis=0)\n        for i in range(3)\n    ])\n    batch_means = jnp.mean(all_members, axis=0)\n    np.testing.assert_allclose(batch_output, batch_means, atol=1E-5, rtol=1E-5)",
  "class MBOPBuilder(builders.OfflineBuilder[mbop_networks.MBOPNetworks,\n                                          acting.ActorCore, types.Transition]):\n  \"\"\"MBOP Builder.\n\n  This builder uses ensemble regressor learners for the world model, policy\n  prior and the n-step return models with fixed learning rates. The ensembles\n  and the learning rate are configured in the config.\n  \"\"\"\n\n  def __init__(\n      self,\n      config: mbop_config.MBOPConfig,\n      losses: mbop_losses.MBOPLosses,\n      mean_std: Optional[running_statistics.NestedMeanStd] = None,\n  ):\n    \"\"\"Initializes an MBOP builder.\n\n    Args:\n      config: a config with MBOP hyperparameters.\n      losses: MBOP losses.\n      mean_std: NestedMeanStd used to normalize the samples.\n    \"\"\"\n    self._config = config\n    self._losses = losses\n    self._mean_std = mean_std\n\n  def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: mbop_networks.MBOPNetworks,\n      dataset: Iterator[types.Transition],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    \"\"\"See base class.\"\"\"\n\n    def make_ensemble_regressor_learner(\n        name: str,\n        logger_fn: loggers.LoggerFactory,\n        counter: counting.Counter,\n        rng_key: networks_lib.PRNGKey,\n        iterator: Iterator[types.Transition],\n        network: networks_lib.FeedForwardNetwork,\n        loss: mbop_losses.TransitionLoss,\n    ) -> core.Learner:\n      \"\"\"Creates an ensemble regressor learner.\"\"\"\n      return learning.make_ensemble_regressor_learner(\n          name,\n          self._config.num_networks,\n          logger_fn,\n          counter,\n          rng_key,\n          iterator,\n          network,\n          loss,\n          optax.adam(self._config.learning_rate),\n          self._config.num_sgd_steps_per_step,\n      )\n\n    make_world_model_learner = functools.partial(\n        make_ensemble_regressor_learner, 'world_model')\n    make_policy_prior_learner = functools.partial(\n        make_ensemble_regressor_learner, 'policy_prior')\n    make_n_step_return_learner = functools.partial(\n        make_ensemble_regressor_learner, 'n_step_return')\n    counter = counter or counting.Counter(time_delta=0.)\n    return learning.MBOPLearner(\n        networks,\n        self._losses,\n        dataset,\n        random_key,\n        logger_fn,\n        make_world_model_learner,\n        make_policy_prior_learner,\n        make_n_step_return_learner,\n        counter,\n    )\n\n  def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: acting.ActorCore,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n  ) -> core.Actor:\n    \"\"\"See base class.\"\"\"\n    del environment_spec\n    return acting.make_actor(policy, random_key, variable_source)\n\n  def make_policy(\n      self,\n      networks: mbop_networks.MBOPNetworks,\n      environment_spec: specs.EnvironmentSpec,\n      evaluation: bool,\n  ) -> acting.ActorCore:\n    \"\"\"See base class.\"\"\"\n    return acting.make_ensemble_actor_core(\n        networks,\n        self._config.mppi_config,\n        environment_spec,\n        self._mean_std,\n        use_round_robin=not evaluation)",
  "def __init__(\n      self,\n      config: mbop_config.MBOPConfig,\n      losses: mbop_losses.MBOPLosses,\n      mean_std: Optional[running_statistics.NestedMeanStd] = None,\n  ):\n    \"\"\"Initializes an MBOP builder.\n\n    Args:\n      config: a config with MBOP hyperparameters.\n      losses: MBOP losses.\n      mean_std: NestedMeanStd used to normalize the samples.\n    \"\"\"\n    self._config = config\n    self._losses = losses\n    self._mean_std = mean_std",
  "def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: mbop_networks.MBOPNetworks,\n      dataset: Iterator[types.Transition],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    \"\"\"See base class.\"\"\"\n\n    def make_ensemble_regressor_learner(\n        name: str,\n        logger_fn: loggers.LoggerFactory,\n        counter: counting.Counter,\n        rng_key: networks_lib.PRNGKey,\n        iterator: Iterator[types.Transition],\n        network: networks_lib.FeedForwardNetwork,\n        loss: mbop_losses.TransitionLoss,\n    ) -> core.Learner:\n      \"\"\"Creates an ensemble regressor learner.\"\"\"\n      return learning.make_ensemble_regressor_learner(\n          name,\n          self._config.num_networks,\n          logger_fn,\n          counter,\n          rng_key,\n          iterator,\n          network,\n          loss,\n          optax.adam(self._config.learning_rate),\n          self._config.num_sgd_steps_per_step,\n      )\n\n    make_world_model_learner = functools.partial(\n        make_ensemble_regressor_learner, 'world_model')\n    make_policy_prior_learner = functools.partial(\n        make_ensemble_regressor_learner, 'policy_prior')\n    make_n_step_return_learner = functools.partial(\n        make_ensemble_regressor_learner, 'n_step_return')\n    counter = counter or counting.Counter(time_delta=0.)\n    return learning.MBOPLearner(\n        networks,\n        self._losses,\n        dataset,\n        random_key,\n        logger_fn,\n        make_world_model_learner,\n        make_policy_prior_learner,\n        make_n_step_return_learner,\n        counter,\n    )",
  "def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: acting.ActorCore,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n  ) -> core.Actor:\n    \"\"\"See base class.\"\"\"\n    del environment_spec\n    return acting.make_actor(policy, random_key, variable_source)",
  "def make_policy(\n      self,\n      networks: mbop_networks.MBOPNetworks,\n      environment_spec: specs.EnvironmentSpec,\n      evaluation: bool,\n  ) -> acting.ActorCore:\n    \"\"\"See base class.\"\"\"\n    return acting.make_ensemble_actor_core(\n        networks,\n        self._config.mppi_config,\n        environment_spec,\n        self._mean_std,\n        use_round_robin=not evaluation)",
  "def make_ensemble_regressor_learner(\n        name: str,\n        logger_fn: loggers.LoggerFactory,\n        counter: counting.Counter,\n        rng_key: networks_lib.PRNGKey,\n        iterator: Iterator[types.Transition],\n        network: networks_lib.FeedForwardNetwork,\n        loss: mbop_losses.TransitionLoss,\n    ) -> core.Learner:\n      \"\"\"Creates an ensemble regressor learner.\"\"\"\n      return learning.make_ensemble_regressor_learner(\n          name,\n          self._config.num_networks,\n          logger_fn,\n          counter,\n          rng_key,\n          iterator,\n          network,\n          loss,\n          optax.adam(self._config.learning_rate),\n          self._config.num_sgd_steps_per_step,\n      )",
  "def sample_episode() -> rlds.Episode:\n  \"\"\"Returns a sample episode.\"\"\"\n  steps = {\n      rlds.OBSERVATION: [\n          [1, 1],\n          [2, 2],\n          [3, 3],\n          [4, 4],\n          [5, 5],\n      ],\n      rlds.ACTION: [[1], [2], [3], [4], [5]],\n      rlds.REWARD: [1.0, 2.0, 3.0, 4.0, 5.0],\n      rlds.DISCOUNT: [1, 1, 1, 1, 1],\n      rlds.IS_FIRST: [True, False, False, False, False],\n      rlds.IS_LAST: [False, False, False, False, True],\n      rlds.IS_TERMINAL: [False, False, False, False, True],\n  }\n  return {rlds.STEPS: tf.data.Dataset.from_tensor_slices(steps)}",
  "class DatasetTest(transformations_testlib.TransformationsTest):\n\n  def test_episode_to_timestep_batch(self):\n    batched = dataset_lib.episode_to_timestep_batch(\n        sample_episode(), return_horizon=2)\n\n    # Scalars should be expanded and the n-step return should be present. Each\n    # element of a step should be a triplet containing the previous, current and\n    # next values of the corresponding fields. Since the return horizon is 2 and\n    # the number of steps in the episode is 5, there can be only 2 triplets for\n    # time steps 1 and 2.\n    expected_steps = {\n        rlds.OBSERVATION: [\n            [[1, 1], [2, 2], [3, 3]],\n            [[2, 2], [3, 3], [4, 4]],\n        ],\n        rlds.ACTION: [\n            [[1], [2], [3]],\n            [[2], [3], [4]],\n        ],\n        rlds.REWARD: [\n            [[1.0], [2.0], [3.0]],\n            [[2.0], [3.0], [4.0]],\n        ],\n        rlds.DISCOUNT: [\n            [[1], [1], [1]],\n            [[1], [1], [1]],\n        ],\n        rlds.IS_FIRST: [\n            [[True], [False], [False]],\n            [[False], [False], [False]],\n        ],\n        rlds.IS_LAST: [\n            [[False], [False], [False]],\n            [[False], [False], [False]],\n        ],\n        rlds.IS_TERMINAL: [\n            [[False], [False], [False]],\n            [[False], [False], [False]],\n        ],\n        dataset_lib.N_STEP_RETURN: [\n            [[3.0], [5.0], [7.0]],\n            [[5.0], [7.0], [9.0]],\n        ],\n    }\n\n    self.expect_equal_datasets(\n        batched, tf.data.Dataset.from_tensor_slices(expected_steps))\n\n  def test_episode_to_timestep_batch_episode_return(self):\n    batched = dataset_lib.episode_to_timestep_batch(\n        sample_episode(), return_horizon=3, calculate_episode_return=True)\n\n    expected_steps = {\n        rlds.OBSERVATION: [[[1, 1], [2, 2], [3, 3]]],\n        rlds.ACTION: [[[1], [2], [3]]],\n        rlds.REWARD: [[[1.0], [2.0], [3.0]]],\n        rlds.DISCOUNT: [[[1], [1], [1]]],\n        rlds.IS_FIRST: [[[True], [False], [False]]],\n        rlds.IS_LAST: [[[False], [False], [False]]],\n        rlds.IS_TERMINAL: [[[False], [False], [False]]],\n        dataset_lib.N_STEP_RETURN: [[[6.0], [9.0], [12.0]]],\n        # This should match to the sum of the rewards in the input.\n        dataset_lib.EPISODE_RETURN: [[[15.0], [15.0], [15.0]]],\n    }\n\n    self.expect_equal_datasets(\n        batched, tf.data.Dataset.from_tensor_slices(expected_steps))\n\n  def test_episode_to_timestep_batch_no_return_horizon(self):\n    batched = dataset_lib.episode_to_timestep_batch(\n        sample_episode(), return_horizon=1)\n\n    expected_steps = {\n        rlds.OBSERVATION: [\n            [[1, 1], [2, 2], [3, 3]],\n            [[2, 2], [3, 3], [4, 4]],\n            [[3, 3], [4, 4], [5, 5]],\n        ],\n        rlds.ACTION: [\n            [[1], [2], [3]],\n            [[2], [3], [4]],\n            [[3], [4], [5]],\n        ],\n        rlds.REWARD: [\n            [[1.0], [2.0], [3.0]],\n            [[2.0], [3.0], [4.0]],\n            [[3.0], [4.0], [5.0]],\n        ],\n        rlds.DISCOUNT: [\n            [[1], [1], [1]],\n            [[1], [1], [1]],\n            [[1], [1], [1]],\n        ],\n        rlds.IS_FIRST: [\n            [[True], [False], [False]],\n            [[False], [False], [False]],\n            [[False], [False], [False]],\n        ],\n        rlds.IS_LAST: [\n            [[False], [False], [False]],\n            [[False], [False], [False]],\n            [[False], [False], [True]],\n        ],\n        rlds.IS_TERMINAL: [\n            [[False], [False], [False]],\n            [[False], [False], [False]],\n            [[False], [False], [True]],\n        ],\n        # n-step return should be equal to the rewards.\n        dataset_lib.N_STEP_RETURN: [\n            [[1.0], [2.0], [3.0]],\n            [[2.0], [3.0], [4.0]],\n            [[3.0], [4.0], [5.0]],\n        ],\n    }\n\n    self.expect_equal_datasets(\n        batched, tf.data.Dataset.from_tensor_slices(expected_steps))\n\n  def test_episode_to_timestep_batch_drop_return_horizon(self):\n    steps = {\n        rlds.OBSERVATION: [[1], [2], [3], [4], [5], [6]],\n        rlds.REWARD: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0],\n    }\n    episode = {rlds.STEPS: tf.data.Dataset.from_tensor_slices(steps)}\n\n    batched = dataset_lib.episode_to_timestep_batch(\n        episode,\n        return_horizon=2,\n        calculate_episode_return=True,\n        drop_return_horizon=True)\n\n    # The two steps of the episode should be dropped. There will be 4 steps left\n    # and since the return horizon is 2, only a single 3-batched step should be\n    # emitted. The episode return should be the sum of the rewards of the first\n    # 4 steps.\n    expected_steps = {\n        rlds.OBSERVATION: [[[1], [2], [3]]],\n        rlds.REWARD: [[[1.0], [2.0], [3.0]]],\n        dataset_lib.N_STEP_RETURN: [[[3.0], [5.0], [7.0]]],\n        dataset_lib.EPISODE_RETURN: [[[10.0], [10.0], [10.0]]],\n    }\n\n    self.expect_equal_datasets(\n        batched, tf.data.Dataset.from_tensor_slices(expected_steps))",
  "def test_episode_to_timestep_batch(self):\n    batched = dataset_lib.episode_to_timestep_batch(\n        sample_episode(), return_horizon=2)\n\n    # Scalars should be expanded and the n-step return should be present. Each\n    # element of a step should be a triplet containing the previous, current and\n    # next values of the corresponding fields. Since the return horizon is 2 and\n    # the number of steps in the episode is 5, there can be only 2 triplets for\n    # time steps 1 and 2.\n    expected_steps = {\n        rlds.OBSERVATION: [\n            [[1, 1], [2, 2], [3, 3]],\n            [[2, 2], [3, 3], [4, 4]],\n        ],\n        rlds.ACTION: [\n            [[1], [2], [3]],\n            [[2], [3], [4]],\n        ],\n        rlds.REWARD: [\n            [[1.0], [2.0], [3.0]],\n            [[2.0], [3.0], [4.0]],\n        ],\n        rlds.DISCOUNT: [\n            [[1], [1], [1]],\n            [[1], [1], [1]],\n        ],\n        rlds.IS_FIRST: [\n            [[True], [False], [False]],\n            [[False], [False], [False]],\n        ],\n        rlds.IS_LAST: [\n            [[False], [False], [False]],\n            [[False], [False], [False]],\n        ],\n        rlds.IS_TERMINAL: [\n            [[False], [False], [False]],\n            [[False], [False], [False]],\n        ],\n        dataset_lib.N_STEP_RETURN: [\n            [[3.0], [5.0], [7.0]],\n            [[5.0], [7.0], [9.0]],\n        ],\n    }\n\n    self.expect_equal_datasets(\n        batched, tf.data.Dataset.from_tensor_slices(expected_steps))",
  "def test_episode_to_timestep_batch_episode_return(self):\n    batched = dataset_lib.episode_to_timestep_batch(\n        sample_episode(), return_horizon=3, calculate_episode_return=True)\n\n    expected_steps = {\n        rlds.OBSERVATION: [[[1, 1], [2, 2], [3, 3]]],\n        rlds.ACTION: [[[1], [2], [3]]],\n        rlds.REWARD: [[[1.0], [2.0], [3.0]]],\n        rlds.DISCOUNT: [[[1], [1], [1]]],\n        rlds.IS_FIRST: [[[True], [False], [False]]],\n        rlds.IS_LAST: [[[False], [False], [False]]],\n        rlds.IS_TERMINAL: [[[False], [False], [False]]],\n        dataset_lib.N_STEP_RETURN: [[[6.0], [9.0], [12.0]]],\n        # This should match to the sum of the rewards in the input.\n        dataset_lib.EPISODE_RETURN: [[[15.0], [15.0], [15.0]]],\n    }\n\n    self.expect_equal_datasets(\n        batched, tf.data.Dataset.from_tensor_slices(expected_steps))",
  "def test_episode_to_timestep_batch_no_return_horizon(self):\n    batched = dataset_lib.episode_to_timestep_batch(\n        sample_episode(), return_horizon=1)\n\n    expected_steps = {\n        rlds.OBSERVATION: [\n            [[1, 1], [2, 2], [3, 3]],\n            [[2, 2], [3, 3], [4, 4]],\n            [[3, 3], [4, 4], [5, 5]],\n        ],\n        rlds.ACTION: [\n            [[1], [2], [3]],\n            [[2], [3], [4]],\n            [[3], [4], [5]],\n        ],\n        rlds.REWARD: [\n            [[1.0], [2.0], [3.0]],\n            [[2.0], [3.0], [4.0]],\n            [[3.0], [4.0], [5.0]],\n        ],\n        rlds.DISCOUNT: [\n            [[1], [1], [1]],\n            [[1], [1], [1]],\n            [[1], [1], [1]],\n        ],\n        rlds.IS_FIRST: [\n            [[True], [False], [False]],\n            [[False], [False], [False]],\n            [[False], [False], [False]],\n        ],\n        rlds.IS_LAST: [\n            [[False], [False], [False]],\n            [[False], [False], [False]],\n            [[False], [False], [True]],\n        ],\n        rlds.IS_TERMINAL: [\n            [[False], [False], [False]],\n            [[False], [False], [False]],\n            [[False], [False], [True]],\n        ],\n        # n-step return should be equal to the rewards.\n        dataset_lib.N_STEP_RETURN: [\n            [[1.0], [2.0], [3.0]],\n            [[2.0], [3.0], [4.0]],\n            [[3.0], [4.0], [5.0]],\n        ],\n    }\n\n    self.expect_equal_datasets(\n        batched, tf.data.Dataset.from_tensor_slices(expected_steps))",
  "def test_episode_to_timestep_batch_drop_return_horizon(self):\n    steps = {\n        rlds.OBSERVATION: [[1], [2], [3], [4], [5], [6]],\n        rlds.REWARD: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0],\n    }\n    episode = {rlds.STEPS: tf.data.Dataset.from_tensor_slices(steps)}\n\n    batched = dataset_lib.episode_to_timestep_batch(\n        episode,\n        return_horizon=2,\n        calculate_episode_return=True,\n        drop_return_horizon=True)\n\n    # The two steps of the episode should be dropped. There will be 4 steps left\n    # and since the return horizon is 2, only a single 3-batched step should be\n    # emitted. The episode return should be the sum of the rewards of the first\n    # 4 steps.\n    expected_steps = {\n        rlds.OBSERVATION: [[[1], [2], [3]]],\n        rlds.REWARD: [[[1.0], [2.0], [3.0]]],\n        dataset_lib.N_STEP_RETURN: [[[3.0], [5.0], [7.0]]],\n        dataset_lib.EPISODE_RETURN: [[[10.0], [10.0], [10.0]]],\n    }\n\n    self.expect_equal_datasets(\n        batched, tf.data.Dataset.from_tensor_slices(expected_steps))",
  "class MBOPTest(absltest.TestCase):\n\n  def test_learner(self):\n    with chex.fake_pmap_and_jit():\n      num_sgd_steps_per_step = 1\n      num_steps = 5\n      num_networks = 7\n\n      # Create a fake environment to test with.\n      environment = fakes.ContinuousEnvironment(\n          episode_length=10, bounded=True, observation_dim=3, action_dim=2)\n\n      spec = specs.make_environment_spec(environment)\n      dataset = fakes.transition_dataset(environment)\n\n      # Add dummy n-step return to the transitions.\n      def _add_dummy_n_step_return(sample):\n        return types.Transition(*sample.data)._replace(\n            extras={'n_step_return': 1.0})\n\n      dataset = dataset.map(_add_dummy_n_step_return)\n      # Convert into time-batched format with previous, current and next\n      # transitions.\n      dataset = rlds.transformations.batch(dataset, 3)\n      dataset = dataset.batch(8).as_numpy_iterator()\n\n      # Use the default networks and losses.\n      networks = mbop_networks.make_networks(spec)\n      losses = mbop_losses.MBOPLosses()\n\n      def logger_fn(label: str, steps_key: str):\n        return loggers.make_default_logger(label, steps_key=steps_key)\n\n      def make_learner_fn(name, logger_fn, counter, rng_key, dataset, network,\n                          loss):\n        return learning.make_ensemble_regressor_learner(name, num_networks,\n                                                        logger_fn, counter,\n                                                        rng_key, dataset,\n                                                        network, loss,\n                                                        optax.adam(0.01),\n                                                        num_sgd_steps_per_step)\n\n      learner = learning.MBOPLearner(\n          networks, losses, dataset, jax.random.PRNGKey(0), logger_fn,\n          functools.partial(make_learner_fn, 'world_model'),\n          functools.partial(make_learner_fn, 'policy_prior'),\n          functools.partial(make_learner_fn, 'n_step_return'))\n\n      # Train the agent\n      for _ in range(num_steps):\n        learner.step()\n\n      # Save and restore.\n      learner_state = learner.save()\n      learner.restore(learner_state)",
  "def test_learner(self):\n    with chex.fake_pmap_and_jit():\n      num_sgd_steps_per_step = 1\n      num_steps = 5\n      num_networks = 7\n\n      # Create a fake environment to test with.\n      environment = fakes.ContinuousEnvironment(\n          episode_length=10, bounded=True, observation_dim=3, action_dim=2)\n\n      spec = specs.make_environment_spec(environment)\n      dataset = fakes.transition_dataset(environment)\n\n      # Add dummy n-step return to the transitions.\n      def _add_dummy_n_step_return(sample):\n        return types.Transition(*sample.data)._replace(\n            extras={'n_step_return': 1.0})\n\n      dataset = dataset.map(_add_dummy_n_step_return)\n      # Convert into time-batched format with previous, current and next\n      # transitions.\n      dataset = rlds.transformations.batch(dataset, 3)\n      dataset = dataset.batch(8).as_numpy_iterator()\n\n      # Use the default networks and losses.\n      networks = mbop_networks.make_networks(spec)\n      losses = mbop_losses.MBOPLosses()\n\n      def logger_fn(label: str, steps_key: str):\n        return loggers.make_default_logger(label, steps_key=steps_key)\n\n      def make_learner_fn(name, logger_fn, counter, rng_key, dataset, network,\n                          loss):\n        return learning.make_ensemble_regressor_learner(name, num_networks,\n                                                        logger_fn, counter,\n                                                        rng_key, dataset,\n                                                        network, loss,\n                                                        optax.adam(0.01),\n                                                        num_sgd_steps_per_step)\n\n      learner = learning.MBOPLearner(\n          networks, losses, dataset, jax.random.PRNGKey(0), logger_fn,\n          functools.partial(make_learner_fn, 'world_model'),\n          functools.partial(make_learner_fn, 'policy_prior'),\n          functools.partial(make_learner_fn, 'n_step_return'))\n\n      # Train the agent\n      for _ in range(num_steps):\n        learner.step()\n\n      # Save and restore.\n      learner_state = learner.save()\n      learner.restore(learner_state)",
  "def _add_dummy_n_step_return(sample):\n        return types.Transition(*sample.data)._replace(\n            extras={'n_step_return': 1.0})",
  "def logger_fn(label: str, steps_key: str):\n        return loggers.make_default_logger(label, steps_key=steps_key)",
  "def make_learner_fn(name, logger_fn, counter, rng_key, dataset, network,\n                          loss):\n        return learning.make_ensemble_regressor_learner(name, num_networks,\n                                                        logger_fn, counter,\n                                                        rng_key, dataset,\n                                                        network, loss,\n                                                        optax.adam(0.01),\n                                                        num_sgd_steps_per_step)",
  "def get_fake_world_model() -> networks_lib.FeedForwardNetwork:\n\n  def apply(params: Any, observation_t: jnp.ndarray, action_t: jnp.ndarray):\n    del params\n    return observation_t, jnp.ones((\n        action_t.shape[0],\n        1,\n    ))\n\n  return networks_lib.FeedForwardNetwork(init=lambda: None, apply=apply)",
  "def get_fake_policy_prior() -> networks_lib.FeedForwardNetwork:\n  return networks_lib.FeedForwardNetwork(\n      init=lambda: None,\n      apply=lambda params, observation_t, action_tm1: action_tm1)",
  "def get_fake_n_step_return() -> networks_lib.FeedForwardNetwork:\n\n  def apply(params, observation_t, action_t):\n    del params, action_t\n    return jnp.ones((observation_t.shape[0], 1))\n\n  return networks_lib.FeedForwardNetwork(init=lambda: None, apply=apply)",
  "class WeightedAverageTests(parameterized.TestCase):\n\n  @parameterized.parameters((np.array([1, 1, 1]), 1), (np.array([0, 1, 0]), 10),\n                            (np.array([-1, 1, -1]), 4),\n                            (np.array([-10, 30, 0]), -0.5))\n  def test_weighted_averages(self, cum_reward, kappa):\n    \"\"\"Compares method with a local version of the exp-weighted averaging.\"\"\"\n    action_trajectories = jnp.reshape(\n        jnp.arange(3 * 10 * 4), (3, 10, 4), order='F')\n    averaged_trajectory = mppi.return_weighted_average(\n        action_trajectories=action_trajectories,\n        cum_reward=cum_reward,\n        kappa=kappa)\n    exp_weights = jnp.exp(kappa * cum_reward)\n    # Verify single-value averaging lines up with the global averaging call:\n    for i in range(10):\n      for j in range(4):\n        np.testing.assert_allclose(\n            averaged_trajectory[i, j],\n            jnp.sum(exp_weights * action_trajectories[:, i, j]) /\n            jnp.sum(exp_weights),\n            atol=1E-5,\n            rtol=1E-5)",
  "class MPPITest(parameterized.TestCase):\n  \"\"\"This tests the MPPI planner to make sure it is correctly rolling out.\n\n  It does not check the actual performance of the planner, as this would be a\n  bit more complicated to set up.\n  \"\"\"\n\n  # TODO(dulacarnold): Look at how we can check this is actually finding an\n  # optimal path through the model.\n\n  def setUp(self):\n    super().setUp()\n    self.state_dims = 8\n    self.action_dims = 4\n    self.params = {\n        'world': jnp.ones((3,)),\n        'policy': jnp.ones((3,)),\n        'value': jnp.ones((3,))\n    }\n    self.env_spec = specs.EnvironmentSpec(\n        observations=specs.Array(shape=(self.state_dims,), dtype=float),\n        actions=specs.Array(shape=(self.action_dims,), dtype=float),\n        rewards=specs.Array(shape=(1,), dtype=float, name='reward'),\n        discounts=specs.BoundedArray(\n            shape=(), dtype=float, minimum=0., maximum=1., name='discount'))\n\n  @parameterized.named_parameters(('NO-PLAN', 0), ('NORMAL', 10))\n  def test_planner_init(self, horizon: int):\n    world_model = get_fake_world_model()\n    rr_world_model = functools.partial(ensemble.apply_round_robin,\n                                       world_model.apply)\n    policy_prior = get_fake_policy_prior()\n\n    def _rr_policy_prior(params, key, observation_t, action_tm1):\n      del key\n      return ensemble.apply_round_robin(\n          policy_prior.apply,\n          params,\n          observation_t=observation_t,\n          action_tm1=action_tm1)\n\n    rr_policy_prior = models.feed_forward_policy_prior_to_actor_core(\n        _rr_policy_prior, jnp.zeros((1, self.action_dims)))\n\n    n_step_return = get_fake_n_step_return()\n    n_step_return = functools.partial(ensemble.apply_mean, n_step_return.apply)\n\n    config = mppi.MPPIConfig(\n        sigma=1,\n        beta=0.2,\n        horizon=horizon,\n        n_trajectories=9,\n        action_aggregation_fn=functools.partial(\n            mppi.return_weighted_average, kappa=1))\n    previous_trajectory = mppi.get_initial_trajectory(config, self.env_spec)\n    key = jax.random.PRNGKey(0)\n    for _ in range(5):\n      previous_trajectory = mppi.mppi_planner(\n          config,\n          world_model=rr_world_model,\n          policy_prior=rr_policy_prior,\n          n_step_return=n_step_return,\n          world_model_params=self.params,\n          policy_prior_params=self.params,\n          n_step_return_params=self.params,\n          random_key=key,\n          observation=jnp.ones(self.state_dims),\n          previous_trajectory=previous_trajectory)",
  "def apply(params: Any, observation_t: jnp.ndarray, action_t: jnp.ndarray):\n    del params\n    return observation_t, jnp.ones((\n        action_t.shape[0],\n        1,\n    ))",
  "def apply(params, observation_t, action_t):\n    del params, action_t\n    return jnp.ones((observation_t.shape[0], 1))",
  "def test_weighted_averages(self, cum_reward, kappa):\n    \"\"\"Compares method with a local version of the exp-weighted averaging.\"\"\"\n    action_trajectories = jnp.reshape(\n        jnp.arange(3 * 10 * 4), (3, 10, 4), order='F')\n    averaged_trajectory = mppi.return_weighted_average(\n        action_trajectories=action_trajectories,\n        cum_reward=cum_reward,\n        kappa=kappa)\n    exp_weights = jnp.exp(kappa * cum_reward)\n    # Verify single-value averaging lines up with the global averaging call:\n    for i in range(10):\n      for j in range(4):\n        np.testing.assert_allclose(\n            averaged_trajectory[i, j],\n            jnp.sum(exp_weights * action_trajectories[:, i, j]) /\n            jnp.sum(exp_weights),\n            atol=1E-5,\n            rtol=1E-5)",
  "def setUp(self):\n    super().setUp()\n    self.state_dims = 8\n    self.action_dims = 4\n    self.params = {\n        'world': jnp.ones((3,)),\n        'policy': jnp.ones((3,)),\n        'value': jnp.ones((3,))\n    }\n    self.env_spec = specs.EnvironmentSpec(\n        observations=specs.Array(shape=(self.state_dims,), dtype=float),\n        actions=specs.Array(shape=(self.action_dims,), dtype=float),\n        rewards=specs.Array(shape=(1,), dtype=float, name='reward'),\n        discounts=specs.BoundedArray(\n            shape=(), dtype=float, minimum=0., maximum=1., name='discount'))",
  "def test_planner_init(self, horizon: int):\n    world_model = get_fake_world_model()\n    rr_world_model = functools.partial(ensemble.apply_round_robin,\n                                       world_model.apply)\n    policy_prior = get_fake_policy_prior()\n\n    def _rr_policy_prior(params, key, observation_t, action_tm1):\n      del key\n      return ensemble.apply_round_robin(\n          policy_prior.apply,\n          params,\n          observation_t=observation_t,\n          action_tm1=action_tm1)\n\n    rr_policy_prior = models.feed_forward_policy_prior_to_actor_core(\n        _rr_policy_prior, jnp.zeros((1, self.action_dims)))\n\n    n_step_return = get_fake_n_step_return()\n    n_step_return = functools.partial(ensemble.apply_mean, n_step_return.apply)\n\n    config = mppi.MPPIConfig(\n        sigma=1,\n        beta=0.2,\n        horizon=horizon,\n        n_trajectories=9,\n        action_aggregation_fn=functools.partial(\n            mppi.return_weighted_average, kappa=1))\n    previous_trajectory = mppi.get_initial_trajectory(config, self.env_spec)\n    key = jax.random.PRNGKey(0)\n    for _ in range(5):\n      previous_trajectory = mppi.mppi_planner(\n          config,\n          world_model=rr_world_model,\n          policy_prior=rr_policy_prior,\n          n_step_return=n_step_return,\n          world_model_params=self.params,\n          policy_prior_params=self.params,\n          n_step_return_params=self.params,\n          random_key=key,\n          observation=jnp.ones(self.state_dims),\n          previous_trajectory=previous_trajectory)",
  "def _rr_policy_prior(params, key, observation_t, action_tm1):\n      del key\n      return ensemble.apply_round_robin(\n          policy_prior.apply,\n          params,\n          observation_t=observation_t,\n          action_tm1=action_tm1)",
  "class TrainingState(NamedTuple):\n  \"\"\"Holds the agent's training state.\"\"\"\n  params: networks_lib.Params\n  target_params: networks_lib.Params\n  opt_state: optax.OptState\n  steps: int\n  random_key: networks_lib.PRNGKey",
  "class R2D2Learner(acme.Learner):\n  \"\"\"R2D2 learner.\"\"\"\n\n  def __init__(self,\n               networks: r2d2_networks.R2D2Networks,\n               batch_size: int,\n               random_key: networks_lib.PRNGKey,\n               burn_in_length: int,\n               discount: float,\n               importance_sampling_exponent: float,\n               max_priority_weight: float,\n               target_update_period: int,\n               iterator: Iterator[R2D2ReplaySample],\n               optimizer: optax.GradientTransformation,\n               bootstrap_n: int = 5,\n               tx_pair: rlax.TxPair = rlax.SIGNED_HYPERBOLIC_PAIR,\n               clip_rewards: bool = False,\n               max_abs_reward: float = 1.,\n               use_core_state: bool = True,\n               prefetch_size: int = 2,\n               replay_client: Optional[reverb.Client] = None,\n               counter: Optional[counting.Counter] = None,\n               logger: Optional[loggers.Logger] = None):\n    \"\"\"Initializes the learner.\"\"\"\n\n    def loss(\n        params: networks_lib.Params,\n        target_params: networks_lib.Params,\n        key_grad: networks_lib.PRNGKey,\n        sample: reverb.ReplaySample,\n    ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n      \"\"\"Computes mean transformed N-step loss for a batch of sequences.\"\"\"\n\n      # Get core state & warm it up on observations for a burn-in period.\n      if use_core_state:\n        # Replay core state.\n        # NOTE: We may need to recover the type of the hk.LSTMState if the user\n        # specifies a dynamically unrolled RNN as it will strictly enforce the\n        # match between input/output state types.\n        online_state = utils.maybe_recover_lstm_type(\n            sample.data.extras.get('core_state'))\n      else:\n        key_grad, initial_state_rng = jax.random.split(key_grad)\n        online_state = networks.init_recurrent_state(initial_state_rng,\n                                                     batch_size)\n      target_state = online_state\n\n      # Convert sample data to sequence-major format [T, B, ...].\n      data = utils.batch_to_sequence(sample.data)\n\n      # Maybe burn the core state in.\n      if burn_in_length:\n        burn_obs = jax.tree_map(lambda x: x[:burn_in_length], data.observation)\n        key_grad, key1, key2 = jax.random.split(key_grad, 3)\n        _, online_state = networks.unroll(params, key1, burn_obs, online_state)\n        _, target_state = networks.unroll(target_params, key2, burn_obs,\n                                          target_state)\n\n      # Only get data to learn on from after the end of the burn in period.\n      data = jax.tree_map(lambda seq: seq[burn_in_length:], data)\n\n      # Unroll on sequences to get online and target Q-Values.\n      key1, key2 = jax.random.split(key_grad)\n      online_q, _ = networks.unroll(params, key1, data.observation,\n                                    online_state)\n      target_q, _ = networks.unroll(target_params, key2, data.observation,\n                                    target_state)\n\n      # Get value-selector actions from online Q-values for double Q-learning.\n      selector_actions = jnp.argmax(online_q, axis=-1)\n      # Preprocess discounts & rewards.\n      discounts = (data.discount * discount).astype(online_q.dtype)\n      rewards = data.reward\n      if clip_rewards:\n        rewards = jnp.clip(rewards, -max_abs_reward, max_abs_reward)\n      rewards = rewards.astype(online_q.dtype)\n\n      # Get N-step transformed TD error and loss.\n      batch_td_error_fn = jax.vmap(\n          functools.partial(\n              rlax.transformed_n_step_q_learning,\n              n=bootstrap_n,\n              tx_pair=tx_pair),\n          in_axes=1,\n          out_axes=1)\n      batch_td_error = batch_td_error_fn(\n          online_q[:-1],\n          data.action[:-1],\n          target_q[1:],\n          selector_actions[1:],\n          rewards[:-1],\n          discounts[:-1])\n      batch_loss = 0.5 * jnp.square(batch_td_error).sum(axis=0)\n\n      # Importance weighting.\n      probs = sample.info.probability\n      importance_weights = (1. / (probs + 1e-6)).astype(online_q.dtype)\n      importance_weights **= importance_sampling_exponent\n      importance_weights /= jnp.max(importance_weights)\n      mean_loss = jnp.mean(importance_weights * batch_loss)\n\n      # Calculate priorities as a mixture of max and mean sequence errors.\n      abs_td_error = jnp.abs(batch_td_error).astype(online_q.dtype)\n      max_priority = max_priority_weight * jnp.max(abs_td_error, axis=0)\n      mean_priority = (1 - max_priority_weight) * jnp.mean(abs_td_error, axis=0)\n      priorities = (max_priority + mean_priority)\n\n      return mean_loss, priorities\n\n    def sgd_step(\n        state: TrainingState,\n        samples: reverb.ReplaySample\n    ) -> Tuple[TrainingState, jnp.ndarray, Dict[str, jnp.ndarray]]:\n      \"\"\"Performs an update step, averaging over pmap replicas.\"\"\"\n\n      # Compute loss and gradients.\n      grad_fn = jax.value_and_grad(loss, has_aux=True)\n      key, key_grad = jax.random.split(state.random_key)\n      (loss_value, priorities), gradients = grad_fn(state.params,\n                                                    state.target_params,\n                                                    key_grad,\n                                                    samples)\n\n      # Average gradients over pmap replicas before optimizer update.\n      gradients = jax.lax.pmean(gradients, _PMAP_AXIS_NAME)\n\n      # Apply optimizer updates.\n      updates, new_opt_state = optimizer.update(gradients, state.opt_state)\n      new_params = optax.apply_updates(state.params, updates)\n\n      # Periodically update target networks.\n      steps = state.steps + 1\n      target_params = optax.periodic_update(new_params, state.target_params,  # pytype: disable=wrong-arg-types  # numpy-scalars\n                                            steps, self._target_update_period)\n\n      new_state = TrainingState(\n          params=new_params,\n          target_params=target_params,\n          opt_state=new_opt_state,\n          steps=steps,\n          random_key=key)\n      return new_state, priorities, {'loss': loss_value}\n\n    def update_priorities(\n        keys_and_priorities: Tuple[jnp.ndarray, jnp.ndarray]):\n      keys, priorities = keys_and_priorities\n      keys, priorities = tree.map_structure(\n          # Fetch array and combine device and batch dimensions.\n          lambda x: utils.fetch_devicearray(x).reshape((-1,) + x.shape[2:]),\n          (keys, priorities))\n      replay_client.mutate_priorities(  # pytype: disable=attribute-error\n          table=adders.DEFAULT_PRIORITY_TABLE,\n          updates=dict(zip(keys, priorities)))\n\n    # Internalise components, hyperparameters, logger, counter, and methods.\n    self._iterator = iterator\n    self._replay_client = replay_client\n    self._target_update_period = target_update_period\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger(\n        'learner',\n        asynchronous=True,\n        serialize_fn=utils.fetch_devicearray,\n        time_delta=1.,\n        steps_key=self._counter.get_steps_key())\n\n    self._sgd_step = jax.pmap(sgd_step, axis_name=_PMAP_AXIS_NAME)\n    self._async_priority_updater = async_utils.AsyncExecutor(update_priorities)\n\n    # Initialise and internalise training state (parameters/optimiser state).\n    random_key, key_init = jax.random.split(random_key)\n    initial_params = networks.init(key_init)\n    opt_state = optimizer.init(initial_params)\n\n    # Log how many parameters the network has.\n    sizes = tree.map_structure(jnp.size, initial_params)\n    logging.info('Total number of params: %d',\n                 sum(tree.flatten(sizes.values())))\n\n    state = TrainingState(\n        params=initial_params,\n        target_params=initial_params,\n        opt_state=opt_state,\n        steps=jnp.array(0),\n        random_key=random_key)\n    # Replicate parameters.\n    self._state = utils.replicate_in_all_devices(state)\n\n  def step(self):\n    prefetching_split = next(self._iterator)\n    # The split_sample method passed to utils.sharded_prefetch specifies what\n    # parts of the objects returned by the original iterator are kept in the\n    # host and what parts are prefetched on-device.\n    # In this case the host property of the prefetching split contains only the\n    # replay keys and the device property is the prefetched full original\n    # sample.\n    keys = prefetching_split.host\n    samples: reverb.ReplaySample = prefetching_split.device\n\n    # Do a batch of SGD.\n    start = time.time()\n    self._state, priorities, metrics = self._sgd_step(self._state, samples)\n    # Take metrics from first replica.\n    metrics = utils.get_from_first_device(metrics)\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1, time_elapsed=time.time() - start)\n\n    # Update priorities in replay.\n    if self._replay_client:\n      self._async_priority_updater.put((keys, priorities))\n\n    # Attempt to write logs.\n    self._logger.write({**metrics, **counts})\n\n  def get_variables(self, names: List[str]) -> List[networks_lib.Params]:\n    del names  # There's only one available set of params in this agent.\n    # Return first replica of parameters.\n    return utils.get_from_first_device([self._state.params])\n\n  def save(self) -> TrainingState:\n    # Serialize only the first replica of parameters and optimizer state.\n    return utils.get_from_first_device(self._state)\n\n  def restore(self, state: TrainingState):\n    self._state = utils.replicate_in_all_devices(state)",
  "def __init__(self,\n               networks: r2d2_networks.R2D2Networks,\n               batch_size: int,\n               random_key: networks_lib.PRNGKey,\n               burn_in_length: int,\n               discount: float,\n               importance_sampling_exponent: float,\n               max_priority_weight: float,\n               target_update_period: int,\n               iterator: Iterator[R2D2ReplaySample],\n               optimizer: optax.GradientTransformation,\n               bootstrap_n: int = 5,\n               tx_pair: rlax.TxPair = rlax.SIGNED_HYPERBOLIC_PAIR,\n               clip_rewards: bool = False,\n               max_abs_reward: float = 1.,\n               use_core_state: bool = True,\n               prefetch_size: int = 2,\n               replay_client: Optional[reverb.Client] = None,\n               counter: Optional[counting.Counter] = None,\n               logger: Optional[loggers.Logger] = None):\n    \"\"\"Initializes the learner.\"\"\"\n\n    def loss(\n        params: networks_lib.Params,\n        target_params: networks_lib.Params,\n        key_grad: networks_lib.PRNGKey,\n        sample: reverb.ReplaySample,\n    ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n      \"\"\"Computes mean transformed N-step loss for a batch of sequences.\"\"\"\n\n      # Get core state & warm it up on observations for a burn-in period.\n      if use_core_state:\n        # Replay core state.\n        # NOTE: We may need to recover the type of the hk.LSTMState if the user\n        # specifies a dynamically unrolled RNN as it will strictly enforce the\n        # match between input/output state types.\n        online_state = utils.maybe_recover_lstm_type(\n            sample.data.extras.get('core_state'))\n      else:\n        key_grad, initial_state_rng = jax.random.split(key_grad)\n        online_state = networks.init_recurrent_state(initial_state_rng,\n                                                     batch_size)\n      target_state = online_state\n\n      # Convert sample data to sequence-major format [T, B, ...].\n      data = utils.batch_to_sequence(sample.data)\n\n      # Maybe burn the core state in.\n      if burn_in_length:\n        burn_obs = jax.tree_map(lambda x: x[:burn_in_length], data.observation)\n        key_grad, key1, key2 = jax.random.split(key_grad, 3)\n        _, online_state = networks.unroll(params, key1, burn_obs, online_state)\n        _, target_state = networks.unroll(target_params, key2, burn_obs,\n                                          target_state)\n\n      # Only get data to learn on from after the end of the burn in period.\n      data = jax.tree_map(lambda seq: seq[burn_in_length:], data)\n\n      # Unroll on sequences to get online and target Q-Values.\n      key1, key2 = jax.random.split(key_grad)\n      online_q, _ = networks.unroll(params, key1, data.observation,\n                                    online_state)\n      target_q, _ = networks.unroll(target_params, key2, data.observation,\n                                    target_state)\n\n      # Get value-selector actions from online Q-values for double Q-learning.\n      selector_actions = jnp.argmax(online_q, axis=-1)\n      # Preprocess discounts & rewards.\n      discounts = (data.discount * discount).astype(online_q.dtype)\n      rewards = data.reward\n      if clip_rewards:\n        rewards = jnp.clip(rewards, -max_abs_reward, max_abs_reward)\n      rewards = rewards.astype(online_q.dtype)\n\n      # Get N-step transformed TD error and loss.\n      batch_td_error_fn = jax.vmap(\n          functools.partial(\n              rlax.transformed_n_step_q_learning,\n              n=bootstrap_n,\n              tx_pair=tx_pair),\n          in_axes=1,\n          out_axes=1)\n      batch_td_error = batch_td_error_fn(\n          online_q[:-1],\n          data.action[:-1],\n          target_q[1:],\n          selector_actions[1:],\n          rewards[:-1],\n          discounts[:-1])\n      batch_loss = 0.5 * jnp.square(batch_td_error).sum(axis=0)\n\n      # Importance weighting.\n      probs = sample.info.probability\n      importance_weights = (1. / (probs + 1e-6)).astype(online_q.dtype)\n      importance_weights **= importance_sampling_exponent\n      importance_weights /= jnp.max(importance_weights)\n      mean_loss = jnp.mean(importance_weights * batch_loss)\n\n      # Calculate priorities as a mixture of max and mean sequence errors.\n      abs_td_error = jnp.abs(batch_td_error).astype(online_q.dtype)\n      max_priority = max_priority_weight * jnp.max(abs_td_error, axis=0)\n      mean_priority = (1 - max_priority_weight) * jnp.mean(abs_td_error, axis=0)\n      priorities = (max_priority + mean_priority)\n\n      return mean_loss, priorities\n\n    def sgd_step(\n        state: TrainingState,\n        samples: reverb.ReplaySample\n    ) -> Tuple[TrainingState, jnp.ndarray, Dict[str, jnp.ndarray]]:\n      \"\"\"Performs an update step, averaging over pmap replicas.\"\"\"\n\n      # Compute loss and gradients.\n      grad_fn = jax.value_and_grad(loss, has_aux=True)\n      key, key_grad = jax.random.split(state.random_key)\n      (loss_value, priorities), gradients = grad_fn(state.params,\n                                                    state.target_params,\n                                                    key_grad,\n                                                    samples)\n\n      # Average gradients over pmap replicas before optimizer update.\n      gradients = jax.lax.pmean(gradients, _PMAP_AXIS_NAME)\n\n      # Apply optimizer updates.\n      updates, new_opt_state = optimizer.update(gradients, state.opt_state)\n      new_params = optax.apply_updates(state.params, updates)\n\n      # Periodically update target networks.\n      steps = state.steps + 1\n      target_params = optax.periodic_update(new_params, state.target_params,  # pytype: disable=wrong-arg-types  # numpy-scalars\n                                            steps, self._target_update_period)\n\n      new_state = TrainingState(\n          params=new_params,\n          target_params=target_params,\n          opt_state=new_opt_state,\n          steps=steps,\n          random_key=key)\n      return new_state, priorities, {'loss': loss_value}\n\n    def update_priorities(\n        keys_and_priorities: Tuple[jnp.ndarray, jnp.ndarray]):\n      keys, priorities = keys_and_priorities\n      keys, priorities = tree.map_structure(\n          # Fetch array and combine device and batch dimensions.\n          lambda x: utils.fetch_devicearray(x).reshape((-1,) + x.shape[2:]),\n          (keys, priorities))\n      replay_client.mutate_priorities(  # pytype: disable=attribute-error\n          table=adders.DEFAULT_PRIORITY_TABLE,\n          updates=dict(zip(keys, priorities)))\n\n    # Internalise components, hyperparameters, logger, counter, and methods.\n    self._iterator = iterator\n    self._replay_client = replay_client\n    self._target_update_period = target_update_period\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger(\n        'learner',\n        asynchronous=True,\n        serialize_fn=utils.fetch_devicearray,\n        time_delta=1.,\n        steps_key=self._counter.get_steps_key())\n\n    self._sgd_step = jax.pmap(sgd_step, axis_name=_PMAP_AXIS_NAME)\n    self._async_priority_updater = async_utils.AsyncExecutor(update_priorities)\n\n    # Initialise and internalise training state (parameters/optimiser state).\n    random_key, key_init = jax.random.split(random_key)\n    initial_params = networks.init(key_init)\n    opt_state = optimizer.init(initial_params)\n\n    # Log how many parameters the network has.\n    sizes = tree.map_structure(jnp.size, initial_params)\n    logging.info('Total number of params: %d',\n                 sum(tree.flatten(sizes.values())))\n\n    state = TrainingState(\n        params=initial_params,\n        target_params=initial_params,\n        opt_state=opt_state,\n        steps=jnp.array(0),\n        random_key=random_key)\n    # Replicate parameters.\n    self._state = utils.replicate_in_all_devices(state)",
  "def step(self):\n    prefetching_split = next(self._iterator)\n    # The split_sample method passed to utils.sharded_prefetch specifies what\n    # parts of the objects returned by the original iterator are kept in the\n    # host and what parts are prefetched on-device.\n    # In this case the host property of the prefetching split contains only the\n    # replay keys and the device property is the prefetched full original\n    # sample.\n    keys = prefetching_split.host\n    samples: reverb.ReplaySample = prefetching_split.device\n\n    # Do a batch of SGD.\n    start = time.time()\n    self._state, priorities, metrics = self._sgd_step(self._state, samples)\n    # Take metrics from first replica.\n    metrics = utils.get_from_first_device(metrics)\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=1, time_elapsed=time.time() - start)\n\n    # Update priorities in replay.\n    if self._replay_client:\n      self._async_priority_updater.put((keys, priorities))\n\n    # Attempt to write logs.\n    self._logger.write({**metrics, **counts})",
  "def get_variables(self, names: List[str]) -> List[networks_lib.Params]:\n    del names  # There's only one available set of params in this agent.\n    # Return first replica of parameters.\n    return utils.get_from_first_device([self._state.params])",
  "def save(self) -> TrainingState:\n    # Serialize only the first replica of parameters and optimizer state.\n    return utils.get_from_first_device(self._state)",
  "def restore(self, state: TrainingState):\n    self._state = utils.replicate_in_all_devices(state)",
  "def loss(\n        params: networks_lib.Params,\n        target_params: networks_lib.Params,\n        key_grad: networks_lib.PRNGKey,\n        sample: reverb.ReplaySample,\n    ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n      \"\"\"Computes mean transformed N-step loss for a batch of sequences.\"\"\"\n\n      # Get core state & warm it up on observations for a burn-in period.\n      if use_core_state:\n        # Replay core state.\n        # NOTE: We may need to recover the type of the hk.LSTMState if the user\n        # specifies a dynamically unrolled RNN as it will strictly enforce the\n        # match between input/output state types.\n        online_state = utils.maybe_recover_lstm_type(\n            sample.data.extras.get('core_state'))\n      else:\n        key_grad, initial_state_rng = jax.random.split(key_grad)\n        online_state = networks.init_recurrent_state(initial_state_rng,\n                                                     batch_size)\n      target_state = online_state\n\n      # Convert sample data to sequence-major format [T, B, ...].\n      data = utils.batch_to_sequence(sample.data)\n\n      # Maybe burn the core state in.\n      if burn_in_length:\n        burn_obs = jax.tree_map(lambda x: x[:burn_in_length], data.observation)\n        key_grad, key1, key2 = jax.random.split(key_grad, 3)\n        _, online_state = networks.unroll(params, key1, burn_obs, online_state)\n        _, target_state = networks.unroll(target_params, key2, burn_obs,\n                                          target_state)\n\n      # Only get data to learn on from after the end of the burn in period.\n      data = jax.tree_map(lambda seq: seq[burn_in_length:], data)\n\n      # Unroll on sequences to get online and target Q-Values.\n      key1, key2 = jax.random.split(key_grad)\n      online_q, _ = networks.unroll(params, key1, data.observation,\n                                    online_state)\n      target_q, _ = networks.unroll(target_params, key2, data.observation,\n                                    target_state)\n\n      # Get value-selector actions from online Q-values for double Q-learning.\n      selector_actions = jnp.argmax(online_q, axis=-1)\n      # Preprocess discounts & rewards.\n      discounts = (data.discount * discount).astype(online_q.dtype)\n      rewards = data.reward\n      if clip_rewards:\n        rewards = jnp.clip(rewards, -max_abs_reward, max_abs_reward)\n      rewards = rewards.astype(online_q.dtype)\n\n      # Get N-step transformed TD error and loss.\n      batch_td_error_fn = jax.vmap(\n          functools.partial(\n              rlax.transformed_n_step_q_learning,\n              n=bootstrap_n,\n              tx_pair=tx_pair),\n          in_axes=1,\n          out_axes=1)\n      batch_td_error = batch_td_error_fn(\n          online_q[:-1],\n          data.action[:-1],\n          target_q[1:],\n          selector_actions[1:],\n          rewards[:-1],\n          discounts[:-1])\n      batch_loss = 0.5 * jnp.square(batch_td_error).sum(axis=0)\n\n      # Importance weighting.\n      probs = sample.info.probability\n      importance_weights = (1. / (probs + 1e-6)).astype(online_q.dtype)\n      importance_weights **= importance_sampling_exponent\n      importance_weights /= jnp.max(importance_weights)\n      mean_loss = jnp.mean(importance_weights * batch_loss)\n\n      # Calculate priorities as a mixture of max and mean sequence errors.\n      abs_td_error = jnp.abs(batch_td_error).astype(online_q.dtype)\n      max_priority = max_priority_weight * jnp.max(abs_td_error, axis=0)\n      mean_priority = (1 - max_priority_weight) * jnp.mean(abs_td_error, axis=0)\n      priorities = (max_priority + mean_priority)\n\n      return mean_loss, priorities",
  "def sgd_step(\n        state: TrainingState,\n        samples: reverb.ReplaySample\n    ) -> Tuple[TrainingState, jnp.ndarray, Dict[str, jnp.ndarray]]:\n      \"\"\"Performs an update step, averaging over pmap replicas.\"\"\"\n\n      # Compute loss and gradients.\n      grad_fn = jax.value_and_grad(loss, has_aux=True)\n      key, key_grad = jax.random.split(state.random_key)\n      (loss_value, priorities), gradients = grad_fn(state.params,\n                                                    state.target_params,\n                                                    key_grad,\n                                                    samples)\n\n      # Average gradients over pmap replicas before optimizer update.\n      gradients = jax.lax.pmean(gradients, _PMAP_AXIS_NAME)\n\n      # Apply optimizer updates.\n      updates, new_opt_state = optimizer.update(gradients, state.opt_state)\n      new_params = optax.apply_updates(state.params, updates)\n\n      # Periodically update target networks.\n      steps = state.steps + 1\n      target_params = optax.periodic_update(new_params, state.target_params,  # pytype: disable=wrong-arg-types  # numpy-scalars\n                                            steps, self._target_update_period)\n\n      new_state = TrainingState(\n          params=new_params,\n          target_params=target_params,\n          opt_state=new_opt_state,\n          steps=steps,\n          random_key=key)\n      return new_state, priorities, {'loss': loss_value}",
  "def update_priorities(\n        keys_and_priorities: Tuple[jnp.ndarray, jnp.ndarray]):\n      keys, priorities = keys_and_priorities\n      keys, priorities = tree.map_structure(\n          # Fetch array and combine device and batch dimensions.\n          lambda x: utils.fetch_devicearray(x).reshape((-1,) + x.shape[2:]),\n          (keys, priorities))\n      replay_client.mutate_priorities(  # pytype: disable=attribute-error\n          table=adders.DEFAULT_PRIORITY_TABLE,\n          updates=dict(zip(keys, priorities)))",
  "def make_atari_networks(env_spec: specs.EnvironmentSpec) -> R2D2Networks:\n  \"\"\"Builds default R2D2 networks for Atari games.\"\"\"\n\n  def make_core_module() -> networks_lib.R2D2AtariNetwork:\n    return networks_lib.R2D2AtariNetwork(env_spec.actions.num_values)\n\n  return networks_lib.make_unrollable_network(env_spec, make_core_module)",
  "def make_core_module() -> networks_lib.R2D2AtariNetwork:\n    return networks_lib.R2D2AtariNetwork(env_spec.actions.num_values)",
  "class R2D2Config:\n  \"\"\"Configuration options for R2D2 agent.\"\"\"\n  discount: float = 0.997\n  target_update_period: int = 2500\n  evaluation_epsilon: float = 0.\n  num_epsilons: int = 256\n  variable_update_period: int = 400\n\n  # Learner options\n  burn_in_length: int = 40\n  trace_length: int = 80\n  sequence_period: int = 40\n  learning_rate: float = 1e-3\n  bootstrap_n: int = 5\n  clip_rewards: bool = False\n  tx_pair: rlax.TxPair = rlax.SIGNED_HYPERBOLIC_PAIR\n\n  # Replay options\n  samples_per_insert_tolerance_rate: float = 0.1\n  samples_per_insert: float = 4.0\n  min_replay_size: int = 50_000\n  max_replay_size: int = 100_000\n  batch_size: int = 64\n  prefetch_size: int = 2\n  num_parallel_calls: int = 16\n  replay_table_name: str = adders_reverb.DEFAULT_PRIORITY_TABLE\n\n  # Priority options\n  importance_sampling_exponent: float = 0.6\n  priority_exponent: float = 0.9\n  max_priority_weight: float = 0.9",
  "def _build_sequence(length: int,\n                    step_spec: reverb_base.Step) -> reverb_base.Trajectory:\n  \"\"\"Constructs the sequence using only the first value of core_state.\"\"\"\n  step_dict = step_spec._asdict()\n  extras_dict = step_dict.pop('extras')\n  return reverb_base.Trajectory(\n      **tree.map_structure(lambda x: x[-length:], step_dict),\n      extras=tree.map_structure(lambda x: x[-length], extras_dict))",
  "def _zero_pad(sequence_length: int) -> datasets.Transform:\n  \"\"\"Adds zero padding to the right so all samples have the same length.\"\"\"\n\n  def _zero_pad_transform(sample: reverb.ReplaySample) -> reverb.ReplaySample:\n    trajectory: reverb_base.Trajectory = sample.data\n\n    # Split steps and extras data (the extras won't be padded as they only\n    # contain one element)\n    trajectory_steps = trajectory._asdict()\n    trajectory_extras = trajectory_steps.pop('extras')\n\n    unpadded_length = len(tree.flatten(trajectory_steps)[0])\n\n    # Do nothing if the sequence is already full.\n    if unpadded_length != sequence_length:\n      to_pad = sequence_length - unpadded_length\n      pad = lambda x: tf.pad(x, [[0, to_pad]] + [[0, 0]] * (len(x.shape) - 1))\n\n      trajectory_steps = tree.map_structure(pad, trajectory_steps)\n\n    # Set the shape to be statically known, and checks it at runtime.\n    def _ensure_shape(x):\n      shape = tf.TensorShape([sequence_length]).concatenate(x.shape[1:])\n      return tf.ensure_shape(x, shape)\n\n    trajectory_steps = tree.map_structure(_ensure_shape, trajectory_steps)\n    return reverb.ReplaySample(\n        info=sample.info,\n        data=reverb_base.Trajectory(\n            **trajectory_steps, extras=trajectory_extras))\n\n  return _zero_pad_transform",
  "def _make_adder_config(step_spec: reverb_base.Step, seq_len: int,\n                       seq_period: int) -> List[sw.Config]:\n  return structured.create_sequence_config(\n      step_spec=step_spec,\n      sequence_length=seq_len,\n      period=seq_period,\n      end_of_episode_behavior=adders_reverb.EndBehavior.TRUNCATE,\n      sequence_pattern=_build_sequence)",
  "class R2D2Builder(Generic[actor_core_lib.RecurrentState],\n                  builders.ActorLearnerBuilder[r2d2_networks.R2D2Networks,\n                                               r2d2_actor.R2D2Policy,\n                                               r2d2_learning.R2D2ReplaySample]):\n  \"\"\"R2D2 Builder.\n\n  This is constructs all of the components for Recurrent Experience Replay in\n  Distributed Reinforcement Learning (Kapturowski et al.)\n  https://openreview.net/pdf?id=r1lyTjAqYX.\n  \"\"\"\n\n  def __init__(self, config: r2d2_config.R2D2Config):\n    \"\"\"Creates a R2D2 learner, a behavior policy and an eval actor.\"\"\"\n    self._config = config\n    self._sequence_length = (\n        self._config.burn_in_length + self._config.trace_length + 1)\n\n  @property\n  def _batch_size_per_device(self) -> int:\n    \"\"\"Splits batch size across all learner devices evenly.\"\"\"\n    # TODO(bshahr): Using jax.device_count will not be valid when colocating\n    # learning and inference.\n    return self._config.batch_size // jax.device_count()\n\n  def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: r2d2_networks.R2D2Networks,\n      dataset: Iterator[r2d2_learning.R2D2ReplaySample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    del environment_spec\n\n    # The learner updates the parameters (and initializes them).\n    return r2d2_learning.R2D2Learner(\n        networks=networks,\n        batch_size=self._batch_size_per_device,\n        random_key=random_key,\n        burn_in_length=self._config.burn_in_length,\n        discount=self._config.discount,\n        importance_sampling_exponent=(\n            self._config.importance_sampling_exponent),\n        max_priority_weight=self._config.max_priority_weight,\n        target_update_period=self._config.target_update_period,\n        iterator=dataset,\n        optimizer=optax.adam(self._config.learning_rate),\n        bootstrap_n=self._config.bootstrap_n,\n        tx_pair=self._config.tx_pair,\n        clip_rewards=self._config.clip_rewards,\n        replay_client=replay_client,\n        counter=counter,\n        logger=logger_fn('learner'))\n\n  def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: r2d2_actor.R2D2Policy,\n  ) -> List[reverb.Table]:\n    \"\"\"Create tables to insert data into.\"\"\"\n    dummy_actor_state = policy.init(jax.random.PRNGKey(0))\n    extras_spec = policy.get_extras(dummy_actor_state)\n    step_spec = structured.create_step_spec(\n        environment_spec=environment_spec, extras_spec=extras_spec)\n    if self._config.samples_per_insert:\n      samples_per_insert_tolerance = (\n          self._config.samples_per_insert_tolerance_rate *\n          self._config.samples_per_insert)\n      error_buffer = self._config.min_replay_size * samples_per_insert_tolerance\n      limiter = reverb.rate_limiters.SampleToInsertRatio(\n          min_size_to_sample=self._config.min_replay_size,\n          samples_per_insert=self._config.samples_per_insert,\n          error_buffer=error_buffer)\n    else:\n      limiter = reverb.rate_limiters.MinSize(self._config.min_replay_size)\n    return [\n        reverb.Table(\n            name=self._config.replay_table_name,\n            sampler=reverb.selectors.Prioritized(\n                self._config.priority_exponent),\n            remover=reverb.selectors.Fifo(),\n            max_size=self._config.max_replay_size,\n            rate_limiter=limiter,\n            signature=sw.infer_signature(\n                configs=_make_adder_config(step_spec, self._sequence_length,\n                                           self._config.sequence_period),\n                step_spec=step_spec))\n    ]\n\n  def make_dataset_iterator(\n      self,\n      replay_client: reverb.Client) -> Iterator[r2d2_learning.R2D2ReplaySample]:\n    \"\"\"Create a dataset iterator to use for learning/updating the agent.\"\"\"\n    batch_size_per_learner = self._config.batch_size // jax.process_count()\n    dataset = datasets.make_reverb_dataset(\n        table=self._config.replay_table_name,\n        server_address=replay_client.server_address,\n        batch_size=self._batch_size_per_device,\n        num_parallel_calls=None,\n        max_in_flight_samples_per_worker=2 * batch_size_per_learner,\n        postprocess=_zero_pad(self._sequence_length),\n    )\n\n    return utils.multi_device_put(\n        dataset.as_numpy_iterator(),\n        devices=jax.local_devices(),\n        split_fn=utils.keep_key_on_host)\n\n  def make_adder(\n      self, replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[r2d2_actor.R2D2Policy]) -> Optional[adders.Adder]:\n    \"\"\"Create an adder which records data generated by the actor/environment.\"\"\"\n    if environment_spec is None or policy is None:\n      raise ValueError('`environment_spec` and `policy` cannot be None.')\n    dummy_actor_state = policy.init(jax.random.PRNGKey(0))\n    extras_spec = policy.get_extras(dummy_actor_state)\n    step_spec = structured.create_step_spec(\n        environment_spec=environment_spec, extras_spec=extras_spec)\n    return structured.StructuredAdder(\n        client=replay_client,\n        max_in_flight_items=5,\n        configs=_make_adder_config(step_spec, self._sequence_length,\n                                   self._config.sequence_period),\n        step_spec=step_spec)\n\n  def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: r2d2_actor.R2D2Policy,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> acme.Actor:\n    del environment_spec\n    # Create variable client.\n    variable_client = variable_utils.VariableClient(\n        variable_source,\n        key='actor_variables',\n        update_period=self._config.variable_update_period)\n\n    return actors.GenericActor(\n        policy, random_key, variable_client, adder, backend='cpu')\n\n  def make_policy(self,\n                  networks: r2d2_networks.R2D2Networks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> r2d2_actor.R2D2Policy:\n    if evaluation:\n      return r2d2_actor.get_actor_core(\n          networks,\n          num_epsilons=None,\n          evaluation_epsilon=self._config.evaluation_epsilon)\n    else:\n      return r2d2_actor.get_actor_core(networks, self._config.num_epsilons)",
  "def _zero_pad_transform(sample: reverb.ReplaySample) -> reverb.ReplaySample:\n    trajectory: reverb_base.Trajectory = sample.data\n\n    # Split steps and extras data (the extras won't be padded as they only\n    # contain one element)\n    trajectory_steps = trajectory._asdict()\n    trajectory_extras = trajectory_steps.pop('extras')\n\n    unpadded_length = len(tree.flatten(trajectory_steps)[0])\n\n    # Do nothing if the sequence is already full.\n    if unpadded_length != sequence_length:\n      to_pad = sequence_length - unpadded_length\n      pad = lambda x: tf.pad(x, [[0, to_pad]] + [[0, 0]] * (len(x.shape) - 1))\n\n      trajectory_steps = tree.map_structure(pad, trajectory_steps)\n\n    # Set the shape to be statically known, and checks it at runtime.\n    def _ensure_shape(x):\n      shape = tf.TensorShape([sequence_length]).concatenate(x.shape[1:])\n      return tf.ensure_shape(x, shape)\n\n    trajectory_steps = tree.map_structure(_ensure_shape, trajectory_steps)\n    return reverb.ReplaySample(\n        info=sample.info,\n        data=reverb_base.Trajectory(\n            **trajectory_steps, extras=trajectory_extras))",
  "def __init__(self, config: r2d2_config.R2D2Config):\n    \"\"\"Creates a R2D2 learner, a behavior policy and an eval actor.\"\"\"\n    self._config = config\n    self._sequence_length = (\n        self._config.burn_in_length + self._config.trace_length + 1)",
  "def _batch_size_per_device(self) -> int:\n    \"\"\"Splits batch size across all learner devices evenly.\"\"\"\n    # TODO(bshahr): Using jax.device_count will not be valid when colocating\n    # learning and inference.\n    return self._config.batch_size // jax.device_count()",
  "def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: r2d2_networks.R2D2Networks,\n      dataset: Iterator[r2d2_learning.R2D2ReplaySample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    del environment_spec\n\n    # The learner updates the parameters (and initializes them).\n    return r2d2_learning.R2D2Learner(\n        networks=networks,\n        batch_size=self._batch_size_per_device,\n        random_key=random_key,\n        burn_in_length=self._config.burn_in_length,\n        discount=self._config.discount,\n        importance_sampling_exponent=(\n            self._config.importance_sampling_exponent),\n        max_priority_weight=self._config.max_priority_weight,\n        target_update_period=self._config.target_update_period,\n        iterator=dataset,\n        optimizer=optax.adam(self._config.learning_rate),\n        bootstrap_n=self._config.bootstrap_n,\n        tx_pair=self._config.tx_pair,\n        clip_rewards=self._config.clip_rewards,\n        replay_client=replay_client,\n        counter=counter,\n        logger=logger_fn('learner'))",
  "def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: r2d2_actor.R2D2Policy,\n  ) -> List[reverb.Table]:\n    \"\"\"Create tables to insert data into.\"\"\"\n    dummy_actor_state = policy.init(jax.random.PRNGKey(0))\n    extras_spec = policy.get_extras(dummy_actor_state)\n    step_spec = structured.create_step_spec(\n        environment_spec=environment_spec, extras_spec=extras_spec)\n    if self._config.samples_per_insert:\n      samples_per_insert_tolerance = (\n          self._config.samples_per_insert_tolerance_rate *\n          self._config.samples_per_insert)\n      error_buffer = self._config.min_replay_size * samples_per_insert_tolerance\n      limiter = reverb.rate_limiters.SampleToInsertRatio(\n          min_size_to_sample=self._config.min_replay_size,\n          samples_per_insert=self._config.samples_per_insert,\n          error_buffer=error_buffer)\n    else:\n      limiter = reverb.rate_limiters.MinSize(self._config.min_replay_size)\n    return [\n        reverb.Table(\n            name=self._config.replay_table_name,\n            sampler=reverb.selectors.Prioritized(\n                self._config.priority_exponent),\n            remover=reverb.selectors.Fifo(),\n            max_size=self._config.max_replay_size,\n            rate_limiter=limiter,\n            signature=sw.infer_signature(\n                configs=_make_adder_config(step_spec, self._sequence_length,\n                                           self._config.sequence_period),\n                step_spec=step_spec))\n    ]",
  "def make_dataset_iterator(\n      self,\n      replay_client: reverb.Client) -> Iterator[r2d2_learning.R2D2ReplaySample]:\n    \"\"\"Create a dataset iterator to use for learning/updating the agent.\"\"\"\n    batch_size_per_learner = self._config.batch_size // jax.process_count()\n    dataset = datasets.make_reverb_dataset(\n        table=self._config.replay_table_name,\n        server_address=replay_client.server_address,\n        batch_size=self._batch_size_per_device,\n        num_parallel_calls=None,\n        max_in_flight_samples_per_worker=2 * batch_size_per_learner,\n        postprocess=_zero_pad(self._sequence_length),\n    )\n\n    return utils.multi_device_put(\n        dataset.as_numpy_iterator(),\n        devices=jax.local_devices(),\n        split_fn=utils.keep_key_on_host)",
  "def make_adder(\n      self, replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[r2d2_actor.R2D2Policy]) -> Optional[adders.Adder]:\n    \"\"\"Create an adder which records data generated by the actor/environment.\"\"\"\n    if environment_spec is None or policy is None:\n      raise ValueError('`environment_spec` and `policy` cannot be None.')\n    dummy_actor_state = policy.init(jax.random.PRNGKey(0))\n    extras_spec = policy.get_extras(dummy_actor_state)\n    step_spec = structured.create_step_spec(\n        environment_spec=environment_spec, extras_spec=extras_spec)\n    return structured.StructuredAdder(\n        client=replay_client,\n        max_in_flight_items=5,\n        configs=_make_adder_config(step_spec, self._sequence_length,\n                                   self._config.sequence_period),\n        step_spec=step_spec)",
  "def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: r2d2_actor.R2D2Policy,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> acme.Actor:\n    del environment_spec\n    # Create variable client.\n    variable_client = variable_utils.VariableClient(\n        variable_source,\n        key='actor_variables',\n        update_period=self._config.variable_update_period)\n\n    return actors.GenericActor(\n        policy, random_key, variable_client, adder, backend='cpu')",
  "def make_policy(self,\n                  networks: r2d2_networks.R2D2Networks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> r2d2_actor.R2D2Policy:\n    if evaluation:\n      return r2d2_actor.get_actor_core(\n          networks,\n          num_epsilons=None,\n          evaluation_epsilon=self._config.evaluation_epsilon)\n    else:\n      return r2d2_actor.get_actor_core(networks, self._config.num_epsilons)",
  "def _ensure_shape(x):\n      shape = tf.TensorShape([sequence_length]).concatenate(x.shape[1:])\n      return tf.ensure_shape(x, shape)",
  "class R2D2ActorState(Generic[actor_core_lib.RecurrentState]):\n  rng: networks_lib.PRNGKey\n  epsilon: jnp.ndarray\n  recurrent_state: actor_core_lib.RecurrentState\n  prev_recurrent_state: actor_core_lib.RecurrentState",
  "def get_actor_core(\n    networks: r2d2_networks.R2D2Networks,\n    num_epsilons: Optional[int],\n    evaluation_epsilon: Optional[float] = None,\n) -> R2D2Policy:\n  \"\"\"Returns ActorCore for R2D2.\"\"\"\n\n  if (not num_epsilons and evaluation_epsilon is None) or (num_epsilons and\n                                                           evaluation_epsilon):\n    raise ValueError(\n        'Exactly one of `num_epsilons` or `evaluation_epsilon` must be '\n        f'specified. Received num_epsilon={num_epsilons} and '\n        f'evaluation_epsilon={evaluation_epsilon}.')\n\n  def select_action(params: networks_lib.Params,\n                    observation: networks_lib.Observation,\n                    state: R2D2ActorState[actor_core_lib.RecurrentState]):\n    rng, policy_rng = jax.random.split(state.rng)\n\n    q_values, recurrent_state = networks.apply(params, policy_rng, observation,\n                                               state.recurrent_state)\n    action = rlax.epsilon_greedy(state.epsilon).sample(policy_rng, q_values)\n\n    return action, R2D2ActorState(\n        rng=rng,\n        epsilon=state.epsilon,\n        recurrent_state=recurrent_state,\n        prev_recurrent_state=state.recurrent_state)\n\n  def init(\n      rng: networks_lib.PRNGKey\n  ) -> R2D2ActorState[actor_core_lib.RecurrentState]:\n    rng, epsilon_rng, state_rng = jax.random.split(rng, 3)\n    if num_epsilons:\n      epsilon = jax.random.choice(epsilon_rng,\n                                  np.logspace(1, 3, num_epsilons, base=0.1))\n    else:\n      epsilon = evaluation_epsilon\n    initial_core_state = networks.init_recurrent_state(state_rng, None)\n    return R2D2ActorState(\n        rng=rng,\n        epsilon=epsilon,\n        recurrent_state=initial_core_state,\n        prev_recurrent_state=initial_core_state)\n\n  def get_extras(\n      state: R2D2ActorState[actor_core_lib.RecurrentState]) -> R2D2Extras:\n    return {'core_state': state.prev_recurrent_state}\n\n  return actor_core_lib.ActorCore(init=init, select_action=select_action,\n                                  get_extras=get_extras)",
  "def make_behavior_policy(networks: r2d2_networks.R2D2Networks,\n                         config: r2d2_config.R2D2Config,\n                         evaluation: bool = False) -> EpsilonRecurrentPolicy:\n  \"\"\"Selects action according to the policy.\"\"\"\n\n  def behavior_policy(params: networks_lib.Params, key: networks_lib.PRNGKey,\n                      observation: types.NestedArray,\n                      core_state: types.NestedArray, epsilon: float):\n    q_values, core_state = networks.apply(params, key, observation, core_state)\n    epsilon = config.evaluation_epsilon if evaluation else epsilon\n    return rlax.epsilon_greedy(epsilon).sample(key, q_values), core_state\n\n  return behavior_policy",
  "def select_action(params: networks_lib.Params,\n                    observation: networks_lib.Observation,\n                    state: R2D2ActorState[actor_core_lib.RecurrentState]):\n    rng, policy_rng = jax.random.split(state.rng)\n\n    q_values, recurrent_state = networks.apply(params, policy_rng, observation,\n                                               state.recurrent_state)\n    action = rlax.epsilon_greedy(state.epsilon).sample(policy_rng, q_values)\n\n    return action, R2D2ActorState(\n        rng=rng,\n        epsilon=state.epsilon,\n        recurrent_state=recurrent_state,\n        prev_recurrent_state=state.recurrent_state)",
  "def init(\n      rng: networks_lib.PRNGKey\n  ) -> R2D2ActorState[actor_core_lib.RecurrentState]:\n    rng, epsilon_rng, state_rng = jax.random.split(rng, 3)\n    if num_epsilons:\n      epsilon = jax.random.choice(epsilon_rng,\n                                  np.logspace(1, 3, num_epsilons, base=0.1))\n    else:\n      epsilon = evaluation_epsilon\n    initial_core_state = networks.init_recurrent_state(state_rng, None)\n    return R2D2ActorState(\n        rng=rng,\n        epsilon=epsilon,\n        recurrent_state=initial_core_state,\n        prev_recurrent_state=initial_core_state)",
  "def get_extras(\n      state: R2D2ActorState[actor_core_lib.RecurrentState]) -> R2D2Extras:\n    return {'core_state': state.prev_recurrent_state}",
  "def behavior_policy(params: networks_lib.Params, key: networks_lib.PRNGKey,\n                      observation: types.NestedArray,\n                      core_state: types.NestedArray, epsilon: float):\n    q_values, core_state = networks.apply(params, key, observation, core_state)\n    epsilon = config.evaluation_epsilon if evaluation else epsilon\n    return rlax.epsilon_greedy(epsilon).sample(key, q_values), core_state",
  "class Batch(NamedTuple):\n  \"\"\"A batch of data; all shapes are expected to be [B, ...].\"\"\"\n  observations: types.NestedArray\n  actions: jnp.ndarray\n  advantages: jnp.ndarray\n\n  # Target value estimate used to bootstrap the value function.\n  target_values: jnp.ndarray\n\n  # Value estimate and action log-prob at behavior time.\n  behavior_values: jnp.ndarray\n  behavior_log_probs: jnp.ndarray",
  "class TrainingState(NamedTuple):\n  \"\"\"Training state for the PPO learner.\"\"\"\n  params: PPOParams\n  opt_state: optax.OptState\n  random_key: networks_lib.PRNGKey\n\n  # Optional counter used for exponential moving average zero debiasing\n  # Using float32 as it covers a larger range than int32. If using int64 we\n  # would need to do jax_enable_x64.\n  ema_counter: Optional[jnp.float32] = None\n\n  # Optional parameter for maintaining a running estimate of the scale of\n  # advantage estimates\n  biased_advantage_scale: Optional[networks_lib.Params] = None\n  advantage_scale: Optional[networks_lib.Params] = None\n\n  # Optional parameter for maintaining a running estimate of the mean and\n  # standard deviation of value estimates\n  biased_value_first_moment: Optional[networks_lib.Params] = None\n  biased_value_second_moment: Optional[networks_lib.Params] = None\n  value_mean: Optional[networks_lib.Params] = None\n  value_std: Optional[networks_lib.Params] = None\n\n  # Optional parameters for observation normalization\n  obs_normalization_params: Optional[normalization.NormalizationParams] = None",
  "class PPOLearner(acme.Learner):\n  \"\"\"Learner for PPO.\"\"\"\n\n  def __init__(\n      self,\n      ppo_networks: networks.PPONetworks,\n      iterator: Iterator[reverb.ReplaySample],\n      optimizer: optax.GradientTransformation,\n      random_key: networks_lib.PRNGKey,\n      ppo_clipping_epsilon: float = 0.2,\n      normalize_advantage: bool = True,\n      normalize_value: bool = False,\n      normalization_ema_tau: float = 0.995,\n      clip_value: bool = False,\n      value_clipping_epsilon: float = 0.2,\n      max_abs_reward: Optional[float] = None,\n      gae_lambda: float = 0.95,\n      discount: float = 0.99,\n      entropy_cost: float = 0.,\n      value_cost: float = 1.,\n      num_epochs: int = 4,\n      num_minibatches: int = 1,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      log_global_norm_metrics: bool = False,\n      metrics_logging_period: int = 100,\n      pmap_axis_name: str = 'devices',\n      obs_normalization_fns: Optional[normalization.NormalizationFns] = None,\n  ):\n    self.local_learner_devices = jax.local_devices()\n    self.num_local_learner_devices = jax.local_device_count()\n    self.learner_devices = jax.devices()\n    self.num_epochs = num_epochs\n    self.num_minibatches = num_minibatches\n    self.metrics_logging_period = metrics_logging_period\n    self._num_full_update_steps = 0\n    self._iterator = iterator\n\n    normalize_obs = obs_normalization_fns is not None\n    if normalize_obs:\n      assert obs_normalization_fns is not None\n\n    # Set up logging/counting.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger('learner')\n\n    def ppo_loss(\n        params: networks_lib.Params,\n        observations: networks_lib.Observation,\n        actions: networks_lib.Action,\n        advantages: jnp.ndarray,\n        target_values: networks_lib.Value,\n        behavior_values: networks_lib.Value,\n        behavior_log_probs: networks_lib.LogProb,\n        value_mean: jnp.ndarray,\n        value_std: jnp.ndarray,\n        key: networks_lib.PRNGKey,\n    ) -> Tuple[jnp.ndarray, Dict[str, jnp.ndarray]]:\n      \"\"\"PPO loss for the policy and the critic.\"\"\"\n      distribution_params, values = ppo_networks.network.apply(\n          params, observations)\n      if normalize_value:\n        # values = values * jnp.fmax(value_std, 1e-6) + value_mean\n        target_values = (target_values - value_mean) / jnp.fmax(value_std, 1e-6)\n      policy_log_probs = ppo_networks.log_prob(distribution_params, actions)\n      key, sub_key = jax.random.split(key)\n      policy_entropies = ppo_networks.entropy(distribution_params, sub_key)\n\n      # Compute the policy losses\n      rhos = jnp.exp(policy_log_probs - behavior_log_probs)\n      clipped_ppo_policy_loss = rlax.clipped_surrogate_pg_loss(\n          rhos, advantages, ppo_clipping_epsilon)\n      policy_entropy_loss = -jnp.mean(policy_entropies)\n      total_policy_loss = (\n          clipped_ppo_policy_loss + entropy_cost * policy_entropy_loss)\n\n      # Compute the critic losses\n      unclipped_value_loss = (values - target_values)**2\n\n      if clip_value:\n        # Clip values to reduce variablility during critic training.\n        clipped_values = behavior_values + jnp.clip(values - behavior_values,\n                                                    -value_clipping_epsilon,\n                                                    value_clipping_epsilon)\n        clipped_value_error = target_values - clipped_values\n        clipped_value_loss = clipped_value_error ** 2\n        value_loss = jnp.mean(jnp.fmax(unclipped_value_loss,\n                                       clipped_value_loss))\n      else:\n        # For Mujoco envs clipping hurts a lot. Evidenced by Figure 43 in\n        # https://arxiv.org/pdf/2006.05990.pdf\n        value_loss = jnp.mean(unclipped_value_loss)\n\n      total_ppo_loss = total_policy_loss + value_cost * value_loss\n      return total_ppo_loss, {  # pytype: disable=bad-return-type  # numpy-scalars\n          'loss_total': total_ppo_loss,\n          'loss_policy_total': total_policy_loss,\n          'loss_policy_pg': clipped_ppo_policy_loss,\n          'loss_policy_entropy': policy_entropy_loss,\n          'loss_critic': value_loss,\n      }\n\n    ppo_loss_grad = jax.grad(ppo_loss, has_aux=True)\n\n    def sgd_step(state: TrainingState, minibatch: Batch):\n      observations = minibatch.observations\n      actions = minibatch.actions\n      advantages = minibatch.advantages\n      target_values = minibatch.target_values\n      behavior_values = minibatch.behavior_values\n      behavior_log_probs = minibatch.behavior_log_probs\n      key, sub_key = jax.random.split(state.random_key)\n\n      loss_grad, metrics = ppo_loss_grad(\n          state.params.model_params,\n          observations,\n          actions,\n          advantages,\n          target_values,\n          behavior_values,\n          behavior_log_probs,\n          state.value_mean,\n          state.value_std,\n          sub_key,\n      )\n\n      # Apply updates\n      loss_grad = jax.lax.pmean(loss_grad, axis_name=pmap_axis_name)\n      updates, opt_state = optimizer.update(loss_grad, state.opt_state)\n      model_params = optax.apply_updates(state.params.model_params, updates)\n      params = PPOParams(\n          model_params=model_params,\n          num_sgd_steps=state.params.num_sgd_steps + 1)\n\n      if log_global_norm_metrics:\n        metrics['norm_grad'] = optax.global_norm(loss_grad)\n        metrics['norm_updates'] = optax.global_norm(updates)\n\n      state = state._replace(params=params, opt_state=opt_state, random_key=key)\n\n      return state, metrics\n\n    def epoch_update(\n        carry: Tuple[TrainingState, Batch],\n        unused_t: Tuple[()],\n    ):\n      state, carry_batch = carry\n\n      # Shuffling into minibatches\n      batch_size = carry_batch.advantages.shape[0]\n      key, sub_key = jax.random.split(state.random_key)\n      # TODO(kamyar) For effiency could use same permutation for all epochs\n      permuted_batch = jax.tree_util.tree_map(\n          lambda x: jax.random.permutation(  # pylint: disable=g-long-lambda\n              sub_key,\n              x,\n              axis=0,\n              independent=False),\n          carry_batch)\n      state = state._replace(random_key=key)\n      minibatches = jax.tree_util.tree_map(\n          lambda x: jnp.reshape(  # pylint: disable=g-long-lambda\n              x,\n              [  # pylint: disable=g-long-lambda\n                  num_minibatches, batch_size // num_minibatches\n              ] + list(x.shape[1:])),\n          permuted_batch)\n\n      # Scan over the minibatches\n      state, metrics = jax.lax.scan(\n          sgd_step, state, minibatches, length=num_minibatches)\n      metrics = jax.tree_util.tree_map(jnp.mean, metrics)\n\n      return (state, carry_batch), metrics\n\n    vmapped_network_apply = jax.vmap(\n        ppo_networks.network.apply, in_axes=(None, 0), out_axes=0)\n\n    def single_device_update(\n        state: TrainingState,\n        trajectories: types.NestedArray,\n    ):\n      params_num_sgd_steps_before_update = state.params.num_sgd_steps\n\n      # Update the EMA counter and obtain the zero debiasing multiplier\n      if normalize_advantage or normalize_value:\n        ema_counter = state.ema_counter + 1\n        state = state._replace(ema_counter=ema_counter)\n        zero_debias = 1. / (1. - jnp.power(normalization_ema_tau, ema_counter))\n\n      # Extract the data.\n      data = trajectories.data\n      observations, actions, rewards, termination, extra = (data.observation,\n                                                            data.action,\n                                                            data.reward,\n                                                            data.discount,\n                                                            data.extras)\n\n      if normalize_obs:\n        obs_norm_params = obs_normalization_fns.update(\n            state.obs_normalization_params, observations, pmap_axis_name)\n        state = state._replace(obs_normalization_params=obs_norm_params)\n        observations = obs_normalization_fns.normalize(\n            observations, state.obs_normalization_params)\n\n      if max_abs_reward is not None:\n        # Apply reward clipping.\n        rewards = jnp.clip(rewards, -1. * max_abs_reward, max_abs_reward)\n      discounts = termination * discount\n      behavior_log_probs = extra['log_prob']\n      _, behavior_values = vmapped_network_apply(state.params.model_params,\n                                                 observations)\n\n      if normalize_value:\n        batch_value_first_moment = jnp.mean(behavior_values)\n        batch_value_second_moment = jnp.mean(behavior_values**2)\n        batch_value_first_moment, batch_value_second_moment = jax.lax.pmean(\n            (batch_value_first_moment, batch_value_second_moment),\n            axis_name=pmap_axis_name)\n\n        biased_value_first_moment = (\n            normalization_ema_tau * state.biased_value_first_moment +\n            (1. - normalization_ema_tau) * batch_value_first_moment)\n        biased_value_second_moment = (\n            normalization_ema_tau * state.biased_value_second_moment +\n            (1. - normalization_ema_tau) * batch_value_second_moment)\n\n        value_mean = biased_value_first_moment * zero_debias\n        value_second_moment = biased_value_second_moment * zero_debias\n        value_std = jnp.sqrt(jax.nn.relu(value_second_moment - value_mean**2))\n\n        state = state._replace(\n            biased_value_first_moment=biased_value_first_moment,\n            biased_value_second_moment=biased_value_second_moment,\n            value_mean=value_mean,\n            value_std=value_std,\n        )\n\n        behavior_values = behavior_values * jnp.fmax(state.value_std,\n                                                     1e-6) + state.value_mean\n\n      behavior_values = jax.lax.stop_gradient(behavior_values)\n\n      # Compute GAE using rlax\n      vmapped_rlax_truncated_generalized_advantage_estimation = jax.vmap(\n          rlax.truncated_generalized_advantage_estimation,\n          in_axes=(0, 0, None, 0))\n      advantages = vmapped_rlax_truncated_generalized_advantage_estimation(\n          rewards[:, :-1], discounts[:, :-1], gae_lambda, behavior_values)\n      advantages = jax.lax.stop_gradient(advantages)\n      target_values = behavior_values[:, :-1] + advantages\n      target_values = jax.lax.stop_gradient(target_values)\n\n      # Exclude the last step - it was only used for bootstrapping.\n      # The shape is [num_sequences, num_steps, ..]\n      (observations, actions, behavior_log_probs, behavior_values) = (\n          jax.tree_util.tree_map(\n              lambda x: x[:, :-1],\n              (observations, actions, behavior_log_probs, behavior_values),\n          )\n      )\n\n      # Shuffle the data and break into minibatches\n      batch_size = advantages.shape[0] * advantages.shape[1]\n      batch = Batch(\n          observations=observations,\n          actions=actions,\n          advantages=advantages,\n          target_values=target_values,\n          behavior_values=behavior_values,\n          behavior_log_probs=behavior_log_probs)\n      batch = jax.tree_util.tree_map(\n          lambda x: jnp.reshape(x, [batch_size] + list(x.shape[2:])), batch)\n\n      if normalize_advantage:\n        batch_advantage_scale = jnp.mean(jnp.abs(batch.advantages))\n        batch_advantage_scale = jax.lax.pmean(batch_advantage_scale,\n                                              pmap_axis_name)\n\n        # update the running statistics\n        biased_advantage_scale = (\n            normalization_ema_tau * state.biased_advantage_scale +\n            (1. - normalization_ema_tau) * batch_advantage_scale)\n        advantage_scale = biased_advantage_scale * zero_debias\n        state = state._replace(\n            biased_advantage_scale=biased_advantage_scale,\n            advantage_scale=advantage_scale)\n\n        # scale the advantages\n        scaled_advantages = batch.advantages / jnp.fmax(state.advantage_scale,\n                                                        1e-6)\n        batch = batch._replace(advantages=scaled_advantages)\n\n      # Scan desired number of epoch updates\n      (state, _), metrics = jax.lax.scan(\n          epoch_update, (state, batch), (), length=num_epochs)\n      metrics = jax.tree_util.tree_map(jnp.mean, metrics)\n\n      if normalize_advantage:\n        metrics['advantage_scale'] = state.advantage_scale\n\n      if normalize_value:\n        metrics['value_mean'] = value_mean\n        metrics['value_std'] = value_std\n\n      delta_params_sgd_steps = (\n          data.extras['params_num_sgd_steps'][:, 0] -\n          params_num_sgd_steps_before_update)\n      metrics['delta_params_sgd_steps_min'] = jnp.min(delta_params_sgd_steps)\n      metrics['delta_params_sgd_steps_max'] = jnp.max(delta_params_sgd_steps)\n      metrics['delta_params_sgd_steps_mean'] = jnp.mean(delta_params_sgd_steps)\n      metrics['delta_params_sgd_steps_std'] = jnp.std(delta_params_sgd_steps)\n\n      return state, metrics\n\n    pmapped_update_step = jax.pmap(\n        single_device_update,\n        axis_name=pmap_axis_name,\n        devices=self.learner_devices)\n\n    def full_update_step(\n        state: TrainingState,\n        trajectories: types.NestedArray,\n    ):\n      state, metrics = pmapped_update_step(state, trajectories)\n      return state, metrics\n\n    self._full_update_step = full_update_step\n\n    def make_initial_state(key: networks_lib.PRNGKey) -> TrainingState:\n      \"\"\"Initialises the training state (parameters and optimiser state).\"\"\"\n      all_keys = jax.random.split(key, num=self.num_local_learner_devices + 1)\n      key_init, key_state = all_keys[0], all_keys[1:]\n      key_state = [key_state[i] for i in range(self.num_local_learner_devices)]\n      key_state = jax.device_put_sharded(key_state, self.local_learner_devices)\n\n      initial_params = ppo_networks.network.init(key_init)\n      initial_opt_state = optimizer.init(initial_params)\n      # Using float32 as it covers a larger range than int32. If using int64 we\n      # would need to do jax_enable_x64.\n      params_num_sgd_steps = jnp.zeros(shape=(), dtype=jnp.float32)\n\n      initial_params = jax.device_put_replicated(initial_params,\n                                                 self.local_learner_devices)\n      initial_opt_state = jax.device_put_replicated(initial_opt_state,\n                                                    self.local_learner_devices)\n      params_num_sgd_steps = jax.device_put_replicated(\n          params_num_sgd_steps, self.local_learner_devices)\n\n      ema_counter = jnp.float32(0)\n      ema_counter = jax.device_put_replicated(ema_counter,\n                                              self.local_learner_devices)\n\n      init_state = TrainingState(\n          params=PPOParams(\n              model_params=initial_params, num_sgd_steps=params_num_sgd_steps),\n          opt_state=initial_opt_state,\n          random_key=key_state,\n          ema_counter=ema_counter,\n      )\n\n      if normalize_advantage:\n        biased_advantage_scale = jax.device_put_replicated(\n            jnp.zeros([]), self.local_learner_devices)\n        advantage_scale = jax.device_put_replicated(\n            jnp.zeros([]), self.local_learner_devices)\n\n        init_state = init_state._replace(\n            biased_advantage_scale=biased_advantage_scale,\n            advantage_scale=advantage_scale)\n\n      if normalize_value:\n        biased_value_first_moment = jax.device_put_replicated(\n            jnp.zeros([]), self.local_learner_devices)\n        value_mean = biased_value_first_moment\n\n        biased_value_second_moment = jax.device_put_replicated(\n            jnp.zeros([]), self.local_learner_devices)\n        value_second_moment = biased_value_second_moment\n        value_std = jnp.sqrt(jax.nn.relu(value_second_moment - value_mean**2))\n\n        init_state = init_state._replace(\n            biased_value_first_moment=biased_value_first_moment,\n            biased_value_second_moment=biased_value_second_moment,\n            value_mean=value_mean,\n            value_std=value_std)\n\n      if normalize_obs:\n        obs_norm_params = obs_normalization_fns.init()  # pytype: disable=attribute-error\n        obs_norm_params = jax.device_put_replicated(obs_norm_params,\n                                                    self.local_learner_devices)\n        init_state = init_state._replace(\n            obs_normalization_params=obs_norm_params)\n\n      return init_state\n\n    # Initialise training state (parameters and optimizer state).\n    self._state = make_initial_state(random_key)\n    self._cached_state = get_from_first_device(self._state, as_numpy=True)\n\n  def step(self):\n    \"\"\"Does a learner step and logs the results.\n\n    One learner step consists of (possibly multiple) epochs of PPO updates on\n    a batch of NxT steps collected by the actors.\n    \"\"\"\n    sample = next(self._iterator)\n    self._state, results = self._full_update_step(self._state, sample)\n    self._cached_state = get_from_first_device(self._state, as_numpy=True)\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=self.num_epochs *\n                                     self.num_minibatches)\n\n    # Snapshot and attempt to write logs.\n    if self._num_full_update_steps % self.metrics_logging_period == 0:\n      results = jax.tree_util.tree_map(jnp.mean, results)\n      self._logger.write({**results, **counts})\n\n    self._num_full_update_steps += 1\n\n  def get_variables(self, names: List[str]) -> List[networks_lib.Params]:\n    variables = self._cached_state\n    return [getattr(variables, name) for name in names]\n\n  def save(self) -> TrainingState:\n    return self._cached_state\n\n  def restore(self, state: TrainingState):\n    # TODO(kamyar) Should the random_key come from self._state instead?\n    random_key = state.random_key\n    random_key = jax.random.split(\n        random_key, num=self.num_local_learner_devices)\n    random_key = jax.device_put_sharded(\n        [random_key[i] for i in range(self.num_local_learner_devices)],\n        self.local_learner_devices)\n\n    state = jax.device_put_replicated(state, self.local_learner_devices)\n    state = state._replace(random_key=random_key)\n    self._state = state\n    self._cached_state = get_from_first_device(self._state, as_numpy=True)",
  "def __init__(\n      self,\n      ppo_networks: networks.PPONetworks,\n      iterator: Iterator[reverb.ReplaySample],\n      optimizer: optax.GradientTransformation,\n      random_key: networks_lib.PRNGKey,\n      ppo_clipping_epsilon: float = 0.2,\n      normalize_advantage: bool = True,\n      normalize_value: bool = False,\n      normalization_ema_tau: float = 0.995,\n      clip_value: bool = False,\n      value_clipping_epsilon: float = 0.2,\n      max_abs_reward: Optional[float] = None,\n      gae_lambda: float = 0.95,\n      discount: float = 0.99,\n      entropy_cost: float = 0.,\n      value_cost: float = 1.,\n      num_epochs: int = 4,\n      num_minibatches: int = 1,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      log_global_norm_metrics: bool = False,\n      metrics_logging_period: int = 100,\n      pmap_axis_name: str = 'devices',\n      obs_normalization_fns: Optional[normalization.NormalizationFns] = None,\n  ):\n    self.local_learner_devices = jax.local_devices()\n    self.num_local_learner_devices = jax.local_device_count()\n    self.learner_devices = jax.devices()\n    self.num_epochs = num_epochs\n    self.num_minibatches = num_minibatches\n    self.metrics_logging_period = metrics_logging_period\n    self._num_full_update_steps = 0\n    self._iterator = iterator\n\n    normalize_obs = obs_normalization_fns is not None\n    if normalize_obs:\n      assert obs_normalization_fns is not None\n\n    # Set up logging/counting.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger('learner')\n\n    def ppo_loss(\n        params: networks_lib.Params,\n        observations: networks_lib.Observation,\n        actions: networks_lib.Action,\n        advantages: jnp.ndarray,\n        target_values: networks_lib.Value,\n        behavior_values: networks_lib.Value,\n        behavior_log_probs: networks_lib.LogProb,\n        value_mean: jnp.ndarray,\n        value_std: jnp.ndarray,\n        key: networks_lib.PRNGKey,\n    ) -> Tuple[jnp.ndarray, Dict[str, jnp.ndarray]]:\n      \"\"\"PPO loss for the policy and the critic.\"\"\"\n      distribution_params, values = ppo_networks.network.apply(\n          params, observations)\n      if normalize_value:\n        # values = values * jnp.fmax(value_std, 1e-6) + value_mean\n        target_values = (target_values - value_mean) / jnp.fmax(value_std, 1e-6)\n      policy_log_probs = ppo_networks.log_prob(distribution_params, actions)\n      key, sub_key = jax.random.split(key)\n      policy_entropies = ppo_networks.entropy(distribution_params, sub_key)\n\n      # Compute the policy losses\n      rhos = jnp.exp(policy_log_probs - behavior_log_probs)\n      clipped_ppo_policy_loss = rlax.clipped_surrogate_pg_loss(\n          rhos, advantages, ppo_clipping_epsilon)\n      policy_entropy_loss = -jnp.mean(policy_entropies)\n      total_policy_loss = (\n          clipped_ppo_policy_loss + entropy_cost * policy_entropy_loss)\n\n      # Compute the critic losses\n      unclipped_value_loss = (values - target_values)**2\n\n      if clip_value:\n        # Clip values to reduce variablility during critic training.\n        clipped_values = behavior_values + jnp.clip(values - behavior_values,\n                                                    -value_clipping_epsilon,\n                                                    value_clipping_epsilon)\n        clipped_value_error = target_values - clipped_values\n        clipped_value_loss = clipped_value_error ** 2\n        value_loss = jnp.mean(jnp.fmax(unclipped_value_loss,\n                                       clipped_value_loss))\n      else:\n        # For Mujoco envs clipping hurts a lot. Evidenced by Figure 43 in\n        # https://arxiv.org/pdf/2006.05990.pdf\n        value_loss = jnp.mean(unclipped_value_loss)\n\n      total_ppo_loss = total_policy_loss + value_cost * value_loss\n      return total_ppo_loss, {  # pytype: disable=bad-return-type  # numpy-scalars\n          'loss_total': total_ppo_loss,\n          'loss_policy_total': total_policy_loss,\n          'loss_policy_pg': clipped_ppo_policy_loss,\n          'loss_policy_entropy': policy_entropy_loss,\n          'loss_critic': value_loss,\n      }\n\n    ppo_loss_grad = jax.grad(ppo_loss, has_aux=True)\n\n    def sgd_step(state: TrainingState, minibatch: Batch):\n      observations = minibatch.observations\n      actions = minibatch.actions\n      advantages = minibatch.advantages\n      target_values = minibatch.target_values\n      behavior_values = minibatch.behavior_values\n      behavior_log_probs = minibatch.behavior_log_probs\n      key, sub_key = jax.random.split(state.random_key)\n\n      loss_grad, metrics = ppo_loss_grad(\n          state.params.model_params,\n          observations,\n          actions,\n          advantages,\n          target_values,\n          behavior_values,\n          behavior_log_probs,\n          state.value_mean,\n          state.value_std,\n          sub_key,\n      )\n\n      # Apply updates\n      loss_grad = jax.lax.pmean(loss_grad, axis_name=pmap_axis_name)\n      updates, opt_state = optimizer.update(loss_grad, state.opt_state)\n      model_params = optax.apply_updates(state.params.model_params, updates)\n      params = PPOParams(\n          model_params=model_params,\n          num_sgd_steps=state.params.num_sgd_steps + 1)\n\n      if log_global_norm_metrics:\n        metrics['norm_grad'] = optax.global_norm(loss_grad)\n        metrics['norm_updates'] = optax.global_norm(updates)\n\n      state = state._replace(params=params, opt_state=opt_state, random_key=key)\n\n      return state, metrics\n\n    def epoch_update(\n        carry: Tuple[TrainingState, Batch],\n        unused_t: Tuple[()],\n    ):\n      state, carry_batch = carry\n\n      # Shuffling into minibatches\n      batch_size = carry_batch.advantages.shape[0]\n      key, sub_key = jax.random.split(state.random_key)\n      # TODO(kamyar) For effiency could use same permutation for all epochs\n      permuted_batch = jax.tree_util.tree_map(\n          lambda x: jax.random.permutation(  # pylint: disable=g-long-lambda\n              sub_key,\n              x,\n              axis=0,\n              independent=False),\n          carry_batch)\n      state = state._replace(random_key=key)\n      minibatches = jax.tree_util.tree_map(\n          lambda x: jnp.reshape(  # pylint: disable=g-long-lambda\n              x,\n              [  # pylint: disable=g-long-lambda\n                  num_minibatches, batch_size // num_minibatches\n              ] + list(x.shape[1:])),\n          permuted_batch)\n\n      # Scan over the minibatches\n      state, metrics = jax.lax.scan(\n          sgd_step, state, minibatches, length=num_minibatches)\n      metrics = jax.tree_util.tree_map(jnp.mean, metrics)\n\n      return (state, carry_batch), metrics\n\n    vmapped_network_apply = jax.vmap(\n        ppo_networks.network.apply, in_axes=(None, 0), out_axes=0)\n\n    def single_device_update(\n        state: TrainingState,\n        trajectories: types.NestedArray,\n    ):\n      params_num_sgd_steps_before_update = state.params.num_sgd_steps\n\n      # Update the EMA counter and obtain the zero debiasing multiplier\n      if normalize_advantage or normalize_value:\n        ema_counter = state.ema_counter + 1\n        state = state._replace(ema_counter=ema_counter)\n        zero_debias = 1. / (1. - jnp.power(normalization_ema_tau, ema_counter))\n\n      # Extract the data.\n      data = trajectories.data\n      observations, actions, rewards, termination, extra = (data.observation,\n                                                            data.action,\n                                                            data.reward,\n                                                            data.discount,\n                                                            data.extras)\n\n      if normalize_obs:\n        obs_norm_params = obs_normalization_fns.update(\n            state.obs_normalization_params, observations, pmap_axis_name)\n        state = state._replace(obs_normalization_params=obs_norm_params)\n        observations = obs_normalization_fns.normalize(\n            observations, state.obs_normalization_params)\n\n      if max_abs_reward is not None:\n        # Apply reward clipping.\n        rewards = jnp.clip(rewards, -1. * max_abs_reward, max_abs_reward)\n      discounts = termination * discount\n      behavior_log_probs = extra['log_prob']\n      _, behavior_values = vmapped_network_apply(state.params.model_params,\n                                                 observations)\n\n      if normalize_value:\n        batch_value_first_moment = jnp.mean(behavior_values)\n        batch_value_second_moment = jnp.mean(behavior_values**2)\n        batch_value_first_moment, batch_value_second_moment = jax.lax.pmean(\n            (batch_value_first_moment, batch_value_second_moment),\n            axis_name=pmap_axis_name)\n\n        biased_value_first_moment = (\n            normalization_ema_tau * state.biased_value_first_moment +\n            (1. - normalization_ema_tau) * batch_value_first_moment)\n        biased_value_second_moment = (\n            normalization_ema_tau * state.biased_value_second_moment +\n            (1. - normalization_ema_tau) * batch_value_second_moment)\n\n        value_mean = biased_value_first_moment * zero_debias\n        value_second_moment = biased_value_second_moment * zero_debias\n        value_std = jnp.sqrt(jax.nn.relu(value_second_moment - value_mean**2))\n\n        state = state._replace(\n            biased_value_first_moment=biased_value_first_moment,\n            biased_value_second_moment=biased_value_second_moment,\n            value_mean=value_mean,\n            value_std=value_std,\n        )\n\n        behavior_values = behavior_values * jnp.fmax(state.value_std,\n                                                     1e-6) + state.value_mean\n\n      behavior_values = jax.lax.stop_gradient(behavior_values)\n\n      # Compute GAE using rlax\n      vmapped_rlax_truncated_generalized_advantage_estimation = jax.vmap(\n          rlax.truncated_generalized_advantage_estimation,\n          in_axes=(0, 0, None, 0))\n      advantages = vmapped_rlax_truncated_generalized_advantage_estimation(\n          rewards[:, :-1], discounts[:, :-1], gae_lambda, behavior_values)\n      advantages = jax.lax.stop_gradient(advantages)\n      target_values = behavior_values[:, :-1] + advantages\n      target_values = jax.lax.stop_gradient(target_values)\n\n      # Exclude the last step - it was only used for bootstrapping.\n      # The shape is [num_sequences, num_steps, ..]\n      (observations, actions, behavior_log_probs, behavior_values) = (\n          jax.tree_util.tree_map(\n              lambda x: x[:, :-1],\n              (observations, actions, behavior_log_probs, behavior_values),\n          )\n      )\n\n      # Shuffle the data and break into minibatches\n      batch_size = advantages.shape[0] * advantages.shape[1]\n      batch = Batch(\n          observations=observations,\n          actions=actions,\n          advantages=advantages,\n          target_values=target_values,\n          behavior_values=behavior_values,\n          behavior_log_probs=behavior_log_probs)\n      batch = jax.tree_util.tree_map(\n          lambda x: jnp.reshape(x, [batch_size] + list(x.shape[2:])), batch)\n\n      if normalize_advantage:\n        batch_advantage_scale = jnp.mean(jnp.abs(batch.advantages))\n        batch_advantage_scale = jax.lax.pmean(batch_advantage_scale,\n                                              pmap_axis_name)\n\n        # update the running statistics\n        biased_advantage_scale = (\n            normalization_ema_tau * state.biased_advantage_scale +\n            (1. - normalization_ema_tau) * batch_advantage_scale)\n        advantage_scale = biased_advantage_scale * zero_debias\n        state = state._replace(\n            biased_advantage_scale=biased_advantage_scale,\n            advantage_scale=advantage_scale)\n\n        # scale the advantages\n        scaled_advantages = batch.advantages / jnp.fmax(state.advantage_scale,\n                                                        1e-6)\n        batch = batch._replace(advantages=scaled_advantages)\n\n      # Scan desired number of epoch updates\n      (state, _), metrics = jax.lax.scan(\n          epoch_update, (state, batch), (), length=num_epochs)\n      metrics = jax.tree_util.tree_map(jnp.mean, metrics)\n\n      if normalize_advantage:\n        metrics['advantage_scale'] = state.advantage_scale\n\n      if normalize_value:\n        metrics['value_mean'] = value_mean\n        metrics['value_std'] = value_std\n\n      delta_params_sgd_steps = (\n          data.extras['params_num_sgd_steps'][:, 0] -\n          params_num_sgd_steps_before_update)\n      metrics['delta_params_sgd_steps_min'] = jnp.min(delta_params_sgd_steps)\n      metrics['delta_params_sgd_steps_max'] = jnp.max(delta_params_sgd_steps)\n      metrics['delta_params_sgd_steps_mean'] = jnp.mean(delta_params_sgd_steps)\n      metrics['delta_params_sgd_steps_std'] = jnp.std(delta_params_sgd_steps)\n\n      return state, metrics\n\n    pmapped_update_step = jax.pmap(\n        single_device_update,\n        axis_name=pmap_axis_name,\n        devices=self.learner_devices)\n\n    def full_update_step(\n        state: TrainingState,\n        trajectories: types.NestedArray,\n    ):\n      state, metrics = pmapped_update_step(state, trajectories)\n      return state, metrics\n\n    self._full_update_step = full_update_step\n\n    def make_initial_state(key: networks_lib.PRNGKey) -> TrainingState:\n      \"\"\"Initialises the training state (parameters and optimiser state).\"\"\"\n      all_keys = jax.random.split(key, num=self.num_local_learner_devices + 1)\n      key_init, key_state = all_keys[0], all_keys[1:]\n      key_state = [key_state[i] for i in range(self.num_local_learner_devices)]\n      key_state = jax.device_put_sharded(key_state, self.local_learner_devices)\n\n      initial_params = ppo_networks.network.init(key_init)\n      initial_opt_state = optimizer.init(initial_params)\n      # Using float32 as it covers a larger range than int32. If using int64 we\n      # would need to do jax_enable_x64.\n      params_num_sgd_steps = jnp.zeros(shape=(), dtype=jnp.float32)\n\n      initial_params = jax.device_put_replicated(initial_params,\n                                                 self.local_learner_devices)\n      initial_opt_state = jax.device_put_replicated(initial_opt_state,\n                                                    self.local_learner_devices)\n      params_num_sgd_steps = jax.device_put_replicated(\n          params_num_sgd_steps, self.local_learner_devices)\n\n      ema_counter = jnp.float32(0)\n      ema_counter = jax.device_put_replicated(ema_counter,\n                                              self.local_learner_devices)\n\n      init_state = TrainingState(\n          params=PPOParams(\n              model_params=initial_params, num_sgd_steps=params_num_sgd_steps),\n          opt_state=initial_opt_state,\n          random_key=key_state,\n          ema_counter=ema_counter,\n      )\n\n      if normalize_advantage:\n        biased_advantage_scale = jax.device_put_replicated(\n            jnp.zeros([]), self.local_learner_devices)\n        advantage_scale = jax.device_put_replicated(\n            jnp.zeros([]), self.local_learner_devices)\n\n        init_state = init_state._replace(\n            biased_advantage_scale=biased_advantage_scale,\n            advantage_scale=advantage_scale)\n\n      if normalize_value:\n        biased_value_first_moment = jax.device_put_replicated(\n            jnp.zeros([]), self.local_learner_devices)\n        value_mean = biased_value_first_moment\n\n        biased_value_second_moment = jax.device_put_replicated(\n            jnp.zeros([]), self.local_learner_devices)\n        value_second_moment = biased_value_second_moment\n        value_std = jnp.sqrt(jax.nn.relu(value_second_moment - value_mean**2))\n\n        init_state = init_state._replace(\n            biased_value_first_moment=biased_value_first_moment,\n            biased_value_second_moment=biased_value_second_moment,\n            value_mean=value_mean,\n            value_std=value_std)\n\n      if normalize_obs:\n        obs_norm_params = obs_normalization_fns.init()  # pytype: disable=attribute-error\n        obs_norm_params = jax.device_put_replicated(obs_norm_params,\n                                                    self.local_learner_devices)\n        init_state = init_state._replace(\n            obs_normalization_params=obs_norm_params)\n\n      return init_state\n\n    # Initialise training state (parameters and optimizer state).\n    self._state = make_initial_state(random_key)\n    self._cached_state = get_from_first_device(self._state, as_numpy=True)",
  "def step(self):\n    \"\"\"Does a learner step and logs the results.\n\n    One learner step consists of (possibly multiple) epochs of PPO updates on\n    a batch of NxT steps collected by the actors.\n    \"\"\"\n    sample = next(self._iterator)\n    self._state, results = self._full_update_step(self._state, sample)\n    self._cached_state = get_from_first_device(self._state, as_numpy=True)\n\n    # Update our counts and record it.\n    counts = self._counter.increment(steps=self.num_epochs *\n                                     self.num_minibatches)\n\n    # Snapshot and attempt to write logs.\n    if self._num_full_update_steps % self.metrics_logging_period == 0:\n      results = jax.tree_util.tree_map(jnp.mean, results)\n      self._logger.write({**results, **counts})\n\n    self._num_full_update_steps += 1",
  "def get_variables(self, names: List[str]) -> List[networks_lib.Params]:\n    variables = self._cached_state\n    return [getattr(variables, name) for name in names]",
  "def save(self) -> TrainingState:\n    return self._cached_state",
  "def restore(self, state: TrainingState):\n    # TODO(kamyar) Should the random_key come from self._state instead?\n    random_key = state.random_key\n    random_key = jax.random.split(\n        random_key, num=self.num_local_learner_devices)\n    random_key = jax.device_put_sharded(\n        [random_key[i] for i in range(self.num_local_learner_devices)],\n        self.local_learner_devices)\n\n    state = jax.device_put_replicated(state, self.local_learner_devices)\n    state = state._replace(random_key=random_key)\n    self._state = state\n    self._cached_state = get_from_first_device(self._state, as_numpy=True)",
  "def ppo_loss(\n        params: networks_lib.Params,\n        observations: networks_lib.Observation,\n        actions: networks_lib.Action,\n        advantages: jnp.ndarray,\n        target_values: networks_lib.Value,\n        behavior_values: networks_lib.Value,\n        behavior_log_probs: networks_lib.LogProb,\n        value_mean: jnp.ndarray,\n        value_std: jnp.ndarray,\n        key: networks_lib.PRNGKey,\n    ) -> Tuple[jnp.ndarray, Dict[str, jnp.ndarray]]:\n      \"\"\"PPO loss for the policy and the critic.\"\"\"\n      distribution_params, values = ppo_networks.network.apply(\n          params, observations)\n      if normalize_value:\n        # values = values * jnp.fmax(value_std, 1e-6) + value_mean\n        target_values = (target_values - value_mean) / jnp.fmax(value_std, 1e-6)\n      policy_log_probs = ppo_networks.log_prob(distribution_params, actions)\n      key, sub_key = jax.random.split(key)\n      policy_entropies = ppo_networks.entropy(distribution_params, sub_key)\n\n      # Compute the policy losses\n      rhos = jnp.exp(policy_log_probs - behavior_log_probs)\n      clipped_ppo_policy_loss = rlax.clipped_surrogate_pg_loss(\n          rhos, advantages, ppo_clipping_epsilon)\n      policy_entropy_loss = -jnp.mean(policy_entropies)\n      total_policy_loss = (\n          clipped_ppo_policy_loss + entropy_cost * policy_entropy_loss)\n\n      # Compute the critic losses\n      unclipped_value_loss = (values - target_values)**2\n\n      if clip_value:\n        # Clip values to reduce variablility during critic training.\n        clipped_values = behavior_values + jnp.clip(values - behavior_values,\n                                                    -value_clipping_epsilon,\n                                                    value_clipping_epsilon)\n        clipped_value_error = target_values - clipped_values\n        clipped_value_loss = clipped_value_error ** 2\n        value_loss = jnp.mean(jnp.fmax(unclipped_value_loss,\n                                       clipped_value_loss))\n      else:\n        # For Mujoco envs clipping hurts a lot. Evidenced by Figure 43 in\n        # https://arxiv.org/pdf/2006.05990.pdf\n        value_loss = jnp.mean(unclipped_value_loss)\n\n      total_ppo_loss = total_policy_loss + value_cost * value_loss\n      return total_ppo_loss, {  # pytype: disable=bad-return-type  # numpy-scalars\n          'loss_total': total_ppo_loss,\n          'loss_policy_total': total_policy_loss,\n          'loss_policy_pg': clipped_ppo_policy_loss,\n          'loss_policy_entropy': policy_entropy_loss,\n          'loss_critic': value_loss,\n      }",
  "def sgd_step(state: TrainingState, minibatch: Batch):\n      observations = minibatch.observations\n      actions = minibatch.actions\n      advantages = minibatch.advantages\n      target_values = minibatch.target_values\n      behavior_values = minibatch.behavior_values\n      behavior_log_probs = minibatch.behavior_log_probs\n      key, sub_key = jax.random.split(state.random_key)\n\n      loss_grad, metrics = ppo_loss_grad(\n          state.params.model_params,\n          observations,\n          actions,\n          advantages,\n          target_values,\n          behavior_values,\n          behavior_log_probs,\n          state.value_mean,\n          state.value_std,\n          sub_key,\n      )\n\n      # Apply updates\n      loss_grad = jax.lax.pmean(loss_grad, axis_name=pmap_axis_name)\n      updates, opt_state = optimizer.update(loss_grad, state.opt_state)\n      model_params = optax.apply_updates(state.params.model_params, updates)\n      params = PPOParams(\n          model_params=model_params,\n          num_sgd_steps=state.params.num_sgd_steps + 1)\n\n      if log_global_norm_metrics:\n        metrics['norm_grad'] = optax.global_norm(loss_grad)\n        metrics['norm_updates'] = optax.global_norm(updates)\n\n      state = state._replace(params=params, opt_state=opt_state, random_key=key)\n\n      return state, metrics",
  "def epoch_update(\n        carry: Tuple[TrainingState, Batch],\n        unused_t: Tuple[()],\n    ):\n      state, carry_batch = carry\n\n      # Shuffling into minibatches\n      batch_size = carry_batch.advantages.shape[0]\n      key, sub_key = jax.random.split(state.random_key)\n      # TODO(kamyar) For effiency could use same permutation for all epochs\n      permuted_batch = jax.tree_util.tree_map(\n          lambda x: jax.random.permutation(  # pylint: disable=g-long-lambda\n              sub_key,\n              x,\n              axis=0,\n              independent=False),\n          carry_batch)\n      state = state._replace(random_key=key)\n      minibatches = jax.tree_util.tree_map(\n          lambda x: jnp.reshape(  # pylint: disable=g-long-lambda\n              x,\n              [  # pylint: disable=g-long-lambda\n                  num_minibatches, batch_size // num_minibatches\n              ] + list(x.shape[1:])),\n          permuted_batch)\n\n      # Scan over the minibatches\n      state, metrics = jax.lax.scan(\n          sgd_step, state, minibatches, length=num_minibatches)\n      metrics = jax.tree_util.tree_map(jnp.mean, metrics)\n\n      return (state, carry_batch), metrics",
  "def single_device_update(\n        state: TrainingState,\n        trajectories: types.NestedArray,\n    ):\n      params_num_sgd_steps_before_update = state.params.num_sgd_steps\n\n      # Update the EMA counter and obtain the zero debiasing multiplier\n      if normalize_advantage or normalize_value:\n        ema_counter = state.ema_counter + 1\n        state = state._replace(ema_counter=ema_counter)\n        zero_debias = 1. / (1. - jnp.power(normalization_ema_tau, ema_counter))\n\n      # Extract the data.\n      data = trajectories.data\n      observations, actions, rewards, termination, extra = (data.observation,\n                                                            data.action,\n                                                            data.reward,\n                                                            data.discount,\n                                                            data.extras)\n\n      if normalize_obs:\n        obs_norm_params = obs_normalization_fns.update(\n            state.obs_normalization_params, observations, pmap_axis_name)\n        state = state._replace(obs_normalization_params=obs_norm_params)\n        observations = obs_normalization_fns.normalize(\n            observations, state.obs_normalization_params)\n\n      if max_abs_reward is not None:\n        # Apply reward clipping.\n        rewards = jnp.clip(rewards, -1. * max_abs_reward, max_abs_reward)\n      discounts = termination * discount\n      behavior_log_probs = extra['log_prob']\n      _, behavior_values = vmapped_network_apply(state.params.model_params,\n                                                 observations)\n\n      if normalize_value:\n        batch_value_first_moment = jnp.mean(behavior_values)\n        batch_value_second_moment = jnp.mean(behavior_values**2)\n        batch_value_first_moment, batch_value_second_moment = jax.lax.pmean(\n            (batch_value_first_moment, batch_value_second_moment),\n            axis_name=pmap_axis_name)\n\n        biased_value_first_moment = (\n            normalization_ema_tau * state.biased_value_first_moment +\n            (1. - normalization_ema_tau) * batch_value_first_moment)\n        biased_value_second_moment = (\n            normalization_ema_tau * state.biased_value_second_moment +\n            (1. - normalization_ema_tau) * batch_value_second_moment)\n\n        value_mean = biased_value_first_moment * zero_debias\n        value_second_moment = biased_value_second_moment * zero_debias\n        value_std = jnp.sqrt(jax.nn.relu(value_second_moment - value_mean**2))\n\n        state = state._replace(\n            biased_value_first_moment=biased_value_first_moment,\n            biased_value_second_moment=biased_value_second_moment,\n            value_mean=value_mean,\n            value_std=value_std,\n        )\n\n        behavior_values = behavior_values * jnp.fmax(state.value_std,\n                                                     1e-6) + state.value_mean\n\n      behavior_values = jax.lax.stop_gradient(behavior_values)\n\n      # Compute GAE using rlax\n      vmapped_rlax_truncated_generalized_advantage_estimation = jax.vmap(\n          rlax.truncated_generalized_advantage_estimation,\n          in_axes=(0, 0, None, 0))\n      advantages = vmapped_rlax_truncated_generalized_advantage_estimation(\n          rewards[:, :-1], discounts[:, :-1], gae_lambda, behavior_values)\n      advantages = jax.lax.stop_gradient(advantages)\n      target_values = behavior_values[:, :-1] + advantages\n      target_values = jax.lax.stop_gradient(target_values)\n\n      # Exclude the last step - it was only used for bootstrapping.\n      # The shape is [num_sequences, num_steps, ..]\n      (observations, actions, behavior_log_probs, behavior_values) = (\n          jax.tree_util.tree_map(\n              lambda x: x[:, :-1],\n              (observations, actions, behavior_log_probs, behavior_values),\n          )\n      )\n\n      # Shuffle the data and break into minibatches\n      batch_size = advantages.shape[0] * advantages.shape[1]\n      batch = Batch(\n          observations=observations,\n          actions=actions,\n          advantages=advantages,\n          target_values=target_values,\n          behavior_values=behavior_values,\n          behavior_log_probs=behavior_log_probs)\n      batch = jax.tree_util.tree_map(\n          lambda x: jnp.reshape(x, [batch_size] + list(x.shape[2:])), batch)\n\n      if normalize_advantage:\n        batch_advantage_scale = jnp.mean(jnp.abs(batch.advantages))\n        batch_advantage_scale = jax.lax.pmean(batch_advantage_scale,\n                                              pmap_axis_name)\n\n        # update the running statistics\n        biased_advantage_scale = (\n            normalization_ema_tau * state.biased_advantage_scale +\n            (1. - normalization_ema_tau) * batch_advantage_scale)\n        advantage_scale = biased_advantage_scale * zero_debias\n        state = state._replace(\n            biased_advantage_scale=biased_advantage_scale,\n            advantage_scale=advantage_scale)\n\n        # scale the advantages\n        scaled_advantages = batch.advantages / jnp.fmax(state.advantage_scale,\n                                                        1e-6)\n        batch = batch._replace(advantages=scaled_advantages)\n\n      # Scan desired number of epoch updates\n      (state, _), metrics = jax.lax.scan(\n          epoch_update, (state, batch), (), length=num_epochs)\n      metrics = jax.tree_util.tree_map(jnp.mean, metrics)\n\n      if normalize_advantage:\n        metrics['advantage_scale'] = state.advantage_scale\n\n      if normalize_value:\n        metrics['value_mean'] = value_mean\n        metrics['value_std'] = value_std\n\n      delta_params_sgd_steps = (\n          data.extras['params_num_sgd_steps'][:, 0] -\n          params_num_sgd_steps_before_update)\n      metrics['delta_params_sgd_steps_min'] = jnp.min(delta_params_sgd_steps)\n      metrics['delta_params_sgd_steps_max'] = jnp.max(delta_params_sgd_steps)\n      metrics['delta_params_sgd_steps_mean'] = jnp.mean(delta_params_sgd_steps)\n      metrics['delta_params_sgd_steps_std'] = jnp.std(delta_params_sgd_steps)\n\n      return state, metrics",
  "def full_update_step(\n        state: TrainingState,\n        trajectories: types.NestedArray,\n    ):\n      state, metrics = pmapped_update_step(state, trajectories)\n      return state, metrics",
  "def make_initial_state(key: networks_lib.PRNGKey) -> TrainingState:\n      \"\"\"Initialises the training state (parameters and optimiser state).\"\"\"\n      all_keys = jax.random.split(key, num=self.num_local_learner_devices + 1)\n      key_init, key_state = all_keys[0], all_keys[1:]\n      key_state = [key_state[i] for i in range(self.num_local_learner_devices)]\n      key_state = jax.device_put_sharded(key_state, self.local_learner_devices)\n\n      initial_params = ppo_networks.network.init(key_init)\n      initial_opt_state = optimizer.init(initial_params)\n      # Using float32 as it covers a larger range than int32. If using int64 we\n      # would need to do jax_enable_x64.\n      params_num_sgd_steps = jnp.zeros(shape=(), dtype=jnp.float32)\n\n      initial_params = jax.device_put_replicated(initial_params,\n                                                 self.local_learner_devices)\n      initial_opt_state = jax.device_put_replicated(initial_opt_state,\n                                                    self.local_learner_devices)\n      params_num_sgd_steps = jax.device_put_replicated(\n          params_num_sgd_steps, self.local_learner_devices)\n\n      ema_counter = jnp.float32(0)\n      ema_counter = jax.device_put_replicated(ema_counter,\n                                              self.local_learner_devices)\n\n      init_state = TrainingState(\n          params=PPOParams(\n              model_params=initial_params, num_sgd_steps=params_num_sgd_steps),\n          opt_state=initial_opt_state,\n          random_key=key_state,\n          ema_counter=ema_counter,\n      )\n\n      if normalize_advantage:\n        biased_advantage_scale = jax.device_put_replicated(\n            jnp.zeros([]), self.local_learner_devices)\n        advantage_scale = jax.device_put_replicated(\n            jnp.zeros([]), self.local_learner_devices)\n\n        init_state = init_state._replace(\n            biased_advantage_scale=biased_advantage_scale,\n            advantage_scale=advantage_scale)\n\n      if normalize_value:\n        biased_value_first_moment = jax.device_put_replicated(\n            jnp.zeros([]), self.local_learner_devices)\n        value_mean = biased_value_first_moment\n\n        biased_value_second_moment = jax.device_put_replicated(\n            jnp.zeros([]), self.local_learner_devices)\n        value_second_moment = biased_value_second_moment\n        value_std = jnp.sqrt(jax.nn.relu(value_second_moment - value_mean**2))\n\n        init_state = init_state._replace(\n            biased_value_first_moment=biased_value_first_moment,\n            biased_value_second_moment=biased_value_second_moment,\n            value_mean=value_mean,\n            value_std=value_std)\n\n      if normalize_obs:\n        obs_norm_params = obs_normalization_fns.init()  # pytype: disable=attribute-error\n        obs_norm_params = jax.device_put_replicated(obs_norm_params,\n                                                    self.local_learner_devices)\n        init_state = init_state._replace(\n            obs_normalization_params=obs_norm_params)\n\n      return init_state",
  "class MVNDiagParams(NamedTuple):\n  \"\"\"Parameters for a diagonal multi-variate normal distribution.\"\"\"\n  loc: jnp.ndarray\n  scale_diag: jnp.ndarray",
  "class TanhNormalParams(NamedTuple):\n  \"\"\"Parameters for a tanh squashed diagonal MVN distribution.\"\"\"\n  loc: jnp.ndarray\n  scale: jnp.ndarray",
  "class CategoricalParams(NamedTuple):\n  \"\"\"Parameters for a categorical distribution.\"\"\"\n  logits: jnp.ndarray",
  "class PPOParams(NamedTuple):\n  model_params: networks_lib.Params\n  # Using float32 as it covers a larger range than int32. If using int64 we\n  # would need to do jax_enable_x64.\n  num_sgd_steps: jnp.float32",
  "class PPONetworks:\n  \"\"\"Network and pure functions for the PPO agent.\n\n  If 'network' returns tfd.Distribution, you can use make_ppo_networks() to\n  create this object properly.\n  If one is building this object manually, one has a freedom to make 'network'\n  object return anything that is later being passed as input to\n  log_prob/entropy/sample functions to perform the corresponding computations.\n  An example scenario where you would want to do this due to\n  tfd.Distribution not playing nice with jax.vmap. Please refer to the\n  make_continuous_networks() for an example where the network does not return a\n  tfd.Distribution object.\n  \"\"\"\n  network: networks_lib.FeedForwardNetwork\n  log_prob: networks_lib.LogProbFn\n  entropy: EntropyFn\n  sample: networks_lib.SampleFn\n  sample_eval: Optional[networks_lib.SampleFn] = None",
  "def make_inference_fn(\n    ppo_networks: PPONetworks,\n    evaluation: bool = False) -> actor_core_lib.FeedForwardPolicyWithExtra:\n  \"\"\"Returns a function to be used for inference by a PPO actor.\"\"\"\n\n  def inference(\n      params: networks_lib.Params,\n      key: networks_lib.PRNGKey,\n      observations: networks_lib.Observation,\n  ):\n    dist_params, _ = ppo_networks.network.apply(params.model_params,\n                                                observations)\n    if evaluation and ppo_networks.sample_eval:\n      actions = ppo_networks.sample_eval(dist_params, key)\n    else:\n      actions = ppo_networks.sample(dist_params, key)\n    if evaluation:\n      return actions, {}\n    log_prob = ppo_networks.log_prob(dist_params, actions)\n    extras = {\n        'log_prob': log_prob,\n        # Add batch dimension.\n        'params_num_sgd_steps': params.num_sgd_steps[None, ...]\n    }\n    return actions, extras\n\n  return inference",
  "def make_networks(\n    spec: specs.EnvironmentSpec, hidden_layer_sizes: Sequence[int] = (256, 256)\n) -> PPONetworks:\n  if isinstance(spec.actions, specs.DiscreteArray):\n    return make_discrete_networks(spec, hidden_layer_sizes)\n  else:\n    return make_continuous_networks(\n        spec,\n        policy_layer_sizes=hidden_layer_sizes,\n        value_layer_sizes=hidden_layer_sizes)",
  "def make_ppo_networks(network: networks_lib.FeedForwardNetwork) -> PPONetworks:\n  \"\"\"Constructs a PPONetworks instance from the given FeedForwardNetwork.\n\n  This method assumes that the network returns a tfd.Distribution. Sometimes it\n  may be preferable to have networks that do not return tfd.Distribution\n  objects, for example, due to tfd.Distribution not playing nice with jax.vmap.\n  Please refer to the make_continuous_networks() for an example where the\n  network does not return a tfd.Distribution object.\n\n  Args:\n    network: a transformed Haiku network that takes in observations and returns\n      the action distribution and value.\n\n  Returns:\n    A PPONetworks instance with pure functions wrapping the input network.\n  \"\"\"\n  return PPONetworks(\n      network=network,\n      log_prob=lambda distribution, action: distribution.log_prob(action),\n      entropy=lambda distribution, key=None: distribution.entropy(),\n      sample=lambda distribution, key: distribution.sample(seed=key),\n      sample_eval=lambda distribution, key: distribution.mode())",
  "def make_mvn_diag_ppo_networks(\n    network: networks_lib.FeedForwardNetwork) -> PPONetworks:\n  \"\"\"Constructs a PPONetworks for MVN Diag policy from the FeedForwardNetwork.\n\n  Args:\n    network: a transformed Haiku network (or equivalent in other libraries) that\n      takes in observations and returns the action distribution and value.\n\n  Returns:\n    A PPONetworks instance with pure functions wrapping the input network.\n  \"\"\"\n\n  def log_prob(params: MVNDiagParams, action):\n    return tfd.MultivariateNormalDiag(\n        loc=params.loc, scale_diag=params.scale_diag).log_prob(action)\n\n  def entropy(\n      params: MVNDiagParams, key: networks_lib.PRNGKey\n  ) -> networks_lib.Entropy:\n    del key\n    return tfd.MultivariateNormalDiag(\n        loc=params.loc, scale_diag=params.scale_diag).entropy()\n\n  def sample(params: MVNDiagParams, key: networks_lib.PRNGKey):\n    return tfd.MultivariateNormalDiag(\n        loc=params.loc, scale_diag=params.scale_diag).sample(seed=key)\n\n  def sample_eval(params: MVNDiagParams, key: networks_lib.PRNGKey):\n    del key\n    return tfd.MultivariateNormalDiag(\n        loc=params.loc, scale_diag=params.scale_diag).mode()\n\n  return PPONetworks(\n      network=network,\n      log_prob=log_prob,\n      entropy=entropy,\n      sample=sample,\n      sample_eval=sample_eval)",
  "def make_tanh_normal_ppo_networks(\n    network: networks_lib.FeedForwardNetwork) -> PPONetworks:\n  \"\"\"Constructs a PPONetworks for Tanh MVN Diag policy from the FeedForwardNetwork.\n\n  Args:\n    network: a transformed Haiku network (or equivalent in other libraries) that\n      takes in observations and returns the action distribution and value.\n\n  Returns:\n    A PPONetworks instance with pure functions wrapping the input network.\n  \"\"\"\n\n  def build_distribution(params: TanhNormalParams):\n    distribution = tfd.Normal(loc=params.loc, scale=params.scale)\n    distribution = tfd.Independent(\n        networks_lib.TanhTransformedDistribution(distribution),\n        reinterpreted_batch_ndims=1)\n    return distribution\n\n  def log_prob(params: TanhNormalParams, action):\n    distribution = build_distribution(params)\n    return distribution.log_prob(action)\n\n  def entropy(\n      params: TanhNormalParams, key: networks_lib.PRNGKey\n  ) -> networks_lib.Entropy:\n    distribution = build_distribution(params)\n    return distribution.entropy(seed=key)\n\n  def sample(params: TanhNormalParams, key: networks_lib.PRNGKey):\n    distribution = build_distribution(params)\n    return distribution.sample(seed=key)\n\n  def sample_eval(params: TanhNormalParams, key: networks_lib.PRNGKey):\n    del key\n    distribution = build_distribution(params)\n    return distribution.mode()\n\n  return PPONetworks(\n      network=network,\n      log_prob=log_prob,\n      entropy=entropy,\n      sample=sample,\n      sample_eval=sample_eval)",
  "def make_discrete_networks(\n    environment_spec: specs.EnvironmentSpec,\n    hidden_layer_sizes: Sequence[int] = (512,),\n    use_conv: bool = True,\n) -> PPONetworks:\n  \"\"\"Creates networks used by the agent for discrete action environments.\n\n  Args:\n    environment_spec: Environment spec used to define number of actions.\n    hidden_layer_sizes: Network definition.\n    use_conv: Whether to use a conv or MLP feature extractor.\n  Returns:\n    PPONetworks\n  \"\"\"\n\n  num_actions = environment_spec.actions.num_values\n\n  def forward_fn(inputs):\n    layers = []\n    if use_conv:\n      layers.extend([networks_lib.AtariTorso()])\n    layers.extend([hk.nets.MLP(hidden_layer_sizes, activate_final=True)])\n    trunk = hk.Sequential(layers)\n    h = utils.batch_concat(inputs)\n    h = trunk(h)\n    logits = hk.Linear(num_actions)(h)\n    values = hk.Linear(1)(h)\n    values = jnp.squeeze(values, axis=-1)\n    return (CategoricalParams(logits=logits), values)\n\n  forward_fn = hk.without_apply_rng(hk.transform(forward_fn))\n  dummy_obs = utils.zeros_like(environment_spec.observations)\n  dummy_obs = utils.add_batch_dim(dummy_obs)  # Dummy 'sequence' dim.\n  network = networks_lib.FeedForwardNetwork(\n      lambda rng: forward_fn.init(rng, dummy_obs), forward_fn.apply)\n  # Create PPONetworks to add functionality required by the agent.\n  return make_categorical_ppo_networks(network)",
  "def make_categorical_ppo_networks(\n    network: networks_lib.FeedForwardNetwork) -> PPONetworks:\n  \"\"\"Constructs a PPONetworks for Categorical Policy from FeedForwardNetwork.\n\n  Args:\n    network: a transformed Haiku network (or equivalent in other libraries) that\n      takes in observations and returns the action distribution and value.\n\n  Returns:\n    A PPONetworks instance with pure functions wrapping the input network.\n  \"\"\"\n\n  def log_prob(params: CategoricalParams, action):\n    return tfd.Categorical(logits=params.logits).log_prob(action)\n\n  def entropy(\n      params: CategoricalParams, key: networks_lib.PRNGKey\n  ) -> networks_lib.Entropy:\n    del key\n    return tfd.Categorical(logits=params.logits).entropy()\n\n  def sample(params: CategoricalParams, key: networks_lib.PRNGKey):\n    return tfd.Categorical(logits=params.logits).sample(seed=key)\n\n  def sample_eval(params: CategoricalParams, key: networks_lib.PRNGKey):\n    del key\n    return tfd.Categorical(logits=params.logits).mode()\n\n  return PPONetworks(\n      network=network,\n      log_prob=log_prob,\n      entropy=entropy,\n      sample=sample,\n      sample_eval=sample_eval)",
  "def make_continuous_networks(\n    environment_spec: specs.EnvironmentSpec,\n    policy_layer_sizes: Sequence[int] = (64, 64),\n    value_layer_sizes: Sequence[int] = (64, 64),\n    use_tanh_gaussian_policy: bool = True,\n) -> PPONetworks:\n  \"\"\"Creates PPONetworks to be used for continuous action environments.\"\"\"\n\n  # Get total number of action dimensions from action spec.\n  num_dimensions = np.prod(environment_spec.actions.shape, dtype=int)\n\n  def forward_fn(inputs: networks_lib.Observation):\n\n    def _policy_network(obs: networks_lib.Observation):\n      h = utils.batch_concat(obs)\n      h = hk.nets.MLP(policy_layer_sizes, activate_final=True)(h)\n\n      # tfd distributions have a weird bug in jax when vmapping is used, so the\n      # safer implementation in general is for the policy network to output the\n      # distribution parameters, and for the distribution to be constructed\n      # in a method such as make_ppo_networks above\n      if not use_tanh_gaussian_policy:\n        # Following networks_lib.MultivariateNormalDiagHead\n        init_scale = 0.3\n        min_scale = 1e-6\n        w_init = hk.initializers.VarianceScaling(1e-4)\n        b_init = hk.initializers.Constant(0.)\n        loc_layer = hk.Linear(num_dimensions, w_init=w_init, b_init=b_init)\n        scale_layer = hk.Linear(num_dimensions, w_init=w_init, b_init=b_init)\n\n        loc = loc_layer(h)\n        scale = jax.nn.softplus(scale_layer(h))\n        scale *= init_scale / jax.nn.softplus(0.)\n        scale += min_scale\n\n        return MVNDiagParams(loc=loc, scale_diag=scale)\n\n      # Following networks_lib.NormalTanhDistribution\n      min_scale = 1e-3\n      w_init = hk.initializers.VarianceScaling(1.0, 'fan_in', 'uniform')\n      b_init = hk.initializers.Constant(0.)\n      loc_layer = hk.Linear(num_dimensions, w_init=w_init, b_init=b_init)\n      scale_layer = hk.Linear(num_dimensions, w_init=w_init, b_init=b_init)\n\n      loc = loc_layer(h)\n      scale = scale_layer(h)\n      scale = jax.nn.softplus(scale) + min_scale\n\n      return TanhNormalParams(loc=loc, scale=scale)\n\n    value_network = hk.Sequential([\n        utils.batch_concat,\n        hk.nets.MLP(value_layer_sizes, activate_final=True),\n        hk.Linear(1),\n        lambda x: jnp.squeeze(x, axis=-1)\n    ])\n\n    policy_output = _policy_network(inputs)\n    value = value_network(inputs)\n    return (policy_output, value)\n\n  # Transform into pure functions.\n  forward_fn = hk.without_apply_rng(hk.transform(forward_fn))\n\n  dummy_obs = utils.zeros_like(environment_spec.observations)\n  dummy_obs = utils.add_batch_dim(dummy_obs)  # Dummy 'sequence' dim.\n  network = networks_lib.FeedForwardNetwork(\n      lambda rng: forward_fn.init(rng, dummy_obs), forward_fn.apply)\n\n  # Create PPONetworks to add functionality required by the agent.\n\n  if not use_tanh_gaussian_policy:\n    return make_mvn_diag_ppo_networks(network)\n\n  return make_tanh_normal_ppo_networks(network)",
  "def inference(\n      params: networks_lib.Params,\n      key: networks_lib.PRNGKey,\n      observations: networks_lib.Observation,\n  ):\n    dist_params, _ = ppo_networks.network.apply(params.model_params,\n                                                observations)\n    if evaluation and ppo_networks.sample_eval:\n      actions = ppo_networks.sample_eval(dist_params, key)\n    else:\n      actions = ppo_networks.sample(dist_params, key)\n    if evaluation:\n      return actions, {}\n    log_prob = ppo_networks.log_prob(dist_params, actions)\n    extras = {\n        'log_prob': log_prob,\n        # Add batch dimension.\n        'params_num_sgd_steps': params.num_sgd_steps[None, ...]\n    }\n    return actions, extras",
  "def log_prob(params: MVNDiagParams, action):\n    return tfd.MultivariateNormalDiag(\n        loc=params.loc, scale_diag=params.scale_diag).log_prob(action)",
  "def entropy(\n      params: MVNDiagParams, key: networks_lib.PRNGKey\n  ) -> networks_lib.Entropy:\n    del key\n    return tfd.MultivariateNormalDiag(\n        loc=params.loc, scale_diag=params.scale_diag).entropy()",
  "def sample(params: MVNDiagParams, key: networks_lib.PRNGKey):\n    return tfd.MultivariateNormalDiag(\n        loc=params.loc, scale_diag=params.scale_diag).sample(seed=key)",
  "def sample_eval(params: MVNDiagParams, key: networks_lib.PRNGKey):\n    del key\n    return tfd.MultivariateNormalDiag(\n        loc=params.loc, scale_diag=params.scale_diag).mode()",
  "def build_distribution(params: TanhNormalParams):\n    distribution = tfd.Normal(loc=params.loc, scale=params.scale)\n    distribution = tfd.Independent(\n        networks_lib.TanhTransformedDistribution(distribution),\n        reinterpreted_batch_ndims=1)\n    return distribution",
  "def log_prob(params: TanhNormalParams, action):\n    distribution = build_distribution(params)\n    return distribution.log_prob(action)",
  "def entropy(\n      params: TanhNormalParams, key: networks_lib.PRNGKey\n  ) -> networks_lib.Entropy:\n    distribution = build_distribution(params)\n    return distribution.entropy(seed=key)",
  "def sample(params: TanhNormalParams, key: networks_lib.PRNGKey):\n    distribution = build_distribution(params)\n    return distribution.sample(seed=key)",
  "def sample_eval(params: TanhNormalParams, key: networks_lib.PRNGKey):\n    del key\n    distribution = build_distribution(params)\n    return distribution.mode()",
  "def forward_fn(inputs):\n    layers = []\n    if use_conv:\n      layers.extend([networks_lib.AtariTorso()])\n    layers.extend([hk.nets.MLP(hidden_layer_sizes, activate_final=True)])\n    trunk = hk.Sequential(layers)\n    h = utils.batch_concat(inputs)\n    h = trunk(h)\n    logits = hk.Linear(num_actions)(h)\n    values = hk.Linear(1)(h)\n    values = jnp.squeeze(values, axis=-1)\n    return (CategoricalParams(logits=logits), values)",
  "def log_prob(params: CategoricalParams, action):\n    return tfd.Categorical(logits=params.logits).log_prob(action)",
  "def entropy(\n      params: CategoricalParams, key: networks_lib.PRNGKey\n  ) -> networks_lib.Entropy:\n    del key\n    return tfd.Categorical(logits=params.logits).entropy()",
  "def sample(params: CategoricalParams, key: networks_lib.PRNGKey):\n    return tfd.Categorical(logits=params.logits).sample(seed=key)",
  "def sample_eval(params: CategoricalParams, key: networks_lib.PRNGKey):\n    del key\n    return tfd.Categorical(logits=params.logits).mode()",
  "def forward_fn(inputs: networks_lib.Observation):\n\n    def _policy_network(obs: networks_lib.Observation):\n      h = utils.batch_concat(obs)\n      h = hk.nets.MLP(policy_layer_sizes, activate_final=True)(h)\n\n      # tfd distributions have a weird bug in jax when vmapping is used, so the\n      # safer implementation in general is for the policy network to output the\n      # distribution parameters, and for the distribution to be constructed\n      # in a method such as make_ppo_networks above\n      if not use_tanh_gaussian_policy:\n        # Following networks_lib.MultivariateNormalDiagHead\n        init_scale = 0.3\n        min_scale = 1e-6\n        w_init = hk.initializers.VarianceScaling(1e-4)\n        b_init = hk.initializers.Constant(0.)\n        loc_layer = hk.Linear(num_dimensions, w_init=w_init, b_init=b_init)\n        scale_layer = hk.Linear(num_dimensions, w_init=w_init, b_init=b_init)\n\n        loc = loc_layer(h)\n        scale = jax.nn.softplus(scale_layer(h))\n        scale *= init_scale / jax.nn.softplus(0.)\n        scale += min_scale\n\n        return MVNDiagParams(loc=loc, scale_diag=scale)\n\n      # Following networks_lib.NormalTanhDistribution\n      min_scale = 1e-3\n      w_init = hk.initializers.VarianceScaling(1.0, 'fan_in', 'uniform')\n      b_init = hk.initializers.Constant(0.)\n      loc_layer = hk.Linear(num_dimensions, w_init=w_init, b_init=b_init)\n      scale_layer = hk.Linear(num_dimensions, w_init=w_init, b_init=b_init)\n\n      loc = loc_layer(h)\n      scale = scale_layer(h)\n      scale = jax.nn.softplus(scale) + min_scale\n\n      return TanhNormalParams(loc=loc, scale=scale)\n\n    value_network = hk.Sequential([\n        utils.batch_concat,\n        hk.nets.MLP(value_layer_sizes, activate_final=True),\n        hk.Linear(1),\n        lambda x: jnp.squeeze(x, axis=-1)\n    ])\n\n    policy_output = _policy_network(inputs)\n    value = value_network(inputs)\n    return (policy_output, value)",
  "def _policy_network(obs: networks_lib.Observation):\n      h = utils.batch_concat(obs)\n      h = hk.nets.MLP(policy_layer_sizes, activate_final=True)(h)\n\n      # tfd distributions have a weird bug in jax when vmapping is used, so the\n      # safer implementation in general is for the policy network to output the\n      # distribution parameters, and for the distribution to be constructed\n      # in a method such as make_ppo_networks above\n      if not use_tanh_gaussian_policy:\n        # Following networks_lib.MultivariateNormalDiagHead\n        init_scale = 0.3\n        min_scale = 1e-6\n        w_init = hk.initializers.VarianceScaling(1e-4)\n        b_init = hk.initializers.Constant(0.)\n        loc_layer = hk.Linear(num_dimensions, w_init=w_init, b_init=b_init)\n        scale_layer = hk.Linear(num_dimensions, w_init=w_init, b_init=b_init)\n\n        loc = loc_layer(h)\n        scale = jax.nn.softplus(scale_layer(h))\n        scale *= init_scale / jax.nn.softplus(0.)\n        scale += min_scale\n\n        return MVNDiagParams(loc=loc, scale_diag=scale)\n\n      # Following networks_lib.NormalTanhDistribution\n      min_scale = 1e-3\n      w_init = hk.initializers.VarianceScaling(1.0, 'fan_in', 'uniform')\n      b_init = hk.initializers.Constant(0.)\n      loc_layer = hk.Linear(num_dimensions, w_init=w_init, b_init=b_init)\n      scale_layer = hk.Linear(num_dimensions, w_init=w_init, b_init=b_init)\n\n      loc = loc_layer(h)\n      scale = scale_layer(h)\n      scale = jax.nn.softplus(scale) + min_scale\n\n      return TanhNormalParams(loc=loc, scale=scale)",
  "class PPOConfig:\n  \"\"\"Configuration options for PPO.\n\n  Attributes:\n    unroll_length: Length of sequences added to the replay buffer.\n    num_minibatches: The number of minibatches to split an epoch into.\n      i.e. minibatch size = batch_size * unroll_length / num_minibatches.\n    num_epochs: How many times to loop over the set of minibatches.\n    batch_size: Number of trajectory segments of length unroll_length to gather\n      for use in a call to the learner's step function.\n    replay_table_name: Replay table name.\n    ppo_clipping_epsilon: PPO clipping epsilon.\n    normalize_advantage: Whether to normalize the advantages in the batch.\n    normalize_value: Whether the critic should predict normalized values.\n    normalization_ema_tau: Float tau for the exponential moving average used to\n      maintain statistics for normalizing advantages and values.\n    clip_value: Whether to clip the values as described in \"What Matters in\n      On-Policy Reinforcement Learning?\".\n    value_clipping_epsilon: Epsilon for value clipping.\n    max_abs_reward: If provided clips the rewards in the trajectory to have\n      absolute value less than or equal to max_abs_reward.\n    gae_lambda: Lambda parameter in Generalized Advantage Estimation.\n    discount: Discount factor.\n    learning_rate: Learning rate for updating the policy and critic networks.\n    adam_epsilon: Adam epsilon parameter.\n    entropy_cost: Weight of the entropy regularizer term in policy optimization.\n    value_cost: Weight of the value loss term in optimization.\n    max_gradient_norm: Threshold for clipping the gradient norm.\n    variable_update_period: Determines how frequently actors pull the parameters\n      from the learner.\n    log_global_norm_metrics: Whether to log global norm of gradients and\n      updates.\n    metrics_logging_period: How often metrics should be aggregated to host and\n      logged.\n    pmap_axis_name: The name of the axis used for pmapping\n    obs_normalization_fns_factory: The factory used for create observation\n      normalization functions. Setting to None (default) disables observation\n      normalization.\n  \"\"\"\n  unroll_length: int = 8\n  num_minibatches: int = 8\n  num_epochs: int = 2\n  batch_size: int = 256\n  replay_table_name: str = adders_reverb.DEFAULT_PRIORITY_TABLE\n  ppo_clipping_epsilon: float = 0.2\n  normalize_advantage: bool = False\n  normalize_value: bool = False\n  normalization_ema_tau: float = 0.995\n  clip_value: bool = False\n  value_clipping_epsilon: float = 0.2\n  max_abs_reward: Optional[float] = None\n  gae_lambda: float = 0.95\n  discount: float = 0.99\n  learning_rate: Union[float, Callable[[int], float]] = 3e-4\n  adam_epsilon: float = 1e-7\n  entropy_cost: float = 3e-4\n  value_cost: float = 1.\n  max_gradient_norm: float = 0.5\n  variable_update_period: int = 1\n  log_global_norm_metrics: bool = False\n  metrics_logging_period: int = 100\n  pmap_axis_name: str = 'devices'\n  obs_normalization_fns_factory: Optional[Callable[\n      [types.NestedSpec], normalization.NormalizationFns]] = None",
  "class PPOBuilder(\n    builders.ActorLearnerBuilder[ppo_networks.PPONetworks,\n                                 actor_core_lib.FeedForwardPolicyWithExtra,\n                                 reverb.ReplaySample]):\n  \"\"\"PPO Builder.\"\"\"\n\n  def __init__(\n      self,\n      config: ppo_config.PPOConfig,\n  ):\n    \"\"\"Creates PPO builder.\"\"\"\n    self._config = config\n\n    # An extra step is used for bootstrapping when computing advantages.\n    self._sequence_length = config.unroll_length + 1\n\n  def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: actor_core_lib.FeedForwardPolicyWithExtra,\n  ) -> List[reverb.Table]:\n    \"\"\"Creates reverb tables for the algorithm.\"\"\"\n    del policy\n    # params_num_sgd_steps is used to track how old the actor parameters are\n    extra_spec = {\n        'log_prob': np.ones(shape=(), dtype=np.float32),\n        'params_num_sgd_steps': np.ones(shape=(), dtype=np.float32),\n    }\n    signature = adders_reverb.SequenceAdder.signature(\n        environment_spec, extra_spec, sequence_length=self._sequence_length)\n    return [\n        reverb.Table.queue(\n            name=self._config.replay_table_name,\n            max_size=self._config.batch_size,\n            signature=signature)\n    ]\n\n  def make_dataset_iterator(\n      self, replay_client: reverb.Client) -> Iterator[reverb.ReplaySample]:\n    \"\"\"Creates a dataset.\n\n    The iterator batch size is computed as follows:\n\n    Let:\n      B := learner batch size (config.batch_size)\n      H := number of hosts (jax.process_count())\n      D := number of local devices per host\n\n    The Reverb iterator will load batches of size B // (H * D). After wrapping\n    the iterator with utils.multi_device_put, this will result in an iterable\n    that provides B // H samples per item, with B // (H * D) samples placed on\n    each local device. In a multi-host setup, each host has its own learner\n    node and builds its own instance of the iterator. This will result\n    in a total batch size of H * (B // H) == B being consumed per learner\n    step (since the learner is pmapped across all devices). Note that\n    jax.device_count() returns the total number of devices across hosts,\n    i.e. H * D.\n\n    Args:\n      replay_client: the reverb replay client\n\n    Returns:\n      A replay buffer iterator to be used by the local devices.\n    \"\"\"\n    iterator_batch_size, ragged = divmod(self._config.batch_size,\n                                         jax.device_count())\n    if ragged:\n      raise ValueError(\n          'Learner batch size must be divisible by total number of devices!')\n\n    # We don't use datasets.make_reverb_dataset() here to avoid interleaving\n    # and prefetching, that doesn't work well with can_sample() check on update.\n    # NOTE: Value for max_in_flight_samples_per_worker comes from a\n    # recommendation here: https://git.io/JYzXB\n    dataset = reverb.TrajectoryDataset.from_table_signature(\n        server_address=replay_client.server_address,\n        table=self._config.replay_table_name,\n        max_in_flight_samples_per_worker=(\n            2 * self._config.batch_size // jax.process_count()\n        ),\n    )\n    dataset = dataset.batch(iterator_batch_size, drop_remainder=True)\n    dataset = dataset.as_numpy_iterator()\n    return utils.multi_device_put(iterable=dataset, devices=jax.local_devices())\n\n  def make_adder(\n      self,\n      replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[actor_core_lib.FeedForwardPolicyWithExtra],\n  ) -> Optional[adders.Adder]:\n    \"\"\"Creates an adder which handles observations.\"\"\"\n    del environment_spec, policy\n    # Note that the last transition in the sequence is used for bootstrapping\n    # only and is ignored otherwise. So we need to make sure that sequences\n    # overlap on one transition, thus \"-1\" in the period length computation.\n    return adders_reverb.SequenceAdder(\n        client=replay_client,\n        priority_fns={self._config.replay_table_name: None},\n        period=self._sequence_length - 1,\n        sequence_length=self._sequence_length,\n    )\n\n  def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: ppo_networks.PPONetworks,\n      dataset: Iterator[reverb.ReplaySample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    del replay_client\n\n    if callable(self._config.learning_rate):\n      optimizer = optax.chain(\n          optax.clip_by_global_norm(self._config.max_gradient_norm),\n          optax.scale_by_adam(eps=self._config.adam_epsilon),\n          optax.scale_by_schedule(self._config.learning_rate), optax.scale(-1))  # pytype: disable=wrong-arg-types  # numpy-scalars\n    else:\n      optimizer = optax.chain(\n          optax.clip_by_global_norm(self._config.max_gradient_norm),\n          optax.scale_by_adam(eps=self._config.adam_epsilon),\n          optax.scale(-self._config.learning_rate))\n\n    obs_normalization_fns = None\n    if self._config.obs_normalization_fns_factory is not None:\n      obs_normalization_fns = self._config.obs_normalization_fns_factory(\n          environment_spec.observations)\n\n    return learning.PPOLearner(\n        ppo_networks=networks,\n        iterator=dataset,\n        discount=self._config.discount,\n        entropy_cost=self._config.entropy_cost,\n        value_cost=self._config.value_cost,\n        ppo_clipping_epsilon=self._config.ppo_clipping_epsilon,\n        normalize_advantage=self._config.normalize_advantage,\n        normalize_value=self._config.normalize_value,\n        normalization_ema_tau=self._config.normalization_ema_tau,\n        clip_value=self._config.clip_value,\n        value_clipping_epsilon=self._config.value_clipping_epsilon,\n        max_abs_reward=self._config.max_abs_reward,\n        gae_lambda=self._config.gae_lambda,\n        counter=counter,\n        random_key=random_key,\n        optimizer=optimizer,\n        num_epochs=self._config.num_epochs,\n        num_minibatches=self._config.num_minibatches,\n        logger=logger_fn('learner'),\n        log_global_norm_metrics=self._config.log_global_norm_metrics,\n        metrics_logging_period=self._config.metrics_logging_period,\n        pmap_axis_name=self._config.pmap_axis_name,\n        obs_normalization_fns=obs_normalization_fns,\n    )\n\n  def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: actor_core_lib.FeedForwardPolicyWithExtra,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> core.Actor:\n    assert variable_source is not None\n    actor_core = actor_core_lib.batched_feed_forward_with_extras_to_actor_core(\n        policy)\n    if self._config.obs_normalization_fns_factory is not None:\n      variable_client = variable_utils.VariableClient(\n          variable_source, ['params', 'obs_normalization_params'],\n          device='cpu',\n          update_period=self._config.variable_update_period)\n      obs_normalization_fns = self._config.obs_normalization_fns_factory(\n          environment_spec.observations)\n      actor = normalization.NormalizedGenericActor(\n          actor_core,\n          obs_normalization_fns,\n          random_key,\n          variable_client,\n          adder,\n          jit=True,\n          backend='cpu',\n          per_episode_update=False,\n      )\n    else:\n      variable_client = variable_utils.VariableClient(\n          variable_source,\n          'params',\n          device='cpu',\n          update_period=self._config.variable_update_period)\n      actor = actors.GenericActor(\n          actor_core, random_key, variable_client, adder, backend='cpu')\n    return actor\n\n  def make_policy(\n      self,\n      networks: ppo_networks.PPONetworks,\n      environment_spec: specs.EnvironmentSpec,\n      evaluation: bool = False) -> actor_core_lib.FeedForwardPolicyWithExtra:\n    del environment_spec\n    return ppo_networks.make_inference_fn(networks, evaluation)",
  "def __init__(\n      self,\n      config: ppo_config.PPOConfig,\n  ):\n    \"\"\"Creates PPO builder.\"\"\"\n    self._config = config\n\n    # An extra step is used for bootstrapping when computing advantages.\n    self._sequence_length = config.unroll_length + 1",
  "def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: actor_core_lib.FeedForwardPolicyWithExtra,\n  ) -> List[reverb.Table]:\n    \"\"\"Creates reverb tables for the algorithm.\"\"\"\n    del policy\n    # params_num_sgd_steps is used to track how old the actor parameters are\n    extra_spec = {\n        'log_prob': np.ones(shape=(), dtype=np.float32),\n        'params_num_sgd_steps': np.ones(shape=(), dtype=np.float32),\n    }\n    signature = adders_reverb.SequenceAdder.signature(\n        environment_spec, extra_spec, sequence_length=self._sequence_length)\n    return [\n        reverb.Table.queue(\n            name=self._config.replay_table_name,\n            max_size=self._config.batch_size,\n            signature=signature)\n    ]",
  "def make_dataset_iterator(\n      self, replay_client: reverb.Client) -> Iterator[reverb.ReplaySample]:\n    \"\"\"Creates a dataset.\n\n    The iterator batch size is computed as follows:\n\n    Let:\n      B := learner batch size (config.batch_size)\n      H := number of hosts (jax.process_count())\n      D := number of local devices per host\n\n    The Reverb iterator will load batches of size B // (H * D). After wrapping\n    the iterator with utils.multi_device_put, this will result in an iterable\n    that provides B // H samples per item, with B // (H * D) samples placed on\n    each local device. In a multi-host setup, each host has its own learner\n    node and builds its own instance of the iterator. This will result\n    in a total batch size of H * (B // H) == B being consumed per learner\n    step (since the learner is pmapped across all devices). Note that\n    jax.device_count() returns the total number of devices across hosts,\n    i.e. H * D.\n\n    Args:\n      replay_client: the reverb replay client\n\n    Returns:\n      A replay buffer iterator to be used by the local devices.\n    \"\"\"\n    iterator_batch_size, ragged = divmod(self._config.batch_size,\n                                         jax.device_count())\n    if ragged:\n      raise ValueError(\n          'Learner batch size must be divisible by total number of devices!')\n\n    # We don't use datasets.make_reverb_dataset() here to avoid interleaving\n    # and prefetching, that doesn't work well with can_sample() check on update.\n    # NOTE: Value for max_in_flight_samples_per_worker comes from a\n    # recommendation here: https://git.io/JYzXB\n    dataset = reverb.TrajectoryDataset.from_table_signature(\n        server_address=replay_client.server_address,\n        table=self._config.replay_table_name,\n        max_in_flight_samples_per_worker=(\n            2 * self._config.batch_size // jax.process_count()\n        ),\n    )\n    dataset = dataset.batch(iterator_batch_size, drop_remainder=True)\n    dataset = dataset.as_numpy_iterator()\n    return utils.multi_device_put(iterable=dataset, devices=jax.local_devices())",
  "def make_adder(\n      self,\n      replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[actor_core_lib.FeedForwardPolicyWithExtra],\n  ) -> Optional[adders.Adder]:\n    \"\"\"Creates an adder which handles observations.\"\"\"\n    del environment_spec, policy\n    # Note that the last transition in the sequence is used for bootstrapping\n    # only and is ignored otherwise. So we need to make sure that sequences\n    # overlap on one transition, thus \"-1\" in the period length computation.\n    return adders_reverb.SequenceAdder(\n        client=replay_client,\n        priority_fns={self._config.replay_table_name: None},\n        period=self._sequence_length - 1,\n        sequence_length=self._sequence_length,\n    )",
  "def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: ppo_networks.PPONetworks,\n      dataset: Iterator[reverb.ReplaySample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    del replay_client\n\n    if callable(self._config.learning_rate):\n      optimizer = optax.chain(\n          optax.clip_by_global_norm(self._config.max_gradient_norm),\n          optax.scale_by_adam(eps=self._config.adam_epsilon),\n          optax.scale_by_schedule(self._config.learning_rate), optax.scale(-1))  # pytype: disable=wrong-arg-types  # numpy-scalars\n    else:\n      optimizer = optax.chain(\n          optax.clip_by_global_norm(self._config.max_gradient_norm),\n          optax.scale_by_adam(eps=self._config.adam_epsilon),\n          optax.scale(-self._config.learning_rate))\n\n    obs_normalization_fns = None\n    if self._config.obs_normalization_fns_factory is not None:\n      obs_normalization_fns = self._config.obs_normalization_fns_factory(\n          environment_spec.observations)\n\n    return learning.PPOLearner(\n        ppo_networks=networks,\n        iterator=dataset,\n        discount=self._config.discount,\n        entropy_cost=self._config.entropy_cost,\n        value_cost=self._config.value_cost,\n        ppo_clipping_epsilon=self._config.ppo_clipping_epsilon,\n        normalize_advantage=self._config.normalize_advantage,\n        normalize_value=self._config.normalize_value,\n        normalization_ema_tau=self._config.normalization_ema_tau,\n        clip_value=self._config.clip_value,\n        value_clipping_epsilon=self._config.value_clipping_epsilon,\n        max_abs_reward=self._config.max_abs_reward,\n        gae_lambda=self._config.gae_lambda,\n        counter=counter,\n        random_key=random_key,\n        optimizer=optimizer,\n        num_epochs=self._config.num_epochs,\n        num_minibatches=self._config.num_minibatches,\n        logger=logger_fn('learner'),\n        log_global_norm_metrics=self._config.log_global_norm_metrics,\n        metrics_logging_period=self._config.metrics_logging_period,\n        pmap_axis_name=self._config.pmap_axis_name,\n        obs_normalization_fns=obs_normalization_fns,\n    )",
  "def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: actor_core_lib.FeedForwardPolicyWithExtra,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> core.Actor:\n    assert variable_source is not None\n    actor_core = actor_core_lib.batched_feed_forward_with_extras_to_actor_core(\n        policy)\n    if self._config.obs_normalization_fns_factory is not None:\n      variable_client = variable_utils.VariableClient(\n          variable_source, ['params', 'obs_normalization_params'],\n          device='cpu',\n          update_period=self._config.variable_update_period)\n      obs_normalization_fns = self._config.obs_normalization_fns_factory(\n          environment_spec.observations)\n      actor = normalization.NormalizedGenericActor(\n          actor_core,\n          obs_normalization_fns,\n          random_key,\n          variable_client,\n          adder,\n          jit=True,\n          backend='cpu',\n          per_episode_update=False,\n      )\n    else:\n      variable_client = variable_utils.VariableClient(\n          variable_source,\n          'params',\n          device='cpu',\n          update_period=self._config.variable_update_period)\n      actor = actors.GenericActor(\n          actor_core, random_key, variable_client, adder, backend='cpu')\n    return actor",
  "def make_policy(\n      self,\n      networks: ppo_networks.PPONetworks,\n      environment_spec: specs.EnvironmentSpec,\n      evaluation: bool = False) -> actor_core_lib.FeedForwardPolicyWithExtra:\n    del environment_spec\n    return ppo_networks.make_inference_fn(networks, evaluation)",
  "class NormalizationFns:\n  \"\"\"Holds pure functions for normalization.\n\n  Attributes:\n    init: A pure function: ``params = init()``\n    normalize: A pure function: ``norm_x = normalize(x, params)``\n    update: A pure function: ``params = update(params, x, pmap_axis_name)``\n  \"\"\"\n  # Returns the initial parameters for the normalization utility.\n  init: Callable[[], NormalizationParams]\n  # Returns the normalized input nested array.\n  normalize: Callable[[types.NestedArray, NormalizationParams],\n                      types.NestedArray]\n  # Returns updates normalization parameters.\n  update: Callable[[NormalizationParams, types.NestedArray, Optional[str]],\n                   NormalizationParams]",
  "class NormalizedGenericActor(actors.GenericActor[actor_core.State,\n                                                 actor_core.Extras],\n                             Generic[actor_core.State, actor_core.Extras]):\n  \"\"\"A GenericActor that uses observation normalization.\"\"\"\n\n  def __init__(self,\n               actor: actor_core.ActorCore[actor_core.State, actor_core.Extras],\n               normalization_fns: NormalizationFns,\n               random_key: network_lib.PRNGKey,\n               variable_client: Optional[variable_utils.VariableClient],\n               adder: Optional[adders.Adder] = None,\n               jit: bool = True,\n               backend: Optional[str] = 'cpu',\n               per_episode_update: bool = False):\n    \"\"\"Initializes a feed forward actor.\n\n    Args:\n      actor: actor core.\n      normalization_fns: Function that are used for normalizing observations.\n      random_key: Random key.\n      variable_client: The variable client to get policy and observation\n        normalization parameters from. The variable client should be defined to\n        provide [policy_params, obs_norm_params].\n      adder: An adder to add experiences to.\n      jit: Whether or not to jit the ActorCore and normalization functions.\n      backend: Which backend to use when jitting.\n      per_episode_update: if True, updates variable client params once at the\n        beginning of each episode\n    \"\"\"\n    super().__init__(actor, random_key, variable_client, adder, jit, backend,\n                     per_episode_update)\n    if jit:\n      self._apply_normalization = jax.jit(\n          normalization_fns.normalize, backend=backend)\n    else:\n      self._apply_normalization = normalization_fns.normalize\n\n  def select_action(self,\n                    observation: network_lib.Observation) -> types.NestedArray:\n    policy_params, obs_norm_params = tuple(self._params)\n    observation = self._apply_normalization(observation, obs_norm_params)\n    action, self._state = self._policy(policy_params, observation, self._state)\n    return utils.to_numpy(action)",
  "class EMAMeanStdNormalizerParams(NamedTuple):\n  \"\"\"Using technique form Adam optimizer paper for computing running stats.\"\"\"\n  ema_counter: jnp.int32\n  biased_first_moment: types.NestedArray\n  biased_second_moment: types.NestedArray",
  "def build_ema_mean_std_normalizer(\n    nested_spec: types.NestedSpec,\n    tau: float = 0.995,\n    epsilon: float = 1e-6,) -> NormalizationFns:\n  \"\"\"Builds pure functions used for normalizing based on EMA mean and std.\n\n  The built normalizer functions can be used to normalize nested arrays that\n  have a structure corresponding to nested_spec. Currently only supports\n  nested_spec where all leafs have float dtype.\n\n  Arguments:\n    nested_spec: A nested spec where all leaves have float dtype\n    tau: tau parameter for exponential moving average\n    epsilon: epsilon for avoiding division by zero std\n\n  Returns:\n    NormalizationFns to be used for normalization\n  \"\"\"\n  nested_dims = jax.tree_util.tree_map(lambda x: len(x.shape), nested_spec)\n\n  def init() -> EMAMeanStdNormalizerParams:\n    first_moment = utils.zeros_like(nested_spec)\n    second_moment = utils.zeros_like(nested_spec)\n\n    return EMAMeanStdNormalizerParams(\n        ema_counter=jnp.int32(0),\n        biased_first_moment=first_moment,\n        biased_second_moment=second_moment,\n    )\n\n  def _normalize_leaf(\n      x: jnp.ndarray,\n      ema_counter: jnp.int32,\n      biased_first_moment: jnp.ndarray,\n      biased_second_moment: jnp.ndarray,\n  ) -> jnp.ndarray:\n    zero_debias = 1. / (1. - jnp.power(tau, ema_counter))\n    mean = biased_first_moment * zero_debias\n    second_moment = biased_second_moment * zero_debias\n    std = jnp.sqrt(jax.nn.relu(second_moment - mean**2))\n\n    mean = jnp.broadcast_to(mean, x.shape)\n    std = jnp.broadcast_to(std, x.shape)\n    return (x - mean) / jnp.fmax(std, epsilon)\n\n  def _normalize(nested_array: types.NestedArray,\n                 params: EMAMeanStdNormalizerParams) -> types.NestedArray:\n    ema_counter = params.ema_counter\n    normalized_nested_array = jax.tree_util.tree_map(\n        lambda x, f, s: _normalize_leaf(x, ema_counter, f, s),\n        nested_array,\n        params.biased_first_moment,\n        params.biased_second_moment)\n    return normalized_nested_array\n\n  def normalize(nested_array: types.NestedArray,\n                params: EMAMeanStdNormalizerParams) -> types.NestedArray:\n    ema_counter = params.ema_counter\n    norm_obs = jax.lax.cond(\n        ema_counter > 0,\n        _normalize,\n        lambda o, p: o,\n        nested_array, params)\n    return norm_obs\n\n  def _compute_first_moment(x: jnp.ndarray, ndim: int):\n    reduce_axes = tuple(range(len(x.shape) - ndim))\n    first_moment = jnp.mean(x, axis=reduce_axes)\n    return first_moment\n\n  def _compute_second_moment(x: jnp.ndarray, ndim: int):\n    reduce_axes = tuple(range(len(x.shape) - ndim))\n    second_moment = jnp.mean(x**2, axis=reduce_axes)\n    return second_moment\n\n  def update(\n      params: EMAMeanStdNormalizerParams,\n      nested_array: types.NestedArray,\n      pmap_axis_name: Optional[str] = None) -> EMAMeanStdNormalizerParams:\n    # compute the stats\n    first_moment = jax.tree_util.tree_map(\n        _compute_first_moment, nested_array, nested_dims)\n    second_moment = jax.tree_util.tree_map(\n        _compute_second_moment, nested_array, nested_dims)\n\n    # propagate across devices\n    if pmap_axis_name is not None:\n      first_moment, second_moment = jax.lax.pmean(\n          (first_moment, second_moment), axis_name=pmap_axis_name)\n\n    # update running statistics\n    new_first_moment = jax.tree_util.tree_map(\n        lambda x, y: tau * x +  # pylint: disable=g-long-lambda\n        (1. - tau) * y,\n        params.biased_first_moment,\n        first_moment)\n    new_second_moment = jax.tree_util.tree_map(\n        lambda x, y: tau * x +  # pylint: disable=g-long-lambda\n        (1. - tau) * y,\n        params.biased_second_moment,\n        second_moment)\n\n    # update ema_counter and return updated params\n    new_params = EMAMeanStdNormalizerParams(\n        ema_counter=params.ema_counter + 1,\n        biased_first_moment=new_first_moment,\n        biased_second_moment=new_second_moment,\n    )\n\n    return new_params\n\n  return NormalizationFns(\n      init=init,\n      normalize=normalize,\n      update=update,\n  )",
  "def build_mean_std_normalizer(\n    nested_spec: types.NestedSpec,\n    max_abs_value: Optional[float] = None) -> NormalizationFns:\n  \"\"\"Builds pure functions used for normalizing based on mean and std.\n\n  Arguments:\n    nested_spec: A nested spec where all leaves have float dtype\n    max_abs_value: Normalized nested arrays will be clipped so that all values\n      will be between -max_abs_value and +max_abs_value. Setting to None\n      (default) does not perform this clipping.\n\n  Returns:\n    NormalizationFns to be used for normalization\n  \"\"\"\n\n  def init() -> RunningStatisticsState:\n    return running_statistics.init_state(nested_spec)\n\n  def normalize(\n      nested_array: types.NestedArray,\n      params: RunningStatisticsState) -> types.NestedArray:\n    return running_statistics.normalize(\n        nested_array, params, max_abs_value=max_abs_value)\n\n  def update(\n      params: RunningStatisticsState,\n      nested_array: types.NestedArray,\n      pmap_axis_name: Optional[str]) -> RunningStatisticsState:\n    return running_statistics.update(\n        params, nested_array, pmap_axis_name=pmap_axis_name)\n\n  return NormalizationFns(\n      init=init,\n      normalize=normalize,\n      update=update)",
  "def __init__(self,\n               actor: actor_core.ActorCore[actor_core.State, actor_core.Extras],\n               normalization_fns: NormalizationFns,\n               random_key: network_lib.PRNGKey,\n               variable_client: Optional[variable_utils.VariableClient],\n               adder: Optional[adders.Adder] = None,\n               jit: bool = True,\n               backend: Optional[str] = 'cpu',\n               per_episode_update: bool = False):\n    \"\"\"Initializes a feed forward actor.\n\n    Args:\n      actor: actor core.\n      normalization_fns: Function that are used for normalizing observations.\n      random_key: Random key.\n      variable_client: The variable client to get policy and observation\n        normalization parameters from. The variable client should be defined to\n        provide [policy_params, obs_norm_params].\n      adder: An adder to add experiences to.\n      jit: Whether or not to jit the ActorCore and normalization functions.\n      backend: Which backend to use when jitting.\n      per_episode_update: if True, updates variable client params once at the\n        beginning of each episode\n    \"\"\"\n    super().__init__(actor, random_key, variable_client, adder, jit, backend,\n                     per_episode_update)\n    if jit:\n      self._apply_normalization = jax.jit(\n          normalization_fns.normalize, backend=backend)\n    else:\n      self._apply_normalization = normalization_fns.normalize",
  "def select_action(self,\n                    observation: network_lib.Observation) -> types.NestedArray:\n    policy_params, obs_norm_params = tuple(self._params)\n    observation = self._apply_normalization(observation, obs_norm_params)\n    action, self._state = self._policy(policy_params, observation, self._state)\n    return utils.to_numpy(action)",
  "def init() -> EMAMeanStdNormalizerParams:\n    first_moment = utils.zeros_like(nested_spec)\n    second_moment = utils.zeros_like(nested_spec)\n\n    return EMAMeanStdNormalizerParams(\n        ema_counter=jnp.int32(0),\n        biased_first_moment=first_moment,\n        biased_second_moment=second_moment,\n    )",
  "def _normalize_leaf(\n      x: jnp.ndarray,\n      ema_counter: jnp.int32,\n      biased_first_moment: jnp.ndarray,\n      biased_second_moment: jnp.ndarray,\n  ) -> jnp.ndarray:\n    zero_debias = 1. / (1. - jnp.power(tau, ema_counter))\n    mean = biased_first_moment * zero_debias\n    second_moment = biased_second_moment * zero_debias\n    std = jnp.sqrt(jax.nn.relu(second_moment - mean**2))\n\n    mean = jnp.broadcast_to(mean, x.shape)\n    std = jnp.broadcast_to(std, x.shape)\n    return (x - mean) / jnp.fmax(std, epsilon)",
  "def _normalize(nested_array: types.NestedArray,\n                 params: EMAMeanStdNormalizerParams) -> types.NestedArray:\n    ema_counter = params.ema_counter\n    normalized_nested_array = jax.tree_util.tree_map(\n        lambda x, f, s: _normalize_leaf(x, ema_counter, f, s),\n        nested_array,\n        params.biased_first_moment,\n        params.biased_second_moment)\n    return normalized_nested_array",
  "def normalize(nested_array: types.NestedArray,\n                params: EMAMeanStdNormalizerParams) -> types.NestedArray:\n    ema_counter = params.ema_counter\n    norm_obs = jax.lax.cond(\n        ema_counter > 0,\n        _normalize,\n        lambda o, p: o,\n        nested_array, params)\n    return norm_obs",
  "def _compute_first_moment(x: jnp.ndarray, ndim: int):\n    reduce_axes = tuple(range(len(x.shape) - ndim))\n    first_moment = jnp.mean(x, axis=reduce_axes)\n    return first_moment",
  "def _compute_second_moment(x: jnp.ndarray, ndim: int):\n    reduce_axes = tuple(range(len(x.shape) - ndim))\n    second_moment = jnp.mean(x**2, axis=reduce_axes)\n    return second_moment",
  "def update(\n      params: EMAMeanStdNormalizerParams,\n      nested_array: types.NestedArray,\n      pmap_axis_name: Optional[str] = None) -> EMAMeanStdNormalizerParams:\n    # compute the stats\n    first_moment = jax.tree_util.tree_map(\n        _compute_first_moment, nested_array, nested_dims)\n    second_moment = jax.tree_util.tree_map(\n        _compute_second_moment, nested_array, nested_dims)\n\n    # propagate across devices\n    if pmap_axis_name is not None:\n      first_moment, second_moment = jax.lax.pmean(\n          (first_moment, second_moment), axis_name=pmap_axis_name)\n\n    # update running statistics\n    new_first_moment = jax.tree_util.tree_map(\n        lambda x, y: tau * x +  # pylint: disable=g-long-lambda\n        (1. - tau) * y,\n        params.biased_first_moment,\n        first_moment)\n    new_second_moment = jax.tree_util.tree_map(\n        lambda x, y: tau * x +  # pylint: disable=g-long-lambda\n        (1. - tau) * y,\n        params.biased_second_moment,\n        second_moment)\n\n    # update ema_counter and return updated params\n    new_params = EMAMeanStdNormalizerParams(\n        ema_counter=params.ema_counter + 1,\n        biased_first_moment=new_first_moment,\n        biased_second_moment=new_second_moment,\n    )\n\n    return new_params",
  "def init() -> RunningStatisticsState:\n    return running_statistics.init_state(nested_spec)",
  "def normalize(\n      nested_array: types.NestedArray,\n      params: RunningStatisticsState) -> types.NestedArray:\n    return running_statistics.normalize(\n        nested_array, params, max_abs_value=max_abs_value)",
  "def update(\n      params: RunningStatisticsState,\n      nested_array: types.NestedArray,\n      pmap_axis_name: Optional[str]) -> RunningStatisticsState:\n    return running_statistics.update(\n        params, nested_array, pmap_axis_name=pmap_axis_name)",
  "class TrainingState(NamedTuple):\n  \"\"\"Contains training state for the learner.\"\"\"\n  policy_params: networks_lib.Params\n  target_policy_params: networks_lib.Params\n  critic_params: networks_lib.Params\n  target_critic_params: networks_lib.Params\n  twin_critic_params: networks_lib.Params\n  target_twin_critic_params: networks_lib.Params\n  policy_opt_state: optax.OptState\n  critic_opt_state: optax.OptState\n  twin_critic_opt_state: optax.OptState\n  steps: int\n  random_key: networks_lib.PRNGKey",
  "class TD3Learner(acme.Learner):\n  \"\"\"TD3 learner.\"\"\"\n\n  _state: TrainingState\n\n  def __init__(self,\n               networks: td3_networks.TD3Networks,\n               random_key: networks_lib.PRNGKey,\n               discount: float,\n               iterator: Iterator[reverb.ReplaySample],\n               policy_optimizer: optax.GradientTransformation,\n               critic_optimizer: optax.GradientTransformation,\n               twin_critic_optimizer: optax.GradientTransformation,\n               delay: int = 2,\n               target_sigma: float = 0.2,\n               noise_clip: float = 0.5,\n               tau: float = 0.005,\n               use_sarsa_target: bool = False,\n               bc_alpha: Optional[float] = None,\n               counter: Optional[counting.Counter] = None,\n               logger: Optional[loggers.Logger] = None,\n               num_sgd_steps_per_step: int = 1):\n    \"\"\"Initializes the TD3 learner.\n\n    Args:\n      networks: TD3 networks.\n      random_key: a key for random number generation.\n      discount: discount to use for TD updates\n      iterator: an iterator over training data.\n      policy_optimizer: the policy optimizer.\n      critic_optimizer: the Q-function optimizer.\n      twin_critic_optimizer: the twin Q-function optimizer.\n      delay: ratio of policy updates for critic updates (see TD3),\n        delay=2 means 2 updates of the critic for 1 policy update.\n      target_sigma: std of zero mean Gaussian added to the action of\n        the next_state, for critic evaluation (reducing overestimation bias).\n      noise_clip: hard constraint on target noise.\n      tau: target parameters smoothing coefficient.\n      use_sarsa_target: compute on-policy target using iterator's actions rather\n        than sampled actions.\n        Useful for 1-step offline RL (https://arxiv.org/pdf/2106.08909.pdf).\n        When set to `True`, `target_policy_params` are unused.\n        This is only working when the learner is used as an offline algorithm.\n        I.e. TD3Builder does not support adding the SARSA target to the replay\n        buffer.\n      bc_alpha: bc_alpha: Implements TD3+BC.\n        See comments in TD3Config.bc_alpha for details.\n      counter: counter object used to keep track of steps.\n      logger: logger object to be used by learner.\n      num_sgd_steps_per_step: number of sgd steps to perform per learner 'step'.\n    \"\"\"\n\n    def policy_loss(\n        policy_params: networks_lib.Params,\n        critic_params: networks_lib.Params,\n        transition: types.NestedArray,\n    ) -> jnp.ndarray:\n      # Computes the discrete policy gradient loss.\n      action = networks.policy_network.apply(\n          policy_params, transition.observation)\n      grad_critic = jax.vmap(\n          jax.grad(networks.critic_network.apply, argnums=2),\n          in_axes=(None, 0, 0))\n      dq_da = grad_critic(critic_params, transition.observation, action)\n      batch_dpg_learning = jax.vmap(rlax.dpg_loss, in_axes=(0, 0))\n      loss = jnp.mean(batch_dpg_learning(action, dq_da))\n      if bc_alpha is not None:\n        # BC regularization for offline RL\n        q_sa = networks.critic_network.apply(critic_params,\n                                             transition.observation, action)\n        bc_factor = jax.lax.stop_gradient(bc_alpha / jnp.mean(jnp.abs(q_sa)))\n        loss += jnp.mean(jnp.square(action - transition.action)) / bc_factor\n      return loss\n\n    def critic_loss(\n        critic_params: networks_lib.Params,\n        state: TrainingState,\n        transition: types.Transition,\n        random_key: jnp.ndarray,\n    ):\n      # Computes the critic loss.\n      q_tm1 = networks.critic_network.apply(\n          critic_params, transition.observation, transition.action)\n\n      if use_sarsa_target:\n        # TODO(b/222674779): use N-steps Trajectories to get the next actions.\n        assert 'next_action' in transition.extras, (\n            'next actions should be given as extras for one step RL.')\n        action = transition.extras['next_action']\n      else:\n        action = networks.policy_network.apply(state.target_policy_params,\n                                               transition.next_observation)\n        action = networks.add_policy_noise(action, random_key,\n                                           target_sigma, noise_clip)\n\n      q_t = networks.critic_network.apply(\n          state.target_critic_params,\n          transition.next_observation,\n          action)\n      twin_q_t = networks.twin_critic_network.apply(\n          state.target_twin_critic_params,\n          transition.next_observation,\n          action)\n\n      q_t = jnp.minimum(q_t, twin_q_t)\n\n      target_q_tm1 = transition.reward + discount * transition.discount * q_t\n      td_error = jax.lax.stop_gradient(target_q_tm1) - q_tm1\n\n      return jnp.mean(jnp.square(td_error))\n\n    def update_step(\n        state: TrainingState,\n        transitions: types.Transition,\n    ) -> Tuple[TrainingState, Dict[str, jnp.ndarray]]:\n\n      random_key, key_critic, key_twin = jax.random.split(state.random_key, 3)\n\n      # Updates on the critic: compute the gradients, and update using\n      # Polyak averaging.\n      critic_loss_and_grad = jax.value_and_grad(critic_loss)\n      critic_loss_value, critic_gradients = critic_loss_and_grad(\n          state.critic_params, state, transitions, key_critic)\n      critic_updates, critic_opt_state = critic_optimizer.update(\n          critic_gradients, state.critic_opt_state)\n      critic_params = optax.apply_updates(state.critic_params, critic_updates)\n      # In the original authors' implementation the critic target update is\n      # delayed similarly to the policy update which we found empirically to\n      # perform slightly worse.\n      target_critic_params = optax.incremental_update(\n          new_tensors=critic_params,\n          old_tensors=state.target_critic_params,\n          step_size=tau)\n\n      # Updates on the twin critic: compute the gradients, and update using\n      # Polyak averaging.\n      twin_critic_loss_value, twin_critic_gradients = critic_loss_and_grad(\n          state.twin_critic_params, state, transitions, key_twin)\n      twin_critic_updates, twin_critic_opt_state = twin_critic_optimizer.update(\n          twin_critic_gradients, state.twin_critic_opt_state)\n      twin_critic_params = optax.apply_updates(state.twin_critic_params,\n                                               twin_critic_updates)\n      # In the original authors' implementation the twin critic target update is\n      # delayed similarly to the policy update which we found empirically to\n      # perform slightly worse.\n      target_twin_critic_params = optax.incremental_update(\n          new_tensors=twin_critic_params,\n          old_tensors=state.target_twin_critic_params,\n          step_size=tau)\n\n      # Updates on the policy: compute the gradients, and update using\n      # Polyak averaging (if delay enabled, the update might not be applied).\n      policy_loss_and_grad = jax.value_and_grad(policy_loss)\n      policy_loss_value, policy_gradients = policy_loss_and_grad(\n          state.policy_params, state.critic_params, transitions)\n      def update_policy_step():\n        policy_updates, policy_opt_state = policy_optimizer.update(\n            policy_gradients, state.policy_opt_state)\n        policy_params = optax.apply_updates(state.policy_params, policy_updates)\n        target_policy_params = optax.incremental_update(\n            new_tensors=policy_params,\n            old_tensors=state.target_policy_params,\n            step_size=tau)\n        return policy_params, target_policy_params, policy_opt_state\n\n      # The update on the policy is applied every `delay` steps.\n      current_policy_state = (state.policy_params, state.target_policy_params,\n                              state.policy_opt_state)\n      policy_params, target_policy_params, policy_opt_state = jax.lax.cond(\n          state.steps % delay == 0,\n          lambda _: update_policy_step(),\n          lambda _: current_policy_state,\n          operand=None)\n\n      steps = state.steps + 1\n\n      new_state = TrainingState(\n          policy_params=policy_params,\n          critic_params=critic_params,\n          twin_critic_params=twin_critic_params,\n          target_policy_params=target_policy_params,\n          target_critic_params=target_critic_params,\n          target_twin_critic_params=target_twin_critic_params,\n          policy_opt_state=policy_opt_state,\n          critic_opt_state=critic_opt_state,\n          twin_critic_opt_state=twin_critic_opt_state,\n          steps=steps,\n          random_key=random_key,\n      )\n\n      metrics = {\n          'policy_loss': policy_loss_value,\n          'critic_loss': critic_loss_value,\n          'twin_critic_loss': twin_critic_loss_value,\n      }\n\n      return new_state, metrics\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger(\n        'learner',\n        asynchronous=True,\n        serialize_fn=utils.fetch_devicearray,\n        steps_key=self._counter.get_steps_key())\n\n    # Create prefetching dataset iterator.\n    self._iterator = iterator\n\n    # Faster sgd step\n    update_step = utils.process_multiple_batches(update_step,\n                                                 num_sgd_steps_per_step)\n    # Use the JIT compiler.\n    self._update_step = jax.jit(update_step)\n\n    (key_init_policy, key_init_twin, key_init_target, key_state\n     ) = jax.random.split(random_key, 4)\n    # Create the network parameters and copy into the target network parameters.\n    initial_policy_params = networks.policy_network.init(key_init_policy)\n    initial_critic_params = networks.critic_network.init(key_init_twin)\n    initial_twin_critic_params = networks.twin_critic_network.init(\n        key_init_target)\n\n    initial_target_policy_params = initial_policy_params\n    initial_target_critic_params = initial_critic_params\n    initial_target_twin_critic_params = initial_twin_critic_params\n\n    # Initialize optimizers.\n    initial_policy_opt_state = policy_optimizer.init(initial_policy_params)\n    initial_critic_opt_state = critic_optimizer.init(initial_critic_params)\n    initial_twin_critic_opt_state = twin_critic_optimizer.init(\n        initial_twin_critic_params)\n\n    # Create initial state.\n    self._state = TrainingState(\n        policy_params=initial_policy_params,\n        target_policy_params=initial_target_policy_params,\n        critic_params=initial_critic_params,\n        twin_critic_params=initial_twin_critic_params,\n        target_critic_params=initial_target_critic_params,\n        target_twin_critic_params=initial_target_twin_critic_params,\n        policy_opt_state=initial_policy_opt_state,\n        critic_opt_state=initial_critic_opt_state,\n        twin_critic_opt_state=initial_twin_critic_opt_state,\n        steps=0,\n        random_key=key_state\n    )\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None\n\n  def step(self):\n    # Get data from replay (dropping extras if any). Note there is no\n    # extra data here because we do not insert any into Reverb.\n    sample = next(self._iterator)\n    transitions = types.Transition(*sample.data)\n\n    self._state, metrics = self._update_step(self._state, transitions)\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Increment counts and record the current time\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n\n    # Attempts to write the logs.\n    self._logger.write({**metrics, **counts})\n\n  def get_variables(self, names: List[str]) -> List[networks_lib.Params]:\n    variables = {\n        'policy': self._state.policy_params,\n        'critic': self._state.critic_params,\n        'twin_critic': self._state.twin_critic_params,\n    }\n    return [variables[name] for name in names]\n\n  def save(self) -> TrainingState:\n    return self._state\n\n  def restore(self, state: TrainingState):\n    self._state = state",
  "def __init__(self,\n               networks: td3_networks.TD3Networks,\n               random_key: networks_lib.PRNGKey,\n               discount: float,\n               iterator: Iterator[reverb.ReplaySample],\n               policy_optimizer: optax.GradientTransformation,\n               critic_optimizer: optax.GradientTransformation,\n               twin_critic_optimizer: optax.GradientTransformation,\n               delay: int = 2,\n               target_sigma: float = 0.2,\n               noise_clip: float = 0.5,\n               tau: float = 0.005,\n               use_sarsa_target: bool = False,\n               bc_alpha: Optional[float] = None,\n               counter: Optional[counting.Counter] = None,\n               logger: Optional[loggers.Logger] = None,\n               num_sgd_steps_per_step: int = 1):\n    \"\"\"Initializes the TD3 learner.\n\n    Args:\n      networks: TD3 networks.\n      random_key: a key for random number generation.\n      discount: discount to use for TD updates\n      iterator: an iterator over training data.\n      policy_optimizer: the policy optimizer.\n      critic_optimizer: the Q-function optimizer.\n      twin_critic_optimizer: the twin Q-function optimizer.\n      delay: ratio of policy updates for critic updates (see TD3),\n        delay=2 means 2 updates of the critic for 1 policy update.\n      target_sigma: std of zero mean Gaussian added to the action of\n        the next_state, for critic evaluation (reducing overestimation bias).\n      noise_clip: hard constraint on target noise.\n      tau: target parameters smoothing coefficient.\n      use_sarsa_target: compute on-policy target using iterator's actions rather\n        than sampled actions.\n        Useful for 1-step offline RL (https://arxiv.org/pdf/2106.08909.pdf).\n        When set to `True`, `target_policy_params` are unused.\n        This is only working when the learner is used as an offline algorithm.\n        I.e. TD3Builder does not support adding the SARSA target to the replay\n        buffer.\n      bc_alpha: bc_alpha: Implements TD3+BC.\n        See comments in TD3Config.bc_alpha for details.\n      counter: counter object used to keep track of steps.\n      logger: logger object to be used by learner.\n      num_sgd_steps_per_step: number of sgd steps to perform per learner 'step'.\n    \"\"\"\n\n    def policy_loss(\n        policy_params: networks_lib.Params,\n        critic_params: networks_lib.Params,\n        transition: types.NestedArray,\n    ) -> jnp.ndarray:\n      # Computes the discrete policy gradient loss.\n      action = networks.policy_network.apply(\n          policy_params, transition.observation)\n      grad_critic = jax.vmap(\n          jax.grad(networks.critic_network.apply, argnums=2),\n          in_axes=(None, 0, 0))\n      dq_da = grad_critic(critic_params, transition.observation, action)\n      batch_dpg_learning = jax.vmap(rlax.dpg_loss, in_axes=(0, 0))\n      loss = jnp.mean(batch_dpg_learning(action, dq_da))\n      if bc_alpha is not None:\n        # BC regularization for offline RL\n        q_sa = networks.critic_network.apply(critic_params,\n                                             transition.observation, action)\n        bc_factor = jax.lax.stop_gradient(bc_alpha / jnp.mean(jnp.abs(q_sa)))\n        loss += jnp.mean(jnp.square(action - transition.action)) / bc_factor\n      return loss\n\n    def critic_loss(\n        critic_params: networks_lib.Params,\n        state: TrainingState,\n        transition: types.Transition,\n        random_key: jnp.ndarray,\n    ):\n      # Computes the critic loss.\n      q_tm1 = networks.critic_network.apply(\n          critic_params, transition.observation, transition.action)\n\n      if use_sarsa_target:\n        # TODO(b/222674779): use N-steps Trajectories to get the next actions.\n        assert 'next_action' in transition.extras, (\n            'next actions should be given as extras for one step RL.')\n        action = transition.extras['next_action']\n      else:\n        action = networks.policy_network.apply(state.target_policy_params,\n                                               transition.next_observation)\n        action = networks.add_policy_noise(action, random_key,\n                                           target_sigma, noise_clip)\n\n      q_t = networks.critic_network.apply(\n          state.target_critic_params,\n          transition.next_observation,\n          action)\n      twin_q_t = networks.twin_critic_network.apply(\n          state.target_twin_critic_params,\n          transition.next_observation,\n          action)\n\n      q_t = jnp.minimum(q_t, twin_q_t)\n\n      target_q_tm1 = transition.reward + discount * transition.discount * q_t\n      td_error = jax.lax.stop_gradient(target_q_tm1) - q_tm1\n\n      return jnp.mean(jnp.square(td_error))\n\n    def update_step(\n        state: TrainingState,\n        transitions: types.Transition,\n    ) -> Tuple[TrainingState, Dict[str, jnp.ndarray]]:\n\n      random_key, key_critic, key_twin = jax.random.split(state.random_key, 3)\n\n      # Updates on the critic: compute the gradients, and update using\n      # Polyak averaging.\n      critic_loss_and_grad = jax.value_and_grad(critic_loss)\n      critic_loss_value, critic_gradients = critic_loss_and_grad(\n          state.critic_params, state, transitions, key_critic)\n      critic_updates, critic_opt_state = critic_optimizer.update(\n          critic_gradients, state.critic_opt_state)\n      critic_params = optax.apply_updates(state.critic_params, critic_updates)\n      # In the original authors' implementation the critic target update is\n      # delayed similarly to the policy update which we found empirically to\n      # perform slightly worse.\n      target_critic_params = optax.incremental_update(\n          new_tensors=critic_params,\n          old_tensors=state.target_critic_params,\n          step_size=tau)\n\n      # Updates on the twin critic: compute the gradients, and update using\n      # Polyak averaging.\n      twin_critic_loss_value, twin_critic_gradients = critic_loss_and_grad(\n          state.twin_critic_params, state, transitions, key_twin)\n      twin_critic_updates, twin_critic_opt_state = twin_critic_optimizer.update(\n          twin_critic_gradients, state.twin_critic_opt_state)\n      twin_critic_params = optax.apply_updates(state.twin_critic_params,\n                                               twin_critic_updates)\n      # In the original authors' implementation the twin critic target update is\n      # delayed similarly to the policy update which we found empirically to\n      # perform slightly worse.\n      target_twin_critic_params = optax.incremental_update(\n          new_tensors=twin_critic_params,\n          old_tensors=state.target_twin_critic_params,\n          step_size=tau)\n\n      # Updates on the policy: compute the gradients, and update using\n      # Polyak averaging (if delay enabled, the update might not be applied).\n      policy_loss_and_grad = jax.value_and_grad(policy_loss)\n      policy_loss_value, policy_gradients = policy_loss_and_grad(\n          state.policy_params, state.critic_params, transitions)\n      def update_policy_step():\n        policy_updates, policy_opt_state = policy_optimizer.update(\n            policy_gradients, state.policy_opt_state)\n        policy_params = optax.apply_updates(state.policy_params, policy_updates)\n        target_policy_params = optax.incremental_update(\n            new_tensors=policy_params,\n            old_tensors=state.target_policy_params,\n            step_size=tau)\n        return policy_params, target_policy_params, policy_opt_state\n\n      # The update on the policy is applied every `delay` steps.\n      current_policy_state = (state.policy_params, state.target_policy_params,\n                              state.policy_opt_state)\n      policy_params, target_policy_params, policy_opt_state = jax.lax.cond(\n          state.steps % delay == 0,\n          lambda _: update_policy_step(),\n          lambda _: current_policy_state,\n          operand=None)\n\n      steps = state.steps + 1\n\n      new_state = TrainingState(\n          policy_params=policy_params,\n          critic_params=critic_params,\n          twin_critic_params=twin_critic_params,\n          target_policy_params=target_policy_params,\n          target_critic_params=target_critic_params,\n          target_twin_critic_params=target_twin_critic_params,\n          policy_opt_state=policy_opt_state,\n          critic_opt_state=critic_opt_state,\n          twin_critic_opt_state=twin_critic_opt_state,\n          steps=steps,\n          random_key=random_key,\n      )\n\n      metrics = {\n          'policy_loss': policy_loss_value,\n          'critic_loss': critic_loss_value,\n          'twin_critic_loss': twin_critic_loss_value,\n      }\n\n      return new_state, metrics\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger(\n        'learner',\n        asynchronous=True,\n        serialize_fn=utils.fetch_devicearray,\n        steps_key=self._counter.get_steps_key())\n\n    # Create prefetching dataset iterator.\n    self._iterator = iterator\n\n    # Faster sgd step\n    update_step = utils.process_multiple_batches(update_step,\n                                                 num_sgd_steps_per_step)\n    # Use the JIT compiler.\n    self._update_step = jax.jit(update_step)\n\n    (key_init_policy, key_init_twin, key_init_target, key_state\n     ) = jax.random.split(random_key, 4)\n    # Create the network parameters and copy into the target network parameters.\n    initial_policy_params = networks.policy_network.init(key_init_policy)\n    initial_critic_params = networks.critic_network.init(key_init_twin)\n    initial_twin_critic_params = networks.twin_critic_network.init(\n        key_init_target)\n\n    initial_target_policy_params = initial_policy_params\n    initial_target_critic_params = initial_critic_params\n    initial_target_twin_critic_params = initial_twin_critic_params\n\n    # Initialize optimizers.\n    initial_policy_opt_state = policy_optimizer.init(initial_policy_params)\n    initial_critic_opt_state = critic_optimizer.init(initial_critic_params)\n    initial_twin_critic_opt_state = twin_critic_optimizer.init(\n        initial_twin_critic_params)\n\n    # Create initial state.\n    self._state = TrainingState(\n        policy_params=initial_policy_params,\n        target_policy_params=initial_target_policy_params,\n        critic_params=initial_critic_params,\n        twin_critic_params=initial_twin_critic_params,\n        target_critic_params=initial_target_critic_params,\n        target_twin_critic_params=initial_target_twin_critic_params,\n        policy_opt_state=initial_policy_opt_state,\n        critic_opt_state=initial_critic_opt_state,\n        twin_critic_opt_state=initial_twin_critic_opt_state,\n        steps=0,\n        random_key=key_state\n    )\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None",
  "def step(self):\n    # Get data from replay (dropping extras if any). Note there is no\n    # extra data here because we do not insert any into Reverb.\n    sample = next(self._iterator)\n    transitions = types.Transition(*sample.data)\n\n    self._state, metrics = self._update_step(self._state, transitions)\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Increment counts and record the current time\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n\n    # Attempts to write the logs.\n    self._logger.write({**metrics, **counts})",
  "def get_variables(self, names: List[str]) -> List[networks_lib.Params]:\n    variables = {\n        'policy': self._state.policy_params,\n        'critic': self._state.critic_params,\n        'twin_critic': self._state.twin_critic_params,\n    }\n    return [variables[name] for name in names]",
  "def save(self) -> TrainingState:\n    return self._state",
  "def restore(self, state: TrainingState):\n    self._state = state",
  "def policy_loss(\n        policy_params: networks_lib.Params,\n        critic_params: networks_lib.Params,\n        transition: types.NestedArray,\n    ) -> jnp.ndarray:\n      # Computes the discrete policy gradient loss.\n      action = networks.policy_network.apply(\n          policy_params, transition.observation)\n      grad_critic = jax.vmap(\n          jax.grad(networks.critic_network.apply, argnums=2),\n          in_axes=(None, 0, 0))\n      dq_da = grad_critic(critic_params, transition.observation, action)\n      batch_dpg_learning = jax.vmap(rlax.dpg_loss, in_axes=(0, 0))\n      loss = jnp.mean(batch_dpg_learning(action, dq_da))\n      if bc_alpha is not None:\n        # BC regularization for offline RL\n        q_sa = networks.critic_network.apply(critic_params,\n                                             transition.observation, action)\n        bc_factor = jax.lax.stop_gradient(bc_alpha / jnp.mean(jnp.abs(q_sa)))\n        loss += jnp.mean(jnp.square(action - transition.action)) / bc_factor\n      return loss",
  "def critic_loss(\n        critic_params: networks_lib.Params,\n        state: TrainingState,\n        transition: types.Transition,\n        random_key: jnp.ndarray,\n    ):\n      # Computes the critic loss.\n      q_tm1 = networks.critic_network.apply(\n          critic_params, transition.observation, transition.action)\n\n      if use_sarsa_target:\n        # TODO(b/222674779): use N-steps Trajectories to get the next actions.\n        assert 'next_action' in transition.extras, (\n            'next actions should be given as extras for one step RL.')\n        action = transition.extras['next_action']\n      else:\n        action = networks.policy_network.apply(state.target_policy_params,\n                                               transition.next_observation)\n        action = networks.add_policy_noise(action, random_key,\n                                           target_sigma, noise_clip)\n\n      q_t = networks.critic_network.apply(\n          state.target_critic_params,\n          transition.next_observation,\n          action)\n      twin_q_t = networks.twin_critic_network.apply(\n          state.target_twin_critic_params,\n          transition.next_observation,\n          action)\n\n      q_t = jnp.minimum(q_t, twin_q_t)\n\n      target_q_tm1 = transition.reward + discount * transition.discount * q_t\n      td_error = jax.lax.stop_gradient(target_q_tm1) - q_tm1\n\n      return jnp.mean(jnp.square(td_error))",
  "def update_step(\n        state: TrainingState,\n        transitions: types.Transition,\n    ) -> Tuple[TrainingState, Dict[str, jnp.ndarray]]:\n\n      random_key, key_critic, key_twin = jax.random.split(state.random_key, 3)\n\n      # Updates on the critic: compute the gradients, and update using\n      # Polyak averaging.\n      critic_loss_and_grad = jax.value_and_grad(critic_loss)\n      critic_loss_value, critic_gradients = critic_loss_and_grad(\n          state.critic_params, state, transitions, key_critic)\n      critic_updates, critic_opt_state = critic_optimizer.update(\n          critic_gradients, state.critic_opt_state)\n      critic_params = optax.apply_updates(state.critic_params, critic_updates)\n      # In the original authors' implementation the critic target update is\n      # delayed similarly to the policy update which we found empirically to\n      # perform slightly worse.\n      target_critic_params = optax.incremental_update(\n          new_tensors=critic_params,\n          old_tensors=state.target_critic_params,\n          step_size=tau)\n\n      # Updates on the twin critic: compute the gradients, and update using\n      # Polyak averaging.\n      twin_critic_loss_value, twin_critic_gradients = critic_loss_and_grad(\n          state.twin_critic_params, state, transitions, key_twin)\n      twin_critic_updates, twin_critic_opt_state = twin_critic_optimizer.update(\n          twin_critic_gradients, state.twin_critic_opt_state)\n      twin_critic_params = optax.apply_updates(state.twin_critic_params,\n                                               twin_critic_updates)\n      # In the original authors' implementation the twin critic target update is\n      # delayed similarly to the policy update which we found empirically to\n      # perform slightly worse.\n      target_twin_critic_params = optax.incremental_update(\n          new_tensors=twin_critic_params,\n          old_tensors=state.target_twin_critic_params,\n          step_size=tau)\n\n      # Updates on the policy: compute the gradients, and update using\n      # Polyak averaging (if delay enabled, the update might not be applied).\n      policy_loss_and_grad = jax.value_and_grad(policy_loss)\n      policy_loss_value, policy_gradients = policy_loss_and_grad(\n          state.policy_params, state.critic_params, transitions)\n      def update_policy_step():\n        policy_updates, policy_opt_state = policy_optimizer.update(\n            policy_gradients, state.policy_opt_state)\n        policy_params = optax.apply_updates(state.policy_params, policy_updates)\n        target_policy_params = optax.incremental_update(\n            new_tensors=policy_params,\n            old_tensors=state.target_policy_params,\n            step_size=tau)\n        return policy_params, target_policy_params, policy_opt_state\n\n      # The update on the policy is applied every `delay` steps.\n      current_policy_state = (state.policy_params, state.target_policy_params,\n                              state.policy_opt_state)\n      policy_params, target_policy_params, policy_opt_state = jax.lax.cond(\n          state.steps % delay == 0,\n          lambda _: update_policy_step(),\n          lambda _: current_policy_state,\n          operand=None)\n\n      steps = state.steps + 1\n\n      new_state = TrainingState(\n          policy_params=policy_params,\n          critic_params=critic_params,\n          twin_critic_params=twin_critic_params,\n          target_policy_params=target_policy_params,\n          target_critic_params=target_critic_params,\n          target_twin_critic_params=target_twin_critic_params,\n          policy_opt_state=policy_opt_state,\n          critic_opt_state=critic_opt_state,\n          twin_critic_opt_state=twin_critic_opt_state,\n          steps=steps,\n          random_key=random_key,\n      )\n\n      metrics = {\n          'policy_loss': policy_loss_value,\n          'critic_loss': critic_loss_value,\n          'twin_critic_loss': twin_critic_loss_value,\n      }\n\n      return new_state, metrics",
  "def update_policy_step():\n        policy_updates, policy_opt_state = policy_optimizer.update(\n            policy_gradients, state.policy_opt_state)\n        policy_params = optax.apply_updates(state.policy_params, policy_updates)\n        target_policy_params = optax.incremental_update(\n            new_tensors=policy_params,\n            old_tensors=state.target_policy_params,\n            step_size=tau)\n        return policy_params, target_policy_params, policy_opt_state",
  "class TD3Networks:\n  \"\"\"Network and pure functions for the TD3 agent.\"\"\"\n  policy_network: networks_lib.FeedForwardNetwork\n  critic_network: networks_lib.FeedForwardNetwork\n  twin_critic_network: networks_lib.FeedForwardNetwork\n  add_policy_noise: Callable[[types.NestedArray, networks_lib.PRNGKey,\n                              float, float], types.NestedArray]",
  "def get_default_behavior_policy(\n    networks: TD3Networks, action_specs: specs.BoundedArray,\n    sigma: float) -> actor_core_lib.FeedForwardPolicy:\n  \"\"\"Selects action according to the policy.\"\"\"\n  def behavior_policy(params: networks_lib.Params, key: networks_lib.PRNGKey,\n                      observation: types.NestedArray):\n    action = networks.policy_network.apply(params, observation)\n    noise = jax.random.normal(key, shape=action.shape) * sigma\n    noisy_action = jnp.clip(action + noise,\n                            action_specs.minimum, action_specs.maximum)\n    return noisy_action\n  return behavior_policy",
  "def make_networks(\n    spec: specs.EnvironmentSpec,\n    hidden_layer_sizes: Sequence[int] = (256, 256)) -> TD3Networks:\n  \"\"\"Creates networks used by the agent.\n\n  The networks used are based on LayerNormMLP, which is different than the\n  MLP with relu activation described in TD3 (which empirically performs worse).\n\n  Args:\n    spec: Environment specs\n    hidden_layer_sizes: list of sizes of hidden layers in actor/critic networks\n\n  Returns:\n    network: TD3Networks\n  \"\"\"\n\n  action_specs = spec.actions\n  num_dimensions = np.prod(action_specs.shape, dtype=int)\n\n  def add_policy_noise(action: types.NestedArray,\n                       key: networks_lib.PRNGKey,\n                       target_sigma: float,\n                       noise_clip: float) -> types.NestedArray:\n    \"\"\"Adds action noise to bootstrapped Q-value estimate in critic loss.\"\"\"\n    noise = jax.random.normal(key=key, shape=action_specs.shape) * target_sigma\n    noise = jnp.clip(noise, -noise_clip, noise_clip)\n    return jnp.clip(action + noise, action_specs.minimum, action_specs.maximum)\n\n  def _actor_fn(obs: types.NestedArray) -> types.NestedArray:\n    network = hk.Sequential([\n        networks_lib.LayerNormMLP(hidden_layer_sizes,\n                                  activate_final=True),\n        networks_lib.NearZeroInitializedLinear(num_dimensions),\n        networks_lib.TanhToSpec(spec.actions),\n    ])\n    return network(obs)\n\n  def _critic_fn(obs: types.NestedArray,\n                 action: types.NestedArray) -> types.NestedArray:\n    network1 = hk.Sequential([\n        networks_lib.LayerNormMLP(list(hidden_layer_sizes) + [1]),\n    ])\n    input_ = jnp.concatenate([obs, action], axis=-1)\n    value = network1(input_)\n    return jnp.squeeze(value)\n\n  policy = hk.without_apply_rng(hk.transform(_actor_fn))\n  critic = hk.without_apply_rng(hk.transform(_critic_fn))\n\n  # Create dummy observations and actions to create network parameters.\n  dummy_action = utils.zeros_like(spec.actions)\n  dummy_obs = utils.zeros_like(spec.observations)\n  dummy_action = utils.add_batch_dim(dummy_action)\n  dummy_obs = utils.add_batch_dim(dummy_obs)\n\n  network = TD3Networks(\n      policy_network=networks_lib.FeedForwardNetwork(\n          lambda key: policy.init(key, dummy_obs), policy.apply),\n      critic_network=networks_lib.FeedForwardNetwork(\n          lambda key: critic.init(key, dummy_obs, dummy_action), critic.apply),\n      twin_critic_network=networks_lib.FeedForwardNetwork(\n          lambda key: critic.init(key, dummy_obs, dummy_action), critic.apply),\n      add_policy_noise=add_policy_noise)\n\n  return network",
  "def behavior_policy(params: networks_lib.Params, key: networks_lib.PRNGKey,\n                      observation: types.NestedArray):\n    action = networks.policy_network.apply(params, observation)\n    noise = jax.random.normal(key, shape=action.shape) * sigma\n    noisy_action = jnp.clip(action + noise,\n                            action_specs.minimum, action_specs.maximum)\n    return noisy_action",
  "def add_policy_noise(action: types.NestedArray,\n                       key: networks_lib.PRNGKey,\n                       target_sigma: float,\n                       noise_clip: float) -> types.NestedArray:\n    \"\"\"Adds action noise to bootstrapped Q-value estimate in critic loss.\"\"\"\n    noise = jax.random.normal(key=key, shape=action_specs.shape) * target_sigma\n    noise = jnp.clip(noise, -noise_clip, noise_clip)\n    return jnp.clip(action + noise, action_specs.minimum, action_specs.maximum)",
  "def _actor_fn(obs: types.NestedArray) -> types.NestedArray:\n    network = hk.Sequential([\n        networks_lib.LayerNormMLP(hidden_layer_sizes,\n                                  activate_final=True),\n        networks_lib.NearZeroInitializedLinear(num_dimensions),\n        networks_lib.TanhToSpec(spec.actions),\n    ])\n    return network(obs)",
  "def _critic_fn(obs: types.NestedArray,\n                 action: types.NestedArray) -> types.NestedArray:\n    network1 = hk.Sequential([\n        networks_lib.LayerNormMLP(list(hidden_layer_sizes) + [1]),\n    ])\n    input_ = jnp.concatenate([obs, action], axis=-1)\n    value = network1(input_)\n    return jnp.squeeze(value)",
  "class TD3Config:\n  \"\"\"Configuration options for TD3.\"\"\"\n\n  # Loss options\n  batch_size: int = 256\n  policy_learning_rate: Union[optax.Schedule, float] = 3e-4\n  critic_learning_rate: Union[optax.Schedule, float] = 3e-4\n  # Policy gradient clipping is not part of the original TD3 implementation,\n  # used e.g. in DAC https://arxiv.org/pdf/1809.02925.pdf\n  policy_gradient_clipping: Optional[float] = None\n  discount: float = 0.99\n  n_step: int = 1\n\n  # TD3 specific options (https://arxiv.org/pdf/1802.09477.pdf)\n  sigma: float = 0.1\n  delay: int = 2\n  target_sigma: float = 0.2\n  noise_clip: float = 0.5\n  tau: float = 0.005\n\n  # Replay options\n  min_replay_size: int = 1000\n  max_replay_size: int = 1000000\n  replay_table_name: str = adders_reverb.DEFAULT_PRIORITY_TABLE\n  prefetch_size: int = 4\n  samples_per_insert: float = 256\n  # Rate to be used for the SampleToInsertRatio rate limiter tolerance.\n  # See a formula in make_replay_tables for more details.\n  samples_per_insert_tolerance_rate: float = 0.1\n\n  # How many gradient updates to perform per step.\n  num_sgd_steps_per_step: int = 1\n\n  # Offline RL options\n  # if bc_alpha: if given, will add a bc regularization term to the policy loss,\n  # (https://arxiv.org/pdf/2106.06860.pdf), useful for offline training.\n  bc_alpha: Optional[float] = None",
  "class TD3Builder(builders.ActorLearnerBuilder[td3_networks.TD3Networks,\n                                              actor_core_lib.FeedForwardPolicy,\n                                              reverb.ReplaySample]):\n  \"\"\"TD3 Builder.\"\"\"\n\n  def __init__(\n      self,\n      config: td3_config.TD3Config,\n  ):\n    \"\"\"Creates a TD3 learner, a behavior policy and an eval actor.\n\n    Args:\n      config: a config with TD3 hps\n    \"\"\"\n    self._config = config\n\n  def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: td3_networks.TD3Networks,\n      dataset: Iterator[reverb.ReplaySample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    del environment_spec, replay_client\n\n    critic_optimizer = optax.adam(self._config.critic_learning_rate)\n    twin_critic_optimizer = optax.adam(self._config.critic_learning_rate)\n    policy_optimizer = optax.adam(self._config.policy_learning_rate)\n\n    if self._config.policy_gradient_clipping is not None:\n      policy_optimizer = optax.chain(\n          optax.clip_by_global_norm(self._config.policy_gradient_clipping),\n          policy_optimizer)\n\n    return learning.TD3Learner(\n        networks=networks,\n        random_key=random_key,\n        discount=self._config.discount,\n        target_sigma=self._config.target_sigma,\n        noise_clip=self._config.noise_clip,\n        policy_optimizer=policy_optimizer,\n        critic_optimizer=critic_optimizer,\n        twin_critic_optimizer=twin_critic_optimizer,\n        num_sgd_steps_per_step=self._config.num_sgd_steps_per_step,\n        bc_alpha=self._config.bc_alpha,\n        iterator=dataset,\n        logger=logger_fn('learner'),\n        counter=counter)\n\n  def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: actor_core_lib.FeedForwardPolicy,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> core.Actor:\n    del environment_spec\n    assert variable_source is not None\n    actor_core = actor_core_lib.batched_feed_forward_to_actor_core(policy)\n    # Inference happens on CPU, so it's better to move variables there too.\n    variable_client = variable_utils.VariableClient(variable_source, 'policy',\n                                                    device='cpu')\n    return actors.GenericActor(\n        actor_core, random_key, variable_client, adder, backend='cpu')\n\n  def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: actor_core_lib.FeedForwardPolicy,\n  ) -> List[reverb.Table]:\n    \"\"\"Creates reverb tables for the algorithm.\"\"\"\n    del policy\n    samples_per_insert_tolerance = (\n        self._config.samples_per_insert_tolerance_rate *\n        self._config.samples_per_insert)\n    error_buffer = self._config.min_replay_size * samples_per_insert_tolerance\n    limiter = rate_limiters.SampleToInsertRatio(\n        min_size_to_sample=self._config.min_replay_size,\n        samples_per_insert=self._config.samples_per_insert,\n        error_buffer=error_buffer)\n    return [reverb.Table(\n        name=self._config.replay_table_name,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=self._config.max_replay_size,\n        rate_limiter=limiter,\n        signature=adders_reverb.NStepTransitionAdder.signature(\n            environment_spec))]\n\n  def make_dataset_iterator(\n      self, replay_client: reverb.Client) -> Iterator[reverb.ReplaySample]:\n    \"\"\"Creates a dataset iterator to use for learning.\"\"\"\n    dataset = datasets.make_reverb_dataset(\n        table=self._config.replay_table_name,\n        server_address=replay_client.server_address,\n        batch_size=(\n            self._config.batch_size * self._config.num_sgd_steps_per_step),\n        prefetch_size=self._config.prefetch_size,\n        transition_adder=True)\n    return utils.device_put(dataset.as_numpy_iterator(), jax.devices()[0])\n\n  def make_adder(\n      self, replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[actor_core_lib.FeedForwardPolicy]\n  ) -> Optional[adders.Adder]:\n    \"\"\"Creates an adder which handles observations.\"\"\"\n    del environment_spec, policy\n    return adders_reverb.NStepTransitionAdder(\n        priority_fns={self._config.replay_table_name: None},\n        client=replay_client,\n        n_step=self._config.n_step,\n        discount=self._config.discount)\n\n  def make_policy(self,\n                  networks: td3_networks.TD3Networks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> actor_core_lib.FeedForwardPolicy:\n    \"\"\"Creates a policy.\"\"\"\n    sigma = 0 if evaluation else self._config.sigma\n    return td3_networks.get_default_behavior_policy(\n        networks=networks, action_specs=environment_spec.actions, sigma=sigma)",
  "def __init__(\n      self,\n      config: td3_config.TD3Config,\n  ):\n    \"\"\"Creates a TD3 learner, a behavior policy and an eval actor.\n\n    Args:\n      config: a config with TD3 hps\n    \"\"\"\n    self._config = config",
  "def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: td3_networks.TD3Networks,\n      dataset: Iterator[reverb.ReplaySample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    del environment_spec, replay_client\n\n    critic_optimizer = optax.adam(self._config.critic_learning_rate)\n    twin_critic_optimizer = optax.adam(self._config.critic_learning_rate)\n    policy_optimizer = optax.adam(self._config.policy_learning_rate)\n\n    if self._config.policy_gradient_clipping is not None:\n      policy_optimizer = optax.chain(\n          optax.clip_by_global_norm(self._config.policy_gradient_clipping),\n          policy_optimizer)\n\n    return learning.TD3Learner(\n        networks=networks,\n        random_key=random_key,\n        discount=self._config.discount,\n        target_sigma=self._config.target_sigma,\n        noise_clip=self._config.noise_clip,\n        policy_optimizer=policy_optimizer,\n        critic_optimizer=critic_optimizer,\n        twin_critic_optimizer=twin_critic_optimizer,\n        num_sgd_steps_per_step=self._config.num_sgd_steps_per_step,\n        bc_alpha=self._config.bc_alpha,\n        iterator=dataset,\n        logger=logger_fn('learner'),\n        counter=counter)",
  "def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: actor_core_lib.FeedForwardPolicy,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> core.Actor:\n    del environment_spec\n    assert variable_source is not None\n    actor_core = actor_core_lib.batched_feed_forward_to_actor_core(policy)\n    # Inference happens on CPU, so it's better to move variables there too.\n    variable_client = variable_utils.VariableClient(variable_source, 'policy',\n                                                    device='cpu')\n    return actors.GenericActor(\n        actor_core, random_key, variable_client, adder, backend='cpu')",
  "def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: actor_core_lib.FeedForwardPolicy,\n  ) -> List[reverb.Table]:\n    \"\"\"Creates reverb tables for the algorithm.\"\"\"\n    del policy\n    samples_per_insert_tolerance = (\n        self._config.samples_per_insert_tolerance_rate *\n        self._config.samples_per_insert)\n    error_buffer = self._config.min_replay_size * samples_per_insert_tolerance\n    limiter = rate_limiters.SampleToInsertRatio(\n        min_size_to_sample=self._config.min_replay_size,\n        samples_per_insert=self._config.samples_per_insert,\n        error_buffer=error_buffer)\n    return [reverb.Table(\n        name=self._config.replay_table_name,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=self._config.max_replay_size,\n        rate_limiter=limiter,\n        signature=adders_reverb.NStepTransitionAdder.signature(\n            environment_spec))]",
  "def make_dataset_iterator(\n      self, replay_client: reverb.Client) -> Iterator[reverb.ReplaySample]:\n    \"\"\"Creates a dataset iterator to use for learning.\"\"\"\n    dataset = datasets.make_reverb_dataset(\n        table=self._config.replay_table_name,\n        server_address=replay_client.server_address,\n        batch_size=(\n            self._config.batch_size * self._config.num_sgd_steps_per_step),\n        prefetch_size=self._config.prefetch_size,\n        transition_adder=True)\n    return utils.device_put(dataset.as_numpy_iterator(), jax.devices()[0])",
  "def make_adder(\n      self, replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[actor_core_lib.FeedForwardPolicy]\n  ) -> Optional[adders.Adder]:\n    \"\"\"Creates an adder which handles observations.\"\"\"\n    del environment_spec, policy\n    return adders_reverb.NStepTransitionAdder(\n        priority_fns={self._config.replay_table_name: None},\n        client=replay_client,\n        n_step=self._config.n_step,\n        discount=self._config.discount)",
  "def make_policy(self,\n                  networks: td3_networks.TD3Networks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> actor_core_lib.FeedForwardPolicy:\n    \"\"\"Creates a policy.\"\"\"\n    sigma = 0 if evaluation else self._config.sigma\n    return td3_networks.get_default_behavior_policy(\n        networks=networks, action_specs=environment_spec.actions, sigma=sigma)",
  "class RNDTrainingState(NamedTuple):\n  \"\"\"Contains training state for the learner.\"\"\"\n  optimizer_state: optax.OptState\n  params: networks_lib.Params\n  target_params: networks_lib.Params\n  steps: int",
  "class GlobalTrainingState(NamedTuple):\n  \"\"\"Contains training state of the RND learner.\"\"\"\n  rewarder_state: RNDTrainingState\n  learner_state: Any",
  "def rnd_update_step(\n    state: RNDTrainingState, transitions: types.Transition,\n    loss_fn: RNDLoss, optimizer: optax.GradientTransformation\n) -> Tuple[RNDTrainingState, Dict[str, jnp.ndarray]]:\n  \"\"\"Run an update steps on the given transitions.\n\n  Args:\n    state: The learner state.\n    transitions: Transitions to update on.\n    loss_fn: The loss function.\n    optimizer: The optimizer of the predictor network.\n\n  Returns:\n    A new state and metrics.\n  \"\"\"\n  loss, grads = jax.value_and_grad(loss_fn)(\n      state.params,\n      state.target_params,\n      transitions=transitions)\n\n  update, optimizer_state = optimizer.update(grads, state.optimizer_state)\n  params = optax.apply_updates(state.params, update)\n\n  new_state = RNDTrainingState(\n      optimizer_state=optimizer_state,\n      params=params,\n      target_params=state.target_params,\n      steps=state.steps + 1,\n  )\n  return new_state, {'rnd_loss': loss}",
  "def rnd_loss(\n    predictor_params: networks_lib.Params,\n    target_params: networks_lib.Params,\n    transitions: types.Transition,\n    networks: rnd_networks.RNDNetworks,\n) -> float:\n  \"\"\"The Random Network Distillation loss.\n\n  See https://arxiv.org/pdf/1810.12894.pdf A.2\n\n  Args:\n    predictor_params: Parameters of the predictor\n    target_params: Parameters of the target\n    transitions: Transitions to compute the loss on.\n    networks: RND networks\n\n  Returns:\n    The MSE loss as a float.\n  \"\"\"\n  target_output = networks.target.apply(target_params,\n                                        transitions.observation,\n                                        transitions.action)\n  predictor_output = networks.predictor.apply(predictor_params,\n                                              transitions.observation,\n                                              transitions.action)\n  return jnp.mean(jnp.square(target_output - predictor_output))",
  "class RNDLearner(acme.Learner):\n  \"\"\"RND learner.\"\"\"\n\n  def __init__(\n      self,\n      direct_rl_learner_factory: Callable[[Any, Iterator[reverb.ReplaySample]],\n                                          acme.Learner],\n      iterator: Iterator[reverb.ReplaySample],\n      optimizer: optax.GradientTransformation,\n      rnd_network: rnd_networks.RNDNetworks,\n      rng_key: jnp.ndarray,\n      grad_updates_per_batch: int,\n      is_sequence_based: bool,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None):\n    self._is_sequence_based = is_sequence_based\n\n    target_key, predictor_key = jax.random.split(rng_key)\n    target_params = rnd_network.target.init(target_key)\n    predictor_params = rnd_network.predictor.init(predictor_key)\n    optimizer_state = optimizer.init(predictor_params)\n\n    self._state = RNDTrainingState(\n        optimizer_state=optimizer_state,\n        params=predictor_params,\n        target_params=target_params,\n        steps=0)\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger(\n        'learner',\n        asynchronous=True,\n        serialize_fn=utils.fetch_devicearray,\n        steps_key=self._counter.get_steps_key())\n\n    loss = functools.partial(rnd_loss, networks=rnd_network)\n    self._update = functools.partial(rnd_update_step,\n                                     loss_fn=loss,\n                                     optimizer=optimizer)\n    self._update = utils.process_multiple_batches(self._update,\n                                                  grad_updates_per_batch)\n    self._update = jax.jit(self._update)\n\n    self._get_reward = jax.jit(\n        functools.partial(\n            rnd_networks.compute_rnd_reward, networks=rnd_network))\n\n    # Generator expression that works the same as an iterator.\n    # https://pymbook.readthedocs.io/en/latest/igd.html#generator-expressions\n    updated_iterator = (self._process_sample(sample) for sample in iterator)\n\n    self._direct_rl_learner = direct_rl_learner_factory(\n        rnd_network.direct_rl_networks, updated_iterator)\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None\n\n  def _process_sample(self, sample: reverb.ReplaySample) -> reverb.ReplaySample:\n    \"\"\"Uses the replay sample to train and update its reward.\n\n    Args:\n      sample: Replay sample to train on.\n\n    Returns:\n      The sample replay sample with an updated reward.\n    \"\"\"\n    transitions = reverb_utils.replay_sample_to_sars_transition(\n        sample, is_sequence=self._is_sequence_based)\n    self._state, metrics = self._update(self._state, transitions)\n    rewards = self._get_reward(self._state.params, self._state.target_params,\n                               transitions)\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Increment counts and record the current time\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n\n    # Attempts to write the logs.\n    self._logger.write({**metrics, **counts})\n\n    return sample._replace(data=sample.data._replace(reward=rewards))\n\n  def step(self):\n    self._direct_rl_learner.step()\n\n  def get_variables(self, names: List[str]) -> List[Any]:\n    rnd_variables = {\n        'target_params': self._state.target_params,\n        'predictor_params': self._state.params\n    }\n\n    learner_names = [name for name in names if name not in rnd_variables]\n    learner_dict = {}\n    if learner_names:\n      learner_dict = dict(\n          zip(learner_names,\n              self._direct_rl_learner.get_variables(learner_names)))\n\n    variables = [\n        rnd_variables.get(name, learner_dict.get(name, None)) for name in names\n    ]\n    return variables\n\n  def save(self) -> GlobalTrainingState:\n    return GlobalTrainingState(\n        rewarder_state=self._state,\n        learner_state=self._direct_rl_learner.save())\n\n  def restore(self, state: GlobalTrainingState):\n    self._state = state.rewarder_state\n    self._direct_rl_learner.restore(state.learner_state)",
  "def __init__(\n      self,\n      direct_rl_learner_factory: Callable[[Any, Iterator[reverb.ReplaySample]],\n                                          acme.Learner],\n      iterator: Iterator[reverb.ReplaySample],\n      optimizer: optax.GradientTransformation,\n      rnd_network: rnd_networks.RNDNetworks,\n      rng_key: jnp.ndarray,\n      grad_updates_per_batch: int,\n      is_sequence_based: bool,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None):\n    self._is_sequence_based = is_sequence_based\n\n    target_key, predictor_key = jax.random.split(rng_key)\n    target_params = rnd_network.target.init(target_key)\n    predictor_params = rnd_network.predictor.init(predictor_key)\n    optimizer_state = optimizer.init(predictor_params)\n\n    self._state = RNDTrainingState(\n        optimizer_state=optimizer_state,\n        params=predictor_params,\n        target_params=target_params,\n        steps=0)\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger(\n        'learner',\n        asynchronous=True,\n        serialize_fn=utils.fetch_devicearray,\n        steps_key=self._counter.get_steps_key())\n\n    loss = functools.partial(rnd_loss, networks=rnd_network)\n    self._update = functools.partial(rnd_update_step,\n                                     loss_fn=loss,\n                                     optimizer=optimizer)\n    self._update = utils.process_multiple_batches(self._update,\n                                                  grad_updates_per_batch)\n    self._update = jax.jit(self._update)\n\n    self._get_reward = jax.jit(\n        functools.partial(\n            rnd_networks.compute_rnd_reward, networks=rnd_network))\n\n    # Generator expression that works the same as an iterator.\n    # https://pymbook.readthedocs.io/en/latest/igd.html#generator-expressions\n    updated_iterator = (self._process_sample(sample) for sample in iterator)\n\n    self._direct_rl_learner = direct_rl_learner_factory(\n        rnd_network.direct_rl_networks, updated_iterator)\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None",
  "def _process_sample(self, sample: reverb.ReplaySample) -> reverb.ReplaySample:\n    \"\"\"Uses the replay sample to train and update its reward.\n\n    Args:\n      sample: Replay sample to train on.\n\n    Returns:\n      The sample replay sample with an updated reward.\n    \"\"\"\n    transitions = reverb_utils.replay_sample_to_sars_transition(\n        sample, is_sequence=self._is_sequence_based)\n    self._state, metrics = self._update(self._state, transitions)\n    rewards = self._get_reward(self._state.params, self._state.target_params,\n                               transitions)\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Increment counts and record the current time\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n\n    # Attempts to write the logs.\n    self._logger.write({**metrics, **counts})\n\n    return sample._replace(data=sample.data._replace(reward=rewards))",
  "def step(self):\n    self._direct_rl_learner.step()",
  "def get_variables(self, names: List[str]) -> List[Any]:\n    rnd_variables = {\n        'target_params': self._state.target_params,\n        'predictor_params': self._state.params\n    }\n\n    learner_names = [name for name in names if name not in rnd_variables]\n    learner_dict = {}\n    if learner_names:\n      learner_dict = dict(\n          zip(learner_names,\n              self._direct_rl_learner.get_variables(learner_names)))\n\n    variables = [\n        rnd_variables.get(name, learner_dict.get(name, None)) for name in names\n    ]\n    return variables",
  "def save(self) -> GlobalTrainingState:\n    return GlobalTrainingState(\n        rewarder_state=self._state,\n        learner_state=self._direct_rl_learner.save())",
  "def restore(self, state: GlobalTrainingState):\n    self._state = state.rewarder_state\n    self._direct_rl_learner.restore(state.learner_state)",
  "class RNDNetworks(Generic[DirectRLNetworks]):\n  \"\"\"Container of RND networks factories.\"\"\"\n  target: networks_lib.FeedForwardNetwork\n  predictor: networks_lib.FeedForwardNetwork\n  # Function from predictor output, target output, and original reward to reward\n  get_reward: Callable[\n      [networks_lib.NetworkOutput, networks_lib.NetworkOutput, jnp.ndarray],\n      jnp.ndarray]\n  direct_rl_networks: DirectRLNetworks = None",
  "def rnd_reward_fn(\n    predictor_output: networks_lib.NetworkOutput,\n    target_output: networks_lib.NetworkOutput,\n    original_reward: jnp.ndarray,\n    intrinsic_reward_coefficient: float = 1.0,\n    extrinsic_reward_coefficient: float = 0.0,\n) -> jnp.ndarray:\n  intrinsic_reward = jnp.mean(\n      jnp.square(predictor_output - target_output), axis=-1)\n  return (intrinsic_reward_coefficient * intrinsic_reward +\n          extrinsic_reward_coefficient * original_reward)",
  "def make_networks(\n    spec: specs.EnvironmentSpec,\n    direct_rl_networks: DirectRLNetworks,\n    layer_sizes: Tuple[int, ...] = (256, 256),\n    intrinsic_reward_coefficient: float = 1.0,\n    extrinsic_reward_coefficient: float = 0.0,\n) -> RNDNetworks[DirectRLNetworks]:\n  \"\"\"Creates networks used by the agent and returns RNDNetworks.\n\n  Args:\n    spec: Environment spec.\n    direct_rl_networks: Networks used by a direct rl algorithm.\n    layer_sizes: Layer sizes.\n    intrinsic_reward_coefficient: Multiplier on intrinsic reward.\n    extrinsic_reward_coefficient: Multiplier on extrinsic reward.\n\n  Returns:\n    The RND networks.\n  \"\"\"\n\n  def _rnd_fn(obs, act):\n    # RND does not use the action but other variants like RED do.\n    del act\n    network = networks_lib.LayerNormMLP(list(layer_sizes))\n    return network(obs)\n\n  target = hk.without_apply_rng(hk.transform(_rnd_fn))\n  predictor = hk.without_apply_rng(hk.transform(_rnd_fn))\n\n  # Create dummy observations and actions to create network parameters.\n  dummy_obs = utils.zeros_like(spec.observations)\n  dummy_obs = utils.add_batch_dim(dummy_obs)\n\n  return RNDNetworks(\n      target=networks_lib.FeedForwardNetwork(\n          lambda key: target.init(key, dummy_obs, ()), target.apply),\n      predictor=networks_lib.FeedForwardNetwork(\n          lambda key: predictor.init(key, dummy_obs, ()), predictor.apply),\n      direct_rl_networks=direct_rl_networks,\n      get_reward=functools.partial(\n          rnd_reward_fn,\n          intrinsic_reward_coefficient=intrinsic_reward_coefficient,\n          extrinsic_reward_coefficient=extrinsic_reward_coefficient))",
  "def compute_rnd_reward(predictor_params: networks_lib.Params,\n                       target_params: networks_lib.Params,\n                       transitions: types.Transition,\n                       networks: RNDNetworks) -> jnp.ndarray:\n  \"\"\"Computes the intrinsic RND reward for a given transition.\n\n  Args:\n    predictor_params: Parameters of the predictor network.\n    target_params: Parameters of the target network.\n    transitions: The sample to compute rewards for.\n    networks: RND networks\n\n  Returns:\n    The rewards as an ndarray.\n  \"\"\"\n  target_output = networks.target.apply(target_params, transitions.observation,\n                                        transitions.action)\n  predictor_output = networks.predictor.apply(predictor_params,\n                                              transitions.observation,\n                                              transitions.action)\n  return networks.get_reward(predictor_output, target_output,\n                             transitions.reward)",
  "def _rnd_fn(obs, act):\n    # RND does not use the action but other variants like RED do.\n    del act\n    network = networks_lib.LayerNormMLP(list(layer_sizes))\n    return network(obs)",
  "class RNDConfig:\n  \"\"\"Configuration options for RND.\"\"\"\n\n  # Learning rate for the predictor.\n  predictor_learning_rate: float = 1e-4\n\n  # If True, the direct rl algorithm is using the SequenceAdder data format.\n  is_sequence_based: bool = False\n\n  # How many gradient updates to perform per step.\n  num_sgd_steps_per_step: int = 1",
  "class RNDBuilder(Generic[rnd_networks.DirectRLNetworks, Policy],\n                 builders.ActorLearnerBuilder[rnd_networks.RNDNetworks, Policy,\n                                              reverb.ReplaySample]):\n  \"\"\"RND Builder.\"\"\"\n\n  def __init__(\n      self,\n      rl_agent: builders.ActorLearnerBuilder[rnd_networks.DirectRLNetworks,\n                                             Policy, reverb.ReplaySample],\n      config: rnd_config.RNDConfig,\n      logger_fn: Callable[[], loggers.Logger] = lambda: None,\n  ):\n    \"\"\"Implements a builder for RND using rl_agent as forward RL algorithm.\n\n    Args:\n      rl_agent: The standard RL agent used by RND to optimize the generator.\n      config: A config with RND HPs.\n      logger_fn: a logger factory for the rl_agent's learner.\n    \"\"\"\n    self._rl_agent = rl_agent\n    self._config = config\n    self._logger_fn = logger_fn\n\n  def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: rnd_networks.RNDNetworks[rnd_networks.DirectRLNetworks],\n      dataset: Iterator[reverb.ReplaySample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    direct_rl_learner_key, rnd_learner_key = jax.random.split(random_key)\n\n    counter = counter or counting.Counter()\n    direct_rl_counter = counting.Counter(counter, 'direct_rl')\n\n    def direct_rl_learner_factory(\n        networks: rnd_networks.DirectRLNetworks,\n        dataset: Iterator[reverb.ReplaySample]) -> core.Learner:\n      return self._rl_agent.make_learner(\n          direct_rl_learner_key,\n          networks,\n          dataset,\n          logger_fn=lambda name: self._logger_fn(),\n          environment_spec=environment_spec,\n          replay_client=replay_client,\n          counter=direct_rl_counter)\n\n    optimizer = optax.adam(learning_rate=self._config.predictor_learning_rate)\n\n    return rnd_learning.RNDLearner(\n        direct_rl_learner_factory=direct_rl_learner_factory,\n        iterator=dataset,\n        optimizer=optimizer,\n        rnd_network=networks,\n        rng_key=rnd_learner_key,\n        is_sequence_based=self._config.is_sequence_based,\n        grad_updates_per_batch=self._config.num_sgd_steps_per_step,\n        counter=counter,\n        logger=logger_fn('learner'))\n\n  def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: Policy,\n  ) -> List[reverb.Table]:\n    return self._rl_agent.make_replay_tables(environment_spec, policy)\n\n  def make_dataset_iterator(  # pytype: disable=signature-mismatch  # overriding-return-type-checks\n      self,\n      replay_client: reverb.Client) -> Optional[Iterator[reverb.ReplaySample]]:\n    return self._rl_agent.make_dataset_iterator(replay_client)\n\n  def make_adder(self, replay_client: reverb.Client,\n                 environment_spec: Optional[specs.EnvironmentSpec],\n                 policy: Optional[Policy]) -> Optional[adders.Adder]:\n    return self._rl_agent.make_adder(replay_client, environment_spec, policy)\n\n  def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: Policy,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> core.Actor:\n    return self._rl_agent.make_actor(random_key, policy, environment_spec,\n                                     variable_source, adder)\n\n  def make_policy(self,\n                  networks: rnd_networks.RNDNetworks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> actor_core_lib.FeedForwardPolicy:\n    \"\"\"Construct the policy.\"\"\"\n    return self._rl_agent.make_policy(networks.direct_rl_networks,\n                                      environment_spec, evaluation)",
  "def __init__(\n      self,\n      rl_agent: builders.ActorLearnerBuilder[rnd_networks.DirectRLNetworks,\n                                             Policy, reverb.ReplaySample],\n      config: rnd_config.RNDConfig,\n      logger_fn: Callable[[], loggers.Logger] = lambda: None,\n  ):\n    \"\"\"Implements a builder for RND using rl_agent as forward RL algorithm.\n\n    Args:\n      rl_agent: The standard RL agent used by RND to optimize the generator.\n      config: A config with RND HPs.\n      logger_fn: a logger factory for the rl_agent's learner.\n    \"\"\"\n    self._rl_agent = rl_agent\n    self._config = config\n    self._logger_fn = logger_fn",
  "def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: rnd_networks.RNDNetworks[rnd_networks.DirectRLNetworks],\n      dataset: Iterator[reverb.ReplaySample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    direct_rl_learner_key, rnd_learner_key = jax.random.split(random_key)\n\n    counter = counter or counting.Counter()\n    direct_rl_counter = counting.Counter(counter, 'direct_rl')\n\n    def direct_rl_learner_factory(\n        networks: rnd_networks.DirectRLNetworks,\n        dataset: Iterator[reverb.ReplaySample]) -> core.Learner:\n      return self._rl_agent.make_learner(\n          direct_rl_learner_key,\n          networks,\n          dataset,\n          logger_fn=lambda name: self._logger_fn(),\n          environment_spec=environment_spec,\n          replay_client=replay_client,\n          counter=direct_rl_counter)\n\n    optimizer = optax.adam(learning_rate=self._config.predictor_learning_rate)\n\n    return rnd_learning.RNDLearner(\n        direct_rl_learner_factory=direct_rl_learner_factory,\n        iterator=dataset,\n        optimizer=optimizer,\n        rnd_network=networks,\n        rng_key=rnd_learner_key,\n        is_sequence_based=self._config.is_sequence_based,\n        grad_updates_per_batch=self._config.num_sgd_steps_per_step,\n        counter=counter,\n        logger=logger_fn('learner'))",
  "def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: Policy,\n  ) -> List[reverb.Table]:\n    return self._rl_agent.make_replay_tables(environment_spec, policy)",
  "def make_dataset_iterator(  # pytype: disable=signature-mismatch  # overriding-return-type-checks\n      self,\n      replay_client: reverb.Client) -> Optional[Iterator[reverb.ReplaySample]]:\n    return self._rl_agent.make_dataset_iterator(replay_client)",
  "def make_adder(self, replay_client: reverb.Client,\n                 environment_spec: Optional[specs.EnvironmentSpec],\n                 policy: Optional[Policy]) -> Optional[adders.Adder]:\n    return self._rl_agent.make_adder(replay_client, environment_spec, policy)",
  "def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: Policy,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> core.Actor:\n    return self._rl_agent.make_actor(random_key, policy, environment_spec,\n                                     variable_source, adder)",
  "def make_policy(self,\n                  networks: rnd_networks.RNDNetworks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> actor_core_lib.FeedForwardPolicy:\n    \"\"\"Construct the policy.\"\"\"\n    return self._rl_agent.make_policy(networks.direct_rl_networks,\n                                      environment_spec, evaluation)",
  "def direct_rl_learner_factory(\n        networks: rnd_networks.DirectRLNetworks,\n        dataset: Iterator[reverb.ReplaySample]) -> core.Learner:\n      return self._rl_agent.make_learner(\n          direct_rl_learner_key,\n          networks,\n          dataset,\n          logger_fn=lambda name: self._logger_fn(),\n          environment_spec=environment_spec,\n          replay_client=replay_client,\n          counter=direct_rl_counter)",
  "def _generate_sqil_samples(\n    demonstration_iterator: Iterator[types.Transition],\n    replay_iterator: Iterator[reverb.ReplaySample]\n) -> Iterator[reverb.ReplaySample]:\n  \"\"\"Generator which creates the sample iterator for SQIL.\n\n  Args:\n    demonstration_iterator: Iterator of demonstrations.\n    replay_iterator: Replay buffer sample iterator.\n\n  Yields:\n    Samples having a mix of demonstrations with reward 1 and replay samples with\n    reward 0.\n  \"\"\"\n  for demonstrations, replay_sample in zip(demonstration_iterator,\n                                           replay_iterator):\n    demonstrations = demonstrations._replace(\n        reward=np.ones_like(demonstrations.reward))\n\n    replay_transitions = replay_sample.data\n    replay_transitions = replay_transitions._replace(\n        reward=np.zeros_like(replay_transitions.reward))\n\n    double_batch = tree.map_structure(lambda x, y: np.concatenate([x, y]),\n                                      demonstrations, replay_transitions)\n\n    # Split the double batch in an interleaving fashion.\n    # e.g [1, 2, 3, 4 ,5 ,6] -> [1, 3, 5] and [2, 4, 6]\n    yield reverb.ReplaySample(\n        info=replay_sample.info,\n        data=tree.map_structure(lambda x: x[0::2], double_batch))\n    yield reverb.ReplaySample(\n        info=replay_sample.info,\n        data=tree.map_structure(lambda x: x[1::2], double_batch))",
  "class SQILBuilder(Generic[DirectRLNetworks, DirectPolicyNetwork],\n                  builders.ActorLearnerBuilder[DirectRLNetworks,\n                                               DirectPolicyNetwork,\n                                               reverb.ReplaySample]):\n  \"\"\"SQIL Builder (https://openreview.net/pdf?id=S1xKd24twB).\"\"\"\n\n  def __init__(self,\n               rl_agent: builders.ActorLearnerBuilder[DirectRLNetworks,\n                                                      DirectPolicyNetwork,\n                                                      reverb.ReplaySample],\n               rl_agent_batch_size: int,\n               make_demonstrations: Callable[[int],\n                                             Iterator[types.Transition]]):\n    \"\"\"Builds a SQIL agent.\n\n    Args:\n      rl_agent: An off policy direct RL agent..\n      rl_agent_batch_size: The batch size of the above algorithm.\n      make_demonstrations: A function that returns an infinite iterator with\n        demonstrations.\n    \"\"\"\n    self._rl_agent = rl_agent\n    self._rl_agent_batch_size = rl_agent_batch_size\n    self._make_demonstrations = make_demonstrations\n\n  def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: DirectRLNetworks,\n      dataset: Iterator[reverb.ReplaySample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: Optional[specs.EnvironmentSpec] = None,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    \"\"\"Creates the learner.\"\"\"\n    counter = counter or counting.Counter()\n    direct_rl_counter = counting.Counter(counter, 'direct_rl')\n    return self._rl_agent.make_learner(\n        random_key,\n        networks,\n        dataset=dataset,\n        logger_fn=logger_fn,\n        environment_spec=environment_spec,\n        replay_client=replay_client,\n        counter=direct_rl_counter)\n\n  def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: DirectPolicyNetwork,\n  ) -> List[reverb.Table]:\n    return self._rl_agent.make_replay_tables(environment_spec, policy)\n\n  def make_dataset_iterator(  # pytype: disable=signature-mismatch  # overriding-return-type-checks\n      self,\n      replay_client: reverb.Client) -> Optional[Iterator[reverb.ReplaySample]]:\n    \"\"\"The returned iterator returns batches with both expert and policy data.\n\n    Batch items will alternate between expert data and policy data.\n\n    Args:\n      replay_client: Reverb client.\n\n    Returns:\n      The Replay sample iterator.\n    \"\"\"\n    # TODO(eorsini): Make sure we have the exact same format as the rl_agent's\n    # adder writes in.\n    demonstration_iterator = self._make_demonstrations(\n        self._rl_agent_batch_size)\n\n    rb_iterator = self._rl_agent.make_dataset_iterator(replay_client)\n\n    return utils.device_put(\n        _generate_sqil_samples(demonstration_iterator, rb_iterator),\n        jax.devices()[0])\n\n  def make_adder(\n      self, replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[DirectPolicyNetwork]) -> Optional[adders.Adder]:\n    return self._rl_agent.make_adder(replay_client, environment_spec, policy)\n\n  def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: DirectPolicyNetwork,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> core.Actor:\n    return self._rl_agent.make_actor(random_key, policy, environment_spec,\n                                     variable_source, adder)\n\n  def make_policy(self,\n                  networks: DirectRLNetworks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> DirectPolicyNetwork:\n    return self._rl_agent.make_policy(networks, environment_spec, evaluation)",
  "def __init__(self,\n               rl_agent: builders.ActorLearnerBuilder[DirectRLNetworks,\n                                                      DirectPolicyNetwork,\n                                                      reverb.ReplaySample],\n               rl_agent_batch_size: int,\n               make_demonstrations: Callable[[int],\n                                             Iterator[types.Transition]]):\n    \"\"\"Builds a SQIL agent.\n\n    Args:\n      rl_agent: An off policy direct RL agent..\n      rl_agent_batch_size: The batch size of the above algorithm.\n      make_demonstrations: A function that returns an infinite iterator with\n        demonstrations.\n    \"\"\"\n    self._rl_agent = rl_agent\n    self._rl_agent_batch_size = rl_agent_batch_size\n    self._make_demonstrations = make_demonstrations",
  "def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: DirectRLNetworks,\n      dataset: Iterator[reverb.ReplaySample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: Optional[specs.EnvironmentSpec] = None,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    \"\"\"Creates the learner.\"\"\"\n    counter = counter or counting.Counter()\n    direct_rl_counter = counting.Counter(counter, 'direct_rl')\n    return self._rl_agent.make_learner(\n        random_key,\n        networks,\n        dataset=dataset,\n        logger_fn=logger_fn,\n        environment_spec=environment_spec,\n        replay_client=replay_client,\n        counter=direct_rl_counter)",
  "def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: DirectPolicyNetwork,\n  ) -> List[reverb.Table]:\n    return self._rl_agent.make_replay_tables(environment_spec, policy)",
  "def make_dataset_iterator(  # pytype: disable=signature-mismatch  # overriding-return-type-checks\n      self,\n      replay_client: reverb.Client) -> Optional[Iterator[reverb.ReplaySample]]:\n    \"\"\"The returned iterator returns batches with both expert and policy data.\n\n    Batch items will alternate between expert data and policy data.\n\n    Args:\n      replay_client: Reverb client.\n\n    Returns:\n      The Replay sample iterator.\n    \"\"\"\n    # TODO(eorsini): Make sure we have the exact same format as the rl_agent's\n    # adder writes in.\n    demonstration_iterator = self._make_demonstrations(\n        self._rl_agent_batch_size)\n\n    rb_iterator = self._rl_agent.make_dataset_iterator(replay_client)\n\n    return utils.device_put(\n        _generate_sqil_samples(demonstration_iterator, rb_iterator),\n        jax.devices()[0])",
  "def make_adder(\n      self, replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[DirectPolicyNetwork]) -> Optional[adders.Adder]:\n    return self._rl_agent.make_adder(replay_client, environment_spec, policy)",
  "def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: DirectPolicyNetwork,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> core.Actor:\n    return self._rl_agent.make_actor(random_key, policy, environment_spec,\n                                     variable_source, adder)",
  "def make_policy(self,\n                  networks: DirectRLNetworks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> DirectPolicyNetwork:\n    return self._rl_agent.make_policy(networks, environment_spec, evaluation)",
  "class BuilderTest(absltest.TestCase):\n\n  def test_sqil_iterator(self):\n    demonstrations = [\n        types.Transition(np.array([[1], [2], [3]]), (), (), (), ())\n    ]\n    replay = [\n        reverb.ReplaySample(\n            info=(),\n            data=types.Transition(np.array([[4], [5], [6]]), (), (), (), ()))\n    ]\n    sqil_it = builder._generate_sqil_samples(iter(demonstrations), iter(replay))\n    np.testing.assert_array_equal(\n        next(sqil_it).data.observation, np.array([[1], [3], [5]]))\n    np.testing.assert_array_equal(\n        next(sqil_it).data.observation, np.array([[2], [4], [6]]))\n    self.assertRaises(StopIteration, lambda: next(sqil_it))",
  "def test_sqil_iterator(self):\n    demonstrations = [\n        types.Transition(np.array([[1], [2], [3]]), (), (), (), ())\n    ]\n    replay = [\n        reverb.ReplaySample(\n            info=(),\n            data=types.Transition(np.array([[4], [5], [6]]), (), (), (), ()))\n    ]\n    sqil_it = builder._generate_sqil_samples(iter(demonstrations), iter(replay))\n    np.testing.assert_array_equal(\n        next(sqil_it).data.observation, np.array([[1], [3], [5]]))\n    np.testing.assert_array_equal(\n        next(sqil_it).data.observation, np.array([[2], [4], [6]]))\n    self.assertRaises(StopIteration, lambda: next(sqil_it))",
  "def make_networks(spec: specs.EnvironmentSpec) -> bc.BCNetworks:\n  \"\"\"Creates networks used by the agent.\"\"\"\n\n  final_layer_size = np.prod(spec.actions.shape, dtype=int)\n\n  def _actor_fn(obs, is_training=False, key=None):\n    # is_training and key allows to defined train/test dependant modules\n    # like dropout.\n    del is_training\n    del key\n    network = networks_lib.LayerNormMLP([64, 64, final_layer_size],\n                                        activate_final=False)\n    return jax.nn.tanh(network(obs))\n\n  policy = hk.without_apply_rng(hk.transform(_actor_fn))\n\n  # Create dummy observations and actions to create network parameters.\n  dummy_obs = utils.zeros_like(spec.observations)\n  dummy_obs = utils.add_batch_dim(dummy_obs)\n  policy_network = bc.BCPolicyNetwork(lambda key: policy.init(key, dummy_obs),\n                                      policy.apply)\n\n  return bc.BCNetworks(policy_network)",
  "class BcPretrainingTest(absltest.TestCase):\n\n  def test_bc_initialization(self):\n    # Create a fake environment to test with.\n    environment = fakes.ContinuousEnvironment(\n        episode_length=10, bounded=True, action_dim=6)\n    spec = specs.make_environment_spec(environment)\n\n    # Construct the agent.\n    nets = make_networks(spec)\n\n    loss = bc.mse()\n\n    bc.pretraining.train_with_bc(\n        fakes.transition_iterator(environment), nets, loss, num_steps=100)\n\n  def test_sac_to_bc_networks(self):\n    # Create a fake environment to test with.\n    environment = fakes.ContinuousEnvironment(\n        episode_length=10, bounded=True, action_dim=6)\n    spec = specs.make_environment_spec(environment)\n\n    sac_nets = sac.make_networks(spec, hidden_layer_sizes=(4, 4))\n    bc_nets = bc.convert_to_bc_network(sac_nets.policy_network)\n\n    rng = jax.random.PRNGKey(0)\n    dummy_obs = utils.zeros_like(spec.observations)\n    dummy_obs = utils.add_batch_dim(dummy_obs)\n\n    sac_params = sac_nets.policy_network.init(rng)\n    sac_output = sac_nets.policy_network.apply(sac_params, dummy_obs)\n\n    bc_params = bc_nets.init(rng)\n    bc_output = bc_nets.apply(bc_params, dummy_obs, is_training=False, key=None)\n\n    np.testing.assert_array_equal(sac_output.mode(), bc_output.mode())",
  "def _actor_fn(obs, is_training=False, key=None):\n    # is_training and key allows to defined train/test dependant modules\n    # like dropout.\n    del is_training\n    del key\n    network = networks_lib.LayerNormMLP([64, 64, final_layer_size],\n                                        activate_final=False)\n    return jax.nn.tanh(network(obs))",
  "def test_bc_initialization(self):\n    # Create a fake environment to test with.\n    environment = fakes.ContinuousEnvironment(\n        episode_length=10, bounded=True, action_dim=6)\n    spec = specs.make_environment_spec(environment)\n\n    # Construct the agent.\n    nets = make_networks(spec)\n\n    loss = bc.mse()\n\n    bc.pretraining.train_with_bc(\n        fakes.transition_iterator(environment), nets, loss, num_steps=100)",
  "def test_sac_to_bc_networks(self):\n    # Create a fake environment to test with.\n    environment = fakes.ContinuousEnvironment(\n        episode_length=10, bounded=True, action_dim=6)\n    spec = specs.make_environment_spec(environment)\n\n    sac_nets = sac.make_networks(spec, hidden_layer_sizes=(4, 4))\n    bc_nets = bc.convert_to_bc_network(sac_nets.policy_network)\n\n    rng = jax.random.PRNGKey(0)\n    dummy_obs = utils.zeros_like(spec.observations)\n    dummy_obs = utils.add_batch_dim(dummy_obs)\n\n    sac_params = sac_nets.policy_network.init(rng)\n    sac_output = sac_nets.policy_network.apply(sac_params, dummy_obs)\n\n    bc_params = bc_nets.init(rng)\n    bc_output = bc_nets.apply(bc_params, dummy_obs, is_training=False, key=None)\n\n    np.testing.assert_array_equal(sac_output.mode(), bc_output.mode())",
  "def mse() -> BCLossWithoutAux:\n  \"\"\"Mean Squared Error loss.\"\"\"\n\n  def loss(networks: bc_networks.BCNetworks, params: networks_lib.Params,\n           key: jax_types.PRNGKey,\n           transitions: types.Transition) -> jnp.ndarray:\n    key, key_dropout = jax.random.split(key)\n    dist_params = networks.policy_network.apply(\n        params, transitions.observation, is_training=True, key=key_dropout)\n    action = networks.sample_fn(dist_params, key)\n    return jnp.mean(jnp.square(action - transitions.action))\n\n  return loss",
  "def logp() -> BCLossWithoutAux:\n  \"\"\"Log probability loss.\"\"\"\n\n  def loss(networks: bc_networks.BCNetworks, params: networks_lib.Params,\n           key: jax_types.PRNGKey,\n           transitions: types.Transition) -> jnp.ndarray:\n    logits = networks.policy_network.apply(\n        params, transitions.observation, is_training=True, key=key)\n    logp_action = networks.log_prob(logits, transitions.action)\n    return -jnp.mean(logp_action)\n\n  return loss",
  "def peerbc(base_loss_fn: BCLossWithoutAux, zeta: float) -> BCLossWithoutAux:\n  \"\"\"Peer-BC loss from https://arxiv.org/pdf/2010.01748.pdf.\n\n  Args:\n    base_loss_fn: the base loss to add RCAL on top of.\n    zeta: the weight of the regularization.\n  Returns:\n    The loss.\n  \"\"\"\n\n  def loss(networks: bc_networks.BCNetworks, params: networks_lib.Params,\n           key: jax_types.PRNGKey,\n           transitions: types.Transition) -> jnp.ndarray:\n    key_perm, key_bc_loss, key_permuted_loss = jax.random.split(key, 3)\n\n    permutation_keys = jax.random.split(key_perm, transitions.action.shape[0])\n    permuted_actions = jax.vmap(\n        jax.random.permutation, in_axes=(0, 0))(permutation_keys,\n                                                transitions.action)\n    permuted_transition = transitions._replace(action=permuted_actions)\n    bc_loss = base_loss_fn(networks, params, key_bc_loss, transitions)\n    permuted_loss = base_loss_fn(networks, params, key_permuted_loss,\n                                 permuted_transition)\n    return bc_loss - zeta * permuted_loss\n\n  return loss",
  "def rcal(base_loss_fn: BCLossWithoutAux,\n         discount: float,\n         alpha: float,\n         num_bins: Optional[int] = None) -> BCLossWithoutAux:\n  \"\"\"https://www.cristal.univ-lille.fr/~pietquin/pdf/AAMAS_2014_BPMGOP.pdf.\n\n  Args:\n    base_loss_fn: the base loss to add RCAL on top of.\n    discount: the gamma discount used in RCAL.\n    alpha: the regularization parameter.\n    num_bins: how many bins were used for discretization. If None the\n      environment was originally discrete already.\n  Returns:\n    The loss function.\n  \"\"\"\n\n  def loss(networks: bc_networks.BCNetworks, params: networks_lib.Params,\n           key: jax_types.PRNGKey,\n           transitions: types.Transition) -> jnp.ndarray:\n\n    def logits_fn(key: jax_types.PRNGKey,\n                  observations: networks_lib.Observation,\n                  actions: Optional[networks_lib.Action] = None):\n      logits = networks.policy_network.apply(\n          params, observations, key=key, is_training=True)\n      if num_bins:\n        logits = jnp.reshape(logits, list(logits.shape[:-1]) + [-1, num_bins])\n      if actions is None:\n        actions = jnp.argmax(logits, axis=-1)\n      logits_actions = jnp.sum(\n          jax.nn.one_hot(actions, logits.shape[-1]) * logits, axis=-1)\n      return logits_actions\n\n    key, key1, key2 = jax.random.split(key, 3)\n\n    logits_a_tm1 = logits_fn(key1, transitions.observation, transitions.action)\n    logits_a_t = logits_fn(key2, transitions.next_observation)\n\n    # RCAL, by making a parallel between the logits of BC and Q-values,\n    # defines a regularization loss that encourages the implicit reward\n    # (inferred by inversing the Bellman Equation) to be sparse.\n    # NOTE: In case of discretized envs jnp.mean goes over batch and num_bins\n    # dimensions.\n    regularization_loss = jnp.mean(\n        jnp.abs(logits_a_tm1 - discount * logits_a_t)\n        )\n\n    loss = base_loss_fn(networks, params, key, transitions)\n    return loss + alpha * regularization_loss\n\n  return loss",
  "def loss(networks: bc_networks.BCNetworks, params: networks_lib.Params,\n           key: jax_types.PRNGKey,\n           transitions: types.Transition) -> jnp.ndarray:\n    key, key_dropout = jax.random.split(key)\n    dist_params = networks.policy_network.apply(\n        params, transitions.observation, is_training=True, key=key_dropout)\n    action = networks.sample_fn(dist_params, key)\n    return jnp.mean(jnp.square(action - transitions.action))",
  "def loss(networks: bc_networks.BCNetworks, params: networks_lib.Params,\n           key: jax_types.PRNGKey,\n           transitions: types.Transition) -> jnp.ndarray:\n    logits = networks.policy_network.apply(\n        params, transitions.observation, is_training=True, key=key)\n    logp_action = networks.log_prob(logits, transitions.action)\n    return -jnp.mean(logp_action)",
  "def loss(networks: bc_networks.BCNetworks, params: networks_lib.Params,\n           key: jax_types.PRNGKey,\n           transitions: types.Transition) -> jnp.ndarray:\n    key_perm, key_bc_loss, key_permuted_loss = jax.random.split(key, 3)\n\n    permutation_keys = jax.random.split(key_perm, transitions.action.shape[0])\n    permuted_actions = jax.vmap(\n        jax.random.permutation, in_axes=(0, 0))(permutation_keys,\n                                                transitions.action)\n    permuted_transition = transitions._replace(action=permuted_actions)\n    bc_loss = base_loss_fn(networks, params, key_bc_loss, transitions)\n    permuted_loss = base_loss_fn(networks, params, key_permuted_loss,\n                                 permuted_transition)\n    return bc_loss - zeta * permuted_loss",
  "def loss(networks: bc_networks.BCNetworks, params: networks_lib.Params,\n           key: jax_types.PRNGKey,\n           transitions: types.Transition) -> jnp.ndarray:\n\n    def logits_fn(key: jax_types.PRNGKey,\n                  observations: networks_lib.Observation,\n                  actions: Optional[networks_lib.Action] = None):\n      logits = networks.policy_network.apply(\n          params, observations, key=key, is_training=True)\n      if num_bins:\n        logits = jnp.reshape(logits, list(logits.shape[:-1]) + [-1, num_bins])\n      if actions is None:\n        actions = jnp.argmax(logits, axis=-1)\n      logits_actions = jnp.sum(\n          jax.nn.one_hot(actions, logits.shape[-1]) * logits, axis=-1)\n      return logits_actions\n\n    key, key1, key2 = jax.random.split(key, 3)\n\n    logits_a_tm1 = logits_fn(key1, transitions.observation, transitions.action)\n    logits_a_t = logits_fn(key2, transitions.next_observation)\n\n    # RCAL, by making a parallel between the logits of BC and Q-values,\n    # defines a regularization loss that encourages the implicit reward\n    # (inferred by inversing the Bellman Equation) to be sparse.\n    # NOTE: In case of discretized envs jnp.mean goes over batch and num_bins\n    # dimensions.\n    regularization_loss = jnp.mean(\n        jnp.abs(logits_a_tm1 - discount * logits_a_t)\n        )\n\n    loss = base_loss_fn(networks, params, key, transitions)\n    return loss + alpha * regularization_loss",
  "def logits_fn(key: jax_types.PRNGKey,\n                  observations: networks_lib.Observation,\n                  actions: Optional[networks_lib.Action] = None):\n      logits = networks.policy_network.apply(\n          params, observations, key=key, is_training=True)\n      if num_bins:\n        logits = jnp.reshape(logits, list(logits.shape[:-1]) + [-1, num_bins])\n      if actions is None:\n        actions = jnp.argmax(logits, axis=-1)\n      logits_actions = jnp.sum(\n          jax.nn.one_hot(actions, logits.shape[-1]) * logits, axis=-1)\n      return logits_actions",
  "class TrainingState(NamedTuple):\n  \"\"\"Contains training state for the learner.\"\"\"\n  optimizer_state: optax.OptState\n  policy_params: networks_lib.Params\n  key: networks_lib.PRNGKey\n  steps: int",
  "def _create_loss_metrics(\n    loss_has_aux: bool,\n    loss_result: Union[jnp.ndarray, Tuple[jnp.ndarray, loggers.LoggingData]],\n    gradients: jnp.ndarray,\n):\n  \"\"\"Creates loss metrics for logging.\"\"\"\n  # Validate input.\n  if loss_has_aux and not (len(loss_result) == 2 and isinstance(\n      loss_result[0], jnp.ndarray) and isinstance(loss_result[1], dict)):\n    raise ValueError('Could not parse loss value and metrics from loss_fn\\'s '\n                     'output. Since loss_has_aux is enabled, loss_fn must '\n                     'return loss_value and auxiliary metrics.')\n\n  if not loss_has_aux and not isinstance(loss_result, jnp.ndarray):\n    raise ValueError(f'Loss returns type {loss_result}. However, it should '\n                     'return a jnp.ndarray, given that loss_has_aux = False.')\n\n  # Maybe unpack loss result.\n  if loss_has_aux:\n    loss, metrics = loss_result\n  else:\n    loss = loss_result\n    metrics = {}\n\n  # Complete metrics dict and return it.\n  metrics['loss'] = loss\n  metrics['gradient_norm'] = optax.global_norm(gradients)\n  return metrics",
  "class BCLearner(acme.Learner):\n  \"\"\"BC learner.\n\n  This is the learning component of a BC agent. It takes a Transitions iterator\n  as input and implements update functionality to learn from this iterator.\n  \"\"\"\n\n  _state: TrainingState\n\n  def __init__(self,\n               networks: bc_networks.BCNetworks,\n               random_key: networks_lib.PRNGKey,\n               loss_fn: losses.BCLoss,\n               optimizer: optax.GradientTransformation,\n               prefetching_iterator: Iterator[types.Transition],\n               num_sgd_steps_per_step: int,\n               loss_has_aux: bool = False,\n               logger: Optional[loggers.Logger] = None,\n               counter: Optional[counting.Counter] = None):\n    \"\"\"Behavior Cloning Learner.\n\n    Args:\n      networks: BC networks\n      random_key: RNG key.\n      loss_fn: BC loss to use.\n      optimizer: Optax optimizer.\n      prefetching_iterator: A sharded prefetching iterator as outputted from\n        `acme.jax.utils.sharded_prefetch`. Please see the documentation for\n        `sharded_prefetch` for more details.\n      num_sgd_steps_per_step: Number of gradient updates per step.\n      loss_has_aux: Whether the loss function returns auxiliary metrics as a\n        second argument.\n      logger: Logger.\n      counter: Counter.\n    \"\"\"\n    def sgd_step(\n        state: TrainingState,\n        transitions: types.Transition,\n    ) -> Tuple[TrainingState, Dict[str, jnp.ndarray]]:\n\n      loss_and_grad = jax.value_and_grad(\n          loss_fn, argnums=1, has_aux=loss_has_aux)\n\n      # Compute losses and their gradients.\n      key, key_input = jax.random.split(state.key)\n      loss_result, gradients = loss_and_grad(networks, state.policy_params,\n                                             key_input, transitions)\n\n      # Combine the gradient across all devices (by taking their mean).\n      gradients = jax.lax.pmean(gradients, axis_name=_PMAP_AXIS_NAME)\n\n      # Compute and combine metrics across all devices.\n      metrics = _create_loss_metrics(loss_has_aux, loss_result, gradients)\n      metrics = jax.lax.pmean(metrics, axis_name=_PMAP_AXIS_NAME)\n\n      policy_update, optimizer_state = optimizer.update(gradients,\n                                                        state.optimizer_state,\n                                                        state.policy_params)\n      policy_params = optax.apply_updates(state.policy_params, policy_update)\n\n      new_state = TrainingState(\n          optimizer_state=optimizer_state,\n          policy_params=policy_params,\n          key=key,\n          steps=state.steps + 1,\n      )\n\n      return new_state, metrics\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter(prefix='learner')\n    self._logger = logger or loggers.make_default_logger(\n        'learner',\n        asynchronous=True,\n        serialize_fn=utils.fetch_devicearray,\n        steps_key=self._counter.get_steps_key())\n\n    # Split the input batch to `num_sgd_steps_per_step` minibatches in order\n    # to achieve better performance on accelerators.\n    sgd_step = utils.process_multiple_batches(sgd_step, num_sgd_steps_per_step)\n    self._sgd_step = jax.pmap(sgd_step, axis_name=_PMAP_AXIS_NAME)\n\n    random_key, init_key = jax.random.split(random_key)\n    policy_params = networks.policy_network.init(init_key)\n    optimizer_state = optimizer.init(policy_params)\n\n    # Create initial state.\n    state = TrainingState(\n        optimizer_state=optimizer_state,\n        policy_params=policy_params,\n        key=random_key,\n        steps=0,\n    )\n    self._state = utils.replicate_in_all_devices(state)\n\n    self._timestamp = None\n\n    self._prefetching_iterator = prefetching_iterator\n\n  def step(self):\n    # Get a batch of Transitions.\n    transitions = next(self._prefetching_iterator)\n    self._state, metrics = self._sgd_step(self._state, transitions)\n    metrics = utils.get_from_first_device(metrics)\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Increment counts and record the current time\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n\n    # Attempts to write the logs.\n    self._logger.write({**metrics, **counts})\n\n  def get_variables(self, names: List[str]) -> List[networks_lib.Params]:\n    variables = {\n        'policy': utils.get_from_first_device(self._state.policy_params),\n    }\n    return [variables[name] for name in names]\n\n  def save(self) -> TrainingState:\n    # Serialize only the first replica of parameters and optimizer state.\n    return jax.tree_map(utils.get_from_first_device, self._state)\n\n  def restore(self, state: TrainingState):\n    self._state = utils.replicate_in_all_devices(state)",
  "def __init__(self,\n               networks: bc_networks.BCNetworks,\n               random_key: networks_lib.PRNGKey,\n               loss_fn: losses.BCLoss,\n               optimizer: optax.GradientTransformation,\n               prefetching_iterator: Iterator[types.Transition],\n               num_sgd_steps_per_step: int,\n               loss_has_aux: bool = False,\n               logger: Optional[loggers.Logger] = None,\n               counter: Optional[counting.Counter] = None):\n    \"\"\"Behavior Cloning Learner.\n\n    Args:\n      networks: BC networks\n      random_key: RNG key.\n      loss_fn: BC loss to use.\n      optimizer: Optax optimizer.\n      prefetching_iterator: A sharded prefetching iterator as outputted from\n        `acme.jax.utils.sharded_prefetch`. Please see the documentation for\n        `sharded_prefetch` for more details.\n      num_sgd_steps_per_step: Number of gradient updates per step.\n      loss_has_aux: Whether the loss function returns auxiliary metrics as a\n        second argument.\n      logger: Logger.\n      counter: Counter.\n    \"\"\"\n    def sgd_step(\n        state: TrainingState,\n        transitions: types.Transition,\n    ) -> Tuple[TrainingState, Dict[str, jnp.ndarray]]:\n\n      loss_and_grad = jax.value_and_grad(\n          loss_fn, argnums=1, has_aux=loss_has_aux)\n\n      # Compute losses and their gradients.\n      key, key_input = jax.random.split(state.key)\n      loss_result, gradients = loss_and_grad(networks, state.policy_params,\n                                             key_input, transitions)\n\n      # Combine the gradient across all devices (by taking their mean).\n      gradients = jax.lax.pmean(gradients, axis_name=_PMAP_AXIS_NAME)\n\n      # Compute and combine metrics across all devices.\n      metrics = _create_loss_metrics(loss_has_aux, loss_result, gradients)\n      metrics = jax.lax.pmean(metrics, axis_name=_PMAP_AXIS_NAME)\n\n      policy_update, optimizer_state = optimizer.update(gradients,\n                                                        state.optimizer_state,\n                                                        state.policy_params)\n      policy_params = optax.apply_updates(state.policy_params, policy_update)\n\n      new_state = TrainingState(\n          optimizer_state=optimizer_state,\n          policy_params=policy_params,\n          key=key,\n          steps=state.steps + 1,\n      )\n\n      return new_state, metrics\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter(prefix='learner')\n    self._logger = logger or loggers.make_default_logger(\n        'learner',\n        asynchronous=True,\n        serialize_fn=utils.fetch_devicearray,\n        steps_key=self._counter.get_steps_key())\n\n    # Split the input batch to `num_sgd_steps_per_step` minibatches in order\n    # to achieve better performance on accelerators.\n    sgd_step = utils.process_multiple_batches(sgd_step, num_sgd_steps_per_step)\n    self._sgd_step = jax.pmap(sgd_step, axis_name=_PMAP_AXIS_NAME)\n\n    random_key, init_key = jax.random.split(random_key)\n    policy_params = networks.policy_network.init(init_key)\n    optimizer_state = optimizer.init(policy_params)\n\n    # Create initial state.\n    state = TrainingState(\n        optimizer_state=optimizer_state,\n        policy_params=policy_params,\n        key=random_key,\n        steps=0,\n    )\n    self._state = utils.replicate_in_all_devices(state)\n\n    self._timestamp = None\n\n    self._prefetching_iterator = prefetching_iterator",
  "def step(self):\n    # Get a batch of Transitions.\n    transitions = next(self._prefetching_iterator)\n    self._state, metrics = self._sgd_step(self._state, transitions)\n    metrics = utils.get_from_first_device(metrics)\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Increment counts and record the current time\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n\n    # Attempts to write the logs.\n    self._logger.write({**metrics, **counts})",
  "def get_variables(self, names: List[str]) -> List[networks_lib.Params]:\n    variables = {\n        'policy': utils.get_from_first_device(self._state.policy_params),\n    }\n    return [variables[name] for name in names]",
  "def save(self) -> TrainingState:\n    # Serialize only the first replica of parameters and optimizer state.\n    return jax.tree_map(utils.get_from_first_device, self._state)",
  "def restore(self, state: TrainingState):\n    self._state = utils.replicate_in_all_devices(state)",
  "def sgd_step(\n        state: TrainingState,\n        transitions: types.Transition,\n    ) -> Tuple[TrainingState, Dict[str, jnp.ndarray]]:\n\n      loss_and_grad = jax.value_and_grad(\n          loss_fn, argnums=1, has_aux=loss_has_aux)\n\n      # Compute losses and their gradients.\n      key, key_input = jax.random.split(state.key)\n      loss_result, gradients = loss_and_grad(networks, state.policy_params,\n                                             key_input, transitions)\n\n      # Combine the gradient across all devices (by taking their mean).\n      gradients = jax.lax.pmean(gradients, axis_name=_PMAP_AXIS_NAME)\n\n      # Compute and combine metrics across all devices.\n      metrics = _create_loss_metrics(loss_has_aux, loss_result, gradients)\n      metrics = jax.lax.pmean(metrics, axis_name=_PMAP_AXIS_NAME)\n\n      policy_update, optimizer_state = optimizer.update(gradients,\n                                                        state.optimizer_state,\n                                                        state.policy_params)\n      policy_params = optax.apply_updates(state.policy_params, policy_update)\n\n      new_state = TrainingState(\n          optimizer_state=optimizer_state,\n          policy_params=policy_params,\n          key=key,\n          steps=state.steps + 1,\n      )\n\n      return new_state, metrics",
  "class ApplyFn(Protocol):\n\n  def __call__(self,\n               params: networks_lib.Params,\n               observation: networks_lib.Observation,\n               *args,\n               is_training: bool,\n               key: Optional[types.PRNGKey] = None,\n               **kwargs) -> networks_lib.NetworkOutput:\n    ...",
  "class BCPolicyNetwork:\n  \"\"\"Holds a pair of pure functions defining a policy network for BC.\n\n  This is a feed-forward network taking params, obs, is_training, key as input.\n\n  Attributes:\n    init: A pure function. Initializes and returns the networks parameters.\n    apply: A pure function. Computes and returns the outputs of a forward pass.\n  \"\"\"\n  init: Callable[[types.PRNGKey], networks_lib.Params]\n  apply: ApplyFn",
  "def identity_sample(output: networks_lib.NetworkOutput,\n                    key: types.PRNGKey) -> networks_lib.Action:\n  \"\"\"Placeholder sampling function for non-distributional networks.\"\"\"\n  del key\n  return output",
  "class BCNetworks:\n  \"\"\"The network and pure functions for the BC agent.\n\n  Attributes:\n    policy_network: The policy network.\n    sample_fn: A pure function. Samples an action based on the network output.\n      Must be set for distributional networks. Otherwise identity.\n    log_prob: A pure function. Computes log-probability for an action.\n      Must be set for distributional networks. Otherwise None.\n  \"\"\"\n  policy_network: BCPolicyNetwork\n  sample_fn: networks_lib.SampleFn = identity_sample\n  log_prob: Optional[networks_lib.LogProbFn] = None",
  "def convert_to_bc_network(\n    policy_network: networks_lib.FeedForwardNetwork) -> BCPolicyNetwork:\n  \"\"\"Converts a policy network from SAC/TD3/D4PG/.. into a BC policy network.\n\n  Args:\n    policy_network: FeedForwardNetwork taking the observation as input and\n      returning action representation compatible with one of the BC losses.\n\n  Returns:\n    The BC policy network taking observation, is_training, key as input.\n  \"\"\"\n\n  def apply(params: networks_lib.Params,\n            observation: networks_lib.Observation,\n            *args,\n            is_training: bool = False,\n            key: Optional[types.PRNGKey] = None,\n            **kwargs) -> networks_lib.NetworkOutput:\n    del is_training, key\n    return policy_network.apply(params, observation, *args, **kwargs)\n\n  return BCPolicyNetwork(policy_network.init, apply)",
  "def convert_policy_value_to_bc_network(\n    policy_value_network: networks_lib.FeedForwardNetwork) -> BCPolicyNetwork:\n  \"\"\"Converts a policy-value network (e.g. from PPO) into a BC policy network.\n\n  Args:\n    policy_value_network: FeedForwardNetwork taking the observation as input.\n\n  Returns:\n    The BC policy network taking observation, is_training, key as input.\n  \"\"\"\n\n  def apply(params: networks_lib.Params,\n            observation: networks_lib.Observation,\n            *args,\n            is_training: bool = False,\n            key: Optional[types.PRNGKey] = None,\n            **kwargs) -> networks_lib.NetworkOutput:\n    del is_training, key\n    actions, _ = policy_value_network.apply(params, observation, *args,\n                                            **kwargs)\n    return actions\n\n  return BCPolicyNetwork(policy_value_network.init, apply)",
  "def __call__(self,\n               params: networks_lib.Params,\n               observation: networks_lib.Observation,\n               *args,\n               is_training: bool,\n               key: Optional[types.PRNGKey] = None,\n               **kwargs) -> networks_lib.NetworkOutput:\n    ...",
  "def apply(params: networks_lib.Params,\n            observation: networks_lib.Observation,\n            *args,\n            is_training: bool = False,\n            key: Optional[types.PRNGKey] = None,\n            **kwargs) -> networks_lib.NetworkOutput:\n    del is_training, key\n    return policy_network.apply(params, observation, *args, **kwargs)",
  "def apply(params: networks_lib.Params,\n            observation: networks_lib.Observation,\n            *args,\n            is_training: bool = False,\n            key: Optional[types.PRNGKey] = None,\n            **kwargs) -> networks_lib.NetworkOutput:\n    del is_training, key\n    actions, _ = policy_value_network.apply(params, observation, *args,\n                                            **kwargs)\n    return actions",
  "class BCConfig:\n  \"\"\"Configuration options for BC.\n\n  Attributes:\n    learning_rate: Learning rate.\n    num_sgd_steps_per_step: How many gradient updates to perform per step.\n  \"\"\"\n  learning_rate: float = 1e-4\n  num_sgd_steps_per_step: int = 1",
  "def train_with_bc(make_demonstrations: Callable[[int],\n                                                Iterator[types.Transition]],\n                  networks: bc_networks.BCNetworks,\n                  loss: losses.BCLoss,\n                  num_steps: int = 100000) -> networks_lib.Params:\n  \"\"\"Trains the given network with BC and returns the params.\n\n  Args:\n    make_demonstrations: A function (batch_size) -> iterator with demonstrations\n      to be imitated.\n    networks: Network taking (params, obs, is_training, key) as input\n    loss: BC loss to use.\n    num_steps: number of training steps\n\n  Returns:\n    The trained network params.\n  \"\"\"\n  demonstration_iterator = make_demonstrations(256)\n  prefetching_iterator = utils.sharded_prefetch(\n      demonstration_iterator,\n      buffer_size=2,\n      num_threads=jax.local_device_count())\n\n  learner = learning.BCLearner(\n      networks=networks,\n      random_key=jax.random.PRNGKey(0),\n      loss_fn=loss,\n      prefetching_iterator=prefetching_iterator,\n      optimizer=optax.adam(1e-4),\n      num_sgd_steps_per_step=1)\n\n  # Train the agent\n  for _ in range(num_steps):\n    learner.step()\n\n  return learner.get_variables(['policy'])[0]",
  "class BCBuilder(builders.OfflineBuilder[bc_networks.BCNetworks,\n                                        actor_core_lib.FeedForwardPolicy,\n                                        types.Transition]):\n  \"\"\"BC Builder.\"\"\"\n\n  def __init__(\n      self,\n      config: bc_config.BCConfig,\n      loss_fn: losses.BCLoss,\n      loss_has_aux: bool = False,\n  ):\n    \"\"\"Creates a BC learner, an evaluation policy and an eval actor.\n\n    Args:\n      config: a config with BC hps.\n      loss_fn: BC loss to use.\n      loss_has_aux: Whether the loss function returns auxiliary metrics as a\n        second argument.\n    \"\"\"\n    self._config = config\n    self._loss_fn = loss_fn\n    self._loss_has_aux = loss_has_aux\n\n  def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: bc_networks.BCNetworks,\n      dataset: Iterator[types.Transition],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      *,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    del environment_spec\n\n    return learning.BCLearner(\n        networks=networks,\n        random_key=random_key,\n        loss_fn=self._loss_fn,\n        optimizer=optax.adam(learning_rate=self._config.learning_rate),\n        prefetching_iterator=utils.sharded_prefetch(dataset),\n        num_sgd_steps_per_step=self._config.num_sgd_steps_per_step,\n        loss_has_aux=self._loss_has_aux,\n        logger=logger_fn('learner'),\n        counter=counter)\n\n  def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: actor_core_lib.FeedForwardPolicy,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n  ) -> core.Actor:\n    del environment_spec\n    assert variable_source is not None\n    actor_core = actor_core_lib.batched_feed_forward_to_actor_core(policy)\n    variable_client = variable_utils.VariableClient(\n        variable_source, 'policy', device='cpu')\n    return actors.GenericActor(\n        actor_core, random_key, variable_client, backend='cpu')\n\n  def make_policy(self,\n                  networks: bc_networks.BCNetworks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> actor_core_lib.FeedForwardPolicy:\n    \"\"\"Construct the policy.\"\"\"\n    del environment_spec, evaluation\n\n    def evaluation_policy(\n        params: networks_lib.Params, key: networks_lib.PRNGKey,\n        observation: networks_lib.Observation) -> networks_lib.Action:\n      apply_key, sample_key = jax.random.split(key)\n      network_output = networks.policy_network.apply(\n          params, observation, is_training=False, key=apply_key)\n      return networks.sample_fn(network_output, sample_key)\n\n    return evaluation_policy",
  "def __init__(\n      self,\n      config: bc_config.BCConfig,\n      loss_fn: losses.BCLoss,\n      loss_has_aux: bool = False,\n  ):\n    \"\"\"Creates a BC learner, an evaluation policy and an eval actor.\n\n    Args:\n      config: a config with BC hps.\n      loss_fn: BC loss to use.\n      loss_has_aux: Whether the loss function returns auxiliary metrics as a\n        second argument.\n    \"\"\"\n    self._config = config\n    self._loss_fn = loss_fn\n    self._loss_has_aux = loss_has_aux",
  "def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: bc_networks.BCNetworks,\n      dataset: Iterator[types.Transition],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      *,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    del environment_spec\n\n    return learning.BCLearner(\n        networks=networks,\n        random_key=random_key,\n        loss_fn=self._loss_fn,\n        optimizer=optax.adam(learning_rate=self._config.learning_rate),\n        prefetching_iterator=utils.sharded_prefetch(dataset),\n        num_sgd_steps_per_step=self._config.num_sgd_steps_per_step,\n        loss_has_aux=self._loss_has_aux,\n        logger=logger_fn('learner'),\n        counter=counter)",
  "def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: actor_core_lib.FeedForwardPolicy,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n  ) -> core.Actor:\n    del environment_spec\n    assert variable_source is not None\n    actor_core = actor_core_lib.batched_feed_forward_to_actor_core(policy)\n    variable_client = variable_utils.VariableClient(\n        variable_source, 'policy', device='cpu')\n    return actors.GenericActor(\n        actor_core, random_key, variable_client, backend='cpu')",
  "def make_policy(self,\n                  networks: bc_networks.BCNetworks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> actor_core_lib.FeedForwardPolicy:\n    \"\"\"Construct the policy.\"\"\"\n    del environment_spec, evaluation\n\n    def evaluation_policy(\n        params: networks_lib.Params, key: networks_lib.PRNGKey,\n        observation: networks_lib.Observation) -> networks_lib.Action:\n      apply_key, sample_key = jax.random.split(key)\n      network_output = networks.policy_network.apply(\n          params, observation, is_training=False, key=apply_key)\n      return networks.sample_fn(network_output, sample_key)\n\n    return evaluation_policy",
  "def evaluation_policy(\n        params: networks_lib.Params, key: networks_lib.PRNGKey,\n        observation: networks_lib.Observation) -> networks_lib.Action:\n      apply_key, sample_key = jax.random.split(key)\n      network_output = networks.policy_network.apply(\n          params, observation, is_training=False, key=apply_key)\n      return networks.sample_fn(network_output, sample_key)",
  "def make_networks(spec: specs.EnvironmentSpec,\n                  discrete_actions: bool = False) -> bc.BCNetworks:\n  \"\"\"Creates networks used by the agent.\"\"\"\n\n  if discrete_actions:\n    final_layer_size = spec.actions.num_values\n  else:\n    final_layer_size = np.prod(spec.actions.shape, dtype=int)\n\n  def _actor_fn(obs, is_training=False, key=None):\n    # is_training and key allows to defined train/test dependant modules\n    # like dropout.\n    del is_training\n    del key\n    if discrete_actions:\n      network = hk.nets.MLP([64, 64, final_layer_size])\n    else:\n      network = hk.Sequential([\n          networks_lib.LayerNormMLP([64, 64], activate_final=True),\n          networks_lib.NormalTanhDistribution(final_layer_size),\n      ])\n    return network(obs)\n\n  policy = hk.without_apply_rng(hk.transform(_actor_fn))\n\n  # Create dummy observations and actions to create network parameters.\n  dummy_obs = utils.zeros_like(spec.observations)\n  dummy_obs = utils.add_batch_dim(dummy_obs)\n  policy_network = networks_lib.FeedForwardNetwork(\n      lambda key: policy.init(key, dummy_obs), policy.apply)\n  bc_policy_network = bc.convert_to_bc_network(policy_network)\n\n  if discrete_actions:\n\n    def sample_fn(logits: networks_lib.NetworkOutput,\n                  key: jax_types.PRNGKey) -> networks_lib.Action:\n      return rlax.epsilon_greedy(epsilon=0.0).sample(key, logits)\n\n    def log_prob(logits: networks_lib.NetworkOutput,\n                 actions: networks_lib.Action) -> networks_lib.LogProb:\n      max_logits = jnp.max(logits, axis=-1, keepdims=True)\n      logits = logits - max_logits\n      logits_actions = jnp.sum(\n          jax.nn.one_hot(actions, spec.actions.num_values) * logits, axis=-1)\n\n      log_prob = logits_actions - special.logsumexp(logits, axis=-1)\n      return log_prob\n\n  else:\n\n    def sample_fn(distribution: networks_lib.NetworkOutput,\n                  key: jax_types.PRNGKey) -> networks_lib.Action:\n      return distribution.sample(seed=key)\n\n    def log_prob(distribuition: networks_lib.NetworkOutput,\n                 actions: networks_lib.Action) -> networks_lib.LogProb:\n      return distribuition.log_prob(actions)\n\n  return bc.BCNetworks(bc_policy_network, sample_fn, log_prob)",
  "class BCTest(parameterized.TestCase):\n\n  @parameterized.parameters(\n      ('logp',),\n      ('mse',),\n      ('peerbc',)\n      )\n  def test_continuous_actions(self, loss_name):\n    with chex.fake_pmap_and_jit():\n      num_sgd_steps_per_step = 1\n      num_steps = 5\n\n      # Create a fake environment to test with.\n      environment = fakes.ContinuousEnvironment(\n          episode_length=10, bounded=True, action_dim=6)\n\n      spec = specs.make_environment_spec(environment)\n      dataset_demonstration = fakes.transition_dataset(environment)\n      dataset_demonstration = dataset_demonstration.map(\n          lambda sample: types.Transition(*sample.data))\n      dataset_demonstration = dataset_demonstration.batch(8).as_numpy_iterator()\n\n      # Construct the agent.\n      networks = make_networks(spec)\n\n      if loss_name == 'logp':\n        loss_fn = bc.logp()\n      elif loss_name == 'mse':\n        loss_fn = bc.mse()\n      elif loss_name == 'peerbc':\n        loss_fn = bc.peerbc(bc.logp(), zeta=0.1)\n      else:\n        raise ValueError\n\n      learner = bc.BCLearner(\n          networks=networks,\n          random_key=jax.random.PRNGKey(0),\n          loss_fn=loss_fn,\n          optimizer=optax.adam(0.01),\n          prefetching_iterator=utils.sharded_prefetch(dataset_demonstration),\n          num_sgd_steps_per_step=num_sgd_steps_per_step)\n\n      # Train the agent\n      for _ in range(num_steps):\n        learner.step()\n\n  @parameterized.parameters(\n      ('logp',),\n      ('rcal',))\n  def test_discrete_actions(self, loss_name):\n    with chex.fake_pmap_and_jit():\n\n      num_sgd_steps_per_step = 1\n      num_steps = 5\n\n      # Create a fake environment to test with.\n      environment = fakes.DiscreteEnvironment(\n          num_actions=10, num_observations=100, obs_shape=(10,),\n          obs_dtype=np.float32)\n\n      spec = specs.make_environment_spec(environment)\n      dataset_demonstration = fakes.transition_dataset(environment)\n      dataset_demonstration = dataset_demonstration.map(\n          lambda sample: types.Transition(*sample.data))\n      dataset_demonstration = dataset_demonstration.batch(8).as_numpy_iterator()\n\n      # Construct the agent.\n      networks = make_networks(spec, discrete_actions=True)\n\n      if loss_name == 'logp':\n        loss_fn = bc.logp()\n\n      elif loss_name == 'rcal':\n        base_loss_fn = bc.logp()\n        loss_fn = bc.rcal(base_loss_fn, discount=0.99, alpha=0.1)\n\n      else:\n        raise ValueError\n\n      learner = bc.BCLearner(\n          networks=networks,\n          random_key=jax.random.PRNGKey(0),\n          loss_fn=loss_fn,\n          optimizer=optax.adam(0.01),\n          prefetching_iterator=utils.sharded_prefetch(dataset_demonstration),\n          num_sgd_steps_per_step=num_sgd_steps_per_step)\n\n      # Train the agent\n      for _ in range(num_steps):\n        learner.step()",
  "def _actor_fn(obs, is_training=False, key=None):\n    # is_training and key allows to defined train/test dependant modules\n    # like dropout.\n    del is_training\n    del key\n    if discrete_actions:\n      network = hk.nets.MLP([64, 64, final_layer_size])\n    else:\n      network = hk.Sequential([\n          networks_lib.LayerNormMLP([64, 64], activate_final=True),\n          networks_lib.NormalTanhDistribution(final_layer_size),\n      ])\n    return network(obs)",
  "def test_continuous_actions(self, loss_name):\n    with chex.fake_pmap_and_jit():\n      num_sgd_steps_per_step = 1\n      num_steps = 5\n\n      # Create a fake environment to test with.\n      environment = fakes.ContinuousEnvironment(\n          episode_length=10, bounded=True, action_dim=6)\n\n      spec = specs.make_environment_spec(environment)\n      dataset_demonstration = fakes.transition_dataset(environment)\n      dataset_demonstration = dataset_demonstration.map(\n          lambda sample: types.Transition(*sample.data))\n      dataset_demonstration = dataset_demonstration.batch(8).as_numpy_iterator()\n\n      # Construct the agent.\n      networks = make_networks(spec)\n\n      if loss_name == 'logp':\n        loss_fn = bc.logp()\n      elif loss_name == 'mse':\n        loss_fn = bc.mse()\n      elif loss_name == 'peerbc':\n        loss_fn = bc.peerbc(bc.logp(), zeta=0.1)\n      else:\n        raise ValueError\n\n      learner = bc.BCLearner(\n          networks=networks,\n          random_key=jax.random.PRNGKey(0),\n          loss_fn=loss_fn,\n          optimizer=optax.adam(0.01),\n          prefetching_iterator=utils.sharded_prefetch(dataset_demonstration),\n          num_sgd_steps_per_step=num_sgd_steps_per_step)\n\n      # Train the agent\n      for _ in range(num_steps):\n        learner.step()",
  "def test_discrete_actions(self, loss_name):\n    with chex.fake_pmap_and_jit():\n\n      num_sgd_steps_per_step = 1\n      num_steps = 5\n\n      # Create a fake environment to test with.\n      environment = fakes.DiscreteEnvironment(\n          num_actions=10, num_observations=100, obs_shape=(10,),\n          obs_dtype=np.float32)\n\n      spec = specs.make_environment_spec(environment)\n      dataset_demonstration = fakes.transition_dataset(environment)\n      dataset_demonstration = dataset_demonstration.map(\n          lambda sample: types.Transition(*sample.data))\n      dataset_demonstration = dataset_demonstration.batch(8).as_numpy_iterator()\n\n      # Construct the agent.\n      networks = make_networks(spec, discrete_actions=True)\n\n      if loss_name == 'logp':\n        loss_fn = bc.logp()\n\n      elif loss_name == 'rcal':\n        base_loss_fn = bc.logp()\n        loss_fn = bc.rcal(base_loss_fn, discount=0.99, alpha=0.1)\n\n      else:\n        raise ValueError\n\n      learner = bc.BCLearner(\n          networks=networks,\n          random_key=jax.random.PRNGKey(0),\n          loss_fn=loss_fn,\n          optimizer=optax.adam(0.01),\n          prefetching_iterator=utils.sharded_prefetch(dataset_demonstration),\n          num_sgd_steps_per_step=num_sgd_steps_per_step)\n\n      # Train the agent\n      for _ in range(num_steps):\n        learner.step()",
  "def sample_fn(logits: networks_lib.NetworkOutput,\n                  key: jax_types.PRNGKey) -> networks_lib.Action:\n      return rlax.epsilon_greedy(epsilon=0.0).sample(key, logits)",
  "def log_prob(logits: networks_lib.NetworkOutput,\n                 actions: networks_lib.Action) -> networks_lib.LogProb:\n      max_logits = jnp.max(logits, axis=-1, keepdims=True)\n      logits = logits - max_logits\n      logits_actions = jnp.sum(\n          jax.nn.one_hot(actions, spec.actions.num_values) * logits, axis=-1)\n\n      log_prob = logits_actions - special.logsumexp(logits, axis=-1)\n      return log_prob",
  "def sample_fn(distribution: networks_lib.NetworkOutput,\n                  key: jax_types.PRNGKey) -> networks_lib.Action:\n      return distribution.sample(seed=key)",
  "def log_prob(distribuition: networks_lib.NetworkOutput,\n                 actions: networks_lib.Action) -> networks_lib.LogProb:\n      return distribuition.log_prob(actions)",
  "class TrainingState(NamedTuple):\n  \"\"\"Training state consists of network parameters and optimiser state.\"\"\"\n  params: networks_lib.Params\n  opt_state: optax.OptState",
  "class IMPALALearner(acme.Learner):\n  \"\"\"Learner for an importanced-weighted advantage actor-critic.\"\"\"\n\n  def __init__(\n      self,\n      networks: impala_networks.IMPALANetworks,\n      iterator: Iterator[reverb.ReplaySample],\n      optimizer: optax.GradientTransformation,\n      random_key: networks_lib.PRNGKey,\n      discount: float = 0.99,\n      entropy_cost: float = 0.0,\n      baseline_cost: float = 1.0,\n      max_abs_reward: float = np.inf,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      devices: Optional[Sequence[jax.Device]] = None,\n      prefetch_size: int = 2,\n  ):\n    local_devices = jax.local_devices()\n    process_id = jax.process_index()\n    logging.info('Learner process id: %s. Devices passed: %s', process_id,\n                 devices)\n    logging.info('Learner process id: %s. Local devices from JAX API: %s',\n                 process_id, local_devices)\n    self._devices = devices or local_devices\n    self._local_devices = [d for d in self._devices if d in local_devices]\n\n    self._iterator = iterator\n\n    def unroll_without_rng(\n        params: networks_lib.Params, observations: networks_lib.Observation,\n        initial_state: networks_lib.RecurrentState\n    ) -> Tuple[networks_lib.NetworkOutput, networks_lib.RecurrentState]:\n      unused_rng = jax.random.PRNGKey(0)\n      return networks.unroll(params, unused_rng, observations, initial_state)\n\n    loss_fn = losses.impala_loss(\n        # TODO(b/244319884): Consider supporting the use of RNG in impala_loss.\n        unroll_fn=unroll_without_rng,\n        discount=discount,\n        max_abs_reward=max_abs_reward,\n        baseline_cost=baseline_cost,\n        entropy_cost=entropy_cost)\n\n    @jax.jit\n    def sgd_step(\n        state: TrainingState, sample: reverb.ReplaySample\n    ) -> Tuple[TrainingState, Dict[str, jnp.ndarray]]:\n      \"\"\"Computes an SGD step, returning new state and metrics for logging.\"\"\"\n\n      # Compute gradients.\n      grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n      (loss_value, metrics), gradients = grad_fn(state.params, sample)\n\n      # Average gradients over pmap replicas before optimizer update.\n      gradients = jax.lax.pmean(gradients, _PMAP_AXIS_NAME)\n\n      # Apply updates.\n      updates, new_opt_state = optimizer.update(gradients, state.opt_state)\n      new_params = optax.apply_updates(state.params, updates)\n\n      metrics.update({\n          'loss': loss_value,\n          'param_norm': optax.global_norm(new_params),\n          'param_updates_norm': optax.global_norm(updates),\n      })\n\n      new_state = TrainingState(params=new_params, opt_state=new_opt_state)\n\n      return new_state, metrics\n\n    def make_initial_state(key: jnp.ndarray) -> TrainingState:\n      \"\"\"Initialises the training state (parameters and optimiser state).\"\"\"\n      initial_params = networks.init(key)\n      return TrainingState(\n          params=initial_params, opt_state=optimizer.init(initial_params))\n\n    # Initialise training state (parameters and optimiser state).\n    state = make_initial_state(random_key)\n    self._state = utils.replicate_in_all_devices(state, self._local_devices)\n\n    self._sgd_step = jax.pmap(\n        sgd_step, axis_name=_PMAP_AXIS_NAME, devices=self._devices)\n\n    # Set up logging/counting.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger(\n        'learner', steps_key=self._counter.get_steps_key())\n\n  def step(self):\n    \"\"\"Does a step of SGD and logs the results.\"\"\"\n    samples = next(self._iterator)\n\n    # Do a batch of SGD.\n    start = time.time()\n    self._state, results = self._sgd_step(self._state, samples)\n\n    # Take results from first replica.\n    # NOTE: This measure will be a noisy estimate for the purposes of the logs\n    # as it does not pmean over all devices.\n    results = utils.get_from_first_device(results)\n\n    # Update our counts and record them.\n    counts = self._counter.increment(steps=1, time_elapsed=time.time() - start)\n\n    # Maybe write logs.\n    self._logger.write({**results, **counts})\n\n  def get_variables(self, names: Sequence[str]) -> List[networks_lib.Params]:\n    # Return first replica of parameters.\n    return utils.get_from_first_device([self._state.params], as_numpy=False)\n\n  def save(self) -> TrainingState:\n    # Serialize only the first replica of parameters and optimizer state.\n    return utils.get_from_first_device(self._state)\n\n  def restore(self, state: TrainingState):\n    self._state = utils.replicate_in_all_devices(state, self._local_devices)",
  "def __init__(\n      self,\n      networks: impala_networks.IMPALANetworks,\n      iterator: Iterator[reverb.ReplaySample],\n      optimizer: optax.GradientTransformation,\n      random_key: networks_lib.PRNGKey,\n      discount: float = 0.99,\n      entropy_cost: float = 0.0,\n      baseline_cost: float = 1.0,\n      max_abs_reward: float = np.inf,\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      devices: Optional[Sequence[jax.Device]] = None,\n      prefetch_size: int = 2,\n  ):\n    local_devices = jax.local_devices()\n    process_id = jax.process_index()\n    logging.info('Learner process id: %s. Devices passed: %s', process_id,\n                 devices)\n    logging.info('Learner process id: %s. Local devices from JAX API: %s',\n                 process_id, local_devices)\n    self._devices = devices or local_devices\n    self._local_devices = [d for d in self._devices if d in local_devices]\n\n    self._iterator = iterator\n\n    def unroll_without_rng(\n        params: networks_lib.Params, observations: networks_lib.Observation,\n        initial_state: networks_lib.RecurrentState\n    ) -> Tuple[networks_lib.NetworkOutput, networks_lib.RecurrentState]:\n      unused_rng = jax.random.PRNGKey(0)\n      return networks.unroll(params, unused_rng, observations, initial_state)\n\n    loss_fn = losses.impala_loss(\n        # TODO(b/244319884): Consider supporting the use of RNG in impala_loss.\n        unroll_fn=unroll_without_rng,\n        discount=discount,\n        max_abs_reward=max_abs_reward,\n        baseline_cost=baseline_cost,\n        entropy_cost=entropy_cost)\n\n    @jax.jit\n    def sgd_step(\n        state: TrainingState, sample: reverb.ReplaySample\n    ) -> Tuple[TrainingState, Dict[str, jnp.ndarray]]:\n      \"\"\"Computes an SGD step, returning new state and metrics for logging.\"\"\"\n\n      # Compute gradients.\n      grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n      (loss_value, metrics), gradients = grad_fn(state.params, sample)\n\n      # Average gradients over pmap replicas before optimizer update.\n      gradients = jax.lax.pmean(gradients, _PMAP_AXIS_NAME)\n\n      # Apply updates.\n      updates, new_opt_state = optimizer.update(gradients, state.opt_state)\n      new_params = optax.apply_updates(state.params, updates)\n\n      metrics.update({\n          'loss': loss_value,\n          'param_norm': optax.global_norm(new_params),\n          'param_updates_norm': optax.global_norm(updates),\n      })\n\n      new_state = TrainingState(params=new_params, opt_state=new_opt_state)\n\n      return new_state, metrics\n\n    def make_initial_state(key: jnp.ndarray) -> TrainingState:\n      \"\"\"Initialises the training state (parameters and optimiser state).\"\"\"\n      initial_params = networks.init(key)\n      return TrainingState(\n          params=initial_params, opt_state=optimizer.init(initial_params))\n\n    # Initialise training state (parameters and optimiser state).\n    state = make_initial_state(random_key)\n    self._state = utils.replicate_in_all_devices(state, self._local_devices)\n\n    self._sgd_step = jax.pmap(\n        sgd_step, axis_name=_PMAP_AXIS_NAME, devices=self._devices)\n\n    # Set up logging/counting.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger(\n        'learner', steps_key=self._counter.get_steps_key())",
  "def step(self):\n    \"\"\"Does a step of SGD and logs the results.\"\"\"\n    samples = next(self._iterator)\n\n    # Do a batch of SGD.\n    start = time.time()\n    self._state, results = self._sgd_step(self._state, samples)\n\n    # Take results from first replica.\n    # NOTE: This measure will be a noisy estimate for the purposes of the logs\n    # as it does not pmean over all devices.\n    results = utils.get_from_first_device(results)\n\n    # Update our counts and record them.\n    counts = self._counter.increment(steps=1, time_elapsed=time.time() - start)\n\n    # Maybe write logs.\n    self._logger.write({**results, **counts})",
  "def get_variables(self, names: Sequence[str]) -> List[networks_lib.Params]:\n    # Return first replica of parameters.\n    return utils.get_from_first_device([self._state.params], as_numpy=False)",
  "def save(self) -> TrainingState:\n    # Serialize only the first replica of parameters and optimizer state.\n    return utils.get_from_first_device(self._state)",
  "def restore(self, state: TrainingState):\n    self._state = utils.replicate_in_all_devices(state, self._local_devices)",
  "def unroll_without_rng(\n        params: networks_lib.Params, observations: networks_lib.Observation,\n        initial_state: networks_lib.RecurrentState\n    ) -> Tuple[networks_lib.NetworkOutput, networks_lib.RecurrentState]:\n      unused_rng = jax.random.PRNGKey(0)\n      return networks.unroll(params, unused_rng, observations, initial_state)",
  "def sgd_step(\n        state: TrainingState, sample: reverb.ReplaySample\n    ) -> Tuple[TrainingState, Dict[str, jnp.ndarray]]:\n      \"\"\"Computes an SGD step, returning new state and metrics for logging.\"\"\"\n\n      # Compute gradients.\n      grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n      (loss_value, metrics), gradients = grad_fn(state.params, sample)\n\n      # Average gradients over pmap replicas before optimizer update.\n      gradients = jax.lax.pmean(gradients, _PMAP_AXIS_NAME)\n\n      # Apply updates.\n      updates, new_opt_state = optimizer.update(gradients, state.opt_state)\n      new_params = optax.apply_updates(state.params, updates)\n\n      metrics.update({\n          'loss': loss_value,\n          'param_norm': optax.global_norm(new_params),\n          'param_updates_norm': optax.global_norm(updates),\n      })\n\n      new_state = TrainingState(params=new_params, opt_state=new_opt_state)\n\n      return new_state, metrics",
  "def make_initial_state(key: jnp.ndarray) -> TrainingState:\n      \"\"\"Initialises the training state (parameters and optimiser state).\"\"\"\n      initial_params = networks.init(key)\n      return TrainingState(\n          params=initial_params, opt_state=optimizer.init(initial_params))",
  "def make_atari_networks(env_spec: specs.EnvironmentSpec) -> IMPALANetworks:\n  \"\"\"Builds default IMPALA networks for Atari games.\"\"\"\n\n  def make_core_module() -> networks_lib.DeepIMPALAAtariNetwork:\n    return networks_lib.DeepIMPALAAtariNetwork(env_spec.actions.num_values)\n\n  return networks_lib.make_unrollable_network(env_spec, make_core_module)",
  "def make_core_module() -> networks_lib.DeepIMPALAAtariNetwork:\n    return networks_lib.DeepIMPALAAtariNetwork(env_spec.actions.num_values)",
  "class IMPALAConfig:\n  \"\"\"Configuration options for IMPALA.\"\"\"\n  seed: int = 0\n  discount: float = 0.99\n  sequence_length: int = 20\n  sequence_period: Optional[int] = None\n  variable_update_period: int = 1000\n\n  # Optimizer configuration.\n  batch_size: int = 32\n  learning_rate: Union[float, optax.Schedule] = 2e-4\n  adam_momentum_decay: float = 0.0\n  adam_variance_decay: float = 0.99\n  adam_eps: float = 1e-8\n  adam_eps_root: float = 0.0\n  max_gradient_norm: float = 40.0\n\n  # Loss configuration.\n  baseline_cost: float = 0.5\n  entropy_cost: float = 0.01\n  max_abs_reward: float = np.inf\n\n  # Replay options\n  replay_table_name: str = adders_reverb.DEFAULT_PRIORITY_TABLE\n  num_prefetch_threads: Optional[int] = None\n  samples_per_insert: Optional[float] = 1.0\n  max_queue_size: Union[int, types.Batches] = types.Batches(10)\n\n  def __post_init__(self):\n    if isinstance(self.max_queue_size, types.Batches):\n      self.max_queue_size *= self.batch_size\n    assert self.max_queue_size > self.batch_size + 1, (\"\"\"\n        max_queue_size must be strictly larger than the batch size:\n        - during the last step in an episode we might write 2 sequences to\n          Reverb at once (that's how SequenceAdder works)\n        - Reverb does insertion/sampling in multiple threads, so data is\n          added asynchronously at unpredictable times. Therefore we need\n          additional buffer size in order to avoid deadlocks.\"\"\")",
  "def __post_init__(self):\n    if isinstance(self.max_queue_size, types.Batches):\n      self.max_queue_size *= self.batch_size\n    assert self.max_queue_size > self.batch_size + 1, (\"\"\"\n        max_queue_size must be strictly larger than the batch size:\n        - during the last step in an episode we might write 2 sequences to\n          Reverb at once (that's how SequenceAdder works)\n        - Reverb does insertion/sampling in multiple threads, so data is\n          added asynchronously at unpredictable times. Therefore we need\n          additional buffer size in order to avoid deadlocks.\"\"\")",
  "class ImpalaActorState(Generic[actor_core_lib.RecurrentState]):\n  rng: jax_types.PRNGKey\n  logits: networks_lib.Logits\n  recurrent_state: actor_core_lib.RecurrentState\n  prev_recurrent_state: actor_core_lib.RecurrentState",
  "def get_actor_core(\n    networks: impala_networks.IMPALANetworks,\n    environment_spec: specs.EnvironmentSpec,\n    evaluation: bool = False,\n) -> ImpalaPolicy:\n  \"\"\"Creates an Impala ActorCore.\"\"\"\n\n  dummy_logits = jnp.zeros(environment_spec.actions.num_values)\n\n  def init(\n      rng: jax_types.PRNGKey\n  ) -> ImpalaActorState[actor_core_lib.RecurrentState]:\n    rng, init_state_rng = jax.random.split(rng)\n    initial_state = networks.init_recurrent_state(init_state_rng, None)\n    return ImpalaActorState(\n        rng=rng,\n        logits=dummy_logits,\n        recurrent_state=initial_state,\n        prev_recurrent_state=initial_state)\n\n  def select_action(\n      params: networks_lib.Params,\n      observation: networks_lib.Observation,\n      state: ImpalaActorState[actor_core_lib.RecurrentState],\n  ) -> Tuple[networks_lib.Action,\n             ImpalaActorState[actor_core_lib.RecurrentState]]:\n\n    rng, apply_rng, policy_rng = jax.random.split(state.rng, 3)\n    (logits, _), new_recurrent_state = networks.apply(\n        params,\n        apply_rng,\n        observation,\n        state.recurrent_state,\n    )\n\n    if evaluation:\n      action = jnp.argmax(logits, axis=-1)\n    else:\n      action = jax.random.categorical(policy_rng, logits)\n\n    return action, ImpalaActorState(\n        rng=rng,\n        logits=logits,\n        recurrent_state=new_recurrent_state,\n        prev_recurrent_state=state.recurrent_state)\n\n  def get_extras(\n      state: ImpalaActorState[actor_core_lib.RecurrentState]) -> ImpalaExtras:\n    return {'logits': state.logits, 'core_state': state.prev_recurrent_state}\n\n  return actor_core_lib.ActorCore(\n      init=init, select_action=select_action, get_extras=get_extras)",
  "def init(\n      rng: jax_types.PRNGKey\n  ) -> ImpalaActorState[actor_core_lib.RecurrentState]:\n    rng, init_state_rng = jax.random.split(rng)\n    initial_state = networks.init_recurrent_state(init_state_rng, None)\n    return ImpalaActorState(\n        rng=rng,\n        logits=dummy_logits,\n        recurrent_state=initial_state,\n        prev_recurrent_state=initial_state)",
  "def select_action(\n      params: networks_lib.Params,\n      observation: networks_lib.Observation,\n      state: ImpalaActorState[actor_core_lib.RecurrentState],\n  ) -> Tuple[networks_lib.Action,\n             ImpalaActorState[actor_core_lib.RecurrentState]]:\n\n    rng, apply_rng, policy_rng = jax.random.split(state.rng, 3)\n    (logits, _), new_recurrent_state = networks.apply(\n        params,\n        apply_rng,\n        observation,\n        state.recurrent_state,\n    )\n\n    if evaluation:\n      action = jnp.argmax(logits, axis=-1)\n    else:\n      action = jax.random.categorical(policy_rng, logits)\n\n    return action, ImpalaActorState(\n        rng=rng,\n        logits=logits,\n        recurrent_state=new_recurrent_state,\n        prev_recurrent_state=state.recurrent_state)",
  "def get_extras(\n      state: ImpalaActorState[actor_core_lib.RecurrentState]) -> ImpalaExtras:\n    return {'logits': state.logits, 'core_state': state.prev_recurrent_state}",
  "class IMPALABuilder(Generic[actor_core_lib.RecurrentState],\n                    builders.ActorLearnerBuilder[impala_networks.IMPALANetworks,\n                                                 acting.ImpalaPolicy,\n                                                 reverb.ReplaySample]):\n  \"\"\"IMPALA Builder.\"\"\"\n\n  def __init__(\n      self,\n      config: impala_config.IMPALAConfig,\n      table_extension: Optional[Callable[[], Any]] = None,\n  ):\n    \"\"\"Creates an IMPALA learner.\"\"\"\n    self._config = config\n    self._sequence_length = self._config.sequence_length\n    self._table_extension = table_extension\n\n  def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: acting.ImpalaPolicy,\n  ) -> List[reverb.Table]:\n    \"\"\"The queue; use XData or INFO log.\"\"\"\n    dummy_actor_state = policy.init(jax.random.PRNGKey(0))\n    signature = reverb_adders.SequenceAdder.signature(\n        environment_spec,\n        policy.get_extras(dummy_actor_state),\n        sequence_length=self._config.sequence_length)\n\n    # Maybe create rate limiter.\n    # Setting the samples_per_insert ratio less than the default of 1.0, allows\n    # the agent to drop data for the benefit of using data from most up-to-date\n    # policies to compute its learner updates.\n    samples_per_insert = self._config.samples_per_insert\n    if samples_per_insert:\n      if samples_per_insert > 1.0 or samples_per_insert <= 0.0:\n        raise ValueError(\n            'Impala requires a samples_per_insert ratio in the range (0, 1],'\n            f' but received {samples_per_insert}.')\n      limiter = reverb.rate_limiters.SampleToInsertRatio(\n          samples_per_insert=samples_per_insert,\n          min_size_to_sample=1,\n          error_buffer=self._config.batch_size)\n    else:\n      limiter = reverb.rate_limiters.MinSize(1)\n\n    table_extensions = []\n    if self._table_extension is not None:\n      table_extensions = [self._table_extension()]\n    queue = reverb.Table(\n        name=self._config.replay_table_name,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=self._config.max_queue_size,\n        max_times_sampled=1,\n        rate_limiter=limiter,\n        extensions=table_extensions,\n        signature=signature)\n    return [queue]\n\n  def make_dataset_iterator(\n      self, replay_client: reverb.Client) -> Iterator[reverb.ReplaySample]:\n    \"\"\"Creates a dataset.\"\"\"\n    batch_size_per_learner = self._config.batch_size // jax.process_count()\n    batch_size_per_device, ragged = divmod(self._config.batch_size,\n                                           jax.device_count())\n    if ragged:\n      raise ValueError(\n          'Learner batch size must be divisible by total number of devices!')\n\n    dataset = datasets.make_reverb_dataset(\n        table=self._config.replay_table_name,\n        server_address=replay_client.server_address,\n        batch_size=batch_size_per_device,\n        num_parallel_calls=None,\n        max_in_flight_samples_per_worker=2 * batch_size_per_learner)\n\n    return utils.multi_device_put(dataset.as_numpy_iterator(),\n                                  jax.local_devices())\n\n  def make_adder(\n      self,\n      replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[acting.ImpalaPolicy],\n  ) -> Optional[adders.Adder]:\n    \"\"\"Creates an adder which handles observations.\"\"\"\n    del environment_spec, policy\n    # Note that the last transition in the sequence is used for bootstrapping\n    # only and is ignored otherwise. So we need to make sure that sequences\n    # overlap on one transition, thus \"-1\" in the period length computation.\n    return reverb_adders.SequenceAdder(\n        client=replay_client,\n        priority_fns={self._config.replay_table_name: None},\n        period=self._config.sequence_period or (self._sequence_length - 1),\n        sequence_length=self._sequence_length,\n    )\n\n  def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: impala_networks.IMPALANetworks,\n      dataset: Iterator[reverb.ReplaySample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    del environment_spec, replay_client\n\n    optimizer = optax.chain(\n        optax.clip_by_global_norm(self._config.max_gradient_norm),\n        optax.adam(\n            self._config.learning_rate,\n            b1=self._config.adam_momentum_decay,\n            b2=self._config.adam_variance_decay,\n            eps=self._config.adam_eps,\n            eps_root=self._config.adam_eps_root))\n\n    return learning.IMPALALearner(\n        networks=networks,\n        iterator=dataset,\n        optimizer=optimizer,\n        random_key=random_key,\n        discount=self._config.discount,\n        entropy_cost=self._config.entropy_cost,\n        baseline_cost=self._config.baseline_cost,\n        max_abs_reward=self._config.max_abs_reward,\n        counter=counter,\n        logger=logger_fn('learner'),\n    )\n\n  def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: acting.ImpalaPolicy,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> acme.Actor:\n    del environment_spec\n    variable_client = variable_utils.VariableClient(\n        client=variable_source,\n        key='network',\n        update_period=self._config.variable_update_period)\n    return actors_lib.GenericActor(policy, random_key, variable_client, adder)\n\n  def make_policy(self,\n                  networks: impala_networks.IMPALANetworks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> acting.ImpalaPolicy:\n    return acting.get_actor_core(networks, environment_spec, evaluation)",
  "def __init__(\n      self,\n      config: impala_config.IMPALAConfig,\n      table_extension: Optional[Callable[[], Any]] = None,\n  ):\n    \"\"\"Creates an IMPALA learner.\"\"\"\n    self._config = config\n    self._sequence_length = self._config.sequence_length\n    self._table_extension = table_extension",
  "def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: acting.ImpalaPolicy,\n  ) -> List[reverb.Table]:\n    \"\"\"The queue; use XData or INFO log.\"\"\"\n    dummy_actor_state = policy.init(jax.random.PRNGKey(0))\n    signature = reverb_adders.SequenceAdder.signature(\n        environment_spec,\n        policy.get_extras(dummy_actor_state),\n        sequence_length=self._config.sequence_length)\n\n    # Maybe create rate limiter.\n    # Setting the samples_per_insert ratio less than the default of 1.0, allows\n    # the agent to drop data for the benefit of using data from most up-to-date\n    # policies to compute its learner updates.\n    samples_per_insert = self._config.samples_per_insert\n    if samples_per_insert:\n      if samples_per_insert > 1.0 or samples_per_insert <= 0.0:\n        raise ValueError(\n            'Impala requires a samples_per_insert ratio in the range (0, 1],'\n            f' but received {samples_per_insert}.')\n      limiter = reverb.rate_limiters.SampleToInsertRatio(\n          samples_per_insert=samples_per_insert,\n          min_size_to_sample=1,\n          error_buffer=self._config.batch_size)\n    else:\n      limiter = reverb.rate_limiters.MinSize(1)\n\n    table_extensions = []\n    if self._table_extension is not None:\n      table_extensions = [self._table_extension()]\n    queue = reverb.Table(\n        name=self._config.replay_table_name,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=self._config.max_queue_size,\n        max_times_sampled=1,\n        rate_limiter=limiter,\n        extensions=table_extensions,\n        signature=signature)\n    return [queue]",
  "def make_dataset_iterator(\n      self, replay_client: reverb.Client) -> Iterator[reverb.ReplaySample]:\n    \"\"\"Creates a dataset.\"\"\"\n    batch_size_per_learner = self._config.batch_size // jax.process_count()\n    batch_size_per_device, ragged = divmod(self._config.batch_size,\n                                           jax.device_count())\n    if ragged:\n      raise ValueError(\n          'Learner batch size must be divisible by total number of devices!')\n\n    dataset = datasets.make_reverb_dataset(\n        table=self._config.replay_table_name,\n        server_address=replay_client.server_address,\n        batch_size=batch_size_per_device,\n        num_parallel_calls=None,\n        max_in_flight_samples_per_worker=2 * batch_size_per_learner)\n\n    return utils.multi_device_put(dataset.as_numpy_iterator(),\n                                  jax.local_devices())",
  "def make_adder(\n      self,\n      replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[acting.ImpalaPolicy],\n  ) -> Optional[adders.Adder]:\n    \"\"\"Creates an adder which handles observations.\"\"\"\n    del environment_spec, policy\n    # Note that the last transition in the sequence is used for bootstrapping\n    # only and is ignored otherwise. So we need to make sure that sequences\n    # overlap on one transition, thus \"-1\" in the period length computation.\n    return reverb_adders.SequenceAdder(\n        client=replay_client,\n        priority_fns={self._config.replay_table_name: None},\n        period=self._config.sequence_period or (self._sequence_length - 1),\n        sequence_length=self._sequence_length,\n    )",
  "def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: impala_networks.IMPALANetworks,\n      dataset: Iterator[reverb.ReplaySample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    del environment_spec, replay_client\n\n    optimizer = optax.chain(\n        optax.clip_by_global_norm(self._config.max_gradient_norm),\n        optax.adam(\n            self._config.learning_rate,\n            b1=self._config.adam_momentum_decay,\n            b2=self._config.adam_variance_decay,\n            eps=self._config.adam_eps,\n            eps_root=self._config.adam_eps_root))\n\n    return learning.IMPALALearner(\n        networks=networks,\n        iterator=dataset,\n        optimizer=optimizer,\n        random_key=random_key,\n        discount=self._config.discount,\n        entropy_cost=self._config.entropy_cost,\n        baseline_cost=self._config.baseline_cost,\n        max_abs_reward=self._config.max_abs_reward,\n        counter=counter,\n        logger=logger_fn('learner'),\n    )",
  "def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: acting.ImpalaPolicy,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> acme.Actor:\n    del environment_spec\n    variable_client = variable_utils.VariableClient(\n        client=variable_source,\n        key='network',\n        update_period=self._config.variable_update_period)\n    return actors_lib.GenericActor(policy, random_key, variable_client, adder)",
  "def make_policy(self,\n                  networks: impala_networks.IMPALANetworks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> acting.ImpalaPolicy:\n    return acting.get_actor_core(networks, environment_spec, evaluation)",
  "class TestStatisticsAdder(adders.Adder):\n\n  def __init__(self):\n    self.counts = collections.defaultdict(int)\n\n  def reset(self):\n    pass\n\n  def add_first(self, timestep: dm_env.TimeStep):\n    self.counts[int(timestep.observation[0])] += 1\n\n  def add(self,\n          action: types.NestedArray,\n          next_timestep: dm_env.TimeStep,\n          extras: types.NestedArray = ()):\n    del action\n    del extras\n    self.counts[int(next_timestep.observation[0])] += 1",
  "class LfdAdderTest(absltest.TestCase):\n\n  def setUp(self):\n    super().setUp()\n    self._demonstration_episode_type = 1\n    self._demonstration_episode_length = 10\n    self._collected_episode_type = 2\n    self._collected_episode_length = 5\n\n  def generate_episode(self, episode_type, episode_index, length):\n    episode = []\n    action_dim = 8\n    obs_dim = 16\n    for k in range(length):\n      if k == 0:\n        action = None\n      else:\n        action = np.concatenate([\n            np.asarray([episode_type, episode_index], dtype=np.float32),\n            np.random.uniform(0., 1., (action_dim - 2,))])\n      observation = np.concatenate([\n          np.asarray([episode_type, episode_index], dtype=np.float32),\n          np.random.uniform(0., 1., (obs_dim - 2,))])\n      if k == 0:\n        timestep = dm_env.restart(observation)\n      elif k == length - 1:\n        timestep = dm_env.termination(0., observation)\n      else:\n        timestep = dm_env.transition(0., observation, 1.)\n      episode.append((action, timestep))\n    return episode\n\n  def generate_demonstration(self):\n    episode_index = 0\n    while True:\n      episode = self.generate_episode(self._demonstration_episode_type,\n                                      episode_index,\n                                      self._demonstration_episode_length)\n      for x in episode:\n        yield x\n      episode_index += 1\n\n  def test_adder(self):\n    stats_adder = TestStatisticsAdder()\n    demonstration_ratio = 0.2\n    initial_insert_count = 50\n    adder = lfd_adder.LfdAdder(\n        stats_adder,\n        self.generate_demonstration(),\n        initial_insert_count=initial_insert_count,\n        demonstration_ratio=demonstration_ratio)\n\n    num_episodes = 100\n    for episode_index in range(num_episodes):\n      episode = self.generate_episode(self._collected_episode_type,\n                                      episode_index,\n                                      self._collected_episode_length)\n      for k, (action, timestep) in enumerate(episode):\n        if k == 0:\n          adder.add_first(timestep)\n          if episode_index == 0:\n            self.assertGreaterEqual(\n                stats_adder.counts[self._demonstration_episode_type],\n                initial_insert_count - self._demonstration_episode_length)\n            self.assertLessEqual(\n                stats_adder.counts[self._demonstration_episode_type],\n                initial_insert_count + self._demonstration_episode_length)\n        else:\n          adder.add(action, timestep)\n\n    # Only 2 types of episodes.\n    self.assertLen(stats_adder.counts, 2)\n\n    total_count = (stats_adder.counts[self._demonstration_episode_type] +\n                   stats_adder.counts[self._collected_episode_type])\n    # The demonstration ratio does not account for the initial demonstration\n    # insertion. Computes a ratio that takes it into account.\n    target_ratio = (\n        demonstration_ratio * (float)(total_count - initial_insert_count)\n        + initial_insert_count) / (float)(total_count)\n    # Effective ratio of demonstrations.\n    effective_ratio = (\n        float(stats_adder.counts[self._demonstration_episode_type]) /\n        float(total_count))\n    # Only full episodes can be fed to the adder so the effective ratio\n    # might be slightly different from the requested demonstration ratio.\n    min_ratio = (target_ratio  -\n                 self._demonstration_episode_length / float(total_count))\n    max_ratio = (target_ratio +\n                 self._demonstration_episode_length / float(total_count))\n    self.assertGreaterEqual(effective_ratio, min_ratio)\n    self.assertLessEqual(effective_ratio, max_ratio)",
  "def __init__(self):\n    self.counts = collections.defaultdict(int)",
  "def reset(self):\n    pass",
  "def add_first(self, timestep: dm_env.TimeStep):\n    self.counts[int(timestep.observation[0])] += 1",
  "def add(self,\n          action: types.NestedArray,\n          next_timestep: dm_env.TimeStep,\n          extras: types.NestedArray = ()):\n    del action\n    del extras\n    self.counts[int(next_timestep.observation[0])] += 1",
  "def setUp(self):\n    super().setUp()\n    self._demonstration_episode_type = 1\n    self._demonstration_episode_length = 10\n    self._collected_episode_type = 2\n    self._collected_episode_length = 5",
  "def generate_episode(self, episode_type, episode_index, length):\n    episode = []\n    action_dim = 8\n    obs_dim = 16\n    for k in range(length):\n      if k == 0:\n        action = None\n      else:\n        action = np.concatenate([\n            np.asarray([episode_type, episode_index], dtype=np.float32),\n            np.random.uniform(0., 1., (action_dim - 2,))])\n      observation = np.concatenate([\n          np.asarray([episode_type, episode_index], dtype=np.float32),\n          np.random.uniform(0., 1., (obs_dim - 2,))])\n      if k == 0:\n        timestep = dm_env.restart(observation)\n      elif k == length - 1:\n        timestep = dm_env.termination(0., observation)\n      else:\n        timestep = dm_env.transition(0., observation, 1.)\n      episode.append((action, timestep))\n    return episode",
  "def generate_demonstration(self):\n    episode_index = 0\n    while True:\n      episode = self.generate_episode(self._demonstration_episode_type,\n                                      episode_index,\n                                      self._demonstration_episode_length)\n      for x in episode:\n        yield x\n      episode_index += 1",
  "def test_adder(self):\n    stats_adder = TestStatisticsAdder()\n    demonstration_ratio = 0.2\n    initial_insert_count = 50\n    adder = lfd_adder.LfdAdder(\n        stats_adder,\n        self.generate_demonstration(),\n        initial_insert_count=initial_insert_count,\n        demonstration_ratio=demonstration_ratio)\n\n    num_episodes = 100\n    for episode_index in range(num_episodes):\n      episode = self.generate_episode(self._collected_episode_type,\n                                      episode_index,\n                                      self._collected_episode_length)\n      for k, (action, timestep) in enumerate(episode):\n        if k == 0:\n          adder.add_first(timestep)\n          if episode_index == 0:\n            self.assertGreaterEqual(\n                stats_adder.counts[self._demonstration_episode_type],\n                initial_insert_count - self._demonstration_episode_length)\n            self.assertLessEqual(\n                stats_adder.counts[self._demonstration_episode_type],\n                initial_insert_count + self._demonstration_episode_length)\n        else:\n          adder.add(action, timestep)\n\n    # Only 2 types of episodes.\n    self.assertLen(stats_adder.counts, 2)\n\n    total_count = (stats_adder.counts[self._demonstration_episode_type] +\n                   stats_adder.counts[self._collected_episode_type])\n    # The demonstration ratio does not account for the initial demonstration\n    # insertion. Computes a ratio that takes it into account.\n    target_ratio = (\n        demonstration_ratio * (float)(total_count - initial_insert_count)\n        + initial_insert_count) / (float)(total_count)\n    # Effective ratio of demonstrations.\n    effective_ratio = (\n        float(stats_adder.counts[self._demonstration_episode_type]) /\n        float(total_count))\n    # Only full episodes can be fed to the adder so the effective ratio\n    # might be slightly different from the requested demonstration ratio.\n    min_ratio = (target_ratio  -\n                 self._demonstration_episode_length / float(total_count))\n    max_ratio = (target_ratio +\n                 self._demonstration_episode_length / float(total_count))\n    self.assertGreaterEqual(effective_ratio, min_ratio)\n    self.assertLessEqual(effective_ratio, max_ratio)",
  "class TD3fDConfig:\n  \"\"\"Configuration options specific to TD3 with demonstrations.\n\n  Attributes:\n    lfd_config: LfD config.\n    td3_config: TD3 config.\n  \"\"\"\n  lfd_config: config.LfdConfig\n  td3_config: td3.TD3Config",
  "class TD3fDBuilder(builder.LfdBuilder[td3.TD3Networks,\n                                      actor_core_lib.FeedForwardPolicy,\n                                      reverb.ReplaySample]):\n  \"\"\"Builder for TD3 agent learning from demonstrations.\"\"\"\n\n  def __init__(self, td3_fd_config: TD3fDConfig,\n               lfd_iterator_fn: Callable[[], Iterator[builder.LfdStep]]):\n    td3_builder = td3.TD3Builder(td3_fd_config.td3_config)\n    super().__init__(td3_builder, lfd_iterator_fn, td3_fd_config.lfd_config)",
  "def __init__(self, td3_fd_config: TD3fDConfig,\n               lfd_iterator_fn: Callable[[], Iterator[builder.LfdStep]]):\n    td3_builder = td3.TD3Builder(td3_fd_config.td3_config)\n    super().__init__(td3_builder, lfd_iterator_fn, td3_fd_config.lfd_config)",
  "class LfdAdder(adders.Adder):\n  \"\"\"Adder which adds from time to time some demonstrations.\n\n  Lfd stands for Learning From Demonstrations and is the same technique\n  as the one used in R2D3.\n  \"\"\"\n\n  def __init__(self,\n               adder: adders.Adder,\n               demonstrations: Iterator[Tuple[Any, dm_env.TimeStep]],\n               initial_insert_count: int,\n               demonstration_ratio: float):\n    \"\"\"LfdAdder constructor.\n\n    Args:\n      adder: The underlying adder used to add mixed episodes.\n      demonstrations: An iterator on infinite stream of (action, next_timestep)\n        pairs. Episode boundaries are defined by TimeStep.FIRST and\n        timestep.LAST markers. Note that the first action of an episode is\n        ignored. Note also that proper uniform sampling of demonstrations is the\n        responsibility of the iterator.\n      initial_insert_count: Number of steps of demonstrations to add before\n        adding any step of the collected episodes. Note that since only full\n        episodes can be added, this number of steps is only a target.\n      demonstration_ratio: Ratio of demonstration steps to add to the underlying\n        adder. ratio = num_demonstration_steps_added / total_num_steps_added\n        and must be in [0, 1).\n        Note that this ratio is the desired ratio in the steady behavior\n        and does not account for the initial inserts of demonstrations.\n        Note also that this ratio is only a target ratio since the granularity\n        is the episode.\n    \"\"\"\n    self._adder = adder\n    self._demonstrations = demonstrations\n    self._demonstration_ratio = demonstration_ratio\n    if demonstration_ratio < 0 or demonstration_ratio >= 1.:\n      raise ValueError('Invalid demonstration ratio.')\n\n    # Number of demonstration steps that should have been added to the replay\n    # buffer to meet the target demonstration ratio minus what has been really\n    # added.\n    # As a consequence:\n    # - when this delta is zero, the effective ratio exactly matches the desired\n    #   ratio\n    # - when it is positive, more demonstrations need to be added to\n    #   reestablish the balance\n    # The initial value is set so that after exactly initial_insert_count\n    # inserts of demonstration steps, _delta_demonstration_step_count will be\n    # zero.\n    self._delta_demonstration_step_count = (\n        (1. - self._demonstration_ratio) * initial_insert_count)\n\n  def reset(self):\n    self._adder.reset()\n\n  def _add_demonstration_episode(self):\n    _, timestep = next(self._demonstrations)\n    if not timestep.first():\n      raise ValueError('Expecting the start of an episode.')\n    self._adder.add_first(timestep)\n    self._delta_demonstration_step_count -= (1. - self._demonstration_ratio)\n    while not timestep.last():\n      action, timestep = next(self._demonstrations)\n      self._adder.add(action, timestep)\n      self._delta_demonstration_step_count -= (1. - self._demonstration_ratio)\n\n    # Reset is being called periodically to reset the connection to reverb.\n    # TODO(damienv, bshahr): Make the reset an internal detail of the reverb\n    # adder and remove it from the adder API.\n    self._adder.reset()\n\n  def add_first(self, timestep: dm_env.TimeStep):\n    while self._delta_demonstration_step_count > 0.:\n      self._add_demonstration_episode()\n\n    self._adder.add_first(timestep)\n    self._delta_demonstration_step_count += self._demonstration_ratio\n\n  def add(self,\n          action: types.NestedArray,\n          next_timestep: dm_env.TimeStep,\n          extras: types.NestedArray = ()):\n    self._adder.add(action, next_timestep)\n    self._delta_demonstration_step_count += self._demonstration_ratio",
  "def __init__(self,\n               adder: adders.Adder,\n               demonstrations: Iterator[Tuple[Any, dm_env.TimeStep]],\n               initial_insert_count: int,\n               demonstration_ratio: float):\n    \"\"\"LfdAdder constructor.\n\n    Args:\n      adder: The underlying adder used to add mixed episodes.\n      demonstrations: An iterator on infinite stream of (action, next_timestep)\n        pairs. Episode boundaries are defined by TimeStep.FIRST and\n        timestep.LAST markers. Note that the first action of an episode is\n        ignored. Note also that proper uniform sampling of demonstrations is the\n        responsibility of the iterator.\n      initial_insert_count: Number of steps of demonstrations to add before\n        adding any step of the collected episodes. Note that since only full\n        episodes can be added, this number of steps is only a target.\n      demonstration_ratio: Ratio of demonstration steps to add to the underlying\n        adder. ratio = num_demonstration_steps_added / total_num_steps_added\n        and must be in [0, 1).\n        Note that this ratio is the desired ratio in the steady behavior\n        and does not account for the initial inserts of demonstrations.\n        Note also that this ratio is only a target ratio since the granularity\n        is the episode.\n    \"\"\"\n    self._adder = adder\n    self._demonstrations = demonstrations\n    self._demonstration_ratio = demonstration_ratio\n    if demonstration_ratio < 0 or demonstration_ratio >= 1.:\n      raise ValueError('Invalid demonstration ratio.')\n\n    # Number of demonstration steps that should have been added to the replay\n    # buffer to meet the target demonstration ratio minus what has been really\n    # added.\n    # As a consequence:\n    # - when this delta is zero, the effective ratio exactly matches the desired\n    #   ratio\n    # - when it is positive, more demonstrations need to be added to\n    #   reestablish the balance\n    # The initial value is set so that after exactly initial_insert_count\n    # inserts of demonstration steps, _delta_demonstration_step_count will be\n    # zero.\n    self._delta_demonstration_step_count = (\n        (1. - self._demonstration_ratio) * initial_insert_count)",
  "def reset(self):\n    self._adder.reset()",
  "def _add_demonstration_episode(self):\n    _, timestep = next(self._demonstrations)\n    if not timestep.first():\n      raise ValueError('Expecting the start of an episode.')\n    self._adder.add_first(timestep)\n    self._delta_demonstration_step_count -= (1. - self._demonstration_ratio)\n    while not timestep.last():\n      action, timestep = next(self._demonstrations)\n      self._adder.add(action, timestep)\n      self._delta_demonstration_step_count -= (1. - self._demonstration_ratio)\n\n    # Reset is being called periodically to reset the connection to reverb.\n    # TODO(damienv, bshahr): Make the reset an internal detail of the reverb\n    # adder and remove it from the adder API.\n    self._adder.reset()",
  "def add_first(self, timestep: dm_env.TimeStep):\n    while self._delta_demonstration_step_count > 0.:\n      self._add_demonstration_episode()\n\n    self._adder.add_first(timestep)\n    self._delta_demonstration_step_count += self._demonstration_ratio",
  "def add(self,\n          action: types.NestedArray,\n          next_timestep: dm_env.TimeStep,\n          extras: types.NestedArray = ()):\n    self._adder.add(action, next_timestep)\n    self._delta_demonstration_step_count += self._demonstration_ratio",
  "class SACfDConfig:\n  \"\"\"Configuration options specific to SAC with demonstrations.\n\n  Attributes:\n    lfd_config: LfD config.\n    sac_config: SAC config.\n  \"\"\"\n  lfd_config: config.LfdConfig\n  sac_config: sac.SACConfig",
  "class SACfDBuilder(builder.LfdBuilder[sac.SACNetworks,\n                                      actor_core_lib.FeedForwardPolicy,\n                                      reverb.ReplaySample]):\n  \"\"\"Builder for SAC agent learning from demonstrations.\"\"\"\n\n  def __init__(self, sac_fd_config: SACfDConfig,\n               lfd_iterator_fn: Callable[[], Iterator[builder.LfdStep]]):\n    sac_builder = sac.SACBuilder(sac_fd_config.sac_config)\n    super().__init__(sac_builder, lfd_iterator_fn, sac_fd_config.lfd_config)",
  "def __init__(self, sac_fd_config: SACfDConfig,\n               lfd_iterator_fn: Callable[[], Iterator[builder.LfdStep]]):\n    sac_builder = sac.SACBuilder(sac_fd_config.sac_config)\n    super().__init__(sac_builder, lfd_iterator_fn, sac_fd_config.lfd_config)",
  "class LfdConfig:\n  \"\"\"Configuration options for LfD.\n\n  Attributes:\n    initial_insert_count: Number of steps of demonstrations to add to the replay\n      buffer before adding any step of the collected episodes. Note that since\n      only full episodes can be added, this number of steps is only a target.\n    demonstration_ratio: Ratio of demonstration steps to add to the replay\n      buffer. ratio = num_demonstration_steps_added / total_num_steps_added.\n      The ratio must be in [0, 1).\n      Note that this ratio is the desired ratio in the steady behavior and does\n      not account for the initial demonstrations inserts.\n      Note also that this ratio is only a target ratio since the granularity\n      is the episode.\n  \"\"\"\n  initial_insert_count: int = 0\n  demonstration_ratio: float = 0.01",
  "class LfdBuilder(builders.ActorLearnerBuilder[builders.Networks,\n                                              builders.Policy,\n                                              builders.Sample,],\n                 Generic[builders.Networks, builders.Policy, builders.Sample]):\n  \"\"\"Builder that enables Learning From demonstrations.\n\n  This builder is not self contained and requires an underlying builder\n  implementing an off-policy algorithm.\n  \"\"\"\n\n  def __init__(self, builder: builders.ActorLearnerBuilder[builders.Networks,\n                                                           builders.Policy,\n                                                           builders.Sample],\n               demonstrations_factory: Callable[[], Iterator[LfdStep]],\n               config: lfd_config.LfdConfig):\n    \"\"\"LfdBuilder constructor.\n\n    Args:\n      builder: The underlying builder implementing the off-policy algorithm.\n      demonstrations_factory: Factory returning an infinite stream (as an\n        iterator) of (action, next_timesteps). Episode boundaries in this stream\n        are given by timestep.first() and timestep.last(). Note that in the\n        distributed version of this algorithm, each actor is mixing the same\n        demonstrations with its online experience. This effectively results in\n        the demonstrations being replicated in the replay buffer as many times\n        as the number of actors being used.\n      config: LfD configuration.\n    \"\"\"\n    self._builder = builder\n    self._demonstrations_factory = demonstrations_factory\n    self._config = config\n\n  def make_replay_tables(self, *args, **kwargs):\n    return self._builder.make_replay_tables(*args, **kwargs)\n\n  def make_dataset_iterator(self, *args, **kwargs):\n    return self._builder.make_dataset_iterator(*args, **kwargs)\n\n  def make_adder(self, *args, **kwargs):\n    demonstrations = self._demonstrations_factory()\n    return lfd_adder.LfdAdder(self._builder.make_adder(*args, **kwargs),\n                              demonstrations,\n                              self._config.initial_insert_count,\n                              self._config.demonstration_ratio)\n\n  def make_actor(self, *args, **kwargs):\n    return self._builder.make_actor(*args, **kwargs)\n\n  def make_learner(self, *args, **kwargs):\n    return self._builder.make_learner(*args, **kwargs)\n\n  def make_policy(self, *args, **kwargs):\n    return self._builder.make_policy(*args, **kwargs)",
  "def __init__(self, builder: builders.ActorLearnerBuilder[builders.Networks,\n                                                           builders.Policy,\n                                                           builders.Sample],\n               demonstrations_factory: Callable[[], Iterator[LfdStep]],\n               config: lfd_config.LfdConfig):\n    \"\"\"LfdBuilder constructor.\n\n    Args:\n      builder: The underlying builder implementing the off-policy algorithm.\n      demonstrations_factory: Factory returning an infinite stream (as an\n        iterator) of (action, next_timesteps). Episode boundaries in this stream\n        are given by timestep.first() and timestep.last(). Note that in the\n        distributed version of this algorithm, each actor is mixing the same\n        demonstrations with its online experience. This effectively results in\n        the demonstrations being replicated in the replay buffer as many times\n        as the number of actors being used.\n      config: LfD configuration.\n    \"\"\"\n    self._builder = builder\n    self._demonstrations_factory = demonstrations_factory\n    self._config = config",
  "def make_replay_tables(self, *args, **kwargs):\n    return self._builder.make_replay_tables(*args, **kwargs)",
  "def make_dataset_iterator(self, *args, **kwargs):\n    return self._builder.make_dataset_iterator(*args, **kwargs)",
  "def make_adder(self, *args, **kwargs):\n    demonstrations = self._demonstrations_factory()\n    return lfd_adder.LfdAdder(self._builder.make_adder(*args, **kwargs),\n                              demonstrations,\n                              self._config.initial_insert_count,\n                              self._config.demonstration_ratio)",
  "def make_actor(self, *args, **kwargs):\n    return self._builder.make_actor(*args, **kwargs)",
  "def make_learner(self, *args, **kwargs):\n    return self._builder.make_learner(*args, **kwargs)",
  "def make_policy(self, *args, **kwargs):\n    return self._builder.make_policy(*args, **kwargs)",
  "class PerturbationKey(NamedTuple):\n  training_iteration: int\n  perturbation_id: int\n  is_opposite: bool",
  "class EvaluationResult(NamedTuple):\n  total_reward: float\n  observation: networks_lib.Observation",
  "class EvaluationRequest(NamedTuple):\n  key: PerturbationKey\n  policy_params: networks_lib.Params\n  normalization_params: networks_lib.Params",
  "class TrainingState(NamedTuple):\n  \"\"\"Contains training state for the learner.\"\"\"\n  key: networks_lib.PRNGKey\n  normalizer_params: networks_lib.Params\n  policy_params: networks_lib.Params\n  training_iteration: int",
  "class EvaluationState(NamedTuple):\n  \"\"\"Contains training state for the learner.\"\"\"\n  key: networks_lib.PRNGKey\n  evaluation_queue: Deque[EvaluationRequest]\n  received_results: Dict[PerturbationKey, EvaluationResult]\n  noises: List[networks_lib.Params]",
  "class ARSLearner(acme.Learner):\n  \"\"\"ARS learner.\"\"\"\n\n  _state: TrainingState\n\n  def __init__(\n      self,\n      spec: specs.EnvironmentSpec,\n      networks: networks_lib.FeedForwardNetwork,\n      rng: networks_lib.PRNGKey,\n      config: ars_config.ARSConfig,\n      iterator: Iterator[reverb.ReplaySample],\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None):\n\n    self._config = config\n    self._lock = threading.Lock()\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger(\n        'learner',\n        asynchronous=True,\n        serialize_fn=utils.fetch_devicearray,\n        steps_key=self._counter.get_steps_key())\n\n    # Iterator on demonstration transitions.\n    self._iterator = iterator\n\n    if self._config.normalize_observations:\n      normalizer_params = running_statistics.init_state(spec.observations)\n      self._normalizer_update_fn = running_statistics.update\n    else:\n      normalizer_params = ()\n      self._normalizer_update_fn = lambda a, b: a\n\n    rng1, rng2, tmp = jax.random.split(rng, 3)\n    # Create initial state.\n    self._training_state = TrainingState(\n        key=rng1,\n        policy_params=networks.init(tmp),\n        normalizer_params=normalizer_params,\n        training_iteration=0)\n    self._evaluation_state = EvaluationState(\n        key=rng2,\n        evaluation_queue=collections.deque(),\n        received_results={},\n        noises=[])\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None\n\n  def _generate_perturbations(self):\n    with self._lock:\n      rng, noise_key = jax.random.split(self._evaluation_state.key)\n      self._evaluation_state = EvaluationState(\n          key=rng,\n          evaluation_queue=collections.deque(),\n          received_results={},\n          noises=[])\n\n      all_noise = jax.random.normal(\n          noise_key,\n          shape=(self._config.num_directions,) +\n          self._training_state.policy_params.shape,\n          dtype=self._training_state.policy_params.dtype)\n      for i in range(self._config.num_directions):\n        noise = all_noise[i]\n        self._evaluation_state.noises.append(noise)\n        for direction in (-1, 1):\n          self._evaluation_state.evaluation_queue.append(\n              EvaluationRequest(\n                  PerturbationKey(self._training_state.training_iteration, i,\n                                  direction == -1),\n                  self._training_state.policy_params +\n                  direction * noise * self._config.exploration_noise_std,\n                  self._training_state.normalizer_params))\n\n  def _read_results(self):\n    while len(self._evaluation_state.received_results\n             ) != self._config.num_directions * 2:\n      data = next(self._iterator).data\n      data = acme_reverb.Step(*data)\n\n      # validation\n      params_key = data.extras['params_key']\n      training_step, perturbation_id, is_opposite = params_key\n      # If the incoming data does not correspond to the current iteration,\n      # we simply ignore it.\n      if not np.all(\n          training_step[:-1] == self._training_state.training_iteration):\n        continue\n\n      # The whole episode should be run with the same policy, so let's check\n      # for that.\n      assert np.all(perturbation_id[:-1] == perturbation_id[0])\n      assert np.all(is_opposite[:-1] == is_opposite[0])\n\n      perturbation_id = perturbation_id[0].item()\n      is_opposite = is_opposite[0].item()\n\n      total_reward = np.sum(data.reward - self._config.reward_shift)\n      k = PerturbationKey(self._training_state.training_iteration,\n                          perturbation_id, is_opposite)\n      if k in self._evaluation_state.received_results:\n        continue\n      self._evaluation_state.received_results[k] = EvaluationResult(\n          total_reward, data.observation)\n\n  def _update_model(self) -> int:\n    # Update normalization params.\n    real_actor_steps = 0\n    normalizer_params = self._training_state.normalizer_params\n    for _, value in self._evaluation_state.received_results.items():\n      real_actor_steps += value.observation.shape[0] - 1\n      normalizer_params = self._normalizer_update_fn(normalizer_params,\n                                                     value.observation)\n\n    # Keep only top directions.\n    top_directions = []\n    for i in range(self._config.num_directions):\n      reward_forward = self._evaluation_state.received_results[PerturbationKey(\n          self._training_state.training_iteration, i, False)].total_reward\n      reward_reverse = self._evaluation_state.received_results[PerturbationKey(\n          self._training_state.training_iteration, i, True)].total_reward\n      top_directions.append((max(reward_forward, reward_reverse), i))\n    top_directions.sort()\n    top_directions = top_directions[-self._config.top_directions:]\n\n    # Compute reward_std.\n    reward = []\n    for _, i in top_directions:\n      reward.append(self._evaluation_state.received_results[PerturbationKey(\n          self._training_state.training_iteration, i, False)].total_reward)\n      reward.append(self._evaluation_state.received_results[PerturbationKey(\n          self._training_state.training_iteration, i, True)].total_reward)\n    reward_std = np.std(reward)\n\n    # Compute new policy params.\n    policy_params = self._training_state.policy_params\n    curr_sum = np.zeros_like(policy_params)\n    for _, i in top_directions:\n      reward_forward = self._evaluation_state.received_results[PerturbationKey(\n          self._training_state.training_iteration, i, False)].total_reward\n      reward_reverse = self._evaluation_state.received_results[PerturbationKey(\n          self._training_state.training_iteration, i, True)].total_reward\n      curr_sum += self._evaluation_state.noises[i] * (\n          reward_forward - reward_reverse)\n\n    policy_params = policy_params + self._config.step_size / (\n        self._config.top_directions * reward_std) * curr_sum\n\n    self._training_state = TrainingState(\n        key=self._training_state.key,\n        normalizer_params=normalizer_params,\n        policy_params=policy_params,\n        training_iteration=self._training_state.training_iteration)\n    return real_actor_steps\n\n  def step(self):\n    self._training_state = self._training_state._replace(\n        training_iteration=self._training_state.training_iteration + 1)\n    self._generate_perturbations()\n    self._read_results()\n    real_actor_steps = self._update_model()\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Increment counts and record the current time\n    counts = self._counter.increment(\n        steps=1,\n        real_actor_steps=real_actor_steps,\n        learner_episodes=2 * self._config.num_directions,\n        walltime=elapsed_time)\n\n    # Attempts to write the logs.\n    self._logger.write(counts)\n\n  def get_variables(self, names: List[str]) -> List[Any]:\n    assert (names == [ars_networks.BEHAVIOR_PARAMS_NAME] or\n            names == [ars_networks.EVAL_PARAMS_NAME])\n    if names == [ars_networks.EVAL_PARAMS_NAME]:\n      return [PerturbationKey(-1, -1, False),\n              self._training_state.policy_params,\n              self._training_state.normalizer_params]\n    should_sleep = False\n    while True:\n      if should_sleep:\n        time.sleep(0.1)\n      should_sleep = False\n      with self._lock:\n        if not self._evaluation_state.evaluation_queue:\n          should_sleep = True\n          continue\n        data = self._evaluation_state.evaluation_queue.pop()\n        # If this perturbation was already evaluated, we simply skip it.\n        if data.key in self._evaluation_state.received_results:\n          continue\n        # In case if an actor fails we still need to reevaluate the same\n        # perturbation, so we just add it to the end of the queue.\n        self._evaluation_state.evaluation_queue.append(data)\n        return [data]\n\n  def save(self) -> TrainingState:\n    return self._training_state\n\n  def restore(self, state: TrainingState):\n    self._training_state = state",
  "def __init__(\n      self,\n      spec: specs.EnvironmentSpec,\n      networks: networks_lib.FeedForwardNetwork,\n      rng: networks_lib.PRNGKey,\n      config: ars_config.ARSConfig,\n      iterator: Iterator[reverb.ReplaySample],\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None):\n\n    self._config = config\n    self._lock = threading.Lock()\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger(\n        'learner',\n        asynchronous=True,\n        serialize_fn=utils.fetch_devicearray,\n        steps_key=self._counter.get_steps_key())\n\n    # Iterator on demonstration transitions.\n    self._iterator = iterator\n\n    if self._config.normalize_observations:\n      normalizer_params = running_statistics.init_state(spec.observations)\n      self._normalizer_update_fn = running_statistics.update\n    else:\n      normalizer_params = ()\n      self._normalizer_update_fn = lambda a, b: a\n\n    rng1, rng2, tmp = jax.random.split(rng, 3)\n    # Create initial state.\n    self._training_state = TrainingState(\n        key=rng1,\n        policy_params=networks.init(tmp),\n        normalizer_params=normalizer_params,\n        training_iteration=0)\n    self._evaluation_state = EvaluationState(\n        key=rng2,\n        evaluation_queue=collections.deque(),\n        received_results={},\n        noises=[])\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None",
  "def _generate_perturbations(self):\n    with self._lock:\n      rng, noise_key = jax.random.split(self._evaluation_state.key)\n      self._evaluation_state = EvaluationState(\n          key=rng,\n          evaluation_queue=collections.deque(),\n          received_results={},\n          noises=[])\n\n      all_noise = jax.random.normal(\n          noise_key,\n          shape=(self._config.num_directions,) +\n          self._training_state.policy_params.shape,\n          dtype=self._training_state.policy_params.dtype)\n      for i in range(self._config.num_directions):\n        noise = all_noise[i]\n        self._evaluation_state.noises.append(noise)\n        for direction in (-1, 1):\n          self._evaluation_state.evaluation_queue.append(\n              EvaluationRequest(\n                  PerturbationKey(self._training_state.training_iteration, i,\n                                  direction == -1),\n                  self._training_state.policy_params +\n                  direction * noise * self._config.exploration_noise_std,\n                  self._training_state.normalizer_params))",
  "def _read_results(self):\n    while len(self._evaluation_state.received_results\n             ) != self._config.num_directions * 2:\n      data = next(self._iterator).data\n      data = acme_reverb.Step(*data)\n\n      # validation\n      params_key = data.extras['params_key']\n      training_step, perturbation_id, is_opposite = params_key\n      # If the incoming data does not correspond to the current iteration,\n      # we simply ignore it.\n      if not np.all(\n          training_step[:-1] == self._training_state.training_iteration):\n        continue\n\n      # The whole episode should be run with the same policy, so let's check\n      # for that.\n      assert np.all(perturbation_id[:-1] == perturbation_id[0])\n      assert np.all(is_opposite[:-1] == is_opposite[0])\n\n      perturbation_id = perturbation_id[0].item()\n      is_opposite = is_opposite[0].item()\n\n      total_reward = np.sum(data.reward - self._config.reward_shift)\n      k = PerturbationKey(self._training_state.training_iteration,\n                          perturbation_id, is_opposite)\n      if k in self._evaluation_state.received_results:\n        continue\n      self._evaluation_state.received_results[k] = EvaluationResult(\n          total_reward, data.observation)",
  "def _update_model(self) -> int:\n    # Update normalization params.\n    real_actor_steps = 0\n    normalizer_params = self._training_state.normalizer_params\n    for _, value in self._evaluation_state.received_results.items():\n      real_actor_steps += value.observation.shape[0] - 1\n      normalizer_params = self._normalizer_update_fn(normalizer_params,\n                                                     value.observation)\n\n    # Keep only top directions.\n    top_directions = []\n    for i in range(self._config.num_directions):\n      reward_forward = self._evaluation_state.received_results[PerturbationKey(\n          self._training_state.training_iteration, i, False)].total_reward\n      reward_reverse = self._evaluation_state.received_results[PerturbationKey(\n          self._training_state.training_iteration, i, True)].total_reward\n      top_directions.append((max(reward_forward, reward_reverse), i))\n    top_directions.sort()\n    top_directions = top_directions[-self._config.top_directions:]\n\n    # Compute reward_std.\n    reward = []\n    for _, i in top_directions:\n      reward.append(self._evaluation_state.received_results[PerturbationKey(\n          self._training_state.training_iteration, i, False)].total_reward)\n      reward.append(self._evaluation_state.received_results[PerturbationKey(\n          self._training_state.training_iteration, i, True)].total_reward)\n    reward_std = np.std(reward)\n\n    # Compute new policy params.\n    policy_params = self._training_state.policy_params\n    curr_sum = np.zeros_like(policy_params)\n    for _, i in top_directions:\n      reward_forward = self._evaluation_state.received_results[PerturbationKey(\n          self._training_state.training_iteration, i, False)].total_reward\n      reward_reverse = self._evaluation_state.received_results[PerturbationKey(\n          self._training_state.training_iteration, i, True)].total_reward\n      curr_sum += self._evaluation_state.noises[i] * (\n          reward_forward - reward_reverse)\n\n    policy_params = policy_params + self._config.step_size / (\n        self._config.top_directions * reward_std) * curr_sum\n\n    self._training_state = TrainingState(\n        key=self._training_state.key,\n        normalizer_params=normalizer_params,\n        policy_params=policy_params,\n        training_iteration=self._training_state.training_iteration)\n    return real_actor_steps",
  "def step(self):\n    self._training_state = self._training_state._replace(\n        training_iteration=self._training_state.training_iteration + 1)\n    self._generate_perturbations()\n    self._read_results()\n    real_actor_steps = self._update_model()\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Increment counts and record the current time\n    counts = self._counter.increment(\n        steps=1,\n        real_actor_steps=real_actor_steps,\n        learner_episodes=2 * self._config.num_directions,\n        walltime=elapsed_time)\n\n    # Attempts to write the logs.\n    self._logger.write(counts)",
  "def get_variables(self, names: List[str]) -> List[Any]:\n    assert (names == [ars_networks.BEHAVIOR_PARAMS_NAME] or\n            names == [ars_networks.EVAL_PARAMS_NAME])\n    if names == [ars_networks.EVAL_PARAMS_NAME]:\n      return [PerturbationKey(-1, -1, False),\n              self._training_state.policy_params,\n              self._training_state.normalizer_params]\n    should_sleep = False\n    while True:\n      if should_sleep:\n        time.sleep(0.1)\n      should_sleep = False\n      with self._lock:\n        if not self._evaluation_state.evaluation_queue:\n          should_sleep = True\n          continue\n        data = self._evaluation_state.evaluation_queue.pop()\n        # If this perturbation was already evaluated, we simply skip it.\n        if data.key in self._evaluation_state.received_results:\n          continue\n        # In case if an actor fails we still need to reevaluate the same\n        # perturbation, so we just add it to the end of the queue.\n        self._evaluation_state.evaluation_queue.append(data)\n        return [data]",
  "def save(self) -> TrainingState:\n    return self._training_state",
  "def restore(self, state: TrainingState):\n    self._training_state = state",
  "def make_networks(\n    spec: specs.EnvironmentSpec) -> networks_lib.FeedForwardNetwork:\n  \"\"\"Creates networks used by the agent.\n\n  The model used by the ARS paper is a simple clipped linear model.\n\n  Args:\n    spec: an environment spec\n\n  Returns:\n    A FeedForwardNetwork network.\n  \"\"\"\n\n  obs_size = spec.observations.shape[0]\n  act_size = spec.actions.shape[0]\n  return networks_lib.FeedForwardNetwork(\n      init=lambda _: jnp.zeros((obs_size, act_size)),\n      apply=lambda matrix, obs: jnp.clip(jnp.matmul(obs, matrix), -1, 1))",
  "def make_policy_network(\n    network: networks_lib.FeedForwardNetwork,\n    eval_mode: bool = True) -> Tuple[str, networks_lib.FeedForwardNetwork]:\n  params_name = EVAL_PARAMS_NAME if eval_mode else BEHAVIOR_PARAMS_NAME\n  return (params_name, network)",
  "class ARSConfig:\n  \"\"\"Configuration options for ARS.\"\"\"\n  num_steps: int = 1000000\n  normalize_observations: bool = True\n  step_size: float = 0.015\n  num_directions: int = 60\n  exploration_noise_std: float = 0.025\n  top_directions: int = 20\n  reward_shift: float = 1.0\n  replay_table_name: str = adders_reverb.DEFAULT_PRIORITY_TABLE",
  "def get_policy(policy_network: networks_lib.FeedForwardNetwork,\n               normalization_apply_fn) -> actor_core_lib.FeedForwardPolicy:\n  \"\"\"Returns a function that computes actions.\"\"\"\n\n  def apply(\n      params: networks_lib.Params, key: networks_lib.PRNGKey,\n      obs: networks_lib.Observation\n  ) -> Tuple[networks_lib.Action, Dict[str, jnp.ndarray]]:\n    del key\n    params_key, policy_params, normalization_params = params\n    normalized_obs = normalization_apply_fn(obs, normalization_params)\n    action = policy_network.apply(policy_params, normalized_obs)\n    return action, {\n        'params_key':\n            jax.tree_map(lambda x: jnp.expand_dims(x, axis=0), params_key)\n    }\n\n  return apply",
  "class ARSBuilder(\n    builders.ActorLearnerBuilder[networks_lib.FeedForwardNetwork,\n                                 Tuple[str, networks_lib.FeedForwardNetwork],\n                                 reverb.ReplaySample]):\n  \"\"\"ARS Builder.\"\"\"\n\n  def __init__(\n      self,\n      config: ars_config.ARSConfig,\n      spec: specs.EnvironmentSpec,\n  ):\n    self._config = config\n    self._spec = spec\n\n  def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: networks_lib.FeedForwardNetwork,\n      dataset: Iterator[reverb.ReplaySample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    del environment_spec, replay_client\n    return learning.ARSLearner(self._spec, networks, random_key, self._config,\n                               dataset, counter, logger_fn('learner'))\n\n  def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: Tuple[str, networks_lib.FeedForwardNetwork],\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> acme.Actor:\n    del environment_spec\n    assert variable_source is not None\n\n    kname, policy = policy\n\n    normalization_apply_fn = (\n        running_statistics.normalize if self._config.normalize_observations else\n        (lambda a, b: a))\n    policy_to_run = get_policy(policy, normalization_apply_fn)\n\n    actor_core = actor_core_lib.batched_feed_forward_with_extras_to_actor_core(\n        policy_to_run)\n    variable_client = variable_utils.VariableClient(variable_source, kname,\n                                                    device='cpu')\n    return actors.GenericActor(\n        actor_core,\n        random_key,\n        variable_client,\n        adder,\n        backend='cpu',\n        per_episode_update=True)\n\n  def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: Tuple[str, networks_lib.FeedForwardNetwork],\n  ) -> List[reverb.Table]:\n    \"\"\"Create tables to insert data into.\"\"\"\n    del policy\n    extra_spec = {\n        'params_key': (np.zeros(shape=(), dtype=np.int32),\n                       np.zeros(shape=(), dtype=np.int32),\n                       np.zeros(shape=(), dtype=np.bool_)),\n    }\n    signature = adders_reverb.EpisodeAdder.signature(\n        environment_spec, sequence_length=None, extras_spec=extra_spec)\n    return [\n        reverb.Table.queue(\n            name=self._config.replay_table_name,\n            max_size=10000,  # a big number\n            signature=signature)\n    ]\n\n  def make_dataset_iterator(\n      self, replay_client: reverb.Client) -> Iterator[reverb.ReplaySample]:\n    \"\"\"Create a dataset iterator to use for learning/updating the agent.\"\"\"\n    dataset = reverb.TrajectoryDataset.from_table_signature(\n        server_address=replay_client.server_address,\n        table=self._config.replay_table_name,\n        max_in_flight_samples_per_worker=1)\n    return utils.device_put(dataset.as_numpy_iterator(), jax.devices()[0])\n\n  def make_adder(\n      self, replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[Tuple[str, networks_lib.FeedForwardNetwork]]\n  ) -> Optional[adders.Adder]:\n    \"\"\"Create an adder which records data generated by the actor/environment.\"\"\"\n    del environment_spec, policy\n\n    return adders_reverb.EpisodeAdder(\n        priority_fns={self._config.replay_table_name: None},\n        client=replay_client,\n        max_sequence_length=2000,\n    )",
  "def apply(\n      params: networks_lib.Params, key: networks_lib.PRNGKey,\n      obs: networks_lib.Observation\n  ) -> Tuple[networks_lib.Action, Dict[str, jnp.ndarray]]:\n    del key\n    params_key, policy_params, normalization_params = params\n    normalized_obs = normalization_apply_fn(obs, normalization_params)\n    action = policy_network.apply(policy_params, normalized_obs)\n    return action, {\n        'params_key':\n            jax.tree_map(lambda x: jnp.expand_dims(x, axis=0), params_key)\n    }",
  "def __init__(\n      self,\n      config: ars_config.ARSConfig,\n      spec: specs.EnvironmentSpec,\n  ):\n    self._config = config\n    self._spec = spec",
  "def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: networks_lib.FeedForwardNetwork,\n      dataset: Iterator[reverb.ReplaySample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    del environment_spec, replay_client\n    return learning.ARSLearner(self._spec, networks, random_key, self._config,\n                               dataset, counter, logger_fn('learner'))",
  "def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: Tuple[str, networks_lib.FeedForwardNetwork],\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> acme.Actor:\n    del environment_spec\n    assert variable_source is not None\n\n    kname, policy = policy\n\n    normalization_apply_fn = (\n        running_statistics.normalize if self._config.normalize_observations else\n        (lambda a, b: a))\n    policy_to_run = get_policy(policy, normalization_apply_fn)\n\n    actor_core = actor_core_lib.batched_feed_forward_with_extras_to_actor_core(\n        policy_to_run)\n    variable_client = variable_utils.VariableClient(variable_source, kname,\n                                                    device='cpu')\n    return actors.GenericActor(\n        actor_core,\n        random_key,\n        variable_client,\n        adder,\n        backend='cpu',\n        per_episode_update=True)",
  "def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: Tuple[str, networks_lib.FeedForwardNetwork],\n  ) -> List[reverb.Table]:\n    \"\"\"Create tables to insert data into.\"\"\"\n    del policy\n    extra_spec = {\n        'params_key': (np.zeros(shape=(), dtype=np.int32),\n                       np.zeros(shape=(), dtype=np.int32),\n                       np.zeros(shape=(), dtype=np.bool_)),\n    }\n    signature = adders_reverb.EpisodeAdder.signature(\n        environment_spec, sequence_length=None, extras_spec=extra_spec)\n    return [\n        reverb.Table.queue(\n            name=self._config.replay_table_name,\n            max_size=10000,  # a big number\n            signature=signature)\n    ]",
  "def make_dataset_iterator(\n      self, replay_client: reverb.Client) -> Iterator[reverb.ReplaySample]:\n    \"\"\"Create a dataset iterator to use for learning/updating the agent.\"\"\"\n    dataset = reverb.TrajectoryDataset.from_table_signature(\n        server_address=replay_client.server_address,\n        table=self._config.replay_table_name,\n        max_in_flight_samples_per_worker=1)\n    return utils.device_put(dataset.as_numpy_iterator(), jax.devices()[0])",
  "def make_adder(\n      self, replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[Tuple[str, networks_lib.FeedForwardNetwork]]\n  ) -> Optional[adders.Adder]:\n    \"\"\"Create an adder which records data generated by the actor/environment.\"\"\"\n    del environment_spec, policy\n\n    return adders_reverb.EpisodeAdder(\n        priority_fns={self._config.replay_table_name: None},\n        client=replay_client,\n        max_sequence_length=2000,\n    )",
  "class TrainingState(NamedTuple):\n  \"\"\"Contains training state for the learner.\"\"\"\n  policy_optimizer_state: optax.OptState\n  policy_params: networks_lib.Params\n  nu_optimizer_state: optax.OptState\n  nu_params: networks_lib.Params\n  key: jnp.ndarray\n  steps: int",
  "def _orthogonal_regularization_loss(params: networks_lib.Params):\n  \"\"\"Orthogonal regularization.\n\n  See equation (3) in https://arxiv.org/abs/1809.11096.\n\n  Args:\n    params: Dictionary of parameters to apply regualization for.\n\n  Returns:\n    A regularization loss term.\n  \"\"\"\n  reg_loss = 0\n  for key in params:\n    if isinstance(params[key], Mapping):\n      reg_loss += _orthogonal_regularization_loss(params[key])\n      continue\n    variable = params[key]\n    assert len(variable.shape) in [1, 2, 4]\n    if len(variable.shape) == 1:\n      # This is a bias so do not apply regularization.\n      continue\n    if len(variable.shape) == 4:\n      # CNN\n      variable = jnp.reshape(variable, (-1, variable.shape[-1]))\n    prod = jnp.matmul(jnp.transpose(variable), variable)\n    reg_loss += jnp.sum(jnp.square(prod * (1 - jnp.eye(prod.shape[0]))))\n  return reg_loss",
  "class ValueDiceLearner(acme.Learner):\n  \"\"\"ValueDice learner.\"\"\"\n\n  _state: TrainingState\n\n  def __init__(self,\n               networks: value_dice_networks.ValueDiceNetworks,\n               policy_optimizer: optax.GradientTransformation,\n               nu_optimizer: optax.GradientTransformation,\n               discount: float,\n               rng: jnp.ndarray,\n               iterator_replay: Iterator[reverb.ReplaySample],\n               iterator_demonstrations: Iterator[types.Transition],\n               alpha: float = 0.05,\n               policy_reg_scale: float = 1e-4,\n               nu_reg_scale: float = 10.0,\n               num_sgd_steps_per_step: int = 1,\n               counter: Optional[counting.Counter] = None,\n               logger: Optional[loggers.Logger] = None):\n\n    rng, policy_key, nu_key = jax.random.split(rng, 3)\n    policy_init_params = networks.policy_network.init(policy_key)\n    policy_optimizer_state = policy_optimizer.init(policy_init_params)\n\n    nu_init_params = networks.nu_network.init(nu_key)\n    nu_optimizer_state = nu_optimizer.init(nu_init_params)\n\n    def compute_losses(\n        policy_params: networks_lib.Params,\n        nu_params: networks_lib.Params,\n        key: jnp.ndarray,\n        replay_o_tm1: types.NestedArray,\n        replay_a_tm1: types.NestedArray,\n        replay_o_t: types.NestedArray,\n        demo_o_tm1: types.NestedArray,\n        demo_a_tm1: types.NestedArray,\n        demo_o_t: types.NestedArray,\n    ) -> jnp.ndarray:\n      # TODO(damienv, hussenot): what to do with the discounts ?\n\n      def policy(obs, key):\n        dist_params = networks.policy_network.apply(policy_params, obs)\n        return networks.sample(dist_params, key)\n\n      key1, key2, key3, key4 = jax.random.split(key, 4)\n\n      # Predicted actions.\n      demo_o_t0 = demo_o_tm1\n      policy_demo_a_t0 = policy(demo_o_t0, key1)\n      policy_demo_a_t = policy(demo_o_t, key2)\n      policy_replay_a_t = policy(replay_o_t, key3)\n\n      replay_a_tm1 = networks.encode_action(replay_a_tm1)\n      demo_a_tm1 = networks.encode_action(demo_a_tm1)\n      policy_demo_a_t0 = networks.encode_action(policy_demo_a_t0)\n      policy_demo_a_t = networks.encode_action(policy_demo_a_t)\n      policy_replay_a_t = networks.encode_action(policy_replay_a_t)\n\n      # \"Value function\" nu over the expert states.\n      nu_demo_t0 = networks.nu_network.apply(nu_params, demo_o_t0,\n                                             policy_demo_a_t0)\n      nu_demo_tm1 = networks.nu_network.apply(nu_params, demo_o_tm1, demo_a_tm1)\n      nu_demo_t = networks.nu_network.apply(nu_params, demo_o_t,\n                                            policy_demo_a_t)\n      nu_demo_diff = nu_demo_tm1 - discount * nu_demo_t\n\n      # \"Value function\" nu over the replay buffer states.\n      nu_replay_tm1 = networks.nu_network.apply(nu_params, replay_o_tm1,\n                                                replay_a_tm1)\n      nu_replay_t = networks.nu_network.apply(nu_params, replay_o_t,\n                                              policy_replay_a_t)\n      nu_replay_diff = nu_replay_tm1 - discount * nu_replay_t\n\n      # Linear part of the loss.\n      linear_loss_demo = jnp.mean(nu_demo_t0 * (1.0 - discount))\n      linear_loss_rb = jnp.mean(nu_replay_diff)\n      linear_loss = (linear_loss_demo * (1 - alpha) + linear_loss_rb * alpha)\n\n      # Non linear part of the loss.\n      nu_replay_demo_diff = jnp.concatenate([nu_demo_diff, nu_replay_diff],\n                                            axis=0)\n      replay_demo_weights = jnp.concatenate([\n          jnp.ones_like(nu_demo_diff) * (1 - alpha),\n          jnp.ones_like(nu_replay_diff) * alpha\n      ],\n                                            axis=0)\n      replay_demo_weights /= jnp.mean(replay_demo_weights)\n      non_linear_loss = jnp.sum(\n          jax.lax.stop_gradient(\n              utils.weighted_softmax(nu_replay_demo_diff, replay_demo_weights,\n                                     axis=0)) *\n          nu_replay_demo_diff)\n\n      # Final loss.\n      loss = (non_linear_loss - linear_loss)\n\n      # Regularized policy loss.\n      if policy_reg_scale > 0.:\n        policy_reg = _orthogonal_regularization_loss(policy_params)\n      else:\n        policy_reg = 0.\n\n      # Gradient penality on nu\n      if nu_reg_scale > 0.0:\n        batch_size = demo_o_tm1.shape[0]\n        c = jax.random.uniform(key4, shape=(batch_size,))\n        shape_o = [\n            dim if i == 0 else 1 for i, dim in enumerate(replay_o_tm1.shape)\n        ]\n        shape_a = [\n            dim if i == 0 else 1 for i, dim in enumerate(replay_a_tm1.shape)\n        ]\n        c_o = jnp.reshape(c, shape_o)\n        c_a = jnp.reshape(c, shape_a)\n        mixed_o_tm1 = c_o * demo_o_tm1 + (1 - c_o) * replay_o_tm1\n        mixed_a_tm1 = c_a * demo_a_tm1 + (1 - c_a) * replay_a_tm1\n        mixed_o_t = c_o * demo_o_t + (1 - c_o) * replay_o_t\n        mixed_policy_a_t = c_a * policy_demo_a_t + (1 - c_a) * policy_replay_a_t\n        mixed_o = jnp.concatenate([mixed_o_tm1, mixed_o_t], axis=0)\n        mixed_a = jnp.concatenate([mixed_a_tm1, mixed_policy_a_t], axis=0)\n\n        def sum_nu(o, a):\n          return jnp.sum(networks.nu_network.apply(nu_params, o, a))\n\n        nu_grad_o_fn = jax.grad(sum_nu, argnums=0)\n        nu_grad_a_fn = jax.grad(sum_nu, argnums=1)\n        nu_grad_o = nu_grad_o_fn(mixed_o, mixed_a)\n        nu_grad_a = nu_grad_a_fn(mixed_o, mixed_a)\n        nu_grad = jnp.concatenate([\n            jnp.reshape(nu_grad_o, [batch_size, -1]),\n            jnp.reshape(nu_grad_a, [batch_size, -1])], axis=-1)\n        # TODO(damienv, hussenot): check for the need of eps\n        # (like in the original value dice code).\n        nu_grad_penalty = jnp.mean(\n            jnp.square(\n                jnp.linalg.norm(nu_grad + 1e-8, axis=-1, keepdims=True) - 1))\n      else:\n        nu_grad_penalty = 0.0\n\n      policy_loss = -loss + policy_reg_scale * policy_reg\n      nu_loss = loss + nu_reg_scale * nu_grad_penalty\n\n      return policy_loss, nu_loss  # pytype: disable=bad-return-type  # jax-ndarray\n\n    def sgd_step(\n        state: TrainingState,\n        data: Tuple[types.Transition, types.Transition]\n    ) -> Tuple[TrainingState, Dict[str, jnp.ndarray]]:\n      replay_transitions, demo_transitions = data\n      key, key_loss = jax.random.split(state.key)\n      compute_losses_with_input = functools.partial(\n          compute_losses,\n          replay_o_tm1=replay_transitions.observation,\n          replay_a_tm1=replay_transitions.action,\n          replay_o_t=replay_transitions.next_observation,\n          demo_o_tm1=demo_transitions.observation,\n          demo_a_tm1=demo_transitions.action,\n          demo_o_t=demo_transitions.next_observation,\n          key=key_loss)\n      (policy_loss_value, nu_loss_value), vjpfun = jax.vjp(\n          compute_losses_with_input,\n          state.policy_params, state.nu_params)\n      policy_gradients, _ = vjpfun((1.0, 0.0))\n      _, nu_gradients = vjpfun((0.0, 1.0))\n\n      # Update optimizers.\n      policy_update, policy_optimizer_state = policy_optimizer.update(\n          policy_gradients, state.policy_optimizer_state)\n      policy_params = optax.apply_updates(state.policy_params, policy_update)\n\n      nu_update, nu_optimizer_state = nu_optimizer.update(\n          nu_gradients, state.nu_optimizer_state)\n      nu_params = optax.apply_updates(state.nu_params, nu_update)\n\n      new_state = TrainingState(\n          policy_optimizer_state=policy_optimizer_state,\n          policy_params=policy_params,\n          nu_optimizer_state=nu_optimizer_state,\n          nu_params=nu_params,\n          key=key,\n          steps=state.steps + 1,\n      )\n\n      metrics = {\n          'policy_loss': policy_loss_value,\n          'nu_loss': nu_loss_value,\n      }\n\n      return new_state, metrics\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger(\n        'learner',\n        asynchronous=True,\n        serialize_fn=utils.fetch_devicearray,\n        steps_key=self._counter.get_steps_key())\n\n    # Iterator on demonstration transitions.\n    self._iterator_demonstrations = iterator_demonstrations\n    self._iterator_replay = iterator_replay\n\n    self._sgd_step = jax.jit(utils.process_multiple_batches(\n        sgd_step, num_sgd_steps_per_step))\n\n    # Create initial state.\n    self._state = TrainingState(\n        policy_optimizer_state=policy_optimizer_state,\n        policy_params=policy_init_params,\n        nu_optimizer_state=nu_optimizer_state,\n        nu_params=nu_init_params,\n        key=rng,\n        steps=0,\n    )\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None\n\n  def step(self):\n    # Get data from replay (dropping extras if any). Note there is no\n    # extra data here because we do not insert any into Reverb.\n    # TODO(raveman): Add a support for offline training, where we do not consume\n    # data from the replay buffer.\n    sample = next(self._iterator_replay)\n    replay_transitions = types.Transition(*sample.data)\n\n    # Get a batch of Transitions from the demonstration.\n    demonstration_transitions = next(self._iterator_demonstrations)\n\n    self._state, metrics = self._sgd_step(\n        self._state, (replay_transitions, demonstration_transitions))\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Increment counts and record the current time\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n\n    # Attempts to write the logs.\n    self._logger.write({**metrics, **counts})\n\n  def get_variables(self, names: List[str]) -> List[Any]:\n    variables = {\n        'policy': self._state.policy_params,\n        'nu': self._state.nu_params,\n    }\n    return [variables[name] for name in names]\n\n  def save(self) -> TrainingState:\n    return self._state\n\n  def restore(self, state: TrainingState):\n    self._state = state",
  "def __init__(self,\n               networks: value_dice_networks.ValueDiceNetworks,\n               policy_optimizer: optax.GradientTransformation,\n               nu_optimizer: optax.GradientTransformation,\n               discount: float,\n               rng: jnp.ndarray,\n               iterator_replay: Iterator[reverb.ReplaySample],\n               iterator_demonstrations: Iterator[types.Transition],\n               alpha: float = 0.05,\n               policy_reg_scale: float = 1e-4,\n               nu_reg_scale: float = 10.0,\n               num_sgd_steps_per_step: int = 1,\n               counter: Optional[counting.Counter] = None,\n               logger: Optional[loggers.Logger] = None):\n\n    rng, policy_key, nu_key = jax.random.split(rng, 3)\n    policy_init_params = networks.policy_network.init(policy_key)\n    policy_optimizer_state = policy_optimizer.init(policy_init_params)\n\n    nu_init_params = networks.nu_network.init(nu_key)\n    nu_optimizer_state = nu_optimizer.init(nu_init_params)\n\n    def compute_losses(\n        policy_params: networks_lib.Params,\n        nu_params: networks_lib.Params,\n        key: jnp.ndarray,\n        replay_o_tm1: types.NestedArray,\n        replay_a_tm1: types.NestedArray,\n        replay_o_t: types.NestedArray,\n        demo_o_tm1: types.NestedArray,\n        demo_a_tm1: types.NestedArray,\n        demo_o_t: types.NestedArray,\n    ) -> jnp.ndarray:\n      # TODO(damienv, hussenot): what to do with the discounts ?\n\n      def policy(obs, key):\n        dist_params = networks.policy_network.apply(policy_params, obs)\n        return networks.sample(dist_params, key)\n\n      key1, key2, key3, key4 = jax.random.split(key, 4)\n\n      # Predicted actions.\n      demo_o_t0 = demo_o_tm1\n      policy_demo_a_t0 = policy(demo_o_t0, key1)\n      policy_demo_a_t = policy(demo_o_t, key2)\n      policy_replay_a_t = policy(replay_o_t, key3)\n\n      replay_a_tm1 = networks.encode_action(replay_a_tm1)\n      demo_a_tm1 = networks.encode_action(demo_a_tm1)\n      policy_demo_a_t0 = networks.encode_action(policy_demo_a_t0)\n      policy_demo_a_t = networks.encode_action(policy_demo_a_t)\n      policy_replay_a_t = networks.encode_action(policy_replay_a_t)\n\n      # \"Value function\" nu over the expert states.\n      nu_demo_t0 = networks.nu_network.apply(nu_params, demo_o_t0,\n                                             policy_demo_a_t0)\n      nu_demo_tm1 = networks.nu_network.apply(nu_params, demo_o_tm1, demo_a_tm1)\n      nu_demo_t = networks.nu_network.apply(nu_params, demo_o_t,\n                                            policy_demo_a_t)\n      nu_demo_diff = nu_demo_tm1 - discount * nu_demo_t\n\n      # \"Value function\" nu over the replay buffer states.\n      nu_replay_tm1 = networks.nu_network.apply(nu_params, replay_o_tm1,\n                                                replay_a_tm1)\n      nu_replay_t = networks.nu_network.apply(nu_params, replay_o_t,\n                                              policy_replay_a_t)\n      nu_replay_diff = nu_replay_tm1 - discount * nu_replay_t\n\n      # Linear part of the loss.\n      linear_loss_demo = jnp.mean(nu_demo_t0 * (1.0 - discount))\n      linear_loss_rb = jnp.mean(nu_replay_diff)\n      linear_loss = (linear_loss_demo * (1 - alpha) + linear_loss_rb * alpha)\n\n      # Non linear part of the loss.\n      nu_replay_demo_diff = jnp.concatenate([nu_demo_diff, nu_replay_diff],\n                                            axis=0)\n      replay_demo_weights = jnp.concatenate([\n          jnp.ones_like(nu_demo_diff) * (1 - alpha),\n          jnp.ones_like(nu_replay_diff) * alpha\n      ],\n                                            axis=0)\n      replay_demo_weights /= jnp.mean(replay_demo_weights)\n      non_linear_loss = jnp.sum(\n          jax.lax.stop_gradient(\n              utils.weighted_softmax(nu_replay_demo_diff, replay_demo_weights,\n                                     axis=0)) *\n          nu_replay_demo_diff)\n\n      # Final loss.\n      loss = (non_linear_loss - linear_loss)\n\n      # Regularized policy loss.\n      if policy_reg_scale > 0.:\n        policy_reg = _orthogonal_regularization_loss(policy_params)\n      else:\n        policy_reg = 0.\n\n      # Gradient penality on nu\n      if nu_reg_scale > 0.0:\n        batch_size = demo_o_tm1.shape[0]\n        c = jax.random.uniform(key4, shape=(batch_size,))\n        shape_o = [\n            dim if i == 0 else 1 for i, dim in enumerate(replay_o_tm1.shape)\n        ]\n        shape_a = [\n            dim if i == 0 else 1 for i, dim in enumerate(replay_a_tm1.shape)\n        ]\n        c_o = jnp.reshape(c, shape_o)\n        c_a = jnp.reshape(c, shape_a)\n        mixed_o_tm1 = c_o * demo_o_tm1 + (1 - c_o) * replay_o_tm1\n        mixed_a_tm1 = c_a * demo_a_tm1 + (1 - c_a) * replay_a_tm1\n        mixed_o_t = c_o * demo_o_t + (1 - c_o) * replay_o_t\n        mixed_policy_a_t = c_a * policy_demo_a_t + (1 - c_a) * policy_replay_a_t\n        mixed_o = jnp.concatenate([mixed_o_tm1, mixed_o_t], axis=0)\n        mixed_a = jnp.concatenate([mixed_a_tm1, mixed_policy_a_t], axis=0)\n\n        def sum_nu(o, a):\n          return jnp.sum(networks.nu_network.apply(nu_params, o, a))\n\n        nu_grad_o_fn = jax.grad(sum_nu, argnums=0)\n        nu_grad_a_fn = jax.grad(sum_nu, argnums=1)\n        nu_grad_o = nu_grad_o_fn(mixed_o, mixed_a)\n        nu_grad_a = nu_grad_a_fn(mixed_o, mixed_a)\n        nu_grad = jnp.concatenate([\n            jnp.reshape(nu_grad_o, [batch_size, -1]),\n            jnp.reshape(nu_grad_a, [batch_size, -1])], axis=-1)\n        # TODO(damienv, hussenot): check for the need of eps\n        # (like in the original value dice code).\n        nu_grad_penalty = jnp.mean(\n            jnp.square(\n                jnp.linalg.norm(nu_grad + 1e-8, axis=-1, keepdims=True) - 1))\n      else:\n        nu_grad_penalty = 0.0\n\n      policy_loss = -loss + policy_reg_scale * policy_reg\n      nu_loss = loss + nu_reg_scale * nu_grad_penalty\n\n      return policy_loss, nu_loss  # pytype: disable=bad-return-type  # jax-ndarray\n\n    def sgd_step(\n        state: TrainingState,\n        data: Tuple[types.Transition, types.Transition]\n    ) -> Tuple[TrainingState, Dict[str, jnp.ndarray]]:\n      replay_transitions, demo_transitions = data\n      key, key_loss = jax.random.split(state.key)\n      compute_losses_with_input = functools.partial(\n          compute_losses,\n          replay_o_tm1=replay_transitions.observation,\n          replay_a_tm1=replay_transitions.action,\n          replay_o_t=replay_transitions.next_observation,\n          demo_o_tm1=demo_transitions.observation,\n          demo_a_tm1=demo_transitions.action,\n          demo_o_t=demo_transitions.next_observation,\n          key=key_loss)\n      (policy_loss_value, nu_loss_value), vjpfun = jax.vjp(\n          compute_losses_with_input,\n          state.policy_params, state.nu_params)\n      policy_gradients, _ = vjpfun((1.0, 0.0))\n      _, nu_gradients = vjpfun((0.0, 1.0))\n\n      # Update optimizers.\n      policy_update, policy_optimizer_state = policy_optimizer.update(\n          policy_gradients, state.policy_optimizer_state)\n      policy_params = optax.apply_updates(state.policy_params, policy_update)\n\n      nu_update, nu_optimizer_state = nu_optimizer.update(\n          nu_gradients, state.nu_optimizer_state)\n      nu_params = optax.apply_updates(state.nu_params, nu_update)\n\n      new_state = TrainingState(\n          policy_optimizer_state=policy_optimizer_state,\n          policy_params=policy_params,\n          nu_optimizer_state=nu_optimizer_state,\n          nu_params=nu_params,\n          key=key,\n          steps=state.steps + 1,\n      )\n\n      metrics = {\n          'policy_loss': policy_loss_value,\n          'nu_loss': nu_loss_value,\n      }\n\n      return new_state, metrics\n\n    # General learner book-keeping and loggers.\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger(\n        'learner',\n        asynchronous=True,\n        serialize_fn=utils.fetch_devicearray,\n        steps_key=self._counter.get_steps_key())\n\n    # Iterator on demonstration transitions.\n    self._iterator_demonstrations = iterator_demonstrations\n    self._iterator_replay = iterator_replay\n\n    self._sgd_step = jax.jit(utils.process_multiple_batches(\n        sgd_step, num_sgd_steps_per_step))\n\n    # Create initial state.\n    self._state = TrainingState(\n        policy_optimizer_state=policy_optimizer_state,\n        policy_params=policy_init_params,\n        nu_optimizer_state=nu_optimizer_state,\n        nu_params=nu_init_params,\n        key=rng,\n        steps=0,\n    )\n\n    # Do not record timestamps until after the first learning step is done.\n    # This is to avoid including the time it takes for actors to come online and\n    # fill the replay buffer.\n    self._timestamp = None",
  "def step(self):\n    # Get data from replay (dropping extras if any). Note there is no\n    # extra data here because we do not insert any into Reverb.\n    # TODO(raveman): Add a support for offline training, where we do not consume\n    # data from the replay buffer.\n    sample = next(self._iterator_replay)\n    replay_transitions = types.Transition(*sample.data)\n\n    # Get a batch of Transitions from the demonstration.\n    demonstration_transitions = next(self._iterator_demonstrations)\n\n    self._state, metrics = self._sgd_step(\n        self._state, (replay_transitions, demonstration_transitions))\n\n    # Compute elapsed time.\n    timestamp = time.time()\n    elapsed_time = timestamp - self._timestamp if self._timestamp else 0\n    self._timestamp = timestamp\n\n    # Increment counts and record the current time\n    counts = self._counter.increment(steps=1, walltime=elapsed_time)\n\n    # Attempts to write the logs.\n    self._logger.write({**metrics, **counts})",
  "def get_variables(self, names: List[str]) -> List[Any]:\n    variables = {\n        'policy': self._state.policy_params,\n        'nu': self._state.nu_params,\n    }\n    return [variables[name] for name in names]",
  "def save(self) -> TrainingState:\n    return self._state",
  "def restore(self, state: TrainingState):\n    self._state = state",
  "def compute_losses(\n        policy_params: networks_lib.Params,\n        nu_params: networks_lib.Params,\n        key: jnp.ndarray,\n        replay_o_tm1: types.NestedArray,\n        replay_a_tm1: types.NestedArray,\n        replay_o_t: types.NestedArray,\n        demo_o_tm1: types.NestedArray,\n        demo_a_tm1: types.NestedArray,\n        demo_o_t: types.NestedArray,\n    ) -> jnp.ndarray:\n      # TODO(damienv, hussenot): what to do with the discounts ?\n\n      def policy(obs, key):\n        dist_params = networks.policy_network.apply(policy_params, obs)\n        return networks.sample(dist_params, key)\n\n      key1, key2, key3, key4 = jax.random.split(key, 4)\n\n      # Predicted actions.\n      demo_o_t0 = demo_o_tm1\n      policy_demo_a_t0 = policy(demo_o_t0, key1)\n      policy_demo_a_t = policy(demo_o_t, key2)\n      policy_replay_a_t = policy(replay_o_t, key3)\n\n      replay_a_tm1 = networks.encode_action(replay_a_tm1)\n      demo_a_tm1 = networks.encode_action(demo_a_tm1)\n      policy_demo_a_t0 = networks.encode_action(policy_demo_a_t0)\n      policy_demo_a_t = networks.encode_action(policy_demo_a_t)\n      policy_replay_a_t = networks.encode_action(policy_replay_a_t)\n\n      # \"Value function\" nu over the expert states.\n      nu_demo_t0 = networks.nu_network.apply(nu_params, demo_o_t0,\n                                             policy_demo_a_t0)\n      nu_demo_tm1 = networks.nu_network.apply(nu_params, demo_o_tm1, demo_a_tm1)\n      nu_demo_t = networks.nu_network.apply(nu_params, demo_o_t,\n                                            policy_demo_a_t)\n      nu_demo_diff = nu_demo_tm1 - discount * nu_demo_t\n\n      # \"Value function\" nu over the replay buffer states.\n      nu_replay_tm1 = networks.nu_network.apply(nu_params, replay_o_tm1,\n                                                replay_a_tm1)\n      nu_replay_t = networks.nu_network.apply(nu_params, replay_o_t,\n                                              policy_replay_a_t)\n      nu_replay_diff = nu_replay_tm1 - discount * nu_replay_t\n\n      # Linear part of the loss.\n      linear_loss_demo = jnp.mean(nu_demo_t0 * (1.0 - discount))\n      linear_loss_rb = jnp.mean(nu_replay_diff)\n      linear_loss = (linear_loss_demo * (1 - alpha) + linear_loss_rb * alpha)\n\n      # Non linear part of the loss.\n      nu_replay_demo_diff = jnp.concatenate([nu_demo_diff, nu_replay_diff],\n                                            axis=0)\n      replay_demo_weights = jnp.concatenate([\n          jnp.ones_like(nu_demo_diff) * (1 - alpha),\n          jnp.ones_like(nu_replay_diff) * alpha\n      ],\n                                            axis=0)\n      replay_demo_weights /= jnp.mean(replay_demo_weights)\n      non_linear_loss = jnp.sum(\n          jax.lax.stop_gradient(\n              utils.weighted_softmax(nu_replay_demo_diff, replay_demo_weights,\n                                     axis=0)) *\n          nu_replay_demo_diff)\n\n      # Final loss.\n      loss = (non_linear_loss - linear_loss)\n\n      # Regularized policy loss.\n      if policy_reg_scale > 0.:\n        policy_reg = _orthogonal_regularization_loss(policy_params)\n      else:\n        policy_reg = 0.\n\n      # Gradient penality on nu\n      if nu_reg_scale > 0.0:\n        batch_size = demo_o_tm1.shape[0]\n        c = jax.random.uniform(key4, shape=(batch_size,))\n        shape_o = [\n            dim if i == 0 else 1 for i, dim in enumerate(replay_o_tm1.shape)\n        ]\n        shape_a = [\n            dim if i == 0 else 1 for i, dim in enumerate(replay_a_tm1.shape)\n        ]\n        c_o = jnp.reshape(c, shape_o)\n        c_a = jnp.reshape(c, shape_a)\n        mixed_o_tm1 = c_o * demo_o_tm1 + (1 - c_o) * replay_o_tm1\n        mixed_a_tm1 = c_a * demo_a_tm1 + (1 - c_a) * replay_a_tm1\n        mixed_o_t = c_o * demo_o_t + (1 - c_o) * replay_o_t\n        mixed_policy_a_t = c_a * policy_demo_a_t + (1 - c_a) * policy_replay_a_t\n        mixed_o = jnp.concatenate([mixed_o_tm1, mixed_o_t], axis=0)\n        mixed_a = jnp.concatenate([mixed_a_tm1, mixed_policy_a_t], axis=0)\n\n        def sum_nu(o, a):\n          return jnp.sum(networks.nu_network.apply(nu_params, o, a))\n\n        nu_grad_o_fn = jax.grad(sum_nu, argnums=0)\n        nu_grad_a_fn = jax.grad(sum_nu, argnums=1)\n        nu_grad_o = nu_grad_o_fn(mixed_o, mixed_a)\n        nu_grad_a = nu_grad_a_fn(mixed_o, mixed_a)\n        nu_grad = jnp.concatenate([\n            jnp.reshape(nu_grad_o, [batch_size, -1]),\n            jnp.reshape(nu_grad_a, [batch_size, -1])], axis=-1)\n        # TODO(damienv, hussenot): check for the need of eps\n        # (like in the original value dice code).\n        nu_grad_penalty = jnp.mean(\n            jnp.square(\n                jnp.linalg.norm(nu_grad + 1e-8, axis=-1, keepdims=True) - 1))\n      else:\n        nu_grad_penalty = 0.0\n\n      policy_loss = -loss + policy_reg_scale * policy_reg\n      nu_loss = loss + nu_reg_scale * nu_grad_penalty\n\n      return policy_loss, nu_loss",
  "def sgd_step(\n        state: TrainingState,\n        data: Tuple[types.Transition, types.Transition]\n    ) -> Tuple[TrainingState, Dict[str, jnp.ndarray]]:\n      replay_transitions, demo_transitions = data\n      key, key_loss = jax.random.split(state.key)\n      compute_losses_with_input = functools.partial(\n          compute_losses,\n          replay_o_tm1=replay_transitions.observation,\n          replay_a_tm1=replay_transitions.action,\n          replay_o_t=replay_transitions.next_observation,\n          demo_o_tm1=demo_transitions.observation,\n          demo_a_tm1=demo_transitions.action,\n          demo_o_t=demo_transitions.next_observation,\n          key=key_loss)\n      (policy_loss_value, nu_loss_value), vjpfun = jax.vjp(\n          compute_losses_with_input,\n          state.policy_params, state.nu_params)\n      policy_gradients, _ = vjpfun((1.0, 0.0))\n      _, nu_gradients = vjpfun((0.0, 1.0))\n\n      # Update optimizers.\n      policy_update, policy_optimizer_state = policy_optimizer.update(\n          policy_gradients, state.policy_optimizer_state)\n      policy_params = optax.apply_updates(state.policy_params, policy_update)\n\n      nu_update, nu_optimizer_state = nu_optimizer.update(\n          nu_gradients, state.nu_optimizer_state)\n      nu_params = optax.apply_updates(state.nu_params, nu_update)\n\n      new_state = TrainingState(\n          policy_optimizer_state=policy_optimizer_state,\n          policy_params=policy_params,\n          nu_optimizer_state=nu_optimizer_state,\n          nu_params=nu_params,\n          key=key,\n          steps=state.steps + 1,\n      )\n\n      metrics = {\n          'policy_loss': policy_loss_value,\n          'nu_loss': nu_loss_value,\n      }\n\n      return new_state, metrics",
  "def policy(obs, key):\n        dist_params = networks.policy_network.apply(policy_params, obs)\n        return networks.sample(dist_params, key)",
  "def sum_nu(o, a):\n          return jnp.sum(networks.nu_network.apply(nu_params, o, a))",
  "class ValueDiceNetworks:\n  \"\"\"ValueDice networks.\"\"\"\n  policy_network: networks_lib.FeedForwardNetwork\n  nu_network: networks_lib.FeedForwardNetwork\n  # Functions for actors and evaluators, resp., to sample actions.\n  sample: networks_lib.SampleFn\n  sample_eval: Optional[networks_lib.SampleFn] = None\n  # Function that transforms an action before a mixture is applied, typically\n  # the identity for continuous actions and one-hot encoding for discrete\n  # actions.\n  encode_action: Callable[[networks_lib.Action], jnp.ndarray] = lambda x: x",
  "def apply_policy_and_sample(\n    networks: ValueDiceNetworks,\n    eval_mode: bool = False) -> actor_core_lib.FeedForwardPolicy:\n  \"\"\"Returns a function that computes actions.\"\"\"\n  sample_fn = networks.sample if not eval_mode else networks.sample_eval\n  if not sample_fn:\n    raise ValueError('sample function is not provided')\n\n  def apply_and_sample(params, key, obs):\n    return sample_fn(networks.policy_network.apply(params, obs), key)\n  return apply_and_sample",
  "def make_networks(\n    spec: specs.EnvironmentSpec,\n    hidden_layer_sizes: Tuple[int, ...] = (256, 256)) -> ValueDiceNetworks:\n  \"\"\"Creates networks used by the agent.\"\"\"\n\n  num_dimensions = np.prod(spec.actions.shape, dtype=int)\n\n  def _actor_fn(obs):\n    network = hk.Sequential([\n        hk.nets.MLP(\n            list(hidden_layer_sizes),\n            w_init=hk.initializers.VarianceScaling(1.0, 'fan_in', 'uniform'),\n            activation=jax.nn.relu,\n            activate_final=True),\n        networks_lib.NormalTanhDistribution(num_dimensions),\n    ])\n    return network(obs)\n\n  def _nu_fn(obs, action):\n    network = hk.Sequential([\n        hk.nets.MLP(\n            list(hidden_layer_sizes) + [1],\n            w_init=hk.initializers.VarianceScaling(1.0, 'fan_in', 'uniform'),\n            activation=jax.nn.relu),\n    ])\n    return network(jnp.concatenate([obs, action], axis=-1))\n\n  policy = hk.without_apply_rng(hk.transform(_actor_fn))\n  nu = hk.without_apply_rng(hk.transform(_nu_fn))\n\n  # Create dummy observations and actions to create network parameters.\n  dummy_action = utils.zeros_like(spec.actions)\n  dummy_obs = utils.zeros_like(spec.observations)\n  dummy_action = utils.add_batch_dim(dummy_action)\n  dummy_obs = utils.add_batch_dim(dummy_obs)\n\n  return ValueDiceNetworks(\n      policy_network=networks_lib.FeedForwardNetwork(\n          lambda key: policy.init(key, dummy_obs), policy.apply),\n      nu_network=networks_lib.FeedForwardNetwork(\n          lambda key: nu.init(key, dummy_obs, dummy_action), nu.apply),\n      sample=lambda params, key: params.sample(seed=key),\n      sample_eval=lambda params, key: params.mode())",
  "def apply_and_sample(params, key, obs):\n    return sample_fn(networks.policy_network.apply(params, obs), key)",
  "def _actor_fn(obs):\n    network = hk.Sequential([\n        hk.nets.MLP(\n            list(hidden_layer_sizes),\n            w_init=hk.initializers.VarianceScaling(1.0, 'fan_in', 'uniform'),\n            activation=jax.nn.relu,\n            activate_final=True),\n        networks_lib.NormalTanhDistribution(num_dimensions),\n    ])\n    return network(obs)",
  "def _nu_fn(obs, action):\n    network = hk.Sequential([\n        hk.nets.MLP(\n            list(hidden_layer_sizes) + [1],\n            w_init=hk.initializers.VarianceScaling(1.0, 'fan_in', 'uniform'),\n            activation=jax.nn.relu),\n    ])\n    return network(jnp.concatenate([obs, action], axis=-1))",
  "class ValueDiceConfig:\n  \"\"\"Configuration options for ValueDice.\"\"\"\n\n  policy_learning_rate: float = 1e-5\n  nu_learning_rate: float = 1e-3\n  discount: float = .99\n  batch_size: int = 256\n  alpha: float = 0.05\n  policy_reg_scale: float = 1e-4\n  nu_reg_scale: float = 10.0\n\n  # Replay options\n  replay_table_name: str = adders_reverb.DEFAULT_PRIORITY_TABLE\n  samples_per_insert: float = 256 * 4\n  # Rate to be used for the SampleToInsertRatio rate limitter tolerance.\n  # See a formula in make_replay_tables for more details.\n  samples_per_insert_tolerance_rate: float = 0.1\n  min_replay_size: int = 1000\n  max_replay_size: int = 1000000\n  prefetch_size: int = 4\n\n  # How many gradient updates to perform per step.\n  num_sgd_steps_per_step: int = 1",
  "class ValueDiceBuilder(\n    builders.ActorLearnerBuilder[value_dice_networks.ValueDiceNetworks,\n                                 actor_core_lib.FeedForwardPolicy,\n                                 reverb.ReplaySample]):\n  \"\"\"ValueDice Builder.\n\n  This builder is an entry point for online version of ValueDice.\n  For offline please use the ValueDiceLearner directly.\n  \"\"\"\n\n  def __init__(self, config: value_dice_config.ValueDiceConfig,\n               make_demonstrations: Callable[[int],\n                                             Iterator[types.Transition]]):\n    self._make_demonstrations = make_demonstrations\n    self._config = config\n\n  def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: value_dice_networks.ValueDiceNetworks,\n      dataset: Iterator[reverb.ReplaySample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    del environment_spec, replay_client\n    iterator_demonstration = self._make_demonstrations(\n        self._config.batch_size * self._config.num_sgd_steps_per_step)\n    policy_optimizer = optax.adam(\n        learning_rate=self._config.policy_learning_rate)\n    nu_optimizer = optax.adam(learning_rate=self._config.nu_learning_rate)\n    return learning.ValueDiceLearner(\n        networks=networks,\n        policy_optimizer=policy_optimizer,\n        nu_optimizer=nu_optimizer,\n        discount=self._config.discount,\n        rng=random_key,\n        alpha=self._config.alpha,\n        policy_reg_scale=self._config.policy_reg_scale,\n        nu_reg_scale=self._config.nu_reg_scale,\n        num_sgd_steps_per_step=self._config.num_sgd_steps_per_step,\n        iterator_replay=dataset,\n        iterator_demonstrations=iterator_demonstration,\n        logger=logger_fn('learner'),\n        counter=counter,\n    )\n\n  def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: actor_core_lib.FeedForwardPolicy,\n  ) -> List[reverb.Table]:\n    del policy\n    samples_per_insert_tolerance = (\n        self._config.samples_per_insert_tolerance_rate *\n        self._config.samples_per_insert)\n    error_buffer = self._config.min_replay_size * samples_per_insert_tolerance\n    limiter = rate_limiters.SampleToInsertRatio(\n        min_size_to_sample=self._config.min_replay_size,\n        samples_per_insert=self._config.samples_per_insert,\n        error_buffer=error_buffer)\n    return [reverb.Table(\n        name=self._config.replay_table_name,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=self._config.max_replay_size,\n        rate_limiter=limiter,\n        signature=adders_reverb.NStepTransitionAdder.signature(\n            environment_spec))]\n\n  def make_dataset_iterator(\n      self, replay_client: reverb.Client) -> Iterator[reverb.ReplaySample]:\n    \"\"\"Creates a dataset iterator to use for learning.\"\"\"\n    dataset = datasets.make_reverb_dataset(\n        table=self._config.replay_table_name,\n        server_address=replay_client.server_address,\n        batch_size=(\n            self._config.batch_size * self._config.num_sgd_steps_per_step),\n        prefetch_size=self._config.prefetch_size)\n    return utils.device_put(dataset.as_numpy_iterator(), jax.devices()[0])\n\n  def make_adder(\n      self, replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[actor_core_lib.FeedForwardPolicy]\n  ) -> Optional[adders.Adder]:\n    del environment_spec, policy\n    return adders_reverb.NStepTransitionAdder(\n        priority_fns={self._config.replay_table_name: None},\n        client=replay_client,\n        n_step=1,\n        discount=self._config.discount)\n\n  def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: actor_core_lib.FeedForwardPolicy,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> core.Actor:\n    del environment_spec\n    assert variable_source is not None\n    actor_core = actor_core_lib.batched_feed_forward_to_actor_core(policy)\n    # Inference happens on CPU, so it's better to move variables there too.\n    variable_client = variable_utils.VariableClient(variable_source, 'policy',\n                                                    device='cpu')\n    return actors.GenericActor(\n        actor_core, random_key, variable_client, adder, backend='cpu')\n\n  def make_policy(self,\n                  networks: value_dice_networks.ValueDiceNetworks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> actor_core_lib.FeedForwardPolicy:\n    del environment_spec\n    return value_dice_networks.apply_policy_and_sample(\n        networks, eval_mode=evaluation)",
  "def __init__(self, config: value_dice_config.ValueDiceConfig,\n               make_demonstrations: Callable[[int],\n                                             Iterator[types.Transition]]):\n    self._make_demonstrations = make_demonstrations\n    self._config = config",
  "def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: value_dice_networks.ValueDiceNetworks,\n      dataset: Iterator[reverb.ReplaySample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    del environment_spec, replay_client\n    iterator_demonstration = self._make_demonstrations(\n        self._config.batch_size * self._config.num_sgd_steps_per_step)\n    policy_optimizer = optax.adam(\n        learning_rate=self._config.policy_learning_rate)\n    nu_optimizer = optax.adam(learning_rate=self._config.nu_learning_rate)\n    return learning.ValueDiceLearner(\n        networks=networks,\n        policy_optimizer=policy_optimizer,\n        nu_optimizer=nu_optimizer,\n        discount=self._config.discount,\n        rng=random_key,\n        alpha=self._config.alpha,\n        policy_reg_scale=self._config.policy_reg_scale,\n        nu_reg_scale=self._config.nu_reg_scale,\n        num_sgd_steps_per_step=self._config.num_sgd_steps_per_step,\n        iterator_replay=dataset,\n        iterator_demonstrations=iterator_demonstration,\n        logger=logger_fn('learner'),\n        counter=counter,\n    )",
  "def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: actor_core_lib.FeedForwardPolicy,\n  ) -> List[reverb.Table]:\n    del policy\n    samples_per_insert_tolerance = (\n        self._config.samples_per_insert_tolerance_rate *\n        self._config.samples_per_insert)\n    error_buffer = self._config.min_replay_size * samples_per_insert_tolerance\n    limiter = rate_limiters.SampleToInsertRatio(\n        min_size_to_sample=self._config.min_replay_size,\n        samples_per_insert=self._config.samples_per_insert,\n        error_buffer=error_buffer)\n    return [reverb.Table(\n        name=self._config.replay_table_name,\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=self._config.max_replay_size,\n        rate_limiter=limiter,\n        signature=adders_reverb.NStepTransitionAdder.signature(\n            environment_spec))]",
  "def make_dataset_iterator(\n      self, replay_client: reverb.Client) -> Iterator[reverb.ReplaySample]:\n    \"\"\"Creates a dataset iterator to use for learning.\"\"\"\n    dataset = datasets.make_reverb_dataset(\n        table=self._config.replay_table_name,\n        server_address=replay_client.server_address,\n        batch_size=(\n            self._config.batch_size * self._config.num_sgd_steps_per_step),\n        prefetch_size=self._config.prefetch_size)\n    return utils.device_put(dataset.as_numpy_iterator(), jax.devices()[0])",
  "def make_adder(\n      self, replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[actor_core_lib.FeedForwardPolicy]\n  ) -> Optional[adders.Adder]:\n    del environment_spec, policy\n    return adders_reverb.NStepTransitionAdder(\n        priority_fns={self._config.replay_table_name: None},\n        client=replay_client,\n        n_step=1,\n        discount=self._config.discount)",
  "def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: actor_core_lib.FeedForwardPolicy,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> core.Actor:\n    del environment_spec\n    assert variable_source is not None\n    actor_core = actor_core_lib.batched_feed_forward_to_actor_core(policy)\n    # Inference happens on CPU, so it's better to move variables there too.\n    variable_client = variable_utils.VariableClient(variable_source, 'policy',\n                                                    device='cpu')\n    return actors.GenericActor(\n        actor_core, random_key, variable_client, adder, backend='cpu')",
  "def make_policy(self,\n                  networks: value_dice_networks.ValueDiceNetworks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> actor_core_lib.FeedForwardPolicy:\n    del environment_spec\n    return value_dice_networks.apply_policy_and_sample(\n        networks, eval_mode=evaluation)",
  "def _validate_spec(spec: types.NestedSpec, value: types.NestedArray):\n  \"\"\"Validate a value from a potentially nested spec.\"\"\"\n  tree.assert_same_structure(value, spec)\n  tree.map_structure(lambda s, v: s.validate(v), spec, value)",
  "class OpenSpielEnvironmentLoopTest(parameterized.TestCase):\n\n  def test_loop_run(self):\n    raw_env = rl_environment.Environment('tic_tac_toe')\n    env = open_spiel_wrapper.OpenSpielWrapper(raw_env)\n    env = wrappers.SinglePrecisionWrapper(env)\n    environment_spec = acme.make_environment_spec(env)\n\n    actors = []\n    for _ in range(env.num_players):\n      actors.append(RandomActor(environment_spec))\n\n    loop = open_spiel_environment_loop.OpenSpielEnvironmentLoop(env, actors)\n    result = loop.run_episode()\n    self.assertIn('episode_length', result)\n    self.assertIn('episode_return', result)\n    self.assertIn('steps_per_second', result)\n\n    loop.run(num_episodes=10)\n    loop.run(num_steps=100)",
  "class RandomActor(core.Actor):\n    \"\"\"Fake actor which generates random actions and validates specs.\"\"\"\n\n    def __init__(self, spec: specs.EnvironmentSpec):\n      self._spec = spec\n      self.num_updates = 0\n\n    def select_action(self, observation: open_spiel_wrapper.OLT) -> int:\n      _validate_spec(self._spec.observations, observation)\n      legals = np.array(np.nonzero(observation.legal_actions), dtype=np.int32)\n      return np.random.choice(legals[0])\n\n    def observe_first(self, timestep: dm_env.TimeStep):\n      _validate_spec(self._spec.observations, timestep.observation)\n\n    def observe(self, action: types.NestedArray,\n                next_timestep: dm_env.TimeStep):\n      _validate_spec(self._spec.actions, action)\n      _validate_spec(self._spec.rewards, next_timestep.reward)\n      _validate_spec(self._spec.discounts, next_timestep.discount)\n      _validate_spec(self._spec.observations, next_timestep.observation)\n\n    def update(self, wait: bool = False):\n      self.num_updates += 1",
  "def test_loop_run(self):\n    raw_env = rl_environment.Environment('tic_tac_toe')\n    env = open_spiel_wrapper.OpenSpielWrapper(raw_env)\n    env = wrappers.SinglePrecisionWrapper(env)\n    environment_spec = acme.make_environment_spec(env)\n\n    actors = []\n    for _ in range(env.num_players):\n      actors.append(RandomActor(environment_spec))\n\n    loop = open_spiel_environment_loop.OpenSpielEnvironmentLoop(env, actors)\n    result = loop.run_episode()\n    self.assertIn('episode_length', result)\n    self.assertIn('episode_return', result)\n    self.assertIn('steps_per_second', result)\n\n    loop.run(num_episodes=10)\n    loop.run(num_steps=100)",
  "def __init__(self, spec: specs.EnvironmentSpec):\n      self._spec = spec\n      self.num_updates = 0",
  "def select_action(self, observation: open_spiel_wrapper.OLT) -> int:\n      _validate_spec(self._spec.observations, observation)\n      legals = np.array(np.nonzero(observation.legal_actions), dtype=np.int32)\n      return np.random.choice(legals[0])",
  "def observe_first(self, timestep: dm_env.TimeStep):\n      _validate_spec(self._spec.observations, timestep.observation)",
  "def observe(self, action: types.NestedArray,\n                next_timestep: dm_env.TimeStep):\n      _validate_spec(self._spec.actions, action)\n      _validate_spec(self._spec.rewards, next_timestep.reward)\n      _validate_spec(self._spec.discounts, next_timestep.discount)\n      _validate_spec(self._spec.observations, next_timestep.observation)",
  "def update(self, wait: bool = False):\n      self.num_updates += 1",
  "class OpenSpielEnvironmentLoop(core.Worker):\n  \"\"\"An OpenSpiel RL environment loop.\n\n  This takes `Environment` and list of `Actor` instances and coordinates their\n  interaction. Agents are updated if `should_update=True`. This can be used as:\n\n    loop = EnvironmentLoop(environment, actors)\n    loop.run(num_episodes)\n\n  A `Counter` instance can optionally be given in order to maintain counts\n  between different Acme components. If not given a local Counter will be\n  created to maintain counts between calls to the `run` method.\n\n  A `Logger` instance can also be passed in order to control the output of the\n  loop. If not given a platform-specific default logger will be used as defined\n  by utils.loggers.make_default_logger. A string `label` can be passed to easily\n  change the label associated with the default logger; this is ignored if a\n  `Logger` instance is given.\n  \"\"\"\n\n  def __init__(\n      self,\n      environment: open_spiel_wrapper.OpenSpielWrapper,\n      actors: Sequence[core.Actor],\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      should_update: bool = True,\n      label: str = 'open_spiel_environment_loop',\n  ):\n    # Internalize agent and environment.\n    self._environment = environment\n    self._actors = actors\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger(label)\n    self._should_update = should_update\n\n    # Track information necessary to coordinate updates among multiple actors.\n    self._observed_first = [False] * len(self._actors)\n    self._prev_actions = [pyspiel.INVALID_ACTION] * len(self._actors)\n\n  def _send_observation(self, timestep: dm_env.TimeStep, player: int):\n    # If terminal all actors must update\n    if player == pyspiel.PlayerId.TERMINAL:\n      for player_id in range(len(self._actors)):\n        # Note: we must account for situations where the first observation\n        # is a terminal state, e.g. if an opponent folds in poker before we get\n        # to act.\n        if self._observed_first[player_id]:\n          player_timestep = self._get_player_timestep(timestep, player_id)\n          self._actors[player_id].observe(self._prev_actions[player_id],\n                                          player_timestep)\n          if self._should_update:\n            self._actors[player_id].update()\n      self._observed_first = [False] * len(self._actors)\n      self._prev_actions = [pyspiel.INVALID_ACTION] * len(self._actors)\n    else:\n      if not self._observed_first[player]:\n        player_timestep = dm_env.TimeStep(\n            observation=timestep.observation[player],\n            reward=None,\n            discount=None,\n            step_type=dm_env.StepType.FIRST)\n        self._actors[player].observe_first(player_timestep)\n        self._observed_first[player] = True\n      else:\n        player_timestep = self._get_player_timestep(timestep, player)\n        self._actors[player].observe(self._prev_actions[player],\n                                     player_timestep)\n        if self._should_update:\n          self._actors[player].update()\n\n  def _get_action(self, timestep: dm_env.TimeStep, player: int) -> int:\n    self._prev_actions[player] = self._actors[player].select_action(\n        timestep.observation[player])\n    return self._prev_actions[player]\n\n  def _get_player_timestep(self, timestep: dm_env.TimeStep,\n                           player: int) -> dm_env.TimeStep:\n    return dm_env.TimeStep(observation=timestep.observation[player],\n                           reward=timestep.reward[player],\n                           discount=timestep.discount[player],\n                           step_type=timestep.step_type)\n\n  def run_episode(self) -> loggers.LoggingData:\n    \"\"\"Run one episode.\n\n    Each episode is a loop which interacts first with the environment to get an\n    observation and then give that observation to the agent in order to retrieve\n    an action.\n\n    Returns:\n      An instance of `loggers.LoggingData`.\n    \"\"\"\n    # Reset any counts and start the environment.\n    start_time = time.time()\n    episode_steps = 0\n\n    # For evaluation, this keeps track of the total undiscounted reward\n    # for each player accumulated during the episode.\n    multiplayer_reward_spec = specs.BoundedArray(\n        (self._environment.game.num_players(),),\n        np.float32,\n        minimum=self._environment.game.min_utility(),\n        maximum=self._environment.game.max_utility())\n    episode_return = tree.map_structure(_generate_zeros_from_spec,\n                                        multiplayer_reward_spec)\n\n    timestep = self._environment.reset()\n\n    # Make the first observation.\n    self._send_observation(timestep, self._environment.current_player)\n\n    # Run an episode.\n    while not timestep.last():\n      # Generate an action from the agent's policy and step the environment.\n      if self._environment.is_turn_based:\n        action_list = [\n            self._get_action(timestep, self._environment.current_player)\n        ]\n      else:\n        # FIXME: Support simultaneous move games.\n        raise ValueError('Currently only supports sequential games.')\n\n      timestep = self._environment.step(action_list)\n\n      # Have the agent observe the timestep and let the actor update itself.\n      self._send_observation(timestep, self._environment.current_player)\n\n      # Book-keeping.\n      episode_steps += 1\n\n      # Equivalent to: episode_return += timestep.reward\n      # We capture the return value because if timestep.reward is a JAX\n      # DeviceArray, episode_return will not be mutated in-place. (In all other\n      # cases, the returned episode_return will be the same object as the\n      # argument episode_return.)\n      episode_return = tree.map_structure(operator.iadd,\n                                          episode_return,\n                                          timestep.reward)\n\n    # Record counts.\n    counts = self._counter.increment(episodes=1, steps=episode_steps)\n\n    # Collect the results and combine with counts.\n    steps_per_second = episode_steps / (time.time() - start_time)\n    result = {\n        'episode_length': episode_steps,\n        'episode_return': episode_return,\n        'steps_per_second': steps_per_second,\n    }\n    result.update(counts)\n    return result\n\n  def run(self,\n          num_episodes: Optional[int] = None,\n          num_steps: Optional[int] = None):\n    \"\"\"Perform the run loop.\n\n    Run the environment loop either for `num_episodes` episodes or for at\n    least `num_steps` steps (the last episode is always run until completion,\n    so the total number of steps may be slightly more than `num_steps`).\n    At least one of these two arguments has to be None.\n\n    Upon termination of an episode a new episode will be started. If the number\n    of episodes and the number of steps are not given then this will interact\n    with the environment infinitely.\n\n    Args:\n      num_episodes: number of episodes to run the loop for.\n      num_steps: minimal number of steps to run the loop for.\n\n    Raises:\n      ValueError: If both 'num_episodes' and 'num_steps' are not None.\n    \"\"\"\n\n    if not (num_episodes is None or num_steps is None):\n      raise ValueError('Either \"num_episodes\" or \"num_steps\" should be None.')\n\n    def should_terminate(episode_count: int, step_count: int) -> bool:\n      return ((num_episodes is not None and episode_count >= num_episodes) or\n              (num_steps is not None and step_count >= num_steps))\n\n    episode_count, step_count = 0, 0\n    while not should_terminate(episode_count, step_count):\n      result = self.run_episode()\n      episode_count += 1\n      step_count += result['episode_length']\n      # Log the given results.\n      self._logger.write(result)",
  "def _generate_zeros_from_spec(spec: specs.Array) -> np.ndarray:\n  return np.zeros(spec.shape, spec.dtype)",
  "def __init__(\n      self,\n      environment: open_spiel_wrapper.OpenSpielWrapper,\n      actors: Sequence[core.Actor],\n      counter: Optional[counting.Counter] = None,\n      logger: Optional[loggers.Logger] = None,\n      should_update: bool = True,\n      label: str = 'open_spiel_environment_loop',\n  ):\n    # Internalize agent and environment.\n    self._environment = environment\n    self._actors = actors\n    self._counter = counter or counting.Counter()\n    self._logger = logger or loggers.make_default_logger(label)\n    self._should_update = should_update\n\n    # Track information necessary to coordinate updates among multiple actors.\n    self._observed_first = [False] * len(self._actors)\n    self._prev_actions = [pyspiel.INVALID_ACTION] * len(self._actors)",
  "def _send_observation(self, timestep: dm_env.TimeStep, player: int):\n    # If terminal all actors must update\n    if player == pyspiel.PlayerId.TERMINAL:\n      for player_id in range(len(self._actors)):\n        # Note: we must account for situations where the first observation\n        # is a terminal state, e.g. if an opponent folds in poker before we get\n        # to act.\n        if self._observed_first[player_id]:\n          player_timestep = self._get_player_timestep(timestep, player_id)\n          self._actors[player_id].observe(self._prev_actions[player_id],\n                                          player_timestep)\n          if self._should_update:\n            self._actors[player_id].update()\n      self._observed_first = [False] * len(self._actors)\n      self._prev_actions = [pyspiel.INVALID_ACTION] * len(self._actors)\n    else:\n      if not self._observed_first[player]:\n        player_timestep = dm_env.TimeStep(\n            observation=timestep.observation[player],\n            reward=None,\n            discount=None,\n            step_type=dm_env.StepType.FIRST)\n        self._actors[player].observe_first(player_timestep)\n        self._observed_first[player] = True\n      else:\n        player_timestep = self._get_player_timestep(timestep, player)\n        self._actors[player].observe(self._prev_actions[player],\n                                     player_timestep)\n        if self._should_update:\n          self._actors[player].update()",
  "def _get_action(self, timestep: dm_env.TimeStep, player: int) -> int:\n    self._prev_actions[player] = self._actors[player].select_action(\n        timestep.observation[player])\n    return self._prev_actions[player]",
  "def _get_player_timestep(self, timestep: dm_env.TimeStep,\n                           player: int) -> dm_env.TimeStep:\n    return dm_env.TimeStep(observation=timestep.observation[player],\n                           reward=timestep.reward[player],\n                           discount=timestep.discount[player],\n                           step_type=timestep.step_type)",
  "def run_episode(self) -> loggers.LoggingData:\n    \"\"\"Run one episode.\n\n    Each episode is a loop which interacts first with the environment to get an\n    observation and then give that observation to the agent in order to retrieve\n    an action.\n\n    Returns:\n      An instance of `loggers.LoggingData`.\n    \"\"\"\n    # Reset any counts and start the environment.\n    start_time = time.time()\n    episode_steps = 0\n\n    # For evaluation, this keeps track of the total undiscounted reward\n    # for each player accumulated during the episode.\n    multiplayer_reward_spec = specs.BoundedArray(\n        (self._environment.game.num_players(),),\n        np.float32,\n        minimum=self._environment.game.min_utility(),\n        maximum=self._environment.game.max_utility())\n    episode_return = tree.map_structure(_generate_zeros_from_spec,\n                                        multiplayer_reward_spec)\n\n    timestep = self._environment.reset()\n\n    # Make the first observation.\n    self._send_observation(timestep, self._environment.current_player)\n\n    # Run an episode.\n    while not timestep.last():\n      # Generate an action from the agent's policy and step the environment.\n      if self._environment.is_turn_based:\n        action_list = [\n            self._get_action(timestep, self._environment.current_player)\n        ]\n      else:\n        # FIXME: Support simultaneous move games.\n        raise ValueError('Currently only supports sequential games.')\n\n      timestep = self._environment.step(action_list)\n\n      # Have the agent observe the timestep and let the actor update itself.\n      self._send_observation(timestep, self._environment.current_player)\n\n      # Book-keeping.\n      episode_steps += 1\n\n      # Equivalent to: episode_return += timestep.reward\n      # We capture the return value because if timestep.reward is a JAX\n      # DeviceArray, episode_return will not be mutated in-place. (In all other\n      # cases, the returned episode_return will be the same object as the\n      # argument episode_return.)\n      episode_return = tree.map_structure(operator.iadd,\n                                          episode_return,\n                                          timestep.reward)\n\n    # Record counts.\n    counts = self._counter.increment(episodes=1, steps=episode_steps)\n\n    # Collect the results and combine with counts.\n    steps_per_second = episode_steps / (time.time() - start_time)\n    result = {\n        'episode_length': episode_steps,\n        'episode_return': episode_return,\n        'steps_per_second': steps_per_second,\n    }\n    result.update(counts)\n    return result",
  "def run(self,\n          num_episodes: Optional[int] = None,\n          num_steps: Optional[int] = None):\n    \"\"\"Perform the run loop.\n\n    Run the environment loop either for `num_episodes` episodes or for at\n    least `num_steps` steps (the last episode is always run until completion,\n    so the total number of steps may be slightly more than `num_steps`).\n    At least one of these two arguments has to be None.\n\n    Upon termination of an episode a new episode will be started. If the number\n    of episodes and the number of steps are not given then this will interact\n    with the environment infinitely.\n\n    Args:\n      num_episodes: number of episodes to run the loop for.\n      num_steps: minimal number of steps to run the loop for.\n\n    Raises:\n      ValueError: If both 'num_episodes' and 'num_steps' are not None.\n    \"\"\"\n\n    if not (num_episodes is None or num_steps is None):\n      raise ValueError('Either \"num_episodes\" or \"num_steps\" should be None.')\n\n    def should_terminate(episode_count: int, step_count: int) -> bool:\n      return ((num_episodes is not None and episode_count >= num_episodes) or\n              (num_steps is not None and step_count >= num_steps))\n\n    episode_count, step_count = 0, 0\n    while not should_terminate(episode_count, step_count):\n      result = self.run_episode()\n      episode_count += 1\n      step_count += result['episode_length']\n      # Log the given results.\n      self._logger.write(result)",
  "def should_terminate(episode_count: int, step_count: int) -> bool:\n      return ((num_episodes is not None and episode_count >= num_episodes) or\n              (num_steps is not None and step_count >= num_steps))",
  "def _is_prefix(a: Path, b: Path) -> bool:\n  \"\"\"Returns whether `a` is a prefix of `b`.\"\"\"\n  return b[:len(a)] == a",
  "def _zeros_like(nest: types.Nest, dtype=None) -> types.NestedArray:\n  return jax.tree_map(lambda x: jnp.zeros(x.shape, dtype or x.dtype), nest)",
  "def _ones_like(nest: types.Nest, dtype=None) -> types.NestedArray:\n  return jax.tree_map(lambda x: jnp.ones(x.shape, dtype or x.dtype), nest)",
  "class NestedMeanStd:\n  \"\"\"A container for running statistics (mean, std) of possibly nested data.\"\"\"\n  mean: types.NestedArray\n  std: types.NestedArray",
  "class RunningStatisticsState(NestedMeanStd):\n  \"\"\"Full state of running statistics computation.\"\"\"\n  count: Union[int, jnp.ndarray]\n  summed_variance: types.NestedArray",
  "class NestStatisticsConfig:\n  \"\"\"Specifies how to compute statistics for Nests with the same structure.\n\n  Attributes:\n    paths: A sequence of Nest paths to compute statistics for. If there is a\n      collision between paths (one is a prefix of the other), the shorter path\n      takes precedence.\n  \"\"\"\n  paths: Tuple[Path, ...] = ((),)",
  "def _is_path_included(config: NestStatisticsConfig, path: Path) -> bool:\n  \"\"\"Returns whether the path is included in the config.\"\"\"\n  # A path is included in the config if it corresponds to a tree node that\n  # belongs to a subtree rooted at the node corresponding to some path in\n  # the config.\n  return any(_is_prefix(config_path, path) for config_path in config.paths)",
  "def init_state(nest: types.Nest) -> RunningStatisticsState:\n  \"\"\"Initializes the running statistics for the given nested structure.\"\"\"\n  dtype = jnp.float64 if jax.config.jax_enable_x64 else jnp.float32\n\n  return RunningStatisticsState(  # pytype: disable=wrong-arg-types  # jax-ndarray\n      count=0.,\n      mean=_zeros_like(nest, dtype=dtype),\n      summed_variance=_zeros_like(nest, dtype=dtype),\n      # Initialize with ones to make sure normalization works correctly\n      # in the initial state.\n      std=_ones_like(nest, dtype=dtype))",
  "def _validate_batch_shapes(batch: types.NestedArray,\n                           reference_sample: types.NestedArray,\n                           batch_dims: Tuple[int, ...]) -> None:\n  \"\"\"Verifies shapes of the batch leaves against the reference sample.\n\n  Checks that batch dimensions are the same in all leaves in the batch.\n  Checks that non-batch dimensions for all leaves in the batch are the same\n  as in the reference sample.\n\n  Arguments:\n    batch: the nested batch of data to be verified.\n    reference_sample: the nested array to check non-batch dimensions.\n    batch_dims: a Tuple of indices of batch dimensions in the batch shape.\n\n  Returns:\n    None.\n  \"\"\"\n  def validate_node_shape(reference_sample: jnp.ndarray,\n                          batch: jnp.ndarray) -> None:\n    expected_shape = batch_dims + reference_sample.shape\n    assert batch.shape == expected_shape, f'{batch.shape} != {expected_shape}'\n\n  tree_utils.fast_map_structure(validate_node_shape, reference_sample, batch)",
  "def update(state: RunningStatisticsState,\n           batch: types.NestedArray,\n           *,\n           config: NestStatisticsConfig = NestStatisticsConfig(),\n           weights: Optional[jnp.ndarray] = None,\n           std_min_value: float = 1e-6,\n           std_max_value: float = 1e6,\n           pmap_axis_name: Optional[str] = None,\n           validate_shapes: bool = True) -> RunningStatisticsState:\n  \"\"\"Updates the running statistics with the given batch of data.\n\n  Note: data batch and state elements (mean, etc.) must have the same structure.\n\n  Note: by default will use int32 for counts and float32 for accumulated\n  variance. This results in an integer overflow after 2^31 data points and\n  degrading precision after 2^24 batch updates or even earlier if variance\n  updates have large dynamic range.\n  To improve precision, consider setting jax_enable_x64 to True, see\n  https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#double-64bit-precision\n\n  Arguments:\n    state: The running statistics before the update.\n    batch: The data to be used to update the running statistics.\n    config: The config that specifies which leaves of the nested structure\n      should the running statistics be computed for.\n    weights: Weights of the batch data. Should match the batch dimensions.\n      Passing a weight of 2. should be equivalent to updating on the\n      corresponding data point twice.\n    std_min_value: Minimum value for the standard deviation.\n    std_max_value: Maximum value for the standard deviation.\n    pmap_axis_name: Name of the pmapped axis, if any.\n    validate_shapes: If true, the shapes of all leaves of the batch will be\n      validated. Enabled by default. Doesn't impact performance when jitted.\n\n  Returns:\n    Updated running statistics.\n  \"\"\"\n  # We require exactly the same structure to avoid issues when flattened\n  # batch and state have different order of elements.\n  tree.assert_same_structure(batch, state.mean)\n  batch_shape = tree.flatten(batch)[0].shape\n  # We assume the batch dimensions always go first.\n  batch_dims = batch_shape[:len(batch_shape) - tree.flatten(state.mean)[0].ndim]\n  batch_axis = range(len(batch_dims))\n  if weights is None:\n    step_increment = np.prod(batch_dims)\n  else:\n    step_increment = jnp.sum(weights)\n  if pmap_axis_name is not None:\n    step_increment = jax.lax.psum(step_increment, axis_name=pmap_axis_name)\n  count = state.count + step_increment\n\n  # Validation is important. If the shapes don't match exactly, but are\n  # compatible, arrays will be silently broadcasted resulting in incorrect\n  # statistics.\n  if validate_shapes:\n    if weights is not None:\n      if weights.shape != batch_dims:\n        raise ValueError(f'{weights.shape} != {batch_dims}')\n    _validate_batch_shapes(batch, state.mean, batch_dims)\n\n  def _compute_node_statistics(\n      path: Path, mean: jnp.ndarray, summed_variance: jnp.ndarray,\n      batch: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:\n    assert isinstance(mean, jnp.ndarray), type(mean)\n    assert isinstance(summed_variance, jnp.ndarray), type(summed_variance)\n    if not _is_path_included(config, path):\n      # Return unchanged.\n      return mean, summed_variance\n    # The mean and the sum of past variances are updated with Welford's\n    # algorithm using batches (see https://stackoverflow.com/q/56402955).\n    diff_to_old_mean = batch - mean\n    if weights is not None:\n      expanded_weights = jnp.reshape(\n          weights,\n          list(weights.shape) + [1] * (batch.ndim - weights.ndim))\n      diff_to_old_mean = diff_to_old_mean * expanded_weights\n    mean_update = jnp.sum(diff_to_old_mean, axis=batch_axis) / count\n    if pmap_axis_name is not None:\n      mean_update = jax.lax.psum(\n          mean_update, axis_name=pmap_axis_name)\n    mean = mean + mean_update\n\n    diff_to_new_mean = batch - mean\n    variance_update = diff_to_old_mean * diff_to_new_mean\n    variance_update = jnp.sum(variance_update, axis=batch_axis)\n    if pmap_axis_name is not None:\n      variance_update = jax.lax.psum(variance_update, axis_name=pmap_axis_name)\n    summed_variance = summed_variance + variance_update\n    return mean, summed_variance\n\n  updated_stats = tree_utils.fast_map_structure_with_path(\n      _compute_node_statistics, state.mean, state.summed_variance, batch)\n  # map_structure_up_to is slow, so shortcut if we know the input is not\n  # structured.\n  if isinstance(state.mean, jnp.ndarray):\n    mean, summed_variance = updated_stats\n  else:\n    # Reshape the updated stats from `nest(mean, summed_variance)` to\n    # `nest(mean), nest(summed_variance)`.\n    mean, summed_variance = [\n        tree.map_structure_up_to(\n            state.mean, lambda s, i=idx: s[i], updated_stats)\n        for idx in range(2)\n    ]\n\n  def compute_std(path: Path, summed_variance: jnp.ndarray,\n                  std: jnp.ndarray) -> jnp.ndarray:\n    assert isinstance(summed_variance, jnp.ndarray)\n    if not _is_path_included(config, path):\n      return std\n    # Summed variance can get negative due to rounding errors.\n    summed_variance = jnp.maximum(summed_variance, 0)\n    std = jnp.sqrt(summed_variance / count)\n    std = jnp.clip(std, std_min_value, std_max_value)\n    return std\n\n  std = tree_utils.fast_map_structure_with_path(compute_std, summed_variance,\n                                                state.std)\n\n  return RunningStatisticsState(\n      count=count, mean=mean, summed_variance=summed_variance, std=std)",
  "def normalize(batch: types.NestedArray,\n              mean_std: NestedMeanStd,\n              max_abs_value: Optional[float] = None) -> types.NestedArray:\n  \"\"\"Normalizes data using running statistics.\"\"\"\n\n  def normalize_leaf(data: jnp.ndarray, mean: jnp.ndarray,\n                     std: jnp.ndarray) -> jnp.ndarray:\n    # Only normalize inexact types.\n    if not jnp.issubdtype(data.dtype, jnp.inexact):\n      return data\n    data = (data - mean) / std\n    if max_abs_value is not None:\n      # TODO(b/124318564): remove pylint directive\n      data = jnp.clip(data, -max_abs_value, +max_abs_value)  # pylint: disable=invalid-unary-operand-type\n    return data\n\n  return tree_utils.fast_map_structure(normalize_leaf, batch, mean_std.mean,\n                                       mean_std.std)",
  "def denormalize(batch: types.NestedArray,\n                mean_std: NestedMeanStd) -> types.NestedArray:\n  \"\"\"Denormalizes values in a nested structure using the given mean/std.\n\n  Only values of inexact types are denormalized.\n  See https://numpy.org/doc/stable/_images/dtype-hierarchy.png for Numpy type\n  hierarchy.\n\n  Args:\n    batch: a nested structure containing batch of data.\n    mean_std: mean and standard deviation used for denormalization.\n\n  Returns:\n    Nested structure with denormalized values.\n  \"\"\"\n\n  def denormalize_leaf(data: jnp.ndarray, mean: jnp.ndarray,\n                       std: jnp.ndarray) -> jnp.ndarray:\n    # Only denormalize inexact types.\n    if not np.issubdtype(data.dtype, np.inexact):\n      return data\n    return data * std + mean\n\n  return tree_utils.fast_map_structure(denormalize_leaf, batch, mean_std.mean,\n                                       mean_std.std)",
  "class NestClippingConfig:\n  \"\"\"Specifies how to clip Nests with the same structure.\n\n  Attributes:\n    path_map: A map that specifies how to clip values in Nests with the same\n      structure. Keys correspond to paths in the nest. Values are maximum\n      absolute values to use for clipping. If there is a collision between paths\n      (one path is a prefix of the other), the behavior is undefined.\n  \"\"\"\n  path_map: Tuple[Tuple[Path, float], ...] = ()",
  "def get_clip_config_for_path(config: NestClippingConfig,\n                             path: Path) -> NestClippingConfig:\n  \"\"\"Returns the config for a subtree from the leaf defined by the path.\"\"\"\n  # Start with an empty config.\n  path_map = []\n  for map_path, max_abs_value in config.path_map:\n    if _is_prefix(map_path, path):\n      return NestClippingConfig(path_map=(((), max_abs_value),))\n    if _is_prefix(path, map_path):\n      path_map.append((map_path[len(path):], max_abs_value))\n  return NestClippingConfig(path_map=tuple(path_map))",
  "def clip(batch: types.NestedArray,\n         clipping_config: NestClippingConfig) -> types.NestedArray:\n  \"\"\"Clips the batch.\"\"\"\n\n  def max_abs_value_for_path(path: Path, x: jnp.ndarray) -> Optional[float]:\n    del x  # Unused, needed by interface.\n    return next((max_abs_value\n                 for clipping_path, max_abs_value in clipping_config.path_map\n                 if _is_prefix(clipping_path, path)), None)\n\n  max_abs_values = tree_utils.fast_map_structure_with_path(\n      max_abs_value_for_path, batch)\n\n  def clip_leaf(data: jnp.ndarray,\n                max_abs_value: Optional[float]) -> jnp.ndarray:\n    if max_abs_value is not None:\n      # TODO(b/124318564): remove pylint directive\n      data = jnp.clip(data, -max_abs_value, +max_abs_value)  # pylint: disable=invalid-unary-operand-type\n    return data\n\n  return tree_utils.fast_map_structure(clip_leaf, batch, max_abs_values)",
  "class NestNormalizationConfig:\n  \"\"\"Specifies how to normalize Nests with the same structure.\n\n  Attributes:\n    stats_config: A config that defines how to compute running statistics to be\n      used for normalization.\n    clip_config: A config that defines how to clip normalized values.\n  \"\"\"\n  stats_config: NestStatisticsConfig = NestStatisticsConfig()\n  clip_config: NestClippingConfig = NestClippingConfig()",
  "def validate_node_shape(reference_sample: jnp.ndarray,\n                          batch: jnp.ndarray) -> None:\n    expected_shape = batch_dims + reference_sample.shape\n    assert batch.shape == expected_shape, f'{batch.shape} != {expected_shape}'",
  "def _compute_node_statistics(\n      path: Path, mean: jnp.ndarray, summed_variance: jnp.ndarray,\n      batch: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:\n    assert isinstance(mean, jnp.ndarray), type(mean)\n    assert isinstance(summed_variance, jnp.ndarray), type(summed_variance)\n    if not _is_path_included(config, path):\n      # Return unchanged.\n      return mean, summed_variance\n    # The mean and the sum of past variances are updated with Welford's\n    # algorithm using batches (see https://stackoverflow.com/q/56402955).\n    diff_to_old_mean = batch - mean\n    if weights is not None:\n      expanded_weights = jnp.reshape(\n          weights,\n          list(weights.shape) + [1] * (batch.ndim - weights.ndim))\n      diff_to_old_mean = diff_to_old_mean * expanded_weights\n    mean_update = jnp.sum(diff_to_old_mean, axis=batch_axis) / count\n    if pmap_axis_name is not None:\n      mean_update = jax.lax.psum(\n          mean_update, axis_name=pmap_axis_name)\n    mean = mean + mean_update\n\n    diff_to_new_mean = batch - mean\n    variance_update = diff_to_old_mean * diff_to_new_mean\n    variance_update = jnp.sum(variance_update, axis=batch_axis)\n    if pmap_axis_name is not None:\n      variance_update = jax.lax.psum(variance_update, axis_name=pmap_axis_name)\n    summed_variance = summed_variance + variance_update\n    return mean, summed_variance",
  "def compute_std(path: Path, summed_variance: jnp.ndarray,\n                  std: jnp.ndarray) -> jnp.ndarray:\n    assert isinstance(summed_variance, jnp.ndarray)\n    if not _is_path_included(config, path):\n      return std\n    # Summed variance can get negative due to rounding errors.\n    summed_variance = jnp.maximum(summed_variance, 0)\n    std = jnp.sqrt(summed_variance / count)\n    std = jnp.clip(std, std_min_value, std_max_value)\n    return std",
  "def normalize_leaf(data: jnp.ndarray, mean: jnp.ndarray,\n                     std: jnp.ndarray) -> jnp.ndarray:\n    # Only normalize inexact types.\n    if not jnp.issubdtype(data.dtype, jnp.inexact):\n      return data\n    data = (data - mean) / std\n    if max_abs_value is not None:\n      # TODO(b/124318564): remove pylint directive\n      data = jnp.clip(data, -max_abs_value, +max_abs_value)  # pylint: disable=invalid-unary-operand-type\n    return data",
  "def denormalize_leaf(data: jnp.ndarray, mean: jnp.ndarray,\n                       std: jnp.ndarray) -> jnp.ndarray:\n    # Only denormalize inexact types.\n    if not np.issubdtype(data.dtype, np.inexact):\n      return data\n    return data * std + mean",
  "def max_abs_value_for_path(path: Path, x: jnp.ndarray) -> Optional[float]:\n    del x  # Unused, needed by interface.\n    return next((max_abs_value\n                 for clipping_path, max_abs_value in clipping_config.path_map\n                 if _is_prefix(clipping_path, path)), None)",
  "def clip_leaf(data: jnp.ndarray,\n                max_abs_value: Optional[float]) -> jnp.ndarray:\n    if max_abs_value is not None:\n      # TODO(b/124318564): remove pylint directive\n      data = jnp.clip(data, -max_abs_value, +max_abs_value)  # pylint: disable=invalid-unary-operand-type\n    return data",
  "def add_batch_dim(values: types.Nest) -> types.NestedArray:\n  return jax.tree_map(lambda x: jnp.expand_dims(x, axis=0), values)",
  "def _flatten(x: jnp.ndarray, num_batch_dims: int) -> jnp.ndarray:\n  \"\"\"Flattens the input, preserving the first ``num_batch_dims`` dimensions.\n\n  If the input has fewer than ``num_batch_dims`` dimensions, it is returned\n  unchanged.\n  If the input has exactly ``num_batch_dims`` dimensions, an extra dimension\n  is added. This is needed to handle batched scalars.\n\n  Arguments:\n    x: the input array to flatten.\n    num_batch_dims: number of dimensions to preserve.\n\n  Returns:\n    flattened input.\n  \"\"\"\n  # TODO(b/173492429): consider throwing an error instead.\n  if x.ndim < num_batch_dims:\n    return x\n  return jnp.reshape(x, list(x.shape[:num_batch_dims]) + [-1])",
  "def batch_concat(\n    values: types.NestedArray,\n    num_batch_dims: int = 1,\n) -> jnp.ndarray:\n  \"\"\"Flatten and concatenate nested array structure, keeping batch dims.\"\"\"\n  flatten_fn = lambda x: _flatten(x, num_batch_dims)\n  flat_leaves = tree.map_structure(flatten_fn, values)\n  return jnp.concatenate(tree.flatten(flat_leaves), axis=-1)",
  "def zeros_like(nest: types.Nest, dtype=None) -> types.NestedArray:\n  return jax.tree_map(lambda x: jnp.zeros(x.shape, dtype or x.dtype), nest)",
  "def ones_like(nest: types.Nest, dtype=None) -> types.NestedArray:\n  return jax.tree_map(lambda x: jnp.ones(x.shape, dtype or x.dtype), nest)",
  "def squeeze_batch_dim(nest: types.Nest) -> types.NestedArray:\n  return jax.tree_map(lambda x: jnp.squeeze(x, axis=0), nest)",
  "def to_numpy_squeeze(values: types.Nest) -> types.NestedArray:\n  \"\"\"Converts to numpy and squeezes out dummy batch dimension.\"\"\"\n  return jax.tree_map(lambda x: np.asarray(x).squeeze(axis=0), values)",
  "def to_numpy(values: types.Nest) -> types.NestedArray:\n  return jax.tree_map(np.asarray, values)",
  "def fetch_devicearray(values: types.Nest) -> types.Nest:\n  \"\"\"Fetches and converts any DeviceArrays to np.ndarrays.\"\"\"\n  return tree.map_structure(_fetch_devicearray, values)",
  "def _fetch_devicearray(x):\n  if isinstance(x, jax.Array):\n    return np.asarray(x)\n  return x",
  "def batch_to_sequence(values: types.Nest) -> types.NestedArray:\n  return jax.tree_map(\n      lambda x: jnp.transpose(x, axes=(1, 0, *range(2, len(x.shape)))), values)",
  "def tile_array(array: jnp.ndarray, multiple: int) -> jnp.ndarray:\n  \"\"\"Tiles `multiple` copies of `array` along a new leading axis.\"\"\"\n  return jnp.stack([array] * multiple)",
  "def tile_nested(inputs: types.Nest, multiple: int) -> types.Nest:\n  \"\"\"Tiles tensors in a nested structure along a new leading axis.\"\"\"\n  tile = functools.partial(tile_array, multiple=multiple)\n  return jax.tree_map(tile, inputs)",
  "def maybe_recover_lstm_type(state: types.NestedArray) -> types.NestedArray:\n  \"\"\"Recovers the type hk.LSTMState if LSTMState is in the type name.\n\n  When the recurrent state of recurrent neural networks (RNN) is deserialized,\n  for example when it is sampled from replay, it is sometimes repacked in a type\n  that is identical to the source type but not the correct type itself. When\n  using this state as the initial state in an hk.dynamic_unroll, this will\n  cause hk.dynamic_unroll to raise an error as it requires its input and output\n  states to be identical.\n\n  Args:\n    state: a nested structure of arrays representing the state of an RNN.\n\n  Returns:\n    Either the state unchanged if it is anything but an LSTMState, otherwise\n    returns the state arrays properly contained in an hk.LSTMState.\n  \"\"\"\n  return hk.LSTMState(*state) if type(state).__name__ == 'LSTMState' else state",
  "def prefetch(\n    iterable: Iterable[T],\n    buffer_size: int = 5,\n    device: Optional[jax.Device] = None,\n    num_threads: int = NUM_PREFETCH_THREADS,\n) -> core.PrefetchingIterator[T]:\n  \"\"\"Returns prefetching iterator with additional 'ready' method.\"\"\"\n\n  return PrefetchIterator(iterable, buffer_size, device, num_threads)",
  "class PrefetchingSplit(NamedTuple):\n  host: types.NestedArray\n  device: types.NestedArray",
  "def keep_key_on_host(sample: reverb.ReplaySample) -> PrefetchingSplit:\n  \"\"\"Returns PrefetchingSplit which keeps uint64 reverb key on the host.\n\n  We want to avoid truncation of the uint64 reverb key by JAX.\n\n  Args:\n    sample: a sample from a Reverb replay buffer.\n\n  Returns:\n    PrefetchingSplit with device having the reverb sample, and key on host.\n  \"\"\"\n  return PrefetchingSplit(host=sample.info.key, device=sample)",
  "def device_put(\n    iterable: Iterable[types.NestedArray],\n    device: jax.Device,\n    split_fn: Optional[_SplitFunction] = None,\n):\n  \"\"\"Returns iterator that samples an item and places it on the device.\"\"\"\n\n  return PutToDevicesIterable(\n      iterable=iterable,\n      pmapped_user=False,\n      devices=[device],\n      split_fn=split_fn)",
  "def multi_device_put(\n    iterable: Iterable[types.NestedArray],\n    devices: Sequence[jax.Device],\n    split_fn: Optional[_SplitFunction] = None,\n):\n  \"\"\"Returns iterator that, per device, samples an item and places on device.\"\"\"\n\n  return PutToDevicesIterable(\n      iterable=iterable, pmapped_user=True, devices=devices, split_fn=split_fn)",
  "class PutToDevicesIterable(Iterable[types.NestedArray]):\n  \"\"\"Per device, samples an item from iterator and places on device.\n\n  if pmapped_user:\n    Items from the resulting generator are intended to be used in a pmapped\n    function. Every element is a ShardedDeviceArray or (nested) Python container\n    thereof. A single next() call to this iterator results in len(devices)\n    calls to the underlying iterator. The returned items are put one on each\n    device.\n  if not pmapped_user:\n    Places a sample from the iterator on the given device.\n\n  Yields:\n    If no split_fn is specified:\n      DeviceArray/ShardedDeviceArray or (nested) Python container thereof\n      representing the elements of shards stacked together, with each shard\n      backed by physical device memory specified by the corresponding entry in\n      devices.\n\n    If split_fn is specified:\n      PrefetchingSplit where the .host element is a stacked numpy array or\n      (nested) Python contained thereof. The .device element is a\n      DeviceArray/ShardedDeviceArray or (nested) Python container thereof.\n\n  Raises:\n    StopIteration: if there are not enough items left in the iterator to place\n      one sample on each device.\n    Any error thrown by the iterable_function. Note this is not raised inside\n      the producer, but after it finishes executing.\n  \"\"\"\n\n  def __init__(\n      self,\n      iterable: Iterable[types.NestedArray],\n      pmapped_user: bool,\n      devices: Sequence[jax.Device],\n      split_fn: Optional[_SplitFunction] = None,\n  ):\n    \"\"\"Constructs PutToDevicesIterable.\n\n    Args:\n      iterable: A python iterable. This is used to build the python prefetcher.\n        Note that each iterable should only be passed to this function once as\n        iterables aren't thread safe.\n      pmapped_user: whether the user of data from this iterator is implemented\n        using pmapping.\n      devices: Devices used for prefecthing.\n      split_fn: Optional function applied to every element from the iterable to\n        split the parts of it that will be kept in the host and the parts that\n        will sent to the device.\n\n    Raises:\n      ValueError: If devices list is empty, or if pmapped_use=False and more\n        than 1 device is provided.\n    \"\"\"\n    self.num_devices = len(devices)\n    if self.num_devices == 0:\n      raise ValueError('At least one device must be specified.')\n    if (not pmapped_user) and (self.num_devices != 1):\n      raise ValueError('User is not implemented with pmapping but len(devices) '\n                       f'= {len(devices)} is not equal to 1! Devices given are:'\n                       f'\\n{devices}')\n\n    self.iterable = iterable\n    self.pmapped_user = pmapped_user\n    self.split_fn = split_fn\n    self.devices = devices\n    self.iterator = iter(self.iterable)\n\n  def __iter__(self) -> Iterator[types.NestedArray]:\n    # It is important to structure the Iterable like this, because in\n    # JustPrefetchIterator we must build a new iterable for each thread.\n    # This is crucial if working with tensorflow datasets because tf.Graph\n    # objects are thread local.\n    self.iterator = iter(self.iterable)\n    return self\n\n  def __next__(self) -> types.NestedArray:\n    try:\n      if not self.pmapped_user:\n        item = next(self.iterator)\n        if self.split_fn is None:\n          return jax.device_put(item, self.devices[0])\n        item_split = self.split_fn(item)\n        return PrefetchingSplit(\n            host=item_split.host,\n            device=jax.device_put(item_split.device, self.devices[0]))\n\n      items = itertools.islice(self.iterator, self.num_devices)\n      items = tuple(items)\n      if len(items) < self.num_devices:\n        raise StopIteration\n      if self.split_fn is None:\n        return jax.device_put_sharded(tuple(items), self.devices)\n      else:\n        # ((host: x1, device: y1), ..., (host: xN, device: yN)).\n        items_split = (self.split_fn(item) for item in items)\n        # (host: (x1, ..., xN), device: (y1, ..., yN)).\n        split = tree.map_structure_up_to(\n            PrefetchingSplit(None, None), lambda *x: x, *items_split)\n\n        return PrefetchingSplit(\n            host=np.stack(split.host),\n            device=jax.device_put_sharded(split.device, self.devices))\n\n    except StopIteration:\n      raise\n\n    except Exception:  # pylint: disable=broad-except\n      logging.exception('Error for %s', self.iterable)\n      raise",
  "def sharded_prefetch(\n    iterable: Iterable[types.NestedArray],\n    buffer_size: int = 5,\n    num_threads: int = 1,\n    split_fn: Optional[_SplitFunction] = None,\n    devices: Optional[Sequence[jax.Device]] = None,\n) -> core.PrefetchingIterator:\n  \"\"\"Performs sharded prefetching from an iterable in separate threads.\n\n  Elements from the resulting generator are intended to be used in a jax.pmap\n  call. Every element is a sharded prefetched array with an additional replica\n  dimension and corresponds to jax.local_device_count() elements from the\n  original iterable.\n\n  Args:\n    iterable: A python iterable. This is used to build the python prefetcher.\n      Note that each iterable should only be passed to this function once as\n      iterables aren't thread safe.\n    buffer_size (int): Number of elements to keep in the prefetch buffer.\n    num_threads (int): Number of threads.\n    split_fn: Optional function applied to every element from the iterable to\n      split the parts of it that will be kept in the host and the parts that\n      will sent to the device.\n    devices: Devices used for prefecthing. Optional, jax.local_devices() by\n      default.\n\n  Returns:\n    Prefetched elements from the original iterable with additional replica\n    dimension.\n  Raises:\n    ValueError if the buffer_size <= 1.\n    Any error thrown by the iterable_function. Note this is not raised inside\n      the producer, but after it finishes executing.\n  \"\"\"\n\n  devices = devices or jax.local_devices()\n\n  iterable = PutToDevicesIterable(\n      iterable=iterable, pmapped_user=True, devices=devices, split_fn=split_fn)\n\n  return prefetch(iterable, buffer_size, device=None, num_threads=num_threads)",
  "def replicate_in_all_devices(\n    nest: N, devices: Optional[Sequence[jax.Device]] = None\n) -> N:\n  \"\"\"Replicate array nest in all available devices.\"\"\"\n  devices = devices or jax.local_devices()\n  return jax.device_put_sharded([nest] * len(devices), devices)",
  "def get_from_first_device(nest: N, as_numpy: bool = True) -> N:\n  \"\"\"Gets the first array of a nest of `jax.Array`s.\n\n  Args:\n    nest: A nest of `jax.Array`s.\n    as_numpy: If `True` then each `DeviceArray` that is retrieved is transformed\n      (and copied if not on the host machine) into a `np.ndarray`.\n\n  Returns:\n    The first array of a nest of `jax.Array`s. Note that if\n    `as_numpy=False` then the array will be a `DeviceArray` (which will live on\n    the same device as the sharded device array). If `as_numpy=True` then the\n    array will be copied to the host machine and converted into a `np.ndarray`.\n  \"\"\"\n  zeroth_nest = jax.tree_map(lambda x: x[0], nest)\n  return jax.device_get(zeroth_nest) if as_numpy else zeroth_nest",
  "def mapreduce(\n    f: F,\n    reduce_fn: Optional[Callable[[jax.Array], jax.Array]] = None,\n    **vmap_kwargs,\n) -> F:\n  \"\"\"A simple decorator that transforms `f` into (`reduce_fn` o vmap o f).\n\n  By default, we vmap over axis 0, and the `reduce_fn` is jnp.mean over axis 0.\n  Note that the call signature of `f` is invariant under this transformation.\n\n  If, for example, f has shape signature [H, W] -> [N], then mapreduce(f)\n  (with the default arguments) will have shape signature [B, H, W] -> [N].\n\n  Args:\n    f: A pure function over examples.\n    reduce_fn: A pure function that reduces DeviceArrays -> DeviceArrays.\n    **vmap_kwargs: Keyword arguments to forward to `jax.vmap`.\n\n  Returns:\n    g: A pure function over batches of examples.\n  \"\"\"\n\n  if reduce_fn is None:\n    reduce_fn = lambda x: jnp.mean(x, axis=0)\n\n  vmapped_f = jax.vmap(f, **vmap_kwargs)\n\n  def g(*args, **kwargs):\n    return jax.tree_map(reduce_fn, vmapped_f(*args, **kwargs))\n\n  return g",
  "def process_multiple_batches(\n    process_one_batch: Callable[[_TrainingState, _TrainingData],\n                                Tuple[_TrainingState, _TrainingAux]],\n    num_batches: int,\n    postprocess_aux: Optional[Callable[[_TrainingAux], _TrainingAux]] = None\n) -> Callable[[_TrainingState, _TrainingData], Tuple[_TrainingState,\n                                                     _TrainingAux]]:\n  \"\"\"Makes 'process_one_batch' process multiple batches at once.\n\n  Args:\n    process_one_batch: a function that takes 'state' and 'data', and returns\n      'new_state' and 'aux' (for example 'metrics').\n    num_batches: how many batches to process at once\n    postprocess_aux: how to merge the extra information, defaults to taking the\n      mean.\n\n  Returns:\n    A function with the same interface as 'process_one_batch' which processes\n    multiple batches at once.\n  \"\"\"\n  assert num_batches >= 1\n  if num_batches == 1:\n    if not postprocess_aux:\n      return process_one_batch\n    def _process_one_batch(state, data):\n      state, aux = process_one_batch(state, data)\n      return state, postprocess_aux(aux)\n    return _process_one_batch\n\n  if postprocess_aux is None:\n    postprocess_aux = lambda x: jax.tree_map(jnp.mean, x)\n\n  def _process_multiple_batches(state, data):\n    data = jax.tree_map(\n        lambda a: jnp.reshape(a, (num_batches, -1, *a.shape[1:])), data)\n\n    state, aux = jax.lax.scan(\n        process_one_batch, state, data, length=num_batches)\n    return state, postprocess_aux(aux)\n\n  return _process_multiple_batches",
  "def process_many_batches(\n    process_one_batch: Callable[[_TrainingState, _TrainingData],\n                                jax_types.TrainingStepOutput[_TrainingState]],\n    num_batches: int,\n    postprocess_aux: Optional[Callable[[jax_types.TrainingMetrics],\n                                       jax_types.TrainingMetrics]] = None\n) -> Callable[[_TrainingState, _TrainingData],\n              jax_types.TrainingStepOutput[_TrainingState]]:\n  \"\"\"The version of 'process_multiple_batches' with stronger typing.\"\"\"\n\n  def _process_one_batch(\n      state: _TrainingState,\n      data: _TrainingData) -> Tuple[_TrainingState, jax_types.TrainingMetrics]:\n    result = process_one_batch(state, data)\n    return result.state, result.metrics\n\n  func = process_multiple_batches(_process_one_batch, num_batches,\n                                  postprocess_aux)\n\n  def _process_many_batches(\n      state: _TrainingState,\n      data: _TrainingData) -> jax_types.TrainingStepOutput[_TrainingState]:\n    state, aux = func(state, data)\n    return jax_types.TrainingStepOutput(state, aux)\n\n  return _process_many_batches",
  "def weighted_softmax(x: jnp.ndarray, weights: jnp.ndarray, axis: int = 0):\n  x = x - jnp.max(x, axis=axis)\n  return weights * jnp.exp(x) / jnp.sum(weights * jnp.exp(x),\n                                        axis=axis, keepdims=True)",
  "def sample_uint32(random_key: jax_types.PRNGKey) -> int:\n  \"\"\"Returns an integer uniformly distributed in 0..2^32-1.\"\"\"\n  iinfo = jnp.iinfo(jnp.int32)\n  # randint only accepts int32 values as min and max.\n  jax_random = jax.random.randint(\n      random_key, shape=(), minval=iinfo.min, maxval=iinfo.max, dtype=jnp.int32)\n  return np.uint32(jax_random).item()",
  "class PrefetchIterator(core.PrefetchingIterator):\n  \"\"\"Performs prefetching from an iterable in separate threads.\n\n  Its interface is additionally extended with `ready` method which tells whether\n  there is any data waiting for processing and a `retrieved_elements` method\n  specifying number of elements retrieved from the iterator.\n\n  Yields:\n    Prefetched elements from the original iterable.\n\n  Raises:\n    ValueError: if the buffer_size < 1.\n    StopIteration: If the iterable contains no more items.\n    Any error thrown by the iterable_function. Note this is not raised inside\n      the producer, but after it finishes executing.\n  \"\"\"\n\n  def __init__(\n      self,\n      iterable: Iterable[types.NestedArray],\n      buffer_size: int = 5,\n      device: Optional[jax.Device] = None,\n      num_threads: int = NUM_PREFETCH_THREADS,\n  ):\n    \"\"\"Constructs PrefetchIterator.\n\n    Args:\n      iterable: A python iterable. This is used to build the python prefetcher.\n        Note that each iterable should only be passed to this function once as\n        iterables aren't thread safe.\n      buffer_size (int): Number of elements to keep in the prefetch buffer.\n      device (deprecated): Optionally place items from the iterable on the given\n        device. If None, the items are returns as given by the iterable. This\n        argument is deprecated and the recommended usage is to wrap the\n        iterables using utils.device_put or utils.multi_device_put before using\n        utils.prefetch.\n      num_threads (int): Number of threads.\n    \"\"\"\n\n    if buffer_size < 1:\n      raise ValueError('the buffer_size should be >= 1')\n    self.buffer = queue.Queue(maxsize=buffer_size)\n    self.producer_error = []\n    self.end = object()\n    self.iterable = iterable\n    self.device = device\n    self.count = 0\n\n    # Start producer threads.\n    for _ in range(num_threads):\n      threading.Thread(target=self.producer, daemon=True).start()\n\n  def producer(self):\n    \"\"\"Enqueues items from `iterable` on a given thread.\"\"\"\n    try:\n      # Build a new iterable for each thread. This is crucial if working with\n      # tensorflow datasets because tf.Graph objects are thread local.\n      for item in self.iterable:\n        if self.device:\n          jax.device_put(item, self.device)\n        self.buffer.put(item)\n    except Exception as e:  # pylint: disable=broad-except\n      logging.exception('Error in producer thread for %s', self.iterable)\n      self.producer_error.append(e)\n    finally:\n      self.buffer.put(self.end)\n\n  def __iter__(self):\n    return self\n\n  def ready(self):\n    return not self.buffer.empty()\n\n  def retrieved_elements(self):\n    return self.count\n\n  def __next__(self):\n    value = self.buffer.get()\n    if value is self.end:\n      if self.producer_error:\n        raise self.producer_error[0] from self.producer_error[0]\n      raise StopIteration\n    self.count += 1\n    return value",
  "def __init__(\n      self,\n      iterable: Iterable[types.NestedArray],\n      pmapped_user: bool,\n      devices: Sequence[jax.Device],\n      split_fn: Optional[_SplitFunction] = None,\n  ):\n    \"\"\"Constructs PutToDevicesIterable.\n\n    Args:\n      iterable: A python iterable. This is used to build the python prefetcher.\n        Note that each iterable should only be passed to this function once as\n        iterables aren't thread safe.\n      pmapped_user: whether the user of data from this iterator is implemented\n        using pmapping.\n      devices: Devices used for prefecthing.\n      split_fn: Optional function applied to every element from the iterable to\n        split the parts of it that will be kept in the host and the parts that\n        will sent to the device.\n\n    Raises:\n      ValueError: If devices list is empty, or if pmapped_use=False and more\n        than 1 device is provided.\n    \"\"\"\n    self.num_devices = len(devices)\n    if self.num_devices == 0:\n      raise ValueError('At least one device must be specified.')\n    if (not pmapped_user) and (self.num_devices != 1):\n      raise ValueError('User is not implemented with pmapping but len(devices) '\n                       f'= {len(devices)} is not equal to 1! Devices given are:'\n                       f'\\n{devices}')\n\n    self.iterable = iterable\n    self.pmapped_user = pmapped_user\n    self.split_fn = split_fn\n    self.devices = devices\n    self.iterator = iter(self.iterable)",
  "def __iter__(self) -> Iterator[types.NestedArray]:\n    # It is important to structure the Iterable like this, because in\n    # JustPrefetchIterator we must build a new iterable for each thread.\n    # This is crucial if working with tensorflow datasets because tf.Graph\n    # objects are thread local.\n    self.iterator = iter(self.iterable)\n    return self",
  "def __next__(self) -> types.NestedArray:\n    try:\n      if not self.pmapped_user:\n        item = next(self.iterator)\n        if self.split_fn is None:\n          return jax.device_put(item, self.devices[0])\n        item_split = self.split_fn(item)\n        return PrefetchingSplit(\n            host=item_split.host,\n            device=jax.device_put(item_split.device, self.devices[0]))\n\n      items = itertools.islice(self.iterator, self.num_devices)\n      items = tuple(items)\n      if len(items) < self.num_devices:\n        raise StopIteration\n      if self.split_fn is None:\n        return jax.device_put_sharded(tuple(items), self.devices)\n      else:\n        # ((host: x1, device: y1), ..., (host: xN, device: yN)).\n        items_split = (self.split_fn(item) for item in items)\n        # (host: (x1, ..., xN), device: (y1, ..., yN)).\n        split = tree.map_structure_up_to(\n            PrefetchingSplit(None, None), lambda *x: x, *items_split)\n\n        return PrefetchingSplit(\n            host=np.stack(split.host),\n            device=jax.device_put_sharded(split.device, self.devices))\n\n    except StopIteration:\n      raise\n\n    except Exception:  # pylint: disable=broad-except\n      logging.exception('Error for %s', self.iterable)\n      raise",
  "def g(*args, **kwargs):\n    return jax.tree_map(reduce_fn, vmapped_f(*args, **kwargs))",
  "def _process_multiple_batches(state, data):\n    data = jax.tree_map(\n        lambda a: jnp.reshape(a, (num_batches, -1, *a.shape[1:])), data)\n\n    state, aux = jax.lax.scan(\n        process_one_batch, state, data, length=num_batches)\n    return state, postprocess_aux(aux)",
  "def _process_one_batch(\n      state: _TrainingState,\n      data: _TrainingData) -> Tuple[_TrainingState, jax_types.TrainingMetrics]:\n    result = process_one_batch(state, data)\n    return result.state, result.metrics",
  "def _process_many_batches(\n      state: _TrainingState,\n      data: _TrainingData) -> jax_types.TrainingStepOutput[_TrainingState]:\n    state, aux = func(state, data)\n    return jax_types.TrainingStepOutput(state, aux)",
  "def __init__(\n      self,\n      iterable: Iterable[types.NestedArray],\n      buffer_size: int = 5,\n      device: Optional[jax.Device] = None,\n      num_threads: int = NUM_PREFETCH_THREADS,\n  ):\n    \"\"\"Constructs PrefetchIterator.\n\n    Args:\n      iterable: A python iterable. This is used to build the python prefetcher.\n        Note that each iterable should only be passed to this function once as\n        iterables aren't thread safe.\n      buffer_size (int): Number of elements to keep in the prefetch buffer.\n      device (deprecated): Optionally place items from the iterable on the given\n        device. If None, the items are returns as given by the iterable. This\n        argument is deprecated and the recommended usage is to wrap the\n        iterables using utils.device_put or utils.multi_device_put before using\n        utils.prefetch.\n      num_threads (int): Number of threads.\n    \"\"\"\n\n    if buffer_size < 1:\n      raise ValueError('the buffer_size should be >= 1')\n    self.buffer = queue.Queue(maxsize=buffer_size)\n    self.producer_error = []\n    self.end = object()\n    self.iterable = iterable\n    self.device = device\n    self.count = 0\n\n    # Start producer threads.\n    for _ in range(num_threads):\n      threading.Thread(target=self.producer, daemon=True).start()",
  "def producer(self):\n    \"\"\"Enqueues items from `iterable` on a given thread.\"\"\"\n    try:\n      # Build a new iterable for each thread. This is crucial if working with\n      # tensorflow datasets because tf.Graph objects are thread local.\n      for item in self.iterable:\n        if self.device:\n          jax.device_put(item, self.device)\n        self.buffer.put(item)\n    except Exception as e:  # pylint: disable=broad-except\n      logging.exception('Error in producer thread for %s', self.iterable)\n      self.producer_error.append(e)\n    finally:\n      self.buffer.put(self.end)",
  "def __iter__(self):\n    return self",
  "def ready(self):\n    return not self.buffer.empty()",
  "def retrieved_elements(self):\n    return self.count",
  "def __next__(self):\n    value = self.buffer.get()\n    if value is self.end:\n      if self.producer_error:\n        raise self.producer_error[0] from self.producer_error[0]\n      raise StopIteration\n    self.count += 1\n    return value",
  "def _process_one_batch(state, data):\n      state, aux = process_one_batch(state, data)\n      return state, postprocess_aux(aux)",
  "def restore_from_path(ckpt_dir: str) -> CheckpointState:\n  \"\"\"Restore the state stored in ckpt_dir.\"\"\"\n  array_path = os.path.join(ckpt_dir, _ARRAY_NAME)\n  exemplar_path = os.path.join(ckpt_dir, _EXEMPLAR_NAME)\n\n  with open(exemplar_path, 'rb') as f:\n    exemplar = pickle.load(f)\n\n  with open(array_path, 'rb') as f:\n    files = np.load(f, allow_pickle=True)\n    flat_state = [files[key] for key in files.files]\n  unflattened_tree = tree.unflatten_as(exemplar, flat_state)\n\n  def maybe_convert_to_python(value, numpy):\n    return value if numpy else value.item()\n\n  return tree.map_structure(maybe_convert_to_python, unflattened_tree, exemplar)",
  "def save_to_path(ckpt_dir: str, state: CheckpointState):\n  \"\"\"Save the state in ckpt_dir.\"\"\"\n\n  if not os.path.exists(ckpt_dir):\n    os.makedirs(ckpt_dir)\n\n  is_numpy = lambda x: isinstance(x, (np.ndarray, jax.Array))\n  flat_state = tree.flatten(state)\n  nest_exemplar = tree.map_structure(is_numpy, state)\n\n  array_path = os.path.join(ckpt_dir, _ARRAY_NAME)\n  logging.info('Saving flattened array nest to %s', array_path)\n  def _disabled_seek(*_):\n    raise AttributeError('seek() is disabled on this object.')\n  with open(array_path, 'wb') as f:\n    setattr(f, 'seek', _disabled_seek)\n    np.savez(f, *flat_state)\n\n  exemplar_path = os.path.join(ckpt_dir, _EXEMPLAR_NAME)\n  logging.info('Saving nest exemplar to %s', exemplar_path)\n  with open(exemplar_path, 'wb') as f:\n    pickle.dump(nest_exemplar, f)",
  "class Checkpointer(tf_savers.Checkpointer):\n\n  def __init__(\n      self,\n      object_to_save: core.Saveable,\n      directory: str = '~/acme',\n      subdirectory: str = 'default',\n      **tf_checkpointer_kwargs):\n    super().__init__(dict(saveable=object_to_save),\n                     directory=directory,\n                     subdirectory=subdirectory,\n                     **tf_checkpointer_kwargs)",
  "def maybe_convert_to_python(value, numpy):\n    return value if numpy else value.item()",
  "def _disabled_seek(*_):\n    raise AttributeError('seek() is disabled on this object.')",
  "def __init__(\n      self,\n      object_to_save: core.Saveable,\n      directory: str = '~/acme',\n      subdirectory: str = 'default',\n      **tf_checkpointer_kwargs):\n    super().__init__(dict(saveable=object_to_save),\n                     directory=directory,\n                     subdirectory=subdirectory,\n                     **tf_checkpointer_kwargs)",
  "class JAXSnapshotter(core.Worker):\n  \"\"\"Periodically fetches new version of params and stores tf.saved_models.\"\"\"\n\n  # NOTE: External contributor please refrain from modifying the high level of\n  # the API defined here.\n\n  def __init__(self,\n               variable_source: core.VariableSource,\n               models: Dict[str, Callable[[core.VariableSource],\n                                          types.ModelToSnapshot]],\n               path: str,\n               subdirectory: Optional[str] = None,\n               max_to_keep: Optional[int] = None,\n               add_uid: bool = False):\n    self._variable_source = variable_source\n    self._models = models\n    if subdirectory is not None:\n      self._path = paths.process_path(path, subdirectory, add_uid=add_uid)\n    else:\n      self._path = paths.process_path(path, add_uid=add_uid)\n    self._max_to_keep = max_to_keep\n    self._snapshot_paths: Optional[List[str]] = None\n\n  # Handle preemption signal. Note that this must happen in the main thread.\n  def _signal_handler(self):\n    logging.info('Caught SIGTERM: forcing models save.')\n    self._save()\n\n  def _save(self):\n    if not self._snapshot_paths:\n      # Lazy discovery of already existing snapshots.\n      self._snapshot_paths = os.listdir(self._path)\n      self._snapshot_paths.sort(reverse=True)\n\n    snapshot_location = os.path.join(self._path, time.strftime('%Y%m%d-%H%M%S'))\n    if self._snapshot_paths and self._snapshot_paths[0] == snapshot_location:\n      logging.info('Snapshot for the current time already exists.')\n      return\n\n    # To make sure models are captured as close as possible from the same time\n    # we gather all the `ModelToSnapshot` in a 1st loop. We then convert/saved\n    # them in another loop as this operation can be slow.\n    models_and_paths = self._get_models_and_paths(path=snapshot_location)\n    self._snapshot_paths.insert(0, snapshot_location)\n\n    for model, saving_path in models_and_paths:\n      self._snapshot_model(model=model, saving_path=saving_path)\n\n    # Delete any excess snapshots.\n    while self._max_to_keep and len(self._snapshot_paths) > self._max_to_keep:\n      paths.rmdir(os.path.join(self._path, self._snapshot_paths.pop()))\n\n  def _get_models_and_paths(\n      self, path: str) -> Sequence[Tuple[types.ModelToSnapshot, str]]:\n    \"\"\"Gets the models to save asssociated with their saving path.\"\"\"\n    models_and_paths = []\n    for name, model_fn in self._models.items():\n      model = model_fn(self._variable_source)\n      model_path = os.path.join(path, name)\n      models_and_paths.append((model, model_path))\n    return models_and_paths\n\n  def _snapshot_model(self, model: types.ModelToSnapshot,\n                      saving_path: str) -> None:\n    module = model_to_tf_module(model)\n    tf.saved_model.save(module, saving_path)\n\n  def run(self):\n    \"\"\"Runs the saver.\"\"\"\n    with signals.runtime_terminator(self._signal_handler):\n      while True:\n        self._save()\n        time.sleep(5 * 60)",
  "def model_to_tf_module(model: types.ModelToSnapshot) -> tf.Module:\n\n  def jax_fn_to_save(**kwargs):\n    return model.model(model.params, **kwargs)\n\n  module = tf.Module()\n  module.f = tf.function(jax2tf.convert(jax_fn_to_save), autograph=False)\n  # Traces input to ensure the model has the correct shapes.\n  module.f(**model.dummy_kwargs)\n  return module",
  "def __init__(self,\n               variable_source: core.VariableSource,\n               models: Dict[str, Callable[[core.VariableSource],\n                                          types.ModelToSnapshot]],\n               path: str,\n               subdirectory: Optional[str] = None,\n               max_to_keep: Optional[int] = None,\n               add_uid: bool = False):\n    self._variable_source = variable_source\n    self._models = models\n    if subdirectory is not None:\n      self._path = paths.process_path(path, subdirectory, add_uid=add_uid)\n    else:\n      self._path = paths.process_path(path, add_uid=add_uid)\n    self._max_to_keep = max_to_keep\n    self._snapshot_paths: Optional[List[str]] = None",
  "def _signal_handler(self):\n    logging.info('Caught SIGTERM: forcing models save.')\n    self._save()",
  "def _save(self):\n    if not self._snapshot_paths:\n      # Lazy discovery of already existing snapshots.\n      self._snapshot_paths = os.listdir(self._path)\n      self._snapshot_paths.sort(reverse=True)\n\n    snapshot_location = os.path.join(self._path, time.strftime('%Y%m%d-%H%M%S'))\n    if self._snapshot_paths and self._snapshot_paths[0] == snapshot_location:\n      logging.info('Snapshot for the current time already exists.')\n      return\n\n    # To make sure models are captured as close as possible from the same time\n    # we gather all the `ModelToSnapshot` in a 1st loop. We then convert/saved\n    # them in another loop as this operation can be slow.\n    models_and_paths = self._get_models_and_paths(path=snapshot_location)\n    self._snapshot_paths.insert(0, snapshot_location)\n\n    for model, saving_path in models_and_paths:\n      self._snapshot_model(model=model, saving_path=saving_path)\n\n    # Delete any excess snapshots.\n    while self._max_to_keep and len(self._snapshot_paths) > self._max_to_keep:\n      paths.rmdir(os.path.join(self._path, self._snapshot_paths.pop()))",
  "def _get_models_and_paths(\n      self, path: str) -> Sequence[Tuple[types.ModelToSnapshot, str]]:\n    \"\"\"Gets the models to save asssociated with their saving path.\"\"\"\n    models_and_paths = []\n    for name, model_fn in self._models.items():\n      model = model_fn(self._variable_source)\n      model_path = os.path.join(path, name)\n      models_and_paths.append((model, model_path))\n    return models_and_paths",
  "def _snapshot_model(self, model: types.ModelToSnapshot,\n                      saving_path: str) -> None:\n    module = model_to_tf_module(model)\n    tf.saved_model.save(module, saving_path)",
  "def run(self):\n    \"\"\"Runs the saver.\"\"\"\n    with signals.runtime_terminator(self._signal_handler):\n      while True:\n        self._save()\n        time.sleep(5 * 60)",
  "def jax_fn_to_save(**kwargs):\n    return model.model(model.params, **kwargs)",
  "def dummy_network(x):\n  return hk.nets.MLP([50, 10])(x)",
  "class VariableClientTest(absltest.TestCase):\n\n  def test_update(self):\n    init_fn, _ = hk.without_apply_rng(\n        hk.transform(dummy_network))\n    params = init_fn(jax.random.PRNGKey(1), jnp.zeros(shape=(1, 32)))\n    variable_source = fakes.VariableSource(params)\n    variable_client = variable_utils.VariableClient(\n        variable_source, key='policy')\n    variable_client.update_and_wait()\n    tree.map_structure(np.testing.assert_array_equal, variable_client.params,\n                       params)\n\n  def test_multiple_keys(self):\n    init_fn, _ = hk.without_apply_rng(\n        hk.transform(dummy_network))\n    params = init_fn(jax.random.PRNGKey(1), jnp.zeros(shape=(1, 32)))\n    steps = jnp.zeros(shape=1)\n    variables = {'network': params, 'steps': steps}\n    variable_source = fakes.VariableSource(variables, use_default_key=False)\n    variable_client = variable_utils.VariableClient(\n        variable_source, key=['network', 'steps'])\n    variable_client.update_and_wait()\n\n    tree.map_structure(np.testing.assert_array_equal, variable_client.params[0],\n                       params)\n    tree.map_structure(np.testing.assert_array_equal, variable_client.params[1],\n                       steps)",
  "def test_update(self):\n    init_fn, _ = hk.without_apply_rng(\n        hk.transform(dummy_network))\n    params = init_fn(jax.random.PRNGKey(1), jnp.zeros(shape=(1, 32)))\n    variable_source = fakes.VariableSource(params)\n    variable_client = variable_utils.VariableClient(\n        variable_source, key='policy')\n    variable_client.update_and_wait()\n    tree.map_structure(np.testing.assert_array_equal, variable_client.params,\n                       params)",
  "def test_multiple_keys(self):\n    init_fn, _ = hk.without_apply_rng(\n        hk.transform(dummy_network))\n    params = init_fn(jax.random.PRNGKey(1), jnp.zeros(shape=(1, 32)))\n    steps = jnp.zeros(shape=1)\n    variables = {'network': params, 'steps': steps}\n    variable_source = fakes.VariableSource(variables, use_default_key=False)\n    variable_client = variable_utils.VariableClient(\n        variable_source, key=['network', 'steps'])\n    variable_client.update_and_wait()\n\n    tree.map_structure(np.testing.assert_array_equal, variable_client.params[0],\n                       params)\n    tree.map_structure(np.testing.assert_array_equal, variable_client.params[1],\n                       steps)",
  "class DummySaveable(core.Saveable):\n\n  def __init__(self, state):\n    self.state = state\n\n  def save(self):\n    return self.state\n\n  def restore(self, state):\n    self.state = state",
  "def nest_assert_equal(a, b):\n  tree.map_structure(np.testing.assert_array_equal, a, b)",
  "class SaverTest(test_utils.TestCase):\n\n  def setUp(self):\n    super().setUp()\n    self._test_state = {\n        'foo': jnp.ones(shape=(8, 4), dtype=jnp.float32),\n        'bar': [jnp.zeros(shape=(3, 2), dtype=jnp.int32)],\n        'baz': 3,\n    }\n\n  def test_save_restore(self):\n    \"\"\"Checks that we can save and restore state.\"\"\"\n    directory = self.get_tempdir()\n    savers.save_to_path(directory, self._test_state)\n    result = savers.restore_from_path(directory)\n    nest_assert_equal(result, self._test_state)\n\n  def test_checkpointer(self):\n    \"\"\"Checks that the Checkpointer class saves and restores as expected.\"\"\"\n\n    with mock.patch.object(paths, 'get_unique_id') as mock_unique_id:\n      mock_unique_id.return_value = ('test',)\n\n      # Given a path and some stateful object...\n      directory = self.get_tempdir()\n      x = DummySaveable(self._test_state)\n\n      # If we checkpoint it...\n      checkpointer = savers.Checkpointer(x, directory, time_delta_minutes=0)\n      checkpointer.save()\n\n      # The checkpointer should restore the object's state.\n      x.state = None\n      checkpointer.restore()\n      nest_assert_equal(x.state, self._test_state)\n\n      # Checkpointers should also attempt a restore at construction time.\n      x.state = None\n      savers.Checkpointer(x, directory, time_delta_minutes=0)\n      nest_assert_equal(x.state, self._test_state)",
  "def __init__(self, state):\n    self.state = state",
  "def save(self):\n    return self.state",
  "def restore(self, state):\n    self.state = state",
  "def setUp(self):\n    super().setUp()\n    self._test_state = {\n        'foo': jnp.ones(shape=(8, 4), dtype=jnp.float32),\n        'bar': [jnp.zeros(shape=(3, 2), dtype=jnp.int32)],\n        'baz': 3,\n    }",
  "def test_save_restore(self):\n    \"\"\"Checks that we can save and restore state.\"\"\"\n    directory = self.get_tempdir()\n    savers.save_to_path(directory, self._test_state)\n    result = savers.restore_from_path(directory)\n    nest_assert_equal(result, self._test_state)",
  "def test_checkpointer(self):\n    \"\"\"Checks that the Checkpointer class saves and restores as expected.\"\"\"\n\n    with mock.patch.object(paths, 'get_unique_id') as mock_unique_id:\n      mock_unique_id.return_value = ('test',)\n\n      # Given a path and some stateful object...\n      directory = self.get_tempdir()\n      x = DummySaveable(self._test_state)\n\n      # If we checkpoint it...\n      checkpointer = savers.Checkpointer(x, directory, time_delta_minutes=0)\n      checkpointer.save()\n\n      # The checkpointer should restore the object's state.\n      x.state = None\n      checkpointer.restore()\n      nest_assert_equal(x.state, self._test_state)\n\n      # Checkpointers should also attempt a restore at construction time.\n      x.state = None\n      savers.Checkpointer(x, directory, time_delta_minutes=0)\n      nest_assert_equal(x.state, self._test_state)",
  "class TrainingStepOutput(Generic[TrainingState]):\n  state: TrainingState\n  metrics: TrainingMetrics",
  "class ModelToSnapshot:\n  \"\"\"Stores all necessary info to be able to save a model.\n\n  Attributes:\n    model: a jax function to be saved.\n    params: fixed params to be passed to the function.\n    dummy_kwargs: arguments to be passed to the function.\n  \"\"\"\n  model: Any  # Callable[params, **dummy_kwargs]\n  params: Any\n  dummy_kwargs: Dict[str, Any]",
  "class TestNestedSpec(NamedTuple):\n  # Note: the fields are intentionally in reverse order to test ordering.\n  a: specs.Array\n  b: specs.Array",
  "class RunningStatisticsTest(absltest.TestCase):\n\n  def setUp(self):\n    super().setUp()\n    jax_config.update('jax_enable_x64', False)\n\n  def assert_allclose(self,\n                      actual: jnp.ndarray,\n                      desired: jnp.ndarray,\n                      err_msg: str = '') -> None:\n    np.testing.assert_allclose(\n        actual, desired, atol=1e-5, rtol=1e-5, err_msg=err_msg)\n\n  def test_normalize(self):\n    state = running_statistics.init_state(specs.Array((5,), jnp.float32))\n\n    x = jnp.arange(200, dtype=jnp.float32).reshape(20, 2, 5)\n    x1, x2, x3, x4 = jnp.split(x, 4, axis=0)\n\n    state = update_and_validate(state, x1)\n    state = update_and_validate(state, x2)\n    state = update_and_validate(state, x3)\n    state = update_and_validate(state, x4)\n    normalized = running_statistics.normalize(x, state)\n\n    mean = jnp.mean(normalized)\n    std = jnp.std(normalized)\n    self.assert_allclose(mean, jnp.zeros_like(mean))\n    self.assert_allclose(std, jnp.ones_like(std))\n\n  def test_init_normalize(self):\n    state = running_statistics.init_state(specs.Array((5,), jnp.float32))\n\n    x = jnp.arange(200, dtype=jnp.float32).reshape(20, 2, 5)\n    normalized = running_statistics.normalize(x, state)\n\n    self.assert_allclose(normalized, x)\n\n  def test_one_batch_dim(self):\n    state = running_statistics.init_state(specs.Array((5,), jnp.float32))\n\n    x = jnp.arange(10, dtype=jnp.float32).reshape(2, 5)\n\n    state = update_and_validate(state, x)\n    normalized = running_statistics.normalize(x, state)\n\n    mean = jnp.mean(normalized, axis=0)\n    std = jnp.std(normalized, axis=0)\n    self.assert_allclose(mean, jnp.zeros_like(mean))\n    self.assert_allclose(std, jnp.ones_like(std))\n\n  def test_clip(self):\n    state = running_statistics.init_state(specs.Array((), jnp.float32))\n\n    x = jnp.arange(5, dtype=jnp.float32)\n\n    state = update_and_validate(state, x)\n    normalized = running_statistics.normalize(x, state, max_abs_value=1.0)\n\n    mean = jnp.mean(normalized)\n    std = jnp.std(normalized)\n    self.assert_allclose(mean, jnp.zeros_like(mean))\n    self.assert_allclose(std, jnp.ones_like(std) * math.sqrt(0.6))\n\n  def test_nested_normalize(self):\n    state = running_statistics.init_state({\n        'a': specs.Array((5,), jnp.float32),\n        'b': specs.Array((2,), jnp.float32)\n    })\n\n    x1 = {\n        'a': jnp.arange(20, dtype=jnp.float32).reshape(2, 2, 5),\n        'b': jnp.arange(8, dtype=jnp.float32).reshape(2, 2, 2)\n    }\n    x2 = {\n        'a': jnp.arange(20, dtype=jnp.float32).reshape(2, 2, 5) + 20,\n        'b': jnp.arange(8, dtype=jnp.float32).reshape(2, 2, 2) + 8\n    }\n    x3 = {\n        'a': jnp.arange(40, dtype=jnp.float32).reshape(4, 2, 5),\n        'b': jnp.arange(16, dtype=jnp.float32).reshape(4, 2, 2)\n    }\n\n    state = update_and_validate(state, x1)\n    state = update_and_validate(state, x2)\n    state = update_and_validate(state, x3)\n    normalized = running_statistics.normalize(x3, state)\n\n    mean = tree.map_structure(lambda x: jnp.mean(x, axis=(0, 1)), normalized)\n    std = tree.map_structure(lambda x: jnp.std(x, axis=(0, 1)), normalized)\n    tree.map_structure(\n        lambda x: self.assert_allclose(x, jnp.zeros_like(x)),\n        mean)\n    tree.map_structure(\n        lambda x: self.assert_allclose(x, jnp.ones_like(x)),\n        std)\n\n  def test_validation(self):\n    state = running_statistics.init_state(specs.Array((1, 2, 3), jnp.float32))\n\n    x = jnp.arange(12, dtype=jnp.float32).reshape(2, 2, 3)\n    with self.assertRaises(AssertionError):\n      update_and_validate(state, x)\n\n    x = jnp.arange(3, dtype=jnp.float32).reshape(1, 1, 3)\n    with self.assertRaises(AssertionError):\n      update_and_validate(state, x)\n\n  def test_int_not_normalized(self):\n    state = running_statistics.init_state(specs.Array((), jnp.int32))\n\n    x = jnp.arange(5, dtype=jnp.int32)\n\n    state = update_and_validate(state, x)\n    normalized = running_statistics.normalize(x, state)\n\n    np.testing.assert_array_equal(normalized, x)\n\n  def test_pmap_update_nested(self):\n    local_device_count = jax.local_device_count()\n    state = running_statistics.init_state({\n        'a': specs.Array((5,), jnp.float32),\n        'b': specs.Array((2,), jnp.float32)\n    })\n\n    x = {\n        'a': (jnp.arange(15 * local_device_count,\n                         dtype=jnp.float32)).reshape(local_device_count, 3, 5),\n        'b': (jnp.arange(6 * local_device_count,\n                         dtype=jnp.float32)).reshape(local_device_count, 3, 2),\n    }\n\n    devices = jax.local_devices()\n    state = jax.device_put_replicated(state, devices)\n    pmap_axis_name = 'i'\n    state = jax.pmap(\n        functools.partial(update_and_validate, pmap_axis_name=pmap_axis_name),\n        pmap_axis_name)(state, x)\n    state = jax.pmap(\n        functools.partial(update_and_validate, pmap_axis_name=pmap_axis_name),\n        pmap_axis_name)(state, x)\n    normalized = jax.pmap(running_statistics.normalize)(x, state)\n\n    mean = tree.map_structure(lambda x: jnp.mean(x, axis=(0, 1)), normalized)\n    std = tree.map_structure(lambda x: jnp.std(x, axis=(0, 1)), normalized)\n    tree.map_structure(\n        lambda x: self.assert_allclose(x, jnp.zeros_like(x)), mean)\n    tree.map_structure(\n        lambda x: self.assert_allclose(x, jnp.ones_like(x)), std)\n\n  def test_different_structure_normalize(self):\n    spec = TestNestedSpec(\n        a=specs.Array((5,), jnp.float32), b=specs.Array((2,), jnp.float32))\n    state = running_statistics.init_state(spec)\n\n    x = {\n        'a': jnp.arange(20, dtype=jnp.float32).reshape(2, 2, 5),\n        'b': jnp.arange(8, dtype=jnp.float32).reshape(2, 2, 2)\n    }\n\n    with self.assertRaises(TypeError):\n      state = update_and_validate(state, x)\n\n  def test_weights(self):\n    state = running_statistics.init_state(specs.Array((), jnp.float32))\n\n    x = jnp.arange(5, dtype=jnp.float32)\n    x_weights = jnp.ones_like(x)\n    y = 2 * x + 5\n    y_weights = 2 * x_weights\n    z = jnp.concatenate([x, y])\n    weights = jnp.concatenate([x_weights, y_weights])\n\n    state = update_and_validate(state, z, weights=weights)\n\n    self.assertEqual(state.mean, (jnp.mean(x) + 2 * jnp.mean(y)) / 3)\n    big_z = jnp.concatenate([x, y, y])\n    normalized = running_statistics.normalize(big_z, state)\n    self.assertAlmostEqual(jnp.mean(normalized), 0., places=6)\n    self.assertAlmostEqual(jnp.std(normalized), 1., places=6)\n\n  def test_normalize_config(self):\n    x = jnp.arange(200, dtype=jnp.float32).reshape(20, 2, 5)\n    x_split = jnp.split(x, 5, axis=0)\n\n    y = jnp.arange(160, dtype=jnp.float32).reshape(20, 2, 4)\n    y_split = jnp.split(y, 5, axis=0)\n\n    z = {'a': x, 'b': y}\n\n    z_split = [{'a': xx, 'b': yy} for xx, yy in zip(x_split, y_split)]\n\n    update = jax.jit(running_statistics.update, static_argnames=('config',))\n\n    config = running_statistics.NestStatisticsConfig((('a',),))\n    state = running_statistics.init_state({\n        'a': specs.Array((5,), jnp.float32),\n        'b': specs.Array((4,), jnp.float32)\n    })\n    # Test initialization from the first element.\n    state = update(state, z_split[0], config=config)\n    state = update(state, z_split[1], config=config)\n    state = update(state, z_split[2], config=config)\n    state = update(state, z_split[3], config=config)\n    state = update(state, z_split[4], config=config)\n\n    normalize = jax.jit(running_statistics.normalize)\n    normalized = normalize(z, state)\n\n    for key in normalized:\n      mean = jnp.mean(normalized[key], axis=(0, 1))\n      std = jnp.std(normalized[key], axis=(0, 1))\n      if key == 'a':\n        self.assert_allclose(\n            mean,\n            jnp.zeros_like(mean),\n            err_msg=f'key:{key} mean:{mean} normalized:{normalized[key]}')\n        self.assert_allclose(\n            std,\n            jnp.ones_like(std),\n            err_msg=f'key:{key} std:{std} normalized:{normalized[key]}')\n      else:\n        assert key == 'b'\n        np.testing.assert_array_equal(\n            normalized[key],\n            z[key],\n            err_msg=f'z:{z[key]} normalized:{normalized[key]}')\n\n  def test_clip_config(self):\n    x = jnp.arange(10, dtype=jnp.float32) - 5\n    y = jnp.arange(8, dtype=jnp.float32) - 4\n\n    z = {'x': x, 'y': y}\n\n    max_abs_x = 2\n    config = running_statistics.NestClippingConfig(((('x',), max_abs_x),))\n\n    clipped_z = running_statistics.clip(z, config)\n\n    clipped_x = jnp.clip(a=x, a_min=-max_abs_x, a_max=max_abs_x)\n    np.testing.assert_array_equal(clipped_z['x'], clipped_x)\n\n    np.testing.assert_array_equal(clipped_z['y'], z['y'])\n\n  def test_denormalize(self):\n    state = running_statistics.init_state(specs.Array((5,), jnp.float32))\n\n    x = jnp.arange(100, dtype=jnp.float32).reshape(10, 2, 5)\n    x1, x2 = jnp.split(x, 2, axis=0)\n\n    state = update_and_validate(state, x1)\n    state = update_and_validate(state, x2)\n    normalized = running_statistics.normalize(x, state)\n\n    mean = jnp.mean(normalized)\n    std = jnp.std(normalized)\n    self.assert_allclose(mean, jnp.zeros_like(mean))\n    self.assert_allclose(std, jnp.ones_like(std))\n\n    denormalized = running_statistics.denormalize(normalized, state)\n    self.assert_allclose(denormalized, x)",
  "def setUp(self):\n    super().setUp()\n    jax_config.update('jax_enable_x64', False)",
  "def assert_allclose(self,\n                      actual: jnp.ndarray,\n                      desired: jnp.ndarray,\n                      err_msg: str = '') -> None:\n    np.testing.assert_allclose(\n        actual, desired, atol=1e-5, rtol=1e-5, err_msg=err_msg)",
  "def test_normalize(self):\n    state = running_statistics.init_state(specs.Array((5,), jnp.float32))\n\n    x = jnp.arange(200, dtype=jnp.float32).reshape(20, 2, 5)\n    x1, x2, x3, x4 = jnp.split(x, 4, axis=0)\n\n    state = update_and_validate(state, x1)\n    state = update_and_validate(state, x2)\n    state = update_and_validate(state, x3)\n    state = update_and_validate(state, x4)\n    normalized = running_statistics.normalize(x, state)\n\n    mean = jnp.mean(normalized)\n    std = jnp.std(normalized)\n    self.assert_allclose(mean, jnp.zeros_like(mean))\n    self.assert_allclose(std, jnp.ones_like(std))",
  "def test_init_normalize(self):\n    state = running_statistics.init_state(specs.Array((5,), jnp.float32))\n\n    x = jnp.arange(200, dtype=jnp.float32).reshape(20, 2, 5)\n    normalized = running_statistics.normalize(x, state)\n\n    self.assert_allclose(normalized, x)",
  "def test_one_batch_dim(self):\n    state = running_statistics.init_state(specs.Array((5,), jnp.float32))\n\n    x = jnp.arange(10, dtype=jnp.float32).reshape(2, 5)\n\n    state = update_and_validate(state, x)\n    normalized = running_statistics.normalize(x, state)\n\n    mean = jnp.mean(normalized, axis=0)\n    std = jnp.std(normalized, axis=0)\n    self.assert_allclose(mean, jnp.zeros_like(mean))\n    self.assert_allclose(std, jnp.ones_like(std))",
  "def test_clip(self):\n    state = running_statistics.init_state(specs.Array((), jnp.float32))\n\n    x = jnp.arange(5, dtype=jnp.float32)\n\n    state = update_and_validate(state, x)\n    normalized = running_statistics.normalize(x, state, max_abs_value=1.0)\n\n    mean = jnp.mean(normalized)\n    std = jnp.std(normalized)\n    self.assert_allclose(mean, jnp.zeros_like(mean))\n    self.assert_allclose(std, jnp.ones_like(std) * math.sqrt(0.6))",
  "def test_nested_normalize(self):\n    state = running_statistics.init_state({\n        'a': specs.Array((5,), jnp.float32),\n        'b': specs.Array((2,), jnp.float32)\n    })\n\n    x1 = {\n        'a': jnp.arange(20, dtype=jnp.float32).reshape(2, 2, 5),\n        'b': jnp.arange(8, dtype=jnp.float32).reshape(2, 2, 2)\n    }\n    x2 = {\n        'a': jnp.arange(20, dtype=jnp.float32).reshape(2, 2, 5) + 20,\n        'b': jnp.arange(8, dtype=jnp.float32).reshape(2, 2, 2) + 8\n    }\n    x3 = {\n        'a': jnp.arange(40, dtype=jnp.float32).reshape(4, 2, 5),\n        'b': jnp.arange(16, dtype=jnp.float32).reshape(4, 2, 2)\n    }\n\n    state = update_and_validate(state, x1)\n    state = update_and_validate(state, x2)\n    state = update_and_validate(state, x3)\n    normalized = running_statistics.normalize(x3, state)\n\n    mean = tree.map_structure(lambda x: jnp.mean(x, axis=(0, 1)), normalized)\n    std = tree.map_structure(lambda x: jnp.std(x, axis=(0, 1)), normalized)\n    tree.map_structure(\n        lambda x: self.assert_allclose(x, jnp.zeros_like(x)),\n        mean)\n    tree.map_structure(\n        lambda x: self.assert_allclose(x, jnp.ones_like(x)),\n        std)",
  "def test_validation(self):\n    state = running_statistics.init_state(specs.Array((1, 2, 3), jnp.float32))\n\n    x = jnp.arange(12, dtype=jnp.float32).reshape(2, 2, 3)\n    with self.assertRaises(AssertionError):\n      update_and_validate(state, x)\n\n    x = jnp.arange(3, dtype=jnp.float32).reshape(1, 1, 3)\n    with self.assertRaises(AssertionError):\n      update_and_validate(state, x)",
  "def test_int_not_normalized(self):\n    state = running_statistics.init_state(specs.Array((), jnp.int32))\n\n    x = jnp.arange(5, dtype=jnp.int32)\n\n    state = update_and_validate(state, x)\n    normalized = running_statistics.normalize(x, state)\n\n    np.testing.assert_array_equal(normalized, x)",
  "def test_pmap_update_nested(self):\n    local_device_count = jax.local_device_count()\n    state = running_statistics.init_state({\n        'a': specs.Array((5,), jnp.float32),\n        'b': specs.Array((2,), jnp.float32)\n    })\n\n    x = {\n        'a': (jnp.arange(15 * local_device_count,\n                         dtype=jnp.float32)).reshape(local_device_count, 3, 5),\n        'b': (jnp.arange(6 * local_device_count,\n                         dtype=jnp.float32)).reshape(local_device_count, 3, 2),\n    }\n\n    devices = jax.local_devices()\n    state = jax.device_put_replicated(state, devices)\n    pmap_axis_name = 'i'\n    state = jax.pmap(\n        functools.partial(update_and_validate, pmap_axis_name=pmap_axis_name),\n        pmap_axis_name)(state, x)\n    state = jax.pmap(\n        functools.partial(update_and_validate, pmap_axis_name=pmap_axis_name),\n        pmap_axis_name)(state, x)\n    normalized = jax.pmap(running_statistics.normalize)(x, state)\n\n    mean = tree.map_structure(lambda x: jnp.mean(x, axis=(0, 1)), normalized)\n    std = tree.map_structure(lambda x: jnp.std(x, axis=(0, 1)), normalized)\n    tree.map_structure(\n        lambda x: self.assert_allclose(x, jnp.zeros_like(x)), mean)\n    tree.map_structure(\n        lambda x: self.assert_allclose(x, jnp.ones_like(x)), std)",
  "def test_different_structure_normalize(self):\n    spec = TestNestedSpec(\n        a=specs.Array((5,), jnp.float32), b=specs.Array((2,), jnp.float32))\n    state = running_statistics.init_state(spec)\n\n    x = {\n        'a': jnp.arange(20, dtype=jnp.float32).reshape(2, 2, 5),\n        'b': jnp.arange(8, dtype=jnp.float32).reshape(2, 2, 2)\n    }\n\n    with self.assertRaises(TypeError):\n      state = update_and_validate(state, x)",
  "def test_weights(self):\n    state = running_statistics.init_state(specs.Array((), jnp.float32))\n\n    x = jnp.arange(5, dtype=jnp.float32)\n    x_weights = jnp.ones_like(x)\n    y = 2 * x + 5\n    y_weights = 2 * x_weights\n    z = jnp.concatenate([x, y])\n    weights = jnp.concatenate([x_weights, y_weights])\n\n    state = update_and_validate(state, z, weights=weights)\n\n    self.assertEqual(state.mean, (jnp.mean(x) + 2 * jnp.mean(y)) / 3)\n    big_z = jnp.concatenate([x, y, y])\n    normalized = running_statistics.normalize(big_z, state)\n    self.assertAlmostEqual(jnp.mean(normalized), 0., places=6)\n    self.assertAlmostEqual(jnp.std(normalized), 1., places=6)",
  "def test_normalize_config(self):\n    x = jnp.arange(200, dtype=jnp.float32).reshape(20, 2, 5)\n    x_split = jnp.split(x, 5, axis=0)\n\n    y = jnp.arange(160, dtype=jnp.float32).reshape(20, 2, 4)\n    y_split = jnp.split(y, 5, axis=0)\n\n    z = {'a': x, 'b': y}\n\n    z_split = [{'a': xx, 'b': yy} for xx, yy in zip(x_split, y_split)]\n\n    update = jax.jit(running_statistics.update, static_argnames=('config',))\n\n    config = running_statistics.NestStatisticsConfig((('a',),))\n    state = running_statistics.init_state({\n        'a': specs.Array((5,), jnp.float32),\n        'b': specs.Array((4,), jnp.float32)\n    })\n    # Test initialization from the first element.\n    state = update(state, z_split[0], config=config)\n    state = update(state, z_split[1], config=config)\n    state = update(state, z_split[2], config=config)\n    state = update(state, z_split[3], config=config)\n    state = update(state, z_split[4], config=config)\n\n    normalize = jax.jit(running_statistics.normalize)\n    normalized = normalize(z, state)\n\n    for key in normalized:\n      mean = jnp.mean(normalized[key], axis=(0, 1))\n      std = jnp.std(normalized[key], axis=(0, 1))\n      if key == 'a':\n        self.assert_allclose(\n            mean,\n            jnp.zeros_like(mean),\n            err_msg=f'key:{key} mean:{mean} normalized:{normalized[key]}')\n        self.assert_allclose(\n            std,\n            jnp.ones_like(std),\n            err_msg=f'key:{key} std:{std} normalized:{normalized[key]}')\n      else:\n        assert key == 'b'\n        np.testing.assert_array_equal(\n            normalized[key],\n            z[key],\n            err_msg=f'z:{z[key]} normalized:{normalized[key]}')",
  "def test_clip_config(self):\n    x = jnp.arange(10, dtype=jnp.float32) - 5\n    y = jnp.arange(8, dtype=jnp.float32) - 4\n\n    z = {'x': x, 'y': y}\n\n    max_abs_x = 2\n    config = running_statistics.NestClippingConfig(((('x',), max_abs_x),))\n\n    clipped_z = running_statistics.clip(z, config)\n\n    clipped_x = jnp.clip(a=x, a_min=-max_abs_x, a_max=max_abs_x)\n    np.testing.assert_array_equal(clipped_z['x'], clipped_x)\n\n    np.testing.assert_array_equal(clipped_z['y'], z['y'])",
  "def test_denormalize(self):\n    state = running_statistics.init_state(specs.Array((5,), jnp.float32))\n\n    x = jnp.arange(100, dtype=jnp.float32).reshape(10, 2, 5)\n    x1, x2 = jnp.split(x, 2, axis=0)\n\n    state = update_and_validate(state, x1)\n    state = update_and_validate(state, x2)\n    normalized = running_statistics.normalize(x, state)\n\n    mean = jnp.mean(normalized)\n    std = jnp.std(normalized)\n    self.assert_allclose(mean, jnp.zeros_like(mean))\n    self.assert_allclose(std, jnp.ones_like(std))\n\n    denormalized = running_statistics.denormalize(normalized, state)\n    self.assert_allclose(denormalized, x)",
  "class VariableReference(NamedTuple):\n  variable_name: str",
  "class ReferenceVariableSource(core.VariableSource):\n  \"\"\"Variable source which returns references instead of values.\n\n  This is passed to each actor when using a centralized inference server. The\n  actor uses this special variable source to get references rather than values.\n  These references are then passed to calls to the inference server, which will\n  dereference them to obtain the value of the corresponding variables at\n  inference time. This avoids passing around copies of variables from each\n  actor to the inference server.\n  \"\"\"\n\n  def get_variables(self, names: Sequence[str]) -> List[VariableReference]:\n    return [VariableReference(name) for name in names]",
  "class VariableClient:\n  \"\"\"A variable client for updating variables from a remote source.\"\"\"\n\n  def __init__(\n      self,\n      client: core.VariableSource,\n      key: Union[str, Sequence[str]],\n      update_period: Union[int, datetime.timedelta] = 1,\n      device: Optional[Union[str, jax.Device]] = None,\n  ):\n    \"\"\"Initializes the variable client.\n\n    Args:\n      client: A variable source from which we fetch variables.\n      key: Which variables to request. When multiple keys are used, params\n        property will return a list of params.\n      update_period: Interval between fetches, specified as either (int) a\n        number of calls to update() between actual fetches or (timedelta) a time\n        interval that has to pass since the last fetch.\n      device: The name of a JAX device to put variables on. If None (default),\n        VariableClient won't put params on any device.\n    \"\"\"\n    self._update_period = update_period\n    self._call_counter = 0\n    self._last_call = time.time()\n    self._client = client\n    self._params: Sequence[network_types.Params] = None\n\n    self._device = device\n    if isinstance(self._device, str):\n      self._device = jax.devices(device)[0]\n\n    self._executor = futures.ThreadPoolExecutor(max_workers=1)\n\n    if isinstance(key, str):\n      key = [key]\n\n    self._key = key\n    self._request = lambda k=key: client.get_variables(k)\n    self._future: Optional[futures.Future] = None  # pylint: disable=g-bare-generic\n    self._async_request = lambda: self._executor.submit(self._request)\n\n  def update(self, wait: bool = False) -> None:\n    \"\"\"Periodically updates the variables with the latest copy from the source.\n\n    If wait is True, a blocking request is executed. Any active request will be\n    cancelled.\n    If wait is False, this method makes an asynchronous request for variables.\n\n    Args:\n      wait: Whether to execute asynchronous (False) or blocking updates (True).\n        Defaults to False.\n    \"\"\"\n    # Track calls (we only update periodically).\n    self._call_counter += 1\n\n    # Return if it's not time to fetch another update.\n    if isinstance(self._update_period, datetime.timedelta):\n      if self._update_period.total_seconds() + self._last_call > time.time():\n        return\n    else:\n      if self._call_counter < self._update_period:\n        return\n\n    if wait:\n      if self._future is not None:\n        if self._future.running():\n          self._future.cancel()\n        self._future = None\n      self._call_counter = 0\n      self._last_call = time.time()\n      self.update_and_wait()\n      return\n\n    # Return early if we are still waiting for a previous request to come back.\n    if self._future and not self._future.done():\n      return\n\n    # Get a future and add the copy function as a callback.\n    self._call_counter = 0\n    self._last_call = time.time()\n    self._future = self._async_request()\n    self._future.add_done_callback(lambda f: self._callback(f.result()))\n\n  def update_and_wait(self):\n    \"\"\"Immediately update and block until we get the result.\"\"\"\n    self._callback(self._request())\n\n  def _callback(self, params_list: List[network_types.Params]):\n    if self._device and not isinstance(self._client, ReferenceVariableSource):\n      # Move variables to a proper device.\n      self._params = jax.device_put(params_list, self._device)\n    else:\n      self._params = params_list\n\n  @property\n  def device(self) -> Optional[jax.Device]:\n    return self._device\n\n  @property\n  def params(self) -> Union[network_types.Params, List[network_types.Params]]:\n    \"\"\"Returns the first params for one key, otherwise the whole params list.\"\"\"\n    if self._params is None:\n      self.update_and_wait()\n\n    if len(self._params) == 1:\n      return self._params[0]\n    else:\n      return self._params",
  "def get_variables(self, names: Sequence[str]) -> List[VariableReference]:\n    return [VariableReference(name) for name in names]",
  "def __init__(\n      self,\n      client: core.VariableSource,\n      key: Union[str, Sequence[str]],\n      update_period: Union[int, datetime.timedelta] = 1,\n      device: Optional[Union[str, jax.Device]] = None,\n  ):\n    \"\"\"Initializes the variable client.\n\n    Args:\n      client: A variable source from which we fetch variables.\n      key: Which variables to request. When multiple keys are used, params\n        property will return a list of params.\n      update_period: Interval between fetches, specified as either (int) a\n        number of calls to update() between actual fetches or (timedelta) a time\n        interval that has to pass since the last fetch.\n      device: The name of a JAX device to put variables on. If None (default),\n        VariableClient won't put params on any device.\n    \"\"\"\n    self._update_period = update_period\n    self._call_counter = 0\n    self._last_call = time.time()\n    self._client = client\n    self._params: Sequence[network_types.Params] = None\n\n    self._device = device\n    if isinstance(self._device, str):\n      self._device = jax.devices(device)[0]\n\n    self._executor = futures.ThreadPoolExecutor(max_workers=1)\n\n    if isinstance(key, str):\n      key = [key]\n\n    self._key = key\n    self._request = lambda k=key: client.get_variables(k)\n    self._future: Optional[futures.Future] = None  # pylint: disable=g-bare-generic\n    self._async_request = lambda: self._executor.submit(self._request)",
  "def update(self, wait: bool = False) -> None:\n    \"\"\"Periodically updates the variables with the latest copy from the source.\n\n    If wait is True, a blocking request is executed. Any active request will be\n    cancelled.\n    If wait is False, this method makes an asynchronous request for variables.\n\n    Args:\n      wait: Whether to execute asynchronous (False) or blocking updates (True).\n        Defaults to False.\n    \"\"\"\n    # Track calls (we only update periodically).\n    self._call_counter += 1\n\n    # Return if it's not time to fetch another update.\n    if isinstance(self._update_period, datetime.timedelta):\n      if self._update_period.total_seconds() + self._last_call > time.time():\n        return\n    else:\n      if self._call_counter < self._update_period:\n        return\n\n    if wait:\n      if self._future is not None:\n        if self._future.running():\n          self._future.cancel()\n        self._future = None\n      self._call_counter = 0\n      self._last_call = time.time()\n      self.update_and_wait()\n      return\n\n    # Return early if we are still waiting for a previous request to come back.\n    if self._future and not self._future.done():\n      return\n\n    # Get a future and add the copy function as a callback.\n    self._call_counter = 0\n    self._last_call = time.time()\n    self._future = self._async_request()\n    self._future.add_done_callback(lambda f: self._callback(f.result()))",
  "def update_and_wait(self):\n    \"\"\"Immediately update and block until we get the result.\"\"\"\n    self._callback(self._request())",
  "def _callback(self, params_list: List[network_types.Params]):\n    if self._device and not isinstance(self._client, ReferenceVariableSource):\n      # Move variables to a proper device.\n      self._params = jax.device_put(params_list, self._device)\n    else:\n      self._params = params_list",
  "def device(self) -> Optional[jax.Device]:\n    return self._device",
  "def params(self) -> Union[network_types.Params, List[network_types.Params]]:\n    \"\"\"Returns the first params for one key, otherwise the whole params list.\"\"\"\n    if self._params is None:\n      self.update_and_wait()\n\n    if len(self._params) == 1:\n      return self._params[0]\n    else:\n      return self._params",
  "class JaxUtilsTest(absltest.TestCase):\n\n  def test_batch_concat(self):\n    batch_size = 32\n    inputs = [\n        jnp.zeros(shape=(batch_size, 2)),\n        {\n            'foo': jnp.zeros(shape=(batch_size, 5, 3))\n        },\n        [jnp.zeros(shape=(batch_size, 1))],\n        jnp.zeros(shape=(batch_size,)),\n    ]\n\n    output_shape = utils.batch_concat(inputs).shape\n    expected_shape = [batch_size, 2 + 5 * 3 + 1 + 1]\n    self.assertSequenceEqual(output_shape, expected_shape)\n\n  def test_mapreduce(self):\n\n    @utils.mapreduce\n    def f(y, x):\n      return jnp.square(x + y)\n\n    z = f(jnp.ones(shape=(32,)), jnp.ones(shape=(32,)))\n    z = jax.device_get(z)\n    self.assertEqual(z, 4)\n\n  def test_get_from_first_device(self):\n    sharded = {\n        'a':\n            jax.device_put_sharded(\n                list(jnp.arange(16).reshape([jax.local_device_count(), 4])),\n                jax.local_devices()),\n        'b':\n            jax.device_put_sharded(\n                list(jnp.arange(8).reshape([jax.local_device_count(), 2])),\n                jax.local_devices(),\n            ),\n    }\n\n    want = {\n        'a': jnp.arange(4),\n        'b': jnp.arange(2),\n    }\n\n    # Get zeroth device content as DeviceArray.\n    device_arrays = utils.get_from_first_device(sharded, as_numpy=False)\n    jax.tree_map(\n        lambda x: self.assertIsInstance(x, jax.Array),\n        device_arrays)\n    jax.tree_map(np.testing.assert_array_equal, want, device_arrays)\n\n    # Get the zeroth device content as numpy arrays.\n    numpy_arrays = utils.get_from_first_device(sharded, as_numpy=True)\n    jax.tree_map(lambda x: self.assertIsInstance(x, np.ndarray), numpy_arrays)\n    jax.tree_map(np.testing.assert_array_equal, want, numpy_arrays)",
  "def test_batch_concat(self):\n    batch_size = 32\n    inputs = [\n        jnp.zeros(shape=(batch_size, 2)),\n        {\n            'foo': jnp.zeros(shape=(batch_size, 5, 3))\n        },\n        [jnp.zeros(shape=(batch_size, 1))],\n        jnp.zeros(shape=(batch_size,)),\n    ]\n\n    output_shape = utils.batch_concat(inputs).shape\n    expected_shape = [batch_size, 2 + 5 * 3 + 1 + 1]\n    self.assertSequenceEqual(output_shape, expected_shape)",
  "def test_mapreduce(self):\n\n    @utils.mapreduce\n    def f(y, x):\n      return jnp.square(x + y)\n\n    z = f(jnp.ones(shape=(32,)), jnp.ones(shape=(32,)))\n    z = jax.device_get(z)\n    self.assertEqual(z, 4)",
  "def test_get_from_first_device(self):\n    sharded = {\n        'a':\n            jax.device_put_sharded(\n                list(jnp.arange(16).reshape([jax.local_device_count(), 4])),\n                jax.local_devices()),\n        'b':\n            jax.device_put_sharded(\n                list(jnp.arange(8).reshape([jax.local_device_count(), 2])),\n                jax.local_devices(),\n            ),\n    }\n\n    want = {\n        'a': jnp.arange(4),\n        'b': jnp.arange(2),\n    }\n\n    # Get zeroth device content as DeviceArray.\n    device_arrays = utils.get_from_first_device(sharded, as_numpy=False)\n    jax.tree_map(\n        lambda x: self.assertIsInstance(x, jax.Array),\n        device_arrays)\n    jax.tree_map(np.testing.assert_array_equal, want, device_arrays)\n\n    # Get the zeroth device content as numpy arrays.\n    numpy_arrays = utils.get_from_first_device(sharded, as_numpy=True)\n    jax.tree_map(lambda x: self.assertIsInstance(x, np.ndarray), numpy_arrays)\n    jax.tree_map(np.testing.assert_array_equal, want, numpy_arrays)",
  "def f(y, x):\n      return jnp.square(x + y)",
  "class InferenceServerConfig:\n  \"\"\"Configuration options for centralised inference.\n\n  Attributes:\n    batch_size: How many elements to batch together per single inference call.\n        Auto-computed when not specified.\n    update_period: Frequency of updating variables from the variable source.\n        It is passed to VariableClient. Auto-computed when not specified.\n    timeout: Time after which incomplete batch is executed (batch is padded,\n        so there batch handler is always called with batch_size elements).\n        By default timeout is effectively disabled (set to 30 days).\n  \"\"\"\n  batch_size: Optional[int] = None\n  update_period: Optional[int] = None\n  timeout: datetime.timedelta = datetime.timedelta(days=30)",
  "class InferenceServer(Generic[InferenceServerHandler]):\n  \"\"\"Centralised, batched inference server.\"\"\"\n\n  def __init__(\n      self,\n      handler: InferenceServerHandler,\n      variable_source: acme.VariableSource,\n      devices: Sequence[jax.Device],\n      config: InferenceServerConfig,\n  ):\n    \"\"\"Constructs an inference server object.\n\n    Args:\n      handler: A callable or a mapping of callables to be exposed\n        through the inference server.\n      variable_source: Source of variables\n      devices: Devices used for executing handlers. All devices are used in\n        parallel.\n      config: Inference Server configuration.\n    \"\"\"\n    self._variable_source = variable_source\n    self._variable_client = None\n    self._keys = []\n    self._devices = devices\n    self._config = config\n    self._call_cnt = 0\n    self._device_params = [None] * len(self._devices)\n    self._device_params_ids = [None] * len(self._devices)\n    self._mutex = threading.Lock()\n    self._handler = jax.tree_map(self._build_handler, handler, is_leaf=callable)\n\n  @property\n  def handler(self) -> InferenceServerHandler:\n    return self._handler\n\n  def _dereference_params(self, arg):\n    \"\"\"Replaces VariableReferences with their corresponding param values.\"\"\"\n\n    if not isinstance(arg, variable_utils.VariableReference):\n      # All arguments but VariableReference are returned without modifications.\n      return arg\n\n    # Due to batching dimension we take the first element.\n    variable_name = arg.variable_name[0]\n\n    if variable_name not in self._keys:\n      # Create a new VariableClient which also serves new variables.\n      self._keys.append(variable_name)\n      self._variable_client = variable_utils.VariableClient(\n          client=self._variable_source,\n          key=self._keys,\n          update_period=self._config.update_period)\n\n    params = self._variable_client.params\n    device_idx = self._call_cnt % len(self._devices)\n    # Select device via round robin, and update its params if they changed.\n    if self._device_params_ids[device_idx] != id(params):\n      self._device_params_ids[device_idx] = id(params)\n      self._device_params[device_idx] = jax.device_put(\n          params, self._devices[device_idx])\n\n    # Return the params that are located on the chosen device.\n    device_params = self._device_params[device_idx]\n    if len(self._keys) == 1:\n      return device_params\n    return device_params[self._keys.index(variable_name)]\n\n  def _build_handler(self, handler: Callable[..., Any]) -> Callable[..., Any]:\n    \"\"\"Builds a batched handler for a given callable handler and its name.\"\"\"\n\n    def dereference_params_and_call_handler(*args, **kwargs):\n      with self._mutex:\n        # Dereference args corresponding to params, leaving others unchanged.\n        args_with_dereferenced_params = [\n            self._dereference_params(arg) for arg in args\n        ]\n        kwargs_with_dereferenced_params = {\n            key: self._dereference_params(value)\n            for key, value in kwargs.items()\n        }\n        self._call_cnt += 1\n\n        # Maybe update params, depending on client configuration.\n        if self._variable_client is not None:\n          self._variable_client.update()\n\n      return handler(*args_with_dereferenced_params,\n                     **kwargs_with_dereferenced_params)\n\n    return lp.batched_handler(\n        batch_size=self._config.batch_size,\n        timeout=self._config.timeout,\n        pad_batch=True,\n        max_parallelism=2 * len(self._devices))(\n            dereference_params_and_call_handler)",
  "def __init__(\n      self,\n      handler: InferenceServerHandler,\n      variable_source: acme.VariableSource,\n      devices: Sequence[jax.Device],\n      config: InferenceServerConfig,\n  ):\n    \"\"\"Constructs an inference server object.\n\n    Args:\n      handler: A callable or a mapping of callables to be exposed\n        through the inference server.\n      variable_source: Source of variables\n      devices: Devices used for executing handlers. All devices are used in\n        parallel.\n      config: Inference Server configuration.\n    \"\"\"\n    self._variable_source = variable_source\n    self._variable_client = None\n    self._keys = []\n    self._devices = devices\n    self._config = config\n    self._call_cnt = 0\n    self._device_params = [None] * len(self._devices)\n    self._device_params_ids = [None] * len(self._devices)\n    self._mutex = threading.Lock()\n    self._handler = jax.tree_map(self._build_handler, handler, is_leaf=callable)",
  "def handler(self) -> InferenceServerHandler:\n    return self._handler",
  "def _dereference_params(self, arg):\n    \"\"\"Replaces VariableReferences with their corresponding param values.\"\"\"\n\n    if not isinstance(arg, variable_utils.VariableReference):\n      # All arguments but VariableReference are returned without modifications.\n      return arg\n\n    # Due to batching dimension we take the first element.\n    variable_name = arg.variable_name[0]\n\n    if variable_name not in self._keys:\n      # Create a new VariableClient which also serves new variables.\n      self._keys.append(variable_name)\n      self._variable_client = variable_utils.VariableClient(\n          client=self._variable_source,\n          key=self._keys,\n          update_period=self._config.update_period)\n\n    params = self._variable_client.params\n    device_idx = self._call_cnt % len(self._devices)\n    # Select device via round robin, and update its params if they changed.\n    if self._device_params_ids[device_idx] != id(params):\n      self._device_params_ids[device_idx] = id(params)\n      self._device_params[device_idx] = jax.device_put(\n          params, self._devices[device_idx])\n\n    # Return the params that are located on the chosen device.\n    device_params = self._device_params[device_idx]\n    if len(self._keys) == 1:\n      return device_params\n    return device_params[self._keys.index(variable_name)]",
  "def _build_handler(self, handler: Callable[..., Any]) -> Callable[..., Any]:\n    \"\"\"Builds a batched handler for a given callable handler and its name.\"\"\"\n\n    def dereference_params_and_call_handler(*args, **kwargs):\n      with self._mutex:\n        # Dereference args corresponding to params, leaving others unchanged.\n        args_with_dereferenced_params = [\n            self._dereference_params(arg) for arg in args\n        ]\n        kwargs_with_dereferenced_params = {\n            key: self._dereference_params(value)\n            for key, value in kwargs.items()\n        }\n        self._call_cnt += 1\n\n        # Maybe update params, depending on client configuration.\n        if self._variable_client is not None:\n          self._variable_client.update()\n\n      return handler(*args_with_dereferenced_params,\n                     **kwargs_with_dereferenced_params)\n\n    return lp.batched_handler(\n        batch_size=self._config.batch_size,\n        timeout=self._config.timeout,\n        pad_batch=True,\n        max_parallelism=2 * len(self._devices))(\n            dereference_params_and_call_handler)",
  "def dereference_params_and_call_handler(*args, **kwargs):\n      with self._mutex:\n        # Dereference args corresponding to params, leaving others unchanged.\n        args_with_dereferenced_params = [\n            self._dereference_params(arg) for arg in args\n        ]\n        kwargs_with_dereferenced_params = {\n            key: self._dereference_params(value)\n            for key, value in kwargs.items()\n        }\n        self._call_cnt += 1\n\n        # Maybe update params, depending on client configuration.\n        if self._variable_client is not None:\n          self._variable_client.update()\n\n      return handler(*args_with_dereferenced_params,\n                     **kwargs_with_dereferenced_params)",
  "def _model0(params, x1, x2):\n  return params['w0'] * jnp.sin(x1) + params['w1'] * jnp.cos(x2)",
  "def _model1(params, x):\n  return params['p0'] * jnp.log(x)",
  "class _DummyVariableSource(core.VariableSource):\n\n  def __init__(self):\n    self._params_model0 = {\n        'w0': jnp.ones([2, 3], dtype=jnp.float32),\n        'w1': 2 * jnp.ones([2, 3], dtype=jnp.float32),\n    }\n\n    self._params_model1 = {\n        'p0': jnp.ones([3, 1], dtype=jnp.float32),\n    }\n\n  def get_variables(self, names: Sequence[str]) -> Sequence[Any]:  # pytype: disable=signature-mismatch  # overriding-return-type-checks\n    variables = []\n    for n in names:\n      if n == 'params_model0':\n        variables.append(self._params_model0)\n      elif n == 'params_model1':\n        variables.append(self._params_model1)\n      else:\n        raise ValueError('Unknow variable name: {n}')\n    return variables",
  "def _get_model0(variable_source: core.VariableSource) -> types.ModelToSnapshot:\n  return types.ModelToSnapshot(\n      model=_model0,\n      params=variable_source.get_variables(['params_model0'])[0],\n      dummy_kwargs={\n          'x1': jnp.ones([2, 3], dtype=jnp.float32),\n          'x2': jnp.ones([2, 3], dtype=jnp.float32),\n      },\n  )",
  "def _get_model1(variable_source: core.VariableSource) -> types.ModelToSnapshot:\n  return types.ModelToSnapshot(\n      model=_model1,\n      params=variable_source.get_variables(['params_model1'])[0],\n      dummy_kwargs={\n          'x': jnp.ones([3, 1], dtype=jnp.float32),\n      },\n  )",
  "class SnapshotterTest(test_utils.TestCase):\n\n  def setUp(self):\n    super().setUp()\n    self._test_models = {'model0': _get_model0, 'model1': _get_model1}\n\n  def _check_snapshot(self, directory: str, name: str):\n    self.assertTrue(os.path.exists(os.path.join(directory, name, 'model0')))\n    self.assertTrue(os.path.exists(os.path.join(directory, name, 'model1')))\n\n  def test_snapshotter(self):\n    \"\"\"Checks that the Snapshotter class saves as expected.\"\"\"\n    directory = self.get_tempdir()\n\n    models_snapshotter = snapshotter.JAXSnapshotter(\n        variable_source=_DummyVariableSource(),\n        models=self._test_models,\n        path=directory,\n        max_to_keep=2,\n        add_uid=False,\n    )\n    models_snapshotter._save()\n\n    # The snapshots are written in a folder of the form:\n    # PATH/{time.strftime}/MODEL_NAME\n    first_snapshots = os.listdir(directory)\n    self.assertEqual(len(first_snapshots), 1)\n    self._check_snapshot(directory, first_snapshots[0])\n    # Make sure that the second snapshot is constructed.\n    time.sleep(1.1)\n    models_snapshotter._save()\n    snapshots = os.listdir(directory)\n    self.assertEqual(len(snapshots), 2)\n    self._check_snapshot(directory, snapshots[0])\n    self._check_snapshot(directory, snapshots[1])\n\n    # Make sure that new snapshotter deletes the oldest snapshot upon _save().\n    time.sleep(1.1)\n    models_snapshotter2 = snapshotter.JAXSnapshotter(\n        variable_source=_DummyVariableSource(),\n        models=self._test_models,\n        path=directory,\n        max_to_keep=2,\n        add_uid=False,\n    )\n    self.assertEqual(snapshots, os.listdir(directory))\n    time.sleep(1.1)\n    models_snapshotter2._save()\n    snapshots = os.listdir(directory)\n    self.assertNotIn(first_snapshots[0], snapshots)\n    self.assertEqual(len(snapshots), 2)\n    self._check_snapshot(directory, snapshots[0])\n    self._check_snapshot(directory, snapshots[1])",
  "def __init__(self):\n    self._params_model0 = {\n        'w0': jnp.ones([2, 3], dtype=jnp.float32),\n        'w1': 2 * jnp.ones([2, 3], dtype=jnp.float32),\n    }\n\n    self._params_model1 = {\n        'p0': jnp.ones([3, 1], dtype=jnp.float32),\n    }",
  "def get_variables(self, names: Sequence[str]) -> Sequence[Any]:  # pytype: disable=signature-mismatch  # overriding-return-type-checks\n    variables = []\n    for n in names:\n      if n == 'params_model0':\n        variables.append(self._params_model0)\n      elif n == 'params_model1':\n        variables.append(self._params_model1)\n      else:\n        raise ValueError('Unknow variable name: {n}')\n    return variables",
  "def setUp(self):\n    super().setUp()\n    self._test_models = {'model0': _get_model0, 'model1': _get_model1}",
  "def _check_snapshot(self, directory: str, name: str):\n    self.assertTrue(os.path.exists(os.path.join(directory, name, 'model0')))\n    self.assertTrue(os.path.exists(os.path.join(directory, name, 'model1')))",
  "def test_snapshotter(self):\n    \"\"\"Checks that the Snapshotter class saves as expected.\"\"\"\n    directory = self.get_tempdir()\n\n    models_snapshotter = snapshotter.JAXSnapshotter(\n        variable_source=_DummyVariableSource(),\n        models=self._test_models,\n        path=directory,\n        max_to_keep=2,\n        add_uid=False,\n    )\n    models_snapshotter._save()\n\n    # The snapshots are written in a folder of the form:\n    # PATH/{time.strftime}/MODEL_NAME\n    first_snapshots = os.listdir(directory)\n    self.assertEqual(len(first_snapshots), 1)\n    self._check_snapshot(directory, first_snapshots[0])\n    # Make sure that the second snapshot is constructed.\n    time.sleep(1.1)\n    models_snapshotter._save()\n    snapshots = os.listdir(directory)\n    self.assertEqual(len(snapshots), 2)\n    self._check_snapshot(directory, snapshots[0])\n    self._check_snapshot(directory, snapshots[1])\n\n    # Make sure that new snapshotter deletes the oldest snapshot upon _save().\n    time.sleep(1.1)\n    models_snapshotter2 = snapshotter.JAXSnapshotter(\n        variable_source=_DummyVariableSource(),\n        models=self._test_models,\n        path=directory,\n        max_to_keep=2,\n        add_uid=False,\n    )\n    self.assertEqual(snapshots, os.listdir(directory))\n    time.sleep(1.1)\n    models_snapshotter2._save()\n    snapshots = os.listdir(directory)\n    self.assertNotIn(first_snapshots[0], snapshots)\n    self.assertEqual(len(snapshots), 2)\n    self._check_snapshot(directory, snapshots[0])\n    self._check_snapshot(directory, snapshots[1])",
  "class StackerState(NamedTuple):\n  stack: jax.Array  # Observations stacked along the final dimension.\n  needs_reset: jax.Array",
  "class StackingActorState(NamedTuple):\n  actor_state: ActorState\n  stacker_state: StackerState",
  "def tile_nested_array(nest: acme_types.NestedArray, num: int, axis: int):\n\n  def _tile_array(array: jnp.ndarray) -> jnp.ndarray:\n    reps = [1] * array.ndim\n    reps[axis] = num\n    return jnp.tile(array, reps)\n\n  return jax.tree_map(_tile_array, nest)",
  "class ObservationStacker:\n  \"\"\"Class used to handle agent-side observation stacking.\n\n  Once an ObservationStacker is initialized and an initial_state is obtained\n  from it, one can stack nested observations by simply calling the\n  ObservationStacker and passing it the new observation and current state of its\n  observation stack.\n\n  See also observation_stacking.wrap_actor_core for hints on how to use it.\n  \"\"\"\n\n  def __init__(self,\n               observation_spec: acme_types.NestedSpec,\n               stack_size: int = 4):\n\n    def _repeat_observation(state: StackerState,\n                            first_observation: Observation) -> StackerState:\n      return state._replace(\n          needs_reset=jnp.array(False),\n          stack=tile_nested_array(first_observation, stack_size - 1, axis=-1))\n\n    self._zero_stack = tile_nested_array(\n        jax_utils.zeros_like(observation_spec), stack_size - 1, axis=-1)\n    self._repeat_observation = _repeat_observation\n\n  def __call__(self, inputs: Observation,\n               state: StackerState) -> Tuple[Observation, StackerState]:\n\n    # If this is a first observation, initialize the stack by repeating it,\n    # otherwise leave it intact.\n    state = jax.lax.cond(\n        state.needs_reset,\n        self._repeat_observation,\n        lambda state, *args: state,  # No-op on state.\n        state,\n        inputs)\n\n    # Concatenate frames along the final axis (assumed to be for channels).\n    output = jax.tree_map(lambda *x: jnp.concatenate(x, axis=-1),\n                          state.stack, inputs)\n\n    # Update the frame stack by adding the input and dropping the first\n    # observation in the stack. Note that we use the final dimension as each\n    # leaf in the nested observation may have a different last dim.\n    new_state = state._replace(\n        stack=jax.tree_map(lambda x, y: y[..., x.shape[-1]:], inputs, output))\n\n    return output, new_state\n\n  def initial_state(self) -> StackerState:\n    return StackerState(stack=self._zero_stack, needs_reset=jnp.array(True))",
  "def get_adjusted_environment_spec(environment_spec: specs.EnvironmentSpec,\n                                  stack_size: int) -> specs.EnvironmentSpec:\n  \"\"\"Returns a spec where the observation spec accounts for stacking.\"\"\"\n\n  def stack_observation_spec(obs_spec: specs.Array) -> specs.Array:\n    \"\"\"Adjusts last axis shape to account for observation stacking.\"\"\"\n    new_shape = obs_spec.shape[:-1] + (obs_spec.shape[-1] * stack_size,)\n    return obs_spec.replace(shape=new_shape)\n\n  adjusted_observation_spec = jax.tree_map(stack_observation_spec,\n                                           environment_spec.observations)\n\n  return environment_spec._replace(observations=adjusted_observation_spec)",
  "def wrap_actor_core(\n    actor_core: actor_core_lib.ActorCore,\n    observation_spec: specs.Array,\n    num_stacked_observations: int = 1) -> actor_core_lib.ActorCore:\n  \"\"\"Wraps an actor core so that it performs observation stacking.\"\"\"\n\n  if num_stacked_observations <= 0:\n    raise ValueError(\n        'Number of stacked observations must be strictly positive.'\n        f' Received num_stacked_observations={num_stacked_observations}.')\n\n  if num_stacked_observations == 1:\n    # Return unwrapped core when a trivial stack size is requested.\n    return actor_core\n\n  obs_stacker = ObservationStacker(\n      observation_spec=observation_spec, stack_size=num_stacked_observations)\n\n  def init(key: jax_types.PRNGKey) -> StackingActorState:\n    return StackingActorState(\n        actor_state=actor_core.init(key),\n        stacker_state=obs_stacker.initial_state())\n\n  def select_action(\n      params: Params,\n      observations: Observation,\n      state: StackingActorState,\n  ) -> Tuple[Action, StackingActorState]:\n\n    stacked_observations, stacker_state = obs_stacker(observations,\n                                                      state.stacker_state)\n\n    actions, actor_state = actor_core.select_action(params,\n                                                    stacked_observations,\n                                                    state.actor_state)\n    new_state = StackingActorState(\n        actor_state=actor_state, stacker_state=stacker_state)\n\n    return actions, new_state\n\n  def get_extras(state: StackingActorState) -> Mapping[str, jnp.ndarray]:\n    return actor_core.get_extras(state.actor_state)\n\n  return actor_core_lib.ActorCore(\n      init=init, select_action=select_action, get_extras=get_extras)",
  "def stack_reverb_observation(sample: reverb.ReplaySample,\n                             stack_size: int) -> reverb.ReplaySample:\n  \"\"\"Stacks observations in a Reverb sample.\n\n  This function is meant to be used on the dataset creation side as a\n  post-processing function before batching.\n\n  Warnings!\n    * Only works if SequenceAdder is in end_of_episode_behavior=CONTINUE mode.\n    * Only tested on RGB and scalar (shape = (1,)) observations.\n    * At episode starts, this function repeats the first observation to form a\n      stack. Could consider using zeroed observations instead.\n    * At episode starts, this function always selects the latest possible\n      stacked trajectory. Could consider randomizing the start index of the\n      sequence.\n\n  Args:\n    sample: A sample coming from a Reverb replay table. Should be an unbatched\n      sequence so that sample.data.observation is a nested structure of\n      time-major tensors.\n    stack_size: Number of observations to stack.\n\n  Returns:\n    A new sample where sample.data.observation has the same nested structure as\n    the incoming sample but with every tensor having its final dimension\n    multiplied by `stack_size`.\n  \"\"\"\n\n  def _repeat_first(sequence: tf.Tensor) -> tf.Tensor:\n    repeated_first_step = tf_utils.tile_tensor(sequence[0], stack_size - 1)\n    return tf.concat([repeated_first_step, sequence], 0)[:-(stack_size - 1)]\n\n  def _stack_observation(observation: tf.Tensor) -> tf.Tensor:\n    stack = [tf.roll(observation, i, axis=0) for i in range(stack_size)]\n    stack.reverse()  # Reverse stack order to be chronological.\n    return tf.concat(stack, axis=-1)\n\n  # Maybe repeat the first observation, if at the start of an episode.\n  data = tf.cond(sample.data.start_of_episode[0],\n                 lambda: tree.map_structure(_repeat_first, sample.data),\n                 lambda: sample.data)\n\n  # Stack observation in the sample's data.\n  data_with_stacked_obs = data._replace(\n      observation=tree.map_structure(_stack_observation, data.observation))\n\n  # Truncate the start of the sequence due to the first stacks containing the\n  # final observations that were rolled over to the start.\n  data = tree.map_structure(lambda x: x[stack_size - 1:], data_with_stacked_obs)\n\n  return reverb.ReplaySample(info=sample.info, data=data)",
  "def _tile_array(array: jnp.ndarray) -> jnp.ndarray:\n    reps = [1] * array.ndim\n    reps[axis] = num\n    return jnp.tile(array, reps)",
  "def __init__(self,\n               observation_spec: acme_types.NestedSpec,\n               stack_size: int = 4):\n\n    def _repeat_observation(state: StackerState,\n                            first_observation: Observation) -> StackerState:\n      return state._replace(\n          needs_reset=jnp.array(False),\n          stack=tile_nested_array(first_observation, stack_size - 1, axis=-1))\n\n    self._zero_stack = tile_nested_array(\n        jax_utils.zeros_like(observation_spec), stack_size - 1, axis=-1)\n    self._repeat_observation = _repeat_observation",
  "def __call__(self, inputs: Observation,\n               state: StackerState) -> Tuple[Observation, StackerState]:\n\n    # If this is a first observation, initialize the stack by repeating it,\n    # otherwise leave it intact.\n    state = jax.lax.cond(\n        state.needs_reset,\n        self._repeat_observation,\n        lambda state, *args: state,  # No-op on state.\n        state,\n        inputs)\n\n    # Concatenate frames along the final axis (assumed to be for channels).\n    output = jax.tree_map(lambda *x: jnp.concatenate(x, axis=-1),\n                          state.stack, inputs)\n\n    # Update the frame stack by adding the input and dropping the first\n    # observation in the stack. Note that we use the final dimension as each\n    # leaf in the nested observation may have a different last dim.\n    new_state = state._replace(\n        stack=jax.tree_map(lambda x, y: y[..., x.shape[-1]:], inputs, output))\n\n    return output, new_state",
  "def initial_state(self) -> StackerState:\n    return StackerState(stack=self._zero_stack, needs_reset=jnp.array(True))",
  "def stack_observation_spec(obs_spec: specs.Array) -> specs.Array:\n    \"\"\"Adjusts last axis shape to account for observation stacking.\"\"\"\n    new_shape = obs_spec.shape[:-1] + (obs_spec.shape[-1] * stack_size,)\n    return obs_spec.replace(shape=new_shape)",
  "def init(key: jax_types.PRNGKey) -> StackingActorState:\n    return StackingActorState(\n        actor_state=actor_core.init(key),\n        stacker_state=obs_stacker.initial_state())",
  "def select_action(\n      params: Params,\n      observations: Observation,\n      state: StackingActorState,\n  ) -> Tuple[Action, StackingActorState]:\n\n    stacked_observations, stacker_state = obs_stacker(observations,\n                                                      state.stacker_state)\n\n    actions, actor_state = actor_core.select_action(params,\n                                                    stacked_observations,\n                                                    state.actor_state)\n    new_state = StackingActorState(\n        actor_state=actor_state, stacker_state=stacker_state)\n\n    return actions, new_state",
  "def get_extras(state: StackingActorState) -> Mapping[str, jnp.ndarray]:\n    return actor_core.get_extras(state.actor_state)",
  "def _repeat_first(sequence: tf.Tensor) -> tf.Tensor:\n    repeated_first_step = tf_utils.tile_tensor(sequence[0], stack_size - 1)\n    return tf.concat([repeated_first_step, sequence], 0)[:-(stack_size - 1)]",
  "def _stack_observation(observation: tf.Tensor) -> tf.Tensor:\n    stack = [tf.roll(observation, i, axis=0) for i in range(stack_size)]\n    stack.reverse()  # Reverse stack order to be chronological.\n    return tf.concat(stack, axis=-1)",
  "def _repeat_observation(state: StackerState,\n                            first_observation: Observation) -> StackerState:\n      return state._replace(\n          needs_reset=jnp.array(False),\n          stack=tile_nested_array(first_observation, stack_size - 1, axis=-1))",
  "class MPOParams(NamedTuple):\n  \"\"\"NamedTuple to store trainable loss parameters.\"\"\"\n  log_temperature: jnp.ndarray\n  log_alpha_mean: jnp.ndarray\n  log_alpha_stddev: jnp.ndarray\n  log_penalty_temperature: Optional[jnp.ndarray] = None",
  "class MPOStats(NamedTuple):\n  \"\"\"NamedTuple to store loss statistics.\"\"\"\n  dual_alpha_mean: Union[float, jnp.ndarray]\n  dual_alpha_stddev: Union[float, jnp.ndarray]\n  dual_temperature: Union[float, jnp.ndarray]\n\n  loss_policy: Union[float, jnp.ndarray]\n  loss_alpha: Union[float, jnp.ndarray]\n  loss_temperature: Union[float, jnp.ndarray]\n  kl_q_rel: Union[float, jnp.ndarray]\n\n  kl_mean_rel: Union[float, jnp.ndarray]\n  kl_stddev_rel: Union[float, jnp.ndarray]\n\n  q_min: Union[float, jnp.ndarray]\n  q_max: Union[float, jnp.ndarray]\n\n  pi_stddev_min: Union[float, jnp.ndarray]\n  pi_stddev_max: Union[float, jnp.ndarray]\n  pi_stddev_cond: Union[float, jnp.ndarray]\n\n  penalty_kl_q_rel: Optional[float] = None",
  "class MPO:\n  \"\"\"MPO loss with decoupled KL constraints as in (Abdolmaleki et al., 2018).\n\n  This implementation of the MPO loss includes the following features, as\n  options:\n  - Satisfying the KL-constraint on a per-dimension basis (on by default);\n  - Penalizing actions that fall outside of [-1, 1] (on by default) as a\n      special case of multi-objective MPO (MO-MPO; Abdolmaleki et al., 2020).\n  For best results on the control suite, keep both of these on.\n\n  (Abdolmaleki et al., 2018): https://arxiv.org/pdf/1812.02256.pdf\n  (Abdolmaleki et al., 2020): https://arxiv.org/pdf/2005.07513.pdf\n  \"\"\"\n\n  def __init__(self,\n               epsilon: float,\n               epsilon_mean: float,\n               epsilon_stddev: float,\n               init_log_temperature: float,\n               init_log_alpha_mean: float,\n               init_log_alpha_stddev: float,\n               per_dim_constraining: bool = True,\n               action_penalization: bool = True,\n               epsilon_penalty: float = 0.001):\n    \"\"\"Initialize and configure the MPO loss.\n\n    Args:\n      epsilon: KL constraint on the non-parametric auxiliary policy, the one\n        associated with the dual variable called temperature.\n      epsilon_mean: KL constraint on the mean of the Gaussian policy, the one\n        associated with the dual variable called alpha_mean.\n      epsilon_stddev: KL constraint on the stddev of the Gaussian policy, the\n        one associated with the dual variable called alpha_mean.\n      init_log_temperature: initial value for the temperature in log-space, note\n        a softplus (rather than an exp) will be used to transform this.\n      init_log_alpha_mean: initial value for the alpha_mean in log-space, note a\n        softplus (rather than an exp) will be used to transform this.\n      init_log_alpha_stddev: initial value for the alpha_stddev in log-space,\n        note a softplus (rather than an exp) will be used to transform this.\n      per_dim_constraining: whether to enforce the KL constraint on each\n        dimension independently; this is the default. Otherwise the overall KL\n        is constrained, which allows some dimensions to change more at the\n        expense of others staying put.\n      action_penalization: whether to use a KL constraint to penalize actions\n        via the MO-MPO algorithm.\n      epsilon_penalty: KL constraint on the probability of violating the action\n        constraint.\n    \"\"\"\n\n    # MPO constrain thresholds.\n    self._epsilon = epsilon\n    self._epsilon_mean = epsilon_mean\n    self._epsilon_stddev = epsilon_stddev\n\n    # Initial values for the constraints' dual variables.\n    self._init_log_temperature = init_log_temperature\n    self._init_log_alpha_mean = init_log_alpha_mean\n    self._init_log_alpha_stddev = init_log_alpha_stddev\n\n    # Whether to penalize out-of-bound actions via MO-MPO and its corresponding\n    # constraint threshold.\n    self._action_penalization = action_penalization\n    self._epsilon_penalty = epsilon_penalty\n\n    # Whether to ensure per-dimension KL constraint satisfication.\n    self._per_dim_constraining = per_dim_constraining\n\n  @property\n  def per_dim_constraining(self):\n    return self._per_dim_constraining\n\n  def init_params(self, action_dim: int, dtype: DType = jnp.float32):\n    \"\"\"Creates an initial set of parameters.\"\"\"\n\n    if self._per_dim_constraining:\n      dual_variable_shape = [action_dim]\n    else:\n      dual_variable_shape = [1]\n\n    log_temperature = jnp.full([1], self._init_log_temperature, dtype=dtype)\n\n    log_alpha_mean = jnp.full(\n        dual_variable_shape, self._init_log_alpha_mean, dtype=dtype)\n\n    log_alpha_stddev = jnp.full(\n        dual_variable_shape, self._init_log_alpha_stddev, dtype=dtype)\n\n    if self._action_penalization:\n      log_penalty_temperature = jnp.full([1],\n                                         self._init_log_temperature,\n                                         dtype=dtype)\n    else:\n      log_penalty_temperature = None\n\n    return MPOParams(\n        log_temperature=log_temperature,\n        log_alpha_mean=log_alpha_mean,\n        log_alpha_stddev=log_alpha_stddev,\n        log_penalty_temperature=log_penalty_temperature)\n\n  def __call__(\n      self,\n      params: MPOParams,\n      online_action_distribution: Union[tfd.MultivariateNormalDiag,\n                                        tfd.Independent],\n      target_action_distribution: Union[tfd.MultivariateNormalDiag,\n                                        tfd.Independent],\n      actions: jnp.ndarray,  # Shape [N, B, D].\n      q_values: jnp.ndarray,  # Shape [N, B].\n  ) -> Tuple[jnp.ndarray, MPOStats]:\n    \"\"\"Computes the decoupled MPO loss.\n\n    Args:\n      params: parameters tracking the temperature and the dual variables.\n      online_action_distribution: online distribution returned by the online\n        policy network; expects batch_dims of [B] and event_dims of [D].\n      target_action_distribution: target distribution returned by the target\n        policy network; expects same shapes as online distribution.\n      actions: actions sampled from the target policy; expects shape [N, B, D].\n      q_values: Q-values associated with each action; expects shape [N, B].\n\n    Returns:\n      Loss, combining the policy loss, KL penalty, and dual losses required to\n        adapt the dual variables.\n      Stats, for diagnostics and tracking performance.\n    \"\"\"\n\n    # Cast `MultivariateNormalDiag`s to Independent Normals.\n    # The latter allows us to satisfy KL constraints per-dimension.\n    if isinstance(target_action_distribution, tfd.MultivariateNormalDiag):\n      target_action_distribution = tfd.Independent(\n          tfd.Normal(target_action_distribution.mean(),\n                     target_action_distribution.stddev()))\n      online_action_distribution = tfd.Independent(\n          tfd.Normal(online_action_distribution.mean(),\n                     online_action_distribution.stddev()))\n\n    # Transform dual variables from log-space.\n    # Note: using softplus instead of exponential for numerical stability.\n    temperature = jax.nn.softplus(params.log_temperature) + _MPO_FLOAT_EPSILON\n    alpha_mean = jax.nn.softplus(params.log_alpha_mean) + _MPO_FLOAT_EPSILON\n    alpha_stddev = jax.nn.softplus(params.log_alpha_stddev) + _MPO_FLOAT_EPSILON\n\n    # Get online and target means and stddevs in preparation for decomposition.\n    online_mean = online_action_distribution.distribution.mean()\n    online_scale = online_action_distribution.distribution.stddev()\n    target_mean = target_action_distribution.distribution.mean()\n    target_scale = target_action_distribution.distribution.stddev()\n\n    # Compute normalized importance weights, used to compute expectations with\n    # respect to the non-parametric policy; and the temperature loss, used to\n    # adapt the tempering of Q-values.\n    normalized_weights, loss_temperature = compute_weights_and_temperature_loss(\n        q_values, self._epsilon, temperature)\n\n    # Only needed for diagnostics: Compute estimated actualized KL between the\n    # non-parametric and current target policies.\n    kl_nonparametric = compute_nonparametric_kl_from_normalized_weights(\n        normalized_weights)\n\n    if self._action_penalization:\n      # Transform action penalization temperature.\n      penalty_temperature = jax.nn.softplus(\n          params.log_penalty_temperature) + _MPO_FLOAT_EPSILON\n\n      # Compute action penalization cost.\n      # Note: the cost is zero in [-1, 1] and quadratic beyond.\n      diff_out_of_bound = actions - jnp.clip(actions, -1.0, 1.0)\n      cost_out_of_bound = -jnp.linalg.norm(diff_out_of_bound, axis=-1)\n\n      penalty_normalized_weights, loss_penalty_temperature = compute_weights_and_temperature_loss(\n          cost_out_of_bound, self._epsilon_penalty, penalty_temperature)\n\n      # Only needed for diagnostics: Compute estimated actualized KL between the\n      # non-parametric and current target policies.\n      penalty_kl_nonparametric = compute_nonparametric_kl_from_normalized_weights(\n          penalty_normalized_weights)\n\n      # Combine normalized weights.\n      normalized_weights += penalty_normalized_weights\n      loss_temperature += loss_penalty_temperature\n\n    # Decompose the online policy into fixed-mean & fixed-stddev distributions.\n    # This has been documented as having better performance in bandit settings,\n    # see e.g. https://arxiv.org/pdf/1812.02256.pdf.\n    fixed_stddev_distribution = tfd.Independent(\n        tfd.Normal(loc=online_mean, scale=target_scale))\n    fixed_mean_distribution = tfd.Independent(\n        tfd.Normal(loc=target_mean, scale=online_scale))\n\n    # Compute the decomposed policy losses.\n    loss_policy_mean = compute_cross_entropy_loss(actions, normalized_weights,\n                                                  fixed_stddev_distribution)\n    loss_policy_stddev = compute_cross_entropy_loss(actions, normalized_weights,\n                                                    fixed_mean_distribution)\n\n    # Compute the decomposed KL between the target and online policies.\n    if self._per_dim_constraining:\n      kl_mean = target_action_distribution.distribution.kl_divergence(\n          fixed_stddev_distribution.distribution)  # Shape [B, D].\n      kl_stddev = target_action_distribution.distribution.kl_divergence(\n          fixed_mean_distribution.distribution)  # Shape [B, D].\n    else:\n      kl_mean = target_action_distribution.kl_divergence(\n          fixed_stddev_distribution)  # Shape [B].\n      kl_stddev = target_action_distribution.kl_divergence(\n          fixed_mean_distribution)  # Shape [B].\n\n    # Compute the alpha-weighted KL-penalty and dual losses to adapt the alphas.\n    loss_kl_mean, loss_alpha_mean = compute_parametric_kl_penalty_and_dual_loss(\n        kl_mean, alpha_mean, self._epsilon_mean)\n    loss_kl_stddev, loss_alpha_stddev = compute_parametric_kl_penalty_and_dual_loss(\n        kl_stddev, alpha_stddev, self._epsilon_stddev)\n\n    # Combine losses.\n    loss_policy = loss_policy_mean + loss_policy_stddev\n    loss_kl_penalty = loss_kl_mean + loss_kl_stddev\n    loss_dual = loss_alpha_mean + loss_alpha_stddev + loss_temperature\n    loss = loss_policy + loss_kl_penalty + loss_dual\n\n    # Create statistics.\n    pi_stddev = online_action_distribution.distribution.stddev()\n    stats = MPOStats(\n        # Dual Variables.\n        dual_alpha_mean=jnp.mean(alpha_mean),\n        dual_alpha_stddev=jnp.mean(alpha_stddev),\n        dual_temperature=jnp.mean(temperature),\n        # Losses.\n        loss_policy=jnp.mean(loss),\n        loss_alpha=jnp.mean(loss_alpha_mean + loss_alpha_stddev),\n        loss_temperature=jnp.mean(loss_temperature),\n        # KL measurements.\n        kl_q_rel=jnp.mean(kl_nonparametric) / self._epsilon,\n        penalty_kl_q_rel=((jnp.mean(penalty_kl_nonparametric) /\n                           self._epsilon_penalty)\n                          if self._action_penalization else None),\n        kl_mean_rel=jnp.mean(kl_mean, axis=0) / self._epsilon_mean,\n        kl_stddev_rel=jnp.mean(kl_stddev, axis=0) / self._epsilon_stddev,\n        # Q measurements.\n        q_min=jnp.mean(jnp.min(q_values, axis=0)),\n        q_max=jnp.mean(jnp.max(q_values, axis=0)),\n        # If the policy has stddev, log summary stats for this as well.\n        pi_stddev_min=jnp.mean(jnp.min(pi_stddev, axis=-1)),\n        pi_stddev_max=jnp.mean(jnp.max(pi_stddev, axis=-1)),\n        # Condition number of the diagonal covariance (actually, stddev) matrix.\n        pi_stddev_cond=jnp.mean(\n            jnp.max(pi_stddev, axis=-1) / jnp.min(pi_stddev, axis=-1)),\n    )\n\n    return loss, stats",
  "def compute_weights_and_temperature_loss(\n    q_values: jnp.ndarray,\n    epsilon: float,\n    temperature: jnp.ndarray,\n) -> Tuple[jnp.ndarray, jnp.ndarray]:\n  \"\"\"Computes normalized importance weights for the policy optimization.\n\n  Args:\n    q_values: Q-values associated with the actions sampled from the target\n      policy; expected shape [N, B].\n    epsilon: Desired constraint on the KL between the target and non-parametric\n      policies.\n    temperature: Scalar used to temper the Q-values before computing normalized\n      importance weights from them. This is really the Lagrange dual variable in\n      the constrained optimization problem, the solution of which is the\n      non-parametric policy targeted by the policy loss.\n\n  Returns:\n    Normalized importance weights, used for policy optimization.\n    Temperature loss, used to adapt the temperature.\n  \"\"\"\n\n  # Temper the given Q-values using the current temperature.\n  tempered_q_values = jax.lax.stop_gradient(q_values) / temperature\n\n  # Compute the normalized importance weights used to compute expectations with\n  # respect to the non-parametric policy.\n  normalized_weights = jax.nn.softmax(tempered_q_values, axis=0)\n  normalized_weights = jax.lax.stop_gradient(normalized_weights)\n\n  # Compute the temperature loss (dual of the E-step optimization problem).\n  q_logsumexp = jax.scipy.special.logsumexp(tempered_q_values, axis=0)\n  log_num_actions = jnp.log(q_values.shape[0] / 1.)\n  loss_temperature = epsilon + jnp.mean(q_logsumexp) - log_num_actions\n  loss_temperature = temperature * loss_temperature\n\n  return normalized_weights, loss_temperature",
  "def compute_nonparametric_kl_from_normalized_weights(\n    normalized_weights: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Estimate the actualized KL between the non-parametric and target policies.\"\"\"\n\n  # Compute integrand.\n  num_action_samples = normalized_weights.shape[0] / 1.\n  integrand = jnp.log(num_action_samples * normalized_weights + 1e-8)\n\n  # Return the expectation with respect to the non-parametric policy.\n  return jnp.sum(normalized_weights * integrand, axis=0)",
  "def compute_cross_entropy_loss(\n    sampled_actions: jnp.ndarray,\n    normalized_weights: jnp.ndarray,\n    online_action_distribution: tfd.Distribution,\n) -> jnp.ndarray:\n  \"\"\"Compute cross-entropy online and the reweighted target policy.\n\n  Args:\n    sampled_actions: samples used in the Monte Carlo integration in the policy\n      loss. Expected shape is [N, B, ...], where N is the number of sampled\n      actions and B is the number of sampled states.\n    normalized_weights: target policy multiplied by the exponentiated Q values\n      and normalized; expected shape is [N, B].\n    online_action_distribution: policy to be optimized.\n\n  Returns:\n    loss_policy_gradient: the cross-entropy loss that, when differentiated,\n      produces the policy gradient.\n  \"\"\"\n\n  # Compute the M-step loss.\n  log_prob = online_action_distribution.log_prob(sampled_actions)\n\n  # Compute the weighted average log-prob using the normalized weights.\n  loss_policy_gradient = -jnp.sum(log_prob * normalized_weights, axis=0)\n\n  # Return the mean loss over the batch of states.\n  return jnp.mean(loss_policy_gradient, axis=0)",
  "def compute_parametric_kl_penalty_and_dual_loss(\n    kl: jnp.ndarray,\n    alpha: jnp.ndarray,\n    epsilon: float,\n) -> Tuple[jnp.ndarray, jnp.ndarray]:\n  \"\"\"Computes the KL cost to be added to the Lagragian and its dual loss.\n\n  The KL cost is simply the alpha-weighted KL divergence and it is added as a\n  regularizer to the policy loss. The dual variable alpha itself has a loss that\n  can be minimized to adapt the strength of the regularizer to keep the KL\n  between consecutive updates at the desired target value of epsilon.\n\n  Args:\n    kl: KL divergence between the target and online policies.\n    alpha: Lagrange multipliers (dual variables) for the KL constraints.\n    epsilon: Desired value for the KL.\n\n  Returns:\n    loss_kl: alpha-weighted KL regularization to be added to the policy loss.\n    loss_alpha: The Lagrange dual loss minimized to adapt alpha.\n  \"\"\"\n\n  # Compute the mean KL over the batch.\n  mean_kl = jnp.mean(kl, axis=0)\n\n  # Compute the regularization.\n  loss_kl = jnp.sum(jax.lax.stop_gradient(alpha) * mean_kl)\n\n  # Compute the dual loss.\n  loss_alpha = jnp.sum(alpha * (epsilon - jax.lax.stop_gradient(mean_kl)))\n\n  return loss_kl, loss_alpha",
  "def clip_mpo_params(params: MPOParams, per_dim_constraining: bool) -> MPOParams:\n  clipped_params = MPOParams(\n      log_temperature=jnp.maximum(_MIN_LOG_TEMPERATURE, params.log_temperature),\n      log_alpha_mean=jnp.maximum(_MIN_LOG_ALPHA, params.log_alpha_mean),\n      log_alpha_stddev=jnp.maximum(_MIN_LOG_ALPHA, params.log_alpha_stddev))\n  if not per_dim_constraining:\n    return clipped_params\n  else:\n    return clipped_params._replace(\n        log_penalty_temperature=jnp.maximum(_MIN_LOG_TEMPERATURE,\n                                            params.log_penalty_temperature))",
  "def __init__(self,\n               epsilon: float,\n               epsilon_mean: float,\n               epsilon_stddev: float,\n               init_log_temperature: float,\n               init_log_alpha_mean: float,\n               init_log_alpha_stddev: float,\n               per_dim_constraining: bool = True,\n               action_penalization: bool = True,\n               epsilon_penalty: float = 0.001):\n    \"\"\"Initialize and configure the MPO loss.\n\n    Args:\n      epsilon: KL constraint on the non-parametric auxiliary policy, the one\n        associated with the dual variable called temperature.\n      epsilon_mean: KL constraint on the mean of the Gaussian policy, the one\n        associated with the dual variable called alpha_mean.\n      epsilon_stddev: KL constraint on the stddev of the Gaussian policy, the\n        one associated with the dual variable called alpha_mean.\n      init_log_temperature: initial value for the temperature in log-space, note\n        a softplus (rather than an exp) will be used to transform this.\n      init_log_alpha_mean: initial value for the alpha_mean in log-space, note a\n        softplus (rather than an exp) will be used to transform this.\n      init_log_alpha_stddev: initial value for the alpha_stddev in log-space,\n        note a softplus (rather than an exp) will be used to transform this.\n      per_dim_constraining: whether to enforce the KL constraint on each\n        dimension independently; this is the default. Otherwise the overall KL\n        is constrained, which allows some dimensions to change more at the\n        expense of others staying put.\n      action_penalization: whether to use a KL constraint to penalize actions\n        via the MO-MPO algorithm.\n      epsilon_penalty: KL constraint on the probability of violating the action\n        constraint.\n    \"\"\"\n\n    # MPO constrain thresholds.\n    self._epsilon = epsilon\n    self._epsilon_mean = epsilon_mean\n    self._epsilon_stddev = epsilon_stddev\n\n    # Initial values for the constraints' dual variables.\n    self._init_log_temperature = init_log_temperature\n    self._init_log_alpha_mean = init_log_alpha_mean\n    self._init_log_alpha_stddev = init_log_alpha_stddev\n\n    # Whether to penalize out-of-bound actions via MO-MPO and its corresponding\n    # constraint threshold.\n    self._action_penalization = action_penalization\n    self._epsilon_penalty = epsilon_penalty\n\n    # Whether to ensure per-dimension KL constraint satisfication.\n    self._per_dim_constraining = per_dim_constraining",
  "def per_dim_constraining(self):\n    return self._per_dim_constraining",
  "def init_params(self, action_dim: int, dtype: DType = jnp.float32):\n    \"\"\"Creates an initial set of parameters.\"\"\"\n\n    if self._per_dim_constraining:\n      dual_variable_shape = [action_dim]\n    else:\n      dual_variable_shape = [1]\n\n    log_temperature = jnp.full([1], self._init_log_temperature, dtype=dtype)\n\n    log_alpha_mean = jnp.full(\n        dual_variable_shape, self._init_log_alpha_mean, dtype=dtype)\n\n    log_alpha_stddev = jnp.full(\n        dual_variable_shape, self._init_log_alpha_stddev, dtype=dtype)\n\n    if self._action_penalization:\n      log_penalty_temperature = jnp.full([1],\n                                         self._init_log_temperature,\n                                         dtype=dtype)\n    else:\n      log_penalty_temperature = None\n\n    return MPOParams(\n        log_temperature=log_temperature,\n        log_alpha_mean=log_alpha_mean,\n        log_alpha_stddev=log_alpha_stddev,\n        log_penalty_temperature=log_penalty_temperature)",
  "def __call__(\n      self,\n      params: MPOParams,\n      online_action_distribution: Union[tfd.MultivariateNormalDiag,\n                                        tfd.Independent],\n      target_action_distribution: Union[tfd.MultivariateNormalDiag,\n                                        tfd.Independent],\n      actions: jnp.ndarray,  # Shape [N, B, D].\n      q_values: jnp.ndarray,  # Shape [N, B].\n  ) -> Tuple[jnp.ndarray, MPOStats]:\n    \"\"\"Computes the decoupled MPO loss.\n\n    Args:\n      params: parameters tracking the temperature and the dual variables.\n      online_action_distribution: online distribution returned by the online\n        policy network; expects batch_dims of [B] and event_dims of [D].\n      target_action_distribution: target distribution returned by the target\n        policy network; expects same shapes as online distribution.\n      actions: actions sampled from the target policy; expects shape [N, B, D].\n      q_values: Q-values associated with each action; expects shape [N, B].\n\n    Returns:\n      Loss, combining the policy loss, KL penalty, and dual losses required to\n        adapt the dual variables.\n      Stats, for diagnostics and tracking performance.\n    \"\"\"\n\n    # Cast `MultivariateNormalDiag`s to Independent Normals.\n    # The latter allows us to satisfy KL constraints per-dimension.\n    if isinstance(target_action_distribution, tfd.MultivariateNormalDiag):\n      target_action_distribution = tfd.Independent(\n          tfd.Normal(target_action_distribution.mean(),\n                     target_action_distribution.stddev()))\n      online_action_distribution = tfd.Independent(\n          tfd.Normal(online_action_distribution.mean(),\n                     online_action_distribution.stddev()))\n\n    # Transform dual variables from log-space.\n    # Note: using softplus instead of exponential for numerical stability.\n    temperature = jax.nn.softplus(params.log_temperature) + _MPO_FLOAT_EPSILON\n    alpha_mean = jax.nn.softplus(params.log_alpha_mean) + _MPO_FLOAT_EPSILON\n    alpha_stddev = jax.nn.softplus(params.log_alpha_stddev) + _MPO_FLOAT_EPSILON\n\n    # Get online and target means and stddevs in preparation for decomposition.\n    online_mean = online_action_distribution.distribution.mean()\n    online_scale = online_action_distribution.distribution.stddev()\n    target_mean = target_action_distribution.distribution.mean()\n    target_scale = target_action_distribution.distribution.stddev()\n\n    # Compute normalized importance weights, used to compute expectations with\n    # respect to the non-parametric policy; and the temperature loss, used to\n    # adapt the tempering of Q-values.\n    normalized_weights, loss_temperature = compute_weights_and_temperature_loss(\n        q_values, self._epsilon, temperature)\n\n    # Only needed for diagnostics: Compute estimated actualized KL between the\n    # non-parametric and current target policies.\n    kl_nonparametric = compute_nonparametric_kl_from_normalized_weights(\n        normalized_weights)\n\n    if self._action_penalization:\n      # Transform action penalization temperature.\n      penalty_temperature = jax.nn.softplus(\n          params.log_penalty_temperature) + _MPO_FLOAT_EPSILON\n\n      # Compute action penalization cost.\n      # Note: the cost is zero in [-1, 1] and quadratic beyond.\n      diff_out_of_bound = actions - jnp.clip(actions, -1.0, 1.0)\n      cost_out_of_bound = -jnp.linalg.norm(diff_out_of_bound, axis=-1)\n\n      penalty_normalized_weights, loss_penalty_temperature = compute_weights_and_temperature_loss(\n          cost_out_of_bound, self._epsilon_penalty, penalty_temperature)\n\n      # Only needed for diagnostics: Compute estimated actualized KL between the\n      # non-parametric and current target policies.\n      penalty_kl_nonparametric = compute_nonparametric_kl_from_normalized_weights(\n          penalty_normalized_weights)\n\n      # Combine normalized weights.\n      normalized_weights += penalty_normalized_weights\n      loss_temperature += loss_penalty_temperature\n\n    # Decompose the online policy into fixed-mean & fixed-stddev distributions.\n    # This has been documented as having better performance in bandit settings,\n    # see e.g. https://arxiv.org/pdf/1812.02256.pdf.\n    fixed_stddev_distribution = tfd.Independent(\n        tfd.Normal(loc=online_mean, scale=target_scale))\n    fixed_mean_distribution = tfd.Independent(\n        tfd.Normal(loc=target_mean, scale=online_scale))\n\n    # Compute the decomposed policy losses.\n    loss_policy_mean = compute_cross_entropy_loss(actions, normalized_weights,\n                                                  fixed_stddev_distribution)\n    loss_policy_stddev = compute_cross_entropy_loss(actions, normalized_weights,\n                                                    fixed_mean_distribution)\n\n    # Compute the decomposed KL between the target and online policies.\n    if self._per_dim_constraining:\n      kl_mean = target_action_distribution.distribution.kl_divergence(\n          fixed_stddev_distribution.distribution)  # Shape [B, D].\n      kl_stddev = target_action_distribution.distribution.kl_divergence(\n          fixed_mean_distribution.distribution)  # Shape [B, D].\n    else:\n      kl_mean = target_action_distribution.kl_divergence(\n          fixed_stddev_distribution)  # Shape [B].\n      kl_stddev = target_action_distribution.kl_divergence(\n          fixed_mean_distribution)  # Shape [B].\n\n    # Compute the alpha-weighted KL-penalty and dual losses to adapt the alphas.\n    loss_kl_mean, loss_alpha_mean = compute_parametric_kl_penalty_and_dual_loss(\n        kl_mean, alpha_mean, self._epsilon_mean)\n    loss_kl_stddev, loss_alpha_stddev = compute_parametric_kl_penalty_and_dual_loss(\n        kl_stddev, alpha_stddev, self._epsilon_stddev)\n\n    # Combine losses.\n    loss_policy = loss_policy_mean + loss_policy_stddev\n    loss_kl_penalty = loss_kl_mean + loss_kl_stddev\n    loss_dual = loss_alpha_mean + loss_alpha_stddev + loss_temperature\n    loss = loss_policy + loss_kl_penalty + loss_dual\n\n    # Create statistics.\n    pi_stddev = online_action_distribution.distribution.stddev()\n    stats = MPOStats(\n        # Dual Variables.\n        dual_alpha_mean=jnp.mean(alpha_mean),\n        dual_alpha_stddev=jnp.mean(alpha_stddev),\n        dual_temperature=jnp.mean(temperature),\n        # Losses.\n        loss_policy=jnp.mean(loss),\n        loss_alpha=jnp.mean(loss_alpha_mean + loss_alpha_stddev),\n        loss_temperature=jnp.mean(loss_temperature),\n        # KL measurements.\n        kl_q_rel=jnp.mean(kl_nonparametric) / self._epsilon,\n        penalty_kl_q_rel=((jnp.mean(penalty_kl_nonparametric) /\n                           self._epsilon_penalty)\n                          if self._action_penalization else None),\n        kl_mean_rel=jnp.mean(kl_mean, axis=0) / self._epsilon_mean,\n        kl_stddev_rel=jnp.mean(kl_stddev, axis=0) / self._epsilon_stddev,\n        # Q measurements.\n        q_min=jnp.mean(jnp.min(q_values, axis=0)),\n        q_max=jnp.mean(jnp.max(q_values, axis=0)),\n        # If the policy has stddev, log summary stats for this as well.\n        pi_stddev_min=jnp.mean(jnp.min(pi_stddev, axis=-1)),\n        pi_stddev_max=jnp.mean(jnp.max(pi_stddev, axis=-1)),\n        # Condition number of the diagonal covariance (actually, stddev) matrix.\n        pi_stddev_cond=jnp.mean(\n            jnp.max(pi_stddev, axis=-1) / jnp.min(pi_stddev, axis=-1)),\n    )\n\n    return loss, stats",
  "class ImpalaTest(absltest.TestCase):\n\n  def test_shapes(self):\n\n    #\n    batch_size = 2\n    sequence_len = 3\n    num_actions = 5\n    hidden_size = 7\n\n    # Define a trivial recurrent actor-critic network.\n    @hk.without_apply_rng\n    @hk.transform\n    def unroll_fn_transformed(observations, state):\n      lstm = hk.LSTM(hidden_size)\n      embedding, state = hk.dynamic_unroll(lstm, observations, state)\n      logits = hk.Linear(num_actions)(embedding)\n      values = jnp.squeeze(hk.Linear(1)(embedding), axis=-1)\n\n      return (logits, values), state\n\n    @hk.without_apply_rng\n    @hk.transform\n    def initial_state_fn():\n      return hk.LSTM(hidden_size).initial_state(None)\n\n    # Initial recurrent network state.\n    initial_state = initial_state_fn.apply(None)\n\n    # Make some fake data.\n    observations = np.ones(shape=(sequence_len, 50))\n    actions = np.random.randint(num_actions, size=sequence_len)\n    rewards = np.random.rand(sequence_len)\n    discounts = np.ones(shape=(sequence_len,))\n\n    batch_tile = tree_map(lambda x: np.tile(x, [batch_size, *([1] * x.ndim)]))\n    seq_tile = tree_map(lambda x: np.tile(x, [sequence_len, *([1] * x.ndim)]))\n\n    extras = {\n        'logits': np.random.rand(sequence_len, num_actions),\n        'core_state': seq_tile(initial_state),\n    }\n\n    # Package up the data into a ReverbSample.\n    data = adders.Step(\n        observations,\n        actions,\n        rewards,\n        discounts,\n        extras=extras,\n        start_of_episode=())\n    data = batch_tile(data)\n    sample = reverb.ReplaySample(info=None, data=data)\n\n    # Initialise parameters.\n    rng = hk.PRNGSequence(1)\n    params = unroll_fn_transformed.init(next(rng), observations, initial_state)\n\n    # Make loss function.\n    loss_fn = impala.impala_loss(\n        unroll_fn_transformed.apply, discount=0.99)\n\n    # Return value should be scalar.\n    loss, metrics = loss_fn(params, sample)\n    loss = jax.device_get(loss)\n    self.assertEqual(loss.shape, ())\n    for value in metrics.values():\n      value = jax.device_get(value)\n      self.assertEqual(value.shape, ())",
  "def test_shapes(self):\n\n    #\n    batch_size = 2\n    sequence_len = 3\n    num_actions = 5\n    hidden_size = 7\n\n    # Define a trivial recurrent actor-critic network.\n    @hk.without_apply_rng\n    @hk.transform\n    def unroll_fn_transformed(observations, state):\n      lstm = hk.LSTM(hidden_size)\n      embedding, state = hk.dynamic_unroll(lstm, observations, state)\n      logits = hk.Linear(num_actions)(embedding)\n      values = jnp.squeeze(hk.Linear(1)(embedding), axis=-1)\n\n      return (logits, values), state\n\n    @hk.without_apply_rng\n    @hk.transform\n    def initial_state_fn():\n      return hk.LSTM(hidden_size).initial_state(None)\n\n    # Initial recurrent network state.\n    initial_state = initial_state_fn.apply(None)\n\n    # Make some fake data.\n    observations = np.ones(shape=(sequence_len, 50))\n    actions = np.random.randint(num_actions, size=sequence_len)\n    rewards = np.random.rand(sequence_len)\n    discounts = np.ones(shape=(sequence_len,))\n\n    batch_tile = tree_map(lambda x: np.tile(x, [batch_size, *([1] * x.ndim)]))\n    seq_tile = tree_map(lambda x: np.tile(x, [sequence_len, *([1] * x.ndim)]))\n\n    extras = {\n        'logits': np.random.rand(sequence_len, num_actions),\n        'core_state': seq_tile(initial_state),\n    }\n\n    # Package up the data into a ReverbSample.\n    data = adders.Step(\n        observations,\n        actions,\n        rewards,\n        discounts,\n        extras=extras,\n        start_of_episode=())\n    data = batch_tile(data)\n    sample = reverb.ReplaySample(info=None, data=data)\n\n    # Initialise parameters.\n    rng = hk.PRNGSequence(1)\n    params = unroll_fn_transformed.init(next(rng), observations, initial_state)\n\n    # Make loss function.\n    loss_fn = impala.impala_loss(\n        unroll_fn_transformed.apply, discount=0.99)\n\n    # Return value should be scalar.\n    loss, metrics = loss_fn(params, sample)\n    loss = jax.device_get(loss)\n    self.assertEqual(loss.shape, ())\n    for value in metrics.values():\n      value = jax.device_get(value)\n      self.assertEqual(value.shape, ())",
  "def unroll_fn_transformed(observations, state):\n      lstm = hk.LSTM(hidden_size)\n      embedding, state = hk.dynamic_unroll(lstm, observations, state)\n      logits = hk.Linear(num_actions)(embedding)\n      values = jnp.squeeze(hk.Linear(1)(embedding), axis=-1)\n\n      return (logits, values), state",
  "def initial_state_fn():\n      return hk.LSTM(hidden_size).initial_state(None)",
  "def impala_loss(\n    unroll_fn: types.PolicyValueFn,\n    *,\n    discount: float,\n    max_abs_reward: float = np.inf,\n    baseline_cost: float = 1.0,\n    entropy_cost: float = 0.0,\n) -> Callable[[hk.Params, reverb.ReplaySample], jax.Array]:\n  \"\"\"Builds the standard entropy-regularised IMPALA loss function.\n\n  Args:\n    unroll_fn: A `hk.Transformed` object containing a callable which maps\n      (params, observations_sequence, initial_state) -> ((logits, value), state)\n    discount: The standard geometric discount rate to apply.\n    max_abs_reward: Optional symmetric reward clipping to apply.\n    baseline_cost: Weighting of the critic loss relative to the policy loss.\n    entropy_cost: Weighting of the entropy regulariser relative to policy loss.\n\n  Returns:\n    A loss function with signature (params, data) -> (loss_scalar, metrics).\n  \"\"\"\n\n  def loss_fn(\n      params: hk.Params,\n      sample: reverb.ReplaySample,\n  ) -> Tuple[jax.Array, Mapping[str, jax.Array]]:\n    \"\"\"Batched, entropy-regularised actor-critic loss with V-trace.\"\"\"\n\n    # Extract the data.\n    data = sample.data\n    observations, actions, rewards, discounts, extra = (data.observation,\n                                                        data.action,\n                                                        data.reward,\n                                                        data.discount,\n                                                        data.extras)\n    initial_state = tree.map_structure(lambda s: s[0], extra['core_state'])\n    behaviour_logits = extra['logits']\n\n    # Apply reward clipping.\n    rewards = jnp.clip(rewards, -max_abs_reward, max_abs_reward)\n\n    # Unroll current policy over observations.\n    (logits, values), _ = unroll_fn(params, observations, initial_state)\n\n    # Compute importance sampling weights: current policy / behavior policy.\n    rhos = rlax.categorical_importance_sampling_ratios(logits[:-1],\n                                                       behaviour_logits[:-1],\n                                                       actions[:-1])\n\n    # Critic loss.\n    vtrace_returns = rlax.vtrace_td_error_and_advantage(\n        v_tm1=values[:-1],\n        v_t=values[1:],\n        r_t=rewards[:-1],\n        discount_t=discounts[:-1] * discount,\n        rho_tm1=rhos)\n    critic_loss = jnp.square(vtrace_returns.errors)\n\n    # Policy gradient loss.\n    policy_gradient_loss = rlax.policy_gradient_loss(\n        logits_t=logits[:-1],\n        a_t=actions[:-1],\n        adv_t=vtrace_returns.pg_advantage,\n        w_t=jnp.ones_like(rewards[:-1]))\n\n    # Entropy regulariser.\n    entropy_loss = rlax.entropy_loss(logits[:-1], jnp.ones_like(rewards[:-1]))\n\n    # Combine weighted sum of actor & critic losses, averaged over the sequence.\n    mean_loss = jnp.mean(policy_gradient_loss + baseline_cost * critic_loss +\n                         entropy_cost * entropy_loss)  # []\n\n    metrics = {\n        'policy_loss': jnp.mean(policy_gradient_loss),\n        'critic_loss': jnp.mean(baseline_cost * critic_loss),\n        'entropy_loss': jnp.mean(entropy_cost * entropy_loss),\n        'entropy': jnp.mean(entropy_loss),\n    }\n\n    return mean_loss, metrics\n\n  return utils.mapreduce(loss_fn, in_axes=(None, 0))",
  "def loss_fn(\n      params: hk.Params,\n      sample: reverb.ReplaySample,\n  ) -> Tuple[jax.Array, Mapping[str, jax.Array]]:\n    \"\"\"Batched, entropy-regularised actor-critic loss with V-trace.\"\"\"\n\n    # Extract the data.\n    data = sample.data\n    observations, actions, rewards, discounts, extra = (data.observation,\n                                                        data.action,\n                                                        data.reward,\n                                                        data.discount,\n                                                        data.extras)\n    initial_state = tree.map_structure(lambda s: s[0], extra['core_state'])\n    behaviour_logits = extra['logits']\n\n    # Apply reward clipping.\n    rewards = jnp.clip(rewards, -max_abs_reward, max_abs_reward)\n\n    # Unroll current policy over observations.\n    (logits, values), _ = unroll_fn(params, observations, initial_state)\n\n    # Compute importance sampling weights: current policy / behavior policy.\n    rhos = rlax.categorical_importance_sampling_ratios(logits[:-1],\n                                                       behaviour_logits[:-1],\n                                                       actions[:-1])\n\n    # Critic loss.\n    vtrace_returns = rlax.vtrace_td_error_and_advantage(\n        v_tm1=values[:-1],\n        v_t=values[1:],\n        r_t=rewards[:-1],\n        discount_t=discounts[:-1] * discount,\n        rho_tm1=rhos)\n    critic_loss = jnp.square(vtrace_returns.errors)\n\n    # Policy gradient loss.\n    policy_gradient_loss = rlax.policy_gradient_loss(\n        logits_t=logits[:-1],\n        a_t=actions[:-1],\n        adv_t=vtrace_returns.pg_advantage,\n        w_t=jnp.ones_like(rewards[:-1]))\n\n    # Entropy regulariser.\n    entropy_loss = rlax.entropy_loss(logits[:-1], jnp.ones_like(rewards[:-1]))\n\n    # Combine weighted sum of actor & critic losses, averaged over the sequence.\n    mean_loss = jnp.mean(policy_gradient_loss + baseline_cost * critic_loss +\n                         entropy_cost * entropy_loss)  # []\n\n    metrics = {\n        'policy_loss': jnp.mean(policy_gradient_loss),\n        'critic_loss': jnp.mean(baseline_cost * critic_loss),\n        'entropy_loss': jnp.mean(entropy_cost * entropy_loss),\n        'entropy': jnp.mean(entropy_loss),\n    }\n\n    return mean_loss, metrics",
  "def run_offline_experiment(experiment: config.OfflineExperimentConfig,\n                           eval_every: int = 100,\n                           num_eval_episodes: int = 1):\n  \"\"\"Runs a simple, single-threaded training loop using the default evaluators.\n\n  It targets simplicity of the code and so only the basic features of the\n  OfflineExperimentConfig are supported.\n\n  Arguments:\n    experiment: Definition and configuration of the agent to run.\n    eval_every: After how many learner steps to perform evaluation.\n    num_eval_episodes: How many evaluation episodes to execute at each\n      evaluation step.\n  \"\"\"\n\n  key = jax.random.PRNGKey(experiment.seed)\n\n  # Create the environment and get its spec.\n  environment = experiment.environment_factory(experiment.seed)\n  environment_spec = experiment.environment_spec or specs.make_environment_spec(\n      environment)\n\n  # Create the networks and policy.\n  networks = experiment.network_factory(environment_spec)\n\n  # Parent counter allows to share step counts between train and eval loops and\n  # the learner, so that it is possible to plot for example evaluator's return\n  # value as a function of the number of training episodes.\n  parent_counter = counting.Counter(time_delta=0.)\n\n  # Create the demonstrations dataset.\n  dataset_key, key = jax.random.split(key)\n  dataset = experiment.demonstration_dataset_factory(dataset_key)\n\n  # Create the learner.\n  learner_key, key = jax.random.split(key)\n  learner = experiment.builder.make_learner(\n      random_key=learner_key,\n      networks=networks,\n      dataset=dataset,\n      logger_fn=experiment.logger_factory,\n      environment_spec=environment_spec,\n      counter=counting.Counter(parent_counter, prefix='learner', time_delta=0.))\n\n  # Define the evaluation loop.\n  eval_loop = None\n  if num_eval_episodes > 0:\n    # Create the evaluation actor and loop.\n    eval_counter = counting.Counter(\n        parent_counter, prefix='evaluator', time_delta=0.)\n    eval_logger = experiment.logger_factory('evaluator',\n                                            eval_counter.get_steps_key(), 0)\n    eval_key, key = jax.random.split(key)\n    eval_actor = experiment.builder.make_actor(\n        random_key=eval_key,\n        policy=experiment.builder.make_policy(networks, environment_spec, True),\n        environment_spec=environment_spec,\n        variable_source=learner)\n    eval_loop = acme.EnvironmentLoop(\n        environment,\n        eval_actor,\n        counter=eval_counter,\n        logger=eval_logger,\n        observers=experiment.observers)\n\n  checkpointer = None\n  if experiment.checkpointing is not None:\n    checkpointing = experiment.checkpointing\n    checkpointer = savers.Checkpointer(\n        objects_to_save={'learner': learner, 'counter': parent_counter},\n        time_delta_minutes=checkpointing.time_delta_minutes,\n        directory=checkpointing.directory,\n        add_uid=checkpointing.add_uid,\n        max_to_keep=checkpointing.max_to_keep,\n        keep_checkpoint_every_n_hours=checkpointing.keep_checkpoint_every_n_hours,\n        checkpoint_ttl_seconds=checkpointing.checkpoint_ttl_seconds,\n    )\n\n  max_num_learner_steps = (\n      experiment.max_num_learner_steps -\n      parent_counter.get_counts().get('learner_steps', 0))\n\n  # Run the training loop.\n  if eval_loop:\n    eval_loop.run(num_eval_episodes)\n  steps = 0\n  while steps < max_num_learner_steps:\n    learner_steps = min(eval_every, max_num_learner_steps - steps)\n    for _ in range(learner_steps):\n      learner.step()\n      if checkpointer is not None:\n        checkpointer.save()\n    if eval_loop:\n      eval_loop.run(num_eval_episodes)\n    steps += learner_steps",
  "class RunOfflineExperimentTest(test_utils.TestCase):\n\n  @parameterized.named_parameters(\n      dict(testcase_name='noeval', num_eval_episodes=0),\n      dict(testcase_name='eval', num_eval_episodes=1))\n  def test_checkpointing(self, num_eval_episodes: int):\n    num_learner_steps = 100\n\n    experiment_config = self._get_experiment_config(\n        num_learner_steps=num_learner_steps)\n\n    experiments.run_offline_experiment(\n        experiment_config, num_eval_episodes=num_eval_episodes)\n\n    checkpoint_counter = experiment_test_utils.restore_counter(\n        experiment_config.checkpointing)\n    self.assertIn('learner_steps', checkpoint_counter.get_counts())\n    self.assertGreater(checkpoint_counter.get_counts()['learner_steps'], 0)\n\n    # Run the second experiment with the same checkpointing config to verify\n    # that it restores from the latest saved checkpoint.\n    experiments.run_offline_experiment(\n        experiment_config, num_eval_episodes=num_eval_episodes)\n\n    checkpoint_counter = experiment_test_utils.restore_counter(\n        experiment_config.checkpointing)\n    self.assertIn('learner_steps', checkpoint_counter.get_counts())\n    # Verify that the steps done in the first run are taken into account.\n    self.assertLessEqual(checkpoint_counter.get_counts()['learner_steps'],\n                         num_learner_steps)\n\n  def test_eval_every(self):\n    num_learner_steps = 100\n\n    experiment_config = self._get_experiment_config(\n        num_learner_steps=num_learner_steps)\n\n    experiments.run_offline_experiment(\n        experiment_config, eval_every=70, num_eval_episodes=1)\n\n    checkpoint_counter = experiment_test_utils.restore_counter(\n        experiment_config.checkpointing)\n    self.assertIn('learner_steps', checkpoint_counter.get_counts())\n    self.assertGreater(checkpoint_counter.get_counts()['learner_steps'], 0)\n    self.assertLessEqual(checkpoint_counter.get_counts()['learner_steps'],\n                         num_learner_steps)\n\n  def _get_experiment_config(\n      self, *, num_learner_steps: int) -> experiments.OfflineExperimentConfig:\n    def environment_factory(seed: int) -> dm_env.Environment:\n      del seed\n      return fakes.ContinuousEnvironment(\n          episode_length=10, action_dim=3, observation_dim=5)\n\n    environment = environment_factory(seed=1)\n    environment_spec = specs.make_environment_spec(environment)\n\n    def demonstration_dataset_factory(\n        random_key: jax_types.PRNGKey) -> Iterator[types.Transition]:\n      del random_key\n      batch_size = 64\n      return fakes.transition_iterator_from_spec(environment_spec)(batch_size)\n\n    crr_config = crr.CRRConfig()\n    crr_builder = crr.CRRBuilder(\n        crr_config, policy_loss_coeff_fn=crr.policy_loss_coeff_advantage_exp)\n    checkpointing_config = experiments.CheckpointingConfig(\n        directory=self.get_tempdir(), time_delta_minutes=0)\n    return experiments.OfflineExperimentConfig(\n        builder=crr_builder,\n        network_factory=crr.make_networks,\n        demonstration_dataset_factory=demonstration_dataset_factory,\n        environment_factory=environment_factory,\n        max_num_learner_steps=num_learner_steps,\n        seed=0,\n        environment_spec=environment_spec,\n        checkpointing=checkpointing_config,\n    )",
  "def test_checkpointing(self, num_eval_episodes: int):\n    num_learner_steps = 100\n\n    experiment_config = self._get_experiment_config(\n        num_learner_steps=num_learner_steps)\n\n    experiments.run_offline_experiment(\n        experiment_config, num_eval_episodes=num_eval_episodes)\n\n    checkpoint_counter = experiment_test_utils.restore_counter(\n        experiment_config.checkpointing)\n    self.assertIn('learner_steps', checkpoint_counter.get_counts())\n    self.assertGreater(checkpoint_counter.get_counts()['learner_steps'], 0)\n\n    # Run the second experiment with the same checkpointing config to verify\n    # that it restores from the latest saved checkpoint.\n    experiments.run_offline_experiment(\n        experiment_config, num_eval_episodes=num_eval_episodes)\n\n    checkpoint_counter = experiment_test_utils.restore_counter(\n        experiment_config.checkpointing)\n    self.assertIn('learner_steps', checkpoint_counter.get_counts())\n    # Verify that the steps done in the first run are taken into account.\n    self.assertLessEqual(checkpoint_counter.get_counts()['learner_steps'],\n                         num_learner_steps)",
  "def test_eval_every(self):\n    num_learner_steps = 100\n\n    experiment_config = self._get_experiment_config(\n        num_learner_steps=num_learner_steps)\n\n    experiments.run_offline_experiment(\n        experiment_config, eval_every=70, num_eval_episodes=1)\n\n    checkpoint_counter = experiment_test_utils.restore_counter(\n        experiment_config.checkpointing)\n    self.assertIn('learner_steps', checkpoint_counter.get_counts())\n    self.assertGreater(checkpoint_counter.get_counts()['learner_steps'], 0)\n    self.assertLessEqual(checkpoint_counter.get_counts()['learner_steps'],\n                         num_learner_steps)",
  "def _get_experiment_config(\n      self, *, num_learner_steps: int) -> experiments.OfflineExperimentConfig:\n    def environment_factory(seed: int) -> dm_env.Environment:\n      del seed\n      return fakes.ContinuousEnvironment(\n          episode_length=10, action_dim=3, observation_dim=5)\n\n    environment = environment_factory(seed=1)\n    environment_spec = specs.make_environment_spec(environment)\n\n    def demonstration_dataset_factory(\n        random_key: jax_types.PRNGKey) -> Iterator[types.Transition]:\n      del random_key\n      batch_size = 64\n      return fakes.transition_iterator_from_spec(environment_spec)(batch_size)\n\n    crr_config = crr.CRRConfig()\n    crr_builder = crr.CRRBuilder(\n        crr_config, policy_loss_coeff_fn=crr.policy_loss_coeff_advantage_exp)\n    checkpointing_config = experiments.CheckpointingConfig(\n        directory=self.get_tempdir(), time_delta_minutes=0)\n    return experiments.OfflineExperimentConfig(\n        builder=crr_builder,\n        network_factory=crr.make_networks,\n        demonstration_dataset_factory=demonstration_dataset_factory,\n        environment_factory=environment_factory,\n        max_num_learner_steps=num_learner_steps,\n        seed=0,\n        environment_spec=environment_spec,\n        checkpointing=checkpointing_config,\n    )",
  "def environment_factory(seed: int) -> dm_env.Environment:\n      del seed\n      return fakes.ContinuousEnvironment(\n          episode_length=10, action_dim=3, observation_dim=5)",
  "def demonstration_dataset_factory(\n        random_key: jax_types.PRNGKey) -> Iterator[types.Transition]:\n      del random_key\n      batch_size = 64\n      return fakes.transition_iterator_from_spec(environment_spec)(batch_size)",
  "def make_distributed_experiment(\n    experiment: config.ExperimentConfig[builders.Networks, Any, Any],\n    num_actors: int,\n    *,\n    inference_server_config: Optional[\n        inference_server_lib.InferenceServerConfig\n    ] = None,\n    num_learner_nodes: int = 1,\n    num_actors_per_node: int = 1,\n    num_inference_servers: int = 1,\n    multiprocessing_colocate_actors: bool = False,\n    multithreading_colocate_learner_and_reverb: bool = False,\n    make_snapshot_models: Optional[\n        config.SnapshotModelFactory[builders.Networks]\n    ] = None,\n    name: str = 'agent',\n    program: Optional[lp.Program] = None,\n) -> lp.Program:\n  \"\"\"Builds a Launchpad program for running the experiment.\n\n  Args:\n    experiment: configuration of the experiment.\n    num_actors: number of actors to run.\n    inference_server_config: If provided we will attempt to use\n      `num_inference_servers` inference servers for selecting actions.\n      There are two assumptions if this config is provided:\n      1) The experiment's policy is an `ActorCore` and a\n      `TypeError` will be raised if not.\n      2) The `ActorCore`'s `select_action` method runs on\n      unbatched observations.\n    num_learner_nodes: number of learner nodes to run. When using multiple\n      learner nodes, make sure the learner class does the appropriate pmap/pmean\n      operations on the loss/gradients, respectively.\n    num_actors_per_node: number of actors per one program node. Actors within\n      one node are colocated in one or multiple processes depending on the value\n      of multiprocessing_colocate_actors.\n    num_inference_servers: number of inference servers to serve actors. (Only\n      used if `inference_server_config` is provided.)\n    multiprocessing_colocate_actors: whether to colocate actor nodes as\n      subprocesses on a single machine. False by default, which means colocate\n      within a single process.\n    multithreading_colocate_learner_and_reverb: whether to colocate the learner\n      and reverb nodes in one process. Not supported if the learner is spread\n      across multiple nodes (num_learner_nodes > 1). False by default, which\n      means no colocation.\n    make_snapshot_models: a factory that defines what is saved in snapshots.\n    name: name of the constructed program. Ignored if an existing program is\n      passed.\n    program: a program where agent nodes are added to. If None, a new program is\n      created.\n\n  Returns:\n    The Launchpad program with all the nodes needed for running the experiment.\n  \"\"\"\n\n  if multithreading_colocate_learner_and_reverb and num_learner_nodes > 1:\n    raise ValueError(\n        'Replay and learner colocation is not yet supported when the learner is'\n        ' spread across multiple nodes (num_learner_nodes > 1). Please contact'\n        ' Acme devs if this is a feature you want. Got:'\n        '\\tmultithreading_colocate_learner_and_reverb='\n        f'{multithreading_colocate_learner_and_reverb}'\n        f'\\tnum_learner_nodes={num_learner_nodes}.')\n\n\n  def build_replay():\n    \"\"\"The replay storage.\"\"\"\n    dummy_seed = 1\n    spec = (\n        experiment.environment_spec or\n        specs.make_environment_spec(experiment.environment_factory(dummy_seed)))\n    network = experiment.network_factory(spec)\n    policy = config.make_policy(\n        experiment=experiment,\n        networks=network,\n        environment_spec=spec,\n        evaluation=False)\n    return experiment.builder.make_replay_tables(spec, policy)\n\n  def build_model_saver(variable_source: core.VariableSource):\n    assert experiment.checkpointing\n    environment = experiment.environment_factory(0)\n    spec = specs.make_environment_spec(environment)\n    networks = experiment.network_factory(spec)\n    models = make_snapshot_models(networks, spec)\n    # TODO(raveman): Decouple checkpointing and snapshotting configs.\n    return snapshotter.JAXSnapshotter(\n        variable_source=variable_source,\n        models=models,\n        path=experiment.checkpointing.directory,\n        subdirectory='snapshots',\n        add_uid=experiment.checkpointing.add_uid)\n\n  def build_counter():\n    counter = counting.Counter()\n    if experiment.checkpointing:\n      checkpointing = experiment.checkpointing\n      counter = savers.CheckpointingRunner(\n          counter,\n          key='counter',\n          subdirectory='counter',\n          time_delta_minutes=checkpointing.time_delta_minutes,\n          directory=checkpointing.directory,\n          add_uid=checkpointing.add_uid,\n          max_to_keep=checkpointing.max_to_keep,\n          keep_checkpoint_every_n_hours=checkpointing.keep_checkpoint_every_n_hours,\n          checkpoint_ttl_seconds=checkpointing.checkpoint_ttl_seconds,\n      )\n    return counter\n\n  def build_learner(\n      random_key: networks_lib.PRNGKey,\n      replay: reverb.Client,\n      counter: Optional[counting.Counter] = None,\n      primary_learner: Optional[core.Learner] = None,\n  ):\n    \"\"\"The Learning part of the agent.\"\"\"\n\n    dummy_seed = 1\n    spec = (\n        experiment.environment_spec or\n        specs.make_environment_spec(experiment.environment_factory(dummy_seed)))\n\n    # Creates the networks to optimize (online) and target networks.\n    networks = experiment.network_factory(spec)\n\n    iterator = experiment.builder.make_dataset_iterator(replay)\n    # make_dataset_iterator is responsible for putting data onto appropriate\n    # training devices, so here we apply prefetch, so that data is copied over\n    # in the background.\n    iterator = utils.prefetch(iterable=iterator, buffer_size=1)\n    counter = counting.Counter(counter, 'learner')\n    learner = experiment.builder.make_learner(random_key, networks, iterator,\n                                              experiment.logger_factory, spec,\n                                              replay, counter)\n\n    if experiment.checkpointing:\n      if primary_learner is None:\n        checkpointing = experiment.checkpointing\n        learner = savers.CheckpointingRunner(\n            learner,\n            key='learner',\n            subdirectory='learner',\n            time_delta_minutes=5,\n            directory=checkpointing.directory,\n            add_uid=checkpointing.add_uid,\n            max_to_keep=checkpointing.max_to_keep,\n            keep_checkpoint_every_n_hours=checkpointing.keep_checkpoint_every_n_hours,\n            checkpoint_ttl_seconds=checkpointing.checkpoint_ttl_seconds,\n        )\n      else:\n        learner.restore(primary_learner.save())\n        # NOTE: This initially synchronizes secondary learner states with the\n        # primary one. Further synchronization should be handled by the learner\n        # properly doing a pmap/pmean on the loss/gradients, respectively.\n\n    return learner\n\n  def build_inference_server(\n      inference_server_config: inference_server_lib.InferenceServerConfig,\n      variable_source: core.VariableSource,\n  ) -> InferenceServer:\n    \"\"\"Builds an inference server for `ActorCore` policies.\"\"\"\n    dummy_seed = 1\n    spec = (\n        experiment.environment_spec or\n        specs.make_environment_spec(experiment.environment_factory(dummy_seed)))\n    networks = experiment.network_factory(spec)\n    policy = config.make_policy(\n        experiment=experiment,\n        networks=networks,\n        environment_spec=spec,\n        evaluation=False,\n    )\n    if not isinstance(policy, actor_core.ActorCore):\n      raise TypeError(\n          f'Using InferenceServer with policy of unsupported type:'\n          f'{type(policy)}. InferenceServer only supports `ActorCore` policies.'\n      )\n\n    return InferenceServer(\n        handler=jax.jit(\n            jax.vmap(\n                policy.select_action,\n                in_axes=(None, 0, 0),\n                # Note on in_axes: Params will not be batched. Only the\n                # observations and actor state will be stacked along a new\n                # leading axis by the inference server.\n            ),),\n        variable_source=variable_source,\n        devices=jax.local_devices(),\n        config=inference_server_config,\n    )\n\n  def build_actor(\n      random_key: networks_lib.PRNGKey,\n      replay: reverb.Client,\n      variable_source: core.VariableSource,\n      counter: counting.Counter,\n      actor_id: ActorId,\n      inference_server: Optional[InferenceServer],\n  ) -> environment_loop.EnvironmentLoop:\n    \"\"\"The actor process.\"\"\"\n    environment_key, actor_key = jax.random.split(random_key)\n    # Create environment and policy core.\n\n    # Environments normally require uint32 as a seed.\n    environment = experiment.environment_factory(\n        utils.sample_uint32(environment_key))\n    environment_spec = specs.make_environment_spec(environment)\n\n    networks = experiment.network_factory(environment_spec)\n    policy_network = config.make_policy(\n        experiment=experiment,\n        networks=networks,\n        environment_spec=environment_spec,\n        evaluation=False)\n    if inference_server is not None:\n      policy_network = actor_core.ActorCore(\n          init=policy_network.init,\n          select_action=inference_server.handler,\n          get_extras=policy_network.get_extras,\n      )\n      variable_source = variable_utils.ReferenceVariableSource()\n\n    adder = experiment.builder.make_adder(replay, environment_spec,\n                                          policy_network)\n    actor = experiment.builder.make_actor(actor_key, policy_network,\n                                          environment_spec, variable_source,\n                                          adder)\n\n    # Create logger and counter.\n    counter = counting.Counter(counter, 'actor')\n    logger = experiment.logger_factory('actor', counter.get_steps_key(),\n                                       actor_id)\n    # Create the loop to connect environment and agent.\n    return environment_loop.EnvironmentLoop(\n        environment, actor, counter, logger, observers=experiment.observers)\n\n  if not program:\n    program = lp.Program(name=name)\n\n  key = jax.random.PRNGKey(experiment.seed)\n\n  checkpoint_time_delta_minutes: Optional[int] = (\n      experiment.checkpointing.replay_checkpointing_time_delta_minutes\n      if experiment.checkpointing else None)\n  replay_node = lp.ReverbNode(\n      build_replay, checkpoint_time_delta_minutes=checkpoint_time_delta_minutes)\n  replay = replay_node.create_handle()\n\n  counter = program.add_node(lp.CourierNode(build_counter), label='counter')\n\n  if experiment.max_num_actor_steps is not None:\n    program.add_node(\n        lp.CourierNode(lp_utils.StepsLimiter, counter,\n                       experiment.max_num_actor_steps),\n        label='counter')\n\n  learner_key, key = jax.random.split(key)\n  learner_node = lp.CourierNode(build_learner, learner_key, replay, counter)\n  learner = learner_node.create_handle()\n  variable_sources = [learner]\n\n  if multithreading_colocate_learner_and_reverb:\n    program.add_node(\n        lp.MultiThreadingColocation([learner_node, replay_node]),\n        label='learner')\n  else:\n    program.add_node(replay_node, label='replay')\n\n    with program.group('learner'):\n      program.add_node(learner_node)\n\n      # Maybe create secondary learners, necessary when using multi-host\n      # accelerators.\n      # Warning! If you set num_learner_nodes > 1, make sure the learner class\n      # does the appropriate pmap/pmean operations on the loss/gradients,\n      # respectively.\n      for _ in range(1, num_learner_nodes):\n        learner_key, key = jax.random.split(key)\n        variable_sources.append(\n            program.add_node(\n                lp.CourierNode(\n                    build_learner, learner_key, replay,\n                    primary_learner=learner)))\n        # NOTE: Secondary learners are used to load-balance get_variables calls,\n        # which is why they get added to the list of available variable sources.\n        # NOTE: Only the primary learner checkpoints.\n        # NOTE: Do not pass the counter to the secondary learners to avoid\n        # double counting of learner steps.\n\n  if inference_server_config is not None:\n    num_actors_per_server = math.ceil(num_actors / num_inference_servers)\n    with program.group('inference_server'):\n      inference_nodes = []\n      for _ in range(num_inference_servers):\n        inference_nodes.append(\n            program.add_node(\n                lp.CourierNode(\n                    build_inference_server,\n                    inference_server_config,\n                    learner,\n                    courier_kwargs={'thread_pool_size': num_actors_per_server\n                                   })))\n  else:\n    num_inference_servers = 1\n    inference_nodes = [None]\n\n  num_actor_nodes, remainder = divmod(num_actors, num_actors_per_node)\n  num_actor_nodes += int(remainder > 0)\n\n\n  with program.group('actor'):\n    # Create all actor threads.\n    *actor_keys, key = jax.random.split(key, num_actors + 1)\n\n    # Create (maybe colocated) actor nodes.\n    for node_id, variable_source, inference_node in zip(\n        range(num_actor_nodes),\n        itertools.cycle(variable_sources),\n        itertools.cycle(inference_nodes),\n    ):\n      colocation_nodes = []\n\n      first_actor_id = node_id * num_actors_per_node\n      for actor_id in range(\n          first_actor_id, min(first_actor_id + num_actors_per_node, num_actors)\n      ):\n        actor = lp.CourierNode(\n            build_actor,\n            actor_keys[actor_id],\n            replay,\n            variable_source,\n            counter,\n            actor_id,\n            inference_node,\n        )\n        colocation_nodes.append(actor)\n\n      if len(colocation_nodes) == 1:\n        program.add_node(colocation_nodes[0])\n      elif multiprocessing_colocate_actors:\n        program.add_node(lp.MultiProcessingColocation(colocation_nodes))\n      else:\n        program.add_node(lp.MultiThreadingColocation(colocation_nodes))\n\n  for evaluator in experiment.get_evaluator_factories():\n    evaluator_key, key = jax.random.split(key)\n    program.add_node(\n        lp.CourierNode(evaluator, evaluator_key, learner, counter,\n                       experiment.builder.make_actor),\n        label='evaluator')\n\n  if make_snapshot_models and experiment.checkpointing:\n    program.add_node(\n        lp.CourierNode(build_model_saver, learner), label='model_saver')\n\n  return program",
  "def build_replay():\n    \"\"\"The replay storage.\"\"\"\n    dummy_seed = 1\n    spec = (\n        experiment.environment_spec or\n        specs.make_environment_spec(experiment.environment_factory(dummy_seed)))\n    network = experiment.network_factory(spec)\n    policy = config.make_policy(\n        experiment=experiment,\n        networks=network,\n        environment_spec=spec,\n        evaluation=False)\n    return experiment.builder.make_replay_tables(spec, policy)",
  "def build_model_saver(variable_source: core.VariableSource):\n    assert experiment.checkpointing\n    environment = experiment.environment_factory(0)\n    spec = specs.make_environment_spec(environment)\n    networks = experiment.network_factory(spec)\n    models = make_snapshot_models(networks, spec)\n    # TODO(raveman): Decouple checkpointing and snapshotting configs.\n    return snapshotter.JAXSnapshotter(\n        variable_source=variable_source,\n        models=models,\n        path=experiment.checkpointing.directory,\n        subdirectory='snapshots',\n        add_uid=experiment.checkpointing.add_uid)",
  "def build_counter():\n    counter = counting.Counter()\n    if experiment.checkpointing:\n      checkpointing = experiment.checkpointing\n      counter = savers.CheckpointingRunner(\n          counter,\n          key='counter',\n          subdirectory='counter',\n          time_delta_minutes=checkpointing.time_delta_minutes,\n          directory=checkpointing.directory,\n          add_uid=checkpointing.add_uid,\n          max_to_keep=checkpointing.max_to_keep,\n          keep_checkpoint_every_n_hours=checkpointing.keep_checkpoint_every_n_hours,\n          checkpoint_ttl_seconds=checkpointing.checkpoint_ttl_seconds,\n      )\n    return counter",
  "def build_learner(\n      random_key: networks_lib.PRNGKey,\n      replay: reverb.Client,\n      counter: Optional[counting.Counter] = None,\n      primary_learner: Optional[core.Learner] = None,\n  ):\n    \"\"\"The Learning part of the agent.\"\"\"\n\n    dummy_seed = 1\n    spec = (\n        experiment.environment_spec or\n        specs.make_environment_spec(experiment.environment_factory(dummy_seed)))\n\n    # Creates the networks to optimize (online) and target networks.\n    networks = experiment.network_factory(spec)\n\n    iterator = experiment.builder.make_dataset_iterator(replay)\n    # make_dataset_iterator is responsible for putting data onto appropriate\n    # training devices, so here we apply prefetch, so that data is copied over\n    # in the background.\n    iterator = utils.prefetch(iterable=iterator, buffer_size=1)\n    counter = counting.Counter(counter, 'learner')\n    learner = experiment.builder.make_learner(random_key, networks, iterator,\n                                              experiment.logger_factory, spec,\n                                              replay, counter)\n\n    if experiment.checkpointing:\n      if primary_learner is None:\n        checkpointing = experiment.checkpointing\n        learner = savers.CheckpointingRunner(\n            learner,\n            key='learner',\n            subdirectory='learner',\n            time_delta_minutes=5,\n            directory=checkpointing.directory,\n            add_uid=checkpointing.add_uid,\n            max_to_keep=checkpointing.max_to_keep,\n            keep_checkpoint_every_n_hours=checkpointing.keep_checkpoint_every_n_hours,\n            checkpoint_ttl_seconds=checkpointing.checkpoint_ttl_seconds,\n        )\n      else:\n        learner.restore(primary_learner.save())\n        # NOTE: This initially synchronizes secondary learner states with the\n        # primary one. Further synchronization should be handled by the learner\n        # properly doing a pmap/pmean on the loss/gradients, respectively.\n\n    return learner",
  "def build_inference_server(\n      inference_server_config: inference_server_lib.InferenceServerConfig,\n      variable_source: core.VariableSource,\n  ) -> InferenceServer:\n    \"\"\"Builds an inference server for `ActorCore` policies.\"\"\"\n    dummy_seed = 1\n    spec = (\n        experiment.environment_spec or\n        specs.make_environment_spec(experiment.environment_factory(dummy_seed)))\n    networks = experiment.network_factory(spec)\n    policy = config.make_policy(\n        experiment=experiment,\n        networks=networks,\n        environment_spec=spec,\n        evaluation=False,\n    )\n    if not isinstance(policy, actor_core.ActorCore):\n      raise TypeError(\n          f'Using InferenceServer with policy of unsupported type:'\n          f'{type(policy)}. InferenceServer only supports `ActorCore` policies.'\n      )\n\n    return InferenceServer(\n        handler=jax.jit(\n            jax.vmap(\n                policy.select_action,\n                in_axes=(None, 0, 0),\n                # Note on in_axes: Params will not be batched. Only the\n                # observations and actor state will be stacked along a new\n                # leading axis by the inference server.\n            ),),\n        variable_source=variable_source,\n        devices=jax.local_devices(),\n        config=inference_server_config,\n    )",
  "def build_actor(\n      random_key: networks_lib.PRNGKey,\n      replay: reverb.Client,\n      variable_source: core.VariableSource,\n      counter: counting.Counter,\n      actor_id: ActorId,\n      inference_server: Optional[InferenceServer],\n  ) -> environment_loop.EnvironmentLoop:\n    \"\"\"The actor process.\"\"\"\n    environment_key, actor_key = jax.random.split(random_key)\n    # Create environment and policy core.\n\n    # Environments normally require uint32 as a seed.\n    environment = experiment.environment_factory(\n        utils.sample_uint32(environment_key))\n    environment_spec = specs.make_environment_spec(environment)\n\n    networks = experiment.network_factory(environment_spec)\n    policy_network = config.make_policy(\n        experiment=experiment,\n        networks=networks,\n        environment_spec=environment_spec,\n        evaluation=False)\n    if inference_server is not None:\n      policy_network = actor_core.ActorCore(\n          init=policy_network.init,\n          select_action=inference_server.handler,\n          get_extras=policy_network.get_extras,\n      )\n      variable_source = variable_utils.ReferenceVariableSource()\n\n    adder = experiment.builder.make_adder(replay, environment_spec,\n                                          policy_network)\n    actor = experiment.builder.make_actor(actor_key, policy_network,\n                                          environment_spec, variable_source,\n                                          adder)\n\n    # Create logger and counter.\n    counter = counting.Counter(counter, 'actor')\n    logger = experiment.logger_factory('actor', counter.get_steps_key(),\n                                       actor_id)\n    # Create the loop to connect environment and agent.\n    return environment_loop.EnvironmentLoop(\n        environment, actor, counter, logger, observers=experiment.observers)",
  "class MakeActorFn(Protocol, Generic[builders.Policy]):\n\n  def __call__(self, random_key: types.PRNGKey, policy: builders.Policy,\n               environment_spec: specs.EnvironmentSpec,\n               variable_source: core.VariableSource) -> core.Actor:\n    ...",
  "class NetworkFactory(Protocol, Generic[builders.Networks]):\n\n  def __call__(self,\n               environment_spec: specs.EnvironmentSpec) -> builders.Networks:\n    ...",
  "class DeprecatedPolicyFactory(Protocol, Generic[builders.Networks,\n                                                builders.Policy]):\n\n  def __call__(self, networks: builders.Networks) -> builders.Policy:\n    ...",
  "class PolicyFactory(Protocol, Generic[builders.Networks, builders.Policy]):\n\n  def __call__(self, networks: builders.Networks,\n               environment_spec: specs.EnvironmentSpec,\n               evaluation: bool) -> builders.Policy:\n    ...",
  "class EvaluatorFactory(Protocol, Generic[builders.Policy]):\n\n  def __call__(self, random_key: types.PRNGKey,\n               variable_source: core.VariableSource, counter: counting.Counter,\n               make_actor_fn: MakeActorFn[builders.Policy]) -> core.Worker:\n    ...",
  "class SnapshotModelFactory(Protocol, Generic[builders.Networks]):\n\n  def __call__(\n      self, networks: builders.Networks, environment_spec: specs.EnvironmentSpec\n  ) -> Dict[str, Callable[[core.VariableSource], types.ModelToSnapshot]]:\n    ...",
  "class CheckpointingConfig:\n  \"\"\"Configuration options for checkpointing.\n\n  Attributes:\n    max_to_keep: Maximum number of checkpoints to keep. Unless preserved by\n      keep_checkpoint_every_n_hours, checkpoints will be deleted from the active\n      set, oldest first, until only max_to_keep checkpoints remain. Does not\n      apply to replay checkpointing.\n    directory: Where to store the checkpoints.\n    add_uid: Whether or not to add a unique identifier, see\n      `paths.get_unique_id()` for how it is generated.\n    time_delta_minutes: How often to save the checkpoint, in minutes.\n    keep_checkpoint_every_n_hours: Upon removal from the active set, a\n      checkpoint will be preserved if it has been at least\n      keep_checkpoint_every_n_hours since the last preserved checkpoint. The\n      default setting of None does not preserve any checkpoints in this way.\n    replay_checkpointing_time_delta_minutes: How frequently to write replay\n      checkpoints; defaults to None, which disables periodic checkpointing.\n      Warning! These are written asynchronously so as not to interrupt other\n      replay duties, however this does pose a risk of OOM since items that would\n      otherwise be removed are temporarily kept alive for checkpointing\n      purposes.\n      Note: Since replay buffers tend to be quite large O(100GiB), writing can\n        take up to 10 minutes so keep that in mind when setting this frequency.\n    checkpoint_ttl_seconds: TTL (time to leave) in seconds for checkpoints.\n      Indefinite if set to None.\n  \"\"\"\n  max_to_keep: int = 1\n  directory: str = '~/acme'\n  add_uid: bool = True\n  time_delta_minutes: int = 5\n  keep_checkpoint_every_n_hours: Optional[int] = None\n  replay_checkpointing_time_delta_minutes: Optional[int] = None\n  checkpoint_ttl_seconds: Optional[int] = int(\n      datetime.timedelta(days=5).total_seconds()\n  )",
  "class ExperimentConfig(Generic[builders.Networks, builders.Policy,\n                               builders.Sample]):\n  \"\"\"Config which defines aspects of constructing an experiment.\n\n  Attributes:\n    builder: Builds components of an RL agent (Learner, Actor...).\n    network_factory: Builds networks used by the agent.\n    environment_factory: Returns an instance of an environment.\n    max_num_actor_steps: How many environment steps to perform.\n    seed: Seed used for agent initialization.\n    policy_network_factory: Policy network factory which is used actors to\n      perform inference.\n    evaluator_factories: Factories of policy evaluators. When not specified the\n      default evaluators are constructed using eval_policy_network_factory. Set\n      to an empty list to disable evaluators.\n    eval_policy_network_factory: Policy network factory used by evaluators.\n      Should be specified to use the default evaluators (when\n      evaluator_factories is not provided).\n    environment_spec: Specification of the environment. Can be specified to\n      reduce the number of times environment_factory is invoked (for performance\n      or resource usage reasons).\n    observers: Observers used for extending logs with custom information.\n    logger_factory: Loggers factory used to construct loggers for learner,\n      actors and evaluators.\n    checkpointing: Configuration options for checkpointing. If None,\n      checkpointing and snapshotting is disabled.\n  \"\"\"\n  # Below fields must be explicitly specified for any Agent.\n  builder: builders.ActorLearnerBuilder[builders.Networks, builders.Policy,\n                                        builders.Sample]\n  network_factory: NetworkFactory[builders.Networks]\n  environment_factory: types.EnvironmentFactory\n  max_num_actor_steps: int\n  seed: int\n  # policy_network_factory is deprecated. Use builder.make_policy to\n  # create the policy.\n  policy_network_factory: Optional[DeprecatedPolicyFactory[\n      builders.Networks, builders.Policy]] = None\n  # Fields below are optional. If you just started with Acme do not worry about\n  # them. You might need them later when you want to customize your RL agent.\n  # TODO(stanczyk): Introduce a marker for the default value (instead of None).\n  evaluator_factories: Optional[Sequence[EvaluatorFactory[\n      builders.Policy]]] = None\n  # eval_policy_network_factory is deprecated. Use builder.make_policy to\n  # create the policy.\n  eval_policy_network_factory: Optional[DeprecatedPolicyFactory[\n      builders.Networks, builders.Policy]] = None\n  environment_spec: Optional[specs.EnvironmentSpec] = None\n  observers: Sequence[observers_lib.EnvLoopObserver] = ()\n  logger_factory: loggers.LoggerFactory = dataclasses.field(\n      default_factory=experiment_utils.create_experiment_logger_factory)\n  checkpointing: Optional[CheckpointingConfig] = CheckpointingConfig()\n\n  # TODO(stanczyk): Make get_evaluator_factories a standalone function.\n  def get_evaluator_factories(self):\n    \"\"\"Constructs the evaluator factories.\"\"\"\n    if self.evaluator_factories is not None:\n      return self.evaluator_factories\n\n    def eval_policy_factory(networks: builders.Networks,\n                            environment_spec: specs.EnvironmentSpec,\n                            evaluation: bool) -> builders.Policy:\n      del evaluation\n      # The config factory has precedence until all agents are migrated to use\n      # builder.make_policy\n      if self.eval_policy_network_factory is not None:\n        return self.eval_policy_network_factory(networks)\n      else:\n        return self.builder.make_policy(\n            networks=networks,\n            environment_spec=environment_spec,\n            evaluation=True)\n\n    return [\n        default_evaluator_factory(\n            environment_factory=self.environment_factory,\n            network_factory=self.network_factory,\n            policy_factory=eval_policy_factory,\n            logger_factory=self.logger_factory,\n            observers=self.observers)\n    ]",
  "class OfflineExperimentConfig(Generic[builders.Networks, builders.Policy,\n                                      builders.Sample]):\n  \"\"\"Config which defines aspects of constructing an offline RL experiment.\n\n  This class is similar to the ExperimentConfig, but is tailored to offline RL\n  setting, so it excludes attributes related to training via interaction with\n  the environment (max_num_actor_steps, policy_network_factory) and instead\n  includes attributes specific to learning from demonstration.\n\n  Attributes:\n    builder: Builds components of an offline RL agent (Learner and Evaluator).\n    network_factory: Builds networks used by the agent.\n    demonstration_dataset_factory: Function that returns an iterator over\n      demonstrations.\n    environment_spec: Specification of the environment.\n    max_num_learner_steps: How many learner steps to perform.\n    seed: Seed used for agent initialization.\n    evaluator_factories: Factories of policy evaluators. When not specified the\n      default evaluators are constructed using eval_policy_network_factory. Set\n      to an empty list to disable evaluators.\n    eval_policy_factory: Policy factory used by evaluators. Should be specified\n      to use the default evaluators (when evaluator_factories is not provided).\n    environment_factory: Returns an instance of an environment to be used for\n      evaluation. Should be specified to use the default evaluators (when\n      evaluator_factories is not provided).\n    observers: Observers used for extending logs with custom information.\n    logger_factory: Loggers factory used to construct loggers for learner,\n      actors and evaluators.\n    checkpointing: Configuration options for checkpointing. If None,\n      checkpointing and snapshotting is disabled.\n  \"\"\"\n  # Below fields must be explicitly specified for any Agent.\n  builder: builders.OfflineBuilder[builders.Networks, builders.Policy,\n                                   builders.Sample]\n  network_factory: Callable[[specs.EnvironmentSpec], builders.Networks]\n  demonstration_dataset_factory: Callable[[types.PRNGKey],\n                                          Iterator[builders.Sample]]\n  environment_factory: types.EnvironmentFactory\n  max_num_learner_steps: int\n  seed: int\n  # Fields below are optional. If you just started with Acme do not worry about\n  # them. You might need them later when you want to customize your RL agent.\n  # TODO(stanczyk): Introduce a marker for the default value (instead of None).\n  evaluator_factories: Optional[Sequence[EvaluatorFactory]] = None\n  environment_spec: Optional[specs.EnvironmentSpec] = None\n  observers: Sequence[observers_lib.EnvLoopObserver] = ()\n  logger_factory: loggers.LoggerFactory = dataclasses.field(\n      default_factory=experiment_utils.create_experiment_logger_factory)\n  checkpointing: Optional[CheckpointingConfig] = CheckpointingConfig()\n\n  # TODO(stanczyk): Make get_evaluator_factories a standalone function.\n  def get_evaluator_factories(self):\n    \"\"\"Constructs the evaluator factories.\"\"\"\n    if self.evaluator_factories is not None:\n      return self.evaluator_factories\n    if self.environment_factory is None:\n      raise ValueError(\n          'You need to set `environment_factory` in `OfflineExperimentConfig` '\n          'when `evaluator_factories` are not specified. To disable evaluation '\n          'altogether just set `evaluator_factories = []`')\n\n    return [\n        default_evaluator_factory(\n            environment_factory=self.environment_factory,\n            network_factory=self.network_factory,\n            policy_factory=self.builder.make_policy,\n            logger_factory=self.logger_factory,\n            observers=self.observers)\n    ]",
  "def default_evaluator_factory(\n    environment_factory: types.EnvironmentFactory,\n    network_factory: NetworkFactory[builders.Networks],\n    policy_factory: PolicyFactory[builders.Networks, builders.Policy],\n    logger_factory: loggers.LoggerFactory,\n    observers: Sequence[observers_lib.EnvLoopObserver] = (),\n) -> EvaluatorFactory[builders.Policy]:\n  \"\"\"Returns a default evaluator process.\"\"\"\n\n  def evaluator(\n      random_key: types.PRNGKey,\n      variable_source: core.VariableSource,\n      counter: counting.Counter,\n      make_actor: MakeActorFn[builders.Policy],\n  ):\n    \"\"\"The evaluation process.\"\"\"\n\n    # Create environment and evaluator networks\n    environment_key, actor_key = jax.random.split(random_key)\n    # Environments normally require uint32 as a seed.\n    environment = environment_factory(utils.sample_uint32(environment_key))\n    environment_spec = specs.make_environment_spec(environment)\n    networks = network_factory(environment_spec)\n    policy = policy_factory(networks, environment_spec, True)\n    actor = make_actor(actor_key, policy, environment_spec, variable_source)\n\n    # Create logger and counter.\n    counter = counting.Counter(counter, 'evaluator')\n    logger = logger_factory('evaluator', 'actor_steps', 0)\n\n    # Create the run loop and return it.\n    return environment_loop.EnvironmentLoop(\n        environment, actor, counter, logger, observers=observers)\n\n  return evaluator",
  "def make_policy(experiment: ExperimentConfig[builders.Networks, builders.Policy,\n                                             Any], networks: builders.Networks,\n                environment_spec: specs.EnvironmentSpec,\n                evaluation: bool) -> builders.Policy:\n  \"\"\"Constructs a policy. It is only meant to be used internally.\"\"\"\n  # TODO(sabela): remove and update callers once all agents use\n  # builder.make_policy\n  if not evaluation and experiment.policy_network_factory:\n    return experiment.policy_network_factory(networks)\n  if evaluation and experiment.eval_policy_network_factory:\n    return experiment.eval_policy_network_factory(networks)\n  return experiment.builder.make_policy(\n      networks=networks,\n      environment_spec=environment_spec,\n      evaluation=evaluation)",
  "def __call__(self, random_key: types.PRNGKey, policy: builders.Policy,\n               environment_spec: specs.EnvironmentSpec,\n               variable_source: core.VariableSource) -> core.Actor:\n    ...",
  "def __call__(self,\n               environment_spec: specs.EnvironmentSpec) -> builders.Networks:\n    ...",
  "def __call__(self, networks: builders.Networks) -> builders.Policy:\n    ...",
  "def __call__(self, networks: builders.Networks,\n               environment_spec: specs.EnvironmentSpec,\n               evaluation: bool) -> builders.Policy:\n    ...",
  "def __call__(self, random_key: types.PRNGKey,\n               variable_source: core.VariableSource, counter: counting.Counter,\n               make_actor_fn: MakeActorFn[builders.Policy]) -> core.Worker:\n    ...",
  "def __call__(\n      self, networks: builders.Networks, environment_spec: specs.EnvironmentSpec\n  ) -> Dict[str, Callable[[core.VariableSource], types.ModelToSnapshot]]:\n    ...",
  "def get_evaluator_factories(self):\n    \"\"\"Constructs the evaluator factories.\"\"\"\n    if self.evaluator_factories is not None:\n      return self.evaluator_factories\n\n    def eval_policy_factory(networks: builders.Networks,\n                            environment_spec: specs.EnvironmentSpec,\n                            evaluation: bool) -> builders.Policy:\n      del evaluation\n      # The config factory has precedence until all agents are migrated to use\n      # builder.make_policy\n      if self.eval_policy_network_factory is not None:\n        return self.eval_policy_network_factory(networks)\n      else:\n        return self.builder.make_policy(\n            networks=networks,\n            environment_spec=environment_spec,\n            evaluation=True)\n\n    return [\n        default_evaluator_factory(\n            environment_factory=self.environment_factory,\n            network_factory=self.network_factory,\n            policy_factory=eval_policy_factory,\n            logger_factory=self.logger_factory,\n            observers=self.observers)\n    ]",
  "def get_evaluator_factories(self):\n    \"\"\"Constructs the evaluator factories.\"\"\"\n    if self.evaluator_factories is not None:\n      return self.evaluator_factories\n    if self.environment_factory is None:\n      raise ValueError(\n          'You need to set `environment_factory` in `OfflineExperimentConfig` '\n          'when `evaluator_factories` are not specified. To disable evaluation '\n          'altogether just set `evaluator_factories = []`')\n\n    return [\n        default_evaluator_factory(\n            environment_factory=self.environment_factory,\n            network_factory=self.network_factory,\n            policy_factory=self.builder.make_policy,\n            logger_factory=self.logger_factory,\n            observers=self.observers)\n    ]",
  "def evaluator(\n      random_key: types.PRNGKey,\n      variable_source: core.VariableSource,\n      counter: counting.Counter,\n      make_actor: MakeActorFn[builders.Policy],\n  ):\n    \"\"\"The evaluation process.\"\"\"\n\n    # Create environment and evaluator networks\n    environment_key, actor_key = jax.random.split(random_key)\n    # Environments normally require uint32 as a seed.\n    environment = environment_factory(utils.sample_uint32(environment_key))\n    environment_spec = specs.make_environment_spec(environment)\n    networks = network_factory(environment_spec)\n    policy = policy_factory(networks, environment_spec, True)\n    actor = make_actor(actor_key, policy, environment_spec, variable_source)\n\n    # Create logger and counter.\n    counter = counting.Counter(counter, 'evaluator')\n    logger = logger_factory('evaluator', 'actor_steps', 0)\n\n    # Create the run loop and return it.\n    return environment_loop.EnvironmentLoop(\n        environment, actor, counter, logger, observers=observers)",
  "def eval_policy_factory(networks: builders.Networks,\n                            environment_spec: specs.EnvironmentSpec,\n                            evaluation: bool) -> builders.Policy:\n      del evaluation\n      # The config factory has precedence until all agents are migrated to use\n      # builder.make_policy\n      if self.eval_policy_network_factory is not None:\n        return self.eval_policy_network_factory(networks)\n      else:\n        return self.builder.make_policy(\n            networks=networks,\n            environment_spec=environment_spec,\n            evaluation=True)",
  "def restore_counter(\n    checkpointing_config: experiments.CheckpointingConfig) -> counting.Counter:\n  \"\"\"Restores a counter from the latest checkpoint saved with this config.\"\"\"\n  counter = counting.Counter()\n  savers.Checkpointer(\n      objects_to_save={'counter': counter},\n      directory=checkpointing_config.directory,\n      add_uid=checkpointing_config.add_uid,\n      max_to_keep=checkpointing_config.max_to_keep)\n  return counter",
  "class RunExperimentTest(test_utils.TestCase):\n\n  @parameterized.named_parameters(\n      dict(testcase_name='noeval', num_eval_episodes=0),\n      dict(testcase_name='eval', num_eval_episodes=1))\n  def test_checkpointing(self, num_eval_episodes: int):\n    num_train_steps = 100\n    experiment_config = self._get_experiment_config(\n        num_train_steps=num_train_steps)\n\n    experiments.run_experiment(\n        experiment_config, eval_every=10, num_eval_episodes=num_eval_episodes)\n\n    checkpoint_counter = experiment_test_utils.restore_counter(\n        experiment_config.checkpointing)\n    self.assertIn('actor_steps', checkpoint_counter.get_counts())\n    self.assertGreater(checkpoint_counter.get_counts()['actor_steps'], 0)\n\n    # Run the second experiment with the same checkpointing config to verify\n    # that it restores from the latest saved checkpoint.\n    experiments.run_experiment(\n        experiment_config, eval_every=50, num_eval_episodes=num_eval_episodes)\n\n    checkpoint_counter = experiment_test_utils.restore_counter(\n        experiment_config.checkpointing)\n    self.assertIn('actor_steps', checkpoint_counter.get_counts())\n    # Verify that the steps done in the first run are taken into account.\n    self.assertLessEqual(checkpoint_counter.get_counts()['actor_steps'],\n                         num_train_steps)\n\n  def test_eval_every(self):\n    num_train_steps = 100\n    experiment_config = self._get_experiment_config(\n        num_train_steps=num_train_steps)\n\n    experiments.run_experiment(\n        experiment_config, eval_every=70, num_eval_episodes=1)\n\n    checkpoint_counter = experiment_test_utils.restore_counter(\n        experiment_config.checkpointing)\n    self.assertIn('actor_steps', checkpoint_counter.get_counts())\n    self.assertGreater(checkpoint_counter.get_counts()['actor_steps'], 0)\n    self.assertLessEqual(checkpoint_counter.get_counts()['actor_steps'],\n                         num_train_steps)\n\n  def _get_experiment_config(\n      self, *, num_train_steps: int) -> experiments.ExperimentConfig:\n    \"\"\"Returns a config for a test experiment with the given number of steps.\"\"\"\n\n    def environment_factory(seed: int) -> dm_env.Environment:\n      del seed\n      return fakes.ContinuousEnvironment(\n          episode_length=10, action_dim=3, observation_dim=5)\n\n    num_train_steps = 100\n\n    sac_config = sac.SACConfig()\n    checkpointing_config = experiments.CheckpointingConfig(\n        directory=self.get_tempdir(), time_delta_minutes=0)\n    return experiments.ExperimentConfig(\n        builder=sac.SACBuilder(sac_config),\n        environment_factory=environment_factory,\n        network_factory=sac.make_networks,\n        seed=0,\n        max_num_actor_steps=num_train_steps,\n        checkpointing=checkpointing_config)",
  "def test_checkpointing(self, num_eval_episodes: int):\n    num_train_steps = 100\n    experiment_config = self._get_experiment_config(\n        num_train_steps=num_train_steps)\n\n    experiments.run_experiment(\n        experiment_config, eval_every=10, num_eval_episodes=num_eval_episodes)\n\n    checkpoint_counter = experiment_test_utils.restore_counter(\n        experiment_config.checkpointing)\n    self.assertIn('actor_steps', checkpoint_counter.get_counts())\n    self.assertGreater(checkpoint_counter.get_counts()['actor_steps'], 0)\n\n    # Run the second experiment with the same checkpointing config to verify\n    # that it restores from the latest saved checkpoint.\n    experiments.run_experiment(\n        experiment_config, eval_every=50, num_eval_episodes=num_eval_episodes)\n\n    checkpoint_counter = experiment_test_utils.restore_counter(\n        experiment_config.checkpointing)\n    self.assertIn('actor_steps', checkpoint_counter.get_counts())\n    # Verify that the steps done in the first run are taken into account.\n    self.assertLessEqual(checkpoint_counter.get_counts()['actor_steps'],\n                         num_train_steps)",
  "def test_eval_every(self):\n    num_train_steps = 100\n    experiment_config = self._get_experiment_config(\n        num_train_steps=num_train_steps)\n\n    experiments.run_experiment(\n        experiment_config, eval_every=70, num_eval_episodes=1)\n\n    checkpoint_counter = experiment_test_utils.restore_counter(\n        experiment_config.checkpointing)\n    self.assertIn('actor_steps', checkpoint_counter.get_counts())\n    self.assertGreater(checkpoint_counter.get_counts()['actor_steps'], 0)\n    self.assertLessEqual(checkpoint_counter.get_counts()['actor_steps'],\n                         num_train_steps)",
  "def _get_experiment_config(\n      self, *, num_train_steps: int) -> experiments.ExperimentConfig:\n    \"\"\"Returns a config for a test experiment with the given number of steps.\"\"\"\n\n    def environment_factory(seed: int) -> dm_env.Environment:\n      del seed\n      return fakes.ContinuousEnvironment(\n          episode_length=10, action_dim=3, observation_dim=5)\n\n    num_train_steps = 100\n\n    sac_config = sac.SACConfig()\n    checkpointing_config = experiments.CheckpointingConfig(\n        directory=self.get_tempdir(), time_delta_minutes=0)\n    return experiments.ExperimentConfig(\n        builder=sac.SACBuilder(sac_config),\n        environment_factory=environment_factory,\n        network_factory=sac.make_networks,\n        seed=0,\n        max_num_actor_steps=num_train_steps,\n        checkpointing=checkpointing_config)",
  "def environment_factory(seed: int) -> dm_env.Environment:\n      del seed\n      return fakes.ContinuousEnvironment(\n          episode_length=10, action_dim=3, observation_dim=5)",
  "def make_distributed_offline_experiment(\n    experiment: config.OfflineExperimentConfig[builders.Networks, Any, Any],\n    *,\n    make_snapshot_models: Optional[config.SnapshotModelFactory[\n        builders.Networks]] = None,\n    name: str = 'agent',\n    program: Optional[lp.Program] = None) -> lp.Program:\n  \"\"\"Builds a Launchpad program for running the experiment.\n\n  Args:\n    experiment: configuration for the experiment.\n    make_snapshot_models: a factory that defines what is saved in snapshots.\n    name: name of the constructed program. Ignored if an existing program is\n      passed.\n    program: a program where agent nodes are added to. If None, a new program is\n      created.\n\n  Returns:\n    The Launchpad program with all the nodes needed for running the experiment.\n  \"\"\"\n\n  def build_model_saver(variable_source: core.VariableSource):\n    assert experiment.checkpointing\n    environment = experiment.environment_factory(0)\n    spec = specs.make_environment_spec(environment)\n    networks = experiment.network_factory(spec)\n    models = make_snapshot_models(networks, spec)\n    # TODO(raveman): Decouple checkpointing and snahpshotting configs.\n    return snapshotter.JAXSnapshotter(\n        variable_source=variable_source,\n        models=models,\n        path=experiment.checkpointing.directory,\n        add_uid=experiment.checkpointing.add_uid)\n\n  def build_counter():\n    counter = counting.Counter()\n    if experiment.checkpointing:\n      counter = savers.CheckpointingRunner(\n          counter,\n          key='counter',\n          subdirectory='counter',\n          time_delta_minutes=experiment.checkpointing.time_delta_minutes,\n          directory=experiment.checkpointing.directory,\n          add_uid=experiment.checkpointing.add_uid,\n          max_to_keep=experiment.checkpointing.max_to_keep,\n          checkpoint_ttl_seconds=experiment.checkpointing.checkpoint_ttl_seconds,\n      )\n    return counter\n\n  def build_learner(\n      random_key: networks_lib.PRNGKey,\n      counter: Optional[counting.Counter] = None,\n  ):\n    \"\"\"The Learning part of the agent.\"\"\"\n\n    dummy_seed = 1\n    spec = (\n        experiment.environment_spec or\n        specs.make_environment_spec(experiment.environment_factory(dummy_seed)))\n\n    # Creates the networks to optimize (online) and target networks.\n    networks = experiment.network_factory(spec)\n\n    dataset_key, random_key = jax.random.split(random_key)\n    iterator = experiment.demonstration_dataset_factory(dataset_key)\n    # make_demonstrations is responsible for putting data onto appropriate\n    # training devices, so here we apply prefetch, so that data is copied over\n    # in the background.\n    iterator = utils.prefetch(iterable=iterator, buffer_size=1)\n    counter = counting.Counter(counter, 'learner')\n    learner = experiment.builder.make_learner(\n        random_key=random_key,\n        networks=networks,\n        dataset=iterator,\n        logger_fn=experiment.logger_factory,\n        environment_spec=spec,\n        counter=counter)\n\n    if experiment.checkpointing:\n      learner = savers.CheckpointingRunner(\n          learner,\n          key='learner',\n          subdirectory='learner',\n          time_delta_minutes=5,\n          directory=experiment.checkpointing.directory,\n          add_uid=experiment.checkpointing.add_uid,\n          max_to_keep=experiment.checkpointing.max_to_keep,\n          checkpoint_ttl_seconds=experiment.checkpointing.checkpoint_ttl_seconds,\n      )\n\n    return learner\n\n  if not program:\n    program = lp.Program(name=name)\n\n  key = jax.random.PRNGKey(experiment.seed)\n\n  counter = program.add_node(lp.CourierNode(build_counter), label='counter')\n\n  if experiment.max_num_learner_steps is not None:\n    program.add_node(\n        lp.CourierNode(\n            lp_utils.StepsLimiter,\n            counter,\n            experiment.max_num_learner_steps,\n            steps_key='learner_steps'),\n        label='counter')\n\n  learner_key, key = jax.random.split(key)\n  learner_node = lp.CourierNode(build_learner, learner_key, counter)\n  learner = learner_node.create_handle()\n  program.add_node(learner_node, label='learner')\n\n  for evaluator in experiment.get_evaluator_factories():\n    evaluator_key, key = jax.random.split(key)\n    program.add_node(\n        lp.CourierNode(evaluator, evaluator_key, learner, counter,\n                       experiment.builder.make_actor),\n        label='evaluator')\n\n  if make_snapshot_models and experiment.checkpointing:\n    program.add_node(lp.CourierNode(build_model_saver, learner),\n                     label='model_saver')\n\n  return program",
  "def build_model_saver(variable_source: core.VariableSource):\n    assert experiment.checkpointing\n    environment = experiment.environment_factory(0)\n    spec = specs.make_environment_spec(environment)\n    networks = experiment.network_factory(spec)\n    models = make_snapshot_models(networks, spec)\n    # TODO(raveman): Decouple checkpointing and snahpshotting configs.\n    return snapshotter.JAXSnapshotter(\n        variable_source=variable_source,\n        models=models,\n        path=experiment.checkpointing.directory,\n        add_uid=experiment.checkpointing.add_uid)",
  "def build_counter():\n    counter = counting.Counter()\n    if experiment.checkpointing:\n      counter = savers.CheckpointingRunner(\n          counter,\n          key='counter',\n          subdirectory='counter',\n          time_delta_minutes=experiment.checkpointing.time_delta_minutes,\n          directory=experiment.checkpointing.directory,\n          add_uid=experiment.checkpointing.add_uid,\n          max_to_keep=experiment.checkpointing.max_to_keep,\n          checkpoint_ttl_seconds=experiment.checkpointing.checkpoint_ttl_seconds,\n      )\n    return counter",
  "def build_learner(\n      random_key: networks_lib.PRNGKey,\n      counter: Optional[counting.Counter] = None,\n  ):\n    \"\"\"The Learning part of the agent.\"\"\"\n\n    dummy_seed = 1\n    spec = (\n        experiment.environment_spec or\n        specs.make_environment_spec(experiment.environment_factory(dummy_seed)))\n\n    # Creates the networks to optimize (online) and target networks.\n    networks = experiment.network_factory(spec)\n\n    dataset_key, random_key = jax.random.split(random_key)\n    iterator = experiment.demonstration_dataset_factory(dataset_key)\n    # make_demonstrations is responsible for putting data onto appropriate\n    # training devices, so here we apply prefetch, so that data is copied over\n    # in the background.\n    iterator = utils.prefetch(iterable=iterator, buffer_size=1)\n    counter = counting.Counter(counter, 'learner')\n    learner = experiment.builder.make_learner(\n        random_key=random_key,\n        networks=networks,\n        dataset=iterator,\n        logger_fn=experiment.logger_factory,\n        environment_spec=spec,\n        counter=counter)\n\n    if experiment.checkpointing:\n      learner = savers.CheckpointingRunner(\n          learner,\n          key='learner',\n          subdirectory='learner',\n          time_delta_minutes=5,\n          directory=experiment.checkpointing.directory,\n          add_uid=experiment.checkpointing.add_uid,\n          max_to_keep=experiment.checkpointing.max_to_keep,\n          checkpoint_ttl_seconds=experiment.checkpointing.checkpoint_ttl_seconds,\n      )\n\n    return learner",
  "def run_experiment(experiment: config.ExperimentConfig,\n                   eval_every: int = 100,\n                   num_eval_episodes: int = 1):\n  \"\"\"Runs a simple, single-threaded training loop using the default evaluators.\n\n  It targets simplicity of the code and so only the basic features of the\n  ExperimentConfig are supported.\n\n  Arguments:\n    experiment: Definition and configuration of the agent to run.\n    eval_every: After how many actor steps to perform evaluation.\n    num_eval_episodes: How many evaluation episodes to execute at each\n      evaluation step.\n  \"\"\"\n\n  key = jax.random.PRNGKey(experiment.seed)\n\n  # Create the environment and get its spec.\n  environment = experiment.environment_factory(experiment.seed)\n  environment_spec = experiment.environment_spec or specs.make_environment_spec(\n      environment)\n\n  # Create the networks and policy.\n  networks = experiment.network_factory(environment_spec)\n  policy = config.make_policy(\n      experiment=experiment,\n      networks=networks,\n      environment_spec=environment_spec,\n      evaluation=False)\n\n  # Create the replay server and grab its address.\n  replay_tables = experiment.builder.make_replay_tables(environment_spec,\n                                                        policy)\n\n  # Disable blocking of inserts by tables' rate limiters, as this function\n  # executes learning (sampling from the table) and data generation\n  # (inserting into the table) sequentially from the same thread\n  # which could result in blocked insert making the algorithm hang.\n  replay_tables, rate_limiters_max_diff = _disable_insert_blocking(\n      replay_tables)\n\n  replay_server = reverb.Server(replay_tables, port=None)\n  replay_client = reverb.Client(f'localhost:{replay_server.port}')\n\n  # Parent counter allows to share step counts between train and eval loops and\n  # the learner, so that it is possible to plot for example evaluator's return\n  # value as a function of the number of training episodes.\n  parent_counter = counting.Counter(time_delta=0.)\n\n  dataset = experiment.builder.make_dataset_iterator(replay_client)\n  # We always use prefetch as it provides an iterator with an additional\n  # 'ready' method.\n  dataset = utils.prefetch(dataset, buffer_size=1)\n\n  # Create actor, adder, and learner for generating, storing, and consuming\n  # data respectively.\n  # NOTE: These are created in reverse order as the actor needs to be given the\n  # adder and the learner (as a source of variables).\n  learner_key, key = jax.random.split(key)\n  learner = experiment.builder.make_learner(\n      random_key=learner_key,\n      networks=networks,\n      dataset=dataset,\n      logger_fn=experiment.logger_factory,\n      environment_spec=environment_spec,\n      replay_client=replay_client,\n      counter=counting.Counter(parent_counter, prefix='learner', time_delta=0.))\n\n  adder = experiment.builder.make_adder(replay_client, environment_spec, policy)\n\n  actor_key, key = jax.random.split(key)\n  actor = experiment.builder.make_actor(\n      actor_key, policy, environment_spec, variable_source=learner, adder=adder)\n\n  # Create the environment loop used for training.\n  train_counter = counting.Counter(\n      parent_counter, prefix='actor', time_delta=0.)\n  train_logger = experiment.logger_factory('actor',\n                                           train_counter.get_steps_key(), 0)\n\n  checkpointer = None\n  if experiment.checkpointing is not None:\n    checkpointing = experiment.checkpointing\n    checkpointer = savers.Checkpointer(\n        objects_to_save={'learner': learner, 'counter': parent_counter},\n        time_delta_minutes=checkpointing.time_delta_minutes,\n        directory=checkpointing.directory,\n        add_uid=checkpointing.add_uid,\n        max_to_keep=checkpointing.max_to_keep,\n        keep_checkpoint_every_n_hours=checkpointing.keep_checkpoint_every_n_hours,\n        checkpoint_ttl_seconds=checkpointing.checkpoint_ttl_seconds,\n    )\n\n  # Replace the actor with a LearningActor. This makes sure that every time\n  # that `update` is called on the actor it checks to see whether there is\n  # any new data to learn from and if so it runs a learner step. The rate\n  # at which new data is released is controlled by the replay table's\n  # rate_limiter which is created by the builder.make_replay_tables call above.\n  actor = _LearningActor(actor, learner, dataset, replay_tables,\n                         rate_limiters_max_diff, checkpointer)\n\n  train_loop = acme.EnvironmentLoop(\n      environment,\n      actor,\n      counter=train_counter,\n      logger=train_logger,\n      observers=experiment.observers)\n\n  max_num_actor_steps = (\n      experiment.max_num_actor_steps -\n      parent_counter.get_counts().get(train_counter.get_steps_key(), 0))\n\n  if num_eval_episodes == 0:\n    # No evaluation. Just run the training loop.\n    train_loop.run(num_steps=max_num_actor_steps)\n    return\n\n  # Create the evaluation actor and loop.\n  eval_counter = counting.Counter(\n      parent_counter, prefix='evaluator', time_delta=0.)\n  eval_logger = experiment.logger_factory('evaluator',\n                                          eval_counter.get_steps_key(), 0)\n  eval_policy = config.make_policy(\n      experiment=experiment,\n      networks=networks,\n      environment_spec=environment_spec,\n      evaluation=True)\n  eval_actor = experiment.builder.make_actor(\n      random_key=jax.random.PRNGKey(experiment.seed),\n      policy=eval_policy,\n      environment_spec=environment_spec,\n      variable_source=learner)\n  eval_loop = acme.EnvironmentLoop(\n      environment,\n      eval_actor,\n      counter=eval_counter,\n      logger=eval_logger,\n      observers=experiment.observers)\n\n  steps = 0\n  while steps < max_num_actor_steps:\n    eval_loop.run(num_episodes=num_eval_episodes)\n    num_steps = min(eval_every, max_num_actor_steps - steps)\n    steps += train_loop.run(num_steps=num_steps)\n  eval_loop.run(num_episodes=num_eval_episodes)\n\n  environment.close()",
  "class _LearningActor(core.Actor):\n  \"\"\"Actor which learns (updates its parameters) when `update` is called.\n\n  This combines a base actor and a learner. Whenever `update` is called\n  on the wrapping actor the learner will take a step (e.g. one step of gradient\n  descent) as long as there is data available for training\n  (provided iterator and replay_tables are used to check for that).\n  Selecting actions and making observations are handled by the base actor.\n  Intended to be used by the `run_experiment` only.\n  \"\"\"\n\n  def __init__(self, actor: core.Actor, learner: core.Learner,\n               iterator: core.PrefetchingIterator,\n               replay_tables: Sequence[reverb.Table],\n               sample_sizes: Sequence[int],\n               checkpointer: Optional[savers.Checkpointer]):\n    \"\"\"Initializes _LearningActor.\n\n    Args:\n      actor: Actor to be wrapped.\n      learner: Learner on which step() is to be called when there is data.\n      iterator: Iterator used by the Learner to fetch training data.\n      replay_tables: Collection of tables from which Learner fetches data\n        through the iterator.\n      sample_sizes: For each table from `replay_tables`, how many elements the\n        table should have available for sampling to wait for the `iterator` to\n        prefetch a batch of data. Otherwise more experience needs to be\n        collected by the actor.\n      checkpointer: Checkpointer to save the state on update.\n    \"\"\"\n    self._actor = actor\n    self._learner = learner\n    self._iterator = iterator\n    self._replay_tables = replay_tables\n    self._sample_sizes = sample_sizes\n    self._learner_steps = 0\n    self._checkpointer = checkpointer\n\n  def select_action(self, observation: types.NestedArray) -> types.NestedArray:\n    return self._actor.select_action(observation)\n\n  def observe_first(self, timestep: dm_env.TimeStep):\n    self._actor.observe_first(timestep)\n\n  def observe(self, action: types.NestedArray, next_timestep: dm_env.TimeStep):\n    self._actor.observe(action, next_timestep)\n\n  def _maybe_train(self):\n    trained = False\n    while True:\n      if self._iterator.ready():\n        self._learner.step()\n        batches = self._iterator.retrieved_elements() - self._learner_steps\n        self._learner_steps += 1\n        assert batches == 1, (\n            'Learner step must retrieve exactly one element from the iterator'\n            f' (retrieved {batches}). Otherwise agent can deadlock. Example '\n            'cause is that your chosen agent'\n            's Builder has a `make_learner` '\n            'factory that prefetches the data but it shouldn'\n            't.')\n        trained = True\n      else:\n        # Wait for the iterator to fetch more data from the table(s) only\n        # if there plenty of data to sample from each table.\n        for table, sample_size in zip(self._replay_tables, self._sample_sizes):\n          if not table.can_sample(sample_size):\n            return trained\n        # Let iterator's prefetching thread get data from the table(s).\n        time.sleep(0.001)\n\n  def update(self):  # pytype: disable=signature-mismatch  # overriding-parameter-count-checks\n    if self._maybe_train():\n      # Update the actor weights only when learner was updated.\n      self._actor.update()\n    if self._checkpointer:\n      self._checkpointer.save()",
  "def _disable_insert_blocking(\n    tables: Sequence[reverb.Table]\n) -> Tuple[Sequence[reverb.Table], Sequence[int]]:\n  \"\"\"Disables blocking of insert operations for a given collection of tables.\"\"\"\n  modified_tables = []\n  sample_sizes = []\n  for table in tables:\n    rate_limiter_info = table.info.rate_limiter_info\n    rate_limiter = reverb.rate_limiters.RateLimiter(\n        samples_per_insert=rate_limiter_info.samples_per_insert,\n        min_size_to_sample=rate_limiter_info.min_size_to_sample,\n        min_diff=rate_limiter_info.min_diff,\n        max_diff=sys.float_info.max)\n    modified_tables.append(table.replace(rate_limiter=rate_limiter))\n    # Target the middle of the rate limiter's insert-sample balance window.\n    sample_sizes.append(\n        max(1, int(\n            (rate_limiter_info.max_diff - rate_limiter_info.min_diff) / 2)))\n  return modified_tables, sample_sizes",
  "def __init__(self, actor: core.Actor, learner: core.Learner,\n               iterator: core.PrefetchingIterator,\n               replay_tables: Sequence[reverb.Table],\n               sample_sizes: Sequence[int],\n               checkpointer: Optional[savers.Checkpointer]):\n    \"\"\"Initializes _LearningActor.\n\n    Args:\n      actor: Actor to be wrapped.\n      learner: Learner on which step() is to be called when there is data.\n      iterator: Iterator used by the Learner to fetch training data.\n      replay_tables: Collection of tables from which Learner fetches data\n        through the iterator.\n      sample_sizes: For each table from `replay_tables`, how many elements the\n        table should have available for sampling to wait for the `iterator` to\n        prefetch a batch of data. Otherwise more experience needs to be\n        collected by the actor.\n      checkpointer: Checkpointer to save the state on update.\n    \"\"\"\n    self._actor = actor\n    self._learner = learner\n    self._iterator = iterator\n    self._replay_tables = replay_tables\n    self._sample_sizes = sample_sizes\n    self._learner_steps = 0\n    self._checkpointer = checkpointer",
  "def select_action(self, observation: types.NestedArray) -> types.NestedArray:\n    return self._actor.select_action(observation)",
  "def observe_first(self, timestep: dm_env.TimeStep):\n    self._actor.observe_first(timestep)",
  "def observe(self, action: types.NestedArray, next_timestep: dm_env.TimeStep):\n    self._actor.observe(action, next_timestep)",
  "def _maybe_train(self):\n    trained = False\n    while True:\n      if self._iterator.ready():\n        self._learner.step()\n        batches = self._iterator.retrieved_elements() - self._learner_steps\n        self._learner_steps += 1\n        assert batches == 1, (\n            'Learner step must retrieve exactly one element from the iterator'\n            f' (retrieved {batches}). Otherwise agent can deadlock. Example '\n            'cause is that your chosen agent'\n            's Builder has a `make_learner` '\n            'factory that prefetches the data but it shouldn'\n            't.')\n        trained = True\n      else:\n        # Wait for the iterator to fetch more data from the table(s) only\n        # if there plenty of data to sample from each table.\n        for table, sample_size in zip(self._replay_tables, self._sample_sizes):\n          if not table.can_sample(sample_size):\n            return trained\n        # Let iterator's prefetching thread get data from the table(s).\n        time.sleep(0.001)",
  "def update(self):  # pytype: disable=signature-mismatch  # overriding-parameter-count-checks\n    if self._maybe_train():\n      # Update the actor weights only when learner was updated.\n      self._actor.update()\n    if self._checkpointer:\n      self._checkpointer.save()",
  "class OAREmbedding(hk.Module):\n  \"\"\"Module for embedding (observation, action, reward) inputs together.\"\"\"\n\n  torso: hk.SupportsCall\n  num_actions: int\n\n  def __call__(self, inputs: observation_action_reward.OAR) -> jnp.ndarray:\n    \"\"\"Embed each of the (observation, action, reward) inputs & concatenate.\"\"\"\n\n    # Add dummy batch dimension to observation if necessary.\n    # This is needed because Conv2D assumes a leading batch dimension, i.e.\n    # that inputs are in [B, H, W, C] format.\n    expand_obs = len(inputs.observation.shape) == 3\n    if expand_obs:\n      inputs = inputs._replace(\n          observation=jnp.expand_dims(inputs.observation, axis=0))\n    features = self.torso(inputs.observation)  # [T?, B, D]\n    if expand_obs:\n      features = jnp.squeeze(features, axis=0)\n\n    # Do a one-hot embedding of the actions.\n    action = jax.nn.one_hot(\n        inputs.action, num_classes=self.num_actions)  # [T?, B, A]\n\n    # Map rewards -> [-1, 1].\n    reward = jnp.tanh(inputs.reward)\n\n    # Add dummy trailing dimensions to rewards if necessary.\n    while reward.ndim < action.ndim:\n      reward = jnp.expand_dims(reward, axis=-1)\n\n    # Concatenate on final dimension.\n    embedding = jnp.concatenate(\n        [features, action, reward], axis=-1)  # [T?, B, D+A+1]\n\n    return embedding",
  "def __call__(self, inputs: observation_action_reward.OAR) -> jnp.ndarray:\n    \"\"\"Embed each of the (observation, action, reward) inputs & concatenate.\"\"\"\n\n    # Add dummy batch dimension to observation if necessary.\n    # This is needed because Conv2D assumes a leading batch dimension, i.e.\n    # that inputs are in [B, H, W, C] format.\n    expand_obs = len(inputs.observation.shape) == 3\n    if expand_obs:\n      inputs = inputs._replace(\n          observation=jnp.expand_dims(inputs.observation, axis=0))\n    features = self.torso(inputs.observation)  # [T?, B, D]\n    if expand_obs:\n      features = jnp.squeeze(features, axis=0)\n\n    # Do a one-hot embedding of the actions.\n    action = jax.nn.one_hot(\n        inputs.action, num_classes=self.num_actions)  # [T?, B, A]\n\n    # Map rewards -> [-1, 1].\n    reward = jnp.tanh(inputs.reward)\n\n    # Add dummy trailing dimensions to rewards if necessary.\n    while reward.ndim < action.ndim:\n      reward = jnp.expand_dims(reward, axis=-1)\n\n    # Concatenate on final dimension.\n    embedding = jnp.concatenate(\n        [features, action, reward], axis=-1)  # [T?, B, D+A+1]\n\n    return embedding",
  "class CriticMultiplexer(hk.Module):\n  \"\"\"Module connecting a critic torso to (transformed) observations/actions.\n\n  This takes as input a `critic_network`, an `observation_network`, and an\n  `action_network` and returns another network whose outputs are given by\n  `critic_network(observation_network(o), action_network(a))`.\n\n  The observations and actions passed to this module are assumed to have a batch\n  dimension that match.\n\n  Notes:\n  - Either the `observation_` or `action_network` can be `None`, in which case\n    the observation or action, resp., are passed to the critic network as is.\n  - If all `critic_`, `observation_` and `action_network` are `None`, this\n    module reduces to a simple `tf2_utils.batch_concat()`.\n  \"\"\"\n\n  def __init__(self,\n               critic_network: Optional[ModuleOrArrayTransform] = None,\n               observation_network: Optional[ModuleOrArrayTransform] = None,\n               action_network: Optional[ModuleOrArrayTransform] = None):\n    self._critic_network = critic_network\n    self._observation_network = observation_network\n    self._action_network = action_network\n    super().__init__(name='critic_multiplexer')\n\n  def __call__(self,\n               observation: jnp.ndarray,\n               action: jnp.ndarray) -> jnp.ndarray:\n\n    # Maybe transform observations and actions before feeding them on.\n    if self._observation_network:\n      observation = self._observation_network(observation)\n    if self._action_network:\n      action = self._action_network(action)\n\n    # Concat observations and actions, with one batch dimension.\n    outputs = utils.batch_concat([observation, action])\n\n    # Maybe transform output before returning.\n    if self._critic_network:\n      outputs = self._critic_network(outputs)\n\n    return outputs",
  "def __init__(self,\n               critic_network: Optional[ModuleOrArrayTransform] = None,\n               observation_network: Optional[ModuleOrArrayTransform] = None,\n               action_network: Optional[ModuleOrArrayTransform] = None):\n    self._critic_network = critic_network\n    self._observation_network = observation_network\n    self._action_network = action_network\n    super().__init__(name='critic_multiplexer')",
  "def __call__(self,\n               observation: jnp.ndarray,\n               action: jnp.ndarray) -> jnp.ndarray:\n\n    # Maybe transform observations and actions before feeding them on.\n    if self._observation_network:\n      observation = self._observation_network(observation)\n    if self._action_network:\n      action = self._action_network(action)\n\n    # Concat observations and actions, with one batch dimension.\n    outputs = utils.batch_concat([observation, action])\n\n    # Maybe transform output before returning.\n    if self._critic_network:\n      outputs = self._critic_network(outputs)\n\n    return outputs",
  "class NearZeroInitializedLinear(hk.Linear):\n  \"\"\"Simple linear layer, initialized at near zero weights and zero biases.\"\"\"\n\n  def __init__(self, output_size: int, scale: float = 1e-4):\n    super().__init__(output_size, w_init=hk.initializers.VarianceScaling(scale))",
  "class LayerNormMLP(hk.Module):\n  \"\"\"Simple feedforward MLP torso with initial layer-norm.\n\n  This MLP's first linear layer is followed by a LayerNorm layer and a tanh\n  non-linearity; subsequent layers use `activation`, which defaults to elu.\n\n  Note! The default activation differs from the usual MLP default of ReLU for\n  legacy reasons.\n  \"\"\"\n\n  def __init__(self,\n               layer_sizes: Sequence[int],\n               w_init: hk.initializers.Initializer = uniform_initializer,\n               activation: Callable[[jnp.ndarray], jnp.ndarray] = jax.nn.elu,\n               activate_final: bool = False,\n               name: str = 'feedforward_mlp_torso'):\n    \"\"\"Construct the MLP.\n\n    Args:\n      layer_sizes: a sequence of ints specifying the size of each layer.\n      w_init: initializer for Linear layers.\n      activation: nonlinearity to use in the MLP, defaults to elu.\n        Note! The default activation differs from the usual MLP default of ReLU\n        for legacy reasons.\n      activate_final: whether or not to use the activation function on the final\n        layer of the neural network.\n      name: a name for the module.\n    \"\"\"\n    super().__init__(name=name)\n\n    self._network = hk.Sequential([\n        hk.Linear(layer_sizes[0], w_init=w_init),\n        hk.LayerNorm(axis=-1, create_scale=True, create_offset=True),\n        jax.lax.tanh,\n        hk.nets.MLP(\n            layer_sizes[1:],\n            w_init=w_init,\n            activation=activation,\n            activate_final=activate_final),\n    ])\n\n  def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Forwards the policy network.\"\"\"\n    return self._network(inputs)",
  "def __init__(self, output_size: int, scale: float = 1e-4):\n    super().__init__(output_size, w_init=hk.initializers.VarianceScaling(scale))",
  "def __init__(self,\n               layer_sizes: Sequence[int],\n               w_init: hk.initializers.Initializer = uniform_initializer,\n               activation: Callable[[jnp.ndarray], jnp.ndarray] = jax.nn.elu,\n               activate_final: bool = False,\n               name: str = 'feedforward_mlp_torso'):\n    \"\"\"Construct the MLP.\n\n    Args:\n      layer_sizes: a sequence of ints specifying the size of each layer.\n      w_init: initializer for Linear layers.\n      activation: nonlinearity to use in the MLP, defaults to elu.\n        Note! The default activation differs from the usual MLP default of ReLU\n        for legacy reasons.\n      activate_final: whether or not to use the activation function on the final\n        layer of the neural network.\n      name: a name for the module.\n    \"\"\"\n    super().__init__(name=name)\n\n    self._network = hk.Sequential([\n        hk.Linear(layer_sizes[0], w_init=w_init),\n        hk.LayerNorm(axis=-1, create_scale=True, create_offset=True),\n        jax.lax.tanh,\n        hk.nets.MLP(\n            layer_sizes[1:],\n            w_init=w_init,\n            activation=activation,\n            activate_final=activate_final),\n    ])",
  "def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Forwards the policy network.\"\"\"\n    return self._network(inputs)",
  "class FeedForwardNetwork:\n  \"\"\"Holds a pair of pure functions defining a feed-forward network.\n\n  Attributes:\n    init: A pure function: ``params = init(rng, *a, **k)``\n    apply: A pure function: ``out = apply(params, rng, *a, **k)``\n  \"\"\"\n  # Initializes and returns the networks parameters.\n  init: Callable[..., Params]\n  # Computes and returns the outputs of a forward pass.\n  apply: Callable[..., NetworkOutput]",
  "class ApplyFn(Protocol):\n\n  def __call__(self,\n               params: Params,\n               observation: Observation,\n               *args,\n               is_training: bool,\n               key: Optional[PRNGKey] = None,\n               **kwargs) -> NetworkOutput:\n    ...",
  "class TypedFeedForwardNetwork:\n  \"\"\"FeedForwardNetwork with more specific types of the member functions.\n\n  Attributes:\n    init: A pure function. Initializes and returns the networks parameters.\n    apply: A pure function. Computes and returns the outputs of a forward pass.\n  \"\"\"\n  init: Callable[[PRNGKey], Params]\n  apply: ApplyFn",
  "def non_stochastic_network_to_typed(\n    network: FeedForwardNetwork) -> TypedFeedForwardNetwork:\n  \"\"\"Converts non-stochastic FeedForwardNetwork to TypedFeedForwardNetwork.\n\n  Non-stochastic network is the one that doesn't take a random key as an input\n  for its `apply` method.\n\n  Arguments:\n    network: non-stochastic feed-forward network.\n\n  Returns:\n    corresponding TypedFeedForwardNetwork\n  \"\"\"\n\n  def apply(params: Params,\n            observation: Observation,\n            *args,\n            is_training: bool,\n            key: Optional[PRNGKey] = None,\n            **kwargs) -> NetworkOutput:\n    del is_training, key\n    return network.apply(params, observation, *args, **kwargs)\n\n  return TypedFeedForwardNetwork(init=network.init, apply=apply)",
  "class UnrollableNetwork:\n  \"\"\"Network that can unroll over an input sequence.\"\"\"\n  init: Callable[[PRNGKey], Params]\n  apply: Callable[[Params, PRNGKey, Observation, RecurrentState],\n                  Tuple[NetworkOutput, RecurrentState]]\n  unroll: Callable[[Params, PRNGKey, Observation, RecurrentState],\n                   Tuple[NetworkOutput, RecurrentState]]\n  init_recurrent_state: Callable[[PRNGKey, Optional[BatchSize]], RecurrentState]",
  "def make_unrollable_network(\n    environment_spec: specs.EnvironmentSpec,\n    make_core_module: Callable[[], hk.RNNCore]) -> UnrollableNetwork:\n  \"\"\"Builds an UnrollableNetwork from a hk.Module factory.\"\"\"\n\n  dummy_observation = jax_utils.zeros_like(environment_spec.observations)\n\n  def make_unrollable_network_functions():\n    model = make_core_module()\n    apply = model.__call__\n\n    def init() -> Tuple[NetworkOutput, RecurrentState]:\n      return model(dummy_observation, model.initial_state(None))\n\n    return init, (apply, model.unroll, model.initial_state)  # pytype: disable=attribute-error\n\n  # Transform and unpack pure functions\n  f = hk.multi_transform(make_unrollable_network_functions)\n  apply, unroll, initial_state_fn = f.apply\n\n  def init_recurrent_state(key: jax_types.PRNGKey,\n                           batch_size: Optional[int]) -> RecurrentState:\n    # TODO(b/244311990): Consider supporting parameterized and learnable initial\n    # state functions.\n    no_params = None\n    return initial_state_fn(no_params, key, batch_size)\n\n  return UnrollableNetwork(f.init, apply, unroll, init_recurrent_state)",
  "def __call__(self,\n               params: Params,\n               observation: Observation,\n               *args,\n               is_training: bool,\n               key: Optional[PRNGKey] = None,\n               **kwargs) -> NetworkOutput:\n    ...",
  "def apply(params: Params,\n            observation: Observation,\n            *args,\n            is_training: bool,\n            key: Optional[PRNGKey] = None,\n            **kwargs) -> NetworkOutput:\n    del is_training, key\n    return network.apply(params, observation, *args, **kwargs)",
  "def make_unrollable_network_functions():\n    model = make_core_module()\n    apply = model.__call__\n\n    def init() -> Tuple[NetworkOutput, RecurrentState]:\n      return model(dummy_observation, model.initial_state(None))\n\n    return init, (apply, model.unroll, model.initial_state)",
  "def init_recurrent_state(key: jax_types.PRNGKey,\n                           batch_size: Optional[int]) -> RecurrentState:\n    # TODO(b/244311990): Consider supporting parameterized and learnable initial\n    # state functions.\n    no_params = None\n    return initial_state_fn(no_params, key, batch_size)",
  "def init() -> Tuple[NetworkOutput, RecurrentState]:\n      return model(dummy_observation, model.initial_state(None))",
  "class ClipToSpec:\n  \"\"\"Clips inputs to within a BoundedArraySpec.\"\"\"\n  spec: specs.BoundedArray\n\n  def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n    return jnp.clip(inputs, self.spec.minimum, self.spec.maximum)",
  "class RescaleToSpec:\n  \"\"\"Rescales inputs in [-1, 1] to match a BoundedArraySpec.\"\"\"\n  spec: specs.BoundedArray\n\n  def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n    scale = self.spec.maximum - self.spec.minimum\n    offset = self.spec.minimum\n    inputs = 0.5 * (inputs + 1.0)  # [0, 1]\n    output = inputs * scale + offset  # [minimum, maximum]\n    return output",
  "class TanhToSpec:\n  \"\"\"Squashes real-valued inputs to match a BoundedArraySpec.\"\"\"\n  spec: specs.BoundedArray\n\n  def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n    scale = self.spec.maximum - self.spec.minimum\n    offset = self.spec.minimum\n    inputs = lax.tanh(inputs)  # [-1, 1]\n    inputs = 0.5 * (inputs + 1.0)  # [0, 1]\n    output = inputs * scale + offset  # [minimum, maximum]\n    return output",
  "def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n    return jnp.clip(inputs, self.spec.minimum, self.spec.maximum)",
  "def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n    scale = self.spec.maximum - self.spec.minimum\n    offset = self.spec.minimum\n    inputs = 0.5 * (inputs + 1.0)  # [0, 1]\n    output = inputs * scale + offset  # [minimum, maximum]\n    return output",
  "def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n    scale = self.spec.maximum - self.spec.minimum\n    offset = self.spec.minimum\n    inputs = lax.tanh(inputs)  # [-1, 1]\n    inputs = 0.5 * (inputs + 1.0)  # [0, 1]\n    output = inputs * scale + offset  # [minimum, maximum]\n    return output",
  "class DuellingMLP(hk.Module):\n  \"\"\"A Duelling MLP Q-network.\"\"\"\n\n  def __init__(\n      self,\n      num_actions: int,\n      hidden_sizes: Sequence[int],\n      w_init: Optional[hk.initializers.Initializer] = None,\n  ):\n    super().__init__(name='duelling_q_network')\n\n    self._value_mlp = hk.nets.MLP([*hidden_sizes, 1], w_init=w_init)\n    self._advantage_mlp = hk.nets.MLP([*hidden_sizes, num_actions],\n                                      w_init=w_init)\n\n  def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Forward pass of the duelling network.\n\n    Args:\n      inputs: 2-D tensor of shape [batch_size, embedding_size].\n\n    Returns:\n      q_values: 2-D tensor of action values of shape [batch_size, num_actions]\n    \"\"\"\n\n    # Compute value & advantage for duelling.\n    value = self._value_mlp(inputs)  # [B, 1]\n    advantages = self._advantage_mlp(inputs)  # [B, A]\n\n    # Advantages have zero mean.\n    advantages -= jnp.mean(advantages, axis=-1, keepdims=True)  # [B, A]\n\n    q_values = value + advantages  # [B, A]\n\n    return q_values",
  "def __init__(\n      self,\n      num_actions: int,\n      hidden_sizes: Sequence[int],\n      w_init: Optional[hk.initializers.Initializer] = None,\n  ):\n    super().__init__(name='duelling_q_network')\n\n    self._value_mlp = hk.nets.MLP([*hidden_sizes, 1], w_init=w_init)\n    self._advantage_mlp = hk.nets.MLP([*hidden_sizes, num_actions],\n                                      w_init=w_init)",
  "def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Forward pass of the duelling network.\n\n    Args:\n      inputs: 2-D tensor of shape [batch_size, embedding_size].\n\n    Returns:\n      q_values: 2-D tensor of action values of shape [batch_size, num_actions]\n    \"\"\"\n\n    # Compute value & advantage for duelling.\n    value = self._value_mlp(inputs)  # [B, 1]\n    advantages = self._advantage_mlp(inputs)  # [B, A]\n\n    # Advantages have zero mean.\n    advantages -= jnp.mean(advantages, axis=-1, keepdims=True)  # [B, A]\n\n    q_values = value + advantages  # [B, A]\n\n    return q_values",
  "class AtariTorso(hk.Module):\n  \"\"\"Simple convolutional stack commonly used for Atari.\"\"\"\n\n  def __init__(self):\n    super().__init__(name='atari_torso')\n    self._network = hk.Sequential([\n        hk.Conv2D(32, [8, 8], 4), jax.nn.relu,\n        hk.Conv2D(64, [4, 4], 2), jax.nn.relu,\n        hk.Conv2D(64, [3, 3], 1), jax.nn.relu\n    ])\n\n  def __call__(self, inputs: Images) -> jnp.ndarray:\n    inputs_rank = jnp.ndim(inputs)\n    batched_inputs = inputs_rank == 4\n    if inputs_rank < 3 or inputs_rank > 4:\n      raise ValueError('Expected input BHWC or HWC. Got rank %d' % inputs_rank)\n\n    outputs = self._network(inputs)\n\n    if batched_inputs:\n      return jnp.reshape(outputs, [outputs.shape[0], -1])  # [B, D]\n    return jnp.reshape(outputs, [-1])",
  "def dqn_atari_network(num_actions: int) -> base.QNetwork:\n  \"\"\"A feed-forward network for use with Ape-X DQN.\"\"\"\n\n  def network(inputs: Images) -> base.QValues:\n    model = hk.Sequential([\n        AtariTorso(),\n        duelling.DuellingMLP(num_actions, hidden_sizes=[512]),\n    ])\n    return model(inputs)\n\n  return network",
  "class DeepAtariTorso(hk.Module):\n  \"\"\"Deep torso for Atari, from the IMPALA paper.\"\"\"\n\n  def __init__(\n      self,\n      channels_per_group: Sequence[int] = (16, 32, 32),\n      blocks_per_group: Sequence[int] = (2, 2, 2),\n      downsampling_strategies: Sequence[resnet.DownsamplingStrategy] = (\n          resnet.DownsamplingStrategy.CONV_MAX,) * 3,\n      hidden_sizes: Sequence[int] = (256,),\n      use_layer_norm: bool = False,\n      name: str = 'deep_atari_torso'):\n    super().__init__(name=name)\n    self._use_layer_norm = use_layer_norm\n    self.resnet = resnet.ResNetTorso(\n        channels_per_group=channels_per_group,\n        blocks_per_group=blocks_per_group,\n        downsampling_strategies=downsampling_strategies,\n        use_layer_norm=use_layer_norm)\n    # Make sure to activate the last layer as this torso is expected to feed\n    # into the rest of a bigger network.\n    self.mlp_head = hk.nets.MLP(output_sizes=hidden_sizes, activate_final=True)\n\n  def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n    output = self.resnet(x)\n    output = jax.nn.relu(output)\n    output = hk.Flatten(preserve_dims=-3)(output)\n    output = self.mlp_head(output)\n    return output",
  "class DeepIMPALAAtariNetwork(hk.RNNCore):\n  \"\"\"A recurrent network for use with IMPALA.\n\n  See https://arxiv.org/pdf/1802.01561.pdf for more information.\n  \"\"\"\n\n  def __init__(self, num_actions: int):\n    super().__init__(name='impala_atari_network')\n    self._embed = embedding.OAREmbedding(\n        DeepAtariTorso(use_layer_norm=True), num_actions)\n    self._core = hk.GRU(256)\n    self._head = policy_value.PolicyValueHead(num_actions)\n    self._num_actions = num_actions\n\n  def __call__(\n      self, inputs: observation_action_reward.OAR, state: hk.LSTMState\n  ) -> Any:\n    embeddings = self._embed(inputs)  # [B?, D+A+1]\n    embeddings, new_state = self._core(embeddings, state)\n    logits, value = self._head(embeddings)  # logits: [B?, A], value: [B?, 1]\n\n    return (logits, value), new_state\n\n  def initial_state(self, batch_size: Optional[int],\n                    **unused_kwargs) -> hk.LSTMState:\n    return self._core.initial_state(batch_size)\n\n  def unroll(\n      self, inputs: observation_action_reward.OAR, state: hk.LSTMState\n  ) -> Any:\n    \"\"\"Efficient unroll that applies embeddings, MLP, & convnet in one pass.\"\"\"\n    embeddings = self._embed(inputs)\n    embeddings, new_states = hk.static_unroll(self._core, embeddings, state)\n    logits, values = self._head(embeddings)\n\n    return (logits, values), new_states",
  "class R2D2AtariNetwork(hk.RNNCore):\n  \"\"\"A duelling recurrent network for use with Atari observations as seen in R2D2.\n\n  See https://openreview.net/forum?id=r1lyTjAqYX for more information.\n  \"\"\"\n\n  def __init__(self, num_actions: int):\n    super().__init__(name='r2d2_atari_network')\n    self._embed = embedding.OAREmbedding(\n        DeepAtariTorso(hidden_sizes=[512], use_layer_norm=True), num_actions)\n    self._core = hk.LSTM(512)\n    self._duelling_head = duelling.DuellingMLP(num_actions, hidden_sizes=[512])\n    self._num_actions = num_actions\n\n  def __call__(\n      self,\n      inputs: observation_action_reward.OAR,  # [B, ...]\n      state: hk.LSTMState  # [B, ...]\n  ) -> Tuple[base.QValues, hk.LSTMState]:\n    embeddings = self._embed(inputs)  # [B, D+A+1]\n    core_outputs, new_state = self._core(embeddings, state)\n    q_values = self._duelling_head(core_outputs)\n    return q_values, new_state\n\n  def initial_state(self, batch_size: Optional[int],\n                    **unused_kwargs) -> hk.LSTMState:\n    return self._core.initial_state(batch_size)\n\n  def unroll(\n      self,\n      inputs: observation_action_reward.OAR,  # [T, B, ...]\n      state: hk.LSTMState  # [T, ...]\n  ) -> Tuple[base.QValues, hk.LSTMState]:\n    \"\"\"Efficient unroll that applies torso, core, and duelling mlp in one pass.\"\"\"\n    embeddings = hk.BatchApply(self._embed)(inputs)  # [T, B, D+A+1]\n    core_outputs, new_states = hk.static_unroll(self._core, embeddings, state)\n    q_values = hk.BatchApply(self._duelling_head)(core_outputs)  # [T, B, A]\n    return q_values, new_states",
  "def __init__(self):\n    super().__init__(name='atari_torso')\n    self._network = hk.Sequential([\n        hk.Conv2D(32, [8, 8], 4), jax.nn.relu,\n        hk.Conv2D(64, [4, 4], 2), jax.nn.relu,\n        hk.Conv2D(64, [3, 3], 1), jax.nn.relu\n    ])",
  "def __call__(self, inputs: Images) -> jnp.ndarray:\n    inputs_rank = jnp.ndim(inputs)\n    batched_inputs = inputs_rank == 4\n    if inputs_rank < 3 or inputs_rank > 4:\n      raise ValueError('Expected input BHWC or HWC. Got rank %d' % inputs_rank)\n\n    outputs = self._network(inputs)\n\n    if batched_inputs:\n      return jnp.reshape(outputs, [outputs.shape[0], -1])  # [B, D]\n    return jnp.reshape(outputs, [-1])",
  "def network(inputs: Images) -> base.QValues:\n    model = hk.Sequential([\n        AtariTorso(),\n        duelling.DuellingMLP(num_actions, hidden_sizes=[512]),\n    ])\n    return model(inputs)",
  "def __init__(\n      self,\n      channels_per_group: Sequence[int] = (16, 32, 32),\n      blocks_per_group: Sequence[int] = (2, 2, 2),\n      downsampling_strategies: Sequence[resnet.DownsamplingStrategy] = (\n          resnet.DownsamplingStrategy.CONV_MAX,) * 3,\n      hidden_sizes: Sequence[int] = (256,),\n      use_layer_norm: bool = False,\n      name: str = 'deep_atari_torso'):\n    super().__init__(name=name)\n    self._use_layer_norm = use_layer_norm\n    self.resnet = resnet.ResNetTorso(\n        channels_per_group=channels_per_group,\n        blocks_per_group=blocks_per_group,\n        downsampling_strategies=downsampling_strategies,\n        use_layer_norm=use_layer_norm)\n    # Make sure to activate the last layer as this torso is expected to feed\n    # into the rest of a bigger network.\n    self.mlp_head = hk.nets.MLP(output_sizes=hidden_sizes, activate_final=True)",
  "def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n    output = self.resnet(x)\n    output = jax.nn.relu(output)\n    output = hk.Flatten(preserve_dims=-3)(output)\n    output = self.mlp_head(output)\n    return output",
  "def __init__(self, num_actions: int):\n    super().__init__(name='impala_atari_network')\n    self._embed = embedding.OAREmbedding(\n        DeepAtariTorso(use_layer_norm=True), num_actions)\n    self._core = hk.GRU(256)\n    self._head = policy_value.PolicyValueHead(num_actions)\n    self._num_actions = num_actions",
  "def __call__(\n      self, inputs: observation_action_reward.OAR, state: hk.LSTMState\n  ) -> Any:\n    embeddings = self._embed(inputs)  # [B?, D+A+1]\n    embeddings, new_state = self._core(embeddings, state)\n    logits, value = self._head(embeddings)  # logits: [B?, A], value: [B?, 1]\n\n    return (logits, value), new_state",
  "def initial_state(self, batch_size: Optional[int],\n                    **unused_kwargs) -> hk.LSTMState:\n    return self._core.initial_state(batch_size)",
  "def unroll(\n      self, inputs: observation_action_reward.OAR, state: hk.LSTMState\n  ) -> Any:\n    \"\"\"Efficient unroll that applies embeddings, MLP, & convnet in one pass.\"\"\"\n    embeddings = self._embed(inputs)\n    embeddings, new_states = hk.static_unroll(self._core, embeddings, state)\n    logits, values = self._head(embeddings)\n\n    return (logits, values), new_states",
  "def __init__(self, num_actions: int):\n    super().__init__(name='r2d2_atari_network')\n    self._embed = embedding.OAREmbedding(\n        DeepAtariTorso(hidden_sizes=[512], use_layer_norm=True), num_actions)\n    self._core = hk.LSTM(512)\n    self._duelling_head = duelling.DuellingMLP(num_actions, hidden_sizes=[512])\n    self._num_actions = num_actions",
  "def __call__(\n      self,\n      inputs: observation_action_reward.OAR,  # [B, ...]\n      state: hk.LSTMState  # [B, ...]\n  ) -> Tuple[base.QValues, hk.LSTMState]:\n    embeddings = self._embed(inputs)  # [B, D+A+1]\n    core_outputs, new_state = self._core(embeddings, state)\n    q_values = self._duelling_head(core_outputs)\n    return q_values, new_state",
  "def initial_state(self, batch_size: Optional[int],\n                    **unused_kwargs) -> hk.LSTMState:\n    return self._core.initial_state(batch_size)",
  "def unroll(\n      self,\n      inputs: observation_action_reward.OAR,  # [T, B, ...]\n      state: hk.LSTMState  # [T, ...]\n  ) -> Tuple[base.QValues, hk.LSTMState]:\n    \"\"\"Efficient unroll that applies torso, core, and duelling mlp in one pass.\"\"\"\n    embeddings = hk.BatchApply(self._embed)(inputs)  # [T, B, D+A+1]\n    core_outputs, new_states = hk.static_unroll(self._core, embeddings, state)\n    q_values = hk.BatchApply(self._duelling_head)(core_outputs)  # [T, B, A]\n    return q_values, new_states",
  "class PolicyValueHead(hk.Module):\n  \"\"\"A network with two linear layers, for policy and value respectively.\"\"\"\n\n  def __init__(self, num_actions: int):\n    super().__init__(name='policy_value_network')\n    self._policy_layer = hk.Linear(num_actions)\n    self._value_layer = hk.Linear(1)\n\n  def __call__(self, inputs: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:\n    \"\"\"Returns a (Logits, Value) tuple.\"\"\"\n    logits = self._policy_layer(inputs)  # [B, A]\n    value = jnp.squeeze(self._value_layer(inputs), axis=-1)  # [B]\n\n    return logits, value",
  "def __init__(self, num_actions: int):\n    super().__init__(name='policy_value_network')\n    self._policy_layer = hk.Linear(num_actions)\n    self._value_layer = hk.Linear(1)",
  "def __call__(self, inputs: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:\n    \"\"\"Returns a (Logits, Value) tuple.\"\"\"\n    logits = self._policy_layer(inputs)  # [B, A]\n    value = jnp.squeeze(self._value_layer(inputs), axis=-1)  # [B]\n\n    return logits, value",
  "class CategoricalHead(hk.Module):\n  \"\"\"Module that produces a categorical distribution with the given number of values.\"\"\"\n\n  def __init__(\n      self,\n      num_values: Union[int, List[int]],\n      dtype: Optional[Any] = jnp.int32,\n      w_init: Optional[Initializer] = None,\n      name: Optional[str] = None,\n  ):\n    super().__init__(name=name)\n    self._dtype = dtype\n    self._logit_shape = num_values\n    self._linear = hk.Linear(np.prod(num_values), w_init=w_init)\n\n  def __call__(self, inputs: jnp.ndarray) -> tfd.Distribution:\n    logits = self._linear(inputs)\n    if not isinstance(self._logit_shape, int):\n      logits = hk.Reshape(self._logit_shape)(logits)\n    return tfd.Categorical(logits=logits, dtype=self._dtype)",
  "class GaussianMixture(hk.Module):\n  \"\"\"Module that outputs a Gaussian Mixture Distribution.\"\"\"\n\n  def __init__(self,\n               num_dimensions: int,\n               num_components: int,\n               multivariate: bool,\n               init_scale: Optional[float] = None,\n               append_singleton_event_dim: bool = False,\n               reinterpreted_batch_ndims: Optional[int] = None,\n               transformation_fn: Optional[Callable[[tfd.Distribution],\n                                                    tfd.Distribution]] = None,\n               name: str = 'GaussianMixture'):\n    \"\"\"Initialization.\n\n    Args:\n      num_dimensions: dimensionality of the output distribution\n      num_components: number of mixture components.\n      multivariate: whether the resulting distribution is multivariate or not.\n      init_scale: the initial scale for the Gaussian mixture components.\n      append_singleton_event_dim: (univariate only) Whether to add an extra\n        singleton dimension to the event shape.\n      reinterpreted_batch_ndims: (univariate only) Number of batch dimensions to\n        reinterpret as event dimensions.\n      transformation_fn: Distribution transform such as TanhTransformation\n        applied to individual components.\n      name: name of the module passed to snt.Module parent class.\n    \"\"\"\n    super().__init__(name=name)\n\n    self._num_dimensions = num_dimensions\n    self._num_components = num_components\n    self._multivariate = multivariate\n    self._append_singleton_event_dim = append_singleton_event_dim\n    self._reinterpreted_batch_ndims = reinterpreted_batch_ndims\n\n    if init_scale is not None:\n      self._scale_factor = init_scale / jax.nn.softplus(0.)\n    else:\n      self._scale_factor = 1.0  # Corresponds to init_scale = softplus(0).\n\n    self._transformation_fn = transformation_fn\n\n  def __call__(self,\n               inputs: jnp.ndarray,\n               low_noise_policy: bool = False) -> tfd.Distribution:\n    \"\"\"Run the networks through inputs.\n\n    Args:\n      inputs: hidden activations of the policy network body.\n      low_noise_policy: whether to set vanishingly small scales for each\n        component. If this flag is set to True, the policy is effectively run\n        without Gaussian noise.\n\n    Returns:\n      Mixture Gaussian distribution.\n    \"\"\"\n\n    # Define the weight initializer.\n    w_init = hk.initializers.VarianceScaling(scale=1e-5)\n\n    # Create a layer that outputs the unnormalized log-weights.\n    if self._multivariate:\n      logits_size = self._num_components\n    else:\n      logits_size = self._num_dimensions * self._num_components\n    logit_layer = hk.Linear(logits_size, w_init=w_init)\n\n    # Create two layers that outputs a location and a scale, respectively, for\n    # each dimension and each component.\n    loc_layer = hk.Linear(\n        self._num_dimensions * self._num_components, w_init=w_init)\n    scale_layer = hk.Linear(\n        self._num_dimensions * self._num_components, w_init=w_init)\n\n    # Compute logits, locs, and scales if necessary.\n    logits = logit_layer(inputs)\n    locs = loc_layer(inputs)\n\n    # When a low_noise_policy is requested, set the scales to its minimum value.\n    if low_noise_policy:\n      scales = jnp.full(locs.shape, _MIN_SCALE)\n    else:\n      scales = scale_layer(inputs)\n      scales = self._scale_factor * jax.nn.softplus(scales) + _MIN_SCALE\n\n    if self._multivariate:\n      components_class = tfd.MultivariateNormalDiag\n      shape = [-1, self._num_components, self._num_dimensions]  # [B, C, D]\n      # In this case, no need to reshape logits as they are in the correct shape\n      # already, namely [batch_size, num_components].\n    else:\n      components_class = tfd.Normal\n      shape = [-1, self._num_dimensions, self._num_components]  # [B, D, C]\n      if self._append_singleton_event_dim:\n        shape.insert(2, 1)  # [B, D, 1, C]\n      logits = logits.reshape(shape)\n\n    # Reshape the mixture's location and scale parameters appropriately.\n    locs = locs.reshape(shape)\n    scales = scales.reshape(shape)\n\n    if self._multivariate:\n      components_distribution = components_class(loc=locs, scale_diag=scales)\n    else:\n      components_distribution = components_class(loc=locs, scale=scales)\n\n    # Transformed the component distributions in the mixture.\n    if self._transformation_fn:\n      components_distribution = self._transformation_fn(components_distribution)\n\n    # Create the mixture distribution.\n    distribution = tfd.MixtureSameFamily(\n        mixture_distribution=tfd.Categorical(logits=logits),\n        components_distribution=components_distribution)\n\n    if not self._multivariate:\n      distribution = tfd.Independent(\n          distribution,\n          reinterpreted_batch_ndims=self._reinterpreted_batch_ndims)\n\n    return distribution",
  "class TanhTransformedDistribution(tfd.TransformedDistribution):\n  \"\"\"Distribution followed by tanh.\"\"\"\n\n  def __init__(self, distribution, threshold=.999, validate_args=False):\n    \"\"\"Initialize the distribution.\n\n    Args:\n      distribution: The distribution to transform.\n      threshold: Clipping value of the action when computing the logprob.\n      validate_args: Passed to super class.\n    \"\"\"\n    super().__init__(\n        distribution=distribution,\n        bijector=tfp.bijectors.Tanh(),\n        validate_args=validate_args)\n    # Computes the log of the average probability distribution outside the\n    # clipping range, i.e. on the interval [-inf, -atanh(threshold)] for\n    # log_prob_left and [atanh(threshold), inf] for log_prob_right.\n    self._threshold = threshold\n    inverse_threshold = self.bijector.inverse(threshold)\n    # average(pdf) = p/epsilon\n    # So log(average(pdf)) = log(p) - log(epsilon)\n    log_epsilon = jnp.log(1. - threshold)\n    # Those 2 values are differentiable w.r.t. model parameters, such that the\n    # gradient is defined everywhere.\n    self._log_prob_left = self.distribution.log_cdf(\n        -inverse_threshold) - log_epsilon\n    self._log_prob_right = self.distribution.log_survival_function(\n        inverse_threshold) - log_epsilon\n\n  def log_prob(self, event):\n    # Without this clip there would be NaNs in the inner tf.where and that\n    # causes issues for some reasons.\n    event = jnp.clip(event, -self._threshold, self._threshold)\n    # The inverse image of {threshold} is the interval [atanh(threshold), inf]\n    # which has a probability of \"log_prob_right\" under the given distribution.\n    return jnp.where(\n        event <= -self._threshold, self._log_prob_left,\n        jnp.where(event >= self._threshold, self._log_prob_right,\n                  super().log_prob(event)))\n\n  def mode(self):\n    return self.bijector.forward(self.distribution.mode())\n\n  def entropy(self, seed=None):\n    # We return an estimation using a single sample of the log_det_jacobian.\n    # We can still do some backpropagation with this estimate.\n    return self.distribution.entropy() + self.bijector.forward_log_det_jacobian(\n        self.distribution.sample(seed=seed), event_ndims=0)\n\n  @classmethod\n  def _parameter_properties(cls, dtype: Optional[Any], num_classes=None):\n    td_properties = super()._parameter_properties(dtype,\n                                                  num_classes=num_classes)\n    del td_properties['bijector']\n    return td_properties",
  "class NormalTanhDistribution(hk.Module):\n  \"\"\"Module that produces a TanhTransformedDistribution distribution.\"\"\"\n\n  def __init__(self,\n               num_dimensions: int,\n               min_scale: float = 1e-3,\n               w_init: hk_init.Initializer = hk_init.VarianceScaling(\n                   1.0, 'fan_in', 'uniform'),\n               b_init: hk_init.Initializer = hk_init.Constant(0.)):\n    \"\"\"Initialization.\n\n    Args:\n      num_dimensions: Number of dimensions of a distribution.\n      min_scale: Minimum standard deviation.\n      w_init: Initialization for linear layer weights.\n      b_init: Initialization for linear layer biases.\n    \"\"\"\n    super().__init__(name='Normal')\n    self._min_scale = min_scale\n    self._loc_layer = hk.Linear(num_dimensions, w_init=w_init, b_init=b_init)\n    self._scale_layer = hk.Linear(num_dimensions, w_init=w_init, b_init=b_init)\n\n  def __call__(self, inputs: jnp.ndarray) -> tfd.Distribution:\n    loc = self._loc_layer(inputs)\n    scale = self._scale_layer(inputs)\n    scale = jax.nn.softplus(scale) + self._min_scale\n    distribution = tfd.Normal(loc=loc, scale=scale)\n    return tfd.Independent(\n        TanhTransformedDistribution(distribution), reinterpreted_batch_ndims=1)",
  "class MultivariateNormalDiagHead(hk.Module):\n  \"\"\"Module that produces a tfd.MultivariateNormalDiag distribution.\"\"\"\n\n  def __init__(self,\n               num_dimensions: int,\n               init_scale: float = 0.3,\n               min_scale: float = 1e-6,\n               w_init: hk_init.Initializer = hk_init.VarianceScaling(1e-4),\n               b_init: hk_init.Initializer = hk_init.Constant(0.)):\n    \"\"\"Initialization.\n\n    Args:\n      num_dimensions: Number of dimensions of MVN distribution.\n      init_scale: Initial standard deviation.\n      min_scale: Minimum standard deviation.\n      w_init: Initialization for linear layer weights.\n      b_init: Initialization for linear layer biases.\n    \"\"\"\n    super().__init__(name='MultivariateNormalDiagHead')\n    self._min_scale = min_scale\n    self._init_scale = init_scale\n    self._loc_layer = hk.Linear(num_dimensions, w_init=w_init, b_init=b_init)\n    self._scale_layer = hk.Linear(num_dimensions, w_init=w_init, b_init=b_init)\n\n  def __call__(self, inputs: jnp.ndarray) -> tfd.Distribution:\n    loc = self._loc_layer(inputs)\n    scale = jax.nn.softplus(self._scale_layer(inputs))\n    scale *= self._init_scale / jax.nn.softplus(0.)\n    scale += self._min_scale\n    return tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale)",
  "class CategoricalValueHead(hk.Module):\n  \"\"\"Network head that produces a categorical distribution and value.\"\"\"\n\n  def __init__(\n      self,\n      num_values: int,\n      name: Optional[str] = None,\n  ):\n    super().__init__(name=name)\n    self._logit_layer = hk.Linear(num_values)\n    self._value_layer = hk.Linear(1)\n\n  def __call__(self, inputs: jnp.ndarray):\n    logits = self._logit_layer(inputs)\n    value = jnp.squeeze(self._value_layer(inputs), axis=-1)\n    return (tfd.Categorical(logits=logits), value)",
  "class DiscreteValued(hk.Module):\n  \"\"\"C51-style head.\n\n  For each action, it produces the logits for a discrete distribution over\n  atoms. Therefore, the returned logits represents several distributions, one\n  for each action.\n  \"\"\"\n\n  def __init__(\n      self,\n      num_actions: int,\n      head_units: int = 512,\n      num_atoms: int = 51,\n      v_min: float = -1.0,\n      v_max: float = 1.0,\n  ):\n    super().__init__('DiscreteValued')\n    self._num_actions = num_actions\n    self._num_atoms = num_atoms\n    self._atoms = jnp.linspace(v_min, v_max, self._num_atoms)\n    self._network = hk.nets.MLP([head_units, num_actions * num_atoms])\n\n  def __call__(self, inputs: jnp.ndarray):\n    q_logits = self._network(inputs)\n    q_logits = jnp.reshape(q_logits, (-1, self._num_actions, self._num_atoms))\n    q_dist = jax.nn.softmax(q_logits)\n    q_values = jnp.sum(q_dist * self._atoms, axis=2)\n    q_values = jax.lax.stop_gradient(q_values)\n    return q_values, q_logits, self._atoms",
  "class CategoricalCriticHead(hk.Module):\n  \"\"\"Critic head that uses a categorical to represent action values.\"\"\"\n\n  def __init__(self,\n               num_bins: int = 601,\n               vmax: Optional[float] = None,\n               vmin: Optional[float] = None,\n               w_init: hk_init.Initializer = hk_init.VarianceScaling(1e-5)):\n    super().__init__(name='categorical_critic_head')\n    vmax = vmax if vmax is not None else 0.5 * (num_bins - 1)\n    vmin = vmin if vmin is not None else -1.0 * vmax\n\n    self._head = DiscreteValuedTfpHead(\n        vmin=vmin,\n        vmax=vmax,\n        logits_shape=(1,),\n        num_atoms=num_bins,\n        w_init=w_init)\n\n  def __call__(self, embedding: chex.Array) -> tfd.Distribution:\n    output = self._head(embedding)\n    return output",
  "class DiscreteValuedTfpHead(hk.Module):\n  \"\"\"Represents a parameterized discrete valued distribution.\n\n  The returned distribution is essentially a `tfd.Categorical` that knows its\n  support and thus can compute the mean value.\n  \"\"\"\n\n  def __init__(self,\n               vmin: float,\n               vmax: float,\n               num_atoms: int,\n               logits_shape: Optional[Sequence[int]] = None,\n               w_init: Optional[Initializer] = None,\n               b_init: Optional[Initializer] = None):\n    \"\"\"Initialization.\n\n    If vmin and vmax have shape S, this will store the category values as a\n    Tensor of shape (S*, num_atoms).\n\n    Args:\n      vmin: Minimum of the value range\n      vmax: Maximum of the value range\n      num_atoms: The atom values associated with each bin.\n      logits_shape: The shape of the logits, excluding batch and num_atoms\n        dimensions.\n      w_init: Initialization for linear layer weights.\n      b_init: Initialization for linear layer biases.\n    \"\"\"\n    super().__init__(name='DiscreteValuedHead')\n    self._values = np.linspace(vmin, vmax, num=num_atoms, axis=-1)\n    if not logits_shape:\n      logits_shape = ()\n    self._logits_shape = logits_shape + (num_atoms,)\n    self._w_init = w_init\n    self._b_init = b_init\n\n  def __call__(self, inputs: chex.Array) -> tfd.Distribution:\n    net = hk.Linear(\n        np.prod(self._logits_shape), w_init=self._w_init, b_init=self._b_init)\n    logits = net(inputs)\n    logits = hk.Reshape(self._logits_shape, preserve_dims=1)(logits)\n    return DiscreteValuedTfpDistribution(values=self._values, logits=logits)",
  "class DiscreteValuedTfpDistribution(tfd.Categorical):\n  \"\"\"This is a generalization of a categorical distribution.\n\n  The support for the DiscreteValued distribution can be any real valued range,\n  whereas the categorical distribution has support [0, n_categories - 1] or\n  [1, n_categories]. This generalization allows us to take the mean of the\n  distribution over its support.\n  \"\"\"\n\n  def __init__(self,\n               values: chex.Array,\n               logits: Optional[chex.Array] = None,\n               probs: Optional[chex.Array] = None,\n               name: str = 'DiscreteValuedDistribution'):\n    \"\"\"Initialization.\n\n    Args:\n      values: Values making up support of the distribution. Should have a shape\n        compatible with logits.\n      logits: An N-D Tensor, N >= 1, representing the log probabilities of a set\n        of Categorical distributions. The first N - 1 dimensions index into a\n        batch of independent distributions and the last dimension indexes into\n        the classes.\n      probs: An N-D Tensor, N >= 1, representing the probabilities of a set of\n        Categorical distributions. The first N - 1 dimensions index into a batch\n        of independent distributions and the last dimension represents a vector\n        of probabilities for each class. Only one of logits or probs should be\n        passed in.\n      name: Name of the distribution object.\n    \"\"\"\n    parameters = dict(locals())\n    self._values = np.asarray(values)\n\n    if logits is not None:\n      logits = jnp.asarray(logits)\n      chex.assert_shape(logits, (..., *self._values.shape))\n\n    if probs is not None:\n      probs = jnp.asarray(probs)\n      chex.assert_shape(probs, (..., *self._values.shape))\n\n    super().__init__(logits=logits, probs=probs, name=name)\n\n    self._parameters = parameters\n\n  @property\n  def values(self):\n    return self._values\n\n  @classmethod\n  def _parameter_properties(cls, dtype, num_classes=None):\n    return dict(\n        values=tfp.util.ParameterProperties(\n            event_ndims=None,\n            shape_fn=lambda shape: (num_classes,),\n            specifies_shape=True),\n        logits=tfp.util.ParameterProperties(event_ndims=1),\n        probs=tfp.util.ParameterProperties(event_ndims=1, is_preferred=False))\n\n  def _sample_n(self, key: chex.PRNGKey, n: int) -> chex.Array:\n    indices = super()._sample_n(key=key, n=n)\n    return jnp.take_along_axis(self._values, indices, axis=-1)\n\n  def mean(self) -> chex.Array:\n    \"\"\"Overrides the Categorical mean by incorporating category values.\"\"\"\n    return jnp.sum(self.probs_parameter() * self._values, axis=-1)\n\n  def variance(self) -> chex.Array:\n    \"\"\"Overrides the Categorical variance by incorporating category values.\"\"\"\n    dist_squared = jnp.square(jnp.expand_dims(self.mean(), -1) - self._values)\n    return jnp.sum(self.probs_parameter() * dist_squared, axis=-1)\n\n  def _event_shape(self):\n    return jnp.zeros((), dtype=jnp.int32)\n\n  def _event_shape_tensor(self):\n    return []",
  "def __init__(\n      self,\n      num_values: Union[int, List[int]],\n      dtype: Optional[Any] = jnp.int32,\n      w_init: Optional[Initializer] = None,\n      name: Optional[str] = None,\n  ):\n    super().__init__(name=name)\n    self._dtype = dtype\n    self._logit_shape = num_values\n    self._linear = hk.Linear(np.prod(num_values), w_init=w_init)",
  "def __call__(self, inputs: jnp.ndarray) -> tfd.Distribution:\n    logits = self._linear(inputs)\n    if not isinstance(self._logit_shape, int):\n      logits = hk.Reshape(self._logit_shape)(logits)\n    return tfd.Categorical(logits=logits, dtype=self._dtype)",
  "def __init__(self,\n               num_dimensions: int,\n               num_components: int,\n               multivariate: bool,\n               init_scale: Optional[float] = None,\n               append_singleton_event_dim: bool = False,\n               reinterpreted_batch_ndims: Optional[int] = None,\n               transformation_fn: Optional[Callable[[tfd.Distribution],\n                                                    tfd.Distribution]] = None,\n               name: str = 'GaussianMixture'):\n    \"\"\"Initialization.\n\n    Args:\n      num_dimensions: dimensionality of the output distribution\n      num_components: number of mixture components.\n      multivariate: whether the resulting distribution is multivariate or not.\n      init_scale: the initial scale for the Gaussian mixture components.\n      append_singleton_event_dim: (univariate only) Whether to add an extra\n        singleton dimension to the event shape.\n      reinterpreted_batch_ndims: (univariate only) Number of batch dimensions to\n        reinterpret as event dimensions.\n      transformation_fn: Distribution transform such as TanhTransformation\n        applied to individual components.\n      name: name of the module passed to snt.Module parent class.\n    \"\"\"\n    super().__init__(name=name)\n\n    self._num_dimensions = num_dimensions\n    self._num_components = num_components\n    self._multivariate = multivariate\n    self._append_singleton_event_dim = append_singleton_event_dim\n    self._reinterpreted_batch_ndims = reinterpreted_batch_ndims\n\n    if init_scale is not None:\n      self._scale_factor = init_scale / jax.nn.softplus(0.)\n    else:\n      self._scale_factor = 1.0  # Corresponds to init_scale = softplus(0).\n\n    self._transformation_fn = transformation_fn",
  "def __call__(self,\n               inputs: jnp.ndarray,\n               low_noise_policy: bool = False) -> tfd.Distribution:\n    \"\"\"Run the networks through inputs.\n\n    Args:\n      inputs: hidden activations of the policy network body.\n      low_noise_policy: whether to set vanishingly small scales for each\n        component. If this flag is set to True, the policy is effectively run\n        without Gaussian noise.\n\n    Returns:\n      Mixture Gaussian distribution.\n    \"\"\"\n\n    # Define the weight initializer.\n    w_init = hk.initializers.VarianceScaling(scale=1e-5)\n\n    # Create a layer that outputs the unnormalized log-weights.\n    if self._multivariate:\n      logits_size = self._num_components\n    else:\n      logits_size = self._num_dimensions * self._num_components\n    logit_layer = hk.Linear(logits_size, w_init=w_init)\n\n    # Create two layers that outputs a location and a scale, respectively, for\n    # each dimension and each component.\n    loc_layer = hk.Linear(\n        self._num_dimensions * self._num_components, w_init=w_init)\n    scale_layer = hk.Linear(\n        self._num_dimensions * self._num_components, w_init=w_init)\n\n    # Compute logits, locs, and scales if necessary.\n    logits = logit_layer(inputs)\n    locs = loc_layer(inputs)\n\n    # When a low_noise_policy is requested, set the scales to its minimum value.\n    if low_noise_policy:\n      scales = jnp.full(locs.shape, _MIN_SCALE)\n    else:\n      scales = scale_layer(inputs)\n      scales = self._scale_factor * jax.nn.softplus(scales) + _MIN_SCALE\n\n    if self._multivariate:\n      components_class = tfd.MultivariateNormalDiag\n      shape = [-1, self._num_components, self._num_dimensions]  # [B, C, D]\n      # In this case, no need to reshape logits as they are in the correct shape\n      # already, namely [batch_size, num_components].\n    else:\n      components_class = tfd.Normal\n      shape = [-1, self._num_dimensions, self._num_components]  # [B, D, C]\n      if self._append_singleton_event_dim:\n        shape.insert(2, 1)  # [B, D, 1, C]\n      logits = logits.reshape(shape)\n\n    # Reshape the mixture's location and scale parameters appropriately.\n    locs = locs.reshape(shape)\n    scales = scales.reshape(shape)\n\n    if self._multivariate:\n      components_distribution = components_class(loc=locs, scale_diag=scales)\n    else:\n      components_distribution = components_class(loc=locs, scale=scales)\n\n    # Transformed the component distributions in the mixture.\n    if self._transformation_fn:\n      components_distribution = self._transformation_fn(components_distribution)\n\n    # Create the mixture distribution.\n    distribution = tfd.MixtureSameFamily(\n        mixture_distribution=tfd.Categorical(logits=logits),\n        components_distribution=components_distribution)\n\n    if not self._multivariate:\n      distribution = tfd.Independent(\n          distribution,\n          reinterpreted_batch_ndims=self._reinterpreted_batch_ndims)\n\n    return distribution",
  "def __init__(self, distribution, threshold=.999, validate_args=False):\n    \"\"\"Initialize the distribution.\n\n    Args:\n      distribution: The distribution to transform.\n      threshold: Clipping value of the action when computing the logprob.\n      validate_args: Passed to super class.\n    \"\"\"\n    super().__init__(\n        distribution=distribution,\n        bijector=tfp.bijectors.Tanh(),\n        validate_args=validate_args)\n    # Computes the log of the average probability distribution outside the\n    # clipping range, i.e. on the interval [-inf, -atanh(threshold)] for\n    # log_prob_left and [atanh(threshold), inf] for log_prob_right.\n    self._threshold = threshold\n    inverse_threshold = self.bijector.inverse(threshold)\n    # average(pdf) = p/epsilon\n    # So log(average(pdf)) = log(p) - log(epsilon)\n    log_epsilon = jnp.log(1. - threshold)\n    # Those 2 values are differentiable w.r.t. model parameters, such that the\n    # gradient is defined everywhere.\n    self._log_prob_left = self.distribution.log_cdf(\n        -inverse_threshold) - log_epsilon\n    self._log_prob_right = self.distribution.log_survival_function(\n        inverse_threshold) - log_epsilon",
  "def log_prob(self, event):\n    # Without this clip there would be NaNs in the inner tf.where and that\n    # causes issues for some reasons.\n    event = jnp.clip(event, -self._threshold, self._threshold)\n    # The inverse image of {threshold} is the interval [atanh(threshold), inf]\n    # which has a probability of \"log_prob_right\" under the given distribution.\n    return jnp.where(\n        event <= -self._threshold, self._log_prob_left,\n        jnp.where(event >= self._threshold, self._log_prob_right,\n                  super().log_prob(event)))",
  "def mode(self):\n    return self.bijector.forward(self.distribution.mode())",
  "def entropy(self, seed=None):\n    # We return an estimation using a single sample of the log_det_jacobian.\n    # We can still do some backpropagation with this estimate.\n    return self.distribution.entropy() + self.bijector.forward_log_det_jacobian(\n        self.distribution.sample(seed=seed), event_ndims=0)",
  "def _parameter_properties(cls, dtype: Optional[Any], num_classes=None):\n    td_properties = super()._parameter_properties(dtype,\n                                                  num_classes=num_classes)\n    del td_properties['bijector']\n    return td_properties",
  "def __init__(self,\n               num_dimensions: int,\n               min_scale: float = 1e-3,\n               w_init: hk_init.Initializer = hk_init.VarianceScaling(\n                   1.0, 'fan_in', 'uniform'),\n               b_init: hk_init.Initializer = hk_init.Constant(0.)):\n    \"\"\"Initialization.\n\n    Args:\n      num_dimensions: Number of dimensions of a distribution.\n      min_scale: Minimum standard deviation.\n      w_init: Initialization for linear layer weights.\n      b_init: Initialization for linear layer biases.\n    \"\"\"\n    super().__init__(name='Normal')\n    self._min_scale = min_scale\n    self._loc_layer = hk.Linear(num_dimensions, w_init=w_init, b_init=b_init)\n    self._scale_layer = hk.Linear(num_dimensions, w_init=w_init, b_init=b_init)",
  "def __call__(self, inputs: jnp.ndarray) -> tfd.Distribution:\n    loc = self._loc_layer(inputs)\n    scale = self._scale_layer(inputs)\n    scale = jax.nn.softplus(scale) + self._min_scale\n    distribution = tfd.Normal(loc=loc, scale=scale)\n    return tfd.Independent(\n        TanhTransformedDistribution(distribution), reinterpreted_batch_ndims=1)",
  "def __init__(self,\n               num_dimensions: int,\n               init_scale: float = 0.3,\n               min_scale: float = 1e-6,\n               w_init: hk_init.Initializer = hk_init.VarianceScaling(1e-4),\n               b_init: hk_init.Initializer = hk_init.Constant(0.)):\n    \"\"\"Initialization.\n\n    Args:\n      num_dimensions: Number of dimensions of MVN distribution.\n      init_scale: Initial standard deviation.\n      min_scale: Minimum standard deviation.\n      w_init: Initialization for linear layer weights.\n      b_init: Initialization for linear layer biases.\n    \"\"\"\n    super().__init__(name='MultivariateNormalDiagHead')\n    self._min_scale = min_scale\n    self._init_scale = init_scale\n    self._loc_layer = hk.Linear(num_dimensions, w_init=w_init, b_init=b_init)\n    self._scale_layer = hk.Linear(num_dimensions, w_init=w_init, b_init=b_init)",
  "def __call__(self, inputs: jnp.ndarray) -> tfd.Distribution:\n    loc = self._loc_layer(inputs)\n    scale = jax.nn.softplus(self._scale_layer(inputs))\n    scale *= self._init_scale / jax.nn.softplus(0.)\n    scale += self._min_scale\n    return tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale)",
  "def __init__(\n      self,\n      num_values: int,\n      name: Optional[str] = None,\n  ):\n    super().__init__(name=name)\n    self._logit_layer = hk.Linear(num_values)\n    self._value_layer = hk.Linear(1)",
  "def __call__(self, inputs: jnp.ndarray):\n    logits = self._logit_layer(inputs)\n    value = jnp.squeeze(self._value_layer(inputs), axis=-1)\n    return (tfd.Categorical(logits=logits), value)",
  "def __init__(\n      self,\n      num_actions: int,\n      head_units: int = 512,\n      num_atoms: int = 51,\n      v_min: float = -1.0,\n      v_max: float = 1.0,\n  ):\n    super().__init__('DiscreteValued')\n    self._num_actions = num_actions\n    self._num_atoms = num_atoms\n    self._atoms = jnp.linspace(v_min, v_max, self._num_atoms)\n    self._network = hk.nets.MLP([head_units, num_actions * num_atoms])",
  "def __call__(self, inputs: jnp.ndarray):\n    q_logits = self._network(inputs)\n    q_logits = jnp.reshape(q_logits, (-1, self._num_actions, self._num_atoms))\n    q_dist = jax.nn.softmax(q_logits)\n    q_values = jnp.sum(q_dist * self._atoms, axis=2)\n    q_values = jax.lax.stop_gradient(q_values)\n    return q_values, q_logits, self._atoms",
  "def __init__(self,\n               num_bins: int = 601,\n               vmax: Optional[float] = None,\n               vmin: Optional[float] = None,\n               w_init: hk_init.Initializer = hk_init.VarianceScaling(1e-5)):\n    super().__init__(name='categorical_critic_head')\n    vmax = vmax if vmax is not None else 0.5 * (num_bins - 1)\n    vmin = vmin if vmin is not None else -1.0 * vmax\n\n    self._head = DiscreteValuedTfpHead(\n        vmin=vmin,\n        vmax=vmax,\n        logits_shape=(1,),\n        num_atoms=num_bins,\n        w_init=w_init)",
  "def __call__(self, embedding: chex.Array) -> tfd.Distribution:\n    output = self._head(embedding)\n    return output",
  "def __init__(self,\n               vmin: float,\n               vmax: float,\n               num_atoms: int,\n               logits_shape: Optional[Sequence[int]] = None,\n               w_init: Optional[Initializer] = None,\n               b_init: Optional[Initializer] = None):\n    \"\"\"Initialization.\n\n    If vmin and vmax have shape S, this will store the category values as a\n    Tensor of shape (S*, num_atoms).\n\n    Args:\n      vmin: Minimum of the value range\n      vmax: Maximum of the value range\n      num_atoms: The atom values associated with each bin.\n      logits_shape: The shape of the logits, excluding batch and num_atoms\n        dimensions.\n      w_init: Initialization for linear layer weights.\n      b_init: Initialization for linear layer biases.\n    \"\"\"\n    super().__init__(name='DiscreteValuedHead')\n    self._values = np.linspace(vmin, vmax, num=num_atoms, axis=-1)\n    if not logits_shape:\n      logits_shape = ()\n    self._logits_shape = logits_shape + (num_atoms,)\n    self._w_init = w_init\n    self._b_init = b_init",
  "def __call__(self, inputs: chex.Array) -> tfd.Distribution:\n    net = hk.Linear(\n        np.prod(self._logits_shape), w_init=self._w_init, b_init=self._b_init)\n    logits = net(inputs)\n    logits = hk.Reshape(self._logits_shape, preserve_dims=1)(logits)\n    return DiscreteValuedTfpDistribution(values=self._values, logits=logits)",
  "def __init__(self,\n               values: chex.Array,\n               logits: Optional[chex.Array] = None,\n               probs: Optional[chex.Array] = None,\n               name: str = 'DiscreteValuedDistribution'):\n    \"\"\"Initialization.\n\n    Args:\n      values: Values making up support of the distribution. Should have a shape\n        compatible with logits.\n      logits: An N-D Tensor, N >= 1, representing the log probabilities of a set\n        of Categorical distributions. The first N - 1 dimensions index into a\n        batch of independent distributions and the last dimension indexes into\n        the classes.\n      probs: An N-D Tensor, N >= 1, representing the probabilities of a set of\n        Categorical distributions. The first N - 1 dimensions index into a batch\n        of independent distributions and the last dimension represents a vector\n        of probabilities for each class. Only one of logits or probs should be\n        passed in.\n      name: Name of the distribution object.\n    \"\"\"\n    parameters = dict(locals())\n    self._values = np.asarray(values)\n\n    if logits is not None:\n      logits = jnp.asarray(logits)\n      chex.assert_shape(logits, (..., *self._values.shape))\n\n    if probs is not None:\n      probs = jnp.asarray(probs)\n      chex.assert_shape(probs, (..., *self._values.shape))\n\n    super().__init__(logits=logits, probs=probs, name=name)\n\n    self._parameters = parameters",
  "def values(self):\n    return self._values",
  "def _parameter_properties(cls, dtype, num_classes=None):\n    return dict(\n        values=tfp.util.ParameterProperties(\n            event_ndims=None,\n            shape_fn=lambda shape: (num_classes,),\n            specifies_shape=True),\n        logits=tfp.util.ParameterProperties(event_ndims=1),\n        probs=tfp.util.ParameterProperties(event_ndims=1, is_preferred=False))",
  "def _sample_n(self, key: chex.PRNGKey, n: int) -> chex.Array:\n    indices = super()._sample_n(key=key, n=n)\n    return jnp.take_along_axis(self._values, indices, axis=-1)",
  "def mean(self) -> chex.Array:\n    \"\"\"Overrides the Categorical mean by incorporating category values.\"\"\"\n    return jnp.sum(self.probs_parameter() * self._values, axis=-1)",
  "def variance(self) -> chex.Array:\n    \"\"\"Overrides the Categorical variance by incorporating category values.\"\"\"\n    dist_squared = jnp.square(jnp.expand_dims(self.mean(), -1) - self._values)\n    return jnp.sum(self.probs_parameter() * dist_squared, axis=-1)",
  "def _event_shape(self):\n    return jnp.zeros((), dtype=jnp.int32)",
  "def _event_shape_tensor(self):\n    return []",
  "class ResidualBlock(hk.Module):\n  \"\"\"Residual block of operations, e.g. convolutional or MLP.\"\"\"\n\n  def __init__(self,\n               make_inner_op: MakeInnerOp,\n               non_linearity: NonLinearity = jax.nn.relu,\n               use_layer_norm: bool = False,\n               name: str = 'residual_block'):\n    super().__init__(name=name)\n    self.inner_op1 = make_inner_op()\n    self.inner_op2 = make_inner_op()\n    self.non_linearity = non_linearity\n    self.use_layer_norm = use_layer_norm\n\n    if use_layer_norm:\n      self.layernorm1 = hk.LayerNorm(\n          axis=(-3, -2, -1), create_scale=True, create_offset=True, eps=1e-6)\n      self.layernorm2 = hk.LayerNorm(\n          axis=(-3, -2, -1), create_scale=True, create_offset=True, eps=1e-6)\n\n  def __call__(self, x: jnp.ndarray):\n    output = x\n\n    # First layer in residual block.\n    if self.use_layer_norm:\n      output = self.layernorm1(output)\n    output = self.non_linearity(output)\n    output = self.inner_op1(output)\n\n    # Second layer in residual block.\n    if self.use_layer_norm:\n      output = self.layernorm2(output)\n    output = self.non_linearity(output)\n    output = self.inner_op2(output)\n    return x + output",
  "class DownsamplingStrategy(enum.Enum):\n  AVG_POOL = 'avg_pool'\n  CONV_MAX = 'conv+max'  # Used in IMPALA\n  LAYERNORM_RELU_CONV = 'layernorm+relu+conv'  # Used in MuZero\n  CONV = 'conv'",
  "def make_downsampling_layer(\n    strategy: Union[str, DownsamplingStrategy],\n    output_channels: int,\n) -> hk.SupportsCall:\n  \"\"\"Returns a sequence of modules corresponding to the desired downsampling.\"\"\"\n  strategy = DownsamplingStrategy(strategy)\n\n  if strategy is DownsamplingStrategy.AVG_POOL:\n    return hk.AvgPool(window_shape=(3, 3, 1), strides=(2, 2, 1), padding='SAME')\n\n  elif strategy is DownsamplingStrategy.CONV:\n    return hk.Sequential([\n        hk.Conv2D(\n            output_channels,\n            kernel_shape=3,\n            stride=2,\n            w_init=hk.initializers.TruncatedNormal(1e-2)),\n    ])\n\n  elif strategy is DownsamplingStrategy.LAYERNORM_RELU_CONV:\n    return hk.Sequential([\n        hk.LayerNorm(\n            axis=(-3, -2, -1), create_scale=True, create_offset=True, eps=1e-6),\n        jax.nn.relu,\n        hk.Conv2D(\n            output_channels,\n            kernel_shape=3,\n            stride=2,\n            w_init=hk.initializers.TruncatedNormal(1e-2)),\n    ])\n\n  elif strategy is DownsamplingStrategy.CONV_MAX:\n    return hk.Sequential([\n        hk.Conv2D(output_channels, kernel_shape=3, stride=1),\n        hk.MaxPool(window_shape=(3, 3, 1), strides=(2, 2, 1), padding='SAME')\n    ])\n  else:\n    raise ValueError('Unrecognized downsampling strategy. Expected one of'\n                     f' {[strategy.value for strategy in DownsamplingStrategy]}'\n                     f' but received {strategy}.')",
  "class ResNetTorso(hk.Module):\n  \"\"\"ResNetTorso for visual inputs, inspired by the IMPALA paper.\"\"\"\n\n  def __init__(self,\n               channels_per_group: Sequence[int] = (16, 32, 32),\n               blocks_per_group: Sequence[int] = (2, 2, 2),\n               downsampling_strategies: Sequence[DownsamplingStrategy] = (\n                   DownsamplingStrategy.CONV_MAX,) * 3,\n               use_layer_norm: bool = False,\n               name: str = 'resnet_torso'):\n    super().__init__(name=name)\n    self._channels_per_group = channels_per_group\n    self._blocks_per_group = blocks_per_group\n    self._downsampling_strategies = downsampling_strategies\n    self._use_layer_norm = use_layer_norm\n\n    if (len(channels_per_group) != len(blocks_per_group) or\n        len(channels_per_group) != len(downsampling_strategies)):\n      raise ValueError('Length of channels_per_group, blocks_per_group, and '\n                       'downsampling_strategies must be equal. '\n                       f'Got channels_per_group={channels_per_group}, '\n                       f'blocks_per_group={blocks_per_group}, and'\n                       f'downsampling_strategies={downsampling_strategies}.')\n\n  def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n    output = inputs\n    channels_blocks_strategies = zip(self._channels_per_group,\n                                     self._blocks_per_group,\n                                     self._downsampling_strategies)\n\n    for i, (num_channels, num_blocks,\n            strategy) in enumerate(channels_blocks_strategies):\n      output = make_downsampling_layer(strategy, num_channels)(output)\n\n      for j in range(num_blocks):\n        output = ResidualBlock(\n            make_inner_op=functools.partial(\n                hk.Conv2D, output_channels=num_channels, kernel_shape=3),\n            use_layer_norm=self._use_layer_norm,\n            name=f'residual_{i}_{j}')(\n                output)\n\n    return output",
  "def __init__(self,\n               make_inner_op: MakeInnerOp,\n               non_linearity: NonLinearity = jax.nn.relu,\n               use_layer_norm: bool = False,\n               name: str = 'residual_block'):\n    super().__init__(name=name)\n    self.inner_op1 = make_inner_op()\n    self.inner_op2 = make_inner_op()\n    self.non_linearity = non_linearity\n    self.use_layer_norm = use_layer_norm\n\n    if use_layer_norm:\n      self.layernorm1 = hk.LayerNorm(\n          axis=(-3, -2, -1), create_scale=True, create_offset=True, eps=1e-6)\n      self.layernorm2 = hk.LayerNorm(\n          axis=(-3, -2, -1), create_scale=True, create_offset=True, eps=1e-6)",
  "def __call__(self, x: jnp.ndarray):\n    output = x\n\n    # First layer in residual block.\n    if self.use_layer_norm:\n      output = self.layernorm1(output)\n    output = self.non_linearity(output)\n    output = self.inner_op1(output)\n\n    # Second layer in residual block.\n    if self.use_layer_norm:\n      output = self.layernorm2(output)\n    output = self.non_linearity(output)\n    output = self.inner_op2(output)\n    return x + output",
  "def __init__(self,\n               channels_per_group: Sequence[int] = (16, 32, 32),\n               blocks_per_group: Sequence[int] = (2, 2, 2),\n               downsampling_strategies: Sequence[DownsamplingStrategy] = (\n                   DownsamplingStrategy.CONV_MAX,) * 3,\n               use_layer_norm: bool = False,\n               name: str = 'resnet_torso'):\n    super().__init__(name=name)\n    self._channels_per_group = channels_per_group\n    self._blocks_per_group = blocks_per_group\n    self._downsampling_strategies = downsampling_strategies\n    self._use_layer_norm = use_layer_norm\n\n    if (len(channels_per_group) != len(blocks_per_group) or\n        len(channels_per_group) != len(downsampling_strategies)):\n      raise ValueError('Length of channels_per_group, blocks_per_group, and '\n                       'downsampling_strategies must be equal. '\n                       f'Got channels_per_group={channels_per_group}, '\n                       f'blocks_per_group={blocks_per_group}, and'\n                       f'downsampling_strategies={downsampling_strategies}.')",
  "def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n    output = inputs\n    channels_blocks_strategies = zip(self._channels_per_group,\n                                     self._blocks_per_group,\n                                     self._downsampling_strategies)\n\n    for i, (num_channels, num_blocks,\n            strategy) in enumerate(channels_blocks_strategies):\n      output = make_downsampling_layer(strategy, num_channels)(output)\n\n      for j in range(num_blocks):\n        output = ResidualBlock(\n            make_inner_op=functools.partial(\n                hk.Conv2D, output_channels=num_channels, kernel_shape=3),\n            use_layer_norm=self._use_layer_norm,\n            name=f'residual_{i}_{j}')(\n                output)\n\n    return output",
  "class IteratorUtilsTest(absltest.TestCase):\n\n  def test_iterator_zipping(self):\n\n    def get_iters():\n      x = iter(range(0, 10))\n      y = iter(range(20, 30))\n      return [x, y]\n\n    zipped = zip(*get_iters())\n    unzipped = iterator_utils.unzip_iterators(zipped, num_sub_iterators=2)\n    expected_x, expected_y = get_iters()\n    np.testing.assert_equal(list(unzipped[0]), list(expected_x))\n    np.testing.assert_equal(list(unzipped[1]), list(expected_y))",
  "def test_iterator_zipping(self):\n\n    def get_iters():\n      x = iter(range(0, 10))\n      y = iter(range(20, 30))\n      return [x, y]\n\n    zipped = zip(*get_iters())\n    unzipped = iterator_utils.unzip_iterators(zipped, num_sub_iterators=2)\n    expected_x, expected_y = get_iters()\n    np.testing.assert_equal(list(unzipped[0]), list(expected_x))\n    np.testing.assert_equal(list(unzipped[1]), list(expected_y))",
  "def get_iters():\n      x = iter(range(0, 10))\n      y = iter(range(20, 30))\n      return [x, y]",
  "def runtime_terminator(callback: Optional[_Handler] = None):\n  \"\"\"Runtime terminator used for stopping computation upon agent termination.\n\n    Runtime terminator optionally executed a provided `callback` and then raises\n    `SystemExit` exception in the thread performing the computation.\n\n  Args:\n    callback: callback to execute before raising exception.\n\n  Yields:\n      None.\n  \"\"\"\n  worker_id = threading.get_ident()\n  def signal_handler():\n    if callback:\n      callback()\n    res = ctypes.pythonapi.PyThreadState_SetAsyncExc(\n        ctypes.c_long(worker_id), ctypes.py_object(SystemExit))\n    assert res < 2, 'Stopping worker failed'\n  launchpad.register_stop_handler(signal_handler)\n  yield\n  launchpad.unregister_stop_handler(signal_handler)",
  "def signal_handler():\n    if callback:\n      callback()\n    res = ctypes.pythonapi.PyThreadState_SetAsyncExc(\n        ctypes.c_long(worker_id), ctypes.py_object(SystemExit))\n    assert res < 2, 'Stopping worker failed'",
  "class ReverbUtilsTest(absltest.TestCase):\n\n  def test_make_replay_table_preserves_table_info(self):\n    limiter = reverb.rate_limiters.SampleToInsertRatio(\n        samples_per_insert=1, min_size_to_sample=2, error_buffer=(0, 10))\n    table = reverb.Table(\n        name='test',\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=10,\n        rate_limiter=limiter)\n    new_table = reverb_utils.make_replay_table_from_info(table.info)\n    new_info = new_table.info\n\n    # table_worker_time is not set by the above utility since this is meant to\n    # be monitoring information about any given table. So instead we copy this\n    # so that the assertion below checks that everything else matches.\n\n    new_info.table_worker_time.sleeping_ms = (\n        table.info.table_worker_time.sleeping_ms)\n\n    self.assertEqual(new_info, table.info)\n\n  _EMPTY_INFO = reverb.SampleInfo(*[() for _ in reverb.SampleInfo.tf_dtypes()])\n  _DUMMY_OBS = np.array([[[0], [1], [2]]])\n  _DUMMY_ACTION = np.array([[[3], [4], [5]]])\n  _DUMMY_REWARD = np.array([[6, 7, 8]])\n  _DUMMY_DISCOUNT = np.array([[.99, .99, .99]])\n  _DUMMY_NEXT_OBS = np.array([[[1], [2], [0]]])\n  _DUMMY_RETURN = np.array([[20.77, 14.92, 8.]])\n\n  def _create_dummy_steps(self):\n    return reverb_adders.Step(\n        observation=self._DUMMY_OBS,\n        action=self._DUMMY_ACTION,\n        reward=self._DUMMY_REWARD,\n        discount=self._DUMMY_DISCOUNT,\n        start_of_episode=True,\n        extras={'return': self._DUMMY_RETURN})\n\n  def _create_dummy_transitions(self):\n    return types.Transition(\n        observation=self._DUMMY_OBS,\n        action=self._DUMMY_ACTION,\n        reward=self._DUMMY_REWARD,\n        discount=self._DUMMY_DISCOUNT,\n        next_observation=self._DUMMY_NEXT_OBS,\n        extras={'return': self._DUMMY_RETURN})\n\n  def test_replay_sample_to_sars_transition_is_sequence(self):\n    fake_sample = reverb.ReplaySample(\n        info=self._EMPTY_INFO, data=self._create_dummy_steps())\n    fake_transition = self._create_dummy_transitions()\n    transition_from_sample = reverb_utils.replay_sample_to_sars_transition(\n        fake_sample, is_sequence=True)\n    tree.map_structure(np.testing.assert_array_equal, transition_from_sample,\n                       fake_transition)",
  "def test_make_replay_table_preserves_table_info(self):\n    limiter = reverb.rate_limiters.SampleToInsertRatio(\n        samples_per_insert=1, min_size_to_sample=2, error_buffer=(0, 10))\n    table = reverb.Table(\n        name='test',\n        sampler=reverb.selectors.Uniform(),\n        remover=reverb.selectors.Fifo(),\n        max_size=10,\n        rate_limiter=limiter)\n    new_table = reverb_utils.make_replay_table_from_info(table.info)\n    new_info = new_table.info\n\n    # table_worker_time is not set by the above utility since this is meant to\n    # be monitoring information about any given table. So instead we copy this\n    # so that the assertion below checks that everything else matches.\n\n    new_info.table_worker_time.sleeping_ms = (\n        table.info.table_worker_time.sleeping_ms)\n\n    self.assertEqual(new_info, table.info)",
  "def _create_dummy_steps(self):\n    return reverb_adders.Step(\n        observation=self._DUMMY_OBS,\n        action=self._DUMMY_ACTION,\n        reward=self._DUMMY_REWARD,\n        discount=self._DUMMY_DISCOUNT,\n        start_of_episode=True,\n        extras={'return': self._DUMMY_RETURN})",
  "def _create_dummy_transitions(self):\n    return types.Transition(\n        observation=self._DUMMY_OBS,\n        action=self._DUMMY_ACTION,\n        reward=self._DUMMY_REWARD,\n        discount=self._DUMMY_DISCOUNT,\n        next_observation=self._DUMMY_NEXT_OBS,\n        extras={'return': self._DUMMY_RETURN})",
  "def test_replay_sample_to_sars_transition_is_sequence(self):\n    fake_sample = reverb.ReplaySample(\n        info=self._EMPTY_INFO, data=self._create_dummy_steps())\n    fake_transition = self._create_dummy_transitions()\n    transition_from_sample = reverb_utils.replay_sample_to_sars_transition(\n        fake_sample, is_sequence=True)\n    tree.map_structure(np.testing.assert_array_equal, transition_from_sample,\n                       fake_transition)",
  "def record_class_usage(cls: Type[T]) -> Type[T]:\n  return cls",
  "def unzip_iterators(zipped_iterators: Iterator[Sequence[Any]],\n                    num_sub_iterators: int) -> List[Iterator[Any]]:\n  \"\"\"Returns unzipped iterators.\n\n  Note that simply returning:\n    [(x[i] for x in iter_tuple[i]) for i in range(num_sub_iterators)]\n  seems to cause all iterators to point to the final value of i, thus causing\n  all sub_learners to consume data from this final iterator.\n\n  Args:\n    zipped_iterators: zipped iterators (e.g., from zip_iterators()).\n    num_sub_iterators: the number of sub-iterators in the zipped iterator.\n  \"\"\"\n  iter_tuple = itertools.tee(zipped_iterators, num_sub_iterators)\n  return [\n      map(operator.itemgetter(i), iter_tuple[i])\n      for i in range(num_sub_iterators)\n  ]",
  "def fast_map_structure(func, *structure):\n  \"\"\"Faster map_structure implementation which skips some error checking.\"\"\"\n  flat_structure = (tree.flatten(s) for s in structure)\n  entries = zip(*flat_structure)\n  # Arbitrarily choose one of the structures of the original sequence (the last)\n  # to match the structure for the flattened sequence.\n  return tree.unflatten_as(structure[-1], [func(*x) for x in entries])",
  "def fast_map_structure_with_path(func, *structure):\n  \"\"\"Faster map_structure_with_path implementation.\"\"\"\n  head_entries_with_path = tree.flatten_with_path(structure[0])\n  if len(structure) > 1:\n    tail_entries = (tree.flatten(s) for s in structure[1:])\n    entries_with_path = [\n        e[0] + e[1:] for e in zip(head_entries_with_path, *tail_entries)\n    ]\n  else:\n    entries_with_path = head_entries_with_path\n  # Arbitrarily choose one of the structures of the original sequence (the last)\n  # to match the structure for the flattened sequence.\n  return tree.unflatten_as(structure[-1], [func(*x) for x in entries_with_path])",
  "def stack_sequence_fields(sequence: Sequence[ElementType]) -> ElementType:\n  \"\"\"Stacks a list of identically nested objects.\n\n  This takes a sequence of identically nested objects and returns a single\n  nested object whose ith leaf is a stacked numpy array of the corresponding\n  ith leaf from each element of the sequence.\n\n  For example, if `sequence` is:\n\n  ```python\n  [{\n        'action': np.array([1.0]),\n        'observation': (np.array([0.0, 1.0, 2.0]),),\n        'reward': 1.0\n   }, {\n        'action': np.array([0.5]),\n        'observation': (np.array([1.0, 2.0, 3.0]),),\n        'reward': 0.0\n   }, {\n        'action': np.array([0.3]),1\n        'observation': (np.array([2.0, 3.0, 4.0]),),\n        'reward': 0.5\n   }]\n  ```\n\n  Then this function will return:\n\n  ```python\n  {\n      'action': np.array([....])         # array shape = [3 x 1]\n      'observation': (np.array([...]),)  # array shape = [3 x 3]\n      'reward': np.array([...])          # array shape = [3]\n  }\n  ```\n\n  Note that the 'observation' entry in the above example has two levels of\n  nesting, i.e it is a tuple of arrays.\n\n  Args:\n    sequence: a list of identically nested objects.\n\n  Returns:\n    A nested object with numpy.\n\n  Raises:\n    ValueError: If `sequence` is an empty sequence.\n  \"\"\"\n  # Handle empty input sequences.\n  if not sequence:\n    raise ValueError('Input sequence must not be empty')\n\n  # Default to asarray when arrays don't have the same shape to be compatible\n  # with old behaviour.\n  try:\n    return fast_map_structure(lambda *values: np.stack(values), *sequence)\n  except ValueError:\n    return fast_map_structure(lambda *values: np.asarray(values, dtype=object),\n                              *sequence)",
  "def unstack_sequence_fields(struct: ElementType,\n                            batch_size: int) -> List[ElementType]:\n  \"\"\"Converts a struct of batched arrays to a list of structs.\n\n  This is effectively the inverse of `stack_sequence_fields`.\n\n  Args:\n    struct: An (arbitrarily nested) structure of arrays.\n    batch_size: The length of the leading dimension of each array in the struct.\n      This is assumed to be static and known.\n\n  Returns:\n    A list of structs with the same structure as `struct`, where each leaf node\n     is an unbatched element of the original leaf node.\n  \"\"\"\n\n  return [\n      tree.map_structure(lambda s, i=i: s[i], struct) for i in range(batch_size)\n  ]",
  "def broadcast_structures(*args: Any) -> Any:\n  \"\"\"Returns versions of the arguments that give them the same nested structure.\n\n  Any nested items in *args must have the same structure.\n\n  Any non-nested item will be replaced with a nested version that shares that\n  structure. The leaves will all be references to the same original non-nested\n  item.\n\n  If all *args are nested, or all *args are non-nested, this function will\n  return *args unchanged.\n\n  Example:\n  ```\n  a = ('a', 'b')\n  b = 'c'\n  tree_a, tree_b = broadcast_structure(a, b)\n  tree_a\n  > ('a', 'b')\n  tree_b\n  > ('c', 'c')\n  ```\n\n  Args:\n    *args: A Sequence of nested or non-nested items.\n\n  Returns:\n    `*args`, except with all items sharing the same nest structure.\n  \"\"\"\n  if not args:\n    return\n\n  reference_tree = None\n  for arg in args:\n    if tree.is_nested(arg):\n      reference_tree = arg\n      break\n\n  # If reference_tree is None then none of args are nested and we can skip over\n  # the rest of this function, which would be a no-op.\n  if reference_tree is None:\n    return args\n\n  def mirror_structure(value, reference_tree):\n    if tree.is_nested(value):\n      # Use check_types=True so that the types of the trees we construct aren't\n      # dependent on our arbitrary choice of which nested arg to use as the\n      # reference_tree.\n      tree.assert_same_structure(value, reference_tree, check_types=True)\n      return value\n    else:\n      return tree.map_structure(lambda _: value, reference_tree)\n\n  return tuple(mirror_structure(arg, reference_tree) for arg in args)",
  "def tree_map(f):\n  \"\"\"Transforms `f` into a tree-mapped version.\"\"\"\n\n  def mapped_f(*structures):\n    return tree.map_structure(f, *structures)\n\n  return mapped_f",
  "def mirror_structure(value, reference_tree):\n    if tree.is_nested(value):\n      # Use check_types=True so that the types of the trees we construct aren't\n      # dependent on our arbitrary choice of which nested arg to use as the\n      # reference_tree.\n      tree.assert_same_structure(value, reference_tree, check_types=True)\n      return value\n    else:\n      return tree.map_structure(lambda _: value, reference_tree)",
  "def mapped_f(*structures):\n    return tree.map_structure(f, *structures)",
  "class Barrier:\n  \"\"\"Defines a simple barrier class to synchronize on a particular event.\"\"\"\n\n  def __init__(self, num_threads):\n    \"\"\"Constructor.\n\n    Args:\n      num_threads: int - how many threads will be syncronizing on this barrier\n    \"\"\"\n    self._num_threads = num_threads\n    self._count = 0\n    self._cond = threading.Condition()\n\n  def wait(self):\n    \"\"\"Waits on the barrier until all threads have called this method.\"\"\"\n    with self._cond:\n      self._count += 1\n      self._cond.notifyAll()\n      while self._count < self._num_threads:\n        self._cond.wait()",
  "class CountingTest(absltest.TestCase):\n\n  def test_counter_threading(self):\n    counter = counting.Counter()\n    num_threads = 10\n    barrier = Barrier(num_threads)\n\n    # Increment in every thread at the same time.\n    def add_to_counter():\n      barrier.wait()\n      counter.increment(foo=1)\n\n    # Run the threads.\n    threads = []\n    for _ in range(num_threads):\n      t = threading.Thread(target=add_to_counter)\n      t.start()\n      threads.append(t)\n    for t in threads:\n      t.join()\n\n    # Make sure the counter has been incremented once per thread.\n    counts = counter.get_counts()\n    self.assertEqual(counts['foo'], num_threads)\n\n  def test_counter_caching(self):\n    parent = counting.Counter()\n    counter = counting.Counter(parent, time_delta=0.)\n    counter.increment(foo=12)\n    self.assertEqual(parent.get_counts(), counter.get_counts())\n\n  def test_shared_counts(self):\n    # Two counters with shared parent should share counts (modulo namespacing).\n    parent = counting.Counter()\n    child1 = counting.Counter(parent, 'child1')\n    child2 = counting.Counter(parent, 'child2')\n    child1.increment(foo=1)\n    result = child2.increment(foo=2)\n    expected = {'child1_foo': 1, 'child2_foo': 2}\n    self.assertEqual(result, expected)\n\n  def test_return_only_prefixed(self):\n    parent = counting.Counter()\n    child1 = counting.Counter(\n        parent, 'child1', time_delta=0., return_only_prefixed=False)\n    child2 = counting.Counter(\n        parent, 'child2', time_delta=0., return_only_prefixed=True)\n    child1.increment(foo=1)\n    child2.increment(bar=1)\n    self.assertEqual(child1.get_counts(), {'child1_foo': 1, 'child2_bar': 1})\n    self.assertEqual(child2.get_counts(), {'bar': 1})\n\n  def test_get_steps_key(self):\n    parent = counting.Counter()\n    child1 = counting.Counter(\n        parent, 'child1', time_delta=0., return_only_prefixed=False)\n    child2 = counting.Counter(\n        parent, 'child2', time_delta=0., return_only_prefixed=True)\n    self.assertEqual(child1.get_steps_key(), 'child1_steps')\n    self.assertEqual(child2.get_steps_key(), 'steps')\n    child1.increment(steps=1)\n    child2.increment(steps=2)\n    self.assertEqual(child1.get_counts().get(child1.get_steps_key()), 1)\n    self.assertEqual(child2.get_counts().get(child2.get_steps_key()), 2)\n\n  def test_parent_prefix(self):\n    parent = counting.Counter(prefix='parent')\n    child = counting.Counter(parent, prefix='child', time_delta=0.)\n    self.assertEqual(child.get_steps_key(), 'child_steps')",
  "def __init__(self, num_threads):\n    \"\"\"Constructor.\n\n    Args:\n      num_threads: int - how many threads will be syncronizing on this barrier\n    \"\"\"\n    self._num_threads = num_threads\n    self._count = 0\n    self._cond = threading.Condition()",
  "def wait(self):\n    \"\"\"Waits on the barrier until all threads have called this method.\"\"\"\n    with self._cond:\n      self._count += 1\n      self._cond.notifyAll()\n      while self._count < self._num_threads:\n        self._cond.wait()",
  "def test_counter_threading(self):\n    counter = counting.Counter()\n    num_threads = 10\n    barrier = Barrier(num_threads)\n\n    # Increment in every thread at the same time.\n    def add_to_counter():\n      barrier.wait()\n      counter.increment(foo=1)\n\n    # Run the threads.\n    threads = []\n    for _ in range(num_threads):\n      t = threading.Thread(target=add_to_counter)\n      t.start()\n      threads.append(t)\n    for t in threads:\n      t.join()\n\n    # Make sure the counter has been incremented once per thread.\n    counts = counter.get_counts()\n    self.assertEqual(counts['foo'], num_threads)",
  "def test_counter_caching(self):\n    parent = counting.Counter()\n    counter = counting.Counter(parent, time_delta=0.)\n    counter.increment(foo=12)\n    self.assertEqual(parent.get_counts(), counter.get_counts())",
  "def test_shared_counts(self):\n    # Two counters with shared parent should share counts (modulo namespacing).\n    parent = counting.Counter()\n    child1 = counting.Counter(parent, 'child1')\n    child2 = counting.Counter(parent, 'child2')\n    child1.increment(foo=1)\n    result = child2.increment(foo=2)\n    expected = {'child1_foo': 1, 'child2_foo': 2}\n    self.assertEqual(result, expected)",
  "def test_return_only_prefixed(self):\n    parent = counting.Counter()\n    child1 = counting.Counter(\n        parent, 'child1', time_delta=0., return_only_prefixed=False)\n    child2 = counting.Counter(\n        parent, 'child2', time_delta=0., return_only_prefixed=True)\n    child1.increment(foo=1)\n    child2.increment(bar=1)\n    self.assertEqual(child1.get_counts(), {'child1_foo': 1, 'child2_bar': 1})\n    self.assertEqual(child2.get_counts(), {'bar': 1})",
  "def test_get_steps_key(self):\n    parent = counting.Counter()\n    child1 = counting.Counter(\n        parent, 'child1', time_delta=0., return_only_prefixed=False)\n    child2 = counting.Counter(\n        parent, 'child2', time_delta=0., return_only_prefixed=True)\n    self.assertEqual(child1.get_steps_key(), 'child1_steps')\n    self.assertEqual(child2.get_steps_key(), 'steps')\n    child1.increment(steps=1)\n    child2.increment(steps=2)\n    self.assertEqual(child1.get_counts().get(child1.get_steps_key()), 1)\n    self.assertEqual(child2.get_counts().get(child2.get_steps_key()), 2)",
  "def test_parent_prefix(self):\n    parent = counting.Counter(prefix='parent')\n    child = counting.Counter(parent, prefix='child', time_delta=0.)\n    self.assertEqual(child.get_steps_key(), 'child_steps')",
  "def add_to_counter():\n      barrier.wait()\n      counter.increment(foo=1)",
  "def make_experiment_logger(label: str,\n                           steps_key: Optional[str] = None,\n                           task_instance: int = 0) -> loggers.Logger:\n  del task_instance\n  if steps_key is None:\n    steps_key = f'{label}_steps'\n  return loggers.make_default_logger(label=label, steps_key=steps_key)",
  "def create_experiment_logger_factory() -> loggers.LoggerFactory:\n  return make_experiment_logger",
  "def make_replay_table_from_info(\n    table_info: reverb_types.TableInfo) -> reverb.Table:\n  \"\"\"Build a replay table out of its specs in a TableInfo.\n\n  Args:\n    table_info: A TableInfo containing the Table specs.\n\n  Returns:\n    A reverb replay table matching the info specs.\n  \"\"\"\n  sampler = _make_selector_from_key_distribution_options(\n      table_info.sampler_options)\n  remover = _make_selector_from_key_distribution_options(\n      table_info.remover_options)\n  rate_limiter = _make_rate_limiter_from_rate_limiter_info(\n      table_info.rate_limiter_info)\n  return reverb.Table(\n      name=table_info.name,\n      sampler=sampler,\n      remover=remover,\n      max_size=table_info.max_size,\n      rate_limiter=rate_limiter,\n      max_times_sampled=table_info.max_times_sampled,\n      signature=table_info.signature)",
  "def _make_selector_from_key_distribution_options(\n    options) -> reverb_types.SelectorType:\n  \"\"\"Returns a Selector from its KeyDistributionOptions description.\"\"\"\n  one_of = options.WhichOneof('distribution')\n  if one_of == 'fifo':\n    return item_selectors.Fifo()\n  if one_of == 'uniform':\n    return item_selectors.Uniform()\n  if one_of == 'prioritized':\n    return item_selectors.Prioritized(options.prioritized.priority_exponent)\n  if one_of == 'heap':\n    if options.heap.min_heap:\n      return item_selectors.MinHeap()\n    return item_selectors.MaxHeap()\n  if one_of == 'lifo':\n    return item_selectors.Lifo()\n  raise ValueError(f'Unknown distribution field: {one_of}')",
  "def _make_rate_limiter_from_rate_limiter_info(\n    info) -> rate_limiters.RateLimiter:\n  return rate_limiters.SampleToInsertRatio(\n      samples_per_insert=info.samples_per_insert,\n      min_size_to_sample=info.min_size_to_sample,\n      error_buffer=(info.min_diff, info.max_diff))",
  "def replay_sample_to_sars_transition(\n    sample: reverb.ReplaySample,\n    is_sequence: bool,\n    strip_last_transition: bool = False,\n    flatten_batch: bool = False) -> types.Transition:\n  \"\"\"Converts the replay sample to a types.Transition.\n\n  NB: If is_sequence is True then the last next_observation of each sequence is\n  rubbish. Don't train on it.\n\n  Args:\n    sample: The replay sample\n    is_sequence: If False we expect the sample data to match the\n      types.Transition already. Otherwise we expect a batch of sequences of\n      steps.\n    strip_last_transition: If True and is_sequence, the last transition will be\n      stripped as its next_observation field is incorrect.\n    flatten_batch: If True and is_sequence, the two batch dimensions will be\n      flatten to one.\n\n  Returns:\n    A types.Transition built from the sample data.\n    If is_sequence and strip_last_transition are both True, the output will be\n    smaller than the output as the last transition of every sequence will have\n    been removed.\n  \"\"\"\n  if not is_sequence:\n    return types.Transition(*sample.data)\n  # Note that the last next_observation is invalid.\n  steps = sample.data\n  def roll(observation):\n    return np.roll(observation, shift=-1, axis=1)\n  transitions = types.Transition(\n      observation=steps.observation,\n      action=steps.action,\n      reward=steps.reward,\n      discount=steps.discount,\n      next_observation=tree.map_structure(roll, steps.observation),\n      extras=steps.extras)\n  if strip_last_transition:\n    # We remove the last transition as its next_observation field is incorrect.\n    # It has been obtained by rolling the observation field, such that\n    # transitions.next_observations[:, -1] is transitions.observations[:, 0]\n    transitions = jax.tree_map(lambda x: x[:, :-1, ...], transitions)\n  if flatten_batch:\n    # Merge the 2 leading batch dimensions into 1.\n    transitions = jax.tree_map(lambda x: np.reshape(x, (-1,) + x.shape[2:]),\n                               transitions)\n  return transitions",
  "def transition_to_replaysample(\n    transitions: types.Transition) -> reverb.ReplaySample:\n  \"\"\"Converts a types.Transition to a reverb.ReplaySample.\"\"\"\n  info = tree.map_structure(lambda dtype: tf.ones([], dtype),\n                            reverb.SampleInfo.tf_dtypes())\n  return reverb.ReplaySample(info=info, data=transitions)",
  "def roll(observation):\n    return np.roll(observation, shift=-1, axis=1)",
  "class AsyncExecutor(Generic[E]):\n  \"\"\"Executes a blocking function asynchronously on a queue of items.\"\"\"\n\n  def __init__(\n      self,\n      fn: Callable[[E], None],\n      queue_size: int = 1,\n      interruptible_interval_secs: float = 1.0,\n  ):\n    \"\"\"Buffers elements in a queue and runs `fn` asynchronously..\n\n    NOTE: Once closed, `AsyncExecutor` will block until current `fn` finishes\n      but is not guaranteed to dequeue all elements currently stored in\n      the data queue. This is intentional so as to prevent a blocking `fn` call\n      from preventing `AsyncExecutor` from closing.\n\n    Args:\n      fn: A callable to be executed upon dequeuing an element from data\n        queue.\n      queue_size: The maximum size of the synchronized buffer queue.\n      interruptible_interval_secs: Timeout interval in seconds for blocking\n        queue operations after which the background threads check for errors and\n        if background threads should stop.\n    \"\"\"\n    self._data = queue.Queue(maxsize=queue_size)\n    self._should_stop = threading.Event()\n    self._errors = queue.Queue()\n    self._interruptible_interval_secs = interruptible_interval_secs\n\n    def _dequeue() -> None:\n      \"\"\"Dequeue data from a queue and invoke blocking call.\"\"\"\n      while not self._should_stop.is_set():\n        try:\n          element = self._data.get(timeout=self._interruptible_interval_secs)\n          # Execute fn upon dequeuing an element from the data queue.\n          fn(element)\n        except queue.Empty:\n          # If queue is Empty for longer than the specified time interval,\n          # check again if should_stop has been requested and retry.\n          continue\n        except Exception as e:\n          logging.error(\"AsyncExecuter thread terminated with error.\")\n          logging.exception(e)\n          self._errors.put(e)\n          self._should_stop.set()\n          raise  # Never caught by anything, just terminates the thread.\n\n    self._thread = threading.Thread(target=_dequeue, daemon=True)\n    self._thread.start()\n\n  def _raise_on_error(self) -> None:\n    try:\n      # Raise the error on the caller thread if an error has been raised in the\n      # looper thread.\n      raise self._errors.get_nowait()\n    except queue.Empty:\n      pass\n\n  def close(self):\n    self._should_stop.set()\n    # Join all background threads.\n    self._thread.join()\n    # Raise errors produced by background threads.\n    self._raise_on_error()\n\n  def put(self, element: E) -> None:\n    \"\"\"Puts `element` asynchronuously onto the underlying data queue.\n\n    The write call blocks if the underlying data_queue contains `queue_size`\n      elements for over `self._interruptible_interval_secs` second, in which\n      case we check if stop has been requested or if there has been an error\n      raised on the looper thread. If neither happened, retry enqueue.\n\n    Args:\n      element: an element to be put into the underlying data queue and dequeued\n        asynchronuously for `fn(element)` call.\n    \"\"\"\n    while not self._should_stop.is_set():\n      try:\n        self._data.put(element, timeout=self._interruptible_interval_secs)\n        break\n      except queue.Full:\n        continue\n    else:\n      # If `should_stop` has been set, then raises if any has been raised on\n      # the background thread.\n      self._raise_on_error()",
  "def __init__(\n      self,\n      fn: Callable[[E], None],\n      queue_size: int = 1,\n      interruptible_interval_secs: float = 1.0,\n  ):\n    \"\"\"Buffers elements in a queue and runs `fn` asynchronously..\n\n    NOTE: Once closed, `AsyncExecutor` will block until current `fn` finishes\n      but is not guaranteed to dequeue all elements currently stored in\n      the data queue. This is intentional so as to prevent a blocking `fn` call\n      from preventing `AsyncExecutor` from closing.\n\n    Args:\n      fn: A callable to be executed upon dequeuing an element from data\n        queue.\n      queue_size: The maximum size of the synchronized buffer queue.\n      interruptible_interval_secs: Timeout interval in seconds for blocking\n        queue operations after which the background threads check for errors and\n        if background threads should stop.\n    \"\"\"\n    self._data = queue.Queue(maxsize=queue_size)\n    self._should_stop = threading.Event()\n    self._errors = queue.Queue()\n    self._interruptible_interval_secs = interruptible_interval_secs\n\n    def _dequeue() -> None:\n      \"\"\"Dequeue data from a queue and invoke blocking call.\"\"\"\n      while not self._should_stop.is_set():\n        try:\n          element = self._data.get(timeout=self._interruptible_interval_secs)\n          # Execute fn upon dequeuing an element from the data queue.\n          fn(element)\n        except queue.Empty:\n          # If queue is Empty for longer than the specified time interval,\n          # check again if should_stop has been requested and retry.\n          continue\n        except Exception as e:\n          logging.error(\"AsyncExecuter thread terminated with error.\")\n          logging.exception(e)\n          self._errors.put(e)\n          self._should_stop.set()\n          raise  # Never caught by anything, just terminates the thread.\n\n    self._thread = threading.Thread(target=_dequeue, daemon=True)\n    self._thread.start()",
  "def _raise_on_error(self) -> None:\n    try:\n      # Raise the error on the caller thread if an error has been raised in the\n      # looper thread.\n      raise self._errors.get_nowait()\n    except queue.Empty:\n      pass",
  "def close(self):\n    self._should_stop.set()\n    # Join all background threads.\n    self._thread.join()\n    # Raise errors produced by background threads.\n    self._raise_on_error()",
  "def put(self, element: E) -> None:\n    \"\"\"Puts `element` asynchronuously onto the underlying data queue.\n\n    The write call blocks if the underlying data_queue contains `queue_size`\n      elements for over `self._interruptible_interval_secs` second, in which\n      case we check if stop has been requested or if there has been an error\n      raised on the looper thread. If neither happened, retry enqueue.\n\n    Args:\n      element: an element to be put into the underlying data queue and dequeued\n        asynchronuously for `fn(element)` call.\n    \"\"\"\n    while not self._should_stop.is_set():\n      try:\n        self._data.put(element, timeout=self._interruptible_interval_secs)\n        break\n      except queue.Full:\n        continue\n    else:\n      # If `should_stop` has been set, then raises if any has been raised on\n      # the background thread.\n      self._raise_on_error()",
  "def _dequeue() -> None:\n      \"\"\"Dequeue data from a queue and invoke blocking call.\"\"\"\n      while not self._should_stop.is_set():\n        try:\n          element = self._data.get(timeout=self._interruptible_interval_secs)\n          # Execute fn upon dequeuing an element from the data queue.\n          fn(element)\n        except queue.Empty:\n          # If queue is Empty for longer than the specified time interval,\n          # check again if should_stop has been requested and retry.\n          continue\n        except Exception as e:\n          logging.error(\"AsyncExecuter thread terminated with error.\")\n          logging.exception(e)\n          self._errors.put(e)\n          self._should_stop.set()\n          raise",
  "def partial_kwargs(function: Callable[..., Any],\n                   **kwargs: Any) -> Callable[..., Any]:\n  \"\"\"Return a partial function application by overriding default keywords.\n\n  This function is equivalent to `functools.partial(function, **kwargs)` but\n  will raise a `ValueError` when called if either the given keyword arguments\n  are not defined by `function` or if they do not have defaults.\n\n  This is useful as a way to define a factory function with default parameters\n  and then to override them in a safe way.\n\n  Args:\n    function: the base function before partial application.\n    **kwargs: keyword argument overrides.\n\n  Returns:\n    A function.\n  \"\"\"\n  # Try to get the argspec of our function which we'll use to get which keywords\n  # have defaults.\n  argspec = inspect.getfullargspec(function)\n\n  # Figure out which keywords have defaults.\n  if argspec.defaults is None:\n    defaults = []\n  else:\n    defaults = argspec.args[-len(argspec.defaults):]\n\n  # Find any keys not given as defaults by the function.\n  unknown_kwargs = set(kwargs.keys()).difference(defaults)\n\n  # Raise an error\n  if unknown_kwargs:\n    error_string = 'Cannot override unknown or non-default kwargs: {}'\n    raise ValueError(error_string.format(', '.join(unknown_kwargs)))\n\n  return functools.partial(function, **kwargs)",
  "class StepsLimiter:\n  \"\"\"Process that terminates an experiment when `max_steps` is reached.\"\"\"\n\n  def __init__(self,\n               counter: counting.Counter,\n               max_steps: int,\n               steps_key: str = 'actor_steps'):\n    self._counter = counter\n    self._max_steps = max_steps\n    self._steps_key = steps_key\n\n  def run(self):\n    \"\"\"Run steps limiter to terminate an experiment when max_steps is reached.\n    \"\"\"\n\n    logging.info('StepsLimiter: Starting with max_steps = %d (%s)',\n                 self._max_steps, self._steps_key)\n    with signals.runtime_terminator():\n      while True:\n        # Update the counts.\n        counts = self._counter.get_counts()\n        num_steps = counts.get(self._steps_key, 0)\n\n        logging.info('StepsLimiter: Reached %d recorded steps', num_steps)\n\n        if num_steps > self._max_steps:\n          logging.info('StepsLimiter: Max steps of %d was reached, terminating',\n                       self._max_steps)\n          # Avoid importing Launchpad until it is actually used.\n          import launchpad as lp  # pylint: disable=g-import-not-at-top\n          lp.stop()\n\n        # Don't spam the counter.\n        for _ in range(10):\n          # Do not sleep for a long period of time to avoid LaunchPad program\n          # termination hangs (time.sleep is not interruptible).\n          time.sleep(1)",
  "def is_local_run() -> bool:\n  return FLAGS.lp_launch_type.startswith('local')",
  "def make_xm_docker_resources(program,\n                             requirements: Optional[str] = None):\n  \"\"\"Returns Docker XManager resources for each program's node.\n\n  For each node of the Launchpad's program appropriate hardware requirements are\n  specified (CPU, memory...), while the list of PyPi packages specified in\n  the requirements file will be installed inside the Docker images.\n\n  Args:\n    program: program for which to construct Docker XManager resources.\n    requirements: file containing additional requirements to use.\n      If not specified, default Acme dependencies are used instead.\n  \"\"\"\n  if (FLAGS.lp_launch_type != 'vertex_ai' and\n      FLAGS.lp_launch_type != 'local_docker'):\n    # Avoid importing 'xmanager' for local runs.\n    return None\n\n  # Avoid importing Launchpad until it is actually used.\n  import launchpad as lp  # pylint: disable=g-import-not-at-top\n  # Reference lp.DockerConfig to force lazy import of xmanager by Launchpad and\n  # then import it. It is done this way to avoid heavy imports by default.\n  lp.DockerConfig  # pylint: disable=pointless-statement\n  from xmanager import xm  # pylint: disable=g-import-not-at-top\n\n  # Get number of each type of node.\n  num_nodes = {k: len(v) for k, v in program.groups.items()}\n\n  xm_resources = {}\n\n  acme_location = os.path.dirname(os.path.dirname(__file__))\n  if not requirements:\n    # Acme requirements are located in the Acme directory (when installed\n    # with pip), or need to be extracted from setup.py when using Acme codebase\n    # from GitHub without PyPi installation.\n    requirements = os.path.join(acme_location, 'requirements.txt')\n    if not os.path.isfile(requirements):\n      # Try to generate requirements.txt from setup.py\n      setup = os.path.join(os.path.dirname(acme_location), 'setup.py')\n      if os.path.isfile(setup):\n        # Generate requirements.txt file using setup.py.\n        import importlib.util  # pylint: disable=g-import-not-at-top\n        spec = importlib.util.spec_from_file_location('setup', setup)\n        setup = importlib.util.module_from_spec(spec)\n        try:\n          spec.loader.exec_module(setup)  # pytype: disable=attribute-error\n        except SystemExit:\n          pass\n        atexit.register(os.remove, requirements)\n        setup.generate_requirements_file(requirements)\n\n  # Extend PYTHONPATH with paths used by the launcher.\n  python_path = []\n  for path in sys.path:\n    if path.startswith(acme_location) and acme_location != path:\n      python_path.append(path[len(acme_location):])\n\n  if 'replay' in num_nodes:\n    replay_cpu = 6 + num_nodes.get('actor', 0) * 0.01\n    replay_cpu = min(40, replay_cpu)\n\n    xm_resources['replay'] = lp.DockerConfig(\n        acme_location,\n        requirements,\n        hw_requirements=xm.JobRequirements(cpu=replay_cpu, ram=10 * xm.GiB),\n        python_path=python_path)\n\n  if 'evaluator' in num_nodes:\n    xm_resources['evaluator'] = lp.DockerConfig(\n        acme_location,\n        requirements,\n        hw_requirements=xm.JobRequirements(cpu=2, ram=4 * xm.GiB),\n        python_path=python_path)\n\n  if 'actor' in num_nodes:\n    xm_resources['actor'] = lp.DockerConfig(\n        acme_location,\n        requirements,\n        hw_requirements=xm.JobRequirements(cpu=2, ram=4 * xm.GiB),\n        python_path=python_path)\n\n  if 'learner' in num_nodes:\n    learner_cpu = 6 + num_nodes.get('actor', 0) * 0.01\n    learner_cpu = min(40, learner_cpu)\n    xm_resources['learner'] = lp.DockerConfig(\n        acme_location,\n        requirements,\n        hw_requirements=xm.JobRequirements(\n            cpu=learner_cpu, ram=6 * xm.GiB, P100=1),\n        python_path=python_path)\n\n  if 'environment_loop' in num_nodes:\n    xm_resources['environment_loop'] = lp.DockerConfig(\n        acme_location,\n        requirements,\n        hw_requirements=xm.JobRequirements(\n            cpu=6, ram=6 * xm.GiB, P100=1),\n        python_path=python_path)\n\n  if 'counter' in num_nodes:\n    xm_resources['counter'] = lp.DockerConfig(\n        acme_location,\n        requirements,\n        hw_requirements=xm.JobRequirements(cpu=3, ram=4 * xm.GiB),\n        python_path=python_path)\n\n  if 'cacher' in num_nodes:\n    xm_resources['cacher'] = lp.DockerConfig(\n        acme_location,\n        requirements,\n        hw_requirements=xm.JobRequirements(cpu=3, ram=6 * xm.GiB),\n        python_path=python_path)\n\n  return xm_resources",
  "def __init__(self,\n               counter: counting.Counter,\n               max_steps: int,\n               steps_key: str = 'actor_steps'):\n    self._counter = counter\n    self._max_steps = max_steps\n    self._steps_key = steps_key",
  "def run(self):\n    \"\"\"Run steps limiter to terminate an experiment when max_steps is reached.\n    \"\"\"\n\n    logging.info('StepsLimiter: Starting with max_steps = %d (%s)',\n                 self._max_steps, self._steps_key)\n    with signals.runtime_terminator():\n      while True:\n        # Update the counts.\n        counts = self._counter.get_counts()\n        num_steps = counts.get(self._steps_key, 0)\n\n        logging.info('StepsLimiter: Reached %d recorded steps', num_steps)\n\n        if num_steps > self._max_steps:\n          logging.info('StepsLimiter: Max steps of %d was reached, terminating',\n                       self._max_steps)\n          # Avoid importing Launchpad until it is actually used.\n          import launchpad as lp  # pylint: disable=g-import-not-at-top\n          lp.stop()\n\n        # Don't spam the counter.\n        for _ in range(10):\n          # Do not sleep for a long period of time to avoid LaunchPad program\n          # termination hangs (time.sleep is not interruptible).\n          time.sleep(1)",
  "class FrozenLearnerTest(absltest.TestCase):\n\n  @mock.patch.object(acme, 'Learner', autospec=True)\n  def test_step_fn(self, mock_learner):\n    num_calls = 0\n\n    def step_fn():\n      nonlocal num_calls\n      num_calls += 1\n\n    learner = frozen_learner.FrozenLearner(mock_learner, step_fn=step_fn)\n\n    # Step two times.\n    learner.step()\n    learner.step()\n\n    self.assertEqual(num_calls, 2)\n    # step() method of the wrapped learner should not be called.\n    mock_learner.step.assert_not_called()\n\n  @mock.patch.object(acme, 'Learner', autospec=True)\n  def test_no_step_fn(self, mock_learner):\n    learner = frozen_learner.FrozenLearner(mock_learner)\n    learner.step()\n    # step() method of the wrapped learner should not be called.\n    mock_learner.step.assert_not_called()\n\n  @mock.patch.object(acme, 'Learner', autospec=True)\n  def test_save_and_restore(self, mock_learner):\n    learner = frozen_learner.FrozenLearner(mock_learner)\n\n    mock_learner.save.return_value = 'state1'\n\n    state = learner.save()\n    self.assertEqual(state, 'state1')\n\n    learner.restore('state2')\n    # State of the wrapped learner should be restored.\n    mock_learner.restore.assert_called_once_with('state2')\n\n  @mock.patch.object(acme, 'Learner', autospec=True)\n  def test_get_variables(self, mock_learner):\n    learner = frozen_learner.FrozenLearner(mock_learner)\n\n    mock_learner.get_variables.return_value = [1, 2]\n\n    variables = learner.get_variables(['a', 'b'])\n    # Values should match with those returned by the wrapped learner.\n    self.assertEqual(variables, [1, 2])\n    mock_learner.get_variables.assert_called_once_with(['a', 'b'])",
  "def test_step_fn(self, mock_learner):\n    num_calls = 0\n\n    def step_fn():\n      nonlocal num_calls\n      num_calls += 1\n\n    learner = frozen_learner.FrozenLearner(mock_learner, step_fn=step_fn)\n\n    # Step two times.\n    learner.step()\n    learner.step()\n\n    self.assertEqual(num_calls, 2)\n    # step() method of the wrapped learner should not be called.\n    mock_learner.step.assert_not_called()",
  "def test_no_step_fn(self, mock_learner):\n    learner = frozen_learner.FrozenLearner(mock_learner)\n    learner.step()\n    # step() method of the wrapped learner should not be called.\n    mock_learner.step.assert_not_called()",
  "def test_save_and_restore(self, mock_learner):\n    learner = frozen_learner.FrozenLearner(mock_learner)\n\n    mock_learner.save.return_value = 'state1'\n\n    state = learner.save()\n    self.assertEqual(state, 'state1')\n\n    learner.restore('state2')\n    # State of the wrapped learner should be restored.\n    mock_learner.restore.assert_called_once_with('state2')",
  "def test_get_variables(self, mock_learner):\n    learner = frozen_learner.FrozenLearner(mock_learner)\n\n    mock_learner.get_variables.return_value = [1, 2]\n\n    variables = learner.get_variables(['a', 'b'])\n    # Values should match with those returned by the wrapped learner.\n    self.assertEqual(variables, [1, 2])\n    mock_learner.get_variables.assert_called_once_with(['a', 'b'])",
  "def step_fn():\n      nonlocal num_calls\n      num_calls += 1",
  "def process_path(path: str,\n                 *subpaths: str,\n                 ttl_seconds: Optional[int] = None,\n                 backups: Optional[bool] = None,\n                 add_uid: bool = True) -> str:\n  \"\"\"Process the path string.\n\n  This will process the path string by running `os.path.expanduser` to replace\n  any initial \"~\". It will also append a unique string on the end of the path\n  and create the directories leading to this path if necessary.\n\n  Args:\n    path: string defining the path to process and create.\n    *subpaths: potential subpaths to include after uniqification.\n    ttl_seconds: ignored.\n    backups: ignored.\n    add_uid: Whether to add a unique directory identifier between `path` and\n      `subpaths`. If the `--acme_id` flag is set, will use that as the\n      identifier.\n\n  Returns:\n    the processed, expanded path string.\n  \"\"\"\n  del backups, ttl_seconds\n\n  path = os.path.expanduser(path)\n  if add_uid:\n    path = os.path.join(path, *get_unique_id())\n  path = os.path.join(path, *subpaths)\n  os.makedirs(path, exist_ok=True)\n  return path",
  "def get_unique_id() -> Tuple[str, ...]:\n  \"\"\"Makes a unique identifier for this process; override with --acme_id.\"\"\"\n  # By default we'll use the global id.\n  identifier = _DATETIME\n\n  # If the --acme_id flag is given prefer that; ignore if flag processing has\n  # been skipped (this happens in colab or in tests).\n  try:\n    identifier = ACME_ID.value or identifier\n  except flags.UnparsedFlagAccessError:\n    pass\n\n  # Return as a tuple (for future proofing).\n  return (identifier,)",
  "def rmdir(path: str):\n  \"\"\"Remove directory recursively.\"\"\"\n  shutil.rmtree(path)",
  "class SequenceStackTest(absltest.TestCase):\n  \"\"\"Tests for various tree utilities.\"\"\"\n\n  def test_stack_sequence_fields(self):\n    \"\"\"Tests that `stack_sequence_fields` behaves correctly on nested data.\"\"\"\n\n    stacked = tree_utils.stack_sequence_fields(TEST_SEQUENCE)\n\n    # Check that the stacked output has the correct structure.\n    tree.assert_same_structure(stacked, TEST_SEQUENCE[0])\n\n    # Check that the leaves have the correct array shapes.\n    self.assertEqual(stacked['action'].shape, (3, 1))\n    self.assertEqual(stacked['observation'][0].shape, (3, 3))\n    self.assertEqual(stacked['reward'].shape, (3,))\n\n    # Check values.\n    self.assertEqual(stacked['observation'][0].tolist(), [\n        [0., 1., 2.],\n        [1., 2., 3.],\n        [2., 3., 4.],\n    ])\n    self.assertEqual(stacked['action'].tolist(), [[1.], [0.5], [0.3]])\n    self.assertEqual(stacked['reward'].tolist(), [1., 0., 0.5])\n\n  def test_unstack_sequence_fields(self):\n    \"\"\"Tests that `unstack_sequence_fields(stack_sequence_fields(x)) == x`.\"\"\"\n    stacked = tree_utils.stack_sequence_fields(TEST_SEQUENCE)\n    batch_size = len(TEST_SEQUENCE)\n    unstacked = tree_utils.unstack_sequence_fields(stacked, batch_size)\n    tree.map_structure(np.testing.assert_array_equal, unstacked, TEST_SEQUENCE)\n\n  def test_fast_map_structure_with_path(self):\n    structure = {\n        'a': {\n            'b': np.array([0.0])\n        },\n        'c': (np.array([1.0]), np.array([2.0])),\n        'd': [np.array(3.0), np.array(4.0)],\n    }\n\n    def map_fn(path: Sequence[str], x: np.ndarray, y: np.ndarray):\n      return x + y + len(path)\n\n    single_arg_map_fn = functools.partial(map_fn, y=np.array([0.0]))\n\n    expected_mapped_structure = (\n        tree.map_structure_with_path(single_arg_map_fn, structure))\n    mapped_structure = (\n        tree_utils.fast_map_structure_with_path(single_arg_map_fn, structure))\n    self.assertEqual(mapped_structure, expected_mapped_structure)\n\n    expected_double_mapped_structure = (\n        tree.map_structure_with_path(map_fn, structure, mapped_structure))\n    double_mapped_structure = (\n        tree_utils.fast_map_structure_with_path(map_fn, structure,\n                                                mapped_structure))\n    self.assertEqual(double_mapped_structure, expected_double_mapped_structure)",
  "def test_stack_sequence_fields(self):\n    \"\"\"Tests that `stack_sequence_fields` behaves correctly on nested data.\"\"\"\n\n    stacked = tree_utils.stack_sequence_fields(TEST_SEQUENCE)\n\n    # Check that the stacked output has the correct structure.\n    tree.assert_same_structure(stacked, TEST_SEQUENCE[0])\n\n    # Check that the leaves have the correct array shapes.\n    self.assertEqual(stacked['action'].shape, (3, 1))\n    self.assertEqual(stacked['observation'][0].shape, (3, 3))\n    self.assertEqual(stacked['reward'].shape, (3,))\n\n    # Check values.\n    self.assertEqual(stacked['observation'][0].tolist(), [\n        [0., 1., 2.],\n        [1., 2., 3.],\n        [2., 3., 4.],\n    ])\n    self.assertEqual(stacked['action'].tolist(), [[1.], [0.5], [0.3]])\n    self.assertEqual(stacked['reward'].tolist(), [1., 0., 0.5])",
  "def test_unstack_sequence_fields(self):\n    \"\"\"Tests that `unstack_sequence_fields(stack_sequence_fields(x)) == x`.\"\"\"\n    stacked = tree_utils.stack_sequence_fields(TEST_SEQUENCE)\n    batch_size = len(TEST_SEQUENCE)\n    unstacked = tree_utils.unstack_sequence_fields(stacked, batch_size)\n    tree.map_structure(np.testing.assert_array_equal, unstacked, TEST_SEQUENCE)",
  "def test_fast_map_structure_with_path(self):\n    structure = {\n        'a': {\n            'b': np.array([0.0])\n        },\n        'c': (np.array([1.0]), np.array([2.0])),\n        'd': [np.array(3.0), np.array(4.0)],\n    }\n\n    def map_fn(path: Sequence[str], x: np.ndarray, y: np.ndarray):\n      return x + y + len(path)\n\n    single_arg_map_fn = functools.partial(map_fn, y=np.array([0.0]))\n\n    expected_mapped_structure = (\n        tree.map_structure_with_path(single_arg_map_fn, structure))\n    mapped_structure = (\n        tree_utils.fast_map_structure_with_path(single_arg_map_fn, structure))\n    self.assertEqual(mapped_structure, expected_mapped_structure)\n\n    expected_double_mapped_structure = (\n        tree.map_structure_with_path(map_fn, structure, mapped_structure))\n    double_mapped_structure = (\n        tree_utils.fast_map_structure_with_path(map_fn, structure,\n                                                mapped_structure))\n    self.assertEqual(double_mapped_structure, expected_double_mapped_structure)",
  "def map_fn(path: Sequence[str], x: np.ndarray, y: np.ndarray):\n      return x + y + len(path)",
  "class PathTest(test_utils.TestCase):\n\n  def test_process_path(self):\n    root_directory = self.get_tempdir()\n    with mock.patch.object(paths, 'get_unique_id') as mock_unique_id:\n      mock_unique_id.return_value = ('test',)\n      path = paths.process_path(root_directory, 'foo', 'bar')\n    self.assertEqual(path, f'{root_directory}/test/foo/bar')\n\n  def test_unique_id_with_flag(self):\n    with mock.patch.object(paths, 'ACME_ID') as mock_acme_id:\n      mock_acme_id.value = 'test_flag'\n      self.assertEqual(paths.get_unique_id(), ('test_flag',))",
  "def test_process_path(self):\n    root_directory = self.get_tempdir()\n    with mock.patch.object(paths, 'get_unique_id') as mock_unique_id:\n      mock_unique_id.return_value = ('test',)\n      path = paths.process_path(root_directory, 'foo', 'bar')\n    self.assertEqual(path, f'{root_directory}/test/foo/bar')",
  "def test_unique_id_with_flag(self):\n    with mock.patch.object(paths, 'ACME_ID') as mock_acme_id:\n      mock_acme_id.value = 'test_flag'\n      self.assertEqual(paths.get_unique_id(), ('test_flag',))",
  "class FrozenLearner(acme.Learner):\n  \"\"\"Wraps a learner ignoring the step calls, i.e. freezing it.\"\"\"\n\n  def __init__(self,\n               learner: acme.Learner,\n               step_fn: Optional[Callable[[], None]] = None):\n    \"\"\"Initializes the frozen learner.\n\n    Args:\n      learner: Learner to be wrapped.\n      step_fn: Function to call instead of the step() method of the learner.\n        This can be used, e.g. to drop samples from an iterator that would\n        normally be consumed by the learner.\n    \"\"\"\n    self._learner = learner\n    self._step_fn = step_fn\n\n  def step(self):\n    \"\"\"See base class.\"\"\"\n    if self._step_fn:\n      self._step_fn()\n\n  def run(self, num_steps: Optional[int] = None):\n    \"\"\"See base class.\"\"\"\n    self._learner.run(num_steps)\n\n  def save(self):\n    \"\"\"See base class.\"\"\"\n    return self._learner.save()\n\n  def restore(self, state):\n    \"\"\"See base class.\"\"\"\n    self._learner.restore(state)\n\n  def get_variables(self, names: Sequence[str]) -> List[acme.types.NestedArray]:\n    \"\"\"See base class.\"\"\"\n    return self._learner.get_variables(names)",
  "def __init__(self,\n               learner: acme.Learner,\n               step_fn: Optional[Callable[[], None]] = None):\n    \"\"\"Initializes the frozen learner.\n\n    Args:\n      learner: Learner to be wrapped.\n      step_fn: Function to call instead of the step() method of the learner.\n        This can be used, e.g. to drop samples from an iterator that would\n        normally be consumed by the learner.\n    \"\"\"\n    self._learner = learner\n    self._step_fn = step_fn",
  "def step(self):\n    \"\"\"See base class.\"\"\"\n    if self._step_fn:\n      self._step_fn()",
  "def run(self, num_steps: Optional[int] = None):\n    \"\"\"See base class.\"\"\"\n    self._learner.run(num_steps)",
  "def save(self):\n    \"\"\"See base class.\"\"\"\n    return self._learner.save()",
  "def restore(self, state):\n    \"\"\"See base class.\"\"\"\n    self._learner.restore(state)",
  "def get_variables(self, names: Sequence[str]) -> List[acme.types.NestedArray]:\n    \"\"\"See base class.\"\"\"\n    return self._learner.get_variables(names)",
  "class Counter(core.Saveable):\n  \"\"\"A simple counter object that can periodically sync with a parent.\"\"\"\n\n  def __init__(self,\n               parent: Optional['Counter'] = None,\n               prefix: str = '',\n               time_delta: float = 1.0,\n               return_only_prefixed: bool = False):\n    \"\"\"Initialize the counter.\n\n    Args:\n      parent: a Counter object to cache locally (or None for no caching).\n      prefix: string prefix to use for all local counts.\n      time_delta: time difference in seconds between syncing with the parent\n        counter.\n      return_only_prefixed: if True, and if `prefix` isn't empty, return counts\n        restricted to the given `prefix` on each call to `increment` and\n        `get_counts`. The `prefix` is stripped from returned count names.\n    \"\"\"\n\n    self._parent = parent\n    self._prefix = prefix\n    self._time_delta = time_delta\n\n    # Hold local counts and we'll lock around that.\n    # These are counts to be synced to the parent and the cache.\n    self._counts = {}\n    self._lock = threading.Lock()\n\n    # We'll sync periodically (when the last sync was more than self._time_delta\n    # seconds ago.)\n    self._cache = {}\n    self._last_sync_time = 0.0\n\n    self._return_only_prefixed = return_only_prefixed\n\n  def increment(self, **counts: Number) -> Dict[str, Number]:\n    \"\"\"Increment a set of counters.\n\n    Args:\n      **counts: keyword arguments specifying count increments.\n\n    Returns:\n      The [name, value] mapping of all counters stored, i.e. this will also\n      include counts that were not updated by this call to increment.\n    \"\"\"\n    with self._lock:\n      for key, value in counts.items():\n        self._counts.setdefault(key, 0)\n        self._counts[key] += value\n    return self.get_counts()\n\n  def get_counts(self) -> Dict[str, Number]:\n    \"\"\"Return all counts tracked by this counter.\"\"\"\n    now = time.time()\n    # TODO(b/144421838): use futures instead of blocking.\n    if self._parent and (now - self._last_sync_time) > self._time_delta:\n      with self._lock:\n        counts = _prefix_keys(self._counts, self._prefix)\n        # Reset the local counts, as they will be merged into the parent and the\n        # cache.\n        self._counts = {}\n      self._cache = self._parent.increment(**counts)\n      self._last_sync_time = now\n\n    # Potentially prefix the keys in the counts dictionary.\n    counts = _prefix_keys(self._counts, self._prefix)\n\n    # If there's no prefix make a copy of the dictionary so we don't modify the\n    # internal self._counts.\n    if not self._prefix:\n      counts = dict(counts)\n\n    # Combine local counts with any parent counts.\n    for key, value in self._cache.items():\n      counts[key] = counts.get(key, 0) + value\n\n    if self._prefix and self._return_only_prefixed:\n      counts = dict([(key[len(self._prefix) + 1:], value)\n                     for key, value in counts.items()\n                     if key.startswith(f'{self._prefix}_')])\n    return counts\n\n  def save(self) -> Mapping[str, Mapping[str, Number]]:\n    return {'counts': self._counts, 'cache': self._cache}\n\n  def restore(self, state: Mapping[str, Mapping[str, Number]]):\n    # Force a sync, if necessary, on the next get_counts call.\n    self._last_sync_time = 0.\n    self._counts = state['counts']\n    self._cache = state['cache']\n\n  def get_steps_key(self) -> str:\n    \"\"\"Returns the key to use for steps by this counter.\"\"\"\n    if not self._prefix or self._return_only_prefixed:\n      return 'steps'\n    return f'{self._prefix}_steps'",
  "def _prefix_keys(dictionary: Dict[str, Number], prefix: str):\n  \"\"\"Return a dictionary with prefixed keys.\n\n  Args:\n    dictionary: dictionary to return a copy of.\n    prefix: string to use as the prefix.\n\n  Returns:\n    Return a copy of the given dictionary whose keys are replaced by\n    \"{prefix}_{key}\". If the prefix is the empty string it returns the given\n    dictionary unchanged.\n  \"\"\"\n  if prefix:\n    dictionary = {f'{prefix}_{k}': v for k, v in dictionary.items()}\n  return dictionary",
  "def __init__(self,\n               parent: Optional['Counter'] = None,\n               prefix: str = '',\n               time_delta: float = 1.0,\n               return_only_prefixed: bool = False):\n    \"\"\"Initialize the counter.\n\n    Args:\n      parent: a Counter object to cache locally (or None for no caching).\n      prefix: string prefix to use for all local counts.\n      time_delta: time difference in seconds between syncing with the parent\n        counter.\n      return_only_prefixed: if True, and if `prefix` isn't empty, return counts\n        restricted to the given `prefix` on each call to `increment` and\n        `get_counts`. The `prefix` is stripped from returned count names.\n    \"\"\"\n\n    self._parent = parent\n    self._prefix = prefix\n    self._time_delta = time_delta\n\n    # Hold local counts and we'll lock around that.\n    # These are counts to be synced to the parent and the cache.\n    self._counts = {}\n    self._lock = threading.Lock()\n\n    # We'll sync periodically (when the last sync was more than self._time_delta\n    # seconds ago.)\n    self._cache = {}\n    self._last_sync_time = 0.0\n\n    self._return_only_prefixed = return_only_prefixed",
  "def increment(self, **counts: Number) -> Dict[str, Number]:\n    \"\"\"Increment a set of counters.\n\n    Args:\n      **counts: keyword arguments specifying count increments.\n\n    Returns:\n      The [name, value] mapping of all counters stored, i.e. this will also\n      include counts that were not updated by this call to increment.\n    \"\"\"\n    with self._lock:\n      for key, value in counts.items():\n        self._counts.setdefault(key, 0)\n        self._counts[key] += value\n    return self.get_counts()",
  "def get_counts(self) -> Dict[str, Number]:\n    \"\"\"Return all counts tracked by this counter.\"\"\"\n    now = time.time()\n    # TODO(b/144421838): use futures instead of blocking.\n    if self._parent and (now - self._last_sync_time) > self._time_delta:\n      with self._lock:\n        counts = _prefix_keys(self._counts, self._prefix)\n        # Reset the local counts, as they will be merged into the parent and the\n        # cache.\n        self._counts = {}\n      self._cache = self._parent.increment(**counts)\n      self._last_sync_time = now\n\n    # Potentially prefix the keys in the counts dictionary.\n    counts = _prefix_keys(self._counts, self._prefix)\n\n    # If there's no prefix make a copy of the dictionary so we don't modify the\n    # internal self._counts.\n    if not self._prefix:\n      counts = dict(counts)\n\n    # Combine local counts with any parent counts.\n    for key, value in self._cache.items():\n      counts[key] = counts.get(key, 0) + value\n\n    if self._prefix and self._return_only_prefixed:\n      counts = dict([(key[len(self._prefix) + 1:], value)\n                     for key, value in counts.items()\n                     if key.startswith(f'{self._prefix}_')])\n    return counts",
  "def save(self) -> Mapping[str, Mapping[str, Number]]:\n    return {'counts': self._counts, 'cache': self._cache}",
  "def restore(self, state: Mapping[str, Mapping[str, Number]]):\n    # Force a sync, if necessary, on the next get_counts call.\n    self._last_sync_time = 0.\n    self._counts = state['counts']\n    self._cache = state['cache']",
  "def get_steps_key(self) -> str:\n    \"\"\"Returns the key to use for steps by this counter.\"\"\"\n    if not self._prefix or self._return_only_prefixed:\n      return 'steps'\n    return f'{self._prefix}_steps'",
  "class LpUtilsTest(absltest.TestCase):\n\n  def test_partial_kwargs(self):\n\n    def foo(a, b, c=2):\n      return a, b, c\n\n    def bar(a, b):\n      return a, b\n\n    # Override the default values. The last two should be no-ops.\n    foo1 = lp_utils.partial_kwargs(foo, c=1)\n    foo2 = lp_utils.partial_kwargs(foo)\n    bar1 = lp_utils.partial_kwargs(bar)\n\n    # Check that we raise errors on overriding kwargs with no default values\n    with self.assertRaises(ValueError):\n      lp_utils.partial_kwargs(foo, a=2)\n\n    # CHeck the we raise if we try to override a kwarg that doesn't exist.\n    with self.assertRaises(ValueError):\n      lp_utils.partial_kwargs(foo, d=2)\n\n    # Make sure we get back the correct values.\n    self.assertEqual(foo1(1, 2), (1, 2, 1))\n    self.assertEqual(foo2(1, 2), (1, 2, 2))\n    self.assertEqual(bar1(1, 2), (1, 2))",
  "def test_partial_kwargs(self):\n\n    def foo(a, b, c=2):\n      return a, b, c\n\n    def bar(a, b):\n      return a, b\n\n    # Override the default values. The last two should be no-ops.\n    foo1 = lp_utils.partial_kwargs(foo, c=1)\n    foo2 = lp_utils.partial_kwargs(foo)\n    bar1 = lp_utils.partial_kwargs(bar)\n\n    # Check that we raise errors on overriding kwargs with no default values\n    with self.assertRaises(ValueError):\n      lp_utils.partial_kwargs(foo, a=2)\n\n    # CHeck the we raise if we try to override a kwarg that doesn't exist.\n    with self.assertRaises(ValueError):\n      lp_utils.partial_kwargs(foo, d=2)\n\n    # Make sure we get back the correct values.\n    self.assertEqual(foo1(1, 2), (1, 2, 1))\n    self.assertEqual(foo2(1, 2), (1, 2, 2))\n    self.assertEqual(bar1(1, 2), (1, 2))",
  "def foo(a, b, c=2):\n      return a, b, c",
  "def bar(a, b):\n      return a, b",
  "class FlattenDictLogger(base.Logger):\n  \"\"\"Logger which flattens sub-dictionaries into the top level dict.\"\"\"\n\n  def __init__(self,\n               logger: base.Logger,\n               label: str = 'Logs',\n               raw_keys: Sequence[str] = ()):\n    \"\"\"Initializer.\n\n    Args:\n      logger: The wrapped logger.\n      label: The label to add as a prefix to all keys except for raw ones.\n      raw_keys: The keys that should not be prefixed. The values for these keys\n        must always be flat. Metric visualisation tools may require certain\n        keys to be present in the logs (e.g. 'step', 'timestamp'), so these\n        keys should not be prefixed.\n    \"\"\"\n    self._logger = logger\n    self._label = label\n    self._raw_keys = raw_keys\n\n  def write(self, values: base.LoggingData):\n    flattened_values = {}\n    for key, value in values.items():\n      if key in self._raw_keys:\n        flattened_values[key] = value\n        continue\n      name = f'{self._label}/{key}'\n      if isinstance(value, dict):\n        for sub_key, sub_value in value.items():\n          flattened_values[f'{name}/{sub_key}'] = sub_value\n      else:\n        flattened_values[name] = value\n\n    self._logger.write(flattened_values)\n\n  def close(self):\n    self._logger.close()",
  "def __init__(self,\n               logger: base.Logger,\n               label: str = 'Logs',\n               raw_keys: Sequence[str] = ()):\n    \"\"\"Initializer.\n\n    Args:\n      logger: The wrapped logger.\n      label: The label to add as a prefix to all keys except for raw ones.\n      raw_keys: The keys that should not be prefixed. The values for these keys\n        must always be flat. Metric visualisation tools may require certain\n        keys to be present in the logs (e.g. 'step', 'timestamp'), so these\n        keys should not be prefixed.\n    \"\"\"\n    self._logger = logger\n    self._label = label\n    self._raw_keys = raw_keys",
  "def write(self, values: base.LoggingData):\n    flattened_values = {}\n    for key, value in values.items():\n      if key in self._raw_keys:\n        flattened_values[key] = value\n        continue\n      name = f'{self._label}/{key}'\n      if isinstance(value, dict):\n        for sub_key, sub_value in value.items():\n          flattened_values[f'{name}/{sub_key}'] = sub_value\n      else:\n        flattened_values[name] = value\n\n    self._logger.write(flattened_values)",
  "def close(self):\n    self._logger.close()",
  "class ImageLogger(base.Logger):\n  \"\"\"Logger for writing NumPy arrays as PNG images to disk.\n\n  Assumes that all data passed are NumPy arrays that can be converted to images.\n\n  TODO(jaslanides): Make this stateless/robust to preemptions.\n  \"\"\"\n\n  def __init__(\n      self,\n      directory: str,\n      *,\n      label: str = '',\n      mode: Optional[str] = None,\n  ):\n    \"\"\"Initialises the writer.\n\n    Args:\n      directory: Base directory to which images are logged.\n      label: Optional subdirectory in which to save images.\n      mode: Image mode for use with Pillow. If `None` (default), mode is\n        determined by data type. See [0] for details.\n\n    [0] https://pillow.readthedocs.io/en/stable/handbook/concepts.html#modes\n    \"\"\"\n\n    self._path = self._get_path(directory, label)\n    if not self._path.exists():\n      self._path.mkdir(parents=True)\n\n    self._mode = mode\n    self._indices = collections.defaultdict(int)\n\n  def write(self, data: base.LoggingData):\n    for k, v in data.items():\n      image = Image.fromarray(v, mode=self._mode)\n      path = self._path / f'{k}_{self._indices[k]:06}.png'\n      self._indices[k] += 1\n      with path.open(mode='wb') as f:\n        logging.info('Writing image to %s.', str(path))\n        image.save(f)\n\n  def close(self):\n    pass\n\n  @property\n  def directory(self) -> str:\n    return str(self._path)\n\n  def _get_path(self, *args, **kwargs) -> pathlib.Path:\n    return pathlib.Path(*args, **kwargs)",
  "def __init__(\n      self,\n      directory: str,\n      *,\n      label: str = '',\n      mode: Optional[str] = None,\n  ):\n    \"\"\"Initialises the writer.\n\n    Args:\n      directory: Base directory to which images are logged.\n      label: Optional subdirectory in which to save images.\n      mode: Image mode for use with Pillow. If `None` (default), mode is\n        determined by data type. See [0] for details.\n\n    [0] https://pillow.readthedocs.io/en/stable/handbook/concepts.html#modes\n    \"\"\"\n\n    self._path = self._get_path(directory, label)\n    if not self._path.exists():\n      self._path.mkdir(parents=True)\n\n    self._mode = mode\n    self._indices = collections.defaultdict(int)",
  "def write(self, data: base.LoggingData):\n    for k, v in data.items():\n      image = Image.fromarray(v, mode=self._mode)\n      path = self._path / f'{k}_{self._indices[k]:06}.png'\n      self._indices[k] += 1\n      with path.open(mode='wb') as f:\n        logging.info('Writing image to %s.', str(path))\n        image.save(f)",
  "def close(self):\n    pass",
  "def directory(self) -> str:\n    return str(self._path)",
  "def _get_path(self, *args, **kwargs) -> pathlib.Path:\n    return pathlib.Path(*args, **kwargs)",
  "class FakeLogger(base.Logger):\n  \"\"\"A fake logger for testing.\"\"\"\n\n  def __init__(self):\n    self.data = []\n\n  def write(self, data):\n    self.data.append(data)\n\n  @property\n  def last_write(self):\n    return self.data[-1]\n\n  def close(self):\n    pass",
  "class GatedFilterTest(absltest.TestCase):\n\n  def test_logarithmic_filter(self):\n    logger = FakeLogger()\n    filtered = filters.GatedFilter.logarithmic(logger, n=10)\n    for t in range(100):\n      filtered.write({'t': t})\n    rows = [row['t'] for row in logger.data]\n    self.assertEqual(rows, [*range(10), *range(10, 100, 10)])\n\n  def test_periodic_filter(self):\n    logger = FakeLogger()\n    filtered = filters.GatedFilter.periodic(logger, interval=10)\n    for t in range(100):\n      filtered.write({'t': t})\n    rows = [row['t'] for row in logger.data]\n    self.assertEqual(rows, list(range(0, 100, 10)))",
  "class TimeFilterTest(absltest.TestCase):\n\n  def test_delta(self):\n    logger = FakeLogger()\n    filtered = filters.TimeFilter(logger, time_delta=0.1)\n\n    # Logged.\n    filtered.write({'foo': 1})\n    self.assertIn('foo', logger.last_write)\n\n    # *Not* logged.\n    filtered.write({'bar': 2})\n    self.assertNotIn('bar', logger.last_write)\n\n    # Wait out delta.\n    time.sleep(0.11)\n\n    # Logged.\n    filtered.write({'baz': 3})\n    self.assertIn('baz', logger.last_write)\n\n    self.assertLen(logger.data, 2)",
  "class KeyFilterTest(absltest.TestCase):\n\n  def test_keep_filter(self):\n    logger = FakeLogger()\n    filtered = filters.KeyFilter(logger, keep=('foo',))\n    filtered.write({'foo': 'bar', 'baz': 12})\n    row, *_ = logger.data\n    self.assertIn('foo', row)\n    self.assertNotIn('baz', row)\n\n  def test_drop_filter(self):\n    logger = FakeLogger()\n    filtered = filters.KeyFilter(logger, drop=('foo',))\n    filtered.write({'foo': 'bar', 'baz': 12})\n    row, *_ = logger.data\n    self.assertIn('baz', row)\n    self.assertNotIn('foo', row)\n\n  def test_bad_arguments(self):\n    with self.assertRaises(ValueError):\n      filters.KeyFilter(FakeLogger())\n    with self.assertRaises(ValueError):\n      filters.KeyFilter(FakeLogger(), keep=('a',), drop=('b',))",
  "def __init__(self):\n    self.data = []",
  "def write(self, data):\n    self.data.append(data)",
  "def last_write(self):\n    return self.data[-1]",
  "def close(self):\n    pass",
  "def test_logarithmic_filter(self):\n    logger = FakeLogger()\n    filtered = filters.GatedFilter.logarithmic(logger, n=10)\n    for t in range(100):\n      filtered.write({'t': t})\n    rows = [row['t'] for row in logger.data]\n    self.assertEqual(rows, [*range(10), *range(10, 100, 10)])",
  "def test_periodic_filter(self):\n    logger = FakeLogger()\n    filtered = filters.GatedFilter.periodic(logger, interval=10)\n    for t in range(100):\n      filtered.write({'t': t})\n    rows = [row['t'] for row in logger.data]\n    self.assertEqual(rows, list(range(0, 100, 10)))",
  "def test_delta(self):\n    logger = FakeLogger()\n    filtered = filters.TimeFilter(logger, time_delta=0.1)\n\n    # Logged.\n    filtered.write({'foo': 1})\n    self.assertIn('foo', logger.last_write)\n\n    # *Not* logged.\n    filtered.write({'bar': 2})\n    self.assertNotIn('bar', logger.last_write)\n\n    # Wait out delta.\n    time.sleep(0.11)\n\n    # Logged.\n    filtered.write({'baz': 3})\n    self.assertIn('baz', logger.last_write)\n\n    self.assertLen(logger.data, 2)",
  "def test_keep_filter(self):\n    logger = FakeLogger()\n    filtered = filters.KeyFilter(logger, keep=('foo',))\n    filtered.write({'foo': 'bar', 'baz': 12})\n    row, *_ = logger.data\n    self.assertIn('foo', row)\n    self.assertNotIn('baz', row)",
  "def test_drop_filter(self):\n    logger = FakeLogger()\n    filtered = filters.KeyFilter(logger, drop=('foo',))\n    filtered.write({'foo': 'bar', 'baz': 12})\n    row, *_ = logger.data\n    self.assertIn('baz', row)\n    self.assertNotIn('foo', row)",
  "def test_bad_arguments(self):\n    with self.assertRaises(ValueError):\n      filters.KeyFilter(FakeLogger())\n    with self.assertRaises(ValueError):\n      filters.KeyFilter(FakeLogger(), keep=('a',), drop=('b',))",
  "def _format_key(key: str) -> str:\n  \"\"\"Internal function for formatting keys.\"\"\"\n  return key.replace('_', ' ').title()",
  "def _format_value(value: Any) -> str:\n  \"\"\"Internal function for formatting values.\"\"\"\n  value = base.to_numpy(value)\n  if isinstance(value, (float, np.number)):\n    return f'{value:0.3f}'\n  return f'{value}'",
  "def serialize(values: base.LoggingData) -> str:\n  \"\"\"Converts `values` to a pretty-printed string.\n\n  This takes a dictionary `values` whose keys are strings and returns\n  a formatted string such that each [key, value] pair is separated by ' = ' and\n  each entry is separated by ' | '. The keys are sorted alphabetically to ensure\n  a consistent order, and snake case is split into words.\n\n  For example:\n\n      values = {'a': 1, 'b' = 2.33333333, 'c': 'hello', 'big_value': 10}\n      # Returns 'A = 1 | B = 2.333 | Big Value = 10 | C = hello'\n      values_string = serialize(values)\n\n  Args:\n    values: A dictionary with string keys.\n\n  Returns:\n    A formatted string.\n  \"\"\"\n  return ' | '.join(f'{_format_key(k)} = {_format_value(v)}'\n                    for k, v in sorted(values.items()))",
  "class TerminalLogger(base.Logger):\n  \"\"\"Logs to terminal.\"\"\"\n\n  def __init__(\n      self,\n      label: str = '',\n      print_fn: Callable[[str], None] = logging.info,\n      serialize_fn: Callable[[base.LoggingData], str] = serialize,\n      time_delta: float = 0.0,\n  ):\n    \"\"\"Initializes the logger.\n\n    Args:\n      label: label string to use when logging.\n      print_fn: function to call which acts like print.\n      serialize_fn: function to call which transforms values into a str.\n      time_delta: How often (in seconds) to write values. This can be used to\n        minimize terminal spam, but is 0 by default---ie everything is written.\n    \"\"\"\n\n    self._print_fn = print_fn\n    self._serialize_fn = serialize_fn\n    self._label = label and f'[{_format_key(label)}] '\n    self._time = time.time()\n    self._time_delta = time_delta\n\n  def write(self, values: base.LoggingData):\n    now = time.time()\n    if (now - self._time) > self._time_delta:\n      self._print_fn(f'{self._label}{self._serialize_fn(values)}')\n      self._time = now\n\n  def close(self):\n    pass",
  "def __init__(\n      self,\n      label: str = '',\n      print_fn: Callable[[str], None] = logging.info,\n      serialize_fn: Callable[[base.LoggingData], str] = serialize,\n      time_delta: float = 0.0,\n  ):\n    \"\"\"Initializes the logger.\n\n    Args:\n      label: label string to use when logging.\n      print_fn: function to call which acts like print.\n      serialize_fn: function to call which transforms values into a str.\n      time_delta: How often (in seconds) to write values. This can be used to\n        minimize terminal spam, but is 0 by default---ie everything is written.\n    \"\"\"\n\n    self._print_fn = print_fn\n    self._serialize_fn = serialize_fn\n    self._label = label and f'[{_format_key(label)}] '\n    self._time = time.time()\n    self._time_delta = time_delta",
  "def write(self, values: base.LoggingData):\n    now = time.time()\n    if (now - self._time) > self._time_delta:\n      self._print_fn(f'{self._label}{self._serialize_fn(values)}')\n      self._time = now",
  "def close(self):\n    pass",
  "class Logger(abc.ABC):\n  \"\"\"A logger has a `write` method.\"\"\"\n\n  @abc.abstractmethod\n  def write(self, data: LoggingData) -> None:\n    \"\"\"Writes `data` to destination (file, terminal, database, etc).\"\"\"\n\n  @abc.abstractmethod\n  def close(self) -> None:\n    \"\"\"Closes the logger, not expecting any further write.\"\"\"",
  "class LoggerFactory(Protocol):\n\n  def __call__(self,\n               label: LoggerLabel,\n               steps_key: Optional[LoggerStepsKey] = None,\n               instance: Optional[TaskInstance] = None) -> Logger:\n    ...",
  "class NoOpLogger(Logger):\n  \"\"\"Simple Logger which does nothing and outputs no logs.\n\n  This should be used sparingly, but it can prove useful if we want to quiet an\n  individual component and have it produce no logging whatsoever.\n  \"\"\"\n\n  def write(self, data: LoggingData):\n    pass\n\n  def close(self):\n    pass",
  "def tensor_to_numpy(value: Any):\n  if hasattr(value, 'numpy'):\n    return value.numpy()  # tf.Tensor (TF2).\n  if hasattr(value, 'device_buffer'):\n    return np.asarray(value)  # jnp.DeviceArray.\n  return value",
  "def to_numpy(values: Any):\n  \"\"\"Converts tensors in a nested structure to numpy.\n\n  Converts tensors from TensorFlow to Numpy if needed without importing TF\n  dependency.\n\n  Args:\n    values: nested structure with numpy and / or TF tensors.\n\n  Returns:\n    Same nested structure as values, but with numpy tensors.\n  \"\"\"\n  return tree.map_structure(tensor_to_numpy, values)",
  "def write(self, data: LoggingData) -> None:\n    \"\"\"Writes `data` to destination (file, terminal, database, etc).\"\"\"",
  "def close(self) -> None:\n    \"\"\"Closes the logger, not expecting any further write.\"\"\"",
  "def __call__(self,\n               label: LoggerLabel,\n               steps_key: Optional[LoggerStepsKey] = None,\n               instance: Optional[TaskInstance] = None) -> Logger:\n    ...",
  "def write(self, data: LoggingData):\n    pass",
  "def close(self):\n    pass",
  "class Dispatcher(base.Logger):\n  \"\"\"Writes data to multiple `Logger` objects.\"\"\"\n\n  def __init__(\n      self,\n      to: Sequence[base.Logger],\n      serialize_fn: Optional[Callable[[base.LoggingData], str]] = None,\n  ):\n    \"\"\"Initialize `Dispatcher` connected to several `Logger` objects.\"\"\"\n    self._to = to\n    self._serialize_fn = serialize_fn\n\n  def write(self, values: base.LoggingData):\n    \"\"\"Writes `values` to the underlying `Logger` objects.\"\"\"\n    if self._serialize_fn:\n      values = self._serialize_fn(values)\n    for logger in self._to:\n      logger.write(values)\n\n  def close(self):\n    for logger in self._to:\n      logger.close()",
  "def __init__(\n      self,\n      to: Sequence[base.Logger],\n      serialize_fn: Optional[Callable[[base.LoggingData], str]] = None,\n  ):\n    \"\"\"Initialize `Dispatcher` connected to several `Logger` objects.\"\"\"\n    self._to = to\n    self._serialize_fn = serialize_fn",
  "def write(self, values: base.LoggingData):\n    \"\"\"Writes `values` to the underlying `Logger` objects.\"\"\"\n    if self._serialize_fn:\n      values = self._serialize_fn(values)\n    for logger in self._to:\n      logger.write(values)",
  "def close(self):\n    for logger in self._to:\n      logger.close()",
  "class InMemoryLogger(base.Logger):\n  \"\"\"A simple logger that keeps all data in memory.\"\"\"\n\n  def __init__(self):\n    self._data = []\n\n  def write(self, data: base.LoggingData):\n    self._data.append(data)\n\n  def close(self):\n    pass\n\n  @property\n  def data(self) -> Sequence[base.LoggingData]:\n    return self._data",
  "def __init__(self):\n    self._data = []",
  "def write(self, data: base.LoggingData):\n    self._data.append(data)",
  "def close(self):\n    pass",
  "def data(self) -> Sequence[base.LoggingData]:\n    return self._data",
  "def make_default_logger(\n    label: str,\n    save_data: bool = True,\n    time_delta: float = 1.0,\n    asynchronous: bool = False,\n    print_fn: Optional[Callable[[str], None]] = None,\n    serialize_fn: Optional[Callable[[Mapping[str, Any]], str]] = base.to_numpy,\n    steps_key: str = 'steps',\n) -> base.Logger:\n  \"\"\"Makes a default Acme logger.\n\n  Args:\n    label: Name to give to the logger.\n    save_data: Whether to persist data.\n    time_delta: Time (in seconds) between logging events.\n    asynchronous: Whether the write function should block or not.\n    print_fn: How to print to terminal (defaults to print).\n    serialize_fn: An optional function to apply to the write inputs before\n      passing them to the various loggers.\n    steps_key: Ignored.\n\n  Returns:\n    A logger object that responds to logger.write(some_dict).\n  \"\"\"\n  del steps_key\n  if not print_fn:\n    print_fn = logging.info\n  terminal_logger = terminal.TerminalLogger(label=label, print_fn=print_fn)\n\n  loggers = [terminal_logger]\n\n  if save_data:\n    loggers.append(csv.CSVLogger(label=label))\n\n  # Dispatch to all writers and filter Nones and by time.\n  logger = aggregators.Dispatcher(loggers, serialize_fn)\n  logger = filters.NoneFilter(logger)\n  if asynchronous:\n    logger = async_logger.AsyncLogger(logger)\n  logger = filters.TimeFilter(logger, time_delta)\n\n  return logger",
  "class ImageTest(test_utils.TestCase):\n\n  def test_save_load_identity(self):\n    directory = self.get_tempdir()\n    logger = image.ImageLogger(directory, label='foo')\n    array = (np.random.rand(10, 10) * 255).astype(np.uint8)\n    logger.write({'img': array})\n\n    with open(f'{directory}/foo/img_000000.png', mode='rb') as f:\n      out = np.asarray(Image.open(f))\n    np.testing.assert_array_equal(array, out)\n\n  def test_indexing(self):\n    directory = self.get_tempdir()\n    logger = image.ImageLogger(directory, label='foo')\n    zeros = np.zeros(shape=(3, 3), dtype=np.uint8)\n    logger.write({'img': zeros, 'other_img': zeros + 1})\n    logger.write({'img': zeros - 1})\n    logger.write({'other_img': zeros + 1})\n    logger.write({'other_img': zeros + 2})\n\n    fnames = sorted(os.listdir(f'{directory}/foo'))\n    expected = [\n        'img_000000.png',\n        'img_000001.png',\n        'other_img_000000.png',\n        'other_img_000001.png',\n        'other_img_000002.png',\n    ]\n    self.assertEqual(fnames, expected)",
  "def test_save_load_identity(self):\n    directory = self.get_tempdir()\n    logger = image.ImageLogger(directory, label='foo')\n    array = (np.random.rand(10, 10) * 255).astype(np.uint8)\n    logger.write({'img': array})\n\n    with open(f'{directory}/foo/img_000000.png', mode='rb') as f:\n      out = np.asarray(Image.open(f))\n    np.testing.assert_array_equal(array, out)",
  "def test_indexing(self):\n    directory = self.get_tempdir()\n    logger = image.ImageLogger(directory, label='foo')\n    zeros = np.zeros(shape=(3, 3), dtype=np.uint8)\n    logger.write({'img': zeros, 'other_img': zeros + 1})\n    logger.write({'img': zeros - 1})\n    logger.write({'other_img': zeros + 1})\n    logger.write({'other_img': zeros + 2})\n\n    fnames = sorted(os.listdir(f'{directory}/foo'))\n    expected = [\n        'img_000000.png',\n        'img_000001.png',\n        'other_img_000000.png',\n        'other_img_000001.png',\n        'other_img_000002.png',\n    ]\n    self.assertEqual(fnames, expected)",
  "class BaseTest(absltest.TestCase):\n\n  def test_tensor_serialisation(self):\n    data = {'x': tf.zeros(shape=(32,))}\n    output = base.to_numpy(data)\n    expected = {'x': np.zeros(shape=(32,))}\n    np.testing.assert_array_equal(output['x'], expected['x'])\n\n  def test_device_array_serialisation(self):\n    data = {'x': jnp.zeros(shape=(32,))}\n    output = base.to_numpy(data)\n    expected = {'x': np.zeros(shape=(32,))}\n    np.testing.assert_array_equal(output['x'], expected['x'])",
  "def test_tensor_serialisation(self):\n    data = {'x': tf.zeros(shape=(32,))}\n    output = base.to_numpy(data)\n    expected = {'x': np.zeros(shape=(32,))}\n    np.testing.assert_array_equal(output['x'], expected['x'])",
  "def test_device_array_serialisation(self):\n    data = {'x': jnp.zeros(shape=(32,))}\n    output = base.to_numpy(data)\n    expected = {'x': np.zeros(shape=(32,))}\n    np.testing.assert_array_equal(output['x'], expected['x'])",
  "class AsyncLogger(base.Logger):\n  \"\"\"Logger which makes the logging to another logger asyncronous.\"\"\"\n\n  def __init__(self, to: base.Logger):\n    \"\"\"Initializes the logger.\n\n    Args:\n      to: A `Logger` object to which the current object will forward its results\n        when `write` is called.\n    \"\"\"\n    self._to = to\n    self._async_worker = async_utils.AsyncExecutor(self._to.write, queue_size=5)\n\n  def write(self, values: Mapping[str, Any]):\n    self._async_worker.put(values)\n\n  def close(self):\n    \"\"\"Closes the logger, closing is synchronous.\"\"\"\n    self._async_worker.close()\n    self._to.close()",
  "def __init__(self, to: base.Logger):\n    \"\"\"Initializes the logger.\n\n    Args:\n      to: A `Logger` object to which the current object will forward its results\n        when `write` is called.\n    \"\"\"\n    self._to = to\n    self._async_worker = async_utils.AsyncExecutor(self._to.write, queue_size=5)",
  "def write(self, values: Mapping[str, Any]):\n    self._async_worker.put(values)",
  "def close(self):\n    \"\"\"Closes the logger, closing is synchronous.\"\"\"\n    self._async_worker.close()\n    self._to.close()",
  "def _format_key(key: str) -> str:\n  \"\"\"Internal function for formatting keys in Tensorboard format.\"\"\"\n  return key.title().replace('_', '')",
  "class TFSummaryLogger(base.Logger):\n  \"\"\"Logs to a tf.summary created in a given logdir.\n\n  If multiple TFSummaryLogger are created with the same logdir, results will be\n  categorized by labels.\n  \"\"\"\n\n  def __init__(\n      self,\n      logdir: str,\n      label: str = 'Logs',\n      steps_key: Optional[str] = None\n  ):\n    \"\"\"Initializes the logger.\n\n    Args:\n      logdir: directory to which we should log files.\n      label: label string to use when logging. Default to 'Logs'.\n      steps_key: key to use for steps. Must be in the values passed to write.\n    \"\"\"\n    self._time = time.time()\n    self.label = label\n    self._iter = 0\n    self.summary = tf.summary.create_file_writer(logdir)\n    self._steps_key = steps_key\n\n  def write(self, values: base.LoggingData):\n    if self._steps_key is not None and self._steps_key not in values:\n      logging.warning('steps key %s not found. Skip logging.', self._steps_key)\n      return\n\n    step = values[\n        self._steps_key] if self._steps_key is not None else self._iter\n\n    with self.summary.as_default():\n      # TODO(b/159065169): Remove this suppression once the bug is resolved.\n      # pytype: disable=unsupported-operands\n      for key in values.keys() - [self._steps_key]:\n        # pytype: enable=unsupported-operands\n        tf.summary.scalar(\n            f'{self.label}/{_format_key(key)}', data=values[key], step=step)\n    self._iter += 1\n\n  def close(self):\n    self.summary.close()",
  "def __init__(\n      self,\n      logdir: str,\n      label: str = 'Logs',\n      steps_key: Optional[str] = None\n  ):\n    \"\"\"Initializes the logger.\n\n    Args:\n      logdir: directory to which we should log files.\n      label: label string to use when logging. Default to 'Logs'.\n      steps_key: key to use for steps. Must be in the values passed to write.\n    \"\"\"\n    self._time = time.time()\n    self.label = label\n    self._iter = 0\n    self.summary = tf.summary.create_file_writer(logdir)\n    self._steps_key = steps_key",
  "def write(self, values: base.LoggingData):\n    if self._steps_key is not None and self._steps_key not in values:\n      logging.warning('steps key %s not found. Skip logging.', self._steps_key)\n      return\n\n    step = values[\n        self._steps_key] if self._steps_key is not None else self._iter\n\n    with self.summary.as_default():\n      # TODO(b/159065169): Remove this suppression once the bug is resolved.\n      # pytype: disable=unsupported-operands\n      for key in values.keys() - [self._steps_key]:\n        # pytype: enable=unsupported-operands\n        tf.summary.scalar(\n            f'{self.label}/{_format_key(key)}', data=values[key], step=step)\n    self._iter += 1",
  "def close(self):\n    self.summary.close()",
  "class TimestampLogger(base.Logger):\n  \"\"\"Logger which populates the timestamp key with the current timestamp.\"\"\"\n\n  def __init__(self, logger: base.Logger, timestamp_key: str):\n    self._logger = logger\n    self._timestamp_key = timestamp_key\n\n  def write(self, values: base.LoggingData):\n    values = dict(values)\n    values[self._timestamp_key] = time.time()\n    self._logger.write(values)\n\n  def close(self):\n    self._logger.close()",
  "def __init__(self, logger: base.Logger, timestamp_key: str):\n    self._logger = logger\n    self._timestamp_key = timestamp_key",
  "def write(self, values: base.LoggingData):\n    values = dict(values)\n    values[self._timestamp_key] = time.time()\n    self._logger.write(values)",
  "def close(self):\n    self._logger.close()",
  "class ConstantLogger(base.Logger):\n  \"\"\"Logger for values that remain constant throughout the experiment.\n\n  This logger is used to log additional values e.g. level_name or\n  hyperparameters that do not change in an experiment. Having these values\n  allows to group or facet plots when analysing data post-experiment.\n  \"\"\"\n\n  def __init__(\n      self,\n      constant_data: base.LoggingData,\n      to: base.Logger,\n  ):\n    \"\"\"Initialise the extra info logger.\n\n    Args:\n      constant_data: Key-value pairs containing the constant info to be logged.\n      to: The logger to add these extra info to.\n    \"\"\"\n    self._constant_data = constant_data\n    self._to = to\n\n  def write(self, data: base.LoggingData):\n    self._to.write({**self._constant_data, **data})\n\n  def close(self):\n    self._to.close()",
  "def __init__(\n      self,\n      constant_data: base.LoggingData,\n      to: base.Logger,\n  ):\n    \"\"\"Initialise the extra info logger.\n\n    Args:\n      constant_data: Key-value pairs containing the constant info to be logged.\n      to: The logger to add these extra info to.\n    \"\"\"\n    self._constant_data = constant_data\n    self._to = to",
  "def write(self, data: base.LoggingData):\n    self._to.write({**self._constant_data, **data})",
  "def close(self):\n    self._to.close()",
  "class AutoCloseLogger(base.Logger):\n  \"\"\"Logger which auto closes itself on exit if not already closed.\"\"\"\n\n  def __init__(self, logger: base.Logger):\n    self._logger = logger\n    # The finalizer \"logger.close\" is invoked in one of the following scenario:\n    # 1) the current logger is GC\n    # 2) from the python doc, when the program exits, each remaining live\n    #    finalizer is called.\n    # Note that in the normal flow, where \"close\" is explicitly called,\n    # the finalizer is marked as dead using the detach function so that\n    # the underlying logger is not closed twice (once explicitly and once\n    # implicitly when the object is GC or when the program exits).\n    self._finalizer = weakref.finalize(self, logger.close)\n\n  def write(self, values: base.LoggingData):\n    self._logger.write(values)\n\n  def close(self):\n    if self._finalizer.detach():\n      self._logger.close()\n    self._logger = None",
  "def __init__(self, logger: base.Logger):\n    self._logger = logger\n    # The finalizer \"logger.close\" is invoked in one of the following scenario:\n    # 1) the current logger is GC\n    # 2) from the python doc, when the program exits, each remaining live\n    #    finalizer is called.\n    # Note that in the normal flow, where \"close\" is explicitly called,\n    # the finalizer is marked as dead using the detach function so that\n    # the underlying logger is not closed twice (once explicitly and once\n    # implicitly when the object is GC or when the program exits).\n    self._finalizer = weakref.finalize(self, logger.close)",
  "def write(self, values: base.LoggingData):\n    self._logger.write(values)",
  "def close(self):\n    if self._finalizer.detach():\n      self._logger.close()\n    self._logger = None",
  "class LoggingTest(absltest.TestCase):\n\n  def test_logging_output_format(self):\n    inputs = {\n        'c': 'foo',\n        'a': 1337,\n        'b': 42.0001,\n    }\n    expected_outputs = 'A = 1337 | B = 42.000 | C = foo'\n    test_fn = lambda outputs: self.assertEqual(outputs, expected_outputs)\n\n    logger = terminal.TerminalLogger(print_fn=test_fn)\n    logger.write(inputs)\n\n  def test_label(self):\n    inputs = {'foo': 'bar', 'baz': 123}\n    expected_outputs = '[Test] Baz = 123 | Foo = bar'\n    test_fn = lambda outputs: self.assertEqual(outputs, expected_outputs)\n\n    logger = terminal.TerminalLogger(print_fn=test_fn, label='test')\n    logger.write(inputs)",
  "def test_logging_output_format(self):\n    inputs = {\n        'c': 'foo',\n        'a': 1337,\n        'b': 42.0001,\n    }\n    expected_outputs = 'A = 1337 | B = 42.000 | C = foo'\n    test_fn = lambda outputs: self.assertEqual(outputs, expected_outputs)\n\n    logger = terminal.TerminalLogger(print_fn=test_fn)\n    logger.write(inputs)",
  "def test_label(self):\n    inputs = {'foo': 'bar', 'baz': 123}\n    expected_outputs = '[Test] Baz = 123 | Foo = bar'\n    test_fn = lambda outputs: self.assertEqual(outputs, expected_outputs)\n\n    logger = terminal.TerminalLogger(print_fn=test_fn, label='test')\n    logger.write(inputs)",
  "class CSVLogger(base.Logger):\n  \"\"\"Standard CSV logger.\n\n  The fields are inferred from the first call to write() and any additional\n  fields afterwards are ignored.\n\n  TODO(jaslanides): Consider making this stateless/robust to preemption.\n  \"\"\"\n\n  _open = open\n\n  def __init__(\n      self,\n      directory_or_file: Union[str, TextIO] = '~/acme',\n      label: str = '',\n      time_delta: float = 0.,\n      add_uid: bool = True,\n      flush_every: int = 30,\n  ):\n    \"\"\"Instantiates the logger.\n\n    Args:\n      directory_or_file: Either a directory path as a string, or a file TextIO\n        object.\n      label: Extra label to add to logger. This is added as a suffix to the\n        directory.\n      time_delta: Interval in seconds between which writes are dropped to\n        throttle throughput.\n      add_uid: Whether to add a UID to the file path. See `paths.process_path`\n        for details.\n      flush_every: Interval (in writes) between flushes.\n    \"\"\"\n\n    if flush_every <= 0:\n      raise ValueError(\n          f'`flush_every` must be a positive integer (got {flush_every}).')\n\n    self._last_log_time = time.time() - time_delta\n    self._time_delta = time_delta\n    self._flush_every = flush_every\n    self._add_uid = add_uid\n    self._writer = None\n    self._file_owner = False\n    self._file = self._create_file(directory_or_file, label)\n    self._writes = 0\n    logging.info('Logging to %s', self.file_path)\n\n  def _create_file(\n      self,\n      directory_or_file: Union[str, TextIO],\n      label: str,\n  ) -> TextIO:\n    \"\"\"Opens a file if input is a directory or use existing file.\"\"\"\n    if isinstance(directory_or_file, str):\n      directory = paths.process_path(\n          directory_or_file, 'logs', label, add_uid=self._add_uid)\n      file_path = os.path.join(directory, 'logs.csv')\n      self._file_owner = True\n      return self._open(file_path, mode='a')\n\n    # TextIO instance.\n    file = directory_or_file\n    if label:\n      logging.info('File, not directory, passed to CSVLogger; label not used.')\n    if not file.mode.startswith('a'):\n      raise ValueError('File must be open in append mode; instead got '\n                       f'mode=\"{file.mode}\".')\n    return file\n\n  def write(self, data: base.LoggingData):\n    \"\"\"Writes a `data` into a row of comma-separated values.\"\"\"\n    # Only log if `time_delta` seconds have passed since last logging event.\n    now = time.time()\n\n    # TODO(b/192227744): Remove this in favour of filters.TimeFilter.\n    elapsed = now - self._last_log_time\n    if elapsed < self._time_delta:\n      logging.debug('Not due to log for another %.2f seconds, dropping data.',\n                    self._time_delta - elapsed)\n      return\n    self._last_log_time = now\n\n    # Append row to CSV.\n    data = base.to_numpy(data)\n    # Use fields from initial `data` to create the header. If extra fields are\n    # present in subsequent `data`, we ignore them.\n    if not self._writer:\n      fields = sorted(data.keys())\n      self._writer = csv.DictWriter(self._file, fieldnames=fields,\n                                    extrasaction='ignore')\n      # Write header only if the file is empty.\n      if not self._file.tell():\n        self._writer.writeheader()\n    self._writer.writerow(data)\n\n    # Flush every `flush_every` writes.\n    if self._writes % self._flush_every == 0:\n      self.flush()\n    self._writes += 1\n\n  def close(self):\n    self.flush()\n    if self._file_owner:\n      self._file.close()\n\n  def flush(self):\n    self._file.flush()\n\n  @property\n  def file_path(self) -> str:\n    return self._file.name",
  "def __init__(\n      self,\n      directory_or_file: Union[str, TextIO] = '~/acme',\n      label: str = '',\n      time_delta: float = 0.,\n      add_uid: bool = True,\n      flush_every: int = 30,\n  ):\n    \"\"\"Instantiates the logger.\n\n    Args:\n      directory_or_file: Either a directory path as a string, or a file TextIO\n        object.\n      label: Extra label to add to logger. This is added as a suffix to the\n        directory.\n      time_delta: Interval in seconds between which writes are dropped to\n        throttle throughput.\n      add_uid: Whether to add a UID to the file path. See `paths.process_path`\n        for details.\n      flush_every: Interval (in writes) between flushes.\n    \"\"\"\n\n    if flush_every <= 0:\n      raise ValueError(\n          f'`flush_every` must be a positive integer (got {flush_every}).')\n\n    self._last_log_time = time.time() - time_delta\n    self._time_delta = time_delta\n    self._flush_every = flush_every\n    self._add_uid = add_uid\n    self._writer = None\n    self._file_owner = False\n    self._file = self._create_file(directory_or_file, label)\n    self._writes = 0\n    logging.info('Logging to %s', self.file_path)",
  "def _create_file(\n      self,\n      directory_or_file: Union[str, TextIO],\n      label: str,\n  ) -> TextIO:\n    \"\"\"Opens a file if input is a directory or use existing file.\"\"\"\n    if isinstance(directory_or_file, str):\n      directory = paths.process_path(\n          directory_or_file, 'logs', label, add_uid=self._add_uid)\n      file_path = os.path.join(directory, 'logs.csv')\n      self._file_owner = True\n      return self._open(file_path, mode='a')\n\n    # TextIO instance.\n    file = directory_or_file\n    if label:\n      logging.info('File, not directory, passed to CSVLogger; label not used.')\n    if not file.mode.startswith('a'):\n      raise ValueError('File must be open in append mode; instead got '\n                       f'mode=\"{file.mode}\".')\n    return file",
  "def write(self, data: base.LoggingData):\n    \"\"\"Writes a `data` into a row of comma-separated values.\"\"\"\n    # Only log if `time_delta` seconds have passed since last logging event.\n    now = time.time()\n\n    # TODO(b/192227744): Remove this in favour of filters.TimeFilter.\n    elapsed = now - self._last_log_time\n    if elapsed < self._time_delta:\n      logging.debug('Not due to log for another %.2f seconds, dropping data.',\n                    self._time_delta - elapsed)\n      return\n    self._last_log_time = now\n\n    # Append row to CSV.\n    data = base.to_numpy(data)\n    # Use fields from initial `data` to create the header. If extra fields are\n    # present in subsequent `data`, we ignore them.\n    if not self._writer:\n      fields = sorted(data.keys())\n      self._writer = csv.DictWriter(self._file, fieldnames=fields,\n                                    extrasaction='ignore')\n      # Write header only if the file is empty.\n      if not self._file.tell():\n        self._writer.writeheader()\n    self._writer.writerow(data)\n\n    # Flush every `flush_every` writes.\n    if self._writes % self._flush_every == 0:\n      self.flush()\n    self._writes += 1",
  "def close(self):\n    self.flush()\n    if self._file_owner:\n      self._file.close()",
  "def flush(self):\n    self._file.flush()",
  "def file_path(self) -> str:\n    return self._file.name",
  "class CSVLoggingTest(test_utils.TestCase):\n\n  def test_logging_input_is_directory(self):\n\n    # Set up logger.\n    directory = self.get_tempdir()\n    label = 'test'\n    logger = csv_logger.CSVLogger(directory_or_file=directory, label=label)\n\n    # Write data and close.\n    for inp in _TEST_INPUTS:\n      logger.write(inp)\n    logger.close()\n\n    # Read back data.\n    outputs = []\n    with open(logger.file_path) as f:\n      csv_reader = csv.DictReader(f)\n      for row in csv_reader:\n        outputs.append(dict(row))\n    self.assertEqual(outputs, _TEST_INPUTS)\n\n  @parameterized.parameters(True, False)\n  def test_logging_input_is_file(self, add_uid: bool):\n\n    # Set up logger.\n    directory = paths.process_path(\n        self.get_tempdir(), 'logs', 'my_label', add_uid=add_uid)\n    file = open(os.path.join(directory, 'logs.csv'), 'a')\n    logger = csv_logger.CSVLogger(directory_or_file=file, add_uid=add_uid)\n\n    # Write data and close.\n    for inp in _TEST_INPUTS:\n      logger.write(inp)\n    logger.close()\n\n    # Logger doesn't close the file; caller must do this manually.\n    self.assertFalse(file.closed)\n    file.close()\n\n    # Read back data.\n    outputs = []\n    with open(logger.file_path) as f:\n      csv_reader = csv.DictReader(f)\n      for row in csv_reader:\n        outputs.append(dict(row))\n    self.assertEqual(outputs, _TEST_INPUTS)\n\n  def test_flush(self):\n\n    logger = csv_logger.CSVLogger(self.get_tempdir(), flush_every=1)\n    for inp in _TEST_INPUTS:\n      logger.write(inp)\n\n    # Read back data.\n    outputs = []\n    with open(logger.file_path) as f:\n      csv_reader = csv.DictReader(f)\n      for row in csv_reader:\n        outputs.append(dict(row))\n    self.assertEqual(outputs, _TEST_INPUTS)",
  "def test_logging_input_is_directory(self):\n\n    # Set up logger.\n    directory = self.get_tempdir()\n    label = 'test'\n    logger = csv_logger.CSVLogger(directory_or_file=directory, label=label)\n\n    # Write data and close.\n    for inp in _TEST_INPUTS:\n      logger.write(inp)\n    logger.close()\n\n    # Read back data.\n    outputs = []\n    with open(logger.file_path) as f:\n      csv_reader = csv.DictReader(f)\n      for row in csv_reader:\n        outputs.append(dict(row))\n    self.assertEqual(outputs, _TEST_INPUTS)",
  "def test_logging_input_is_file(self, add_uid: bool):\n\n    # Set up logger.\n    directory = paths.process_path(\n        self.get_tempdir(), 'logs', 'my_label', add_uid=add_uid)\n    file = open(os.path.join(directory, 'logs.csv'), 'a')\n    logger = csv_logger.CSVLogger(directory_or_file=file, add_uid=add_uid)\n\n    # Write data and close.\n    for inp in _TEST_INPUTS:\n      logger.write(inp)\n    logger.close()\n\n    # Logger doesn't close the file; caller must do this manually.\n    self.assertFalse(file.closed)\n    file.close()\n\n    # Read back data.\n    outputs = []\n    with open(logger.file_path) as f:\n      csv_reader = csv.DictReader(f)\n      for row in csv_reader:\n        outputs.append(dict(row))\n    self.assertEqual(outputs, _TEST_INPUTS)",
  "def test_flush(self):\n\n    logger = csv_logger.CSVLogger(self.get_tempdir(), flush_every=1)\n    for inp in _TEST_INPUTS:\n      logger.write(inp)\n\n    # Read back data.\n    outputs = []\n    with open(logger.file_path) as f:\n      csv_reader = csv.DictReader(f)\n      for row in csv_reader:\n        outputs.append(dict(row))\n    self.assertEqual(outputs, _TEST_INPUTS)",
  "class NoneFilter(base.Logger):\n  \"\"\"Logger which writes to another logger, filtering any `None` values.\"\"\"\n\n  def __init__(self, to: base.Logger):\n    \"\"\"Initializes the logger.\n\n    Args:\n      to: A `Logger` object to which the current object will forward its results\n        when `write` is called.\n    \"\"\"\n    self._to = to\n\n  def write(self, values: base.LoggingData):\n    values = {k: v for k, v in values.items() if v is not None}\n    self._to.write(values)\n\n  def close(self):\n    self._to.close()",
  "class TimeFilter(base.Logger):\n  \"\"\"Logger which writes to another logger at a given time interval.\"\"\"\n\n  def __init__(self, to: base.Logger, time_delta: float):\n    \"\"\"Initializes the logger.\n\n    Args:\n      to: A `Logger` object to which the current object will forward its results\n        when `write` is called.\n      time_delta: How often to write values out in seconds.\n        Note that writes within `time_delta` are dropped.\n    \"\"\"\n    self._to = to\n    self._time = 0\n    self._time_delta = time_delta\n    if time_delta < 0:\n      raise ValueError(f'time_delta must be greater than 0 (got {time_delta}).')\n\n  def write(self, values: base.LoggingData):\n    now = time.time()\n    if (now - self._time) > self._time_delta:\n      self._to.write(values)\n      self._time = now\n\n  def close(self):\n    self._to.close()",
  "class KeyFilter(base.Logger):\n  \"\"\"Logger which filters keys in logged data.\"\"\"\n\n  def __init__(\n      self,\n      to: base.Logger,\n      *,\n      keep: Optional[Sequence[str]] = None,\n      drop: Optional[Sequence[str]] = None,\n  ):\n    \"\"\"Creates the filter.\n\n    Args:\n      to: A `Logger` object to which the current object will forward its writes.\n      keep: Keys that are kept by the filter. Note that `keep` and `drop` cannot\n        be both set at once.\n      drop: Keys that are dropped by the filter. Note that `keep` and `drop`\n        cannot be both set at once.\n    \"\"\"\n    if bool(keep) == bool(drop):\n      raise ValueError('Exactly one of `keep` & `drop` arguments must be set.')\n    self._to = to\n    self._keep = keep\n    self._drop = drop\n\n  def write(self, data: base.LoggingData):\n    if self._keep:\n      data = {k: data[k] for k in self._keep}\n    if self._drop:\n      data = {k: v for k, v in data.items() if k not in self._drop}\n    self._to.write(data)\n\n  def close(self):\n    self._to.close()",
  "class GatedFilter(base.Logger):\n  \"\"\"Logger which writes to another logger based on a gating function.\n\n  This logger tracks the number of times its `write` method is called, and uses\n  a gating function on this number to decide when to write.\n  \"\"\"\n\n  def __init__(self, to: base.Logger, gating_fn: Callable[[int], bool]):\n    \"\"\"Initialises the logger.\n\n    Args:\n      to: A `Logger` object to which the current object will forward its results\n        when `write` is called.\n      gating_fn: A function that takes an integer (number of calls) as input.\n        For example, to log every tenth call: gating_fn=lambda t: t % 10 == 0.\n    \"\"\"\n    self._to = to\n    self._gating_fn = gating_fn\n    self._calls = 0\n\n  def write(self, values: base.LoggingData):\n    if self._gating_fn(self._calls):\n      self._to.write(values)\n    self._calls += 1\n\n  def close(self):\n    self._to.close()\n\n  @classmethod\n  def logarithmic(cls, to: base.Logger, n: int = 10) -> 'GatedFilter':\n    \"\"\"Builds a logger for writing at logarithmically-spaced intervals.\n\n    This will log on a linear scale at each order of magnitude of `n`.\n    For example, with n=10, this will log at times:\n        [0, 1, 2, ..., 9, 10, 20, 30, ... 90, 100, 200, 300, ... 900, 1000]\n\n    Args:\n      to: The underlying logger to write to.\n      n: Base (default 10) on which to operate.\n    Returns:\n      A GatedFilter logger, which gates logarithmically as described above.\n    \"\"\"\n    def logarithmic_filter(t: int) -> bool:\n      magnitude = math.floor(math.log10(max(t, 1))/math.log10(n))\n      return t % (n**magnitude) == 0\n    return cls(to, gating_fn=logarithmic_filter)\n\n  @classmethod\n  def periodic(cls, to: base.Logger, interval: int = 10) -> 'GatedFilter':\n    \"\"\"Builds a logger for writing at linearly-spaced intervals.\n\n    Args:\n      to: The underlying logger to write to.\n      interval: The interval between writes.\n    Returns:\n      A GatedFilter logger, which gates periodically as described above.\n    \"\"\"\n    return cls(to, gating_fn=lambda t: t % interval == 0)",
  "def __init__(self, to: base.Logger):\n    \"\"\"Initializes the logger.\n\n    Args:\n      to: A `Logger` object to which the current object will forward its results\n        when `write` is called.\n    \"\"\"\n    self._to = to",
  "def write(self, values: base.LoggingData):\n    values = {k: v for k, v in values.items() if v is not None}\n    self._to.write(values)",
  "def close(self):\n    self._to.close()",
  "def __init__(self, to: base.Logger, time_delta: float):\n    \"\"\"Initializes the logger.\n\n    Args:\n      to: A `Logger` object to which the current object will forward its results\n        when `write` is called.\n      time_delta: How often to write values out in seconds.\n        Note that writes within `time_delta` are dropped.\n    \"\"\"\n    self._to = to\n    self._time = 0\n    self._time_delta = time_delta\n    if time_delta < 0:\n      raise ValueError(f'time_delta must be greater than 0 (got {time_delta}).')",
  "def write(self, values: base.LoggingData):\n    now = time.time()\n    if (now - self._time) > self._time_delta:\n      self._to.write(values)\n      self._time = now",
  "def close(self):\n    self._to.close()",
  "def __init__(\n      self,\n      to: base.Logger,\n      *,\n      keep: Optional[Sequence[str]] = None,\n      drop: Optional[Sequence[str]] = None,\n  ):\n    \"\"\"Creates the filter.\n\n    Args:\n      to: A `Logger` object to which the current object will forward its writes.\n      keep: Keys that are kept by the filter. Note that `keep` and `drop` cannot\n        be both set at once.\n      drop: Keys that are dropped by the filter. Note that `keep` and `drop`\n        cannot be both set at once.\n    \"\"\"\n    if bool(keep) == bool(drop):\n      raise ValueError('Exactly one of `keep` & `drop` arguments must be set.')\n    self._to = to\n    self._keep = keep\n    self._drop = drop",
  "def write(self, data: base.LoggingData):\n    if self._keep:\n      data = {k: data[k] for k in self._keep}\n    if self._drop:\n      data = {k: v for k, v in data.items() if k not in self._drop}\n    self._to.write(data)",
  "def close(self):\n    self._to.close()",
  "def __init__(self, to: base.Logger, gating_fn: Callable[[int], bool]):\n    \"\"\"Initialises the logger.\n\n    Args:\n      to: A `Logger` object to which the current object will forward its results\n        when `write` is called.\n      gating_fn: A function that takes an integer (number of calls) as input.\n        For example, to log every tenth call: gating_fn=lambda t: t % 10 == 0.\n    \"\"\"\n    self._to = to\n    self._gating_fn = gating_fn\n    self._calls = 0",
  "def write(self, values: base.LoggingData):\n    if self._gating_fn(self._calls):\n      self._to.write(values)\n    self._calls += 1",
  "def close(self):\n    self._to.close()",
  "def logarithmic(cls, to: base.Logger, n: int = 10) -> 'GatedFilter':\n    \"\"\"Builds a logger for writing at logarithmically-spaced intervals.\n\n    This will log on a linear scale at each order of magnitude of `n`.\n    For example, with n=10, this will log at times:\n        [0, 1, 2, ..., 9, 10, 20, 30, ... 90, 100, 200, 300, ... 900, 1000]\n\n    Args:\n      to: The underlying logger to write to.\n      n: Base (default 10) on which to operate.\n    Returns:\n      A GatedFilter logger, which gates logarithmically as described above.\n    \"\"\"\n    def logarithmic_filter(t: int) -> bool:\n      magnitude = math.floor(math.log10(max(t, 1))/math.log10(n))\n      return t % (n**magnitude) == 0\n    return cls(to, gating_fn=logarithmic_filter)",
  "def periodic(cls, to: base.Logger, interval: int = 10) -> 'GatedFilter':\n    \"\"\"Builds a logger for writing at linearly-spaced intervals.\n\n    Args:\n      to: The underlying logger to write to.\n      interval: The interval between writes.\n    Returns:\n      A GatedFilter logger, which gates periodically as described above.\n    \"\"\"\n    return cls(to, gating_fn=lambda t: t % interval == 0)",
  "def logarithmic_filter(t: int) -> bool:\n      magnitude = math.floor(math.log10(max(t, 1))/math.log10(n))\n      return t % (n**magnitude) == 0",
  "def _make_fake_env() -> dm_env.Environment:\n  env_spec = specs.EnvironmentSpec(\n      observations=specs.Array(shape=(10, 5), dtype=np.float32),\n      actions=specs.BoundedArray(\n          shape=(1,), dtype=np.float32, minimum=-100., maximum=100.),\n      rewards=specs.Array(shape=(), dtype=np.float32),\n      discounts=specs.BoundedArray(\n          shape=(), dtype=np.float32, minimum=0., maximum=1.),\n  )\n  return fakes.Environment(env_spec, episode_length=10)",
  "class ActionMetricsTest(absltest.TestCase):\n\n  def test_observe_nothing(self):\n    observer = action_metrics.ContinuousActionObserver()\n    self.assertEqual({}, observer.get_metrics())\n\n  def test_observe_first(self):\n    observer = action_metrics.ContinuousActionObserver()\n    observer.observe_first(env=_FAKE_ENV, timestep=_TIMESTEP)\n    self.assertEqual({}, observer.get_metrics())\n\n  def test_observe_single_step(self):\n    observer = action_metrics.ContinuousActionObserver()\n    observer.observe_first(env=_FAKE_ENV, timestep=_TIMESTEP)\n    observer.observe(env=_FAKE_ENV, timestep=_TIMESTEP, action=np.array([1]))\n    self.assertEqual(\n        {\n            'action[0]_max': 1,\n            'action[0]_min': 1,\n            'action[0]_mean': 1,\n            'action[0]_p50': 1,\n        },\n        observer.get_metrics(),\n    )\n\n  def test_observe_multiple_step(self):\n    observer = action_metrics.ContinuousActionObserver()\n    observer.observe_first(env=_FAKE_ENV, timestep=_TIMESTEP)\n    observer.observe(env=_FAKE_ENV, timestep=_TIMESTEP, action=np.array([1]))\n    observer.observe(env=_FAKE_ENV, timestep=_TIMESTEP, action=np.array([4]))\n    observer.observe(env=_FAKE_ENV, timestep=_TIMESTEP, action=np.array([5]))\n    self.assertEqual(\n        {\n            'action[0]_max': 5,\n            'action[0]_min': 1,\n            'action[0]_mean': 10 / 3,\n            'action[0]_p50': 4,\n        },\n        observer.get_metrics(),\n    )\n\n  def test_observe_zero_dimensions(self):\n    observer = action_metrics.ContinuousActionObserver()\n    observer.observe_first(env=_FAKE_ENV, timestep=_TIMESTEP)\n    observer.observe(env=_FAKE_ENV, timestep=_TIMESTEP, action=np.array(1))\n    self.assertEqual(\n        {\n            'action[]_max': 1,\n            'action[]_min': 1,\n            'action[]_mean': 1,\n            'action[]_p50': 1,\n        },\n        observer.get_metrics(),\n    )\n\n  def test_observe_multiple_dimensions(self):\n    observer = action_metrics.ContinuousActionObserver()\n    observer.observe_first(env=_FAKE_ENV, timestep=_TIMESTEP)\n    observer.observe(\n        env=_FAKE_ENV, timestep=_TIMESTEP, action=np.array([[1, 2], [3, 4]]))\n    np.testing.assert_equal(\n        {\n            'action[0, 0]_max': 1,\n            'action[0, 0]_min': 1,\n            'action[0, 0]_mean': 1,\n            'action[0, 0]_p50': 1,\n            'action[0, 1]_max': 2,\n            'action[0, 1]_min': 2,\n            'action[0, 1]_mean': 2,\n            'action[0, 1]_p50': 2,\n            'action[1, 0]_max': 3,\n            'action[1, 0]_min': 3,\n            'action[1, 0]_mean': 3,\n            'action[1, 0]_p50': 3,\n            'action[1, 1]_max': 4,\n            'action[1, 1]_min': 4,\n            'action[1, 1]_mean': 4,\n            'action[1, 1]_p50': 4,\n        },\n        observer.get_metrics(),\n    )",
  "def test_observe_nothing(self):\n    observer = action_metrics.ContinuousActionObserver()\n    self.assertEqual({}, observer.get_metrics())",
  "def test_observe_first(self):\n    observer = action_metrics.ContinuousActionObserver()\n    observer.observe_first(env=_FAKE_ENV, timestep=_TIMESTEP)\n    self.assertEqual({}, observer.get_metrics())",
  "def test_observe_single_step(self):\n    observer = action_metrics.ContinuousActionObserver()\n    observer.observe_first(env=_FAKE_ENV, timestep=_TIMESTEP)\n    observer.observe(env=_FAKE_ENV, timestep=_TIMESTEP, action=np.array([1]))\n    self.assertEqual(\n        {\n            'action[0]_max': 1,\n            'action[0]_min': 1,\n            'action[0]_mean': 1,\n            'action[0]_p50': 1,\n        },\n        observer.get_metrics(),\n    )",
  "def test_observe_multiple_step(self):\n    observer = action_metrics.ContinuousActionObserver()\n    observer.observe_first(env=_FAKE_ENV, timestep=_TIMESTEP)\n    observer.observe(env=_FAKE_ENV, timestep=_TIMESTEP, action=np.array([1]))\n    observer.observe(env=_FAKE_ENV, timestep=_TIMESTEP, action=np.array([4]))\n    observer.observe(env=_FAKE_ENV, timestep=_TIMESTEP, action=np.array([5]))\n    self.assertEqual(\n        {\n            'action[0]_max': 5,\n            'action[0]_min': 1,\n            'action[0]_mean': 10 / 3,\n            'action[0]_p50': 4,\n        },\n        observer.get_metrics(),\n    )",
  "def test_observe_zero_dimensions(self):\n    observer = action_metrics.ContinuousActionObserver()\n    observer.observe_first(env=_FAKE_ENV, timestep=_TIMESTEP)\n    observer.observe(env=_FAKE_ENV, timestep=_TIMESTEP, action=np.array(1))\n    self.assertEqual(\n        {\n            'action[]_max': 1,\n            'action[]_min': 1,\n            'action[]_mean': 1,\n            'action[]_p50': 1,\n        },\n        observer.get_metrics(),\n    )",
  "def test_observe_multiple_dimensions(self):\n    observer = action_metrics.ContinuousActionObserver()\n    observer.observe_first(env=_FAKE_ENV, timestep=_TIMESTEP)\n    observer.observe(\n        env=_FAKE_ENV, timestep=_TIMESTEP, action=np.array([[1, 2], [3, 4]]))\n    np.testing.assert_equal(\n        {\n            'action[0, 0]_max': 1,\n            'action[0, 0]_min': 1,\n            'action[0, 0]_mean': 1,\n            'action[0, 0]_p50': 1,\n            'action[0, 1]_max': 2,\n            'action[0, 1]_min': 2,\n            'action[0, 1]_mean': 2,\n            'action[0, 1]_p50': 2,\n            'action[1, 0]_max': 3,\n            'action[1, 0]_min': 3,\n            'action[1, 0]_mean': 3,\n            'action[1, 0]_p50': 3,\n            'action[1, 1]_max': 4,\n            'action[1, 1]_min': 4,\n            'action[1, 1]_mean': 4,\n            'action[1, 1]_p50': 4,\n        },\n        observer.get_metrics(),\n    )",
  "class EnvLoopObserver(abc.ABC):\n  \"\"\"An interface for collecting metrics/counters in EnvironmentLoop.\"\"\"\n\n  @abc.abstractmethod\n  def observe_first(self, env: dm_env.Environment, timestep: dm_env.TimeStep\n                    ) -> None:\n    \"\"\"Observes the initial state.\"\"\"\n\n  @abc.abstractmethod\n  def observe(self, env: dm_env.Environment, timestep: dm_env.TimeStep,\n              action: np.ndarray) -> None:\n    \"\"\"Records one environment step.\"\"\"\n\n  @abc.abstractmethod\n  def get_metrics(self) -> Dict[str, Number]:\n    \"\"\"Returns metrics collected for the current episode.\"\"\"",
  "def observe_first(self, env: dm_env.Environment, timestep: dm_env.TimeStep\n                    ) -> None:\n    \"\"\"Observes the initial state.\"\"\"",
  "def observe(self, env: dm_env.Environment, timestep: dm_env.TimeStep,\n              action: np.ndarray) -> None:\n    \"\"\"Records one environment step.\"\"\"",
  "def get_metrics(self) -> Dict[str, Number]:\n    \"\"\"Returns metrics collected for the current episode.\"\"\"",
  "class ActionNormObserver(base.EnvLoopObserver):\n  \"\"\"An observer that collects action norm stats.\"\"\"\n\n  def __init__(self):\n    self._action_norms = None\n\n  def observe_first(self, env: dm_env.Environment, timestep: dm_env.TimeStep\n                    ) -> None:\n    \"\"\"Observes the initial state.\"\"\"\n    self._action_norms = []\n\n  def observe(self, env: dm_env.Environment, timestep: dm_env.TimeStep,\n              action: np.ndarray) -> None:\n    \"\"\"Records one environment step.\"\"\"\n    self._action_norms.append(np.linalg.norm(action))\n\n  def get_metrics(self) -> Dict[str, base.Number]:\n    \"\"\"Returns metrics collected for the current episode.\"\"\"\n    return {'action_norm_avg': np.mean(self._action_norms),\n            'action_norm_min': np.min(self._action_norms),\n            'action_norm_max': np.max(self._action_norms)}",
  "def __init__(self):\n    self._action_norms = None",
  "def observe_first(self, env: dm_env.Environment, timestep: dm_env.TimeStep\n                    ) -> None:\n    \"\"\"Observes the initial state.\"\"\"\n    self._action_norms = []",
  "def observe(self, env: dm_env.Environment, timestep: dm_env.TimeStep,\n              action: np.ndarray) -> None:\n    \"\"\"Records one environment step.\"\"\"\n    self._action_norms.append(np.linalg.norm(action))",
  "def get_metrics(self) -> Dict[str, base.Number]:\n    \"\"\"Returns metrics collected for the current episode.\"\"\"\n    return {'action_norm_avg': np.mean(self._action_norms),\n            'action_norm_min': np.min(self._action_norms),\n            'action_norm_max': np.max(self._action_norms)}",
  "class ContinuousActionObserver(base.EnvLoopObserver):\n  \"\"\"Observer that tracks statstics of continuous actions taken by the agent.\n\n  Assumes the action is a np.ndarray, and for each dimension in the action,\n  calculates some useful statistics for a particular episode.\n  \"\"\"\n\n  def __init__(self):\n    self._actions = None\n\n  def observe_first(self, env: dm_env.Environment,\n                    timestep: dm_env.TimeStep) -> None:\n    \"\"\"Observes the initial state.\"\"\"\n    self._actions = []\n\n  def observe(self, env: dm_env.Environment, timestep: dm_env.TimeStep,\n              action: np.ndarray) -> None:\n    \"\"\"Records one environment step.\"\"\"\n    self._actions.append(action)\n\n  def get_metrics(self) -> Dict[str, base.Number]:\n    \"\"\"Returns metrics collected for the current episode.\"\"\"\n    aggregate_metrics = {}\n    if not self._actions:\n      return aggregate_metrics\n\n    metrics = {\n        'action_max': np.max(self._actions, axis=0),\n        'action_min': np.min(self._actions, axis=0),\n        'action_mean': np.mean(self._actions, axis=0),\n        'action_p50': np.percentile(self._actions, q=50., axis=0)\n    }\n\n    for index, sub_action_metric in np.ndenumerate(metrics['action_max']):\n      aggregate_metrics[f'action{list(index)}_max'] = sub_action_metric\n      aggregate_metrics[f'action{list(index)}_min'] = metrics['action_min'][\n          index]\n      aggregate_metrics[f'action{list(index)}_mean'] = metrics['action_mean'][\n          index]\n      aggregate_metrics[f'action{list(index)}_p50'] = metrics['action_p50'][\n          index]\n\n    return aggregate_metrics",
  "def __init__(self):\n    self._actions = None",
  "def observe_first(self, env: dm_env.Environment,\n                    timestep: dm_env.TimeStep) -> None:\n    \"\"\"Observes the initial state.\"\"\"\n    self._actions = []",
  "def observe(self, env: dm_env.Environment, timestep: dm_env.TimeStep,\n              action: np.ndarray) -> None:\n    \"\"\"Records one environment step.\"\"\"\n    self._actions.append(action)",
  "def get_metrics(self) -> Dict[str, base.Number]:\n    \"\"\"Returns metrics collected for the current episode.\"\"\"\n    aggregate_metrics = {}\n    if not self._actions:\n      return aggregate_metrics\n\n    metrics = {\n        'action_max': np.max(self._actions, axis=0),\n        'action_min': np.min(self._actions, axis=0),\n        'action_mean': np.mean(self._actions, axis=0),\n        'action_p50': np.percentile(self._actions, q=50., axis=0)\n    }\n\n    for index, sub_action_metric in np.ndenumerate(metrics['action_max']):\n      aggregate_metrics[f'action{list(index)}_max'] = sub_action_metric\n      aggregate_metrics[f'action{list(index)}_min'] = metrics['action_min'][\n          index]\n      aggregate_metrics[f'action{list(index)}_mean'] = metrics['action_mean'][\n          index]\n      aggregate_metrics[f'action{list(index)}_p50'] = metrics['action_p50'][\n          index]\n\n    return aggregate_metrics",
  "def _make_fake_env() -> dm_env.Environment:\n  env_spec = specs.EnvironmentSpec(\n      observations=specs.Array(shape=(10, 5), dtype=np.float32),\n      actions=specs.BoundedArray(\n          shape=(1,), dtype=np.float32, minimum=-10., maximum=10.),\n      rewards=specs.Array(shape=(), dtype=np.float32),\n      discounts=specs.BoundedArray(\n          shape=(), dtype=np.float32, minimum=0., maximum=1.),\n  )\n  return fakes.Environment(env_spec, episode_length=10)",
  "class ActionNormTest(absltest.TestCase):\n\n  def test_basic(self):\n    env = _make_fake_env()\n    observer = action_norm.ActionNormObserver()\n    timestep = env.reset()\n    observer.observe_first(env, timestep)\n    for it in range(5):\n      action = np.ones((1,), dtype=np.float32) * it\n      timestep = env.step(action)\n      observer.observe(env, timestep, action)\n    metrics = observer.get_metrics()\n    self.assertLen(metrics, 3)\n    np.testing.assert_equal(metrics['action_norm_min'], 0)\n    np.testing.assert_equal(metrics['action_norm_max'], 4)\n    np.testing.assert_equal(metrics['action_norm_avg'], 2)",
  "def test_basic(self):\n    env = _make_fake_env()\n    observer = action_norm.ActionNormObserver()\n    timestep = env.reset()\n    observer.observe_first(env, timestep)\n    for it in range(5):\n      action = np.ones((1,), dtype=np.float32) * it\n      timestep = env.step(action)\n      observer.observe(env, timestep, action)\n    metrics = observer.get_metrics()\n    self.assertLen(metrics, 3)\n    np.testing.assert_equal(metrics['action_norm_min'], 0)\n    np.testing.assert_equal(metrics['action_norm_max'], 4)\n    np.testing.assert_equal(metrics['action_norm_avg'], 2)",
  "def _make_fake_env() -> dm_env.Environment:\n  env_spec = specs.EnvironmentSpec(\n      observations=specs.Array(shape=(10, 5), dtype=np.float32),\n      actions=specs.BoundedArray(\n          shape=(1,), dtype=np.float32, minimum=-100., maximum=100.),\n      rewards=specs.Array(shape=(), dtype=np.float32),\n      discounts=specs.BoundedArray(\n          shape=(), dtype=np.float32, minimum=0., maximum=1.),\n  )\n  return fakes.Environment(env_spec, episode_length=10)",
  "class MeasurementMetricsTest(absltest.TestCase):\n\n  def test_observe_nothing(self):\n    observer = measurement_metrics.MeasurementObserver()\n    self.assertEqual({}, observer.get_metrics())\n\n  def test_observe_first(self):\n    observer = measurement_metrics.MeasurementObserver()\n    observer.observe_first(env=_FAKE_ENV, timestep=_TIMESTEP)\n    self.assertEqual({}, observer.get_metrics())\n\n  def test_observe_single_step(self):\n    observer = measurement_metrics.MeasurementObserver()\n    observer.observe_first(env=_FAKE_ENV, timestep=_TIMESTEP)\n    observer.observe(env=_FAKE_ENV, timestep=_TIMESTEP, action=np.array([1]))\n    self.assertEqual(\n        {\n            'measurement[0]_max': 1.0,\n            'measurement[0]_mean': 1.0,\n            'measurement[0]_p25': 1.0,\n            'measurement[0]_p50': 1.0,\n            'measurement[0]_p75': 1.0,\n            'measurement[1]_max': -2.0,\n            'measurement[1]_mean': -2.0,\n            'measurement[1]_p25': -2.0,\n            'measurement[1]_p50': -2.0,\n            'measurement[1]_p75': -2.0,\n            'measurement[0]_min': 1.0,\n            'measurement[1]_min': -2.0,\n        },\n        observer.get_metrics(),\n    )\n\n  def test_observe_multiple_step_same_observation(self):\n    observer = measurement_metrics.MeasurementObserver()\n    observer.observe_first(env=_FAKE_ENV, timestep=_TIMESTEP)\n    observer.observe(env=_FAKE_ENV, timestep=_TIMESTEP, action=np.array([1]))\n    observer.observe(env=_FAKE_ENV, timestep=_TIMESTEP, action=np.array([4]))\n    observer.observe(env=_FAKE_ENV, timestep=_TIMESTEP, action=np.array([5]))\n    self.assertEqual(\n        {\n            'measurement[0]_max': 1.0,\n            'measurement[0]_mean': 1.0,\n            'measurement[0]_p25': 1.0,\n            'measurement[0]_p50': 1.0,\n            'measurement[0]_p75': 1.0,\n            'measurement[1]_max': -2.0,\n            'measurement[1]_mean': -2.0,\n            'measurement[1]_p25': -2.0,\n            'measurement[1]_p50': -2.0,\n            'measurement[1]_p75': -2.0,\n            'measurement[0]_min': 1.0,\n            'measurement[1]_min': -2.0,\n        },\n        observer.get_metrics(),\n    )\n\n  def test_observe_multiple_step(self):\n    observer = measurement_metrics.MeasurementObserver()\n    observer.observe_first(env=_FAKE_ENV, timestep=_TIMESTEP)\n    observer.observe(env=_FAKE_ENV, timestep=_TIMESTEP, action=np.array([1]))\n    first_obs_timestep = copy.deepcopy(_TIMESTEP)\n    first_obs_timestep.observation = [1000.0, -50.0]\n    observer.observe(\n        env=_FAKE_ENV, timestep=first_obs_timestep, action=np.array([4]))\n    second_obs_timestep = copy.deepcopy(_TIMESTEP)\n    second_obs_timestep.observation = [-1000.0, 500.0]\n    observer.observe(\n        env=_FAKE_ENV, timestep=second_obs_timestep, action=np.array([4]))\n    self.assertEqual(\n        {\n            'measurement[0]_max': 1000.0,\n            'measurement[0]_mean': 1.0/3,\n            'measurement[0]_p25': -499.5,\n            'measurement[0]_p50': 1.0,\n            'measurement[0]_p75': 500.5,\n            'measurement[1]_max': 500.0,\n            'measurement[1]_mean': 448.0/3.0,\n            'measurement[1]_p25': -26.0,\n            'measurement[1]_p50': -2.0,\n            'measurement[1]_p75': 249.0,\n            'measurement[0]_min': -1000.0,\n            'measurement[1]_min': -50.0,\n        },\n        observer.get_metrics(),\n    )\n\n  def test_observe_empty_observation(self):\n    observer = measurement_metrics.MeasurementObserver()\n    empty_timestep = copy.deepcopy(_TIMESTEP)\n    empty_timestep.observation = {}\n    observer.observe_first(env=_FAKE_ENV, timestep=empty_timestep)\n    self.assertEqual({}, observer.get_metrics())\n\n  def test_observe_single_dimensions(self):\n    observer = measurement_metrics.MeasurementObserver()\n    observer.observe_first(env=_FAKE_ENV, timestep=_TIMESTEP)\n    single_obs_timestep = copy.deepcopy(_TIMESTEP)\n    single_obs_timestep.observation = [1000.0, -50.0]\n\n    observer.observe(\n        env=_FAKE_ENV,\n        timestep=single_obs_timestep,\n        action=np.array([[1, 2], [3, 4]]))\n\n    np.testing.assert_equal(\n        {\n            'measurement[0]_max': 1000.0,\n            'measurement[0]_min': 1000.0,\n            'measurement[0]_mean': 1000.0,\n            'measurement[0]_p25': 1000.0,\n            'measurement[0]_p50': 1000.0,\n            'measurement[0]_p75': 1000.0,\n            'measurement[1]_max': -50.0,\n            'measurement[1]_mean': -50.0,\n            'measurement[1]_p25': -50.0,\n            'measurement[1]_p50': -50.0,\n            'measurement[1]_p75': -50.0,\n            'measurement[1]_min': -50.0,\n        },\n        observer.get_metrics(),\n    )",
  "def test_observe_nothing(self):\n    observer = measurement_metrics.MeasurementObserver()\n    self.assertEqual({}, observer.get_metrics())",
  "def test_observe_first(self):\n    observer = measurement_metrics.MeasurementObserver()\n    observer.observe_first(env=_FAKE_ENV, timestep=_TIMESTEP)\n    self.assertEqual({}, observer.get_metrics())",
  "def test_observe_single_step(self):\n    observer = measurement_metrics.MeasurementObserver()\n    observer.observe_first(env=_FAKE_ENV, timestep=_TIMESTEP)\n    observer.observe(env=_FAKE_ENV, timestep=_TIMESTEP, action=np.array([1]))\n    self.assertEqual(\n        {\n            'measurement[0]_max': 1.0,\n            'measurement[0]_mean': 1.0,\n            'measurement[0]_p25': 1.0,\n            'measurement[0]_p50': 1.0,\n            'measurement[0]_p75': 1.0,\n            'measurement[1]_max': -2.0,\n            'measurement[1]_mean': -2.0,\n            'measurement[1]_p25': -2.0,\n            'measurement[1]_p50': -2.0,\n            'measurement[1]_p75': -2.0,\n            'measurement[0]_min': 1.0,\n            'measurement[1]_min': -2.0,\n        },\n        observer.get_metrics(),\n    )",
  "def test_observe_multiple_step_same_observation(self):\n    observer = measurement_metrics.MeasurementObserver()\n    observer.observe_first(env=_FAKE_ENV, timestep=_TIMESTEP)\n    observer.observe(env=_FAKE_ENV, timestep=_TIMESTEP, action=np.array([1]))\n    observer.observe(env=_FAKE_ENV, timestep=_TIMESTEP, action=np.array([4]))\n    observer.observe(env=_FAKE_ENV, timestep=_TIMESTEP, action=np.array([5]))\n    self.assertEqual(\n        {\n            'measurement[0]_max': 1.0,\n            'measurement[0]_mean': 1.0,\n            'measurement[0]_p25': 1.0,\n            'measurement[0]_p50': 1.0,\n            'measurement[0]_p75': 1.0,\n            'measurement[1]_max': -2.0,\n            'measurement[1]_mean': -2.0,\n            'measurement[1]_p25': -2.0,\n            'measurement[1]_p50': -2.0,\n            'measurement[1]_p75': -2.0,\n            'measurement[0]_min': 1.0,\n            'measurement[1]_min': -2.0,\n        },\n        observer.get_metrics(),\n    )",
  "def test_observe_multiple_step(self):\n    observer = measurement_metrics.MeasurementObserver()\n    observer.observe_first(env=_FAKE_ENV, timestep=_TIMESTEP)\n    observer.observe(env=_FAKE_ENV, timestep=_TIMESTEP, action=np.array([1]))\n    first_obs_timestep = copy.deepcopy(_TIMESTEP)\n    first_obs_timestep.observation = [1000.0, -50.0]\n    observer.observe(\n        env=_FAKE_ENV, timestep=first_obs_timestep, action=np.array([4]))\n    second_obs_timestep = copy.deepcopy(_TIMESTEP)\n    second_obs_timestep.observation = [-1000.0, 500.0]\n    observer.observe(\n        env=_FAKE_ENV, timestep=second_obs_timestep, action=np.array([4]))\n    self.assertEqual(\n        {\n            'measurement[0]_max': 1000.0,\n            'measurement[0]_mean': 1.0/3,\n            'measurement[0]_p25': -499.5,\n            'measurement[0]_p50': 1.0,\n            'measurement[0]_p75': 500.5,\n            'measurement[1]_max': 500.0,\n            'measurement[1]_mean': 448.0/3.0,\n            'measurement[1]_p25': -26.0,\n            'measurement[1]_p50': -2.0,\n            'measurement[1]_p75': 249.0,\n            'measurement[0]_min': -1000.0,\n            'measurement[1]_min': -50.0,\n        },\n        observer.get_metrics(),\n    )",
  "def test_observe_empty_observation(self):\n    observer = measurement_metrics.MeasurementObserver()\n    empty_timestep = copy.deepcopy(_TIMESTEP)\n    empty_timestep.observation = {}\n    observer.observe_first(env=_FAKE_ENV, timestep=empty_timestep)\n    self.assertEqual({}, observer.get_metrics())",
  "def test_observe_single_dimensions(self):\n    observer = measurement_metrics.MeasurementObserver()\n    observer.observe_first(env=_FAKE_ENV, timestep=_TIMESTEP)\n    single_obs_timestep = copy.deepcopy(_TIMESTEP)\n    single_obs_timestep.observation = [1000.0, -50.0]\n\n    observer.observe(\n        env=_FAKE_ENV,\n        timestep=single_obs_timestep,\n        action=np.array([[1, 2], [3, 4]]))\n\n    np.testing.assert_equal(\n        {\n            'measurement[0]_max': 1000.0,\n            'measurement[0]_min': 1000.0,\n            'measurement[0]_mean': 1000.0,\n            'measurement[0]_p25': 1000.0,\n            'measurement[0]_p50': 1000.0,\n            'measurement[0]_p75': 1000.0,\n            'measurement[1]_max': -50.0,\n            'measurement[1]_mean': -50.0,\n            'measurement[1]_p25': -50.0,\n            'measurement[1]_p50': -50.0,\n            'measurement[1]_p75': -50.0,\n            'measurement[1]_min': -50.0,\n        },\n        observer.get_metrics(),\n    )",
  "class EnvInfoObserver(base.EnvLoopObserver):\n  \"\"\"An observer that collects and accumulates scalars from env's info.\"\"\"\n\n  def __init__(self):\n    self._metrics = None\n\n  def _accumulate_metrics(self, env: dm_env.Environment) -> None:\n    if not hasattr(env, 'get_info'):\n      return\n    info = getattr(env, 'get_info')()\n    if not info:\n      return\n    for k, v in info.items():\n      if np.isscalar(v):\n        self._metrics[k] = self._metrics.get(k, 0) + v\n\n  def observe_first(self, env: dm_env.Environment, timestep: dm_env.TimeStep\n                    ) -> None:\n    \"\"\"Observes the initial state.\"\"\"\n    self._metrics = {}\n    self._accumulate_metrics(env)\n\n  def observe(self, env: dm_env.Environment, timestep: dm_env.TimeStep,\n              action: np.ndarray) -> None:\n    \"\"\"Records one environment step.\"\"\"\n    self._accumulate_metrics(env)\n\n  def get_metrics(self) -> Dict[str, base.Number]:\n    \"\"\"Returns metrics collected for the current episode.\"\"\"\n    return self._metrics",
  "def __init__(self):\n    self._metrics = None",
  "def _accumulate_metrics(self, env: dm_env.Environment) -> None:\n    if not hasattr(env, 'get_info'):\n      return\n    info = getattr(env, 'get_info')()\n    if not info:\n      return\n    for k, v in info.items():\n      if np.isscalar(v):\n        self._metrics[k] = self._metrics.get(k, 0) + v",
  "def observe_first(self, env: dm_env.Environment, timestep: dm_env.TimeStep\n                    ) -> None:\n    \"\"\"Observes the initial state.\"\"\"\n    self._metrics = {}\n    self._accumulate_metrics(env)",
  "def observe(self, env: dm_env.Environment, timestep: dm_env.TimeStep,\n              action: np.ndarray) -> None:\n    \"\"\"Records one environment step.\"\"\"\n    self._accumulate_metrics(env)",
  "def get_metrics(self) -> Dict[str, base.Number]:\n    \"\"\"Returns metrics collected for the current episode.\"\"\"\n    return self._metrics",
  "class MeasurementObserver(base.EnvLoopObserver):\n  \"\"\"Observer the provides statistics for measurements at every timestep.\n\n  This assumes the measurements is a multidimensional array with a static spec.\n  Warning! It is not intended to be used for high dimensional observations.\n\n  self._measurements: List[np.ndarray]\n  \"\"\"\n\n  def __init__(self):\n    self._measurements = []\n\n  def observe_first(self, env: dm_env.Environment,\n                    timestep: dm_env.TimeStep) -> None:\n    \"\"\"Observes the initial state.\"\"\"\n    self._measurements = []\n\n  def observe(self, env: dm_env.Environment, timestep: dm_env.TimeStep,\n              action: np.ndarray) -> None:\n    \"\"\"Records one environment step.\"\"\"\n    self._measurements.append(timestep.observation)\n\n  def get_metrics(self) -> Mapping[str, List[base.Number]]:  # pytype: disable=signature-mismatch  # overriding-return-type-checks\n    \"\"\"Returns metrics collected for the current episode.\"\"\"\n    aggregate_metrics = {}\n    if not self._measurements:\n      return aggregate_metrics\n\n    metrics = {\n        'measurement_max': np.max(self._measurements, axis=0),\n        'measurement_min': np.min(self._measurements, axis=0),\n        'measurement_mean': np.mean(self._measurements, axis=0),\n        'measurement_p25': np.percentile(self._measurements, q=25., axis=0),\n        'measurement_p50': np.percentile(self._measurements, q=50., axis=0),\n        'measurement_p75': np.percentile(self._measurements, q=75., axis=0),\n    }\n    for index, sub_observation_metric in np.ndenumerate(\n        metrics['measurement_max']):\n      aggregate_metrics[\n          f'measurement{list(index)}_max'] = sub_observation_metric\n      aggregate_metrics[f'measurement{list(index)}_min'] = metrics[\n          'measurement_min'][index]\n      aggregate_metrics[f'measurement{list(index)}_mean'] = metrics[\n          'measurement_mean'][index]\n      aggregate_metrics[f'measurement{list(index)}_p50'] = metrics[\n          'measurement_p50'][index]\n      aggregate_metrics[f'measurement{list(index)}_p25'] = metrics[\n          'measurement_p25'][index]\n      aggregate_metrics[f'measurement{list(index)}_p75'] = metrics[\n          'measurement_p75'][index]\n    return aggregate_metrics",
  "def __init__(self):\n    self._measurements = []",
  "def observe_first(self, env: dm_env.Environment,\n                    timestep: dm_env.TimeStep) -> None:\n    \"\"\"Observes the initial state.\"\"\"\n    self._measurements = []",
  "def observe(self, env: dm_env.Environment, timestep: dm_env.TimeStep,\n              action: np.ndarray) -> None:\n    \"\"\"Records one environment step.\"\"\"\n    self._measurements.append(timestep.observation)",
  "def get_metrics(self) -> Mapping[str, List[base.Number]]:  # pytype: disable=signature-mismatch  # overriding-return-type-checks\n    \"\"\"Returns metrics collected for the current episode.\"\"\"\n    aggregate_metrics = {}\n    if not self._measurements:\n      return aggregate_metrics\n\n    metrics = {\n        'measurement_max': np.max(self._measurements, axis=0),\n        'measurement_min': np.min(self._measurements, axis=0),\n        'measurement_mean': np.mean(self._measurements, axis=0),\n        'measurement_p25': np.percentile(self._measurements, q=25., axis=0),\n        'measurement_p50': np.percentile(self._measurements, q=50., axis=0),\n        'measurement_p75': np.percentile(self._measurements, q=75., axis=0),\n    }\n    for index, sub_observation_metric in np.ndenumerate(\n        metrics['measurement_max']):\n      aggregate_metrics[\n          f'measurement{list(index)}_max'] = sub_observation_metric\n      aggregate_metrics[f'measurement{list(index)}_min'] = metrics[\n          'measurement_min'][index]\n      aggregate_metrics[f'measurement{list(index)}_mean'] = metrics[\n          'measurement_mean'][index]\n      aggregate_metrics[f'measurement{list(index)}_p50'] = metrics[\n          'measurement_p50'][index]\n      aggregate_metrics[f'measurement{list(index)}_p25'] = metrics[\n          'measurement_p25'][index]\n      aggregate_metrics[f'measurement{list(index)}_p75'] = metrics[\n          'measurement_p75'][index]\n    return aggregate_metrics",
  "class GymEnvWithInfo(gym.Env):\n\n  def __init__(self):\n    obs_space = np.ones((10,))\n    self.observation_space = spaces.Box(-obs_space, obs_space, dtype=np.float32)\n    act_space = np.ones((3,))\n    self.action_space = spaces.Box(-act_space, act_space, dtype=np.float32)\n    self._step = 0\n\n  def reset(self):\n    self._step = 0\n    return self.observation_space.sample()\n\n  def step(self, action: np.ndarray):\n    self._step += 1\n    info = {'survival_bonus': 1}\n    if self._step == 1 or self._step == 7:\n      info['found_checkpoint'] = 1\n    if self._step == 5:\n      info['picked_up_an_apple'] = 1\n    return self.observation_space.sample(), 0, False, info",
  "class ActionNormTest(absltest.TestCase):\n\n  def test_basic(self):\n    env = GymEnvWithInfo()\n    env = gym_wrapper.GymWrapper(env)\n    observer = env_info.EnvInfoObserver()\n    timestep = env.reset()\n    observer.observe_first(env, timestep)\n    for _ in range(20):\n      action = np.zeros((3,))\n      timestep = env.step(action)\n      observer.observe(env, timestep, action)\n    metrics = observer.get_metrics()\n    self.assertLen(metrics, 3)\n    np.testing.assert_equal(metrics['found_checkpoint'], 2)\n    np.testing.assert_equal(metrics['picked_up_an_apple'], 1)\n    np.testing.assert_equal(metrics['survival_bonus'], 20)",
  "def __init__(self):\n    obs_space = np.ones((10,))\n    self.observation_space = spaces.Box(-obs_space, obs_space, dtype=np.float32)\n    act_space = np.ones((3,))\n    self.action_space = spaces.Box(-act_space, act_space, dtype=np.float32)\n    self._step = 0",
  "def reset(self):\n    self._step = 0\n    return self.observation_space.sample()",
  "def step(self, action: np.ndarray):\n    self._step += 1\n    info = {'survival_bonus': 1}\n    if self._step == 1 or self._step == 7:\n      info['found_checkpoint'] = 1\n    if self._step == 5:\n      info['picked_up_an_apple'] = 1\n    return self.observation_space.sample(), 0, False, info",
  "def test_basic(self):\n    env = GymEnvWithInfo()\n    env = gym_wrapper.GymWrapper(env)\n    observer = env_info.EnvInfoObserver()\n    timestep = env.reset()\n    observer.observe_first(env, timestep)\n    for _ in range(20):\n      action = np.zeros((3,))\n      timestep = env.step(action)\n      observer.observe(env, timestep, action)\n    metrics = observer.get_metrics()\n    self.assertLen(metrics, 3)\n    np.testing.assert_equal(metrics['found_checkpoint'], 2)\n    np.testing.assert_equal(metrics['picked_up_an_apple'], 1)\n    np.testing.assert_equal(metrics['survival_bonus'], 20)"
]