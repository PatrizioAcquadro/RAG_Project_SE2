[
  "def read(rel_path):\n    here = os.path.abspath(os.path.dirname(__file__))\n    with codecs.open(os.path.join(here, rel_path), 'r') as fp:\n        return fp.read()",
  "def get_version(rel_path):\n    for line in read(rel_path).splitlines():\n        if line.startswith('__version__'):\n            delim = '\"' if '\"' in line else \"'\"\n            return line.split(delim)[1]\n    else:\n        raise RuntimeError(\"Unable to find version string.\")",
  "class StrategyCallbacks(Generic[CallbackResult], ABC):\n    \"\"\"\n    Strategy callbacks provide access before/after each phase of the training\n    and evaluation loops. Subclasses can override the desired callbacks to\n    customize the loops. In Avalanche, callbacks are used by\n    :class:`StrategyPlugin` to implement continual strategies, and\n    :class:`StrategyLogger` for automatic logging.\n\n    For each method of the training and evaluation loops, `StrategyCallbacks`\n    provide two functions `before_{method}` and `after_{method}`, called\n    before and after the method, respectively.\n\n    As a reminder, `BaseStrategy` loops follow the structure shown below:\n\n    **Training loop**\n    The training loop is organized as follows::\n        train\n            train_exp  # for each experience\n                adapt_train_dataset\n                train_dataset_adaptation\n                make_train_dataloader\n                train_epoch  # for each epoch\n                    # forward\n                    # backward\n                    # model update\n\n    **Evaluation loop**\n    The evaluation loop is organized as follows::\n        eval\n            eval_exp  # for each experience\n                adapt_eval_dataset\n                eval_dataset_adaptation\n                make_eval_dataloader\n                eval_epoch  # for each epoch\n                    # forward\n                    # backward\n                    # model update\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    def before_training(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called before `train` by the `BaseStrategy`. \"\"\"\n        pass\n\n    def before_training_exp(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called before `train_exp` by the `BaseStrategy`. \"\"\"\n        pass\n\n    def before_train_dataset_adaptation(self, *args,\n                                        **kwargs) -> CallbackResult:\n        \"\"\" Called before `train_dataset_adapatation` by the `BaseStrategy`. \"\"\"\n        pass\n\n    def after_train_dataset_adaptation(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called after `train_dataset_adapatation` by the `BaseStrategy`. \"\"\"\n        pass\n\n    def before_training_epoch(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called before `train_epoch` by the `BaseStrategy`. \"\"\"\n        pass\n\n    def before_training_iteration(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called before the start of a training iteration by the\n        `BaseStrategy`. \"\"\"\n        pass\n\n    def before_forward(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called before `model.forward()` by the `BaseStrategy`. \"\"\"\n        pass\n\n    def after_forward(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called after `model.forward()` by the `BaseStrategy`. \"\"\"\n        pass\n\n    def before_backward(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called before `criterion.backward()` by the `BaseStrategy`. \"\"\"\n        pass\n\n    def after_backward(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called after `criterion.backward()` by the `BaseStrategy`. \"\"\"\n        pass\n\n    def after_training_iteration(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called after the end of a training iteration by the\n        `BaseStrategy`. \"\"\"\n        pass\n\n    def before_update(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called before `optimizer.update()` by the `BaseStrategy`. \"\"\"\n        pass\n\n    def after_update(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called after `optimizer.update()` by the `BaseStrategy`. \"\"\"\n        pass\n\n    def after_training_epoch(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called after `train_epoch` by the `BaseStrategy`. \"\"\"\n        pass\n\n    def after_training_exp(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called after `train_exp` by the `BaseStrategy`. \"\"\"\n        pass\n\n    def after_training(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called after `train` by the `BaseStrategy`. \"\"\"\n        pass\n\n    def before_eval(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called before `eval` by the `BaseStrategy`. \"\"\"\n        pass\n\n    def before_eval_dataset_adaptation(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called before `eval_dataset_adaptation` by the `BaseStrategy`. \"\"\"\n        pass\n\n    def after_eval_dataset_adaptation(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called after `eval_dataset_adaptation` by the `BaseStrategy`. \"\"\"\n        pass\n\n    def before_eval_exp(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called before `eval_exp` by the `BaseStrategy`. \"\"\"\n        pass\n\n    def after_eval_exp(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called after `eval_exp` by the `BaseStrategy`. \"\"\"\n        pass\n\n    def after_eval(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called after `eval` by the `BaseStrategy`. \"\"\"\n        pass\n\n    def before_eval_iteration(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called before the start of a training iteration by the\n        `BaseStrategy`. \"\"\"\n        pass\n\n    def before_eval_forward(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called before `model.forward()` by the `BaseStrategy`. \"\"\"\n        pass\n\n    def after_eval_forward(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called after `model.forward()` by the `BaseStrategy`. \"\"\"\n        pass\n\n    def after_eval_iteration(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called after the end of an iteration by the\n        `BaseStrategy`. \"\"\"\n        pass",
  "def __init__(self):\n        pass",
  "def before_training(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called before `train` by the `BaseStrategy`. \"\"\"\n        pass",
  "def before_training_exp(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called before `train_exp` by the `BaseStrategy`. \"\"\"\n        pass",
  "def before_train_dataset_adaptation(self, *args,\n                                        **kwargs) -> CallbackResult:\n        \"\"\" Called before `train_dataset_adapatation` by the `BaseStrategy`. \"\"\"\n        pass",
  "def after_train_dataset_adaptation(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called after `train_dataset_adapatation` by the `BaseStrategy`. \"\"\"\n        pass",
  "def before_training_epoch(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called before `train_epoch` by the `BaseStrategy`. \"\"\"\n        pass",
  "def before_training_iteration(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called before the start of a training iteration by the\n        `BaseStrategy`. \"\"\"\n        pass",
  "def before_forward(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called before `model.forward()` by the `BaseStrategy`. \"\"\"\n        pass",
  "def after_forward(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called after `model.forward()` by the `BaseStrategy`. \"\"\"\n        pass",
  "def before_backward(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called before `criterion.backward()` by the `BaseStrategy`. \"\"\"\n        pass",
  "def after_backward(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called after `criterion.backward()` by the `BaseStrategy`. \"\"\"\n        pass",
  "def after_training_iteration(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called after the end of a training iteration by the\n        `BaseStrategy`. \"\"\"\n        pass",
  "def before_update(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called before `optimizer.update()` by the `BaseStrategy`. \"\"\"\n        pass",
  "def after_update(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called after `optimizer.update()` by the `BaseStrategy`. \"\"\"\n        pass",
  "def after_training_epoch(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called after `train_epoch` by the `BaseStrategy`. \"\"\"\n        pass",
  "def after_training_exp(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called after `train_exp` by the `BaseStrategy`. \"\"\"\n        pass",
  "def after_training(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called after `train` by the `BaseStrategy`. \"\"\"\n        pass",
  "def before_eval(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called before `eval` by the `BaseStrategy`. \"\"\"\n        pass",
  "def before_eval_dataset_adaptation(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called before `eval_dataset_adaptation` by the `BaseStrategy`. \"\"\"\n        pass",
  "def after_eval_dataset_adaptation(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called after `eval_dataset_adaptation` by the `BaseStrategy`. \"\"\"\n        pass",
  "def before_eval_exp(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called before `eval_exp` by the `BaseStrategy`. \"\"\"\n        pass",
  "def after_eval_exp(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called after `eval_exp` by the `BaseStrategy`. \"\"\"\n        pass",
  "def after_eval(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called after `eval` by the `BaseStrategy`. \"\"\"\n        pass",
  "def before_eval_iteration(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called before the start of a training iteration by the\n        `BaseStrategy`. \"\"\"\n        pass",
  "def before_eval_forward(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called before `model.forward()` by the `BaseStrategy`. \"\"\"\n        pass",
  "def after_eval_forward(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called after `model.forward()` by the `BaseStrategy`. \"\"\"\n        pass",
  "def after_eval_iteration(self, *args, **kwargs) -> CallbackResult:\n        \"\"\" Called after the end of an iteration by the\n        `BaseStrategy`. \"\"\"\n        pass",
  "def _avdataset_radd(self, other, *args, **kwargs):\n    from avalanche.benchmarks.utils import AvalancheDataset\n    global _dataset_add\n    if isinstance(other, AvalancheDataset):\n        return NotImplemented\n\n    return _dataset_add(self, other, *args, **kwargs)",
  "def _avalanche_monkey_patches():\n    from torch.utils.data.dataset import Dataset\n    global _dataset_add\n    _dataset_add = Dataset.__add__\n    Dataset.__add__ = _avdataset_radd",
  "def train_eval_transforms(dataset_train, dataset_test):\n    \"\"\"\n    Internal utility used to create the transform groups from a couple of\n    train and test datasets.\n\n    :param dataset_train: The training dataset.\n    :param dataset_test: The test dataset.\n    :return: The transformations groups.\n    \"\"\"\n\n    if isinstance(dataset_train, AvalancheDataset):\n        train_group = dataset_train.get_transforms('train')\n    else:\n        train_group = (\n            getattr(dataset_train, 'transform', None),\n            getattr(dataset_train, 'target_transform', None)\n        )\n\n    if isinstance(dataset_test, AvalancheDataset):\n        eval_group = dataset_test.get_transforms('eval')\n    else:\n        eval_group = (\n            getattr(dataset_test, 'transform', None),\n            getattr(dataset_test, 'target_transform', None)\n        )\n\n    return dict(\n        train=train_group,\n        eval=eval_group\n    )",
  "class StreamUserDef(NamedTuple):\n    exps_data: TStreamDataOrigin\n    exps_task_labels: TStreamTaskLabels = None\n    origin_dataset: TOriginDataset = None\n    is_lazy: Optional[bool] = None",
  "class StreamDef(NamedTuple):\n    exps_data: LazyDatasetSequence\n    exps_task_labels: Sequence[Set[int]]\n    origin_dataset: TOriginDataset\n    is_lazy: bool",
  "class GenericCLScenario(Generic[TExperience]):\n    \"\"\"\n    Base implementation of a Continual Learning benchmark instance.\n    A Continual Learning benchmark instance is defined by a set of streams of\n    experiences (batches or tasks depending on the terminology). Each experience\n    contains the training (or test, or validation, ...) data that becomes\n    available at a certain time instant.\n\n    Experiences are usually defined in children classes, with this class serving\n    as the more general implementation. This class handles the most simple type\n    of assignment: each stream is defined as a list of experiences, each\n    experience is defined by a dataset.\n\n    Defining the \"train\" and \"test\" streams is mandatory. This class supports\n    custom streams as well. Custom streams can be accessed by using the\n    `streamname_stream` field of the created instance.\n\n    The name of custom streams can only contain letters, numbers or the \"_\"\n    character and must not start with a number.\n    \"\"\"\n\n    def __init__(self: TGenericCLScenario,\n                 *,\n                 stream_definitions: TStreamsUserDict,\n                 complete_test_set_only: bool = False,\n                 experience_factory: Callable[['GenericScenarioStream', int],\n                                              TExperience] = None):\n        \"\"\"\n        Creates an instance of a Continual Learning benchmark instance.\n\n        The benchmark instance is defined by a stream definition dictionary,\n        which describes the content of each stream. The \"train\" and \"test\"\n        stream are mandatory. Any other custom stream can be added.\n\n        There is no constraint on the amount of experiences in each stream\n        (excluding the case in which `complete_test_set_only` is set).\n\n        :param stream_definitions: The stream definitions dictionary. Must\n            be a dictionary where the key is the stream name and the value\n            is the definition of that stream. \"train\" and \"test\" streams are\n            mandatory. This class supports custom streams as well. The name of\n            custom streams can only contain letters, numbers and the \"_\"\n            character and must not start with a number. Streams can be defined\n            is two ways: static and lazy. In the static case, the\n            stream must be a tuple containing 1, 2 or 3 elements:\n            - The first element must be a list containing the datasets\n            describing each experience. Datasets must be instances of\n            :class:`AvalancheDataset`.\n            - The second element is optional and must be a list containing the\n            task labels of each experience (as an int or a set of ints).\n            If the stream definition tuple contains only one element (the list\n            of datasets), then the task labels for each experience will be\n            obtained by inspecting the content of the datasets.\n            - The third element is optional and must be a reference to the\n            originating dataset (if applicable). For instance, for SplitMNIST\n            this may be a reference to the whole MNIST dataset. If the stream\n            definition tuple contains less than 3 elements, then the reference\n            to the original dataset will be set to None.\n            In the lazy case, the stream must be defined as a tuple with 2\n            elements:\n            - The first element must be a tuple containing the dataset generator\n            (one for each experience) and the number of experiences in that\n            stream.\n            - The second element must be a list containing the task labels of\n            each experience (as an int or a set of ints).\n        :param complete_test_set_only: If True, the test stream will contain\n            a single experience containing the complete test set. This also\n            means that the definition for the test stream must contain the\n            definition for a single experience.\n        :param experience_factory: If not None, a callable that, given the\n            benchmark instance and the experience ID, returns a experience\n            instance. This parameter is usually used in subclasses (when\n            invoking the super constructor) to specialize the experience class.\n            Defaults to None, which means that the :class:`GenericExperience`\n            constructor will be used.\n        \"\"\"\n\n        self.stream_definitions = \\\n            GenericCLScenario._check_stream_definitions(stream_definitions)\n        \"\"\"\n        A structure containing the definition of the streams.\n        \"\"\"\n\n        self.original_train_dataset: Optional[Dataset] = \\\n            self.stream_definitions['train'].origin_dataset\n        \"\"\" The original training set. May be None. \"\"\"\n\n        self.original_test_dataset: Optional[Dataset] = \\\n            self.stream_definitions['test'].origin_dataset\n        \"\"\" The original test set. May be None. \"\"\"\n\n        self.train_stream: GenericScenarioStream[\n            TExperience, TGenericCLScenario] = GenericScenarioStream('train',\n                                                                     self)\n        \"\"\"\n        The stream used to obtain the training experiences. \n        This stream can be sliced in order to obtain a subset of this stream.\n        \"\"\"\n\n        self.test_stream: GenericScenarioStream[\n            TExperience, TGenericCLScenario] = GenericScenarioStream('test',\n                                                                     self)\n        \"\"\"\n        The stream used to obtain the test experiences. This stream can be \n        sliced in order to obtain a subset of this stream.\n\n        Beware that, in certain scenarios, this stream may contain a single\n        element. Check the ``complete_test_set_only`` field for more details.\n        \"\"\"\n\n        self.complete_test_set_only: bool = bool(complete_test_set_only)\n        \"\"\"\n        If True, only the complete test set will be returned from experience\n        instances.\n\n        This flag is usually set to True in scenarios where having one separate\n        test set aligned to each training experience is impossible or doesn't\n        make sense from a semantic point of view.\n        \"\"\"\n\n        if self.complete_test_set_only:\n            if len(self.stream_definitions['test'].exps_data) > 1:\n                raise ValueError(\n                    'complete_test_set_only is True, but the test stream'\n                    ' contains more than one experience')\n\n        if experience_factory is None:\n            experience_factory = GenericExperience\n\n        self.experience_factory: Callable[[TGenericScenarioStream, int],\n                                          TExperience] = experience_factory\n\n        # Create the original_<stream_name>_dataset fields for other streams\n        self._make_original_dataset_fields()\n\n        # Create the <stream_name>_stream fields for other streams\n        self._make_stream_fields()\n\n    @property\n    def streams(self) -> Dict[str, 'GenericScenarioStream['\n                                   'TExperience, TGenericCLScenario]']:\n        streams_dict = dict()\n        for stream_name in self.stream_definitions.keys():\n            streams_dict[stream_name] = getattr(self, f'{stream_name}_stream')\n\n        return streams_dict\n\n    @property\n    def n_experiences(self) -> int:\n        \"\"\"  The number of incremental training experiences contained\n        in the train stream. \"\"\"\n        return len(self.stream_definitions['train'].exps_data)\n\n    @property\n    def task_labels(self) -> Sequence[List[int]]:\n        \"\"\" The task label of each training experience. \"\"\"\n        t_labels = []\n\n        for exp_t_labels in self.stream_definitions['train'].exps_task_labels:\n            t_labels.append(list(exp_t_labels))\n\n        return t_labels\n\n    def get_reproducibility_data(self) -> Dict[str, Any]:\n        \"\"\"\n        Gets the data needed to reproduce this experiment.\n\n        This data can be stored using the pickle module or some other mechanism.\n        It can then be loaded by passing it as the ``reproducibility_data``\n        parameter in the constructor.\n\n        Child classes should create their own reproducibility dictionary.\n        This means that the implementation found in :class:`GenericCLScenario`\n        will return an empty dictionary, which is meaningless.\n\n        In order to obtain the same benchmark instance, the reproducibility\n        data must be passed to the constructor along with the exact same\n        input datasets.\n\n        :return: A dictionary containing the data needed to reproduce the\n            experiment.\n        \"\"\"\n\n        return dict()\n\n    @property\n    def classes_in_experience(self) -> Mapping[str,\n                                               Sequence[Optional[Set[int]]]]:\n        \"\"\"\n        A dictionary mapping each stream (by name) to a list.\n\n        Each element of the list is a set describing the classes included in\n        that experience (identified by its index).\n\n        In previous releases this field contained the list of sets for the\n        training stream (that is, there was no way to obtain the list for other\n        streams). That behavior is deprecated and support for that usage way\n        will be removed in the future.\n        \"\"\"\n\n        return LazyStreamClassesInExps(self)\n\n    def get_classes_timeline(self, current_experience: int,\n                             stream: str = 'train'):\n        \"\"\"\n        Returns the classes timeline given the ID of a experience.\n\n        Given a experience ID, this method returns the classes in that\n        experience, previously seen classes, the cumulative class list and a\n        list of classes that will be encountered in next experiences of the\n        same stream.\n\n        Beware that by default this will obtain the timeline of an experience\n        of the **training** stream. Use the stream parameter to select another\n        stream.\n\n        :param current_experience: The reference experience ID.\n        :param stream: The stream name.\n        :return: A tuple composed of four lists: the first list contains the\n            IDs of classes in this experience, the second contains IDs of\n            classes seen in previous experiences, the third returns a cumulative\n            list of classes (that is, the union of the first two list) while the\n            last one returns a list of classes that will be encountered in next\n            experiences. Beware that each of these elements can be None when\n            the benchmark is initialized by using a lazy generator.\n        \"\"\"\n\n        class_set_current_exp = \\\n            self.classes_in_experience[stream][current_experience]\n\n        if class_set_current_exp is not None:\n            # May be None in lazy benchmarks\n            classes_in_this_exp = list(class_set_current_exp)\n        else:\n            classes_in_this_exp = None\n\n        class_set_prev_exps = set()\n        for exp_id in range(0, current_experience):\n            prev_exp_classes = self.classes_in_experience[stream][exp_id]\n            if prev_exp_classes is None:\n                # May be None in lazy benchmarks\n                class_set_prev_exps = None\n                break\n            class_set_prev_exps.update(prev_exp_classes)\n\n        if class_set_prev_exps is not None:\n            previous_classes = list(class_set_prev_exps)\n        else:\n            previous_classes = None\n\n        if class_set_current_exp is not None and \\\n                class_set_prev_exps is not None:\n            classes_seen_so_far = \\\n                list(class_set_current_exp.union(class_set_prev_exps))\n        else:\n            classes_seen_so_far = None\n\n        class_set_future_exps = set()\n        stream_n_exps = len(self.classes_in_experience[stream])\n        for exp_id in range(current_experience + 1, stream_n_exps):\n            future_exp_classes = self.classes_in_experience[stream][exp_id]\n            if future_exp_classes is None:\n                class_set_future_exps = None\n                break\n            class_set_future_exps.update(future_exp_classes)\n\n        if class_set_future_exps is not None:\n            future_classes = list(class_set_future_exps)\n        else:\n            future_classes = None\n\n        return (classes_in_this_exp, previous_classes, classes_seen_so_far,\n                future_classes)\n\n    def _make_original_dataset_fields(self):\n        for stream_name, stream_def in self.stream_definitions.items():\n            if stream_name in ['train', 'test']:\n                continue\n\n            orig_dataset = stream_def.origin_dataset\n            setattr(self, f'original_{stream_name}_dataset', orig_dataset)\n\n    def _make_stream_fields(self):\n        for stream_name, stream_def in self.stream_definitions.items():\n            if stream_name in ['train', 'test']:\n                continue\n\n            stream_obj = GenericScenarioStream(stream_name, self)\n            setattr(self, f'{stream_name}_stream', stream_obj)\n\n    @staticmethod\n    def _check_stream_definitions(\n            stream_definitions: TStreamsUserDict) -> TStreamsDict:\n        \"\"\"\n        A function used to check the input stream definitions.\n\n        This function should returns the adapted definition in which the\n        missing optional fields are filled. If the input definition doesn't\n        follow the expected structure, a `ValueError` will be raised.\n\n        :param stream_definitions: The input stream definitions.\n        :return: The checked and adapted stream definitions.\n        \"\"\"\n        streams_defs = dict()\n\n        if 'train' not in stream_definitions:\n            raise ValueError('No train stream found!')\n\n        if 'test' not in stream_definitions:\n            raise ValueError('No test stream found!')\n\n        for stream_name, stream_def in stream_definitions.items():\n            GenericCLScenario._check_stream_name(stream_name)\n            stream_def = GenericCLScenario._check_and_adapt_user_stream_def(\n                stream_def, stream_name)\n            streams_defs[stream_name] = stream_def\n\n        return streams_defs\n\n    @staticmethod\n    def _check_stream_name(stream_name: Any):\n        if not isinstance(stream_name, str):\n            raise ValueError('Invalid type for stream name. Must be a \"str\"')\n\n        if STREAM_NAME_REGEX.fullmatch(stream_name) is None:\n            raise ValueError(f'Invalid name for stream {stream_name}')\n\n    @staticmethod\n    def _check_and_adapt_user_stream_def(\n            stream_def: TStreamUserDef, stream_name: str) -> StreamDef:\n        exp_data = stream_def[0]\n        task_labels = None\n        origin_dataset = None\n        is_lazy = None\n\n        if len(stream_def) > 1:\n            task_labels = stream_def[1]\n\n        if len(stream_def) > 2:\n            origin_dataset = stream_def[2]\n\n        if len(stream_def) > 3:\n            is_lazy = stream_def[3]\n\n        if is_lazy or (isinstance(exp_data, tuple) and (is_lazy is None)):\n            # Creation based on a generator\n            if is_lazy:\n                # We also check for LazyDatasetSequence, which is sufficient\n                # per se (only if is_lazy==True, otherwise is treated as a\n                # standard Sequence)\n                if not isinstance(exp_data, LazyDatasetSequence):\n                    if (not isinstance(exp_data, tuple)) or \\\n                            (not len(exp_data) == 2):\n                        raise ValueError(\n                            f'The stream {stream_name} was flagged as '\n                            f'lazy-generated but its definition is not a '\n                            f'2-elements tuple (generator and stream length).')\n            else:\n                if (not len(exp_data) == 2) or \\\n                        (not isinstance(exp_data[1], int)):\n                    raise ValueError(\n                        f'The stream {stream_name} was detected '\n                        f'as lazy-generated but its definition is not a '\n                        f'2-elements tuple. If you\\'re trying to define a '\n                        f'non-lazily generated stream, don\\'t use a tuple '\n                        f'when passing the list of datasets, use a list '\n                        f'instead.')\n\n            if isinstance(exp_data, LazyDatasetSequence):\n                stream_length = len(exp_data)\n            else:\n                # exp_data[0] must contain the generator\n                stream_length = exp_data[1]\n            is_lazy = True\n        elif isinstance(exp_data, AvalancheDataset):\n            # Single element\n            exp_data = [exp_data]\n            is_lazy = False\n            stream_length = 1\n        else:\n            # Standard def\n            stream_length = len(exp_data)\n            is_lazy = False\n\n        if not is_lazy:\n            for i, dataset in enumerate(exp_data):\n                if not isinstance(dataset, AvalancheDataset):\n                    raise ValueError(\n                        'All experience datasets must be subclasses of'\n                        ' AvalancheDataset')\n\n        if task_labels is None:\n            if is_lazy:\n                raise ValueError(\n                    'Task labels must be defined for each experience when '\n                    'creating the stream using a generator.')\n\n            # Extract task labels from the dataset\n            task_labels = []\n            for i in range(len(exp_data)):\n                exp_dataset: AvalancheDataset = exp_data[i]\n                task_labels.append(set(exp_dataset.targets_task_labels))\n        else:\n            # Standardize task labels structure\n            task_labels = list(task_labels)\n            for i in range(len(task_labels)):\n                if isinstance(task_labels[i], int):\n                    task_labels[i] = {task_labels[i]}\n                elif not isinstance(task_labels[i], set):\n                    task_labels[i] = set(task_labels[i])\n\n        if stream_length != len(task_labels):\n            raise ValueError(\n                f'{len(exp_data)} experiences have been defined, but task '\n                f'labels for {len(task_labels)} experiences are given.')\n\n        if is_lazy:\n            if isinstance(exp_data, LazyDatasetSequence):\n                lazy_sequence = exp_data\n            else:\n                lazy_sequence = LazyDatasetSequence(exp_data[0], stream_length)\n        else:\n            lazy_sequence = LazyDatasetSequence(exp_data, stream_length)\n            lazy_sequence.load_all_experiences()\n\n        return StreamDef(\n            lazy_sequence,\n            task_labels,\n            origin_dataset,\n            is_lazy)",
  "class GenericScenarioStream(Generic[TExperience, TGenericCLScenario],\n                            ScenarioStream[TGenericCLScenario, TExperience],\n                            Sequence[TExperience]):\n\n    def __init__(self: TGenericScenarioStream,\n                 name: str,\n                 benchmark: TGenericCLScenario,\n                 *,\n                 slice_ids: List[int] = None):\n        self.slice_ids: Optional[List[int]] = slice_ids\n        \"\"\"\n        Describes which experiences are contained in the current stream slice. \n        Can be None, which means that this object is the original stream. \"\"\"\n\n        self.name: str = name\n        \"\"\"\n        The name of the stream (for instance: \"train\", \"test\", \"valid\", ...).\n        \"\"\"\n\n        self.benchmark = benchmark\n        \"\"\"\n        A reference to the benchmark.\n        \"\"\"\n\n    def __len__(self) -> int:\n        \"\"\"\n        Gets the number of experiences this stream it's made of.\n\n        :return: The number of experiences in this stream.\n        \"\"\"\n        if self.slice_ids is None:\n            return len(self.benchmark.stream_definitions[self.name].exps_data)\n        else:\n            return len(self.slice_ids)\n\n    def __getitem__(self, exp_idx: Union[int, slice, Iterable[int]]) -> \\\n            Union[TExperience, TScenarioStream]:\n        \"\"\"\n        Gets a experience given its experience index (or a stream slice given\n        the experience order).\n\n        :param exp_idx: An int describing the experience index or an\n            iterable/slice object describing a slice of this stream.\n\n        :return: The experience instance associated to the given experience\n            index or a sliced stream instance.\n        \"\"\"\n        if isinstance(exp_idx, int):\n            if exp_idx < len(self):\n                if self.slice_ids is None:\n                    return self.benchmark.experience_factory(self, exp_idx)\n                else:\n                    return self.benchmark.experience_factory(\n                        self, self.slice_ids[exp_idx])\n            raise IndexError('Experience index out of bounds' +\n                             str(int(exp_idx)))\n        else:\n            return self._create_slice(exp_idx)\n\n    def _create_slice(self: TGenericScenarioStream,\n                      exps_slice: Union[int, slice, Iterable[int]]) \\\n            -> TScenarioStream:\n        \"\"\"\n        Creates a sliced version of this stream.\n\n        In its base version, a shallow copy of this stream is created and\n        then its ``slice_ids`` field is adapted.\n\n        :param exps_slice: The slice to use.\n        :return: A sliced version of this stream.\n        \"\"\"\n        stream_copy = copy.copy(self)\n        slice_exps = _get_slice_ids(exps_slice, len(self))\n\n        if self.slice_ids is None:\n            stream_copy.slice_ids = slice_exps\n        else:\n            stream_copy.slice_ids = [self.slice_ids[x] for x in slice_exps]\n        return stream_copy\n\n    def drop_previous_experiences(self, to_exp: int) -> None:\n        \"\"\"\n        Drop the reference to experiences up to a certain experience ID\n        (inclusive).\n\n        This means that any reference to experiences with ID [0, from_exp] will\n        be released. By dropping the reference to previous experiences, the\n        memory associated with them can be freed, especially the one occupied by\n        the dataset. However, if external references to the experience or the\n        dataset still exist, dropping previous experiences at the stream level\n        will have little to no impact on the memory usage.\n\n        To make sure that the underlying dataset can be freed, make sure that:\n        - No reference to previous datasets or experiences are kept in you code;\n        - The replay implementation doesn't keep a reference to previous\n            datasets (in which case, is better to store a copy of the raw\n            tensors instead);\n        - The benchmark is being generated using a lazy initializer.\n\n        By dropping previous experiences, those experiences will no longer be\n        available in the stream. Trying to access them will result in an\n        exception.\n\n        :param to_exp: The ID of the last exp to drop (inclusive). Can be a\n            negative number, in which case this method doesn't have any effect.\n            Can be greater or equal to the stream length, in which case all\n            currently loaded experiences will be dropped.\n        :return: None\n        \"\"\"\n        self.benchmark.stream_definitions[\n            self.name].exps_data.drop_previous_experiences(to_exp)",
  "class LazyStreamClassesInExps(Mapping[str, Sequence[Optional[Set[int]]]]):\n    def __init__(self, benchmark: GenericCLScenario):\n        self._benchmark = benchmark\n        self._default_lcie = LazyClassesInExps(benchmark, stream='train')\n\n    def __len__(self):\n        return len(self._benchmark.stream_definitions)\n\n    def __getitem__(self, stream_name_or_exp_id):\n        if isinstance(stream_name_or_exp_id, str):\n            return LazyClassesInExps(self._benchmark,\n                                     stream=stream_name_or_exp_id)\n\n        warnings.warn(\n            'Using classes_in_experience[exp_id] is deprecated. '\n            'Consider using classes_in_experience[stream_name][exp_id]'\n            'instead.', stacklevel=2)\n        return self._default_lcie[stream_name_or_exp_id]\n\n    def __iter__(self):\n        yield from self._benchmark.stream_definitions.keys()",
  "class LazyClassesInExps(Sequence[Optional[Set[int]]]):\n    def __init__(self, benchmark: GenericCLScenario, stream: str = 'train'):\n        self._benchmark = benchmark\n        self._stream = stream\n\n    def __len__(self):\n        return len(self._benchmark.streams[self._stream])\n\n    def __getitem__(self, exp_id) -> Set[int]:\n        return manage_advanced_indexing(\n            exp_id, self._get_single_exp_classes,\n            len(self), LazyClassesInExps._slice_collate)\n\n    def __str__(self):\n        return '[' + \\\n               ', '.join([str(self[idx]) for idx in range(len(self))]) + \\\n               ']'\n\n    def _get_single_exp_classes(self, exp_id):\n        targets = self._benchmark.stream_definitions[\n            self._stream].exps_data.targets_field_sequence[exp_id]\n        if targets is None:\n            return None\n        return set(targets)\n\n    @staticmethod\n    def _slice_collate(*classes_in_exps: Optional[Set[int]]):\n        if any(x is None for x in classes_in_exps):\n            return None\n\n        return [\n            list(x) for x in classes_in_exps\n        ]",
  "def _get_slice_ids(slice_definition: Union[int, slice, Iterable[int]],\n                   sliceable_len: int) -> List[int]:\n    # Obtain experiences list from slice object (or any iterable)\n    exps_list: List[int]\n    if isinstance(slice_definition, slice):\n        exps_list = list(\n            range(*slice_definition.indices(sliceable_len)))\n    elif isinstance(slice_definition, int):\n        exps_list = [slice_definition]\n    elif hasattr(slice_definition, 'shape') and \\\n            len(getattr(slice_definition, 'shape')) == 0:\n        exps_list = [int(slice_definition)]\n    else:\n        exps_list = list(slice_definition)\n\n    # Check experience id(s) boundaries\n    if max(exps_list) >= sliceable_len:\n        raise IndexError(\n            'Experience index out of range: ' + str(max(exps_list)))\n\n    if min(exps_list) < 0:\n        raise IndexError(\n            'Experience index out of range: ' + str(min(exps_list)))\n\n    return exps_list",
  "class AbstractExperience(Experience[TScenario, TScenarioStream], ABC):\n    \"\"\"\n    Definition of a learning experience. A learning experience contains a set of\n    patterns which has become available at a particular time instant. The\n    content and size of an Experience is defined by the specific benchmark that\n    creates the experience.\n\n    For instance, an experience of a New Classes scenario will contain all\n    patterns belonging to a subset of classes of the original training set. An\n    experience of a New Instance scenario will contain patterns from previously\n    seen classes.\n    \"\"\"\n\n    def __init__(\n            self: TExperience,\n            origin_stream: TScenarioStream,\n            current_experience: int,\n            classes_in_this_exp: Sequence[int],\n            previous_classes: Sequence[int],\n            classes_seen_so_far: Sequence[int],\n            future_classes: Optional[Sequence[int]]):\n        \"\"\"\n        Creates an instance of the abstract experience given the benchmark\n        stream, the current experience ID and data about the classes timeline.\n\n        :param origin_stream: The stream from which this experience was\n            obtained.\n        :param current_experience: The current experience ID, as an integer.\n        :param classes_in_this_exp: The list of classes in this experience.\n        :param previous_classes: The list of classes in previous experiences.\n        :param classes_seen_so_far: List of classes of current and previous\n            experiences.\n        :param future_classes: The list of classes of next experiences.\n        \"\"\"\n\n        self.origin_stream: TScenarioStream = origin_stream\n\n        # benchmark keeps a reference to the base benchmark\n        self.benchmark: TScenario = origin_stream.benchmark\n\n        # current_experience is usually an incremental, 0-indexed, value used to\n        # keep track of the current batch/task.\n        self.current_experience: int = current_experience\n\n        self.classes_in_this_experience: Sequence[int] = classes_in_this_exp\n        \"\"\" The list of classes in this experience \"\"\"\n\n        self.previous_classes: Sequence[int] = previous_classes\n        \"\"\" The list of classes in previous experiences \"\"\"\n\n        self.classes_seen_so_far: Sequence[int] = classes_seen_so_far\n        \"\"\" List of classes of current and previous experiences \"\"\"\n\n        self.future_classes: Optional[Sequence[int]] = future_classes\n        \"\"\" The list of classes of next experiences \"\"\"\n\n    @property\n    def task_label(self) -> int:\n        \"\"\"\n        The task label. This value will never have value \"None\". However,\n        for scenarios that don't produce task labels a placeholder value like 0\n        is usually set. Beware that this field is meant as a shortcut to obtain\n        a unique task label: it assumes that only patterns labeled with a\n        single task label are present. If this experience contains patterns from\n        multiple tasks, accessing this property will result in an exception.\n        \"\"\"\n        if len(self.task_labels) != 1:\n            raise ValueError('The task_label property can only be accessed '\n                             'when the experience contains a single task label')\n\n        return self.task_labels[0]",
  "class GenericExperience(AbstractExperience[TGenericCLScenario,\n                                           GenericScenarioStream[\n                                               TGenericExperience,\n                                               TGenericCLScenario]]):\n    \"\"\"\n    Definition of a learning experience based on a :class:`GenericCLScenario`\n    instance.\n\n    This experience implementation uses the generic experience-patterns\n    assignment defined in the :class:`GenericCLScenario` instance. Instances of\n    this class are usually obtained from a benchmark stream.\n    \"\"\"\n\n    def __init__(self: TGenericExperience,\n                 origin_stream: GenericScenarioStream[TGenericExperience,\n                                                      TGenericCLScenario],\n                 current_experience: int):\n        \"\"\"\n        Creates an instance of a generic experience given the stream from this\n        experience was taken and and the current experience ID.\n\n        :param origin_stream: The stream from which this experience was\n            obtained.\n        :param current_experience: The current experience ID, as an integer.\n        \"\"\"\n        self.dataset: AvalancheDataset = \\\n            origin_stream.benchmark.stream_definitions[\n                origin_stream.name].exps_data[current_experience]\n\n        (classes_in_this_exp, previous_classes, classes_seen_so_far,\n         future_classes) = origin_stream.benchmark.get_classes_timeline(\n            current_experience, stream=origin_stream.name)\n\n        super(GenericExperience, self).__init__(\n            origin_stream, current_experience, classes_in_this_exp,\n            previous_classes, classes_seen_so_far, future_classes)\n\n    def _get_stream_def(self):\n        return self.benchmark.stream_definitions[self.origin_stream.name]\n\n    @property\n    def task_labels(self) -> List[int]:\n        stream_def = self._get_stream_def()\n        return list(stream_def.exps_task_labels[self.current_experience])",
  "def __init__(self: TGenericCLScenario,\n                 *,\n                 stream_definitions: TStreamsUserDict,\n                 complete_test_set_only: bool = False,\n                 experience_factory: Callable[['GenericScenarioStream', int],\n                                              TExperience] = None):\n        \"\"\"\n        Creates an instance of a Continual Learning benchmark instance.\n\n        The benchmark instance is defined by a stream definition dictionary,\n        which describes the content of each stream. The \"train\" and \"test\"\n        stream are mandatory. Any other custom stream can be added.\n\n        There is no constraint on the amount of experiences in each stream\n        (excluding the case in which `complete_test_set_only` is set).\n\n        :param stream_definitions: The stream definitions dictionary. Must\n            be a dictionary where the key is the stream name and the value\n            is the definition of that stream. \"train\" and \"test\" streams are\n            mandatory. This class supports custom streams as well. The name of\n            custom streams can only contain letters, numbers and the \"_\"\n            character and must not start with a number. Streams can be defined\n            is two ways: static and lazy. In the static case, the\n            stream must be a tuple containing 1, 2 or 3 elements:\n            - The first element must be a list containing the datasets\n            describing each experience. Datasets must be instances of\n            :class:`AvalancheDataset`.\n            - The second element is optional and must be a list containing the\n            task labels of each experience (as an int or a set of ints).\n            If the stream definition tuple contains only one element (the list\n            of datasets), then the task labels for each experience will be\n            obtained by inspecting the content of the datasets.\n            - The third element is optional and must be a reference to the\n            originating dataset (if applicable). For instance, for SplitMNIST\n            this may be a reference to the whole MNIST dataset. If the stream\n            definition tuple contains less than 3 elements, then the reference\n            to the original dataset will be set to None.\n            In the lazy case, the stream must be defined as a tuple with 2\n            elements:\n            - The first element must be a tuple containing the dataset generator\n            (one for each experience) and the number of experiences in that\n            stream.\n            - The second element must be a list containing the task labels of\n            each experience (as an int or a set of ints).\n        :param complete_test_set_only: If True, the test stream will contain\n            a single experience containing the complete test set. This also\n            means that the definition for the test stream must contain the\n            definition for a single experience.\n        :param experience_factory: If not None, a callable that, given the\n            benchmark instance and the experience ID, returns a experience\n            instance. This parameter is usually used in subclasses (when\n            invoking the super constructor) to specialize the experience class.\n            Defaults to None, which means that the :class:`GenericExperience`\n            constructor will be used.\n        \"\"\"\n\n        self.stream_definitions = \\\n            GenericCLScenario._check_stream_definitions(stream_definitions)\n        \"\"\"\n        A structure containing the definition of the streams.\n        \"\"\"\n\n        self.original_train_dataset: Optional[Dataset] = \\\n            self.stream_definitions['train'].origin_dataset\n        \"\"\" The original training set. May be None. \"\"\"\n\n        self.original_test_dataset: Optional[Dataset] = \\\n            self.stream_definitions['test'].origin_dataset\n        \"\"\" The original test set. May be None. \"\"\"\n\n        self.train_stream: GenericScenarioStream[\n            TExperience, TGenericCLScenario] = GenericScenarioStream('train',\n                                                                     self)\n        \"\"\"\n        The stream used to obtain the training experiences. \n        This stream can be sliced in order to obtain a subset of this stream.\n        \"\"\"\n\n        self.test_stream: GenericScenarioStream[\n            TExperience, TGenericCLScenario] = GenericScenarioStream('test',\n                                                                     self)\n        \"\"\"\n        The stream used to obtain the test experiences. This stream can be \n        sliced in order to obtain a subset of this stream.\n\n        Beware that, in certain scenarios, this stream may contain a single\n        element. Check the ``complete_test_set_only`` field for more details.\n        \"\"\"\n\n        self.complete_test_set_only: bool = bool(complete_test_set_only)\n        \"\"\"\n        If True, only the complete test set will be returned from experience\n        instances.\n\n        This flag is usually set to True in scenarios where having one separate\n        test set aligned to each training experience is impossible or doesn't\n        make sense from a semantic point of view.\n        \"\"\"\n\n        if self.complete_test_set_only:\n            if len(self.stream_definitions['test'].exps_data) > 1:\n                raise ValueError(\n                    'complete_test_set_only is True, but the test stream'\n                    ' contains more than one experience')\n\n        if experience_factory is None:\n            experience_factory = GenericExperience\n\n        self.experience_factory: Callable[[TGenericScenarioStream, int],\n                                          TExperience] = experience_factory\n\n        # Create the original_<stream_name>_dataset fields for other streams\n        self._make_original_dataset_fields()\n\n        # Create the <stream_name>_stream fields for other streams\n        self._make_stream_fields()",
  "def streams(self) -> Dict[str, 'GenericScenarioStream['\n                                   'TExperience, TGenericCLScenario]']:\n        streams_dict = dict()\n        for stream_name in self.stream_definitions.keys():\n            streams_dict[stream_name] = getattr(self, f'{stream_name}_stream')\n\n        return streams_dict",
  "def n_experiences(self) -> int:\n        \"\"\"  The number of incremental training experiences contained\n        in the train stream. \"\"\"\n        return len(self.stream_definitions['train'].exps_data)",
  "def task_labels(self) -> Sequence[List[int]]:\n        \"\"\" The task label of each training experience. \"\"\"\n        t_labels = []\n\n        for exp_t_labels in self.stream_definitions['train'].exps_task_labels:\n            t_labels.append(list(exp_t_labels))\n\n        return t_labels",
  "def get_reproducibility_data(self) -> Dict[str, Any]:\n        \"\"\"\n        Gets the data needed to reproduce this experiment.\n\n        This data can be stored using the pickle module or some other mechanism.\n        It can then be loaded by passing it as the ``reproducibility_data``\n        parameter in the constructor.\n\n        Child classes should create their own reproducibility dictionary.\n        This means that the implementation found in :class:`GenericCLScenario`\n        will return an empty dictionary, which is meaningless.\n\n        In order to obtain the same benchmark instance, the reproducibility\n        data must be passed to the constructor along with the exact same\n        input datasets.\n\n        :return: A dictionary containing the data needed to reproduce the\n            experiment.\n        \"\"\"\n\n        return dict()",
  "def classes_in_experience(self) -> Mapping[str,\n                                               Sequence[Optional[Set[int]]]]:\n        \"\"\"\n        A dictionary mapping each stream (by name) to a list.\n\n        Each element of the list is a set describing the classes included in\n        that experience (identified by its index).\n\n        In previous releases this field contained the list of sets for the\n        training stream (that is, there was no way to obtain the list for other\n        streams). That behavior is deprecated and support for that usage way\n        will be removed in the future.\n        \"\"\"\n\n        return LazyStreamClassesInExps(self)",
  "def get_classes_timeline(self, current_experience: int,\n                             stream: str = 'train'):\n        \"\"\"\n        Returns the classes timeline given the ID of a experience.\n\n        Given a experience ID, this method returns the classes in that\n        experience, previously seen classes, the cumulative class list and a\n        list of classes that will be encountered in next experiences of the\n        same stream.\n\n        Beware that by default this will obtain the timeline of an experience\n        of the **training** stream. Use the stream parameter to select another\n        stream.\n\n        :param current_experience: The reference experience ID.\n        :param stream: The stream name.\n        :return: A tuple composed of four lists: the first list contains the\n            IDs of classes in this experience, the second contains IDs of\n            classes seen in previous experiences, the third returns a cumulative\n            list of classes (that is, the union of the first two list) while the\n            last one returns a list of classes that will be encountered in next\n            experiences. Beware that each of these elements can be None when\n            the benchmark is initialized by using a lazy generator.\n        \"\"\"\n\n        class_set_current_exp = \\\n            self.classes_in_experience[stream][current_experience]\n\n        if class_set_current_exp is not None:\n            # May be None in lazy benchmarks\n            classes_in_this_exp = list(class_set_current_exp)\n        else:\n            classes_in_this_exp = None\n\n        class_set_prev_exps = set()\n        for exp_id in range(0, current_experience):\n            prev_exp_classes = self.classes_in_experience[stream][exp_id]\n            if prev_exp_classes is None:\n                # May be None in lazy benchmarks\n                class_set_prev_exps = None\n                break\n            class_set_prev_exps.update(prev_exp_classes)\n\n        if class_set_prev_exps is not None:\n            previous_classes = list(class_set_prev_exps)\n        else:\n            previous_classes = None\n\n        if class_set_current_exp is not None and \\\n                class_set_prev_exps is not None:\n            classes_seen_so_far = \\\n                list(class_set_current_exp.union(class_set_prev_exps))\n        else:\n            classes_seen_so_far = None\n\n        class_set_future_exps = set()\n        stream_n_exps = len(self.classes_in_experience[stream])\n        for exp_id in range(current_experience + 1, stream_n_exps):\n            future_exp_classes = self.classes_in_experience[stream][exp_id]\n            if future_exp_classes is None:\n                class_set_future_exps = None\n                break\n            class_set_future_exps.update(future_exp_classes)\n\n        if class_set_future_exps is not None:\n            future_classes = list(class_set_future_exps)\n        else:\n            future_classes = None\n\n        return (classes_in_this_exp, previous_classes, classes_seen_so_far,\n                future_classes)",
  "def _make_original_dataset_fields(self):\n        for stream_name, stream_def in self.stream_definitions.items():\n            if stream_name in ['train', 'test']:\n                continue\n\n            orig_dataset = stream_def.origin_dataset\n            setattr(self, f'original_{stream_name}_dataset', orig_dataset)",
  "def _make_stream_fields(self):\n        for stream_name, stream_def in self.stream_definitions.items():\n            if stream_name in ['train', 'test']:\n                continue\n\n            stream_obj = GenericScenarioStream(stream_name, self)\n            setattr(self, f'{stream_name}_stream', stream_obj)",
  "def _check_stream_definitions(\n            stream_definitions: TStreamsUserDict) -> TStreamsDict:\n        \"\"\"\n        A function used to check the input stream definitions.\n\n        This function should returns the adapted definition in which the\n        missing optional fields are filled. If the input definition doesn't\n        follow the expected structure, a `ValueError` will be raised.\n\n        :param stream_definitions: The input stream definitions.\n        :return: The checked and adapted stream definitions.\n        \"\"\"\n        streams_defs = dict()\n\n        if 'train' not in stream_definitions:\n            raise ValueError('No train stream found!')\n\n        if 'test' not in stream_definitions:\n            raise ValueError('No test stream found!')\n\n        for stream_name, stream_def in stream_definitions.items():\n            GenericCLScenario._check_stream_name(stream_name)\n            stream_def = GenericCLScenario._check_and_adapt_user_stream_def(\n                stream_def, stream_name)\n            streams_defs[stream_name] = stream_def\n\n        return streams_defs",
  "def _check_stream_name(stream_name: Any):\n        if not isinstance(stream_name, str):\n            raise ValueError('Invalid type for stream name. Must be a \"str\"')\n\n        if STREAM_NAME_REGEX.fullmatch(stream_name) is None:\n            raise ValueError(f'Invalid name for stream {stream_name}')",
  "def _check_and_adapt_user_stream_def(\n            stream_def: TStreamUserDef, stream_name: str) -> StreamDef:\n        exp_data = stream_def[0]\n        task_labels = None\n        origin_dataset = None\n        is_lazy = None\n\n        if len(stream_def) > 1:\n            task_labels = stream_def[1]\n\n        if len(stream_def) > 2:\n            origin_dataset = stream_def[2]\n\n        if len(stream_def) > 3:\n            is_lazy = stream_def[3]\n\n        if is_lazy or (isinstance(exp_data, tuple) and (is_lazy is None)):\n            # Creation based on a generator\n            if is_lazy:\n                # We also check for LazyDatasetSequence, which is sufficient\n                # per se (only if is_lazy==True, otherwise is treated as a\n                # standard Sequence)\n                if not isinstance(exp_data, LazyDatasetSequence):\n                    if (not isinstance(exp_data, tuple)) or \\\n                            (not len(exp_data) == 2):\n                        raise ValueError(\n                            f'The stream {stream_name} was flagged as '\n                            f'lazy-generated but its definition is not a '\n                            f'2-elements tuple (generator and stream length).')\n            else:\n                if (not len(exp_data) == 2) or \\\n                        (not isinstance(exp_data[1], int)):\n                    raise ValueError(\n                        f'The stream {stream_name} was detected '\n                        f'as lazy-generated but its definition is not a '\n                        f'2-elements tuple. If you\\'re trying to define a '\n                        f'non-lazily generated stream, don\\'t use a tuple '\n                        f'when passing the list of datasets, use a list '\n                        f'instead.')\n\n            if isinstance(exp_data, LazyDatasetSequence):\n                stream_length = len(exp_data)\n            else:\n                # exp_data[0] must contain the generator\n                stream_length = exp_data[1]\n            is_lazy = True\n        elif isinstance(exp_data, AvalancheDataset):\n            # Single element\n            exp_data = [exp_data]\n            is_lazy = False\n            stream_length = 1\n        else:\n            # Standard def\n            stream_length = len(exp_data)\n            is_lazy = False\n\n        if not is_lazy:\n            for i, dataset in enumerate(exp_data):\n                if not isinstance(dataset, AvalancheDataset):\n                    raise ValueError(\n                        'All experience datasets must be subclasses of'\n                        ' AvalancheDataset')\n\n        if task_labels is None:\n            if is_lazy:\n                raise ValueError(\n                    'Task labels must be defined for each experience when '\n                    'creating the stream using a generator.')\n\n            # Extract task labels from the dataset\n            task_labels = []\n            for i in range(len(exp_data)):\n                exp_dataset: AvalancheDataset = exp_data[i]\n                task_labels.append(set(exp_dataset.targets_task_labels))\n        else:\n            # Standardize task labels structure\n            task_labels = list(task_labels)\n            for i in range(len(task_labels)):\n                if isinstance(task_labels[i], int):\n                    task_labels[i] = {task_labels[i]}\n                elif not isinstance(task_labels[i], set):\n                    task_labels[i] = set(task_labels[i])\n\n        if stream_length != len(task_labels):\n            raise ValueError(\n                f'{len(exp_data)} experiences have been defined, but task '\n                f'labels for {len(task_labels)} experiences are given.')\n\n        if is_lazy:\n            if isinstance(exp_data, LazyDatasetSequence):\n                lazy_sequence = exp_data\n            else:\n                lazy_sequence = LazyDatasetSequence(exp_data[0], stream_length)\n        else:\n            lazy_sequence = LazyDatasetSequence(exp_data, stream_length)\n            lazy_sequence.load_all_experiences()\n\n        return StreamDef(\n            lazy_sequence,\n            task_labels,\n            origin_dataset,\n            is_lazy)",
  "def __init__(self: TGenericScenarioStream,\n                 name: str,\n                 benchmark: TGenericCLScenario,\n                 *,\n                 slice_ids: List[int] = None):\n        self.slice_ids: Optional[List[int]] = slice_ids\n        \"\"\"\n        Describes which experiences are contained in the current stream slice. \n        Can be None, which means that this object is the original stream. \"\"\"\n\n        self.name: str = name\n        \"\"\"\n        The name of the stream (for instance: \"train\", \"test\", \"valid\", ...).\n        \"\"\"\n\n        self.benchmark = benchmark\n        \"\"\"\n        A reference to the benchmark.\n        \"\"\"",
  "def __len__(self) -> int:\n        \"\"\"\n        Gets the number of experiences this stream it's made of.\n\n        :return: The number of experiences in this stream.\n        \"\"\"\n        if self.slice_ids is None:\n            return len(self.benchmark.stream_definitions[self.name].exps_data)\n        else:\n            return len(self.slice_ids)",
  "def __getitem__(self, exp_idx: Union[int, slice, Iterable[int]]) -> \\\n            Union[TExperience, TScenarioStream]:\n        \"\"\"\n        Gets a experience given its experience index (or a stream slice given\n        the experience order).\n\n        :param exp_idx: An int describing the experience index or an\n            iterable/slice object describing a slice of this stream.\n\n        :return: The experience instance associated to the given experience\n            index or a sliced stream instance.\n        \"\"\"\n        if isinstance(exp_idx, int):\n            if exp_idx < len(self):\n                if self.slice_ids is None:\n                    return self.benchmark.experience_factory(self, exp_idx)\n                else:\n                    return self.benchmark.experience_factory(\n                        self, self.slice_ids[exp_idx])\n            raise IndexError('Experience index out of bounds' +\n                             str(int(exp_idx)))\n        else:\n            return self._create_slice(exp_idx)",
  "def _create_slice(self: TGenericScenarioStream,\n                      exps_slice: Union[int, slice, Iterable[int]]) \\\n            -> TScenarioStream:\n        \"\"\"\n        Creates a sliced version of this stream.\n\n        In its base version, a shallow copy of this stream is created and\n        then its ``slice_ids`` field is adapted.\n\n        :param exps_slice: The slice to use.\n        :return: A sliced version of this stream.\n        \"\"\"\n        stream_copy = copy.copy(self)\n        slice_exps = _get_slice_ids(exps_slice, len(self))\n\n        if self.slice_ids is None:\n            stream_copy.slice_ids = slice_exps\n        else:\n            stream_copy.slice_ids = [self.slice_ids[x] for x in slice_exps]\n        return stream_copy",
  "def drop_previous_experiences(self, to_exp: int) -> None:\n        \"\"\"\n        Drop the reference to experiences up to a certain experience ID\n        (inclusive).\n\n        This means that any reference to experiences with ID [0, from_exp] will\n        be released. By dropping the reference to previous experiences, the\n        memory associated with them can be freed, especially the one occupied by\n        the dataset. However, if external references to the experience or the\n        dataset still exist, dropping previous experiences at the stream level\n        will have little to no impact on the memory usage.\n\n        To make sure that the underlying dataset can be freed, make sure that:\n        - No reference to previous datasets or experiences are kept in you code;\n        - The replay implementation doesn't keep a reference to previous\n            datasets (in which case, is better to store a copy of the raw\n            tensors instead);\n        - The benchmark is being generated using a lazy initializer.\n\n        By dropping previous experiences, those experiences will no longer be\n        available in the stream. Trying to access them will result in an\n        exception.\n\n        :param to_exp: The ID of the last exp to drop (inclusive). Can be a\n            negative number, in which case this method doesn't have any effect.\n            Can be greater or equal to the stream length, in which case all\n            currently loaded experiences will be dropped.\n        :return: None\n        \"\"\"\n        self.benchmark.stream_definitions[\n            self.name].exps_data.drop_previous_experiences(to_exp)",
  "def __init__(self, benchmark: GenericCLScenario):\n        self._benchmark = benchmark\n        self._default_lcie = LazyClassesInExps(benchmark, stream='train')",
  "def __len__(self):\n        return len(self._benchmark.stream_definitions)",
  "def __getitem__(self, stream_name_or_exp_id):\n        if isinstance(stream_name_or_exp_id, str):\n            return LazyClassesInExps(self._benchmark,\n                                     stream=stream_name_or_exp_id)\n\n        warnings.warn(\n            'Using classes_in_experience[exp_id] is deprecated. '\n            'Consider using classes_in_experience[stream_name][exp_id]'\n            'instead.', stacklevel=2)\n        return self._default_lcie[stream_name_or_exp_id]",
  "def __iter__(self):\n        yield from self._benchmark.stream_definitions.keys()",
  "def __init__(self, benchmark: GenericCLScenario, stream: str = 'train'):\n        self._benchmark = benchmark\n        self._stream = stream",
  "def __len__(self):\n        return len(self._benchmark.streams[self._stream])",
  "def __getitem__(self, exp_id) -> Set[int]:\n        return manage_advanced_indexing(\n            exp_id, self._get_single_exp_classes,\n            len(self), LazyClassesInExps._slice_collate)",
  "def __str__(self):\n        return '[' + \\\n               ', '.join([str(self[idx]) for idx in range(len(self))]) + \\\n               ']'",
  "def _get_single_exp_classes(self, exp_id):\n        targets = self._benchmark.stream_definitions[\n            self._stream].exps_data.targets_field_sequence[exp_id]\n        if targets is None:\n            return None\n        return set(targets)",
  "def _slice_collate(*classes_in_exps: Optional[Set[int]]):\n        if any(x is None for x in classes_in_exps):\n            return None\n\n        return [\n            list(x) for x in classes_in_exps\n        ]",
  "def __init__(\n            self: TExperience,\n            origin_stream: TScenarioStream,\n            current_experience: int,\n            classes_in_this_exp: Sequence[int],\n            previous_classes: Sequence[int],\n            classes_seen_so_far: Sequence[int],\n            future_classes: Optional[Sequence[int]]):\n        \"\"\"\n        Creates an instance of the abstract experience given the benchmark\n        stream, the current experience ID and data about the classes timeline.\n\n        :param origin_stream: The stream from which this experience was\n            obtained.\n        :param current_experience: The current experience ID, as an integer.\n        :param classes_in_this_exp: The list of classes in this experience.\n        :param previous_classes: The list of classes in previous experiences.\n        :param classes_seen_so_far: List of classes of current and previous\n            experiences.\n        :param future_classes: The list of classes of next experiences.\n        \"\"\"\n\n        self.origin_stream: TScenarioStream = origin_stream\n\n        # benchmark keeps a reference to the base benchmark\n        self.benchmark: TScenario = origin_stream.benchmark\n\n        # current_experience is usually an incremental, 0-indexed, value used to\n        # keep track of the current batch/task.\n        self.current_experience: int = current_experience\n\n        self.classes_in_this_experience: Sequence[int] = classes_in_this_exp\n        \"\"\" The list of classes in this experience \"\"\"\n\n        self.previous_classes: Sequence[int] = previous_classes\n        \"\"\" The list of classes in previous experiences \"\"\"\n\n        self.classes_seen_so_far: Sequence[int] = classes_seen_so_far\n        \"\"\" List of classes of current and previous experiences \"\"\"\n\n        self.future_classes: Optional[Sequence[int]] = future_classes\n        \"\"\" The list of classes of next experiences \"\"\"",
  "def task_label(self) -> int:\n        \"\"\"\n        The task label. This value will never have value \"None\". However,\n        for scenarios that don't produce task labels a placeholder value like 0\n        is usually set. Beware that this field is meant as a shortcut to obtain\n        a unique task label: it assumes that only patterns labeled with a\n        single task label are present. If this experience contains patterns from\n        multiple tasks, accessing this property will result in an exception.\n        \"\"\"\n        if len(self.task_labels) != 1:\n            raise ValueError('The task_label property can only be accessed '\n                             'when the experience contains a single task label')\n\n        return self.task_labels[0]",
  "def __init__(self: TGenericExperience,\n                 origin_stream: GenericScenarioStream[TGenericExperience,\n                                                      TGenericCLScenario],\n                 current_experience: int):\n        \"\"\"\n        Creates an instance of a generic experience given the stream from this\n        experience was taken and and the current experience ID.\n\n        :param origin_stream: The stream from which this experience was\n            obtained.\n        :param current_experience: The current experience ID, as an integer.\n        \"\"\"\n        self.dataset: AvalancheDataset = \\\n            origin_stream.benchmark.stream_definitions[\n                origin_stream.name].exps_data[current_experience]\n\n        (classes_in_this_exp, previous_classes, classes_seen_so_far,\n         future_classes) = origin_stream.benchmark.get_classes_timeline(\n            current_experience, stream=origin_stream.name)\n\n        super(GenericExperience, self).__init__(\n            origin_stream, current_experience, classes_in_this_exp,\n            previous_classes, classes_seen_so_far, future_classes)",
  "def _get_stream_def(self):\n        return self.benchmark.stream_definitions[self.origin_stream.name]",
  "def task_labels(self) -> List[int]:\n        stream_def = self._get_stream_def()\n        return list(stream_def.exps_task_labels[self.current_experience])",
  "def create_multi_dataset_generic_benchmark(\n        train_datasets: Sequence[SupportedDataset],\n        test_datasets: Sequence[SupportedDataset],\n        *,\n        other_streams_datasets: Dict[str, Sequence[SupportedDataset]] = None,\n        complete_test_set_only: bool = False,\n        train_transform=None, train_target_transform=None,\n        eval_transform=None, eval_target_transform=None,\n        other_streams_transforms: Dict[str, Tuple[Any, Any]] = None,\n        dataset_type: AvalancheDatasetType = None) -> GenericCLScenario:\n    \"\"\"\n    Creates a benchmark instance given a list of datasets. Each dataset will be\n    considered as a separate experience.\n\n    Contents of the datasets must already be set, including task labels.\n    Transformations will be applied if defined.\n\n    This function allows for the creation of custom streams as well.\n    While \"train\" and \"test\" datasets must always be set, the experience list\n    for other streams can be defined by using the `other_streams_datasets`\n    parameter.\n\n    If transformations are defined, they will be applied to the datasets\n    of the related stream.\n\n    :param train_datasets: A list of training datasets.\n    :param test_datasets: A list of test datasets.\n    :param other_streams_datasets: A dictionary describing the content of custom\n        streams. Keys must be valid stream names (letters and numbers,\n        not starting with a number) while the value must be a list of dataset.\n        If this dictionary contains the definition for \"train\" or \"test\"\n        streams then those definition will override the `train_datasets` and\n        `test_datasets` parameters.\n    :param complete_test_set_only: If True, only the complete test set will\n        be returned by the benchmark. This means that the ``test_dataset_list``\n        parameter must be list with a single element (the complete test set).\n        Defaults to False.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param train_target_transform: The transformation to apply to training\n        patterns targets. Defaults to None.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param eval_target_transform: The transformation to apply to test\n        patterns targets. Defaults to None.\n    :param other_streams_transforms: Transformations to apply to custom\n        streams. If no transformations are defined for a custom stream,\n        then \"train\" transformations will be used. This parameter must be a\n        dictionary mapping stream names to transformations. The transformations\n        must be a two elements tuple where the first element defines the\n        X transformation while the second element is the Y transformation.\n        Those elements can be None. If this dictionary contains the\n        transformations for \"train\" or \"test\" streams then those transformations\n        will override the `train_transform`, `train_target_transform`,\n        `eval_transform` and `eval_target_transform` parameters.\n    :param dataset_type: The type of the dataset. Defaults to None, which\n        means that the type will be obtained from the input datasets. If input\n        datasets are not instances of :class:`AvalancheDataset`, the type\n        UNDEFINED will be used.\n\n    :returns: A :class:`GenericCLScenario` instance.\n    \"\"\"\n\n    transform_groups = dict(\n        train=(train_transform, train_target_transform),\n        eval=(eval_transform, eval_target_transform))\n\n    if other_streams_transforms is not None:\n        for stream_name, stream_transforms in other_streams_transforms.items():\n            if isinstance(stream_transforms, Sequence):\n                if len(stream_transforms) == 1:\n                    # Suppose we got only the transformation for X values\n                    stream_transforms = (stream_transforms[0], None)\n            else:\n                # Suppose it's the transformation for X values\n                stream_transforms = (stream_transforms, None)\n\n            transform_groups[stream_name] = stream_transforms\n\n    input_streams = dict(\n        train=train_datasets,\n        test=test_datasets)\n\n    if other_streams_datasets is not None:\n        input_streams = {**input_streams, **other_streams_datasets}\n\n    if complete_test_set_only:\n        if len(input_streams['test']) != 1:\n            raise ValueError('Test stream must contain one experience when'\n                             'complete_test_set_only is True')\n\n    stream_definitions = dict()\n\n    for stream_name, dataset_list in input_streams.items():\n        initial_transform_group = 'train'\n        if stream_name in transform_groups:\n            initial_transform_group = stream_name\n\n        stream_datasets = []\n        for dataset_idx in range(len(dataset_list)):\n            dataset = dataset_list[dataset_idx]\n            stream_datasets.append(AvalancheDataset(\n                dataset,\n                transform_groups=transform_groups,\n                initial_transform_group=initial_transform_group,\n                dataset_type=dataset_type))\n        stream_definitions[stream_name] = (stream_datasets,)\n\n    return GenericCLScenario(\n        stream_definitions=stream_definitions,\n        complete_test_set_only=complete_test_set_only)",
  "def _adapt_lazy_stream(\n        generator, transform_groups, initial_transform_group, dataset_type):\n    \"\"\"\n    A simple internal utility to apply transforms and dataset type to all lazily\n    generated datasets. Used in the :func:`create_lazy_generic_benchmark`\n    benchmark creation helper.\n\n    :return: A datasets in which the proper transformation groups and dataset\n        type are applied.\n    \"\"\"\n\n    for dataset in generator:\n        dataset = AvalancheDataset(\n            dataset, transform_groups=transform_groups,\n            initial_transform_group=initial_transform_group,\n            dataset_type=dataset_type)\n        yield dataset",
  "class LazyStreamDefinition(NamedTuple):\n    \"\"\"\n    A simple class that can be used when preparing the parameters for the\n    :func:`create_lazy_generic_benchmark` helper.\n\n    This class is a named tuple containing the fields required for defining\n    a lazily-created benchmark.\n\n    - exps_generator: The experiences generator. Can be a \"yield\"-based\n      generator, a custom sequence, a standard list or any kind of\n      iterable returning :class:`AvalancheDataset`.\n    - stream_length: The number of experiences in the stream. Must match the\n      number of experiences returned by the generator.\n    - exps_task_labels: A list containing the list of task labels of each\n      experience. If an experience contains a single task label, a single int\n      can be used.\n    \"\"\"\n\n    exps_generator: Iterable[AvalancheDataset]\n    \"\"\"\n    The experiences generator. Can be a \"yield\"-based generator, a custom\n    sequence, a standard list or any kind of iterable returning\n    :class:`AvalancheDataset`.\n    \"\"\"\n\n    stream_length: int\n    \"\"\"\n    The number of experiences in the stream. Must match the number of\n    experiences returned by the generator\n    \"\"\"\n\n    exps_task_labels: Sequence[Union[int, Iterable[int]]]\n    \"\"\"\n    A list containing the list of task labels of each experience.\n    If an experience contains a single task label, a single int can be used.\n    \n    This field is temporary required for internal purposes to support lazy\n    streams. This field may become optional in the future.\n    \"\"\"",
  "def create_lazy_generic_benchmark(\n        train_generator: LazyStreamDefinition,\n        test_generator: LazyStreamDefinition,\n        *,\n        other_streams_generators: Dict[str, LazyStreamDefinition] = None,\n        complete_test_set_only: bool = False,\n        train_transform=None, train_target_transform=None,\n        eval_transform=None, eval_target_transform=None,\n        other_streams_transforms: Dict[str, Tuple[Any, Any]] = None,\n        dataset_type: AvalancheDatasetType = None) \\\n        -> GenericCLScenario:\n    \"\"\"\n    Creates a lazily-defined benchmark instance given a dataset generator for\n    each stream.\n\n    Generators must return properly initialized instances of\n    :class:`AvalancheDataset` which will be used to create experiences.\n\n    The created datasets can have transformations already set.\n    However, if transformations are shared across all datasets of the same\n    stream, it is recommended to use the `train_transform`, `eval_transform`\n    and `other_streams_transforms` parameters, so that transformations groups\n    can be correctly applied (transformations are lazily added atop the datasets\n    returned by the generators). The same reasoning applies to the\n    `dataset_type` parameter.\n\n    This function allows for the creation of custom streams as well.\n    While \"train\" and \"test\" streams must be always set, the generators\n    for other streams can be defined by using the `other_streams_generators`\n    parameter.\n\n    :param train_generator: A proper lazy-generation definition for the training\n        stream. It is recommended to pass an instance\n        of :class:`LazyStreamDefinition`. See its description for more details.\n    :param test_generator: A proper lazy-generation definition for the test\n        stream. It is recommended to pass an instance\n        of :class:`LazyStreamDefinition`. See its description for more details.\n    :param other_streams_generators: A dictionary describing the content of\n        custom streams. Keys must be valid stream names (letters and numbers,\n        not starting with a number) while the value must be a\n        lazy-generation definition (like the ones of the training and\n        test streams). If this dictionary contains the definition for\n        \"train\" or \"test\" streams then those definition will override the\n        `train_generator` and `test_generator` parameters.\n    :param complete_test_set_only: If True, only the complete test set will\n        be returned by the benchmark. This means that the ``test_generator``\n        parameter must define a stream with a single experience (the complete\n        test set). Defaults to False.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param train_target_transform: The transformation to apply to training\n        patterns targets. Defaults to None.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param eval_target_transform: The transformation to apply to test\n        patterns targets. Defaults to None.\n    :param other_streams_transforms: Transformations to apply to custom\n        streams. If no transformations are defined for a custom stream,\n        then \"train\" transformations will be used. This parameter must be a\n        dictionary mapping stream names to transformations. The transformations\n        must be a two elements tuple where the first element defines the\n        X transformation while the second element is the Y transformation.\n        Those elements can be None. If this dictionary contains the\n        transformations for \"train\" or \"test\" streams then those transformations\n        will override the `train_transform`, `train_target_transform`,\n        `eval_transform` and `eval_target_transform` parameters.\n    :param dataset_type: The type of the datasets. Defaults to None, which\n        means that the type will be obtained from the input datasets. This\n        type will be applied to all the datasets returned by the generators.\n\n    :returns: A lazily-initialized :class:`GenericCLScenario` instance.\n    \"\"\"\n\n    transform_groups = dict(\n        train=(train_transform, train_target_transform),\n        eval=(eval_transform, eval_target_transform))\n\n    if other_streams_transforms is not None:\n        for stream_name, stream_transforms in other_streams_transforms.items():\n            if isinstance(stream_transforms, Sequence):\n                if len(stream_transforms) == 1:\n                    # Suppose we got only the transformation for X values\n                    stream_transforms = (stream_transforms[0], None)\n            else:\n                # Suppose it's the transformation for X values\n                stream_transforms = (stream_transforms, None)\n\n            transform_groups[stream_name] = stream_transforms\n\n    input_streams = dict(\n        train=train_generator,\n        test=test_generator)\n\n    if other_streams_generators is not None:\n        input_streams = {**input_streams, **other_streams_generators}\n\n    if complete_test_set_only:\n        if input_streams['test'][1] != 1:\n            raise ValueError('Test stream must contain one experience when'\n                             'complete_test_set_only is True')\n\n    stream_definitions = dict()\n\n    for stream_name, (generator, stream_length, task_labels) in \\\n            input_streams.items():\n        initial_transform_group = 'train'\n        if stream_name in transform_groups:\n            initial_transform_group = stream_name\n\n        adapted_stream_generator = _adapt_lazy_stream(\n            generator, transform_groups,\n            initial_transform_group=initial_transform_group,\n            dataset_type=dataset_type)\n\n        stream_definitions[stream_name] = (\n            (adapted_stream_generator, stream_length), task_labels\n        )\n\n    return GenericCLScenario(\n        stream_definitions=stream_definitions,\n        complete_test_set_only=complete_test_set_only)",
  "def create_generic_benchmark_from_filelists(\n        root: Optional[Union[str, Path]],\n        train_file_lists: Sequence[Union[str, Path]],\n        test_file_lists: Sequence[Union[str, Path]],\n        *,\n        other_streams_file_lists: Dict[str, Sequence[Union[str, Path]]] = None,\n        task_labels: Sequence[int],\n        complete_test_set_only: bool = False,\n        train_transform=None, train_target_transform=None,\n        eval_transform=None, eval_target_transform=None,\n        other_streams_transforms: Dict[str, Tuple[Any, Any]] = None) \\\n        -> GenericCLScenario:\n    \"\"\"\n    Creates a benchmark instance given a list of filelists and the respective\n    task labels. A separate dataset will be created for each filelist and each\n    of those datasets will be considered a separate experience.\n\n    This helper functions is the best shot when loading Caffe-style dataset\n    based on filelists.\n\n    Beware that this helper function is limited is the following two aspects:\n\n    - The resulting benchmark instance and the intermediate datasets used to\n      populate it will be of type CLASSIFICATION. There is no way to change\n      this.\n    - Task labels can only be defined by choosing a single task label for\n      each experience (the same task label is applied to all patterns of\n      experiences sharing the same position in different streams).\n\n    Despite those constraints, this helper function is usually sufficiently\n    powerful to cover most continual learning benchmarks based on file lists.\n\n    When in need to create a similar benchmark instance starting from an\n    in-memory list of paths, then the similar helper function\n    :func:`create_generic_benchmark_from_paths` can be used.\n\n    When in need to create a benchmark instance in which task labels are defined\n    in a more fine-grained way, then consider using\n    :func:`create_multi_dataset_generic_benchmark` by passing properly\n    initialized :class:`AvalancheDataset` instances.\n\n    :param root: The root path of the dataset. Can be None.\n    :param train_file_lists: A list of filelists describing the\n        paths of the training patterns for each experience.\n    :param test_file_lists: A list of filelists describing the\n        paths of the test patterns for each experience.\n    :param other_streams_file_lists: A dictionary describing the content of\n        custom streams. Keys must be valid stream names (letters and numbers,\n        not starting with a number) while the value must be a list of filelists\n        (same as `train_file_lists` and `test_file_lists` parameters). If this\n        dictionary contains the definition for \"train\" or \"test\" streams then\n        those definition will  override the `train_file_lists` and\n        `test_file_lists` parameters.\n    :param task_labels: A list of task labels. Must contain at least a value\n        for each experience. Each value describes the task label that will be\n        applied to all patterns of a certain experience. For more info on that,\n        see the function description.\n    :param complete_test_set_only: If True, only the complete test set will\n        be returned by the benchmark. This means that the ``test_file_lists``\n        parameter must be list with a single element (the complete test set).\n        Alternatively, can be a plain string or :class:`Path` object.\n        Defaults to False.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param train_target_transform: The transformation to apply to training\n        patterns targets. Defaults to None.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param eval_target_transform: The transformation to apply to test\n        patterns targets. Defaults to None.\n    :param other_streams_transforms: Transformations to apply to custom\n        streams. If no transformations are defined for a custom stream,\n        then \"train\" transformations will be used. This parameter must be a\n        dictionary mapping stream names to transformations. The transformations\n        must be a two elements tuple where the first element defines the\n        X transformation while the second element is the Y transformation.\n        Those elements can be None. If this dictionary contains the\n        transformations for \"train\" or \"test\" streams then those transformations\n        will override the `train_transform`, `train_target_transform`,\n        `eval_transform` and `eval_target_transform` parameters.\n\n    :returns: A :class:`GenericCLScenario` instance.\n    \"\"\"\n\n    input_streams = dict(\n        train=train_file_lists,\n        test=test_file_lists)\n\n    if other_streams_file_lists is not None:\n        input_streams = {**input_streams, **other_streams_file_lists}\n\n    stream_definitions = dict()\n\n    for stream_name, file_lists in input_streams.items():\n        stream_datasets = []\n        for exp_id, f_list in enumerate(file_lists):\n\n            f_list_dataset = FilelistDataset(root, f_list)\n            stream_datasets.append(AvalancheDataset(\n                f_list_dataset,\n                task_labels=task_labels[exp_id]))\n\n        stream_definitions[stream_name] = stream_datasets\n\n    return create_multi_dataset_generic_benchmark(\n        [], [],\n        other_streams_datasets=stream_definitions,\n        train_transform=train_transform,\n        train_target_transform=train_target_transform,\n        eval_transform=eval_transform,\n        eval_target_transform=eval_target_transform,\n        complete_test_set_only=complete_test_set_only,\n        other_streams_transforms=other_streams_transforms,\n        dataset_type=AvalancheDatasetType.CLASSIFICATION)",
  "def create_generic_benchmark_from_paths(\n        train_lists_of_files: Sequence[Sequence[FileAndLabel]],\n        test_lists_of_files: Union[Sequence[FileAndLabel],\n                                   Sequence[Sequence[FileAndLabel]]],\n        *,\n        other_streams_lists_of_files: Dict[str, Sequence[\n            Sequence[FileAndLabel]]] = None,\n        task_labels: Sequence[int],\n        complete_test_set_only: bool = False,\n        train_transform=None, train_target_transform=None,\n        eval_transform=None, eval_target_transform=None,\n        other_streams_transforms: Dict[str, Tuple[Any, Any]] = None,\n        dataset_type: AvalancheDatasetType = AvalancheDatasetType.UNDEFINED) \\\n        -> GenericCLScenario:\n    \"\"\"\n    Creates a benchmark instance given a sequence of lists of files. A separate\n    dataset will be created for each list. Each of those datasets\n    will be considered a separate experience.\n\n    This is very similar to :func:`create_generic_benchmark_from_filelists`,\n    with the main difference being that\n    :func:`create_generic_benchmark_from_filelists` accepts, for each\n    experience, a file list formatted in Caffe-style. On the contrary, this\n    accepts a list of tuples where each tuple contains two elements: the full\n    path to the pattern and its label. Optionally, the tuple may contain a third\n    element describing the bounding box of the element to crop. This last\n    bounding box may be useful when trying to extract the part of the image\n    depicting the desired element.\n\n    Apart from that, the same limitations of\n    :func:`create_generic_benchmark_from_filelists` regarding task labels apply.\n\n    The label of each pattern doesn't have to be an int. Also, a dataset type\n    can be defined.\n\n    :param train_lists_of_files: A list of lists. Each list describes the paths\n        and labels of patterns to include in that training experience, as\n        tuples. Each tuple must contain two elements: the full path to the\n        pattern and its class label. Optionally, the tuple may contain a\n        third element describing the bounding box to use for cropping (top,\n        left, height, width).\n    :param test_lists_of_files: A list of lists. Each list describes the paths\n        and labels of patterns to include in that test experience, as tuples.\n        Each tuple must contain two elements: the full path to the pattern\n        and its class label. Optionally, the tuple may contain a third element\n        describing the bounding box to use for cropping (top, left, height,\n        width).\n    :param other_streams_lists_of_files: A dictionary describing the content of\n        custom streams. Keys must be valid stream names (letters and numbers,\n        not starting with a number) while the value follow the same structure\n        of `train_lists_of_files` and `test_lists_of_files` parameters. If this\n        dictionary contains the definition for \"train\" or \"test\" streams then\n        those definition will  override the `train_lists_of_files` and\n        `test_lists_of_files` parameters.\n    :param task_labels: A list of task labels. Must contain at least a value\n        for each experience. Each value describes the task label that will be\n        applied to all patterns of a certain experience. For more info on that,\n        see the function description.\n    :param complete_test_set_only: If True, only the complete test set will\n        be returned by the benchmark. This means that the ``test_list_of_files``\n        parameter must define a single experience (the complete test set).\n        Defaults to False.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param train_target_transform: The transformation to apply to training\n        patterns targets. Defaults to None.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param eval_target_transform: The transformation to apply to test\n        patterns targets. Defaults to None.\n    :param other_streams_transforms: Transformations to apply to custom\n        streams. If no transformations are defined for a custom stream,\n        then \"train\" transformations will be used. This parameter must be a\n        dictionary mapping stream names to transformations. The transformations\n        must be a two elements tuple where the first element defines the\n        X transformation while the second element is the Y transformation.\n        Those elements can be None. If this dictionary contains the\n        transformations for \"train\" or \"test\" streams then those transformations\n        will override the `train_transform`, `train_target_transform`,\n        `eval_transform` and `eval_target_transform` parameters.\n    :param dataset_type: The type of the dataset. Defaults to UNDEFINED.\n\n    :returns: A :class:`GenericCLScenario` instance.\n    \"\"\"\n\n    input_streams = dict(\n        train=train_lists_of_files,\n        test=test_lists_of_files)\n\n    if other_streams_lists_of_files is not None:\n        input_streams = {**input_streams, **other_streams_lists_of_files}\n\n    stream_definitions = dict()\n\n    for stream_name, lists_of_files in input_streams.items():\n        stream_datasets = []\n        for exp_id, list_of_files in enumerate(lists_of_files):\n            common_root, exp_paths_list = common_paths_root(list_of_files)\n            paths_dataset = PathsDataset(common_root, exp_paths_list)\n            stream_datasets.append(AvalancheDataset(\n                paths_dataset,\n                task_labels=task_labels[exp_id]))\n\n        stream_definitions[stream_name] = stream_datasets\n\n    return create_multi_dataset_generic_benchmark(\n        [], [],\n        other_streams_datasets=stream_definitions,\n        train_transform=train_transform,\n        train_target_transform=train_target_transform,\n        eval_transform=eval_transform,\n        eval_target_transform=eval_target_transform,\n        complete_test_set_only=complete_test_set_only,\n        other_streams_transforms=other_streams_transforms,\n        dataset_type=dataset_type)",
  "def create_generic_benchmark_from_tensor_lists(\n        train_tensors: Sequence[Sequence[Any]],\n        test_tensors: Sequence[Sequence[Any]],\n        *,\n        other_streams_tensors: Dict[str, Sequence[Sequence[Any]]] = None,\n        task_labels: Sequence[int],\n        complete_test_set_only: bool = False,\n        train_transform=None, train_target_transform=None,\n        eval_transform=None, eval_target_transform=None,\n        other_streams_transforms: Dict[str, Tuple[Any, Any]] = None,\n        dataset_type: AvalancheDatasetType = None) -> GenericCLScenario:\n    \"\"\"\n    Creates a benchmark instance given lists of Tensors. A separate dataset will\n    be created from each Tensor tuple (x, y, z, ...) and each of those training\n    datasets will be considered a separate training experience. Using this\n    helper function is the lowest-level way to create a Continual Learning\n    benchmark. When possible, consider using higher level helpers.\n\n    Experiences are defined by passing lists of tensors as the `train_tensors`,\n    `test_tensors` (and `other_streams_tensors`) parameters. Those parameters\n    must be lists containing lists of tensors, one list for each experience.\n    Each tensor defines the value of a feature (\"x\", \"y\", \"z\", ...) for all\n    patterns of that experience.\n\n    By default the second tensor of each experience will be used to fill the\n    `targets` value (label of each pattern).\n\n    Beware that task labels can only be defined by choosing a single task label\n    for each experience (the same task label is applied to all patterns of\n    experiences sharing the same position in different streams).\n\n    When in need to create a benchmark instance in which task labels are defined\n    in a more fine-grained way, then consider using\n    :func:`create_multi_dataset_generic_benchmark` by passing properly\n    initialized :class:`AvalancheDataset` instances.\n\n    :param train_tensors: A list of lists. The first list must contain the\n        tensors for the first training experience (one tensor per feature), the\n        second list must contain the tensors for the second training experience,\n        and so on.\n    :param test_tensors: A list of lists. The first list must contain the\n        tensors for the first test experience (one tensor per feature), the\n        second list must contain the tensors for the second test experience,\n        and so on. When using `complete_test_set_only`, this parameter\n        must be a list containing a single sub-list for the single test\n        experience.\n    :param other_streams_tensors: A dictionary describing the content of\n        custom streams. Keys must be valid stream names (letters and numbers,\n        not starting with a number) while the value follow the same structure\n        of `train_tensors` and `test_tensors` parameters. If this\n        dictionary contains the definition for \"train\" or \"test\" streams then\n        those definition will  override the `train_tensors` and `test_tensors`\n        parameters.\n    :param task_labels: A list of task labels. Must contain at least a value\n        for each experience. Each value describes the task label that will be\n        applied to all patterns of a certain experience. For more info on that,\n        see the function description.\n    :param complete_test_set_only: If True, only the complete test set will\n        be returned by the benchmark. This means that ``test_tensors`` must\n        define a single experience. Defaults to False.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param train_target_transform: The transformation to apply to training\n        patterns targets. Defaults to None.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param eval_target_transform: The transformation to apply to test\n        patterns targets. Defaults to None.\n    :param other_streams_transforms: Transformations to apply to custom\n        streams. If no transformations are defined for a custom stream,\n        then \"train\" transformations will be used. This parameter must be a\n        dictionary mapping stream names to transformations. The transformations\n        must be a two elements tuple where the first element defines the\n        X transformation while the second element is the Y transformation.\n        Those elements can be None. If this dictionary contains the\n        transformations for \"train\" or \"test\" streams then those transformations\n        will override the `train_transform`, `train_target_transform`,\n        `eval_transform` and `eval_target_transform` parameters.\n    :param dataset_type: The type of the dataset. Defaults to UNDEFINED.\n\n    :returns: A :class:`GenericCLScenario` instance.\n    \"\"\"\n\n    input_streams = dict(\n        train=train_tensors,\n        test=test_tensors)\n\n    if other_streams_tensors is not None:\n        input_streams = {**input_streams, **other_streams_tensors}\n\n    stream_definitions = dict()\n\n    for stream_name, list_of_exps_tensors in input_streams.items():\n        stream_datasets = []\n        for exp_id, exp_tensors in enumerate(list_of_exps_tensors):\n            stream_datasets.append(AvalancheTensorDataset(\n                *exp_tensors, dataset_type=dataset_type,\n                task_labels=task_labels[exp_id]))\n\n        stream_definitions[stream_name] = stream_datasets\n\n    return create_multi_dataset_generic_benchmark(\n        [], [],\n        other_streams_datasets=stream_definitions,\n        train_transform=train_transform,\n        train_target_transform=train_target_transform,\n        eval_transform=eval_transform,\n        eval_target_transform=eval_target_transform,\n        complete_test_set_only=complete_test_set_only,\n        other_streams_transforms=other_streams_transforms,\n        dataset_type=dataset_type)",
  "class Experience(Protocol[TScenario, TScenarioStream]):\n    \"\"\"\n    Definition of an experience. An experience contains a set of patterns\n    which has become available at a particular time instant. The content and\n    size of an Experience is defined by the specific benchmark that creates the\n    IExperience instance.\n\n    For instance, an experience of a New Classes scenario will contain all\n    patterns belonging to a subset of classes of the original training set. An\n    experience of a New Instance scenario will contain patterns from previously\n    seen classes.\n\n    Experiences of Single Incremental Task (a.k.a. task-free) scenarios are\n    usually called \"batches\" while in Multi Task scenarios an Experience is\n    usually associated to a \"task\". Finally, in a Multi Incremental Task\n    scenario the Experience may be composed by patterns from different tasks.\n    \"\"\"\n\n    origin_stream: TScenarioStream\n    \"\"\"\n    A reference to the original stream from which this experience was obtained.\n    \"\"\"\n\n    benchmark: TScenario\n    \"\"\"\n    A reference to the benchmark.\n    \"\"\"\n\n    current_experience: int\n    \"\"\"\n    This is an incremental, 0-indexed, value used to keep track of the position \n    of current experience in the original stream.\n    \n    Beware that this value only describes the experience position in the \n    original stream and may be unrelated to the order in which the strategy will\n    encounter experiences.\n    \"\"\"\n\n    dataset: AvalancheDataset\n    \"\"\"\n    The dataset containing the patterns available in this experience.\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def task_labels(self) -> List[int]:\n        \"\"\"\n        This list will contain the unique task labels of the patterns contained\n        in this experience. In the most common scenarios this will be a list\n        with a single value. Note: for scenarios that don't produce task labels,\n        a placeholder task label value like 0 is usually set to each pattern\n        (see the description of the originating scenario for details).\n        \"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def task_label(self) -> int:\n        \"\"\"\n        The task label. This value will never have value \"None\". However,\n        for scenarios that don't produce task labels a placeholder value like 0\n        is usually set. Beware that this field is meant as a shortcut to obtain\n        a unique task label: it assumes that only patterns labeled with a\n        single task label are present. If this experience contains patterns from\n        multiple tasks, accessing this property will result in an exception.\n        \"\"\"\n        ...\n\n    @property\n    def scenario(self) -> TScenario:\n        \"\"\" This property is DEPRECATED, use self.benchmark instead.\"\"\"\n        warnings.warn(\n            'Using self.scenario is deprecated in Experience. '\n            'Consider using self.benchmark instead.', stacklevel=2)\n        return self.benchmark",
  "class ScenarioStream(Protocol[TScenario, TExperience]):\n    \"\"\"\n    A scenario stream describes a sequence of incremental experiences.\n    Experiences are described as :class:`IExperience` instances. They contain a\n    set of patterns which has become available at a particular time instant\n    along with any optional, scenario specific, metadata.\n\n    Most scenario expose two different streams: the training stream and the test\n    stream.\n    \"\"\"\n\n    name: str\n    \"\"\"\n    The name of the stream.\n    \"\"\"\n\n    benchmark: TScenario\n    \"\"\"\n    A reference to the scenario this stream belongs to.\n    \"\"\"\n\n    @property\n    def scenario(self) -> TScenario:\n        \"\"\" This property is DEPRECATED, use self.benchmark instead.\"\"\"\n        warnings.warn(\n            'Using self.scenario is deprecated ScenarioStream. '\n            'Consider using self.benchmark instead.', stacklevel=2)\n        return self.benchmark\n\n    def __getitem__(self: TScenarioStream,\n                    experience_idx: Union[int, slice, Iterable[int]]) \\\n            -> Union[TExperience, TScenarioStream]:\n        \"\"\"\n        Gets an experience given its experience index (or a stream slice given\n        the experience order).\n\n        :param experience_idx: An int describing the experience index or an\n            iterable/slice object describing a slice of this stream.\n        :return: The Experience instance associated to the given experience\n            index or a sliced stream instance.\n        \"\"\"\n        ...\n\n    def __len__(self) -> int:\n        \"\"\"\n        Used to get the length of this stream (the amount of experiences).\n\n        :return: The amount of experiences in this stream.\n        \"\"\"\n        ...",
  "def task_labels(self) -> List[int]:\n        \"\"\"\n        This list will contain the unique task labels of the patterns contained\n        in this experience. In the most common scenarios this will be a list\n        with a single value. Note: for scenarios that don't produce task labels,\n        a placeholder task label value like 0 is usually set to each pattern\n        (see the description of the originating scenario for details).\n        \"\"\"\n        ...",
  "def task_label(self) -> int:\n        \"\"\"\n        The task label. This value will never have value \"None\". However,\n        for scenarios that don't produce task labels a placeholder value like 0\n        is usually set. Beware that this field is meant as a shortcut to obtain\n        a unique task label: it assumes that only patterns labeled with a\n        single task label are present. If this experience contains patterns from\n        multiple tasks, accessing this property will result in an exception.\n        \"\"\"\n        ...",
  "def scenario(self) -> TScenario:\n        \"\"\" This property is DEPRECATED, use self.benchmark instead.\"\"\"\n        warnings.warn(\n            'Using self.scenario is deprecated in Experience. '\n            'Consider using self.benchmark instead.', stacklevel=2)\n        return self.benchmark",
  "def scenario(self) -> TScenario:\n        \"\"\" This property is DEPRECATED, use self.benchmark instead.\"\"\"\n        warnings.warn(\n            'Using self.scenario is deprecated ScenarioStream. '\n            'Consider using self.benchmark instead.', stacklevel=2)\n        return self.benchmark",
  "def __getitem__(self: TScenarioStream,\n                    experience_idx: Union[int, slice, Iterable[int]]) \\\n            -> Union[TExperience, TScenarioStream]:\n        \"\"\"\n        Gets an experience given its experience index (or a stream slice given\n        the experience order).\n\n        :param experience_idx: An int describing the experience index or an\n            iterable/slice object describing a slice of this stream.\n        :return: The Experience instance associated to the given experience\n            index or a sliced stream instance.\n        \"\"\"\n        ...",
  "def __len__(self) -> int:\n        \"\"\"\n        Used to get the length of this stream (the amount of experiences).\n\n        :return: The amount of experiences in this stream.\n        \"\"\"\n        ...",
  "class LazyDatasetSequence(Sequence[AvalancheDataset]):\n    \"\"\"\n    A lazily initialized sequence of datasets.\n\n    This class provides a way to lazily generate and store the datasets\n    linked to each experience. This class uses a generator to get the sequence\n    of datasets but it can also be used with a more classic statically\n    initialized Sequence (like a list).\n\n    This class will also keep track of the targets and task labels field of the\n    generated datasets.\n    \"\"\"\n\n    def __init__(\n            self,\n            experience_generator: Iterable[AvalancheDataset],\n            stream_length: int):\n        self._exp_source: Optional[Iterable[AvalancheDataset]] = \\\n            experience_generator\n        \"\"\"\n        The source of the experiences stream, as an Iterable.\n        \n        Can be a simple Sequence or a Generator.\n        \n        This field is kept for reference and debugging. The actual generator\n        is kept in the `_exp_generator` field, which stores the iterator.\n        \n        This field is None when if all the experiences have been loaded.\n        \"\"\"\n\n        self._next_exp_id: int = 0\n        \"\"\"\n        The ID of the next experience that will be generated.\n        \"\"\"\n\n        self._loaded_experiences: Dict[int, AvalancheDataset] = dict()\n        \"\"\"\n        The sequence of experiences obtained from the generator.\n        \"\"\"\n\n        self._stream_length: int = stream_length\n        \"\"\"\n        The length of the stream.\n        \"\"\"\n        try:\n            self._exp_generator: Optional[Iterator[AvalancheDataset]] = iter(\n                self._exp_source\n            )\n        except TypeError as e:\n            if callable(self._exp_source):\n                # https://stackoverflow.com/a/17092033\n                raise ValueError(\n                    'The provided generator is not iterable. When using a '\n                    'generator function based on \"yield\", remember to pass the'\n                    ' result of that function, not the '\n                    'function itself!') from None\n            raise e\n        \"\"\"\n        The experience generator, as an Iterator.\n        \n        This field is None when if all the experiences have been loaded.\n        \"\"\"\n\n        self.targets_field_sequence: Dict[int, Optional[Sequence]] = \\\n            defaultdict(lambda: None)\n        \"\"\"\n        A dictionary mapping each experience to its `targets` field.\n        \n        This dictionary contains the targets field of datasets generated up to\n        now, including the ones of dropped experiences.\n        \"\"\"\n\n        self.task_labels_field_sequence: Dict[int, Optional[Sequence[int]]] = \\\n            defaultdict(lambda: None)\n        \"\"\"\n        A dictionary mapping each experience to its `targets_task_labels` field.\n\n        This dictionary contains the task labels of datasets generated up to\n        now, including the ones of dropped experiences.\n        \"\"\"\n\n    def __len__(self) -> int:\n        \"\"\"\n        Gets the length of the stream (number of experiences).\n\n        :return: The length of the stream.\n        \"\"\"\n        return self._stream_length\n\n    def __getitem__(self, exp_idx: int) -> AvalancheDataset:\n        \"\"\"\n        Gets the dataset associated to an experience.\n\n        :param exp_idx: The ID of the experience.\n        :return: The dataset associated to the experience.\n        \"\"\"\n        exp_idx = int(exp_idx)  # Handle single element tensors\n        self.load_all_experiences(exp_idx)\n        if exp_idx not in self._loaded_experiences:\n            raise RuntimeError(f'Experience {exp_idx} has been dropped')\n\n        return self._loaded_experiences[exp_idx]\n\n    def get_experience_if_loaded(self, exp_idx: int) -> \\\n            Optional[AvalancheDataset]:\n        \"\"\"\n        Gets the dataset associated to an experience.\n\n        Differently from `__getitem__`, this will return None if the experience\n        has not been (lazily) loaded yet.\n\n        :param exp_idx: The ID of the experience.\n        :return: The dataset associated to the experience or None if the\n            experience has not been loaded yet or if it has been dropped.\n        \"\"\"\n        exp_idx = int(exp_idx)  # Handle single element tensors\n        if exp_idx >= len(self):\n            raise IndexError(f'The stream doesn\\'t contain {exp_idx+1}'\n                             f'experiences')\n\n        return self._loaded_experiences.get(exp_idx, None)\n\n    def drop_previous_experiences(self, to_exp: int) -> None:\n        \"\"\"\n        Drop the reference to experiences up to a certain experience ID\n        (inclusive).\n\n        This means that experiences with ID [0, from_exp] will be released.\n        Beware that the associated object will be valid until all the references\n        to it are dropped.\n\n        :param to_exp: The ID of the last exp to drop (inclusive). If None,\n            the whole stream will be loaded. Can be a negative number, in\n            which case this method doesn't have any effect. Can be greater\n            or equal to the stream length, in which case all currently loaded\n            experiences will be dropped.\n        :return: None\n        \"\"\"\n\n        to_exp = int(to_exp)  # Handle single element tensors\n        if to_exp < 0:\n            return\n\n        to_exp = min(to_exp, len(self)-1)\n\n        for exp_id in range(0, to_exp+1):\n            if exp_id in self._loaded_experiences:\n                del self._loaded_experiences[exp_id]\n\n    def load_all_experiences(self, to_exp: int = None) -> None:\n        \"\"\"\n        Load all experiences up to a certain experience ID (inclusive).\n\n        Beware that this won't re-load any already dropped experience.\n\n        :param to_exp: The ID of the last exp to load (inclusive). If None,\n            the whole stream will be loaded.\n        :return: None\n        \"\"\"\n        if to_exp is None:\n            to_exp = len(self) - 1\n        else:\n            to_exp = int(to_exp)  # Handle single element tensors\n\n        if to_exp >= len(self):\n            raise IndexError(f'The stream doesn\\'t contain {to_exp+1}'\n                             f'experiences')\n\n        if self._next_exp_id > to_exp:\n            # Nothing to do\n            return\n\n        for exp_id in range(self._next_exp_id, to_exp+1):\n            try:\n                generated_exp: AvalancheDataset = next(self._exp_generator)\n            except StopIteration:\n                raise RuntimeError(\n                    f'Unexpected end of stream. The generator was supposed to '\n                    f'generate {len(self)} experiences, but an error occurred '\n                    f'while generating experience {exp_id}.')\n\n            if not isinstance(generated_exp, AvalancheDataset):\n                raise ValueError(\n                    'All experience datasets must be subclasses of'\n                    ' AvalancheDataset')\n\n            self._loaded_experiences[exp_id] = generated_exp\n            self.targets_field_sequence[exp_id] = generated_exp.targets\n            self.task_labels_field_sequence[exp_id] = \\\n                generated_exp.targets_task_labels\n            self._next_exp_id += 1\n\n        if self._next_exp_id == len(self):\n            # Release all references to the generator\n            self._exp_generator = None\n            self._exp_source = None",
  "def __init__(\n            self,\n            experience_generator: Iterable[AvalancheDataset],\n            stream_length: int):\n        self._exp_source: Optional[Iterable[AvalancheDataset]] = \\\n            experience_generator\n        \"\"\"\n        The source of the experiences stream, as an Iterable.\n        \n        Can be a simple Sequence or a Generator.\n        \n        This field is kept for reference and debugging. The actual generator\n        is kept in the `_exp_generator` field, which stores the iterator.\n        \n        This field is None when if all the experiences have been loaded.\n        \"\"\"\n\n        self._next_exp_id: int = 0\n        \"\"\"\n        The ID of the next experience that will be generated.\n        \"\"\"\n\n        self._loaded_experiences: Dict[int, AvalancheDataset] = dict()\n        \"\"\"\n        The sequence of experiences obtained from the generator.\n        \"\"\"\n\n        self._stream_length: int = stream_length\n        \"\"\"\n        The length of the stream.\n        \"\"\"\n        try:\n            self._exp_generator: Optional[Iterator[AvalancheDataset]] = iter(\n                self._exp_source\n            )\n        except TypeError as e:\n            if callable(self._exp_source):\n                # https://stackoverflow.com/a/17092033\n                raise ValueError(\n                    'The provided generator is not iterable. When using a '\n                    'generator function based on \"yield\", remember to pass the'\n                    ' result of that function, not the '\n                    'function itself!') from None\n            raise e\n        \"\"\"\n        The experience generator, as an Iterator.\n        \n        This field is None when if all the experiences have been loaded.\n        \"\"\"\n\n        self.targets_field_sequence: Dict[int, Optional[Sequence]] = \\\n            defaultdict(lambda: None)\n        \"\"\"\n        A dictionary mapping each experience to its `targets` field.\n        \n        This dictionary contains the targets field of datasets generated up to\n        now, including the ones of dropped experiences.\n        \"\"\"\n\n        self.task_labels_field_sequence: Dict[int, Optional[Sequence[int]]] = \\\n            defaultdict(lambda: None)\n        \"\"\"\n        A dictionary mapping each experience to its `targets_task_labels` field.\n\n        This dictionary contains the task labels of datasets generated up to\n        now, including the ones of dropped experiences.\n        \"\"\"",
  "def __len__(self) -> int:\n        \"\"\"\n        Gets the length of the stream (number of experiences).\n\n        :return: The length of the stream.\n        \"\"\"\n        return self._stream_length",
  "def __getitem__(self, exp_idx: int) -> AvalancheDataset:\n        \"\"\"\n        Gets the dataset associated to an experience.\n\n        :param exp_idx: The ID of the experience.\n        :return: The dataset associated to the experience.\n        \"\"\"\n        exp_idx = int(exp_idx)  # Handle single element tensors\n        self.load_all_experiences(exp_idx)\n        if exp_idx not in self._loaded_experiences:\n            raise RuntimeError(f'Experience {exp_idx} has been dropped')\n\n        return self._loaded_experiences[exp_idx]",
  "def get_experience_if_loaded(self, exp_idx: int) -> \\\n            Optional[AvalancheDataset]:\n        \"\"\"\n        Gets the dataset associated to an experience.\n\n        Differently from `__getitem__`, this will return None if the experience\n        has not been (lazily) loaded yet.\n\n        :param exp_idx: The ID of the experience.\n        :return: The dataset associated to the experience or None if the\n            experience has not been loaded yet or if it has been dropped.\n        \"\"\"\n        exp_idx = int(exp_idx)  # Handle single element tensors\n        if exp_idx >= len(self):\n            raise IndexError(f'The stream doesn\\'t contain {exp_idx+1}'\n                             f'experiences')\n\n        return self._loaded_experiences.get(exp_idx, None)",
  "def drop_previous_experiences(self, to_exp: int) -> None:\n        \"\"\"\n        Drop the reference to experiences up to a certain experience ID\n        (inclusive).\n\n        This means that experiences with ID [0, from_exp] will be released.\n        Beware that the associated object will be valid until all the references\n        to it are dropped.\n\n        :param to_exp: The ID of the last exp to drop (inclusive). If None,\n            the whole stream will be loaded. Can be a negative number, in\n            which case this method doesn't have any effect. Can be greater\n            or equal to the stream length, in which case all currently loaded\n            experiences will be dropped.\n        :return: None\n        \"\"\"\n\n        to_exp = int(to_exp)  # Handle single element tensors\n        if to_exp < 0:\n            return\n\n        to_exp = min(to_exp, len(self)-1)\n\n        for exp_id in range(0, to_exp+1):\n            if exp_id in self._loaded_experiences:\n                del self._loaded_experiences[exp_id]",
  "def load_all_experiences(self, to_exp: int = None) -> None:\n        \"\"\"\n        Load all experiences up to a certain experience ID (inclusive).\n\n        Beware that this won't re-load any already dropped experience.\n\n        :param to_exp: The ID of the last exp to load (inclusive). If None,\n            the whole stream will be loaded.\n        :return: None\n        \"\"\"\n        if to_exp is None:\n            to_exp = len(self) - 1\n        else:\n            to_exp = int(to_exp)  # Handle single element tensors\n\n        if to_exp >= len(self):\n            raise IndexError(f'The stream doesn\\'t contain {to_exp+1}'\n                             f'experiences')\n\n        if self._next_exp_id > to_exp:\n            # Nothing to do\n            return\n\n        for exp_id in range(self._next_exp_id, to_exp+1):\n            try:\n                generated_exp: AvalancheDataset = next(self._exp_generator)\n            except StopIteration:\n                raise RuntimeError(\n                    f'Unexpected end of stream. The generator was supposed to '\n                    f'generate {len(self)} experiences, but an error occurred '\n                    f'while generating experience {exp_id}.')\n\n            if not isinstance(generated_exp, AvalancheDataset):\n                raise ValueError(\n                    'All experience datasets must be subclasses of'\n                    ' AvalancheDataset')\n\n            self._loaded_experiences[exp_id] = generated_exp\n            self.targets_field_sequence[exp_id] = generated_exp.targets\n            self.task_labels_field_sequence[exp_id] = \\\n                generated_exp.targets_task_labels\n            self._next_exp_id += 1\n\n        if self._next_exp_id == len(self):\n            # Release all references to the generator\n            self._exp_generator = None\n            self._exp_source = None",
  "def create_multi_dataset_generic_scenario(\n        train_dataset_list: Sequence[SupportedDataset],\n        test_dataset_list: Sequence[SupportedDataset],\n        task_labels: Sequence[int],\n        complete_test_set_only: bool = False,\n        train_transform=None, train_target_transform=None,\n        eval_transform=None, eval_target_transform=None,\n        dataset_type: AvalancheDatasetType = None) \\\n        -> GenericCLScenario:\n    \"\"\"\n    This helper function is DEPRECATED in favor of\n    `create_multi_dataset_generic_benchmark`.\n\n    Creates a generic scenario given a list of datasets and the respective task\n    labels. Each training dataset will be considered as a separate training\n    experience. Contents of the datasets will not be changed, including the\n    targets.\n\n    When loading the datasets from a set of fixed filelist, consider using\n    the :func:`create_generic_scenario_from_filelists` helper method instead.\n\n    In its base form, this function accepts a list of test datsets that must\n    contain the same amount of datasets of the training list.\n    Those pairs are then used to create the \"past\", \"cumulative\"\n    (a.k.a. growing) and \"future\" test sets. However, in certain Continual\n    Learning scenarios only the concept of \"complete\" test set makes sense. In\n    that case, the ``complete_test_set_only`` should be set to True (see the\n    parameter description for more info).\n\n    Beware that pattern transformations must already be included in the\n    datasets (when needed).\n\n    :param train_dataset_list: A list of training datasets.\n    :param test_dataset_list: A list of test datasets.\n    :param task_labels: A list of task labels. Must contain the same amount of\n        elements of the ``train_dataset_list`` parameter. For\n        Single-Incremental-Task (a.k.a. Task-Free) scenarios, this is usually\n        a list of zeros. For Multi Task scenario, this is usually a list of\n        ascending task labels (starting from 0).\n    :param complete_test_set_only: If True, only the complete test set will\n        be returned by the scenario. This means that the ``test_dataset_list``\n        parameter must be list with a single element (the complete test set).\n        Defaults to False, which means that ``train_dataset_list`` and\n        ``test_dataset_list`` must contain the same amount of datasets.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param train_target_transform: The transformation to apply to training\n        patterns targets. Defaults to None.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param eval_target_transform: The transformation to apply to test\n        patterns targets. Defaults to None.\n    :param dataset_type: The type of the dataset. Defaults to None, which\n        means that the type will be obtained from the input datasets. If input\n        datasets are not instances of :class:`AvalancheDataset`, the type\n        UNDEFINED will be used.\n\n    :returns: A :class:`GenericCLScenario` instance.\n    \"\"\"\n\n    warnings.warn('create_multi_dataset_generic_scenario is deprecated in favor'\n                  ' of create_multi_dataset_generic_benchmark.',\n                  DeprecationWarning)\n\n    transform_groups = dict(\n        train=(train_transform, train_target_transform),\n        eval=(eval_transform, eval_target_transform))\n\n    if complete_test_set_only:\n        if len(test_dataset_list) != 1:\n            raise ValueError('Test must contain 1 element when'\n                             'complete_test_set_only is True')\n    else:\n        if len(test_dataset_list) != len(train_dataset_list):\n            raise ValueError('Train and test lists must define the same '\n                             ' amount of experiences')\n\n    train_t_labels = []\n    train_dataset_list = list(train_dataset_list)\n    for dataset_idx in range(len(train_dataset_list)):\n        dataset = train_dataset_list[dataset_idx]\n        train_t_labels.append(task_labels[dataset_idx])\n        train_dataset_list[dataset_idx] = AvalancheDataset(\n            dataset,\n            task_labels=ConstantSequence(task_labels[dataset_idx],\n                                         len(dataset)),\n            transform_groups=transform_groups,\n            initial_transform_group='train',\n            dataset_type=dataset_type)\n\n    test_t_labels = []\n    test_dataset_list = list(test_dataset_list)\n    for dataset_idx in range(len(test_dataset_list)):\n        dataset = test_dataset_list[dataset_idx]\n\n        test_t_label = task_labels[dataset_idx]\n        if complete_test_set_only:\n            test_t_label = 0\n\n        test_t_labels.append(test_t_label)\n\n        test_dataset_list[dataset_idx] = AvalancheDataset(\n            dataset,\n            task_labels=ConstantSequence(test_t_label,\n                                         len(dataset)),\n            transform_groups=transform_groups,\n            initial_transform_group='eval',\n            dataset_type=dataset_type)\n\n    return GenericCLScenario(\n        stream_definitions={\n            'train': (train_dataset_list, train_t_labels),\n            'test': (test_dataset_list, test_t_labels)\n        },\n        complete_test_set_only=complete_test_set_only)",
  "def create_generic_scenario_from_filelists(\n        root: Union[str, Path],\n        train_file_lists: Sequence[Union[str, Path]],\n        test_file_lists: Union[Union[str, Path], Sequence[Union[str, Path]]],\n        task_labels: Sequence[int],\n        complete_test_set_only: bool = False,\n        train_transform=None, train_target_transform=None,\n        eval_transform=None, eval_target_transform=None) -> GenericCLScenario:\n    \"\"\"\n    This helper function is DEPRECATED in favor of\n    `create_generic_benchmark_from_filelists`.\n\n    Creates a generic scenario given a list of filelists and the respective task\n    labels. A separate dataset will be created for each filelist and each of\n    those training datasets will be considered a separate training experience.\n\n    In its base form, this function accepts a list of filelists for the test\n    datsets that must contain the same amount of elements of the training list.\n    Those pairs of datasets are then used to create the \"past\", \"cumulative\"\n    (a.k.a. growing) and \"future\" test sets. However, in certain Continual\n    Learning scenarios only the concept of \"complete\" test set makes sense. In\n    that case, the ``complete_test_set_only`` should be set to True (see the\n    parameter description for more info).\n\n    This helper functions is the best shot when loading Caffe-style dataset\n    based on filelists.\n\n    The resulting benchmark instance and the intermediate datasets used to\n    populate it will be of type CLASSIFICATION.\n\n    :param root: The root path of the dataset.\n    :param train_file_lists: A list of filelists describing the\n        paths of the training patterns for each experience.\n    :param test_file_lists: A list of filelists describing the\n        paths of the test patterns for each experience.\n    :param task_labels: A list of task labels. Must contain the same amount of\n        elements of the ``train_file_lists`` parameter. For\n        Single-Incremental-Task (a.k.a. Task-Free) scenarios, this is usually\n        a list of zeros. For Multi Task scenario, this is usually a list of\n        ascending task labels (starting from 0).\n    :param complete_test_set_only: If True, only the complete test set will\n        be returned by the scenario. This means that the ``test_file_lists``\n        parameter must be list with a single element (the complete test set).\n        Alternatively, can be a plain string or :class:`Path` object.\n        Defaults to False, which means that ``train_file_lists`` and\n        ``test_file_lists`` must contain the same amount of filelists paths.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param train_target_transform: The transformation to apply to training\n        patterns targets. Defaults to None.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param eval_target_transform: The transformation to apply to test\n        patterns targets. Defaults to None.\n\n    :returns: A :class:`GenericCLScenario` instance.\n    \"\"\"\n\n    warnings.warn('create_generic_scenario_from_filelists is deprecated in '\n                  'favor of create_generic_benchmark_from_filelists.',\n                  DeprecationWarning)\n\n    train_datasets, test_dataset = datasets_from_filelists(\n        root, train_file_lists, test_file_lists,\n        complete_test_set_only=complete_test_set_only)\n\n    return create_multi_dataset_generic_scenario(\n        train_datasets, test_dataset, task_labels,\n        train_transform=train_transform,\n        train_target_transform=train_target_transform,\n        eval_transform=eval_transform,\n        eval_target_transform=eval_target_transform,\n        complete_test_set_only=complete_test_set_only,\n        dataset_type=AvalancheDatasetType.CLASSIFICATION)",
  "def create_generic_scenario_from_paths(\n        train_list_of_files: Sequence[Sequence[FileAndLabel]],\n        test_list_of_files: Union[Sequence[FileAndLabel],\n                                  Sequence[Sequence[FileAndLabel]]],\n        task_labels: Sequence[int],\n        complete_test_set_only: bool = False,\n        train_transform=None, train_target_transform=None,\n        eval_transform=None, eval_target_transform=None,\n        dataset_type: AvalancheDatasetType = AvalancheDatasetType.UNDEFINED) \\\n        -> GenericCLScenario:\n    \"\"\"\n    This helper function is DEPRECATED in favor of\n    `create_generic_benchmark_from_paths`.\n\n    Creates a generic scenario given a sequence of lists of files. A separate\n    dataset will be created for each list. Each of those training datasets\n    will be considered a separate training experience.\n\n    This is very similar to `create_generic_scenario_from_filelists`, with the\n    main difference being that `create_generic_scenario_from_filelists`\n    accepts, for each experience, a file list formatted in Caffe-style.\n    On the contrary, this accepts a list of tuples where each tuple contains\n    two elements: the full path to the pattern and its label.\n    Optionally, the tuple may contain a third element describing the bounding\n    box of the element to crop. This last bounding box may be useful when trying\n    to extract the part of the image depicting the desired element.\n\n    In its base form, this function accepts a list for the test datasets that\n    must contain the same amount of elements of the training list.\n    Those pairs of datasets are then used to create the \"past\", \"cumulative\"\n    (a.k.a. growing) and \"future\" test sets. However, in certain Continual\n    Learning scenarios only the concept of \"complete\" test set makes sense. In\n    that case, the ``complete_test_set_only`` should be set to True (see the\n    parameter description for more info).\n\n    The label of each pattern doesn't have to be an int.\n\n    :param train_list_of_files: A list of lists. Each list describes the paths\n        and labels of patterns to include in that training experience, as\n        tuples. Each tuple must contain two elements: the full path to the\n        pattern and its class label. Optionally, the tuple may contain a\n        third element describing the bounding box to use for cropping (top,\n        left, height, width).\n    :param test_list_of_files: A list of lists. Each list describes the paths\n        and labels of patterns to include in that test experience, as tuples.\n        Each tuple must contain two elements: the full path to the pattern\n        and its class label. Optionally, the tuple may contain a third element\n        describing the bounding box to use for cropping (top, left, height,\n        width).\n    :param task_labels: A list of task labels. Must contain the same amount of\n        elements of the ``train_file_lists`` parameter. For\n        Single-Incremental-Task (a.k.a. Task-Free) scenarios, this is usually\n        a list of zeros. For Multi Task scenario, this is usually a list of\n        ascending task labels (starting from 0).\n    :param complete_test_set_only: If True, only the complete test set will\n        be returned by the scenario. This means that the ``test_list_of_files``\n        parameter must define a single experience (the complete test set).\n        Defaults to False, which means that ``train_list_of_files`` and\n        ``test_list_of_files`` must contain the same amount of paths.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param train_target_transform: The transformation to apply to training\n        patterns targets. Defaults to None.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param eval_target_transform: The transformation to apply to test\n        patterns targets. Defaults to None.\n    :param dataset_type: The type of the dataset. Defaults to UNDEFINED.\n\n    :returns: A :class:`GenericCLScenario` instance.\n    \"\"\"\n\n    warnings.warn('create_generic_scenario_from_paths is deprecated in favor'\n                  ' of create_generic_benchmark_from_paths.',\n                  DeprecationWarning)\n\n    train_datasets, test_dataset = datasets_from_paths(\n        train_list_of_files, test_list_of_files,\n        complete_test_set_only=complete_test_set_only)\n\n    return create_multi_dataset_generic_scenario(\n        train_datasets, test_dataset, task_labels,\n        train_transform=train_transform,\n        train_target_transform=train_target_transform,\n        eval_transform=eval_transform,\n        eval_target_transform=eval_target_transform,\n        complete_test_set_only=complete_test_set_only,\n        dataset_type=dataset_type)",
  "def create_generic_scenario_from_tensor_lists(\n        train_tensors: Sequence[Sequence[Any]],\n        test_tensors: Sequence[Sequence[Any]],\n        task_labels: Sequence[int],\n        *,\n        complete_test_set_only: bool = False,\n        train_transform=None, train_target_transform=None,\n        eval_transform=None, eval_target_transform=None,\n        dataset_type: AvalancheDatasetType = None) -> GenericCLScenario:\n    \"\"\"\n    This helper function is DEPRECATED in favor of\n    `create_generic_benchmark_from_tensor_lists`.\n\n    Creates a generic scenario given lists of Tensors. A separate dataset will\n    be created from each Tensor tuple (x, y, z, ...) and each of those training\n    datasets will be considered a separate training experience. Using this\n    helper function is the lowest-level way to create a Continual Learning\n    scenario. When possible, consider using higher level helpers.\n\n    Experiences are defined by passing lists of tensors as the `train_tensors`\n    and `test_tensors` parameter. Those parameters must be lists containing\n    sub-lists of tensors, one for each experience. Each tensor defines the value\n    of a feature (\"x\", \"y\", \"z\", ...) for all patterns of that experience.\n\n    By default the second tensor of each experience will be used to fill the\n    `targets` value (label of each pattern).\n\n    In its base form, the test lists must contain the same amount of elements of\n    the training lists. Those pairs of datasets are then used to create the\n    \"past\", \"cumulative\" (a.k.a. growing) and \"future\" test sets.\n    However, in certain Continual Learning scenarios only the concept of\n    \"complete\" test set makes sense. In that case, the\n    ``complete_test_set_only`` should be set to True (see the parameter\n    description for more info).\n\n    :param train_tensors: A list of lists. The first list must contain the\n        tensors for the first training experience (one tensor per feature), the\n        second list must contain the tensors for the second training experience,\n        and so on.\n    :param test_tensors: A list of lists. The first list must contain the\n        tensors for the first test experience (one tensor per feature), the\n        second list must contain the tensors for the second test experience,\n        and so on. When using `complete_test_set_only`, this parameter\n        must be a list containing a single sub-list for the single test\n        experience.\n    :param task_labels: A list of task labels. Must contain a task label for\n        each experience. For Single-Incremental-Task (a.k.a. Task-Free)\n        scenarios, this is usually a list of zeros. For Multi Task scenario,\n        this is usually a list of ascending task labels (starting from 0).\n    :param complete_test_set_only: If True, only the complete test set will\n        be returned by the scenario. This means that ``test_tensors`` must\n        define a single experience. Defaults to False, which means that\n        ``train_tensors`` and ``test_tensors`` must define the same\n        amount of experiences.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param train_target_transform: The transformation to apply to training\n        patterns targets. Defaults to None.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param eval_target_transform: The transformation to apply to test\n        patterns targets. Defaults to None.\n    :param dataset_type: The type of the dataset. Defaults to None, which\n        means that the type will be obtained from the input datasets. If input\n        datasets are not instances of :class:`AvalancheDataset`, the type\n        UNDEFINED will be used.\n\n    :returns: A :class:`GenericCLScenario` instance.\n    \"\"\"\n\n    warnings.warn('create_generic_scenario_from_tensor_lists is deprecated in '\n                  'favor of create_generic_benchmark_from_tensor_lists.',\n                  DeprecationWarning)\n\n    train_datasets = [\n        AvalancheTensorDataset(*exp_tensors, dataset_type=dataset_type)\n        for exp_tensors in train_tensors]\n\n    test_datasets = [\n        AvalancheTensorDataset(*exp_tensors, dataset_type=dataset_type)\n        for exp_tensors in test_tensors]\n\n    return create_multi_dataset_generic_scenario(\n        train_datasets, test_datasets, task_labels,\n        train_transform=train_transform,\n        train_target_transform=train_target_transform,\n        eval_transform=eval_transform,\n        eval_target_transform=eval_target_transform,\n        complete_test_set_only=complete_test_set_only,\n        dataset_type=dataset_type)",
  "def create_generic_scenario_from_tensors(\n        train_data_x: Sequence[Any],\n        train_data_y: Sequence[Sequence[SupportsInt]],\n        test_data_x: Union[Any, Sequence[Any]],\n        test_data_y: Union[Any, Sequence[Sequence[SupportsInt]]],\n        task_labels: Sequence[int],\n        complete_test_set_only: bool = False,\n        train_transform=None, train_target_transform=None,\n        eval_transform=None, eval_target_transform=None,\n        dataset_type: AvalancheDatasetType = AvalancheDatasetType.UNDEFINED) \\\n        -> GenericCLScenario:\n    \"\"\"\n    This helper function is DEPRECATED in favor of\n    `create_generic_benchmark_from_tensor_lists`.\n\n    Please consider using :func:`create_generic_scenario_from_tensor_lists`\n    instead. When switching to the new function, please keep in mind that the\n    format of the parameters is completely different!\n\n    Creates a generic scenario given lists of Tensors and the respective task\n    labels. A separate dataset will be created from each Tensor pair (x + y)\n    and each of those training datasets will be considered a separate\n    training experience. Contents of the datasets will not be changed, including\n    the targets. Using this helper function is the lower level way to create a\n    Continual Learning scenario. When possible, consider using higher level\n    helpers.\n\n    By default the second tensor of each experience will be used to fill the\n    `targets` value (label of each pattern).\n\n    In its base form, the test lists must contain the same amount of elements of\n    the training lists. Those pairs of datasets are then used to create the\n    \"past\", \"cumulative\" (a.k.a. growing) and \"future\" test sets.\n    However, in certain Continual Learning scenarios only the concept of\n    \"complete\" test set makes sense. In that case, the\n    ``complete_test_set_only`` should be set to True (see the parameter\n    description for more info).\n\n    :param train_data_x: A list of Tensors (one per experience) containing the\n        patterns of the training sets.\n    :param train_data_y: A list of Tensors or int lists containing the\n        labels of the patterns of the training sets. Must contain the same\n        number of elements of ``train_datasets_x``.\n    :param test_data_x: A Tensor or a list of Tensors (one per experience)\n        containing the patterns of the test sets.\n    :param test_data_y: A Tensor or a list of Tensors or int lists containing\n        the labels of the patterns of the test sets. Must contain the same\n        number of elements of ``test_datasets_x``.\n    :param task_labels: A list of task labels. Must contain the same amount of\n        elements of the ``train_datasets_x`` parameter. For\n        Single-Incremental-Task (a.k.a. Task-Free) scenarios, this is usually\n        a list of zeros. For Multi Task scenario, this is usually a list of\n        ascending task labels (starting from 0).\n    :param complete_test_set_only: If True, only the complete test set will\n        be returned by the scenario. This means that ``test_data_x`` and\n        ``test_data_y`` must define a single experience. Defaults to False,\n        which means that ``train_data_*`` and ``test_data_*`` must define the\n        same amount of experiences.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param train_target_transform: The transformation to apply to training\n        patterns targets. Defaults to None.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param eval_target_transform: The transformation to apply to test\n        patterns targets. Defaults to None.\n    :param dataset_type: The type of the dataset. Defaults to UNDEFINED.\n\n    :returns: A :class:`GenericCLScenario` instance.\n    \"\"\"\n\n    warnings.warn('create_generic_scenario_from_tensors is deprecated in favor '\n                  'of create_generic_benchmark_from_tensor_lists.',\n                  DeprecationWarning)\n\n    if len(train_data_x) != len(train_data_y):\n        raise ValueError('train_data_x and train_data_y must contain'\n                         ' the same amount of elements')\n\n    if type(test_data_x) != type(test_data_y):\n        raise ValueError('test_data_x and test_data_y must be of'\n                         ' the same type')\n\n    if isinstance(test_data_x, Tensor):\n        test_data_x = [test_data_x]\n        test_data_y = [test_data_y]\n    else:\n        if len(test_data_x) != len(test_data_y):\n            raise ValueError('test_data_x and test_data_y must contain'\n                             ' the same amount of elements')\n\n    exp_train_first_structure = []\n    exp_test_first_structure = []\n    for exp_idx in range(len(train_data_x)):\n        exp_x = train_data_x[exp_idx]\n        exp_y = train_data_y[exp_idx]\n\n        exp_train_first_structure.append([exp_x, exp_y])\n\n    for exp_idx in range(len(test_data_x)):\n        exp_x = test_data_x[exp_idx]\n        exp_y = test_data_y[exp_idx]\n\n        exp_test_first_structure.append([exp_x, exp_y])\n\n    return create_generic_scenario_from_tensor_lists(\n        train_tensors=exp_train_first_structure,\n        test_tensors=exp_test_first_structure,\n        task_labels=task_labels,\n        complete_test_set_only=complete_test_set_only,\n        train_transform=train_transform,\n        train_target_transform=train_target_transform,\n        eval_transform=eval_transform,\n        eval_target_transform=eval_target_transform,\n        dataset_type=dataset_type)",
  "def _indexes_grouped_by_classes(sequence: Sequence[SupportsInt],\n                                search_elements: Union[None, Sequence[int]],\n                                sort_indexes: bool = True,\n                                sort_classes: bool = True) \\\n        -> Union[List[int], None]:\n    result_per_class: Dict[int, List[int]] = OrderedDict()\n    result: List[int] = []\n\n    # tensor_as_list() handles the situation in which sequence and\n    # search_elements are a torch.Tensor\n    #\n    # Without the tensor_as_list conversion:\n    # result_per_class[element].append(idx) -> error\n    # because result_per_class[0] won't exist (result_per_class[tensor(0)] will)\n    if search_elements is not None:\n        search_elements = tensor_as_list(search_elements)\n    sequence = tensor_as_list(sequence)\n\n    if sort_classes:\n        if search_elements is None:\n            search_elements = torch.unique(torch.as_tensor(sequence)).tolist()\n\n        # Consider that result_per_class is an OrderedDict\n        # This means that, if sort_classes is True, the next for statement\n        # will initialize the \"result_per_class\" in sorted order ->\n        # -> patterns will be ordered by ascending class ID\n        search_elements = sorted(search_elements)\n\n    for search_element in search_elements:\n        result_per_class[search_element] = []\n\n    # Set based \"in\" operator is **much** faster that its list counterpart!\n    search_elements_set = set()\n    if search_elements is not None:\n        search_elements_set = set(search_elements)\n\n    # Stores each pattern index in the appropriate class list\n    for idx, element in enumerate(sequence):\n        if search_elements is None or element in search_elements_set:\n            result_per_class[element].append(idx)\n\n    # Concatenate all the pattern indexes\n    for search_element in search_elements:\n        if sort_indexes:\n            result_per_class[search_element].sort()\n        result.extend(result_per_class[search_element])\n\n    if result == sequence:\n        # The resulting index order is the same as the input one\n        # Return None to flag that the whole sequence can be\n        # taken as it already is\n        return None\n\n    return result",
  "def _indexes_without_grouping(sequence: Sequence[SupportsInt],\n                              search_elements: Union[None, Sequence[int]],\n                              sort_indexes: bool = False) \\\n        -> Union[List[int], None]:\n    sequence = tensor_as_list(sequence)\n\n    if search_elements is None and not sort_indexes:\n        # No-op\n        return sequence\n\n    if search_elements is not None:\n        search_elements = tensor_as_list(search_elements)\n\n    result: List[int]\n    if search_elements is None:\n        result = list(sequence)\n    else:\n        # Set based \"in\" operator is **much** faster that its list counterpart!\n        search_elements = set(search_elements)\n        result = []\n        for idx, element in enumerate(sequence):\n            if element in search_elements:\n                result.append(idx)\n\n    if sort_indexes:\n        result.sort()\n    elif not sort_indexes and len(result) == len(sequence):\n        # All patterns selected. Also, no sorting is required\n        # Return None to flag that the whole sequence can be\n        # taken as it already is\n        return None\n    return result",
  "def _indexes_from_set(sequence: Sequence[SupportsInt],\n                      search_elements: Union[Sequence[int], None],\n                      bucket_classes: bool = True,\n                      sort_classes: bool = False, sort_indexes: bool = False) \\\n        -> Union[List[int], None]:\n    \"\"\"\n    Given the target list of a dataset, returns the indexes of patterns\n    belonging to classes listed in the search_elements parameter.\n\n    :param sequence: The list of pattern targets, as a list.\n    :param search_elements: A list of classes used to filter the dataset\n        patterns. Patterns belonging to one of those classes will be included.\n        If None, all patterns will be included.\n    :param bucket_classes: If True, pattern indexes will be returned so that\n        patterns will be grouped by class. Defaults to True.\n    :param sort_classes: If both ``bucket_classes`` and ``sort_classes`` are\n        True, class groups will be sorted by class index. Ignored if\n        ``bucket_classes`` is False. Defaults to False.\n    :param sort_indexes: If True, patterns indexes will be sorted. When\n        bucketing by class, patterns will be sorted inside their buckets.\n        Defaults to False.\n\n    :returns: The indexes of patterns belonging to the required classes,\n        as a list. Can return None, which means that the original pattern\n        sequence already satisfies all the constraints.\n    \"\"\"\n    if bucket_classes:\n        return _indexes_grouped_by_classes(sequence, search_elements,\n                                           sort_indexes=sort_indexes,\n                                           sort_classes=sort_classes)\n\n    return _indexes_without_grouping(sequence, search_elements,\n                                     sort_indexes=sort_indexes)",
  "def make_nc_transformation_subset(dataset: SupportedDataset,\n                                  transform: Any, target_transform: Any,\n                                  classes: Union[None, Sequence[int]],\n                                  bucket_classes: bool = False,\n                                  sort_classes: bool = False,\n                                  sort_indexes: bool = False) \\\n        -> AvalancheSubset:\n    \"\"\"\n    Creates a subset given the list of classes the patterns should belong to.\n\n    :param dataset: The original dataset\n    :param transform: The transform function for patterns. Can be None.\n    :param target_transform: The transform function for targets. Can be None.\n    :param classes: A list of classes used to filter the dataset patterns.\n        Patterns belonging to one of those classes will be included. If None,\n        all patterns will be included.\n    :param bucket_classes: If True, the final Dataset will output patterns by\n        grouping them by class. Defaults to True.\n    :param sort_classes: If ``bucket_classes`` and ``sort_classes`` are both\n        True, the final Dataset will output patterns by grouping them by class\n        and the class groups will be ordered by class ID (ascending). Ignored\n        if ``bucket_classes`` is False. Defaults to False.\n    :param sort_indexes: If True, pattern indexes will be sorted (ascending).\n        When grouping by class, patterns will be sorted inside their respective\n        class buckets. Defaults to False.\n\n    :returns: A :class:`TransformationSubset` that includes only patterns\n        belonging to the given classes, in the order controlled by the\n        ``bucket_classes``, ``sort_classes`` and ``sort_indexes`` parameters.\n    \"\"\"\n    return AvalancheSubset(\n        dataset,\n        indices=_indexes_from_set(dataset.targets, classes,\n                                  bucket_classes=bucket_classes,\n                                  sort_classes=sort_classes,\n                                  sort_indexes=sort_indexes),\n        transform=transform,\n        target_transform=target_transform)",
  "class NCScenario(GenericCLScenario['NCExperience']):\n    \"\"\"\n    This class defines a \"New Classes\" scenario. Once created, an instance\n    of this class can be iterated in order to obtain the experience sequence\n    under the form of instances of :class:`NCExperience`.\n\n    This class can be used directly. However, we recommend using facilities like\n    :func:`avalanche.benchmarks.generators.nc_benchmark`.\n    \"\"\"\n\n    def __init__(self, train_dataset: AvalancheDataset,\n                 test_dataset: AvalancheDataset,\n                 n_experiences: int,\n                 task_labels: bool,\n                 shuffle: bool = True,\n                 seed: Optional[int] = None,\n                 fixed_class_order: Optional[Sequence[int]] = None,\n                 per_experience_classes: Optional[Dict[int, int]] = None,\n                 class_ids_from_zero_from_first_exp: bool = False,\n                 class_ids_from_zero_in_each_exp: bool = False,\n                 reproducibility_data: Optional[Dict[str, Any]] = None):\n        \"\"\"\n        Creates a ``NCGenericScenario`` instance given the training and test\n        Datasets and the number of experiences.\n\n        By default, the number of classes will be automatically detected by\n        looking at the training Dataset ``targets`` field. Classes will be\n        uniformly distributed across ``n_experiences`` unless a\n        ``per_experience_classes`` argument is specified.\n\n        The number of classes must be divisible without remainder by the number\n        of experiences. This also applies when the ``per_experience_classes``\n        argument is not None.\n\n        :param train_dataset: The training dataset. The dataset must be a\n            subclass of :class:`AvalancheDataset`. For instance, one can\n            use the datasets from the torchvision package like that:\n            ``train_dataset=AvalancheDataset(torchvision_dataset)``.\n        :param test_dataset: The test dataset. The dataset must be a\n            subclass of :class:`AvalancheDataset`. For instance, one can\n            use the datasets from the torchvision package like that:\n            ``test_dataset=AvalancheDataset(torchvision_dataset)``.\n        :param n_experiences: The number of experiences.\n        :param task_labels: If True, each experience will have an ascending task\n            label. If False, the task label will be 0 for all the experiences.\n        :param shuffle: If True, the class order will be shuffled. Defaults to\n            True.\n        :param seed: If shuffle is True and seed is not None, the class order\n            will be shuffled according to the seed. When None, the current\n            PyTorch random number generator state will be used.\n            Defaults to None.\n        :param fixed_class_order: If not None, the class order to use (overrides\n            the shuffle argument). Very useful for enhancing\n            reproducibility. Defaults to None.\n        :param per_experience_classes: Is not None, a dictionary whose keys are\n            (0-indexed) experience IDs and their values are the number of\n            classes to include in the respective experiences. The dictionary\n            doesn't have to contain a key for each experience! All the remaining\n            experiences will contain an equal amount of the remaining classes.\n            The remaining number of classes must be divisible without remainder\n            by the remaining number of experiences. For instance,\n            if you want to include 50 classes in the first experience\n            while equally distributing remaining classes across remaining\n            experiences, just pass the \"{0: 50}\" dictionary as the\n            per_experience_classes parameter. Defaults to None.\n        :param class_ids_from_zero_from_first_exp: If True, original class IDs\n            will be remapped so that they will appear as having an ascending\n            order. For instance, if the resulting class order after shuffling\n            (or defined by fixed_class_order) is [23, 34, 11, 7, 6, ...] and\n            class_ids_from_zero_from_first_exp is True, then all the patterns\n            belonging to class 23 will appear as belonging to class \"0\",\n            class \"34\" will be mapped to \"1\", class \"11\" to \"2\" and so on.\n            This is very useful when drawing confusion matrices and when dealing\n            with algorithms with dynamic head expansion. Defaults to False.\n            Mutually exclusive with the ``class_ids_from_zero_in_each_exp``\n            parameter.\n        :param class_ids_from_zero_in_each_exp: If True, original class IDs\n            will be mapped to range [0, n_classes_in_exp) for each experience.\n            Defaults to False. Mutually exclusive with the\n            ``class_ids_from_zero_from_first_exp parameter``.\n        :param reproducibility_data: If not None, overrides all the other\n            scenario definition options. This is usually a dictionary containing\n            data used to reproduce a specific experiment. One can use the\n            ``get_reproducibility_data`` method to get (and even distribute)\n            the experiment setup so that it can be loaded by passing it as this\n            parameter. In this way one can be sure that the same specific\n            experimental setup is being used (for reproducibility purposes).\n            Beware that, in order to reproduce an experiment, the same train and\n            test datasets must be used. Defaults to None.\n        \"\"\"\n        if class_ids_from_zero_from_first_exp and \\\n                class_ids_from_zero_in_each_exp:\n            raise ValueError('Invalid mutually exclusive options '\n                             'class_ids_from_zero_from_first_exp and '\n                             'class_ids_from_zero_in_each_exp set at the '\n                             'same time')\n        if reproducibility_data:\n            n_experiences = reproducibility_data['n_experiences']\n\n        if n_experiences < 1:\n            raise ValueError('Invalid number of experiences (n_experiences '\n                             'parameter): must be greater than 0')\n\n        self.classes_order: List[int] = []\n        \"\"\" Stores the class order (remapped class IDs). \"\"\"\n\n        self.classes_order_original_ids: List[int] = torch.unique(\n            torch.as_tensor(train_dataset.targets),\n            sorted=True).tolist()\n        \"\"\" Stores the class order (original class IDs) \"\"\"\n\n        n_original_classes = max(self.classes_order_original_ids) + 1\n\n        self.class_mapping: List[int] = []\n        \"\"\"\n        class_mapping stores the class mapping so that \n        `mapped_class_id = class_mapping[original_class_id]`. \n        \n        If the benchmark is created with an amount of classes which is less than\n        the amount of all classes in the dataset, then class_mapping will \n        contain some -1 values corresponding to ignored classes. This can\n        happen when passing a fixed class order to the constructor.\n        \"\"\"\n\n        self.n_classes_per_exp: List[int] = []\n        \"\"\" A list that, for each experience (identified by its index/ID),\n            stores the number of classes assigned to that experience. \"\"\"\n\n        self._classes_in_exp: List[Set[int]] = []\n\n        self.original_classes_in_exp: List[Set[int]] = []\n        \"\"\"\n        A list that, for each experience (identified by its index/ID), stores a \n        set of the original IDs of classes assigned to that experience. \n        This field applies to both train and test streams.\n        \"\"\"\n\n        self.class_ids_from_zero_from_first_exp: bool = \\\n            class_ids_from_zero_from_first_exp\n        \"\"\" If True the class IDs have been remapped to start from zero. \"\"\"\n\n        self.class_ids_from_zero_in_each_exp: bool = \\\n            class_ids_from_zero_in_each_exp\n        \"\"\" If True the class IDs have been remapped to start from zero in \n        each experience \"\"\"\n\n        # Note: if fixed_class_order is None and shuffle is False,\n        # the class order will be the one encountered\n        # By looking at the train_dataset targets field\n        if reproducibility_data:\n            self.classes_order_original_ids = \\\n                reproducibility_data['classes_order_original_ids']\n            self.class_ids_from_zero_from_first_exp = \\\n                reproducibility_data['class_ids_from_zero_from_first_exp']\n            self.class_ids_from_zero_in_each_exp = \\\n                reproducibility_data['class_ids_from_zero_in_each_exp']\n        elif fixed_class_order is not None:\n            # User defined class order -> just use it\n            if len(set(self.classes_order_original_ids).union(\n                    set(fixed_class_order))) != \\\n                    len(self.classes_order_original_ids):\n                raise ValueError('Invalid classes defined in fixed_class_order')\n\n            self.classes_order_original_ids = list(fixed_class_order)\n        elif shuffle:\n            # No user defined class order.\n            # If a seed is defined, set the random number generator seed.\n            # If no seed has been defined, use the actual\n            # random number generator state.\n            # Finally, shuffle the class list to obtain a random classes\n            # order\n            if seed is not None:\n                torch.random.manual_seed(seed)\n            self.classes_order_original_ids = \\\n                torch.as_tensor(self.classes_order_original_ids)[\n                    torch.randperm(len(self.classes_order_original_ids))\n                ].tolist()\n\n        self.n_classes: int = len(self.classes_order_original_ids)\n        \"\"\" The number of classes \"\"\"\n\n        if reproducibility_data:\n            self.n_classes_per_exp = \\\n                reproducibility_data['n_classes_per_exp']\n        elif per_experience_classes is not None:\n            # per_experience_classes is a user-defined dictionary that defines\n            # the number of classes to include in some (or all) experiences.\n            # Remaining classes are equally distributed across the other\n            # experiences.\n            #\n            # Format of per_experience_classes dictionary:\n            #   - key = experience id\n            #   - value = number of classes for this experience\n\n            if max(per_experience_classes.keys()) >= n_experiences or min(\n                    per_experience_classes.keys()) < 0:\n                # The dictionary contains a key (that is, a experience id) >=\n                # the number of requested experiences... or < 0\n                raise ValueError(\n                    'Invalid experience id in per_experience_classes parameter:'\n                    ' experience ids must be in range [0, n_experiences)')\n            if min(per_experience_classes.values()) < 0:\n                # One or more values (number of classes for each experience) < 0\n                raise ValueError('Wrong number of classes defined for one or '\n                                 'more experiences: must be a non-negative '\n                                 'value')\n\n            if sum(per_experience_classes.values()) > self.n_classes:\n                # The sum of dictionary values (n. of classes for each\n                # experience) >= the number of classes\n                raise ValueError('Insufficient number of classes: '\n                                 'per_experience_classes parameter can\\'t '\n                                 'be satisfied')\n\n            # Remaining classes are equally distributed across remaining\n            # experiences. This amount of classes must be be divisible without\n            # remainder by the number of remaining experiences\n            remaining_exps = n_experiences - len(per_experience_classes)\n            if remaining_exps > 0 and (self.n_classes - sum(\n                    per_experience_classes.values())) % remaining_exps > 0:\n                raise ValueError('Invalid number of experiences: remaining '\n                                 'classes cannot be divided by n_experiences')\n\n            # default_per_exp_classes is the default amount of classes\n            # for the remaining experiences\n            if remaining_exps > 0:\n                default_per_exp_classes = (self.n_classes - sum(\n                    per_experience_classes.values())) // remaining_exps\n            else:\n                default_per_exp_classes = 0\n\n            # Initialize the self.n_classes_per_exp list using\n            # \"default_per_exp_classes\" as the default\n            # amount of classes per experience. Then, loop through the\n            # per_experience_classes dictionary to set the customized,\n            # user defined, classes for the required experiences.\n            self.n_classes_per_exp = \\\n                [default_per_exp_classes] * n_experiences\n            for exp_id in per_experience_classes:\n                self.n_classes_per_exp[exp_id] = per_experience_classes[\n                    exp_id]\n        else:\n            # Classes will be equally distributed across the experiences\n            # The amount of classes must be be divisible without remainder\n            # by the number of experiences\n            if self.n_classes % n_experiences > 0:\n                raise ValueError(\n                    f'Invalid number of experiences: classes contained in '\n                    f'dataset ({self.n_classes}) cannot be divided by '\n                    f'n_experiences ({n_experiences})')\n            self.n_classes_per_exp = \\\n                [self.n_classes // n_experiences] * n_experiences\n\n        # Before populating the classes_in_experience list,\n        # define the remapped class IDs.\n        if reproducibility_data:\n            # Method 0: use reproducibility data\n            self.classes_order = reproducibility_data['classes_order']\n            self.class_mapping = reproducibility_data['class_mapping']\n        elif self.class_ids_from_zero_from_first_exp:\n            # Method 1: remap class IDs so that they appear in ascending order\n            # over all experiences\n            self.classes_order = list(range(0, self.n_classes))\n            self.class_mapping = [-1] * n_original_classes\n            for class_id in range(n_original_classes):\n                # This check is needed because, when a fixed class order is\n                # used, the user may have defined an amount of classes less than\n                # the overall amount of classes in the dataset.\n                if class_id in self.classes_order_original_ids:\n                    self.class_mapping[class_id] = \\\n                        self.classes_order_original_ids.index(class_id)\n        elif self.class_ids_from_zero_in_each_exp:\n            # Method 2: remap class IDs so that they appear in range [0, N] in\n            # each experience\n            self.classes_order = []\n            self.class_mapping = [-1] * n_original_classes\n            next_class_idx = 0\n            for exp_id, exp_n_classes in enumerate(self.n_classes_per_exp):\n                self.classes_order += list(range(exp_n_classes))\n                for exp_class_idx in range(exp_n_classes):\n                    original_class_position = next_class_idx + exp_class_idx\n                    original_class_id = self.classes_order_original_ids[\n                        original_class_position]\n                    self.class_mapping[original_class_id] = exp_class_idx\n                next_class_idx += exp_n_classes\n        else:\n            # Method 3: no remapping of any kind\n            # remapped_id = class_mapping[class_id] -> class_id == remapped_id\n            self.classes_order = self.classes_order_original_ids\n            self.class_mapping = list(range(0, n_original_classes))\n\n        original_training_dataset = train_dataset\n        original_test_dataset = test_dataset\n\n        # Populate the _classes_in_exp and original_classes_in_exp lists\n        # \"_classes_in_exp[exp_id]\": list of (remapped) class IDs assigned\n        # to experience \"exp_id\"\n        # \"original_classes_in_exp[exp_id]\": list of original class IDs\n        # assigned to experience \"exp_id\"\n        for exp_id in range(n_experiences):\n            classes_start_idx = sum(self.n_classes_per_exp[:exp_id])\n            classes_end_idx = classes_start_idx + self.n_classes_per_exp[\n                exp_id]\n\n            self._classes_in_exp.append(\n                set(self.classes_order[classes_start_idx:classes_end_idx]))\n            self.original_classes_in_exp.append(\n                set(self.classes_order_original_ids[classes_start_idx:\n                                                    classes_end_idx]))\n\n        # Finally, create the experience -> patterns assignment.\n        # In order to do this, we don't load all the patterns\n        # instead we use the targets field.\n        train_exps_patterns_assignment = []\n        test_exps_patterns_assignment = []\n\n        self._has_task_labels = task_labels\n        if reproducibility_data is not None:\n            self._has_task_labels = bool(\n                reproducibility_data['has_task_labels'])\n\n        if self._has_task_labels:\n            pattern_train_task_labels = [-1] * len(train_dataset)\n            pattern_test_task_labels = [-1] * len(test_dataset)\n        else:\n            pattern_train_task_labels = ConstantSequence(0, len(train_dataset))\n            pattern_test_task_labels = ConstantSequence(0, len(test_dataset))\n\n        for exp_id in range(n_experiences):\n            selected_classes = self.original_classes_in_exp[exp_id]\n            selected_indexes_train = []\n            for idx, element in enumerate(original_training_dataset.targets):\n                if element in selected_classes:\n                    selected_indexes_train.append(idx)\n                    if self._has_task_labels:\n                        pattern_train_task_labels[idx] = exp_id\n\n            selected_indexes_test = []\n            for idx, element in enumerate(original_test_dataset.targets):\n                if element in selected_classes:\n                    selected_indexes_test.append(idx)\n                    if self._has_task_labels:\n                        pattern_test_task_labels[idx] = exp_id\n\n            train_exps_patterns_assignment.append(selected_indexes_train)\n            test_exps_patterns_assignment.append(selected_indexes_test)\n\n        # Good idea, but doesn't work\n        # transform_groups = train_eval_transforms(train_dataset, test_dataset)\n        #\n        # train_dataset = train_dataset\\\n        #     .replace_transforms(*transform_groups['train'], group='train') \\\n        #     .replace_transforms(*transform_groups['eval'], group='eval')\n        #\n        # test_dataset = test_dataset \\\n        #     .replace_transforms(*transform_groups['train'], group='train') \\\n        #     .replace_transforms(*transform_groups['eval'], group='eval')\n\n        train_dataset = AvalancheSubset(\n            train_dataset, class_mapping=self.class_mapping,\n            initial_transform_group='train')\n        test_dataset = AvalancheSubset(\n            test_dataset, class_mapping=self.class_mapping,\n            initial_transform_group='eval')\n\n        self.train_exps_patterns_assignment = train_exps_patterns_assignment\n        \"\"\" A list containing which training instances are assigned to each\n        experience in the train stream. Instances are identified by their id \n        w.r.t. the dataset found in the original_train_dataset field. \"\"\"\n\n        self.test_exps_patterns_assignment = test_exps_patterns_assignment\n        \"\"\" A list containing which test instances are assigned to each\n        experience in the test stream. Instances are identified by their id \n        w.r.t. the dataset found in the original_test_dataset field. \"\"\"\n\n        train_experiences = []\n        train_task_labels = []\n        for t_id, exp_def in enumerate(train_exps_patterns_assignment):\n            if self._has_task_labels:\n                train_task_labels.append(t_id)\n            else:\n                train_task_labels.append(0)\n            task_labels = ConstantSequence(train_task_labels[-1],\n                                           len(train_dataset))\n            train_experiences.append(\n                AvalancheSubset(train_dataset, indices=exp_def,\n                                task_labels=task_labels))\n\n        test_experiences = []\n        test_task_labels = []\n        for t_id, exp_def in enumerate(test_exps_patterns_assignment):\n            if self._has_task_labels:\n                test_task_labels.append(t_id)\n            else:\n                test_task_labels.append(0)\n            task_labels = ConstantSequence(test_task_labels[-1],\n                                           len(test_dataset))\n            test_experiences.append(\n                AvalancheSubset(test_dataset, indices=exp_def,\n                                task_labels=task_labels))\n\n        super(NCScenario, self).__init__(\n            stream_definitions={\n                'train': (train_experiences, train_task_labels, train_dataset),\n                'test': (test_experiences, test_task_labels, test_dataset)\n            },\n            experience_factory=NCExperience)\n\n    def get_reproducibility_data(self):\n        reproducibility_data = {\n            'class_ids_from_zero_from_first_exp': bool(\n                self.class_ids_from_zero_from_first_exp),\n            'class_ids_from_zero_in_each_exp': bool(\n                self.class_ids_from_zero_in_each_exp),\n            'class_mapping': self.class_mapping,\n            'classes_order': self.classes_order,\n            'classes_order_original_ids': self.classes_order_original_ids,\n            'n_classes_per_exp': self.n_classes_per_exp,\n            'n_experiences': int(self.n_experiences),\n            'has_task_labels': self._has_task_labels}\n        return reproducibility_data\n\n    def classes_in_exp_range(self, exp_start: int,\n                             exp_end: Optional[int] = None) -> List[int]:\n        \"\"\"\n        Gets a list of classes contained in the given experiences. The\n        experiences are defined by range. This means that only the classes in\n        range [exp_start, exp_end) will be included.\n\n        :param exp_start: The starting experience ID.\n        :param exp_end: The final experience ID. Can be None, which means that\n            all the remaining experiences will be taken.\n\n        :returns: The classes contained in the required experience range.\n        \"\"\"\n        # Ref: https://stackoverflow.com/a/952952\n        if exp_end is None:\n            return [\n                item for sublist in\n                self.classes_in_experience['train'][exp_start:]\n                for item in sublist]\n\n        return [\n            item for sublist in\n            self.classes_in_experience['train'][exp_start:exp_end]\n            for item in sublist]",
  "class NCExperience(GenericExperience[NCScenario,\n                                     GenericScenarioStream['NCExperience',\n                                                           NCScenario]]):\n    \"\"\"\n    Defines a \"New Classes\" experience. It defines fields to obtain the current\n    dataset and the associated task label. It also keeps a reference to the\n    stream from which this experience was taken.\n    \"\"\"\n    def __init__(self,\n                 origin_stream: GenericScenarioStream[\n                     'NCExperience', NCScenario],\n                 current_experience: int):\n        \"\"\"\n        Creates a ``NCExperience`` instance given the stream from this\n        experience was taken and and the current experience ID.\n\n        :param origin_stream: The stream from which this experience was\n            obtained.\n        :param current_experience: The current experience ID, as an integer.\n        \"\"\"\n        super(NCExperience, self).__init__(origin_stream, current_experience)",
  "def __init__(self, train_dataset: AvalancheDataset,\n                 test_dataset: AvalancheDataset,\n                 n_experiences: int,\n                 task_labels: bool,\n                 shuffle: bool = True,\n                 seed: Optional[int] = None,\n                 fixed_class_order: Optional[Sequence[int]] = None,\n                 per_experience_classes: Optional[Dict[int, int]] = None,\n                 class_ids_from_zero_from_first_exp: bool = False,\n                 class_ids_from_zero_in_each_exp: bool = False,\n                 reproducibility_data: Optional[Dict[str, Any]] = None):\n        \"\"\"\n        Creates a ``NCGenericScenario`` instance given the training and test\n        Datasets and the number of experiences.\n\n        By default, the number of classes will be automatically detected by\n        looking at the training Dataset ``targets`` field. Classes will be\n        uniformly distributed across ``n_experiences`` unless a\n        ``per_experience_classes`` argument is specified.\n\n        The number of classes must be divisible without remainder by the number\n        of experiences. This also applies when the ``per_experience_classes``\n        argument is not None.\n\n        :param train_dataset: The training dataset. The dataset must be a\n            subclass of :class:`AvalancheDataset`. For instance, one can\n            use the datasets from the torchvision package like that:\n            ``train_dataset=AvalancheDataset(torchvision_dataset)``.\n        :param test_dataset: The test dataset. The dataset must be a\n            subclass of :class:`AvalancheDataset`. For instance, one can\n            use the datasets from the torchvision package like that:\n            ``test_dataset=AvalancheDataset(torchvision_dataset)``.\n        :param n_experiences: The number of experiences.\n        :param task_labels: If True, each experience will have an ascending task\n            label. If False, the task label will be 0 for all the experiences.\n        :param shuffle: If True, the class order will be shuffled. Defaults to\n            True.\n        :param seed: If shuffle is True and seed is not None, the class order\n            will be shuffled according to the seed. When None, the current\n            PyTorch random number generator state will be used.\n            Defaults to None.\n        :param fixed_class_order: If not None, the class order to use (overrides\n            the shuffle argument). Very useful for enhancing\n            reproducibility. Defaults to None.\n        :param per_experience_classes: Is not None, a dictionary whose keys are\n            (0-indexed) experience IDs and their values are the number of\n            classes to include in the respective experiences. The dictionary\n            doesn't have to contain a key for each experience! All the remaining\n            experiences will contain an equal amount of the remaining classes.\n            The remaining number of classes must be divisible without remainder\n            by the remaining number of experiences. For instance,\n            if you want to include 50 classes in the first experience\n            while equally distributing remaining classes across remaining\n            experiences, just pass the \"{0: 50}\" dictionary as the\n            per_experience_classes parameter. Defaults to None.\n        :param class_ids_from_zero_from_first_exp: If True, original class IDs\n            will be remapped so that they will appear as having an ascending\n            order. For instance, if the resulting class order after shuffling\n            (or defined by fixed_class_order) is [23, 34, 11, 7, 6, ...] and\n            class_ids_from_zero_from_first_exp is True, then all the patterns\n            belonging to class 23 will appear as belonging to class \"0\",\n            class \"34\" will be mapped to \"1\", class \"11\" to \"2\" and so on.\n            This is very useful when drawing confusion matrices and when dealing\n            with algorithms with dynamic head expansion. Defaults to False.\n            Mutually exclusive with the ``class_ids_from_zero_in_each_exp``\n            parameter.\n        :param class_ids_from_zero_in_each_exp: If True, original class IDs\n            will be mapped to range [0, n_classes_in_exp) for each experience.\n            Defaults to False. Mutually exclusive with the\n            ``class_ids_from_zero_from_first_exp parameter``.\n        :param reproducibility_data: If not None, overrides all the other\n            scenario definition options. This is usually a dictionary containing\n            data used to reproduce a specific experiment. One can use the\n            ``get_reproducibility_data`` method to get (and even distribute)\n            the experiment setup so that it can be loaded by passing it as this\n            parameter. In this way one can be sure that the same specific\n            experimental setup is being used (for reproducibility purposes).\n            Beware that, in order to reproduce an experiment, the same train and\n            test datasets must be used. Defaults to None.\n        \"\"\"\n        if class_ids_from_zero_from_first_exp and \\\n                class_ids_from_zero_in_each_exp:\n            raise ValueError('Invalid mutually exclusive options '\n                             'class_ids_from_zero_from_first_exp and '\n                             'class_ids_from_zero_in_each_exp set at the '\n                             'same time')\n        if reproducibility_data:\n            n_experiences = reproducibility_data['n_experiences']\n\n        if n_experiences < 1:\n            raise ValueError('Invalid number of experiences (n_experiences '\n                             'parameter): must be greater than 0')\n\n        self.classes_order: List[int] = []\n        \"\"\" Stores the class order (remapped class IDs). \"\"\"\n\n        self.classes_order_original_ids: List[int] = torch.unique(\n            torch.as_tensor(train_dataset.targets),\n            sorted=True).tolist()\n        \"\"\" Stores the class order (original class IDs) \"\"\"\n\n        n_original_classes = max(self.classes_order_original_ids) + 1\n\n        self.class_mapping: List[int] = []\n        \"\"\"\n        class_mapping stores the class mapping so that \n        `mapped_class_id = class_mapping[original_class_id]`. \n        \n        If the benchmark is created with an amount of classes which is less than\n        the amount of all classes in the dataset, then class_mapping will \n        contain some -1 values corresponding to ignored classes. This can\n        happen when passing a fixed class order to the constructor.\n        \"\"\"\n\n        self.n_classes_per_exp: List[int] = []\n        \"\"\" A list that, for each experience (identified by its index/ID),\n            stores the number of classes assigned to that experience. \"\"\"\n\n        self._classes_in_exp: List[Set[int]] = []\n\n        self.original_classes_in_exp: List[Set[int]] = []\n        \"\"\"\n        A list that, for each experience (identified by its index/ID), stores a \n        set of the original IDs of classes assigned to that experience. \n        This field applies to both train and test streams.\n        \"\"\"\n\n        self.class_ids_from_zero_from_first_exp: bool = \\\n            class_ids_from_zero_from_first_exp\n        \"\"\" If True the class IDs have been remapped to start from zero. \"\"\"\n\n        self.class_ids_from_zero_in_each_exp: bool = \\\n            class_ids_from_zero_in_each_exp\n        \"\"\" If True the class IDs have been remapped to start from zero in \n        each experience \"\"\"\n\n        # Note: if fixed_class_order is None and shuffle is False,\n        # the class order will be the one encountered\n        # By looking at the train_dataset targets field\n        if reproducibility_data:\n            self.classes_order_original_ids = \\\n                reproducibility_data['classes_order_original_ids']\n            self.class_ids_from_zero_from_first_exp = \\\n                reproducibility_data['class_ids_from_zero_from_first_exp']\n            self.class_ids_from_zero_in_each_exp = \\\n                reproducibility_data['class_ids_from_zero_in_each_exp']\n        elif fixed_class_order is not None:\n            # User defined class order -> just use it\n            if len(set(self.classes_order_original_ids).union(\n                    set(fixed_class_order))) != \\\n                    len(self.classes_order_original_ids):\n                raise ValueError('Invalid classes defined in fixed_class_order')\n\n            self.classes_order_original_ids = list(fixed_class_order)\n        elif shuffle:\n            # No user defined class order.\n            # If a seed is defined, set the random number generator seed.\n            # If no seed has been defined, use the actual\n            # random number generator state.\n            # Finally, shuffle the class list to obtain a random classes\n            # order\n            if seed is not None:\n                torch.random.manual_seed(seed)\n            self.classes_order_original_ids = \\\n                torch.as_tensor(self.classes_order_original_ids)[\n                    torch.randperm(len(self.classes_order_original_ids))\n                ].tolist()\n\n        self.n_classes: int = len(self.classes_order_original_ids)\n        \"\"\" The number of classes \"\"\"\n\n        if reproducibility_data:\n            self.n_classes_per_exp = \\\n                reproducibility_data['n_classes_per_exp']\n        elif per_experience_classes is not None:\n            # per_experience_classes is a user-defined dictionary that defines\n            # the number of classes to include in some (or all) experiences.\n            # Remaining classes are equally distributed across the other\n            # experiences.\n            #\n            # Format of per_experience_classes dictionary:\n            #   - key = experience id\n            #   - value = number of classes for this experience\n\n            if max(per_experience_classes.keys()) >= n_experiences or min(\n                    per_experience_classes.keys()) < 0:\n                # The dictionary contains a key (that is, a experience id) >=\n                # the number of requested experiences... or < 0\n                raise ValueError(\n                    'Invalid experience id in per_experience_classes parameter:'\n                    ' experience ids must be in range [0, n_experiences)')\n            if min(per_experience_classes.values()) < 0:\n                # One or more values (number of classes for each experience) < 0\n                raise ValueError('Wrong number of classes defined for one or '\n                                 'more experiences: must be a non-negative '\n                                 'value')\n\n            if sum(per_experience_classes.values()) > self.n_classes:\n                # The sum of dictionary values (n. of classes for each\n                # experience) >= the number of classes\n                raise ValueError('Insufficient number of classes: '\n                                 'per_experience_classes parameter can\\'t '\n                                 'be satisfied')\n\n            # Remaining classes are equally distributed across remaining\n            # experiences. This amount of classes must be be divisible without\n            # remainder by the number of remaining experiences\n            remaining_exps = n_experiences - len(per_experience_classes)\n            if remaining_exps > 0 and (self.n_classes - sum(\n                    per_experience_classes.values())) % remaining_exps > 0:\n                raise ValueError('Invalid number of experiences: remaining '\n                                 'classes cannot be divided by n_experiences')\n\n            # default_per_exp_classes is the default amount of classes\n            # for the remaining experiences\n            if remaining_exps > 0:\n                default_per_exp_classes = (self.n_classes - sum(\n                    per_experience_classes.values())) // remaining_exps\n            else:\n                default_per_exp_classes = 0\n\n            # Initialize the self.n_classes_per_exp list using\n            # \"default_per_exp_classes\" as the default\n            # amount of classes per experience. Then, loop through the\n            # per_experience_classes dictionary to set the customized,\n            # user defined, classes for the required experiences.\n            self.n_classes_per_exp = \\\n                [default_per_exp_classes] * n_experiences\n            for exp_id in per_experience_classes:\n                self.n_classes_per_exp[exp_id] = per_experience_classes[\n                    exp_id]\n        else:\n            # Classes will be equally distributed across the experiences\n            # The amount of classes must be be divisible without remainder\n            # by the number of experiences\n            if self.n_classes % n_experiences > 0:\n                raise ValueError(\n                    f'Invalid number of experiences: classes contained in '\n                    f'dataset ({self.n_classes}) cannot be divided by '\n                    f'n_experiences ({n_experiences})')\n            self.n_classes_per_exp = \\\n                [self.n_classes // n_experiences] * n_experiences\n\n        # Before populating the classes_in_experience list,\n        # define the remapped class IDs.\n        if reproducibility_data:\n            # Method 0: use reproducibility data\n            self.classes_order = reproducibility_data['classes_order']\n            self.class_mapping = reproducibility_data['class_mapping']\n        elif self.class_ids_from_zero_from_first_exp:\n            # Method 1: remap class IDs so that they appear in ascending order\n            # over all experiences\n            self.classes_order = list(range(0, self.n_classes))\n            self.class_mapping = [-1] * n_original_classes\n            for class_id in range(n_original_classes):\n                # This check is needed because, when a fixed class order is\n                # used, the user may have defined an amount of classes less than\n                # the overall amount of classes in the dataset.\n                if class_id in self.classes_order_original_ids:\n                    self.class_mapping[class_id] = \\\n                        self.classes_order_original_ids.index(class_id)\n        elif self.class_ids_from_zero_in_each_exp:\n            # Method 2: remap class IDs so that they appear in range [0, N] in\n            # each experience\n            self.classes_order = []\n            self.class_mapping = [-1] * n_original_classes\n            next_class_idx = 0\n            for exp_id, exp_n_classes in enumerate(self.n_classes_per_exp):\n                self.classes_order += list(range(exp_n_classes))\n                for exp_class_idx in range(exp_n_classes):\n                    original_class_position = next_class_idx + exp_class_idx\n                    original_class_id = self.classes_order_original_ids[\n                        original_class_position]\n                    self.class_mapping[original_class_id] = exp_class_idx\n                next_class_idx += exp_n_classes\n        else:\n            # Method 3: no remapping of any kind\n            # remapped_id = class_mapping[class_id] -> class_id == remapped_id\n            self.classes_order = self.classes_order_original_ids\n            self.class_mapping = list(range(0, n_original_classes))\n\n        original_training_dataset = train_dataset\n        original_test_dataset = test_dataset\n\n        # Populate the _classes_in_exp and original_classes_in_exp lists\n        # \"_classes_in_exp[exp_id]\": list of (remapped) class IDs assigned\n        # to experience \"exp_id\"\n        # \"original_classes_in_exp[exp_id]\": list of original class IDs\n        # assigned to experience \"exp_id\"\n        for exp_id in range(n_experiences):\n            classes_start_idx = sum(self.n_classes_per_exp[:exp_id])\n            classes_end_idx = classes_start_idx + self.n_classes_per_exp[\n                exp_id]\n\n            self._classes_in_exp.append(\n                set(self.classes_order[classes_start_idx:classes_end_idx]))\n            self.original_classes_in_exp.append(\n                set(self.classes_order_original_ids[classes_start_idx:\n                                                    classes_end_idx]))\n\n        # Finally, create the experience -> patterns assignment.\n        # In order to do this, we don't load all the patterns\n        # instead we use the targets field.\n        train_exps_patterns_assignment = []\n        test_exps_patterns_assignment = []\n\n        self._has_task_labels = task_labels\n        if reproducibility_data is not None:\n            self._has_task_labels = bool(\n                reproducibility_data['has_task_labels'])\n\n        if self._has_task_labels:\n            pattern_train_task_labels = [-1] * len(train_dataset)\n            pattern_test_task_labels = [-1] * len(test_dataset)\n        else:\n            pattern_train_task_labels = ConstantSequence(0, len(train_dataset))\n            pattern_test_task_labels = ConstantSequence(0, len(test_dataset))\n\n        for exp_id in range(n_experiences):\n            selected_classes = self.original_classes_in_exp[exp_id]\n            selected_indexes_train = []\n            for idx, element in enumerate(original_training_dataset.targets):\n                if element in selected_classes:\n                    selected_indexes_train.append(idx)\n                    if self._has_task_labels:\n                        pattern_train_task_labels[idx] = exp_id\n\n            selected_indexes_test = []\n            for idx, element in enumerate(original_test_dataset.targets):\n                if element in selected_classes:\n                    selected_indexes_test.append(idx)\n                    if self._has_task_labels:\n                        pattern_test_task_labels[idx] = exp_id\n\n            train_exps_patterns_assignment.append(selected_indexes_train)\n            test_exps_patterns_assignment.append(selected_indexes_test)\n\n        # Good idea, but doesn't work\n        # transform_groups = train_eval_transforms(train_dataset, test_dataset)\n        #\n        # train_dataset = train_dataset\\\n        #     .replace_transforms(*transform_groups['train'], group='train') \\\n        #     .replace_transforms(*transform_groups['eval'], group='eval')\n        #\n        # test_dataset = test_dataset \\\n        #     .replace_transforms(*transform_groups['train'], group='train') \\\n        #     .replace_transforms(*transform_groups['eval'], group='eval')\n\n        train_dataset = AvalancheSubset(\n            train_dataset, class_mapping=self.class_mapping,\n            initial_transform_group='train')\n        test_dataset = AvalancheSubset(\n            test_dataset, class_mapping=self.class_mapping,\n            initial_transform_group='eval')\n\n        self.train_exps_patterns_assignment = train_exps_patterns_assignment\n        \"\"\" A list containing which training instances are assigned to each\n        experience in the train stream. Instances are identified by their id \n        w.r.t. the dataset found in the original_train_dataset field. \"\"\"\n\n        self.test_exps_patterns_assignment = test_exps_patterns_assignment\n        \"\"\" A list containing which test instances are assigned to each\n        experience in the test stream. Instances are identified by their id \n        w.r.t. the dataset found in the original_test_dataset field. \"\"\"\n\n        train_experiences = []\n        train_task_labels = []\n        for t_id, exp_def in enumerate(train_exps_patterns_assignment):\n            if self._has_task_labels:\n                train_task_labels.append(t_id)\n            else:\n                train_task_labels.append(0)\n            task_labels = ConstantSequence(train_task_labels[-1],\n                                           len(train_dataset))\n            train_experiences.append(\n                AvalancheSubset(train_dataset, indices=exp_def,\n                                task_labels=task_labels))\n\n        test_experiences = []\n        test_task_labels = []\n        for t_id, exp_def in enumerate(test_exps_patterns_assignment):\n            if self._has_task_labels:\n                test_task_labels.append(t_id)\n            else:\n                test_task_labels.append(0)\n            task_labels = ConstantSequence(test_task_labels[-1],\n                                           len(test_dataset))\n            test_experiences.append(\n                AvalancheSubset(test_dataset, indices=exp_def,\n                                task_labels=task_labels))\n\n        super(NCScenario, self).__init__(\n            stream_definitions={\n                'train': (train_experiences, train_task_labels, train_dataset),\n                'test': (test_experiences, test_task_labels, test_dataset)\n            },\n            experience_factory=NCExperience)",
  "def get_reproducibility_data(self):\n        reproducibility_data = {\n            'class_ids_from_zero_from_first_exp': bool(\n                self.class_ids_from_zero_from_first_exp),\n            'class_ids_from_zero_in_each_exp': bool(\n                self.class_ids_from_zero_in_each_exp),\n            'class_mapping': self.class_mapping,\n            'classes_order': self.classes_order,\n            'classes_order_original_ids': self.classes_order_original_ids,\n            'n_classes_per_exp': self.n_classes_per_exp,\n            'n_experiences': int(self.n_experiences),\n            'has_task_labels': self._has_task_labels}\n        return reproducibility_data",
  "def classes_in_exp_range(self, exp_start: int,\n                             exp_end: Optional[int] = None) -> List[int]:\n        \"\"\"\n        Gets a list of classes contained in the given experiences. The\n        experiences are defined by range. This means that only the classes in\n        range [exp_start, exp_end) will be included.\n\n        :param exp_start: The starting experience ID.\n        :param exp_end: The final experience ID. Can be None, which means that\n            all the remaining experiences will be taken.\n\n        :returns: The classes contained in the required experience range.\n        \"\"\"\n        # Ref: https://stackoverflow.com/a/952952\n        if exp_end is None:\n            return [\n                item for sublist in\n                self.classes_in_experience['train'][exp_start:]\n                for item in sublist]\n\n        return [\n            item for sublist in\n            self.classes_in_experience['train'][exp_start:exp_end]\n            for item in sublist]",
  "def __init__(self,\n                 origin_stream: GenericScenarioStream[\n                     'NCExperience', NCScenario],\n                 current_experience: int):\n        \"\"\"\n        Creates a ``NCExperience`` instance given the stream from this\n        experience was taken and and the current experience ID.\n\n        :param origin_stream: The stream from which this experience was\n            obtained.\n        :param current_experience: The current experience ID, as an integer.\n        \"\"\"\n        super(NCExperience, self).__init__(origin_stream, current_experience)",
  "class NIScenario(GenericCLScenario['NIExperience']):\n    \"\"\"\n    This class defines a \"New Instance\" scenario.\n    Once created, an instance of this class can be iterated in order to obtain\n    the experience sequence under the form of instances of\n    :class:`NIExperience`.\n\n    Instances of this class can be created using the constructor directly.\n    However, we recommend using facilities like\n    :func:`avalanche.benchmarks.generators.ni_scenario`.\n\n    Consider that every method from :class:`NIExperience` used to retrieve\n    parts of the test set (past, current, future, cumulative) always return the\n    complete test set. That is, they behave as the getter for the complete test\n    set.\n    \"\"\"\n\n    def __init__(\n            self,\n            train_dataset: AvalancheDataset,\n            test_dataset: AvalancheDataset,\n            n_experiences: int,\n            task_labels: bool = False,\n            shuffle: bool = True,\n            seed: Optional[int] = None,\n            balance_experiences: bool = False,\n            min_class_patterns_in_exp: int = 0,\n            fixed_exp_assignment: Optional[Sequence[Sequence[int]]] = None,\n            reproducibility_data: Optional[Dict[str, Any]] = None):\n        \"\"\"\n        Creates a NIScenario instance given the training and test Datasets and\n        the number of experiences.\n\n        :param train_dataset: The training dataset. The dataset must be an\n            instance of :class:`AvalancheDataset`. For instance, one can\n            use the datasets from the torchvision package like that:\n            ``train_dataset=AvalancheDataset(torchvision_dataset)``.\n        :param test_dataset: The test dataset. The dataset must be a\n            subclass of :class:`AvalancheDataset`. For instance, one can\n            use the datasets from the torchvision package like that:\n            ``test_dataset=AvalancheDataset(torchvision_dataset)``.\n        :param n_experiences: The number of experiences.\n        :param task_labels: If True, each experience will have an ascending task\n            label. If False, the task label will be 0 for all the experiences.\n            Defaults to False.\n        :param shuffle: If True, the patterns order will be shuffled. Defaults\n            to True.\n        :param seed: If shuffle is True and seed is not None, the class order\n            will be shuffled according to the seed. When None, the current\n            PyTorch random number generator state will be used.\n            Defaults to None.\n        :param balance_experiences: If True, pattern of each class will be\n            equally spread across all experiences. If False, patterns will be\n            assigned to experiences in a complete random way. Defaults to False.\n        :param min_class_patterns_in_exp: The minimum amount of patterns of\n            every class that must be assigned to every experience. Compatible\n            with the ``balance_experiences`` parameter. An exception will be\n            raised if this constraint can't be satisfied. Defaults to 0.\n        :param fixed_exp_assignment: If not None, the pattern assignment\n            to use. It must be a list with an entry for each experience. Each\n            entry is a list that contains the indexes of patterns belonging to\n            that experience. Overrides the ``shuffle``, ``balance_experiences``\n            and ``min_class_patterns_in_exp`` parameters.\n        :param reproducibility_data: If not None, overrides all the other\n            scenario definition options, including ``fixed_exp_assignment``.\n            This is usually a dictionary containing data used to\n            reproduce a specific experiment. One can use the\n            ``get_reproducibility_data`` method to get (and even distribute)\n            the experiment setup so that it can be loaded by passing it as this\n            parameter. In this way one can be sure that the same specific\n            experimental setup is being used (for reproducibility purposes).\n            Beware that, in order to reproduce an experiment, the same train and\n            test datasets must be used. Defaults to None.\n        \"\"\"\n\n        self._has_task_labels = task_labels\n\n        self.train_exps_patterns_assignment = []\n\n        if reproducibility_data is not None:\n            self.train_exps_patterns_assignment = reproducibility_data[\n                'exps_patterns_assignment']\n            self._has_task_labels = reproducibility_data['has_task_labels']\n            n_experiences = len(self.train_exps_patterns_assignment)\n\n        if n_experiences < 1:\n            raise ValueError('Invalid number of experiences (n_experiences '\n                             'parameter): must be greater than 0')\n\n        if min_class_patterns_in_exp < 0 and reproducibility_data is None:\n            raise ValueError('Invalid min_class_patterns_in_exp parameter: '\n                             'must be greater than or equal to 0')\n\n        # # Good idea, but doesn't work\n        # transform_groups = train_eval_transforms(train_dataset, test_dataset)\n        #\n        # train_dataset = train_dataset \\\n        #     .replace_transforms(*transform_groups['train'], group='train') \\\n        #     .replace_transforms(*transform_groups['eval'], group='eval')\n        #\n        # test_dataset = test_dataset \\\n        #     .replace_transforms(*transform_groups['train'], group='train') \\\n        #     .replace_transforms(*transform_groups['eval'], group='eval')\n\n        unique_targets, unique_count = torch.unique(\n            torch.as_tensor(train_dataset.targets), return_counts=True)\n\n        self.n_classes: int = len(unique_targets)\n        \"\"\"\n        The amount of classes in the original training set.\n        \"\"\"\n\n        self.n_patterns_per_class: List[int] = \\\n            [0 for _ in range(self.n_classes)]\n        \"\"\"\n        The amount of patterns for each class in the original training set.\n        \"\"\"\n\n        if fixed_exp_assignment:\n            included_patterns = list()\n            for exp_def in fixed_exp_assignment:\n                included_patterns.extend(exp_def)\n            subset = AvalancheSubset(train_dataset,\n                                     indices=included_patterns)\n            unique_targets, unique_count = torch.unique(\n                torch.as_tensor(subset.targets), return_counts=True)\n\n        for unique_idx in range(len(unique_targets)):\n            class_id = int(unique_targets[unique_idx])\n            class_count = int(unique_count[unique_idx])\n            self.n_patterns_per_class[class_id] = class_count\n\n        self.n_patterns_per_experience: List[int] = []\n        \"\"\"\n        The number of patterns in each experience.\n        \"\"\"\n\n        self.exp_structure: List[List[int]] = []\n        \"\"\" This field contains, for each training experience, the number of\n        instances of each class assigned to that experience. \"\"\"\n\n        if reproducibility_data or fixed_exp_assignment:\n            # fixed_patterns_assignment/reproducibility_data is the user\n            # provided pattern assignment. All we have to do is populate\n            # remaining fields of the class!\n            # n_patterns_per_experience is filled later based on exp_structure\n            # so we only need to fill exp_structure.\n\n            if reproducibility_data:\n                exp_patterns = self.train_exps_patterns_assignment\n            else:\n                exp_patterns = fixed_exp_assignment\n            self.exp_structure = _exp_structure_from_assignment(\n                train_dataset, exp_patterns, self.n_classes\n            )\n        else:\n            # All experiences will all contain the same amount of patterns\n            # The amount of patterns doesn't need to be divisible without\n            # remainder by the number of experience, so we distribute remaining\n            # patterns across randomly selected experience (when shuffling) or\n            # the first N experiences (when not shuffling). However, we first\n            # have to check if the min_class_patterns_in_exp constraint is\n            # satisfiable.\n            min_class_patterns = min(self.n_patterns_per_class)\n            if min_class_patterns < n_experiences * min_class_patterns_in_exp:\n                raise ValueError('min_class_patterns_in_exp constraint '\n                                 'can\\'t be satisfied')\n\n            if seed is not None:\n                torch.random.manual_seed(seed)\n\n            # First, get the patterns indexes for each class\n            targets_as_tensor = torch.as_tensor(train_dataset.targets)\n            classes_to_patterns_idx = [\n                torch.nonzero(\n                    torch.eq(targets_as_tensor, class_id)).view(-1).tolist()\n                for class_id in range(self.n_classes)\n            ]\n\n            if shuffle:\n                classes_to_patterns_idx = [\n                    torch.as_tensor(cls_patterns)[\n                        torch.randperm(len(cls_patterns))\n                    ].tolist() for cls_patterns in classes_to_patterns_idx\n                ]\n\n            # Here we assign patterns to each experience. Two different\n            # strategies are required in order to manage the\n            # balance_experiences parameter.\n            if balance_experiences:\n                # If balance_experiences is True we have to make sure that\n                # patterns of each class are equally distributed across\n                # experiences.\n                #\n                # To do this, populate self.exp_structure, which will\n                # describe how many patterns of each class are assigned to each\n                # experience. Then, for each experience, assign the required\n                # amount of patterns of each class.\n                #\n                # We already checked that there are enough patterns for each\n                # class to satisfy the min_class_patterns_in_exp param so here\n                # we don't need to explicitly enforce that constraint.\n\n                # First, count how many patterns of each class we have to assign\n                # to all the experiences (avg). We also get the number of\n                # remaining patterns which we'll have to assign in a second\n                # experience.\n                class_patterns_per_exp = [\n                    ((n_class_patterns // n_experiences),\n                     (n_class_patterns % n_experiences))\n                    for n_class_patterns in self.n_patterns_per_class\n                ]\n\n                # Remember: exp_structure[exp_id][class_id] is the amount of\n                # patterns of class \"class_id\" in experience \"exp_id\"\n                #\n                # This is the easier experience: just assign the average amount\n                # of class patterns to each experience.\n                self.exp_structure = [\n                    [class_patterns_this_exp[0]\n                     for class_patterns_this_exp\n                     in class_patterns_per_exp] for _ in range(n_experiences)\n                ]\n\n                # Now we have to distribute the remaining patterns of each class\n                #\n                # This means that, for each class, we can (randomly) select\n                # \"n_class_patterns % n_experiences\" experiences to assign a\n                # single additional pattern of that class.\n                for class_id in range(self.n_classes):\n                    n_remaining = class_patterns_per_exp[class_id][1]\n                    if n_remaining == 0:\n                        continue\n                    if shuffle:\n                        assignment_of_remaining_patterns = torch.randperm(\n                            n_experiences).tolist()[:n_remaining]\n                    else:\n                        assignment_of_remaining_patterns = range(n_remaining)\n                    for exp_id in assignment_of_remaining_patterns:\n                        self.exp_structure[exp_id][class_id] += 1\n\n                # Following the self.exp_structure definition, assign\n                # the actual patterns to each experience.\n                #\n                # For each experience we assign exactly\n                # self.exp_structure[exp_id][class_id] patterns of\n                # class \"class_id\"\n                exp_patterns = [[] for _ in range(n_experiences)]\n                next_idx_per_class = [0 for _ in range(self.n_classes)]\n                for exp_id in range(n_experiences):\n                    for class_id in range(self.n_classes):\n                        start_idx = next_idx_per_class[class_id]\n                        n_patterns = self.exp_structure[exp_id][class_id]\n                        end_idx = start_idx + n_patterns\n                        exp_patterns[exp_id].extend(\n                            classes_to_patterns_idx[class_id][start_idx:end_idx]\n                        )\n                        next_idx_per_class[class_id] = end_idx\n            else:\n                # If balance_experiences if False, we just randomly shuffle the\n                # patterns indexes and pick N patterns for each experience.\n                #\n                # However, we have to enforce the min_class_patterns_in_exp\n                # constraint, which makes things difficult.\n                # In the balance_experiences scenario, that constraint is\n                # implicitly enforced by equally distributing class patterns in\n                # each experience (we already checked that there are enough\n                # overall patterns for each class to satisfy\n                # min_class_patterns_in_exp)\n\n                # Here we have to assign the minimum required amount of class\n                # patterns to each experience first, then we can move to\n                # randomly assign the remaining patterns to each experience.\n\n                # First, initialize exp_patterns and exp_structure\n                exp_patterns = [[] for _ in range(n_experiences)]\n                self.exp_structure = [[0 for _ in range(self.n_classes)]\n                                      for _ in range(n_experiences)]\n\n                # For each experience we assign exactly\n                # min_class_patterns_in_exp patterns from each class\n                #\n                # Very similar to the loop found in the balance_experiences\n                # branch! Remember that classes_to_patterns_idx is already\n                # shuffled (if required)\n                next_idx_per_class = [0 for _ in range(self.n_classes)]\n                remaining_patterns = set(range(len(train_dataset)))\n\n                for exp_id in range(n_experiences):\n                    for class_id in range(self.n_classes):\n                        next_idx = next_idx_per_class[class_id]\n                        end_idx = next_idx + min_class_patterns_in_exp\n                        selected_patterns = \\\n                            classes_to_patterns_idx[next_idx:end_idx]\n                        exp_patterns[exp_id].extend(selected_patterns)\n                        self.exp_structure[exp_id][class_id] += \\\n                            min_class_patterns_in_exp\n                        remaining_patterns.difference_update(selected_patterns)\n                        next_idx_per_class[class_id] = end_idx\n\n                remaining_patterns = list(remaining_patterns)\n\n                # We have assigned the required min_class_patterns_in_exp,\n                # now we assign the remaining patterns\n                #\n                # We'll work on remaining_patterns, which contains indexes of\n                # patterns not assigned in the previous experience.\n                if shuffle:\n                    patterns_order = torch.as_tensor(remaining_patterns)[\n                        torch.randperm(len(remaining_patterns))\n                    ].tolist()\n                else:\n                    remaining_patterns.sort()\n                    patterns_order = remaining_patterns\n                targets_order = [train_dataset.targets[pattern_idx]\n                                 for pattern_idx in patterns_order]\n\n                avg_exp_size = len(patterns_order) // n_experiences\n                n_remaining = len(patterns_order) % n_experiences\n                prev_idx = 0\n                for exp_id in range(n_experiences):\n                    next_idx = prev_idx + avg_exp_size\n                    exp_patterns[exp_id].extend(\n                        patterns_order[prev_idx:next_idx])\n                    cls_ids, cls_counts = torch.unique(torch.as_tensor(\n                        targets_order[prev_idx:next_idx]), return_counts=True)\n\n                    cls_ids = cls_ids.tolist()\n                    cls_counts = cls_counts.tolist()\n\n                    for unique_idx in range(len(cls_ids)):\n                        self.exp_structure[exp_id][cls_ids[unique_idx]] += \\\n                            cls_counts[unique_idx]\n                    prev_idx = next_idx\n\n                # Distribute remaining patterns\n                if n_remaining > 0:\n                    if shuffle:\n                        assignment_of_remaining_patterns = torch.randperm(\n                            n_experiences).tolist()[:n_remaining]\n                    else:\n                        assignment_of_remaining_patterns = range(n_remaining)\n                    for exp_id in assignment_of_remaining_patterns:\n                        pattern_idx = patterns_order[prev_idx]\n                        pattern_target = targets_order[prev_idx]\n                        exp_patterns[exp_id].append(pattern_idx)\n\n                        self.exp_structure[exp_id][pattern_target] += 1\n                        prev_idx += 1\n\n        self.n_patterns_per_experience = [len(exp_patterns[exp_id])\n                                          for exp_id in range(n_experiences)]\n\n        self._classes_in_exp = None  # Will be lazy initialized later\n\n        train_experiences = []\n        train_task_labels = []\n        for t_id, exp_def in enumerate(exp_patterns):\n            if self._has_task_labels:\n                train_task_labels.append(t_id)\n            else:\n                train_task_labels.append(0)\n            task_labels = ConstantSequence(train_task_labels[-1],\n                                           len(train_dataset))\n            train_experiences.append(\n                AvalancheSubset(train_dataset, indices=exp_def,\n                                task_labels=task_labels))\n\n        self.train_exps_patterns_assignment = exp_patterns\n        \"\"\" A list containing which training instances are assigned to each\n        experience in the train stream. Instances are identified by their id \n        w.r.t. the dataset found in the original_train_dataset field. \"\"\"\n\n        super(NIScenario, self).__init__(\n            stream_definitions={\n                'train': (train_experiences, train_task_labels,\n                          train_dataset),\n                'test': (test_dataset, [0], test_dataset)\n            },\n            complete_test_set_only=True,\n            experience_factory=NIExperience)\n\n    def get_reproducibility_data(self) -> Dict[str, Any]:\n        reproducibility_data = {\n            'exps_patterns_assignment': self.train_exps_patterns_assignment,\n            'has_task_labels': bool(self._has_task_labels),\n\n        }\n        return reproducibility_data",
  "class NIExperience(GenericExperience[NIScenario,\n                                     GenericScenarioStream['NIExperience',\n                                                           NIScenario]]):\n    \"\"\"\n    Defines a \"New Instances\" experience. It defines fields to obtain the\n    current dataset and the associated task label. It also keeps a reference\n    to the stream from which this experience was taken.\n    \"\"\"\n    def __init__(self, origin_stream: GenericScenarioStream['NIExperience',\n                                                            NIScenario],\n                 current_experience: int):\n        \"\"\"\n        Creates a ``NIExperience`` instance given the stream from this\n        experience was taken and and the current experience ID.\n\n        :param origin_stream: The stream from which this experience was\n            obtained.\n        :param current_experience: The current experience ID, as an integer.\n        \"\"\"\n        super(NIExperience, self).__init__(\n            origin_stream, current_experience)",
  "def __init__(\n            self,\n            train_dataset: AvalancheDataset,\n            test_dataset: AvalancheDataset,\n            n_experiences: int,\n            task_labels: bool = False,\n            shuffle: bool = True,\n            seed: Optional[int] = None,\n            balance_experiences: bool = False,\n            min_class_patterns_in_exp: int = 0,\n            fixed_exp_assignment: Optional[Sequence[Sequence[int]]] = None,\n            reproducibility_data: Optional[Dict[str, Any]] = None):\n        \"\"\"\n        Creates a NIScenario instance given the training and test Datasets and\n        the number of experiences.\n\n        :param train_dataset: The training dataset. The dataset must be an\n            instance of :class:`AvalancheDataset`. For instance, one can\n            use the datasets from the torchvision package like that:\n            ``train_dataset=AvalancheDataset(torchvision_dataset)``.\n        :param test_dataset: The test dataset. The dataset must be a\n            subclass of :class:`AvalancheDataset`. For instance, one can\n            use the datasets from the torchvision package like that:\n            ``test_dataset=AvalancheDataset(torchvision_dataset)``.\n        :param n_experiences: The number of experiences.\n        :param task_labels: If True, each experience will have an ascending task\n            label. If False, the task label will be 0 for all the experiences.\n            Defaults to False.\n        :param shuffle: If True, the patterns order will be shuffled. Defaults\n            to True.\n        :param seed: If shuffle is True and seed is not None, the class order\n            will be shuffled according to the seed. When None, the current\n            PyTorch random number generator state will be used.\n            Defaults to None.\n        :param balance_experiences: If True, pattern of each class will be\n            equally spread across all experiences. If False, patterns will be\n            assigned to experiences in a complete random way. Defaults to False.\n        :param min_class_patterns_in_exp: The minimum amount of patterns of\n            every class that must be assigned to every experience. Compatible\n            with the ``balance_experiences`` parameter. An exception will be\n            raised if this constraint can't be satisfied. Defaults to 0.\n        :param fixed_exp_assignment: If not None, the pattern assignment\n            to use. It must be a list with an entry for each experience. Each\n            entry is a list that contains the indexes of patterns belonging to\n            that experience. Overrides the ``shuffle``, ``balance_experiences``\n            and ``min_class_patterns_in_exp`` parameters.\n        :param reproducibility_data: If not None, overrides all the other\n            scenario definition options, including ``fixed_exp_assignment``.\n            This is usually a dictionary containing data used to\n            reproduce a specific experiment. One can use the\n            ``get_reproducibility_data`` method to get (and even distribute)\n            the experiment setup so that it can be loaded by passing it as this\n            parameter. In this way one can be sure that the same specific\n            experimental setup is being used (for reproducibility purposes).\n            Beware that, in order to reproduce an experiment, the same train and\n            test datasets must be used. Defaults to None.\n        \"\"\"\n\n        self._has_task_labels = task_labels\n\n        self.train_exps_patterns_assignment = []\n\n        if reproducibility_data is not None:\n            self.train_exps_patterns_assignment = reproducibility_data[\n                'exps_patterns_assignment']\n            self._has_task_labels = reproducibility_data['has_task_labels']\n            n_experiences = len(self.train_exps_patterns_assignment)\n\n        if n_experiences < 1:\n            raise ValueError('Invalid number of experiences (n_experiences '\n                             'parameter): must be greater than 0')\n\n        if min_class_patterns_in_exp < 0 and reproducibility_data is None:\n            raise ValueError('Invalid min_class_patterns_in_exp parameter: '\n                             'must be greater than or equal to 0')\n\n        # # Good idea, but doesn't work\n        # transform_groups = train_eval_transforms(train_dataset, test_dataset)\n        #\n        # train_dataset = train_dataset \\\n        #     .replace_transforms(*transform_groups['train'], group='train') \\\n        #     .replace_transforms(*transform_groups['eval'], group='eval')\n        #\n        # test_dataset = test_dataset \\\n        #     .replace_transforms(*transform_groups['train'], group='train') \\\n        #     .replace_transforms(*transform_groups['eval'], group='eval')\n\n        unique_targets, unique_count = torch.unique(\n            torch.as_tensor(train_dataset.targets), return_counts=True)\n\n        self.n_classes: int = len(unique_targets)\n        \"\"\"\n        The amount of classes in the original training set.\n        \"\"\"\n\n        self.n_patterns_per_class: List[int] = \\\n            [0 for _ in range(self.n_classes)]\n        \"\"\"\n        The amount of patterns for each class in the original training set.\n        \"\"\"\n\n        if fixed_exp_assignment:\n            included_patterns = list()\n            for exp_def in fixed_exp_assignment:\n                included_patterns.extend(exp_def)\n            subset = AvalancheSubset(train_dataset,\n                                     indices=included_patterns)\n            unique_targets, unique_count = torch.unique(\n                torch.as_tensor(subset.targets), return_counts=True)\n\n        for unique_idx in range(len(unique_targets)):\n            class_id = int(unique_targets[unique_idx])\n            class_count = int(unique_count[unique_idx])\n            self.n_patterns_per_class[class_id] = class_count\n\n        self.n_patterns_per_experience: List[int] = []\n        \"\"\"\n        The number of patterns in each experience.\n        \"\"\"\n\n        self.exp_structure: List[List[int]] = []\n        \"\"\" This field contains, for each training experience, the number of\n        instances of each class assigned to that experience. \"\"\"\n\n        if reproducibility_data or fixed_exp_assignment:\n            # fixed_patterns_assignment/reproducibility_data is the user\n            # provided pattern assignment. All we have to do is populate\n            # remaining fields of the class!\n            # n_patterns_per_experience is filled later based on exp_structure\n            # so we only need to fill exp_structure.\n\n            if reproducibility_data:\n                exp_patterns = self.train_exps_patterns_assignment\n            else:\n                exp_patterns = fixed_exp_assignment\n            self.exp_structure = _exp_structure_from_assignment(\n                train_dataset, exp_patterns, self.n_classes\n            )\n        else:\n            # All experiences will all contain the same amount of patterns\n            # The amount of patterns doesn't need to be divisible without\n            # remainder by the number of experience, so we distribute remaining\n            # patterns across randomly selected experience (when shuffling) or\n            # the first N experiences (when not shuffling). However, we first\n            # have to check if the min_class_patterns_in_exp constraint is\n            # satisfiable.\n            min_class_patterns = min(self.n_patterns_per_class)\n            if min_class_patterns < n_experiences * min_class_patterns_in_exp:\n                raise ValueError('min_class_patterns_in_exp constraint '\n                                 'can\\'t be satisfied')\n\n            if seed is not None:\n                torch.random.manual_seed(seed)\n\n            # First, get the patterns indexes for each class\n            targets_as_tensor = torch.as_tensor(train_dataset.targets)\n            classes_to_patterns_idx = [\n                torch.nonzero(\n                    torch.eq(targets_as_tensor, class_id)).view(-1).tolist()\n                for class_id in range(self.n_classes)\n            ]\n\n            if shuffle:\n                classes_to_patterns_idx = [\n                    torch.as_tensor(cls_patterns)[\n                        torch.randperm(len(cls_patterns))\n                    ].tolist() for cls_patterns in classes_to_patterns_idx\n                ]\n\n            # Here we assign patterns to each experience. Two different\n            # strategies are required in order to manage the\n            # balance_experiences parameter.\n            if balance_experiences:\n                # If balance_experiences is True we have to make sure that\n                # patterns of each class are equally distributed across\n                # experiences.\n                #\n                # To do this, populate self.exp_structure, which will\n                # describe how many patterns of each class are assigned to each\n                # experience. Then, for each experience, assign the required\n                # amount of patterns of each class.\n                #\n                # We already checked that there are enough patterns for each\n                # class to satisfy the min_class_patterns_in_exp param so here\n                # we don't need to explicitly enforce that constraint.\n\n                # First, count how many patterns of each class we have to assign\n                # to all the experiences (avg). We also get the number of\n                # remaining patterns which we'll have to assign in a second\n                # experience.\n                class_patterns_per_exp = [\n                    ((n_class_patterns // n_experiences),\n                     (n_class_patterns % n_experiences))\n                    for n_class_patterns in self.n_patterns_per_class\n                ]\n\n                # Remember: exp_structure[exp_id][class_id] is the amount of\n                # patterns of class \"class_id\" in experience \"exp_id\"\n                #\n                # This is the easier experience: just assign the average amount\n                # of class patterns to each experience.\n                self.exp_structure = [\n                    [class_patterns_this_exp[0]\n                     for class_patterns_this_exp\n                     in class_patterns_per_exp] for _ in range(n_experiences)\n                ]\n\n                # Now we have to distribute the remaining patterns of each class\n                #\n                # This means that, for each class, we can (randomly) select\n                # \"n_class_patterns % n_experiences\" experiences to assign a\n                # single additional pattern of that class.\n                for class_id in range(self.n_classes):\n                    n_remaining = class_patterns_per_exp[class_id][1]\n                    if n_remaining == 0:\n                        continue\n                    if shuffle:\n                        assignment_of_remaining_patterns = torch.randperm(\n                            n_experiences).tolist()[:n_remaining]\n                    else:\n                        assignment_of_remaining_patterns = range(n_remaining)\n                    for exp_id in assignment_of_remaining_patterns:\n                        self.exp_structure[exp_id][class_id] += 1\n\n                # Following the self.exp_structure definition, assign\n                # the actual patterns to each experience.\n                #\n                # For each experience we assign exactly\n                # self.exp_structure[exp_id][class_id] patterns of\n                # class \"class_id\"\n                exp_patterns = [[] for _ in range(n_experiences)]\n                next_idx_per_class = [0 for _ in range(self.n_classes)]\n                for exp_id in range(n_experiences):\n                    for class_id in range(self.n_classes):\n                        start_idx = next_idx_per_class[class_id]\n                        n_patterns = self.exp_structure[exp_id][class_id]\n                        end_idx = start_idx + n_patterns\n                        exp_patterns[exp_id].extend(\n                            classes_to_patterns_idx[class_id][start_idx:end_idx]\n                        )\n                        next_idx_per_class[class_id] = end_idx\n            else:\n                # If balance_experiences if False, we just randomly shuffle the\n                # patterns indexes and pick N patterns for each experience.\n                #\n                # However, we have to enforce the min_class_patterns_in_exp\n                # constraint, which makes things difficult.\n                # In the balance_experiences scenario, that constraint is\n                # implicitly enforced by equally distributing class patterns in\n                # each experience (we already checked that there are enough\n                # overall patterns for each class to satisfy\n                # min_class_patterns_in_exp)\n\n                # Here we have to assign the minimum required amount of class\n                # patterns to each experience first, then we can move to\n                # randomly assign the remaining patterns to each experience.\n\n                # First, initialize exp_patterns and exp_structure\n                exp_patterns = [[] for _ in range(n_experiences)]\n                self.exp_structure = [[0 for _ in range(self.n_classes)]\n                                      for _ in range(n_experiences)]\n\n                # For each experience we assign exactly\n                # min_class_patterns_in_exp patterns from each class\n                #\n                # Very similar to the loop found in the balance_experiences\n                # branch! Remember that classes_to_patterns_idx is already\n                # shuffled (if required)\n                next_idx_per_class = [0 for _ in range(self.n_classes)]\n                remaining_patterns = set(range(len(train_dataset)))\n\n                for exp_id in range(n_experiences):\n                    for class_id in range(self.n_classes):\n                        next_idx = next_idx_per_class[class_id]\n                        end_idx = next_idx + min_class_patterns_in_exp\n                        selected_patterns = \\\n                            classes_to_patterns_idx[next_idx:end_idx]\n                        exp_patterns[exp_id].extend(selected_patterns)\n                        self.exp_structure[exp_id][class_id] += \\\n                            min_class_patterns_in_exp\n                        remaining_patterns.difference_update(selected_patterns)\n                        next_idx_per_class[class_id] = end_idx\n\n                remaining_patterns = list(remaining_patterns)\n\n                # We have assigned the required min_class_patterns_in_exp,\n                # now we assign the remaining patterns\n                #\n                # We'll work on remaining_patterns, which contains indexes of\n                # patterns not assigned in the previous experience.\n                if shuffle:\n                    patterns_order = torch.as_tensor(remaining_patterns)[\n                        torch.randperm(len(remaining_patterns))\n                    ].tolist()\n                else:\n                    remaining_patterns.sort()\n                    patterns_order = remaining_patterns\n                targets_order = [train_dataset.targets[pattern_idx]\n                                 for pattern_idx in patterns_order]\n\n                avg_exp_size = len(patterns_order) // n_experiences\n                n_remaining = len(patterns_order) % n_experiences\n                prev_idx = 0\n                for exp_id in range(n_experiences):\n                    next_idx = prev_idx + avg_exp_size\n                    exp_patterns[exp_id].extend(\n                        patterns_order[prev_idx:next_idx])\n                    cls_ids, cls_counts = torch.unique(torch.as_tensor(\n                        targets_order[prev_idx:next_idx]), return_counts=True)\n\n                    cls_ids = cls_ids.tolist()\n                    cls_counts = cls_counts.tolist()\n\n                    for unique_idx in range(len(cls_ids)):\n                        self.exp_structure[exp_id][cls_ids[unique_idx]] += \\\n                            cls_counts[unique_idx]\n                    prev_idx = next_idx\n\n                # Distribute remaining patterns\n                if n_remaining > 0:\n                    if shuffle:\n                        assignment_of_remaining_patterns = torch.randperm(\n                            n_experiences).tolist()[:n_remaining]\n                    else:\n                        assignment_of_remaining_patterns = range(n_remaining)\n                    for exp_id in assignment_of_remaining_patterns:\n                        pattern_idx = patterns_order[prev_idx]\n                        pattern_target = targets_order[prev_idx]\n                        exp_patterns[exp_id].append(pattern_idx)\n\n                        self.exp_structure[exp_id][pattern_target] += 1\n                        prev_idx += 1\n\n        self.n_patterns_per_experience = [len(exp_patterns[exp_id])\n                                          for exp_id in range(n_experiences)]\n\n        self._classes_in_exp = None  # Will be lazy initialized later\n\n        train_experiences = []\n        train_task_labels = []\n        for t_id, exp_def in enumerate(exp_patterns):\n            if self._has_task_labels:\n                train_task_labels.append(t_id)\n            else:\n                train_task_labels.append(0)\n            task_labels = ConstantSequence(train_task_labels[-1],\n                                           len(train_dataset))\n            train_experiences.append(\n                AvalancheSubset(train_dataset, indices=exp_def,\n                                task_labels=task_labels))\n\n        self.train_exps_patterns_assignment = exp_patterns\n        \"\"\" A list containing which training instances are assigned to each\n        experience in the train stream. Instances are identified by their id \n        w.r.t. the dataset found in the original_train_dataset field. \"\"\"\n\n        super(NIScenario, self).__init__(\n            stream_definitions={\n                'train': (train_experiences, train_task_labels,\n                          train_dataset),\n                'test': (test_dataset, [0], test_dataset)\n            },\n            complete_test_set_only=True,\n            experience_factory=NIExperience)",
  "def get_reproducibility_data(self) -> Dict[str, Any]:\n        reproducibility_data = {\n            'exps_patterns_assignment': self.train_exps_patterns_assignment,\n            'has_task_labels': bool(self._has_task_labels),\n\n        }\n        return reproducibility_data",
  "def __init__(self, origin_stream: GenericScenarioStream['NIExperience',\n                                                            NIScenario],\n                 current_experience: int):\n        \"\"\"\n        Creates a ``NIExperience`` instance given the stream from this\n        experience was taken and and the current experience ID.\n\n        :param origin_stream: The stream from which this experience was\n            obtained.\n        :param current_experience: The current experience ID, as an integer.\n        \"\"\"\n        super(NIExperience, self).__init__(\n            origin_stream, current_experience)",
  "def _exp_structure_from_assignment(\n        dataset: ISupportedClassificationDataset[Any],\n        assignment: Sequence[Sequence[int]],\n        n_classes: int):\n    n_experiences = len(assignment)\n    exp_structure = [[0 for _ in range(n_classes)]\n                     for _ in range(n_experiences)]\n\n    for exp_id in range(n_experiences):\n        exp_targets = [int(dataset.targets[pattern_idx])\n                       for pattern_idx in assignment[exp_id]]\n        cls_ids, cls_counts = torch.unique(torch.as_tensor(\n            exp_targets), return_counts=True)\n\n        for unique_idx in range(len(cls_ids)):\n            exp_structure[exp_id][int(cls_ids[unique_idx])] += \\\n                int(cls_counts[unique_idx])\n\n    return exp_structure",
  "def nc_benchmark(\n        train_dataset: Union[\n            Sequence[SupportedDataset], SupportedDataset],\n        test_dataset: Union[\n            Sequence[SupportedDataset], SupportedDataset],\n        n_experiences: int,\n        task_labels: bool,\n        *,\n        shuffle: bool = True,\n        seed: Optional[int] = None,\n        fixed_class_order: Sequence[int] = None,\n        per_exp_classes: Dict[int, int] = None,\n        class_ids_from_zero_from_first_exp: bool = False,\n        class_ids_from_zero_in_each_exp: bool = False,\n        one_dataset_per_exp: bool = False,\n        train_transform=None,\n        eval_transform=None,\n        reproducibility_data: Dict[str, Any] = None) -> NCScenario:\n    \"\"\"\n    This is the high-level benchmark instances generator for the\n    \"New Classes\" (NC) case. Given a sequence of train and test datasets creates\n    the continual stream of data as a series of experiences. Each experience\n    will contain all the instances belonging to a certain set of classes and a\n    class won't be assigned to more than one experience.\n\n    This is the reference helper function for creating instances of Class- or\n    Task-Incremental benchmarks.\n\n    The ``task_labels`` parameter determines if each incremental experience has\n    an increasing task label or if, at the contrary, a default task label \"0\"\n    has to be assigned to all experiences. This can be useful when\n    differentiating between Single-Incremental-Task and Multi-Task scenarios.\n\n    There are other important parameters that can be specified in order to tweak\n    the behaviour of the resulting benchmark. Please take a few minutes to read\n    and understand them as they may save you a lot of work.\n\n    This generator features a integrated reproducibility mechanism that allows\n    the user to store and later re-load a benchmark. For more info see the\n    ``reproducibility_data`` parameter.\n\n    :param train_dataset: A list of training datasets, or a single dataset.\n    :param test_dataset: A list of test datasets, or a single test dataset.\n    :param n_experiences: The number of incremental experience. This is not used\n        when using multiple train/test datasets with the ``one_dataset_per_exp``\n        parameter set to True.\n    :param task_labels: If True, each experience will have an ascending task\n            label. If False, the task label will be 0 for all the experiences.\n    :param shuffle: If True, the class (or experience) order will be shuffled.\n        Defaults to True.\n    :param seed: If ``shuffle`` is True and seed is not None, the class (or\n        experience) order will be shuffled according to the seed. When None, the\n        current PyTorch random number generator state will be used. Defaults to\n        None.\n    :param fixed_class_order: If not None, the class order to use (overrides\n        the shuffle argument). Very useful for enhancing reproducibility.\n        Defaults to None.\n    :param per_exp_classes: Is not None, a dictionary whose keys are\n        (0-indexed) experience IDs and their values are the number of classes\n        to include in the respective experiences. The dictionary doesn't\n        have to contain a key for each experience! All the remaining experiences\n        will contain an equal amount of the remaining classes. The\n        remaining number of classes must be divisible without remainder\n        by the remaining number of experiences. For instance,\n        if you want to include 50 classes in the first experience\n        while equally distributing remaining classes across remaining\n        experiences, just pass the \"{0: 50}\" dictionary as the\n        per_experience_classes parameter. Defaults to None.\n    :param class_ids_from_zero_from_first_exp: If True, original class IDs\n        will be remapped so that they will appear as having an ascending\n        order. For instance, if the resulting class order after shuffling\n        (or defined by fixed_class_order) is [23, 34, 11, 7, 6, ...] and\n        class_ids_from_zero_from_first_exp is True, then all the patterns\n        belonging to class 23 will appear as belonging to class \"0\",\n        class \"34\" will be mapped to \"1\", class \"11\" to \"2\" and so on.\n        This is very useful when drawing confusion matrices and when dealing\n        with algorithms with dynamic head expansion. Defaults to False.\n        Mutually exclusive with the ``class_ids_from_zero_in_each_exp``\n        parameter.\n    :param class_ids_from_zero_in_each_exp: If True, original class IDs\n        will be mapped to range [0, n_classes_in_exp) for each experience.\n        Defaults to False. Mutually exclusive with the\n        ``class_ids_from_zero_from_first_exp`` parameter.\n    :param one_dataset_per_exp: available only when multiple train-test\n        datasets are provided. If True, each dataset will be treated as a\n        experience. Mutually exclusive with the ``per_experience_classes`` and\n        ``fixed_class_order`` parameters. Overrides the ``n_experiences`` \n        parameter. Defaults to False.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param reproducibility_data: If not None, overrides all the other\n        benchmark definition options. This is usually a dictionary containing\n        data used to reproduce a specific experiment. One can use the\n        ``get_reproducibility_data`` method to get (and even distribute)\n        the experiment setup so that it can be loaded by passing it as this\n        parameter. In this way one can be sure that the same specific\n        experimental setup is being used (for reproducibility purposes).\n        Beware that, in order to reproduce an experiment, the same train and\n        test datasets must be used. Defaults to None.\n\n    :return: A properly initialized :class:`NCScenario` instance.\n    \"\"\"\n\n    if class_ids_from_zero_from_first_exp and class_ids_from_zero_in_each_exp:\n        raise ValueError('Invalid mutually exclusive options '\n                         'class_ids_from_zero_from_first_exp and '\n                         'classes_ids_from_zero_in_each_exp set at the '\n                         'same time')\n\n    if isinstance(train_dataset, list) or isinstance(train_dataset, tuple):\n        # Multi-dataset setting\n\n        if len(train_dataset) != len(test_dataset):\n            raise ValueError('Train/test dataset lists must contain the '\n                             'exact same number of datasets')\n\n        if per_exp_classes and one_dataset_per_exp:\n            raise ValueError(\n                'Both per_experience_classes and one_dataset_per_exp are'\n                'used, but those options are mutually exclusive')\n\n        if fixed_class_order and one_dataset_per_exp:\n            raise ValueError(\n                'Both fixed_class_order and one_dataset_per_exp are'\n                'used, but those options are mutually exclusive')\n\n        seq_train_dataset, seq_test_dataset, mapping = \\\n            concat_datasets_sequentially(train_dataset, test_dataset)\n\n        if one_dataset_per_exp:\n            # If one_dataset_per_exp is True, each dataset will be treated as\n            # a experience. In this benchmark, shuffle refers to the experience\n            # order, not to the class one.\n            fixed_class_order, per_exp_classes = \\\n                _one_dataset_per_exp_class_order(mapping, shuffle, seed)\n\n            # We pass a fixed_class_order to the NCGenericScenario\n            # constructor, so we don't need shuffling.\n            shuffle = False\n            seed = None\n\n            # Overrides n_experiences (and per_experience_classes, already done)\n            n_experiences = len(train_dataset)\n        train_dataset, test_dataset = seq_train_dataset, seq_test_dataset\n\n    transform_groups = dict(\n        train=(train_transform, None),\n        eval=(eval_transform, None)\n    )\n\n    # Datasets should be instances of AvalancheDataset\n    train_dataset = AvalancheDataset(\n        train_dataset,\n        transform_groups=transform_groups,\n        initial_transform_group='train',\n        dataset_type=AvalancheDatasetType.CLASSIFICATION)\n\n    test_dataset = AvalancheDataset(\n        test_dataset,\n        transform_groups=transform_groups,\n        initial_transform_group='eval',\n        dataset_type=AvalancheDatasetType.CLASSIFICATION)\n\n    return NCScenario(train_dataset, test_dataset, n_experiences, task_labels,\n                      shuffle, seed, fixed_class_order, per_exp_classes,\n                      class_ids_from_zero_from_first_exp,\n                      class_ids_from_zero_in_each_exp,\n                      reproducibility_data)",
  "def ni_benchmark(\n        train_dataset: Union[\n            Sequence[SupportedDataset], SupportedDataset],\n        test_dataset: Union[\n            Sequence[SupportedDataset], SupportedDataset],\n        n_experiences: int,\n        *,\n        task_labels: bool = False,\n        shuffle: bool = True,\n        seed: Optional[int] = None,\n        balance_experiences: bool = False,\n        min_class_patterns_in_exp: int = 0,\n        fixed_exp_assignment: Optional[Sequence[Sequence[int]]] = None,\n        train_transform=None,\n        eval_transform=None,\n        reproducibility_data: Optional[Dict[str, Any]] = None) \\\n        -> NIScenario:\n    \"\"\"\n    This is the high-level benchmark instances generator for the\n    \"New Instances\" (NI) case. Given a sequence of train and test datasets\n    creates the continual stream of data as a series of experiences.\n\n    This is the reference helper function for creating instances of\n    Domain-Incremental benchmarks.\n\n    The ``task_labels`` parameter determines if each incremental experience has\n    an increasing task label or if, at the contrary, a default task label \"0\"\n    has to be assigned to all experiences. This can be useful when\n    differentiating between Single-Incremental-Task and Multi-Task scenarios.\n\n    There are other important parameters that can be specified in order to tweak\n    the behaviour of the resulting benchmark. Please take a few minutes to read\n    and understand them as they may save you a lot of work.\n\n    This generator features an integrated reproducibility mechanism that allows\n    the user to store and later re-load a benchmark. For more info see the\n    ``reproducibility_data`` parameter.\n\n    :param train_dataset: A list of training datasets, or a single dataset.\n    :param test_dataset: A list of test datasets, or a single test dataset.\n    :param n_experiences: The number of experiences.\n    :param task_labels: If True, each experience will have an ascending task\n            label. If False, the task label will be 0 for all the experiences.\n    :param shuffle: If True, patterns order will be shuffled.\n    :param seed: A valid int used to initialize the random number generator.\n        Can be None.\n    :param balance_experiences: If True, pattern of each class will be equally\n        spread across all experiences. If False, patterns will be assigned to\n        experiences in a complete random way. Defaults to False.\n    :param min_class_patterns_in_exp: The minimum amount of patterns of\n        every class that must be assigned to every experience. Compatible with\n        the ``balance_experiences`` parameter. An exception will be raised if\n        this constraint can't be satisfied. Defaults to 0.\n    :param fixed_exp_assignment: If not None, the pattern assignment\n        to use. It must be a list with an entry for each experience. Each entry\n        is a list that contains the indexes of patterns belonging to that\n        experience. Overrides the ``shuffle``, ``balance_experiences`` and\n        ``min_class_patterns_in_exp`` parameters.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param reproducibility_data: If not None, overrides all the other\n        benchmark definition options, including ``fixed_exp_assignment``.\n        This is usually a dictionary containing data used to\n        reproduce a specific experiment. One can use the\n        ``get_reproducibility_data`` method to get (and even distribute)\n        the experiment setup so that it can be loaded by passing it as this\n        parameter. In this way one can be sure that the same specific\n        experimental setup is being used (for reproducibility purposes).\n        Beware that, in order to reproduce an experiment, the same train and\n        test datasets must be used. Defaults to None.\n\n    :return: A properly initialized :class:`NIScenario` instance.\n    \"\"\"\n\n    seq_train_dataset, seq_test_dataset = train_dataset, test_dataset\n    if isinstance(train_dataset, list) or isinstance(train_dataset, tuple):\n        if len(train_dataset) != len(test_dataset):\n            raise ValueError('Train/test dataset lists must contain the '\n                             'exact same number of datasets')\n\n        seq_train_dataset, seq_test_dataset, _ = \\\n            concat_datasets_sequentially(train_dataset, test_dataset)\n\n    transform_groups = dict(\n        train=(train_transform, None),\n        eval=(eval_transform, None)\n    )\n\n    # Datasets should be instances of AvalancheDataset\n    seq_train_dataset = AvalancheDataset(\n        seq_train_dataset,\n        transform_groups=transform_groups,\n        initial_transform_group='train',\n        dataset_type=AvalancheDatasetType.CLASSIFICATION)\n\n    seq_test_dataset = AvalancheDataset(\n        seq_test_dataset,\n        transform_groups=transform_groups,\n        initial_transform_group='eval',\n        dataset_type=AvalancheDatasetType.CLASSIFICATION)\n\n    return NIScenario(\n        seq_train_dataset, seq_test_dataset,\n        n_experiences,\n        task_labels,\n        shuffle=shuffle, seed=seed,\n        balance_experiences=balance_experiences,\n        min_class_patterns_in_exp=min_class_patterns_in_exp,\n        fixed_exp_assignment=fixed_exp_assignment,\n        reproducibility_data=reproducibility_data)",
  "def _one_dataset_per_exp_class_order(\n        class_list_per_exp: Sequence[Sequence[int]],\n        shuffle: bool, seed: Union[int, None]) -> (List[int], Dict[int, int]):\n    \"\"\"\n    Utility function that shuffles the class order by keeping classes from the\n    same experience together. Each experience is defined by a different entry in\n    the class_list_per_exp parameter.\n\n    :param class_list_per_exp: A list of class lists, one for each experience\n    :param shuffle: If True, the experience order will be shuffled. If False,\n        this function will return the concatenation of lists from the\n        class_list_per_exp parameter.\n    :param seed: If not None, an integer used to initialize the random\n        number generator.\n\n    :returns: A class order that keeps class IDs from the same experience\n        together (adjacent).\n    \"\"\"\n    dataset_order = list(range(len(class_list_per_exp)))\n    if shuffle:\n        if seed is not None:\n            torch.random.manual_seed(seed)\n        dataset_order = torch.as_tensor(dataset_order)[\n            torch.randperm(len(dataset_order))].tolist()\n    fixed_class_order = []\n    classes_per_exp = {}\n    for dataset_position, dataset_idx in enumerate(dataset_order):\n        fixed_class_order.extend(class_list_per_exp[dataset_idx])\n        classes_per_exp[dataset_position] = \\\n            len(class_list_per_exp[dataset_idx])\n    return fixed_class_order, classes_per_exp",
  "def fixed_size_experience_split_strategy(\n        experience_size: int, shuffle: bool, drop_last: bool,\n        experience: Experience):\n    \"\"\"\n    The default splitting strategy used by :func:`data_incremental_benchmark`.\n\n    This splitting strategy simply splits the experience in smaller experiences\n    of size `experience_size`.\n\n    When taking inspiration for your custom splitting strategy, please consider\n    that all parameters preceding `experience` are filled by\n    :func:`data_incremental_benchmark` by using `partial` from the `functools`\n    standard library. A custom splitting strategy must have only a single\n    parameter: the experience. Consider wrapping your custom splitting strategy\n    with `partial` if more parameters are needed.\n\n    Also consider that the stream name of the experience can be obtained by\n    using `experience.origin_stream.name`.\n\n    :param experience_size: The experience size (number of instances).\n    :param shuffle: If True, instances will be shuffled before splitting.\n    :param drop_last: If True, the last mini-experience will be dropped if\n        not of size `experience_size`\n    :param experience: The experience to split.\n    :return: The list of datasets that will be used to create the\n        mini-experiences.\n    \"\"\"\n\n    exp_dataset = experience.dataset\n    exp_indices = list(range(len(exp_dataset)))\n\n    result_datasets = []\n\n    if shuffle:\n        exp_indices = \\\n            torch.as_tensor(exp_indices)[\n                torch.randperm(len(exp_indices))\n            ].tolist()\n\n    init_idx = 0\n    while init_idx < len(exp_indices):\n        final_idx = init_idx + experience_size  # Exclusive\n        if final_idx > len(exp_indices):\n            if drop_last:\n                break\n\n            final_idx = len(exp_indices)\n\n        result_datasets.append(AvalancheSubset(\n            exp_dataset, indices=exp_indices[init_idx:final_idx]))\n        init_idx = final_idx\n\n    return result_datasets",
  "def data_incremental_benchmark(\n        benchmark_instance: GenericCLScenario,\n        experience_size: int,\n        shuffle: bool = False,\n        drop_last: bool = False,\n        split_streams: Sequence[str] = ('train',),\n        custom_split_strategy: Callable[[Experience],\n                                        Sequence[AvalancheDataset]] = None,\n        experience_factory: Callable[[GenericScenarioStream, int],\n                                     Experience] = None):\n    \"\"\"\n    High-level benchmark generator for a Data Incremental setup.\n\n    This generator accepts an existing benchmark instance and returns a version\n    of it in which experiences have been split in order to produce a\n    Data Incremental stream.\n\n    In its base form this generator will split train experiences in experiences\n    of a fixed, configurable, size. The split can be also performed on other\n    streams (like the test one) if needed.\n\n    The `custom_split_strategy` parameter can be used if a more specific\n    splitting is required.\n\n    Beware that experience splitting is NOT executed in a lazy way. This\n    means that the splitting process takes place immediately. Consider\n    optimizing the split process for speed when using a custom splitting\n    strategy.\n\n    Please note that each mini-experience will have a task labels field\n    equal to the one of the originating experience.\n\n    The `complete_test_set_only` field of the resulting benchmark instance\n    will be `True` only if the same field of original benchmark instance is\n    `True` and if the resulting test stream contains exactly one experience.\n\n    :param benchmark_instance: The benchmark to split.\n    :param experience_size: The size of the experience, as an int. Ignored\n        if `custom_split_strategy` is used.\n    :param shuffle: If True, experiences will be split by first shuffling\n        instances in each experience. This will use the default PyTorch\n        random number generator at its current state. Defaults to False.\n        Ignored if `custom_split_strategy` is used.\n    :param drop_last: If True, if the last experience doesn't contain\n        `experience_size` instances, then the last experience will be dropped.\n        Defaults to False. Ignored if `custom_split_strategy` is used.\n    :param split_streams: The list of streams to split. By default only the\n        \"train\" stream will be split.\n    :param custom_split_strategy: A function that implements a custom splitting\n        strategy. The function must accept an experience and return a list\n        of datasets each describing an experience. Defaults to None, which means\n        that the standard splitting strategy will be used (which creates\n        experiences of size `experience_size`).\n        A good starting to understand the mechanism is to look at the\n        implementation of the standard splitting function\n        :func:`fixed_size_experience_split_strategy`.\n\n    :param experience_factory: The experience factory.\n        Defaults to :class:`GenericExperience`.\n    :return: The Data Incremental benchmark instance.\n    \"\"\"\n\n    split_strategy = custom_split_strategy\n    if split_strategy is None:\n        split_strategy = partial(\n            fixed_size_experience_split_strategy, experience_size, shuffle,\n            drop_last)\n\n    stream_definitions: TStreamsUserDict = dict(\n        benchmark_instance.stream_definitions)\n\n    for stream_name in split_streams:\n        if stream_name not in stream_definitions:\n            raise ValueError(f'Stream {stream_name} could not be found in the '\n                             f'benchmark instance')\n\n        stream = getattr(benchmark_instance, f'{stream_name}_stream')\n\n        split_datasets: List[AvalancheDataset] = []\n        split_task_labels: List[Set[int]] = []\n\n        exp: Experience\n        for exp in stream:\n            experiences = split_strategy(exp)\n            split_datasets += experiences\n            for _ in range(len(experiences)):\n                split_task_labels.append(set(exp.task_labels))\n\n        stream_def = StreamUserDef(\n            split_datasets, split_task_labels,\n            stream_definitions[stream_name].origin_dataset,\n            False)\n\n        stream_definitions[stream_name] = stream_def\n\n    complete_test_set_only = benchmark_instance.complete_test_set_only and \\\n        len(stream_definitions['test'].exps_data) == 1\n\n    return GenericCLScenario(stream_definitions=stream_definitions,\n                             complete_test_set_only=complete_test_set_only,\n                             experience_factory=experience_factory)",
  "def random_validation_split_strategy(\n        validation_size: Union[int, float],\n        shuffle: bool,\n        experience: Experience):\n    \"\"\"\n    The default splitting strategy used by\n    :func:`benchmark_with_validation_stream`.\n\n    This splitting strategy simply splits the experience in two experiences (\n    train and validation) of size `validation_size`.\n\n    When taking inspiration for your custom splitting strategy, please consider\n    that all parameters preceding `experience` are filled by\n    :func:`benchmark_with_validation_stream` by using `partial` from the\n    `functools` standard library. A custom splitting strategy must have only\n    a single parameter: the experience. Consider wrapping your custom\n    splitting strategy with `partial` if more parameters are needed.\n\n    Also consider that the stream name of the experience can be obtained by\n    using `experience.origin_stream.name`.\n\n    :param validation_size: The number of instances to allocate to the\n    validation experience. Can be an int value or a float between 0 and 1.\n    :param shuffle: If True, instances will be shuffled before splitting.\n        Otherwise, the first instances will be allocated to the training\n        dataset by leaving the last ones to the validation dataset.\n    :param experience: The experience to split.\n    :return: A tuple containing 2 elements: the new training and validation\n        datasets.\n    \"\"\"\n\n    exp_dataset = experience.dataset\n    exp_indices = list(range(len(exp_dataset)))\n\n    if shuffle:\n        exp_indices = \\\n            torch.as_tensor(exp_indices)[\n                torch.randperm(len(exp_indices))\n            ].tolist()\n\n    if 0.0 <= validation_size <= 1.0:\n        valid_n_instances = int(validation_size * len(exp_dataset))\n    else:\n        valid_n_instances = int(validation_size)\n        if valid_n_instances > len(exp_dataset):\n            raise ValueError(\n                f'Can\\'t create the validation experience: nott enough '\n                f'instances. Required {valid_n_instances}, got only'\n                f'{len(exp_dataset)}')\n\n    train_n_instances = len(exp_dataset) - valid_n_instances\n\n    result_train_dataset = AvalancheSubset(\n        exp_dataset, indices=exp_indices[:train_n_instances])\n    result_valid_dataset = AvalancheSubset(\n        exp_dataset, indices=exp_indices[train_n_instances:])\n\n    return result_train_dataset, result_valid_dataset",
  "def _gen_split(split_generator: Iterable[Tuple[AvalancheDataset,\n                                               AvalancheDataset]]) -> \\\n    Tuple[Generator[AvalancheDataset, None, None],\n          Generator[AvalancheDataset, None, None]]:\n    \"\"\"\n    Internal utility function to split the train-validation generator\n    into two distinct generators (one for the train stream and another one\n    for the valid stream).\n\n    :param split_generator: The lazy stream generator returning tuples of train\n        and valid datasets.\n    :return: Two generators (one for the train, one for the valuid).\n    \"\"\"\n\n    # For more info: https://stackoverflow.com/a/28030261\n    gen_a, gen_b = tee(split_generator, 2)\n    return (a for a, b in gen_a), (b for a, b in gen_b)",
  "def _lazy_train_val_split(\n        split_strategy: Callable[[Experience],\n                                 Tuple[AvalancheDataset, AvalancheDataset]],\n        experiences: Iterable[Experience]) -> \\\n        Generator[Tuple[AvalancheDataset, AvalancheDataset], None, None]:\n    \"\"\"\n    Creates a generator operating around the split strategy and the\n    experiences stream.\n\n    :param split_strategy: The strategy used to split each experience in train\n        and validation datasets.\n    :return: A generator returning a 2 elements tuple (the train and validation\n        datasets).\n    \"\"\"\n\n    for new_experience in experiences:\n        yield split_strategy(new_experience)",
  "def benchmark_with_validation_stream(\n        benchmark_instance: GenericCLScenario,\n        validation_size: Union[int, float],\n        shuffle: bool = False,\n        input_stream: str = 'train',\n        output_stream: str = 'valid',\n        custom_split_strategy: Callable[[Experience],\n                                        Tuple[AvalancheDataset,\n                                              AvalancheDataset]] = None,\n        *,\n        experience_factory: Callable[[GenericScenarioStream, int],\n                                     Experience] = None,\n        lazy_splitting: bool = None):\n    \"\"\"\n    Helper that can be used to obtain a benchmark with a validation stream.\n\n    This generator accepts an existing benchmark instance and returns a version\n    of it in which a validation stream has been added.\n\n    In its base form this generator will split train experiences to extract\n    validation experiences of a fixed (by number of instances or relative\n    size), configurable, size. The split can be also performed on other\n    streams if needed and the name of the resulting validation stream can\n    be configured too.\n\n    Each validation experience will be extracted directly from a single training\n    experience. Patterns selected for the validation experience will be removed\n    from the training one.\n\n    If shuffle is True, the validation stream will be created randomly.\n    Beware that no kind of class balancing is done.\n\n    The `custom_split_strategy` parameter can be used if a more specific\n    splitting is required.\n\n    Please note that the resulting experiences will have a task labels field\n    equal to the one of the originating experience.\n\n    Experience splitting can be executed in a lazy way. This behavior can be\n    controlled using the `lazy_splitting` parameter. By default, experiences\n    are split in a lazy way only when the input stream is lazily generated.\n\n    :param benchmark_instance: The benchmark to split.\n    :param validation_size: The size of the validation experience, as an int\n        or a float between 0 and 1. Ignored if `custom_split_strategy` is used.\n    :param shuffle: If True, patterns will be allocated to the validation\n        stream randomly. This will use the default PyTorch random number\n        generator at its current state. Defaults to False. Ignored if\n        `custom_split_strategy` is used. If False, the first instances will be\n        allocated to the training  dataset by leaving the last ones to the\n        validation dataset.\n    :param input_stream: The name of the input stream. Defaults to 'train'.\n    :param output_stream: The name of the output stream. Defaults to 'valid'.\n    :param custom_split_strategy: A function that implements a custom splitting\n        strategy. The function must accept an experience and return a tuple\n        containing the new train and validation dataset. Defaults to None,\n        which means that the standard splitting strategy will be used (which\n        creates experiences according to `validation_size` and `shuffle`).\n        A good starting to understand the mechanism is to look at the\n        implementation of the standard splitting function\n        :func:`random_validation_split_strategy`.\n    :param experience_factory: The experience factory. Defaults to\n        :class:`GenericExperience`.\n    :param lazy_splitting: If True, the stream will be split in a lazy way.\n        If False, the stream will be split immediately. Defaults to None, which\n        means that the stream will be split in a lazy or non-lazy way depending\n        on the laziness of the `input_stream`.\n    :return: A benchmark instance in which the validation stream has been added.\n    \"\"\"\n\n    split_strategy = custom_split_strategy\n    if split_strategy is None:\n        split_strategy = partial(\n            random_validation_split_strategy, validation_size,\n            shuffle)\n\n    stream_definitions: TStreamsUserDict = dict(\n        benchmark_instance.stream_definitions)\n    streams = benchmark_instance.streams\n\n    if input_stream not in streams:\n        raise ValueError(f'Stream {input_stream} could not be found in the '\n                         f'benchmark instance')\n\n    if output_stream in streams:\n        raise ValueError(f'Stream {output_stream} already exists in the '\n                         f'benchmark instance')\n\n    stream = streams[input_stream]\n\n    split_lazily = lazy_splitting\n    if split_lazily is None:\n        split_lazily = stream_definitions[input_stream].is_lazy\n\n    exps_tasks_labels = list(\n        stream_definitions[input_stream].exps_task_labels\n    )\n\n    if not split_lazily:\n        # Classic static splitting\n        train_exps_source = []\n        valid_exps_source = []\n\n        exp: Experience\n        for exp in stream:\n            train_exp, valid_exp = split_strategy(exp)\n            train_exps_source.append(train_exp)\n            valid_exps_source.append(valid_exp)\n    else:\n        # Lazy splitting (based on a generator)\n        split_generator = _lazy_train_val_split(split_strategy, stream)\n        train_exps_gen, valid_exps_gen = _gen_split(split_generator)\n        train_exps_source = (train_exps_gen, len(stream))\n        valid_exps_source = (valid_exps_gen, len(stream))\n\n    train_stream_def = \\\n        StreamUserDef(\n            train_exps_source,\n            exps_tasks_labels,\n            stream_definitions[input_stream].origin_dataset,\n            split_lazily)\n\n    valid_stream_def = \\\n        StreamUserDef(\n            valid_exps_source,\n            exps_tasks_labels,\n            stream_definitions[input_stream].origin_dataset,\n            split_lazily)\n\n    stream_definitions[input_stream] = train_stream_def\n    stream_definitions[output_stream] = valid_stream_def\n\n    complete_test_set_only = benchmark_instance.complete_test_set_only\n\n    return GenericCLScenario(stream_definitions=stream_definitions,\n                             complete_test_set_only=complete_test_set_only,\n                             experience_factory=experience_factory)",
  "def nc_scenario(\n        train_dataset: Union[\n            Sequence[SupportedDataset], SupportedDataset],\n        test_dataset: Union[\n            Sequence[SupportedDataset], SupportedDataset],\n        n_experiences: int,\n        task_labels: bool,\n        *,\n        shuffle: bool = True,\n        seed: Optional[int] = None,\n        fixed_class_order: Sequence[int] = None,\n        per_exp_classes: Dict[int, int] = None,\n        class_ids_from_zero_from_first_exp: bool = False,\n        class_ids_from_zero_in_each_exp: bool = False,\n        one_dataset_per_exp: bool = False,\n        reproducibility_data: Dict[str, Any] = None) -> NCScenario:\n    \"\"\"\n    This helper function is DEPRECATED in favor of `nc_benchmark`.\n\n    This method is the high-level specific scenario generator for the\n    \"New Classes\" (NC) case. Given a sequence of train and test datasets creates\n    the continual stream of data as a series of experiences. Each experience\n    will contain all the patterns belonging to a certain set of classes and a\n    class won't be assigned to more than one experience.\n\n    The ``task_labels`` parameter determines if each incremental experience has\n    an increasing task label or if, at the contrary, a default task label \"0\"\n    has to be assigned to all experiences. This can be useful when\n    differentiating between Single-Incremental-Task and Multi-Task scenarios.\n\n    There are other important parameters that can be specified in order to tweak\n    the behaviour of the resulting scenario. Please take a few minutes to read\n    and understand them as they may save you a lot of work.\n\n    This generator features a integrated reproducibility mechanism that allows\n    the user to store and later re-load a scenario. For more info see the\n    ``reproducibility_data`` parameter.\n\n    :param train_dataset: A list of training datasets, or a single dataset.\n    :param test_dataset: A list of test datasets, or a single test dataset.\n    :param n_experiences: The number of incremental experience. This is not used\n        when using multiple train/test datasets with the ``one_dataset_per_exp``\n        parameter set to True.\n    :param task_labels: If True, each experience will have an ascending task\n            label. If False, the task label will be 0 for all the experiences.\n    :param shuffle: If True, the class (or experience) order will be shuffled.\n        Defaults to True.\n    :param seed: If ``shuffle`` is True and seed is not None, the class (or\n        experience) order will be shuffled according to the seed. When None, the\n        current PyTorch random number generator state will be used. Defaults to\n        None.\n    :param fixed_class_order: If not None, the class order to use (overrides\n        the shuffle argument). Very useful for enhancing reproducibility.\n        Defaults to None.\n    :param per_exp_classes: Is not None, a dictionary whose keys are\n        (0-indexed) experience IDs and their values are the number of classes\n        to include in the respective experiences. The dictionary doesn't\n        have to contain a key for each experience! All the remaining experiences\n        will contain an equal amount of the remaining classes. The\n        remaining number of classes must be divisible without remainder\n        by the remaining number of experiences. For instance,\n        if you want to include 50 classes in the first experience\n        while equally distributing remaining classes across remaining\n        experiences, just pass the \"{0: 50}\" dictionary as the\n        per_experience_classes parameter. Defaults to None.\n    :param class_ids_from_zero_from_first_exp: If True, original class IDs\n        will be remapped so that they will appear as having an ascending\n        order. For instance, if the resulting class order after shuffling\n        (or defined by fixed_class_order) is [23, 34, 11, 7, 6, ...] and\n        class_ids_from_zero_from_first_exp is True, then all the patterns\n        belonging to class 23 will appear as belonging to class \"0\",\n        class \"34\" will be mapped to \"1\", class \"11\" to \"2\" and so on.\n        This is very useful when drawing confusion matrices and when dealing\n        with algorithms with dynamic head expansion. Defaults to False.\n        Mutually exclusive with the ``class_ids_from_zero_in_each_exp``\n        parameter.\n    :param class_ids_from_zero_in_each_exp: If True, original class IDs\n        will be mapped to range [0, n_classes_in_exp) for each experience.\n        Defaults to False. Mutually exclusive with the\n        ``class_ids_from_zero_from_first_exp`` parameter.\n    :param one_dataset_per_exp: available only when multiple train-test\n        datasets are provided. If True, each dataset will be treated as a\n        experience. Mutually exclusive with the ``per_experience_classes`` and\n        ``fixed_class_order`` parameters. Overrides the ``n_experiences`` \n        parameter. Defaults to False.\n    :param reproducibility_data: If not None, overrides all the other\n        scenario definition options. This is usually a dictionary containing\n        data used to reproduce a specific experiment. One can use the\n        ``get_reproducibility_data`` method to get (and even distribute)\n        the experiment setup so that it can be loaded by passing it as this\n        parameter. In this way one can be sure that the same specific\n        experimental setup is being used (for reproducibility purposes).\n        Beware that, in order to reproduce an experiment, the same train and\n        test datasets must be used. Defaults to None.\n\n    :return: A properly initialized :class:`NCScenario` instance.\n    \"\"\"\n\n    warnings.warn('nc_scenario is deprecated in favor of nc_benchmark.',\n                  DeprecationWarning)\n\n    if class_ids_from_zero_from_first_exp and class_ids_from_zero_in_each_exp:\n        raise ValueError('Invalid mutually exclusive options '\n                         'class_ids_from_zero_from_first_exp and '\n                         'classes_ids_from_zero_in_each_exp set at the '\n                         'same time')\n\n    if isinstance(train_dataset, list) or isinstance(train_dataset, tuple):\n        # Multi-dataset setting\n\n        if len(train_dataset) != len(test_dataset):\n            raise ValueError('Train/test dataset lists must contain the '\n                             'exact same number of datasets')\n\n        if per_exp_classes and one_dataset_per_exp:\n            raise ValueError(\n                'Both per_experience_classes and one_dataset_per_exp are'\n                'used, but those options are mutually exclusive')\n\n        if fixed_class_order and one_dataset_per_exp:\n            raise ValueError(\n                'Both fixed_class_order and one_dataset_per_exp are'\n                'used, but those options are mutually exclusive')\n\n        seq_train_dataset, seq_test_dataset, mapping = \\\n            concat_datasets_sequentially(train_dataset, test_dataset)\n\n        if one_dataset_per_exp:\n            # If one_dataset_per_exp is True, each dataset will be treated as\n            # a experience. In this scenario, shuffle refers to the experience\n            # order, not to the class one.\n            fixed_class_order, per_exp_classes = \\\n                _one_dataset_per_exp_class_order(mapping, shuffle, seed)\n\n            # We pass a fixed_class_order to the NCGenericScenario\n            # constructor, so we don't need shuffling.\n            shuffle = False\n            seed = None\n\n            # Overrides n_experiences (and per_experience_classes, already done)\n            n_experiences = len(train_dataset)\n        train_dataset, test_dataset = seq_train_dataset, seq_test_dataset\n\n    # Datasets should be instances of AvalancheDataset\n    train_dataset = as_classification_dataset(train_dataset).train()\n    test_dataset = as_classification_dataset(test_dataset).eval()\n\n    return NCScenario(train_dataset, test_dataset, n_experiences, task_labels,\n                      shuffle, seed, fixed_class_order, per_exp_classes,\n                      class_ids_from_zero_from_first_exp,\n                      class_ids_from_zero_in_each_exp,\n                      reproducibility_data)",
  "def ni_scenario(\n        train_dataset: Union[\n            Sequence[SupportedDataset], SupportedDataset],\n        test_dataset: Union[\n            Sequence[SupportedDataset], SupportedDataset],\n        n_experiences: int,\n        *,\n        task_labels: bool = False,\n        shuffle: bool = True,\n        seed: Optional[int] = None,\n        balance_experiences: bool = False,\n        min_class_patterns_in_exp: int = 0,\n        fixed_exp_assignment: Optional[Sequence[Sequence[int]]] = None,\n        reproducibility_data: Optional[Dict[str, Any]] = None) \\\n        -> NIScenario:\n    \"\"\"\n    This helper function is DEPRECATED in favor of `ni_benchmark`.\n\n    This method is the high-level specific scenario generator for the\n    \"New Instances\" (NI) case. Given a sequence of train and test datasets\n    creates the continual stream of data as a series of experiences. Each\n    experience will contain patterns belonging to the same classes.\n\n    The ``task_labels`` parameter determines if each incremental experience has\n    an increasing task label or if, at the contrary, a default task label \"0\"\n    has to be assigned to all experiences. This can be useful when\n    differentiating between Single-Incremental-Task and Multi-Task scenarios.\n\n    There are other important parameters that can be specified in order to tweak\n    the behaviour of the resulting scenario. Please take a few minutes to read\n    and understand them as they may save you a lot of work.\n\n    This generator features an integrated reproducibility mechanism that allows\n    the user to store and later re-load a scenario. For more info see the\n    ``reproducibility_data`` parameter.\n\n    :param train_dataset: A list of training datasets, or a single dataset.\n    :param test_dataset: A list of test datasets, or a single test dataset.\n    :param n_experiences: The number of experiences.\n    :param task_labels: If True, each experience will have an ascending task\n            label. If False, the task label will be 0 for all the experiences.\n    :param shuffle: If True, patterns order will be shuffled.\n    :param seed: A valid int used to initialize the random number generator.\n        Can be None.\n    :param balance_experiences: If True, pattern of each class will be equally\n        spread across all experiences. If False, patterns will be assigned to\n        experiences in a complete random way. Defaults to False.\n    :param min_class_patterns_in_exp: The minimum amount of patterns of\n        every class that must be assigned to every experience. Compatible with\n        the ``balance_experiences`` parameter. An exception will be raised if\n        this constraint can't be satisfied. Defaults to 0.\n    :param fixed_exp_assignment: If not None, the pattern assignment\n        to use. It must be a list with an entry for each experience. Each entry\n        is a list that contains the indexes of patterns belonging to that\n        experience. Overrides the ``shuffle``, ``balance_experiences`` and\n        ``min_class_patterns_in_exp`` parameters.\n    :param reproducibility_data: If not None, overrides all the other\n        scenario definition options, including ``fixed_exp_assignment``.\n        This is usually a dictionary containing data used to\n        reproduce a specific experiment. One can use the\n        ``get_reproducibility_data`` method to get (and even distribute)\n        the experiment setup so that it can be loaded by passing it as this\n        parameter. In this way one can be sure that the same specific\n        experimental setup is being used (for reproducibility purposes).\n        Beware that, in order to reproduce an experiment, the same train and\n        test datasets must be used. Defaults to None.\n\n    :return: A properly initialized :class:`NIScenario` instance.\n    \"\"\"\n\n    warnings.warn('ni_scenario is deprecated in favor of ni_benchmark.',\n                  DeprecationWarning)\n\n    seq_train_dataset, seq_test_dataset = train_dataset, test_dataset\n    if isinstance(train_dataset, list) or isinstance(train_dataset, tuple):\n        if len(train_dataset) != len(test_dataset):\n            raise ValueError('Train/test dataset lists must contain the '\n                             'exact same number of datasets')\n\n        seq_train_dataset, seq_test_dataset, _ = \\\n            concat_datasets_sequentially(train_dataset, test_dataset)\n\n    # Datasets should be instances of AvalancheDataset\n    seq_train_dataset = as_classification_dataset(seq_train_dataset).train()\n    seq_test_dataset = as_classification_dataset(seq_test_dataset).eval()\n\n    return NIScenario(\n        seq_train_dataset, seq_test_dataset,\n        n_experiences,\n        task_labels,\n        shuffle=shuffle, seed=seed,\n        balance_experiences=balance_experiences,\n        min_class_patterns_in_exp=min_class_patterns_in_exp,\n        fixed_exp_assignment=fixed_exp_assignment,\n        reproducibility_data=reproducibility_data)",
  "def dataset_scenario(\n        train_dataset_list: Sequence[SupportedDataset],\n        test_dataset_list: Sequence[SupportedDataset],\n        task_labels: Sequence[int],\n        *,\n        complete_test_set_only: bool = False,\n        dataset_type: AvalancheDatasetType = AvalancheDatasetType.UNDEFINED) \\\n        -> GenericCLScenario:\n    \"\"\"\n    This helper function is DEPRECATED in favor of `dataset_benchmark`.\n\n    Creates a generic scenario given a list of datasets and the respective task\n    labels. Each training dataset will be considered as a separate training\n    experience. Contents of the datasets will not be changed, including the\n    targets.\n\n    When loading the datasets from a set of fixed file lists, consider using\n    the :func:`filelist_scenario` helper method instead. Also, loading from\n    a list of paths is supported through the :func:`paths_scenario` helper.\n\n    In its base form, this function accepts a list of test datasets that must\n    contain the same amount of datasets of the training list.\n    Those pairs are then used to create the \"past\", \"cumulative\"\n    (a.k.a. growing) and \"future\" test sets. However, in certain Continual\n    Learning scenarios only the concept of \"complete\" test set makes sense. In\n    that case, the ``complete_test_set_only`` parameter should be set to True\n    (see the parameter description for more info).\n\n    Beware that pattern transformations must already be included in the\n    datasets (when needed).\n\n    :param train_dataset_list: A list of training datasets.\n    :param test_dataset_list: A list of test datasets.\n    :param task_labels: A list of task labels. Must contain the same amount of\n        elements of the ``train_dataset_list`` parameter. For\n        Single-Incremental-Task (a.k.a. Task-Free) scenarios, this is usually\n        a list of zeros. For Multi Task scenario, this is usually a list of\n        ascending task labels (starting from 0).\n    :param complete_test_set_only: If True, only the complete test set will\n        be returned by the scenario. This means that the ``test_dataset_list``\n        parameter must be list with a single element (the complete test set).\n        Defaults to False, which means that ``train_dataset_list`` and\n        ``test_dataset_list`` must contain the same amount of datasets.\n    :param dataset_type: The type of the dataset. Defaults to None, which\n        means that the type will be obtained from the input datasets. If input\n        datasets are not instances of :class:`AvalancheDataset`, the type\n        UNDEFINED will be used.\n\n    :returns: A properly initialized :class:`GenericCLScenario` instance.\n    \"\"\"\n\n    warnings.warn('dataset_scenario is deprecated in favor of '\n                  'dataset_benchmark.', DeprecationWarning)\n\n    return create_multi_dataset_generic_scenario(\n        train_dataset_list=train_dataset_list,\n        test_dataset_list=test_dataset_list,\n        task_labels=task_labels,\n        complete_test_set_only=complete_test_set_only,\n        dataset_type=dataset_type)",
  "def filelist_scenario(\n        root: Union[str, Path],\n        train_file_lists: Sequence[Union[str, Path]],\n        test_file_lists: Union[Union[str, Path], Sequence[Union[str, Path]]],\n        task_labels: Sequence[int],\n        *,\n        complete_test_set_only: bool = False,\n        train_transform=None, train_target_transform=None,\n        eval_transform=None, eval_target_transform=None) -> GenericCLScenario:\n    \"\"\"\n    This helper function is DEPRECATED in favor of `filelist_benchmark`.\n\n    Creates a generic scenario given a list of filelists and the respective task\n    labels. A separate dataset will be created for each filelist and each of\n    those training datasets will be considered a separate training experience.\n\n    In its base form, this function accepts a list of filelists for the test\n    datsets that must contain the same amount of elements of the training list.\n    Those pairs of datasets are then used to create the \"past\", \"cumulative\"\n    (a.k.a. growing) and \"future\" test sets. However, in certain Continual\n    Learning scenarios only the concept of \"complete\" test set makes sense. In\n    that case, the ``complete_test_set_only`` should be set to True (see the\n    parameter description for more info).\n\n    This helper functions is the best shot when loading Caffe-style dataset\n    based on filelists.\n\n    The resulting benchmark instance and the intermediate datasets used to\n    populate it will be of type CLASSIFICATION.\n\n    :param root: The root path of the dataset.\n    :param train_file_lists: A list of filelists describing the\n        paths of the training patterns for each experience.\n    :param test_file_lists: A list of filelists describing the\n        paths of the test patterns for each experience.\n    :param task_labels: A list of task labels. Must contain the same amount of\n        elements of the ``train_file_lists`` parameter. For\n        Single-Incremental-Task (a.k.a. Task-Free) scenarios, this is usually\n        a list of zeros. For Multi Task scenario, this is usually a list of\n        ascending task labels (starting from 0).\n    :param complete_test_set_only: If True, only the complete test set will\n        be returned by the scenario. This means that the ``test_file_lists``\n        parameter must be list with a single element (the complete test set).\n        Alternatively, can be a plain string or :class:`Path` object.\n        Defaults to False, which means that ``train_file_lists`` and\n        ``test_file_lists`` must contain the same amount of filelists paths.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param train_target_transform: The transformation to apply to training\n        patterns targets. Defaults to None.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param eval_target_transform: The transformation to apply to test\n        patterns targets. Defaults to None.\n\n    :returns: A properly initialized :class:`GenericCLScenario` instance.\n    \"\"\"\n\n    warnings.warn('filelist_scenario is deprecated in favor of '\n                  'filelist_benchmark.', DeprecationWarning)\n\n    return create_generic_scenario_from_filelists(\n        root=root,\n        train_file_lists=train_file_lists,\n        test_file_lists=test_file_lists,\n        task_labels=task_labels,\n        complete_test_set_only=complete_test_set_only,\n        train_transform=train_transform,\n        train_target_transform=train_target_transform,\n        eval_transform=eval_transform,\n        eval_target_transform=eval_target_transform)",
  "def paths_scenario(\n        train_list_of_files: Sequence[Sequence[FileAndLabel]],\n        test_list_of_files: Union[Sequence[FileAndLabel],\n                                  Sequence[Sequence[FileAndLabel]]],\n        task_labels: Sequence[int],\n        *,\n        complete_test_set_only: bool = False,\n        train_transform=None, train_target_transform=None,\n        eval_transform=None, eval_target_transform=None,\n        dataset_type: AvalancheDatasetType = AvalancheDatasetType.UNDEFINED) \\\n        -> GenericCLScenario:\n    \"\"\"\n    This helper function is DEPRECATED in favor of `paths_benchmark`.\n\n    Creates a generic scenario given a list of files and class labels.\n    A separate dataset will be created for each list and each of\n    those training datasets will be considered a separate training experience.\n\n    This is very similar to `filelist_scenario`, with the main difference being\n    that `filelist_scenario` accepts, for each experience, a file list formatted\n    in Caffe-style. On the contrary, this accepts a list of tuples where each\n    tuple contains two elements: the full path to the pattern and its label.\n    Optionally, the tuple may contain a third element describing the bounding\n    box of the element to crop. This last bounding box may be useful when trying\n    to extract the part of the image depicting the desired element.\n\n    In its base form, this function accepts a list of lists of tuples for the\n    test datsets that must contain the same amount of lists of the training\n    list. Those pairs of datasets are then used to create the \"past\",\n    \"cumulative\" (a.k.a. growing) and \"future\" test sets. However, in certain\n    Continual Learning scenarios only the concept of \"complete\" test set makes\n    sense. In that case, the ``complete_test_set_only`` should be set to True\n    (see the parameter description for more info).\n\n    The label of each pattern doesn't have to be an int.\n\n    :param train_list_of_files: A list of lists. Each list describes the paths\n        and labels of patterns to include in that training experience as tuples.\n        Each tuple must contain two elements: the full path to the pattern\n        and its class label. Optionally, the tuple may contain a third element\n        describing the bounding box to use for cropping (top, left, height,\n        width).\n    :param test_list_of_files: A list of lists. Each list describes the paths\n        and labels of patterns to include in that test experience as tuples.\n        Each tuple must contain two elements: the full path to the pattern\n        and its class label. Optionally, the tuple may contain a third element\n        describing the bounding box to use for cropping (top, left, height,\n        width).\n    :param task_labels: A list of task labels. Must contain the same amount of\n        elements of the ``train_file_lists`` parameter. For\n        Single-Incremental-Task (a.k.a. Task-Free) scenarios, this is usually\n        a list of zeros. For Multi Task scenario, this is usually a list of\n        ascending task labels (starting from 0).\n    :param complete_test_set_only: If True, only the complete test set will\n        be returned by the scenario. This means that the ``test_file_lists``\n        parameter must be list with a single element (the complete test set).\n        Alternatively, can be a plain string or :class:`Path` object.\n        Defaults to False, which means that ``train_file_lists`` and\n        ``test_file_lists`` must contain the same amount of filelists paths.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param train_target_transform: The transformation to apply to training\n        patterns targets. Defaults to None.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param eval_target_transform: The transformation to apply to test\n        patterns targets. Defaults to None.\n    :param dataset_type: The type of the dataset. Defaults to UNDEFINED.\n\n    :returns: A properly initialized :class:`GenericCLScenario` instance.\n    \"\"\"\n\n    warnings.warn('paths_scenario is deprecated in favor of paths_benchmark.',\n                  DeprecationWarning)\n\n    return create_generic_scenario_from_paths(\n        train_list_of_files=train_list_of_files,\n        test_list_of_files=test_list_of_files,\n        task_labels=task_labels,\n        complete_test_set_only=complete_test_set_only,\n        train_transform=train_transform,\n        train_target_transform=train_target_transform,\n        eval_transform=eval_transform,\n        eval_target_transform=eval_target_transform,\n        dataset_type=dataset_type)",
  "def tensors_scenario(\n        train_tensors: Sequence[Sequence[Any]],\n        test_tensors: Sequence[Sequence[Any]],\n        task_labels: Sequence[int],\n        *,\n        complete_test_set_only: bool = False,\n        train_transform=None, train_target_transform=None,\n        eval_transform=None, eval_target_transform=None,\n        dataset_type: AvalancheDatasetType = AvalancheDatasetType.UNDEFINED) \\\n        -> GenericCLScenario:\n    \"\"\"\n    This helper function is DEPRECATED in favor of `tensors_benchmark`.\n\n    Creates a generic scenario given lists of Tensors and the respective task\n    labels. A separate dataset will be created from each Tensor tuple\n    (x, y, ...) and each of those training datasets will be considered a\n    separate training experience. Using this helper function is the lowest-level\n    way to create a Continual Learning scenario. When possible, consider using\n    higher level helpers.\n\n    Experiences are defined by passing lists of tensors as the `train_tensors`\n    and `test_tensors` parameter. Those parameters must be lists containing\n    sub-lists of tensors, one for each experience. Each tensor defines the value\n    of a feature (\"x\", \"y\", \"z\", ...) for all patterns of that experience.\n\n    By default the second tensor of each experience will be used to fill the\n    `targets` value (label of each pattern).\n\n    In its base form, the test lists must contain the same amount of elements of\n    the training lists. Those pairs of datasets are then used to create the\n    \"past\", \"cumulative\" (a.k.a. growing) and \"future\" test sets.\n    However, in certain Continual Learning scenarios only the concept of\n    \"complete\" test set makes sense. In that case, the\n    ``complete_test_set_only`` should be set to True (see the parameter\n    description for more info).\n\n    :param train_tensors: A list of lists. The first list must contain the\n        tensors for the first training experience (one tensor per feature), the\n        second list must contain the tensors for the second training experience,\n        and so on.\n    :param test_tensors: A list of lists. The first list must contain the\n        tensors for the first test experience (one tensor per feature), the\n        second list must contain the tensors for the second test experience,\n        and so on.\n    :param task_labels: A list of task labels. Must contain a task label for\n        each experience. For Single-Incremental-Task (a.k.a. Task-Free)\n        scenarios, this is usually a list of zeros. For Multi Task scenario,\n        this is usually a list of ascending task labels (starting from 0).\n    :param complete_test_set_only: If True, only the complete test set will\n        be returned by the scenario. This means that ``test_tensors`` must\n        define a single experience. Defaults to False, which means that\n        ``train_tensors`` and ``test_tensors`` must define the same\n        amount of experiences.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param train_target_transform: The transformation to apply to training\n        patterns targets. Defaults to None.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param eval_target_transform: The transformation to apply to test\n        patterns targets. Defaults to None.\n    :param dataset_type: The type of the dataset. Defaults to UNDEFINED.\n\n    :returns: A properly initialized :class:`GenericCLScenario` instance.\n    \"\"\"\n\n    warnings.warn('tensors_scenario is deprecated in favor of '\n                  'tensors_benchmark.', DeprecationWarning)\n\n    return create_generic_scenario_from_tensor_lists(\n        train_tensors=train_tensors,\n        test_tensors=test_tensors,\n        task_labels=task_labels,\n        complete_test_set_only=complete_test_set_only,\n        train_transform=train_transform,\n        train_target_transform=train_target_transform,\n        eval_transform=eval_transform,\n        eval_target_transform=eval_target_transform,\n        dataset_type=dataset_type)",
  "def tensor_scenario(\n        train_data_x: Sequence[Any],\n        train_data_y: Sequence[Sequence[SupportsInt]],\n        test_data_x: Union[Any, Sequence[Any]],\n        test_data_y: Union[Any, Sequence[Sequence[SupportsInt]]],\n        task_labels: Sequence[int],\n        *,\n        complete_test_set_only: bool = False,\n        train_transform=None, train_target_transform=None,\n        eval_transform=None, eval_target_transform=None,\n        dataset_type: AvalancheDatasetType = AvalancheDatasetType.UNDEFINED) \\\n        -> GenericCLScenario:\n    \"\"\"\n    This helper function is DEPRECATED in favor of `tensors_benchmark`.\n\n    Please consider using :func:`tensors_benchmark` instead. When switching to\n    the new function, please keep in mind that the format of the parameters is\n    completely different!\n\n    Creates a generic scenario given lists of Tensors and the respective task\n    labels. A separate dataset will be created from each Tensor pair (x + y)\n    and each of those training datasets will be considered a separate\n    training experience. Contents of the datasets will not be changed, including\n    the targets. Using this helper function is the lower level way to create a\n    Continual Learning scenario. When possible, consider using higher level\n    helpers.\n\n    By default the second tensor of each experience will be used to fill the\n    `targets` value (label of each pattern).\n\n    In its base form, the test lists must contain the same amount of elements of\n    the training lists. Those pairs of datasets are then used to create the\n    \"past\", \"cumulative\" (a.k.a. growing) and \"future\" test sets.\n    However, in certain Continual Learning scenarios only the concept of\n    \"complete\" test set makes sense. In that case, the\n    ``complete_test_set_only`` should be set to True (see the parameter\n    description for more info).\n\n    :param train_data_x: A list of Tensors (one per experience) containing the\n        patterns of the training sets.\n    :param train_data_y: A list of Tensors or int lists containing the\n        labels of the patterns of the training sets. Must contain the same\n        number of elements of ``train_datasets_x``.\n    :param test_data_x: A Tensor or a list of Tensors (one per experience)\n        containing the patterns of the test sets.\n    :param test_data_y: A Tensor or a list of Tensors or int lists containing\n        the labels of the patterns of the test sets. Must contain the same\n        number of elements of ``test_datasets_x``.\n    :param task_labels: A list of task labels. Must contain the same amount of\n        elements of the ``train_datasets_x`` parameter. For\n        Single-Incremental-Task (a.k.a. Task-Free) scenarios, this is usually\n        a list of zeros. For Multi Task scenario, this is usually a list of\n        ascending task labels (starting from 0).\n    :param complete_test_set_only: If True, only the complete test set will\n        be returned by the scenario. This means that the ``test_datasets_x`` and\n        ``test_datasets_y`` parameters must be lists with a single element\n        (the complete test set). Defaults to False, which means that\n        ``train_file_lists`` and ``test_file_lists`` must contain the same\n        amount of filelists paths.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param train_target_transform: The transformation to apply to training\n        patterns targets. Defaults to None.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param eval_target_transform: The transformation to apply to test\n        patterns targets. Defaults to None.\n    :param dataset_type: The type of the dataset. Defaults to UNDEFINED.\n\n    :returns: A properly initialized :class:`GenericCLScenario` instance.\n    \"\"\"\n\n    warnings.warn('tensor_scenario is deprecated in favor '\n                  'of tensors_benchmark. When switching'\n                  ' to the new function, please keep in mind that the format of'\n                  ' the parameters is completely different!',\n                  DeprecationWarning)\n\n    if isinstance(test_data_x, Tensor):\n        test_data_x = [test_data_x]\n        test_data_y = [test_data_y]\n    else:\n        if len(test_data_x) != len(test_data_y):\n            raise ValueError('test_data_x and test_data_y must contain'\n                             ' the same amount of elements')\n\n    if len(train_data_x) != len(train_data_y):\n        raise ValueError('train_data_x and train_data_y must contain'\n                         ' the same amount of elements')\n\n    exp_train_first_structure = []\n    exp_test_first_structure = []\n    for exp_idx in range(len(train_data_x)):\n        exp_x = train_data_x[exp_idx]\n        exp_y = train_data_y[exp_idx]\n\n        exp_train_first_structure.append([exp_x, exp_y])\n\n    for exp_idx in range(len(test_data_x)):\n        exp_x = test_data_x[exp_idx]\n        exp_y = test_data_y[exp_idx]\n\n        exp_test_first_structure.append([exp_x, exp_y])\n\n    return tensors_scenario(\n        train_tensors=exp_train_first_structure,\n        test_tensors=exp_test_first_structure,\n        task_labels=task_labels,\n        complete_test_set_only=complete_test_set_only,\n        train_transform=train_transform,\n        train_target_transform=train_target_transform,\n        eval_transform=eval_transform,\n        eval_target_transform=eval_target_transform,\n        dataset_type=dataset_type)",
  "def _one_dataset_per_exp_class_order(\n        class_list_per_exp: Sequence[Sequence[int]],\n        shuffle: bool, seed: Union[int, None]) -> (List[int], Dict[int, int]):\n    \"\"\"\n    Utility function that shuffles the class order by keeping classes from the\n    same experience together. Each experience is defined by a different entry in\n    the class_list_per_exp parameter.\n\n    :param class_list_per_exp: A list of class lists, one for each experience\n    :param shuffle: If True, the experience order will be shuffled. If False,\n        this function will return the concatenation of lists from the\n        class_list_per_exp parameter.\n    :param seed: If not None, an integer used to initialize the random\n        number generator.\n\n    :returns: A class order that keeps class IDs from the same experience\n        together (adjacent).\n    \"\"\"\n    dataset_order = list(range(len(class_list_per_exp)))\n    if shuffle:\n        if seed is not None:\n            torch.random.manual_seed(seed)\n        dataset_order = torch.as_tensor(dataset_order)[\n            torch.randperm(len(dataset_order))].tolist()\n    fixed_class_order = []\n    classes_per_exp = {}\n    for dataset_position, dataset_idx in enumerate(dataset_order):\n        fixed_class_order.extend(class_list_per_exp[dataset_idx])\n        classes_per_exp[dataset_position] = \\\n            len(class_list_per_exp[dataset_idx])\n    return fixed_class_order, classes_per_exp",
  "class DownloadableDataset(Dataset[T_co], ABC):\n    \"\"\"\n    Base class for a downloadable dataset.\n\n    It is recommended to extend this class if a dataset can be downloaded from\n    the internet. This implementation codes the recommended behaviour for\n    downloading and verifying the dataset.\n\n    The dataset child class must implement the `_download_dataset`,\n    `_load_metadata` and `_download_error_message` methods\n\n    The child class, in its constructor, must call the already implemented\n    `_load_dataset` method (otherwise nothing will happen).\n\n    A further simplification can be obtained by extending\n    :class:`SimpleDownloadableDataset` instead of this class.\n    :class:`SimpleDownloadableDataset` is recommended if a single archive is to\n    be downloaded and extracted to the root folder \"as is\".\n\n    The standardized procedure coded by `_load_dataset` is as follows:\n\n    - First, `_load_metadata` is called to check if the dataset can be correctly\n      loaded at the `root` path. This method must check if the data found\n      at the `root` path is correct and that metadata can be correctly loaded.\n      If this method succeeds (by returning True) the process is completed.\n    - If `_load_metadata` fails (by returning False or by raising an error),\n      then a download will be attempted if the download flag was set to True.\n      The download must be implemented in `_download_dataset`. The\n      procedure can be drastically simplified by using the `_download_file`,\n      `_extract_archive` and `_download_and_extract_archive` helpers.\n    - If the download succeeds (doesn't raise an error), then `_load_metadata`\n      will be called again.\n\n    If an error occurs, the `_download_error_message` will be called to obtain\n    a message (a string) to show to the user. That message should contain\n    instructions on how to download and prepare the dataset manually.\n    \"\"\"\n    def __init__(\n            self,\n            root: Union[str, Path],\n            download: bool = True,\n            verbose: bool = False):\n        \"\"\"\n        Creates an instance of a downloadable dataset.\n\n        Consider looking at the class documentation for the precise details on\n        how to extend this class.\n\n        Beware that calling this constructor only fills the `root` field. The\n        download and metadata loading procedures are triggered only by\n        calling `_load_dataset`.\n\n        :param root: The root path where the dataset will be downloaded.\n            Consider passing a path obtained by calling\n            `default_dataset_location` with the name of the dataset.\n        :param download: If True, the dataset will be downloaded if needed.\n            If False and the dataset can't be loaded from the provided root\n            path, an error will be raised when calling the `_load_dataset`\n            method. Defaults to True.\n        :param verbose: If True, some info about the download process will be\n            printed. Defaults to False.\n        \"\"\"\n\n        super(DownloadableDataset, self).__init__()\n        self.root: Path = Path(root)\n        \"\"\"\n        The path to the dataset.\n        \"\"\"\n\n        self.download: bool = download\n        \"\"\"\n        If True, the dataset will be downloaded (only if needed).\n        \"\"\"\n\n        self.verbose: bool = verbose\n        \"\"\"\n        If True, some info about the download process will be printed.\n        \"\"\"\n\n    def _load_dataset(self) -> None:\n        \"\"\"\n        The standardized dataset download and load procedure.\n\n        For more details on the coded procedure see the class documentation.\n\n        This method shouldn't be overridden.\n\n        This method will raise and error if the dataset couldn't be loaded\n        or downloaded.\n\n        :return: None\n        \"\"\"\n        metadata_loaded = False\n        metadata_load_error = None\n        try:\n            metadata_loaded = self._load_metadata()\n        except Exception as e:\n            metadata_load_error = e\n\n        if metadata_loaded:\n            if self.verbose:\n                print('Files already downloaded and verified')\n            return\n\n        if not self.download:\n            msg = 'Error loading dataset metadata (dataset download was ' \\\n                  'not attempted as \"download\" is set to False)'\n            if metadata_load_error is None:\n                raise RuntimeError(msg)\n            else:\n                print(msg)\n                raise metadata_load_error\n\n        try:\n            self._download_dataset()\n        except Exception as e:\n            err_msg = self._download_error_message()\n            print(err_msg, flush=True)\n            raise e\n\n        if not self._load_metadata():\n            err_msg = self._download_error_message()\n            print(err_msg)\n            raise RuntimeError(\n                'Error loading dataset metadata (... but the download '\n                'procedure completed successfully)')\n\n    @abstractmethod\n    def _download_dataset(self) -> None:\n        \"\"\"\n        The download procedure.\n\n        This procedure is called only if `_load_metadata` fails.\n\n        This method must raise an error if the dataset can't be downloaded.\n\n        Hints: don't re-invent the wheel! There are ready-to-use helper methods\n        like `_download_and_extract_archive`, `_download_file` and\n        `_extract_archive` that can be used.\n\n        :return: None\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _load_metadata(self) -> bool:\n        \"\"\"\n        The dataset metadata loading procedure.\n\n        This procedure is called at least once to load the dataset metadata.\n\n        This procedure should return False if the dataset is corrupted or if it\n        can't be loaded.\n\n        :return: True if the dataset is not corrupted and could be successfully\n        loaded.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _download_error_message(self) -> str:\n        \"\"\"\n        Returns the error message hinting the user on how to download the\n        dataset manually.\n\n        :return: A string representing the message to show to the user.\n        \"\"\"\n        pass\n\n    def _cleanup_dataset_root(self):\n        \"\"\"\n        Utility method that can be used to remove the dataset root directory.\n\n        Can be useful if a cleanup is needed when downloading and extracting the\n        dataset.\n\n        This method will also re-create the root directory.\n\n        :return: None\n        \"\"\"\n        shutil.rmtree(self.root)\n        self.root.mkdir(parents=True, exist_ok=True)\n\n    def _download_file(self,\n                       url: str,\n                       file_name: str,\n                       checksum: Optional[str]) -> Path:\n        \"\"\"\n        Utility method that can be used to download and verify a file.\n\n        :param url: The download url.\n        :param file_name: The name of the file to save. The file will be saved\n            in the `root` with this name. Always fill this parameter.\n            Don't pass a path! Pass a file name only!\n        :param checksum: The MD5 hash to use when verifying the downloaded\n            file. Can be None, in which case the check will be skipped.\n            It is recommended to always fill this parameter.\n        :return: The path to the downloaded file.\n        \"\"\"\n        self.root.mkdir(parents=True, exist_ok=True)\n        download_url(url, str(self.root), filename=file_name,\n                     md5=checksum)\n        return self.root / file_name\n\n    def _extract_archive(self,\n                         path: Union[str, Path],\n                         sub_directory: str = None,\n                         remove_archive: bool = False) -> Path:\n        \"\"\"\n        Utility method that can be used to extract an archive.\n\n        :param path: The complete path to the archive (for instance obtained\n            by calling `_download_file`).\n        :param sub_directory: The name of the sub directory where to extract the\n            archive. Can be None, which means that the archive will be extracted\n            in the root. Beware that some archives already have a root directory\n            inside of them, in which case it's probably better to use None here.\n            Defaults to None.\n        :param remove_archive: If True, the archive will be deleted after a\n            successful extraction. Defaults to False.\n        :return:\n        \"\"\"\n\n        if sub_directory is None:\n            extract_root = self.root\n        else:\n            extract_root = self.root / sub_directory\n\n        extract_archive(str(path), to_path=str(extract_root),\n                        remove_finished=remove_archive)\n\n        return extract_root\n\n    def _download_and_extract_archive(\n            self, url: str, file_name: str, checksum: Optional[str],\n            sub_directory: str = None, remove_archive: bool = False) -> Path:\n        \"\"\"\n        Utility that downloads and extracts an archive.\n\n        :param url: The download url.\n        :param file_name: The name of the archive. The file will be saved\n            in the `root` with this name. Always fill this parameter.\n            Don't pass a path! Pass a file name only!\n        :param checksum: The MD5 hash to use when verifying the downloaded\n            archive. Can be None, in which case the check will be skipped.\n            It is recommended to always fill this parameter.\n        :param sub_directory: The name of the sub directory where to extract the\n            archive. Can be None, which means that the archive will be extracted\n            in the root. Beware that some archives already have a root directory\n            inside of them, in which case it's probably better to use None here.\n            Defaults to None.\n        :param remove_archive: If True, the archive will be deleted after a\n            successful extraction. Defaults to False.\n        :return: The path to the extracted archive. If `sub_directory` is None,\n            then this will be the `root` path.\n        \"\"\"\n        if sub_directory is None:\n            extract_root = self.root\n        else:\n            extract_root = self.root / sub_directory\n\n        self.root.mkdir(parents=True, exist_ok=True)\n        download_and_extract_archive(\n            url, str(self.root), extract_root=str(extract_root),\n            filename=file_name, md5=checksum, remove_finished=remove_archive)\n\n        return extract_root\n\n    def _check_file(self, path: Union[str, Path], checksum: str) -> bool:\n        \"\"\"\n        Utility method to check a file.\n\n        :param path: The path to the file.\n        :param checksum: The MD5 hash to use.\n        :return: True if the MD5 hash of the file matched the given one.\n        \"\"\"\n        return check_integrity(str(path), md5=checksum)",
  "class SimpleDownloadableDataset(DownloadableDataset[T_co], ABC):\n    \"\"\"\n    Base class for a downloadable dataset consisting of a single archive file.\n\n    It is recommended to extend this class if a dataset can be downloaded from\n    the internet as a single archive. For multi-file implementation or if\n    a more fine-grained control is required, consider extending\n    :class:`DownloadableDataset` instead.\n\n    This is a simplified version of :class:`DownloadableDataset` where the\n    following assumptions must hold:\n    - The dataset is made of a single archive.\n    - The archive must be extracted to the root folder \"as is\" (which means\n        that no subdirectories must be created).\n\n    The child class is only required to extend the `_load_metadata` method,\n    which must check the dataset integrity and load the dataset metadata.\n\n    Apart from that, the same assumptions of :class:`DownloadableDataset` hold.\n    Remember to call the `_load_dataset` method in the child class constructor.\n    \"\"\"\n    def __init__(\n            self,\n            root_or_dataset_name: str,\n            url: str,\n            checksum: Optional[str],\n            download: bool = False,\n            verbose: bool = False):\n        \"\"\"\n        Creates an instance of a simple downloadable dataset.\n\n        Consider looking at the class documentation for the precise details on\n        how to extend this class.\n\n        Beware that calling this constructor only fills the `root` field. The\n        download and metadata loading procedures are triggered only by\n        calling `_load_dataset`.\n\n        :param root_or_dataset_name: The root path where the dataset will be\n            downloaded. If a directory name is passed, then the root obtained by\n            calling `default_dataset_location` will be used (recommended).\n            To check if this parameter is a path, the constructor will check if\n            it contains the '\\' or '/' characters or if it is a Path instance.\n        :param url: The url of the archive.\n        :param checksum: The MD5 hash to use when verifying the downloaded\n            archive. Can be None, in which case the check will be skipped.\n            It is recommended to always fill this parameter.\n        :param download: If True, the dataset will be downloaded if needed.\n            If False and the dataset can't be loaded from the provided root\n            path, an error will be raised when calling the `_load_dataset`\n            method. Defaults to False.\n        :param verbose: If True, some info about the download process will be\n            printed. Defaults to False.\n        \"\"\"\n\n        self.url = url\n        self.checksum = checksum\n\n        is_path = (isinstance(root_or_dataset_name, Path) or\n                   '/' in root_or_dataset_name or '\\\\' in root_or_dataset_name)\n\n        if is_path:\n            root = Path(root_or_dataset_name)\n        else:\n            root = default_dataset_location(root_or_dataset_name)\n\n        super(SimpleDownloadableDataset, self).__init__(\n            root, download=download, verbose=verbose)\n\n    def _download_dataset(self) -> None:\n        filename = os.path.basename(self.url)\n        self._download_and_extract_archive(\n            self.url, filename, self.checksum, sub_directory=None,\n            remove_archive=False)\n\n    def _download_error_message(self) -> str:\n        return 'Error downloading the dataset. Consider downloading ' \\\n               'it manually at: ' + self.url + ' and placing it ' \\\n               'in: ' + str(self.root)",
  "def __init__(\n            self,\n            root: Union[str, Path],\n            download: bool = True,\n            verbose: bool = False):\n        \"\"\"\n        Creates an instance of a downloadable dataset.\n\n        Consider looking at the class documentation for the precise details on\n        how to extend this class.\n\n        Beware that calling this constructor only fills the `root` field. The\n        download and metadata loading procedures are triggered only by\n        calling `_load_dataset`.\n\n        :param root: The root path where the dataset will be downloaded.\n            Consider passing a path obtained by calling\n            `default_dataset_location` with the name of the dataset.\n        :param download: If True, the dataset will be downloaded if needed.\n            If False and the dataset can't be loaded from the provided root\n            path, an error will be raised when calling the `_load_dataset`\n            method. Defaults to True.\n        :param verbose: If True, some info about the download process will be\n            printed. Defaults to False.\n        \"\"\"\n\n        super(DownloadableDataset, self).__init__()\n        self.root: Path = Path(root)\n        \"\"\"\n        The path to the dataset.\n        \"\"\"\n\n        self.download: bool = download\n        \"\"\"\n        If True, the dataset will be downloaded (only if needed).\n        \"\"\"\n\n        self.verbose: bool = verbose\n        \"\"\"\n        If True, some info about the download process will be printed.\n        \"\"\"",
  "def _load_dataset(self) -> None:\n        \"\"\"\n        The standardized dataset download and load procedure.\n\n        For more details on the coded procedure see the class documentation.\n\n        This method shouldn't be overridden.\n\n        This method will raise and error if the dataset couldn't be loaded\n        or downloaded.\n\n        :return: None\n        \"\"\"\n        metadata_loaded = False\n        metadata_load_error = None\n        try:\n            metadata_loaded = self._load_metadata()\n        except Exception as e:\n            metadata_load_error = e\n\n        if metadata_loaded:\n            if self.verbose:\n                print('Files already downloaded and verified')\n            return\n\n        if not self.download:\n            msg = 'Error loading dataset metadata (dataset download was ' \\\n                  'not attempted as \"download\" is set to False)'\n            if metadata_load_error is None:\n                raise RuntimeError(msg)\n            else:\n                print(msg)\n                raise metadata_load_error\n\n        try:\n            self._download_dataset()\n        except Exception as e:\n            err_msg = self._download_error_message()\n            print(err_msg, flush=True)\n            raise e\n\n        if not self._load_metadata():\n            err_msg = self._download_error_message()\n            print(err_msg)\n            raise RuntimeError(\n                'Error loading dataset metadata (... but the download '\n                'procedure completed successfully)')",
  "def _download_dataset(self) -> None:\n        \"\"\"\n        The download procedure.\n\n        This procedure is called only if `_load_metadata` fails.\n\n        This method must raise an error if the dataset can't be downloaded.\n\n        Hints: don't re-invent the wheel! There are ready-to-use helper methods\n        like `_download_and_extract_archive`, `_download_file` and\n        `_extract_archive` that can be used.\n\n        :return: None\n        \"\"\"\n        pass",
  "def _load_metadata(self) -> bool:\n        \"\"\"\n        The dataset metadata loading procedure.\n\n        This procedure is called at least once to load the dataset metadata.\n\n        This procedure should return False if the dataset is corrupted or if it\n        can't be loaded.\n\n        :return: True if the dataset is not corrupted and could be successfully\n        loaded.\n        \"\"\"\n        pass",
  "def _download_error_message(self) -> str:\n        \"\"\"\n        Returns the error message hinting the user on how to download the\n        dataset manually.\n\n        :return: A string representing the message to show to the user.\n        \"\"\"\n        pass",
  "def _cleanup_dataset_root(self):\n        \"\"\"\n        Utility method that can be used to remove the dataset root directory.\n\n        Can be useful if a cleanup is needed when downloading and extracting the\n        dataset.\n\n        This method will also re-create the root directory.\n\n        :return: None\n        \"\"\"\n        shutil.rmtree(self.root)\n        self.root.mkdir(parents=True, exist_ok=True)",
  "def _download_file(self,\n                       url: str,\n                       file_name: str,\n                       checksum: Optional[str]) -> Path:\n        \"\"\"\n        Utility method that can be used to download and verify a file.\n\n        :param url: The download url.\n        :param file_name: The name of the file to save. The file will be saved\n            in the `root` with this name. Always fill this parameter.\n            Don't pass a path! Pass a file name only!\n        :param checksum: The MD5 hash to use when verifying the downloaded\n            file. Can be None, in which case the check will be skipped.\n            It is recommended to always fill this parameter.\n        :return: The path to the downloaded file.\n        \"\"\"\n        self.root.mkdir(parents=True, exist_ok=True)\n        download_url(url, str(self.root), filename=file_name,\n                     md5=checksum)\n        return self.root / file_name",
  "def _extract_archive(self,\n                         path: Union[str, Path],\n                         sub_directory: str = None,\n                         remove_archive: bool = False) -> Path:\n        \"\"\"\n        Utility method that can be used to extract an archive.\n\n        :param path: The complete path to the archive (for instance obtained\n            by calling `_download_file`).\n        :param sub_directory: The name of the sub directory where to extract the\n            archive. Can be None, which means that the archive will be extracted\n            in the root. Beware that some archives already have a root directory\n            inside of them, in which case it's probably better to use None here.\n            Defaults to None.\n        :param remove_archive: If True, the archive will be deleted after a\n            successful extraction. Defaults to False.\n        :return:\n        \"\"\"\n\n        if sub_directory is None:\n            extract_root = self.root\n        else:\n            extract_root = self.root / sub_directory\n\n        extract_archive(str(path), to_path=str(extract_root),\n                        remove_finished=remove_archive)\n\n        return extract_root",
  "def _download_and_extract_archive(\n            self, url: str, file_name: str, checksum: Optional[str],\n            sub_directory: str = None, remove_archive: bool = False) -> Path:\n        \"\"\"\n        Utility that downloads and extracts an archive.\n\n        :param url: The download url.\n        :param file_name: The name of the archive. The file will be saved\n            in the `root` with this name. Always fill this parameter.\n            Don't pass a path! Pass a file name only!\n        :param checksum: The MD5 hash to use when verifying the downloaded\n            archive. Can be None, in which case the check will be skipped.\n            It is recommended to always fill this parameter.\n        :param sub_directory: The name of the sub directory where to extract the\n            archive. Can be None, which means that the archive will be extracted\n            in the root. Beware that some archives already have a root directory\n            inside of them, in which case it's probably better to use None here.\n            Defaults to None.\n        :param remove_archive: If True, the archive will be deleted after a\n            successful extraction. Defaults to False.\n        :return: The path to the extracted archive. If `sub_directory` is None,\n            then this will be the `root` path.\n        \"\"\"\n        if sub_directory is None:\n            extract_root = self.root\n        else:\n            extract_root = self.root / sub_directory\n\n        self.root.mkdir(parents=True, exist_ok=True)\n        download_and_extract_archive(\n            url, str(self.root), extract_root=str(extract_root),\n            filename=file_name, md5=checksum, remove_finished=remove_archive)\n\n        return extract_root",
  "def _check_file(self, path: Union[str, Path], checksum: str) -> bool:\n        \"\"\"\n        Utility method to check a file.\n\n        :param path: The path to the file.\n        :param checksum: The MD5 hash to use.\n        :return: True if the MD5 hash of the file matched the given one.\n        \"\"\"\n        return check_integrity(str(path), md5=checksum)",
  "def __init__(\n            self,\n            root_or_dataset_name: str,\n            url: str,\n            checksum: Optional[str],\n            download: bool = False,\n            verbose: bool = False):\n        \"\"\"\n        Creates an instance of a simple downloadable dataset.\n\n        Consider looking at the class documentation for the precise details on\n        how to extend this class.\n\n        Beware that calling this constructor only fills the `root` field. The\n        download and metadata loading procedures are triggered only by\n        calling `_load_dataset`.\n\n        :param root_or_dataset_name: The root path where the dataset will be\n            downloaded. If a directory name is passed, then the root obtained by\n            calling `default_dataset_location` will be used (recommended).\n            To check if this parameter is a path, the constructor will check if\n            it contains the '\\' or '/' characters or if it is a Path instance.\n        :param url: The url of the archive.\n        :param checksum: The MD5 hash to use when verifying the downloaded\n            archive. Can be None, in which case the check will be skipped.\n            It is recommended to always fill this parameter.\n        :param download: If True, the dataset will be downloaded if needed.\n            If False and the dataset can't be loaded from the provided root\n            path, an error will be raised when calling the `_load_dataset`\n            method. Defaults to False.\n        :param verbose: If True, some info about the download process will be\n            printed. Defaults to False.\n        \"\"\"\n\n        self.url = url\n        self.checksum = checksum\n\n        is_path = (isinstance(root_or_dataset_name, Path) or\n                   '/' in root_or_dataset_name or '\\\\' in root_or_dataset_name)\n\n        if is_path:\n            root = Path(root_or_dataset_name)\n        else:\n            root = default_dataset_location(root_or_dataset_name)\n\n        super(SimpleDownloadableDataset, self).__init__(\n            root, download=download, verbose=verbose)",
  "def _download_dataset(self) -> None:\n        filename = os.path.basename(self.url)\n        self._download_and_extract_archive(\n            self.url, filename, self.checksum, sub_directory=None,\n            remove_archive=False)",
  "def _download_error_message(self) -> str:\n        return 'Error downloading the dataset. Consider downloading ' \\\n               'it manually at: ' + self.url + ' and placing it ' \\\n               'in: ' + str(self.root)",
  "class Omniglot(OmniglotTorch):\n    \"\"\"\n    Custom class used to adapt Omniglot (from Torchvision) and make it\n    compatible with the Avalanche API.\n    \"\"\"\n\n    def __init__(\n            self,\n            root: str,\n            train: bool = True,\n            transform: Optional[Callable] = None,\n            target_transform: Optional[Callable] = None,\n            download: bool = False,\n    ) -> None:\n        super().__init__(join(root, self.folder), download=download,\n                         transform=transform,\n                         target_transform=target_transform,\n                         background=train)\n\n        self.targets = [x[1] for x in self._flat_character_images]\n\n    @property\n    def data(self):\n        return [x for x, _ in self]",
  "def __init__(\n            self,\n            root: str,\n            train: bool = True,\n            transform: Optional[Callable] = None,\n            target_transform: Optional[Callable] = None,\n            download: bool = False,\n    ) -> None:\n        super().__init__(join(root, self.folder), download=download,\n                         transform=transform,\n                         target_transform=target_transform,\n                         background=train)\n\n        self.targets = [x[1] for x in self._flat_character_images]",
  "def data(self):\n        return [x for x, _ in self]",
  "def MNIST(*args, **kwargs):\n    return torchMNIST(*args, **kwargs)",
  "def FashionMNIST(*args, **kwargs):\n    return torchFashionMNIST(*args, **kwargs)",
  "def KMNIST(*args, **kwargs):\n    return torchKMNIST(*args, **kwargs)",
  "def EMNIST(*args, **kwargs):\n    return torchEMNIST(*args, **kwargs)",
  "def QMNIST(*args, **kwargs):\n    return torchQMNIST(*args, **kwargs)",
  "def FakeData(*args, **kwargs):\n    return torchFakeData(*args, **kwargs)",
  "def CocoCaptions(*args, **kwargs):\n    return torchCocoCaptions(*args, **kwargs)",
  "def CocoDetection(*args, **kwargs):\n    return torchCocoDetection(*args, **kwargs)",
  "def LSUN(*args, **kwargs):\n    return torchLSUN(*args, **kwargs)",
  "def LSUN(*args, **kwargs):\n    return torchLSUN(*args, **kwargs)",
  "def ImageFolder(*args, **kwargs):\n    return torchImageFolder(*args, **kwargs)",
  "def DatasetFolder(*args, **kwargs):\n    return torchDatasetFolder(*args, **kwargs)",
  "def ImageNet(*args, **kwargs):\n    return torchImageNet(*args, **kwargs)",
  "def CIFAR10(*args, **kwargs):\n    return torchCIFAR10(*args, **kwargs)",
  "def CIFAR100(*args, **kwargs):\n    return torchCIFAR100(*args, **kwargs)",
  "def STL10(*args, **kwargs):\n    return torchSTL10(*args, **kwargs)",
  "def SVHN(*args, **kwargs):\n    return torchSVHN(*args, **kwargs)",
  "def PhotoTour(*args, **kwargs):\n    return torchPhotoTour(*args, **kwargs)",
  "def SBU(*args, **kwargs):\n    return torchSBU(*args, **kwargs)",
  "def Flickr8k(*args, **kwargs):\n    return torchFlickr8k(*args, **kwargs)",
  "def Flickr30k(*args, **kwargs):\n    return torchFlickr30k(*args, **kwargs)",
  "def VOCDetection(*args, **kwargs):\n    return torchVOCDetection(*args, **kwargs)",
  "def VOCSegmentation(*args, **kwargs):\n    return torchVOCSegmentation(*args, **kwargs)",
  "def Cityscapes(*args, **kwargs):\n    return torchCityscapes(*args, **kwargs)",
  "def SBDataset(*args, **kwargs):\n    return torchSBDataset(*args, **kwargs)",
  "def USPS(*args, **kwargs):\n    return torchUSPS(*args, **kwargs)",
  "def Kinetics400(*args, **kwargs):\n    return torchKinetics400(*args, **kwargs)",
  "def HMDB51(*args, **kwargs):\n    return torchKHMDB51(*args, **kwargs)",
  "def UCF101(*args, **kwargs):\n    return torchUCF101(*args, **kwargs)",
  "def CelebA(*args, **kwargs):\n    return torchCelebA(*args, **kwargs)",
  "def default_dataset_location(dataset_name: str) -> Path:\n    \"\"\"\n    Return the default location of a dataset.\n\n    This currently returns \"~/.avalanche/data/<dataset_name>\" but in the future\n    an environment variable bay be introduced to change the root path.\n\n    :param dataset_name: The name of the dataset. Consider using a string that\n        can be used to name a directory in most filesystems!\n    :return: The default path for the dataset.\n    \"\"\"\n    return Path.home() / f\".avalanche/data/{dataset_name}\"",
  "class Stream51(DownloadableDataset):\n    \"\"\" Stream-51 Pytorch Dataset \"\"\"\n\n    def __init__(self, root: Union[str, Path] = None,\n                 *,\n                 train=True, transform=None,\n                 target_transform=None, loader=default_loader, download=True):\n        \"\"\"\n        Creates an instance of the Stream-51 dataset.\n\n        :param root: The directory where the dataset can be found or downloaded.\n            Defaults to None, which means that the default location for\n            'stream51' will be used.\n        :param train: If True, the training set will be returned. If False,\n            the test set will be returned.\n        :param transform: The transformations to apply to the X values.\n        :param target_transform: The transformations to apply to the Y values.\n        :param loader: The image loader to use.\n        :param download: If True, the dataset will be downloaded if needed.\n        \"\"\"\n\n        if root is None:\n            root = default_dataset_location('stream51')\n\n        self.train = train  # training set or test set\n        self.transform = transform\n        self.target_transform = target_transform\n        self.loader = loader\n        self.transform = transform\n        self.target_transform = target_transform\n        self.bbox_crop = True\n        self.ratio = 1.1\n\n        super(Stream51, self).__init__(root, download=download, verbose=True)\n\n        self._load_dataset()\n\n    def _download_dataset(self) -> None:\n        self._download_file(stream51_data.name[1], stream51_data.name[0],\n                            stream51_data.name[2])\n\n        if self.verbose:\n            print('[Stream-51] Extracting dataset...')\n\n        if stream51_data.name[1].endswith('.zip'):\n            lfilename = self.root / stream51_data.name[0]\n            with ZipFile(str(lfilename), 'r') as zipf:\n                for member in zipf.namelist():\n                    filename = os.path.basename(member)\n                    # skip directories\n                    if not filename:\n                        continue\n\n                    # copy file (taken from zipfile's extract)\n                    source = zipf.open(member)\n                    if 'json' in filename:\n                        target = open(str(self.root / filename), \"wb\")\n                    else:\n                        dest_folder = os.path.join(\n                            *(member.split(os.path.sep)[1:-1]))\n                        dest_folder = self.root / dest_folder\n                        dest_folder.mkdir(exist_ok=True, parents=True)\n\n                        target = open(str(dest_folder / filename), \"wb\")\n                    with source, target:\n                        shutil.copyfileobj(source, target)\n\n            # lfilename.unlink()\n\n    def _load_metadata(self) -> bool:\n        if self.train:\n            data_list = json.load(\n                open(str(self.root / 'Stream-51_meta_train.json')))\n        else:\n            data_list = json.load(\n                open(str(self.root / 'Stream-51_meta_test.json')))\n\n        self.samples = data_list\n        self.targets = [s[0] for s in data_list]\n\n        self.bbox_crop = True\n        self.ratio = 1.1\n\n        return True\n\n    def _download_error_message(self) -> str:\n        return '[Stream-51] Error downloading the dataset. Consider ' \\\n               'downloading it manually at: ' + stream51_data.name[1] + \\\n               ' and placing it in: ' + str(self.root)\n\n    @staticmethod\n    def _instance_ordering(data_list, seed):\n        # organize data by video\n        total_videos = 0\n        new_data_list = []\n        temp_video = []\n        for x in data_list:\n            if x[3] == 0:\n                new_data_list.append(temp_video)\n                total_videos += 1\n                temp_video = [x]\n            else:\n                temp_video.append(x)\n        new_data_list.append(temp_video)\n        new_data_list = new_data_list[1:]\n        # shuffle videos\n        random.seed(seed)\n        random.shuffle(new_data_list)\n        # reorganize by clip\n        data_list = []\n        for v in new_data_list:\n            for x in v:\n                data_list.append(x)\n        return data_list\n\n    @staticmethod\n    def _class_ordering(data_list, class_type, seed):\n        # organize data by class\n        new_data_list = []\n        for class_id in range(data_list[-1][0] + 1):\n            class_data_list = [x for x in data_list if x[0] == class_id]\n            if class_type == 'class_iid':\n                # shuffle all class data\n                random.seed(seed)\n                random.shuffle(class_data_list)\n            else:\n                # shuffle clips within class\n                class_data_list = Stream51._instance_ordering(\n                    class_data_list, seed)\n            new_data_list.append(class_data_list)\n        # shuffle classes\n        random.seed(seed)\n        random.shuffle(new_data_list)\n        # reorganize by class\n        data_list = []\n        for v in new_data_list:\n            for x in v:\n                data_list.append(x)\n        return data_list\n\n    @staticmethod\n    def make_dataset(data_list, ordering='class_instance', seed=666):\n        \"\"\"\n        data_list\n        for train: [class_id, clip_num, video_num, frame_num, bbox, file_loc]\n        for test: [class_id, bbox, file_loc]\n        \"\"\"\n        if not ordering or len(data_list[0]) == 3:  # cannot order the test set\n            return data_list\n        if ordering not in ['iid', 'class_iid', 'instance', 'class_instance']:\n            raise ValueError(\n                'dataset ordering must be one of: \"iid\", \"class_iid\", '\n                '\"instance\", or \"class_instance\"')\n        if ordering == 'iid':\n            # shuffle all data\n            random.seed(seed)\n            random.shuffle(data_list)\n            return data_list\n        elif ordering == 'instance':\n            return Stream51._instance_ordering(data_list, seed)\n        elif 'class' in ordering:\n            return Stream51._class_ordering(data_list, ordering, seed)\n\n    def __getitem__(self, index):\n        \"\"\"\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (sample, target) where target is class_index of the target\n            class.\n        \"\"\"\n        fpath, target = self.samples[index][-1], self.targets[index]\n        sample = self.loader(str(self.root / fpath))\n        if self.bbox_crop:\n            bbox = self.samples[index][-2]\n            cw = bbox[0] - bbox[1]\n            ch = bbox[2] - bbox[3]\n            center = [int(bbox[1] + cw / 2), int(bbox[3] + ch / 2)]\n            bbox = [\n                min([int(center[0] + (cw * self.ratio / 2)), sample.size[0]]),\n                max([int(center[0] - (cw * self.ratio / 2)), 0]),\n                min([int(center[1] + (ch * self.ratio / 2)), sample.size[1]]),\n                max([int(center[1] - (ch * self.ratio / 2)), 0])]\n            sample = sample.crop((bbox[1],\n                                  bbox[3],\n                                  bbox[0],\n                                  bbox[2]))\n\n        if self.transform is not None:\n            sample = self.transform(sample)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return sample, target\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __repr__(self):\n        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n        fmt_str += '    Root Location: {}\\n'.format(self.root)\n        tmp = '    Transforms (if any): '\n        fmt_str += '{0}{1}\\n'.format(\n            tmp, self.transform.__repr__().replace(\n                '\\n', '\\n' + ' ' * len(tmp)))\n\n        tmp = '    Target Transforms (if any): '\n        fmt_str += '{0}{1}'.format(\n            tmp, self.target_transform.__repr__().replace(\n                '\\n', '\\n' + ' ' * len(tmp)))\n        return fmt_str",
  "def __init__(self, root: Union[str, Path] = None,\n                 *,\n                 train=True, transform=None,\n                 target_transform=None, loader=default_loader, download=True):\n        \"\"\"\n        Creates an instance of the Stream-51 dataset.\n\n        :param root: The directory where the dataset can be found or downloaded.\n            Defaults to None, which means that the default location for\n            'stream51' will be used.\n        :param train: If True, the training set will be returned. If False,\n            the test set will be returned.\n        :param transform: The transformations to apply to the X values.\n        :param target_transform: The transformations to apply to the Y values.\n        :param loader: The image loader to use.\n        :param download: If True, the dataset will be downloaded if needed.\n        \"\"\"\n\n        if root is None:\n            root = default_dataset_location('stream51')\n\n        self.train = train  # training set or test set\n        self.transform = transform\n        self.target_transform = target_transform\n        self.loader = loader\n        self.transform = transform\n        self.target_transform = target_transform\n        self.bbox_crop = True\n        self.ratio = 1.1\n\n        super(Stream51, self).__init__(root, download=download, verbose=True)\n\n        self._load_dataset()",
  "def _download_dataset(self) -> None:\n        self._download_file(stream51_data.name[1], stream51_data.name[0],\n                            stream51_data.name[2])\n\n        if self.verbose:\n            print('[Stream-51] Extracting dataset...')\n\n        if stream51_data.name[1].endswith('.zip'):\n            lfilename = self.root / stream51_data.name[0]\n            with ZipFile(str(lfilename), 'r') as zipf:\n                for member in zipf.namelist():\n                    filename = os.path.basename(member)\n                    # skip directories\n                    if not filename:\n                        continue\n\n                    # copy file (taken from zipfile's extract)\n                    source = zipf.open(member)\n                    if 'json' in filename:\n                        target = open(str(self.root / filename), \"wb\")\n                    else:\n                        dest_folder = os.path.join(\n                            *(member.split(os.path.sep)[1:-1]))\n                        dest_folder = self.root / dest_folder\n                        dest_folder.mkdir(exist_ok=True, parents=True)\n\n                        target = open(str(dest_folder / filename), \"wb\")\n                    with source, target:\n                        shutil.copyfileobj(source, target)",
  "def _load_metadata(self) -> bool:\n        if self.train:\n            data_list = json.load(\n                open(str(self.root / 'Stream-51_meta_train.json')))\n        else:\n            data_list = json.load(\n                open(str(self.root / 'Stream-51_meta_test.json')))\n\n        self.samples = data_list\n        self.targets = [s[0] for s in data_list]\n\n        self.bbox_crop = True\n        self.ratio = 1.1\n\n        return True",
  "def _download_error_message(self) -> str:\n        return '[Stream-51] Error downloading the dataset. Consider ' \\\n               'downloading it manually at: ' + stream51_data.name[1] + \\\n               ' and placing it in: ' + str(self.root)",
  "def _instance_ordering(data_list, seed):\n        # organize data by video\n        total_videos = 0\n        new_data_list = []\n        temp_video = []\n        for x in data_list:\n            if x[3] == 0:\n                new_data_list.append(temp_video)\n                total_videos += 1\n                temp_video = [x]\n            else:\n                temp_video.append(x)\n        new_data_list.append(temp_video)\n        new_data_list = new_data_list[1:]\n        # shuffle videos\n        random.seed(seed)\n        random.shuffle(new_data_list)\n        # reorganize by clip\n        data_list = []\n        for v in new_data_list:\n            for x in v:\n                data_list.append(x)\n        return data_list",
  "def _class_ordering(data_list, class_type, seed):\n        # organize data by class\n        new_data_list = []\n        for class_id in range(data_list[-1][0] + 1):\n            class_data_list = [x for x in data_list if x[0] == class_id]\n            if class_type == 'class_iid':\n                # shuffle all class data\n                random.seed(seed)\n                random.shuffle(class_data_list)\n            else:\n                # shuffle clips within class\n                class_data_list = Stream51._instance_ordering(\n                    class_data_list, seed)\n            new_data_list.append(class_data_list)\n        # shuffle classes\n        random.seed(seed)\n        random.shuffle(new_data_list)\n        # reorganize by class\n        data_list = []\n        for v in new_data_list:\n            for x in v:\n                data_list.append(x)\n        return data_list",
  "def make_dataset(data_list, ordering='class_instance', seed=666):\n        \"\"\"\n        data_list\n        for train: [class_id, clip_num, video_num, frame_num, bbox, file_loc]\n        for test: [class_id, bbox, file_loc]\n        \"\"\"\n        if not ordering or len(data_list[0]) == 3:  # cannot order the test set\n            return data_list\n        if ordering not in ['iid', 'class_iid', 'instance', 'class_instance']:\n            raise ValueError(\n                'dataset ordering must be one of: \"iid\", \"class_iid\", '\n                '\"instance\", or \"class_instance\"')\n        if ordering == 'iid':\n            # shuffle all data\n            random.seed(seed)\n            random.shuffle(data_list)\n            return data_list\n        elif ordering == 'instance':\n            return Stream51._instance_ordering(data_list, seed)\n        elif 'class' in ordering:\n            return Stream51._class_ordering(data_list, ordering, seed)",
  "def __getitem__(self, index):\n        \"\"\"\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (sample, target) where target is class_index of the target\n            class.\n        \"\"\"\n        fpath, target = self.samples[index][-1], self.targets[index]\n        sample = self.loader(str(self.root / fpath))\n        if self.bbox_crop:\n            bbox = self.samples[index][-2]\n            cw = bbox[0] - bbox[1]\n            ch = bbox[2] - bbox[3]\n            center = [int(bbox[1] + cw / 2), int(bbox[3] + ch / 2)]\n            bbox = [\n                min([int(center[0] + (cw * self.ratio / 2)), sample.size[0]]),\n                max([int(center[0] - (cw * self.ratio / 2)), 0]),\n                min([int(center[1] + (ch * self.ratio / 2)), sample.size[1]]),\n                max([int(center[1] - (ch * self.ratio / 2)), 0])]\n            sample = sample.crop((bbox[1],\n                                  bbox[3],\n                                  bbox[0],\n                                  bbox[2]))\n\n        if self.transform is not None:\n            sample = self.transform(sample)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return sample, target",
  "def __len__(self):\n        return len(self.samples)",
  "def __repr__(self):\n        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n        fmt_str += '    Root Location: {}\\n'.format(self.root)\n        tmp = '    Transforms (if any): '\n        fmt_str += '{0}{1}\\n'.format(\n            tmp, self.transform.__repr__().replace(\n                '\\n', '\\n' + ' ' * len(tmp)))\n\n        tmp = '    Target Transforms (if any): '\n        fmt_str += '{0}{1}'.format(\n            tmp, self.target_transform.__repr__().replace(\n                '\\n', '\\n' + ' ' * len(tmp)))\n        return fmt_str",
  "class TinyImagenet(SimpleDownloadableDataset):\n    \"\"\"Tiny Imagenet Pytorch Dataset\"\"\"\n\n    filename = ('tiny-imagenet-200.zip',\n                'http://cs231n.stanford.edu/tiny-imagenet-200.zip')\n    md5 = '90528d7ca1a48142e341f4ef8d21d0de'\n\n    def __init__(\n            self,\n            root: Union[str, Path] = None,\n            *,\n            train: bool = True,\n            transform=None,\n            target_transform=None,\n            loader=default_loader,\n            download=True):\n        \"\"\"\n        Creates an instance of the Tiny Imagenet dataset.\n\n        :param root: folder in which to download dataset. Defaults to None,\n            which means that the default location for 'tinyimagenet' will be\n            used.\n        :param train: True for training set, False for test set.\n        :param transform: Pytorch transformation function for x.\n        :param target_transform: Pytorch transformation function for y.\n        :param loader: the procedure to load the instance from the storage.\n        :param bool download: If True, the dataset will be  downloaded if\n            needed.\n        \"\"\"\n\n        if root is None:\n            root = default_dataset_location('tinyimagenet')\n\n        self.transform = transform\n        self.target_transform = target_transform\n        self.train = train\n        self.loader = loader\n\n        super(TinyImagenet, self).__init__(\n            root, self.filename[1], self.md5, download=download, verbose=True)\n\n        self._load_dataset()\n\n    def _load_metadata(self) -> bool:\n        self.data_folder = self.root / 'tiny-imagenet-200'\n\n        self.label2id, self.id2label = TinyImagenet.labels2dict(\n            self.data_folder)\n        self.data, self.targets = self.load_data()\n        return True\n\n    @staticmethod\n    def labels2dict(data_folder: Path):\n        \"\"\"\n        Returns dictionaries to convert class names into progressive ids\n        and viceversa.\n\n        :param data_folder: The root path of tiny imagenet\n        :returns: label2id, id2label: two Python dictionaries.\n        \"\"\"\n\n        label2id = {}\n        id2label = {}\n\n        with open(str(data_folder / 'wnids.txt'), 'r') as f:\n\n            reader = csv.reader(f)\n            curr_idx = 0\n            for ll in reader:\n                if ll[0] not in label2id:\n                    label2id[ll[0]] = curr_idx\n                    id2label[curr_idx] = ll[0]\n                    curr_idx += 1\n\n        return label2id, id2label\n\n    def load_data(self):\n        \"\"\"\n        Load all images paths and targets.\n\n        :return: train_set, test_set: (train_X_paths, train_y).\n        \"\"\"\n\n        data = [[], []]\n\n        classes = list(range(200))\n        for class_id in classes:\n            class_name = self.id2label[class_id]\n\n            if self.train:\n                X = self.get_train_images_paths(class_name)\n                Y = [class_id] * len(X)\n            else:\n                # test set\n                X = self.get_test_images_paths(class_name)\n                Y = [class_id] * len(X)\n\n            data[0] += X\n            data[1] += Y\n\n        return data\n\n    def get_train_images_paths(self, class_name):\n        \"\"\"\n        Gets the training set image paths.\n\n        :param class_name: names of the classes of the images to be\n            collected.\n        :returns img_paths: list of strings (paths)\n        \"\"\"\n        train_img_folder = self.data_folder / 'train' / class_name / 'images'\n\n        img_paths = [f for f in train_img_folder.iterdir() if f.is_file()]\n\n        return img_paths\n\n    def get_test_images_paths(self, class_name):\n        \"\"\"\n        Gets the test set image paths\n\n        :param class_name: names of the classes of the images to be\n            collected.\n        :returns img_paths: list of strings (paths)\n        \"\"\"\n\n        val_img_folder = self.data_folder / 'val' / 'images'\n        annotations_file = self.data_folder / 'val' / 'val_annotations.txt'\n\n        valid_names = []\n\n        # filter validation images by class using appropriate file\n        with open(str(annotations_file), 'r') as f:\n\n            reader = csv.reader(f, dialect='excel-tab')\n            for ll in reader:\n                if ll[1] == class_name:\n                    valid_names.append(ll[0])\n\n        img_paths = [val_img_folder / f for f in valid_names]\n\n        return img_paths\n\n    def __len__(self):\n        \"\"\" Returns the length of the set \"\"\"\n        return len(self.data)\n\n    def __getitem__(self, index):\n        \"\"\" Returns the index-th x, y pattern of the set \"\"\"\n\n        path, target = self.data[index], int(self.targets[index])\n\n        # doing this so that it is consistent with all other datasets\n        # to return a PIL Image\n        img = self.loader(path)\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target",
  "def __init__(\n            self,\n            root: Union[str, Path] = None,\n            *,\n            train: bool = True,\n            transform=None,\n            target_transform=None,\n            loader=default_loader,\n            download=True):\n        \"\"\"\n        Creates an instance of the Tiny Imagenet dataset.\n\n        :param root: folder in which to download dataset. Defaults to None,\n            which means that the default location for 'tinyimagenet' will be\n            used.\n        :param train: True for training set, False for test set.\n        :param transform: Pytorch transformation function for x.\n        :param target_transform: Pytorch transformation function for y.\n        :param loader: the procedure to load the instance from the storage.\n        :param bool download: If True, the dataset will be  downloaded if\n            needed.\n        \"\"\"\n\n        if root is None:\n            root = default_dataset_location('tinyimagenet')\n\n        self.transform = transform\n        self.target_transform = target_transform\n        self.train = train\n        self.loader = loader\n\n        super(TinyImagenet, self).__init__(\n            root, self.filename[1], self.md5, download=download, verbose=True)\n\n        self._load_dataset()",
  "def _load_metadata(self) -> bool:\n        self.data_folder = self.root / 'tiny-imagenet-200'\n\n        self.label2id, self.id2label = TinyImagenet.labels2dict(\n            self.data_folder)\n        self.data, self.targets = self.load_data()\n        return True",
  "def labels2dict(data_folder: Path):\n        \"\"\"\n        Returns dictionaries to convert class names into progressive ids\n        and viceversa.\n\n        :param data_folder: The root path of tiny imagenet\n        :returns: label2id, id2label: two Python dictionaries.\n        \"\"\"\n\n        label2id = {}\n        id2label = {}\n\n        with open(str(data_folder / 'wnids.txt'), 'r') as f:\n\n            reader = csv.reader(f)\n            curr_idx = 0\n            for ll in reader:\n                if ll[0] not in label2id:\n                    label2id[ll[0]] = curr_idx\n                    id2label[curr_idx] = ll[0]\n                    curr_idx += 1\n\n        return label2id, id2label",
  "def load_data(self):\n        \"\"\"\n        Load all images paths and targets.\n\n        :return: train_set, test_set: (train_X_paths, train_y).\n        \"\"\"\n\n        data = [[], []]\n\n        classes = list(range(200))\n        for class_id in classes:\n            class_name = self.id2label[class_id]\n\n            if self.train:\n                X = self.get_train_images_paths(class_name)\n                Y = [class_id] * len(X)\n            else:\n                # test set\n                X = self.get_test_images_paths(class_name)\n                Y = [class_id] * len(X)\n\n            data[0] += X\n            data[1] += Y\n\n        return data",
  "def get_train_images_paths(self, class_name):\n        \"\"\"\n        Gets the training set image paths.\n\n        :param class_name: names of the classes of the images to be\n            collected.\n        :returns img_paths: list of strings (paths)\n        \"\"\"\n        train_img_folder = self.data_folder / 'train' / class_name / 'images'\n\n        img_paths = [f for f in train_img_folder.iterdir() if f.is_file()]\n\n        return img_paths",
  "def get_test_images_paths(self, class_name):\n        \"\"\"\n        Gets the test set image paths\n\n        :param class_name: names of the classes of the images to be\n            collected.\n        :returns img_paths: list of strings (paths)\n        \"\"\"\n\n        val_img_folder = self.data_folder / 'val' / 'images'\n        annotations_file = self.data_folder / 'val' / 'val_annotations.txt'\n\n        valid_names = []\n\n        # filter validation images by class using appropriate file\n        with open(str(annotations_file), 'r') as f:\n\n            reader = csv.reader(f, dialect='excel-tab')\n            for ll in reader:\n                if ll[1] == class_name:\n                    valid_names.append(ll[0])\n\n        img_paths = [val_img_folder / f for f in valid_names]\n\n        return img_paths",
  "def __len__(self):\n        \"\"\" Returns the length of the set \"\"\"\n        return len(self.data)",
  "def __getitem__(self, index):\n        \"\"\" Returns the index-th x, y pattern of the set \"\"\"\n\n        path, target = self.data[index], int(self.targets[index])\n\n        # doing this so that it is consistent with all other datasets\n        # to return a PIL Image\n        img = self.loader(path)\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target",
  "class OpenLORIS(DownloadableDataset):\n    \"\"\" OpenLORIS Pytorch Dataset \"\"\"\n\n    def __init__(self, root: Union[str, Path] = None,\n                 *,\n                 train=True, transform=None, target_transform=None,\n                 loader=default_loader, download=True):\n        \"\"\"\n        Creates an instance of the OpenLORIS dataset.\n\n        :param root: The directory where the dataset can be found or downloaded.\n            Defaults to None, which means that the default location for\n            'openloris' will be used.\n        :param train: If True, the training set will be returned. If False,\n            the test set will be returned.\n        :param transform: The transformations to apply to the X values.\n        :param target_transform: The transformations to apply to the Y values.\n        :param loader: The image loader to use.\n        :param download: If True, the dataset will be downloaded if needed.\n        \"\"\"\n\n        if root is None:\n            root = default_dataset_location('openloris')\n\n        self.train = train  # training set or test set\n        self.transform = transform\n        self.target_transform = target_transform\n        self.loader = loader\n\n        super(OpenLORIS, self).__init__(root, download=download, verbose=True)\n        self._load_dataset()\n\n    def _download_dataset(self) -> None:\n        data2download = openloris_data.avl_vps_data\n\n        for name in data2download:\n            if self.verbose:\n                print(\"Downloading \" + name[1] + \"...\")\n            file = self._download_file(name[1], name[0], name[2])\n            if name[1].endswith('.zip'):\n                if self.verbose:\n                    print(f'Extracting {name[0]}...')\n                self._extract_archive(file)\n                if self.verbose:\n                    print('Extraction completed!')\n\n    def _load_metadata(self) -> bool:\n        if not self._check_integrity():\n            return False\n\n        # any scenario and factor is good here since we want just to load the\n        # train images and targets with no particular order\n        scen = 'domain'\n        factor = 0\n        ntask = 9\n\n        print(\"Loading paths...\")\n        with open(str(self.root / 'Paths.pkl'), 'rb') as f:\n            self.train_test_paths = pkl.load(f)\n\n        print(\"Loading labels...\")\n        with open(str(self.root / 'Labels.pkl'), 'rb') as f:\n            self.all_targets = pkl.load(f)\n            self.train_test_targets = []\n            for i in range(ntask + 1):\n                self.train_test_targets += self.all_targets[scen][factor][i]\n\n        print(\"Loading LUP...\")\n        with open(str(self.root / 'LUP.pkl'), 'rb') as f:\n            self.LUP = pkl.load(f)\n\n        self.idx_list = []\n        if self.train:\n            for i in range(ntask + 1):\n                self.idx_list += self.LUP[scen][factor][i]\n        else:\n            self.idx_list = self.LUP[scen][factor][-1]\n\n        self.paths = []\n        self.targets = []\n\n        for idx in self.idx_list:\n            self.paths.append(self.train_test_paths[idx])\n            self.targets.append(self.train_test_targets[idx])\n\n        return True\n\n    def _download_error_message(self) -> str:\n        base_url = openloris_data.base_gdrive_url\n        all_urls = [\n            base_url + name_url[1] for name_url in openloris_data.avl_vps_data\n        ]\n\n        base_msg = \\\n            '[OpenLoris] Direct download may no longer be supported!\\n' \\\n            'You should download data manually using the following links:\\n'\n\n        for url in all_urls:\n            base_msg += url\n            base_msg += '\\n'\n\n        base_msg += 'and place these files in ' + str(self.root)\n\n        return base_msg\n\n    def _check_integrity(self):\n        \"\"\" Checks if the data is already available and intact \"\"\"\n\n        for name, url, md5 in openloris_data.avl_vps_data:\n            filepath = self.root / name\n            if not filepath.is_file():\n                if self.verbose:\n                    print('[OpenLORIS] Error checking integrity of:',\n                          str(filepath))\n                return False\n        return True\n\n    def __getitem__(self, index):\n        target = self.targets[index]\n        img = self.loader(str(self.root / self.paths[index]))\n\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.targets)",
  "def __init__(self, root: Union[str, Path] = None,\n                 *,\n                 train=True, transform=None, target_transform=None,\n                 loader=default_loader, download=True):\n        \"\"\"\n        Creates an instance of the OpenLORIS dataset.\n\n        :param root: The directory where the dataset can be found or downloaded.\n            Defaults to None, which means that the default location for\n            'openloris' will be used.\n        :param train: If True, the training set will be returned. If False,\n            the test set will be returned.\n        :param transform: The transformations to apply to the X values.\n        :param target_transform: The transformations to apply to the Y values.\n        :param loader: The image loader to use.\n        :param download: If True, the dataset will be downloaded if needed.\n        \"\"\"\n\n        if root is None:\n            root = default_dataset_location('openloris')\n\n        self.train = train  # training set or test set\n        self.transform = transform\n        self.target_transform = target_transform\n        self.loader = loader\n\n        super(OpenLORIS, self).__init__(root, download=download, verbose=True)\n        self._load_dataset()",
  "def _download_dataset(self) -> None:\n        data2download = openloris_data.avl_vps_data\n\n        for name in data2download:\n            if self.verbose:\n                print(\"Downloading \" + name[1] + \"...\")\n            file = self._download_file(name[1], name[0], name[2])\n            if name[1].endswith('.zip'):\n                if self.verbose:\n                    print(f'Extracting {name[0]}...')\n                self._extract_archive(file)\n                if self.verbose:\n                    print('Extraction completed!')",
  "def _load_metadata(self) -> bool:\n        if not self._check_integrity():\n            return False\n\n        # any scenario and factor is good here since we want just to load the\n        # train images and targets with no particular order\n        scen = 'domain'\n        factor = 0\n        ntask = 9\n\n        print(\"Loading paths...\")\n        with open(str(self.root / 'Paths.pkl'), 'rb') as f:\n            self.train_test_paths = pkl.load(f)\n\n        print(\"Loading labels...\")\n        with open(str(self.root / 'Labels.pkl'), 'rb') as f:\n            self.all_targets = pkl.load(f)\n            self.train_test_targets = []\n            for i in range(ntask + 1):\n                self.train_test_targets += self.all_targets[scen][factor][i]\n\n        print(\"Loading LUP...\")\n        with open(str(self.root / 'LUP.pkl'), 'rb') as f:\n            self.LUP = pkl.load(f)\n\n        self.idx_list = []\n        if self.train:\n            for i in range(ntask + 1):\n                self.idx_list += self.LUP[scen][factor][i]\n        else:\n            self.idx_list = self.LUP[scen][factor][-1]\n\n        self.paths = []\n        self.targets = []\n\n        for idx in self.idx_list:\n            self.paths.append(self.train_test_paths[idx])\n            self.targets.append(self.train_test_targets[idx])\n\n        return True",
  "def _download_error_message(self) -> str:\n        base_url = openloris_data.base_gdrive_url\n        all_urls = [\n            base_url + name_url[1] for name_url in openloris_data.avl_vps_data\n        ]\n\n        base_msg = \\\n            '[OpenLoris] Direct download may no longer be supported!\\n' \\\n            'You should download data manually using the following links:\\n'\n\n        for url in all_urls:\n            base_msg += url\n            base_msg += '\\n'\n\n        base_msg += 'and place these files in ' + str(self.root)\n\n        return base_msg",
  "def _check_integrity(self):\n        \"\"\" Checks if the data is already available and intact \"\"\"\n\n        for name, url, md5 in openloris_data.avl_vps_data:\n            filepath = self.root / name\n            if not filepath.is_file():\n                if self.verbose:\n                    print('[OpenLORIS] Error checking integrity of:',\n                          str(filepath))\n                return False\n        return True",
  "def __getitem__(self, index):\n        target = self.targets[index]\n        img = self.loader(str(self.root / self.paths[index]))\n\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target",
  "def __len__(self):\n        return len(self.targets)",
  "class CORe50Dataset(DownloadableDataset):\n    \"\"\" CORe50 Pytorch Dataset \"\"\"\n\n    def __init__(\n            self,\n            root: Union[str, Path] = None,\n            *,\n            train=True, transform=None, target_transform=None,\n            loader=default_loader, download=True, mini=False,\n            object_level=True):\n\n        \"\"\"\n        Creates an instance of the CORe50 dataset.\n\n        :param root: root for the datasets data. Defaults to None, which means\n        that the default location for 'core50' will be used.\n        :param train: train or test split.\n        :param transform: eventual transformations to be applied.\n        :param target_transform: eventual transformation to be applied to the\n            targets.\n        :param loader: the procedure to load the instance from the storage.\n        :param download: boolean to automatically download data. Default to\n            True.\n        :param mini: boolean to use the 32x32 version instead of the 128x128.\n            Default to False.\n        :param object_level: if the classification is objects based or\n            category based: 50 or 10 way classification problem. Default to True\n            (50-way object classification problem)\n        \"\"\"\n\n        if root is None:\n            root = default_dataset_location('core50')\n\n        super(CORe50Dataset, self).__init__(\n            root, download=download, verbose=True)\n\n        self.train = train  # training set or test set\n        self.transform = transform\n        self.target_transform = target_transform\n        self.loader = loader\n        self.object_level = object_level\n        self.mini = mini\n\n        # any scenario and run is good here since we want just to load the\n        # train images and targets with no particular order\n        self._scen = 'ni'\n        self._run = 0\n        self._nbatch = 8\n\n        # Download the dataset and initialize metadata\n        self._load_dataset()\n\n    def __getitem__(self, index):\n        \"\"\"\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (sample, target) where target is class_index of the target\n                class.\n        \"\"\"\n\n        target = self.targets[index]\n        if self.mini:\n            bp = \"core50_32x32\"\n        else:\n            bp = \"core50_128x128\"\n\n        img = self.loader(\n            str(self.root / bp / self.paths[index])\n        )\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.targets)\n\n    def _download_dataset(self) -> None:\n        data2download = core50_data.data\n\n        if self.mini:\n            data2download = list(data2download)\n            data2download[0] = core50_data.extra_data[1]\n\n        for name in data2download:\n            if self.verbose:\n                print(\"Downloading \" + name[1] + \"...\")\n            file = self._download_file(name[1], name[0], name[2])\n            if name[1].endswith('.zip'):\n                if self.verbose:\n                    print(f'Extracting {name[0]}...')\n                self._extract_archive(file)\n                if self.verbose:\n                    print('Extraction completed!')\n\n    def _load_metadata(self) -> bool:\n        if self.mini:\n            bp = \"core50_32x32\"\n        else:\n            bp = \"core50_128x128\"\n\n        if not (self.root / bp).exists():\n            return False\n\n        if not (self.root / 'batches_filelists').exists():\n            return False\n\n        with open(self.root / 'paths.pkl', 'rb') as f:\n            self.train_test_paths = pkl.load(f)\n\n        if self.verbose:\n            print(\"Loading labels...\")\n        with open(self.root / 'labels.pkl', 'rb') as f:\n            self.all_targets = pkl.load(f)\n            self.train_test_targets = []\n            for i in range(self._nbatch + 1):\n                self.train_test_targets += \\\n                    self.all_targets[self._scen][self._run][i]\n\n        if self.verbose:\n            print(\"Loading LUP...\")\n        with open(self.root / 'LUP.pkl', 'rb') as f:\n            self.LUP = pkl.load(f)\n\n        if self.verbose:\n            print(\"Loading labels names...\")\n        with open(self.root / 'labels2names.pkl', 'rb') as f:\n            self.labels2names = pkl.load(f)\n\n        self.idx_list = []\n        if self.train:\n            for i in range(self._nbatch + 1):\n                self.idx_list += self.LUP[self._scen][self._run][i]\n        else:\n            self.idx_list = self.LUP[self._scen][self._run][-1]\n\n        self.paths = []\n        self.targets = []\n\n        for idx in self.idx_list:\n            self.paths.append(self.train_test_paths[idx])\n            div = 1\n            if not self.object_level:\n                div = 5\n            self.targets.append(self.train_test_targets[idx] // div)\n\n        with open(self.root / 'labels2names.pkl', 'rb') as f:\n            self.labels2names = pkl.load(f)\n\n        if not (self.root / 'NIC_v2_79_cat').exists():\n            self._create_cat_filelists()\n\n        return True\n\n    def _download_error_message(self) -> str:\n        all_urls = [\n            name_url[1] for name_url in core50_data.data\n        ]\n\n        base_msg = \\\n            '[CORe50] Error downloading the dataset!\\n' \\\n            'You should download data manually using the following links:\\n'\n\n        for url in all_urls:\n            base_msg += url\n            base_msg += '\\n'\n\n        base_msg += 'and place these files in ' + str(self.root)\n\n        return base_msg\n\n    def _create_cat_filelists(self):\n        \"\"\" Generates corresponding filelists with category-wise labels. The\n        default one are based on the object-level labels from 0 to 49.\"\"\"\n\n        for k, v in core50_data.scen2dirs.items():\n            orig_root_path = os.path.join(self.root, v)\n            root_path = os.path.join(self.root, v[:-1] + \"_cat\")\n            if not os.path.exists(root_path):\n                os.makedirs(root_path)\n            for run in range(10):\n                cur_path = os.path.join(root_path, \"run\"+str(run))\n                orig_cur_path = os.path.join(orig_root_path, \"run\"+str(run))\n                if not os.path.exists(cur_path):\n                    os.makedirs(cur_path)\n                for file in glob.glob(os.path.join(orig_cur_path, \"*.txt\")):\n                    o_filename = file\n                    _, d_filename = os.path.split(o_filename)\n                    orig_f = open(o_filename, \"r\")\n                    dst_f = open(os.path.join(cur_path, d_filename), \"w\")\n                    for line in orig_f:\n                        path, label = line.split(\" \")\n                        new_label = self._objlab2cat(int(label), k, run)\n                        dst_f.write(path + \" \" + str(new_label) + \"\\n\")\n                    orig_f.close()\n                    dst_f.close()\n\n    def _objlab2cat(self, label, scen, run):\n        \"\"\" Mapping an object label into its corresponding category label\n        based on the scenario. \"\"\"\n\n        if scen == \"nc\":\n            return core50_data.name2cat[\n                self.labels2names['nc'][run][label][:-1]]\n        else:\n            return int(label) // 5",
  "def CORe50(*args, **kwargs):\n    warn(\"Dataset CORe50 has been renamed CORe50Dataset to prevent confusion \"\n         \"with the CORe50 classic benchmark\", DeprecationWarning, 2)\n    return CORe50Dataset(*args, **kwargs)",
  "def __init__(\n            self,\n            root: Union[str, Path] = None,\n            *,\n            train=True, transform=None, target_transform=None,\n            loader=default_loader, download=True, mini=False,\n            object_level=True):\n\n        \"\"\"\n        Creates an instance of the CORe50 dataset.\n\n        :param root: root for the datasets data. Defaults to None, which means\n        that the default location for 'core50' will be used.\n        :param train: train or test split.\n        :param transform: eventual transformations to be applied.\n        :param target_transform: eventual transformation to be applied to the\n            targets.\n        :param loader: the procedure to load the instance from the storage.\n        :param download: boolean to automatically download data. Default to\n            True.\n        :param mini: boolean to use the 32x32 version instead of the 128x128.\n            Default to False.\n        :param object_level: if the classification is objects based or\n            category based: 50 or 10 way classification problem. Default to True\n            (50-way object classification problem)\n        \"\"\"\n\n        if root is None:\n            root = default_dataset_location('core50')\n\n        super(CORe50Dataset, self).__init__(\n            root, download=download, verbose=True)\n\n        self.train = train  # training set or test set\n        self.transform = transform\n        self.target_transform = target_transform\n        self.loader = loader\n        self.object_level = object_level\n        self.mini = mini\n\n        # any scenario and run is good here since we want just to load the\n        # train images and targets with no particular order\n        self._scen = 'ni'\n        self._run = 0\n        self._nbatch = 8\n\n        # Download the dataset and initialize metadata\n        self._load_dataset()",
  "def __getitem__(self, index):\n        \"\"\"\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (sample, target) where target is class_index of the target\n                class.\n        \"\"\"\n\n        target = self.targets[index]\n        if self.mini:\n            bp = \"core50_32x32\"\n        else:\n            bp = \"core50_128x128\"\n\n        img = self.loader(\n            str(self.root / bp / self.paths[index])\n        )\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target",
  "def __len__(self):\n        return len(self.targets)",
  "def _download_dataset(self) -> None:\n        data2download = core50_data.data\n\n        if self.mini:\n            data2download = list(data2download)\n            data2download[0] = core50_data.extra_data[1]\n\n        for name in data2download:\n            if self.verbose:\n                print(\"Downloading \" + name[1] + \"...\")\n            file = self._download_file(name[1], name[0], name[2])\n            if name[1].endswith('.zip'):\n                if self.verbose:\n                    print(f'Extracting {name[0]}...')\n                self._extract_archive(file)\n                if self.verbose:\n                    print('Extraction completed!')",
  "def _load_metadata(self) -> bool:\n        if self.mini:\n            bp = \"core50_32x32\"\n        else:\n            bp = \"core50_128x128\"\n\n        if not (self.root / bp).exists():\n            return False\n\n        if not (self.root / 'batches_filelists').exists():\n            return False\n\n        with open(self.root / 'paths.pkl', 'rb') as f:\n            self.train_test_paths = pkl.load(f)\n\n        if self.verbose:\n            print(\"Loading labels...\")\n        with open(self.root / 'labels.pkl', 'rb') as f:\n            self.all_targets = pkl.load(f)\n            self.train_test_targets = []\n            for i in range(self._nbatch + 1):\n                self.train_test_targets += \\\n                    self.all_targets[self._scen][self._run][i]\n\n        if self.verbose:\n            print(\"Loading LUP...\")\n        with open(self.root / 'LUP.pkl', 'rb') as f:\n            self.LUP = pkl.load(f)\n\n        if self.verbose:\n            print(\"Loading labels names...\")\n        with open(self.root / 'labels2names.pkl', 'rb') as f:\n            self.labels2names = pkl.load(f)\n\n        self.idx_list = []\n        if self.train:\n            for i in range(self._nbatch + 1):\n                self.idx_list += self.LUP[self._scen][self._run][i]\n        else:\n            self.idx_list = self.LUP[self._scen][self._run][-1]\n\n        self.paths = []\n        self.targets = []\n\n        for idx in self.idx_list:\n            self.paths.append(self.train_test_paths[idx])\n            div = 1\n            if not self.object_level:\n                div = 5\n            self.targets.append(self.train_test_targets[idx] // div)\n\n        with open(self.root / 'labels2names.pkl', 'rb') as f:\n            self.labels2names = pkl.load(f)\n\n        if not (self.root / 'NIC_v2_79_cat').exists():\n            self._create_cat_filelists()\n\n        return True",
  "def _download_error_message(self) -> str:\n        all_urls = [\n            name_url[1] for name_url in core50_data.data\n        ]\n\n        base_msg = \\\n            '[CORe50] Error downloading the dataset!\\n' \\\n            'You should download data manually using the following links:\\n'\n\n        for url in all_urls:\n            base_msg += url\n            base_msg += '\\n'\n\n        base_msg += 'and place these files in ' + str(self.root)\n\n        return base_msg",
  "def _create_cat_filelists(self):\n        \"\"\" Generates corresponding filelists with category-wise labels. The\n        default one are based on the object-level labels from 0 to 49.\"\"\"\n\n        for k, v in core50_data.scen2dirs.items():\n            orig_root_path = os.path.join(self.root, v)\n            root_path = os.path.join(self.root, v[:-1] + \"_cat\")\n            if not os.path.exists(root_path):\n                os.makedirs(root_path)\n            for run in range(10):\n                cur_path = os.path.join(root_path, \"run\"+str(run))\n                orig_cur_path = os.path.join(orig_root_path, \"run\"+str(run))\n                if not os.path.exists(cur_path):\n                    os.makedirs(cur_path)\n                for file in glob.glob(os.path.join(orig_cur_path, \"*.txt\")):\n                    o_filename = file\n                    _, d_filename = os.path.split(o_filename)\n                    orig_f = open(o_filename, \"r\")\n                    dst_f = open(os.path.join(cur_path, d_filename), \"w\")\n                    for line in orig_f:\n                        path, label = line.split(\" \")\n                        new_label = self._objlab2cat(int(label), k, run)\n                        dst_f.write(path + \" \" + str(new_label) + \"\\n\")\n                    orig_f.close()\n                    dst_f.close()",
  "def _objlab2cat(self, label, scen, run):\n        \"\"\" Mapping an object label into its corresponding category label\n        based on the scenario. \"\"\"\n\n        if scen == \"nc\":\n            return core50_data.name2cat[\n                self.labels2names['nc'][run][label][:-1]]\n        else:\n            return int(label) // 5",
  "class MiniImageNetDataset(Dataset):\n    \"\"\"\n    The MiniImageNet dataset.\n\n    This implementation is based on the one from\n    https://github.com/yaoyao-liu/mini-imagenet-tools. Differently from that,\n    this class doesn't rely on a pre-generated mini imagenet folder. Instead,\n    this will use the original ImageNet folder by resizing images on-the-fly.\n\n    The list of included files are the ones defined in the CSVs taken from the\n    aforementioned repository. Those CSVs are generated by Ravi and Larochelle.\n    See the linked repository for more details.\n\n    Exactly as happens with the torchvision :class:`ImageNet` class, textual\n    class labels (wnids) such as \"n02119789\", \"n02102040\", etc. are mapped to\n    numerical labels based on their ascending order.\n\n    All the fields found in the torchvision implementation of the ImageNet\n    dataset (`wnids`, `wnid_to_idx`, `classes`, `class_to_idx`) are available.\n    \"\"\"\n    def __init__(self, imagenet_path: Union[str, Path],\n                 split: Literal['all', 'train', 'val', 'test'] = 'all',\n                 resize_to: Union[int, Tuple[int, int]] = 84,\n                 loader=default_loader):\n        \"\"\"\n        Creates an instance of the Mini ImageNet dataset.\n\n        This dataset allows to obtain the whole dataset or even only specific\n        splits. Beware that, when using a split different that \"all\", the\n        returned dataset will contain patterns of a subset of the 100 classes.\n        This happens because MiniImagenet was created with the idea of training,\n        validating and testing on a disjoint set of classes.\n\n        This implementation uses the filelists provided by\n        https://github.com/yaoyao-liu/mini-imagenet-tools, which are the ones\n        generated by Ravi and Larochelle (see the linked repo for more details).\n\n        :param imagenet_path: The path to the imagenet folder. This has to be\n            the path to the full imagenet 2012 folder (plain, not resized).\n            Only the \"train\" folder will be used. Because of this, passing the\n            path to the imagenet 2012 \"train\" folder is also allowed.\n        :param split: The split to obtain. Defaults to \"all\". Valid values are\n            \"all\", \"train\", \"val\" and \"test\".\n        :param resize_to: The size of the output images. Can be an `int` value\n            or a tuple of two ints. When passing a single `int` value, images\n            will be resized by forcing as 1:1 aspect ratio. Defaults to 84,\n            which means that images will have size 84x84.\n        \"\"\"\n        self.imagenet_path = MiniImageNetDataset.get_train_path(imagenet_path)\n        \"\"\"\n        The path to the \"train\" folder of full imagenet 2012 directory.\n        \"\"\"\n\n        self.split: Literal['all', 'train', 'val', 'test'] = split\n        \"\"\"\n        The required split.\n        \"\"\"\n\n        if isinstance(resize_to, int):\n            resize_to = (resize_to, resize_to)\n\n        self.resize_to: Tuple[int, int] = resize_to\n        \"\"\"\n        The size of the output images, as a two ints tuple.\n        \"\"\"\n\n        # TODO: the original loader from yaoyao-liu uses cv2.INTER_AREA\n        self._transform = Resize(self.resize_to,\n                                 interpolation=PIL.Image.BILINEAR)\n\n        # The following fields are filled by self.prepare_dataset()\n        self.image_paths: List[str] = []\n        \"\"\"\n        The paths to images.\n        \"\"\"\n\n        self.targets: List[int] = []\n        \"\"\"\n        The class labels for the patterns. Aligned with the image_paths field.\n        \"\"\"\n\n        self.wnids: List[str] = []\n        \"\"\"\n        The list of wnids (the textual class labels, such as \"n02119789\").\n        \"\"\"\n\n        self.wnid_to_idx: Dict[str, int] = dict()\n        \"\"\"\n        A dictionary mapping wnids to numerical labels in range [0, 100).\n        \"\"\"\n\n        self.classes: List[Tuple[str, ...]] = []\n        \"\"\"\n        A list mapping numerical labels (the element index) to a tuple of human\n        readable categories. For instance:\n        ('great grey owl', 'great gray owl', 'Strix nebulosa').\n        \"\"\"\n\n        self.class_to_idx: Dict[str, int] = dict()\n        \"\"\"\n        A dictionary mapping each string of the tuples found in the classes \n        field to their numerical label. That is, this dictionary contains the \n        inverse mapping of classes field.\n        \"\"\"\n\n        self.loader = loader\n\n        if not self.imagenet_path.exists():\n            raise ValueError('The provided directory does not exist.')\n\n        if self.split not in ['all', 'train', 'val', 'test']:\n            raise ValueError('Invalid split. Valid values are: \"train\", \"val\", '\n                             '\"test\"')\n\n        self.prepare_dataset()\n\n        super().__init__()\n\n    @staticmethod\n    def get_train_path(root_path: Union[str, Path]):\n        root_path = Path(root_path)\n        if (root_path / 'train').exists():\n            return root_path / 'train'\n        return root_path\n\n    def prepare_dataset(self):\n        # Read the CSV containing the file list for this split\n        images = dict()\n\n        csv_dir = Path(__file__).resolve().parent / 'csv_files'\n        if self.split == 'all':\n            considered_csvs = ['train.csv', 'val.csv', 'test.csv']\n        else:\n            considered_csvs = [self.split + '.csv']\n\n        for csv_name in considered_csvs:\n            csv_path = str(csv_dir / csv_name)\n\n            with open(csv_path) as csvfile:\n                csv_reader = csv.reader(csvfile, delimiter=',')\n                next(csv_reader, None)  # Skip header\n\n                for row in csv_reader:\n                    if row[1] in images.keys():\n                        images[row[1]].append(row[0])\n                    else:\n                        images[row[1]] = [row[0]]\n\n        # Fill fields like wnids, wnid_to_idx, etc.\n        # Those fields have the same meaning of the ones found in the\n        # torchvision implementation of the ImageNet dataset. Of course some\n        # work had to be done to keep this fields aligned for mini imagenet,\n        # which only contains 100 classes of the original 1000.\n        #\n        # wnids are 'n01440764', 'n01443537', 'n01484850', etc.\n        #\n        # self.wnid_to_idx is a dict mapping wnids to numerical labels\n        #\n        # self.classes is a list mapping numerical labels (the element\n        # index) to a tuple of human readable categories. For instance:\n        # ('great grey owl', 'great gray owl', 'Strix nebulosa').\n        #\n        # self.class_to_idx is a dict mapping each string of the\n        # aforementioned tuples to its numerical label. That is, it contains\n        # the inverse mapping of self.classes.\n        self.wnids = MINI_IMAGENET_WNIDS\n        self.wnid_to_idx = MINI_IMAGENET_WNID_TO_IDX\n        self.classes = MINI_IMAGENET_CLASSES\n        self.class_to_idx = MINI_IMAGENET_CLASS_TO_IDX\n\n        for cls in images.keys():\n            cls_numerical_label = self.wnid_to_idx[cls]\n            lst_files = []\n            for file in glob.glob(str(self.imagenet_path / cls /\n                                      (\"*\" + cls + \"*\"))):\n                lst_files.append(file)\n\n            lst_index = [int(i[i.rfind('_') + 1:i.rfind('.')]) for i in\n                         lst_files]\n            index_sorted = sorted(range(len(lst_index)),\n                                  key=lst_index.__getitem__)\n\n            index_selected = [int(i[i.index('.') - 4:i.index('.')]) for\n                              i in images[cls]]\n            selected_images = np.array(index_sorted)[\n                np.array(index_selected) - 1]\n            for i in np.arange(len(selected_images)):\n                self.image_paths.append(lst_files[selected_images[i]])\n                self.targets.append(cls_numerical_label)\n\n    def __len__(self):\n        return len(self.targets)\n\n    def __getitem__(self, item):\n        img = self.loader(self.image_paths[item])\n        img = self._transform(img)\n        return img, self.targets[item]",
  "def __init__(self, imagenet_path: Union[str, Path],\n                 split: Literal['all', 'train', 'val', 'test'] = 'all',\n                 resize_to: Union[int, Tuple[int, int]] = 84,\n                 loader=default_loader):\n        \"\"\"\n        Creates an instance of the Mini ImageNet dataset.\n\n        This dataset allows to obtain the whole dataset or even only specific\n        splits. Beware that, when using a split different that \"all\", the\n        returned dataset will contain patterns of a subset of the 100 classes.\n        This happens because MiniImagenet was created with the idea of training,\n        validating and testing on a disjoint set of classes.\n\n        This implementation uses the filelists provided by\n        https://github.com/yaoyao-liu/mini-imagenet-tools, which are the ones\n        generated by Ravi and Larochelle (see the linked repo for more details).\n\n        :param imagenet_path: The path to the imagenet folder. This has to be\n            the path to the full imagenet 2012 folder (plain, not resized).\n            Only the \"train\" folder will be used. Because of this, passing the\n            path to the imagenet 2012 \"train\" folder is also allowed.\n        :param split: The split to obtain. Defaults to \"all\". Valid values are\n            \"all\", \"train\", \"val\" and \"test\".\n        :param resize_to: The size of the output images. Can be an `int` value\n            or a tuple of two ints. When passing a single `int` value, images\n            will be resized by forcing as 1:1 aspect ratio. Defaults to 84,\n            which means that images will have size 84x84.\n        \"\"\"\n        self.imagenet_path = MiniImageNetDataset.get_train_path(imagenet_path)\n        \"\"\"\n        The path to the \"train\" folder of full imagenet 2012 directory.\n        \"\"\"\n\n        self.split: Literal['all', 'train', 'val', 'test'] = split\n        \"\"\"\n        The required split.\n        \"\"\"\n\n        if isinstance(resize_to, int):\n            resize_to = (resize_to, resize_to)\n\n        self.resize_to: Tuple[int, int] = resize_to\n        \"\"\"\n        The size of the output images, as a two ints tuple.\n        \"\"\"\n\n        # TODO: the original loader from yaoyao-liu uses cv2.INTER_AREA\n        self._transform = Resize(self.resize_to,\n                                 interpolation=PIL.Image.BILINEAR)\n\n        # The following fields are filled by self.prepare_dataset()\n        self.image_paths: List[str] = []\n        \"\"\"\n        The paths to images.\n        \"\"\"\n\n        self.targets: List[int] = []\n        \"\"\"\n        The class labels for the patterns. Aligned with the image_paths field.\n        \"\"\"\n\n        self.wnids: List[str] = []\n        \"\"\"\n        The list of wnids (the textual class labels, such as \"n02119789\").\n        \"\"\"\n\n        self.wnid_to_idx: Dict[str, int] = dict()\n        \"\"\"\n        A dictionary mapping wnids to numerical labels in range [0, 100).\n        \"\"\"\n\n        self.classes: List[Tuple[str, ...]] = []\n        \"\"\"\n        A list mapping numerical labels (the element index) to a tuple of human\n        readable categories. For instance:\n        ('great grey owl', 'great gray owl', 'Strix nebulosa').\n        \"\"\"\n\n        self.class_to_idx: Dict[str, int] = dict()\n        \"\"\"\n        A dictionary mapping each string of the tuples found in the classes \n        field to their numerical label. That is, this dictionary contains the \n        inverse mapping of classes field.\n        \"\"\"\n\n        self.loader = loader\n\n        if not self.imagenet_path.exists():\n            raise ValueError('The provided directory does not exist.')\n\n        if self.split not in ['all', 'train', 'val', 'test']:\n            raise ValueError('Invalid split. Valid values are: \"train\", \"val\", '\n                             '\"test\"')\n\n        self.prepare_dataset()\n\n        super().__init__()",
  "def get_train_path(root_path: Union[str, Path]):\n        root_path = Path(root_path)\n        if (root_path / 'train').exists():\n            return root_path / 'train'\n        return root_path",
  "def prepare_dataset(self):\n        # Read the CSV containing the file list for this split\n        images = dict()\n\n        csv_dir = Path(__file__).resolve().parent / 'csv_files'\n        if self.split == 'all':\n            considered_csvs = ['train.csv', 'val.csv', 'test.csv']\n        else:\n            considered_csvs = [self.split + '.csv']\n\n        for csv_name in considered_csvs:\n            csv_path = str(csv_dir / csv_name)\n\n            with open(csv_path) as csvfile:\n                csv_reader = csv.reader(csvfile, delimiter=',')\n                next(csv_reader, None)  # Skip header\n\n                for row in csv_reader:\n                    if row[1] in images.keys():\n                        images[row[1]].append(row[0])\n                    else:\n                        images[row[1]] = [row[0]]\n\n        # Fill fields like wnids, wnid_to_idx, etc.\n        # Those fields have the same meaning of the ones found in the\n        # torchvision implementation of the ImageNet dataset. Of course some\n        # work had to be done to keep this fields aligned for mini imagenet,\n        # which only contains 100 classes of the original 1000.\n        #\n        # wnids are 'n01440764', 'n01443537', 'n01484850', etc.\n        #\n        # self.wnid_to_idx is a dict mapping wnids to numerical labels\n        #\n        # self.classes is a list mapping numerical labels (the element\n        # index) to a tuple of human readable categories. For instance:\n        # ('great grey owl', 'great gray owl', 'Strix nebulosa').\n        #\n        # self.class_to_idx is a dict mapping each string of the\n        # aforementioned tuples to its numerical label. That is, it contains\n        # the inverse mapping of self.classes.\n        self.wnids = MINI_IMAGENET_WNIDS\n        self.wnid_to_idx = MINI_IMAGENET_WNID_TO_IDX\n        self.classes = MINI_IMAGENET_CLASSES\n        self.class_to_idx = MINI_IMAGENET_CLASS_TO_IDX\n\n        for cls in images.keys():\n            cls_numerical_label = self.wnid_to_idx[cls]\n            lst_files = []\n            for file in glob.glob(str(self.imagenet_path / cls /\n                                      (\"*\" + cls + \"*\"))):\n                lst_files.append(file)\n\n            lst_index = [int(i[i.rfind('_') + 1:i.rfind('.')]) for i in\n                         lst_files]\n            index_sorted = sorted(range(len(lst_index)),\n                                  key=lst_index.__getitem__)\n\n            index_selected = [int(i[i.index('.') - 4:i.index('.')]) for\n                              i in images[cls]]\n            selected_images = np.array(index_sorted)[\n                np.array(index_selected) - 1]\n            for i in np.arange(len(selected_images)):\n                self.image_paths.append(lst_files[selected_images[i]])\n                self.targets.append(cls_numerical_label)",
  "def __len__(self):\n        return len(self.targets)",
  "def __getitem__(self, item):\n        img = self.loader(self.image_paths[item])\n        img = self._transform(img)\n        return img, self.targets[item]",
  "class CUB200(PathsDataset, DownloadableDataset):\n    \"\"\" Basic CUB200 PathsDataset to be used as a standard PyTorch Dataset.\n        A classic continual learning benchmark built on top of this dataset\n        can be found in 'benchmarks.classic', while for more custom benchmark\n        design please use the 'benchmarks.generators'.\"\"\"\n\n    images_folder = 'CUB_200_2011/images'\n    official_url = 'http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/'\\\n                   'CUB_200_2011.tgz'\n    gdrive_url = \"https://drive.google.com/u/0/uc?id=\" \\\n                 \"1hbzc_P1FuxMkcabkgn9ZKinBwW683j45\"\n    filename = 'CUB_200_2011.tgz'\n    tgz_md5 = '97eceeb196236b17998738112f37df78'\n\n    def __init__(\n            self,\n            root: Union[str, Path] = None,\n            *,\n            train=True, transform=None, target_transform=None,\n            loader=default_loader, download=True):\n        \"\"\"\n\n        :param root: root dir where the dataset can be found or downloaded.\n            Defaults to None, which means that the default location for\n            'CUB_200_2011' will be used.\n        :param train: train or test subset of the original dataset. Default\n            to True.\n        :param transform: eventual input data transformations to apply.\n            Default to None.\n        :param target_transform: eventual target data transformations to apply.\n            Default to None.\n        :param loader: method to load the data from disk. Default to\n            torchvision default_loader.\n        :param download: default set to True. If the data is already\n            downloaded it will skip the download.\n        \"\"\"\n\n        if root is None:\n            root = default_dataset_location('CUB_200_2011')\n\n        self.train = train\n\n        DownloadableDataset.__init__(\n            self, root, download=download, verbose=True)\n        self._load_dataset()\n\n        PathsDataset.__init__(\n            self, os.path.join(root, CUB200.images_folder), self._images,\n            transform=transform, target_transform=target_transform,\n            loader=loader)\n\n    def _download_dataset(self) -> None:\n        try:\n            self._download_and_extract_archive(\n                    CUB200.official_url, CUB200.filename,\n                    checksum=CUB200.tgz_md5)\n        except Exception:\n            if self.verbose:\n                print('[CUB200] Direct download may no longer be possible, '\n                      'will try GDrive.')\n\n        filepath = self.root / self.filename\n        gdown.download(self.gdrive_url, str(filepath), quiet=False)\n        gdown.cached_download(\n            self.gdrive_url, str(filepath), md5=self.tgz_md5\n        )\n\n        self._extract_archive(filepath)\n\n    def _download_error_message(self) -> str:\n        return '[CUB200] Error downloading the dataset. Consider downloading ' \\\n               'it manually at: ' + CUB200.official_url + ' and placing it ' \\\n               'in: ' + str(self.root)\n\n    def _load_metadata(self):\n        \"\"\" Main method to load the CUB200 metadata \"\"\"\n\n        cub_dir = self.root / 'CUB_200_2011'\n        self._images = OrderedDict()\n\n        with open(str(cub_dir / 'train_test_split.txt')) as csv_file:\n            csv_reader = csv.reader(csv_file, delimiter=' ')\n            for row in csv_reader:\n                img_id = int(row[0])\n                is_train_instance = int(row[1]) == 1\n                if is_train_instance == self.train:\n                    self._images[img_id] = []\n\n        with open(str(cub_dir / 'images.txt')) as csv_file:\n            csv_reader = csv.reader(csv_file, delimiter=' ')\n            for row in csv_reader:\n                img_id = int(row[0])\n                if img_id in self._images:\n                    self._images[img_id].append(row[1])\n\n        with open(str(cub_dir / 'image_class_labels.txt')) as csv_file:\n            csv_reader = csv.reader(csv_file, delimiter=' ')\n            for row in csv_reader:\n                img_id = int(row[0])\n                if img_id in self._images:\n                    # CUB starts counting classes from 1 ...\n                    self._images[img_id].append(int(row[1]) - 1)\n\n        with open(str(cub_dir / 'bounding_boxes.txt')) as csv_file:\n            csv_reader = csv.reader(csv_file, delimiter=' ')\n            for row in csv_reader:\n                img_id = int(row[0])\n                if img_id in self._images:\n                    box_cub = [int(float(x)) for x in row[1:]]\n                    box_avl = [box_cub[1], box_cub[0], box_cub[3], box_cub[2]]\n                    # PathsDataset accepts (top, left, height, width)\n                    self._images[img_id].append(box_avl)\n\n        images_tuples = []\n        for _, img_tuple in self._images.items():\n            images_tuples.append(tuple(img_tuple))\n        self._images = images_tuples\n\n        # Integrity check\n        for row in self._images:\n            filepath = self.root / CUB200.images_folder / row[0]\n            if not filepath.is_file():\n                if self.verbose:\n                    print('[CUB200] Error checking integrity of:', filepath)\n                return False\n\n        return True",
  "def __init__(\n            self,\n            root: Union[str, Path] = None,\n            *,\n            train=True, transform=None, target_transform=None,\n            loader=default_loader, download=True):\n        \"\"\"\n\n        :param root: root dir where the dataset can be found or downloaded.\n            Defaults to None, which means that the default location for\n            'CUB_200_2011' will be used.\n        :param train: train or test subset of the original dataset. Default\n            to True.\n        :param transform: eventual input data transformations to apply.\n            Default to None.\n        :param target_transform: eventual target data transformations to apply.\n            Default to None.\n        :param loader: method to load the data from disk. Default to\n            torchvision default_loader.\n        :param download: default set to True. If the data is already\n            downloaded it will skip the download.\n        \"\"\"\n\n        if root is None:\n            root = default_dataset_location('CUB_200_2011')\n\n        self.train = train\n\n        DownloadableDataset.__init__(\n            self, root, download=download, verbose=True)\n        self._load_dataset()\n\n        PathsDataset.__init__(\n            self, os.path.join(root, CUB200.images_folder), self._images,\n            transform=transform, target_transform=target_transform,\n            loader=loader)",
  "def _download_dataset(self) -> None:\n        try:\n            self._download_and_extract_archive(\n                    CUB200.official_url, CUB200.filename,\n                    checksum=CUB200.tgz_md5)\n        except Exception:\n            if self.verbose:\n                print('[CUB200] Direct download may no longer be possible, '\n                      'will try GDrive.')\n\n        filepath = self.root / self.filename\n        gdown.download(self.gdrive_url, str(filepath), quiet=False)\n        gdown.cached_download(\n            self.gdrive_url, str(filepath), md5=self.tgz_md5\n        )\n\n        self._extract_archive(filepath)",
  "def _download_error_message(self) -> str:\n        return '[CUB200] Error downloading the dataset. Consider downloading ' \\\n               'it manually at: ' + CUB200.official_url + ' and placing it ' \\\n               'in: ' + str(self.root)",
  "def _load_metadata(self):\n        \"\"\" Main method to load the CUB200 metadata \"\"\"\n\n        cub_dir = self.root / 'CUB_200_2011'\n        self._images = OrderedDict()\n\n        with open(str(cub_dir / 'train_test_split.txt')) as csv_file:\n            csv_reader = csv.reader(csv_file, delimiter=' ')\n            for row in csv_reader:\n                img_id = int(row[0])\n                is_train_instance = int(row[1]) == 1\n                if is_train_instance == self.train:\n                    self._images[img_id] = []\n\n        with open(str(cub_dir / 'images.txt')) as csv_file:\n            csv_reader = csv.reader(csv_file, delimiter=' ')\n            for row in csv_reader:\n                img_id = int(row[0])\n                if img_id in self._images:\n                    self._images[img_id].append(row[1])\n\n        with open(str(cub_dir / 'image_class_labels.txt')) as csv_file:\n            csv_reader = csv.reader(csv_file, delimiter=' ')\n            for row in csv_reader:\n                img_id = int(row[0])\n                if img_id in self._images:\n                    # CUB starts counting classes from 1 ...\n                    self._images[img_id].append(int(row[1]) - 1)\n\n        with open(str(cub_dir / 'bounding_boxes.txt')) as csv_file:\n            csv_reader = csv.reader(csv_file, delimiter=' ')\n            for row in csv_reader:\n                img_id = int(row[0])\n                if img_id in self._images:\n                    box_cub = [int(float(x)) for x in row[1:]]\n                    box_avl = [box_cub[1], box_cub[0], box_cub[3], box_cub[2]]\n                    # PathsDataset accepts (top, left, height, width)\n                    self._images[img_id].append(box_avl)\n\n        images_tuples = []\n        for _, img_tuple in self._images.items():\n            images_tuples.append(tuple(img_tuple))\n        self._images = images_tuples\n\n        # Integrity check\n        for row in self._images:\n            filepath = self.root / CUB200.images_folder / row[0]\n            if not filepath.is_file():\n                if self.verbose:\n                    print('[CUB200] Error checking integrity of:', filepath)\n                return False\n\n        return True",
  "class INATURALIST_DATA(object):\n    \"\"\"\n    INATURALIST downloader.\n    \"\"\"\n\n    def __init__(self, data_folder='data/', trainval=True):\n        \"\"\"\n        Args:\n            data_folder (string): folder in which to download\n            inaturalist dataset.\n        \"\"\"\n        # Get train data (incl val data) or test data\n        self.trainval = trainval\n        self.log = logging.getLogger(\"avalanche\")\n\n        if os.path.isabs(data_folder):\n            self.data_folder = data_folder\n        else:\n            self.data_folder = os.path.join(os.path.dirname(__file__),\n                                            data_folder)\n\n        try:\n            # Create target Directory for INATURALIST data\n            os.makedirs(self.data_folder, exist_ok=True)\n            self.log.info(\"Directory %s created\", self.data_folder)\n            self.download = True\n            self.download_inaturalist()\n\n        except OSError:\n            import traceback\n            traceback.print_exc()\n            self.download = False\n            self.log.error(\"Directory %s already exists\", self.data_folder)\n\n    def download_inaturalist(self):\n        \"\"\" Download and extract inaturalist data\n\n            :param extra: download also additional INATURALIST data not strictly\n                required by the data loader.\n        \"\"\"\n\n        data2download = train_data if self.trainval else test_data\n\n        for name in data2download:\n            self.log.info(\"Downloading \" + name[1] + \"...\")\n            save_name = os.path.join(self.data_folder, name[0])\n            if not os.path.exists(save_name):\n                urlretrieve(name[1], save_name)\n            else:\n                self.log.info(\"Skipping download, exists: \", save_name)\n\n            if name[0].endswith(\"tar.gz\"):\n                untar_save_name = os.path.join(\n                    self.data_folder, '.'.join(name[0].split('.')[:-2]))\n                if not os.path.exists(untar_save_name):\n                    with tarfile.open(\n                            os.path.join(self.data_folder, name[0]),\n                            \"r:gz\") as tar:\n                        self.log.info('Extracting INATURALIST images...')\n                        tar.extractall(self.data_folder)\n                        self.log.info('Done!')\n                else:\n                    self.log.info(\"Skipping untarring, exists: \", save_name)\n        self.log.info(\"Download complete.\")",
  "def __init__(self, data_folder='data/', trainval=True):\n        \"\"\"\n        Args:\n            data_folder (string): folder in which to download\n            inaturalist dataset.\n        \"\"\"\n        # Get train data (incl val data) or test data\n        self.trainval = trainval\n        self.log = logging.getLogger(\"avalanche\")\n\n        if os.path.isabs(data_folder):\n            self.data_folder = data_folder\n        else:\n            self.data_folder = os.path.join(os.path.dirname(__file__),\n                                            data_folder)\n\n        try:\n            # Create target Directory for INATURALIST data\n            os.makedirs(self.data_folder, exist_ok=True)\n            self.log.info(\"Directory %s created\", self.data_folder)\n            self.download = True\n            self.download_inaturalist()\n\n        except OSError:\n            import traceback\n            traceback.print_exc()\n            self.download = False\n            self.log.error(\"Directory %s already exists\", self.data_folder)",
  "def download_inaturalist(self):\n        \"\"\" Download and extract inaturalist data\n\n            :param extra: download also additional INATURALIST data not strictly\n                required by the data loader.\n        \"\"\"\n\n        data2download = train_data if self.trainval else test_data\n\n        for name in data2download:\n            self.log.info(\"Downloading \" + name[1] + \"...\")\n            save_name = os.path.join(self.data_folder, name[0])\n            if not os.path.exists(save_name):\n                urlretrieve(name[1], save_name)\n            else:\n                self.log.info(\"Skipping download, exists: \", save_name)\n\n            if name[0].endswith(\"tar.gz\"):\n                untar_save_name = os.path.join(\n                    self.data_folder, '.'.join(name[0].split('.')[:-2]))\n                if not os.path.exists(untar_save_name):\n                    with tarfile.open(\n                            os.path.join(self.data_folder, name[0]),\n                            \"r:gz\") as tar:\n                        self.log.info('Extracting INATURALIST images...')\n                        tar.extractall(self.data_folder)\n                        self.log.info('Done!')\n                else:\n                    self.log.info(\"Skipping untarring, exists: \", save_name)\n        self.log.info(\"Download complete.\")",
  "def pil_loader(path):\n    \"\"\" Load an Image with PIL \"\"\"\n    # open path as file to avoid ResourceWarning\n    # (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, 'rb') as f:\n        img = Image.open(f)\n        return img.convert('RGB')",
  "def _isArrayLike(obj):\n    return hasattr(obj, '__iter__') and hasattr(obj, '__len__')",
  "class INATURALIST2018(Dataset):\n    \"\"\" INATURALIST Pytorch Dataset\n\n    For default selection of 10 supercategories:\n    - Training Images in total: 428,830\n    - Validation Images in total:  23,229\n    - Shape of images: torch.Size([1, 3, 600, 800])\n    - Class counts per supercategory (both train/val):\n     { 'Amphibia': 144,\n      'Animalia': 178,\n      'Arachnida': 114,\n      'Aves': 1258,\n      'Fungi': 321,\n      'Insecta': 2031,\n      'Mammalia': 234,\n      'Mollusca': 262,\n      'Plantae': 2917,\n      'Reptilia': 284}\n    \"\"\"\n    splits = ['train', 'val', 'test']\n\n    def_supcats = ['Amphibia', 'Animalia', 'Arachnida', 'Aves', 'Fungi',\n                   'Insecta', 'Mammalia', 'Mollusca', 'Plantae', 'Reptilia']\n\n    def __init__(self,\n                 root=expanduser(\"~\") + \"/.avalanche/data/inaturalist2018/\",\n                 split='train', transform=ToTensor(), target_transform=None,\n                 loader=pil_loader, download=True, supcats=None):\n        super().__init__()\n        # conda install -c conda-forge pycocotools\n        from pycocotools.coco import COCO as jsonparser\n\n        assert split in self.splits\n        self.split = split  # training set or test set\n        self.transform = transform\n        self.target_transform = target_transform\n        self.root = root\n        self.loader = loader\n        self.log = logging.getLogger(\"avalanche\")\n\n        # Supercategories to include (None = all)\n        self.supcats = supcats if supcats is not None else self.def_supcats\n\n        if download:\n            download_trainval = self.split in ['train', 'val']\n            self.inat_data = INATURALIST_DATA(data_folder=root,\n                                              trainval=download_trainval)\n\n        # load annotations\n        ann_file = f'{split}2018.json'\n        self.log.info(f'Loading annotations from: {ann_file}')\n        self.ds = jsonparser(annotation_file=os.path.join(root, ann_file))\n\n        self.img_ids, self.targets = [], []  # targets field is required!\n        self.cats_per_supcat = {}\n\n        # Filter full dataset parsed\n        for ann in self.ds.anns.values():\n            img_id = ann[\"image_id\"]\n            cat_id = ann[\"category_id\"]\n\n            # img = self.ds.loadImgs(img_id)[0][\"file_name\"]  # Img Path\n            cat = self.ds.loadCats(cat_id)[0]  # Get category\n            target = cat[\"name\"]  # Is subdirectory\n            supcat = cat[\"supercategory\"]  # Is parent directory\n\n            if self.supcats is None or supcat in self.supcats:  # Made selection\n\n                # Add category to supercategory\n                if supcat not in self.cats_per_supcat:\n                    self.cats_per_supcat[supcat] = set()\n                self.cats_per_supcat[supcat].add(int(target))  # Need int\n\n                # Add to list\n                self.img_ids.append(img_id)\n                self.targets.append(target)\n                # self.suptargets.append(supcat)\n\n        cnt_per_supcat = {k: len(v) for k, v in self.cats_per_supcat.items()}\n        self.log.info(\"Classes per supercategories:\")\n        self.log.info(pprint.pformat(cnt_per_supcat, indent=2))\n        self.log.info(f\"Images in total: {self.__len__()}\")\n\n    def _load_image(self, img_id: int) -> Image.Image:\n        path = self.ds.loadImgs(img_id)[0][\"file_name\"]\n        return Image.open(os.path.join(self.root, path)).convert(\"RGB\")\n\n    def _load_target(self, img_id) -> List[Any]:\n        return self.ds.loadAnns(self.ds.getAnnIds(img_id))\n\n    def __getitem__(self, index):\n        \"\"\"\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (sample, target) where target is class_index of the target\n                class.\n        \"\"\"\n\n        id = self.img_ids[index]\n        img = self._load_image(id)\n        # target = self._load_target(id)\n        target = self.targets[index]\n\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.img_ids)",
  "def __init__(self,\n                 root=expanduser(\"~\") + \"/.avalanche/data/inaturalist2018/\",\n                 split='train', transform=ToTensor(), target_transform=None,\n                 loader=pil_loader, download=True, supcats=None):\n        super().__init__()\n        # conda install -c conda-forge pycocotools\n        from pycocotools.coco import COCO as jsonparser\n\n        assert split in self.splits\n        self.split = split  # training set or test set\n        self.transform = transform\n        self.target_transform = target_transform\n        self.root = root\n        self.loader = loader\n        self.log = logging.getLogger(\"avalanche\")\n\n        # Supercategories to include (None = all)\n        self.supcats = supcats if supcats is not None else self.def_supcats\n\n        if download:\n            download_trainval = self.split in ['train', 'val']\n            self.inat_data = INATURALIST_DATA(data_folder=root,\n                                              trainval=download_trainval)\n\n        # load annotations\n        ann_file = f'{split}2018.json'\n        self.log.info(f'Loading annotations from: {ann_file}')\n        self.ds = jsonparser(annotation_file=os.path.join(root, ann_file))\n\n        self.img_ids, self.targets = [], []  # targets field is required!\n        self.cats_per_supcat = {}\n\n        # Filter full dataset parsed\n        for ann in self.ds.anns.values():\n            img_id = ann[\"image_id\"]\n            cat_id = ann[\"category_id\"]\n\n            # img = self.ds.loadImgs(img_id)[0][\"file_name\"]  # Img Path\n            cat = self.ds.loadCats(cat_id)[0]  # Get category\n            target = cat[\"name\"]  # Is subdirectory\n            supcat = cat[\"supercategory\"]  # Is parent directory\n\n            if self.supcats is None or supcat in self.supcats:  # Made selection\n\n                # Add category to supercategory\n                if supcat not in self.cats_per_supcat:\n                    self.cats_per_supcat[supcat] = set()\n                self.cats_per_supcat[supcat].add(int(target))  # Need int\n\n                # Add to list\n                self.img_ids.append(img_id)\n                self.targets.append(target)\n                # self.suptargets.append(supcat)\n\n        cnt_per_supcat = {k: len(v) for k, v in self.cats_per_supcat.items()}\n        self.log.info(\"Classes per supercategories:\")\n        self.log.info(pprint.pformat(cnt_per_supcat, indent=2))\n        self.log.info(f\"Images in total: {self.__len__()}\")",
  "def _load_image(self, img_id: int) -> Image.Image:\n        path = self.ds.loadImgs(img_id)[0][\"file_name\"]\n        return Image.open(os.path.join(self.root, path)).convert(\"RGB\")",
  "def _load_target(self, img_id) -> List[Any]:\n        return self.ds.loadAnns(self.ds.getAnnIds(img_id))",
  "def __getitem__(self, index):\n        \"\"\"\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (sample, target) where target is class_index of the target\n                class.\n        \"\"\"\n\n        id = self.img_ids[index]\n        img = self._load_image(id)\n        # target = self._load_target(id)\n        target = self.targets[index]\n\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target",
  "def __len__(self):\n        return len(self.img_ids)",
  "class PixelsPermutation(object):\n    \"\"\"\n    Apply a fixed permutation to the pixels of the given image.\n\n    Works with both Tensors and PIL images. Returns an object of the same type\n    of the input element.\n    \"\"\"\n\n    def __init__(self, index_permutation: Sequence[int]):\n        self.permutation = index_permutation\n        self._to_tensor = ToTensor()\n        self._to_image = ToPILImage()\n\n    def __call__(self, img: Union[Image, Tensor]):\n        is_image = isinstance(img, Image)\n        if (not is_image) and (not isinstance(img, Tensor)):\n            raise ValueError('Invalid input: must be a PIL image or a Tensor')\n\n        if is_image:\n            img = self._to_tensor(img)\n\n        img = img.view(-1)[self.permutation].view(*img.shape)\n\n        if is_image:\n            img = self._to_image(img)\n\n        return img",
  "def SplitOmniglot(\n        n_experiences: int,\n        *,\n        return_task_id=False,\n        seed: Optional[int] = None,\n        fixed_class_order: Optional[Sequence[int]] = None,\n        shuffle: bool = True,\n        train_transform: Optional[Any] = _default_omniglot_train_transform,\n        eval_transform: Optional[Any] = _default_omniglot_eval_transform,\n        dataset_root: Union[str, Path] = None):\n    \"\"\"\n    Creates a CL benchmark using the OMNIGLOT dataset.\n\n    If the dataset is not present in the computer, this method will\n    automatically download and store it.\n\n    The returned benchmark will return experiences containing all patterns of a\n    subset of classes, which means that each class is only seen \"once\".\n    This is one of the most common scenarios in the Continual Learning\n    literature. Common names used in literature to describe this kind of\n    scenario are \"Class Incremental\", \"New Classes\", etc.\n\n    By default, an equal amount of classes will be assigned to each experience.\n    OMNIGLOT consists of 964 classes, which means that the number of\n    experiences can be 1, 2, 4, 241, 482, 964.\n\n    This generator doesn't force a choice on the availability of task labels,\n    a choice that is left to the user (see the `return_task_id` parameter for\n    more info on task labels).\n\n    The benchmark instance returned by this method will have two fields,\n    `train_stream` and `test_stream`, which can be iterated to obtain\n    training and test :class:`Experience`. Each Experience contains the\n    `dataset` and the associated task label.\n\n    The benchmark API is quite simple and is uniform across all benchmark\n    generators. It is recommended to check the tutorial of the \"benchmark\" API,\n    which contains usage examples ranging from \"basic\" to \"advanced\".\n\n    :param n_experiences: The number of incremental experiences in the current\n        benchmark. The value of this parameter should be a divisor of 10.\n    :param return_task_id: if True, a progressive task id is returned for every\n        experience. If False, all experiences will have a task ID of 0.\n    :param seed: A valid int used to initialize the random number generator.\n        Can be None.\n    :param fixed_class_order: A list of class IDs used to define the class\n        order. If None, value of ``seed`` will be used to define the class\n        order. If non-None, ``seed`` parameter will be ignored.\n        Defaults to None.\n    :param shuffle: If true, the class order in the incremental experiences is\n        randomly shuffled. Default to false.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations).\n        If no transformation is passed, the default train transformation\n        will be used.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations).\n        If no transformation is passed, the default test transformation\n        will be used.\n    :param dataset_root: The root path of the dataset. Defaults to None, which\n        means that the default location for 'omniglot' will be used.\n\n    :returns: A properly initialized :class:`NCScenario` instance.\n    \"\"\"\n\n    omniglot_train, omniglot_test = _get_omniglot_dataset(dataset_root)\n\n    if return_task_id:\n        return nc_benchmark(\n            train_dataset=omniglot_train,\n            test_dataset=omniglot_test,\n            n_experiences=n_experiences,\n            task_labels=True,\n            seed=seed,\n            fixed_class_order=fixed_class_order,\n            shuffle=shuffle,\n            class_ids_from_zero_in_each_exp=True,\n            train_transform=train_transform,\n            eval_transform=eval_transform)\n    else:\n        return nc_benchmark(\n            train_dataset=omniglot_train,\n            test_dataset=omniglot_test,\n            n_experiences=n_experiences,\n            task_labels=False,\n            seed=seed,\n            fixed_class_order=fixed_class_order,\n            shuffle=shuffle,\n            train_transform=train_transform,\n            eval_transform=eval_transform)",
  "def PermutedOmniglot(\n        n_experiences: int,\n        *,\n        seed: Optional[int] = None,\n        train_transform: Optional[Any] = _default_omniglot_train_transform,\n        eval_transform: Optional[Any] = _default_omniglot_eval_transform,\n        dataset_root: Union[str, Path] = None) -> NCScenario:\n    \"\"\"\n    Creates a Permuted Omniglot benchmark.\n\n    If the dataset is not present in the computer, this method will\n    automatically download and store it.\n\n    Random pixel permutations are used to permute the Omniglot images in\n    ``n_experiences`` different manners. This means that each experience is\n    composed of all the original 964 Omniglot classes, but the pixel in the\n    images are permuted in a different way.\n\n    The benchmark instance returned by this method will have two fields,\n    `train_stream` and `test_stream`, which can be iterated to obtain\n    training and test :class:`Experience`. Each Experience contains the\n    `dataset` and the associated task label.\n\n    A progressive task label, starting from \"0\", is applied to each experience.\n\n    The benchmark API is quite simple and is uniform across all benchmark\n    generators. It is recommended to check the tutorial of the \"benchmark\" API,\n    which contains usage examples ranging from \"basic\" to \"advanced\".\n\n    :param n_experiences: The number of experiences (tasks) in the current\n        benchmark. It indicates how many different permutations of the Omniglot\n        dataset have to be created.\n    :param seed: A valid int used to initialize the random number generator.\n        Can be None.\n    :param train_transform: The transformation to apply to the training data\n        before the random permutation, e.g. a random crop, a normalization or a\n        concatenation of different transformations (see torchvision.transform\n        documentation for a comprehensive list of possible transformations).\n        If no transformation is passed, the default train transformation\n        will be used.\n    :param eval_transform: The transformation to apply to the test data\n        before the random permutation, e.g. a random crop, a normalization or a\n        concatenation of different transformations (see torchvision.transform\n        documentation for a comprehensive list of possible transformations).\n        If no transformation is passed, the default test transformation\n        will be used.\n    :param dataset_root: The root path of the dataset. Defaults to None, which\n        means that the default location for 'omniglot' will be used.\n\n    :returns: A properly initialized :class:`NCScenario` instance.\n    \"\"\"\n\n    list_train_dataset = []\n    list_test_dataset = []\n    rng_permute = np.random.RandomState(seed)\n\n    omniglot_train, omniglot_test = _get_omniglot_dataset(dataset_root)\n\n    # for every incremental experience\n    for _ in range(n_experiences):\n        # choose a random permutation of the pixels in the image\n        idx_permute = torch.from_numpy(rng_permute.permutation(11025)).type(\n            torch.int64)\n\n        permutation = PixelsPermutation(idx_permute)\n\n        permutation_transforms = dict(\n            train=(permutation, None),\n            eval=(permutation, None)\n        )\n\n        # Freeze the permutation\n        permuted_train = AvalancheDataset(\n            omniglot_train,\n            transform_groups=permutation_transforms,\n            initial_transform_group='train').freeze_transforms()\n\n        permuted_test = AvalancheDataset(\n            omniglot_test,\n            transform_groups=permutation_transforms,\n            initial_transform_group='eval').freeze_transforms()\n\n        list_train_dataset.append(permuted_train)\n        list_test_dataset.append(permuted_test)\n\n    return nc_benchmark(\n        list_train_dataset,\n        list_test_dataset,\n        n_experiences=len(list_train_dataset),\n        task_labels=True,\n        shuffle=False,\n        class_ids_from_zero_in_each_exp=True,\n        one_dataset_per_exp=True,\n        train_transform=train_transform,\n        eval_transform=eval_transform)",
  "def RotatedOmniglot(\n        n_experiences: int,\n        *,\n        seed: Optional[int] = None,\n        rotations_list: Optional[Sequence[int]] = None,\n        train_transform: Optional[Any] = _default_omniglot_train_transform,\n        eval_transform: Optional[Any] = _default_omniglot_eval_transform,\n        dataset_root: Union[str, Path] = None) -> NCScenario:\n    \"\"\"\n    Creates a Rotated Omniglot benchmark.\n\n    If the dataset is not present in the computer, this method will\n    automatically download and store it.\n\n    Random angles are used to rotate the Omniglot images in ``n_experiences``\n    different manners. This means that each experience is\n    composed of all the original 964 Omniglot classes, but each image is\n    rotated in a different way.\n\n    The benchmark instance returned by this method will have two fields,\n    `train_stream` and `test_stream`, which can be iterated to obtain\n    training and test :class:`Experience`. Each Experience contains the\n    `dataset` and the associated task label.\n\n    A progressive task label, starting from \"0\", is applied to each experience.\n\n    The benchmark API is quite simple and is uniform across all benchmark\n    generators. It is recommended to check the tutorial of the \"benchmark\" API,\n    which contains usage examples ranging from \"basic\" to \"advanced\".\n\n    :param n_experiences: The number of experiences (tasks) in the current\n        benchmark. It indicates how many different rotations of the Omniglot\n        dataset have to be created.\n    :param seed: A valid int used to initialize the random number generator.\n        Can be None.\n    :param rotations_list: A list of rotations values in degrees (from -180 to\n        180) used to define the rotations. The rotation specified in position\n        0 of the list will be applied to the task 0, the rotation specified in\n        position 1 will be applied to task 1 and so on.\n        If None, value of ``seed`` will be used to define the rotations.\n        If non-None, ``seed`` parameter will be ignored.\n        Defaults to None.\n    :param train_transform: The transformation to apply to the training data\n        after the random rotation, e.g. a random crop, a normalization or a\n        concatenation of different transformations (see torchvision.transform\n        documentation for a comprehensive list of possible transformations).\n        If no transformation is passed, the default train transformation\n        will be used.\n    :param eval_transform: The transformation to apply to the test data\n        after the random rotation, e.g. a random crop, a normalization or a\n        concatenation of different transformations (see torchvision.transform\n        documentation for a comprehensive list of possible transformations).\n        If no transformation is passed, the default test transformation\n        will be used.\n    :param dataset_root: The root path of the dataset. Defaults to None, which\n        means that the default location for 'omniglot' will be used.\n\n    :returns: A properly initialized :class:`NCScenario` instance.\n    \"\"\"\n\n    if rotations_list is None:\n        rng_rotate = np.random.RandomState(seed)\n        rotations_list = [rng_rotate.randint(-180, 181) for _ in range(\n            n_experiences)]\n    else:\n        assert len(rotations_list) == n_experiences, \"The number of rotations\" \\\n                                               \" should match the number\" \\\n                                               \" of incremental experiences.\"\n    assert all(-180 <= rotations_list[i] <= 180\n               for i in range(len(rotations_list))), \"The value of a rotation\" \\\n                                                     \" should be between -180\" \\\n                                                     \" and 180 degrees.\"\n\n    list_train_dataset = []\n    list_test_dataset = []\n\n    omniglot_train, omniglot_test = _get_omniglot_dataset(dataset_root)\n\n    # for every incremental experience\n    for experience in range(n_experiences):\n        rotation_angle = rotations_list[experience]\n\n        rotation = RandomRotation(degrees=(rotation_angle, rotation_angle))\n\n        rotation_transforms = dict(\n            train=(rotation, None),\n            eval=(rotation, None)\n        )\n\n        # Freeze the rotation\n        rotated_train = AvalancheDataset(\n            omniglot_train,\n            transform_groups=rotation_transforms,\n            initial_transform_group='train').freeze_transforms()\n\n        rotated_test = AvalancheDataset(\n            omniglot_test,\n            transform_groups=rotation_transforms,\n            initial_transform_group='eval').freeze_transforms()\n\n        list_train_dataset.append(rotated_train)\n        list_test_dataset.append(rotated_test)\n\n    return nc_benchmark(\n        list_train_dataset,\n        list_test_dataset,\n        n_experiences=len(list_train_dataset),\n        task_labels=True,\n        shuffle=False,\n        class_ids_from_zero_in_each_exp=True,\n        one_dataset_per_exp=True,\n        train_transform=train_transform,\n        eval_transform=eval_transform)",
  "def _get_omniglot_dataset(dataset_root):\n    if dataset_root is None:\n        dataset_root = default_dataset_location('omniglot')\n\n    train = Omniglot(root=dataset_root, train=True, download=True)\n    test = Omniglot(root=dataset_root, train=False, download=True)\n\n    return train, test",
  "def __init__(self, index_permutation: Sequence[int]):\n        self.permutation = index_permutation\n        self._to_tensor = ToTensor()\n        self._to_image = ToPILImage()",
  "def __call__(self, img: Union[Image, Tensor]):\n        is_image = isinstance(img, Image)\n        if (not is_image) and (not isinstance(img, Tensor)):\n            raise ValueError('Invalid input: must be a PIL image or a Tensor')\n\n        if is_image:\n            img = self._to_tensor(img)\n\n        img = img.view(-1)[self.permutation].view(*img.shape)\n\n        if is_image:\n            img = self._to_image(img)\n\n        return img",
  "def check_vision_benchmark(benchmark_instance, show_without_transforms=True):\n    from matplotlib import pyplot as plt\n    from torch.utils.data.dataloader import DataLoader\n    dataset: AvalancheDataset\n\n    print('The benchmark instance contains',\n          len(benchmark_instance.train_stream), 'training experiences.')\n\n    for i, exp in enumerate(benchmark_instance.train_stream):\n        dataset, t = exp.dataset, exp.task_label\n        if show_without_transforms:\n            dataset = dataset.replace_transforms(ToTensor(), None)\n\n        dl = DataLoader(dataset, batch_size=300)\n\n        print('Train experience', exp.current_experience)\n        for mb in dl:\n            x, y, *other = mb\n            print('X tensor:', x.shape)\n            print('Y tensor:', y.shape)\n            if len(other) > 0:\n                print('T tensor:', other[0].shape)\n            img = ToPILImage()(x[0])\n            plt.title('Experience: ' + str(exp.current_experience))\n            plt.imshow(img)\n            plt.show()\n            break",
  "def CORe50(\n        *,\n        scenario: str = \"nicv2_391\",\n        run: int = 0,\n        object_lvl: bool = True,\n        mini: bool = False,\n        train_transform: Optional[Any] = _default_train_transform,\n        eval_transform: Optional[Any] = _default_eval_transform,\n        dataset_root: Union[str, Path] = None):\n    \"\"\"\n    Creates a CL benchmark for CORe50.\n\n    If the dataset is not present in the computer, this method will\n    automatically download and store it.\n\n    This generator can be used to obtain the NI, NC, NIC and NICv2-* scenarios.\n\n    The benchmark instance returned by this method will have two fields,\n    `train_stream` and `test_stream`, which can be iterated to obtain\n    training and test :class:`Experience`. Each Experience contains the\n    `dataset` and the associated task label.\n\n    The task label \"0\" will be assigned to each experience.\n\n    The benchmark API is quite simple and is uniform across all benchmark\n    generators. It is recommended to check the tutorial of the \"benchmark\" API,\n    which contains usage examples ranging from \"basic\" to \"advanced\".\n\n    :param scenario: CORe50 main scenario. It can be chosen between 'ni', 'nc',\n        'nic', 'nicv2_79', 'nicv2_196' or 'nicv2_391.'\n    :param run: number of run for the benchmark. Each run defines a different\n        ordering. Must be a number between 0 and 9.\n    :param object_lvl: True for a 50-way classification at the object level.\n        False if you want to use the categories as classes. Default to True.\n    :param mini: True for processing reduced 32x32 images instead of the\n        original 128x128. Default to False.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param dataset_root: Absolute path indicating where to store the dataset\n        and related metadata. Defaults to None, which means that the default\n        location for\n        'core50' will be used.\n\n    :returns: a properly initialized :class:`GenericCLScenario` instance.\n    \"\"\"\n\n    assert (0 <= run <= 9), \"Pre-defined run of CORe50 are only 10. Indicate \" \\\n                            \"a number between 0 and 9.\"\n    assert (scenario in nbatch.keys()), \"The selected scenario is note \" \\\n                                        \"recognized: it should be 'ni', 'nc',\" \\\n                                        \"'nic', 'nicv2_79', 'nicv2_196' or \" \\\n                                        \"'nicv2_391'.\"\n\n    if dataset_root is None:\n        dataset_root = default_dataset_location('core50')\n\n    # Download the dataset and initialize filelists\n    core_data = CORe50Dataset(root=dataset_root, mini=mini)\n\n    root = core_data.root\n    if mini:\n        bp = \"core50_32x32\"\n    else:\n        bp = \"core50_128x128\"\n    root_img = root / bp\n\n    if object_lvl:\n        suffix = \"/\"\n    else:\n        suffix = \"_cat/\"\n    filelists_bp = scen2dirs[scenario][:-1] + suffix + \"run\" + str(run)\n    train_failists_paths = []\n    for batch_id in range(nbatch[scenario]):\n        train_failists_paths.append(\n            root / filelists_bp / (\"train_batch_\" +\n                                   str(batch_id).zfill(2) + \"_filelist.txt\"))\n\n    benchmark_obj = create_generic_benchmark_from_filelists(\n        root_img, train_failists_paths,\n        [root / filelists_bp / \"test_filelist.txt\"],\n        task_labels=[0 for _ in range(nbatch[scenario])],\n        complete_test_set_only=True,\n        train_transform=train_transform,\n        eval_transform=eval_transform)\n\n    return benchmark_obj",
  "def _adjust_bbox(img_shapes, bbox, ratio=1.1):\n    \"\"\"\n    Adapts bounding box coordinates so that they can be used by\n    torchvision.transforms.functional.crop function.\n\n    This also pads each bounding box according to the `ratio` parameter.\n\n    :param img_shapes: a list of shapes, with each element in the format\n        \"[img.shape[0], img.shape[1]]\".\n    :param bbox: A list of elements in the format \"[right, left, top, bottom]\".\n    :param ratio: The amount of padding. Defaults to \"1.1\".\n\n    :returns: A list of adapted bounding box coordinates.\n    \"\"\"\n    cw = bbox[0] - bbox[1]\n    ch = bbox[2] - bbox[3]\n    center = [int(bbox[1] + cw / 2), int(bbox[3] + ch / 2)]\n    bbox = [\n        min([int(center[0] + (cw * ratio / 2)), img_shapes[0]]),\n        max([int(center[0] - (cw * ratio / 2)), 0]),\n        min([int(center[1] + (ch * ratio / 2)), img_shapes[1]]),\n        max([int(center[1] - (ch * ratio / 2)), 0])]\n    return [bbox[3], bbox[1], bbox[2] - bbox[3], bbox[0] - bbox[1]]",
  "def CLStream51(\n        *,\n        scenario: Literal[\n            'iid', 'class_iid', 'instance',\n            'class_instance'] = \"class_instance\",\n        seed=10,\n        eval_num=None,\n        bbox_crop=True,\n        ratio: float = 1.10,\n        download=True,\n        train_transform=_default_stream51_transform,\n        eval_transform=_default_stream51_transform,\n        dataset_root: Union[str, Path] = None):\n    \"\"\"\n    Creates a CL benchmark for Stream-51.\n\n    If the dataset is not present in the computer, this method will\n    automatically download and store it.\n\n    This generator can be used to obtain the 'iid', 'class_iid', 'instance', and\n    'class_instance' scenarios.\n\n    The benchmark instance returned by this method will have two fields,\n    `train_stream` and `test_stream`, which can be iterated to obtain\n    training and test :class:`Experience`. Avalanche will support the\n    \"out of distribution\" stream in the near future!\n\n    Each Experience contains the `dataset` and the associated task label, which\n    is always 0 for Stream51.\n\n    The benchmark API is quite simple and is uniform across all benchmark\n    generators. It is recommended to check the tutorial of the \"benchmark\" API,\n    which contains usage examples ranging from \"basic\" to \"advanced\".\n\n    :param scenario: A string defining which Stream-51 scenario to return.\n        Can be chosen between 'iid', 'class_iid', 'instance', and\n        'class_instance'. Defaults to 'class_instance'.\n    :param bbox_crop: If True, crops the images by using the bounding boxes\n        defined by Stream51. This is needed to ensure that images depict only\n        the required object (for classification purposes). Defaults to True.\n    :param ratio: A floating point value (>= 1.0) that controls the amount of\n        padding for bounding boxes crop (default: 1.10).\n    :param seed: Random seed for shuffling classes or instances. Defaults to 10.\n    :param eval_num: How many samples to see before evaluating the network for\n        instance ordering and how many classes to see before evaluating the\n        network for the class_instance ordering. Defaults to None, which means\n        that \"30000\" will be used for the 'instance' scenario and \"10\" for the\n        'class_instance' scenario.\n    :param download: If True, the dataset will automatically downloaded.\n        Defaults to True.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations).\n        If no transformation is passed, the default train transformation\n        will be used.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations).\n        If no transformation is passed, the default eval transformation\n        will be used.\n    :param dataset_root: The root path of the dataset.\n        Defaults to None, which means that the default location for\n        'stream51' will be used.\n\n    :returns: A properly initialized :class:`GenericCLScenario` instance.\n    \"\"\"\n\n    # get train and test sets and order them by benchmark\n    train_set = Stream51(root=dataset_root, train=True, download=download)\n    test_set = Stream51(root=dataset_root, train=False, download=download)\n    samples = Stream51.make_dataset(train_set.samples, ordering=scenario,\n                                    seed=seed)\n    dataset_root = train_set.root\n\n    # set appropriate train parameters\n    train_set.samples = samples\n    train_set.targets = [s[0] for s in samples]\n\n    # compute number of tasks\n    if eval_num is None and scenario == 'instance':\n        eval_num = 30000\n        num_tasks = math.ceil(\n            len(train_set) / eval_num)  # evaluate every 30000 samples\n    elif eval_num is None and scenario == 'class_instance':\n        eval_num = 10\n        num_tasks = math.ceil(\n            51 / eval_num)  # evaluate every 10 classes\n    elif scenario == 'instance':\n        num_tasks = math.ceil(\n            len(train_set) / eval_num)  # evaluate every eval_num samples\n    else:\n        num_tasks = math.ceil(\n            51 / eval_num)  # evaluate every eval_num classes\n\n    if scenario == 'instance':\n        # break files into task lists based on eval_num samples\n        train_filelists_paths = []\n        start = 0\n        for i in range(num_tasks):\n            end = min(start + eval_num, len(train_set))\n            train_filelists_paths.append(\n                [(os.path.join(dataset_root, train_set.samples[j][-1]),\n                  train_set.samples[j][0],\n                  _adjust_bbox(train_set.samples[j][-3],\n                               train_set.samples[j][-2],\n                               ratio)) for j in\n                 range(start, end)])\n            start = end\n\n        # use all test data for instance ordering\n        test_filelists_paths = \\\n            [(os.path.join(dataset_root, test_set.samples[j][-1]),\n              test_set.samples[j][0],\n              _adjust_bbox(test_set.samples[j][-3],\n                           test_set.samples[j][-2],\n                           ratio)) for j in range(len(test_set))]\n        test_ood_filelists_paths = None  # no ood testing for instance ordering\n    elif scenario == 'class_instance':\n        # break files into task lists based on classes\n        train_filelists_paths = []\n        test_filelists_paths = []\n        test_ood_filelists_paths = []\n        class_change = [i for i in range(1, len(train_set.targets)) if\n                        train_set.targets[i] != train_set.targets[i - 1]]\n        unique_so_far = []\n        start = 0\n        for i in range(num_tasks):\n            if i == num_tasks - 1:\n                end = len(train_set)\n            else:\n                end = class_change[\n                    min(eval_num + eval_num * i - 1, len(class_change) - 1)]\n            unique_labels = [train_set.targets[k] for k in range(start, end)]\n            unique_labels = list(set(unique_labels))\n            unique_so_far += unique_labels\n            test_files = []\n            test_ood_files = []\n            for ix, test_label in enumerate(test_set.targets):\n                if test_label in unique_so_far:\n                    test_files.append(ix)\n                else:\n                    test_ood_files.append(ix)\n            test_filelists_paths.append(\n                [(os.path.join(dataset_root, test_set.samples[j][-1]),\n                  test_set.samples[j][0],\n                  _adjust_bbox(test_set.samples[j][-3], test_set.samples[j][-2],\n                               ratio)) for j in\n                 test_files])\n            test_ood_filelists_paths.append(\n                [(os.path.join(dataset_root, test_set.samples[j][-1]),\n                  test_set.samples[j][0],\n                  _adjust_bbox(test_set.samples[j][-3], test_set.samples[j][-2],\n                               ratio)) for j in\n                 test_ood_files])\n            train_filelists_paths.append(\n                [(os.path.join(dataset_root, train_set.samples[j][-1]),\n                  train_set.samples[j][0],\n                  _adjust_bbox(train_set.samples[j][-3],\n                               train_set.samples[j][-2],\n                               ratio)) for j in\n                 range(start, end)])\n            start = end\n    else:\n        raise NotImplementedError\n\n    if not bbox_crop:\n        # remove bbox coordinates from lists\n        train_filelists_paths = [[[j[0], j[1]] for j in i] for i in\n                                 train_filelists_paths]\n        test_filelists_paths = [[[j[0], j[1]] for j in i] for i in\n                                test_filelists_paths]\n        if scenario == 'class_instance':\n            test_ood_filelists_paths = [[[j[0], j[1]] for j in i] for i in\n                                        test_ood_filelists_paths]\n\n    benchmark_obj = create_generic_benchmark_from_paths(\n        train_lists_of_files=train_filelists_paths,\n        test_lists_of_files=test_filelists_paths,\n        task_labels=[0 for _ in range(num_tasks)],\n        complete_test_set_only=scenario == 'instance',\n        train_transform=train_transform,\n        eval_transform=eval_transform,\n        dataset_type=AvalancheDatasetType.CLASSIFICATION)\n\n    return benchmark_obj",
  "def SplitCIFAR10(\n        n_experiences: int,\n        *,\n        first_exp_with_half_classes: bool = False,\n        return_task_id=False,\n        seed: Optional[int] = None,\n        fixed_class_order: Optional[Sequence[int]] = None,\n        shuffle: bool = True,\n        train_transform: Optional[Any] = _default_cifar10_train_transform,\n        eval_transform: Optional[Any] = _default_cifar10_eval_transform,\n        dataset_root: Union[str, Path] = None) -> NCScenario:\n    \"\"\"\n    Creates a CL benchmark using the CIFAR10 dataset.\n\n    If the dataset is not present in the computer, this method will\n    automatically download and store it.\n\n    The returned benchmark will return experiences containing all patterns of a\n    subset of classes, which means that each class is only seen \"once\".\n    This is one of the most common scenarios in the Continual Learning\n    literature. Common names used in literature to describe this kind of\n    scenario are \"Class Incremental\", \"New Classes\", etc. By default,\n    an equal amount of classes will be assigned to each experience.\n\n    This generator doesn't force a choice on the availability of task labels,\n    a choice that is left to the user (see the `return_task_id` parameter for\n    more info on task labels).\n\n    The benchmark instance returned by this method will have two fields,\n    `train_stream` and `test_stream`, which can be iterated to obtain\n    training and test :class:`Experience`. Each Experience contains the\n    `dataset` and the associated task label.\n\n    The benchmark API is quite simple and is uniform across all benchmark\n    generators. It is recommended to check the tutorial of the \"benchmark\" API,\n    which contains usage examples ranging from \"basic\" to \"advanced\".\n\n    :param n_experiences: The number of experiences in the current benchmark.\n        The value of this parameter should be a divisor of 10 if\n        `first_task_with_half_classes` is False, a divisor of 5 otherwise.\n    :param first_exp_with_half_classes: A boolean value that indicates if a\n        first pretraining step containing half of the classes should be used.\n        If it's True, the first experience will use half of the classes (5 for\n        cifar10). If this parameter is False, no pretraining step will be\n        used and the dataset is simply split into a the number of experiences\n        defined by the parameter n_experiences. Defaults to False.\n    :param return_task_id: if True, a progressive task id is returned for every\n        experience. If False, all experiences will have a task ID of 0.\n    :param seed: A valid int used to initialize the random number generator.\n        Can be None.\n    :param fixed_class_order: A list of class IDs used to define the class\n        order. If None, value of ``seed`` will be used to define the class\n        order. If not None, the ``seed`` parameter will be ignored.\n        Defaults to None.\n    :param shuffle: If true, the class order in the incremental experiences is\n        randomly shuffled. Default to false.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations).\n        If no transformation is passed, the default train transformation\n        will be used.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations).\n        If no transformation is passed, the default eval transformation\n        will be used.\n    :param dataset_root: The root path of the dataset. Defaults to None, which\n        means that the default location for 'cifar10' will be used.\n\n    :returns: A properly initialized :class:`NCScenario` instance.\n    \"\"\"\n    cifar_train, cifar_test = _get_cifar10_dataset(dataset_root)\n\n    if return_task_id:\n        return nc_benchmark(\n            train_dataset=cifar_train,\n            test_dataset=cifar_test,\n            n_experiences=n_experiences,\n            task_labels=True,\n            seed=seed,\n            fixed_class_order=fixed_class_order,\n            shuffle=shuffle,\n            per_exp_classes={0: 5} if first_exp_with_half_classes else None,\n            class_ids_from_zero_in_each_exp=True,\n            train_transform=train_transform,\n            eval_transform=eval_transform)\n    else:\n        return nc_benchmark(\n            train_dataset=cifar_train,\n            test_dataset=cifar_test,\n            n_experiences=n_experiences,\n            task_labels=False,\n            seed=seed,\n            fixed_class_order=fixed_class_order,\n            shuffle=shuffle,\n            per_exp_classes={0: 5} if first_exp_with_half_classes else None,\n            train_transform=train_transform,\n            eval_transform=eval_transform)",
  "def _get_cifar10_dataset(dataset_root):\n    if dataset_root is None:\n        dataset_root = default_dataset_location('cifar10')\n\n    train_set = CIFAR10(dataset_root, train=True, download=True)\n    test_set = CIFAR10(dataset_root, train=False, download=True)\n\n    return train_set, test_set",
  "def SplitImageNet(\n        dataset_root: Union[str, Path],\n        *,\n        n_experiences=10,\n        per_exp_classes=None,\n        return_task_id=False,\n        seed=0,\n        fixed_class_order=None,\n        shuffle: bool = True,\n        train_transform: Optional[Any] = _default_train_transform,\n        eval_transform: Optional[Any] = _default_eval_transform):\n    \"\"\"\n    Creates a CL benchmark using the ImageNet dataset.\n\n    If the dataset is not present in the computer, **this method will NOT be\n    able automatically download** and store it.\n\n    The returned benchmark will return experiences containing all patterns of a\n    subset of classes, which means that each class is only seen \"once\".\n    This is one of the most common scenarios in the Continual Learning\n    literature. Common names used in literature to describe this kind of\n    scenario are \"Class Incremental\", \"New Classes\", etc. By default,\n    an equal amount of classes will be assigned to each experience.\n\n    This generator doesn't force a choice on the availability of task labels,\n    a choice that is left to the user (see the `return_task_id` parameter for\n    more info on task labels).\n\n    The benchmark instance returned by this method will have two fields,\n    `train_stream` and `test_stream`, which can be iterated to obtain\n    training and test :class:`Experience`. Each Experience contains the\n    `dataset` and the associated task label.\n\n    The benchmark API is quite simple and is uniform across all benchmark\n    generators. It is recommended to check the tutorial of the \"benchmark\" API,\n    which contains usage examples ranging from \"basic\" to \"advanced\".\n\n    :param dataset_root: Base path where Imagenet data is stored.\n    :param n_experiences: The number of experiences in the current benchmark.\n    :param per_exp_classes: Is not None, a dictionary whose keys are\n        (0-indexed) experience IDs and their values are the number of classes\n        to include in the respective experiences. The dictionary doesn't\n        have to contain a key for each experience! All the remaining exps\n        will contain an equal amount of the remaining classes. The\n        remaining number of classes must be divisible without remainder\n        by the remaining number of experiences. For instance,\n        if you want to include 50 classes in the first experience\n        while equally distributing remaining classes across remaining\n        experiences, just pass the \"{0: 50}\" dictionary as the\n        per_experience_classes parameter. Defaults to None.\n    :param return_task_id: if True, a progressive task id is returned for every\n        experience. If False, all experiences will have a task ID of 0.\n    :param seed: A valid int used to initialize the random number generator.\n        Can be None.\n    :param fixed_class_order: A list of class IDs used to define the class\n        order. If None, value of ``seed`` will be used to define the class\n        order. If non-None, ``seed`` parameter will be ignored.\n        Defaults to None.\n    :param shuffle: If true, the class order in the incremental experiences is\n        randomly shuffled. Default to false.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations).\n        If no transformation is passed, the default train transformation\n        will be used.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations).\n        If no transformation is passed, the default test transformation\n        will be used.\n\n    :returns: A properly initialized :class:`NCScenario` instance.\n    \"\"\"\n\n    train_set, test_set = _get_imagenet_dataset(dataset_root)\n\n    if return_task_id:\n        return nc_benchmark(\n            train_dataset=train_set,\n            test_dataset=test_set,\n            n_experiences=n_experiences,\n            task_labels=True,\n            per_exp_classes=per_exp_classes,\n            seed=seed,\n            fixed_class_order=fixed_class_order,\n            shuffle=shuffle,\n            class_ids_from_zero_in_each_exp=True,\n            train_transform=train_transform,\n            eval_transform=eval_transform)\n    else:\n        return nc_benchmark(\n            train_dataset=train_set,\n            test_dataset=test_set,\n            n_experiences=n_experiences,\n            task_labels=False,\n            per_exp_classes=per_exp_classes,\n            seed=seed,\n            fixed_class_order=fixed_class_order,\n            shuffle=shuffle,\n            train_transform=train_transform,\n            eval_transform=eval_transform)",
  "def _get_imagenet_dataset(root):\n    train_set = ImageNet(root, split=\"train\")\n\n    test_set = ImageNet(root, split=\"val\")\n\n    return train_set, test_set",
  "def SplitInaturalist(\n        *,\n        super_categories=None,\n        return_task_id=False,\n        download=False,\n        seed=0,\n        train_transform: Optional[Any] = _default_train_transform,\n        eval_transform: Optional[Any] = _default_eval_transform,\n        dataset_root: Union[str, Path] = None):\n    \"\"\"\n    Creates a CL benchmark using the iNaturalist2018 dataset.\n    A selection of supercategories (by default 10) define the experiences.\n    Note that the supercategories are highly imbalanced in the number of classes\n    and the amount of data available.\n\n    If the dataset is not present in the computer, **this method will\n    automatically download** and store it if `download=True`\n    (120Gtrain/val).\n\n    To parse the dataset jsons you need to install an additional dependency:\n    \"pycocotools\". You can install it like this:\n\n        \"conda install -c conda-forge pycocotools\"\n\n    Implementation is based on the CL survey\n    (https://ieeexplore.ieee.org/document/9349197) but differs slightly.\n    The survey uses only the original iNaturalist2018 training dataset split\n    into 70/10/20 for train/val/test streams. This method instead uses the full\n    iNaturalist2018 training set to make the `train_stream`, whereas the\n    `test_stream` is defined by the original iNaturalist2018 validation data.\n\n    The returned benchmark will return experiences containing all patterns of a\n    subset of classes, which means that each class is only seen \"once\".\n    This is one of the most common scenarios in the Continual Learning\n    literature. Common names used in literature to describe this kind of\n    scenario are \"Class Incremental\", \"New Classes\", etc. By default,\n    an equal amount of classes will be assigned to each experience.\n\n    This generator doesn't force a choice on the availability of task labels,\n    a choice that is left to the user (see the `return_task_id` parameter for\n    more info on task labels).\n\n    The benchmark instance returned by this method will have two fields,\n    `train_stream` and `test_stream`, which can be iterated to obtain\n    training and test :class:`Experience`. Each Experience contains the\n    `dataset` and the associated task label.\n\n    The benchmark API is quite simple and is uniform across all benchmark\n    generators. It is recommended to check the tutorial of the \"benchmark\" API,\n    which contains usage examples ranging from \"basic\" to \"advanced\".\n\n    :param super_categories: The list of supercategories which define the\n    tasks, i.e. each task consists of all classes in a super-category.\n    :param download: If true and the dataset is not present in the computer,\n    this method will automatically download and store it. This will take 120G\n    for the train/val set.\n    :param return_task_id: if True, a progressive task id is returned for every\n        experience. If False, all experiences will have a task ID of 0.\n    :param seed: A valid int used to initialize the random number generator.\n        Can be None.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations).\n        If no transformation is passed, the default train transformation\n        will be used.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations).\n        If no transformation is passed, the default test transformation\n        will be used.\n    :param dataset_root: The root path of the dataset.\n        Defaults to None, which means that the default location for\n        'inatuarlist2018' will be used.\n\n    :returns: A properly initialized :class:`NCScenario` instance.\n    \"\"\"\n\n    # Categories with > 100 datapoints\n    if super_categories is None:\n        super_categories = [\n            'Amphibia', 'Animalia', 'Arachnida', 'Aves', 'Fungi',\n            'Insecta', 'Mammalia', 'Mollusca', 'Plantae', 'Reptilia']\n\n    train_set, test_set = _get_inaturalist_dataset(\n        dataset_root, super_categories, download=download)\n    per_exp_classes, fixed_class_order = _get_split(super_categories, train_set)\n\n    if return_task_id:\n        return nc_benchmark(\n            fixed_class_order=fixed_class_order,\n            per_exp_classes=per_exp_classes,\n            train_dataset=train_set,\n            test_dataset=test_set,\n            n_experiences=len(super_categories),\n            task_labels=True,\n            seed=seed,\n            class_ids_from_zero_in_each_exp=True,\n            train_transform=train_transform,\n            eval_transform=eval_transform,\n        )\n    else:\n        return nc_benchmark(\n            fixed_class_order=fixed_class_order,\n            per_exp_classes=per_exp_classes,\n            train_dataset=train_set,\n            test_dataset=test_set,\n            n_experiences=len(super_categories),\n            task_labels=False,\n            seed=seed,\n            train_transform=train_transform,\n            eval_transform=eval_transform,\n        )",
  "def _get_inaturalist_dataset(dataset_root, super_categories, download):\n    if dataset_root is None:\n        dataset_root = default_dataset_location('inatuarlist2018')\n\n    train_set = INATURALIST2018(\n        dataset_root, split=\"train\", supcats=super_categories,\n        download=download)\n    test_set = INATURALIST2018(\n        dataset_root, split=\"val\", supcats=super_categories,\n        download=download)\n\n    return train_set, test_set",
  "def _get_split(super_categories, train_set):\n    \"\"\" Get number of classes per experience, and\n    the total order of the classes.\"\"\"\n    per_exp_classes, fixed_class_order = {}, []\n    for idx, supcat in enumerate(super_categories):\n        new_cats = list(train_set.cats_per_supcat[supcat])\n        fixed_class_order += new_cats\n        per_exp_classes[idx] = len(new_cats)\n    return per_exp_classes, fixed_class_order",
  "class PixelsPermutation(object):\n    \"\"\"\n    Apply a fixed permutation to the pixels of the given image.\n\n    Works with both Tensors and PIL images. Returns an object of the same type\n    of the input element.\n    \"\"\"\n\n    def __init__(self, index_permutation: Sequence[int]):\n        self.permutation = index_permutation\n        self._to_tensor = ToTensor()\n        self._to_image = ToPILImage()\n\n    def __call__(self, img: Union[Image, Tensor]):\n        is_image = isinstance(img, Image)\n        if (not is_image) and (not isinstance(img, Tensor)):\n            raise ValueError('Invalid input: must be a PIL image or a Tensor')\n\n        if is_image:\n            img = self._to_tensor(img)\n\n        img = img.view(-1)[self.permutation].view(*img.shape)\n\n        if is_image:\n            img = self._to_image(img)\n\n        return img",
  "def SplitMNIST(\n        n_experiences: int,\n        *,\n        return_task_id=False,\n        seed: Optional[int] = None,\n        fixed_class_order: Optional[Sequence[int]] = None,\n        shuffle: bool = True,\n        train_transform: Optional[Any] = _default_mnist_train_transform,\n        eval_transform: Optional[Any] = _default_mnist_eval_transform,\n        dataset_root: Union[str, Path] = None):\n    \"\"\"\n    Creates a CL benchmark using the MNIST dataset.\n\n    If the dataset is not present in the computer, this method will\n    automatically download and store it.\n\n    The returned benchmark will return experiences containing all patterns of a\n    subset of classes, which means that each class is only seen \"once\".\n    This is one of the most common scenarios in the Continual Learning\n    literature. Common names used in literature to describe this kind of\n    scenario are \"Class Incremental\", \"New Classes\", etc. By default,\n    an equal amount of classes will be assigned to each experience.\n\n    This generator doesn't force a choice on the availability of task labels,\n    a choice that is left to the user (see the `return_task_id` parameter for\n    more info on task labels).\n\n    The benchmark instance returned by this method will have two fields,\n    `train_stream` and `test_stream`, which can be iterated to obtain\n    training and test :class:`Experience`. Each Experience contains the\n    `dataset` and the associated task label.\n\n    The benchmark API is quite simple and is uniform across all benchmark\n    generators. It is recommended to check the tutorial of the \"benchmark\" API,\n    which contains usage examples ranging from \"basic\" to \"advanced\".\n\n    :param n_experiences: The number of incremental experiences in the current\n        benchmark.\n        The value of this parameter should be a divisor of 10.\n    :param return_task_id: if True, a progressive task id is returned for every\n        experience. If False, all experiences will have a task ID of 0.\n    :param seed: A valid int used to initialize the random number generator.\n        Can be None.\n    :param fixed_class_order: A list of class IDs used to define the class\n        order. If None, value of ``seed`` will be used to define the class\n        order. If non-None, ``seed`` parameter will be ignored.\n        Defaults to None.\n    :param shuffle: If true, the class order in the incremental experiences is\n        randomly shuffled. Default to false.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations).\n        If no transformation is passed, the default train transformation\n        will be used.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations).\n        If no transformation is passed, the default test transformation\n        will be used.\n    :param dataset_root: The root path of the dataset. Defaults to None, which\n        means that the default location for 'mnist' will be used.\n\n    :returns: A properly initialized :class:`NCScenario` instance.\n    \"\"\"\n\n    mnist_train, mnist_test = _get_mnist_dataset(dataset_root)\n\n    if return_task_id:\n        return nc_benchmark(\n            train_dataset=mnist_train,\n            test_dataset=mnist_test,\n            n_experiences=n_experiences,\n            task_labels=True,\n            seed=seed,\n            fixed_class_order=fixed_class_order,\n            shuffle=shuffle,\n            class_ids_from_zero_in_each_exp=True,\n            train_transform=train_transform,\n            eval_transform=eval_transform)\n    else:\n        return nc_benchmark(\n            train_dataset=mnist_train,\n            test_dataset=mnist_test,\n            n_experiences=n_experiences,\n            task_labels=False,\n            seed=seed,\n            fixed_class_order=fixed_class_order,\n            shuffle=shuffle,\n            train_transform=train_transform,\n            eval_transform=eval_transform)",
  "def PermutedMNIST(\n        n_experiences: int,\n        *,\n        seed: Optional[int] = None,\n        train_transform: Optional[Any] = _default_mnist_train_transform,\n        eval_transform: Optional[Any] = _default_mnist_eval_transform,\n        dataset_root: Union[str, Path] = None) -> NCScenario:\n    \"\"\"\n    Creates a Permuted MNIST benchmark.\n\n    If the dataset is not present in the computer, this method will\n    automatically download and store it.\n\n    Random pixel permutations are used to permute the MNIST images in\n    ``n_experiences`` different manners. This means that each experience is\n    composed of all the original 10 MNIST classes, but the pixel in the images\n    are permuted in a different way.\n\n    The benchmark instance returned by this method will have two fields,\n    `train_stream` and `test_stream`, which can be iterated to obtain\n    training and test :class:`Experience`. Each Experience contains the\n    `dataset` and the associated task label.\n\n    A progressive task label, starting from \"0\", is applied to each experience.\n\n    The benchmark API is quite simple and is uniform across all benchmark\n    generators. It is recommended to check the tutorial of the \"benchmark\" API,\n    which contains usage examples ranging from \"basic\" to \"advanced\".\n\n    :param n_experiences: The number of experiences (tasks) in the current\n        benchmark. It indicates how many different permutations of the MNIST\n        dataset have to be created.\n        The value of this parameter should be a divisor of 10.\n    :param seed: A valid int used to initialize the random number generator.\n        Can be None.\n    :param train_transform: The transformation to apply to the training data\n        before the random permutation, e.g. a random crop, a normalization or a\n        concatenation of different transformations (see torchvision.transform\n        documentation for a comprehensive list of possible transformations).\n        If no transformation is passed, the default train transformation\n        will be used.\n    :param eval_transform: The transformation to apply to the test data\n        before the random permutation, e.g. a random crop, a normalization or a\n        concatenation of different transformations (see torchvision.transform\n        documentation for a comprehensive list of possible transformations).\n        If no transformation is passed, the default test transformation\n        will be used.\n    :param dataset_root: The root path of the dataset. Defaults to None, which\n        means that the default location for 'mnist' will be used.\n\n    :returns: A properly initialized :class:`NCScenario` instance.\n    \"\"\"\n\n    list_train_dataset = []\n    list_test_dataset = []\n    rng_permute = np.random.RandomState(seed)\n\n    mnist_train, mnist_test = _get_mnist_dataset(dataset_root)\n\n    # for every incremental experience\n    for _ in range(n_experiences):\n        # choose a random permutation of the pixels in the image\n        idx_permute = torch.from_numpy(rng_permute.permutation(784)).type(\n            torch.int64)\n\n        permutation = PixelsPermutation(idx_permute)\n\n        permutation_transforms = dict(\n            train=(permutation, None),\n            eval=(permutation, None)\n        )\n\n        # Freeze the permutation\n        permuted_train = AvalancheDataset(\n            mnist_train,\n            transform_groups=permutation_transforms,\n            initial_transform_group='train').freeze_transforms()\n\n        permuted_test = AvalancheDataset(\n            mnist_test,\n            transform_groups=permutation_transforms,\n            initial_transform_group='eval').freeze_transforms()\n\n        list_train_dataset.append(permuted_train)\n        list_test_dataset.append(permuted_test)\n\n    return nc_benchmark(\n        list_train_dataset,\n        list_test_dataset,\n        n_experiences=len(list_train_dataset),\n        task_labels=True,\n        shuffle=False,\n        class_ids_from_zero_in_each_exp=True,\n        one_dataset_per_exp=True,\n        train_transform=train_transform,\n        eval_transform=eval_transform)",
  "def RotatedMNIST(\n        n_experiences: int,\n        *,\n        seed: Optional[int] = None,\n        rotations_list: Optional[Sequence[int]] = None,\n        train_transform: Optional[Any] = _default_mnist_train_transform,\n        eval_transform: Optional[Any] = _default_mnist_eval_transform,\n        dataset_root: Union[str, Path] = None) -> NCScenario:\n    \"\"\"\n    Creates a Rotated MNIST benchmark.\n\n    If the dataset is not present in the computer, this method will\n    automatically download and store it.\n\n    Random angles are used to rotate the MNIST images in ``n_experiences``\n    different manners. This means that each experience is composed of all the\n    original 10 MNIST classes, but each image is rotated in a different way.\n\n    The benchmark instance returned by this method will have two fields,\n    `train_stream` and `test_stream`, which can be iterated to obtain\n    training and test :class:`Experience`. Each Experience contains the\n    `dataset` and the associated task label.\n\n    A progressive task label, starting from \"0\", is applied to each experience.\n\n    The benchmark API is quite simple and is uniform across all benchmark\n    generators. It is recommended to check the tutorial of the \"benchmark\" API,\n    which contains usage examples ranging from \"basic\" to \"advanced\".\n\n    :param n_experiences: The number of experiences (tasks) in the current\n        benchmark. It indicates how many different rotations of the MNIST\n        dataset have to be created.\n        The value of this parameter should be a divisor of 10.\n    :param seed: A valid int used to initialize the random number generator.\n        Can be None.\n    :param rotations_list: A list of rotations values in degrees (from -180 to\n        180) used to define the rotations. The rotation specified in position\n        0 of the list will be applied to the task 0, the rotation specified in\n        position 1 will be applied to task 1 and so on.\n        If None, value of ``seed`` will be used to define the rotations.\n        If non-None, ``seed`` parameter will be ignored.\n        Defaults to None.\n    :param train_transform: The transformation to apply to the training data\n        after the random rotation, e.g. a random crop, a normalization or a\n        concatenation of different transformations (see torchvision.transform\n        documentation for a comprehensive list of possible transformations).\n        If no transformation is passed, the default train transformation\n        will be used.\n    :param eval_transform: The transformation to apply to the test data\n        after the random rotation, e.g. a random crop, a normalization or a\n        concatenation of different transformations (see torchvision.transform\n        documentation for a comprehensive list of possible transformations).\n        If no transformation is passed, the default test transformation\n        will be used.\n    :param dataset_root: The root path of the dataset. Defaults to None, which\n        means that the default location for 'mnist' will be used.\n\n    :returns: A properly initialized :class:`NCScenario` instance.\n    \"\"\"\n\n    if rotations_list is not None and len(rotations_list) != n_experiences:\n        raise ValueError(\"The number of rotations should match the number\"\n                         \" of incremental experiences.\")\n\n    if rotations_list is not None and any(180 < rotations_list[i] < -180\n                                          for i in range(len(rotations_list))):\n        raise ValueError(\"The value of a rotation should be between -180\"\n                         \" and 180 degrees.\")\n\n    list_train_dataset = []\n    list_test_dataset = []\n    rng_rotate = np.random.RandomState(seed)\n\n    mnist_train, mnist_test = _get_mnist_dataset(dataset_root)\n\n    # for every incremental experience\n    for exp in range(n_experiences):\n        if rotations_list is not None:\n            rotation_angle = rotations_list[exp]\n        else:\n            # choose a random rotation of the pixels in the image\n            rotation_angle = rng_rotate.randint(-180, 181)\n\n        rotation = RandomRotation(degrees=(rotation_angle, rotation_angle))\n\n        rotation_transforms = dict(\n            train=(rotation, None),\n            eval=(rotation, None)\n        )\n\n        # Freeze the rotation\n        rotated_train = AvalancheDataset(\n            mnist_train,\n            transform_groups=rotation_transforms,\n            initial_transform_group='train').freeze_transforms()\n\n        rotated_test = AvalancheDataset(\n            mnist_test,\n            transform_groups=rotation_transforms,\n            initial_transform_group='eval').freeze_transforms()\n\n        list_train_dataset.append(rotated_train)\n        list_test_dataset.append(rotated_test)\n\n    return nc_benchmark(\n        list_train_dataset,\n        list_test_dataset,\n        n_experiences=len(list_train_dataset),\n        task_labels=True,\n        shuffle=False,\n        class_ids_from_zero_in_each_exp=True,\n        one_dataset_per_exp=True,\n        train_transform=train_transform,\n        eval_transform=eval_transform)",
  "def _get_mnist_dataset(dataset_root):\n    if dataset_root is None:\n        dataset_root = default_dataset_location('mnist')\n\n    train_set = MNIST(root=dataset_root,\n                      train=True, download=True)\n\n    test_set = MNIST(root=dataset_root,\n                     train=False, download=True)\n\n    return train_set, test_set",
  "def __init__(self, index_permutation: Sequence[int]):\n        self.permutation = index_permutation\n        self._to_tensor = ToTensor()\n        self._to_image = ToPILImage()",
  "def __call__(self, img: Union[Image, Tensor]):\n        is_image = isinstance(img, Image)\n        if (not is_image) and (not isinstance(img, Tensor)):\n            raise ValueError('Invalid input: must be a PIL image or a Tensor')\n\n        if is_image:\n            img = self._to_tensor(img)\n\n        img = img.view(-1)[self.permutation].view(*img.shape)\n\n        if is_image:\n            img = self._to_image(img)\n\n        return img",
  "def SplitFMNIST(\n        n_experiences: int,\n        *,\n        first_batch_with_half_classes: bool = False,\n        return_task_id=False,\n        seed: Optional[int] = None,\n        fixed_class_order: Optional[Sequence[int]] = None,\n        shuffle: bool = True,\n        train_transform: Optional[Any] = _default_fmnist_train_transform,\n        eval_transform: Optional[Any] = _default_fmnist_eval_transform,\n        dataset_root: Union[str, Path] = None):\n    \"\"\"\n    Creates a CL benchmark using the Fashion MNIST dataset.\n\n    If the dataset is not present in the computer, this method will\n    automatically download and store it.\n\n    The returned benchmark will return experiences containing all patterns of a\n    subset of classes, which means that each class is only seen \"once\".\n    This is one of the most common scenarios in the Continual Learning\n    literature. Common names used in literature to describe this kind of\n    scenario are \"Class Incremental\", \"New Classes\", etc. By default,\n    an equal amount of classes will be assigned to each experience.\n\n    This generator doesn't force a choice on the availability of task labels,\n    a choice that is left to the user (see the `return_task_id` parameter for\n    more info on task labels).\n\n    The benchmark instance returned by this method will have two fields,\n    `train_stream` and `test_stream`, which can be iterated to obtain\n    training and test :class:`Experience`. Each Experience contains the\n    `dataset` and the associated task label.\n\n    The benchmark API is quite simple and is uniform across all benchmark\n    generators. It is recommended to check the tutorial of the \"benchmark\" API,\n    which contains usage examples ranging from \"basic\" to \"advanced\".\n\n    :param n_experiences: The number of experiences in the current\n        benchmark. If the first experience is a \"pretraining\" step and it\n        contains half of the classes. The value of this parameter should be a\n        divisor of 10 if first_task_with_half_classes if false, a divisor of 5\n        otherwise.\n    :param first_batch_with_half_classes: A boolean value that indicates if a\n        first pretraining batch containing half of the classes should be used.\n        If it's True, a pretraining batch with half of the classes (5 for\n        cifar100) is used. If this parameter is False no pretraining task\n        will be used, and the dataset is simply split into\n        a the number of experiences defined by the parameter n_experiences.\n        Default to False.\n    :param return_task_id: if True, a progressive task id is returned for every\n        experience. If False, all experiences will have a task ID of 0.\n    :param seed: A valid int used to initialize the random number generator.\n        Can be None.\n    :param fixed_class_order: A list of class IDs used to define the class\n        order. If None, value of ``seed`` will be used to define the class\n        order. If non-None, ``seed`` parameter will be ignored.\n        Defaults to None.\n    :param shuffle: If true, the class order in the incremental experiences is\n        randomly shuffled. Default to false.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations).\n        If no transformation is passed, the default train transformation\n        will be used.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations).\n        If no transformation is passed, the default test transformation\n        will be used.\n    :param dataset_root: The root path of the dataset. Defaults to None, which\n        means that the default location for 'fashionmnist' will be used.\n\n    :returns: A properly initialized :class:`NCScenario` instance.\n    \"\"\"\n\n    fmnist_train, fmnist_test = _get_fmnist_dataset(dataset_root)\n\n    if return_task_id:\n        return nc_benchmark(\n            train_dataset=fmnist_train,\n            test_dataset=fmnist_test,\n            n_experiences=n_experiences,\n            task_labels=True,\n            seed=seed,\n            fixed_class_order=fixed_class_order,\n            shuffle=shuffle,\n            per_exp_classes={0: 5} if first_batch_with_half_classes else None,\n            train_transform=train_transform,\n            eval_transform=eval_transform)\n    else:\n        return nc_benchmark(\n            train_dataset=fmnist_train,\n            test_dataset=fmnist_test,\n            n_experiences=n_experiences,\n            task_labels=False,\n            seed=seed,\n            fixed_class_order=fixed_class_order,\n            shuffle=shuffle,\n            per_exp_classes={0: 5} if first_batch_with_half_classes else None,\n            train_transform=train_transform,\n            eval_transform=eval_transform)",
  "def _get_fmnist_dataset(dataset_root):\n    if dataset_root is None:\n        dataset_root = default_dataset_location('fashionmnist')\n\n    train_set = FashionMNIST(dataset_root,\n                             train=True, download=True)\n    test_set = FashionMNIST(dataset_root,\n                            train=False, download=True)\n    return train_set, test_set",
  "def SplitCIFAR100(\n        n_experiences: int,\n        *,\n        first_exp_with_half_classes: bool = False,\n        return_task_id=False,\n        seed: Optional[int] = None,\n        fixed_class_order: Optional[Sequence[int]] = None,\n        shuffle: bool = True,\n        train_transform: Optional[Any] = _default_cifar100_train_transform,\n        eval_transform: Optional[Any] = _default_cifar100_eval_transform,\n        dataset_root: Union[str, Path] = None):\n    \"\"\"\n    Creates a CL benchmark using the CIFAR100 dataset.\n\n    If the dataset is not present in the computer, this method will\n    automatically download and store it.\n\n    The returned benchmark will return experiences containing all patterns of a\n    subset of classes, which means that each class is only seen \"once\".\n    This is one of the most common scenarios in the Continual Learning\n    literature. Common names used in literature to describe this kind of\n    scenario are \"Class Incremental\", \"New Classes\", etc. By default,\n    an equal amount of classes will be assigned to each experience.\n\n    This generator doesn't force a choice on the availability of task labels,\n    a choice that is left to the user (see the `return_task_id` parameter for\n    more info on task labels).\n\n    The benchmark instance returned by this method will have two fields,\n    `train_stream` and `test_stream`, which can be iterated to obtain\n    training and test :class:`Experience`. Each Experience contains the\n    `dataset` and the associated task label.\n\n    The benchmark API is quite simple and is uniform across all benchmark\n    generators. It is recommended to check the tutorial of the \"benchmark\" API,\n    which contains usage examples ranging from \"basic\" to \"advanced\".\n\n    :param n_experiences: The number of incremental experiences in the current\n        benchmark. The value of this parameter should be a divisor of 100 if\n        first_task_with_half_classes is False, a divisor of 50 otherwise.\n    :param first_exp_with_half_classes: A boolean value that indicates if a\n        first pretraining batch containing half of the classes should be used.\n        If it's True, a pretraining experience with half of the classes (50 for\n        cifar100) is used. If this parameter is False no pretraining task\n        will be used, and the dataset is simply split into a the number of\n        experiences defined by the parameter n_experiences. Default to False.\n    :param return_task_id: if True, a progressive task id is returned for every\n        experience. If False, all experiences will have a task ID of 0.\n    :param seed: A valid int used to initialize the random number generator.\n        Can be None.\n    :param fixed_class_order: A list of class IDs used to define the class\n        order. If None, value of ``seed`` will be used to define the class\n        order. If non-None, ``seed`` parameter will be ignored.\n        Defaults to None.\n    :param shuffle: If true, the class order in the incremental experiences is\n        randomly shuffled. Default to false.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations).\n        If no transformation is passed, the default train transformation\n        will be used.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations).\n        If no transformation is passed, the default test transformation\n        will be used.\n    :param dataset_root: The root path of the dataset. Defaults to None, which\n        means that the default location for 'cifar100' will be used.\n\n    :returns: A properly initialized :class:`NCScenario` instance.\n    \"\"\"\n    cifar_train, cifar_test = _get_cifar100_dataset(dataset_root)\n\n    if return_task_id:\n        return nc_benchmark(\n            train_dataset=cifar_train,\n            test_dataset=cifar_test,\n            n_experiences=n_experiences,\n            task_labels=True,\n            seed=seed,\n            fixed_class_order=fixed_class_order,\n            shuffle=shuffle,\n            per_exp_classes={0: 50} if first_exp_with_half_classes else None,\n            class_ids_from_zero_in_each_exp=True,\n            train_transform=train_transform,\n            eval_transform=eval_transform)\n    else:\n        return nc_benchmark(\n            train_dataset=cifar_train,\n            test_dataset=cifar_test,\n            n_experiences=n_experiences,\n            task_labels=False,\n            seed=seed,\n            fixed_class_order=fixed_class_order,\n            shuffle=shuffle,\n            per_exp_classes={0: 50} if first_exp_with_half_classes else None,\n            train_transform=train_transform,\n            eval_transform=eval_transform)",
  "def SplitCIFAR110(\n        n_experiences: int,\n        *,\n        seed: Optional[int] = None,\n        fixed_class_order: Optional[Sequence[int]] = None,\n        train_transform: Optional[Any] = _default_cifar100_train_transform,\n        eval_transform: Optional[Any] = _default_cifar100_eval_transform,\n        dataset_root_cifar10: Union[str, Path] = None,\n        dataset_root_cifar100: Union[str, Path] = None) -> NCScenario:\n    \"\"\"\n    Creates a CL benchmark using both the CIFAR100 and CIFAR10 datasets.\n\n    If the datasets are not present in the computer, this method will\n    automatically download and store them in the data folder.\n\n    The CIFAR10 dataset is used to create the first experience, while the\n    remaining `n_experiences-1` experiences will be created from CIFAR100.\n\n    The returned benchmark will return experiences containing all patterns of a\n    subset of classes, which means that each class is only seen \"once\".\n    This is one of the most common scenarios in the Continual Learning\n    literature. Common names used in literature to describe this kind of\n    scenario are \"Class Incremental\", \"New Classes\", etc. By default,\n    an equal amount of classes will be assigned to each experience.\n\n    This generator will apply a task label \"0\" to all experiences.\n\n    The benchmark instance returned by this method will have two fields,\n    `train_stream` and `test_stream`, which can be iterated to obtain\n    training and test :class:`Experience`. Each Experience contains the\n    `dataset` and the associated task label (always \"0\" for this specific\n    benchmark).\n\n    The benchmark API is quite simple and is uniform across all benchmark\n    generators. It is recommended to check the tutorial of the \"benchmark\" API,\n    which contains usage examples ranging from \"basic\" to \"advanced\".\n\n    :param n_experiences: The number of experiences for the entire benchmark.\n        The first experience will contain the entire CIFAR10 dataset, while the\n        other n-1 experiences will be obtained from CIFAR100.\n    :param seed: A valid int used to initialize the random number generator.\n        Can be None.\n    :param fixed_class_order: A list of class IDs used to define the class\n        order ONLY for the incremental part, which is based on cifar100. The\n        classes must be in range 0-99.\n        If None, value of ``seed`` will be used to define the class order for\n        the incremental batches on cifar100. If non-None, ``seed`` parameter\n        will be ignored. Defaults to None.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations).\n        If no transformation is passed, the default train transformation\n        will be used.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations).\n        If no transformation is passed, the default test transformation\n        will be used.\n    :param dataset_root_cifar10: The root path of the CIFAR-10 dataset.\n        Defaults to None, which means that the default location for\n        'cifar10' will be used.\n    :param dataset_root_cifar100: The root path of the CIFAR-100 dataset.\n        Defaults to None, which means that the default location for\n        'cifar100' will be used.\n\n    :returns: A properly initialized :class:`NCScenario` instance.\n    \"\"\"\n\n    cifar10_train, cifar10_test = _get_cifar10_dataset(dataset_root_cifar10)\n    cifar100_train, cifar100_test = _get_cifar100_dataset(dataset_root_cifar100)\n\n    cifar_10_100_train, cifar_10_100_test, _ = concat_datasets_sequentially(\n        [cifar10_train, cifar100_train], [cifar10_test, cifar100_test]\n    )\n    # cifar10 classes\n    class_order = [_ for _ in range(10)]\n    # if a class order is defined (for cifar100) the given class labels are\n    # appended to the class_order list, adding 10 to them (since the classes\n    # 0-9 are the classes of cifar10).\n    if fixed_class_order is not None:\n        class_order.extend([c + 10 for c in fixed_class_order])\n    else:\n        random.seed(seed)\n        # random shuffling of the cifar100 classes (labels 10-109)\n        cifar_100_class_order = random.sample(range(10, 110), 100)\n        class_order.extend(cifar_100_class_order)\n\n    return nc_benchmark(\n        cifar_10_100_train, cifar_10_100_test,\n        n_experiences=n_experiences,\n        task_labels=False,\n        shuffle=False,\n        seed=None,\n        fixed_class_order=class_order,\n        per_exp_classes={0: 10},\n        train_transform=train_transform,\n        eval_transform=eval_transform)",
  "def _get_cifar10_dataset(dataset_root):\n    if dataset_root is None:\n        dataset_root = default_dataset_location('cifar10')\n\n    train_set = CIFAR10(dataset_root, train=True, download=True)\n    test_set = CIFAR10(dataset_root, train=False, download=True)\n\n    return train_set, test_set",
  "def _get_cifar100_dataset(dataset_root):\n    if dataset_root is None:\n        dataset_root = default_dataset_location('cifar100')\n\n    train_set = CIFAR100(dataset_root, train=True, download=True)\n    test_set = CIFAR100(dataset_root, train=False, download=True)\n\n    return train_set, test_set",
  "def SplitTinyImageNet(\n        n_experiences=10,\n        *,\n        return_task_id=False,\n        seed=0,\n        fixed_class_order=None,\n        shuffle: bool = True,\n        train_transform: Optional[Any] = _default_train_transform,\n        eval_transform: Optional[Any] = _default_eval_transform,\n        dataset_root: Union[str, Path] = None):\n    \"\"\"\n    Creates a CL benchmark using the Tiny ImageNet dataset.\n\n    If the dataset is not present in the computer, this method will\n    automatically download and store it.\n\n    The returned benchmark will return experiences containing all patterns of a\n    subset of classes, which means that each class is only seen \"once\".\n    This is one of the most common scenarios in the Continual Learning\n    literature. Common names used in literature to describe this kind of\n    scenario are \"Class Incremental\", \"New Classes\", etc. By default,\n    an equal amount of classes will be assigned to each experience.\n\n    This generator doesn't force a choice on the availability of task labels,\n    a choice that is left to the user (see the `return_task_id` parameter for\n    more info on task labels).\n\n    The benchmark instance returned by this method will have two fields,\n    `train_stream` and `test_stream`, which can be iterated to obtain\n    training and test :class:`Experience`. Each Experience contains the\n    `dataset` and the associated task label.\n\n    The benchmark API is quite simple and is uniform across all benchmark\n    generators. It is recommended to check the tutorial of the \"benchmark\" API,\n    which contains usage examples ranging from \"basic\" to \"advanced\".\n\n    :param n_experiences: The number of experiences in the current benchmark.\n    :param return_task_id: if True, a progressive task id is returned for every\n        experience. If False, all experiences will have a task ID of 0.\n    :param seed: A valid int used to initialize the random number generator.\n        Can be None.\n    :param fixed_class_order: A list of class IDs used to define the class\n        order. If None, value of ``seed`` will be used to define the class\n        order. If non-None, ``seed`` parameter will be ignored.\n        Defaults to None.\n    :param shuffle: If true, the class order in the incremental experiences is\n        randomly shuffled. Default to false.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations).\n        If no transformation is passed, the default train transformation\n        will be used.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations).\n        If no transformation is passed, the default test transformation\n        will be used.\n    :param dataset_root: The root path of the dataset.\n        Defaults to None, which means that the default location for\n        'tinyimagenet' will be used.\n\n    :returns: A properly initialized :class:`NCScenario` instance.\n    \"\"\"\n\n    train_set, test_set = _get_tiny_imagenet_dataset(dataset_root)\n\n    if return_task_id:\n        return nc_benchmark(\n            train_dataset=train_set,\n            test_dataset=test_set,\n            n_experiences=n_experiences,\n            task_labels=True,\n            seed=seed,\n            fixed_class_order=fixed_class_order,\n            shuffle=shuffle,\n            class_ids_from_zero_in_each_exp=True,\n            train_transform=train_transform,\n            eval_transform=eval_transform)\n    else:\n        return nc_benchmark(\n            train_dataset=train_set,\n            test_dataset=test_set,\n            n_experiences=n_experiences,\n            task_labels=False,\n            seed=seed,\n            fixed_class_order=fixed_class_order,\n            shuffle=shuffle,\n            train_transform=train_transform,\n            eval_transform=eval_transform)",
  "def _get_tiny_imagenet_dataset(dataset_root):\n    train_set = TinyImagenet(root=dataset_root, train=True)\n\n    test_set = TinyImagenet(root=dataset_root, train=False)\n\n    return train_set, test_set",
  "def SplitCUB200(\n        n_experiences=11,\n        *,\n        classes_first_batch=100,\n        return_task_id=False,\n        seed=0,\n        fixed_class_order=None,\n        shuffle=False,\n        train_transform: Optional[Any] = _default_train_transform,\n        eval_transform: Optional[Any] = _default_eval_transform,\n        dataset_root: Union[str, Path] = None):\n    \"\"\"\n    Creates a CL benchmark using the Cub-200 dataset.\n\n    If the dataset is not present in the computer, **this method will NOT be\n    able automatically download** and store it.\n\n    The returned benchmark will return experiences containing all patterns of a\n    subset of classes, which means that each class is only seen \"once\".\n    This is one of the most common scenarios in the Continual Learning\n    literature. Common names used in literature to describe this kind of\n    scenario are \"Class Incremental\", \"New Classes\", etc. By default,\n    an equal amount of classes will be assigned to each experience.\n\n    This generator doesn't force a choice on the availability of task labels,\n    a choice that is left to the user (see the `return_task_id` parameter for\n    more info on task labels).\n\n    The benchmark instance returned by this method will have two fields,\n    `train_stream` and `test_stream`, which can be iterated to obtain\n    training and test :class:`Experience`. Each Experience contains the\n    `dataset` and the associated task label.\n\n    The benchmark API is quite simple and is uniform across all benchmark\n    generators. It is recommended to check the tutorial of the \"benchmark\" API,\n    which contains usage examples ranging from \"basic\" to \"advanced\".\n\n    :param n_experiences: The number of experiences in the current benchmark.\n        Defaults to 11.\n    :param classes_first_batch: Number of classes in the first batch.\n        Usually this is set to 500. Defaults to 100.\n    :param return_task_id: if True, a progressive task id is returned for every\n        experience. If False, all experiences will have a task ID of 0.\n    :param seed: A valid int used to initialize the random number generator.\n        Can be None.\n    :param fixed_class_order: A list of class IDs used to define the class\n        order. If None, value of ``seed`` will be used to define the class\n        order. If non-None, ``seed`` parameter will be ignored.\n        Defaults to None.\n    :param shuffle: If true, the class order in the incremental experiences is\n        randomly shuffled. Default to false.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations).\n        If no transformation is passed, the default train transformation\n        will be used.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations).\n        If no transformation is passed, the default test transformation\n        will be used.\n    :param dataset_root: The root path of the dataset.\n        Defaults to None, which means that the default location for\n        'CUB_200_2011' will be used.\n\n    :returns: A properly initialized :class:`NCScenario` instance.\n    \"\"\"\n\n    train_set, test_set = _get_cub200_dataset(dataset_root)\n\n    if classes_first_batch is not None:\n        per_exp_classes = {0: classes_first_batch}\n    else:\n        per_exp_classes = None\n\n    if return_task_id:\n        return nc_benchmark(\n            train_dataset=train_set,\n            test_dataset=test_set,\n            n_experiences=n_experiences,\n            task_labels=True,\n            per_exp_classes=per_exp_classes,\n            seed=seed,\n            fixed_class_order=fixed_class_order,\n            shuffle=shuffle,\n            one_dataset_per_exp=True,\n            train_transform=train_transform,\n            eval_transform=eval_transform)\n    else:\n        return nc_benchmark(\n            train_dataset=train_set,\n            test_dataset=test_set,\n            n_experiences=n_experiences,\n            task_labels=False,\n            per_exp_classes=per_exp_classes,\n            seed=seed,\n            fixed_class_order=fixed_class_order,\n            shuffle=shuffle,\n            train_transform=train_transform,\n            eval_transform=eval_transform)",
  "def _get_cub200_dataset(root):\n    train_set = CUB200(root, train=True)\n    test_set = CUB200(root, train=False)\n\n    return train_set, test_set",
  "def OpenLORIS(\n        *,\n        factor: Literal['clutter', 'illumination',\n                        'occlusion', 'pixel', 'mixture-iros'] = \"clutter\",\n        train_transform: Optional[Any] = None,\n        eval_transform: Optional[Any] = None,\n        dataset_root: Union[str, Path] = None):\n    \"\"\"\n    Creates a CL benchmark for OpenLORIS.\n\n    If the dataset is not present in the computer, **this method will NOT be\n    able automatically download** and store it.\n\n    This generator can be used to obtain scenarios based on different \"factors\".\n    Valid factors include 'clutter', 'illumination', 'occlusion', 'pixel', or\n    'mixture-iros'.\n\n    The benchmark instance returned by this method will have two fields,\n    `train_stream` and `test_stream`, which can be iterated to obtain\n    training and test :class:`Experience`. Each Experience contains the\n    `dataset` and the associated task label.\n\n    The task label \"0\" will be assigned to each experience.\n\n    The benchmark API is quite simple and is uniform across all benchmark\n    generators. It is recommended to check the tutorial of the \"benchmark\" API,\n    which contains usage examples ranging from \"basic\" to \"advanced\".\n\n    :param factor: OpenLORIS main factors, indicating different environmental\n        variations. It can be chosen between 'clutter', 'illumination',\n        'occlusion', 'pixel', or 'mixture-iros'. The first three factors are\n        included in the ICRA 2020 paper and the last factor (mixture-iros) is\n        the benchmark setting for IROS 2019 Lifelong robotic vision competition.\n    :param train_transform: The transformation to apply to the training data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param eval_transform: The transformation to apply to the test data,\n        e.g. a random crop, a normalization or a concatenation of different\n        transformations (see torchvision.transform documentation for a\n        comprehensive list of possible transformations). Defaults to None.\n    :param dataset_root: The root path of the dataset.\n        Defaults to None, which means that the default location for\n        'openloris' will be used.\n\n    :returns: a properly initialized :class:`GenericCLScenario` instance.\n    \"\"\"\n\n    assert (factor in nbatch.keys()), \"The selected factor is note \" \\\n                                      \"recognized: it should be 'clutter',\" \\\n                                      \"'illumination', 'occlusion', \" \\\n                                      \"'pixel', or 'mixture-iros'.\"\n\n    # Dataset created just to download it\n    dataset = OpenLORISDataset(dataset_root, download=True)\n\n    filelists_bp = fac2dirs[factor] + \"/\"\n    train_failists_paths = []\n    for i in range(nbatch[factor]):\n        train_failists_paths.append(\n            dataset_root / filelists_bp / (\"train_batch_\" +\n                                           str(i).zfill(2) + \".txt\"))\n\n    factor_obj = create_generic_benchmark_from_filelists(\n        dataset_root, train_failists_paths,\n        [dataset_root / filelists_bp / \"test.txt\"],\n        task_labels=[0 for _ in range(nbatch[factor])],\n        complete_test_set_only=True,\n        train_transform=train_transform,\n        eval_transform=eval_transform)\n\n    return factor_obj",
  "def _default_collate_mbatches_fn(mbatches):\n    \"\"\" Combines multiple mini-batches together.\n\n    Concatenates each tensor in the mini-batches along dimension 0 (usually this\n    is the batch size).\n\n    :param mbatches: sequence of mini-batches.\n    :return: a single mini-batch\n    \"\"\"\n    batch = []\n    for i in range(len(mbatches[0])):\n        t = torch.cat([el[i] for el in mbatches], dim=0)\n        batch.append(t)\n    return batch",
  "class TaskBalancedDataLoader:\n    def __init__(self, data: AvalancheDataset,\n                 oversample_small_tasks: bool = False,\n                 collate_mbatches=_default_collate_mbatches_fn,\n                 **kwargs):\n        \"\"\" Task-balanced data loader for Avalanche's datasets.\n\n        The iterator returns a mini-batch balanced across each task, which\n        makes it useful when training in multi-task scenarios whenever data is\n        highly unbalanced.\n\n        If `oversample_small_tasks == True` smaller tasks are\n        oversampled to match the largest task. Otherwise, once the data for a\n        specific task is terminated, that task will not be present in the\n        subsequent mini-batches.\n\n        :param data: an instance of `AvalancheDataset`.\n        :param oversample_small_tasks: whether smaller tasks should be\n            oversampled to match the largest one.\n        :param collate_mbatches: function that given a sequence of mini-batches\n            (one for each task) combines them into a single mini-batch. Used to\n            combine the mini-batches obtained separately from each task.\n        :param kwargs: data loader arguments used to instantiate the loader for\n            each task separately. See pytorch :class:`DataLoader`.\n        \"\"\"\n        self.data = data\n        self.dataloaders: Dict[int, DataLoader] = {}\n        self.oversample_small_tasks = oversample_small_tasks\n        self.collate_mbatches = collate_mbatches\n\n        # split data by task.\n        task_datasets = []\n        for task_label in self.data.task_set:\n            tdata = self.data.task_set[task_label]\n            task_datasets.append(tdata)\n\n        # the iteration logic is implemented by GroupBalancedDataLoader.\n        # we use kwargs to pass the arguments to avoid passing the same\n        # arguments multiple times.\n        if 'data' in kwargs:\n            del kwargs['data']\n        # needed if they are passed as positional arguments\n        kwargs['oversample_small_groups'] = oversample_small_tasks\n        kwargs['collate_mbatches'] = collate_mbatches\n        self._dl = GroupBalancedDataLoader(datasets=task_datasets, **kwargs)\n\n    def __iter__(self):\n        for el in self._dl.__iter__():\n            yield el\n\n    def __len__(self):\n        return self._dl.__len__()",
  "class GroupBalancedDataLoader:\n    def __init__(self, datasets: Sequence[AvalancheDataset],\n                 oversample_small_groups: bool = False,\n                 collate_mbatches=_default_collate_mbatches_fn,\n                 **kwargs):\n        \"\"\" Data loader that balances data from multiple datasets.\n\n        Mini-batches emitted by this dataloader are created by collating\n        together mini-batches from each group. It may be used to balance data\n        among classes, experiences, tasks, and so on.\n\n        If `oversample_small_groups == True` smaller groups are oversampled to\n        match the largest group. Otherwise, once data from a group is\n        completely iterated, the group will be skipped.\n\n        :param datasets: an instance of `AvalancheDataset`.\n        :param oversample_small_groups: whether smaller groups should be\n            oversampled to match the largest one.\n        :param collate_mbatches: function that given a sequence of mini-batches\n            (one for each task) combines them into a single mini-batch. Used to\n            combine the mini-batches obtained separately from each task.\n        :param kwargs: data loader arguments used to instantiate the loader for\n            each group separately. See pytorch :class:`DataLoader`.\n        \"\"\"\n        self.datasets = datasets\n        self.dataloaders = []\n        self.oversample_small_groups = oversample_small_groups\n        self.collate_mbatches = collate_mbatches\n\n        for data in self.datasets:\n            self.dataloaders.append(DataLoader(data, **kwargs))\n        self.max_len = max([len(d) for d in self.dataloaders])\n\n    def __iter__(self):\n        iter_dataloaders = []\n        for dl in self.dataloaders:\n            iter_dataloaders.append(iter(dl))\n\n        max_num_mbatches = max([len(d) for d in iter_dataloaders])\n        for it in range(max_num_mbatches):\n            mb_curr = []\n            is_removed_dataloader = False\n            # copy() is necessary because we may remove keys from the\n            # dictionary. This would break the generator.\n            for tid, t_loader in enumerate(iter_dataloaders):\n                try:\n                    batch = next(t_loader)\n                except StopIteration:\n                    # StopIteration is thrown if dataset ends.\n                    if self.oversample_small_groups:\n                        # reinitialize data loader\n                        iter_dataloaders[tid] = iter(self.dataloaders[tid])\n                        batch = next(iter_dataloaders[tid])\n                    else:\n                        # We iteratated over all the data from this group\n                        # and we don't need the iterator anymore.\n                        iter_dataloaders[tid] = None\n                        is_removed_dataloader = True\n                        continue\n                mb_curr.append(batch)\n            yield self.collate_mbatches(mb_curr)\n\n            # clear empty data-loaders\n            if is_removed_dataloader:\n                while None in iter_dataloaders:\n                    iter_dataloaders.remove(None)\n\n    def __len__(self):\n        return self.max_len",
  "class GroupBalancedInfiniteDataLoader:\n    def __init__(self, datasets: Sequence[AvalancheDataset],\n                 collate_mbatches=_default_collate_mbatches_fn,\n                 **kwargs):\n        \"\"\" Data loader that balances data from multiple datasets emitting an\n        infinite stream.\n\n        Mini-batches emitted by this dataloader are created by collating\n        together mini-batches from each group. It may be used to balance data\n        among classes, experiences, tasks, and so on.\n\n        :param datasets: an instance of `AvalancheDataset`.\n        :param collate_mbatches: function that given a sequence of mini-batches\n            (one for each task) combines them into a single mini-batch. Used to\n            combine the mini-batches obtained separately from each task.\n        :param kwargs: data loader arguments used to instantiate the loader for\n            each group separately. See pytorch :class:`DataLoader`.\n        \"\"\"\n        self.datasets = datasets\n        self.dataloaders = []\n        self.collate_mbatches = collate_mbatches\n\n        for data in self.datasets:\n            infinite_sampler = RandomSampler(data, replacement=True,\n                                             num_samples=10 ** 10)\n            dl = DataLoader(\n                data,\n                sampler=infinite_sampler,\n                **kwargs)\n            self.dataloaders.append(dl)\n        self.max_len = max([len(d) for d in self.dataloaders])\n\n    def __iter__(self):\n        iter_dataloaders = []\n        for dl in self.dataloaders:\n            iter_dataloaders.append(iter(dl))\n\n        while True:\n            mb_curr = []\n            for tid, t_loader in enumerate(iter_dataloaders):\n                batch = next(t_loader)\n                mb_curr.append(batch)\n            yield self.collate_mbatches(mb_curr)\n\n    def __len__(self):\n        return self.max_len",
  "class ReplayDataLoader:\n    def __init__(self, data: AvalancheDataset, memory: AvalancheDataset = None,\n                 oversample_small_tasks: bool = False,\n                 collate_mbatches=_default_collate_mbatches_fn,\n                 batch_size: int = 32,\n                 force_data_batch_size: int = None,\n                 **kwargs):\n        \"\"\" Custom data loader for rehearsal strategies.\n\n        The iterates in parallel two datasets, the current `data` and the\n        rehearsal `memory`, which are used to create mini-batches by\n        concatenating their data together. Mini-batches from both of them are\n        balanced using the task label (i.e. each mini-batch contains a balanced\n        number of examples from all the tasks in the `data` and `memory`).\n        \n        If `oversample_small_tasks == True` smaller tasks are oversampled to\n        match the largest task.\n\n        :param data: AvalancheDataset.\n        :param memory: AvalancheDataset.\n        :param oversample_small_tasks: whether smaller tasks should be\n            oversampled to match the largest one.\n        :param collate_mbatches: function that given a sequence of mini-batches\n            (one for each task) combines them into a single mini-batch. Used to\n            combine the mini-batches obtained separately from each task.\n        :param batch_size: the size of the batch. It must be greater than or\n            equal to the number of tasks.\n        :param ratio_data_mem: How many of the samples should be from\n        :param kwargs: data loader arguments used to instantiate the loader for\n            each task separately. See pytorch :class:`DataLoader`.\n        \"\"\"\n\n        self.data = data\n        self.memory = memory\n        self.loader_data: Sequence[DataLoader] = {}\n        self.loader_memory: Sequence[DataLoader] = {}\n        self.oversample_small_tasks = oversample_small_tasks\n        self.collate_mbatches = collate_mbatches\n\n        if force_data_batch_size is not None:\n            assert force_data_batch_size <= batch_size, \\\n                \"Forced batch size of data must be <= entire batch size\"\n\n            mem_batch_size = batch_size - force_data_batch_size\n            remaining_example = 0\n            mem_keys = len(self.memory.task_set)\n            assert mem_batch_size >= mem_keys, \\\n                \"Batch size must be greator or equal \" \\\n                \"to the number of tasks in the memory.\"\n\n            self.loader_data, _ = self._create_dataloaders(\n                data, force_data_batch_size,\n                remaining_example, **kwargs)\n            self.loader_memory, _ = self._create_dataloaders(\n                memory, mem_batch_size,\n                remaining_example, **kwargs)\n        else:\n            num_keys = len(self.data.task_set) + len(self.memory.task_set)\n            assert batch_size >= num_keys, \\\n                \"Batch size must be greator or equal \" \\\n                \"to the number of tasks in the memory \" \\\n                \"and current data.\"\n\n            single_group_batch_size = batch_size // num_keys\n            remaining_example = batch_size % num_keys\n\n            self.loader_data, remaining_example = self._create_dataloaders(\n                data, single_group_batch_size,\n                remaining_example, **kwargs)\n            self.loader_memory, remaining_example = self._create_dataloaders(\n                memory, single_group_batch_size,\n                remaining_example, **kwargs)\n\n        self.max_len = max([len(d) for d in chain(\n            self.loader_data.values(), self.loader_memory.values())]\n                           )\n\n    def __iter__(self):\n        iter_data_dataloaders = {}\n        iter_buffer_dataloaders = {}\n\n        for t in self.loader_data.keys():\n            iter_data_dataloaders[t] = iter(self.loader_data[t])\n        for t in self.loader_memory.keys():\n            iter_buffer_dataloaders[t] = iter(self.loader_memory[t])\n\n        max_len = max([len(d) for d in chain(iter_data_dataloaders.values(),\n                                             iter_buffer_dataloaders.values())])\n        try:\n            for it in range(max_len):\n                mb_curr = []\n                self._get_mini_batch_from_data_dict(\n                    self.data, iter_data_dataloaders,\n                    self.loader_data, self.oversample_small_tasks,\n                    mb_curr)\n\n                self._get_mini_batch_from_data_dict(\n                    self.memory, iter_buffer_dataloaders,\n                    self.loader_memory, self.oversample_small_tasks,\n                    mb_curr)\n\n                yield self.collate_mbatches(mb_curr)\n        except StopIteration:\n            return\n\n    def __len__(self):\n        return self.max_len\n\n    def _get_mini_batch_from_data_dict(self, data, iter_dataloaders,\n                                       loaders_dict, oversample_small_tasks,\n                                       mb_curr):\n        # list() is necessary because we may remove keys from the\n        # dictionary. This would break the generator.\n        for t in list(iter_dataloaders.keys()):\n            t_loader = iter_dataloaders[t]\n            try:\n                tbatch = next(t_loader)\n            except StopIteration:\n                # StopIteration is thrown if dataset ends.\n                # reinitialize data loader\n                if oversample_small_tasks:\n                    # reinitialize data loader\n                    iter_dataloaders[t] = iter(loaders_dict[t])\n                    tbatch = next(iter_dataloaders[t])\n                else:\n                    del iter_dataloaders[t]\n                    continue\n            mb_curr.append(tbatch)\n\n    def _create_dataloaders(self, data_dict, single_exp_batch_size,\n                            remaining_example, **kwargs):\n        loaders_dict: Dict[int, DataLoader] = {}\n        for task_id in data_dict.task_set:\n            data = data_dict.task_set[task_id]\n            current_batch_size = single_exp_batch_size\n            if remaining_example > 0:\n                current_batch_size += 1\n                remaining_example -= 1\n            loaders_dict[task_id] = DataLoader(\n                data, batch_size=current_batch_size, **kwargs)\n        return loaders_dict, remaining_example",
  "def __init__(self, data: AvalancheDataset,\n                 oversample_small_tasks: bool = False,\n                 collate_mbatches=_default_collate_mbatches_fn,\n                 **kwargs):\n        \"\"\" Task-balanced data loader for Avalanche's datasets.\n\n        The iterator returns a mini-batch balanced across each task, which\n        makes it useful when training in multi-task scenarios whenever data is\n        highly unbalanced.\n\n        If `oversample_small_tasks == True` smaller tasks are\n        oversampled to match the largest task. Otherwise, once the data for a\n        specific task is terminated, that task will not be present in the\n        subsequent mini-batches.\n\n        :param data: an instance of `AvalancheDataset`.\n        :param oversample_small_tasks: whether smaller tasks should be\n            oversampled to match the largest one.\n        :param collate_mbatches: function that given a sequence of mini-batches\n            (one for each task) combines them into a single mini-batch. Used to\n            combine the mini-batches obtained separately from each task.\n        :param kwargs: data loader arguments used to instantiate the loader for\n            each task separately. See pytorch :class:`DataLoader`.\n        \"\"\"\n        self.data = data\n        self.dataloaders: Dict[int, DataLoader] = {}\n        self.oversample_small_tasks = oversample_small_tasks\n        self.collate_mbatches = collate_mbatches\n\n        # split data by task.\n        task_datasets = []\n        for task_label in self.data.task_set:\n            tdata = self.data.task_set[task_label]\n            task_datasets.append(tdata)\n\n        # the iteration logic is implemented by GroupBalancedDataLoader.\n        # we use kwargs to pass the arguments to avoid passing the same\n        # arguments multiple times.\n        if 'data' in kwargs:\n            del kwargs['data']\n        # needed if they are passed as positional arguments\n        kwargs['oversample_small_groups'] = oversample_small_tasks\n        kwargs['collate_mbatches'] = collate_mbatches\n        self._dl = GroupBalancedDataLoader(datasets=task_datasets, **kwargs)",
  "def __iter__(self):\n        for el in self._dl.__iter__():\n            yield el",
  "def __len__(self):\n        return self._dl.__len__()",
  "def __init__(self, datasets: Sequence[AvalancheDataset],\n                 oversample_small_groups: bool = False,\n                 collate_mbatches=_default_collate_mbatches_fn,\n                 **kwargs):\n        \"\"\" Data loader that balances data from multiple datasets.\n\n        Mini-batches emitted by this dataloader are created by collating\n        together mini-batches from each group. It may be used to balance data\n        among classes, experiences, tasks, and so on.\n\n        If `oversample_small_groups == True` smaller groups are oversampled to\n        match the largest group. Otherwise, once data from a group is\n        completely iterated, the group will be skipped.\n\n        :param datasets: an instance of `AvalancheDataset`.\n        :param oversample_small_groups: whether smaller groups should be\n            oversampled to match the largest one.\n        :param collate_mbatches: function that given a sequence of mini-batches\n            (one for each task) combines them into a single mini-batch. Used to\n            combine the mini-batches obtained separately from each task.\n        :param kwargs: data loader arguments used to instantiate the loader for\n            each group separately. See pytorch :class:`DataLoader`.\n        \"\"\"\n        self.datasets = datasets\n        self.dataloaders = []\n        self.oversample_small_groups = oversample_small_groups\n        self.collate_mbatches = collate_mbatches\n\n        for data in self.datasets:\n            self.dataloaders.append(DataLoader(data, **kwargs))\n        self.max_len = max([len(d) for d in self.dataloaders])",
  "def __iter__(self):\n        iter_dataloaders = []\n        for dl in self.dataloaders:\n            iter_dataloaders.append(iter(dl))\n\n        max_num_mbatches = max([len(d) for d in iter_dataloaders])\n        for it in range(max_num_mbatches):\n            mb_curr = []\n            is_removed_dataloader = False\n            # copy() is necessary because we may remove keys from the\n            # dictionary. This would break the generator.\n            for tid, t_loader in enumerate(iter_dataloaders):\n                try:\n                    batch = next(t_loader)\n                except StopIteration:\n                    # StopIteration is thrown if dataset ends.\n                    if self.oversample_small_groups:\n                        # reinitialize data loader\n                        iter_dataloaders[tid] = iter(self.dataloaders[tid])\n                        batch = next(iter_dataloaders[tid])\n                    else:\n                        # We iteratated over all the data from this group\n                        # and we don't need the iterator anymore.\n                        iter_dataloaders[tid] = None\n                        is_removed_dataloader = True\n                        continue\n                mb_curr.append(batch)\n            yield self.collate_mbatches(mb_curr)\n\n            # clear empty data-loaders\n            if is_removed_dataloader:\n                while None in iter_dataloaders:\n                    iter_dataloaders.remove(None)",
  "def __len__(self):\n        return self.max_len",
  "def __init__(self, datasets: Sequence[AvalancheDataset],\n                 collate_mbatches=_default_collate_mbatches_fn,\n                 **kwargs):\n        \"\"\" Data loader that balances data from multiple datasets emitting an\n        infinite stream.\n\n        Mini-batches emitted by this dataloader are created by collating\n        together mini-batches from each group. It may be used to balance data\n        among classes, experiences, tasks, and so on.\n\n        :param datasets: an instance of `AvalancheDataset`.\n        :param collate_mbatches: function that given a sequence of mini-batches\n            (one for each task) combines them into a single mini-batch. Used to\n            combine the mini-batches obtained separately from each task.\n        :param kwargs: data loader arguments used to instantiate the loader for\n            each group separately. See pytorch :class:`DataLoader`.\n        \"\"\"\n        self.datasets = datasets\n        self.dataloaders = []\n        self.collate_mbatches = collate_mbatches\n\n        for data in self.datasets:\n            infinite_sampler = RandomSampler(data, replacement=True,\n                                             num_samples=10 ** 10)\n            dl = DataLoader(\n                data,\n                sampler=infinite_sampler,\n                **kwargs)\n            self.dataloaders.append(dl)\n        self.max_len = max([len(d) for d in self.dataloaders])",
  "def __iter__(self):\n        iter_dataloaders = []\n        for dl in self.dataloaders:\n            iter_dataloaders.append(iter(dl))\n\n        while True:\n            mb_curr = []\n            for tid, t_loader in enumerate(iter_dataloaders):\n                batch = next(t_loader)\n                mb_curr.append(batch)\n            yield self.collate_mbatches(mb_curr)",
  "def __len__(self):\n        return self.max_len",
  "def __init__(self, data: AvalancheDataset, memory: AvalancheDataset = None,\n                 oversample_small_tasks: bool = False,\n                 collate_mbatches=_default_collate_mbatches_fn,\n                 batch_size: int = 32,\n                 force_data_batch_size: int = None,\n                 **kwargs):\n        \"\"\" Custom data loader for rehearsal strategies.\n\n        The iterates in parallel two datasets, the current `data` and the\n        rehearsal `memory`, which are used to create mini-batches by\n        concatenating their data together. Mini-batches from both of them are\n        balanced using the task label (i.e. each mini-batch contains a balanced\n        number of examples from all the tasks in the `data` and `memory`).\n        \n        If `oversample_small_tasks == True` smaller tasks are oversampled to\n        match the largest task.\n\n        :param data: AvalancheDataset.\n        :param memory: AvalancheDataset.\n        :param oversample_small_tasks: whether smaller tasks should be\n            oversampled to match the largest one.\n        :param collate_mbatches: function that given a sequence of mini-batches\n            (one for each task) combines them into a single mini-batch. Used to\n            combine the mini-batches obtained separately from each task.\n        :param batch_size: the size of the batch. It must be greater than or\n            equal to the number of tasks.\n        :param ratio_data_mem: How many of the samples should be from\n        :param kwargs: data loader arguments used to instantiate the loader for\n            each task separately. See pytorch :class:`DataLoader`.\n        \"\"\"\n\n        self.data = data\n        self.memory = memory\n        self.loader_data: Sequence[DataLoader] = {}\n        self.loader_memory: Sequence[DataLoader] = {}\n        self.oversample_small_tasks = oversample_small_tasks\n        self.collate_mbatches = collate_mbatches\n\n        if force_data_batch_size is not None:\n            assert force_data_batch_size <= batch_size, \\\n                \"Forced batch size of data must be <= entire batch size\"\n\n            mem_batch_size = batch_size - force_data_batch_size\n            remaining_example = 0\n            mem_keys = len(self.memory.task_set)\n            assert mem_batch_size >= mem_keys, \\\n                \"Batch size must be greator or equal \" \\\n                \"to the number of tasks in the memory.\"\n\n            self.loader_data, _ = self._create_dataloaders(\n                data, force_data_batch_size,\n                remaining_example, **kwargs)\n            self.loader_memory, _ = self._create_dataloaders(\n                memory, mem_batch_size,\n                remaining_example, **kwargs)\n        else:\n            num_keys = len(self.data.task_set) + len(self.memory.task_set)\n            assert batch_size >= num_keys, \\\n                \"Batch size must be greator or equal \" \\\n                \"to the number of tasks in the memory \" \\\n                \"and current data.\"\n\n            single_group_batch_size = batch_size // num_keys\n            remaining_example = batch_size % num_keys\n\n            self.loader_data, remaining_example = self._create_dataloaders(\n                data, single_group_batch_size,\n                remaining_example, **kwargs)\n            self.loader_memory, remaining_example = self._create_dataloaders(\n                memory, single_group_batch_size,\n                remaining_example, **kwargs)\n\n        self.max_len = max([len(d) for d in chain(\n            self.loader_data.values(), self.loader_memory.values())]\n                           )",
  "def __iter__(self):\n        iter_data_dataloaders = {}\n        iter_buffer_dataloaders = {}\n\n        for t in self.loader_data.keys():\n            iter_data_dataloaders[t] = iter(self.loader_data[t])\n        for t in self.loader_memory.keys():\n            iter_buffer_dataloaders[t] = iter(self.loader_memory[t])\n\n        max_len = max([len(d) for d in chain(iter_data_dataloaders.values(),\n                                             iter_buffer_dataloaders.values())])\n        try:\n            for it in range(max_len):\n                mb_curr = []\n                self._get_mini_batch_from_data_dict(\n                    self.data, iter_data_dataloaders,\n                    self.loader_data, self.oversample_small_tasks,\n                    mb_curr)\n\n                self._get_mini_batch_from_data_dict(\n                    self.memory, iter_buffer_dataloaders,\n                    self.loader_memory, self.oversample_small_tasks,\n                    mb_curr)\n\n                yield self.collate_mbatches(mb_curr)\n        except StopIteration:\n            return",
  "def __len__(self):\n        return self.max_len",
  "def _get_mini_batch_from_data_dict(self, data, iter_dataloaders,\n                                       loaders_dict, oversample_small_tasks,\n                                       mb_curr):\n        # list() is necessary because we may remove keys from the\n        # dictionary. This would break the generator.\n        for t in list(iter_dataloaders.keys()):\n            t_loader = iter_dataloaders[t]\n            try:\n                tbatch = next(t_loader)\n            except StopIteration:\n                # StopIteration is thrown if dataset ends.\n                # reinitialize data loader\n                if oversample_small_tasks:\n                    # reinitialize data loader\n                    iter_dataloaders[t] = iter(loaders_dict[t])\n                    tbatch = next(iter_dataloaders[t])\n                else:\n                    del iter_dataloaders[t]\n                    continue\n            mb_curr.append(tbatch)",
  "def _create_dataloaders(self, data_dict, single_exp_batch_size,\n                            remaining_example, **kwargs):\n        loaders_dict: Dict[int, DataLoader] = {}\n        for task_id in data_dict.task_set:\n            data = data_dict.task_set[task_id]\n            current_batch_size = single_exp_batch_size\n            if remaining_example > 0:\n                current_batch_size += 1\n                remaining_example -= 1\n            loaders_dict[task_id] = DataLoader(\n                data, batch_size=current_batch_size, **kwargs)\n        return loaders_dict, remaining_example",
  "class AvalancheDatasetType(Enum):\n    UNDEFINED = auto()\n    CLASSIFICATION = auto()\n    REGRESSION = auto()\n    SEGMENTATION = auto()",
  "class AvalancheDataset(IDatasetWithTargets[T_co, TTargetType], Dataset[T_co]):\n    \"\"\"\n    The Dataset used as the base implementation for Avalanche.\n\n    Instances of this dataset are usually returned from benchmarks, but it can\n    also be used in a completely standalone manner. This dataset can be used\n    to apply transformations before returning patterns/targets, it supports\n    slicing and advanced indexing and it also contains useful fields as\n    `targets`, which contains the pattern labels, and `targets_task_labels`,\n    which contains the pattern task labels. The `task_set` field can be used to\n    obtain a the subset of patterns labeled with a given task label.\n\n    This dataset can also be used to apply several advanced operations involving\n    transformations. For instance, it allows the user to add and replace\n    transformations, freeze them so that they can't be changed, etc.\n\n    This dataset also allows the user to keep distinct transformations groups.\n    Simply put, a transformation group is a pair of transform+target_transform\n    (exactly as in torchvision datasets). This dataset natively supports keeping\n    two transformation groups: the first, 'train', contains transformations\n    applied to training patterns. Those transformations usually involve some\n    kind of data augmentation. The second one is 'eval', that will contain\n    transformations applied to test patterns. Having both groups can be\n    useful when, for instance, in need to test on the training data (as this\n    process usually involves removing data augmentation operations). Switching\n    between transformations can be easily achieved by using the\n    :func:`train` and :func:`eval` methods.\n\n    Moreover, arbitrary transformation groups can be added and used. For more\n    info see the constructor and the :func:`with_transforms` method.\n\n    This dataset will try to inherit the task labels from the input\n    dataset. If none are available and none are given via the `task_labels`\n    parameter, each pattern will be assigned a default task label \"0\".\n    See the constructor for more details.\n    \"\"\"\n    def __init__(self,\n                 dataset: SupportedDataset,\n                 *,\n                 transform: XTransform = None,\n                 target_transform: YTransform = None,\n                 transform_groups: Dict[str, Tuple[XTransform,\n                                                   YTransform]] = None,\n                 initial_transform_group: str = None,\n                 task_labels: Union[int, Sequence[int]] = None,\n                 targets: Sequence[TTargetType] = None,\n                 dataset_type: AvalancheDatasetType = None,\n                 collate_fn: Callable[[List], Any] = None,\n                 targets_adapter: Callable[[Any], TTargetType] = None):\n        \"\"\"\n        Creates a ``AvalancheDataset`` instance.\n\n        :param dataset: The dataset to decorate. Beware that\n            AvalancheDataset will not overwrite transformations already\n            applied by this dataset.\n        :param transform: A function/transform that takes the X value of a\n            pattern from the original dataset and returns a transformed version.\n        :param target_transform: A function/transform that takes in the target\n            and transforms it.\n        :param transform_groups: A dictionary containing the transform groups.\n            Transform groups are used to quickly switch between training and\n            eval (test) transformations. This becomes useful when in need to\n            test on the training dataset as test transformations usually don't\n            contain random augmentations. ``AvalancheDataset`` natively supports\n            the 'train' and 'eval' groups by calling the ``train()`` and\n            ``eval()`` methods. When using custom groups one can use the\n            ``with_transforms(group_name)`` method instead. Defaults to None,\n            which means that the current transforms will be used to\n            handle both 'train' and 'eval' groups (just like in standard\n            ``torchvision`` datasets).\n        :param initial_transform_group: The name of the initial transform group\n            to be used. Defaults to None, which means that the current group of\n            the input dataset will be used (if an AvalancheDataset). If the\n            input dataset is not an AvalancheDataset, then 'train' will be\n            used.\n        :param task_labels: The task label of each instance. Must be a sequence\n            of ints, one for each instance in the dataset. Alternatively can be\n            a single int value, in which case that value will be used as the\n            task label for all the instances. Defaults to None, which means that\n            the dataset will try to obtain the task labels from the original\n            dataset. If no task labels could be found, a default task label\n            \"0\" will be applied to all instances.\n        :param targets: The label of each pattern. Defaults to None, which\n            means that the targets will be retrieved from the dataset (if\n            possible).\n        :param dataset_type: The type of the dataset. Defaults to None,\n            which means that the type will be inferred from the input dataset.\n            When the `dataset_type` is different than UNDEFINED, a\n            proper value for `collate_fn` and `targets_adapter` will be set.\n            If the `dataset_type` is different than UNDEFINED, then\n            `collate_fn` and `targets_adapter` must not be set.\n        :param collate_fn: The function to use when slicing to merge single\n            patterns. In the future this function may become the function\n            used in the data loading process, too. If None and the\n            `dataset_type` is UNDEFINED, the constructor will check if a\n            `collate_fn` field exists in the dataset. If no such field exists,\n            the default collate function will be used.\n        :param targets_adapter: A function used to convert the values of the\n            targets field. Defaults to None. Note: the adapter will not change\n            the value of the second element returned by `__getitem__`.\n            The adapter is used to adapt the values of the targets field only.\n        \"\"\"\n        super().__init__()\n\n        if transform_groups is not None and (\n                transform is not None or target_transform is not None):\n            raise ValueError('transform_groups can\\'t be used with transform'\n                             'and target_transform values')\n\n        detected_type = False\n        if dataset_type is None:\n            detected_type = True\n            if isinstance(dataset, AvalancheDataset):\n                dataset_type = dataset.dataset_type\n            else:\n                dataset_type = AvalancheDatasetType.UNDEFINED\n\n        if dataset_type != AvalancheDatasetType.UNDEFINED and \\\n                (collate_fn is not None or targets_adapter is not None):\n            if detected_type:\n                raise ValueError(\n                    'dataset_type {} was inferred from the input dataset. '\n                    'This dataset type can\\'t be used '\n                    'with custom collate_fn or targets_adapter '\n                    'parameters. Only the UNDEFINED type supports '\n                    'custom collate_fn or targets_adapter '\n                    'parameters'.format(dataset_type))\n            else:\n                raise ValueError(\n                    'dataset_type {} can\\'t be used with custom collate_fn '\n                    'or targets_adapter. Only the UNDEFINED type supports '\n                    'custom collate_fn or targets_adapter '\n                    'parameters.'.format(dataset_type))\n\n        if transform_groups is not None:\n            AvalancheDataset._check_groups_dict_format(transform_groups)\n\n        if not isinstance(dataset_type, AvalancheDatasetType):\n            raise ValueError('dataset_type must be a value of type '\n                             'AvalancheDatasetType')\n\n        self._dataset: SupportedDataset = dataset\n        \"\"\"\n        The original dataset.\n        \"\"\"\n\n        self.dataset_type = dataset_type\n        \"\"\"\n        The type of this dataset (UNDEFINED, CLASSIFICATION, ...).\n        \"\"\"\n\n        self.targets: Sequence[TTargetType] = self._initialize_targets_sequence(\n            dataset, targets, dataset_type, targets_adapter)\n        \"\"\"\n        A sequence of values describing the label of each pattern contained in\n        the dataset.\n        \"\"\"\n\n        self.targets_task_labels: Sequence[int] = \\\n            self._initialize_task_labels_sequence(dataset, task_labels)\n        \"\"\"\n        A sequence of ints describing the task label of each pattern contained \n        in the dataset.\n        \"\"\"\n\n        self.tasks_pattern_indices: Dict[int, Sequence[int]] = \\\n            self._initialize_tasks_dict(dataset, self.targets_task_labels)\n        \"\"\"\n        A dictionary mapping task labels to the indices of the patterns with \n        that task label. If you need to obtain the subset of patterns labeled\n        with a certain task label, consider using the `task_set` field.\n        \"\"\"\n\n        self.collate_fn = self._initialize_collate_fn(\n            dataset, dataset_type, collate_fn)\n        \"\"\"\n        The collate function to use when creating mini-batches from this\n        dataset.\n        \"\"\"\n\n        # Compress targets and task labels to save some memory\n        self._optimize_targets()\n        self._optimize_task_labels()\n        self._optimize_task_dict()\n\n        self.task_set = _TaskSubsetDict(self)\n        \"\"\"\n        A dictionary that can be used to obtain the subset of patterns given\n        a specific task label.\n        \"\"\"\n\n        if initial_transform_group is None:\n            # Detect from the input dataset. If not an AvalancheDataset then\n            # use 'train' as the initial transform group\n            if isinstance(dataset, AvalancheDataset):\n                initial_transform_group = dataset.current_transform_group\n            else:\n                initial_transform_group = 'train'\n\n        self.current_transform_group = initial_transform_group\n        \"\"\"\n        The name of the transform group currently in use.\n        \"\"\"\n\n        self.transform_groups: Dict[str, Tuple[XTransform, YTransform]] = \\\n            self._initialize_groups_dict(transform_groups, dataset,\n                                         transform, target_transform)\n        \"\"\"\n        A dictionary containing the transform groups. Transform groups are\n        used to quickly switch between training and test (eval) transformations.\n        This becomes useful when in need to test on the training dataset as test\n        transformations usually don't contain random augmentations.\n\n        AvalancheDataset natively supports switching between the 'train' and\n        'eval' groups by calling the ``train()`` and ``eval()`` methods. When\n        using custom groups one can use the ``with_transforms(group_name)``\n        method instead.\n\n        May be null, which means that the current transforms will be used to\n        handle both 'train' and 'eval' groups.\n        \"\"\"\n\n        if self.current_transform_group not in self.transform_groups:\n            raise ValueError('Invalid transformations group ' +\n                             str(self.current_transform_group))\n        t_group = self.transform_groups[self.current_transform_group]\n\n        self.transform: XTransform = t_group[0]\n        \"\"\"\n        A function/transform that takes in an PIL image and returns a \n        transformed version.\n        \"\"\"\n\n        self.target_transform: YTransform = t_group[1]\n        \"\"\"\n        A function/transform that takes in the target and transforms it.\n        \"\"\"\n\n        self._frozen_transforms: \\\n            Dict[str, Tuple[XTransform, YTransform]] = dict()\n        \"\"\"\n        A dictionary containing frozen transformations.\n        \"\"\"\n\n        for group_name in self.transform_groups.keys():\n            self._frozen_transforms[group_name] = (None, None)\n\n        self._set_original_dataset_transform_group(self.current_transform_group)\n\n        self._flatten_dataset()\n\n    def __add__(self, other: Dataset) -> 'AvalancheDataset':\n        return AvalancheConcatDataset([self, other])\n\n    def __radd__(self, other: Dataset) -> 'AvalancheDataset':\n        return AvalancheConcatDataset([other, self])\n\n    def __getitem__(self, idx) -> Union[T_co, Sequence[T_co]]:\n        return TupleTLabel(manage_advanced_indexing(\n            idx, self._get_single_item, len(self), self.collate_fn))\n\n    def __len__(self):\n        return len(self._dataset)\n\n    def train(self):\n        \"\"\"\n        Returns a new dataset with the transformations of the 'train' group\n        loaded.\n\n        The current dataset will not be affected.\n\n        :return: A new dataset with the training transformations loaded.\n        \"\"\"\n        return self.with_transforms('train')\n\n    def eval(self):\n        \"\"\"\n        Returns a new dataset with the transformations of the 'eval' group\n        loaded.\n\n        Eval transformations usually don't contain augmentation procedures.\n        This function may be useful when in need to test on training data\n        (for instance, in order to run a validation pass).\n\n        The current dataset will not be affected.\n\n        :return: A new dataset with the eval transformations loaded.\n        \"\"\"\n        return self.with_transforms('eval')\n\n    def freeze_transforms(self: TAvalancheDataset) -> \\\n            TAvalancheDataset:\n        \"\"\"\n        Returns a new dataset where the current transformations are frozen.\n\n        Frozen transformations will be permanently glued to the original\n        dataset so that they can't be changed anymore. This is usually done\n        when using transformations to create derived datasets: in this way\n        freezing the transformations will ensure that the user won't be able\n        to inadvertently change them by directly setting the transformations\n        field or by using the other transformations utility methods like\n        ``replace_transforms``. Please note that transformations of all groups\n        will be frozen. If you want to freeze a specific group, please use\n        ``freeze_group_transforms``.\n\n        The current dataset will not be affected.\n\n        :return: A new dataset with the current transformations frozen.\n        \"\"\"\n\n        dataset_copy = self._fork_dataset()\n\n        for group_name in dataset_copy.transform_groups.keys():\n            AvalancheDataset._freeze_dataset_group(dataset_copy,\n                                                   group_name)\n\n        return dataset_copy\n\n    def freeze_group_transforms(self: TAvalancheDataset,\n                                group_name: str) -> TAvalancheDataset:\n        \"\"\"\n        Returns a new dataset where the transformations for a specific group\n        are frozen.\n\n        Frozen transformations will be permanently glued to the original\n        dataset so that they can't be changed anymore. This is usually done\n        when using transformations to create derived datasets: in this way\n        freezing the transformations will ensure that the user won't be able\n        to inadvertently change them by directly setting the transformations\n        field or by using the other transformations utility methods like\n        ``replace_transforms``. To freeze transformations of all groups\n        please use ``freeze_transforms``.\n\n        The current dataset will not be affected.\n\n        :return: A new dataset with the transformations frozen for the given\n            group.\n        \"\"\"\n        dataset_copy = self._fork_dataset()\n\n        AvalancheDataset._freeze_dataset_group(dataset_copy, group_name)\n\n        return dataset_copy\n\n    def get_transforms(\n            self: TAvalancheDataset,\n            transforms_group: str = None) -> Tuple[Any, Any]:\n        \"\"\"\n        Returns the transformations given a group.\n\n        Beware that this will not return the frozen transformations, nor the\n        ones included in the wrapped dataset. Only transformations directly\n        attached to this dataset will be returned.\n\n        :param transforms_group: The transformations group. Defaults to None,\n            which means that the current group is returned.\n        :return: The transformation group, as a tuple\n            (transform, target_transform).\n        \"\"\"\n        if transforms_group is None:\n            transforms_group = self.current_transform_group\n\n        if transforms_group == self.current_transform_group:\n            return self.transform, self.target_transform\n\n        return self.transform_groups[transforms_group]\n\n    def add_transforms(\n            self: TAvalancheDataset,\n            transform: Callable[[Any], Any] = None,\n            target_transform: Callable[[int], int] = None) -> \\\n            TAvalancheDataset:\n        \"\"\"\n        Returns a new dataset with the given transformations added to\n        the existing ones.\n\n        The transformations will be added to the current transformations group.\n        Other transformation groups will not be affected.\n\n        The given transformations will be added \"at the end\" of previous\n        transformations of the current transformations group. This means\n        that existing transformations will be applied to the patterns first.\n\n        The current dataset will not be affected.\n\n        :param transform: A function/transform that takes the X value of a\n            pattern from the original dataset and returns a transformed version.\n        :param target_transform: A function/transform that takes in the target\n            and transforms it.\n        :return: A new dataset with the added transformations.\n        \"\"\"\n\n        dataset_copy = self._fork_dataset()\n\n        if transform is not None:\n            if dataset_copy.transform is not None:\n                dataset_copy.transform = Compose([\n                    dataset_copy.transform, transform])\n            else:\n                dataset_copy.transform = transform\n\n        if target_transform is not None:\n            if dataset_copy.target_transform is not None:\n                dataset_copy.target_transform = Compose([\n                    dataset_copy.target_transform, target_transform])\n            else:\n                dataset_copy.target_transform = transform\n\n        return dataset_copy\n\n    def add_transforms_to_group(\n            self: TAvalancheDataset,\n            group_name: str,\n            transform: Callable[[Any], Any] = None,\n            target_transform: Callable[[int], int] = None) -> \\\n            TAvalancheDataset:\n        \"\"\"\n        Returns a new dataset with the given transformations added to\n        the existing ones for a certain group.\n\n        The transformations will be added to the given transformations group.\n        Other transformation groups will not be affected. The group must\n        already exist.\n\n        The given transformations will be added \"at the end\" of previous\n        transformations of that group. This means that existing transformations\n        will be applied to the patterns first.\n\n        The current dataset will not be affected.\n\n        :param group_name: The name of the group.\n        :param transform: A function/transform that takes the X value of a\n            pattern from the original dataset and returns a transformed version.\n        :param target_transform: A function/transform that takes in the target\n            and transforms it.\n        :return: A new dataset with the added transformations.\n        \"\"\"\n\n        if self.current_transform_group == group_name:\n            return self.add_transforms(transform, target_transform)\n\n        if group_name not in self.transform_groups:\n            raise ValueError('Invalid group name ' + str(group_name))\n\n        dataset_copy = self._fork_dataset()\n\n        t_group: List[XTransform, YTransform] = \\\n            list(dataset_copy.transform_groups[group_name])\n        if transform is not None:\n            if t_group[0] is not None:\n                t_group[0] = Compose([t_group[0], transform])\n            else:\n                t_group[0] = transform\n\n        if target_transform is not None:\n            if t_group[1] is not None:\n                t_group[1] = Compose([t_group[1], target_transform])\n            else:\n                t_group[1] = transform\n\n        # tuple(t_group) works too, but it triggers a type warning\n        tuple_t_group: Tuple[XTransform, YTransform] = tuple(\n            (t_group[0], t_group[1]))\n        dataset_copy.transform_groups[group_name] = tuple_t_group\n\n        return dataset_copy\n\n    def replace_transforms(\n            self: TAvalancheDataset,\n            transform: XTransform,\n            target_transform: YTransform,\n            group: str = None) -> TAvalancheDataset:\n        \"\"\"\n        Returns a new dataset with the existing transformations replaced with\n        the given ones.\n\n        The given transformations will replace the ones of the current\n        transformations group. Other transformation groups will not be affected.\n\n        If the original dataset is an instance of :class:`AvalancheDataset`,\n        then transformations of the original set will be considered as well\n        (the original dataset will be left untouched).\n\n        The current dataset will not be affected.\n\n        Note that this function will not override frozen transformations. This\n        will also not affect transformations found in datasets that are not\n        instances of :class:`AvalancheDataset`.\n\n        :param transform: A function/transform that takes the X value of a\n            pattern from the original dataset and returns a transformed version.\n        :param target_transform: A function/transform that takes in the target\n            and transforms it.\n        :param group: The transforms group to replace. Defaults to None, which\n            means that the current group will be replaced.\n        :return: A new dataset with the new transformations.\n        \"\"\"\n\n        if group is None:\n            group = self.current_transform_group\n\n        dataset_copy = self._fork_dataset().with_transforms(group)\n        dataset_copy._replace_original_dataset_group(None, None)\n\n        dataset_copy.transform = transform\n        dataset_copy.target_transform = target_transform\n\n        return dataset_copy.with_transforms(self.current_transform_group)\n\n    def with_transforms(self: TAvalancheDataset, group_name: str) -> \\\n            TAvalancheDataset:\n        \"\"\"\n        Returns a new dataset with the transformations of a different group\n        loaded.\n\n        The current dataset will not be affected.\n\n        :param group_name: The name of the transformations group to use.\n        :return: A new dataset with the new transformations.\n        \"\"\"\n        dataset_copy = self._fork_dataset()\n\n        if group_name not in dataset_copy.transform_groups:\n            raise ValueError('Invalid group name ' + str(group_name))\n\n        # Store current group (loaded in transform and target_transform fields)\n        dataset_copy.transform_groups[dataset_copy.current_transform_group] = \\\n            (dataset_copy.transform, dataset_copy.target_transform)\n\n        # Load new group in transform and target_transform fields\n        switch_group = dataset_copy.transform_groups[group_name]\n        dataset_copy.transform = switch_group[0]\n        dataset_copy.target_transform = switch_group[1]\n        dataset_copy.current_transform_group = group_name\n\n        # Finally, align the underlying dataset\n        dataset_copy._set_original_dataset_transform_group(group_name)\n\n        return dataset_copy\n\n    def add_transforms_group(self: TAvalancheDataset,\n                             group_name: str,\n                             transform: XTransform,\n                             target_transform: YTransform) -> \\\n            TAvalancheDataset:\n        \"\"\"\n        Returns a new dataset with a new transformations group.\n\n        The current dataset will not be affected.\n\n        This method raises an exception if a group with the same name already\n        exists.\n\n        :param group_name: The name of the new transformations group.\n        :param transform: A function/transform that takes the X value of a\n            pattern from the original dataset and returns a transformed version.\n        :param target_transform: A function/transform that takes in the target\n            and transforms it.\n        :return: A new dataset with the new transformations.\n        \"\"\"\n        dataset_copy = self._fork_dataset()\n\n        if group_name in dataset_copy.transform_groups:\n            raise ValueError('A group with the same name already exists')\n\n        dataset_copy.transform_groups[group_name] = \\\n            (transform, target_transform)\n\n        AvalancheDataset._check_groups_dict_format(\n            dataset_copy.transform_groups)\n\n        dataset_copy._frozen_transforms[group_name] = (None, None)\n\n        # Finally, align the underlying dataset\n        dataset_copy._add_original_dataset_group(group_name)\n\n        return dataset_copy\n\n    def _fork_dataset(self: TAvalancheDataset) -> TAvalancheDataset:\n        dataset_copy = copy.copy(self)\n        dataset_copy._frozen_transforms = dict(dataset_copy._frozen_transforms)\n        dataset_copy.transform_groups = dict(dataset_copy.transform_groups)\n\n        return dataset_copy\n\n    @staticmethod\n    def _freeze_dataset_group(dataset_copy: TAvalancheDataset,\n                              group_name: str):\n        # Freeze the current transformations. Frozen transformations are saved\n        # in a separate dict.\n\n        # This may rise an error if no group with the given name exists!\n        frozen_group = dataset_copy._frozen_transforms[group_name]\n\n        final_frozen_transform = frozen_group[0]\n        final_frozen_target_transform = frozen_group[1]\n\n        is_current_group = dataset_copy.current_transform_group == group_name\n\n        # If the required group is not the current one, just freeze the ones\n        # found in dataset_copy.transform_groups).\n        to_be_glued = dataset_copy.transform_groups[group_name]\n\n        # Now that transformations are stored in to_be_glued,\n        # we can safely reset them in the transform_groups dictionary.\n        dataset_copy.transform_groups[group_name] = (None, None)\n\n        if is_current_group:\n            # If the required group is the current one, use the transformations\n            # already found in transform and target_transform fields (because\n            # the ones stored in dataset_copy.transform_groups may be not\n            # up-to-date).\n\n            to_be_glued = (dataset_copy.transform,\n                           dataset_copy.target_transform)\n\n            # And of course, once frozen, set transformations to None\n            dataset_copy.transform = None\n            dataset_copy.target_transform = None\n\n        if to_be_glued[0] is not None:\n            if frozen_group[0] is None:\n                final_frozen_transform = to_be_glued[0]\n            else:\n                final_frozen_transform = Compose([\n                    frozen_group[0], to_be_glued[0]])\n\n        if to_be_glued[1] is not None:\n            if frozen_group[1] is None:\n                final_frozen_target_transform = to_be_glued[1]\n            else:\n                final_frozen_target_transform = Compose([\n                    frozen_group[1], to_be_glued[1]])\n\n        # Set the frozen transformations\n        dataset_copy._frozen_transforms[group_name] = (\n            final_frozen_transform, final_frozen_target_transform)\n\n        # Finally, apply the freeze procedure to the original dataset\n        dataset_copy._freeze_original_dataset(group_name)\n\n    def _get_single_item(self, idx: int):\n        return self._process_pattern(self._dataset[idx], idx)\n\n    def _process_pattern(self, element: Tuple, idx: int):\n        has_task_label = isinstance(element, TupleTLabel)\n        if has_task_label:\n            element = element[:-1]\n\n        pattern = element[0]\n        label = element[1]\n\n        pattern, label = self._apply_transforms(pattern, label)\n\n        return TupleTLabel((pattern, label, *element[2:],\n                            self.targets_task_labels[idx]))\n\n    def _apply_transforms(self, pattern: Any, label: int):\n        frozen_group = self._frozen_transforms[self.current_transform_group]\n        if frozen_group[0] is not None:\n            pattern = frozen_group[0](pattern)\n\n        if self.transform is not None:\n            pattern = self.transform(pattern)\n\n        if frozen_group[1] is not None:\n            label = frozen_group[1](label)\n\n        if self.target_transform is not None:\n            label = self.target_transform(label)\n\n        return pattern, label\n\n    @staticmethod\n    def _check_groups_dict_format(groups_dict):\n        # The original groups_dict must be convertible to native Python dict\n        groups_dict = dict(groups_dict)\n\n        # Check if the format of the groups is correct\n        for map_key in groups_dict:\n            if not isinstance(map_key, str):\n                raise ValueError('Every group must be identified by a string.'\n                                 'Wrong key was: \"' + str(map_key) + '\"')\n\n            map_value = groups_dict[map_key]\n            if not isinstance(map_value, tuple):\n                raise ValueError('Transformations for group \"' + str(map_key) +\n                                 '\" must be contained in a tuple')\n\n            if not len(map_value) == 2:\n                raise ValueError(\n                    'Transformations for group \"' + str(map_key) + '\" must be '\n                    'a tuple containing 2 elements: a transformation for the X '\n                    'values and a transformation for the Y values')\n\n        if 'test' in groups_dict:\n            warnings.warn(\n                'A transformation group named \"test\" has been found. Beware '\n                'that by default AvalancheDataset supports test transformations'\n                ' through the \"eval\" group. Consider using that one!')\n\n    def _initialize_groups_dict(\n            self,\n            transform_groups: Optional[Dict[str, Tuple[XTransform,\n                                                       YTransform]]],\n            dataset: Any,\n            transform: XTransform,\n            target_transform: YTransform) -> Dict[str, Tuple[XTransform,\n                                                             YTransform]]:\n        \"\"\"\n        A simple helper method that tries to fill the 'train' and 'eval'\n        groups as those two groups must always exist.\n\n        If no transform_groups are passed to the class constructor, then\n        the transform and target_transform parameters are used for both groups.\n\n        If train transformations are set and eval transformations are not, then\n        train transformations will be used for the eval group.\n\n        :param dataset: The original dataset. Will be used to detect existing\n            groups.\n        :param transform: The transformation passed as a parameter to the\n            class constructor.\n        :param target_transform: The target transformation passed as a parameter\n            to the class constructor.\n        \"\"\"\n        if transform_groups is None:\n            transform_groups = {\n                'train': (transform, target_transform),\n                'eval': (transform, target_transform)\n            }\n        else:\n            transform_groups = dict(transform_groups)\n\n        if 'train' in transform_groups:\n            if 'eval' not in transform_groups:\n                transform_groups['eval'] = transform_groups['train']\n\n        if 'train' not in transform_groups:\n            transform_groups['train'] = (None, None)\n\n        if 'eval' not in transform_groups:\n            transform_groups['eval'] = (None, None)\n\n        self._add_groups_from_original_dataset(dataset, transform_groups)\n\n        return transform_groups\n\n    def _initialize_targets_sequence(self, dataset, targets,\n                                     dataset_type, targets_adapter) \\\n            -> Sequence[TTargetType]:\n        if targets is not None:\n            # User defined targets always take precedence\n            # Note: no adapter is applied!\n            if len(targets) != len(dataset):\n                raise ValueError(\n                    'Invalid amount of target labels. It must be equal to the '\n                    'number of patterns in the dataset. Got {}, expected '\n                    '{}!'.format(len(targets), len(dataset)))\n            return targets\n\n        if targets_adapter is None:\n            if dataset_type == AvalancheDatasetType.CLASSIFICATION:\n                targets_adapter = int\n            else:\n                targets_adapter = None\n        return _make_target_from_supported_dataset(dataset, targets_adapter)\n\n    def _initialize_task_labels_sequence(\n            self, dataset, task_labels: Optional[Sequence[int]]) \\\n            -> Sequence[int]:\n        if task_labels is not None:\n            # task_labels has priority over the dataset fields\n            if isinstance(task_labels, int):\n                task_labels = ConstantSequence(task_labels, len(dataset))\n            elif len(task_labels) != len(dataset):\n                raise ValueError(\n                    'Invalid amount of task labels. It must be equal to the '\n                    'number of patterns in the dataset. Got {}, expected '\n                    '{}!'.format(len(task_labels), len(dataset)))\n\n            return SubSequence(task_labels, converter=int)\n\n        return _make_task_labels_from_supported_dataset(dataset)\n\n    def _initialize_collate_fn(self, dataset, dataset_type, collate_fn):\n        if collate_fn is not None:\n            return collate_fn\n\n        if dataset_type == AvalancheDatasetType.UNDEFINED:\n            if hasattr(dataset, 'collate_fn'):\n                return getattr(dataset, 'collate_fn')\n\n        return default_collate\n\n    def _initialize_tasks_dict(self, dataset, task_labels: Sequence[int]) \\\n            -> Dict[int, Sequence[int]]:\n        if isinstance(task_labels, ConstantSequence) and len(task_labels) > 0:\n            # Shortcut :)\n            return {task_labels[0]: range(len(task_labels))}\n\n        result = dict()\n        for i, x in enumerate(task_labels):\n            if x not in result:\n                result[x] = []\n            result[x].append(i)\n\n        if len(result) == 1:\n            result[next(iter(result.keys()))] = range(len(task_labels))\n\n        return result\n\n    def _set_original_dataset_transform_group(\n            self, group_name: str) -> None:\n        if isinstance(self._dataset, AvalancheDataset):\n            if self._dataset.current_transform_group == group_name:\n                # Prevents a huge slowdown in some corner cases\n                # (apart from being actually more performant)\n                return\n\n            self._dataset = self._dataset.with_transforms(group_name)\n\n    def _freeze_original_dataset(\n            self, group_name: str) -> None:\n        if isinstance(self._dataset, AvalancheDataset):\n            self._dataset = self._dataset.freeze_group_transforms(group_name)\n\n    def _replace_original_dataset_group(\n            self, transform: XTransform, target_transform: YTransform) -> None:\n        if isinstance(self._dataset, AvalancheDataset):\n            self._dataset = self._dataset.replace_transforms(\n                transform, target_transform)\n\n    def _add_original_dataset_group(\n            self, group_name: str) -> None:\n        if isinstance(self._dataset, AvalancheDataset):\n            self._dataset = self._dataset.add_transforms_group(\n                group_name, None, None)\n\n    def _add_groups_from_original_dataset(\n            self, dataset, transform_groups) -> None:\n        if isinstance(dataset, AvalancheDataset):\n            for original_dataset_group in dataset.transform_groups.keys():\n                if original_dataset_group not in transform_groups:\n                    transform_groups[original_dataset_group] = (None, None)\n\n    def _has_own_transformations(self):\n        # Used to check if the current dataset has its own transformations\n        # This method returns False if transformations are applied\n        # by the wrapped dataset only.\n\n        if self.transform is not None:\n            return True\n\n        if self.target_transform is not None:\n            return True\n\n        for transform_group in self.transform_groups.values():\n            for transform in transform_group:\n                if transform is not None:\n                    return True\n\n        for transform_group in self._frozen_transforms.values():\n            for transform in transform_group:\n                if transform is not None:\n                    return True\n\n        return False\n\n    @staticmethod\n    def _borrow_transformations(dataset, transform_groups,\n                                frozen_transform_groups):\n        if not isinstance(dataset, AvalancheDataset):\n            return\n\n        for original_dataset_group in dataset.transform_groups.keys():\n            if original_dataset_group not in transform_groups:\n                transform_groups[original_dataset_group] = (None, None)\n\n        for original_dataset_group in dataset._frozen_transforms.keys():\n            if original_dataset_group not in frozen_transform_groups:\n                frozen_transform_groups[original_dataset_group] = (None, None)\n\n        # Standard transforms\n        for original_dataset_group in dataset.transform_groups.keys():\n            other_dataset_transforms = \\\n                dataset.transform_groups[original_dataset_group]\n            if dataset.current_transform_group == original_dataset_group:\n                other_dataset_transforms = (dataset.transform,\n                                            dataset.target_transform)\n\n            transform_groups[original_dataset_group] = \\\n                AvalancheDataset._prepend_transforms(\n                    transform_groups[original_dataset_group],\n                    other_dataset_transforms)\n\n        # Frozen transforms\n        for original_dataset_group in dataset._frozen_transforms.keys():\n            other_dataset_transforms = \\\n                dataset._frozen_transforms[original_dataset_group]\n\n            frozen_transform_groups[original_dataset_group] = \\\n                AvalancheDataset._prepend_transforms(\n                    frozen_transform_groups[original_dataset_group],\n                    other_dataset_transforms)\n\n    @staticmethod\n    def _prepend_transforms(transforms, to_be_prepended):\n        if len(transforms) != 2:\n            raise ValueError(\n                'Transformation groups must contain exactly 2 transformations')\n\n        if len(transforms) != len(to_be_prepended):\n            raise ValueError(\n                'Transformation group size mismatch: {} vs {}'.format(\n                    len(transforms), len(to_be_prepended)\n                ))\n\n        result = []\n\n        for i in range(len(transforms)):\n            if to_be_prepended[i] is None:\n                # Nothing to prepend\n                result.append(transforms[i])\n            elif transforms[i] is None:\n                result.append(to_be_prepended[i])\n            else:\n                result.append(Compose([\n                    to_be_prepended[i], transforms[i]]))\n\n        return tuple(result)  # Transform to tuple\n\n    def _optimize_targets(self):\n        self.targets = optimize_sequence(self.targets)\n\n    def _optimize_task_labels(self):\n        self.targets_task_labels = optimize_sequence(self.targets_task_labels)\n\n    def _optimize_task_dict(self):\n        for task_label in self.tasks_pattern_indices:\n            self.tasks_pattern_indices[task_label] = optimize_sequence(\n                self.tasks_pattern_indices[task_label])\n\n    def _flatten_dataset(self):\n        pass",
  "class _TaskSubsetDict(OrderedDict):\n\n    def __init__(self, avalanche_dataset: AvalancheDataset):\n        self._full_dataset = avalanche_dataset\n        task_ids = self._full_dataset.tasks_pattern_indices.keys()\n        task_ids = sorted(list(task_ids))\n        base_dict = OrderedDict()\n        for x in task_ids:\n            base_dict[x] = x\n        super().__init__(base_dict)\n\n    def __getitem__(self, task_id: int):\n        if task_id not in self._full_dataset.tasks_pattern_indices:\n            raise KeyError('No pattern with ' + str(task_id) + ' found')\n        pattern_indices = self._full_dataset.tasks_pattern_indices[task_id]\n        return self._make_subset(pattern_indices)\n\n    def or_empty(self, task_id: int):\n        try:\n            return self[task_id]\n        except KeyError:\n            return self._make_subset([])\n\n    def _make_subset(self, indices: Sequence[int]):\n        return AvalancheSubset(self._full_dataset, indices=indices)",
  "class AvalancheSubset(AvalancheDataset[T_co, TTargetType]):\n    \"\"\"\n    A Dataset that behaves like a PyTorch :class:`torch.utils.data.Subset`.\n    This Dataset also supports transformations, slicing, advanced indexing,\n    the targets field, class mapping and all the other goodies listed in\n    :class:`AvalancheDataset`.\n    \"\"\"\n    def __init__(self,\n                 dataset: SupportedDataset,\n                 indices: Sequence[int] = None,\n                 *,\n                 class_mapping: Sequence[int] = None,\n                 transform: Callable[[Any], Any] = None,\n                 target_transform: Callable[[int], int] = None,\n                 transform_groups: Dict[str, Tuple[XTransform,\n                                                   YTransform]] = None,\n                 initial_transform_group: str = None,\n                 task_labels: Union[int, Sequence[int]] = None,\n                 targets: Sequence[TTargetType] = None,\n                 dataset_type: AvalancheDatasetType = None,\n                 collate_fn: Callable[[List], Any] = None,\n                 targets_adapter: Callable[[Any], TTargetType] = None):\n        \"\"\"\n        Creates an ``AvalancheSubset`` instance.\n\n        :param dataset: The whole dataset.\n        :param indices: Indices in the whole set selected for subset. Can\n            be None, which means that the whole dataset will be returned.\n        :param class_mapping: A list that, for each possible target (Y) value,\n            contains its corresponding remapped value. Can be None.\n            Beware that setting this parameter will force the final\n            dataset type to be CLASSIFICATION or UNDEFINED.\n        :param transform: A function/transform that takes the X value of a\n            pattern from the original dataset and returns a transformed version.\n        :param target_transform: A function/transform that takes in the target\n            and transforms it.\n        :param transform_groups: A dictionary containing the transform groups.\n            Transform groups are used to quickly switch between training and\n            eval (test) transformations. This becomes useful when in need to\n            test on the training dataset as test transformations usually don't\n            contain random augmentations. ``AvalancheDataset`` natively supports\n            the 'train' and 'eval' groups by calling the ``train()`` and\n            ``eval()`` methods. When using custom groups one can use the\n            ``with_transforms(group_name)`` method instead. Defaults to None,\n            which means that the current transforms will be used to\n            handle both 'train' and 'eval' groups (just like in standard\n            ``torchvision`` datasets).\n        :param initial_transform_group: The name of the initial transform group\n            to be used. Defaults to None, which means that the current group of\n            the input dataset will be used (if an AvalancheDataset). If the\n            input dataset is not an AvalancheDataset, then 'train' will be\n            used.\n        :param task_labels: The task label for each instance. Must be a sequence\n            of ints, one for each instance in the dataset. This can either be a\n            list of task labels for the original dataset or the list of task\n            labels for the instances of the subset (an automatic detection will\n            be made). In the unfortunate case in which the original dataset and\n            the subset contain the same amount of instances, then this parameter\n            is considered to contain the task labels of the subset.\n            Alternatively can be a single int value, in which case\n            that value will be used as the task label for all the instances.\n            Defaults to None, which means that the dataset will try to\n            obtain the task labels from the original dataset. If no task labels\n            could be found, a default task label \"0\" will be applied to all\n            instances.\n        :param targets: The label of each pattern. Defaults to None, which\n            means that the targets will be retrieved from the dataset (if\n            possible). This can either be a list of target labels for the\n            original dataset or the list of target labels for the instances of\n            the subset (an automatic detection will be made). In the unfortunate\n            case in which the original dataset and the subset contain the same\n            amount of instances, then this parameter is considered to contain\n            the target labels of the subset.\n        :param dataset_type: The type of the dataset. Defaults to None,\n            which means that the type will be inferred from the input dataset.\n            When the `dataset_type` is different than UNDEFINED, a\n            proper value for `collate_fn` and `targets_adapter` will be set.\n            If the `dataset_type` is different than UNDEFINED, then\n            `collate_fn` and `targets_adapter` must not be set.\n            The only exception to this rule regards `class_mapping`.\n            If `class_mapping` is set, the final dataset_type\n            (as set by this parameter or detected from the subset) must be\n            CLASSIFICATION or UNDEFINED.\n        :param collate_fn: The function to use when slicing to merge single\n            patterns. In the future this function may become the function\n            used in the data loading process, too. If None and the\n            `dataset_type` is UNDEFINED, the constructor will check if a\n            `collate_fn` field exists in the dataset. If no such field exists,\n            the default collate function will be used.\n        :param targets_adapter: A function used to convert the values of the\n            targets field. Defaults to None. Note: the adapter will not change\n            the value of the second element returned by `__getitem__`.\n            The adapter is used to adapt the values of the targets field only.\n        \"\"\"\n\n        detected_type = False\n        if dataset_type is None:\n            detected_type = True\n            if isinstance(dataset, AvalancheDataset):\n                dataset_type = dataset.dataset_type\n                if dataset_type == AvalancheDatasetType.UNDEFINED:\n                    if collate_fn is None:\n                        collate_fn = dataset.collate_fn\n            else:\n                dataset_type = AvalancheDatasetType.UNDEFINED\n\n        if dataset_type != AvalancheDatasetType.UNDEFINED and \\\n                (collate_fn is not None or targets_adapter is not None):\n            if detected_type:\n                raise ValueError(\n                    'dataset_type {} was inferred from the input dataset. '\n                    'This dataset type can\\'t be used '\n                    'with custom collate_fn or targets_adapter '\n                    'parameters. Only the UNDEFINED type supports '\n                    'custom collate_fn or targets_adapter '\n                    'parameters'.format(dataset_type))\n            else:\n                raise ValueError(\n                    'dataset_type {} can\\'t be used with custom collate_fn '\n                    'or targets_adapter. Only the UNDEFINED type supports '\n                    'custom collate_fn or targets_adapter '\n                    'parameters.'.format(dataset_type))\n\n        if class_mapping is not None:\n            if dataset_type not in [AvalancheDatasetType.CLASSIFICATION,\n                                    AvalancheDatasetType.UNDEFINED]:\n                raise ValueError('class_mapping is defined but the dataset type'\n                                 ' is neither CLASSIFICATION or UNDEFINED.')\n\n        if class_mapping is not None:\n            subset = ClassificationSubset(dataset, indices=indices,\n                                          class_mapping=class_mapping)\n        elif indices is not None:\n            subset = Subset(dataset, indices=indices)\n        else:\n            subset = dataset  # Exactly like a plain AvalancheDataset\n\n        self._original_dataset = dataset\n        # self._indices and self._class_mapping currently not used apart from\n        # initialization procedures\n        self._class_mapping = class_mapping\n        self._indices = indices\n\n        if initial_transform_group is None:\n            if isinstance(dataset, AvalancheDataset):\n                initial_transform_group = dataset.current_transform_group\n            else:\n                initial_transform_group = 'train'\n\n        super().__init__(subset,\n                         transform=transform,\n                         target_transform=target_transform,\n                         transform_groups=transform_groups,\n                         initial_transform_group=initial_transform_group,\n                         task_labels=task_labels,\n                         targets=targets,\n                         dataset_type=dataset_type,\n                         collate_fn=collate_fn,\n                         targets_adapter=targets_adapter)\n\n    def _initialize_targets_sequence(\n            self, dataset, targets, dataset_type, targets_adapter) \\\n            -> Sequence[TTargetType]:\n        if targets is not None:\n            # For the reasoning behind this, have a look at\n            # _initialize_task_labels_sequence (it's basically the same).\n\n            if len(targets) == len(self._original_dataset) and \\\n                    not len(targets) == len(dataset):\n                return SubSequence(targets, indices=self._indices)\n            elif len(targets) == len(dataset):\n                return targets\n            else:\n                raise ValueError(\n                    'Invalid amount of targets. It must be equal to the '\n                    'number of patterns in the subset. '\n                    'Got {}, expected {}!'.format(\n                        len(targets), len(dataset)))\n\n        return super()._initialize_targets_sequence(\n            dataset, None, dataset_type, targets_adapter)\n\n    def _initialize_task_labels_sequence(\n            self, dataset,\n            task_labels: Optional[Sequence[int]]) -> Sequence[int]:\n\n        if task_labels is not None:\n            # The task_labels parameter is kind of ambiguous...\n            # it may either be the list of task labels of the required subset\n            # or it may be the list of task labels of the original dataset.\n            # Simple solution: check the length of task_labels!\n            # However, if task_labels, the original dataset and this subset have\n            # the same size, then task_labels is considered to contain the task\n            # labels for the subset!\n\n            if isinstance(task_labels, int):\n                # Simplest case: constant task label\n                return ConstantSequence(task_labels, len(dataset))\n            elif len(task_labels) == len(self._original_dataset) and \\\n                    not len(task_labels) == len(dataset):\n                # task_labels refers to the original dataset ...\n                return SubSequence(task_labels, indices=self._indices,\n                                   converter=int)\n            elif len(task_labels) == len(dataset):\n                # One label for each instance\n                return SubSequence(task_labels, converter=int)\n            else:\n                raise ValueError(\n                    'Invalid amount of task labels. It must be equal to the '\n                    'number of patterns in the subset. '\n                    'Got {}, expected {}!'.format(\n                        len(task_labels), len(dataset)))\n\n        return super()._initialize_task_labels_sequence(dataset, None)\n\n    def _set_original_dataset_transform_group(self, group_name: str) -> None:\n        if isinstance(self._original_dataset, AvalancheDataset):\n            if self._original_dataset.current_transform_group == group_name:\n                # Prevents a huge slowdown in some corner cases\n                # (apart from being actually more performant)\n                return\n\n            self._original_dataset = \\\n                self._original_dataset.with_transforms(group_name)\n\n            self._replace_original_dataset_reference()\n\n    def _freeze_original_dataset(self, group_name: str) -> None:\n        if isinstance(self._original_dataset, AvalancheDataset):\n            self._original_dataset = \\\n                self._original_dataset.freeze_group_transforms(group_name)\n\n            self._replace_original_dataset_reference()\n\n    def _replace_original_dataset_group(\n            self, transform: XTransform, target_transform: YTransform) -> None:\n        if isinstance(self._original_dataset, AvalancheDataset):\n            self._original_dataset = \\\n                self._original_dataset.replace_transforms(\n                    transform, target_transform)\n\n            self._replace_original_dataset_reference()\n\n    def _add_original_dataset_group(self, group_name: str) -> None:\n        if isinstance(self._original_dataset, AvalancheDataset):\n            self._original_dataset = \\\n                self._original_dataset.add_transforms_group(\n                    group_name, None, None)\n\n            self._replace_original_dataset_reference()\n\n    def _add_groups_from_original_dataset(\n            self, dataset, transform_groups) -> None:\n        if isinstance(self._original_dataset, AvalancheDataset):\n            for original_dataset_group in \\\n                    self._original_dataset.transform_groups.keys():\n                if original_dataset_group not in transform_groups:\n                    transform_groups[original_dataset_group] = (None, None)\n\n    def _flatten_dataset(self):\n        # Flattens this subset by borrowing indices and class mappings from\n        # the original dataset (if it's an AvalancheSubset or PyTorch Subset)\n\n        if isinstance(self._original_dataset, AvalancheSubset):\n            # In order to flatten the subset, we have to integrate the\n            # transformations (also frozen ones!)\n            AvalancheDataset._borrow_transformations(\n                self._original_dataset, self.transform_groups,\n                self._frozen_transforms)\n\n            # We need to reload transformations after borrowing from the subset\n            # This assumes that _flatten_dataset is called by __init__!\n            self.transform, self.target_transform = \\\n                self.transform_groups[self.current_transform_group]\n\n            forward_dataset = self._original_dataset._original_dataset\n            forward_indices = self._original_dataset._indices\n            forward_class_mapping = self._original_dataset._class_mapping\n\n            if self._class_mapping is not None:\n\n                if forward_class_mapping is not None:\n\n                    new_class_mapping = []\n                    for mapped_class in forward_class_mapping:\n                        # -1 is sometimes used to mark unused classes\n                        # shouldn't be a problem (if it is, is not our fault)\n                        if mapped_class == -1:\n                            forward_mapped = -1\n                        else:\n                            # forward_mapped may be -1\n                            forward_mapped = self._class_mapping[mapped_class]\n\n                        new_class_mapping.append(forward_mapped)\n                else:\n                    new_class_mapping = self._class_mapping\n            else:\n                new_class_mapping = forward_class_mapping  # May be None\n\n            if self._indices is not None:\n                if forward_indices is not None:\n                    new_indices = [forward_indices[x] for x in self._indices]\n                else:\n                    new_indices = self._indices\n            else:\n                new_indices = forward_indices  # May be None\n\n            self._original_dataset = forward_dataset\n            self._indices = new_indices\n            self._class_mapping = new_class_mapping\n            if new_class_mapping is None:\n                # Subset\n                self._dataset = Subset(forward_dataset,\n                                       indices=new_indices)\n            else:\n                # ClassificationSubset\n                self._dataset = ClassificationSubset(\n                    forward_dataset, indices=new_indices,\n                    class_mapping=new_class_mapping)\n\n        # --------\n        # Flattening PyTorch Subset has been temporarily\n        # disabled as the semantic of transformation groups collide\n        # with the flattening process: PyTorch Subset doesn't have\n        # transform groups and flattening it will expose the underlying\n        # dataset, which may contain 'AvalancheDataset's.\n        # --------\n\n        # elif isinstance(self._original_dataset, Subset):\n        #     # Very simple: just borrow indices (no transformations or\n        #     # class mappings to adapt here!)\n        #     forward_dataset = self._original_dataset.dataset\n        #     forward_indices = self._original_dataset.indices\n        #\n        #     if self._indices is not None:\n        #         new_indices = [forward_indices[x] for x in self._indices]\n        #     else:\n        #         new_indices = forward_indices\n        #\n        #     self._original_dataset = forward_dataset\n        #     self._indices = new_indices\n        #\n        #     if self._class_mapping is not None:\n        #         self._dataset = ClassificationSubset(\n        #             forward_dataset, indices=new_indices,\n        #             class_mapping=self._class_mapping)\n        #\n        #     elif self._indices is not None:\n        #         self._dataset = Subset(forward_dataset, indices=new_indices)\n\n    def _replace_original_dataset_reference(self):\n        if isinstance(self._dataset, ClassificationSubset):\n            self._dataset = ClassificationSubset(\n                self._original_dataset, indices=self._indices,\n                class_mapping=self._class_mapping)\n        elif isinstance(self._dataset, Subset):\n            self._dataset = Subset(self._original_dataset,\n                                   indices=self._indices)\n        else:\n            self._dataset = self._original_dataset",
  "class AvalancheTensorDataset(AvalancheDataset[T_co, TTargetType]):\n    \"\"\"\n    A Dataset that wraps existing ndarrays, Tensors, lists... to provide\n    basic Dataset functionalities. Very similar to TensorDataset from PyTorch,\n    this Dataset also supports transformations, slicing, advanced indexing,\n    the targets field and all the other goodies listed in\n    :class:`AvalancheDataset`.\n    \"\"\"\n    def __init__(self,\n                 *dataset_tensors: Sequence,\n                 transform: Callable[[Any], Any] = None,\n                 target_transform: Callable[[int], int] = None,\n                 transform_groups: Dict[str, Tuple[XTransform,\n                                                   YTransform]] = None,\n                 initial_transform_group: str = 'train',\n                 task_labels: Union[int, Sequence[int]] = None,\n                 targets: Union[Sequence[TTargetType], int] = None,\n                 dataset_type: AvalancheDatasetType =\n                 AvalancheDatasetType.UNDEFINED,\n                 collate_fn: Callable[[List], Any] = None,\n                 targets_adapter: Callable[[Any], TTargetType] = None):\n        \"\"\"\n        Creates a ``AvalancheTensorDataset`` instance.\n\n        :param dataset_tensors: Sequences, Tensors or ndarrays representing the\n            content of the dataset.\n        :param transform: A function/transform that takes in a single element\n            from the first tensor and returns a transformed version.\n        :param target_transform: A function/transform that takes a single\n            element of the second tensor and transforms it.\n        :param transform_groups: A dictionary containing the transform groups.\n            Transform groups are used to quickly switch between training and\n            eval (test) transformations. This becomes useful when in need to\n            test on the training dataset as test transformations usually don't\n            contain random augmentations. ``AvalancheDataset`` natively supports\n            the 'train' and 'eval' groups by calling the ``train()`` and\n            ``eval()`` methods. When using custom groups one can use the\n            ``with_transforms(group_name)`` method instead. Defaults to None,\n            which means that the current transforms will be used to\n            handle both 'train' and 'eval' groups (just like in standard\n            ``torchvision`` datasets).\n        :param initial_transform_group: The name of the transform group\n            to be used. Defaults to 'train'.\n        :param task_labels: The task labels for each pattern. Must be a sequence\n            of ints, one for each pattern in the dataset. Alternatively can be a\n            single int value, in which case that value will be used as the task\n            label for all the instances. Defaults to None, which means that a\n            default task label \"0\" will be applied to all patterns.\n        :param targets: The label of each pattern. Defaults to None, which\n            means that the targets will be retrieved from the dataset.\n            Otherwise, can be 1) a sequence of values containing as many\n            elements as the number of patterns, or 2) the index of the sequence\n            to use as the targets field. When using the default value of None,\n            the targets field will be populated using the second\n            tensor. If dataset is made of only one tensor, then that tensor will\n            be used for the targets field, too.\n        :param dataset_type: The type of the dataset. Defaults to UNDEFINED.\n            Setting this parameter will automatically set a proper value for\n            `collate_fn` and `targets_adapter`. If this parameter is set to a\n            value different from UNDEFINED then `collate_fn` and\n            `targets_adapter` must not be set.\n        :param collate_fn: The function to use when slicing to merge single\n            patterns. In the future this function may become the function\n            used in the data loading process, too.\n        :param targets_adapter: A function used to convert the values of the\n            targets field. Defaults to None. Note: the adapter will not change\n            the value of the second element returned by `__getitem__`.\n            The adapter is used to adapt the values of the targets field only.\n        \"\"\"\n\n        if dataset_type != AvalancheDatasetType.UNDEFINED and \\\n                (collate_fn is not None or targets_adapter is not None):\n            raise ValueError(\n                'dataset_type {} can\\'t be used with custom collate_fn '\n                'or targets_adapter. Only the UNDEFINED type supports '\n                'custom collate_fn or targets_adapter '\n                'parameters.'.format(dataset_type))\n\n        if len(dataset_tensors) < 1:\n            raise ValueError('At least one sequence must be passed')\n\n        if targets is None:\n            targets = min(1, len(dataset_tensors))\n\n        if isinstance(targets, int):\n            base_dataset = SequenceDataset(*dataset_tensors, targets=targets)\n            targets = None\n        else:\n            base_dataset = SequenceDataset(*dataset_tensors,\n                                           targets=min(1, len(dataset_tensors)))\n\n        super().__init__(base_dataset,\n                         transform=transform,\n                         target_transform=target_transform,\n                         transform_groups=transform_groups,\n                         initial_transform_group=initial_transform_group,\n                         task_labels=task_labels,\n                         targets=targets,\n                         dataset_type=dataset_type,\n                         collate_fn=collate_fn,\n                         targets_adapter=targets_adapter)",
  "class AvalancheConcatDataset(AvalancheDataset[T_co, TTargetType]):\n    \"\"\"\n    A Dataset that behaves like a PyTorch\n    :class:`torch.utils.data.ConcatDataset`. However, this Dataset also supports\n    transformations, slicing, advanced indexing and the targets field and all\n    the other goodies listed in :class:`AvalancheDataset`.\n\n    This dataset guarantees that the operations involving the transformations\n    and transformations groups are consistent across the concatenated dataset\n    (if they are subclasses of :class:`AvalancheDataset`).\n    \"\"\"\n    def __init__(self,\n                 datasets: Collection[SupportedDataset],\n                 *,\n                 transform: Callable[[Any], Any] = None,\n                 target_transform: Callable[[int], int] = None,\n                 transform_groups: Dict[str, Tuple[XTransform,\n                                                   YTransform]] = None,\n                 initial_transform_group: str = None,\n                 task_labels: Union[int, Sequence[int],\n                                    Sequence[Sequence[int]]] = None,\n                 targets: Union[Sequence[TTargetType],\n                                Sequence[Sequence[TTargetType]]] = None,\n                 dataset_type: AvalancheDatasetType = None,\n                 collate_fn: Callable[[List], Any] = None,\n                 targets_adapter: Callable[[Any], TTargetType] = None):\n        \"\"\"\n        Creates a ``AvalancheConcatDataset`` instance.\n\n        :param datasets: A collection of datasets.\n        :param transform: A function/transform that takes the X value of a\n            pattern from the original dataset and returns a transformed version.\n        :param target_transform: A function/transform that takes in the target\n            and transforms it.\n        :param transform_groups: A dictionary containing the transform groups.\n            Transform groups are used to quickly switch between training and\n            eval (test) transformations. This becomes useful when in need to\n            test on the training dataset as test transformations usually don't\n            contain random augmentations. ``AvalancheDataset`` natively supports\n            the 'train' and 'eval' groups by calling the ``train()`` and\n            ``eval()`` methods. When using custom groups one can use the\n            ``with_transforms(group_name)`` method instead. Defaults to None,\n            which means that the current transforms will be used to\n            handle both 'train' and 'eval' groups (just like in standard\n            ``torchvision`` datasets).\n        :param initial_transform_group: The name of the initial transform group\n            to be used. Defaults to None, which means that if all\n            AvalancheDatasets in the input datasets list agree on a common\n            group (the \"current group\" is the same for all datasets), then that\n            group will be used as the initial one. If the list of input datasets\n            does not contain an AvalancheDataset or if the AvalancheDatasets\n            do not agree on a common group, then 'train' will be used.\n        :param targets: The label of each pattern. Can either be a sequence of\n            labels or, alternatively, a sequence containing sequences of labels\n            (one for each dataset to be concatenated). Defaults to None, which\n            means that the targets will be retrieved from the datasets (if\n            possible).\n        :param task_labels: The task labels for each pattern. Must be a sequence\n            of ints, one for each pattern in the dataset. Alternatively, task\n            labels can be expressed as a sequence containing sequences of ints\n            (one for each dataset to be concatenated) or even a single int,\n            in which case that value will be used as the task label for all\n            instances. Defaults to None, which means that the dataset will try\n            to obtain the task labels from the original datasets. If no task\n            labels could be found for a dataset, a default task label \"0\" will\n            be applied to all patterns of that dataset.\n        :param dataset_type: The type of the dataset. Defaults to None,\n            which means that the type will be inferred from the list of\n            input datasets. When `dataset_type` is None and the list of datasets\n            contains incompatible types, an error will be raised.\n            A list of datasets is compatible if they all have\n            the same type. Datasets that are not instances of `AvalancheDataset`\n            and instances of `AvalancheDataset` with type `UNDEFINED`\n            are always compatible with other types.\n            When the `dataset_type` is different than UNDEFINED, a\n            proper value for `collate_fn` and `targets_adapter` will be set.\n            If the `dataset_type` is different than UNDEFINED, then\n            `collate_fn` and `targets_adapter` must not be set.\n        :param collate_fn: The function to use when slicing to merge single\n            patterns. In the future this function may become the function\n            used in the data loading process, too. If None, the constructor\n            will check if a `collate_fn` field exists in the first dataset. If\n            no such field exists, the default collate function will be used.\n            Beware that the chosen collate function will be applied to all\n            the concatenated datasets even if a different collate is defined\n            in different datasets.\n        :param targets_adapter: A function used to convert the values of the\n            targets field. Defaults to None. Note: the adapter will not change\n            the value of the second element returned by `__getitem__`.\n            The adapter is used to adapt the values of the targets field only.\n        \"\"\"\n        dataset_list = list(datasets)\n\n        dataset_type, collate_fn, targets_adapter = \\\n            self._get_dataset_type_collate_and_adapter(\n                dataset_list, dataset_type, collate_fn, targets_adapter)\n\n        self._dataset_list = dataset_list\n        self._datasets_lengths = [len(dataset) for dataset in dataset_list]\n        self._datasets_cumulative_lengths = ConcatDataset.cumsum(dataset_list)\n        self._overall_length = sum(self._datasets_lengths)\n\n        if initial_transform_group is None:\n            uniform_group = None\n            for d_set in self._dataset_list:\n                if isinstance(d_set, AvalancheDataset):\n                    if uniform_group is None:\n                        uniform_group = d_set.current_transform_group\n                    else:\n                        if uniform_group != d_set.current_transform_group:\n                            uniform_group = None\n                            break\n\n            if uniform_group is None:\n                initial_transform_group = 'train'\n            else:\n                initial_transform_group = uniform_group\n\n        if task_labels is not None:\n            task_labels = self._concat_task_labels(task_labels)\n\n        if targets is not None:\n            targets = self._concat_targets(targets)\n\n        self._adapt_concat_datasets()\n\n        super().__init__(ClassificationDataset(),  # not used\n                         transform=transform,\n                         target_transform=target_transform,\n                         transform_groups=transform_groups,\n                         initial_transform_group=initial_transform_group,\n                         task_labels=task_labels,\n                         targets=targets,\n                         dataset_type=dataset_type,\n                         collate_fn=collate_fn,\n                         targets_adapter=targets_adapter)\n\n    def _get_dataset_type_collate_and_adapter(\n            self, datasets, dataset_type, collate_fn, targets_adapter):\n\n        if dataset_type is not None:\n            return dataset_type, collate_fn, targets_adapter\n\n        identified_types = set()\n        first_collate_fn = None\n\n        for dataset in datasets:\n            if isinstance(dataset, AvalancheDataset):\n                if dataset.dataset_type != AvalancheDatasetType.UNDEFINED:\n                    identified_types.add(dataset.dataset_type)\n\n            if first_collate_fn is None:\n                first_collate_fn = getattr(dataset, 'collate_fn', None)\n\n        if len(identified_types) > 1:\n            raise ValueError(\n                'Error trying to infer a common dataset type while '\n                'concatenating different datasets. '\n                'Incompatible types: {}'.format(list(identified_types)))\n        elif len(identified_types) == 0:\n            dataset_type = AvalancheDatasetType.UNDEFINED\n        else:\n            # len(identified_types) == 1\n            dataset_type = next(iter(identified_types))\n\n        if dataset_type != AvalancheDatasetType.UNDEFINED and \\\n                (collate_fn is not None or targets_adapter is not None):\n            raise ValueError(\n                'dataset_type {} was inferred from the list of '\n                'concatenated dataset. This dataset type can\\'t be used '\n                'with custom collate_fn or targets_adapter '\n                'parameters. Only the UNDEFINED type supports '\n                'custom collate_fn or targets_adapter '\n                'parameters.'.format(dataset_type))\n\n        if collate_fn is None and \\\n                dataset_type == AvalancheDatasetType.UNDEFINED:\n            collate_fn = first_collate_fn\n\n        return dataset_type, collate_fn, targets_adapter\n\n    def __len__(self) -> int:\n        return self._overall_length\n\n    def _get_single_item(self, idx: int):\n        dataset_idx, internal_idx = find_list_from_index(\n            idx, self._datasets_lengths, self._overall_length,\n            cumulative_sizes=self._datasets_cumulative_lengths)\n\n        single_element = self._dataset_list[dataset_idx][internal_idx]\n\n        return self._process_pattern(single_element, idx)\n\n    def _fork_dataset(self: TAvalancheDataset) -> TAvalancheDataset:\n        dataset_copy = super()._fork_dataset()\n\n        dataset_copy._dataset_list = list(dataset_copy._dataset_list)\n        # Note: there is no need to duplicate _datasets_lengths\n\n        return dataset_copy\n\n    def _initialize_targets_sequence(\n            self, dataset, targets, dataset_type, targets_adapter) -> \\\n            Sequence[TTargetType]:\n        if targets is not None:\n            if len(targets) != self._overall_length:\n                raise ValueError(\n                    'Invalid amount of target labels. It must be equal to the '\n                    'number of patterns in the dataset. Got {}, expected '\n                    '{}!'.format(len(targets), self._overall_length))\n\n            return targets\n\n        targets_list = []\n        # Could be easily done with a single line of code\n        # This however, allows the user to better check which was the\n        # problematic dataset by using a debugger.\n        for dataset_idx, single_dataset in enumerate(self._dataset_list):\n            targets_list.append(super()._initialize_targets_sequence(\n                single_dataset, None, dataset_type, targets_adapter\n            ))\n\n        return LazyConcatTargets(targets_list)\n\n    def _initialize_task_labels_sequence(\n            self, dataset, task_labels: Optional[Sequence[int]]) \\\n            -> Sequence[int]:\n        if task_labels is not None:\n            # task_labels has priority over the dataset fields\n\n            if isinstance(task_labels, int):\n                return ConstantSequence(task_labels, self._overall_length)\n            elif len(task_labels) != self._overall_length:\n                raise ValueError(\n                    'Invalid amount of task labels. It must be equal to the '\n                    'number of patterns in the dataset. Got {}, expected '\n                    '{}!'.format(len(task_labels), self._overall_length))\n            return SubSequence(task_labels, converter=int)\n\n        concat_t_labels = []\n        for dataset_idx, single_dataset in enumerate(self._dataset_list):\n            concat_t_labels.append(super()._initialize_task_labels_sequence(\n                single_dataset, None))\n\n        return LazyConcatTargets(concat_t_labels)\n\n    def _initialize_collate_fn(self, dataset, dataset_type, collate_fn):\n        if collate_fn is not None:\n            return collate_fn\n\n        if len(self._dataset_list) > 0 and \\\n                hasattr(self._dataset_list[0], 'collate_fn'):\n            return getattr(self._dataset_list[0], 'collate_fn')\n        return default_collate\n\n    def _set_original_dataset_transform_group(\n            self, group_name: str) -> None:\n        for dataset_idx, dataset in enumerate(self._dataset_list):\n            if isinstance(dataset, AvalancheDataset):\n                if dataset.current_transform_group == group_name:\n                    # Prevents a huge slowdown in some corner cases\n                    # (apart from being actually more performant)\n                    continue\n\n                self._dataset_list[dataset_idx] = \\\n                    dataset.with_transforms(group_name)\n\n    def _freeze_original_dataset(\n            self, group_name: str) -> None:\n        for dataset_idx, dataset in enumerate(self._dataset_list):\n            if isinstance(dataset, AvalancheDataset):\n                self._dataset_list[dataset_idx] = \\\n                    dataset.freeze_group_transforms(group_name)\n\n    def _replace_original_dataset_group(\n            self, transform: XTransform, target_transform: YTransform) -> None:\n        for dataset_idx, dataset in enumerate(self._dataset_list):\n            if isinstance(dataset, AvalancheDataset):\n                self._dataset_list[dataset_idx] = \\\n                    dataset.replace_transforms(transform, target_transform)\n\n    def _add_original_dataset_group(\n            self, group_name: str) -> None:\n        for dataset_idx, dataset in enumerate(self._dataset_list):\n            if isinstance(dataset, AvalancheDataset):\n                self._dataset_list[dataset_idx] = \\\n                    dataset.add_transforms_group(group_name, None, None)\n\n    def _add_groups_from_original_dataset(\n            self, dataset, transform_groups) -> None:\n        for dataset_idx, dataset in enumerate(self._dataset_list):\n            if isinstance(dataset, AvalancheDataset):\n                for original_dataset_group in dataset.transform_groups.keys():\n                    if original_dataset_group not in transform_groups:\n                        transform_groups[original_dataset_group] = (None, None)\n\n    def _adapt_concat_datasets(self):\n        all_groups = set()\n\n        for dataset in self._dataset_list:\n            if isinstance(dataset, AvalancheDataset):\n                all_groups.update(dataset.transform_groups.keys())\n\n        for dataset_idx, dataset in enumerate(self._dataset_list):\n            if isinstance(dataset, AvalancheDataset):\n                for group_name in all_groups:\n                    if group_name not in dataset.transform_groups:\n                        self._dataset_list[dataset_idx] = \\\n                            dataset.add_transforms_group(group_name, None, None)\n\n    @staticmethod\n    def _concat_task_labels(task_labels: Union[int, Sequence[int],\n                                               Sequence[Sequence[int]]]):\n        if isinstance(task_labels, int):\n            # A single value has been passed -> use it for all instances\n            # The value is returned as is because it's already managed when in\n            # this form (in _initialize_task_labels_sequence).\n            return task_labels\n        elif isinstance(task_labels[0], int):\n            # Flat list of task labels -> just return it.\n            # The constructor will check if it has the correct size.\n            return task_labels\n        else:\n            # One list for each dataset, concat them.\n            return LazyConcatIntTargets(task_labels)\n\n    @staticmethod\n    def _concat_targets(targets: Union[Sequence[TTargetType],\n                                       Sequence[Sequence[TTargetType]]]):\n        if isinstance(targets[0], Sequence):\n            return LazyConcatTargets(targets)\n        else:\n            return targets\n\n    def _flatten_dataset(self):\n        # Flattens this subset by borrowing the list of concatenated datasets\n        # from the original datasets (if they're 'AvalancheConcatSubset's or\n        # PyTorch 'ConcatDataset's)\n\n        flattened_list = []\n        for dataset in self._dataset_list:\n            if isinstance(dataset, AvalancheConcatDataset):\n                if dataset._has_own_transformations():\n                    # Can't flatten as the dataset has custom transformations\n                    flattened_list.append(dataset)\n                else:\n                    flattened_list.extend(dataset._dataset_list)\n\n            # PyTorch ConcatDataset doesn't have custom transformations\n            # --------\n            # Flattening PyTorch ConcatDatasets has been temporarily\n            # disabled as the semantic of transformation groups collide\n            # with the flattening process: PyTorch ConcatDataset doesn't have\n            # transform groups and flattening it will expose the underlying\n            # concatenated datasets list, which may contain 'AvalancheDataset's.\n            # --------\n            # elif isinstance(dataset, ConcatDataset):\n            #    flattened_list.extend(dataset.datasets)\n            elif isinstance(dataset, AvalancheSubset):\n                flattened_list += self._flatten_subset_concat_branch(dataset)\n            else:\n                flattened_list.append(dataset)\n\n        self._dataset_list = flattened_list\n        self._datasets_lengths = [len(dataset) for dataset in flattened_list]\n        self._datasets_cumulative_lengths = ConcatDataset.cumsum(flattened_list)\n        self._overall_length = sum(self._datasets_lengths)\n\n    def _flatten_subset_concat_branch(self, dataset: AvalancheSubset) \\\n            -> List[Dataset]:\n        \"\"\"\n        Optimizes the dataset hierarchy in the corner case:\n\n        self -> [Subset, Subset, ] -> ConcatDataset -> [Dataset]\n\n        :param dataset: The dataset. This function returns [dataset] if the\n            dataset is not a subset containing a concat dataset (or if other\n            corner cases are encountered).\n        :return: The flattened list of datasets to be concatenated.\n        \"\"\"\n        if not isinstance(dataset._original_dataset, AvalancheConcatDataset):\n            return [dataset]\n\n        concat_dataset: AvalancheConcatDataset = dataset._original_dataset\n        if concat_dataset._has_own_transformations():\n            # The dataset has custom transforms -> do nothing\n            return [dataset]\n\n        result: List[AvalancheSubset] = []\n        last_c_dataset = None\n        last_c_idxs = []\n        last_c_targets = []\n        last_c_tasks = []\n        for subset_idx, idx in enumerate(dataset._indices):\n            dataset_idx, internal_idx = find_list_from_index(\n                idx, concat_dataset._datasets_lengths,\n                concat_dataset._overall_length,\n                cumulative_sizes=concat_dataset._datasets_cumulative_lengths)\n\n            if last_c_dataset is None:\n                last_c_dataset = dataset_idx\n            elif last_c_dataset != dataset_idx:\n                # Consolidate current subset\n                result.append(AvalancheConcatDataset._make_similar_subset(\n                    dataset, concat_dataset._dataset_list[last_c_dataset],\n                    last_c_idxs, last_c_targets, last_c_tasks))\n\n                # Switch to next dataset\n                last_c_dataset = dataset_idx\n                last_c_idxs = []\n                last_c_targets = []\n                last_c_tasks = []\n\n            last_c_idxs.append(internal_idx)\n            last_c_targets.append(dataset.targets[subset_idx])\n            last_c_tasks.append(dataset.targets_task_labels[subset_idx])\n\n        if last_c_dataset is not None:\n            result.append(AvalancheConcatDataset._make_similar_subset(\n                dataset, concat_dataset._dataset_list[last_c_dataset],\n                last_c_idxs, last_c_targets, last_c_tasks))\n\n        return result\n\n    @staticmethod\n    def _make_similar_subset(subset, ref_dataset, indices, targets, tasks):\n        t_groups = dict()\n        f_groups = dict()\n        AvalancheDataset._borrow_transformations(\n            subset, t_groups, f_groups)\n\n        collate_fn = None\n        if subset.dataset_type == AvalancheDatasetType.UNDEFINED:\n            collate_fn = subset.collate_fn\n\n        result = AvalancheSubset(\n            ref_dataset,\n            indices=indices,\n            class_mapping=subset._class_mapping,\n            transform_groups=t_groups,\n            initial_transform_group=subset.current_transform_group,\n            task_labels=tasks,\n            targets=targets,\n            dataset_type=subset.dataset_type,\n            collate_fn=collate_fn,\n        )\n\n        result._frozen_transforms = f_groups\n        return result",
  "def concat_datasets_sequentially(\n        train_dataset_list: Sequence[ISupportedClassificationDataset],\n        test_dataset_list: Sequence[ISupportedClassificationDataset]) -> \\\n        Tuple[AvalancheConcatDataset,\n              AvalancheConcatDataset,\n              List[list]]:\n    \"\"\"\n    Concatenates a list of datasets. This is completely different from\n    :class:`ConcatDataset`, in which datasets are merged together without\n    other processing. Instead, this function re-maps the datasets class IDs.\n    For instance:\n    let the dataset[0] contain patterns of 3 different classes,\n    let the dataset[1] contain patterns of 2 different classes, then class IDs\n    will be mapped as follows:\n\n    dataset[0] class \"0\" -> new class ID is \"0\"\n\n    dataset[0] class \"1\" -> new class ID is \"1\"\n\n    dataset[0] class \"2\" -> new class ID is \"2\"\n\n    dataset[1] class \"0\" -> new class ID is \"3\"\n\n    dataset[1] class \"1\" -> new class ID is \"4\"\n\n    ... -> ...\n\n    dataset[-1] class \"C-1\" -> new class ID is \"overall_n_classes-1\"\n\n    In contrast, using PyTorch ConcatDataset:\n\n    dataset[0] class \"0\" -> ID is \"0\"\n\n    dataset[0] class \"1\" -> ID is \"1\"\n\n    dataset[0] class \"2\" -> ID is \"2\"\n\n    dataset[1] class \"0\" -> ID is \"0\"\n\n    dataset[1] class \"1\" -> ID is \"1\"\n\n    Note: ``train_dataset_list`` and ``test_dataset_list`` must have the same\n    number of datasets.\n\n    :param train_dataset_list: A list of training datasets\n    :param test_dataset_list: A list of test datasets\n\n    :returns: A concatenated dataset.\n    \"\"\"\n    remapped_train_datasets = []\n    remapped_test_datasets = []\n    next_remapped_idx = 0\n\n    # Obtain the number of classes of each dataset\n    classes_per_dataset = [\n        _count_unique(train_dataset_list[dataset_idx].targets,\n                      test_dataset_list[dataset_idx].targets)\n        for dataset_idx in range(len(train_dataset_list))]\n\n    new_class_ids_per_dataset = []\n    for dataset_idx in range(len(train_dataset_list)):\n        # The class IDs for this dataset will be in range\n        # [n_classes_in_previous_datasets,\n        #       n_classes_in_previous_datasets + classes_in_this_dataset)\n        class_mapping = list(\n            range(next_remapped_idx,\n                  next_remapped_idx + classes_per_dataset[dataset_idx]))\n        new_class_ids_per_dataset.append(class_mapping)\n\n        train_set = train_dataset_list[dataset_idx]\n        test_set = test_dataset_list[dataset_idx]\n\n        # AvalancheSubset is used to apply the class IDs transformation.\n        # Remember, the class_mapping parameter must be a list in which:\n        # new_class_id = class_mapping[original_class_id]\n        remapped_train_datasets.append(\n            AvalancheSubset(train_set, class_mapping=class_mapping))\n        remapped_test_datasets.append(\n            AvalancheSubset(test_set, class_mapping=class_mapping))\n        next_remapped_idx += classes_per_dataset[dataset_idx]\n\n    return (AvalancheConcatDataset(remapped_train_datasets),\n            AvalancheConcatDataset(remapped_test_datasets),\n            new_class_ids_per_dataset)",
  "def as_avalanche_dataset(\n        dataset: ISupportedClassificationDataset[T_co],\n        dataset_type: AvalancheDatasetType = None) \\\n        -> AvalancheDataset[T_co, TTargetType]:\n    if isinstance(dataset, AvalancheDataset) and dataset_type is None:\n        # There is no need to show the warning\n        return dataset\n\n    if dataset_type is None:\n        warnings.warn('\"as_avalanche_dataset\" was called without setting '\n                      '\"dataset_type\": this behaviour is deprecated. Consider '\n                      'setting this value or calling the specific functions '\n                      '\"as_classification_dataset\", \"as_regression_dataset\", '\n                      '\"as_segmentation_dataset\" or \"as_undefined_dataset\"')\n        dataset_type = AvalancheDatasetType.UNDEFINED\n\n    if isinstance(dataset, AvalancheDataset) and \\\n            dataset.dataset_type == dataset_type:\n        return dataset\n\n    return AvalancheDataset(dataset, dataset_type=dataset_type)",
  "def as_classification_dataset(dataset: ISupportedClassificationDataset[T_co]) \\\n        -> AvalancheDataset[T_co, int]:\n    return as_avalanche_dataset(\n        dataset, dataset_type=AvalancheDatasetType.CLASSIFICATION)",
  "def as_regression_dataset(dataset: ISupportedClassificationDataset[T_co]) \\\n        -> AvalancheDataset[T_co, Any]:\n    return as_avalanche_dataset(\n        dataset, dataset_type=AvalancheDatasetType.REGRESSION)",
  "def as_segmentation_dataset(dataset: ISupportedClassificationDataset[T_co]) \\\n        -> AvalancheDataset[T_co, Any]:\n    return as_avalanche_dataset(\n        dataset, dataset_type=AvalancheDatasetType.SEGMENTATION)",
  "def as_undefined_dataset(dataset: ISupportedClassificationDataset[T_co]) \\\n        -> AvalancheDataset[T_co, Any]:\n    return as_avalanche_dataset(\n        dataset, dataset_type=AvalancheDatasetType.UNDEFINED)",
  "def train_eval_avalanche_datasets(\n        train_dataset: ISupportedClassificationDataset,\n        test_dataset: ISupportedClassificationDataset,\n        train_transformation, eval_transformation,\n        dataset_type=None):\n    train = AvalancheDataset(\n        train_dataset,\n        transform_groups=dict(train=(train_transformation, None),\n                              eval=(eval_transformation, None)),\n        initial_transform_group='train',\n        dataset_type=dataset_type)\n\n    test = AvalancheDataset(\n        test_dataset,\n        transform_groups=dict(train=(train_transformation, None),\n                              eval=(eval_transformation, None)),\n        initial_transform_group='eval',\n        dataset_type=dataset_type)\n    return train, test",
  "def _traverse_supported_dataset(\n        dataset, values_selector: Callable[[Dataset, List[int]], List],\n        indices=None) -> List:\n    initial_error = None\n    try:\n        result = values_selector(dataset, indices)\n        if result is not None:\n            return result\n    except BaseException as e:\n        initial_error = e\n\n    if isinstance(dataset, Subset):\n        if indices is None:\n            indices = range(len(dataset))\n        indices = [dataset.indices[x] for x in indices]\n        return list(_traverse_supported_dataset(\n            dataset.dataset, values_selector, indices))\n\n    if isinstance(dataset, ConcatDataset):\n        result = []\n        if indices is None:\n            for c_dataset in dataset.datasets:\n                result += list(_traverse_supported_dataset(\n                    c_dataset, values_selector, indices))\n            return result\n\n        datasets_to_indexes = defaultdict(list)\n        indexes_to_dataset = []\n        datasets_len = []\n        recursion_result = []\n\n        all_size = 0\n        for c_dataset in dataset.datasets:\n            len_dataset = len(c_dataset)\n            datasets_len.append(len_dataset)\n            all_size += len_dataset\n\n        for subset_idx in indices:\n            dataset_idx, pattern_idx = \\\n                find_list_from_index(subset_idx, datasets_len, all_size)\n            datasets_to_indexes[dataset_idx].append(pattern_idx)\n            indexes_to_dataset.append(dataset_idx)\n\n        for dataset_idx, c_dataset in enumerate(dataset.datasets):\n            recursion_result.append(deque(_traverse_supported_dataset(\n                c_dataset, values_selector, datasets_to_indexes[dataset_idx])))\n\n        result = []\n        for idx in range(len(indices)):\n            dataset_idx = indexes_to_dataset[idx]\n            result.append(recursion_result[dataset_idx].popleft())\n\n        return result\n\n    if initial_error is not None:\n        raise initial_error\n\n    raise ValueError('Error: can\\'t find the needed data in the given dataset')",
  "def _count_unique(*sequences: Sequence[SupportsInt]):\n    uniques = set()\n\n    for seq in sequences:\n        for x in seq:\n            uniques.add(int(x))\n\n    return len(uniques)",
  "def _select_targets(dataset, indices):\n    if hasattr(dataset, 'targets'):\n        # Standard supported dataset\n        found_targets = dataset.targets\n    elif hasattr(dataset, 'tensors'):\n        # Support for PyTorch TensorDataset\n        if len(dataset.tensors) < 2:\n            raise ValueError('Tensor dataset has not enough tensors: '\n                             'at least 2 are required.')\n        found_targets = dataset.tensors[1]\n    else:\n        raise ValueError(\n            'Unsupported dataset: must have a valid targets field '\n            'or has to be a Tensor Dataset with at least 2 '\n            'Tensors')\n\n    if indices is not None:\n        found_targets = SubSequence(found_targets, indices=indices)\n\n    return found_targets",
  "def _select_task_labels(dataset, indices):\n    found_task_labels = None\n    if hasattr(dataset, 'targets_task_labels'):\n        found_task_labels = dataset.targets_task_labels\n\n    if found_task_labels is None:\n        if isinstance(dataset, (Subset, ConcatDataset)):\n            return None  # Continue traversing\n\n    if found_task_labels is None:\n        if indices is None:\n            return ConstantSequence(0, len(dataset))\n        return ConstantSequence(0, len(indices))\n\n    if indices is not None:\n        found_task_labels = SubSequence(found_task_labels, indices=indices)\n\n    return found_task_labels",
  "def _make_target_from_supported_dataset(\n        dataset: SupportedDataset,\n        converter: Callable[[Any], TTargetType] = None) -> \\\n        Sequence[TTargetType]:\n    if isinstance(dataset, AvalancheDataset):\n        if converter is None:\n            return dataset.targets\n        elif isinstance(dataset.targets,\n                        (SubSequence, LazyConcatTargets)) and \\\n                dataset.targets.converter == converter:\n            return dataset.targets\n        elif isinstance(dataset.targets, LazyClassMapping) and converter == int:\n            # LazyClassMapping already outputs int targets\n            return dataset.targets\n\n    targets = _traverse_supported_dataset(dataset, _select_targets)\n\n    return SubSequence(targets, converter=converter)",
  "def _make_task_labels_from_supported_dataset(dataset: SupportedDataset) -> \\\n        Sequence[int]:\n    if isinstance(dataset, AvalancheDataset):\n        return dataset.targets_task_labels\n\n    task_labels = _traverse_supported_dataset(dataset, _select_task_labels)\n\n    return SubSequence(task_labels, converter=int)",
  "def __init__(self,\n                 dataset: SupportedDataset,\n                 *,\n                 transform: XTransform = None,\n                 target_transform: YTransform = None,\n                 transform_groups: Dict[str, Tuple[XTransform,\n                                                   YTransform]] = None,\n                 initial_transform_group: str = None,\n                 task_labels: Union[int, Sequence[int]] = None,\n                 targets: Sequence[TTargetType] = None,\n                 dataset_type: AvalancheDatasetType = None,\n                 collate_fn: Callable[[List], Any] = None,\n                 targets_adapter: Callable[[Any], TTargetType] = None):\n        \"\"\"\n        Creates a ``AvalancheDataset`` instance.\n\n        :param dataset: The dataset to decorate. Beware that\n            AvalancheDataset will not overwrite transformations already\n            applied by this dataset.\n        :param transform: A function/transform that takes the X value of a\n            pattern from the original dataset and returns a transformed version.\n        :param target_transform: A function/transform that takes in the target\n            and transforms it.\n        :param transform_groups: A dictionary containing the transform groups.\n            Transform groups are used to quickly switch between training and\n            eval (test) transformations. This becomes useful when in need to\n            test on the training dataset as test transformations usually don't\n            contain random augmentations. ``AvalancheDataset`` natively supports\n            the 'train' and 'eval' groups by calling the ``train()`` and\n            ``eval()`` methods. When using custom groups one can use the\n            ``with_transforms(group_name)`` method instead. Defaults to None,\n            which means that the current transforms will be used to\n            handle both 'train' and 'eval' groups (just like in standard\n            ``torchvision`` datasets).\n        :param initial_transform_group: The name of the initial transform group\n            to be used. Defaults to None, which means that the current group of\n            the input dataset will be used (if an AvalancheDataset). If the\n            input dataset is not an AvalancheDataset, then 'train' will be\n            used.\n        :param task_labels: The task label of each instance. Must be a sequence\n            of ints, one for each instance in the dataset. Alternatively can be\n            a single int value, in which case that value will be used as the\n            task label for all the instances. Defaults to None, which means that\n            the dataset will try to obtain the task labels from the original\n            dataset. If no task labels could be found, a default task label\n            \"0\" will be applied to all instances.\n        :param targets: The label of each pattern. Defaults to None, which\n            means that the targets will be retrieved from the dataset (if\n            possible).\n        :param dataset_type: The type of the dataset. Defaults to None,\n            which means that the type will be inferred from the input dataset.\n            When the `dataset_type` is different than UNDEFINED, a\n            proper value for `collate_fn` and `targets_adapter` will be set.\n            If the `dataset_type` is different than UNDEFINED, then\n            `collate_fn` and `targets_adapter` must not be set.\n        :param collate_fn: The function to use when slicing to merge single\n            patterns. In the future this function may become the function\n            used in the data loading process, too. If None and the\n            `dataset_type` is UNDEFINED, the constructor will check if a\n            `collate_fn` field exists in the dataset. If no such field exists,\n            the default collate function will be used.\n        :param targets_adapter: A function used to convert the values of the\n            targets field. Defaults to None. Note: the adapter will not change\n            the value of the second element returned by `__getitem__`.\n            The adapter is used to adapt the values of the targets field only.\n        \"\"\"\n        super().__init__()\n\n        if transform_groups is not None and (\n                transform is not None or target_transform is not None):\n            raise ValueError('transform_groups can\\'t be used with transform'\n                             'and target_transform values')\n\n        detected_type = False\n        if dataset_type is None:\n            detected_type = True\n            if isinstance(dataset, AvalancheDataset):\n                dataset_type = dataset.dataset_type\n            else:\n                dataset_type = AvalancheDatasetType.UNDEFINED\n\n        if dataset_type != AvalancheDatasetType.UNDEFINED and \\\n                (collate_fn is not None or targets_adapter is not None):\n            if detected_type:\n                raise ValueError(\n                    'dataset_type {} was inferred from the input dataset. '\n                    'This dataset type can\\'t be used '\n                    'with custom collate_fn or targets_adapter '\n                    'parameters. Only the UNDEFINED type supports '\n                    'custom collate_fn or targets_adapter '\n                    'parameters'.format(dataset_type))\n            else:\n                raise ValueError(\n                    'dataset_type {} can\\'t be used with custom collate_fn '\n                    'or targets_adapter. Only the UNDEFINED type supports '\n                    'custom collate_fn or targets_adapter '\n                    'parameters.'.format(dataset_type))\n\n        if transform_groups is not None:\n            AvalancheDataset._check_groups_dict_format(transform_groups)\n\n        if not isinstance(dataset_type, AvalancheDatasetType):\n            raise ValueError('dataset_type must be a value of type '\n                             'AvalancheDatasetType')\n\n        self._dataset: SupportedDataset = dataset\n        \"\"\"\n        The original dataset.\n        \"\"\"\n\n        self.dataset_type = dataset_type\n        \"\"\"\n        The type of this dataset (UNDEFINED, CLASSIFICATION, ...).\n        \"\"\"\n\n        self.targets: Sequence[TTargetType] = self._initialize_targets_sequence(\n            dataset, targets, dataset_type, targets_adapter)\n        \"\"\"\n        A sequence of values describing the label of each pattern contained in\n        the dataset.\n        \"\"\"\n\n        self.targets_task_labels: Sequence[int] = \\\n            self._initialize_task_labels_sequence(dataset, task_labels)\n        \"\"\"\n        A sequence of ints describing the task label of each pattern contained \n        in the dataset.\n        \"\"\"\n\n        self.tasks_pattern_indices: Dict[int, Sequence[int]] = \\\n            self._initialize_tasks_dict(dataset, self.targets_task_labels)\n        \"\"\"\n        A dictionary mapping task labels to the indices of the patterns with \n        that task label. If you need to obtain the subset of patterns labeled\n        with a certain task label, consider using the `task_set` field.\n        \"\"\"\n\n        self.collate_fn = self._initialize_collate_fn(\n            dataset, dataset_type, collate_fn)\n        \"\"\"\n        The collate function to use when creating mini-batches from this\n        dataset.\n        \"\"\"\n\n        # Compress targets and task labels to save some memory\n        self._optimize_targets()\n        self._optimize_task_labels()\n        self._optimize_task_dict()\n\n        self.task_set = _TaskSubsetDict(self)\n        \"\"\"\n        A dictionary that can be used to obtain the subset of patterns given\n        a specific task label.\n        \"\"\"\n\n        if initial_transform_group is None:\n            # Detect from the input dataset. If not an AvalancheDataset then\n            # use 'train' as the initial transform group\n            if isinstance(dataset, AvalancheDataset):\n                initial_transform_group = dataset.current_transform_group\n            else:\n                initial_transform_group = 'train'\n\n        self.current_transform_group = initial_transform_group\n        \"\"\"\n        The name of the transform group currently in use.\n        \"\"\"\n\n        self.transform_groups: Dict[str, Tuple[XTransform, YTransform]] = \\\n            self._initialize_groups_dict(transform_groups, dataset,\n                                         transform, target_transform)\n        \"\"\"\n        A dictionary containing the transform groups. Transform groups are\n        used to quickly switch between training and test (eval) transformations.\n        This becomes useful when in need to test on the training dataset as test\n        transformations usually don't contain random augmentations.\n\n        AvalancheDataset natively supports switching between the 'train' and\n        'eval' groups by calling the ``train()`` and ``eval()`` methods. When\n        using custom groups one can use the ``with_transforms(group_name)``\n        method instead.\n\n        May be null, which means that the current transforms will be used to\n        handle both 'train' and 'eval' groups.\n        \"\"\"\n\n        if self.current_transform_group not in self.transform_groups:\n            raise ValueError('Invalid transformations group ' +\n                             str(self.current_transform_group))\n        t_group = self.transform_groups[self.current_transform_group]\n\n        self.transform: XTransform = t_group[0]\n        \"\"\"\n        A function/transform that takes in an PIL image and returns a \n        transformed version.\n        \"\"\"\n\n        self.target_transform: YTransform = t_group[1]\n        \"\"\"\n        A function/transform that takes in the target and transforms it.\n        \"\"\"\n\n        self._frozen_transforms: \\\n            Dict[str, Tuple[XTransform, YTransform]] = dict()\n        \"\"\"\n        A dictionary containing frozen transformations.\n        \"\"\"\n\n        for group_name in self.transform_groups.keys():\n            self._frozen_transforms[group_name] = (None, None)\n\n        self._set_original_dataset_transform_group(self.current_transform_group)\n\n        self._flatten_dataset()",
  "def __add__(self, other: Dataset) -> 'AvalancheDataset':\n        return AvalancheConcatDataset([self, other])",
  "def __radd__(self, other: Dataset) -> 'AvalancheDataset':\n        return AvalancheConcatDataset([other, self])",
  "def __getitem__(self, idx) -> Union[T_co, Sequence[T_co]]:\n        return TupleTLabel(manage_advanced_indexing(\n            idx, self._get_single_item, len(self), self.collate_fn))",
  "def __len__(self):\n        return len(self._dataset)",
  "def train(self):\n        \"\"\"\n        Returns a new dataset with the transformations of the 'train' group\n        loaded.\n\n        The current dataset will not be affected.\n\n        :return: A new dataset with the training transformations loaded.\n        \"\"\"\n        return self.with_transforms('train')",
  "def eval(self):\n        \"\"\"\n        Returns a new dataset with the transformations of the 'eval' group\n        loaded.\n\n        Eval transformations usually don't contain augmentation procedures.\n        This function may be useful when in need to test on training data\n        (for instance, in order to run a validation pass).\n\n        The current dataset will not be affected.\n\n        :return: A new dataset with the eval transformations loaded.\n        \"\"\"\n        return self.with_transforms('eval')",
  "def freeze_transforms(self: TAvalancheDataset) -> \\\n            TAvalancheDataset:\n        \"\"\"\n        Returns a new dataset where the current transformations are frozen.\n\n        Frozen transformations will be permanently glued to the original\n        dataset so that they can't be changed anymore. This is usually done\n        when using transformations to create derived datasets: in this way\n        freezing the transformations will ensure that the user won't be able\n        to inadvertently change them by directly setting the transformations\n        field or by using the other transformations utility methods like\n        ``replace_transforms``. Please note that transformations of all groups\n        will be frozen. If you want to freeze a specific group, please use\n        ``freeze_group_transforms``.\n\n        The current dataset will not be affected.\n\n        :return: A new dataset with the current transformations frozen.\n        \"\"\"\n\n        dataset_copy = self._fork_dataset()\n\n        for group_name in dataset_copy.transform_groups.keys():\n            AvalancheDataset._freeze_dataset_group(dataset_copy,\n                                                   group_name)\n\n        return dataset_copy",
  "def freeze_group_transforms(self: TAvalancheDataset,\n                                group_name: str) -> TAvalancheDataset:\n        \"\"\"\n        Returns a new dataset where the transformations for a specific group\n        are frozen.\n\n        Frozen transformations will be permanently glued to the original\n        dataset so that they can't be changed anymore. This is usually done\n        when using transformations to create derived datasets: in this way\n        freezing the transformations will ensure that the user won't be able\n        to inadvertently change them by directly setting the transformations\n        field or by using the other transformations utility methods like\n        ``replace_transforms``. To freeze transformations of all groups\n        please use ``freeze_transforms``.\n\n        The current dataset will not be affected.\n\n        :return: A new dataset with the transformations frozen for the given\n            group.\n        \"\"\"\n        dataset_copy = self._fork_dataset()\n\n        AvalancheDataset._freeze_dataset_group(dataset_copy, group_name)\n\n        return dataset_copy",
  "def get_transforms(\n            self: TAvalancheDataset,\n            transforms_group: str = None) -> Tuple[Any, Any]:\n        \"\"\"\n        Returns the transformations given a group.\n\n        Beware that this will not return the frozen transformations, nor the\n        ones included in the wrapped dataset. Only transformations directly\n        attached to this dataset will be returned.\n\n        :param transforms_group: The transformations group. Defaults to None,\n            which means that the current group is returned.\n        :return: The transformation group, as a tuple\n            (transform, target_transform).\n        \"\"\"\n        if transforms_group is None:\n            transforms_group = self.current_transform_group\n\n        if transforms_group == self.current_transform_group:\n            return self.transform, self.target_transform\n\n        return self.transform_groups[transforms_group]",
  "def add_transforms(\n            self: TAvalancheDataset,\n            transform: Callable[[Any], Any] = None,\n            target_transform: Callable[[int], int] = None) -> \\\n            TAvalancheDataset:\n        \"\"\"\n        Returns a new dataset with the given transformations added to\n        the existing ones.\n\n        The transformations will be added to the current transformations group.\n        Other transformation groups will not be affected.\n\n        The given transformations will be added \"at the end\" of previous\n        transformations of the current transformations group. This means\n        that existing transformations will be applied to the patterns first.\n\n        The current dataset will not be affected.\n\n        :param transform: A function/transform that takes the X value of a\n            pattern from the original dataset and returns a transformed version.\n        :param target_transform: A function/transform that takes in the target\n            and transforms it.\n        :return: A new dataset with the added transformations.\n        \"\"\"\n\n        dataset_copy = self._fork_dataset()\n\n        if transform is not None:\n            if dataset_copy.transform is not None:\n                dataset_copy.transform = Compose([\n                    dataset_copy.transform, transform])\n            else:\n                dataset_copy.transform = transform\n\n        if target_transform is not None:\n            if dataset_copy.target_transform is not None:\n                dataset_copy.target_transform = Compose([\n                    dataset_copy.target_transform, target_transform])\n            else:\n                dataset_copy.target_transform = transform\n\n        return dataset_copy",
  "def add_transforms_to_group(\n            self: TAvalancheDataset,\n            group_name: str,\n            transform: Callable[[Any], Any] = None,\n            target_transform: Callable[[int], int] = None) -> \\\n            TAvalancheDataset:\n        \"\"\"\n        Returns a new dataset with the given transformations added to\n        the existing ones for a certain group.\n\n        The transformations will be added to the given transformations group.\n        Other transformation groups will not be affected. The group must\n        already exist.\n\n        The given transformations will be added \"at the end\" of previous\n        transformations of that group. This means that existing transformations\n        will be applied to the patterns first.\n\n        The current dataset will not be affected.\n\n        :param group_name: The name of the group.\n        :param transform: A function/transform that takes the X value of a\n            pattern from the original dataset and returns a transformed version.\n        :param target_transform: A function/transform that takes in the target\n            and transforms it.\n        :return: A new dataset with the added transformations.\n        \"\"\"\n\n        if self.current_transform_group == group_name:\n            return self.add_transforms(transform, target_transform)\n\n        if group_name not in self.transform_groups:\n            raise ValueError('Invalid group name ' + str(group_name))\n\n        dataset_copy = self._fork_dataset()\n\n        t_group: List[XTransform, YTransform] = \\\n            list(dataset_copy.transform_groups[group_name])\n        if transform is not None:\n            if t_group[0] is not None:\n                t_group[0] = Compose([t_group[0], transform])\n            else:\n                t_group[0] = transform\n\n        if target_transform is not None:\n            if t_group[1] is not None:\n                t_group[1] = Compose([t_group[1], target_transform])\n            else:\n                t_group[1] = transform\n\n        # tuple(t_group) works too, but it triggers a type warning\n        tuple_t_group: Tuple[XTransform, YTransform] = tuple(\n            (t_group[0], t_group[1]))\n        dataset_copy.transform_groups[group_name] = tuple_t_group\n\n        return dataset_copy",
  "def replace_transforms(\n            self: TAvalancheDataset,\n            transform: XTransform,\n            target_transform: YTransform,\n            group: str = None) -> TAvalancheDataset:\n        \"\"\"\n        Returns a new dataset with the existing transformations replaced with\n        the given ones.\n\n        The given transformations will replace the ones of the current\n        transformations group. Other transformation groups will not be affected.\n\n        If the original dataset is an instance of :class:`AvalancheDataset`,\n        then transformations of the original set will be considered as well\n        (the original dataset will be left untouched).\n\n        The current dataset will not be affected.\n\n        Note that this function will not override frozen transformations. This\n        will also not affect transformations found in datasets that are not\n        instances of :class:`AvalancheDataset`.\n\n        :param transform: A function/transform that takes the X value of a\n            pattern from the original dataset and returns a transformed version.\n        :param target_transform: A function/transform that takes in the target\n            and transforms it.\n        :param group: The transforms group to replace. Defaults to None, which\n            means that the current group will be replaced.\n        :return: A new dataset with the new transformations.\n        \"\"\"\n\n        if group is None:\n            group = self.current_transform_group\n\n        dataset_copy = self._fork_dataset().with_transforms(group)\n        dataset_copy._replace_original_dataset_group(None, None)\n\n        dataset_copy.transform = transform\n        dataset_copy.target_transform = target_transform\n\n        return dataset_copy.with_transforms(self.current_transform_group)",
  "def with_transforms(self: TAvalancheDataset, group_name: str) -> \\\n            TAvalancheDataset:\n        \"\"\"\n        Returns a new dataset with the transformations of a different group\n        loaded.\n\n        The current dataset will not be affected.\n\n        :param group_name: The name of the transformations group to use.\n        :return: A new dataset with the new transformations.\n        \"\"\"\n        dataset_copy = self._fork_dataset()\n\n        if group_name not in dataset_copy.transform_groups:\n            raise ValueError('Invalid group name ' + str(group_name))\n\n        # Store current group (loaded in transform and target_transform fields)\n        dataset_copy.transform_groups[dataset_copy.current_transform_group] = \\\n            (dataset_copy.transform, dataset_copy.target_transform)\n\n        # Load new group in transform and target_transform fields\n        switch_group = dataset_copy.transform_groups[group_name]\n        dataset_copy.transform = switch_group[0]\n        dataset_copy.target_transform = switch_group[1]\n        dataset_copy.current_transform_group = group_name\n\n        # Finally, align the underlying dataset\n        dataset_copy._set_original_dataset_transform_group(group_name)\n\n        return dataset_copy",
  "def add_transforms_group(self: TAvalancheDataset,\n                             group_name: str,\n                             transform: XTransform,\n                             target_transform: YTransform) -> \\\n            TAvalancheDataset:\n        \"\"\"\n        Returns a new dataset with a new transformations group.\n\n        The current dataset will not be affected.\n\n        This method raises an exception if a group with the same name already\n        exists.\n\n        :param group_name: The name of the new transformations group.\n        :param transform: A function/transform that takes the X value of a\n            pattern from the original dataset and returns a transformed version.\n        :param target_transform: A function/transform that takes in the target\n            and transforms it.\n        :return: A new dataset with the new transformations.\n        \"\"\"\n        dataset_copy = self._fork_dataset()\n\n        if group_name in dataset_copy.transform_groups:\n            raise ValueError('A group with the same name already exists')\n\n        dataset_copy.transform_groups[group_name] = \\\n            (transform, target_transform)\n\n        AvalancheDataset._check_groups_dict_format(\n            dataset_copy.transform_groups)\n\n        dataset_copy._frozen_transforms[group_name] = (None, None)\n\n        # Finally, align the underlying dataset\n        dataset_copy._add_original_dataset_group(group_name)\n\n        return dataset_copy",
  "def _fork_dataset(self: TAvalancheDataset) -> TAvalancheDataset:\n        dataset_copy = copy.copy(self)\n        dataset_copy._frozen_transforms = dict(dataset_copy._frozen_transforms)\n        dataset_copy.transform_groups = dict(dataset_copy.transform_groups)\n\n        return dataset_copy",
  "def _freeze_dataset_group(dataset_copy: TAvalancheDataset,\n                              group_name: str):\n        # Freeze the current transformations. Frozen transformations are saved\n        # in a separate dict.\n\n        # This may rise an error if no group with the given name exists!\n        frozen_group = dataset_copy._frozen_transforms[group_name]\n\n        final_frozen_transform = frozen_group[0]\n        final_frozen_target_transform = frozen_group[1]\n\n        is_current_group = dataset_copy.current_transform_group == group_name\n\n        # If the required group is not the current one, just freeze the ones\n        # found in dataset_copy.transform_groups).\n        to_be_glued = dataset_copy.transform_groups[group_name]\n\n        # Now that transformations are stored in to_be_glued,\n        # we can safely reset them in the transform_groups dictionary.\n        dataset_copy.transform_groups[group_name] = (None, None)\n\n        if is_current_group:\n            # If the required group is the current one, use the transformations\n            # already found in transform and target_transform fields (because\n            # the ones stored in dataset_copy.transform_groups may be not\n            # up-to-date).\n\n            to_be_glued = (dataset_copy.transform,\n                           dataset_copy.target_transform)\n\n            # And of course, once frozen, set transformations to None\n            dataset_copy.transform = None\n            dataset_copy.target_transform = None\n\n        if to_be_glued[0] is not None:\n            if frozen_group[0] is None:\n                final_frozen_transform = to_be_glued[0]\n            else:\n                final_frozen_transform = Compose([\n                    frozen_group[0], to_be_glued[0]])\n\n        if to_be_glued[1] is not None:\n            if frozen_group[1] is None:\n                final_frozen_target_transform = to_be_glued[1]\n            else:\n                final_frozen_target_transform = Compose([\n                    frozen_group[1], to_be_glued[1]])\n\n        # Set the frozen transformations\n        dataset_copy._frozen_transforms[group_name] = (\n            final_frozen_transform, final_frozen_target_transform)\n\n        # Finally, apply the freeze procedure to the original dataset\n        dataset_copy._freeze_original_dataset(group_name)",
  "def _get_single_item(self, idx: int):\n        return self._process_pattern(self._dataset[idx], idx)",
  "def _process_pattern(self, element: Tuple, idx: int):\n        has_task_label = isinstance(element, TupleTLabel)\n        if has_task_label:\n            element = element[:-1]\n\n        pattern = element[0]\n        label = element[1]\n\n        pattern, label = self._apply_transforms(pattern, label)\n\n        return TupleTLabel((pattern, label, *element[2:],\n                            self.targets_task_labels[idx]))",
  "def _apply_transforms(self, pattern: Any, label: int):\n        frozen_group = self._frozen_transforms[self.current_transform_group]\n        if frozen_group[0] is not None:\n            pattern = frozen_group[0](pattern)\n\n        if self.transform is not None:\n            pattern = self.transform(pattern)\n\n        if frozen_group[1] is not None:\n            label = frozen_group[1](label)\n\n        if self.target_transform is not None:\n            label = self.target_transform(label)\n\n        return pattern, label",
  "def _check_groups_dict_format(groups_dict):\n        # The original groups_dict must be convertible to native Python dict\n        groups_dict = dict(groups_dict)\n\n        # Check if the format of the groups is correct\n        for map_key in groups_dict:\n            if not isinstance(map_key, str):\n                raise ValueError('Every group must be identified by a string.'\n                                 'Wrong key was: \"' + str(map_key) + '\"')\n\n            map_value = groups_dict[map_key]\n            if not isinstance(map_value, tuple):\n                raise ValueError('Transformations for group \"' + str(map_key) +\n                                 '\" must be contained in a tuple')\n\n            if not len(map_value) == 2:\n                raise ValueError(\n                    'Transformations for group \"' + str(map_key) + '\" must be '\n                    'a tuple containing 2 elements: a transformation for the X '\n                    'values and a transformation for the Y values')\n\n        if 'test' in groups_dict:\n            warnings.warn(\n                'A transformation group named \"test\" has been found. Beware '\n                'that by default AvalancheDataset supports test transformations'\n                ' through the \"eval\" group. Consider using that one!')",
  "def _initialize_groups_dict(\n            self,\n            transform_groups: Optional[Dict[str, Tuple[XTransform,\n                                                       YTransform]]],\n            dataset: Any,\n            transform: XTransform,\n            target_transform: YTransform) -> Dict[str, Tuple[XTransform,\n                                                             YTransform]]:\n        \"\"\"\n        A simple helper method that tries to fill the 'train' and 'eval'\n        groups as those two groups must always exist.\n\n        If no transform_groups are passed to the class constructor, then\n        the transform and target_transform parameters are used for both groups.\n\n        If train transformations are set and eval transformations are not, then\n        train transformations will be used for the eval group.\n\n        :param dataset: The original dataset. Will be used to detect existing\n            groups.\n        :param transform: The transformation passed as a parameter to the\n            class constructor.\n        :param target_transform: The target transformation passed as a parameter\n            to the class constructor.\n        \"\"\"\n        if transform_groups is None:\n            transform_groups = {\n                'train': (transform, target_transform),\n                'eval': (transform, target_transform)\n            }\n        else:\n            transform_groups = dict(transform_groups)\n\n        if 'train' in transform_groups:\n            if 'eval' not in transform_groups:\n                transform_groups['eval'] = transform_groups['train']\n\n        if 'train' not in transform_groups:\n            transform_groups['train'] = (None, None)\n\n        if 'eval' not in transform_groups:\n            transform_groups['eval'] = (None, None)\n\n        self._add_groups_from_original_dataset(dataset, transform_groups)\n\n        return transform_groups",
  "def _initialize_targets_sequence(self, dataset, targets,\n                                     dataset_type, targets_adapter) \\\n            -> Sequence[TTargetType]:\n        if targets is not None:\n            # User defined targets always take precedence\n            # Note: no adapter is applied!\n            if len(targets) != len(dataset):\n                raise ValueError(\n                    'Invalid amount of target labels. It must be equal to the '\n                    'number of patterns in the dataset. Got {}, expected '\n                    '{}!'.format(len(targets), len(dataset)))\n            return targets\n\n        if targets_adapter is None:\n            if dataset_type == AvalancheDatasetType.CLASSIFICATION:\n                targets_adapter = int\n            else:\n                targets_adapter = None\n        return _make_target_from_supported_dataset(dataset, targets_adapter)",
  "def _initialize_task_labels_sequence(\n            self, dataset, task_labels: Optional[Sequence[int]]) \\\n            -> Sequence[int]:\n        if task_labels is not None:\n            # task_labels has priority over the dataset fields\n            if isinstance(task_labels, int):\n                task_labels = ConstantSequence(task_labels, len(dataset))\n            elif len(task_labels) != len(dataset):\n                raise ValueError(\n                    'Invalid amount of task labels. It must be equal to the '\n                    'number of patterns in the dataset. Got {}, expected '\n                    '{}!'.format(len(task_labels), len(dataset)))\n\n            return SubSequence(task_labels, converter=int)\n\n        return _make_task_labels_from_supported_dataset(dataset)",
  "def _initialize_collate_fn(self, dataset, dataset_type, collate_fn):\n        if collate_fn is not None:\n            return collate_fn\n\n        if dataset_type == AvalancheDatasetType.UNDEFINED:\n            if hasattr(dataset, 'collate_fn'):\n                return getattr(dataset, 'collate_fn')\n\n        return default_collate",
  "def _initialize_tasks_dict(self, dataset, task_labels: Sequence[int]) \\\n            -> Dict[int, Sequence[int]]:\n        if isinstance(task_labels, ConstantSequence) and len(task_labels) > 0:\n            # Shortcut :)\n            return {task_labels[0]: range(len(task_labels))}\n\n        result = dict()\n        for i, x in enumerate(task_labels):\n            if x not in result:\n                result[x] = []\n            result[x].append(i)\n\n        if len(result) == 1:\n            result[next(iter(result.keys()))] = range(len(task_labels))\n\n        return result",
  "def _set_original_dataset_transform_group(\n            self, group_name: str) -> None:\n        if isinstance(self._dataset, AvalancheDataset):\n            if self._dataset.current_transform_group == group_name:\n                # Prevents a huge slowdown in some corner cases\n                # (apart from being actually more performant)\n                return\n\n            self._dataset = self._dataset.with_transforms(group_name)",
  "def _freeze_original_dataset(\n            self, group_name: str) -> None:\n        if isinstance(self._dataset, AvalancheDataset):\n            self._dataset = self._dataset.freeze_group_transforms(group_name)",
  "def _replace_original_dataset_group(\n            self, transform: XTransform, target_transform: YTransform) -> None:\n        if isinstance(self._dataset, AvalancheDataset):\n            self._dataset = self._dataset.replace_transforms(\n                transform, target_transform)",
  "def _add_original_dataset_group(\n            self, group_name: str) -> None:\n        if isinstance(self._dataset, AvalancheDataset):\n            self._dataset = self._dataset.add_transforms_group(\n                group_name, None, None)",
  "def _add_groups_from_original_dataset(\n            self, dataset, transform_groups) -> None:\n        if isinstance(dataset, AvalancheDataset):\n            for original_dataset_group in dataset.transform_groups.keys():\n                if original_dataset_group not in transform_groups:\n                    transform_groups[original_dataset_group] = (None, None)",
  "def _has_own_transformations(self):\n        # Used to check if the current dataset has its own transformations\n        # This method returns False if transformations are applied\n        # by the wrapped dataset only.\n\n        if self.transform is not None:\n            return True\n\n        if self.target_transform is not None:\n            return True\n\n        for transform_group in self.transform_groups.values():\n            for transform in transform_group:\n                if transform is not None:\n                    return True\n\n        for transform_group in self._frozen_transforms.values():\n            for transform in transform_group:\n                if transform is not None:\n                    return True\n\n        return False",
  "def _borrow_transformations(dataset, transform_groups,\n                                frozen_transform_groups):\n        if not isinstance(dataset, AvalancheDataset):\n            return\n\n        for original_dataset_group in dataset.transform_groups.keys():\n            if original_dataset_group not in transform_groups:\n                transform_groups[original_dataset_group] = (None, None)\n\n        for original_dataset_group in dataset._frozen_transforms.keys():\n            if original_dataset_group not in frozen_transform_groups:\n                frozen_transform_groups[original_dataset_group] = (None, None)\n\n        # Standard transforms\n        for original_dataset_group in dataset.transform_groups.keys():\n            other_dataset_transforms = \\\n                dataset.transform_groups[original_dataset_group]\n            if dataset.current_transform_group == original_dataset_group:\n                other_dataset_transforms = (dataset.transform,\n                                            dataset.target_transform)\n\n            transform_groups[original_dataset_group] = \\\n                AvalancheDataset._prepend_transforms(\n                    transform_groups[original_dataset_group],\n                    other_dataset_transforms)\n\n        # Frozen transforms\n        for original_dataset_group in dataset._frozen_transforms.keys():\n            other_dataset_transforms = \\\n                dataset._frozen_transforms[original_dataset_group]\n\n            frozen_transform_groups[original_dataset_group] = \\\n                AvalancheDataset._prepend_transforms(\n                    frozen_transform_groups[original_dataset_group],\n                    other_dataset_transforms)",
  "def _prepend_transforms(transforms, to_be_prepended):\n        if len(transforms) != 2:\n            raise ValueError(\n                'Transformation groups must contain exactly 2 transformations')\n\n        if len(transforms) != len(to_be_prepended):\n            raise ValueError(\n                'Transformation group size mismatch: {} vs {}'.format(\n                    len(transforms), len(to_be_prepended)\n                ))\n\n        result = []\n\n        for i in range(len(transforms)):\n            if to_be_prepended[i] is None:\n                # Nothing to prepend\n                result.append(transforms[i])\n            elif transforms[i] is None:\n                result.append(to_be_prepended[i])\n            else:\n                result.append(Compose([\n                    to_be_prepended[i], transforms[i]]))\n\n        return tuple(result)",
  "def _optimize_targets(self):\n        self.targets = optimize_sequence(self.targets)",
  "def _optimize_task_labels(self):\n        self.targets_task_labels = optimize_sequence(self.targets_task_labels)",
  "def _optimize_task_dict(self):\n        for task_label in self.tasks_pattern_indices:\n            self.tasks_pattern_indices[task_label] = optimize_sequence(\n                self.tasks_pattern_indices[task_label])",
  "def _flatten_dataset(self):\n        pass",
  "def __init__(self, avalanche_dataset: AvalancheDataset):\n        self._full_dataset = avalanche_dataset\n        task_ids = self._full_dataset.tasks_pattern_indices.keys()\n        task_ids = sorted(list(task_ids))\n        base_dict = OrderedDict()\n        for x in task_ids:\n            base_dict[x] = x\n        super().__init__(base_dict)",
  "def __getitem__(self, task_id: int):\n        if task_id not in self._full_dataset.tasks_pattern_indices:\n            raise KeyError('No pattern with ' + str(task_id) + ' found')\n        pattern_indices = self._full_dataset.tasks_pattern_indices[task_id]\n        return self._make_subset(pattern_indices)",
  "def or_empty(self, task_id: int):\n        try:\n            return self[task_id]\n        except KeyError:\n            return self._make_subset([])",
  "def _make_subset(self, indices: Sequence[int]):\n        return AvalancheSubset(self._full_dataset, indices=indices)",
  "def __init__(self,\n                 dataset: SupportedDataset,\n                 indices: Sequence[int] = None,\n                 *,\n                 class_mapping: Sequence[int] = None,\n                 transform: Callable[[Any], Any] = None,\n                 target_transform: Callable[[int], int] = None,\n                 transform_groups: Dict[str, Tuple[XTransform,\n                                                   YTransform]] = None,\n                 initial_transform_group: str = None,\n                 task_labels: Union[int, Sequence[int]] = None,\n                 targets: Sequence[TTargetType] = None,\n                 dataset_type: AvalancheDatasetType = None,\n                 collate_fn: Callable[[List], Any] = None,\n                 targets_adapter: Callable[[Any], TTargetType] = None):\n        \"\"\"\n        Creates an ``AvalancheSubset`` instance.\n\n        :param dataset: The whole dataset.\n        :param indices: Indices in the whole set selected for subset. Can\n            be None, which means that the whole dataset will be returned.\n        :param class_mapping: A list that, for each possible target (Y) value,\n            contains its corresponding remapped value. Can be None.\n            Beware that setting this parameter will force the final\n            dataset type to be CLASSIFICATION or UNDEFINED.\n        :param transform: A function/transform that takes the X value of a\n            pattern from the original dataset and returns a transformed version.\n        :param target_transform: A function/transform that takes in the target\n            and transforms it.\n        :param transform_groups: A dictionary containing the transform groups.\n            Transform groups are used to quickly switch between training and\n            eval (test) transformations. This becomes useful when in need to\n            test on the training dataset as test transformations usually don't\n            contain random augmentations. ``AvalancheDataset`` natively supports\n            the 'train' and 'eval' groups by calling the ``train()`` and\n            ``eval()`` methods. When using custom groups one can use the\n            ``with_transforms(group_name)`` method instead. Defaults to None,\n            which means that the current transforms will be used to\n            handle both 'train' and 'eval' groups (just like in standard\n            ``torchvision`` datasets).\n        :param initial_transform_group: The name of the initial transform group\n            to be used. Defaults to None, which means that the current group of\n            the input dataset will be used (if an AvalancheDataset). If the\n            input dataset is not an AvalancheDataset, then 'train' will be\n            used.\n        :param task_labels: The task label for each instance. Must be a sequence\n            of ints, one for each instance in the dataset. This can either be a\n            list of task labels for the original dataset or the list of task\n            labels for the instances of the subset (an automatic detection will\n            be made). In the unfortunate case in which the original dataset and\n            the subset contain the same amount of instances, then this parameter\n            is considered to contain the task labels of the subset.\n            Alternatively can be a single int value, in which case\n            that value will be used as the task label for all the instances.\n            Defaults to None, which means that the dataset will try to\n            obtain the task labels from the original dataset. If no task labels\n            could be found, a default task label \"0\" will be applied to all\n            instances.\n        :param targets: The label of each pattern. Defaults to None, which\n            means that the targets will be retrieved from the dataset (if\n            possible). This can either be a list of target labels for the\n            original dataset or the list of target labels for the instances of\n            the subset (an automatic detection will be made). In the unfortunate\n            case in which the original dataset and the subset contain the same\n            amount of instances, then this parameter is considered to contain\n            the target labels of the subset.\n        :param dataset_type: The type of the dataset. Defaults to None,\n            which means that the type will be inferred from the input dataset.\n            When the `dataset_type` is different than UNDEFINED, a\n            proper value for `collate_fn` and `targets_adapter` will be set.\n            If the `dataset_type` is different than UNDEFINED, then\n            `collate_fn` and `targets_adapter` must not be set.\n            The only exception to this rule regards `class_mapping`.\n            If `class_mapping` is set, the final dataset_type\n            (as set by this parameter or detected from the subset) must be\n            CLASSIFICATION or UNDEFINED.\n        :param collate_fn: The function to use when slicing to merge single\n            patterns. In the future this function may become the function\n            used in the data loading process, too. If None and the\n            `dataset_type` is UNDEFINED, the constructor will check if a\n            `collate_fn` field exists in the dataset. If no such field exists,\n            the default collate function will be used.\n        :param targets_adapter: A function used to convert the values of the\n            targets field. Defaults to None. Note: the adapter will not change\n            the value of the second element returned by `__getitem__`.\n            The adapter is used to adapt the values of the targets field only.\n        \"\"\"\n\n        detected_type = False\n        if dataset_type is None:\n            detected_type = True\n            if isinstance(dataset, AvalancheDataset):\n                dataset_type = dataset.dataset_type\n                if dataset_type == AvalancheDatasetType.UNDEFINED:\n                    if collate_fn is None:\n                        collate_fn = dataset.collate_fn\n            else:\n                dataset_type = AvalancheDatasetType.UNDEFINED\n\n        if dataset_type != AvalancheDatasetType.UNDEFINED and \\\n                (collate_fn is not None or targets_adapter is not None):\n            if detected_type:\n                raise ValueError(\n                    'dataset_type {} was inferred from the input dataset. '\n                    'This dataset type can\\'t be used '\n                    'with custom collate_fn or targets_adapter '\n                    'parameters. Only the UNDEFINED type supports '\n                    'custom collate_fn or targets_adapter '\n                    'parameters'.format(dataset_type))\n            else:\n                raise ValueError(\n                    'dataset_type {} can\\'t be used with custom collate_fn '\n                    'or targets_adapter. Only the UNDEFINED type supports '\n                    'custom collate_fn or targets_adapter '\n                    'parameters.'.format(dataset_type))\n\n        if class_mapping is not None:\n            if dataset_type not in [AvalancheDatasetType.CLASSIFICATION,\n                                    AvalancheDatasetType.UNDEFINED]:\n                raise ValueError('class_mapping is defined but the dataset type'\n                                 ' is neither CLASSIFICATION or UNDEFINED.')\n\n        if class_mapping is not None:\n            subset = ClassificationSubset(dataset, indices=indices,\n                                          class_mapping=class_mapping)\n        elif indices is not None:\n            subset = Subset(dataset, indices=indices)\n        else:\n            subset = dataset  # Exactly like a plain AvalancheDataset\n\n        self._original_dataset = dataset\n        # self._indices and self._class_mapping currently not used apart from\n        # initialization procedures\n        self._class_mapping = class_mapping\n        self._indices = indices\n\n        if initial_transform_group is None:\n            if isinstance(dataset, AvalancheDataset):\n                initial_transform_group = dataset.current_transform_group\n            else:\n                initial_transform_group = 'train'\n\n        super().__init__(subset,\n                         transform=transform,\n                         target_transform=target_transform,\n                         transform_groups=transform_groups,\n                         initial_transform_group=initial_transform_group,\n                         task_labels=task_labels,\n                         targets=targets,\n                         dataset_type=dataset_type,\n                         collate_fn=collate_fn,\n                         targets_adapter=targets_adapter)",
  "def _initialize_targets_sequence(\n            self, dataset, targets, dataset_type, targets_adapter) \\\n            -> Sequence[TTargetType]:\n        if targets is not None:\n            # For the reasoning behind this, have a look at\n            # _initialize_task_labels_sequence (it's basically the same).\n\n            if len(targets) == len(self._original_dataset) and \\\n                    not len(targets) == len(dataset):\n                return SubSequence(targets, indices=self._indices)\n            elif len(targets) == len(dataset):\n                return targets\n            else:\n                raise ValueError(\n                    'Invalid amount of targets. It must be equal to the '\n                    'number of patterns in the subset. '\n                    'Got {}, expected {}!'.format(\n                        len(targets), len(dataset)))\n\n        return super()._initialize_targets_sequence(\n            dataset, None, dataset_type, targets_adapter)",
  "def _initialize_task_labels_sequence(\n            self, dataset,\n            task_labels: Optional[Sequence[int]]) -> Sequence[int]:\n\n        if task_labels is not None:\n            # The task_labels parameter is kind of ambiguous...\n            # it may either be the list of task labels of the required subset\n            # or it may be the list of task labels of the original dataset.\n            # Simple solution: check the length of task_labels!\n            # However, if task_labels, the original dataset and this subset have\n            # the same size, then task_labels is considered to contain the task\n            # labels for the subset!\n\n            if isinstance(task_labels, int):\n                # Simplest case: constant task label\n                return ConstantSequence(task_labels, len(dataset))\n            elif len(task_labels) == len(self._original_dataset) and \\\n                    not len(task_labels) == len(dataset):\n                # task_labels refers to the original dataset ...\n                return SubSequence(task_labels, indices=self._indices,\n                                   converter=int)\n            elif len(task_labels) == len(dataset):\n                # One label for each instance\n                return SubSequence(task_labels, converter=int)\n            else:\n                raise ValueError(\n                    'Invalid amount of task labels. It must be equal to the '\n                    'number of patterns in the subset. '\n                    'Got {}, expected {}!'.format(\n                        len(task_labels), len(dataset)))\n\n        return super()._initialize_task_labels_sequence(dataset, None)",
  "def _set_original_dataset_transform_group(self, group_name: str) -> None:\n        if isinstance(self._original_dataset, AvalancheDataset):\n            if self._original_dataset.current_transform_group == group_name:\n                # Prevents a huge slowdown in some corner cases\n                # (apart from being actually more performant)\n                return\n\n            self._original_dataset = \\\n                self._original_dataset.with_transforms(group_name)\n\n            self._replace_original_dataset_reference()",
  "def _freeze_original_dataset(self, group_name: str) -> None:\n        if isinstance(self._original_dataset, AvalancheDataset):\n            self._original_dataset = \\\n                self._original_dataset.freeze_group_transforms(group_name)\n\n            self._replace_original_dataset_reference()",
  "def _replace_original_dataset_group(\n            self, transform: XTransform, target_transform: YTransform) -> None:\n        if isinstance(self._original_dataset, AvalancheDataset):\n            self._original_dataset = \\\n                self._original_dataset.replace_transforms(\n                    transform, target_transform)\n\n            self._replace_original_dataset_reference()",
  "def _add_original_dataset_group(self, group_name: str) -> None:\n        if isinstance(self._original_dataset, AvalancheDataset):\n            self._original_dataset = \\\n                self._original_dataset.add_transforms_group(\n                    group_name, None, None)\n\n            self._replace_original_dataset_reference()",
  "def _add_groups_from_original_dataset(\n            self, dataset, transform_groups) -> None:\n        if isinstance(self._original_dataset, AvalancheDataset):\n            for original_dataset_group in \\\n                    self._original_dataset.transform_groups.keys():\n                if original_dataset_group not in transform_groups:\n                    transform_groups[original_dataset_group] = (None, None)",
  "def _flatten_dataset(self):\n        # Flattens this subset by borrowing indices and class mappings from\n        # the original dataset (if it's an AvalancheSubset or PyTorch Subset)\n\n        if isinstance(self._original_dataset, AvalancheSubset):\n            # In order to flatten the subset, we have to integrate the\n            # transformations (also frozen ones!)\n            AvalancheDataset._borrow_transformations(\n                self._original_dataset, self.transform_groups,\n                self._frozen_transforms)\n\n            # We need to reload transformations after borrowing from the subset\n            # This assumes that _flatten_dataset is called by __init__!\n            self.transform, self.target_transform = \\\n                self.transform_groups[self.current_transform_group]\n\n            forward_dataset = self._original_dataset._original_dataset\n            forward_indices = self._original_dataset._indices\n            forward_class_mapping = self._original_dataset._class_mapping\n\n            if self._class_mapping is not None:\n\n                if forward_class_mapping is not None:\n\n                    new_class_mapping = []\n                    for mapped_class in forward_class_mapping:\n                        # -1 is sometimes used to mark unused classes\n                        # shouldn't be a problem (if it is, is not our fault)\n                        if mapped_class == -1:\n                            forward_mapped = -1\n                        else:\n                            # forward_mapped may be -1\n                            forward_mapped = self._class_mapping[mapped_class]\n\n                        new_class_mapping.append(forward_mapped)\n                else:\n                    new_class_mapping = self._class_mapping\n            else:\n                new_class_mapping = forward_class_mapping  # May be None\n\n            if self._indices is not None:\n                if forward_indices is not None:\n                    new_indices = [forward_indices[x] for x in self._indices]\n                else:\n                    new_indices = self._indices\n            else:\n                new_indices = forward_indices  # May be None\n\n            self._original_dataset = forward_dataset\n            self._indices = new_indices\n            self._class_mapping = new_class_mapping\n            if new_class_mapping is None:\n                # Subset\n                self._dataset = Subset(forward_dataset,\n                                       indices=new_indices)\n            else:\n                # ClassificationSubset\n                self._dataset = ClassificationSubset(\n                    forward_dataset, indices=new_indices,\n                    class_mapping=new_class_mapping)",
  "def _replace_original_dataset_reference(self):\n        if isinstance(self._dataset, ClassificationSubset):\n            self._dataset = ClassificationSubset(\n                self._original_dataset, indices=self._indices,\n                class_mapping=self._class_mapping)\n        elif isinstance(self._dataset, Subset):\n            self._dataset = Subset(self._original_dataset,\n                                   indices=self._indices)\n        else:\n            self._dataset = self._original_dataset",
  "def __init__(self,\n                 *dataset_tensors: Sequence,\n                 transform: Callable[[Any], Any] = None,\n                 target_transform: Callable[[int], int] = None,\n                 transform_groups: Dict[str, Tuple[XTransform,\n                                                   YTransform]] = None,\n                 initial_transform_group: str = 'train',\n                 task_labels: Union[int, Sequence[int]] = None,\n                 targets: Union[Sequence[TTargetType], int] = None,\n                 dataset_type: AvalancheDatasetType =\n                 AvalancheDatasetType.UNDEFINED,\n                 collate_fn: Callable[[List], Any] = None,\n                 targets_adapter: Callable[[Any], TTargetType] = None):\n        \"\"\"\n        Creates a ``AvalancheTensorDataset`` instance.\n\n        :param dataset_tensors: Sequences, Tensors or ndarrays representing the\n            content of the dataset.\n        :param transform: A function/transform that takes in a single element\n            from the first tensor and returns a transformed version.\n        :param target_transform: A function/transform that takes a single\n            element of the second tensor and transforms it.\n        :param transform_groups: A dictionary containing the transform groups.\n            Transform groups are used to quickly switch between training and\n            eval (test) transformations. This becomes useful when in need to\n            test on the training dataset as test transformations usually don't\n            contain random augmentations. ``AvalancheDataset`` natively supports\n            the 'train' and 'eval' groups by calling the ``train()`` and\n            ``eval()`` methods. When using custom groups one can use the\n            ``with_transforms(group_name)`` method instead. Defaults to None,\n            which means that the current transforms will be used to\n            handle both 'train' and 'eval' groups (just like in standard\n            ``torchvision`` datasets).\n        :param initial_transform_group: The name of the transform group\n            to be used. Defaults to 'train'.\n        :param task_labels: The task labels for each pattern. Must be a sequence\n            of ints, one for each pattern in the dataset. Alternatively can be a\n            single int value, in which case that value will be used as the task\n            label for all the instances. Defaults to None, which means that a\n            default task label \"0\" will be applied to all patterns.\n        :param targets: The label of each pattern. Defaults to None, which\n            means that the targets will be retrieved from the dataset.\n            Otherwise, can be 1) a sequence of values containing as many\n            elements as the number of patterns, or 2) the index of the sequence\n            to use as the targets field. When using the default value of None,\n            the targets field will be populated using the second\n            tensor. If dataset is made of only one tensor, then that tensor will\n            be used for the targets field, too.\n        :param dataset_type: The type of the dataset. Defaults to UNDEFINED.\n            Setting this parameter will automatically set a proper value for\n            `collate_fn` and `targets_adapter`. If this parameter is set to a\n            value different from UNDEFINED then `collate_fn` and\n            `targets_adapter` must not be set.\n        :param collate_fn: The function to use when slicing to merge single\n            patterns. In the future this function may become the function\n            used in the data loading process, too.\n        :param targets_adapter: A function used to convert the values of the\n            targets field. Defaults to None. Note: the adapter will not change\n            the value of the second element returned by `__getitem__`.\n            The adapter is used to adapt the values of the targets field only.\n        \"\"\"\n\n        if dataset_type != AvalancheDatasetType.UNDEFINED and \\\n                (collate_fn is not None or targets_adapter is not None):\n            raise ValueError(\n                'dataset_type {} can\\'t be used with custom collate_fn '\n                'or targets_adapter. Only the UNDEFINED type supports '\n                'custom collate_fn or targets_adapter '\n                'parameters.'.format(dataset_type))\n\n        if len(dataset_tensors) < 1:\n            raise ValueError('At least one sequence must be passed')\n\n        if targets is None:\n            targets = min(1, len(dataset_tensors))\n\n        if isinstance(targets, int):\n            base_dataset = SequenceDataset(*dataset_tensors, targets=targets)\n            targets = None\n        else:\n            base_dataset = SequenceDataset(*dataset_tensors,\n                                           targets=min(1, len(dataset_tensors)))\n\n        super().__init__(base_dataset,\n                         transform=transform,\n                         target_transform=target_transform,\n                         transform_groups=transform_groups,\n                         initial_transform_group=initial_transform_group,\n                         task_labels=task_labels,\n                         targets=targets,\n                         dataset_type=dataset_type,\n                         collate_fn=collate_fn,\n                         targets_adapter=targets_adapter)",
  "def __init__(self,\n                 datasets: Collection[SupportedDataset],\n                 *,\n                 transform: Callable[[Any], Any] = None,\n                 target_transform: Callable[[int], int] = None,\n                 transform_groups: Dict[str, Tuple[XTransform,\n                                                   YTransform]] = None,\n                 initial_transform_group: str = None,\n                 task_labels: Union[int, Sequence[int],\n                                    Sequence[Sequence[int]]] = None,\n                 targets: Union[Sequence[TTargetType],\n                                Sequence[Sequence[TTargetType]]] = None,\n                 dataset_type: AvalancheDatasetType = None,\n                 collate_fn: Callable[[List], Any] = None,\n                 targets_adapter: Callable[[Any], TTargetType] = None):\n        \"\"\"\n        Creates a ``AvalancheConcatDataset`` instance.\n\n        :param datasets: A collection of datasets.\n        :param transform: A function/transform that takes the X value of a\n            pattern from the original dataset and returns a transformed version.\n        :param target_transform: A function/transform that takes in the target\n            and transforms it.\n        :param transform_groups: A dictionary containing the transform groups.\n            Transform groups are used to quickly switch between training and\n            eval (test) transformations. This becomes useful when in need to\n            test on the training dataset as test transformations usually don't\n            contain random augmentations. ``AvalancheDataset`` natively supports\n            the 'train' and 'eval' groups by calling the ``train()`` and\n            ``eval()`` methods. When using custom groups one can use the\n            ``with_transforms(group_name)`` method instead. Defaults to None,\n            which means that the current transforms will be used to\n            handle both 'train' and 'eval' groups (just like in standard\n            ``torchvision`` datasets).\n        :param initial_transform_group: The name of the initial transform group\n            to be used. Defaults to None, which means that if all\n            AvalancheDatasets in the input datasets list agree on a common\n            group (the \"current group\" is the same for all datasets), then that\n            group will be used as the initial one. If the list of input datasets\n            does not contain an AvalancheDataset or if the AvalancheDatasets\n            do not agree on a common group, then 'train' will be used.\n        :param targets: The label of each pattern. Can either be a sequence of\n            labels or, alternatively, a sequence containing sequences of labels\n            (one for each dataset to be concatenated). Defaults to None, which\n            means that the targets will be retrieved from the datasets (if\n            possible).\n        :param task_labels: The task labels for each pattern. Must be a sequence\n            of ints, one for each pattern in the dataset. Alternatively, task\n            labels can be expressed as a sequence containing sequences of ints\n            (one for each dataset to be concatenated) or even a single int,\n            in which case that value will be used as the task label for all\n            instances. Defaults to None, which means that the dataset will try\n            to obtain the task labels from the original datasets. If no task\n            labels could be found for a dataset, a default task label \"0\" will\n            be applied to all patterns of that dataset.\n        :param dataset_type: The type of the dataset. Defaults to None,\n            which means that the type will be inferred from the list of\n            input datasets. When `dataset_type` is None and the list of datasets\n            contains incompatible types, an error will be raised.\n            A list of datasets is compatible if they all have\n            the same type. Datasets that are not instances of `AvalancheDataset`\n            and instances of `AvalancheDataset` with type `UNDEFINED`\n            are always compatible with other types.\n            When the `dataset_type` is different than UNDEFINED, a\n            proper value for `collate_fn` and `targets_adapter` will be set.\n            If the `dataset_type` is different than UNDEFINED, then\n            `collate_fn` and `targets_adapter` must not be set.\n        :param collate_fn: The function to use when slicing to merge single\n            patterns. In the future this function may become the function\n            used in the data loading process, too. If None, the constructor\n            will check if a `collate_fn` field exists in the first dataset. If\n            no such field exists, the default collate function will be used.\n            Beware that the chosen collate function will be applied to all\n            the concatenated datasets even if a different collate is defined\n            in different datasets.\n        :param targets_adapter: A function used to convert the values of the\n            targets field. Defaults to None. Note: the adapter will not change\n            the value of the second element returned by `__getitem__`.\n            The adapter is used to adapt the values of the targets field only.\n        \"\"\"\n        dataset_list = list(datasets)\n\n        dataset_type, collate_fn, targets_adapter = \\\n            self._get_dataset_type_collate_and_adapter(\n                dataset_list, dataset_type, collate_fn, targets_adapter)\n\n        self._dataset_list = dataset_list\n        self._datasets_lengths = [len(dataset) for dataset in dataset_list]\n        self._datasets_cumulative_lengths = ConcatDataset.cumsum(dataset_list)\n        self._overall_length = sum(self._datasets_lengths)\n\n        if initial_transform_group is None:\n            uniform_group = None\n            for d_set in self._dataset_list:\n                if isinstance(d_set, AvalancheDataset):\n                    if uniform_group is None:\n                        uniform_group = d_set.current_transform_group\n                    else:\n                        if uniform_group != d_set.current_transform_group:\n                            uniform_group = None\n                            break\n\n            if uniform_group is None:\n                initial_transform_group = 'train'\n            else:\n                initial_transform_group = uniform_group\n\n        if task_labels is not None:\n            task_labels = self._concat_task_labels(task_labels)\n\n        if targets is not None:\n            targets = self._concat_targets(targets)\n\n        self._adapt_concat_datasets()\n\n        super().__init__(ClassificationDataset(),  # not used\n                         transform=transform,\n                         target_transform=target_transform,\n                         transform_groups=transform_groups,\n                         initial_transform_group=initial_transform_group,\n                         task_labels=task_labels,\n                         targets=targets,\n                         dataset_type=dataset_type,\n                         collate_fn=collate_fn,\n                         targets_adapter=targets_adapter)",
  "def _get_dataset_type_collate_and_adapter(\n            self, datasets, dataset_type, collate_fn, targets_adapter):\n\n        if dataset_type is not None:\n            return dataset_type, collate_fn, targets_adapter\n\n        identified_types = set()\n        first_collate_fn = None\n\n        for dataset in datasets:\n            if isinstance(dataset, AvalancheDataset):\n                if dataset.dataset_type != AvalancheDatasetType.UNDEFINED:\n                    identified_types.add(dataset.dataset_type)\n\n            if first_collate_fn is None:\n                first_collate_fn = getattr(dataset, 'collate_fn', None)\n\n        if len(identified_types) > 1:\n            raise ValueError(\n                'Error trying to infer a common dataset type while '\n                'concatenating different datasets. '\n                'Incompatible types: {}'.format(list(identified_types)))\n        elif len(identified_types) == 0:\n            dataset_type = AvalancheDatasetType.UNDEFINED\n        else:\n            # len(identified_types) == 1\n            dataset_type = next(iter(identified_types))\n\n        if dataset_type != AvalancheDatasetType.UNDEFINED and \\\n                (collate_fn is not None or targets_adapter is not None):\n            raise ValueError(\n                'dataset_type {} was inferred from the list of '\n                'concatenated dataset. This dataset type can\\'t be used '\n                'with custom collate_fn or targets_adapter '\n                'parameters. Only the UNDEFINED type supports '\n                'custom collate_fn or targets_adapter '\n                'parameters.'.format(dataset_type))\n\n        if collate_fn is None and \\\n                dataset_type == AvalancheDatasetType.UNDEFINED:\n            collate_fn = first_collate_fn\n\n        return dataset_type, collate_fn, targets_adapter",
  "def __len__(self) -> int:\n        return self._overall_length",
  "def _get_single_item(self, idx: int):\n        dataset_idx, internal_idx = find_list_from_index(\n            idx, self._datasets_lengths, self._overall_length,\n            cumulative_sizes=self._datasets_cumulative_lengths)\n\n        single_element = self._dataset_list[dataset_idx][internal_idx]\n\n        return self._process_pattern(single_element, idx)",
  "def _fork_dataset(self: TAvalancheDataset) -> TAvalancheDataset:\n        dataset_copy = super()._fork_dataset()\n\n        dataset_copy._dataset_list = list(dataset_copy._dataset_list)\n        # Note: there is no need to duplicate _datasets_lengths\n\n        return dataset_copy",
  "def _initialize_targets_sequence(\n            self, dataset, targets, dataset_type, targets_adapter) -> \\\n            Sequence[TTargetType]:\n        if targets is not None:\n            if len(targets) != self._overall_length:\n                raise ValueError(\n                    'Invalid amount of target labels. It must be equal to the '\n                    'number of patterns in the dataset. Got {}, expected '\n                    '{}!'.format(len(targets), self._overall_length))\n\n            return targets\n\n        targets_list = []\n        # Could be easily done with a single line of code\n        # This however, allows the user to better check which was the\n        # problematic dataset by using a debugger.\n        for dataset_idx, single_dataset in enumerate(self._dataset_list):\n            targets_list.append(super()._initialize_targets_sequence(\n                single_dataset, None, dataset_type, targets_adapter\n            ))\n\n        return LazyConcatTargets(targets_list)",
  "def _initialize_task_labels_sequence(\n            self, dataset, task_labels: Optional[Sequence[int]]) \\\n            -> Sequence[int]:\n        if task_labels is not None:\n            # task_labels has priority over the dataset fields\n\n            if isinstance(task_labels, int):\n                return ConstantSequence(task_labels, self._overall_length)\n            elif len(task_labels) != self._overall_length:\n                raise ValueError(\n                    'Invalid amount of task labels. It must be equal to the '\n                    'number of patterns in the dataset. Got {}, expected '\n                    '{}!'.format(len(task_labels), self._overall_length))\n            return SubSequence(task_labels, converter=int)\n\n        concat_t_labels = []\n        for dataset_idx, single_dataset in enumerate(self._dataset_list):\n            concat_t_labels.append(super()._initialize_task_labels_sequence(\n                single_dataset, None))\n\n        return LazyConcatTargets(concat_t_labels)",
  "def _initialize_collate_fn(self, dataset, dataset_type, collate_fn):\n        if collate_fn is not None:\n            return collate_fn\n\n        if len(self._dataset_list) > 0 and \\\n                hasattr(self._dataset_list[0], 'collate_fn'):\n            return getattr(self._dataset_list[0], 'collate_fn')\n        return default_collate",
  "def _set_original_dataset_transform_group(\n            self, group_name: str) -> None:\n        for dataset_idx, dataset in enumerate(self._dataset_list):\n            if isinstance(dataset, AvalancheDataset):\n                if dataset.current_transform_group == group_name:\n                    # Prevents a huge slowdown in some corner cases\n                    # (apart from being actually more performant)\n                    continue\n\n                self._dataset_list[dataset_idx] = \\\n                    dataset.with_transforms(group_name)",
  "def _freeze_original_dataset(\n            self, group_name: str) -> None:\n        for dataset_idx, dataset in enumerate(self._dataset_list):\n            if isinstance(dataset, AvalancheDataset):\n                self._dataset_list[dataset_idx] = \\\n                    dataset.freeze_group_transforms(group_name)",
  "def _replace_original_dataset_group(\n            self, transform: XTransform, target_transform: YTransform) -> None:\n        for dataset_idx, dataset in enumerate(self._dataset_list):\n            if isinstance(dataset, AvalancheDataset):\n                self._dataset_list[dataset_idx] = \\\n                    dataset.replace_transforms(transform, target_transform)",
  "def _add_original_dataset_group(\n            self, group_name: str) -> None:\n        for dataset_idx, dataset in enumerate(self._dataset_list):\n            if isinstance(dataset, AvalancheDataset):\n                self._dataset_list[dataset_idx] = \\\n                    dataset.add_transforms_group(group_name, None, None)",
  "def _add_groups_from_original_dataset(\n            self, dataset, transform_groups) -> None:\n        for dataset_idx, dataset in enumerate(self._dataset_list):\n            if isinstance(dataset, AvalancheDataset):\n                for original_dataset_group in dataset.transform_groups.keys():\n                    if original_dataset_group not in transform_groups:\n                        transform_groups[original_dataset_group] = (None, None)",
  "def _adapt_concat_datasets(self):\n        all_groups = set()\n\n        for dataset in self._dataset_list:\n            if isinstance(dataset, AvalancheDataset):\n                all_groups.update(dataset.transform_groups.keys())\n\n        for dataset_idx, dataset in enumerate(self._dataset_list):\n            if isinstance(dataset, AvalancheDataset):\n                for group_name in all_groups:\n                    if group_name not in dataset.transform_groups:\n                        self._dataset_list[dataset_idx] = \\\n                            dataset.add_transforms_group(group_name, None, None)",
  "def _concat_task_labels(task_labels: Union[int, Sequence[int],\n                                               Sequence[Sequence[int]]]):\n        if isinstance(task_labels, int):\n            # A single value has been passed -> use it for all instances\n            # The value is returned as is because it's already managed when in\n            # this form (in _initialize_task_labels_sequence).\n            return task_labels\n        elif isinstance(task_labels[0], int):\n            # Flat list of task labels -> just return it.\n            # The constructor will check if it has the correct size.\n            return task_labels\n        else:\n            # One list for each dataset, concat them.\n            return LazyConcatIntTargets(task_labels)",
  "def _concat_targets(targets: Union[Sequence[TTargetType],\n                                       Sequence[Sequence[TTargetType]]]):\n        if isinstance(targets[0], Sequence):\n            return LazyConcatTargets(targets)\n        else:\n            return targets",
  "def _flatten_dataset(self):\n        # Flattens this subset by borrowing the list of concatenated datasets\n        # from the original datasets (if they're 'AvalancheConcatSubset's or\n        # PyTorch 'ConcatDataset's)\n\n        flattened_list = []\n        for dataset in self._dataset_list:\n            if isinstance(dataset, AvalancheConcatDataset):\n                if dataset._has_own_transformations():\n                    # Can't flatten as the dataset has custom transformations\n                    flattened_list.append(dataset)\n                else:\n                    flattened_list.extend(dataset._dataset_list)\n\n            # PyTorch ConcatDataset doesn't have custom transformations\n            # --------\n            # Flattening PyTorch ConcatDatasets has been temporarily\n            # disabled as the semantic of transformation groups collide\n            # with the flattening process: PyTorch ConcatDataset doesn't have\n            # transform groups and flattening it will expose the underlying\n            # concatenated datasets list, which may contain 'AvalancheDataset's.\n            # --------\n            # elif isinstance(dataset, ConcatDataset):\n            #    flattened_list.extend(dataset.datasets)\n            elif isinstance(dataset, AvalancheSubset):\n                flattened_list += self._flatten_subset_concat_branch(dataset)\n            else:\n                flattened_list.append(dataset)\n\n        self._dataset_list = flattened_list\n        self._datasets_lengths = [len(dataset) for dataset in flattened_list]\n        self._datasets_cumulative_lengths = ConcatDataset.cumsum(flattened_list)\n        self._overall_length = sum(self._datasets_lengths)",
  "def _flatten_subset_concat_branch(self, dataset: AvalancheSubset) \\\n            -> List[Dataset]:\n        \"\"\"\n        Optimizes the dataset hierarchy in the corner case:\n\n        self -> [Subset, Subset, ] -> ConcatDataset -> [Dataset]\n\n        :param dataset: The dataset. This function returns [dataset] if the\n            dataset is not a subset containing a concat dataset (or if other\n            corner cases are encountered).\n        :return: The flattened list of datasets to be concatenated.\n        \"\"\"\n        if not isinstance(dataset._original_dataset, AvalancheConcatDataset):\n            return [dataset]\n\n        concat_dataset: AvalancheConcatDataset = dataset._original_dataset\n        if concat_dataset._has_own_transformations():\n            # The dataset has custom transforms -> do nothing\n            return [dataset]\n\n        result: List[AvalancheSubset] = []\n        last_c_dataset = None\n        last_c_idxs = []\n        last_c_targets = []\n        last_c_tasks = []\n        for subset_idx, idx in enumerate(dataset._indices):\n            dataset_idx, internal_idx = find_list_from_index(\n                idx, concat_dataset._datasets_lengths,\n                concat_dataset._overall_length,\n                cumulative_sizes=concat_dataset._datasets_cumulative_lengths)\n\n            if last_c_dataset is None:\n                last_c_dataset = dataset_idx\n            elif last_c_dataset != dataset_idx:\n                # Consolidate current subset\n                result.append(AvalancheConcatDataset._make_similar_subset(\n                    dataset, concat_dataset._dataset_list[last_c_dataset],\n                    last_c_idxs, last_c_targets, last_c_tasks))\n\n                # Switch to next dataset\n                last_c_dataset = dataset_idx\n                last_c_idxs = []\n                last_c_targets = []\n                last_c_tasks = []\n\n            last_c_idxs.append(internal_idx)\n            last_c_targets.append(dataset.targets[subset_idx])\n            last_c_tasks.append(dataset.targets_task_labels[subset_idx])\n\n        if last_c_dataset is not None:\n            result.append(AvalancheConcatDataset._make_similar_subset(\n                dataset, concat_dataset._dataset_list[last_c_dataset],\n                last_c_idxs, last_c_targets, last_c_tasks))\n\n        return result",
  "def _make_similar_subset(subset, ref_dataset, indices, targets, tasks):\n        t_groups = dict()\n        f_groups = dict()\n        AvalancheDataset._borrow_transformations(\n            subset, t_groups, f_groups)\n\n        collate_fn = None\n        if subset.dataset_type == AvalancheDatasetType.UNDEFINED:\n            collate_fn = subset.collate_fn\n\n        result = AvalancheSubset(\n            ref_dataset,\n            indices=indices,\n            class_mapping=subset._class_mapping,\n            transform_groups=t_groups,\n            initial_transform_group=subset.current_transform_group,\n            task_labels=tasks,\n            targets=targets,\n            dataset_type=subset.dataset_type,\n            collate_fn=collate_fn,\n        )\n\n        result._frozen_transforms = f_groups\n        return result",
  "def tensor_as_list(sequence) -> List:\n    # Numpy: list(np.array([1, 2, 3])) returns [1, 2, 3]\n    # whereas: list(torch.tensor([1, 2, 3])) returns ->\n    # -> [tensor(1), tensor(2), tensor(3)]\n    #\n    # This is why we have to handle Tensor in a different way\n    if isinstance(sequence, list):\n        return sequence\n    if not isinstance(sequence, Iterable):\n        return [sequence]\n    if isinstance(sequence, Tensor):\n        return sequence.tolist()\n    return list(sequence)",
  "def _indexes_grouped_by_classes(targets: Sequence[int],\n                                patterns_indexes: Union[None, Sequence[int]],\n                                sort_indexes: bool = True,\n                                sort_classes: bool = True) \\\n        -> Union[List[int], None]:\n    result_per_class: Dict[int, List[int]] = OrderedDict()\n    result: List[int] = []\n\n    indexes_was_none = patterns_indexes is None\n\n    if patterns_indexes is not None:\n        patterns_indexes = tensor_as_list(patterns_indexes)\n    else:\n        patterns_indexes = list(range(len(targets)))\n\n    targets = tensor_as_list(targets)\n\n    # Consider that result_per_class is an OrderedDict\n    # This means that, if sort_classes is True, the next for statement\n    # will initialize \"result_per_class\" in sorted order which in turn means\n    # that patterns will be ordered by ascending class ID.\n    classes = torch.unique(torch.as_tensor(targets),\n                           sorted=sort_classes).tolist()\n\n    for class_id in classes:\n        result_per_class[class_id] = []\n\n    # Stores each pattern index in the appropriate class list\n    for idx in patterns_indexes:\n        result_per_class[targets[idx]].append(idx)\n\n    # Concatenate all the pattern indexes\n    for class_id in classes:\n        if sort_indexes:\n            result_per_class[class_id].sort()\n        result.extend(result_per_class[class_id])\n\n    if result == patterns_indexes and indexes_was_none:\n        # Result is [0, 1, 2, ..., N] and patterns_indexes was originally None\n        # This means that the user tried to obtain a full Dataset\n        # (indexes_was_none) only ordered according to the sort_indexes and\n        # sort_classes parameters. However, sort_indexes+sort_classes returned\n        # the plain pattern sequence as it already is. So the original Dataset\n        # already satisfies the sort_indexes+sort_classes constraints.\n        # By returning None, we communicate that the Dataset can be taken as-is.\n        return None\n\n    return result",
  "def grouped_and_ordered_indexes(\n        targets: Sequence[int], patterns_indexes: Union[None, Sequence[int]],\n        bucket_classes: bool = True, sort_classes: bool = False,\n        sort_indexes: bool = False) -> Union[List[int], None]:\n    \"\"\"\n    Given the targets list of a dataset and the patterns to include, returns the\n    pattern indexes sorted according to the ``bucket_classes``,\n    ``sort_classes`` and ``sort_indexes`` parameters.\n\n    :param targets: The list of pattern targets, as a list.\n    :param patterns_indexes: A list of pattern indexes to include in the set.\n        If None, all patterns will be included.\n    :param bucket_classes: If True, pattern indexes will be returned so that\n        patterns will be grouped by class. Defaults to True.\n    :param sort_classes: If both ``bucket_classes`` and ``sort_classes`` are\n        True, class groups will be sorted by class index. Ignored if\n        ``bucket_classes`` is False. Defaults to False.\n    :param sort_indexes: If True, patterns indexes will be sorted. When\n        bucketing by class, patterns will be sorted inside their buckets.\n        Defaults to False.\n\n    :returns: The list of pattern indexes sorted according to the\n        ``bucket_classes``, ``sort_classes`` and ``sort_indexes`` parameters or\n        None if the patterns_indexes is None and the whole dataset can be taken\n        using the existing patterns order.\n    \"\"\"\n    if bucket_classes:\n        return _indexes_grouped_by_classes(targets, patterns_indexes,\n                                           sort_indexes=sort_indexes,\n                                           sort_classes=sort_classes)\n\n    if patterns_indexes is None:\n        # No grouping and sub-set creation required... just return None\n        return None\n    if not sort_indexes:\n        # No sorting required, just return patterns_indexes\n        return tensor_as_list(patterns_indexes)\n\n    # We are here only because patterns_indexes != None and sort_indexes is True\n    patterns_indexes = tensor_as_list(patterns_indexes)\n    result = list(patterns_indexes)  # Make sure we're working on a copy\n    result.sort()\n    return result",
  "class IDataset(Protocol[T_co]):\n    \"\"\"\n    Protocol definition of a Dataset.\n\n    Note: no __add__ method is defined.\n    \"\"\"\n\n    def __getitem__(self, index: int) -> T_co:\n        ...\n\n    def __len__(self) -> int:\n        ...",
  "class IDatasetWithTargets(IDataset[T_co], Protocol[T_co, TTargetType]):\n    \"\"\"\n    Protocol definition of a Dataset that has a valid targets field.\n    \"\"\"\n\n    targets: Sequence[TTargetType]\n    \"\"\"\n    A sequence of elements describing the targets of each pattern.\n    \"\"\"",
  "class ISupportedClassificationDataset(IDatasetWithTargets[\n                                          T_co, SupportsInt], Protocol):\n    \"\"\"\n    Protocol definition of a Dataset that has a valid targets field (like the\n    Datasets in the torchvision package) for classification.\n\n    For classification purposes, the targets field must be a sequence of ints.\n    describing the class label of each pattern.\n\n    This class however describes a targets field as a sequence of elements\n    that can be converted to `int`. The main reason for this choice is that\n    the targets field of some torchvision datasets is a Tensor. This means that\n    this protocol class supports both sequence of native ints and Tensor of ints\n    (or longs).\n\n    On the contrary, class :class:`IClassificationDataset` strictly\n    defines a `targets` field as sequence of native `int`s.\n    \"\"\"\n\n    targets: Sequence[SupportsInt]\n    \"\"\"\n    A sequence of ints or a PyTorch Tensor or a NumPy ndarray describing the\n    label of each pattern contained in the dataset.\n    \"\"\"",
  "class ITensorDataset(IDataset[T_co], Protocol):\n    \"\"\"\n    Protocol definition of a Dataset that has a tensors field (like\n    TensorDataset).\n\n    A TensorDataset can be easily converted to a :class:`IDatasetWithTargets`\n    by using one of the provided tensors (usually the second, which commonly\n    contains the \"y\" values).\n    \"\"\"\n\n    tensors: Sequence[T_co]\n    \"\"\"\n    A sequence of PyTorch Tensors describing the contents of the Dataset.\n    \"\"\"",
  "class IClassificationDataset(IDatasetWithTargets[T_co, int], Protocol):\n    \"\"\"\n    Protocol definition of a Dataset that has a valid targets field (like the\n    Datasets in the torchvision package) where the targets field is a sequence\n    of native ints.\n\n    The content of the sequence must be strictly native ints. For a more slack\n    protocol see :class:`ISupportedClassificationDataset`.\n    \"\"\"\n\n    targets: Sequence[int]\n    \"\"\"\n    A sequence of ints describing the label of each pattern contained in the\n    dataset.\n    \"\"\"",
  "class ClassificationDataset(IClassificationDataset[T_co], Dataset):\n    \"\"\"\n    Dataset that has a valid targets field (like the Datasets in the\n    torchvision package) where the targets field is a sequence of native ints.\n\n    The actual value of the targets field should be set by the child class.\n    \"\"\"\n    def __init__(self):\n        self.targets = []\n        \"\"\"\n        A sequence of ints describing the label of each pattern contained in the\n        dataset.\n        \"\"\"",
  "def __getitem__(self, index: int) -> T_co:\n        ...",
  "def __len__(self) -> int:\n        ...",
  "def __init__(self):\n        self.targets = []\n        \"\"\"\n        A sequence of ints describing the label of each pattern contained in the\n        dataset.\n        \"\"\"",
  "def ImageFolder(*args, **kwargs):\n    return torchImageFolder(*args, **kwargs)",
  "def DatasetFolder(*args, **kwargs):\n    return torchDatasetFolder(*args, **kwargs)",
  "def default_image_loader(path):\n    \"\"\"\n    Sets the default image loader for the Pytorch Dataset.\n\n    :param path: relative or absolute path of the file to load.\n\n    :returns: Returns the image as a RGB PIL image.\n    \"\"\"\n    return Image.open(path).convert('RGB')",
  "def default_flist_reader(flist):\n    \"\"\"\n    This reader reads a filelist and return a list of paths.\n\n    :param flist: path of the flislist to read. The flist format should be:\n        impath label, impath label,  ...(same to caffe's filelist)\n\n    :returns: Returns a list of paths (the examples to be loaded).\n    \"\"\"\n\n    imlist = []\n    with open(flist, 'r') as rf:\n        for line in rf.readlines():\n            impath, imlabel = line.strip().split()\n            imlist.append((impath, int(imlabel)))\n\n    return imlist",
  "class PathsDataset(data.Dataset):\n    \"\"\"\n    This class extends the basic Pytorch Dataset class to handle list of paths\n    as the main data source.\n    \"\"\"\n\n    def __init__(\n            self, root, files, transform=None, target_transform=None,\n            loader=default_image_loader):\n        \"\"\"\n        Creates a File Dataset from a list of files and labels.\n\n        :param root: root path where the data to load are stored. May be None.\n        :param files: list of tuples. Each tuple must contain two elements: the\n            full path to the pattern and its class label. Optionally, the tuple\n            may contain a third element describing the bounding box to use for\n            cropping (top, left, height, width).\n        :param transform: eventual transformation to add to the input data (x)\n        :param target_transform: eventual transformation to add to the targets\n            (y)\n        :param loader: loader function to use (for the real data) given path.\n        \"\"\"\n\n        if root is not None:\n            root = Path(root)\n\n        self.root: Optional[Path] = root\n        self.imgs = files\n        self.targets = [img_data[1] for img_data in self.imgs]\n        self.transform = transform\n        self.target_transform = target_transform\n        self.loader = loader\n\n    def __getitem__(self, index):\n        \"\"\"\n        Returns next element in the dataset given the current index.\n\n        :param index: index of the data to get.\n        :return: loaded item.\n        \"\"\"\n\n        img_description = self.imgs[index]\n        impath = img_description[0]\n        target = img_description[1]\n        bbox = None\n        if len(img_description) > 2:\n            bbox = img_description[2]\n\n        if self.root is not None:\n            impath = self.root / impath\n        img = self.loader(impath)\n\n        # If a bounding box is provided, crop the image before passing it to\n        # any user-defined transformation.\n        if bbox is not None:\n            if isinstance(bbox, Tensor):\n                bbox = bbox.tolist()\n            img = crop(img, *bbox)\n\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target\n\n    def __len__(self):\n        \"\"\"\n        Returns the total number of elements in the dataset.\n\n        :return: Total number of dataset items.\n        \"\"\"\n\n        return len(self.imgs)",
  "class FilelistDataset(PathsDataset):\n    \"\"\"\n    This class extends the basic Pytorch Dataset class to handle filelists as\n    main data source.\n    \"\"\"\n\n    def __init__(\n            self, root, flist, transform=None, target_transform=None,\n            flist_reader=default_flist_reader, loader=default_image_loader):\n        \"\"\"\n        This reader reads a filelist and return a list of paths.\n\n        :param root: root path where the data to load are stored. May be None.\n        :param flist: path of the flislist to read. The flist format should be:\n            impath label\\nimpath label\\n ...(same to caffe's filelist).\n        :param transform: eventual transformation to add to the input data (x).\n        :param target_transform: eventual transformation to add to the targets\n            (y).\n        :param flist_reader: loader function to use (for the filelists) given\n            path.\n        :param loader: loader function to use (for the real data) given path.\n        \"\"\"\n\n        flist = str(flist)  # Manages Path objects\n        files_and_labels = flist_reader(flist)\n        super().__init__(root, files_and_labels, transform=transform,\n                         target_transform=target_transform, loader=loader)",
  "def datasets_from_filelists(root, train_filelists, test_filelists,\n                            complete_test_set_only=False,\n                            train_transform=None, train_target_transform=None,\n                            test_transform=None, test_target_transform=None):\n    \"\"\"\n    This reader reads a list of Caffe-style filelists and returns the proper\n    Dataset objects.\n\n    A Caffe-style list is just a text file where, for each line, two elements\n    are described: the path to the pattern (relative to the root parameter)\n    and its class label. Those two elements are separated by a single white\n    space.\n\n    This method reads each file list and returns a separate\n    dataset for each of them.\n\n    Beware that the parameters must be **list of paths to Caffe-style\n    filelists**. If you need to create a dataset given a list of\n    **pattern paths**, use `datasets_from_paths` instead.\n\n    :param root: root path where the data to load are stored. May be None.\n    :param train_filelists: list of paths to train filelists. The flist format\n        should be: impath label\\\\nimpath label\\\\n ...(same to Caffe's filelist).\n    :param test_filelists: list of paths to test filelists. It can be also a\n        single path when the datasets is the same for each batch.\n    :param complete_test_set_only: if True, test_filelists must contain\n        the path to a single filelist that will serve as the complete test set.\n        Alternatively, test_filelists can be the path (str) to the complete test\n        set filelist. If False, train_filelists and test_filelists must contain\n        the same amount of filelists paths. Defaults to False.\n    :param train_transform: The transformation to apply to training patterns.\n        Defaults to None.\n    :param train_target_transform: The transformation to apply to training\n        patterns targets. Defaults to None.\n    :param test_transform: The transformation to apply to test patterns.\n        Defaults to None.\n    :param test_target_transform: The transformation to apply to test\n        patterns targets. Defaults to None.\n\n    :return: list of tuples (train dataset, test dataset) for each train\n        filelist in the list.\n    \"\"\"\n\n    if complete_test_set_only:\n        if not (isinstance(test_filelists, str) or\n                isinstance(test_filelists, Path)):\n            if len(test_filelists) > 1:\n                raise ValueError(\n                    'When complete_test_set_only is True, test_filelists must '\n                    'be a str, Path or a list with a single element describing '\n                    'the path to the complete test set.')\n            else:\n                test_filelists = test_filelists[0]\n        else:\n            test_filelists = [test_filelists]\n    else:\n        if len(test_filelists) != len(train_filelists):\n            raise ValueError(\n                'When complete_test_set_only is False, test_filelists and '\n                'train_filelists must contain the same number of elements.')\n\n    transform_groups = dict(train=(train_transform, train_target_transform),\n                            eval=(test_transform, test_target_transform))\n    train_inc_datasets = \\\n        [AvalancheDataset(FilelistDataset(root, tr_flist),\n                          transform_groups=transform_groups,\n                          initial_transform_group='train')\n         for tr_flist in train_filelists]\n    test_inc_datasets = \\\n        [AvalancheDataset(FilelistDataset(root, te_flist),\n                          transform_groups=transform_groups,\n                          initial_transform_group='eval')\n         for te_flist in test_filelists]\n\n    return train_inc_datasets, test_inc_datasets",
  "def datasets_from_paths(\n        train_list, test_list, complete_test_set_only=False,\n        train_transform=None, train_target_transform=None,\n        test_transform=None, test_target_transform=None):\n    \"\"\"\n    This utility takes, for each dataset to generate, a list of tuples each\n    containing two elements: the full path to the pattern and its class label.\n    Optionally, the tuple may contain a third element describing the bounding\n    box to use for cropping.\n\n    This is equivalent to `datasets_from_filelists`, which description\n    contains more details on the behaviour of this utility. The two utilities\n    differ in which `datasets_from_filelists` accepts paths to Caffe-style\n    filelists while this one is able to create the datasets from an in-memory\n    list.\n\n    Note: this utility may try to detect (and strip) the common root path of\n    all patterns in order to save some RAM memory.\n\n    :param train_list: list of lists. Each list must contain tuples of two\n        elements: the full path to the pattern and its class label. Optionally,\n        the tuple may contain a third element describing the bounding box to use\n        for cropping (top, left, height, width).\n    :param test_list: list of lists. Each list must contain tuples of two\n        elements: the full path to the pattern and its class label. Optionally,\n        the tuple may contain a third element describing the bounding box to use\n        for cropping (top, left, height, width). It can be also a single list\n        when the test dataset is the same for each experience.\n    :param complete_test_set_only: if True, test_list must contain a single list\n        that will serve as the complete test set. If False, train_list and\n        test_list must describe the same amount of datasets. Defaults to False.\n    :param train_transform: The transformation to apply to training patterns.\n        Defaults to None.\n    :param train_target_transform: The transformation to apply to training\n        patterns targets. Defaults to None.\n    :param test_transform: The transformation to apply to test patterns.\n        Defaults to None.\n    :param test_target_transform: The transformation to apply to test\n        patterns targets. Defaults to None.\n\n    :return: A list of tuples (train dataset, test dataset).\n    \"\"\"\n\n    if complete_test_set_only:\n        # Check if the single dataset was passed as [Tuple1, Tuple2, ...]\n        # or as [[Tuple1, Tuple2, ...]]\n        if not isinstance(test_list[0], Tuple):\n            if len(test_list) > 1:\n                raise ValueError(\n                    'When complete_test_set_only is True, test_list must '\n                    'be a single list of tuples or a nested list containing '\n                    'a single lis of tuples')\n            else:\n                test_list = test_list[0]\n        else:\n            test_list = [test_list]\n    else:\n        if len(test_list) != len(train_list):\n            raise ValueError(\n                'When complete_test_set_only is False, test_list and '\n                'train_list must contain the same number of elements.')\n\n    transform_groups = dict(train=(train_transform, train_target_transform),\n                            eval=(test_transform, test_target_transform))\n\n    common_root = None\n\n    # Detect common root\n    try:\n        all_paths = [pattern_tuple[0] for exp_list in train_list\n                     for pattern_tuple in exp_list] + \\\n                    [pattern_tuple[0] for exp_list in test_list\n                     for pattern_tuple in exp_list]\n\n        common_root = os.path.commonpath(all_paths)\n    except ValueError:\n        # commonpath may throw a ValueError in different situations!\n        # See the official documentation for more details\n        pass\n\n    if common_root is not None and len(common_root) > 0 and \\\n            common_root != '/':\n        has_common_root = True\n        common_root = str(common_root)\n    else:\n        has_common_root = False\n        common_root = None\n\n    if has_common_root:\n        # print(f'Common root found: {common_root}!')\n        # All paths have a common filesystem root\n        # Remove it from all paths!\n        single_path_case = False\n        tr_list = list()\n        te_list = list()\n\n        for idx_exp_list in range(len(train_list)):\n            if single_path_case:\n                break\n            st_list = list()\n            for x in train_list[idx_exp_list]:\n                rel = os.path.relpath(x[0], common_root)\n                if len(rel) == 0 or rel == '.':\n                    # May happen if the dataset has a single path\n                    single_path_case = True\n                    break\n                st_list.append((rel, *x[1:]))\n            tr_list.append(st_list)\n\n        for idx_exp_list in range(len(test_list)):\n            if single_path_case:\n                break\n            st_list = list()\n            for x in test_list[idx_exp_list]:\n                rel = os.path.relpath(x[0], common_root)\n                if len(rel) == 0 or rel == '.':\n                    # May happen if the dataset has a single path\n                    single_path_case = True\n                    break\n                st_list.append((rel, *x[1:]))\n            te_list.append(st_list)\n        if not single_path_case:\n            train_list = tr_list\n            test_list = te_list\n        else:\n            has_common_root = False\n            common_root = None\n\n    train_inc_datasets = \\\n        [AvalancheDataset(PathsDataset(common_root, tr_flist),\n                          transform_groups=transform_groups,\n                          initial_transform_group='train')\n         for tr_flist in train_list]\n    test_inc_datasets = \\\n        [AvalancheDataset(PathsDataset(common_root, te_flist),\n                          transform_groups=transform_groups,\n                          initial_transform_group='eval')\n         for te_flist in test_list]\n\n    return train_inc_datasets, test_inc_datasets",
  "def common_paths_root(exp_list):\n    common_root = None\n\n    # Detect common root\n    try:\n        all_paths = [pattern_tuple[0] for pattern_tuple in exp_list]\n\n        common_root = os.path.commonpath(all_paths)\n    except ValueError:\n        # commonpath may throw a ValueError in different situations!\n        # See the official documentation for more details\n        pass\n\n    if common_root is not None and len(common_root) > 0 and \\\n            common_root != '/':\n        has_common_root = True\n        common_root = str(common_root)\n    else:\n        has_common_root = False\n        common_root = None\n\n    if has_common_root:\n        # print(f'Common root found: {common_root}!')\n        # All paths have a common filesystem root\n        # Remove it from all paths!\n        single_path_case = False\n        exp_tuples = list()\n\n        for x in exp_list:\n            if single_path_case:\n                break\n\n            rel = os.path.relpath(x[0], common_root)\n            if len(rel) == 0 or rel == '.':\n                # May happen if the dataset has a single path\n                single_path_case = True\n                break\n            exp_tuples.append((rel, *x[1:]))\n\n        if not single_path_case:\n            exp_list = exp_tuples\n        else:\n            common_root = None\n\n    return common_root, exp_list",
  "def __init__(\n            self, root, files, transform=None, target_transform=None,\n            loader=default_image_loader):\n        \"\"\"\n        Creates a File Dataset from a list of files and labels.\n\n        :param root: root path where the data to load are stored. May be None.\n        :param files: list of tuples. Each tuple must contain two elements: the\n            full path to the pattern and its class label. Optionally, the tuple\n            may contain a third element describing the bounding box to use for\n            cropping (top, left, height, width).\n        :param transform: eventual transformation to add to the input data (x)\n        :param target_transform: eventual transformation to add to the targets\n            (y)\n        :param loader: loader function to use (for the real data) given path.\n        \"\"\"\n\n        if root is not None:\n            root = Path(root)\n\n        self.root: Optional[Path] = root\n        self.imgs = files\n        self.targets = [img_data[1] for img_data in self.imgs]\n        self.transform = transform\n        self.target_transform = target_transform\n        self.loader = loader",
  "def __getitem__(self, index):\n        \"\"\"\n        Returns next element in the dataset given the current index.\n\n        :param index: index of the data to get.\n        :return: loaded item.\n        \"\"\"\n\n        img_description = self.imgs[index]\n        impath = img_description[0]\n        target = img_description[1]\n        bbox = None\n        if len(img_description) > 2:\n            bbox = img_description[2]\n\n        if self.root is not None:\n            impath = self.root / impath\n        img = self.loader(impath)\n\n        # If a bounding box is provided, crop the image before passing it to\n        # any user-defined transformation.\n        if bbox is not None:\n            if isinstance(bbox, Tensor):\n                bbox = bbox.tolist()\n            img = crop(img, *bbox)\n\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target",
  "def __len__(self):\n        \"\"\"\n        Returns the total number of elements in the dataset.\n\n        :return: Total number of dataset items.\n        \"\"\"\n\n        return len(self.imgs)",
  "def __init__(\n            self, root, flist, transform=None, target_transform=None,\n            flist_reader=default_flist_reader, loader=default_image_loader):\n        \"\"\"\n        This reader reads a filelist and return a list of paths.\n\n        :param root: root path where the data to load are stored. May be None.\n        :param flist: path of the flislist to read. The flist format should be:\n            impath label\\nimpath label\\n ...(same to caffe's filelist).\n        :param transform: eventual transformation to add to the input data (x).\n        :param target_transform: eventual transformation to add to the targets\n            (y).\n        :param flist_reader: loader function to use (for the filelists) given\n            path.\n        :param loader: loader function to use (for the real data) given path.\n        \"\"\"\n\n        flist = str(flist)  # Manages Path objects\n        files_and_labels = flist_reader(flist)\n        super().__init__(root, files_and_labels, transform=transform,\n                         target_transform=target_transform, loader=loader)",
  "class SubSequence(Sequence[TTargetType]):\n    \"\"\"\n    A utility class used to define a lazily evaluated sub-sequence.\n    \"\"\"\n    def __init__(self,\n                 targets: Sequence[TTargetType],\n                 *,\n                 indices: Union[Sequence[int], None] = None,\n                 converter: Optional[Callable[[Any], TTargetType]] = None):\n        self._targets = targets\n        self._indices = indices\n        self.converter = converter\n\n    def __len__(self):\n        if self._indices is None:\n            return len(self._targets)\n        return len(self._indices)\n\n    def __getitem__(self, item_idx) -> TTargetType:\n        if self._indices is not None:\n            subset_idx = self._indices[item_idx]\n        else:\n            subset_idx = item_idx\n\n        element = self._targets[subset_idx]\n\n        if self.converter is not None:\n            return self.converter(element)\n\n        return element\n\n    def __str__(self):\n        return '[' + \\\n               ', '.join([str(self[idx]) for idx in range(len(self))]) + \\\n               ']'",
  "class LazyClassMapping(SubSequence[int]):\n    \"\"\"\n    This class is used when in need of lazy populating a targets field whose\n    elements need to be filtered out (when subsetting, see\n    :class:`torch.utils.data.Subset`) and/or transformed (remapped). This will\n    allow for a more efficient memory usage as the conversion is done on the fly\n    instead of actually allocating a new targets list.\n\n    This class should be used only when mapping int targets (classification).\n    \"\"\"\n    def __init__(self,\n                 targets: Sequence[SupportsInt],\n                 indices: Union[Sequence[int], None],\n                 mapping: Optional[Sequence[int]] = None,):\n        super().__init__(targets, indices=indices, converter=int)\n        self._mapping = mapping\n\n    def __getitem__(self, item_idx) -> int:\n        target_value = super().__getitem__(item_idx)\n\n        if self._mapping is not None:\n            return self._mapping[target_value]\n\n        return target_value",
  "class LazyConcatTargets(Sequence[TTargetType]):\n    \"\"\"\n    Defines a lazy targets concatenation.\n\n    This class is used when in need of lazy populating a targets created\n    as the concatenation of the targets field of multiple datasets.\n    This will allow for a more efficient memory usage as the concatenation is\n    done on the fly instead of actually allocating a new targets list.\n    \"\"\"\n    def __init__(self, targets_list: Sequence[Sequence[TTargetType]],\n                 converter: Optional[Callable[[Any], TTargetType]] = None):\n        self._targets_list = targets_list\n        self._targets_lengths = [len(targets) for targets in targets_list]\n        self._overall_length = sum(self._targets_lengths)\n        self._targets_cumulative_lengths = ConcatDataset.cumsum(targets_list)\n        self.converter = converter\n\n    def __len__(self):\n        return self._overall_length\n\n    def __getitem__(self, item_idx) -> TTargetType:\n        targets_idx, internal_idx = find_list_from_index(\n            item_idx, self._targets_lengths, self._overall_length,\n            self._targets_cumulative_lengths)\n\n        target = self._targets_list[targets_idx][internal_idx]\n\n        if self.converter is None:\n            return target\n        return self.converter(target)\n\n    def __iter__(self):\n        if self.converter is None:\n            for x in self._targets_list:\n                for y in x:\n                    yield y\n        else:\n            for x in self._targets_list:\n                for y in x:\n                    yield self.converter(y)\n\n    def __str__(self):\n        return '[' + \\\n               ', '.join([str(self[idx]) for idx in range(len(self))]) + \\\n               ']'",
  "class LazyConcatIntTargets(LazyConcatTargets[int]):\n    \"\"\"\n    Defines a lazy targets concatenation.\n\n    This class is used when in need of lazy populating a targets created\n    as the concatenation of the targets field of multiple datasets.\n    This will allow for a more efficient memory usage as the concatenation is\n    done on the fly instead of actually allocating a new targets list.\n\n    Elements returned by `__getitem__` will be int values.\n    \"\"\"\n    def __init__(self, targets_list: Sequence[Sequence[SupportsInt]]):\n        super().__init__(targets_list, converter=int)",
  "class ConstantSequence(Sequence[int]):\n    \"\"\"\n    Defines a constant sequence given an int value and the length.\n    \"\"\"\n    def __init__(self, constant_value: int, size: int):\n        self._constant_value = constant_value\n        self._size = size\n\n    def __len__(self):\n        return self._size\n\n    def __getitem__(self, item_idx) -> int:\n        if item_idx >= len(self):\n            raise IndexError()\n\n        return self._constant_value\n\n    def __str__(self):\n        return '[' + \\\n               ', '.join([str(self[idx]) for idx in range(len(self))]) + \\\n               ']'",
  "class SubsetWithTargets(Generic[T_co, TTargetType], Subset[T_co]):\n    \"\"\"\n    A Dataset that behaves like a PyTorch :class:`torch.utils.data.Subset`.\n    However, this dataset also supports the targets field.\n    \"\"\"\n    def __init__(self,\n                 dataset: IDatasetWithTargets[T_co, TTargetType],\n                 indices: Union[Sequence[int], None]):\n        if indices is None:\n            indices = range(len(dataset))\n        super().__init__(dataset, indices)\n        self.targets: Sequence[TTargetType] =\\\n            SubSequence(dataset.targets, indices=indices)\n\n    def __getitem__(self, idx):\n        return self.dataset[self.indices[idx]]\n\n    def __len__(self) -> int:\n        return len(self.indices)",
  "class ClassificationSubset(SubsetWithTargets[T_co, int]):\n    \"\"\"\n    A Dataset that behaves like a PyTorch :class:`torch.utils.data.Subset`.\n    However, this dataset also supports the targets field and class mapping.\n\n    Targets will be converted to int.\n    \"\"\"\n    def __init__(self,\n                 dataset: ISupportedClassificationDataset[T_co],\n                 indices: Union[Sequence[int], None],\n                 class_mapping: Optional[Sequence[int]] = None):\n        super().__init__(dataset, indices)\n        self.class_mapping = class_mapping\n        self.targets = LazyClassMapping(dataset.targets, indices,\n                                        mapping=class_mapping)\n\n    def __getitem__(self, idx):\n        result = super().__getitem__(idx)\n\n        if self.class_mapping is not None:\n            return make_tuple(\n                (result[0], self.class_mapping[result[1]], *result[2:]),\n                result)\n\n        return result",
  "class SequenceDataset(IDatasetWithTargets[T_co, TTargetType]):\n    \"\"\"\n    A Dataset that wraps existing ndarrays, Tensors, lists... to provide\n    basic Dataset functionalities. Very similar to TensorDataset.\n    \"\"\"\n    def __init__(self,\n                 *sequences: Sequence,\n                 targets: Union[int, Sequence[TTargetType]] = 1):\n        \"\"\"\n        Creates a ``SequenceDataset`` instance.\n\n        Beware that the second sequence, will be used to fill the targets\n        field without running any kind of type conversion.\n\n        :param sequences: A sequence of sequences, Tensors or ndarrays\n            representing the patterns.\n        :param targets: A sequence representing the targets field of the\n            dataset. Can either be 1) a sequence of values containing as many\n            elements as the number of contained patterns, or 2) the index\n            of the sequence to use as the targets field. Defaults to 1, which\n            means that the second sequence (usually containing the \"y\" values)\n            will be used for the targets field.\n        \"\"\"\n        super().__init__()\n\n        if len(sequences) < 1:\n            raise ValueError('At least one sequence must be passed')\n\n        common_size = len(sequences[0])\n        for seq in sequences:\n            if len(seq) != common_size:\n                raise ValueError('Sequences must contain the same '\n                                 'amount of elements')\n\n        self._sequences = sequences\n        if isinstance(targets, int):\n            targets = sequences[targets]\n\n        self.targets: Sequence[TTargetType] = targets\n\n    def __getitem__(self, idx):\n        return tuple(seq[idx] for seq in self._sequences)\n\n    def __len__(self) -> int:\n        return len(self._sequences[0])",
  "def find_list_from_index(pattern_idx: int,\n                         list_sizes: Sequence[int],\n                         max_size: int,\n                         cumulative_sizes=None):\n    if pattern_idx >= max_size:\n        raise IndexError()\n\n    if cumulative_sizes is None:\n        r, s = [], 0\n        for list_len in list_sizes:\n            r.append(list_len + s)\n            s += list_len\n        cumulative_sizes = r\n\n    list_idx = bisect.bisect_right(cumulative_sizes, pattern_idx)\n    if list_idx != 0:\n        pattern_idx = pattern_idx - cumulative_sizes[list_idx - 1]\n\n    if pattern_idx >= list_sizes[list_idx]:\n        raise ValueError('Index out of bounds, wrong max_size parameter')\n    return list_idx, pattern_idx",
  "def manage_advanced_indexing(idx, single_element_getter, max_length,\n                             collate_fn):\n    \"\"\"\n    Utility function used to manage the advanced indexing and slicing.\n\n    If more than a pattern is selected, the X and Y values will be merged\n    in two separate torch Tensor objects using the stack operation.\n\n    :param idx: Either an in, a slice object or a list (including ndarrays and\n        torch Tensors) of indexes.\n    :param single_element_getter: A callable used to obtain a single element\n        given its int index.\n    :param max_length: The maximum sequence length.\n    :param collate_fn: The function to use to create a batch of data from\n        single elements.\n    :return: A tuple consisting of two tensors containing the X and Y values\n        of the patterns addressed by the idx parameter.\n    \"\"\"\n    indexes_iterator: Iterable[int]\n\n    # Makes dataset sliceable\n    if isinstance(idx, slice):\n        indexes_iterator = range(*idx.indices(max_length))\n    elif isinstance(idx, int):\n        indexes_iterator = [idx]\n    elif hasattr(idx, 'shape') and len(getattr(idx, 'shape')) == 0:\n        # Manages 0-d ndarray / Tensor\n        indexes_iterator = [int(idx)]\n    else:\n        indexes_iterator = idx\n\n    elements = []\n    for single_idx in indexes_iterator:\n        single_element = single_element_getter(int(single_idx))\n        elements.append(single_element)\n\n    if len(elements) == 1:\n        return elements[0]\n\n    return collate_fn(elements)",
  "class LazySubsequence(Sequence[int]):\n    \"\"\"\n    LazySubsequence can be used to define a Sequence based on another sequence\n    and a pair of start and end indices.\n    \"\"\"\n    def __init__(self,\n                 sequence: Sequence[int],\n                 start_idx: int,\n                 end_idx: int):\n        self._sequence = sequence\n        self._start_idx = start_idx\n        self._end_idx = end_idx\n\n    def __len__(self):\n        return self._end_idx - self._start_idx\n\n    def __getitem__(self, item_idx) -> int:\n        if item_idx >= len(self):\n            raise IndexError()\n\n        return self._sequence[self._start_idx + item_idx]\n\n    def __str__(self):\n        return '[' + \\\n               ', '.join([str(self[idx]) for idx in range(len(self))]) + \\\n               ']'",
  "def optimize_sequence(sequence: Sequence[TTargetType]) -> Sequence[TTargetType]:\n    if len(sequence) == 0 or isinstance(sequence, ConstantSequence):\n        return sequence\n\n    if isinstance(sequence, list):\n        return sequence\n\n    return list(sequence)",
  "class TupleTLabel(tuple):\n    \"\"\"\n    A simple tuple class used to describe a value returned from a dataset\n    in which the task label is contained.\n\n    Being a vanilla subclass of tuple, this class can be used to describe both a\n    single instance and a batch.\n    \"\"\"\n    def __new__(cls, *data, **kwargs):\n        return super(TupleTLabel, cls).__new__(cls, *data, **kwargs)",
  "def make_tuple(new_tuple: Iterable[T_co], prev_tuple: tuple):\n    if isinstance(prev_tuple, TupleTLabel):\n        return TupleTLabel(new_tuple)\n\n    return new_tuple",
  "def __init__(self,\n                 targets: Sequence[TTargetType],\n                 *,\n                 indices: Union[Sequence[int], None] = None,\n                 converter: Optional[Callable[[Any], TTargetType]] = None):\n        self._targets = targets\n        self._indices = indices\n        self.converter = converter",
  "def __len__(self):\n        if self._indices is None:\n            return len(self._targets)\n        return len(self._indices)",
  "def __getitem__(self, item_idx) -> TTargetType:\n        if self._indices is not None:\n            subset_idx = self._indices[item_idx]\n        else:\n            subset_idx = item_idx\n\n        element = self._targets[subset_idx]\n\n        if self.converter is not None:\n            return self.converter(element)\n\n        return element",
  "def __str__(self):\n        return '[' + \\\n               ', '.join([str(self[idx]) for idx in range(len(self))]) + \\\n               ']'",
  "def __init__(self,\n                 targets: Sequence[SupportsInt],\n                 indices: Union[Sequence[int], None],\n                 mapping: Optional[Sequence[int]] = None,):\n        super().__init__(targets, indices=indices, converter=int)\n        self._mapping = mapping",
  "def __getitem__(self, item_idx) -> int:\n        target_value = super().__getitem__(item_idx)\n\n        if self._mapping is not None:\n            return self._mapping[target_value]\n\n        return target_value",
  "def __init__(self, targets_list: Sequence[Sequence[TTargetType]],\n                 converter: Optional[Callable[[Any], TTargetType]] = None):\n        self._targets_list = targets_list\n        self._targets_lengths = [len(targets) for targets in targets_list]\n        self._overall_length = sum(self._targets_lengths)\n        self._targets_cumulative_lengths = ConcatDataset.cumsum(targets_list)\n        self.converter = converter",
  "def __len__(self):\n        return self._overall_length",
  "def __getitem__(self, item_idx) -> TTargetType:\n        targets_idx, internal_idx = find_list_from_index(\n            item_idx, self._targets_lengths, self._overall_length,\n            self._targets_cumulative_lengths)\n\n        target = self._targets_list[targets_idx][internal_idx]\n\n        if self.converter is None:\n            return target\n        return self.converter(target)",
  "def __iter__(self):\n        if self.converter is None:\n            for x in self._targets_list:\n                for y in x:\n                    yield y\n        else:\n            for x in self._targets_list:\n                for y in x:\n                    yield self.converter(y)",
  "def __str__(self):\n        return '[' + \\\n               ', '.join([str(self[idx]) for idx in range(len(self))]) + \\\n               ']'",
  "def __init__(self, targets_list: Sequence[Sequence[SupportsInt]]):\n        super().__init__(targets_list, converter=int)",
  "def __init__(self, constant_value: int, size: int):\n        self._constant_value = constant_value\n        self._size = size",
  "def __len__(self):\n        return self._size",
  "def __getitem__(self, item_idx) -> int:\n        if item_idx >= len(self):\n            raise IndexError()\n\n        return self._constant_value",
  "def __str__(self):\n        return '[' + \\\n               ', '.join([str(self[idx]) for idx in range(len(self))]) + \\\n               ']'",
  "def __init__(self,\n                 dataset: IDatasetWithTargets[T_co, TTargetType],\n                 indices: Union[Sequence[int], None]):\n        if indices is None:\n            indices = range(len(dataset))\n        super().__init__(dataset, indices)\n        self.targets: Sequence[TTargetType] =\\\n            SubSequence(dataset.targets, indices=indices)",
  "def __getitem__(self, idx):\n        return self.dataset[self.indices[idx]]",
  "def __len__(self) -> int:\n        return len(self.indices)",
  "def __init__(self,\n                 dataset: ISupportedClassificationDataset[T_co],\n                 indices: Union[Sequence[int], None],\n                 class_mapping: Optional[Sequence[int]] = None):\n        super().__init__(dataset, indices)\n        self.class_mapping = class_mapping\n        self.targets = LazyClassMapping(dataset.targets, indices,\n                                        mapping=class_mapping)",
  "def __getitem__(self, idx):\n        result = super().__getitem__(idx)\n\n        if self.class_mapping is not None:\n            return make_tuple(\n                (result[0], self.class_mapping[result[1]], *result[2:]),\n                result)\n\n        return result",
  "def __init__(self,\n                 *sequences: Sequence,\n                 targets: Union[int, Sequence[TTargetType]] = 1):\n        \"\"\"\n        Creates a ``SequenceDataset`` instance.\n\n        Beware that the second sequence, will be used to fill the targets\n        field without running any kind of type conversion.\n\n        :param sequences: A sequence of sequences, Tensors or ndarrays\n            representing the patterns.\n        :param targets: A sequence representing the targets field of the\n            dataset. Can either be 1) a sequence of values containing as many\n            elements as the number of contained patterns, or 2) the index\n            of the sequence to use as the targets field. Defaults to 1, which\n            means that the second sequence (usually containing the \"y\" values)\n            will be used for the targets field.\n        \"\"\"\n        super().__init__()\n\n        if len(sequences) < 1:\n            raise ValueError('At least one sequence must be passed')\n\n        common_size = len(sequences[0])\n        for seq in sequences:\n            if len(seq) != common_size:\n                raise ValueError('Sequences must contain the same '\n                                 'amount of elements')\n\n        self._sequences = sequences\n        if isinstance(targets, int):\n            targets = sequences[targets]\n\n        self.targets: Sequence[TTargetType] = targets",
  "def __getitem__(self, idx):\n        return tuple(seq[idx] for seq in self._sequences)",
  "def __len__(self) -> int:\n        return len(self._sequences[0])",
  "def __init__(self,\n                 sequence: Sequence[int],\n                 start_idx: int,\n                 end_idx: int):\n        self._sequence = sequence\n        self._start_idx = start_idx\n        self._end_idx = end_idx",
  "def __len__(self):\n        return self._end_idx - self._start_idx",
  "def __getitem__(self, item_idx) -> int:\n        if item_idx >= len(self):\n            raise IndexError()\n\n        return self._sequence[self._start_idx + item_idx]",
  "def __str__(self):\n        return '[' + \\\n               ', '.join([str(self[idx]) for idx in range(len(self))]) + \\\n               ']'",
  "def __new__(cls, *data, **kwargs):\n        return super(TupleTLabel, cls).__new__(cls, *data, **kwargs)",
  "class WandBLogger(StrategyLogger):\n    \"\"\"\n    The `WandBLogger` provides an easy integration with\n    Weights & Biases logging. Each monitored metric is automatically\n    logged to a dedicated Weights & Biases project dashboard.\n\n    External storage for W&B Artifacts (for instance - AWS S3 and GCS\n    buckets) uri are supported.\n\n    The wandb log files are placed by default in \"./wandb/\" unless specified.\n\n    .. note::\n        TensorBoard can be synced on to the W&B dedicated dashboard.\n    \"\"\"\n\n    def __init__(self, project_name: str = \"Avalanche\", \n                 run_name: str = \"Test\", log_artifacts: bool = False,\n                 path: Union[str, Path] = \"Checkpoints\", \n                 uri: str = None, sync_tfboard: bool = False, \n                 save_code: bool = True, config: object = None, \n                 dir: Union[str, Path] = None, params: dict = None):\n        \"\"\"\n        Creates an instance of the `WandBLogger`.\n        :param project_name: Name of the W&B project.\n        :param run_name: Name of the W&B run.\n        :param log_artifacts: Option to log model weights as W&B Artifacts.\n        :param path: Path to locally save the model checkpoints.\n        :param uri: URI identifier for external storage buckets (GCS, S3).\n        :param sync_tfboard: Syncs TensorBoard to the W&B dashboard UI.\n        :param save_code: Saves the main training script to W&B. \n        :param config: Syncs hyper-parameters and config values used to W&B.\n        :param dir: Path to the local log directory for W&B logs to be saved at.\n        :param params: All arguments for wandb.init() function call. \n         Visit https://docs.wandb.ai/ref/python/init to learn about all \n         wand.init() parameters.\n        \"\"\"\n        super().__init__()\n        self.import_wandb()\n        self.project_name = project_name\n        self.run_name = run_name\n        self.log_artifacts = log_artifacts\n        self.path = path\n        self.uri = uri\n        self.sync_tfboard = sync_tfboard\n        self.save_code = save_code\n        self.config = config\n        self.dir = dir\n        self.params = params\n        self.args_parse()\n        self.before_run()\n\n    def import_wandb(self):\n        try:\n            import wandb\n        except ImportError:\n            raise ImportError(\n                'Please run \"pip install wandb\" to install wandb')\n        self.wandb = wandb\n\n    def args_parse(self):\n        self.init_kwargs = {\"project\": self.project_name, \"name\": self.run_name, \n                            \"sync_tensorboard\": self.sync_tfboard, \n                            \"dir\": self.dir, \"save_code\": self.save_code, \n                            \"config\": vars(self.config)}\n        if self.params:\n            self.init_kwargs.update(self.params)\n\n    def before_run(self):\n        if self.wandb is None:\n            self.import_wandb()\n        if self.init_kwargs:\n            self.wandb.init(**self.init_kwargs)\n        else:\n            self.wandb.init()\n        self.wandb.run._label(repo=\"Avalanche\")\n\n    def log_metric(self, metric_value: MetricValue, callback: str):\n        super().log_metric(metric_value, callback)\n        name = metric_value.name\n        value = metric_value.value\n\n        if isinstance(value, AlternativeValues):\n            value = value.best_supported_value(Image, Tensor, TensorImage,\n                                               Figure, float, int,\n                                               self.wandb.viz.CustomChart)\n\n        if not isinstance(value, (Image, Tensor, Figure, float, int,\n                                  self.wandb.viz.CustomChart)):\n            # Unsupported type\n            return\n        \n        if isinstance(value, Image):\n            self.wandb.log({name: self.wandb.Image(value)})\n        \n        elif isinstance(value, Tensor):\n            value = np.histogram(value.view(-1).numpy())\n            self.wandb.log({name: self.wandb.Histogram(np_histogram=value)})\n        \n        elif isinstance(value, (float, int, Figure,\n                                self.wandb.viz.CustomChart)):\n            self.wandb.log({name: value})\n\n        elif isinstance(value, TensorImage):\t\n            self.wandb.log({name: self.wandb.Image(array(value))})\n\n        elif name.startswith(\"WeightCheckpoint\"):\n            if self.log_artifacts:\n                cwd = os.getcwd()\n                ckpt = os.path.join(cwd, self.path)\n                try:\n                    os.makedirs(ckpt)\n                except OSError as e:\n                    if e.errno != errno.EEXIST:\n                        raise\n                suffix = '.pth'\n                dir_name = os.path.join(ckpt, name+suffix)\n                artifact_name = os.path.join('Models', name+suffix)\n                if isinstance(value, Tensor):\n                    torch.save(value, dir_name)\n                    name = os.path.splittext(self.checkpoint)\n                    artifact = self.wandb.Artifact(name, type='model')\n                    artifact.add_file(dir_name, name=artifact_name)\n                    self.wandb.run.log_artifact(artifact)\n                    if self.uri is not None:\n                        artifact.add_reference(self.uri, name=artifact_name)",
  "def __init__(self, project_name: str = \"Avalanche\", \n                 run_name: str = \"Test\", log_artifacts: bool = False,\n                 path: Union[str, Path] = \"Checkpoints\", \n                 uri: str = None, sync_tfboard: bool = False, \n                 save_code: bool = True, config: object = None, \n                 dir: Union[str, Path] = None, params: dict = None):\n        \"\"\"\n        Creates an instance of the `WandBLogger`.\n        :param project_name: Name of the W&B project.\n        :param run_name: Name of the W&B run.\n        :param log_artifacts: Option to log model weights as W&B Artifacts.\n        :param path: Path to locally save the model checkpoints.\n        :param uri: URI identifier for external storage buckets (GCS, S3).\n        :param sync_tfboard: Syncs TensorBoard to the W&B dashboard UI.\n        :param save_code: Saves the main training script to W&B. \n        :param config: Syncs hyper-parameters and config values used to W&B.\n        :param dir: Path to the local log directory for W&B logs to be saved at.\n        :param params: All arguments for wandb.init() function call. \n         Visit https://docs.wandb.ai/ref/python/init to learn about all \n         wand.init() parameters.\n        \"\"\"\n        super().__init__()\n        self.import_wandb()\n        self.project_name = project_name\n        self.run_name = run_name\n        self.log_artifacts = log_artifacts\n        self.path = path\n        self.uri = uri\n        self.sync_tfboard = sync_tfboard\n        self.save_code = save_code\n        self.config = config\n        self.dir = dir\n        self.params = params\n        self.args_parse()\n        self.before_run()",
  "def import_wandb(self):\n        try:\n            import wandb\n        except ImportError:\n            raise ImportError(\n                'Please run \"pip install wandb\" to install wandb')\n        self.wandb = wandb",
  "def args_parse(self):\n        self.init_kwargs = {\"project\": self.project_name, \"name\": self.run_name, \n                            \"sync_tensorboard\": self.sync_tfboard, \n                            \"dir\": self.dir, \"save_code\": self.save_code, \n                            \"config\": vars(self.config)}\n        if self.params:\n            self.init_kwargs.update(self.params)",
  "def before_run(self):\n        if self.wandb is None:\n            self.import_wandb()\n        if self.init_kwargs:\n            self.wandb.init(**self.init_kwargs)\n        else:\n            self.wandb.init()\n        self.wandb.run._label(repo=\"Avalanche\")",
  "def log_metric(self, metric_value: MetricValue, callback: str):\n        super().log_metric(metric_value, callback)\n        name = metric_value.name\n        value = metric_value.value\n\n        if isinstance(value, AlternativeValues):\n            value = value.best_supported_value(Image, Tensor, TensorImage,\n                                               Figure, float, int,\n                                               self.wandb.viz.CustomChart)\n\n        if not isinstance(value, (Image, Tensor, Figure, float, int,\n                                  self.wandb.viz.CustomChart)):\n            # Unsupported type\n            return\n        \n        if isinstance(value, Image):\n            self.wandb.log({name: self.wandb.Image(value)})\n        \n        elif isinstance(value, Tensor):\n            value = np.histogram(value.view(-1).numpy())\n            self.wandb.log({name: self.wandb.Histogram(np_histogram=value)})\n        \n        elif isinstance(value, (float, int, Figure,\n                                self.wandb.viz.CustomChart)):\n            self.wandb.log({name: value})\n\n        elif isinstance(value, TensorImage):\t\n            self.wandb.log({name: self.wandb.Image(array(value))})\n\n        elif name.startswith(\"WeightCheckpoint\"):\n            if self.log_artifacts:\n                cwd = os.getcwd()\n                ckpt = os.path.join(cwd, self.path)\n                try:\n                    os.makedirs(ckpt)\n                except OSError as e:\n                    if e.errno != errno.EEXIST:\n                        raise\n                suffix = '.pth'\n                dir_name = os.path.join(ckpt, name+suffix)\n                artifact_name = os.path.join('Models', name+suffix)\n                if isinstance(value, Tensor):\n                    torch.save(value, dir_name)\n                    name = os.path.splittext(self.checkpoint)\n                    artifact = self.wandb.Artifact(name, type='model')\n                    artifact.add_file(dir_name, name=artifact_name)\n                    self.wandb.run.log_artifact(artifact)\n                    if self.uri is not None:\n                        artifact.add_reference(self.uri, name=artifact_name)",
  "class StrategyLogger(StrategyCallbacks[None], ABC):\n    \"\"\"\n    The base class for the strategy loggers.\n\n    Strategy loggers will receive events, under the form of callback calls,\n    from the :class:`EvaluationPlugin` carrying a reference to the strategy\n    as well as the values emitted by the metrics.\n\n    Each child class should implement the `log_metric` method, which\n    specifies how to report to the user the metrics gathered during\n    training and evaluation flows. The `log_metric` method is invoked\n    by default on each callback.\n    In addition, child classes may override the desired callbacks\n    to customize the logger behavior.\n\n    Make sure, when overriding callbacks, to call\n    the proper `super` method.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def log_metric(self, metric_value: 'MetricValue', callback: str) -> None:\n        \"\"\"\n        This abstract method will has to be implemented by child classes.\n        This method will be invoked on each callback.\n        The `callback` parameter describes the callback from which the metric\n        value is coming from.\n\n        :param metric_value: The value to be logged.\n        :param callback: The name of the callback (event) from which the\n            metric value was obtained.\n        :return: None\n        \"\"\"\n        pass\n\n    def before_training(self, strategy: 'BaseStrategy',\n                        metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'before_training')\n\n    def before_training_exp(self, strategy: 'BaseStrategy',\n                            metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'before_training_exp')\n\n    def after_train_dataset_adaptation(self, strategy: 'BaseStrategy',\n                                       metric_values: List['MetricValue'],\n                                       **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'adapt_train_dataset')\n\n    def before_training_epoch(self, strategy: 'BaseStrategy',\n                              metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'before_training_epoch')\n\n    def before_training_iteration(self, strategy: 'BaseStrategy',\n                                  metric_values: List['MetricValue'],\n                                  **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'before_training_iteration')\n\n    def before_forward(self, strategy: 'BaseStrategy',\n                       metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'before_forward')\n\n    def after_forward(self, strategy: 'BaseStrategy',\n                      metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'after_forward')\n\n    def before_backward(self, strategy: 'BaseStrategy',\n                        metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'before_backward')\n\n    def after_backward(self, strategy: 'BaseStrategy',\n                       metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'after_backward')\n\n    def after_training_iteration(self, strategy: 'BaseStrategy',\n                                 metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'after_training_iteration')\n\n    def before_update(self, strategy: 'BaseStrategy',\n                      metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'before_update')\n\n    def after_update(self, strategy: 'BaseStrategy',\n                     metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'after_update')\n\n    def after_training_epoch(self, strategy: 'BaseStrategy',\n                             metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'after_training_epoch')\n\n    def after_training_exp(self, strategy: 'BaseStrategy',\n                           metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'after_training_exp')\n\n    def after_training(self, strategy: 'BaseStrategy',\n                       metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'after_training')\n\n    def before_eval(self, strategy: 'BaseStrategy',\n                    metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'before_eval')\n\n    def after_eval_dataset_adaptation(self, strategy: 'BaseStrategy',\n                                      metric_values: List['MetricValue'],\n                                      **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'adapt_eval_dataset')\n\n    def before_eval_exp(self, strategy: 'BaseStrategy',\n                        metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'before_eval_exp')\n\n    def after_eval_exp(self, strategy: 'BaseStrategy',\n                       metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'after_eval_exp')\n\n    def after_eval(self, strategy: 'BaseStrategy',\n                   metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'after_eval')\n\n    def before_eval_iteration(self, strategy: 'BaseStrategy',\n                              metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'before_eval_iteration')\n\n    def before_eval_forward(self, strategy: 'BaseStrategy',\n                            metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'before_eval_forward')\n\n    def after_eval_forward(self, strategy: 'BaseStrategy',\n                           metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'after_eval_forward')\n\n    def after_eval_iteration(self, strategy: 'BaseStrategy',\n                             metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'after_eval_iteration')",
  "def __init__(self):\n        super().__init__()",
  "def log_metric(self, metric_value: 'MetricValue', callback: str) -> None:\n        \"\"\"\n        This abstract method will has to be implemented by child classes.\n        This method will be invoked on each callback.\n        The `callback` parameter describes the callback from which the metric\n        value is coming from.\n\n        :param metric_value: The value to be logged.\n        :param callback: The name of the callback (event) from which the\n            metric value was obtained.\n        :return: None\n        \"\"\"\n        pass",
  "def before_training(self, strategy: 'BaseStrategy',\n                        metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'before_training')",
  "def before_training_exp(self, strategy: 'BaseStrategy',\n                            metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'before_training_exp')",
  "def after_train_dataset_adaptation(self, strategy: 'BaseStrategy',\n                                       metric_values: List['MetricValue'],\n                                       **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'adapt_train_dataset')",
  "def before_training_epoch(self, strategy: 'BaseStrategy',\n                              metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'before_training_epoch')",
  "def before_training_iteration(self, strategy: 'BaseStrategy',\n                                  metric_values: List['MetricValue'],\n                                  **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'before_training_iteration')",
  "def before_forward(self, strategy: 'BaseStrategy',\n                       metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'before_forward')",
  "def after_forward(self, strategy: 'BaseStrategy',\n                      metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'after_forward')",
  "def before_backward(self, strategy: 'BaseStrategy',\n                        metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'before_backward')",
  "def after_backward(self, strategy: 'BaseStrategy',\n                       metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'after_backward')",
  "def after_training_iteration(self, strategy: 'BaseStrategy',\n                                 metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'after_training_iteration')",
  "def before_update(self, strategy: 'BaseStrategy',\n                      metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'before_update')",
  "def after_update(self, strategy: 'BaseStrategy',\n                     metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'after_update')",
  "def after_training_epoch(self, strategy: 'BaseStrategy',\n                             metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'after_training_epoch')",
  "def after_training_exp(self, strategy: 'BaseStrategy',\n                           metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'after_training_exp')",
  "def after_training(self, strategy: 'BaseStrategy',\n                       metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'after_training')",
  "def before_eval(self, strategy: 'BaseStrategy',\n                    metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'before_eval')",
  "def after_eval_dataset_adaptation(self, strategy: 'BaseStrategy',\n                                      metric_values: List['MetricValue'],\n                                      **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'adapt_eval_dataset')",
  "def before_eval_exp(self, strategy: 'BaseStrategy',\n                        metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'before_eval_exp')",
  "def after_eval_exp(self, strategy: 'BaseStrategy',\n                       metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'after_eval_exp')",
  "def after_eval(self, strategy: 'BaseStrategy',\n                   metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'after_eval')",
  "def before_eval_iteration(self, strategy: 'BaseStrategy',\n                              metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'before_eval_iteration')",
  "def before_eval_forward(self, strategy: 'BaseStrategy',\n                            metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'before_eval_forward')",
  "def after_eval_forward(self, strategy: 'BaseStrategy',\n                           metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'after_eval_forward')",
  "def after_eval_iteration(self, strategy: 'BaseStrategy',\n                             metric_values: List['MetricValue'], **kwargs):\n        for val in metric_values:\n            self.log_metric(val, 'after_eval_iteration')",
  "class CSVLogger(StrategyLogger):\n    \"\"\"\n    The `CSVLogger` logs accuracy and loss metrics into a csv file.\n    Metrics are logged separately for training and evaluation in files\n    training_results.csv and eval_results.csv, respectively.\n    This Logger assumes that the user is evaluating on only one experience\n    during training (see below for an example of a `train` call).\n\n    Trough the `EvaluationPlugin`, the user should monitor at least\n    EpochAccuracy/Loss and ExperienceAccuracy/Loss.\n    If monitored, the logger will also record Experience Forgetting.\n    In order to monitor the performance on held-out experience\n    associated to the current training experience, set\n    `eval_every=1` (or larger value) in the strategy constructor\n    and pass the eval experience to the `train` method:\n    `for i, exp in enumerate(benchmark.train_stream):`\n        `strategy.train(exp, eval_streams=[benchmark.test_stream[i]])`\n\n    When not provided, validation loss and validation accuracy\n    will be logged as zero.\n\n    The training file header is composed of:\n    training_exp_id, epoch, training_accuracy, val_accuracy,\n    training_loss, val_loss.\n\n    The evaluation file header is composed of:\n    eval_exp, training_exp, eval_accuracy, eval_loss, forgetting\n    \"\"\"\n\n    def __init__(self, log_folder=None):\n        \"\"\"\n        Creates an instance of `CSVLogger` class.\n\n        :param log_folder: folder in which to create log files.\n            If None, `csvlogs` folder in the default current directory\n            will be used.\n        \"\"\"\n\n        super().__init__()\n        self.log_folder = log_folder if log_folder is not None else \"csvlogs\"\n        os.makedirs(self.log_folder, exist_ok=True)\n\n        self.training_file = open(os.path.join(self.log_folder,\n                                               'training_results.csv'), 'w')\n        self.eval_file = open(os.path.join(self.log_folder,\n                                           'eval_results.csv'), 'w')\n        os.makedirs(self.log_folder, exist_ok=True)\n\n        # current training experience id\n        self.training_exp_id = None\n\n        # if we are currently training or evaluating\n        # evaluation within training will not change this flag\n        self.in_train_phase = None\n\n        # validation metrics computed during training\n        self.val_acc, self.val_loss = 0, 0\n\n        # print csv headers\n        print('training_exp', 'epoch', 'training_accuracy', 'val_accuracy',\n              'training_loss', 'val_loss', sep=',', file=self.training_file,\n              flush=True)\n        print('eval_exp', 'training_exp', 'eval_accuracy', 'eval_loss',\n              'forgetting', sep=',', file=self.eval_file, flush=True)\n\n    def log_metric(self, metric_value: 'MetricValue', callback: str) -> None:\n        pass\n\n    def _val_to_str(self, m_val):\n        if isinstance(m_val, torch.Tensor):\n            return '\\n' + str(m_val)\n        elif isinstance(m_val, float):\n            return f'{m_val:.4f}'\n        else:\n            return str(m_val)\n\n    def print_train_metrics(self, training_exp, epoch, train_acc,\n                            val_acc, train_loss, val_loss):\n        print(training_exp, epoch, self._val_to_str(train_acc),\n              self._val_to_str(val_acc), self._val_to_str(train_loss),\n              self._val_to_str(val_loss), sep=',',\n              file=self.training_file, flush=True)\n\n    def print_eval_metrics(self, eval_exp, training_exp, eval_acc,\n                           eval_loss, forgetting):\n        print(eval_exp, training_exp, self._val_to_str(eval_acc),\n              self._val_to_str(eval_loss), self._val_to_str(forgetting),\n              sep=',', file=self.eval_file, flush=True)\n\n    def after_training_epoch(self, strategy: 'BaseStrategy',\n                             metric_values: List['MetricValue'], **kwargs):\n        super().after_training_epoch(strategy, metric_values, **kwargs)\n        train_acc, val_acc, train_loss, val_loss = 0, 0, 0, 0\n        for val in metric_values:\n            if 'train_stream' in val.name:\n                if val.name.startswith('Top1_Acc_Epoch'):\n                    train_acc = val.value\n                elif val.name.startswith('Loss_Epoch'):\n                    train_loss = val.value\n\n        self.print_train_metrics(self.training_exp_id, strategy.epoch,\n                                 train_acc, self.val_acc, train_loss,\n                                 self.val_loss)\n\n    def after_eval_exp(self, strategy: 'BaseStrategy',\n                       metric_values: List['MetricValue'], **kwargs):\n        super().after_eval_exp(strategy, metric_values, **kwargs)\n        acc, loss, forgetting = 0, 0, 0\n\n        for val in metric_values:\n            if self.in_train_phase:  # validation within training\n                if val.name.startswith('Top1_Acc_Exp'):\n                    self.val_acc = val.value\n                elif val.name.startswith('Loss_Exp'):\n                    self.val_loss = val.value\n            else:\n                if val.name.startswith('Top1_Acc_Exp'):\n                    acc = val.value\n                elif val.name.startswith('Loss_Exp'):\n                    loss = val.value\n                elif val.name.startswith('ExperienceForgetting'):\n                    forgetting = val.value\n\n        if not self.in_train_phase:\n            self.print_eval_metrics(strategy.experience.current_experience,\n                                    self.training_exp_id, acc, loss,\n                                    forgetting)\n\n    def before_training_exp(self, strategy: 'BaseStrategy',\n                            metric_values: List['MetricValue'], **kwargs):\n        super().before_training(strategy, metric_values, **kwargs)\n        self.training_exp_id = strategy.experience.current_experience\n\n    def before_eval(self, strategy: 'BaseStrategy',\n                    metric_values: List['MetricValue'], **kwargs):\n        \"\"\"\n        Manage the case in which `eval` is first called before `train`\n        \"\"\"\n        if self.in_train_phase is None:\n            self.in_train_phase = False\n\n    def before_training(self, strategy: 'BaseStrategy',\n                        metric_values: List['MetricValue'], **kwargs):\n        self.in_train_phase = True\n\n    def after_training(self, strategy: 'BaseStrategy',\n                       metric_values: List['MetricValue'], **kwargs):\n        self.in_train_phase = False\n\n    def close(self):\n        self.training_file.close()\n        self.eval_file.close()",
  "def __init__(self, log_folder=None):\n        \"\"\"\n        Creates an instance of `CSVLogger` class.\n\n        :param log_folder: folder in which to create log files.\n            If None, `csvlogs` folder in the default current directory\n            will be used.\n        \"\"\"\n\n        super().__init__()\n        self.log_folder = log_folder if log_folder is not None else \"csvlogs\"\n        os.makedirs(self.log_folder, exist_ok=True)\n\n        self.training_file = open(os.path.join(self.log_folder,\n                                               'training_results.csv'), 'w')\n        self.eval_file = open(os.path.join(self.log_folder,\n                                           'eval_results.csv'), 'w')\n        os.makedirs(self.log_folder, exist_ok=True)\n\n        # current training experience id\n        self.training_exp_id = None\n\n        # if we are currently training or evaluating\n        # evaluation within training will not change this flag\n        self.in_train_phase = None\n\n        # validation metrics computed during training\n        self.val_acc, self.val_loss = 0, 0\n\n        # print csv headers\n        print('training_exp', 'epoch', 'training_accuracy', 'val_accuracy',\n              'training_loss', 'val_loss', sep=',', file=self.training_file,\n              flush=True)\n        print('eval_exp', 'training_exp', 'eval_accuracy', 'eval_loss',\n              'forgetting', sep=',', file=self.eval_file, flush=True)",
  "def log_metric(self, metric_value: 'MetricValue', callback: str) -> None:\n        pass",
  "def _val_to_str(self, m_val):\n        if isinstance(m_val, torch.Tensor):\n            return '\\n' + str(m_val)\n        elif isinstance(m_val, float):\n            return f'{m_val:.4f}'\n        else:\n            return str(m_val)",
  "def print_train_metrics(self, training_exp, epoch, train_acc,\n                            val_acc, train_loss, val_loss):\n        print(training_exp, epoch, self._val_to_str(train_acc),\n              self._val_to_str(val_acc), self._val_to_str(train_loss),\n              self._val_to_str(val_loss), sep=',',\n              file=self.training_file, flush=True)",
  "def print_eval_metrics(self, eval_exp, training_exp, eval_acc,\n                           eval_loss, forgetting):\n        print(eval_exp, training_exp, self._val_to_str(eval_acc),\n              self._val_to_str(eval_loss), self._val_to_str(forgetting),\n              sep=',', file=self.eval_file, flush=True)",
  "def after_training_epoch(self, strategy: 'BaseStrategy',\n                             metric_values: List['MetricValue'], **kwargs):\n        super().after_training_epoch(strategy, metric_values, **kwargs)\n        train_acc, val_acc, train_loss, val_loss = 0, 0, 0, 0\n        for val in metric_values:\n            if 'train_stream' in val.name:\n                if val.name.startswith('Top1_Acc_Epoch'):\n                    train_acc = val.value\n                elif val.name.startswith('Loss_Epoch'):\n                    train_loss = val.value\n\n        self.print_train_metrics(self.training_exp_id, strategy.epoch,\n                                 train_acc, self.val_acc, train_loss,\n                                 self.val_loss)",
  "def after_eval_exp(self, strategy: 'BaseStrategy',\n                       metric_values: List['MetricValue'], **kwargs):\n        super().after_eval_exp(strategy, metric_values, **kwargs)\n        acc, loss, forgetting = 0, 0, 0\n\n        for val in metric_values:\n            if self.in_train_phase:  # validation within training\n                if val.name.startswith('Top1_Acc_Exp'):\n                    self.val_acc = val.value\n                elif val.name.startswith('Loss_Exp'):\n                    self.val_loss = val.value\n            else:\n                if val.name.startswith('Top1_Acc_Exp'):\n                    acc = val.value\n                elif val.name.startswith('Loss_Exp'):\n                    loss = val.value\n                elif val.name.startswith('ExperienceForgetting'):\n                    forgetting = val.value\n\n        if not self.in_train_phase:\n            self.print_eval_metrics(strategy.experience.current_experience,\n                                    self.training_exp_id, acc, loss,\n                                    forgetting)",
  "def before_training_exp(self, strategy: 'BaseStrategy',\n                            metric_values: List['MetricValue'], **kwargs):\n        super().before_training(strategy, metric_values, **kwargs)\n        self.training_exp_id = strategy.experience.current_experience",
  "def before_eval(self, strategy: 'BaseStrategy',\n                    metric_values: List['MetricValue'], **kwargs):\n        \"\"\"\n        Manage the case in which `eval` is first called before `train`\n        \"\"\"\n        if self.in_train_phase is None:\n            self.in_train_phase = False",
  "def before_training(self, strategy: 'BaseStrategy',\n                        metric_values: List['MetricValue'], **kwargs):\n        self.in_train_phase = True",
  "def after_training(self, strategy: 'BaseStrategy',\n                       metric_values: List['MetricValue'], **kwargs):\n        self.in_train_phase = False",
  "def close(self):\n        self.training_file.close()\n        self.eval_file.close()",
  "class InteractiveLogger(TextLogger):\n    \"\"\"\n    The `InteractiveLogger` class provides logging facilities\n    for the console standard output. The logger shows\n    a progress bar during training and evaluation flows and\n    interactively display metric results as soon as they\n    become available. The logger writes metric results after\n    each training epoch, evaluation experience and at the\n    end of the entire evaluation stream.\n\n    .. note::\n        To avoid an excessive amount of printed lines,\n        this logger will **not** print results after\n        each iteration. If the user is monitoring\n        metrics which emit results after each minibatch\n        (e.g., `MinibatchAccuracy`), only the last recorded\n        value of such metrics will be reported at the end\n        of the epoch.\n\n    .. note::\n        Since this logger works on the standard output,\n        metrics producing images or more complex visualizations\n        will be converted to a textual format suitable for\n        console printing. You may want to add more loggers\n        to your `EvaluationPlugin` to better support\n        different formats.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(file=sys.stdout)\n        self._pbar = None\n\n    def before_training_epoch(self, strategy: 'BaseStrategy',\n                              metric_values: List['MetricValue'], **kwargs):\n        super().before_training_epoch(strategy, metric_values, **kwargs)\n        self._progress.total = len(strategy.dataloader)\n\n    def after_training_epoch(self, strategy: 'BaseStrategy',\n                             metric_values: List['MetricValue'], **kwargs):\n        self._end_progress()\n        super().after_training_epoch(strategy, metric_values, **kwargs)\n\n    def before_eval_exp(self, strategy: 'BaseStrategy',\n                        metric_values: List['MetricValue'], **kwargs):\n        super().before_eval_exp(strategy, metric_values, **kwargs)\n        self._progress.total = len(strategy.dataloader)\n\n    def after_eval_exp(self, strategy: 'BaseStrategy',\n                       metric_values: List['MetricValue'], **kwargs):\n        self._end_progress()\n        super().after_eval_exp(strategy, metric_values, **kwargs)\n\n    def after_training_iteration(self, strategy: 'BaseStrategy',\n                                 metric_values: List['MetricValue'], **kwargs):\n        self._progress.update()\n        self._progress.refresh()\n        super().after_training_iteration(strategy, metric_values, **kwargs)\n\n    def after_eval_iteration(self, strategy: 'BaseStrategy',\n                             metric_values: List['MetricValue'], **kwargs):\n        self._progress.update()\n        self._progress.refresh()\n        super().after_eval_iteration(strategy, metric_values, **kwargs)\n\n    @property\n    def _progress(self):\n        if self._pbar is None:\n            self._pbar = tqdm(leave=True, position=0, file=sys.stdout)\n        return self._pbar\n\n    def _end_progress(self):\n        if self._pbar is not None:\n            self._pbar.close()\n            self._pbar = None",
  "def __init__(self):\n        super().__init__(file=sys.stdout)\n        self._pbar = None",
  "def before_training_epoch(self, strategy: 'BaseStrategy',\n                              metric_values: List['MetricValue'], **kwargs):\n        super().before_training_epoch(strategy, metric_values, **kwargs)\n        self._progress.total = len(strategy.dataloader)",
  "def after_training_epoch(self, strategy: 'BaseStrategy',\n                             metric_values: List['MetricValue'], **kwargs):\n        self._end_progress()\n        super().after_training_epoch(strategy, metric_values, **kwargs)",
  "def before_eval_exp(self, strategy: 'BaseStrategy',\n                        metric_values: List['MetricValue'], **kwargs):\n        super().before_eval_exp(strategy, metric_values, **kwargs)\n        self._progress.total = len(strategy.dataloader)",
  "def after_eval_exp(self, strategy: 'BaseStrategy',\n                       metric_values: List['MetricValue'], **kwargs):\n        self._end_progress()\n        super().after_eval_exp(strategy, metric_values, **kwargs)",
  "def after_training_iteration(self, strategy: 'BaseStrategy',\n                                 metric_values: List['MetricValue'], **kwargs):\n        self._progress.update()\n        self._progress.refresh()\n        super().after_training_iteration(strategy, metric_values, **kwargs)",
  "def after_eval_iteration(self, strategy: 'BaseStrategy',\n                             metric_values: List['MetricValue'], **kwargs):\n        self._progress.update()\n        self._progress.refresh()\n        super().after_eval_iteration(strategy, metric_values, **kwargs)",
  "def _progress(self):\n        if self._pbar is None:\n            self._pbar = tqdm(leave=True, position=0, file=sys.stdout)\n        return self._pbar",
  "def _end_progress(self):\n        if self._pbar is not None:\n            self._pbar.close()\n            self._pbar = None",
  "class TextLogger(StrategyLogger):\n    \"\"\"\n    The `TextLogger` class provides logging facilities\n    printed to a user specified file. The logger writes\n    metric results after each training epoch, evaluation\n    experience and at the end of the entire evaluation stream.\n\n    .. note::\n        To avoid an excessive amount of printed lines,\n        this logger will **not** print results after\n        each iteration. If the user is monitoring\n        metrics which emit results after each minibatch\n        (e.g., `MinibatchAccuracy`), only the last recorded\n        value of such metrics will be reported at the end\n        of the epoch.\n\n    .. note::\n        Since this logger works on the standard output,\n        metrics producing images or more complex visualizations\n        will be converted to a textual format suitable for\n        console printing. You may want to add more loggers\n        to your `EvaluationPlugin` to better support\n        different formats.\n    \"\"\"\n    def __init__(self, file=sys.stdout):\n        \"\"\"\n        Creates an instance of `TextLogger` class.\n\n        :param file: destination file to which print metrics\n            (default=sys.stdout).\n        \"\"\"\n        super().__init__()\n        self.file = file\n        self.metric_vals = {}\n\n    def log_metric(self, metric_value: 'MetricValue', callback: str) -> None:\n        name = metric_value.name\n        x = metric_value.x_plot\n        val = metric_value.value\n        self.metric_vals[name] = (name, x, val)\n\n    def _val_to_str(self, m_val):\n        if isinstance(m_val, torch.Tensor):\n            return '\\n' + str(m_val)\n        elif isinstance(m_val, float):\n            return f'{m_val:.4f}'\n        else:\n            return str(m_val)\n\n    def print_current_metrics(self):\n        sorted_vals = sorted(self.metric_vals.values(),\n                             key=lambda x: x[0])\n        for name, x, val in sorted_vals:\n            if isinstance(val, UNSUPPORTED_TYPES):\n                continue\n            val = self._val_to_str(val)\n            print(f'\\t{name} = {val}', file=self.file, flush=True)\n\n    def before_training_exp(self, strategy: 'BaseStrategy',\n                            metric_values: List['MetricValue'], **kwargs):\n        super().before_training_exp(strategy, metric_values, **kwargs)\n        self._on_exp_start(strategy)\n\n    def before_eval_exp(self, strategy: 'BaseStrategy',\n                        metric_values: List['MetricValue'], **kwargs):\n        super().before_eval_exp(strategy, metric_values, **kwargs)\n        self._on_exp_start(strategy)\n\n    def after_training_epoch(self, strategy: 'BaseStrategy',\n                             metric_values: List['MetricValue'], **kwargs):\n        super().after_training_epoch(strategy, metric_values, **kwargs)\n        print(f'Epoch {strategy.epoch} ended.', file=self.file, flush=True)\n        self.print_current_metrics()\n        self.metric_vals = {}\n\n    def after_eval_exp(self, strategy: 'BaseStrategy',\n                       metric_values: List['MetricValue'], **kwargs):\n        super().after_eval_exp(strategy, metric_values, **kwargs)\n        exp_id = strategy.experience.current_experience\n        task_id = phase_and_task(strategy)[1]\n        if task_id is None:\n            print(f'> Eval on experience {exp_id} '\n                  f'from {stream_type(strategy.experience)} stream ended.',\n                  file=self.file, flush=True)\n        else:\n            print(f'> Eval on experience {exp_id} (Task '\n                  f'{task_id}) '\n                  f'from {stream_type(strategy.experience)} stream ended.',\n                  file=self.file, flush=True)\n        self.print_current_metrics()\n        self.metric_vals = {}\n\n    def before_training(self, strategy: 'BaseStrategy',\n                        metric_values: List['MetricValue'], **kwargs):\n        super().before_training(strategy, metric_values, **kwargs)\n        print('-- >> Start of training phase << --', file=self.file, flush=True)\n\n    def before_eval(self, strategy: 'BaseStrategy',\n                    metric_values: List['MetricValue'], **kwargs):\n        super().before_eval(strategy, metric_values, **kwargs)\n        print('-- >> Start of eval phase << --', file=self.file, flush=True)\n\n    def after_training(self, strategy: 'BaseStrategy',\n                       metric_values: List['MetricValue'], **kwargs):\n        super().after_training(strategy, metric_values, **kwargs)\n        print('-- >> End of training phase << --', file=self.file, flush=True)\n\n    def after_eval(self, strategy: 'BaseStrategy',\n                   metric_values: List['MetricValue'], **kwargs):\n        super().after_eval(strategy, metric_values, **kwargs)\n        print('-- >> End of eval phase << --', file=self.file, flush=True)\n        self.print_current_metrics()\n        self.metric_vals = {}\n\n    def _on_exp_start(self, strategy: 'BaseStrategy'):\n        action_name = 'training' if strategy.is_training else 'eval'\n        exp_id = strategy.experience.current_experience\n        task_id = phase_and_task(strategy)[1]\n        stream = stream_type(strategy.experience)\n        if task_id is None:\n            print('-- Starting {} on experience {} from {} stream --'\n                  .format(action_name, exp_id, stream),\n                  file=self.file,\n                  flush=True)\n        else:\n            print('-- Starting {} on experience {} (Task {}) from {} stream --'\n                  .format(action_name, exp_id, task_id, stream),\n                  file=self.file,\n                  flush=True)",
  "def __init__(self, file=sys.stdout):\n        \"\"\"\n        Creates an instance of `TextLogger` class.\n\n        :param file: destination file to which print metrics\n            (default=sys.stdout).\n        \"\"\"\n        super().__init__()\n        self.file = file\n        self.metric_vals = {}",
  "def log_metric(self, metric_value: 'MetricValue', callback: str) -> None:\n        name = metric_value.name\n        x = metric_value.x_plot\n        val = metric_value.value\n        self.metric_vals[name] = (name, x, val)",
  "def _val_to_str(self, m_val):\n        if isinstance(m_val, torch.Tensor):\n            return '\\n' + str(m_val)\n        elif isinstance(m_val, float):\n            return f'{m_val:.4f}'\n        else:\n            return str(m_val)",
  "def print_current_metrics(self):\n        sorted_vals = sorted(self.metric_vals.values(),\n                             key=lambda x: x[0])\n        for name, x, val in sorted_vals:\n            if isinstance(val, UNSUPPORTED_TYPES):\n                continue\n            val = self._val_to_str(val)\n            print(f'\\t{name} = {val}', file=self.file, flush=True)",
  "def before_training_exp(self, strategy: 'BaseStrategy',\n                            metric_values: List['MetricValue'], **kwargs):\n        super().before_training_exp(strategy, metric_values, **kwargs)\n        self._on_exp_start(strategy)",
  "def before_eval_exp(self, strategy: 'BaseStrategy',\n                        metric_values: List['MetricValue'], **kwargs):\n        super().before_eval_exp(strategy, metric_values, **kwargs)\n        self._on_exp_start(strategy)",
  "def after_training_epoch(self, strategy: 'BaseStrategy',\n                             metric_values: List['MetricValue'], **kwargs):\n        super().after_training_epoch(strategy, metric_values, **kwargs)\n        print(f'Epoch {strategy.epoch} ended.', file=self.file, flush=True)\n        self.print_current_metrics()\n        self.metric_vals = {}",
  "def after_eval_exp(self, strategy: 'BaseStrategy',\n                       metric_values: List['MetricValue'], **kwargs):\n        super().after_eval_exp(strategy, metric_values, **kwargs)\n        exp_id = strategy.experience.current_experience\n        task_id = phase_and_task(strategy)[1]\n        if task_id is None:\n            print(f'> Eval on experience {exp_id} '\n                  f'from {stream_type(strategy.experience)} stream ended.',\n                  file=self.file, flush=True)\n        else:\n            print(f'> Eval on experience {exp_id} (Task '\n                  f'{task_id}) '\n                  f'from {stream_type(strategy.experience)} stream ended.',\n                  file=self.file, flush=True)\n        self.print_current_metrics()\n        self.metric_vals = {}",
  "def before_training(self, strategy: 'BaseStrategy',\n                        metric_values: List['MetricValue'], **kwargs):\n        super().before_training(strategy, metric_values, **kwargs)\n        print('-- >> Start of training phase << --', file=self.file, flush=True)",
  "def before_eval(self, strategy: 'BaseStrategy',\n                    metric_values: List['MetricValue'], **kwargs):\n        super().before_eval(strategy, metric_values, **kwargs)\n        print('-- >> Start of eval phase << --', file=self.file, flush=True)",
  "def after_training(self, strategy: 'BaseStrategy',\n                       metric_values: List['MetricValue'], **kwargs):\n        super().after_training(strategy, metric_values, **kwargs)\n        print('-- >> End of training phase << --', file=self.file, flush=True)",
  "def after_eval(self, strategy: 'BaseStrategy',\n                   metric_values: List['MetricValue'], **kwargs):\n        super().after_eval(strategy, metric_values, **kwargs)\n        print('-- >> End of eval phase << --', file=self.file, flush=True)\n        self.print_current_metrics()\n        self.metric_vals = {}",
  "def _on_exp_start(self, strategy: 'BaseStrategy'):\n        action_name = 'training' if strategy.is_training else 'eval'\n        exp_id = strategy.experience.current_experience\n        task_id = phase_and_task(strategy)[1]\n        stream = stream_type(strategy.experience)\n        if task_id is None:\n            print('-- Starting {} on experience {} from {} stream --'\n                  .format(action_name, exp_id, stream),\n                  file=self.file,\n                  flush=True)\n        else:\n            print('-- Starting {} on experience {} (Task {}) from {} stream --'\n                  .format(action_name, exp_id, task_id, stream),\n                  file=self.file,\n                  flush=True)",
  "class TensorboardLogger(StrategyLogger):\n    \"\"\"\n    The `TensorboardLogger` provides an easy integration with\n    Tensorboard logging. Each monitored metric is automatically\n    logged to Tensorboard.\n    The user can inspect results in real time by appropriately launching\n    tensorboard with `tensorboard --logdir=/path/to/tb_log_exp_name`.\n\n    AWS's S3 buckets and (if tensorflow is installed) GCloud storage url are\n    supported.\n\n    If no parameters are provided, the default folder in which tensorboard\n    log files are placed is \"./runs/\".\n    .. note::\n        We rely on PyTorch implementation of Tensorboard. If you\n        don't have Tensorflow installed in your environment,\n        tensorboard will tell you that it is running with reduced\n        feature set. This should not impact on the logger performance.\n    \"\"\"\n\n    def __init__(self, tb_log_dir: Union[str, Path] = \"./tb_data\",\n                 filename_suffix: str = ''):\n        \"\"\"\n        Creates an instance of the `TensorboardLogger`.\n\n        :param tb_log_dir: path to the directory where tensorboard log file\n            will be stored. Default to \"./tb_data\".\n        :param filename_suffix: string suffix to append at the end of\n            tensorboard log file. Default ''.\n        \"\"\"\n\n        super().__init__()\n        tb_log_dir = _make_path_if_local(tb_log_dir)\n        self.writer = SummaryWriter(tb_log_dir,\n                                    filename_suffix=filename_suffix)\n\n    def __del__(self):\n        self.writer.close()\n\n    def log_metric(self, metric_value: MetricValue, callback: str):\n        super().log_metric(metric_value, callback)\n        name = metric_value.name\n        value = metric_value.value\n\n        if isinstance(value, AlternativeValues):\n            value = value.best_supported_value(Image, Tensor, TensorImage,\n                                               Figure, float, int)\n\n        if isinstance(value, Figure):\n            self.writer.add_figure(name, value,\n                                   global_step=metric_value.x_plot)\n\n        elif isinstance(value, Image):\n            self.writer.add_image(name, to_tensor(value),\n                                  global_step=metric_value.x_plot)\n\n        elif isinstance(value, Tensor):\n            self.writer.add_histogram(name, value,\n                                      global_step=metric_value.x_plot)\n\n        elif isinstance(value, (float, int)):\n            self.writer.add_scalar(name, value,\n                                   global_step=metric_value.x_plot)\n\n        elif isinstance(value, TensorImage):\n            self.writer.add_image(name, value.image,\n                                  global_step=metric_value.x_plot)",
  "def _make_path_if_local(tb_log_dir: Union[str, Path]) -> Union[str, Path]:\n    if isinstance(tb_log_dir, str) and _is_aws_or_gcloud_path(tb_log_dir):\n        return tb_log_dir\n\n    tb_log_dir = Path(tb_log_dir)\n    tb_log_dir.mkdir(parents=True, exist_ok=True)\n    return tb_log_dir",
  "def _is_aws_or_gcloud_path(tb_log_dir: str) -> bool:\n    return tb_log_dir.startswith(\"gs://\") or tb_log_dir.startswith(\"s3://\")",
  "def __init__(self, tb_log_dir: Union[str, Path] = \"./tb_data\",\n                 filename_suffix: str = ''):\n        \"\"\"\n        Creates an instance of the `TensorboardLogger`.\n\n        :param tb_log_dir: path to the directory where tensorboard log file\n            will be stored. Default to \"./tb_data\".\n        :param filename_suffix: string suffix to append at the end of\n            tensorboard log file. Default ''.\n        \"\"\"\n\n        super().__init__()\n        tb_log_dir = _make_path_if_local(tb_log_dir)\n        self.writer = SummaryWriter(tb_log_dir,\n                                    filename_suffix=filename_suffix)",
  "def __del__(self):\n        self.writer.close()",
  "def log_metric(self, metric_value: MetricValue, callback: str):\n        super().log_metric(metric_value, callback)\n        name = metric_value.name\n        value = metric_value.value\n\n        if isinstance(value, AlternativeValues):\n            value = value.best_supported_value(Image, Tensor, TensorImage,\n                                               Figure, float, int)\n\n        if isinstance(value, Figure):\n            self.writer.add_figure(name, value,\n                                   global_step=metric_value.x_plot)\n\n        elif isinstance(value, Image):\n            self.writer.add_image(name, to_tensor(value),\n                                  global_step=metric_value.x_plot)\n\n        elif isinstance(value, Tensor):\n            self.writer.add_histogram(name, value,\n                                      global_step=metric_value.x_plot)\n\n        elif isinstance(value, (float, int)):\n            self.writer.add_scalar(name, value,\n                                   global_step=metric_value.x_plot)\n\n        elif isinstance(value, TensorImage):\n            self.writer.add_image(name, value.image,\n                                  global_step=metric_value.x_plot)",
  "class Metric(Protocol[TResult]):\n    \"\"\"\n    Definition of a standalone metric.\n\n    A standalone metric exposes methods to reset its internal state and\n    to emit a result. Emitting a result does not automatically cause\n    a reset in the internal state.\n\n    The specific metric implementation exposes ways to update the internal\n    state. Usually, standalone metrics like :class:`Sum`, :class:`Mean`,\n    :class:`Accuracy`, ... expose an `update` method.\n\n    The `Metric` class can be used as a standalone metric by directly calling\n    its methods.\n    In order to automatically integrate the metric with the training and\n    evaluation flows, you can use :class:`PluginMetric` class. The class\n    receives events directly from the :class:`EvaluationPlugin` and can\n    emits values on each callback. Usually, an instance of `Metric` is\n    created within `PluginMetric`, which is then responsible for its\n    update and results. See :class:`PluginMetric` for more details.\n    \"\"\"\n\n    def result(self, **kwargs) -> Optional[TResult]:\n        \"\"\"\n        Obtains the value of the metric.\n\n        :return: The value of the metric.\n        \"\"\"\n        pass\n\n    def reset(self, **kwargs) -> None:\n        \"\"\"\n        Resets the metric internal state.\n\n        :return: None.\n        \"\"\"\n        pass",
  "class PluginMetric(Metric[TResult], StrategyCallbacks['MetricResult'], ABC):\n    \"\"\"\n    A metric that can be used together with :class:`EvaluationPlugin`.\n\n    This class leaves the implementation of the `result` and `reset` methods\n    to child classes while providing an empty implementation of the callbacks\n    invoked by the :class:`EvaluationPlugin`. Subclasses should implement\n    the `result`, `reset` and the desired callbacks to compute the specific\n    metric.\n\n    Remember to call the `super()` method when overriding\n    `after_train_iteration` or `after_eval_iteration`.\n\n    An instance of this class usually leverages a `Metric` instance to update,\n    reset and emit metric results at appropriate times\n    (during specific callbacks).\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Creates an instance of a plugin metric.\n\n        Child classes can safely invoke this (super) constructor as the first\n        experience.\n        \"\"\"\n        super().__init__()\n\n        self.global_it_counter = 0\n        \"\"\"\n        Counter that can be used by each metric to get increasing x values.\n        \"\"\"\n\n    @abstractmethod\n    def result(self, **kwargs) -> Optional[TResult]:\n        pass\n\n    @abstractmethod\n    def reset(self, **kwargs) -> None:\n        pass\n\n    def get_global_counter(self):\n        \"\"\"\n        :return: the global counter incremented after each minibatch.\n        \"\"\"\n        return self.global_it_counter\n\n    def before_training(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        pass\n\n    def before_training_exp(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        pass\n\n    def before_train_dataset_adaptation(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        pass\n\n    def after_train_dataset_adaptation(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        pass\n\n    def before_training_epoch(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        pass\n\n    def before_training_iteration(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        pass\n\n    def before_forward(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        pass\n\n    def after_forward(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        pass\n\n    def before_backward(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        pass\n\n    def after_backward(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        pass\n\n    def after_training_iteration(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        self.global_it_counter += 1\n\n    def before_update(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        pass\n\n    def after_update(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        pass\n\n    def after_training_epoch(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        pass\n\n    def after_training_exp(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        pass\n\n    def after_training(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        pass\n\n    def before_eval(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        pass\n\n    def before_eval_dataset_adaptation(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        pass\n\n    def after_eval_dataset_adaptation(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        pass\n\n    def before_eval_exp(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        pass\n\n    def after_eval_exp(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        pass\n\n    def after_eval(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        pass\n\n    def before_eval_iteration(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        pass\n\n    def before_eval_forward(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        pass\n\n    def after_eval_forward(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        pass\n\n    def after_eval_iteration(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        self.global_it_counter += 1",
  "class GenericPluginMetric(PluginMetric[TResult]):\n    \"\"\"\n    This class provides a generic implementation of a Plugin Metric.\n    The user can subclass this class to easily implement custom plugin\n    metrics.\n    \"\"\"\n    def __init__(self, metric, reset_at='experience', emit_at='experience',\n                 mode='eval'):\n        super(GenericPluginMetric, self).__init__()\n        assert mode in {'train', 'eval'}\n        if mode == 'train':\n            assert reset_at in {'iteration', 'epoch', 'experience', 'stream'}\n            assert emit_at in {'iteration', 'epoch', 'experience', 'stream'}\n        else:\n            assert reset_at in {'iteration', 'experience', 'stream'}\n            assert emit_at in {'iteration', 'experience', 'stream'}\n        self._metric = metric\n        self._reset_at = reset_at\n        self._emit_at = emit_at\n        self._mode = mode\n\n    def reset(self, strategy) -> None:\n        self._metric.reset()\n\n    def result(self, strategy):\n        return self._metric.result()\n\n    def update(self, strategy):\n        pass\n\n    def _package_result(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        metric_value = self.result(strategy)\n        add_exp = self._emit_at == 'experience'\n        plot_x_position = self.get_global_counter()\n\n        if isinstance(metric_value, dict):\n            metrics = []\n            for k, v in metric_value.items():\n                metric_name = get_metric_name(\n                    self, strategy, add_experience=add_exp, add_task=k)\n                metrics.append(MetricValue(self, metric_name, v,\n                                           plot_x_position))\n            return metrics\n        else:\n            metric_name = get_metric_name(self, strategy,\n                                          add_experience=add_exp,\n                                          add_task=True)\n            return [MetricValue(self, metric_name, metric_value,\n                                plot_x_position)]\n\n    def before_training(self, strategy: 'BaseStrategy'):\n        super().before_training(strategy)\n        if self._reset_at == 'stream' and self._mode == 'train':\n            self.reset()\n\n    def before_training_exp(self, strategy: 'BaseStrategy'):\n        super().before_training_exp(strategy)\n        if self._reset_at == 'experience' and self._mode == 'train':\n            self.reset(strategy)\n\n    def before_training_epoch(self, strategy: 'BaseStrategy'):\n        super().before_training_epoch(strategy)\n        if self._reset_at == 'epoch' and self._mode == 'train':\n            self.reset(strategy)\n\n    def before_training_iteration(self, strategy: 'BaseStrategy'):\n        super().before_training_iteration(strategy)\n        if self._reset_at == 'iteration' and self._mode == 'train':\n            self.reset(strategy)\n\n    def after_training_iteration(self, strategy: 'BaseStrategy') -> None:\n        super().after_training_iteration(strategy)\n        if self._mode == 'train':\n            self.update(strategy)\n        if self._emit_at == 'iteration' and self._mode == 'train':\n            return self._package_result(strategy)\n\n    def after_training_epoch(self, strategy: 'BaseStrategy'):\n        super().after_training_epoch(strategy)\n        if self._emit_at == 'epoch' and self._mode == 'train':\n            return self._package_result(strategy)\n\n    def after_training_exp(self, strategy: 'BaseStrategy'):\n        super().after_training_exp(strategy)\n        if self._emit_at == 'experience' and self._mode == 'train':\n            return self._package_result(strategy)\n\n    def after_training(self, strategy: 'BaseStrategy'):\n        super().after_training(strategy)\n        if self._emit_at == 'stream' and self._mode == 'train':\n            return self._package_result(strategy)\n\n    def before_eval(self, strategy: 'BaseStrategy'):\n        super().before_eval(strategy)\n        if self._reset_at == 'stream' and self._mode == 'eval':\n            self.reset(strategy)\n\n    def before_eval_exp(self, strategy: 'BaseStrategy'):\n        super().before_eval_exp(strategy)\n        if self._reset_at == 'experience' and self._mode == 'eval':\n            self.reset(strategy)\n\n    def after_eval_exp(self, strategy: 'BaseStrategy'):\n        super().after_eval_exp(strategy)\n        if self._emit_at == 'experience' and self._mode == 'eval':\n            return self._package_result(strategy)\n\n    def after_eval(self, strategy: 'BaseStrategy'):\n        super().after_eval(strategy)\n        if self._emit_at == 'stream' and self._mode == 'eval':\n            return self._package_result(strategy)\n\n    def after_eval_iteration(self, strategy: 'BaseStrategy'):\n        super().after_eval_iteration(strategy)\n        if self._mode == 'eval':\n            self.update(strategy)\n        if self._emit_at == 'iteration' and self._mode == 'eval':\n            return self._package_result(strategy)\n\n    def before_eval_iteration(self, strategy: 'BaseStrategy'):\n        super().before_eval_iteration(strategy)\n        if self._reset_at == 'iteration' and self._mode == 'eval':\n            self.reset(strategy)",
  "def result(self, **kwargs) -> Optional[TResult]:\n        \"\"\"\n        Obtains the value of the metric.\n\n        :return: The value of the metric.\n        \"\"\"\n        pass",
  "def reset(self, **kwargs) -> None:\n        \"\"\"\n        Resets the metric internal state.\n\n        :return: None.\n        \"\"\"\n        pass",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of a plugin metric.\n\n        Child classes can safely invoke this (super) constructor as the first\n        experience.\n        \"\"\"\n        super().__init__()\n\n        self.global_it_counter = 0\n        \"\"\"\n        Counter that can be used by each metric to get increasing x values.\n        \"\"\"",
  "def result(self, **kwargs) -> Optional[TResult]:\n        pass",
  "def reset(self, **kwargs) -> None:\n        pass",
  "def get_global_counter(self):\n        \"\"\"\n        :return: the global counter incremented after each minibatch.\n        \"\"\"\n        return self.global_it_counter",
  "def before_training(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        pass",
  "def before_training_exp(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        pass",
  "def before_train_dataset_adaptation(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        pass",
  "def after_train_dataset_adaptation(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        pass",
  "def before_training_epoch(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        pass",
  "def before_training_iteration(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        pass",
  "def before_forward(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        pass",
  "def after_forward(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        pass",
  "def before_backward(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        pass",
  "def after_backward(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        pass",
  "def after_training_iteration(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        self.global_it_counter += 1",
  "def before_update(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        pass",
  "def after_update(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        pass",
  "def after_training_epoch(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        pass",
  "def after_training_exp(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        pass",
  "def after_training(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        pass",
  "def before_eval(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        pass",
  "def before_eval_dataset_adaptation(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        pass",
  "def after_eval_dataset_adaptation(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        pass",
  "def before_eval_exp(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        pass",
  "def after_eval_exp(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        pass",
  "def after_eval(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        pass",
  "def before_eval_iteration(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        pass",
  "def before_eval_forward(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        pass",
  "def after_eval_forward(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        pass",
  "def after_eval_iteration(self, strategy: 'BaseStrategy') \\\n            -> 'MetricResult':\n        self.global_it_counter += 1",
  "def __init__(self, metric, reset_at='experience', emit_at='experience',\n                 mode='eval'):\n        super(GenericPluginMetric, self).__init__()\n        assert mode in {'train', 'eval'}\n        if mode == 'train':\n            assert reset_at in {'iteration', 'epoch', 'experience', 'stream'}\n            assert emit_at in {'iteration', 'epoch', 'experience', 'stream'}\n        else:\n            assert reset_at in {'iteration', 'experience', 'stream'}\n            assert emit_at in {'iteration', 'experience', 'stream'}\n        self._metric = metric\n        self._reset_at = reset_at\n        self._emit_at = emit_at\n        self._mode = mode",
  "def reset(self, strategy) -> None:\n        self._metric.reset()",
  "def result(self, strategy):\n        return self._metric.result()",
  "def update(self, strategy):\n        pass",
  "def _package_result(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        metric_value = self.result(strategy)\n        add_exp = self._emit_at == 'experience'\n        plot_x_position = self.get_global_counter()\n\n        if isinstance(metric_value, dict):\n            metrics = []\n            for k, v in metric_value.items():\n                metric_name = get_metric_name(\n                    self, strategy, add_experience=add_exp, add_task=k)\n                metrics.append(MetricValue(self, metric_name, v,\n                                           plot_x_position))\n            return metrics\n        else:\n            metric_name = get_metric_name(self, strategy,\n                                          add_experience=add_exp,\n                                          add_task=True)\n            return [MetricValue(self, metric_name, metric_value,\n                                plot_x_position)]",
  "def before_training(self, strategy: 'BaseStrategy'):\n        super().before_training(strategy)\n        if self._reset_at == 'stream' and self._mode == 'train':\n            self.reset()",
  "def before_training_exp(self, strategy: 'BaseStrategy'):\n        super().before_training_exp(strategy)\n        if self._reset_at == 'experience' and self._mode == 'train':\n            self.reset(strategy)",
  "def before_training_epoch(self, strategy: 'BaseStrategy'):\n        super().before_training_epoch(strategy)\n        if self._reset_at == 'epoch' and self._mode == 'train':\n            self.reset(strategy)",
  "def before_training_iteration(self, strategy: 'BaseStrategy'):\n        super().before_training_iteration(strategy)\n        if self._reset_at == 'iteration' and self._mode == 'train':\n            self.reset(strategy)",
  "def after_training_iteration(self, strategy: 'BaseStrategy') -> None:\n        super().after_training_iteration(strategy)\n        if self._mode == 'train':\n            self.update(strategy)\n        if self._emit_at == 'iteration' and self._mode == 'train':\n            return self._package_result(strategy)",
  "def after_training_epoch(self, strategy: 'BaseStrategy'):\n        super().after_training_epoch(strategy)\n        if self._emit_at == 'epoch' and self._mode == 'train':\n            return self._package_result(strategy)",
  "def after_training_exp(self, strategy: 'BaseStrategy'):\n        super().after_training_exp(strategy)\n        if self._emit_at == 'experience' and self._mode == 'train':\n            return self._package_result(strategy)",
  "def after_training(self, strategy: 'BaseStrategy'):\n        super().after_training(strategy)\n        if self._emit_at == 'stream' and self._mode == 'train':\n            return self._package_result(strategy)",
  "def before_eval(self, strategy: 'BaseStrategy'):\n        super().before_eval(strategy)\n        if self._reset_at == 'stream' and self._mode == 'eval':\n            self.reset(strategy)",
  "def before_eval_exp(self, strategy: 'BaseStrategy'):\n        super().before_eval_exp(strategy)\n        if self._reset_at == 'experience' and self._mode == 'eval':\n            self.reset(strategy)",
  "def after_eval_exp(self, strategy: 'BaseStrategy'):\n        super().after_eval_exp(strategy)\n        if self._emit_at == 'experience' and self._mode == 'eval':\n            return self._package_result(strategy)",
  "def after_eval(self, strategy: 'BaseStrategy'):\n        super().after_eval(strategy)\n        if self._emit_at == 'stream' and self._mode == 'eval':\n            return self._package_result(strategy)",
  "def after_eval_iteration(self, strategy: 'BaseStrategy'):\n        super().after_eval_iteration(strategy)\n        if self._mode == 'eval':\n            self.update(strategy)\n        if self._emit_at == 'iteration' and self._mode == 'eval':\n            return self._package_result(strategy)",
  "def before_eval_iteration(self, strategy: 'BaseStrategy'):\n        super().before_eval_iteration(strategy)\n        if self._reset_at == 'iteration' and self._mode == 'eval':\n            self.reset(strategy)",
  "class TensorImage:\n    image: Tensor\n\n    def __array__(self):\n        return self.image.numpy()",
  "class AlternativeValues:\n    \"\"\"\n    A container for alternative representations of the same metric value.\n    \"\"\"\n    def __init__(self, *alternatives: MetricType):\n        self.alternatives: Tuple[MetricType] = alternatives\n\n    def best_supported_value(self, *supported_types: type) -> \\\n            Optional[MetricType]:\n        \"\"\"\n        Retrieves a supported representation for this metric value.\n\n        :param supported_types: A list of supported value types.\n        :return: The best supported representation. Returns None if no supported\n            representation is found.\n        \"\"\"\n        for alternative in self.alternatives:\n            if isinstance(alternative, supported_types):\n                return alternative\n        return None",
  "class MetricValue(object):\n    \"\"\"\n    The result of a Metric.\n\n    A result has a name, a value and a \"x\" position in which the metric value\n    should be plotted.\n\n    The \"value\" field can also be an instance of \"AlternativeValues\", in which\n    case it means that alternative representations exist for this value. For\n    instance, the Confusion Matrix can be represented both as a Tensor and as\n    an Image. It's up to the Logger, according to its capabilities, decide which\n    representation to use.\n    \"\"\"\n    def __init__(self, origin: 'Metric', name: str,\n                 value: Union[MetricType, AlternativeValues], x_plot: int):\n        \"\"\"\n        Creates an instance of MetricValue.\n\n        :param origin: The originating Metric instance.\n        :param name: The display name of this value. This value roughly\n            corresponds to the name of the plot in which the value should\n            be logged.\n        :param value: The value of the metric. Can be a scalar value,\n            a PIL Image, or a Tensor. If more than a possible representation\n            of the same value exist, an instance of :class:`AlternativeValues`\n            can be passed. For instance, the Confusion Matrix can be represented\n            both as an Image and a Tensor, in which case an instance of\n            :class:`AlternativeValues` carrying both the Tensor and the Image\n            is more appropriate. The Logger instance will then select the most\n            appropriate way to log the metric according to its capabilities.\n        :param x_plot: The position of the value. This value roughly corresponds\n            to the x-axis position of the value in a plot. When logging a\n            singleton value, pass 0 as a value for this parameter.\n        \"\"\"\n        self.origin: 'Metric' = origin\n        self.name: str = name\n        self.value: Union[MetricType, AlternativeValues] = value\n        self.x_plot: int = x_plot",
  "def __array__(self):\n        return self.image.numpy()",
  "def __init__(self, *alternatives: MetricType):\n        self.alternatives: Tuple[MetricType] = alternatives",
  "def best_supported_value(self, *supported_types: type) -> \\\n            Optional[MetricType]:\n        \"\"\"\n        Retrieves a supported representation for this metric value.\n\n        :param supported_types: A list of supported value types.\n        :return: The best supported representation. Returns None if no supported\n            representation is found.\n        \"\"\"\n        for alternative in self.alternatives:\n            if isinstance(alternative, supported_types):\n                return alternative\n        return None",
  "def __init__(self, origin: 'Metric', name: str,\n                 value: Union[MetricType, AlternativeValues], x_plot: int):\n        \"\"\"\n        Creates an instance of MetricValue.\n\n        :param origin: The originating Metric instance.\n        :param name: The display name of this value. This value roughly\n            corresponds to the name of the plot in which the value should\n            be logged.\n        :param value: The value of the metric. Can be a scalar value,\n            a PIL Image, or a Tensor. If more than a possible representation\n            of the same value exist, an instance of :class:`AlternativeValues`\n            can be passed. For instance, the Confusion Matrix can be represented\n            both as an Image and a Tensor, in which case an instance of\n            :class:`AlternativeValues` carrying both the Tensor and the Image\n            is more appropriate. The Logger instance will then select the most\n            appropriate way to log the metric according to its capabilities.\n        :param x_plot: The position of the value. This value roughly corresponds\n            to the x-axis position of the value in a plot. When logging a\n            singleton value, pass 0 as a value for this parameter.\n        \"\"\"\n        self.origin: 'Metric' = origin\n        self.name: str = name\n        self.value: Union[MetricType, AlternativeValues] = value\n        self.x_plot: int = x_plot",
  "def default_cm_image_creator(confusion_matrix_tensor: Tensor,\n                             display_labels: Sequence = None,\n                             include_values=False,\n                             xticks_rotation=0,\n                             yticks_rotation=0,\n                             values_format=None,\n                             cmap='viridis',\n                             image_title=''):\n    \"\"\"\n    The default Confusion Matrix image creator.\n    Code adapted from\n    `Scikit learn <https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html>`_ # noqa\n\n    :param confusion_matrix_tensor: The tensor describing the confusion matrix.\n        This can be easily obtained through Scikit-learn `confusion_matrix`\n        utility.\n    :param display_labels: Target names used for plotting. By default, `labels`\n        will be used if it is defined, otherwise the values will be inferred by\n        the matrix tensor.\n    :param include_values: Includes values in confusion matrix. Defaults to\n        `False`.\n    :param xticks_rotation: Rotation of xtick labels. Valid values are\n        float point value. Defaults to 0.\n    :param yticks_rotation: Rotation of ytick labels. Valid values are\n        float point value. Defaults to 0.\n    :param values_format: Format specification for values in confusion matrix.\n        Defaults to `None`, which means that the format specification is\n        'd' or '.2g', whichever is shorter.\n    :param cmap: Must be a str or a Colormap recognized by matplotlib.\n        Defaults to 'viridis'.\n    :param image_title: The title of the image. Defaults to an empty string.\n    :return: The Confusion Matrix as a PIL Image.\n    \"\"\"\n\n    fig, ax = plt.subplots()\n\n    cm = confusion_matrix_tensor.numpy()\n    n_classes = cm.shape[0]\n    im_ = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    cmap_min, cmap_max = im_.cmap(0), im_.cmap(256)\n\n    if include_values:\n        text_ = np.empty_like(cm, dtype=object)\n\n        # print text with appropriate color depending on background\n        thresh = (cm.max() + cm.min()) / 2.0\n\n        for i in range(n_classes):\n            for j in range(n_classes):\n                color = cmap_max if cm[i, j] < thresh else cmap_min\n\n                if values_format is None:\n                    text_cm = format(cm[i, j], '.2g')\n                    if cm.dtype.kind != 'f':\n                        text_d = format(cm[i, j], 'd')\n                        if len(text_d) < len(text_cm):\n                            text_cm = text_d\n                else:\n                    text_cm = format(cm[i, j], values_format)\n\n                text_[i, j] = ax.text(\n                    j, i, text_cm,\n                    ha=\"center\", va=\"center\",\n                    color=color)\n\n    if display_labels is None:\n        display_labels = np.arange(n_classes)\n\n    fig.colorbar(im_, ax=ax)\n\n    ax.set(xticks=np.arange(n_classes),\n           yticks=np.arange(n_classes),\n           xticklabels=display_labels,\n           yticklabels=display_labels,\n           ylabel=\"True label\",\n           xlabel=\"Predicted label\")\n\n    if image_title != '':\n        ax.set_title(image_title)\n\n    ax.set_ylim((n_classes - 0.5, -0.5))\n    plt.setp(ax.get_xticklabels(), rotation=xticks_rotation)\n    plt.setp(ax.get_yticklabels(), rotation=yticks_rotation)\n\n    fig.tight_layout()\n    return fig",
  "def repartition_pie_chart_image_creator(\n    label2counts: Dict[int, List[int]],\n    counters: List[int],\n    colors: Union[ndarray, Iterable, int, float] = SEABORN_COLORS,\n    fmt: str = \"%1.1f%%\",\n):\n    \"\"\"\n    Create a pie chart representing the labels repartition.\n\n    :param label2counts: A dict holding the counts for each label, of the form\n        {label: [count_at_step_0, count_at_step_1, ...]}. Only the last count of\n        each label is used here.\n    :param counters: (unused) The steps the counts were taken at.\n    :param colors: The colors to use in the chart.\n    :param fmt: Formatting used to display the text values in the chart.\n    \"\"\"\n    fig, ax = plt.subplots()\n    ax: Axes\n\n    labels, counts = zip(*((label, c[-1]) for label, c in label2counts.items()))\n\n    ax.pie(counts, labels=labels, autopct=fmt, colors=colors)\n\n    fig.tight_layout()\n    return fig",
  "def repartition_bar_chart_image_creator(\n    label2counts: Dict[int, List[int]],\n    counters: List[int],\n    colors: Union[ndarray, Iterable, int, float] = SEABORN_COLORS,\n):\n    \"\"\"\n    Create a bar chart representing the labels repartition.\n\n    :param label2counts: A dict holding the counts for each label, of the form\n        {label: [count_at_step_0, count_at_step_1, ...]}. Only the last count of\n        each label is used here.\n    :param counters: (unused) The steps the counts were taken at.\n    :param colors: The colors to use in the chart.\n    \"\"\"\n    fig, ax = plt.subplots()\n    ax: Axes\n\n    y = -arange(len(label2counts))\n    labels, counts = zip(*((label, c[-1]) for label, c in label2counts.items()))\n    total = sum(counts)\n\n    ax.barh(y, width=counts, color=colors)\n    ax.set_yticks(y)\n    ax.set_yticklabels(labels)\n\n    ax.set_xlabel(\"Number of exemplars\")\n    ax.set_ylabel(\"Class\")\n\n    for i, count in enumerate(counts):\n        ax.text(count / 2, -i, f\"{count/total:.1%}\", va=\"center\", ha=\"center\")\n\n    fig.tight_layout()\n    return fig",
  "def default_history_repartition_image_creator(\n    label2counts: Dict[int, List[int]],\n    counters: List[int],\n    colors: Union[ndarray, Iterable, int, float] = SEABORN_COLORS,\n):\n    \"\"\"\n    Create a stack plot representing the labels repartition with their history.\n\n    :param label2counts: A dict holding the counts for each label, of the form\n        {label: [count_at_step_0, count_at_step_1, ...]}.\n    :param counters: The steps the counts were taken at.\n    :param colors: The colors to use in the chart.\n    \"\"\"\n    fig, ax = plt.subplots()\n    ax: Axes\n\n    ax.stackplot(\n        counters,\n        label2counts.values(),\n        labels=label2counts.keys(),\n        colors=colors,\n    )\n    ax.legend(loc='upper left')\n    ax.set_ylabel(\"Number of examples\")\n    ax.set_xlabel(\"step\")\n\n    fig.tight_layout()\n    return fig",
  "def stream_type(experience: 'Experience') -> str:\n    \"\"\"\n    Returns the stream name from which the experience belongs to.\n    e.g. the experience can be part of train or test stream.\n\n    :param experience: the instance of the experience\n    \"\"\"\n\n    return experience.origin_stream.name",
  "def phase_and_task(strategy: 'BaseStrategy') -> Tuple[str, int]:\n    \"\"\"\n    Returns the current phase name and the associated task label.\n\n    The current task label depends on the phase. During the training\n    phase, the task label is the one defined in the \"train_task_label\"\n    field. On the contrary, during the eval phase the task label is the one\n    defined in the \"eval_task_label\" field.\n\n    :param strategy: The strategy instance to get the task label from.\n    :return: The current phase name as either \"Train\" or \"Task\" and the\n        associated task label.\n    \"\"\"\n\n    task = strategy.experience.task_labels\n    if len(task) > 1:\n        task = None  # task labels per patterns\n    else:\n        task = task[0]\n\n    if strategy.is_eval:\n        return EVAL, task\n    else:\n        return TRAIN, task",
  "def bytes2human(n):\n    # http://code.activestate.com/recipes/578019\n    # >>> bytes2human(10000)\n    # '9.8K'\n    # >>> bytes2human(100001221)\n    # '95.4M'\n    symbols = ('K', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')\n    prefix = {}\n    for i, s in enumerate(symbols):\n        prefix[s] = 1 << (i + 1) * 10\n    for s in reversed(symbols):\n        if n >= prefix[s]:\n            value = float(n) / prefix[s]\n            return '%.1f%s' % (value, s)\n    return \"%sB\" % n",
  "def get_metric_name(metric: 'PluginMetric',\n                    strategy: 'BaseStrategy',\n                    add_experience=False,\n                    add_task=True):\n    \"\"\"\n    Return the complete metric name used to report its current value.\n    The name is composed by:\n    metric string representation /phase type/stream type/task id\n    where metric string representation is a synthetic string\n    describing the metric, phase type describe if the user\n    is training (train) or evaluating (eval), stream type describes\n    the type of stream the current experience belongs to (e.g. train, test)\n    and task id is the current task label.\n\n    :param metric: the metric object for which return the complete name\n    :param strategy: the current strategy object\n    :param add_experience: if True, add eval_exp_id to the main metric name.\n            Default to False.\n    :param add_task: if True the main metric name will include the task\n        information. If False, no task label will be displayed.\n        If an int, that value will be used as task label for the metric name.\n    \"\"\"\n\n    phase_name, task_label = phase_and_task(strategy)\n    stream = stream_type(strategy.experience)\n    base_name = '{}/{}_phase/{}_stream'.format(str(metric),\n                                               phase_name, stream)\n    exp_name = '/Exp{:03}'.format(strategy.experience.current_experience)\n\n    if task_label is None and isinstance(add_task, bool):\n        add_task = False\n    else:\n        if isinstance(add_task, bool) and add_task:\n            task_name = '/Task{:03}'.format(task_label)\n        elif isinstance(add_task, int):\n            task_name = '/Task{:03}'.format(add_task)\n            add_task = True\n\n    if add_experience and not add_task:\n        return base_name + exp_name\n    elif add_experience and add_task:\n        return base_name + task_name + exp_name\n    elif not add_experience and not add_task:\n        return base_name\n    elif not add_experience and add_task:\n        return base_name + task_name",
  "class CPUUsage(Metric[float]):\n    \"\"\"\n    The standalone CPU usage metric.\n\n    Instances of this metric compute the average CPU usage as a float value.\n    The metric starts tracking the CPU usage when the `update` method is called\n    for the first time. That is, the tracking does not start at the time the\n    constructor is invoked.\n\n    Calling the `update` method more than twice will update the metric to the\n    average usage between the first and the last call to `update`.\n\n    The result, obtained using the `result` method, is the usage computed\n    as stated above.\n\n    The reset method will bring the metric to its initial state. By default\n    this metric in its initial state will return an usage value of 0.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the standalone CPU usage metric.\n\n        By default this metric in its initial state will return a CPU usage\n        value of 0. The metric can be updated by using the `update` method\n        while the average CPU usage can be retrieved using the `result` method.\n        \"\"\"\n\n        self._mean_usage = Mean()\n        \"\"\"\n        The mean utility that will be used to store the average usage.\n        \"\"\"\n\n        self._process_handle: Optional[Process] = None\n        \"\"\"\n        The process handle, lazily initialized.\n        \"\"\"\n\n        self._first_update = True\n        \"\"\"\n        An internal flag to keep track of the first call to the `update` method.\n        \"\"\"\n\n    def update(self) -> None:\n        \"\"\"\n        Update the running CPU usage.\n\n        For more info on how to set the starting moment see the class\n        description.\n\n        :return: None.\n        \"\"\"\n        if self._first_update:\n            self._process_handle = Process(os.getpid())\n\n        last_time = getattr(\n            self._process_handle, '_last_sys_cpu_times', None)\n        utilization = self._process_handle.cpu_percent()\n        current_time = getattr(\n            self._process_handle, '_last_sys_cpu_times', None)\n\n        if self._first_update:\n            self._first_update = False\n        else:\n            if current_time is None or last_time is None:\n                warnings.warn('CPUUsage can\\'t detect the elapsed time. It is '\n                              'recommended to update avalanche to the latest '\n                              'version.')\n                # Fallback, shouldn't happen\n                current_time = 1.0\n                last_time = 0.0\n            self._mean_usage.update(utilization, current_time - last_time)\n\n    def result(self) -> float:\n        \"\"\"\n        Retrieves the average CPU usage.\n\n        Calling this method will not change the internal state of the metric.\n\n        :return: The average CPU usage, as a float value.\n        \"\"\"\n        return self._mean_usage.result()\n\n    def reset(self) -> None:\n        \"\"\"\n        Resets the metric.\n\n        :return: None.\n        \"\"\"\n        self._mean_usage.reset()\n        self._process_handle = None\n        self._first_update = True",
  "class CPUPluginMetric(GenericPluginMetric[float]):\n    def __init__(self, reset_at, emit_at, mode):\n        self._cpu = CPUUsage()\n\n        super(CPUPluginMetric, self).__init__(\n            self._cpu, reset_at=reset_at, emit_at=emit_at,\n            mode=mode)\n\n    def update(self, strategy):\n        self._cpu.update()",
  "class MinibatchCPUUsage(CPUPluginMetric):\n    \"\"\"\n    The minibatch CPU usage metric.\n    This plugin metric only works at training time.\n\n    This metric \"logs\" the CPU usage for each iteration.\n\n    If a more coarse-grained logging is needed, consider using\n    :class:`EpochCPUUsage`.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the minibatch CPU usage metric.\n        \"\"\"\n        super(MinibatchCPUUsage, self).__init__(\n            reset_at='iteration', emit_at='iteration', mode='train')\n\n    def before_training_iteration(self, strategy):\n        super().before_training_iteration(strategy)\n        self.update(strategy)  # start monitoring thread\n\n    def __str__(self):\n        return \"CPUUsage_MB\"",
  "class EpochCPUUsage(CPUPluginMetric):\n    \"\"\"\n    The Epoch CPU usage metric.\n    This plugin metric only works at training time.\n\n    The average usage will be logged after each epoch.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the epoch CPU usage metric.\n        \"\"\"\n        super(EpochCPUUsage, self).__init__(\n            reset_at='epoch', emit_at='epoch', mode='train')\n\n    def before_training_epoch(self, strategy):\n        super().before_training_epoch(strategy)\n        self.update(strategy)  # start monitoring thread\n\n    def __str__(self):\n        return \"CPUUsage_Epoch\"",
  "class RunningEpochCPUUsage(CPUPluginMetric):\n    \"\"\"\n    The running epoch CPU usage metric.\n    This plugin metric only works at training time\n\n    After each iteration, the metric logs the average CPU usage up\n    to the current epoch iteration.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the average epoch cpu usage metric.\n        \"\"\"\n        self._mean = Mean()\n        super(RunningEpochCPUUsage, self).__init__(\n            reset_at='epoch', emit_at='iteration', mode='train')\n\n    def result(self, strategy) -> float:\n        return self._mean.result()\n\n    def before_training_epoch(self, strategy):\n        super().before_training_epoch(strategy)\n        self._mean.reset()\n\n    def before_training_iteration(self, strategy):\n        super().before_training_iteration(strategy)\n        self.update(strategy)  # start monitoring thread\n\n    def after_training_iteration(self, strategy):\n        super().after_training_iteration(strategy)\n        self.update(strategy)\n        self._mean.update(self._cpu.result())\n        self._cpu.reset()\n        return self._package_result(strategy)\n\n    def __str__(self):\n        return \"RunningCPUUsage_Epoch\"",
  "class ExperienceCPUUsage(CPUPluginMetric):\n    \"\"\"\n    The average experience CPU usage metric.\n    This plugin metric works only at eval time.\n\n    After each experience, this metric emits the average CPU usage on that\n    experience.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the experience CPU usage metric.\n        \"\"\"\n        super(ExperienceCPUUsage, self).__init__(\n            reset_at='experience', emit_at='experience', mode='eval')\n\n    def before_eval_exp(self, strategy):\n        super().before_eval_exp(strategy)\n        self.update(strategy)  # start monitoring thread\n\n    def __str__(self):\n        return \"CPUUsage_Exp\"",
  "class StreamCPUUsage(CPUPluginMetric):\n    \"\"\"\n    The average stream CPU usage metric.\n    This plugin metric works only at eval time.\n\n    After the entire evaluation stream, this metric emits\n    the average CPU usage on all experiences.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the stream CPU usage metric.\n        \"\"\"\n        super(StreamCPUUsage, self).__init__(\n            reset_at='stream', emit_at='stream', mode='eval')\n\n    def before_eval(self, strategy):\n        super().before_eval(strategy)\n        self.update(strategy)  # start monitoring thread\n\n    def __str__(self):\n        return \"CPUUsage_Stream\"",
  "def cpu_usage_metrics(*, minibatch=False, epoch=False, epoch_running=False,\n                      experience=False, stream=False) -> List[PluginMetric]:\n    \"\"\"\n    Helper method that can be used to obtain the desired set of\n    plugin metrics.\n\n    :param minibatch: If True, will return a metric able to log the minibatch\n        CPU usage\n    :param epoch: If True, will return a metric able to log the epoch\n        CPU usage\n    :param epoch_running: If True, will return a metric able to log the running\n        epoch CPU usage.\n    :param experience: If True, will return a metric able to log the experience\n        CPU usage.\n    :param stream: If True, will return a metric able to log the evaluation\n        stream CPU usage.\n\n    :return: A list of plugin metrics.\n    \"\"\"\n\n    metrics = []\n    if minibatch:\n        metrics.append(MinibatchCPUUsage())\n\n    if epoch:\n        metrics.append(EpochCPUUsage())\n\n    if epoch_running:\n        metrics.append(RunningEpochCPUUsage())\n\n    if experience:\n        metrics.append(ExperienceCPUUsage())\n\n    if stream:\n        metrics.append(StreamCPUUsage())\n\n    return metrics",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the standalone CPU usage metric.\n\n        By default this metric in its initial state will return a CPU usage\n        value of 0. The metric can be updated by using the `update` method\n        while the average CPU usage can be retrieved using the `result` method.\n        \"\"\"\n\n        self._mean_usage = Mean()\n        \"\"\"\n        The mean utility that will be used to store the average usage.\n        \"\"\"\n\n        self._process_handle: Optional[Process] = None\n        \"\"\"\n        The process handle, lazily initialized.\n        \"\"\"\n\n        self._first_update = True\n        \"\"\"\n        An internal flag to keep track of the first call to the `update` method.\n        \"\"\"",
  "def update(self) -> None:\n        \"\"\"\n        Update the running CPU usage.\n\n        For more info on how to set the starting moment see the class\n        description.\n\n        :return: None.\n        \"\"\"\n        if self._first_update:\n            self._process_handle = Process(os.getpid())\n\n        last_time = getattr(\n            self._process_handle, '_last_sys_cpu_times', None)\n        utilization = self._process_handle.cpu_percent()\n        current_time = getattr(\n            self._process_handle, '_last_sys_cpu_times', None)\n\n        if self._first_update:\n            self._first_update = False\n        else:\n            if current_time is None or last_time is None:\n                warnings.warn('CPUUsage can\\'t detect the elapsed time. It is '\n                              'recommended to update avalanche to the latest '\n                              'version.')\n                # Fallback, shouldn't happen\n                current_time = 1.0\n                last_time = 0.0\n            self._mean_usage.update(utilization, current_time - last_time)",
  "def result(self) -> float:\n        \"\"\"\n        Retrieves the average CPU usage.\n\n        Calling this method will not change the internal state of the metric.\n\n        :return: The average CPU usage, as a float value.\n        \"\"\"\n        return self._mean_usage.result()",
  "def reset(self) -> None:\n        \"\"\"\n        Resets the metric.\n\n        :return: None.\n        \"\"\"\n        self._mean_usage.reset()\n        self._process_handle = None\n        self._first_update = True",
  "def __init__(self, reset_at, emit_at, mode):\n        self._cpu = CPUUsage()\n\n        super(CPUPluginMetric, self).__init__(\n            self._cpu, reset_at=reset_at, emit_at=emit_at,\n            mode=mode)",
  "def update(self, strategy):\n        self._cpu.update()",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the minibatch CPU usage metric.\n        \"\"\"\n        super(MinibatchCPUUsage, self).__init__(\n            reset_at='iteration', emit_at='iteration', mode='train')",
  "def before_training_iteration(self, strategy):\n        super().before_training_iteration(strategy)\n        self.update(strategy)",
  "def __str__(self):\n        return \"CPUUsage_MB\"",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the epoch CPU usage metric.\n        \"\"\"\n        super(EpochCPUUsage, self).__init__(\n            reset_at='epoch', emit_at='epoch', mode='train')",
  "def before_training_epoch(self, strategy):\n        super().before_training_epoch(strategy)\n        self.update(strategy)",
  "def __str__(self):\n        return \"CPUUsage_Epoch\"",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the average epoch cpu usage metric.\n        \"\"\"\n        self._mean = Mean()\n        super(RunningEpochCPUUsage, self).__init__(\n            reset_at='epoch', emit_at='iteration', mode='train')",
  "def result(self, strategy) -> float:\n        return self._mean.result()",
  "def before_training_epoch(self, strategy):\n        super().before_training_epoch(strategy)\n        self._mean.reset()",
  "def before_training_iteration(self, strategy):\n        super().before_training_iteration(strategy)\n        self.update(strategy)",
  "def after_training_iteration(self, strategy):\n        super().after_training_iteration(strategy)\n        self.update(strategy)\n        self._mean.update(self._cpu.result())\n        self._cpu.reset()\n        return self._package_result(strategy)",
  "def __str__(self):\n        return \"RunningCPUUsage_Epoch\"",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the experience CPU usage metric.\n        \"\"\"\n        super(ExperienceCPUUsage, self).__init__(\n            reset_at='experience', emit_at='experience', mode='eval')",
  "def before_eval_exp(self, strategy):\n        super().before_eval_exp(strategy)\n        self.update(strategy)",
  "def __str__(self):\n        return \"CPUUsage_Exp\"",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the stream CPU usage metric.\n        \"\"\"\n        super(StreamCPUUsage, self).__init__(\n            reset_at='stream', emit_at='stream', mode='eval')",
  "def before_eval(self, strategy):\n        super().before_eval(strategy)\n        self.update(strategy)",
  "def __str__(self):\n        return \"CPUUsage_Stream\"",
  "class ElapsedTime(Metric[float]):\n    \"\"\"\n    The standalone Elapsed Time metric.\n\n    Instances of this metric keep track of the time elapsed between calls to the\n    `update` method. The starting time is set when the `update` method is called\n    for the first time. That is, the starting time is *not* taken at the time\n    the constructor is invoked.\n\n    Calling the `update` method more than twice will update the metric to the\n    elapsed time between the first and the last call to `update`.\n\n    The result, obtained using the `result` method, is the time, in seconds,\n    computed as stated above.\n\n    The `reset` method will set the metric to its initial state, thus resetting\n    the initial time. This metric in its initial state (or if the `update`\n    method was invoked only once) will return an elapsed time of 0.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the ElapsedTime metric.\n\n        This metric in its initial state (or if the `update` method was invoked\n        only once) will return an elapsed time of 0. The metric can be updated\n        by using the `update` method while the running accuracy can be retrieved\n        using the `result` method.\n        \"\"\"\n        self._init_time = None\n        self._prev_time = None\n\n    def update(self) -> None:\n        \"\"\"\n        Update the elapsed time.\n\n        For more info on how to set the initial time see the class description.\n\n        :return: None.\n        \"\"\"\n        now = time.perf_counter()\n        if self._init_time is None:\n            self._init_time = now\n        self._prev_time = now\n\n    def result(self) -> float:\n        \"\"\"\n        Retrieves the elapsed time.\n\n        Calling this method will not change the internal state of the metric.\n\n        :return: The elapsed time, in seconds, as a float value.\n        \"\"\"\n        if self._init_time is None:\n            return 0.0\n        return self._prev_time - self._init_time\n\n    def reset(self) -> None:\n        \"\"\"\n        Resets the metric, including the initial time.\n\n        :return: None.\n        \"\"\"\n        self._prev_time = None\n        self._init_time = None",
  "class TimePluginMetric(GenericPluginMetric[float]):\n    def __init__(self, reset_at, emit_at, mode):\n        self._time = ElapsedTime()\n\n        super(TimePluginMetric, self).__init__(\n            self._time, reset_at, emit_at, mode)\n\n    def update(self, strategy):\n        self._time.update()",
  "class MinibatchTime(TimePluginMetric):\n    \"\"\"\n    The minibatch time metric.\n    This plugin metric only works at training time.\n\n    This metric \"logs\" the elapsed time for each iteration.\n\n    If a more coarse-grained logging is needed, consider using\n    :class:`EpochTime`.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the minibatch time metric.\n        \"\"\"\n        super(MinibatchTime, self).__init__(\n            reset_at='iteration', emit_at='iteration', mode='train')\n\n    def before_training_iteration(self, strategy) -> MetricResult:\n        super().before_training_iteration(strategy)\n        self._time.update()\n\n    def __str__(self):\n        return \"Time_MB\"",
  "class EpochTime(TimePluginMetric):\n    \"\"\"\n    The epoch elapsed time metric.\n    This plugin metric only works at training time.\n\n    The elapsed time will be logged after each epoch.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the epoch time metric.\n        \"\"\"\n\n        super(EpochTime, self).__init__(\n            reset_at='epoch', emit_at='epoch', mode='train')\n\n    def before_training_epoch(self, strategy):\n        super().before_training_epoch(strategy)\n        self._time.update()\n\n    def __str__(self):\n        return \"Time_Epoch\"",
  "class RunningEpochTime(TimePluginMetric):\n    \"\"\"\n    The running epoch time metric.\n    This plugin metric only works at training time.\n\n    For each iteration, this metric logs the average time\n    between the start of the\n    epoch and the current iteration.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the running epoch time metric..\n        \"\"\"\n        self._time_mean = Mean()\n\n        super(RunningEpochTime, self).__init__(\n            reset_at='epoch', emit_at='iteration', mode='train')\n\n    def before_training_epoch(self, strategy):\n        super().before_training_epoch(strategy)\n        self._time_mean.reset()\n        self._time.update()\n\n    def after_training_iteration(self, strategy: 'BaseStrategy') \\\n            -> MetricResult:\n        super().after_training_iteration(strategy)\n        self._time_mean.update(self._time.result())\n        self._time.reset()\n        return self._package_result(strategy)\n\n    def result(self, strategy) -> float:\n        return self._time_mean.result()\n\n    def __str__(self):\n        return \"RunningTime_Epoch\"",
  "class ExperienceTime(TimePluginMetric):\n    \"\"\"\n    The experience time metric.\n    This plugin metric only works at eval time.\n\n    After each experience, this metric emits the average time of that\n    experience.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the experience time metric.\n        \"\"\"\n        super(ExperienceTime, self).__init__(\n            reset_at='experience', emit_at='experience', mode='eval')\n\n    def before_eval_exp(self, strategy: 'BaseStrategy'):\n        super().before_eval_exp(strategy)\n        self._time.update()\n\n    def __str__(self):\n        return \"Time_Exp\"",
  "class StreamTime(TimePluginMetric):\n    \"\"\"\n    The stream time metric.\n    This metric only works at eval time.\n\n    After the entire evaluation stream,\n    this plugin metric emits the average time of that stream.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the stream time metric.\n        \"\"\"\n        super(StreamTime, self).__init__(\n            reset_at='stream', emit_at='stream', mode='eval')\n\n    def before_eval(self, strategy: 'BaseStrategy'):\n        super().before_eval(strategy)\n        self._time.update()\n\n    def __str__(self):\n        return \"Time_Stream\"",
  "def timing_metrics(*, minibatch=False, epoch=False, epoch_running=False,\n                   experience=False, stream=False) -> List[PluginMetric]:\n    \"\"\"\n    Helper method that can be used to obtain the desired set of\n    plugin metrics.\n\n    :param minibatch: If True, will return a metric able to log the train\n        minibatch elapsed time.\n    :param epoch: If True, will return a metric able to log the train epoch\n        elapsed time.\n    :param epoch_running: If True, will return a metric able to log the running\n        train epoch elapsed time.\n    :param experience: If True, will return a metric able to log the eval\n        experience elapsed time.\n    :param stream: If True, will return a metric able to log the eval stream\n        elapsed time.\n\n    :return: A list of plugin metrics.\n    \"\"\"\n\n    metrics = []\n    if minibatch:\n        metrics.append(MinibatchTime())\n\n    if epoch:\n        metrics.append(EpochTime())\n\n    if epoch_running:\n        metrics.append(RunningEpochTime())\n\n    if experience:\n        metrics.append(ExperienceTime())\n\n    if stream:\n        metrics.append(StreamTime())\n\n    return metrics",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the ElapsedTime metric.\n\n        This metric in its initial state (or if the `update` method was invoked\n        only once) will return an elapsed time of 0. The metric can be updated\n        by using the `update` method while the running accuracy can be retrieved\n        using the `result` method.\n        \"\"\"\n        self._init_time = None\n        self._prev_time = None",
  "def update(self) -> None:\n        \"\"\"\n        Update the elapsed time.\n\n        For more info on how to set the initial time see the class description.\n\n        :return: None.\n        \"\"\"\n        now = time.perf_counter()\n        if self._init_time is None:\n            self._init_time = now\n        self._prev_time = now",
  "def result(self) -> float:\n        \"\"\"\n        Retrieves the elapsed time.\n\n        Calling this method will not change the internal state of the metric.\n\n        :return: The elapsed time, in seconds, as a float value.\n        \"\"\"\n        if self._init_time is None:\n            return 0.0\n        return self._prev_time - self._init_time",
  "def reset(self) -> None:\n        \"\"\"\n        Resets the metric, including the initial time.\n\n        :return: None.\n        \"\"\"\n        self._prev_time = None\n        self._init_time = None",
  "def __init__(self, reset_at, emit_at, mode):\n        self._time = ElapsedTime()\n\n        super(TimePluginMetric, self).__init__(\n            self._time, reset_at, emit_at, mode)",
  "def update(self, strategy):\n        self._time.update()",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the minibatch time metric.\n        \"\"\"\n        super(MinibatchTime, self).__init__(\n            reset_at='iteration', emit_at='iteration', mode='train')",
  "def before_training_iteration(self, strategy) -> MetricResult:\n        super().before_training_iteration(strategy)\n        self._time.update()",
  "def __str__(self):\n        return \"Time_MB\"",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the epoch time metric.\n        \"\"\"\n\n        super(EpochTime, self).__init__(\n            reset_at='epoch', emit_at='epoch', mode='train')",
  "def before_training_epoch(self, strategy):\n        super().before_training_epoch(strategy)\n        self._time.update()",
  "def __str__(self):\n        return \"Time_Epoch\"",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the running epoch time metric..\n        \"\"\"\n        self._time_mean = Mean()\n\n        super(RunningEpochTime, self).__init__(\n            reset_at='epoch', emit_at='iteration', mode='train')",
  "def before_training_epoch(self, strategy):\n        super().before_training_epoch(strategy)\n        self._time_mean.reset()\n        self._time.update()",
  "def after_training_iteration(self, strategy: 'BaseStrategy') \\\n            -> MetricResult:\n        super().after_training_iteration(strategy)\n        self._time_mean.update(self._time.result())\n        self._time.reset()\n        return self._package_result(strategy)",
  "def result(self, strategy) -> float:\n        return self._time_mean.result()",
  "def __str__(self):\n        return \"RunningTime_Epoch\"",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the experience time metric.\n        \"\"\"\n        super(ExperienceTime, self).__init__(\n            reset_at='experience', emit_at='experience', mode='eval')",
  "def before_eval_exp(self, strategy: 'BaseStrategy'):\n        super().before_eval_exp(strategy)\n        self._time.update()",
  "def __str__(self):\n        return \"Time_Exp\"",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the stream time metric.\n        \"\"\"\n        super(StreamTime, self).__init__(\n            reset_at='stream', emit_at='stream', mode='eval')",
  "def before_eval(self, strategy: 'BaseStrategy'):\n        super().before_eval(strategy)\n        self._time.update()",
  "def __str__(self):\n        return \"Time_Stream\"",
  "class MaxGPU(Metric[float]):\n    \"\"\"\n    The standalone GPU usage metric.\n    Important: this metric approximates the real maximum GPU percentage\n     usage since it sample at discrete amount of time the GPU values.\n\n    Instances of this metric keeps the maximum GPU usage percentage detected.\n    The `start_thread` method starts the usage tracking.\n    The `stop_thread` method stops the tracking.\n\n    The result, obtained using the `result` method, is the usage in mega-bytes.\n\n    The reset method will bring the metric to its initial state. By default\n    this metric in its initial state will return an usage value of 0.\n    \"\"\"\n\n    def __init__(self, gpu_id, every=0.5):\n        \"\"\"\n        Creates an instance of the GPU usage metric.\n\n        :param gpu_id: GPU device ID.\n        :param every: seconds after which update the maximum GPU\n            usage\n        \"\"\"\n\n        self.every = every\n        self.gpu_id = gpu_id\n\n        n_gpus = len(GPUtil.getGPUs())\n        if n_gpus == 0:\n            warnings.warn(\"Your system has no GPU!\")\n            self.gpu_id = None\n        elif gpu_id < 0:\n            warnings.warn(\"GPU metric called with negative GPU id.\"\n                          \"GPU logging disabled\")\n            self.gpu_id = None\n        else:\n            if gpu_id >= n_gpus:\n                warnings.warn(f\"GPU {gpu_id} not found. Using GPU 0.\")\n                self.gpu_id = 0\n\n        self.thread = None\n        \"\"\"\n        Thread executing GPU monitoring code\n        \"\"\"\n\n        self.stop_f = False\n        \"\"\"\n        Flag to stop the thread\n        \"\"\"\n\n        self.max_usage = 0\n        \"\"\"\n        Main metric result. Max GPU usage.\n        \"\"\"\n\n    def _f(self):\n        \"\"\"\n        Until a stop signal is encountered,\n        this function monitors each `every` seconds\n        the maximum amount of GPU used by the process\n        \"\"\"\n        start_time = time.monotonic()\n        while not self.stop_f:\n            # GPU percentage\n            gpu_perc = GPUtil.getGPUs()[self.gpu_id].load * 100\n            if gpu_perc > self.max_usage:\n                self.max_usage = gpu_perc\n            time.sleep(self.every - ((time.monotonic() - start_time)\n                                     % self.every))\n\n    def start_thread(self):\n        if self.gpu_id is not None:\n            assert not self.thread, \"Trying to start thread \" \\\n                                    \"without joining the previous.\"\n            self.thread = Thread(target=self._f, daemon=True)\n            self.thread.start()\n\n    def stop_thread(self):\n        if self.thread:\n            self.stop_f = True\n            self.thread.join()\n            self.stop_f = False\n            self.thread = None\n\n    def reset(self) -> None:\n        \"\"\"\n        Resets the metric.\n\n        :return: None.\n        \"\"\"\n        self.max_usage = 0\n\n    def result(self) -> Optional[float]:\n        \"\"\"\n        Returns the max GPU percentage value.\n\n        :return: The percentage GPU usage as a float value in range [0, 1].\n        \"\"\"\n        return self.max_usage\n\n    def update(self):\n        pass",
  "class GPUPluginMetric(GenericPluginMetric[float]):\n    def __init__(self, gpu_id, every, reset_at, emit_at, mode):\n        self.gpu_id = gpu_id\n        self._gpu = MaxGPU(gpu_id, every)\n\n        super(GPUPluginMetric, self).__init__(\n            self._gpu, reset_at=reset_at, emit_at=emit_at,\n            mode=mode)\n\n    def update(self, strategy):\n        self._gpu.update()",
  "class MinibatchMaxGPU(GPUPluginMetric):\n    \"\"\"\n    The Minibatch Max GPU metric.\n    This plugin metric only works at training time.\n    \"\"\"\n\n    def __init__(self, gpu_id, every=0.5):\n        \"\"\"\n        Creates an instance of the Minibatch Max GPU metric\n\n        :param gpu_id: GPU device ID.\n        :param every: seconds after which update the maximum GPU\n            usage\n        \"\"\"\n        super(MinibatchMaxGPU, self).__init__(\n            gpu_id, every,\n            reset_at='iteration', emit_at='iteration', mode='train')\n\n    def before_training(self, strategy: 'BaseStrategy') \\\n            -> None:\n        super().before_training(strategy)\n        self._gpu.start_thread()\n\n    def after_training(self, strategy: 'BaseStrategy') -> None:\n        super().before_training(strategy)\n        self._gpu.stop_thread()\n\n    def __str__(self):\n        return f\"MaxGPU{self.gpu_id}Usage_MB\"",
  "class EpochMaxGPU(GPUPluginMetric):\n    \"\"\"\n    The Epoch Max GPU metric.\n    This plugin metric only works at training time.\n    \"\"\"\n\n    def __init__(self, gpu_id, every=0.5):\n        \"\"\"\n        Creates an instance of the epoch Max GPU metric.\n\n        :param gpu_id: GPU device ID.\n        :param every: seconds after which update the maximum GPU\n            usage\n        \"\"\"\n        super(EpochMaxGPU, self).__init__(\n            gpu_id, every,\n            reset_at='epoch', emit_at='epoch', mode='train')\n\n    def before_training(self, strategy: 'BaseStrategy'):\n        super().before_training(strategy)\n        self._gpu.start_thread()\n\n    def after_training(self, strategy: 'BaseStrategy') -> None:\n        self._gpu.stop_thread()\n\n    def __str__(self):\n        return f\"MaxGPU{self.gpu_id}Usage_Epoch\"",
  "class ExperienceMaxGPU(GPUPluginMetric):\n    \"\"\"\n    The Experience Max GPU metric.\n    This plugin metric only works at eval time.\n    \"\"\"\n\n    def __init__(self, gpu_id, every=0.5):\n        \"\"\"\n        Creates an instance of the Experience CPU usage metric.\n\n        :param gpu_id: GPU device ID.\n        :param every: seconds after which update the maximum GPU\n            usage\n        \"\"\"\n        super(ExperienceMaxGPU, self).__init__(\n            gpu_id, every,\n            reset_at='experience', emit_at='experience', mode='eval')\n\n    def before_eval(self, strategy: 'BaseStrategy'):\n        super().before_eval(strategy)\n        self._gpu.start_thread()\n\n    def after_eval(self, strategy: 'BaseStrategy'):\n        super().after_eval(strategy)\n        self._gpu.stop_thread()\n\n    def __str__(self):\n        return f\"MaxGPU{self.gpu_id}Usage_Experience\"",
  "class StreamMaxGPU(GPUPluginMetric):\n    \"\"\"\n    The Stream Max GPU metric.\n    This plugin metric only works at eval time.\n    \"\"\"\n\n    def __init__(self, gpu_id, every=0.5):\n        \"\"\"\n        Creates an instance of the Experience CPU usage metric.\n\n        :param gpu_id: GPU device ID.\n        :param every: seconds after which update the maximum GPU\n            usage\n        \"\"\"\n        super(StreamMaxGPU, self).__init__(\n            gpu_id, every,\n            reset_at='stream', emit_at='stream', mode='eval')\n\n    def before_eval(self, strategy):\n        super().before_eval(strategy)\n        self._gpu.start_thread()\n\n    def after_eval(self, strategy: 'BaseStrategy') \\\n            -> MetricResult:\n        packed = super().after_eval(strategy)\n        self._gpu.stop_thread()\n        return packed\n\n    def __str__(self):\n        return f\"MaxGPU{self.gpu_id}Usage_Stream\"",
  "def gpu_usage_metrics(gpu_id, every=0.5, minibatch=False, epoch=False,\n                      experience=False, stream=False) -> List[PluginMetric]:\n    \"\"\"\n    Helper method that can be used to obtain the desired set of\n    plugin metrics.\n\n    :param gpu_id: GPU device ID.\n    :param every: seconds after which update the maximum GPU\n        usage\n    :param minibatch: If True, will return a metric able to log the minibatch\n        max GPU usage.\n    :param epoch: If True, will return a metric able to log the epoch\n        max GPU usage.\n    :param experience: If True, will return a metric able to log the experience\n        max GPU usage.\n    :param stream: If True, will return a metric able to log the evaluation\n        max stream GPU usage.\n\n    :return: A list of plugin metrics.\n    \"\"\"\n\n    metrics = []\n    if minibatch:\n        metrics.append(MinibatchMaxGPU(gpu_id, every))\n\n    if epoch:\n        metrics.append(EpochMaxGPU(gpu_id, every))\n\n    if experience:\n        metrics.append(ExperienceMaxGPU(gpu_id, every))\n\n    if stream:\n        metrics.append(StreamMaxGPU(gpu_id, every))\n\n    return metrics",
  "def __init__(self, gpu_id, every=0.5):\n        \"\"\"\n        Creates an instance of the GPU usage metric.\n\n        :param gpu_id: GPU device ID.\n        :param every: seconds after which update the maximum GPU\n            usage\n        \"\"\"\n\n        self.every = every\n        self.gpu_id = gpu_id\n\n        n_gpus = len(GPUtil.getGPUs())\n        if n_gpus == 0:\n            warnings.warn(\"Your system has no GPU!\")\n            self.gpu_id = None\n        elif gpu_id < 0:\n            warnings.warn(\"GPU metric called with negative GPU id.\"\n                          \"GPU logging disabled\")\n            self.gpu_id = None\n        else:\n            if gpu_id >= n_gpus:\n                warnings.warn(f\"GPU {gpu_id} not found. Using GPU 0.\")\n                self.gpu_id = 0\n\n        self.thread = None\n        \"\"\"\n        Thread executing GPU monitoring code\n        \"\"\"\n\n        self.stop_f = False\n        \"\"\"\n        Flag to stop the thread\n        \"\"\"\n\n        self.max_usage = 0\n        \"\"\"\n        Main metric result. Max GPU usage.\n        \"\"\"",
  "def _f(self):\n        \"\"\"\n        Until a stop signal is encountered,\n        this function monitors each `every` seconds\n        the maximum amount of GPU used by the process\n        \"\"\"\n        start_time = time.monotonic()\n        while not self.stop_f:\n            # GPU percentage\n            gpu_perc = GPUtil.getGPUs()[self.gpu_id].load * 100\n            if gpu_perc > self.max_usage:\n                self.max_usage = gpu_perc\n            time.sleep(self.every - ((time.monotonic() - start_time)\n                                     % self.every))",
  "def start_thread(self):\n        if self.gpu_id is not None:\n            assert not self.thread, \"Trying to start thread \" \\\n                                    \"without joining the previous.\"\n            self.thread = Thread(target=self._f, daemon=True)\n            self.thread.start()",
  "def stop_thread(self):\n        if self.thread:\n            self.stop_f = True\n            self.thread.join()\n            self.stop_f = False\n            self.thread = None",
  "def reset(self) -> None:\n        \"\"\"\n        Resets the metric.\n\n        :return: None.\n        \"\"\"\n        self.max_usage = 0",
  "def result(self) -> Optional[float]:\n        \"\"\"\n        Returns the max GPU percentage value.\n\n        :return: The percentage GPU usage as a float value in range [0, 1].\n        \"\"\"\n        return self.max_usage",
  "def update(self):\n        pass",
  "def __init__(self, gpu_id, every, reset_at, emit_at, mode):\n        self.gpu_id = gpu_id\n        self._gpu = MaxGPU(gpu_id, every)\n\n        super(GPUPluginMetric, self).__init__(\n            self._gpu, reset_at=reset_at, emit_at=emit_at,\n            mode=mode)",
  "def update(self, strategy):\n        self._gpu.update()",
  "def __init__(self, gpu_id, every=0.5):\n        \"\"\"\n        Creates an instance of the Minibatch Max GPU metric\n\n        :param gpu_id: GPU device ID.\n        :param every: seconds after which update the maximum GPU\n            usage\n        \"\"\"\n        super(MinibatchMaxGPU, self).__init__(\n            gpu_id, every,\n            reset_at='iteration', emit_at='iteration', mode='train')",
  "def before_training(self, strategy: 'BaseStrategy') \\\n            -> None:\n        super().before_training(strategy)\n        self._gpu.start_thread()",
  "def after_training(self, strategy: 'BaseStrategy') -> None:\n        super().before_training(strategy)\n        self._gpu.stop_thread()",
  "def __str__(self):\n        return f\"MaxGPU{self.gpu_id}Usage_MB\"",
  "def __init__(self, gpu_id, every=0.5):\n        \"\"\"\n        Creates an instance of the epoch Max GPU metric.\n\n        :param gpu_id: GPU device ID.\n        :param every: seconds after which update the maximum GPU\n            usage\n        \"\"\"\n        super(EpochMaxGPU, self).__init__(\n            gpu_id, every,\n            reset_at='epoch', emit_at='epoch', mode='train')",
  "def before_training(self, strategy: 'BaseStrategy'):\n        super().before_training(strategy)\n        self._gpu.start_thread()",
  "def after_training(self, strategy: 'BaseStrategy') -> None:\n        self._gpu.stop_thread()",
  "def __str__(self):\n        return f\"MaxGPU{self.gpu_id}Usage_Epoch\"",
  "def __init__(self, gpu_id, every=0.5):\n        \"\"\"\n        Creates an instance of the Experience CPU usage metric.\n\n        :param gpu_id: GPU device ID.\n        :param every: seconds after which update the maximum GPU\n            usage\n        \"\"\"\n        super(ExperienceMaxGPU, self).__init__(\n            gpu_id, every,\n            reset_at='experience', emit_at='experience', mode='eval')",
  "def before_eval(self, strategy: 'BaseStrategy'):\n        super().before_eval(strategy)\n        self._gpu.start_thread()",
  "def after_eval(self, strategy: 'BaseStrategy'):\n        super().after_eval(strategy)\n        self._gpu.stop_thread()",
  "def __str__(self):\n        return f\"MaxGPU{self.gpu_id}Usage_Experience\"",
  "def __init__(self, gpu_id, every=0.5):\n        \"\"\"\n        Creates an instance of the Experience CPU usage metric.\n\n        :param gpu_id: GPU device ID.\n        :param every: seconds after which update the maximum GPU\n            usage\n        \"\"\"\n        super(StreamMaxGPU, self).__init__(\n            gpu_id, every,\n            reset_at='stream', emit_at='stream', mode='eval')",
  "def before_eval(self, strategy):\n        super().before_eval(strategy)\n        self._gpu.start_thread()",
  "def after_eval(self, strategy: 'BaseStrategy') \\\n            -> MetricResult:\n        packed = super().after_eval(strategy)\n        self._gpu.stop_thread()\n        return packed",
  "def __str__(self):\n        return f\"MaxGPU{self.gpu_id}Usage_Stream\"",
  "class WeightCheckpoint(PluginMetric[Tensor]):\n    \"\"\"\n    The WeightCheckpoint Metric. This is a standalone metric.\n\n    Instances of this metric keeps the weight checkpoint tensor of the\n    model at each experience. \n\n    Each time `result` is called, this metric emits the latest experience's\n    weight checkpoint tensor since the last `reset`.\n\n    The reset method will bring the metric to its initial state. By default\n    this metric in its initial state will return None.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the WeightCheckpoint Metric.\n\n        By default this metric in its initial state will return None.\n        The metric can be updated by using the `update` method\n        while the current experience's weight checkpoint tensor can be \n        retrieved using the `result` method.\n        \"\"\"\n        super().__init__()\n        self.weights = None\n\n    def update(self, weights) -> Tensor:\n        \"\"\"\n        Update the weight checkpoint at the current experience.\n\n        :param weights: the weight tensor at current experience\n        :return: None.\n        \"\"\"\n        self.weights = weights\n\n    def result(self) -> Tensor:\n        \"\"\"\n        Retrieves the weight checkpoint at the current experience.\n\n        :return: The weight checkpoint as a tensor.\n        \"\"\"\n        return self.weights\n    \n    def reset(self) -> None:\n        \"\"\"\n        Resets the metric.\n\n        :return: None.\n        \"\"\"\n        self.weights = None\n\n    def _package_result(self, strategy) -> 'MetricResult':\n        weights = self.result()\n        metric_name = get_metric_name(self, strategy, \n                                      add_experience=True, add_task=False)\n        return [MetricValue(self, metric_name, weights, \n                            self.get_global_counter())]\n\n    def after_eval_exp(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        model_params = copy.deepcopy(strategy.model.parameters())\n        self.update(model_params)\n\n    def __str__(self):\n        return \"WeightCheckpoint\"",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the WeightCheckpoint Metric.\n\n        By default this metric in its initial state will return None.\n        The metric can be updated by using the `update` method\n        while the current experience's weight checkpoint tensor can be \n        retrieved using the `result` method.\n        \"\"\"\n        super().__init__()\n        self.weights = None",
  "def update(self, weights) -> Tensor:\n        \"\"\"\n        Update the weight checkpoint at the current experience.\n\n        :param weights: the weight tensor at current experience\n        :return: None.\n        \"\"\"\n        self.weights = weights",
  "def result(self) -> Tensor:\n        \"\"\"\n        Retrieves the weight checkpoint at the current experience.\n\n        :return: The weight checkpoint as a tensor.\n        \"\"\"\n        return self.weights",
  "def reset(self) -> None:\n        \"\"\"\n        Resets the metric.\n\n        :return: None.\n        \"\"\"\n        self.weights = None",
  "def _package_result(self, strategy) -> 'MetricResult':\n        weights = self.result()\n        metric_name = get_metric_name(self, strategy, \n                                      add_experience=True, add_task=False)\n        return [MetricValue(self, metric_name, weights, \n                            self.get_global_counter())]",
  "def after_eval_exp(self, strategy: 'BaseStrategy') -> 'MetricResult':\n        model_params = copy.deepcopy(strategy.model.parameters())\n        self.update(model_params)",
  "def __str__(self):\n        return \"WeightCheckpoint\"",
  "class Accuracy(Metric[float]):\n    \"\"\"\n    The Accuracy metric. This is a standalone metric.\n\n    The metric keeps a dictionary of <task_label, accuracy value> pairs.\n    and update the values through a running average over multiple\n    <prediction, target> pairs of Tensors, provided incrementally.\n    The \"prediction\" and \"target\" tensors may contain plain labels or\n    one-hot/logit vectors.\n\n    Each time `result` is called, this metric emits the average accuracy\n    across all predictions made since the last `reset`.\n\n    The reset method will bring the metric to its initial state. By default\n    this metric in its initial state will return an accuracy value of 0.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the standalone Accuracy metric.\n\n        By default this metric in its initial state will return an accuracy\n        value of 0. The metric can be updated by using the `update` method\n        while the running accuracy can be retrieved using the `result` method.\n        \"\"\"\n        super().__init__()\n        self._mean_accuracy = defaultdict(Mean)\n        \"\"\"\n        The mean utility that will be used to store the running accuracy\n        for each task label.\n        \"\"\"\n\n    @torch.no_grad()\n    def update(self, predicted_y: Tensor, true_y: Tensor,\n               task_labels: Union[float, Tensor]) -> None:\n        \"\"\"\n        Update the running accuracy given the true and predicted labels.\n        Parameter `task_labels` is used to decide how to update the inner\n        dictionary: if Float, only the dictionary value related to that task\n        is updated. If Tensor, all the dictionary elements belonging to the\n        task labels will be updated.\n\n        :param predicted_y: The model prediction. Both labels and logit vectors\n            are supported.\n        :param true_y: The ground truth. Both labels and one-hot vectors\n            are supported.\n        :param task_labels: the int task label associated to the current\n            experience or the task labels vector showing the task label\n            for each pattern.\n\n        :return: None.\n        \"\"\"\n        if len(true_y) != len(predicted_y):\n            raise ValueError('Size mismatch for true_y and predicted_y tensors')\n\n        if isinstance(task_labels, Tensor) and len(task_labels) != len(true_y):\n            raise ValueError('Size mismatch for true_y and task_labels tensors')\n\n        true_y = torch.as_tensor(true_y)\n        predicted_y = torch.as_tensor(predicted_y)\n\n        # Check if logits or labels\n        if len(predicted_y.shape) > 1:\n            # Logits -> transform to labels\n            predicted_y = torch.max(predicted_y, 1)[1]\n\n        if len(true_y.shape) > 1:\n            # Logits -> transform to labels\n            true_y = torch.max(true_y, 1)[1]\n\n        if isinstance(task_labels, int):\n            true_positives = float(torch.sum(torch.eq(predicted_y, true_y)))\n            total_patterns = len(true_y)\n            self._mean_accuracy[task_labels].update(\n                true_positives / total_patterns, total_patterns)\n        elif isinstance(task_labels, Tensor):\n            for pred, true, t in zip(predicted_y, true_y, task_labels):\n                true_positives = (pred == true).float().item()\n                self._mean_accuracy[t.item()].update(\n                    true_positives, 1)\n        else:\n            raise ValueError(f\"Task label type: {type(task_labels)}, \"\n                             f\"expected int/float or Tensor\")\n\n    def result(self, task_label=None) -> Dict[int, float]:\n        \"\"\"\n        Retrieves the running accuracy.\n\n        Calling this method will not change the internal state of the metric.\n\n        :param task_label: if None, return the entire dictionary of accuracies\n            for each task. Otherwise return the dictionary\n            `{task_label: accuracy}`.\n        :return: A dict of running accuracies for each task label,\n            where each value is a float value between 0 and 1.\n        \"\"\"\n        assert(task_label is None or isinstance(task_label, int))\n        if task_label is None:\n            return {k: v.result() for k, v in self._mean_accuracy.items()}\n        else:\n            return {task_label: self._mean_accuracy[task_label].result()}\n\n    def reset(self, task_label=None) -> None:\n        \"\"\"\n        Resets the metric.\n        :param task_label: if None, reset the entire dictionary.\n            Otherwise, reset the value associated to `task_label`.\n\n        :return: None.\n        \"\"\"\n        assert(task_label is None or isinstance(task_label, int))\n        if task_label is None:\n            self._mean_accuracy = defaultdict(Mean)\n        else:\n            self._mean_accuracy[task_label].reset()",
  "class AccuracyPluginMetric(GenericPluginMetric[float]):\n    \"\"\"\n    Base class for all accuracies plugin metrics\n    \"\"\"\n    def __init__(self, reset_at, emit_at, mode):\n        self._accuracy = Accuracy()\n        super(AccuracyPluginMetric, self).__init__(\n            self._accuracy, reset_at=reset_at, emit_at=emit_at,\n            mode=mode)\n\n    def reset(self, strategy=None) -> None:\n        if self._reset_at == 'stream' or strategy is None:\n            self._metric.reset()\n        else:\n            self._metric.reset(phase_and_task(strategy)[1])\n\n    def result(self, strategy=None) -> float:\n        if self._emit_at == 'stream' or strategy is None:\n            return self._metric.result()\n        else:\n            return self._metric.result(phase_and_task(strategy)[1])\n\n    def update(self, strategy):\n        # task labels defined for each experience\n        task_labels = strategy.experience.task_labels\n        if len(task_labels) > 1:\n            # task labels defined for each pattern\n            task_labels = strategy.mb_task_id\n        else:\n            task_labels = task_labels[0]\n        self._accuracy.update(strategy.mb_output, strategy.mb_y, task_labels)",
  "class MinibatchAccuracy(AccuracyPluginMetric):\n    \"\"\"\n    The minibatch plugin accuracy metric.\n    This metric only works at training time.\n\n    This metric computes the average accuracy over patterns\n    from a single minibatch.\n    It reports the result after each iteration.\n\n    If a more coarse-grained logging is needed, consider using\n    :class:`EpochAccuracy` instead.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the MinibatchAccuracy metric.\n        \"\"\"\n        super(MinibatchAccuracy, self).__init__(\n            reset_at='iteration', emit_at='iteration', mode='train')\n\n    def __str__(self):\n        return \"Top1_Acc_MB\"",
  "class EpochAccuracy(AccuracyPluginMetric):\n    \"\"\"\n    The average accuracy over a single training epoch.\n    This plugin metric only works at training time.\n\n    The accuracy will be logged after each training epoch by computing\n    the number of correctly predicted patterns during the epoch divided by\n    the overall number of patterns encountered in that epoch.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the EpochAccuracy metric.\n        \"\"\"\n\n        super(EpochAccuracy, self).__init__(\n            reset_at='epoch', emit_at='epoch', mode='train')\n\n    def __str__(self):\n        return \"Top1_Acc_Epoch\"",
  "class RunningEpochAccuracy(AccuracyPluginMetric):\n    \"\"\"\n    The average accuracy across all minibatches up to the current\n    epoch iteration.\n    This plugin metric only works at training time.\n\n    At each iteration, this metric logs the accuracy averaged over all patterns\n    seen so far in the current epoch.\n    The metric resets its state after each training epoch.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the RunningEpochAccuracy metric.\n        \"\"\"\n\n        super(RunningEpochAccuracy, self).__init__(\n            reset_at='epoch', emit_at='iteration', mode='train')\n\n    def __str__(self):\n        return \"Top1_RunningAcc_Epoch\"",
  "class ExperienceAccuracy(AccuracyPluginMetric):\n    \"\"\"\n    At the end of each experience, this plugin metric reports\n    the average accuracy over all patterns seen in that experience.\n    This metric only works at eval time.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of ExperienceAccuracy metric\n        \"\"\"\n        super(ExperienceAccuracy, self).__init__(\n            reset_at='experience', emit_at='experience', mode='eval')\n\n    def __str__(self):\n        return \"Top1_Acc_Exp\"",
  "class StreamAccuracy(AccuracyPluginMetric):\n    \"\"\"\n    At the end of the entire stream of experiences, this plugin metric\n    reports the average accuracy over all patterns seen in all experiences.\n    This metric only works at eval time.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of StreamAccuracy metric\n        \"\"\"\n        super(StreamAccuracy, self).__init__(\n            reset_at='stream', emit_at='stream', mode='eval')\n\n    def __str__(self):\n        return \"Top1_Acc_Stream\"",
  "class TrainedExperienceAccuracy(AccuracyPluginMetric):\n    \"\"\"\n    At the end of each experience, this plugin metric reports the average\n    accuracy for only the experiences that the model has been trained on so far.\n\n    This metric only works at eval time.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of TrainedExperienceAccuracy metric by first \n        constructing AccuracyPluginMetric\n        \"\"\"\n        super(TrainedExperienceAccuracy, self).__init__(\n            reset_at='stream', emit_at='stream', mode='eval')\n        self._current_experience = 0\n\n    def after_training_exp(self, strategy) -> None:\n        self._current_experience = strategy.experience.current_experience\n        # Reset average after learning from a new experience \n        AccuracyPluginMetric.reset(self, strategy)\n        return AccuracyPluginMetric.after_training_exp(self, strategy)\n        \n    def update(self, strategy):\n        \"\"\"\n        Only update the accuracy with results from experiences that have been \n        trained on\n        \"\"\"\n        if strategy.experience.current_experience <= self._current_experience:\n            AccuracyPluginMetric.update(self, strategy)\n\n    def __str__(self):\n        return \"Accuracy_On_Trained_Experiences\"",
  "def accuracy_metrics(*, \n                     minibatch=False,\n                     epoch=False,\n                     epoch_running=False,\n                     experience=False,\n                     stream=False,\n                     trained_experience=False) -> List[PluginMetric]:\n    \"\"\"\n    Helper method that can be used to obtain the desired set of\n    plugin metrics.\n\n    :param minibatch: If True, will return a metric able to log\n        the minibatch accuracy at training time.\n    :param epoch: If True, will return a metric able to log\n        the epoch accuracy at training time.\n    :param epoch_running: If True, will return a metric able to log\n        the running epoch accuracy at training time.\n    :param experience: If True, will return a metric able to log\n        the accuracy on each evaluation experience.\n    :param stream: If True, will return a metric able to log\n        the accuracy averaged over the entire evaluation stream of experiences.\n    :param trained_experience: If True, will return a metric able to log\n        the average evaluation accuracy only for experiences that the\n        model has been trained on         \n\n    :return: A list of plugin metrics.\n    \"\"\"\n\n    metrics = []\n    if minibatch:\n        metrics.append(MinibatchAccuracy())\n\n    if epoch:\n        metrics.append(EpochAccuracy())\n\n    if epoch_running:\n        metrics.append(RunningEpochAccuracy())\n\n    if experience:\n        metrics.append(ExperienceAccuracy())\n\n    if stream:\n        metrics.append(StreamAccuracy())\n\n    if trained_experience:\n        metrics.append(TrainedExperienceAccuracy())\n\n    return metrics",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the standalone Accuracy metric.\n\n        By default this metric in its initial state will return an accuracy\n        value of 0. The metric can be updated by using the `update` method\n        while the running accuracy can be retrieved using the `result` method.\n        \"\"\"\n        super().__init__()\n        self._mean_accuracy = defaultdict(Mean)\n        \"\"\"\n        The mean utility that will be used to store the running accuracy\n        for each task label.\n        \"\"\"",
  "def update(self, predicted_y: Tensor, true_y: Tensor,\n               task_labels: Union[float, Tensor]) -> None:\n        \"\"\"\n        Update the running accuracy given the true and predicted labels.\n        Parameter `task_labels` is used to decide how to update the inner\n        dictionary: if Float, only the dictionary value related to that task\n        is updated. If Tensor, all the dictionary elements belonging to the\n        task labels will be updated.\n\n        :param predicted_y: The model prediction. Both labels and logit vectors\n            are supported.\n        :param true_y: The ground truth. Both labels and one-hot vectors\n            are supported.\n        :param task_labels: the int task label associated to the current\n            experience or the task labels vector showing the task label\n            for each pattern.\n\n        :return: None.\n        \"\"\"\n        if len(true_y) != len(predicted_y):\n            raise ValueError('Size mismatch for true_y and predicted_y tensors')\n\n        if isinstance(task_labels, Tensor) and len(task_labels) != len(true_y):\n            raise ValueError('Size mismatch for true_y and task_labels tensors')\n\n        true_y = torch.as_tensor(true_y)\n        predicted_y = torch.as_tensor(predicted_y)\n\n        # Check if logits or labels\n        if len(predicted_y.shape) > 1:\n            # Logits -> transform to labels\n            predicted_y = torch.max(predicted_y, 1)[1]\n\n        if len(true_y.shape) > 1:\n            # Logits -> transform to labels\n            true_y = torch.max(true_y, 1)[1]\n\n        if isinstance(task_labels, int):\n            true_positives = float(torch.sum(torch.eq(predicted_y, true_y)))\n            total_patterns = len(true_y)\n            self._mean_accuracy[task_labels].update(\n                true_positives / total_patterns, total_patterns)\n        elif isinstance(task_labels, Tensor):\n            for pred, true, t in zip(predicted_y, true_y, task_labels):\n                true_positives = (pred == true).float().item()\n                self._mean_accuracy[t.item()].update(\n                    true_positives, 1)\n        else:\n            raise ValueError(f\"Task label type: {type(task_labels)}, \"\n                             f\"expected int/float or Tensor\")",
  "def result(self, task_label=None) -> Dict[int, float]:\n        \"\"\"\n        Retrieves the running accuracy.\n\n        Calling this method will not change the internal state of the metric.\n\n        :param task_label: if None, return the entire dictionary of accuracies\n            for each task. Otherwise return the dictionary\n            `{task_label: accuracy}`.\n        :return: A dict of running accuracies for each task label,\n            where each value is a float value between 0 and 1.\n        \"\"\"\n        assert(task_label is None or isinstance(task_label, int))\n        if task_label is None:\n            return {k: v.result() for k, v in self._mean_accuracy.items()}\n        else:\n            return {task_label: self._mean_accuracy[task_label].result()}",
  "def reset(self, task_label=None) -> None:\n        \"\"\"\n        Resets the metric.\n        :param task_label: if None, reset the entire dictionary.\n            Otherwise, reset the value associated to `task_label`.\n\n        :return: None.\n        \"\"\"\n        assert(task_label is None or isinstance(task_label, int))\n        if task_label is None:\n            self._mean_accuracy = defaultdict(Mean)\n        else:\n            self._mean_accuracy[task_label].reset()",
  "def __init__(self, reset_at, emit_at, mode):\n        self._accuracy = Accuracy()\n        super(AccuracyPluginMetric, self).__init__(\n            self._accuracy, reset_at=reset_at, emit_at=emit_at,\n            mode=mode)",
  "def reset(self, strategy=None) -> None:\n        if self._reset_at == 'stream' or strategy is None:\n            self._metric.reset()\n        else:\n            self._metric.reset(phase_and_task(strategy)[1])",
  "def result(self, strategy=None) -> float:\n        if self._emit_at == 'stream' or strategy is None:\n            return self._metric.result()\n        else:\n            return self._metric.result(phase_and_task(strategy)[1])",
  "def update(self, strategy):\n        # task labels defined for each experience\n        task_labels = strategy.experience.task_labels\n        if len(task_labels) > 1:\n            # task labels defined for each pattern\n            task_labels = strategy.mb_task_id\n        else:\n            task_labels = task_labels[0]\n        self._accuracy.update(strategy.mb_output, strategy.mb_y, task_labels)",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the MinibatchAccuracy metric.\n        \"\"\"\n        super(MinibatchAccuracy, self).__init__(\n            reset_at='iteration', emit_at='iteration', mode='train')",
  "def __str__(self):\n        return \"Top1_Acc_MB\"",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the EpochAccuracy metric.\n        \"\"\"\n\n        super(EpochAccuracy, self).__init__(\n            reset_at='epoch', emit_at='epoch', mode='train')",
  "def __str__(self):\n        return \"Top1_Acc_Epoch\"",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the RunningEpochAccuracy metric.\n        \"\"\"\n\n        super(RunningEpochAccuracy, self).__init__(\n            reset_at='epoch', emit_at='iteration', mode='train')",
  "def __str__(self):\n        return \"Top1_RunningAcc_Epoch\"",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of ExperienceAccuracy metric\n        \"\"\"\n        super(ExperienceAccuracy, self).__init__(\n            reset_at='experience', emit_at='experience', mode='eval')",
  "def __str__(self):\n        return \"Top1_Acc_Exp\"",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of StreamAccuracy metric\n        \"\"\"\n        super(StreamAccuracy, self).__init__(\n            reset_at='stream', emit_at='stream', mode='eval')",
  "def __str__(self):\n        return \"Top1_Acc_Stream\"",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of TrainedExperienceAccuracy metric by first \n        constructing AccuracyPluginMetric\n        \"\"\"\n        super(TrainedExperienceAccuracy, self).__init__(\n            reset_at='stream', emit_at='stream', mode='eval')\n        self._current_experience = 0",
  "def after_training_exp(self, strategy) -> None:\n        self._current_experience = strategy.experience.current_experience\n        # Reset average after learning from a new experience \n        AccuracyPluginMetric.reset(self, strategy)\n        return AccuracyPluginMetric.after_training_exp(self, strategy)",
  "def update(self, strategy):\n        \"\"\"\n        Only update the accuracy with results from experiences that have been \n        trained on\n        \"\"\"\n        if strategy.experience.current_experience <= self._current_experience:\n            AccuracyPluginMetric.update(self, strategy)",
  "def __str__(self):\n        return \"Accuracy_On_Trained_Experiences\"",
  "class Mean(Metric[float]):\n    \"\"\"\n    The standalone mean metric.\n\n    This utility metric is a general purpose metric that can be used to keep\n    track of the mean of a sequence of values.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the mean metric.\n\n        This metric in its initial state will return a mean value of 0.\n        The metric can be updated by using the `update` method while the mean\n        can be retrieved using the `result` method.\n        \"\"\"\n        super().__init__()\n        self.summed: float = 0.0\n        self.weight: float = 0.0\n\n    def update(self, value: SupportsFloat, weight: SupportsFloat = 1.0) -> None:\n        \"\"\"\n        Update the running mean given the value.\n\n        The value can be weighted with a custom value, defined by the `weight`\n        parameter.\n\n        :param value: The value to be used to update the mean.\n        :param weight: The weight of the value. Defaults to 1.\n        :return: None.\n        \"\"\"\n        value = float(value)\n        weight = float(weight)\n        self.summed += value * weight\n        self.weight += weight\n\n    def result(self) -> float:\n        \"\"\"\n        Retrieves the mean.\n\n        Calling this method will not change the internal state of the metric.\n\n        :return: The mean, as a float.\n        \"\"\"\n        if self.weight == 0.0:\n            return 0.0\n        return self.summed / self.weight\n\n    def reset(self) -> None:\n        \"\"\"\n        Resets the metric.\n\n        :return: None.\n        \"\"\"\n        self.summed = 0.0\n        self.weight = 0.0\n\n    def __add__(self, other: 'Mean') -> \"Mean\":\n        \"\"\"\n        Return a metric representing the weighted mean of the 2 means.\n\n        :param other: the other mean\n        :return: The weighted mean\"\"\"\n        res = Mean()\n        res.summed = self.summed + other.summed\n        res.weight = self.weight + other.weight\n        return res",
  "class Sum(Metric[float]):\n    \"\"\"\n    The standalone sum metric.\n\n    This utility metric is a general purpose metric that can be used to keep\n    track of the sum of a sequence of values.\n\n    Beware that this metric only supports summing numbers and the result is\n    always a float value, even when `update` is called by passing `int`s only.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the sum metric.\n\n        This metric in its initial state will return a sum value of 0.\n        The metric can be updated by using the `update` method while the sum\n        can be retrieved using the `result` method.\n        \"\"\"\n        super().__init__()\n        self.summed: float = 0.0\n\n    def update(self, value: SupportsFloat) -> None:\n        \"\"\"\n        Update the running sum given the value.\n\n        :param value: The value to be used to update the sum.\n        :return: None.\n        \"\"\"\n        self.summed += float(value)\n\n    def result(self) -> float:\n        \"\"\"\n        Retrieves the sum.\n\n        Calling this method will not change the internal state of the metric.\n\n        :return: The sum, as a float.\n        \"\"\"\n        return self.summed\n\n    def reset(self) -> None:\n        \"\"\"\n        Resets the metric.\n\n        :return: None.\n        \"\"\"\n        self.summed = 0.0",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the mean metric.\n\n        This metric in its initial state will return a mean value of 0.\n        The metric can be updated by using the `update` method while the mean\n        can be retrieved using the `result` method.\n        \"\"\"\n        super().__init__()\n        self.summed: float = 0.0\n        self.weight: float = 0.0",
  "def update(self, value: SupportsFloat, weight: SupportsFloat = 1.0) -> None:\n        \"\"\"\n        Update the running mean given the value.\n\n        The value can be weighted with a custom value, defined by the `weight`\n        parameter.\n\n        :param value: The value to be used to update the mean.\n        :param weight: The weight of the value. Defaults to 1.\n        :return: None.\n        \"\"\"\n        value = float(value)\n        weight = float(weight)\n        self.summed += value * weight\n        self.weight += weight",
  "def result(self) -> float:\n        \"\"\"\n        Retrieves the mean.\n\n        Calling this method will not change the internal state of the metric.\n\n        :return: The mean, as a float.\n        \"\"\"\n        if self.weight == 0.0:\n            return 0.0\n        return self.summed / self.weight",
  "def reset(self) -> None:\n        \"\"\"\n        Resets the metric.\n\n        :return: None.\n        \"\"\"\n        self.summed = 0.0\n        self.weight = 0.0",
  "def __add__(self, other: 'Mean') -> \"Mean\":\n        \"\"\"\n        Return a metric representing the weighted mean of the 2 means.\n\n        :param other: the other mean\n        :return: The weighted mean\"\"\"\n        res = Mean()\n        res.summed = self.summed + other.summed\n        res.weight = self.weight + other.weight\n        return res",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the sum metric.\n\n        This metric in its initial state will return a sum value of 0.\n        The metric can be updated by using the `update` method while the sum\n        can be retrieved using the `result` method.\n        \"\"\"\n        super().__init__()\n        self.summed: float = 0.0",
  "def update(self, value: SupportsFloat) -> None:\n        \"\"\"\n        Update the running sum given the value.\n\n        :param value: The value to be used to update the sum.\n        :return: None.\n        \"\"\"\n        self.summed += float(value)",
  "def result(self) -> float:\n        \"\"\"\n        Retrieves the sum.\n\n        Calling this method will not change the internal state of the metric.\n\n        :return: The sum, as a float.\n        \"\"\"\n        return self.summed",
  "def reset(self) -> None:\n        \"\"\"\n        Resets the metric.\n\n        :return: None.\n        \"\"\"\n        self.summed = 0.0",
  "class Forgetting(Metric[Union[float, None, Dict[int, float]]]):\n    \"\"\"\n    The standalone Forgetting metric.\n    This metric returns the forgetting relative to a specific key.\n    Alternatively, this metric returns a dict in which each key is associated\n    to the forgetting.\n    Forgetting is computed as the difference between the first value recorded\n    for a specific key and the last value recorded for that key.\n    The value associated to a key can be update with the `update` method.\n\n    At initialization, this metric returns an empty dictionary.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the standalone Forgetting metric\n        \"\"\"\n\n        super().__init__()\n\n        self.initial: Dict[int, float] = dict()\n        \"\"\"\n        The initial value for each key.\n        \"\"\"\n\n        self.last: Dict[int, float] = dict()\n        \"\"\"\n        The last value detected for each key\n        \"\"\"\n\n    def update_initial(self, k, v):\n        self.initial[k] = v\n\n    def update_last(self, k, v):\n        self.last[k] = v\n\n    def update(self, k, v, initial=False):\n        if initial:\n            self.update_initial(k, v)\n        else:\n            self.update_last(k, v)\n\n    def result(self, k=None) -> Union[float, None, Dict[int, float]]:\n        \"\"\"\n        Forgetting is returned only for keys encountered twice.\n\n        :param k: the key for which returning forgetting. If k has not\n            updated at least twice it returns None. If k is None,\n            forgetting will be returned for all keys encountered at least\n            twice.\n\n        :return: the difference between the first and last value encountered\n            for k, if k is not None. It returns None if k has not been updated\n            at least twice. If k is None, returns a dictionary\n            containing keys whose value has been updated at least twice. The\n            associated value is the difference between the first and last\n            value recorded for that key.\n        \"\"\"\n\n        forgetting = {}\n        if k is not None:\n            if k in self.initial and k in self.last:\n                return self.initial[k] - self.last[k]\n            else:\n                return None\n\n        ik = set(self.initial.keys())\n        both_keys = list(ik.intersection(set(self.last.keys())))\n\n        for k in both_keys:\n            forgetting[k] = self.initial[k] - self.last[k]\n\n        return forgetting\n\n    def reset_last(self) -> None:\n        self.last: Dict[int, float] = dict()\n\n    def reset(self) -> None:\n        self.initial: Dict[int, float] = dict()\n        self.last: Dict[int, float] = dict()",
  "class GenericExperienceForgetting(PluginMetric[Dict[int, float]]):\n    \"\"\"\n    The GenericExperienceForgetting metric, describing the change in\n    a metric detected for a certain experience. The user should\n    subclass this and provide the desired metric.\n\n    In particular, the user should override:\n    * __init__ by calling `super` and instantiating the `self.current_metric`\n    property as a valid avalanche metric\n    * `metric_update`, to update `current_metric`\n    * `metric_result` to get the result from `current_metric`.\n    * `__str__` to define the experience forgetting  name.\n\n    This plugin metric, computed separately for each experience,\n    is the difference between the metric result obtained after\n    first training on a experience and the metric result obtained\n    on the same experience at the end of successive experiences.\n\n    This metric is computed during the eval phase only.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the GenericExperienceForgetting metric.\n        \"\"\"\n\n        super().__init__()\n\n        self.forgetting = Forgetting()\n        \"\"\"\n        The general metric to compute forgetting\n        \"\"\"\n\n        self._current_metric = None\n        \"\"\"\n        The metric the user should override\n        \"\"\"\n\n        self.eval_exp_id = None\n        \"\"\"\n        The current evaluation experience id\n        \"\"\"\n\n        self.train_exp_id = None\n        \"\"\"\n        The last encountered training experience id\n        \"\"\"\n\n    def reset(self) -> None:\n        \"\"\"\n        Resets the metric.\n\n        Beware that this will also reset the initial metric of each\n        experience!\n\n        :return: None.\n        \"\"\"\n        self.forgetting.reset()\n\n    def reset_last(self) -> None:\n        \"\"\"\n        Resets the last metric value.\n\n        This will preserve the initial metric value of each experience.\n        To be used at the beginning of each eval experience.\n\n        :return: None.\n        \"\"\"\n        self.forgetting.reset_last()\n\n    def update(self, k, v, initial=False):\n        \"\"\"\n        Update forgetting metric.\n        See `Forgetting` for more detailed information.\n\n        :param k: key to update\n        :param v: value associated to k\n        :param initial: update initial value. If False, update\n            last value.\n        \"\"\"\n        self.forgetting.update(k, v, initial=initial)\n\n    def result(self, k=None) -> Union[float, None, Dict[int, float]]:\n        \"\"\"\n        See `Forgetting` documentation for more detailed information.\n\n        k: optional key from which compute forgetting.\n        \"\"\"\n        return self.forgetting.result(k=k)\n\n    def before_training_exp(self, strategy: 'BaseStrategy') -> None:\n        self.train_exp_id = strategy.experience.current_experience\n\n    def before_eval(self, strategy) -> None:\n        self.reset_last()\n\n    def before_eval_exp(self, strategy: 'BaseStrategy') -> None:\n        self._current_metric.reset()\n\n    def after_eval_iteration(self, strategy: 'BaseStrategy') -> None:\n        super().after_eval_iteration(strategy)\n        self.eval_exp_id = strategy.experience.current_experience\n        self.metric_update(strategy)\n\n    def after_eval_exp(self, strategy: 'BaseStrategy') \\\n            -> MetricResult:\n        # update experience on which training just ended\n        if self.train_exp_id == self.eval_exp_id:\n            self.update(self.eval_exp_id,\n                        self.metric_result(strategy),\n                        initial=True)\n        else:\n            # update other experiences\n            # if experience has not been encountered in training\n            # its value will not be considered in forgetting\n            self.update(self.eval_exp_id,\n                        self.metric_result(strategy))\n\n        return self._package_result(strategy)\n\n    def _package_result(self, strategy: 'BaseStrategy') \\\n            -> MetricResult:\n        # this checks if the evaluation experience has been\n        # already encountered at training time\n        # before the last training.\n        # If not, forgetting should not be returned.\n        forgetting = self.result(k=self.eval_exp_id)\n        if forgetting is not None:\n            metric_name = get_metric_name(self, strategy, add_experience=True)\n            plot_x_position = self.get_global_counter()\n\n            metric_values = [MetricValue(\n                self, metric_name, forgetting, plot_x_position)]\n            return metric_values\n\n    def metric_update(self, strategy):\n        raise NotImplementedError\n\n    def metric_result(self, strategy):\n        raise NotImplementedError\n\n    def __str__(self):\n        raise NotImplementedError",
  "class ExperienceForgetting(GenericExperienceForgetting):\n    \"\"\"\n    The ExperienceForgetting metric, describing the accuracy loss\n    detected for a certain experience.\n\n    This plugin metric, computed separately for each experience,\n    is the difference between the accuracy result obtained after\n    first training on a experience and the accuracy result obtained\n    on the same experience at the end of successive experiences.\n\n    This metric is computed during the eval phase only.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the ExperienceForgetting metric.\n        \"\"\"\n\n        super().__init__()\n\n        self._current_metric = Accuracy()\n        \"\"\"\n        The average accuracy over the current evaluation experience\n        \"\"\"\n\n    def metric_update(self, strategy):\n        self._current_metric.update(strategy.mb_y,\n                                    strategy.mb_output, 0)\n\n    def metric_result(self, strategy):\n        return self._current_metric.result(0)[0]\n\n    def __str__(self):\n        return \"ExperienceForgetting\"",
  "class GenericStreamForgetting(GenericExperienceForgetting):\n    \"\"\"\n    The GenericStreamForgetting metric, describing the average evaluation\n    change in the desired metric detected over all experiences observed\n    during training.\n\n    In particular, the user should override:\n    * __init__ by calling `super` and instantiating the `self.current_metric`\n    property as a valid avalanche metric\n    * `metric_update`, to update `current_metric`\n    * `metric_result` to get the result from `current_metric`.\n    * `__str__` to define the experience forgetting  name.\n\n    This plugin metric, computed over all observed experiences during training,\n    is the average over the difference between the metric result obtained\n    after first training on a experience and the metric result obtained\n    on the same experience at the end of successive experiences.\n\n    This metric is computed during the eval phase only.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the GenericStreamForgetting metric.\n        \"\"\"\n\n        super().__init__()\n\n        self.stream_forgetting = Mean()\n        \"\"\"\n        The average forgetting over all experiences\n        \"\"\"\n\n    def reset(self) -> None:\n        \"\"\"\n        Resets the forgetting metrics.\n\n        Beware that this will also reset the initial metric value of each\n        experience!\n\n        :return: None.\n        \"\"\"\n        super().reset()\n        self.stream_forgetting.reset()\n\n    def exp_update(self, k, v, initial=False):\n        \"\"\"\n        Update forgetting metric.\n        See `Forgetting` for more detailed information.\n\n        :param k: key to update\n        :param v: value associated to k\n        :param initial: update initial value. If False, update\n            last value.\n        \"\"\"\n        super().update(k, v, initial=initial)\n\n    def exp_result(self, k=None) -> Union[float, None, Dict[int, float]]:\n        \"\"\"\n        Result for experience defined by a key.\n        See `Forgetting` documentation for more detailed information.\n\n        k: optional key from which compute forgetting.\n        \"\"\"\n        return super().result(k)\n\n    def result(self, k=None) -> Union[float, None, Dict[int, float]]:\n        \"\"\"\n        The average forgetting over all experience.\n\n        k: optional key from which compute forgetting.\n        \"\"\"\n        return self.stream_forgetting.result()\n\n    def before_eval(self, strategy) -> None:\n        super().before_eval(strategy)\n        self.stream_forgetting.reset()\n\n    def after_eval_exp(self, strategy: 'BaseStrategy') -> None:\n        # update experience on which training just ended\n        if self.train_exp_id == self.eval_exp_id:\n            self.exp_update(self.eval_exp_id,\n                            self.metric_result(strategy),\n                            initial=True)\n        else:\n            # update other experiences\n            # if experience has not been encountered in training\n            # its value will not be considered in forgetting\n            self.exp_update(self.eval_exp_id,\n                            self.metric_result(strategy))\n\n        # this checks if the evaluation experience has been\n        # already encountered at training time\n        # before the last training.\n        # If not, forgetting should not be returned.\n        exp_forgetting = self.exp_result(k=self.eval_exp_id)\n        if exp_forgetting is not None:\n            self.stream_forgetting.update(exp_forgetting, weight=1)\n\n    def after_eval(self, strategy: 'BaseStrategy') -> \\\n            'MetricResult':\n        return self._package_result(strategy)\n\n    def _package_result(self, strategy: 'BaseStrategy') -> \\\n            MetricResult:\n        metric_value = self.result()\n\n        phase_name, _ = phase_and_task(strategy)\n        stream = stream_type(strategy.experience)\n        metric_name = '{}/{}_phase/{}_stream' \\\n            .format(str(self),\n                    phase_name,\n                    stream)\n        plot_x_position = self.get_global_counter()\n\n        return [MetricValue(self, metric_name, metric_value, plot_x_position)]\n\n    def metric_update(self, strategy):\n        raise NotImplementedError\n\n    def metric_result(self, strategy):\n        raise NotImplementedError\n\n    def __str__(self):\n        raise NotImplementedError",
  "class StreamForgetting(GenericStreamForgetting):\n    \"\"\"\n    The StreamForgetting metric, describing the average evaluation accuracy loss\n    detected over all experiences observed during training.\n\n    This plugin metric, computed over all observed experiences during training,\n    is the average over the difference between the accuracy result obtained\n    after first training on a experience and the accuracy result obtained\n    on the same experience at the end of successive experiences.\n\n    This metric is computed during the eval phase only.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the StreamForgetting metric.\n        \"\"\"\n\n        super().__init__()\n\n        self._current_metric = Accuracy()\n        \"\"\"\n        The average accuracy over the current evaluation experience\n        \"\"\"\n\n    def metric_update(self, strategy):\n        self._current_metric.update(strategy.mb_y,\n                                    strategy.mb_output, 0)\n\n    def metric_result(self, strategy):\n        return self._current_metric.result(0)[0]\n\n    def __str__(self):\n        return \"StreamForgetting\"",
  "def forgetting_metrics(*, experience=False, stream=False) \\\n        -> List[PluginMetric]:\n    \"\"\"\n    Helper method that can be used to obtain the desired set of\n    plugin metrics.\n\n    :param experience: If True, will return a metric able to log\n        the forgetting on each evaluation experience.\n    :param stream: If True, will return a metric able to log\n        the forgetting averaged over the evaluation stream experiences,\n        which have been observed during training.\n\n    :return: A list of plugin metrics.\n    \"\"\"\n\n    metrics = []\n\n    if experience:\n        metrics.append(ExperienceForgetting())\n\n    if stream:\n        metrics.append(StreamForgetting())\n\n    return metrics",
  "def forgetting_to_bwt(f):\n    \"\"\"\n    Convert forgetting to backward transfer.\n    BWT = -1 * forgetting\n    \"\"\"\n    if f is None:\n        return f\n    if isinstance(f, dict):\n        bwt = {k: -1 * v for k, v in f.items()}\n    elif isinstance(f, float):\n        bwt = -1 * f\n    else:\n        raise ValueError(\"Forgetting data type not recognized when converting\"\n                         \"to backward transfer.\")\n    return bwt",
  "class BWT(Forgetting):\n    \"\"\"\n    The standalone Backward Transfer metric.\n    This metric returns the backward transfer relative to a specific key.\n    Alternatively, this metric returns a dict in which each key is associated\n    to the backward transfer.\n    Backward transfer is computed as the difference between the last value\n    recorded for a specific key and the first value recorded for that key.\n    The value associated to a key can be update with the `update` method.\n\n    At initialization, this metric returns an empty dictionary.\n    \"\"\"\n\n    def result(self, k=None) -> Union[float, None, Dict[int, float]]:\n        \"\"\"\n        Backward Transfer is returned only for keys encountered twice.\n        Backward Transfer is the negative forgetting.\n\n        :param k: the key for which returning backward transfer. If k has not\n            updated at least twice it returns None. If k is None,\n            backward transfer will be returned for all keys encountered at\n            least twice.\n\n        :return: the difference between the last value encountered for k\n            and its first value, if k is not None.\n            It returns None if k has not been updated\n            at least twice. If k is None, returns a dictionary\n            containing keys whose value has been updated at least twice. The\n            associated value is the difference between the last and first\n            value recorded for that key.\n        \"\"\"\n\n        forgetting = super().result(k)\n        bwt = forgetting_to_bwt(forgetting)\n        return bwt",
  "class ExperienceBWT(ExperienceForgetting):\n    \"\"\"\n    The Experience Backward Transfer metric.\n\n    This plugin metric, computed separately for each experience,\n    is the difference between the last accuracy result obtained on a certain\n    experience and the accuracy result obtained when first training on that\n    experience.\n\n    This metric is computed during the eval phase only.\n    \"\"\"\n\n    def result(self, k=None) -> Union[float, None, Dict[int, float]]:\n        \"\"\"\n        See `Forgetting` documentation for more detailed information.\n\n        k: optional key from which compute forgetting.\n        \"\"\"\n        forgetting = super().result(k)\n        return forgetting_to_bwt(forgetting)\n\n    def __str__(self):\n        return \"ExperienceBWT\"",
  "class StreamBWT(StreamForgetting):\n    \"\"\"\n    The StreamBWT metric, emitting the average BWT across all experiences\n    encountered during training.\n\n    This plugin metric, computed over all observed experiences during training,\n    is the average over the difference between the last accuracy result\n    obtained on an experience and the accuracy result obtained when first\n    training on that experience.\n\n    This metric is computed during the eval phase only.\n    \"\"\"\n\n    def exp_result(self, k=None) -> Union[float, None, Dict[int, float]]:\n        \"\"\"\n        Result for experience defined by a key.\n        See `BWT` documentation for more detailed information.\n\n        k: optional key from which compute backward transfer.\n        \"\"\"\n        forgetting = super().exp_result(k)\n        return forgetting_to_bwt(forgetting)\n\n    def __str__(self):\n        return \"StreamBWT\"",
  "def bwt_metrics(*, experience=False, stream=False) \\\n        -> List[PluginMetric]:\n    \"\"\"\n    Helper method that can be used to obtain the desired set of\n    plugin metrics.\n\n    :param experience: If True, will return a metric able to log\n        the backward transfer on each evaluation experience.\n    :param stream: If True, will return a metric able to log\n        the backward transfer averaged over the evaluation stream experiences\n        which have been observed during training.\n    :return: A list of plugin metrics.\n    \"\"\"\n\n    metrics = []\n\n    if experience:\n        metrics.append(ExperienceBWT())\n\n    if stream:\n        metrics.append(StreamBWT())\n\n    return metrics",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the standalone Forgetting metric\n        \"\"\"\n\n        super().__init__()\n\n        self.initial: Dict[int, float] = dict()\n        \"\"\"\n        The initial value for each key.\n        \"\"\"\n\n        self.last: Dict[int, float] = dict()\n        \"\"\"\n        The last value detected for each key\n        \"\"\"",
  "def update_initial(self, k, v):\n        self.initial[k] = v",
  "def update_last(self, k, v):\n        self.last[k] = v",
  "def update(self, k, v, initial=False):\n        if initial:\n            self.update_initial(k, v)\n        else:\n            self.update_last(k, v)",
  "def result(self, k=None) -> Union[float, None, Dict[int, float]]:\n        \"\"\"\n        Forgetting is returned only for keys encountered twice.\n\n        :param k: the key for which returning forgetting. If k has not\n            updated at least twice it returns None. If k is None,\n            forgetting will be returned for all keys encountered at least\n            twice.\n\n        :return: the difference between the first and last value encountered\n            for k, if k is not None. It returns None if k has not been updated\n            at least twice. If k is None, returns a dictionary\n            containing keys whose value has been updated at least twice. The\n            associated value is the difference between the first and last\n            value recorded for that key.\n        \"\"\"\n\n        forgetting = {}\n        if k is not None:\n            if k in self.initial and k in self.last:\n                return self.initial[k] - self.last[k]\n            else:\n                return None\n\n        ik = set(self.initial.keys())\n        both_keys = list(ik.intersection(set(self.last.keys())))\n\n        for k in both_keys:\n            forgetting[k] = self.initial[k] - self.last[k]\n\n        return forgetting",
  "def reset_last(self) -> None:\n        self.last: Dict[int, float] = dict()",
  "def reset(self) -> None:\n        self.initial: Dict[int, float] = dict()\n        self.last: Dict[int, float] = dict()",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the GenericExperienceForgetting metric.\n        \"\"\"\n\n        super().__init__()\n\n        self.forgetting = Forgetting()\n        \"\"\"\n        The general metric to compute forgetting\n        \"\"\"\n\n        self._current_metric = None\n        \"\"\"\n        The metric the user should override\n        \"\"\"\n\n        self.eval_exp_id = None\n        \"\"\"\n        The current evaluation experience id\n        \"\"\"\n\n        self.train_exp_id = None\n        \"\"\"\n        The last encountered training experience id\n        \"\"\"",
  "def reset(self) -> None:\n        \"\"\"\n        Resets the metric.\n\n        Beware that this will also reset the initial metric of each\n        experience!\n\n        :return: None.\n        \"\"\"\n        self.forgetting.reset()",
  "def reset_last(self) -> None:\n        \"\"\"\n        Resets the last metric value.\n\n        This will preserve the initial metric value of each experience.\n        To be used at the beginning of each eval experience.\n\n        :return: None.\n        \"\"\"\n        self.forgetting.reset_last()",
  "def update(self, k, v, initial=False):\n        \"\"\"\n        Update forgetting metric.\n        See `Forgetting` for more detailed information.\n\n        :param k: key to update\n        :param v: value associated to k\n        :param initial: update initial value. If False, update\n            last value.\n        \"\"\"\n        self.forgetting.update(k, v, initial=initial)",
  "def result(self, k=None) -> Union[float, None, Dict[int, float]]:\n        \"\"\"\n        See `Forgetting` documentation for more detailed information.\n\n        k: optional key from which compute forgetting.\n        \"\"\"\n        return self.forgetting.result(k=k)",
  "def before_training_exp(self, strategy: 'BaseStrategy') -> None:\n        self.train_exp_id = strategy.experience.current_experience",
  "def before_eval(self, strategy) -> None:\n        self.reset_last()",
  "def before_eval_exp(self, strategy: 'BaseStrategy') -> None:\n        self._current_metric.reset()",
  "def after_eval_iteration(self, strategy: 'BaseStrategy') -> None:\n        super().after_eval_iteration(strategy)\n        self.eval_exp_id = strategy.experience.current_experience\n        self.metric_update(strategy)",
  "def after_eval_exp(self, strategy: 'BaseStrategy') \\\n            -> MetricResult:\n        # update experience on which training just ended\n        if self.train_exp_id == self.eval_exp_id:\n            self.update(self.eval_exp_id,\n                        self.metric_result(strategy),\n                        initial=True)\n        else:\n            # update other experiences\n            # if experience has not been encountered in training\n            # its value will not be considered in forgetting\n            self.update(self.eval_exp_id,\n                        self.metric_result(strategy))\n\n        return self._package_result(strategy)",
  "def _package_result(self, strategy: 'BaseStrategy') \\\n            -> MetricResult:\n        # this checks if the evaluation experience has been\n        # already encountered at training time\n        # before the last training.\n        # If not, forgetting should not be returned.\n        forgetting = self.result(k=self.eval_exp_id)\n        if forgetting is not None:\n            metric_name = get_metric_name(self, strategy, add_experience=True)\n            plot_x_position = self.get_global_counter()\n\n            metric_values = [MetricValue(\n                self, metric_name, forgetting, plot_x_position)]\n            return metric_values",
  "def metric_update(self, strategy):\n        raise NotImplementedError",
  "def metric_result(self, strategy):\n        raise NotImplementedError",
  "def __str__(self):\n        raise NotImplementedError",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the ExperienceForgetting metric.\n        \"\"\"\n\n        super().__init__()\n\n        self._current_metric = Accuracy()\n        \"\"\"\n        The average accuracy over the current evaluation experience\n        \"\"\"",
  "def metric_update(self, strategy):\n        self._current_metric.update(strategy.mb_y,\n                                    strategy.mb_output, 0)",
  "def metric_result(self, strategy):\n        return self._current_metric.result(0)[0]",
  "def __str__(self):\n        return \"ExperienceForgetting\"",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the GenericStreamForgetting metric.\n        \"\"\"\n\n        super().__init__()\n\n        self.stream_forgetting = Mean()\n        \"\"\"\n        The average forgetting over all experiences\n        \"\"\"",
  "def reset(self) -> None:\n        \"\"\"\n        Resets the forgetting metrics.\n\n        Beware that this will also reset the initial metric value of each\n        experience!\n\n        :return: None.\n        \"\"\"\n        super().reset()\n        self.stream_forgetting.reset()",
  "def exp_update(self, k, v, initial=False):\n        \"\"\"\n        Update forgetting metric.\n        See `Forgetting` for more detailed information.\n\n        :param k: key to update\n        :param v: value associated to k\n        :param initial: update initial value. If False, update\n            last value.\n        \"\"\"\n        super().update(k, v, initial=initial)",
  "def exp_result(self, k=None) -> Union[float, None, Dict[int, float]]:\n        \"\"\"\n        Result for experience defined by a key.\n        See `Forgetting` documentation for more detailed information.\n\n        k: optional key from which compute forgetting.\n        \"\"\"\n        return super().result(k)",
  "def result(self, k=None) -> Union[float, None, Dict[int, float]]:\n        \"\"\"\n        The average forgetting over all experience.\n\n        k: optional key from which compute forgetting.\n        \"\"\"\n        return self.stream_forgetting.result()",
  "def before_eval(self, strategy) -> None:\n        super().before_eval(strategy)\n        self.stream_forgetting.reset()",
  "def after_eval_exp(self, strategy: 'BaseStrategy') -> None:\n        # update experience on which training just ended\n        if self.train_exp_id == self.eval_exp_id:\n            self.exp_update(self.eval_exp_id,\n                            self.metric_result(strategy),\n                            initial=True)\n        else:\n            # update other experiences\n            # if experience has not been encountered in training\n            # its value will not be considered in forgetting\n            self.exp_update(self.eval_exp_id,\n                            self.metric_result(strategy))\n\n        # this checks if the evaluation experience has been\n        # already encountered at training time\n        # before the last training.\n        # If not, forgetting should not be returned.\n        exp_forgetting = self.exp_result(k=self.eval_exp_id)\n        if exp_forgetting is not None:\n            self.stream_forgetting.update(exp_forgetting, weight=1)",
  "def after_eval(self, strategy: 'BaseStrategy') -> \\\n            'MetricResult':\n        return self._package_result(strategy)",
  "def _package_result(self, strategy: 'BaseStrategy') -> \\\n            MetricResult:\n        metric_value = self.result()\n\n        phase_name, _ = phase_and_task(strategy)\n        stream = stream_type(strategy.experience)\n        metric_name = '{}/{}_phase/{}_stream' \\\n            .format(str(self),\n                    phase_name,\n                    stream)\n        plot_x_position = self.get_global_counter()\n\n        return [MetricValue(self, metric_name, metric_value, plot_x_position)]",
  "def metric_update(self, strategy):\n        raise NotImplementedError",
  "def metric_result(self, strategy):\n        raise NotImplementedError",
  "def __str__(self):\n        raise NotImplementedError",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the StreamForgetting metric.\n        \"\"\"\n\n        super().__init__()\n\n        self._current_metric = Accuracy()\n        \"\"\"\n        The average accuracy over the current evaluation experience\n        \"\"\"",
  "def metric_update(self, strategy):\n        self._current_metric.update(strategy.mb_y,\n                                    strategy.mb_output, 0)",
  "def metric_result(self, strategy):\n        return self._current_metric.result(0)[0]",
  "def __str__(self):\n        return \"StreamForgetting\"",
  "def result(self, k=None) -> Union[float, None, Dict[int, float]]:\n        \"\"\"\n        Backward Transfer is returned only for keys encountered twice.\n        Backward Transfer is the negative forgetting.\n\n        :param k: the key for which returning backward transfer. If k has not\n            updated at least twice it returns None. If k is None,\n            backward transfer will be returned for all keys encountered at\n            least twice.\n\n        :return: the difference between the last value encountered for k\n            and its first value, if k is not None.\n            It returns None if k has not been updated\n            at least twice. If k is None, returns a dictionary\n            containing keys whose value has been updated at least twice. The\n            associated value is the difference between the last and first\n            value recorded for that key.\n        \"\"\"\n\n        forgetting = super().result(k)\n        bwt = forgetting_to_bwt(forgetting)\n        return bwt",
  "def result(self, k=None) -> Union[float, None, Dict[int, float]]:\n        \"\"\"\n        See `Forgetting` documentation for more detailed information.\n\n        k: optional key from which compute forgetting.\n        \"\"\"\n        forgetting = super().result(k)\n        return forgetting_to_bwt(forgetting)",
  "def __str__(self):\n        return \"ExperienceBWT\"",
  "def exp_result(self, k=None) -> Union[float, None, Dict[int, float]]:\n        \"\"\"\n        Result for experience defined by a key.\n        See `BWT` documentation for more detailed information.\n\n        k: optional key from which compute backward transfer.\n        \"\"\"\n        forgetting = super().exp_result(k)\n        return forgetting_to_bwt(forgetting)",
  "def __str__(self):\n        return \"StreamBWT\"",
  "class MaxRAM(Metric[float]):\n    \"\"\"\n    The standalone RAM usage metric.\n    Important: this metric approximates the real maximum RAM usage since\n    it sample at discrete amount of time the RAM values.\n\n    Instances of this metric keeps the maximum RAM usage detected.\n    The `start_thread` method starts the usage tracking.\n    The `stop_thread` method stops the tracking.\n\n    The result, obtained using the `result` method, is the usage in mega-bytes.\n\n    The reset method will bring the metric to its initial state. By default\n    this metric in its initial state will return an usage value of 0.\n    \"\"\"\n\n    def __init__(self, every=1):\n        \"\"\"\n        Creates an instance of the RAM usage metric.\n        :param every: seconds after which update the maximum RAM\n            usage\n        \"\"\"\n\n        self._process_handle: Optional[Process] = Process(os.getpid())\n        \"\"\"\n        The process handle, lazily initialized.\n        \"\"\"\n\n        self.every = every\n\n        self.stop_f = False\n        \"\"\"\n        Flag to stop the thread\n        \"\"\"\n\n        self.max_usage = 0\n        \"\"\"\n        Main metric result. Max RAM usage.\n        \"\"\"\n\n        self.thread = None\n        \"\"\"\n        Thread executing RAM monitoring code\n        \"\"\"\n\n    def _f(self):\n        \"\"\"\n        Until a stop signal is encountered,\n        this function monitors each `every` seconds\n        the maximum amount of RAM used by the process\n        \"\"\"\n        start_time = time.monotonic()\n        while not self.stop_f:\n            # ram usage in MB\n            ram_usage = self._process_handle.memory_info().rss / 1024 / 1024\n            if ram_usage > self.max_usage:\n                self.max_usage = ram_usage\n            time.sleep(self.every - ((time.monotonic() - start_time)\n                                     % self.every))\n\n    def result(self) -> Optional[float]:\n        \"\"\"\n        Retrieves the RAM usage.\n\n        Calling this method will not change the internal state of the metric.\n\n        :return: The average RAM usage in bytes, as a float value.\n        \"\"\"\n        return self.max_usage\n\n    def start_thread(self):\n        assert not self.thread, \"Trying to start thread \" \\\n                                \"without joining the previous.\"\n        self.thread = Thread(target=self._f, daemon=True)\n        self.thread.start()\n\n    def stop_thread(self):\n        if self.thread:\n            self.stop_f = True\n            self.thread.join()\n            self.stop_f = False\n            self.thread = None\n\n    def reset(self) -> None:\n        \"\"\"\n        Resets the metric.\n\n        :return: None.\n        \"\"\"\n        self.max_usage = 0\n\n    def update(self):\n        pass",
  "class RAMPluginMetric(GenericPluginMetric[float]):\n    def __init__(self, every, reset_at, emit_at, mode):\n        self._ram = MaxRAM(every)\n\n        super(RAMPluginMetric, self).__init__(\n            self._ram, reset_at, emit_at, mode)\n\n    def update(self, strategy):\n        self._ram.update()",
  "class MinibatchMaxRAM(RAMPluginMetric):\n    \"\"\"\n    The Minibatch Max RAM metric.\n    This plugin metric only works at training time.\n    \"\"\"\n\n    def __init__(self, every=1):\n        \"\"\"\n        Creates an instance of the Minibatch Max RAM metric\n        :param every: seconds after which update the maximum RAM\n            usage\n        \"\"\"\n        super(MinibatchMaxRAM, self).__init__(\n            every, reset_at='iteration', emit_at='iteration', mode='train')\n\n    def before_training(self, strategy: 'BaseStrategy') \\\n            -> None:\n        super().before_training(strategy)\n        self._ram.start_thread()\n\n    def after_training(self, strategy: 'BaseStrategy') -> None:\n        super().after_training(strategy)\n        self._ram.stop_thread()\n\n    def __str__(self):\n        return \"MaxRAMUsage_MB\"",
  "class EpochMaxRAM(RAMPluginMetric):\n    \"\"\"\n    The Epoch Max RAM metric.\n    This plugin metric only works at training time.\n    \"\"\"\n\n    def __init__(self, every=1):\n        \"\"\"\n        Creates an instance of the epoch Max RAM metric.\n        :param every: seconds after which update the maximum RAM\n            usage\n        \"\"\"\n        super(EpochMaxRAM, self).__init__(\n            every, reset_at='epoch', emit_at='epoch', mode='train')\n\n    def before_training(self, strategy: 'BaseStrategy') \\\n            -> None:\n        super().before_training(strategy)\n        self._ram.start_thread()\n\n    def after_training(self, strategy: 'BaseStrategy') -> None:\n        super().before_training(strategy)\n        self._ram.stop_thread()\n\n    def __str__(self):\n        return \"MaxRAMUsage_Epoch\"",
  "class ExperienceMaxRAM(RAMPluginMetric):\n    \"\"\"\n    The Experience Max RAM metric.\n    This plugin metric only works at eval time.\n    \"\"\"\n\n    def __init__(self, every=1):\n        \"\"\"\n        Creates an instance of the Experience CPU usage metric.\n        :param every: seconds after which update the maximum RAM\n            usage\n        \"\"\"\n        super(ExperienceMaxRAM, self).__init__(\n            every, reset_at='experience', emit_at='experience', mode='eval')\n\n    def before_eval(self, strategy: 'BaseStrategy') \\\n            -> None:\n        super().before_eval(strategy)\n        self._ram.start_thread()\n\n    def after_eval(self, strategy: 'BaseStrategy') -> None:\n        super().after_eval(strategy)\n        self._ram.stop_thread()\n\n    def __str__(self):\n        return \"MaxRAMUsage_Experience\"",
  "class StreamMaxRAM(RAMPluginMetric):\n    \"\"\"\n    The Stream Max RAM metric.\n    This plugin metric only works at eval time.\n    \"\"\"\n\n    def __init__(self, every=1):\n        \"\"\"\n        Creates an instance of the Experience CPU usage metric.\n        :param every: seconds after which update the maximum RAM\n            usage\n        \"\"\"\n        super(StreamMaxRAM, self).__init__(\n            every, reset_at='stream', emit_at='stream', mode='eval')\n\n    def before_eval(self, strategy):\n        super().before_eval(strategy)\n        self._ram.start_thread()\n\n    def after_eval(self, strategy: 'BaseStrategy') \\\n            -> MetricResult:\n        packed = super().after_eval(strategy)\n        self._ram.stop_thread()\n        return packed\n\n    def __str__(self):\n        return \"MaxRAMUsage_Stream\"",
  "def ram_usage_metrics(*, every=1, minibatch=False, epoch=False,\n                      experience=False, stream=False) -> List[PluginMetric]:\n    \"\"\"\n    Helper method that can be used to obtain the desired set of\n    plugin metrics.\n\n    :param every: seconds after which update the maximum RAM\n        usage\n    :param minibatch: If True, will return a metric able to log the minibatch\n        max RAM usage.\n    :param epoch: If True, will return a metric able to log the epoch\n        max RAM usage.\n    :param experience: If True, will return a metric able to log the experience\n        max RAM usage.\n    :param stream: If True, will return a metric able to log the evaluation\n        max stream RAM usage.\n\n    :return: A list of plugin metrics.\n    \"\"\"\n\n    metrics = []\n    if minibatch:\n        metrics.append(MinibatchMaxRAM(every=every))\n\n    if epoch:\n        metrics.append(EpochMaxRAM(every=every))\n\n    if experience:\n        metrics.append(ExperienceMaxRAM(every=every))\n\n    if stream:\n        metrics.append(StreamMaxRAM(every=every))\n\n    return metrics",
  "def __init__(self, every=1):\n        \"\"\"\n        Creates an instance of the RAM usage metric.\n        :param every: seconds after which update the maximum RAM\n            usage\n        \"\"\"\n\n        self._process_handle: Optional[Process] = Process(os.getpid())\n        \"\"\"\n        The process handle, lazily initialized.\n        \"\"\"\n\n        self.every = every\n\n        self.stop_f = False\n        \"\"\"\n        Flag to stop the thread\n        \"\"\"\n\n        self.max_usage = 0\n        \"\"\"\n        Main metric result. Max RAM usage.\n        \"\"\"\n\n        self.thread = None\n        \"\"\"\n        Thread executing RAM monitoring code\n        \"\"\"",
  "def _f(self):\n        \"\"\"\n        Until a stop signal is encountered,\n        this function monitors each `every` seconds\n        the maximum amount of RAM used by the process\n        \"\"\"\n        start_time = time.monotonic()\n        while not self.stop_f:\n            # ram usage in MB\n            ram_usage = self._process_handle.memory_info().rss / 1024 / 1024\n            if ram_usage > self.max_usage:\n                self.max_usage = ram_usage\n            time.sleep(self.every - ((time.monotonic() - start_time)\n                                     % self.every))",
  "def result(self) -> Optional[float]:\n        \"\"\"\n        Retrieves the RAM usage.\n\n        Calling this method will not change the internal state of the metric.\n\n        :return: The average RAM usage in bytes, as a float value.\n        \"\"\"\n        return self.max_usage",
  "def start_thread(self):\n        assert not self.thread, \"Trying to start thread \" \\\n                                \"without joining the previous.\"\n        self.thread = Thread(target=self._f, daemon=True)\n        self.thread.start()",
  "def stop_thread(self):\n        if self.thread:\n            self.stop_f = True\n            self.thread.join()\n            self.stop_f = False\n            self.thread = None",
  "def reset(self) -> None:\n        \"\"\"\n        Resets the metric.\n\n        :return: None.\n        \"\"\"\n        self.max_usage = 0",
  "def update(self):\n        pass",
  "def __init__(self, every, reset_at, emit_at, mode):\n        self._ram = MaxRAM(every)\n\n        super(RAMPluginMetric, self).__init__(\n            self._ram, reset_at, emit_at, mode)",
  "def update(self, strategy):\n        self._ram.update()",
  "def __init__(self, every=1):\n        \"\"\"\n        Creates an instance of the Minibatch Max RAM metric\n        :param every: seconds after which update the maximum RAM\n            usage\n        \"\"\"\n        super(MinibatchMaxRAM, self).__init__(\n            every, reset_at='iteration', emit_at='iteration', mode='train')",
  "def before_training(self, strategy: 'BaseStrategy') \\\n            -> None:\n        super().before_training(strategy)\n        self._ram.start_thread()",
  "def after_training(self, strategy: 'BaseStrategy') -> None:\n        super().after_training(strategy)\n        self._ram.stop_thread()",
  "def __str__(self):\n        return \"MaxRAMUsage_MB\"",
  "def __init__(self, every=1):\n        \"\"\"\n        Creates an instance of the epoch Max RAM metric.\n        :param every: seconds after which update the maximum RAM\n            usage\n        \"\"\"\n        super(EpochMaxRAM, self).__init__(\n            every, reset_at='epoch', emit_at='epoch', mode='train')",
  "def before_training(self, strategy: 'BaseStrategy') \\\n            -> None:\n        super().before_training(strategy)\n        self._ram.start_thread()",
  "def after_training(self, strategy: 'BaseStrategy') -> None:\n        super().before_training(strategy)\n        self._ram.stop_thread()",
  "def __str__(self):\n        return \"MaxRAMUsage_Epoch\"",
  "def __init__(self, every=1):\n        \"\"\"\n        Creates an instance of the Experience CPU usage metric.\n        :param every: seconds after which update the maximum RAM\n            usage\n        \"\"\"\n        super(ExperienceMaxRAM, self).__init__(\n            every, reset_at='experience', emit_at='experience', mode='eval')",
  "def before_eval(self, strategy: 'BaseStrategy') \\\n            -> None:\n        super().before_eval(strategy)\n        self._ram.start_thread()",
  "def after_eval(self, strategy: 'BaseStrategy') -> None:\n        super().after_eval(strategy)\n        self._ram.stop_thread()",
  "def __str__(self):\n        return \"MaxRAMUsage_Experience\"",
  "def __init__(self, every=1):\n        \"\"\"\n        Creates an instance of the Experience CPU usage metric.\n        :param every: seconds after which update the maximum RAM\n            usage\n        \"\"\"\n        super(StreamMaxRAM, self).__init__(\n            every, reset_at='stream', emit_at='stream', mode='eval')",
  "def before_eval(self, strategy):\n        super().before_eval(strategy)\n        self._ram.start_thread()",
  "def after_eval(self, strategy: 'BaseStrategy') \\\n            -> MetricResult:\n        packed = super().after_eval(strategy)\n        self._ram.stop_thread()\n        return packed",
  "def __str__(self):\n        return \"MaxRAMUsage_Stream\"",
  "class Loss(Metric[float]):\n    \"\"\"\n    The standalone Loss metric. This is a general metric\n    used to compute more specific ones.\n\n    Instances of this metric keeps the running average loss\n    over multiple <prediction, target> pairs of Tensors,\n    provided incrementally.\n    The \"prediction\" and \"target\" tensors may contain plain labels or\n    one-hot/logit vectors.\n\n    Each time `result` is called, this metric emits the average loss\n    across all predictions made since the last `reset`.\n\n    The reset method will bring the metric to its initial state. By default\n    this metric in its initial state will return a loss value of 0.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the loss metric.\n\n        By default this metric in its initial state will return a loss\n        value of 0. The metric can be updated by using the `update` method\n        while the running loss can be retrieved using the `result` method.\n        \"\"\"\n        self._mean_loss = defaultdict(Mean)\n        \"\"\"\n        The mean utility that will be used to store the running accuracy\n        for each task label.\n        \"\"\"\n\n    @torch.no_grad()\n    def update(self, loss: Tensor, patterns: int, task_label: int) -> None:\n        \"\"\"\n        Update the running loss given the loss Tensor and the minibatch size.\n\n        :param loss: The loss Tensor. Different reduction types don't affect\n            the result.\n        :param patterns: The number of patterns in the minibatch.\n        :param task_label: the task label associated to the current experience\n        :return: None.\n        \"\"\"\n        self._mean_loss[task_label].update(torch.mean(loss), weight=patterns)\n\n    def result(self, task_label=None) -> Dict[int, float]:\n        \"\"\"\n        Retrieves the running average loss per pattern.\n\n        Calling this method will not change the internal state of the metric.\n        :param task_label: None to return metric values for all the task labels.\n            If an int, return value only for that task label\n        :return: The running loss, as a float.\n        \"\"\"\n        assert(task_label is None or isinstance(task_label, int))\n        if task_label is None:\n            return {k: v.result() for k, v in self._mean_loss.items()}\n        else:\n            return {task_label: self._mean_loss[task_label].result()}\n\n    def reset(self, task_label=None) -> None:\n        \"\"\"\n        Resets the metric.\n\n        :param task_label: None to reset all metric values. If an int,\n            reset metric value corresponding to that task label.\n        :return: None.\n        \"\"\"\n        assert(task_label is None or isinstance(task_label, int))\n        if task_label is None:\n            self._mean_loss = defaultdict(Mean)\n        else:\n            self._mean_loss[task_label].reset()",
  "class LossPluginMetric(GenericPluginMetric[float]):\n    def __init__(self, reset_at, emit_at, mode):\n        self._loss = Loss()\n        super(LossPluginMetric, self).__init__(\n            self._loss, reset_at, emit_at, mode)\n\n    def reset(self, strategy=None) -> None:\n        if self._reset_at == 'stream' or strategy is None:\n            self._metric.reset()\n        else:\n            self._metric.reset(phase_and_task(strategy)[1])\n\n    def result(self, strategy=None) -> float:\n        if self._emit_at == 'stream' or strategy is None:\n            return self._metric.result()\n        else:\n            return self._metric.result(phase_and_task(strategy)[1])\n\n    def update(self, strategy):\n        # task labels defined for each experience\n        task_labels = strategy.experience.task_labels\n        if len(task_labels) > 1:\n            # task labels defined for each pattern\n            # fall back to single task case\n            task_label = 0\n        else:\n            task_label = task_labels[0]\n        self._loss.update(strategy.loss,\n                          patterns=len(strategy.mb_y), task_label=task_label)",
  "class MinibatchLoss(LossPluginMetric):\n    \"\"\"\n    The minibatch loss metric.\n    This plugin metric only works at training time.\n\n    This metric computes the average loss over patterns\n    from a single minibatch.\n    It reports the result after each iteration.\n\n    If a more coarse-grained logging is needed, consider using\n    :class:`EpochLoss` instead.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the MinibatchLoss metric.\n        \"\"\"\n        super(MinibatchLoss, self).__init__(\n            reset_at='iteration', emit_at='iteration', mode='train')\n\n    def __str__(self):\n        return \"Loss_MB\"",
  "class EpochLoss(LossPluginMetric):\n    \"\"\"\n    The average loss over a single training epoch.\n    This plugin metric only works at training time.\n\n    The loss will be logged after each training epoch by computing\n    the loss on the predicted patterns during the epoch divided by\n    the overall number of patterns encountered in that epoch.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the EpochLoss metric.\n        \"\"\"\n\n        super(EpochLoss, self).__init__(\n            reset_at='epoch', emit_at='epoch', mode='train')\n\n    def __str__(self):\n        return \"Loss_Epoch\"",
  "class RunningEpochLoss(LossPluginMetric):\n    \"\"\"\n    The average loss across all minibatches up to the current\n    epoch iteration.\n    This plugin metric only works at training time.\n\n    At each iteration, this metric logs the loss averaged over all patterns\n    seen so far in the current epoch.\n    The metric resets its state after each training epoch.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the RunningEpochLoss metric.\n        \"\"\"\n\n        super(RunningEpochLoss, self).__init__(\n            reset_at='epoch', emit_at='iteration', mode='train')\n\n    def __str__(self):\n        return \"RunningLoss_Epoch\"",
  "class ExperienceLoss(LossPluginMetric):\n    \"\"\"\n    At the end of each experience, this metric reports\n    the average loss over all patterns seen in that experience.\n    This plugin metric only works at eval time.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of ExperienceLoss metric\n        \"\"\"\n        super(ExperienceLoss, self).__init__(\n            reset_at='experience', emit_at='experience', mode='eval')\n\n    def __str__(self):\n        return \"Loss_Exp\"",
  "class StreamLoss(LossPluginMetric):\n    \"\"\"\n    At the end of the entire stream of experiences, this metric reports the\n    average loss over all patterns seen in all experiences.\n    This plugin metric only works at eval time.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of StreamLoss metric\n        \"\"\"\n        super(StreamLoss, self).__init__(\n            reset_at='stream', emit_at='stream', mode='eval')\n\n    def __str__(self):\n        return \"Loss_Stream\"",
  "def loss_metrics(*, minibatch=False, epoch=False, epoch_running=False,\n                 experience=False, stream=False) -> List[PluginMetric]:\n    \"\"\"\n    Helper method that can be used to obtain the desired set of\n    plugin metrics.\n\n    :param minibatch: If True, will return a metric able to log\n        the minibatch loss at training time.\n    :param epoch: If True, will return a metric able to log\n        the epoch loss at training time.\n    :param epoch_running: If True, will return a metric able to log\n        the running epoch loss at training time.\n    :param experience: If True, will return a metric able to log\n        the loss on each evaluation experience.\n    :param stream: If True, will return a metric able to log\n        the loss averaged over the entire evaluation stream of experiences.\n\n    :return: A list of plugin metrics.\n    \"\"\"\n\n    metrics = []\n    if minibatch:\n        metrics.append(MinibatchLoss())\n\n    if epoch:\n        metrics.append(EpochLoss())\n\n    if epoch_running:\n        metrics.append(RunningEpochLoss())\n\n    if experience:\n        metrics.append(ExperienceLoss())\n\n    if stream:\n        metrics.append(StreamLoss())\n\n    return metrics",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the loss metric.\n\n        By default this metric in its initial state will return a loss\n        value of 0. The metric can be updated by using the `update` method\n        while the running loss can be retrieved using the `result` method.\n        \"\"\"\n        self._mean_loss = defaultdict(Mean)\n        \"\"\"\n        The mean utility that will be used to store the running accuracy\n        for each task label.\n        \"\"\"",
  "def update(self, loss: Tensor, patterns: int, task_label: int) -> None:\n        \"\"\"\n        Update the running loss given the loss Tensor and the minibatch size.\n\n        :param loss: The loss Tensor. Different reduction types don't affect\n            the result.\n        :param patterns: The number of patterns in the minibatch.\n        :param task_label: the task label associated to the current experience\n        :return: None.\n        \"\"\"\n        self._mean_loss[task_label].update(torch.mean(loss), weight=patterns)",
  "def result(self, task_label=None) -> Dict[int, float]:\n        \"\"\"\n        Retrieves the running average loss per pattern.\n\n        Calling this method will not change the internal state of the metric.\n        :param task_label: None to return metric values for all the task labels.\n            If an int, return value only for that task label\n        :return: The running loss, as a float.\n        \"\"\"\n        assert(task_label is None or isinstance(task_label, int))\n        if task_label is None:\n            return {k: v.result() for k, v in self._mean_loss.items()}\n        else:\n            return {task_label: self._mean_loss[task_label].result()}",
  "def reset(self, task_label=None) -> None:\n        \"\"\"\n        Resets the metric.\n\n        :param task_label: None to reset all metric values. If an int,\n            reset metric value corresponding to that task label.\n        :return: None.\n        \"\"\"\n        assert(task_label is None or isinstance(task_label, int))\n        if task_label is None:\n            self._mean_loss = defaultdict(Mean)\n        else:\n            self._mean_loss[task_label].reset()",
  "def __init__(self, reset_at, emit_at, mode):\n        self._loss = Loss()\n        super(LossPluginMetric, self).__init__(\n            self._loss, reset_at, emit_at, mode)",
  "def reset(self, strategy=None) -> None:\n        if self._reset_at == 'stream' or strategy is None:\n            self._metric.reset()\n        else:\n            self._metric.reset(phase_and_task(strategy)[1])",
  "def result(self, strategy=None) -> float:\n        if self._emit_at == 'stream' or strategy is None:\n            return self._metric.result()\n        else:\n            return self._metric.result(phase_and_task(strategy)[1])",
  "def update(self, strategy):\n        # task labels defined for each experience\n        task_labels = strategy.experience.task_labels\n        if len(task_labels) > 1:\n            # task labels defined for each pattern\n            # fall back to single task case\n            task_label = 0\n        else:\n            task_label = task_labels[0]\n        self._loss.update(strategy.loss,\n                          patterns=len(strategy.mb_y), task_label=task_label)",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the MinibatchLoss metric.\n        \"\"\"\n        super(MinibatchLoss, self).__init__(\n            reset_at='iteration', emit_at='iteration', mode='train')",
  "def __str__(self):\n        return \"Loss_MB\"",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the EpochLoss metric.\n        \"\"\"\n\n        super(EpochLoss, self).__init__(\n            reset_at='epoch', emit_at='epoch', mode='train')",
  "def __str__(self):\n        return \"Loss_Epoch\"",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the RunningEpochLoss metric.\n        \"\"\"\n\n        super(RunningEpochLoss, self).__init__(\n            reset_at='epoch', emit_at='iteration', mode='train')",
  "def __str__(self):\n        return \"RunningLoss_Epoch\"",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of ExperienceLoss metric\n        \"\"\"\n        super(ExperienceLoss, self).__init__(\n            reset_at='experience', emit_at='experience', mode='eval')",
  "def __str__(self):\n        return \"Loss_Exp\"",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of StreamLoss metric\n        \"\"\"\n        super(StreamLoss, self).__init__(\n            reset_at='stream', emit_at='stream', mode='eval')",
  "def __str__(self):\n        return \"Loss_Stream\"",
  "class ForwardTransfer(Metric[Union[float, None, Dict[int, float]]]):\n    \"\"\"\n        The standalone Forward Transfer metric.\n        This metric returns the forward transfer relative to a specific key.\n        Alternatively, this metric returns a dict in which each key is\n        associated to the forward transfer.\n        Forward transfer is computed as the difference between the value\n        recorded for a specific key after the previous experience has\n        been trained on, and random initialization before training.\n        The value associated to a key can be updated with the `update` method.\n\n        At initialization, this metric returns an empty dictionary.\n        \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the standalone Forward Transfer metric\n        \"\"\"\n\n        super().__init__()\n\n        self.initial: Dict[int, float] = dict()\n        \"\"\"\n        The initial value for each key. This is the accuracy at \n        random initialization.\n        \"\"\"\n\n        self.previous: Dict[int, float] = dict()\n        \"\"\"\n        The previous experience value detected for each key\n        \"\"\"\n\n    def update_initial(self, k, v):\n        self.initial[k] = v\n\n    def update_previous(self, k, v):\n        self.previous[k] = v\n\n    def update(self, k, v, initial=False):\n        if initial:\n            self.update_initial(k, v)\n        else:\n            self.update_previous(k, v)\n\n    def result(self, k=None) -> Union[float, None, Dict[int, float]]:\n        \"\"\"\n        :param k: the key for which returning forward transfer. If k is None,\n            forward transfer will be returned for all keys\n            where the previous experience has been trained on.\n\n        :return: the difference between the key value after training on the\n            previous experience, and the key at random initialization.\n        \"\"\"\n\n        forward_transfer = {}\n        if k is not None:\n            if k in self.previous:\n                return self.previous[k] - self.initial[k]\n            else:\n                return None\n\n        previous_keys = set(self.previous.keys())\n        for k in previous_keys:\n            forward_transfer[k] = self.previous[k] - self.initial[k]\n\n        return forward_transfer\n\n    def reset(self) -> None:\n        self.previous: Dict[int, float] = dict()",
  "class GenericExperienceForwardTransfer(PluginMetric[Dict[int, float]]):\n    \"\"\"\n    The GenericExperienceForwardMetric metric, describing the forward transfer\n    detected after a certain experience. The user should\n    subclass this and provide the desired metric.\n\n    In particular, the user should override:\n    * __init__ by calling `super` and instantiating the `self.current_metric`\n    property as a valid avalanche metric\n    * `metric_update`, to update `current_metric`\n    * `metric_result` to get the result from `current_metric`.\n    * `__str__` to define the experience forward transfer  name.\n\n    This metric is computed during the eval phase only.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the GenericExperienceForwardTransfer metric.\n        \"\"\"\n\n        super().__init__()\n\n        self.forward_transfer = ForwardTransfer()\n        \"\"\"\n        The general metric to compute forward transfer\n        \"\"\"\n\n        self._current_metric = None\n        \"\"\"\n        The metric the user should override\n        \"\"\"\n\n        self.eval_exp_id = None\n        \"\"\"\n        The current evaluation experience id\n        \"\"\"\n\n        self.train_exp_id = None\n        \"\"\"\n        The last encountered training experience id\n        \"\"\"\n\n        self.at_init = True\n\n    def reset(self) -> None:\n        \"\"\"\n        Resets the metric.\n\n        Note that this will reset the previous and initial accuracy of each\n        experience.\n\n        :return: None.\n        \"\"\"\n        self.forward_transfer.reset()\n\n    def update(self, k, v, initial=False):\n        \"\"\"\n        Update forward transfer metric.\n        See `ForwardTransfer` for more detailed information.\n\n        :param k: key to update\n        :param v: value associated to k\n        :param initial: update initial value. If False, update\n            previous value.\n        \"\"\"\n        self.forward_transfer.update(k, v, initial=initial)\n\n    def result(self, k=None) -> Union[float, None, Dict[int, float]]:\n        \"\"\"\n        Result for experience defined by a key.\n        See `ForwardTransfer` documentation for more detailed information.\n\n        k: optional key from which to compute forward transfer.\n        \"\"\"\n        return self.forward_transfer.result(k=k)\n\n    def before_training_exp(self, strategy: 'BaseStrategy') -> None:\n        self.train_exp_id = strategy.experience.current_experience\n\n    def after_eval(self, strategy):\n        if self.at_init:\n            assert strategy.eval_every > -1, \\\n                \"eval every > -1 to compute forward transfer\"\n            self.at_init = False\n\n    def before_eval_exp(self, strategy: 'BaseStrategy') -> None:\n        self._current_metric.reset()\n\n    def after_eval_iteration(self, strategy: 'BaseStrategy') -> None:\n        super().after_eval_iteration(strategy)\n        self.eval_exp_id = strategy.experience.current_experience\n        self.metric_update(strategy)\n\n    def after_eval_exp(self, strategy: 'BaseStrategy') \\\n            -> MetricResult:\n        if self.at_init:\n            self.update(self.eval_exp_id,\n                        self.metric_result(strategy), initial=True)\n        else:\n            if self.train_exp_id == self.eval_exp_id - 1:\n                self.update(self.eval_exp_id,\n                            self.metric_result(strategy))\n\n                return self._package_result(strategy)\n\n    def _package_result(self, strategy: 'BaseStrategy') \\\n            -> MetricResult:\n        # Only after the previous experience was trained on can we return the\n        # forward transfer metric for this experience.\n        result = self.result(k=self.eval_exp_id)\n        if result is not None:\n            metric_name = get_metric_name(self, strategy, add_experience=True)\n            plot_x_position = self.get_global_counter()\n\n            metric_values = [MetricValue(\n                self, metric_name, result, plot_x_position)]\n            return metric_values\n\n    def metric_update(self, strategy):\n        raise NotImplementedError\n\n    def metric_result(self, strategy):\n        raise NotImplementedError\n\n    def __str__(self):\n        raise NotImplementedError",
  "class ExperienceForwardTransfer(GenericExperienceForwardTransfer):\n    \"\"\"\n    The Forward Transfer computed on each experience separately.\n    The transfer is computed based on the accuracy metric.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n\n        self._current_metric = Accuracy()\n        \"\"\"\n        The average accuracy over the current evaluation experience\n        \"\"\"\n\n    def metric_update(self, strategy):\n        self._current_metric.update(strategy.mb_y,\n                                    strategy.mb_output, 0)\n\n    def metric_result(self, strategy):\n        return self._current_metric.result(0)[0]\n\n    def __str__(self):\n        return \"ExperienceForwardTransfer\"",
  "class GenericStreamForwardTransfer(GenericExperienceForwardTransfer):\n    \"\"\"\n    The GenericStreamForwardTransfer metric, describing the average evaluation\n    forward transfer detected over all experiences observed during training.\n\n    In particular, the user should override:\n    * __init__ by calling `super` and instantiating the `self.current_metric`\n    property as a valid avalanche metric\n    * `metric_update`, to update `current_metric`\n    * `metric_result` to get the result from `current_metric`.\n    * `__str__` to define the experience forgetting  name.\n\n    This metric is computed during the eval phase only.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the GenericStreamForwardTransfer metric.\n        \"\"\"\n\n        super().__init__()\n\n        self.stream_forward_transfer = Mean()\n        \"\"\"\n        The average forward transfer over all experiences\n        \"\"\"\n\n    def reset(self) -> None:\n        \"\"\"\n        Resets the forward transfer metrics.\n\n        Note that this will reset the previous and initial accuracy of each\n        experience.\n\n        :return: None.\n        \"\"\"\n        super().reset()\n        self.stream_forward_transfer.reset()\n\n    def exp_update(self, k, v, initial=False):\n        \"\"\"\n        Update forward transfer metric.\n        See `Forward Transfer` for more detailed information.\n\n        :param k: key to update\n        :param v: value associated to k\n        :param initial: update initial value. If False, update\n            previous value.\n        \"\"\"\n        super().update(k, v, initial=initial)\n\n    def exp_result(self, k=None) -> Union[float, None, Dict[int, float]]:\n        \"\"\"\n        Result for experience defined by a key.\n        See `ForwardTransfer` documentation for more detailed information.\n\n        k: optional key from which to compute forward transfer.\n        \"\"\"\n        return super().result(k=k)\n\n    def result(self, k=None) -> Union[float, None, Dict[int, float]]:\n        \"\"\"\n        The average forward transfer over all experiences.\n\n        k: optional key from which to compute forward transfer.\n        \"\"\"\n        return self.stream_forward_transfer.result()\n\n    def before_eval(self, strategy) -> None:\n        super().before_eval(strategy)\n        self.stream_forward_transfer.reset()\n\n    def after_eval_exp(self, strategy: 'BaseStrategy') -> None:\n        if self.at_init:\n            self.update(self.eval_exp_id,\n                        self.metric_result(strategy), initial=True)\n        else:\n            if self.train_exp_id == self.eval_exp_id - 1:\n                self.update(self.eval_exp_id,\n                            self.metric_result(strategy))\n            exp_forward_transfer = self.exp_result(k=self.eval_exp_id)\n            if exp_forward_transfer is not None:\n                self.stream_forward_transfer.update(exp_forward_transfer,\n                                                    weight=1)\n\n    def after_eval(self, strategy: 'BaseStrategy') -> \\\n            'MetricResult':\n        super().after_eval(strategy)\n        return self._package_result(strategy)\n\n    def _package_result(self, strategy: 'BaseStrategy') -> \\\n            MetricResult:\n        metric_value = self.result()\n\n        phase_name, _ = phase_and_task(strategy)\n        stream = stream_type(strategy.experience)\n        metric_name = '{}/{}_phase/{}_stream' \\\n            .format(str(self),\n                    phase_name,\n                    stream)\n        plot_x_position = self.get_global_counter()\n\n        return [MetricValue(self, metric_name, metric_value, plot_x_position)]\n\n    def metric_update(self, strategy):\n        raise NotImplementedError\n\n    def metric_result(self, strategy):\n        raise NotImplementedError\n\n    def __str__(self):\n        raise NotImplementedError",
  "class StreamForwardTransfer(GenericStreamForwardTransfer):\n    \"\"\"\n    The Forward Transfer averaged over all the evaluation experiences.\n\n    This plugin metric, computed over all observed experiences during training,\n    is the average over the difference between the accuracy result obtained\n    after the previous experience and the accuracy result obtained\n    on random initialization.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self._current_metric = Accuracy()\n        \"\"\"\n        The average accuracy over the current evaluation experience\n        \"\"\"\n\n    def metric_update(self, strategy):\n        self._current_metric.update(strategy.mb_y,\n                                    strategy.mb_output, 0)\n\n    def metric_result(self, strategy):\n        return self._current_metric.result(0)[0]\n\n    def __str__(self):\n        return \"StreamForwardTransfer\"",
  "def forward_transfer_metrics(*, experience=False, stream=False):\n    \"\"\"\n    Helper method that can be used to obtain the desired set of\n    plugin metrics.\n\n    :param experience: If True, will return a metric able to log\n        the forward transfer on each evaluation experience.\n    :param stream: If True, will return a metric able to log\n        the forward transfer averaged over the evaluation stream experiences,\n        which have been observed during training.\n\n    :return: A list of plugin metrics.\n    \"\"\"\n\n    metrics = []\n\n    if experience:\n        metrics.append(ExperienceForwardTransfer())\n\n    if stream:\n        metrics.append(StreamForwardTransfer())\n\n    return metrics",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the standalone Forward Transfer metric\n        \"\"\"\n\n        super().__init__()\n\n        self.initial: Dict[int, float] = dict()\n        \"\"\"\n        The initial value for each key. This is the accuracy at \n        random initialization.\n        \"\"\"\n\n        self.previous: Dict[int, float] = dict()\n        \"\"\"\n        The previous experience value detected for each key\n        \"\"\"",
  "def update_initial(self, k, v):\n        self.initial[k] = v",
  "def update_previous(self, k, v):\n        self.previous[k] = v",
  "def update(self, k, v, initial=False):\n        if initial:\n            self.update_initial(k, v)\n        else:\n            self.update_previous(k, v)",
  "def result(self, k=None) -> Union[float, None, Dict[int, float]]:\n        \"\"\"\n        :param k: the key for which returning forward transfer. If k is None,\n            forward transfer will be returned for all keys\n            where the previous experience has been trained on.\n\n        :return: the difference between the key value after training on the\n            previous experience, and the key at random initialization.\n        \"\"\"\n\n        forward_transfer = {}\n        if k is not None:\n            if k in self.previous:\n                return self.previous[k] - self.initial[k]\n            else:\n                return None\n\n        previous_keys = set(self.previous.keys())\n        for k in previous_keys:\n            forward_transfer[k] = self.previous[k] - self.initial[k]\n\n        return forward_transfer",
  "def reset(self) -> None:\n        self.previous: Dict[int, float] = dict()",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the GenericExperienceForwardTransfer metric.\n        \"\"\"\n\n        super().__init__()\n\n        self.forward_transfer = ForwardTransfer()\n        \"\"\"\n        The general metric to compute forward transfer\n        \"\"\"\n\n        self._current_metric = None\n        \"\"\"\n        The metric the user should override\n        \"\"\"\n\n        self.eval_exp_id = None\n        \"\"\"\n        The current evaluation experience id\n        \"\"\"\n\n        self.train_exp_id = None\n        \"\"\"\n        The last encountered training experience id\n        \"\"\"\n\n        self.at_init = True",
  "def reset(self) -> None:\n        \"\"\"\n        Resets the metric.\n\n        Note that this will reset the previous and initial accuracy of each\n        experience.\n\n        :return: None.\n        \"\"\"\n        self.forward_transfer.reset()",
  "def update(self, k, v, initial=False):\n        \"\"\"\n        Update forward transfer metric.\n        See `ForwardTransfer` for more detailed information.\n\n        :param k: key to update\n        :param v: value associated to k\n        :param initial: update initial value. If False, update\n            previous value.\n        \"\"\"\n        self.forward_transfer.update(k, v, initial=initial)",
  "def result(self, k=None) -> Union[float, None, Dict[int, float]]:\n        \"\"\"\n        Result for experience defined by a key.\n        See `ForwardTransfer` documentation for more detailed information.\n\n        k: optional key from which to compute forward transfer.\n        \"\"\"\n        return self.forward_transfer.result(k=k)",
  "def before_training_exp(self, strategy: 'BaseStrategy') -> None:\n        self.train_exp_id = strategy.experience.current_experience",
  "def after_eval(self, strategy):\n        if self.at_init:\n            assert strategy.eval_every > -1, \\\n                \"eval every > -1 to compute forward transfer\"\n            self.at_init = False",
  "def before_eval_exp(self, strategy: 'BaseStrategy') -> None:\n        self._current_metric.reset()",
  "def after_eval_iteration(self, strategy: 'BaseStrategy') -> None:\n        super().after_eval_iteration(strategy)\n        self.eval_exp_id = strategy.experience.current_experience\n        self.metric_update(strategy)",
  "def after_eval_exp(self, strategy: 'BaseStrategy') \\\n            -> MetricResult:\n        if self.at_init:\n            self.update(self.eval_exp_id,\n                        self.metric_result(strategy), initial=True)\n        else:\n            if self.train_exp_id == self.eval_exp_id - 1:\n                self.update(self.eval_exp_id,\n                            self.metric_result(strategy))\n\n                return self._package_result(strategy)",
  "def _package_result(self, strategy: 'BaseStrategy') \\\n            -> MetricResult:\n        # Only after the previous experience was trained on can we return the\n        # forward transfer metric for this experience.\n        result = self.result(k=self.eval_exp_id)\n        if result is not None:\n            metric_name = get_metric_name(self, strategy, add_experience=True)\n            plot_x_position = self.get_global_counter()\n\n            metric_values = [MetricValue(\n                self, metric_name, result, plot_x_position)]\n            return metric_values",
  "def metric_update(self, strategy):\n        raise NotImplementedError",
  "def metric_result(self, strategy):\n        raise NotImplementedError",
  "def __str__(self):\n        raise NotImplementedError",
  "def __init__(self):\n        super().__init__()\n\n        self._current_metric = Accuracy()\n        \"\"\"\n        The average accuracy over the current evaluation experience\n        \"\"\"",
  "def metric_update(self, strategy):\n        self._current_metric.update(strategy.mb_y,\n                                    strategy.mb_output, 0)",
  "def metric_result(self, strategy):\n        return self._current_metric.result(0)[0]",
  "def __str__(self):\n        return \"ExperienceForwardTransfer\"",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the GenericStreamForwardTransfer metric.\n        \"\"\"\n\n        super().__init__()\n\n        self.stream_forward_transfer = Mean()\n        \"\"\"\n        The average forward transfer over all experiences\n        \"\"\"",
  "def reset(self) -> None:\n        \"\"\"\n        Resets the forward transfer metrics.\n\n        Note that this will reset the previous and initial accuracy of each\n        experience.\n\n        :return: None.\n        \"\"\"\n        super().reset()\n        self.stream_forward_transfer.reset()",
  "def exp_update(self, k, v, initial=False):\n        \"\"\"\n        Update forward transfer metric.\n        See `Forward Transfer` for more detailed information.\n\n        :param k: key to update\n        :param v: value associated to k\n        :param initial: update initial value. If False, update\n            previous value.\n        \"\"\"\n        super().update(k, v, initial=initial)",
  "def exp_result(self, k=None) -> Union[float, None, Dict[int, float]]:\n        \"\"\"\n        Result for experience defined by a key.\n        See `ForwardTransfer` documentation for more detailed information.\n\n        k: optional key from which to compute forward transfer.\n        \"\"\"\n        return super().result(k=k)",
  "def result(self, k=None) -> Union[float, None, Dict[int, float]]:\n        \"\"\"\n        The average forward transfer over all experiences.\n\n        k: optional key from which to compute forward transfer.\n        \"\"\"\n        return self.stream_forward_transfer.result()",
  "def before_eval(self, strategy) -> None:\n        super().before_eval(strategy)\n        self.stream_forward_transfer.reset()",
  "def after_eval_exp(self, strategy: 'BaseStrategy') -> None:\n        if self.at_init:\n            self.update(self.eval_exp_id,\n                        self.metric_result(strategy), initial=True)\n        else:\n            if self.train_exp_id == self.eval_exp_id - 1:\n                self.update(self.eval_exp_id,\n                            self.metric_result(strategy))\n            exp_forward_transfer = self.exp_result(k=self.eval_exp_id)\n            if exp_forward_transfer is not None:\n                self.stream_forward_transfer.update(exp_forward_transfer,\n                                                    weight=1)",
  "def after_eval(self, strategy: 'BaseStrategy') -> \\\n            'MetricResult':\n        super().after_eval(strategy)\n        return self._package_result(strategy)",
  "def _package_result(self, strategy: 'BaseStrategy') -> \\\n            MetricResult:\n        metric_value = self.result()\n\n        phase_name, _ = phase_and_task(strategy)\n        stream = stream_type(strategy.experience)\n        metric_name = '{}/{}_phase/{}_stream' \\\n            .format(str(self),\n                    phase_name,\n                    stream)\n        plot_x_position = self.get_global_counter()\n\n        return [MetricValue(self, metric_name, metric_value, plot_x_position)]",
  "def metric_update(self, strategy):\n        raise NotImplementedError",
  "def metric_result(self, strategy):\n        raise NotImplementedError",
  "def __str__(self):\n        raise NotImplementedError",
  "def __init__(self):\n        super().__init__()\n        self._current_metric = Accuracy()\n        \"\"\"\n        The average accuracy over the current evaluation experience\n        \"\"\"",
  "def metric_update(self, strategy):\n        self._current_metric.update(strategy.mb_y,\n                                    strategy.mb_output, 0)",
  "def metric_result(self, strategy):\n        return self._current_metric.result(0)[0]",
  "def __str__(self):\n        return \"StreamForwardTransfer\"",
  "class MAC(Metric[int]):\n    \"\"\"\n    Standalone Multiply-and-accumulate metric. Provides a lower bound of the\n    computational cost of a model in a hardware-independent way by\n    computing the number of multiplications. Currently supports only\n    Linear or Conv2d modules. Other operations are ignored.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the MAC metric.\n        \"\"\"\n        self.hooks = []\n        self._compute_cost: Optional[int] = 0\n\n    def update(self, model: Module, dummy_input: Tensor):\n        \"\"\"\n        Computes the MAC metric.\n\n        :param model: current model.\n        :param dummy_input: A tensor of the correct size to feed as input\n            to model. It includes batch size\n        :return: MAC metric.\n        \"\"\"\n\n        for mod in model.modules():\n            if MAC.is_recognized_module(mod):\n                def foo(a, b, c):\n                    return self.update_compute_cost(a, b, c)\n                handle = mod.register_forward_hook(foo)\n                self.hooks.append(handle)\n\n        self._compute_cost = 0\n        model(dummy_input)  # trigger forward hooks\n\n        for handle in self.hooks:\n            handle.remove()\n        self.hooks = []\n\n    def result(self) -> Optional[int]:\n        \"\"\"\n        Return the number of MAC operations as computed in the previous call\n        to the `update` method.\n\n        :return: The number of MAC operations or None if `update` has not been\n            called yet.\n        \"\"\"\n        return self._compute_cost\n\n    def reset(self):\n        pass\n\n    def update_compute_cost(self, module, dummy_input, output):\n        modname = module.__class__.__name__\n        if modname == 'Linear':\n            self._compute_cost += dummy_input[0].shape[1] * output.shape[1]\n        elif modname == 'Conv2d':\n            n, cout, hout, wout = output.shape  # Batch, Channels, Height, Width\n            ksize = module.kernel_size[0] * module.kernel_size[1]\n            self._compute_cost += cout * hout * wout * ksize\n\n    @staticmethod\n    def is_recognized_module(mod):\n        modname = mod.__class__.__name__\n        return modname == 'Linear' or modname == 'Conv2d'",
  "class MACPluginMetric(GenericPluginMetric):\n    def __init__(self, reset_at, emit_at, mode):\n        self._mac = MAC()\n\n        super(MACPluginMetric, self).__init__(\n            self._mac, reset_at=reset_at, emit_at=emit_at, mode=mode)\n\n    def update(self, strategy):\n        self._mac.update(strategy.model,\n                         strategy.mb_x[0].unsqueeze(0))",
  "class MinibatchMAC(MACPluginMetric):\n    \"\"\"\n    The minibatch MAC metric.\n    This plugin metric only works at training time.\n\n    This metric computes the MAC over 1 pattern\n    from a single minibatch.\n    It reports the result after each iteration.\n\n    If a more coarse-grained logging is needed, consider using\n    :class:`EpochMAC` instead.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the MinibatchMAC metric.\n        \"\"\"\n        super(MinibatchMAC, self).__init__(\n            reset_at='iteration', emit_at='iteration', mode='train')\n\n    def __str__(self):\n        return \"MAC_MB\"",
  "class EpochMAC(MACPluginMetric):\n    \"\"\"\n    The MAC at the end of each epoch computed on a\n    single pattern.\n    This plugin metric only works at training time.\n\n    The MAC will be logged after each training epoch.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the EpochMAC metric.\n        \"\"\"\n        super(EpochMAC, self).__init__(\n            reset_at='epoch', emit_at='epoch', mode='train')\n\n    def __str__(self):\n        return \"MAC_Epoch\"",
  "class ExperienceMAC(MACPluginMetric):\n    \"\"\"\n    At the end of each experience, this metric reports the\n    MAC computed on a single pattern.\n    This plugin metric only works at eval time.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of ExperienceMAC metric\n        \"\"\"\n        super(ExperienceMAC, self).__init__(\n            reset_at='experience', emit_at='experience', mode='eval')\n\n    def __str__(self):\n        return \"MAC_Exp\"",
  "def MAC_metrics(*, minibatch=False, epoch=False, experience=False) \\\n        -> List[PluginMetric]:\n    \"\"\"\n    Helper method that can be used to obtain the desired set of\n    plugin metrics.\n\n    :param minibatch: If True, will return a metric able to log\n        the MAC after each iteration at training time.\n    :param epoch: If True, will return a metric able to log\n        the MAC after each epoch at training time.\n    :param experience: If True, will return a metric able to log\n        the MAC after each eval experience.\n\n    :return: A list of plugin metrics.\n    \"\"\"\n\n    metrics = []\n    if minibatch:\n        metrics.append(MinibatchMAC())\n\n    if epoch:\n        metrics.append(EpochMAC())\n\n    if experience:\n        metrics.append(ExperienceMAC())\n\n    return metrics",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the MAC metric.\n        \"\"\"\n        self.hooks = []\n        self._compute_cost: Optional[int] = 0",
  "def update(self, model: Module, dummy_input: Tensor):\n        \"\"\"\n        Computes the MAC metric.\n\n        :param model: current model.\n        :param dummy_input: A tensor of the correct size to feed as input\n            to model. It includes batch size\n        :return: MAC metric.\n        \"\"\"\n\n        for mod in model.modules():\n            if MAC.is_recognized_module(mod):\n                def foo(a, b, c):\n                    return self.update_compute_cost(a, b, c)\n                handle = mod.register_forward_hook(foo)\n                self.hooks.append(handle)\n\n        self._compute_cost = 0\n        model(dummy_input)  # trigger forward hooks\n\n        for handle in self.hooks:\n            handle.remove()\n        self.hooks = []",
  "def result(self) -> Optional[int]:\n        \"\"\"\n        Return the number of MAC operations as computed in the previous call\n        to the `update` method.\n\n        :return: The number of MAC operations or None if `update` has not been\n            called yet.\n        \"\"\"\n        return self._compute_cost",
  "def reset(self):\n        pass",
  "def update_compute_cost(self, module, dummy_input, output):\n        modname = module.__class__.__name__\n        if modname == 'Linear':\n            self._compute_cost += dummy_input[0].shape[1] * output.shape[1]\n        elif modname == 'Conv2d':\n            n, cout, hout, wout = output.shape  # Batch, Channels, Height, Width\n            ksize = module.kernel_size[0] * module.kernel_size[1]\n            self._compute_cost += cout * hout * wout * ksize",
  "def is_recognized_module(mod):\n        modname = mod.__class__.__name__\n        return modname == 'Linear' or modname == 'Conv2d'",
  "def __init__(self, reset_at, emit_at, mode):\n        self._mac = MAC()\n\n        super(MACPluginMetric, self).__init__(\n            self._mac, reset_at=reset_at, emit_at=emit_at, mode=mode)",
  "def update(self, strategy):\n        self._mac.update(strategy.model,\n                         strategy.mb_x[0].unsqueeze(0))",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the MinibatchMAC metric.\n        \"\"\"\n        super(MinibatchMAC, self).__init__(\n            reset_at='iteration', emit_at='iteration', mode='train')",
  "def __str__(self):\n        return \"MAC_MB\"",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of the EpochMAC metric.\n        \"\"\"\n        super(EpochMAC, self).__init__(\n            reset_at='epoch', emit_at='epoch', mode='train')",
  "def __str__(self):\n        return \"MAC_Epoch\"",
  "def __init__(self):\n        \"\"\"\n        Creates an instance of ExperienceMAC metric\n        \"\"\"\n        super(ExperienceMAC, self).__init__(\n            reset_at='experience', emit_at='experience', mode='eval')",
  "def __str__(self):\n        return \"MAC_Exp\"",
  "def foo(a, b, c):\n                    return self.update_compute_cost(a, b, c)",
  "class ImagesSamplePlugin(PluginMetric):\n    \"\"\"\n    A metric used to sample images at random.\n    No data augmentation is shown.\n    Only images in strategy.adapted dataset are used. Images added in the\n    dataloader (like the replay plugins do) are missed.\n\n    :param n_rows: The numbers of raws to use in the grid of images.\n    :param n_cols: The numbers of columns to use in the grid of images.\n    :param group: If True, images will be grouped by (task, label)\n    :param mode: The plugin can be used at train or eval time.\n    :return: The corresponding plugins.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mode: Literal[\"train\", \"eval\"],\n        n_cols: int,\n        n_rows: int,\n        group: bool = True,\n    ):\n        super().__init__()\n        self.group = group\n        self.n_rows = n_rows\n        self.n_cols = n_cols\n        self.mode = mode\n\n        self.images: List[Tensor] = []\n        self.n_wanted_images = self.n_cols * self.n_rows\n\n    def after_train_dataset_adaptation(\n        self, strategy: \"BaseStrategy\"\n    ) -> \"MetricResult\":\n        if self.mode == \"train\":\n            return self.make_grid_sample(strategy)\n\n    def after_eval_dataset_adaptation(\n        self, strategy: \"BaseStrategy\"\n    ) -> \"MetricResult\":\n        if self.mode == \"eval\":\n            return self.make_grid_sample(strategy)\n\n    def make_grid_sample(self, strategy: \"BaseStrategy\") -> \"MetricResult\":\n        self.load_sorted_images(strategy)\n\n        return [\n            MetricValue(\n                self,\n                name=get_metric_name(\n                    self,\n                    strategy,\n                    add_experience=self.mode == \"eval\",\n                    add_task=True,\n                ),\n                value=TensorImage(\n                    make_grid(\n                        list(self.images), normalize=False, nrow=self.n_cols\n                    )\n                ),\n                x_plot=self.get_global_counter(),\n            )\n        ]\n\n    def load_sorted_images(self, strategy: \"BaseStrategy\"):\n        self.reset()\n        self.images, labels, tasks = self.load_data(strategy)\n        if self.group:\n            self.sort_images(labels, tasks)\n\n    def load_data(\n        self, strategy: \"BaseStrategy\"\n    ) -> Tuple[List[Tensor], List[int], List[int]]:\n        dataloader = self.make_dataloader(strategy)\n\n        images, labels, tasks = [], [], []\n\n        for batch_images, batch_labels, batch_tasks in dataloader:\n            n_missing_images = self.n_wanted_images - len(images)\n            labels.extend(batch_labels[:n_missing_images].tolist())\n            tasks.extend(batch_tasks[:n_missing_images].tolist())\n            images.extend(batch_images[:n_missing_images])\n            if len(images) == self.n_wanted_images:\n                return images, labels, tasks\n\n    def sort_images(self, labels: List[int], tasks: List[int]):\n        self.images = [\n            image\n            for task, label, image in sorted(\n                zip(tasks, labels, self.images), key=lambda t: (t[0], t[1]),\n            )\n        ]\n\n    def make_dataloader(self, strategy: \"BaseStrategy\") -> DataLoader:\n        return DataLoader(\n            dataset=strategy.adapted_dataset.replace_transforms(\n                transform=ToTensor(), target_transform=None,\n            ),\n            batch_size=min(strategy.eval_mb_size, self.n_wanted_images),\n            shuffle=True,\n        )\n\n    def reset(self) -> None:\n        self.images = []\n\n    def result(self) -> List[Tensor]:\n        return self.images\n\n    def __str__(self):\n        return \"images\"",
  "def images_samples_metrics(\n    *,\n    n_rows: int = 8,\n    n_cols: int = 8,\n    group: bool = True,\n    on_train: bool = True,\n    on_eval: bool = False,\n) -> List[PluginMetric]:\n    \"\"\"\n    Create the plugins to log some images samples in grids.\n    No data augmentation is shown.\n    Only images in strategy.adapted dataset are used. Images added in the\n    dataloader (like the replay plugins do) are missed.\n\n    :param n_rows: The numbers of raws to use in the grid of images.\n    :param n_cols: The numbers of columns to use in the grid of images.\n    :param group: If True, images will be grouped by (task, label)\n    :param on_train: If True, will emit some images samples during training.\n    :param on_eval: If True, will emit some images samples during evaluation.\n    :return: The corresponding plugins.\n    \"\"\"\n    plugins = []\n    if on_eval:\n        plugins.append(\n            ImagesSamplePlugin(\n                mode=\"eval\", n_rows=n_rows, n_cols=n_cols, group=group\n            )\n        )\n    if on_train:\n        plugins.append(\n            ImagesSamplePlugin(\n                mode=\"train\", n_rows=n_rows, n_cols=n_cols, group=group\n            )\n        )\n    return plugins",
  "def __init__(\n        self,\n        *,\n        mode: Literal[\"train\", \"eval\"],\n        n_cols: int,\n        n_rows: int,\n        group: bool = True,\n    ):\n        super().__init__()\n        self.group = group\n        self.n_rows = n_rows\n        self.n_cols = n_cols\n        self.mode = mode\n\n        self.images: List[Tensor] = []\n        self.n_wanted_images = self.n_cols * self.n_rows",
  "def after_train_dataset_adaptation(\n        self, strategy: \"BaseStrategy\"\n    ) -> \"MetricResult\":\n        if self.mode == \"train\":\n            return self.make_grid_sample(strategy)",
  "def after_eval_dataset_adaptation(\n        self, strategy: \"BaseStrategy\"\n    ) -> \"MetricResult\":\n        if self.mode == \"eval\":\n            return self.make_grid_sample(strategy)",
  "def make_grid_sample(self, strategy: \"BaseStrategy\") -> \"MetricResult\":\n        self.load_sorted_images(strategy)\n\n        return [\n            MetricValue(\n                self,\n                name=get_metric_name(\n                    self,\n                    strategy,\n                    add_experience=self.mode == \"eval\",\n                    add_task=True,\n                ),\n                value=TensorImage(\n                    make_grid(\n                        list(self.images), normalize=False, nrow=self.n_cols\n                    )\n                ),\n                x_plot=self.get_global_counter(),\n            )\n        ]",
  "def load_sorted_images(self, strategy: \"BaseStrategy\"):\n        self.reset()\n        self.images, labels, tasks = self.load_data(strategy)\n        if self.group:\n            self.sort_images(labels, tasks)",
  "def load_data(\n        self, strategy: \"BaseStrategy\"\n    ) -> Tuple[List[Tensor], List[int], List[int]]:\n        dataloader = self.make_dataloader(strategy)\n\n        images, labels, tasks = [], [], []\n\n        for batch_images, batch_labels, batch_tasks in dataloader:\n            n_missing_images = self.n_wanted_images - len(images)\n            labels.extend(batch_labels[:n_missing_images].tolist())\n            tasks.extend(batch_tasks[:n_missing_images].tolist())\n            images.extend(batch_images[:n_missing_images])\n            if len(images) == self.n_wanted_images:\n                return images, labels, tasks",
  "def sort_images(self, labels: List[int], tasks: List[int]):\n        self.images = [\n            image\n            for task, label, image in sorted(\n                zip(tasks, labels, self.images), key=lambda t: (t[0], t[1]),\n            )\n        ]",
  "def make_dataloader(self, strategy: \"BaseStrategy\") -> DataLoader:\n        return DataLoader(\n            dataset=strategy.adapted_dataset.replace_transforms(\n                transform=ToTensor(), target_transform=None,\n            ),\n            batch_size=min(strategy.eval_mb_size, self.n_wanted_images),\n            shuffle=True,\n        )",
  "def reset(self) -> None:\n        self.images = []",
  "def result(self) -> List[Tensor]:\n        return self.images",
  "def __str__(self):\n        return \"images\"",
  "class DiskUsage(Metric[float]):\n    \"\"\"\n    The standalone disk usage metric.\n\n    This metric can be used to monitor the size of a set of directories.\n    e.g. This can be useful to monitor the size of a replay buffer,\n    \"\"\"\n    def __init__(self,\n                 paths_to_monitor: Union[PathAlike, Sequence[PathAlike]] = None\n                 ):\n        \"\"\"\n        Creates an instance of the standalone disk usage metric.\n\n        The `result` method will return the sum of the size\n        of the directories specified as the first parameter in KiloBytes.\n\n        :param paths_to_monitor: a path or a list of paths to monitor. If None,\n            the current working directory is used. Defaults to None.\n        \"\"\"\n\n        if paths_to_monitor is None:\n            paths_to_monitor = [os.getcwd()]\n        if isinstance(paths_to_monitor, (str, Path)):\n            paths_to_monitor = [paths_to_monitor]\n\n        self._paths_to_monitor: List[str] = [str(p) for p in paths_to_monitor]\n\n        self.total_usage = 0\n\n    def update(self):\n        \"\"\"\n        Updates the disk usage statistics.\n\n        :return None.\n        \"\"\"\n\n        dirs_size = 0\n        for directory in self._paths_to_monitor:\n            dirs_size += DiskUsage.get_dir_size(directory)\n\n        self.total_usage = dirs_size\n\n    def result(self) -> Optional[float]:\n        \"\"\"\n        Retrieves the disk usage as computed during the last call to the\n        `update` method.\n\n        Calling this method will not change the internal state of the metric.\n\n        :return: The disk usage or None if `update` was not invoked yet.\n        \"\"\"\n\n        return self.total_usage\n\n    def reset(self) -> None:\n        \"\"\"\n        Resets the metric.\n\n        :return: None.\n        \"\"\"\n        self.total_usage = 0\n\n    @staticmethod\n    def get_dir_size(path: str):\n        total_size = 0\n        for dirpath, dirnames, filenames in os.walk(path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                # skip if it is symbolic link\n                if not os.path.islink(fp):\n                    # in KB\n                    s = os.path.getsize(fp) / 1024\n                    total_size += s\n\n        return total_size",
  "class DiskPluginMetric(GenericPluginMetric[float]):\n    def __init__(self, paths, reset_at, emit_at, mode):\n        self._disk = DiskUsage(paths_to_monitor=paths)\n\n        super(DiskPluginMetric, self).__init__(\n            self._disk, reset_at=reset_at, emit_at=emit_at,\n            mode=mode)\n\n    def update(self, strategy):\n        self._disk.update()",
  "class MinibatchDiskUsage(DiskPluginMetric):\n    \"\"\"\n    The minibatch Disk usage metric.\n    This plugin metric only works at training time.\n\n    At the end of each iteration, this metric logs the total\n    size (in KB) of all the monitored paths.\n\n    If a more coarse-grained logging is needed, consider using\n    :class:`EpochDiskUsage`.\n    \"\"\"\n\n    def __init__(self, paths_to_monitor):\n        \"\"\"\n        Creates an instance of the minibatch Disk usage metric.\n        \"\"\"\n        super(MinibatchDiskUsage, self).__init__(\n            paths_to_monitor,\n            reset_at='iteration', emit_at='iteration', mode='train')\n\n    def __str__(self):\n        return \"DiskUsage_MB\"",
  "class EpochDiskUsage(DiskPluginMetric):\n    \"\"\"\n    The Epoch Disk usage metric.\n    This plugin metric only works at training time.\n\n    At the end of each epoch, this metric logs the total\n    size (in KB) of all the monitored paths.\n    \"\"\"\n\n    def __init__(self, paths_to_monitor):\n        \"\"\"\n        Creates an instance of the epoch Disk usage metric.\n        \"\"\"\n        super(EpochDiskUsage, self).__init__(\n            paths_to_monitor,\n            reset_at='epoch', emit_at='epoch', mode='train')\n\n    def __str__(self):\n        return \"DiskUsage_Epoch\"",
  "class ExperienceDiskUsage(DiskPluginMetric):\n    \"\"\"\n    The average experience Disk usage metric.\n    This plugin metric works only at eval time.\n\n    At the end of each experience, this metric logs the total\n    size (in KB) of all the monitored paths.\n    \"\"\"\n\n    def __init__(self, paths_to_monitor):\n        \"\"\"\n        Creates an instance of the experience Disk usage metric.\n        \"\"\"\n        super(ExperienceDiskUsage, self).__init__(\n            paths_to_monitor,\n            reset_at='experience', emit_at='experience', mode='eval')\n\n    def __str__(self):\n        return \"DiskUsage_Exp\"",
  "class StreamDiskUsage(DiskPluginMetric):\n    \"\"\"\n    The average stream Disk usage metric.\n    This plugin metric works only at eval time.\n\n    At the end of the eval stream, this metric logs the total\n    size (in KB) of all the monitored paths.\n    \"\"\"\n\n    def __init__(self, paths_to_monitor):\n        \"\"\"\n        Creates an instance of the stream Disk usage metric.\n        \"\"\"\n        super(StreamDiskUsage, self).__init__(\n            paths_to_monitor,\n            reset_at='stream', emit_at='stream', mode='eval')\n\n    def __str__(self):\n        return \"DiskUsage_Stream\"",
  "def disk_usage_metrics(*, paths_to_monitor=None, minibatch=False, epoch=False,\n                       experience=False, stream=False) \\\n        -> List[PluginMetric]:\n    \"\"\"\n    Helper method that can be used to obtain the desired set of\n    standalone metrics.\n\n    :param minibatch: If True, will return a metric able to log the minibatch\n        Disk usage\n    :param epoch: If True, will return a metric able to log the epoch\n        Disk usage\n    :param experience: If True, will return a metric able to log the experience\n        Disk usage.\n    :param stream: If True, will return a metric able to log the evaluation\n        stream Disk usage.\n\n    :return: A list of plugin metrics.\n    \"\"\"\n\n    metrics = []\n    if minibatch:\n        metrics.append(MinibatchDiskUsage(paths_to_monitor=paths_to_monitor))\n\n    if epoch:\n        metrics.append(EpochDiskUsage(paths_to_monitor=paths_to_monitor))\n\n    if experience:\n        metrics.append(ExperienceDiskUsage(paths_to_monitor=paths_to_monitor))\n\n    if stream:\n        metrics.append(StreamDiskUsage(paths_to_monitor=paths_to_monitor))\n\n    return metrics",
  "def __init__(self,\n                 paths_to_monitor: Union[PathAlike, Sequence[PathAlike]] = None\n                 ):\n        \"\"\"\n        Creates an instance of the standalone disk usage metric.\n\n        The `result` method will return the sum of the size\n        of the directories specified as the first parameter in KiloBytes.\n\n        :param paths_to_monitor: a path or a list of paths to monitor. If None,\n            the current working directory is used. Defaults to None.\n        \"\"\"\n\n        if paths_to_monitor is None:\n            paths_to_monitor = [os.getcwd()]\n        if isinstance(paths_to_monitor, (str, Path)):\n            paths_to_monitor = [paths_to_monitor]\n\n        self._paths_to_monitor: List[str] = [str(p) for p in paths_to_monitor]\n\n        self.total_usage = 0",
  "def update(self):\n        \"\"\"\n        Updates the disk usage statistics.\n\n        :return None.\n        \"\"\"\n\n        dirs_size = 0\n        for directory in self._paths_to_monitor:\n            dirs_size += DiskUsage.get_dir_size(directory)\n\n        self.total_usage = dirs_size",
  "def result(self) -> Optional[float]:\n        \"\"\"\n        Retrieves the disk usage as computed during the last call to the\n        `update` method.\n\n        Calling this method will not change the internal state of the metric.\n\n        :return: The disk usage or None if `update` was not invoked yet.\n        \"\"\"\n\n        return self.total_usage",
  "def reset(self) -> None:\n        \"\"\"\n        Resets the metric.\n\n        :return: None.\n        \"\"\"\n        self.total_usage = 0",
  "def get_dir_size(path: str):\n        total_size = 0\n        for dirpath, dirnames, filenames in os.walk(path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                # skip if it is symbolic link\n                if not os.path.islink(fp):\n                    # in KB\n                    s = os.path.getsize(fp) / 1024\n                    total_size += s\n\n        return total_size",
  "def __init__(self, paths, reset_at, emit_at, mode):\n        self._disk = DiskUsage(paths_to_monitor=paths)\n\n        super(DiskPluginMetric, self).__init__(\n            self._disk, reset_at=reset_at, emit_at=emit_at,\n            mode=mode)",
  "def update(self, strategy):\n        self._disk.update()",
  "def __init__(self, paths_to_monitor):\n        \"\"\"\n        Creates an instance of the minibatch Disk usage metric.\n        \"\"\"\n        super(MinibatchDiskUsage, self).__init__(\n            paths_to_monitor,\n            reset_at='iteration', emit_at='iteration', mode='train')",
  "def __str__(self):\n        return \"DiskUsage_MB\"",
  "def __init__(self, paths_to_monitor):\n        \"\"\"\n        Creates an instance of the epoch Disk usage metric.\n        \"\"\"\n        super(EpochDiskUsage, self).__init__(\n            paths_to_monitor,\n            reset_at='epoch', emit_at='epoch', mode='train')",
  "def __str__(self):\n        return \"DiskUsage_Epoch\"",
  "def __init__(self, paths_to_monitor):\n        \"\"\"\n        Creates an instance of the experience Disk usage metric.\n        \"\"\"\n        super(ExperienceDiskUsage, self).__init__(\n            paths_to_monitor,\n            reset_at='experience', emit_at='experience', mode='eval')",
  "def __str__(self):\n        return \"DiskUsage_Exp\"",
  "def __init__(self, paths_to_monitor):\n        \"\"\"\n        Creates an instance of the stream Disk usage metric.\n        \"\"\"\n        super(StreamDiskUsage, self).__init__(\n            paths_to_monitor,\n            reset_at='stream', emit_at='stream', mode='eval')",
  "def __str__(self):\n        return \"DiskUsage_Stream\"",
  "class ConfusionMatrix(Metric[Tensor]):\n    \"\"\"\n    The standalone confusion matrix metric.\n\n    Instances of this metric keep track of the confusion matrix by receiving a\n    pair of \"ground truth\" and \"prediction\" Tensors describing the labels of a\n    minibatch. Those two tensors can both contain plain labels or\n    one-hot/logit vectors.\n\n    The result is the unnormalized running confusion matrix.\n\n    Beware that by default the confusion matrix size will depend on the value of\n    the maximum label as detected by looking at both the ground truth and\n    predictions Tensors. When passing one-hot/logit vectors, this\n    metric will try to infer the number of classes from the vector sizes.\n    Otherwise, the maximum label value encountered in the truth/prediction\n    Tensors will be used.\n\n    If the user sets the `num_classes`, then the confusion matrix will always be\n    of size `num_classes, num_classes`. Whenever a prediction or label tensor is\n    provided as logits, only the first `num_classes` units will be considered in\n    the confusion matrix computation. If they are provided as numerical labels,\n    each of them has to be smaller than `num_classes`.\n\n    The reset method will bring the metric to its initial state. By default\n    this metric in its initial state will return an empty Tensor.\n    \"\"\"\n\n    def __init__(self, num_classes: int = None,\n                 normalize: Literal['true', 'pred', 'all'] = None):\n        \"\"\"\n        Creates an instance of the standalone confusion matrix metric.\n\n        By default this metric in its initial state will return an empty Tensor.\n        The metric can be updated by using the `update` method while the running\n        confusion matrix can be retrieved using the `result` method.\n\n        :param num_classes: The number of classes. Defaults to None,\n            which means that the number of classes will be inferred from\n            ground truth and prediction Tensors (see class description for more\n            details). If not None, the confusion matrix will always be of size\n            `num_classes, num_classes` and only the first `num_classes` values\n            of output logits or target logits will be considered in the update.\n            If the output or targets are provided as numerical labels,\n            there can be no label greater than `num_classes`.\n        :param normalize: how to normalize confusion matrix.\n            None to not normalize\n        \"\"\"\n        self._cm_tensor: Optional[Tensor] = None\n        \"\"\"\n        The Tensor where the running confusion matrix is stored.\n        \"\"\"\n        self._num_classes: Optional[int] = num_classes\n\n        self.normalize = normalize\n\n    @torch.no_grad()\n    def update(self, true_y: Tensor, predicted_y: Tensor) -> None:\n        \"\"\"\n        Update the running confusion matrix given the true and predicted labels.\n\n        :param true_y: The ground truth. Both labels and one-hot vectors\n            are supported.\n        :param predicted_y: The ground truth. Both labels and logit vectors\n            are supported.\n        :return: None.\n        \"\"\"\n        if len(true_y) != len(predicted_y):\n            raise ValueError('Size mismatch for true_y and predicted_y tensors')\n\n        if len(true_y.shape) > 2:\n            raise ValueError('Confusion matrix supports labels with at'\n                             ' most 2 dimensions')\n        if len(predicted_y.shape) > 2:\n            raise ValueError('Confusion matrix supports predictions with at '\n                             'most 2 dimensions')\n\n        max_label = -1 if self._num_classes is None else self._num_classes - 1\n\n        # SELECT VALID PORTION OF TARGET AND PREDICTIONS\n        true_y = torch.as_tensor(true_y)\n        if len(true_y.shape) == 2 and self._num_classes is not None:\n            true_y = true_y[:, :max_label]\n        predicted_y = torch.as_tensor(predicted_y)\n        if len(predicted_y.shape) == 2 and self._num_classes is not None:\n            predicted_y = predicted_y[:, :max_label]\n\n        # COMPUTE MAX LABEL AND CONVERT TARGET AND PREDICTIONS IF NEEDED\n        if len(predicted_y.shape) > 1:\n            # Logits -> transform to labels\n            if self._num_classes is None:\n                max_label = max(max_label, predicted_y.shape[1]-1)\n            predicted_y = torch.max(predicted_y, 1)[1]\n        else:\n            # Labels -> check non-negative\n            min_label = torch.min(predicted_y).item()\n            if min_label < 0:\n                raise ValueError('Label values must be non-negative values')\n            if self._num_classes is None:\n                max_label = max(max_label, torch.max(predicted_y).item())\n            elif torch.max(predicted_y).item() >= self._num_classes:\n                raise ValueError(\"Encountered predicted label larger than\"\n                                 \"num_classes\")\n\n        if len(true_y.shape) > 1:\n            # Logits -> transform to labels\n            if self._num_classes is None:\n                max_label = max(max_label, true_y.shape[1]-1)\n            true_y = torch.max(true_y, 1)[1]\n        else:\n            # Labels -> check non-negative\n            min_label = torch.min(true_y).item()\n            if min_label < 0:\n                raise ValueError('Label values must be non-negative values')\n\n            if self._num_classes is None:\n                max_label = max(max_label, torch.max(true_y).item())\n            elif torch.max(true_y).item() >= self._num_classes:\n                raise ValueError(\"Encountered target label larger than\"\n                                 \"num_classes\")\n\n        if max_label < 0:\n            raise ValueError('The Confusion Matrix metric can only handle '\n                             'positive label values')\n\n        if self._cm_tensor is None:\n            # Create the confusion matrix\n            self._cm_tensor = torch.zeros((max_label+1, max_label+1),\n                                          dtype=torch.long)\n        elif max_label >= self._cm_tensor.shape[0]:\n            # Enlarge the confusion matrix\n            size_diff = 1 + max_label - self._cm_tensor.shape[0]\n            self._cm_tensor = pad(self._cm_tensor,\n                                  (0, size_diff, 0, size_diff))\n\n        for pattern_idx in range(len(true_y)):\n            self._cm_tensor[true_y[pattern_idx]][predicted_y[pattern_idx]] += 1\n\n    def result(self) -> Tensor:\n        \"\"\"\n        Retrieves the unnormalized confusion matrix.\n\n        Calling this method will not change the internal state of the metric.\n\n        :return: The running confusion matrix, as a Tensor.\n        \"\"\"\n        if self._cm_tensor is None:\n            matrix_shape = (0, 0)\n            if self._num_classes is not None:\n                matrix_shape = (self._num_classes, self._num_classes)\n            return torch.zeros(matrix_shape, dtype=torch.long)\n        if self.normalize is not None:\n            return ConfusionMatrix._normalize_cm(self._cm_tensor,\n                                                 self.normalize)\n        return self._cm_tensor\n\n    def reset(self) -> None:\n        \"\"\"\n        Resets the metric.\n\n        Calling this method will *not* reset the default number of classes\n        optionally defined in the constructor optional parameter.\n\n        :return: None.\n        \"\"\"\n        self._cm_tensor = None\n\n    @staticmethod\n    def _normalize_cm(cm: Tensor,\n                      normalization: Literal['true', 'pred', 'all']):\n        if normalization not in ('true', 'pred', 'all'):\n            raise ValueError('Invalid normalization parameter. Can be \\'true\\','\n                             ' \\'pred\\' or \\'all\\'')\n\n        if normalization == 'true':\n            cm = cm / cm.sum(dim=1, keepdim=True, dtype=torch.float64)\n        elif normalization == 'pred':\n            cm = cm / cm.sum(dim=0, keepdim=True, dtype=torch.float64)\n        elif normalization == 'all':\n            cm = cm / cm.sum(dtype=torch.float64)\n        cm = ConfusionMatrix.nan_to_num(cm)\n        return cm\n\n    @staticmethod\n    def nan_to_num(matrix: Tensor) -> Tensor:\n        # if version.parse(torch.__version__) >= version.parse(\"1.8.0\"):\n        #    # noinspection PyUnresolvedReferences\n        #    return torch.nan_to_num(matrix)\n\n        numpy_ndarray = matrix.numpy()\n        numpy_ndarray = np.nan_to_num(numpy_ndarray)\n        return torch.tensor(numpy_ndarray, dtype=matrix.dtype)",
  "class StreamConfusionMatrix(PluginMetric[Tensor]):\n    \"\"\"\n    The Stream Confusion Matrix metric.\n    This plugin metric only works on the eval phase.\n\n    Confusion Matrix computation can be slow if you compute it for a large\n    number of classes. We recommend to set `save_image=False` if the runtime\n    is too large.\n\n    At the end of the eval phase, this metric logs the confusion matrix\n    relative to all the patterns seen during eval.\n\n    The metric can log either a Tensor or a PIL Image representing the\n    confusion matrix.\n    \"\"\"\n\n    def __init__(self,\n                 num_classes: Union[int, Mapping[int, int]] = None,\n                 normalize: Literal['true', 'pred', 'all'] = None,\n                 save_image: bool = True,\n                 image_creator: Callable[[Tensor, Sequence], Image] =\n                 default_cm_image_creator,\n                 absolute_class_order: bool = False):\n        \"\"\"\n        Creates an instance of the Stream Confusion Matrix metric.\n\n        We recommend to set `save_image=False` if the runtime is too large.\n        In fact, a large number of classes may increase the computation time\n        of this metric.\n\n        :param num_classes: The number of classes. Defaults to None,\n            which means that the number of classes will be inferred from\n            ground truth and prediction Tensors (see class description for more\n            details). If not None, the confusion matrix will always be of size\n            `num_classes, num_classes` and only the first `num_classes` values\n            of output logits or target logits will be considered in the update.\n            If the output or targets are provided as numerical labels,\n            there can be no label greater than `num_classes`.\n        :param normalize: Normalizes confusion matrix over the true (rows),\n            predicted (columns) conditions or all the population. If None,\n            confusion matrix will not be normalized. Valid values are: 'true',\n            'pred' and 'all' or None.\n        :param save_image: If True, a graphical representation of the confusion\n            matrix will be logged, too. If False, only the Tensor representation\n            will be logged. Defaults to True.\n        :param image_creator: A callable that, given the tensor representation\n            of the confusion matrix and the corresponding labels, returns a\n            graphical representation of the matrix as a PIL Image. Defaults to\n            `default_cm_image_creator`.\n        :param absolute_class_order: If true, the labels in the created image\n            will be sorted by id, otherwise they will be sorted by order of\n            encounter at training time. This parameter is ignored if\n            `save_image` is False, or the scenario is not a NCScenario.\n        \"\"\"\n        super().__init__()\n\n        self._save_image: bool = save_image\n        self.num_classes = num_classes\n        self.normalize = normalize\n        self.absolute_class_order = absolute_class_order\n        self._matrix: ConfusionMatrix = ConfusionMatrix(num_classes=num_classes,\n                                                        normalize=normalize)\n        self._image_creator = image_creator\n\n    def reset(self) -> None:\n        self._matrix = ConfusionMatrix(num_classes=self.num_classes,\n                                       normalize=self.normalize)\n\n    def result(self) -> Tensor:\n        exp_cm = self._matrix.result()\n        return exp_cm\n\n    def update(self, true_y: Tensor, predicted_y: Tensor) -> None:\n        self._matrix.update(true_y, predicted_y)\n\n    def before_eval(self, strategy) -> None:\n        self.reset()\n\n    def after_eval_iteration(self, strategy: 'BaseStrategy') -> None:\n        super().after_eval_iteration(strategy)\n        self.update(strategy.mb_y,\n                    strategy.mb_output)\n\n    def after_eval(self, strategy: 'BaseStrategy') -> MetricResult:\n        return self._package_result(strategy)\n\n    def _package_result(self, strategy: 'BaseStrategy') -> MetricResult:\n        exp_cm = self.result()\n        phase_name, _ = phase_and_task(strategy)\n        stream = stream_type(strategy.experience)\n        metric_name = '{}/{}_phase/{}_stream' \\\n            .format(str(self),\n                    phase_name,\n                    stream)\n        plot_x_position = self.get_global_counter()\n\n        if self._save_image:\n            class_order = self._get_display_class_order(exp_cm, strategy)\n\n            cm_image = self._image_creator(\n                exp_cm[class_order][:, class_order],\n                class_order\n            )\n            metric_representation = MetricValue(\n                self, metric_name, AlternativeValues(cm_image, exp_cm),\n                plot_x_position)\n        else:\n            metric_representation = MetricValue(\n                self, metric_name, exp_cm, plot_x_position)\n\n        return [metric_representation]\n\n    def _get_display_class_order(self, exp_cm: Tensor, strategy: 'BaseStrategy'\n                                 ) -> ndarray:\n        benchmark = strategy.experience.benchmark\n\n        if self.absolute_class_order or not isinstance(benchmark, NCScenario):\n            return arange(len(exp_cm))\n\n        return benchmark.classes_order\n\n    def __str__(self):\n        return \"ConfusionMatrix_Stream\"",
  "class WandBStreamConfusionMatrix(PluginMetric):\n    \"\"\"\n    Confusion Matrix metric compatible with Weights and Biases logger.\n    Differently from the `StreamConfusionMatrix`, this metric will use W&B\n    built-in functionalities to log the Confusion Matrix.\n\n    This metric may not produce meaningful outputs with other loggers.\n\n    https://docs.wandb.ai/guides/track/log#custom-charts\n    \"\"\"\n\n    def __init__(self, class_names=None):\n        \"\"\"\n        :param class_names: list of names for the classes.\n            E.g. [\"cat\", \"dog\"] if class 0 == \"cat\" and class 1 == \"dog\"\n            If None, no class names will be used. Default None.\n        \"\"\"\n\n        super().__init__()\n\n        self.outputs = []  # softmax-ed or logits outputs\n        self.targets = []  # target classes\n        self.class_names = class_names\n\n    def reset(self) -> None:\n        self.outputs = []\n        self.targets = []\n\n    def before_eval(self, strategy) -> None:\n        self.reset()\n\n    def result(self):\n        outputs = torch.cat(self.outputs, dim=0)\n        targets = torch.cat(self.targets, dim=0)\n        return outputs, targets\n\n    def update(self, output, target):\n        self.outputs.append(output)\n        self.targets.append(target)\n\n    def after_eval_iteration(self, strategy: 'BaseStrategy'):\n        super(WandBStreamConfusionMatrix, self).after_eval_iteration(strategy)\n        self.update(strategy.mb_output, strategy.mb_y)\n\n    def after_eval(self, strategy: 'BaseStrategy') -> MetricResult:\n        return self._package_result(strategy)\n\n    def _package_result(self, strategy: 'BaseStrategy') -> MetricResult:\n        outputs, targets = self.result()\n        phase_name, _ = phase_and_task(strategy)\n        stream = stream_type(strategy.experience)\n        metric_name = '{}/{}_phase/{}_stream' \\\n            .format(str(self),\n                    phase_name,\n                    stream)\n        plot_x_position = self.get_global_counter()\n\n        # compute predicted classes\n        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n        result = wandb.plot.confusion_matrix(preds=preds,\n                                             y_true=targets.cpu().numpy(),\n                                             class_names=self.class_names)\n\n        metric_representation = MetricValue(\n            self, metric_name, AlternativeValues(result),\n            plot_x_position)\n\n        return [metric_representation]\n\n    def __str__(self):\n        return \"W&BConfusionMatrix_Stream\"",
  "def confusion_matrix_metrics(\n        num_classes=None,\n        normalize=None,\n        save_image=True,\n        image_creator=default_cm_image_creator,\n        class_names=None,\n        stream=False,\n        wandb=False,\n        absolute_class_order: bool = False,\n) -> List[PluginMetric]:\n    \"\"\"\n    Helper method that can be used to obtain the desired set of\n    plugin metrics.\n\n    :param num_classes: The number of classes. Defaults to None,\n        which means that the number of classes will be inferred from\n        ground truth and prediction Tensors (see class description for more\n        details). If not None, the confusion matrix will always be of size\n        `num_classes, num_classes` and only the first `num_classes` values\n        of output logits or target logits will be considered in the update.\n        If the output or targets are provided as numerical labels,\n        there can be no label greater than `num_classes`.\n    :param normalize: Normalizes confusion matrix over the true (rows),\n        predicted (columns) conditions or all the population. If None,\n        confusion matrix will not be normalized. Valid values are: 'true',\n        'pred' and 'all' or None.\n    :param save_image: If True, a graphical representation of the confusion\n        matrix will be logged, too. If False, only the Tensor representation\n        will be logged. Defaults to True.\n    :param image_creator: A callable that, given the tensor representation\n        of the confusion matrix, returns a graphical representation of the\n        matrix as a PIL Image. Defaults to `default_cm_image_creator`.\n    :param class_names: W&B only. List of names for the classes.\n        E.g. [\"cat\", \"dog\"] if class 0 == \"cat\" and class 1 == \"dog\"\n        If None, no class names will be used. Default None.\n    :param stream: If True, will return a metric able to log\n        the confusion matrix averaged over the entire evaluation stream\n        of experiences.\n    :param wandb: if True, will return a Weights and Biases confusion matrix\n        together with all the other confusion matrixes requested.\n    :param absolute_class_order: Not W&B. If true, the labels in the created\n        image will be sorted by id, otherwise they will be sorted by order of\n        encounter at training time. This parameter is ignored if `save_image` is\n         False, or the scenario is not a NCScenario.\n\n    :return: A list of plugin metrics.\n    \"\"\"\n\n    metrics = []\n\n    if stream:\n        metrics.append(\n            StreamConfusionMatrix(num_classes=num_classes,\n                                  normalize=normalize,\n                                  save_image=save_image,\n                                  image_creator=image_creator,\n                                  absolute_class_order=absolute_class_order,\n                                  ))\n        if wandb:\n            metrics.append(WandBStreamConfusionMatrix(class_names=class_names))\n\n    return metrics",
  "def __init__(self, num_classes: int = None,\n                 normalize: Literal['true', 'pred', 'all'] = None):\n        \"\"\"\n        Creates an instance of the standalone confusion matrix metric.\n\n        By default this metric in its initial state will return an empty Tensor.\n        The metric can be updated by using the `update` method while the running\n        confusion matrix can be retrieved using the `result` method.\n\n        :param num_classes: The number of classes. Defaults to None,\n            which means that the number of classes will be inferred from\n            ground truth and prediction Tensors (see class description for more\n            details). If not None, the confusion matrix will always be of size\n            `num_classes, num_classes` and only the first `num_classes` values\n            of output logits or target logits will be considered in the update.\n            If the output or targets are provided as numerical labels,\n            there can be no label greater than `num_classes`.\n        :param normalize: how to normalize confusion matrix.\n            None to not normalize\n        \"\"\"\n        self._cm_tensor: Optional[Tensor] = None\n        \"\"\"\n        The Tensor where the running confusion matrix is stored.\n        \"\"\"\n        self._num_classes: Optional[int] = num_classes\n\n        self.normalize = normalize",
  "def update(self, true_y: Tensor, predicted_y: Tensor) -> None:\n        \"\"\"\n        Update the running confusion matrix given the true and predicted labels.\n\n        :param true_y: The ground truth. Both labels and one-hot vectors\n            are supported.\n        :param predicted_y: The ground truth. Both labels and logit vectors\n            are supported.\n        :return: None.\n        \"\"\"\n        if len(true_y) != len(predicted_y):\n            raise ValueError('Size mismatch for true_y and predicted_y tensors')\n\n        if len(true_y.shape) > 2:\n            raise ValueError('Confusion matrix supports labels with at'\n                             ' most 2 dimensions')\n        if len(predicted_y.shape) > 2:\n            raise ValueError('Confusion matrix supports predictions with at '\n                             'most 2 dimensions')\n\n        max_label = -1 if self._num_classes is None else self._num_classes - 1\n\n        # SELECT VALID PORTION OF TARGET AND PREDICTIONS\n        true_y = torch.as_tensor(true_y)\n        if len(true_y.shape) == 2 and self._num_classes is not None:\n            true_y = true_y[:, :max_label]\n        predicted_y = torch.as_tensor(predicted_y)\n        if len(predicted_y.shape) == 2 and self._num_classes is not None:\n            predicted_y = predicted_y[:, :max_label]\n\n        # COMPUTE MAX LABEL AND CONVERT TARGET AND PREDICTIONS IF NEEDED\n        if len(predicted_y.shape) > 1:\n            # Logits -> transform to labels\n            if self._num_classes is None:\n                max_label = max(max_label, predicted_y.shape[1]-1)\n            predicted_y = torch.max(predicted_y, 1)[1]\n        else:\n            # Labels -> check non-negative\n            min_label = torch.min(predicted_y).item()\n            if min_label < 0:\n                raise ValueError('Label values must be non-negative values')\n            if self._num_classes is None:\n                max_label = max(max_label, torch.max(predicted_y).item())\n            elif torch.max(predicted_y).item() >= self._num_classes:\n                raise ValueError(\"Encountered predicted label larger than\"\n                                 \"num_classes\")\n\n        if len(true_y.shape) > 1:\n            # Logits -> transform to labels\n            if self._num_classes is None:\n                max_label = max(max_label, true_y.shape[1]-1)\n            true_y = torch.max(true_y, 1)[1]\n        else:\n            # Labels -> check non-negative\n            min_label = torch.min(true_y).item()\n            if min_label < 0:\n                raise ValueError('Label values must be non-negative values')\n\n            if self._num_classes is None:\n                max_label = max(max_label, torch.max(true_y).item())\n            elif torch.max(true_y).item() >= self._num_classes:\n                raise ValueError(\"Encountered target label larger than\"\n                                 \"num_classes\")\n\n        if max_label < 0:\n            raise ValueError('The Confusion Matrix metric can only handle '\n                             'positive label values')\n\n        if self._cm_tensor is None:\n            # Create the confusion matrix\n            self._cm_tensor = torch.zeros((max_label+1, max_label+1),\n                                          dtype=torch.long)\n        elif max_label >= self._cm_tensor.shape[0]:\n            # Enlarge the confusion matrix\n            size_diff = 1 + max_label - self._cm_tensor.shape[0]\n            self._cm_tensor = pad(self._cm_tensor,\n                                  (0, size_diff, 0, size_diff))\n\n        for pattern_idx in range(len(true_y)):\n            self._cm_tensor[true_y[pattern_idx]][predicted_y[pattern_idx]] += 1",
  "def result(self) -> Tensor:\n        \"\"\"\n        Retrieves the unnormalized confusion matrix.\n\n        Calling this method will not change the internal state of the metric.\n\n        :return: The running confusion matrix, as a Tensor.\n        \"\"\"\n        if self._cm_tensor is None:\n            matrix_shape = (0, 0)\n            if self._num_classes is not None:\n                matrix_shape = (self._num_classes, self._num_classes)\n            return torch.zeros(matrix_shape, dtype=torch.long)\n        if self.normalize is not None:\n            return ConfusionMatrix._normalize_cm(self._cm_tensor,\n                                                 self.normalize)\n        return self._cm_tensor",
  "def reset(self) -> None:\n        \"\"\"\n        Resets the metric.\n\n        Calling this method will *not* reset the default number of classes\n        optionally defined in the constructor optional parameter.\n\n        :return: None.\n        \"\"\"\n        self._cm_tensor = None",
  "def _normalize_cm(cm: Tensor,\n                      normalization: Literal['true', 'pred', 'all']):\n        if normalization not in ('true', 'pred', 'all'):\n            raise ValueError('Invalid normalization parameter. Can be \\'true\\','\n                             ' \\'pred\\' or \\'all\\'')\n\n        if normalization == 'true':\n            cm = cm / cm.sum(dim=1, keepdim=True, dtype=torch.float64)\n        elif normalization == 'pred':\n            cm = cm / cm.sum(dim=0, keepdim=True, dtype=torch.float64)\n        elif normalization == 'all':\n            cm = cm / cm.sum(dtype=torch.float64)\n        cm = ConfusionMatrix.nan_to_num(cm)\n        return cm",
  "def nan_to_num(matrix: Tensor) -> Tensor:\n        # if version.parse(torch.__version__) >= version.parse(\"1.8.0\"):\n        #    # noinspection PyUnresolvedReferences\n        #    return torch.nan_to_num(matrix)\n\n        numpy_ndarray = matrix.numpy()\n        numpy_ndarray = np.nan_to_num(numpy_ndarray)\n        return torch.tensor(numpy_ndarray, dtype=matrix.dtype)",
  "def __init__(self,\n                 num_classes: Union[int, Mapping[int, int]] = None,\n                 normalize: Literal['true', 'pred', 'all'] = None,\n                 save_image: bool = True,\n                 image_creator: Callable[[Tensor, Sequence], Image] =\n                 default_cm_image_creator,\n                 absolute_class_order: bool = False):\n        \"\"\"\n        Creates an instance of the Stream Confusion Matrix metric.\n\n        We recommend to set `save_image=False` if the runtime is too large.\n        In fact, a large number of classes may increase the computation time\n        of this metric.\n\n        :param num_classes: The number of classes. Defaults to None,\n            which means that the number of classes will be inferred from\n            ground truth and prediction Tensors (see class description for more\n            details). If not None, the confusion matrix will always be of size\n            `num_classes, num_classes` and only the first `num_classes` values\n            of output logits or target logits will be considered in the update.\n            If the output or targets are provided as numerical labels,\n            there can be no label greater than `num_classes`.\n        :param normalize: Normalizes confusion matrix over the true (rows),\n            predicted (columns) conditions or all the population. If None,\n            confusion matrix will not be normalized. Valid values are: 'true',\n            'pred' and 'all' or None.\n        :param save_image: If True, a graphical representation of the confusion\n            matrix will be logged, too. If False, only the Tensor representation\n            will be logged. Defaults to True.\n        :param image_creator: A callable that, given the tensor representation\n            of the confusion matrix and the corresponding labels, returns a\n            graphical representation of the matrix as a PIL Image. Defaults to\n            `default_cm_image_creator`.\n        :param absolute_class_order: If true, the labels in the created image\n            will be sorted by id, otherwise they will be sorted by order of\n            encounter at training time. This parameter is ignored if\n            `save_image` is False, or the scenario is not a NCScenario.\n        \"\"\"\n        super().__init__()\n\n        self._save_image: bool = save_image\n        self.num_classes = num_classes\n        self.normalize = normalize\n        self.absolute_class_order = absolute_class_order\n        self._matrix: ConfusionMatrix = ConfusionMatrix(num_classes=num_classes,\n                                                        normalize=normalize)\n        self._image_creator = image_creator",
  "def reset(self) -> None:\n        self._matrix = ConfusionMatrix(num_classes=self.num_classes,\n                                       normalize=self.normalize)",
  "def result(self) -> Tensor:\n        exp_cm = self._matrix.result()\n        return exp_cm",
  "def update(self, true_y: Tensor, predicted_y: Tensor) -> None:\n        self._matrix.update(true_y, predicted_y)",
  "def before_eval(self, strategy) -> None:\n        self.reset()",
  "def after_eval_iteration(self, strategy: 'BaseStrategy') -> None:\n        super().after_eval_iteration(strategy)\n        self.update(strategy.mb_y,\n                    strategy.mb_output)",
  "def after_eval(self, strategy: 'BaseStrategy') -> MetricResult:\n        return self._package_result(strategy)",
  "def _package_result(self, strategy: 'BaseStrategy') -> MetricResult:\n        exp_cm = self.result()\n        phase_name, _ = phase_and_task(strategy)\n        stream = stream_type(strategy.experience)\n        metric_name = '{}/{}_phase/{}_stream' \\\n            .format(str(self),\n                    phase_name,\n                    stream)\n        plot_x_position = self.get_global_counter()\n\n        if self._save_image:\n            class_order = self._get_display_class_order(exp_cm, strategy)\n\n            cm_image = self._image_creator(\n                exp_cm[class_order][:, class_order],\n                class_order\n            )\n            metric_representation = MetricValue(\n                self, metric_name, AlternativeValues(cm_image, exp_cm),\n                plot_x_position)\n        else:\n            metric_representation = MetricValue(\n                self, metric_name, exp_cm, plot_x_position)\n\n        return [metric_representation]",
  "def _get_display_class_order(self, exp_cm: Tensor, strategy: 'BaseStrategy'\n                                 ) -> ndarray:\n        benchmark = strategy.experience.benchmark\n\n        if self.absolute_class_order or not isinstance(benchmark, NCScenario):\n            return arange(len(exp_cm))\n\n        return benchmark.classes_order",
  "def __str__(self):\n        return \"ConfusionMatrix_Stream\"",
  "def __init__(self, class_names=None):\n        \"\"\"\n        :param class_names: list of names for the classes.\n            E.g. [\"cat\", \"dog\"] if class 0 == \"cat\" and class 1 == \"dog\"\n            If None, no class names will be used. Default None.\n        \"\"\"\n\n        super().__init__()\n\n        self.outputs = []  # softmax-ed or logits outputs\n        self.targets = []  # target classes\n        self.class_names = class_names",
  "def reset(self) -> None:\n        self.outputs = []\n        self.targets = []",
  "def before_eval(self, strategy) -> None:\n        self.reset()",
  "def result(self):\n        outputs = torch.cat(self.outputs, dim=0)\n        targets = torch.cat(self.targets, dim=0)\n        return outputs, targets",
  "def update(self, output, target):\n        self.outputs.append(output)\n        self.targets.append(target)",
  "def after_eval_iteration(self, strategy: 'BaseStrategy'):\n        super(WandBStreamConfusionMatrix, self).after_eval_iteration(strategy)\n        self.update(strategy.mb_output, strategy.mb_y)",
  "def after_eval(self, strategy: 'BaseStrategy') -> MetricResult:\n        return self._package_result(strategy)",
  "def _package_result(self, strategy: 'BaseStrategy') -> MetricResult:\n        outputs, targets = self.result()\n        phase_name, _ = phase_and_task(strategy)\n        stream = stream_type(strategy.experience)\n        metric_name = '{}/{}_phase/{}_stream' \\\n            .format(str(self),\n                    phase_name,\n                    stream)\n        plot_x_position = self.get_global_counter()\n\n        # compute predicted classes\n        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n        result = wandb.plot.confusion_matrix(preds=preds,\n                                             y_true=targets.cpu().numpy(),\n                                             class_names=self.class_names)\n\n        metric_representation = MetricValue(\n            self, metric_name, AlternativeValues(result),\n            plot_x_position)\n\n        return [metric_representation]",
  "def __str__(self):\n        return \"W&BConfusionMatrix_Stream\"",
  "class MeanScores(Metric):\n    \"\"\"\n    Average the scores of the true class by label\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.label2mean: Dict[int, Mean] = defaultdict(Mean)\n        self.reset()\n\n    def reset(self) -> None:\n        self.label2mean = defaultdict(Mean)\n\n    @torch.no_grad()\n    def update(self, predicted_y: Tensor, true_y: Tensor):\n        assert (\n            len(predicted_y.size()) == 2\n        ), \"Predictions need to be logits or scores, not labels\"\n\n        if len(true_y.size()) == 2:\n            true_y = true_y.argmax(axis=1)\n\n        scores = predicted_y[arange(len(true_y)), true_y]\n\n        for score, label in zip(scores.tolist(), true_y.tolist()):\n            self.label2mean[label].update(score)\n\n    def result(self) -> Dict[int, float]:\n        return {label: m.result() for label, m in self.label2mean.items()}",
  "class MeanNewOldScores(MeanScores):\n    \"\"\"\n    Average the scores of the true class by old and new classes\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.new_classes: Set[int] = set()\n\n    def reset(self) -> None:\n        super().reset()\n        self.new_classes = set()\n\n    def update_new_classes(self, new_classes: Set[int]):\n        self.new_classes.update(new_classes)\n\n    @property\n    def old_classes(self) -> Set[int]:\n        return set(self.label2mean) - self.new_classes\n\n    def result(self) -> Dict[LabelCat, float]:\n        # print(self.new_classes, self.label2mean)\n        rv = {\n            \"new\": sum(\n                (self.label2mean[label] for label in self.new_classes),\n                start=Mean(),\n            ).result()\n        }\n        if not self.old_classes:\n            return rv\n\n        rv[\"old\"] = sum(\n            (self.label2mean[label] for label in self.old_classes),\n            start=Mean(),\n        ).result()\n\n        return rv",
  "def default_mean_scores_image_creator(\n    label2step2mean_scores: Dict[LabelCat, Dict[int, float]]\n) -> Figure:\n    \"\"\"\n    Default function to create an image of the evolution of the scores of the\n        true class, averaged by new and old classes.\n\n    :param label2step2mean_scores: A dictionary that, for each label category\n        (\"old\" and \"new\") contains a dictionary of mean scores indexed by the\n        step of the observation.\n    :return: The figure containing the graphs.\n    \"\"\"\n    fig, ax = subplots()\n    ax: Axes\n\n    markers = \"*o\"\n\n    for marker, (label, step2mean_scores) in zip(\n        markers, label2step2mean_scores.items()\n    ):\n        ax.plot(\n            step2mean_scores.keys(),\n            step2mean_scores.values(),\n            marker,\n            label=label,\n        )\n\n    ax.legend(loc=\"lower left\")\n    ax.set_xlabel(\"step\")\n    ax.set_ylabel(\"mean score\")\n\n    fig.tight_layout()\n    return fig",
  "class MeanScoresPluginMetricABC(PluginMetric, ABC):\n    \"\"\"\n    Base class for the plugins that show the scores of the true class, averaged\n        by new and old classes.\n\n    :param image_creator: The function to use to create an image of the history\n        of the mean scores grouped by old and new classes\n    \"\"\"\n\n    def __init__(\n        self,\n        image_creator: Optional[\n            MeanScoresImageCreator\n        ] = default_mean_scores_image_creator,\n    ):\n        super().__init__()\n        self.mean_scores = MeanNewOldScores()\n        self.image_creator = image_creator\n        self.label_cat2step2mean: Dict[\n            LabelCat, Dict[int, float]\n        ] = defaultdict(dict)\n\n    def reset(self) -> None:\n        self.mean_scores.reset()\n\n    def update_new_classes(self, strategy: \"BaseStrategy\"):\n        self.mean_scores.update_new_classes(\n            strategy.experience.classes_in_this_experience\n        )\n\n    def update(self, strategy: \"BaseStrategy\"):\n        self.mean_scores.update(\n            predicted_y=strategy.mb_output, true_y=strategy.mb_y\n        )\n\n    def result(self) -> Dict[LabelCat, float]:\n        return self.mean_scores.result()\n\n    def _package_result(self, strategy: \"BaseStrategy\") -> \"MetricResult\":\n        label_cat2mean_score: Dict[LabelCat, float] = self.result()\n\n        for label_cat, m in label_cat2mean_score.items():\n            self.label_cat2step2mean[label_cat][self.global_it_counter] = m\n\n        base_metric_name = get_metric_name(\n            self, strategy, add_experience=False, add_task=False\n        )\n\n        rv = [\n            MetricValue(\n                self,\n                name=base_metric_name + f\"/{label_cat}_classes\",\n                value=m,\n                x_plot=self.global_it_counter,\n            )\n            for label_cat, m in label_cat2mean_score.items()\n        ]\n        if \"old\" in label_cat2mean_score and \"new\" in label_cat2mean_score:\n            rv.append(\n                MetricValue(\n                    self,\n                    name=base_metric_name + f\"/new_old_diff\",\n                    value=label_cat2mean_score[\"new\"]\n                    - label_cat2mean_score[\"old\"],\n                    x_plot=self.global_it_counter,\n                )\n            )\n        if self.image_creator is not None:\n            rv.append(\n                MetricValue(\n                    self,\n                    name=base_metric_name,\n                    value=AlternativeValues(\n                        self.image_creator(self.label_cat2step2mean),\n                        self.label_cat2step2mean,\n                    ),\n                    x_plot=self.global_it_counter,\n                )\n            )\n\n        return rv\n\n    def __str__(self):\n        return \"MeanScores\"",
  "class MeanScoresTrainPluginMetric(MeanScoresPluginMetricABC):\n    \"\"\"\n    Plugin to show the scores of the true class during the lasts training\n        epochs of each experience, averaged  by new and old classes.\n    \"\"\"\n\n    def before_training_epoch(self, strategy: \"BaseStrategy\") -> None:\n        self.reset()\n        self.update_new_classes(strategy)\n\n    def after_training_iteration(self, strategy: \"BaseStrategy\") -> None:\n        if strategy.epoch == strategy.train_epochs - 1:\n            self.update(strategy)\n        super().after_training_iteration(strategy)\n\n    def after_training_epoch(self, strategy: \"BaseStrategy\") -> \"MetricResult\":\n        if strategy.epoch == strategy.train_epochs - 1:\n            return self._package_result(strategy)",
  "class MeanScoresEvalPluginMetric(MeanScoresPluginMetricABC):\n    \"\"\"\n    Plugin to show the scores of the true class during evaluation, averaged by\n        new and old classes.\n    \"\"\"\n\n    def before_training(self, strategy: \"BaseStrategy\") -> None:\n        self.reset()\n\n    def before_training_exp(self, strategy: \"BaseStrategy\") -> None:\n        self.update_new_classes(strategy)\n\n    def after_eval_iteration(self, strategy: \"BaseStrategy\") -> None:\n        self.update(strategy)\n        super().after_eval_iteration(strategy)\n\n    def after_eval(self, strategy: \"BaseStrategy\") -> \"MetricResult\":\n        return self._package_result(strategy)",
  "def mean_scores_metrics(\n    *,\n    on_train: bool = True,\n    on_eval: bool = True,\n    image_creator: Optional[\n        MeanScoresImageCreator\n    ] = default_mean_scores_image_creator,\n) -> List[PluginMetric]:\n    \"\"\"\n    Helper to create plugins to show the scores of the true class, averaged by\n        new and old classes. The plugins are available during training (for the\n        last epoch of each experience) and evaluation.\n\n    :param on_train: If True the train plugin is created\n    :param on_eval: If True the eval plugin is created\n    :param image_creator: The function to use to create an image of the history\n        of the mean scores grouped by old and new classes\n    :return: The list of plugins that were specified\n    \"\"\"\n    plugins = []\n\n    if on_eval:\n        plugins.append(MeanScoresEvalPluginMetric(image_creator=image_creator))\n    if on_train:\n        plugins.append(MeanScoresTrainPluginMetric(image_creator=image_creator))\n\n    return plugins",
  "def __init__(self):\n        super().__init__()\n        self.label2mean: Dict[int, Mean] = defaultdict(Mean)\n        self.reset()",
  "def reset(self) -> None:\n        self.label2mean = defaultdict(Mean)",
  "def update(self, predicted_y: Tensor, true_y: Tensor):\n        assert (\n            len(predicted_y.size()) == 2\n        ), \"Predictions need to be logits or scores, not labels\"\n\n        if len(true_y.size()) == 2:\n            true_y = true_y.argmax(axis=1)\n\n        scores = predicted_y[arange(len(true_y)), true_y]\n\n        for score, label in zip(scores.tolist(), true_y.tolist()):\n            self.label2mean[label].update(score)",
  "def result(self) -> Dict[int, float]:\n        return {label: m.result() for label, m in self.label2mean.items()}",
  "def __init__(self):\n        super().__init__()\n        self.new_classes: Set[int] = set()",
  "def reset(self) -> None:\n        super().reset()\n        self.new_classes = set()",
  "def update_new_classes(self, new_classes: Set[int]):\n        self.new_classes.update(new_classes)",
  "def old_classes(self) -> Set[int]:\n        return set(self.label2mean) - self.new_classes",
  "def result(self) -> Dict[LabelCat, float]:\n        # print(self.new_classes, self.label2mean)\n        rv = {\n            \"new\": sum(\n                (self.label2mean[label] for label in self.new_classes),\n                start=Mean(),\n            ).result()\n        }\n        if not self.old_classes:\n            return rv\n\n        rv[\"old\"] = sum(\n            (self.label2mean[label] for label in self.old_classes),\n            start=Mean(),\n        ).result()\n\n        return rv",
  "def __init__(\n        self,\n        image_creator: Optional[\n            MeanScoresImageCreator\n        ] = default_mean_scores_image_creator,\n    ):\n        super().__init__()\n        self.mean_scores = MeanNewOldScores()\n        self.image_creator = image_creator\n        self.label_cat2step2mean: Dict[\n            LabelCat, Dict[int, float]\n        ] = defaultdict(dict)",
  "def reset(self) -> None:\n        self.mean_scores.reset()",
  "def update_new_classes(self, strategy: \"BaseStrategy\"):\n        self.mean_scores.update_new_classes(\n            strategy.experience.classes_in_this_experience\n        )",
  "def update(self, strategy: \"BaseStrategy\"):\n        self.mean_scores.update(\n            predicted_y=strategy.mb_output, true_y=strategy.mb_y\n        )",
  "def result(self) -> Dict[LabelCat, float]:\n        return self.mean_scores.result()",
  "def _package_result(self, strategy: \"BaseStrategy\") -> \"MetricResult\":\n        label_cat2mean_score: Dict[LabelCat, float] = self.result()\n\n        for label_cat, m in label_cat2mean_score.items():\n            self.label_cat2step2mean[label_cat][self.global_it_counter] = m\n\n        base_metric_name = get_metric_name(\n            self, strategy, add_experience=False, add_task=False\n        )\n\n        rv = [\n            MetricValue(\n                self,\n                name=base_metric_name + f\"/{label_cat}_classes\",\n                value=m,\n                x_plot=self.global_it_counter,\n            )\n            for label_cat, m in label_cat2mean_score.items()\n        ]\n        if \"old\" in label_cat2mean_score and \"new\" in label_cat2mean_score:\n            rv.append(\n                MetricValue(\n                    self,\n                    name=base_metric_name + f\"/new_old_diff\",\n                    value=label_cat2mean_score[\"new\"]\n                    - label_cat2mean_score[\"old\"],\n                    x_plot=self.global_it_counter,\n                )\n            )\n        if self.image_creator is not None:\n            rv.append(\n                MetricValue(\n                    self,\n                    name=base_metric_name,\n                    value=AlternativeValues(\n                        self.image_creator(self.label_cat2step2mean),\n                        self.label_cat2step2mean,\n                    ),\n                    x_plot=self.global_it_counter,\n                )\n            )\n\n        return rv",
  "def __str__(self):\n        return \"MeanScores\"",
  "def before_training_epoch(self, strategy: \"BaseStrategy\") -> None:\n        self.reset()\n        self.update_new_classes(strategy)",
  "def after_training_iteration(self, strategy: \"BaseStrategy\") -> None:\n        if strategy.epoch == strategy.train_epochs - 1:\n            self.update(strategy)\n        super().after_training_iteration(strategy)",
  "def after_training_epoch(self, strategy: \"BaseStrategy\") -> \"MetricResult\":\n        if strategy.epoch == strategy.train_epochs - 1:\n            return self._package_result(strategy)",
  "def before_training(self, strategy: \"BaseStrategy\") -> None:\n        self.reset()",
  "def before_training_exp(self, strategy: \"BaseStrategy\") -> None:\n        self.update_new_classes(strategy)",
  "def after_eval_iteration(self, strategy: \"BaseStrategy\") -> None:\n        self.update(strategy)\n        super().after_eval_iteration(strategy)",
  "def after_eval(self, strategy: \"BaseStrategy\") -> \"MetricResult\":\n        return self._package_result(strategy)",
  "class LabelsRepartition(Metric):\n    \"\"\"\n    Metric used to monitor the labels repartition.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.task2label2count: Dict[int, Dict[int, int]] = {}\n        self.class_order = None\n        self.reset()\n\n    def reset(self) -> None:\n        self.task2label2count = defaultdict(Counter)\n\n    def update(\n        self,\n        tasks: Sequence[int],\n        labels: Sequence[Union[str, int]],\n        class_order: Optional[List[int]],\n    ):\n        self.class_order = class_order\n        for task, label in zip(tasks, labels):\n            self.task2label2count[task][label] += 1\n\n    def update_order(self, class_order: Optional[List[int]]):\n        self.class_order = class_order\n\n    def result(self) -> Dict[int, Dict[int, int]]:\n        if self.class_order is None:\n            return self.task2label2count\n        return {\n            task: {\n                label: label2count[label]\n                for label in self.class_order\n                if label in label2count\n            }\n            for task, label2count in self.task2label2count.items()\n        }",
  "class LabelsRepartitionPlugin(GenericPluginMetric[Figure]):\n    \"\"\"\n    A plugin to monitor the labels repartition.\n\n    :param image_creator: The function to use to create an image from the\n        history of the labels repartition. It will receive a dictionary of the\n        form {label_id: [count_at_step_0, count_at_step_1, ...], ...}\n        and the list of the corresponding steps [step_0, step_1, ...].\n        If set to None, only the raw data is emitted.\n    :param mode: Indicates if this plugin should run on train or eval.\n    :param emit_reset_at: The refreshment rate of the plugin.\n    :return: The list of corresponding plugins.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        image_creator: Optional[\n            LabelsRepartitionImageCreator\n        ] = default_history_repartition_image_creator,\n        mode: Literal[\"train\", \"eval\"] = \"train\",\n        emit_reset_at: Literal[\"stream\", \"experience\", \"epoch\"] = \"epoch\",\n    ):\n        self.labels_repartition = LabelsRepartition()\n        super().__init__(\n            metric=self.labels_repartition,\n            emit_at=emit_reset_at,\n            reset_at=emit_reset_at,\n            mode=mode,\n        )\n        self.emit_reset_at = emit_reset_at\n        self.mode = mode\n        self.image_creator = image_creator\n        self.steps = [0]\n        self.task2label2counts: Dict[int, Dict[int, List[int]]] = defaultdict(\n            dict\n        )\n\n    def reset(self) -> None:\n        self.steps.append(self.global_it_counter)\n        return super().reset()\n\n    def update(self, strategy: \"BaseStrategy\"):\n        if strategy.epoch and self.emit_reset_at != \"epoch\":\n            return\n        self.labels_repartition.update(\n            strategy.mb_task_id.tolist(),\n            strategy.mb_y.tolist(),\n            class_order=getattr(\n                strategy.experience.benchmark, \"classes_order\", None\n            ),\n        )\n\n    def _package_result(self, strategy: \"BaseStrategy\") -> \"MetricResult\":\n        self.steps.append(self.global_it_counter)\n        task2label2count = self.labels_repartition.result()\n        for task, label2count in task2label2count.items():\n            for label, count in label2count.items():\n                self.task2label2counts[task].setdefault(\n                    label, [0] * (len(self.steps) - 2)\n                ).extend((count, count))\n        for task, label2counts in self.task2label2counts.items():\n            for label, counts in label2counts.items():\n                counts.extend([0] * (len(self.steps) - len(counts)))\n        return [\n            MetricValue(\n                self,\n                name=f\"Repartition\"\n                f\"/{self._mode}_phase\"\n                f\"/{stream_type(strategy.experience)}_stream\"\n                f\"/Task_{task:03}\",\n                value=AlternativeValues(\n                    self.image_creator(label2counts, self.steps), label2counts,\n                )\n                if self.image_creator is not None\n                else label2counts,\n                x_plot=self.get_global_counter(),\n            )\n            for task, label2counts in self.task2label2counts.items()\n        ]\n\n    def __str__(self):\n        return \"Repartition\"",
  "def labels_repartition_metrics(\n    *,\n    on_train: bool = True,\n    emit_train_at: Literal[\"stream\", \"experience\", \"epoch\"] = \"epoch\",\n    on_eval: bool = False,\n    emit_eval_at: Literal[\"stream\", \"experience\"] = \"stream\",\n    image_creator: Optional[\n        LabelsRepartitionImageCreator\n    ] = default_history_repartition_image_creator,\n) -> List[PluginMetric]:\n    \"\"\"\n    Create plugins to monitor the labels repartition.\n\n    :param on_train: If True, emit the metrics during training.\n    :param emit_train_at: (only if on_train is True) when to emit the training\n        metrics.\n    :param on_eval:  If True, emit the metrics during evaluation.\n    :param emit_eval_at: (only if on_eval is True) when to emit the evaluation\n        metrics.\n    :param image_creator: The function to use to create an image from the\n        history of the labels repartition. It will receive a dictionary of the\n        form {label_id: [count_at_step_0, count_at_step_1, ...], ...}\n        and the list of the corresponding steps [step_0, step_1, ...].\n        If set to None, only the raw data is emitted.\n    :return: The list of corresponding plugins.\n    \"\"\"\n    plugins = []\n    if on_eval:\n        plugins.append(\n            LabelsRepartitionPlugin(\n                image_creator=image_creator,\n                mode=\"eval\",\n                emit_reset_at=emit_eval_at,\n            )\n        )\n    if on_train:\n        plugins.append(\n            LabelsRepartitionPlugin(\n                image_creator=image_creator,\n                mode=\"train\",\n                emit_reset_at=emit_train_at,\n            )\n        )\n\n    return plugins",
  "def __init__(self):\n        super().__init__()\n        self.task2label2count: Dict[int, Dict[int, int]] = {}\n        self.class_order = None\n        self.reset()",
  "def reset(self) -> None:\n        self.task2label2count = defaultdict(Counter)",
  "def update(\n        self,\n        tasks: Sequence[int],\n        labels: Sequence[Union[str, int]],\n        class_order: Optional[List[int]],\n    ):\n        self.class_order = class_order\n        for task, label in zip(tasks, labels):\n            self.task2label2count[task][label] += 1",
  "def update_order(self, class_order: Optional[List[int]]):\n        self.class_order = class_order",
  "def result(self) -> Dict[int, Dict[int, int]]:\n        if self.class_order is None:\n            return self.task2label2count\n        return {\n            task: {\n                label: label2count[label]\n                for label in self.class_order\n                if label in label2count\n            }\n            for task, label2count in self.task2label2count.items()\n        }",
  "def __init__(\n        self,\n        *,\n        image_creator: Optional[\n            LabelsRepartitionImageCreator\n        ] = default_history_repartition_image_creator,\n        mode: Literal[\"train\", \"eval\"] = \"train\",\n        emit_reset_at: Literal[\"stream\", \"experience\", \"epoch\"] = \"epoch\",\n    ):\n        self.labels_repartition = LabelsRepartition()\n        super().__init__(\n            metric=self.labels_repartition,\n            emit_at=emit_reset_at,\n            reset_at=emit_reset_at,\n            mode=mode,\n        )\n        self.emit_reset_at = emit_reset_at\n        self.mode = mode\n        self.image_creator = image_creator\n        self.steps = [0]\n        self.task2label2counts: Dict[int, Dict[int, List[int]]] = defaultdict(\n            dict\n        )",
  "def reset(self) -> None:\n        self.steps.append(self.global_it_counter)\n        return super().reset()",
  "def update(self, strategy: \"BaseStrategy\"):\n        if strategy.epoch and self.emit_reset_at != \"epoch\":\n            return\n        self.labels_repartition.update(\n            strategy.mb_task_id.tolist(),\n            strategy.mb_y.tolist(),\n            class_order=getattr(\n                strategy.experience.benchmark, \"classes_order\", None\n            ),\n        )",
  "def _package_result(self, strategy: \"BaseStrategy\") -> \"MetricResult\":\n        self.steps.append(self.global_it_counter)\n        task2label2count = self.labels_repartition.result()\n        for task, label2count in task2label2count.items():\n            for label, count in label2count.items():\n                self.task2label2counts[task].setdefault(\n                    label, [0] * (len(self.steps) - 2)\n                ).extend((count, count))\n        for task, label2counts in self.task2label2counts.items():\n            for label, counts in label2counts.items():\n                counts.extend([0] * (len(self.steps) - len(counts)))\n        return [\n            MetricValue(\n                self,\n                name=f\"Repartition\"\n                f\"/{self._mode}_phase\"\n                f\"/{stream_type(strategy.experience)}_stream\"\n                f\"/Task_{task:03}\",\n                value=AlternativeValues(\n                    self.image_creator(label2counts, self.steps), label2counts,\n                )\n                if self.image_creator is not None\n                else label2counts,\n                x_plot=self.get_global_counter(),\n            )\n            for task, label2counts in self.task2label2counts.items()\n        ]",
  "def __str__(self):\n        return \"Repartition\"",
  "class BatchRenorm2D(Module):\n\n    def __init__(self, num_features, gamma=None, beta=None,\n                 running_mean=None, running_var=None, eps=1e-05,\n                 momentum=0.01, r_d_max_inc_step=0.0001, r_max=1.0,\n                 d_max=0.0, max_r_max=3.0, max_d_max=5.0):\n        super(BatchRenorm2D, self).__init__()\n\n        self.eps = eps\n        self.num_features = num_features\n        self.momentum = torch.tensor(momentum, requires_grad=False)\n\n        if gamma is None:\n            self.gamma = torch.nn.Parameter(\n                torch.ones((1, num_features, 1, 1)), requires_grad=True)\n        else:\n            self.gamma = torch.nn.Parameter(gamma.view(1, -1, 1, 1))\n        if beta is None:\n            self.beta = torch.nn.Parameter(\n                torch.zeros((1, num_features, 1, 1)), requires_grad=True)\n        else:\n            self.beta = torch.nn.Parameter(beta.view(1, -1, 1, 1))\n\n        if running_mean is None:\n            self.running_avg_mean = torch.ones(\n                (1, num_features, 1, 1), requires_grad=False)\n            self.running_avg_std = torch.zeros(\n                (1, num_features, 1, 1), requires_grad=False)\n        else:\n            self.running_avg_mean = running_mean.view(1, -1, 1, 1)\n            self.running_avg_std = torch.sqrt(running_var.view(1, -1, 1, 1))\n\n        self.max_r_max = max_r_max\n        self.max_d_max = max_d_max\n\n        self.r_max_inc_step = r_d_max_inc_step\n        self.d_max_inc_step = r_d_max_inc_step\n\n        self.r_max = r_max\n        self.d_max = d_max\n\n    def forward(self, x):\n\n        device = self.gamma.device\n\n        batch_ch_mean = torch.mean(x, dim=(0, 2, 3), keepdim=True).to(device)\n        batch_ch_std = torch.sqrt(torch.var(\n            x, dim=(0, 2, 3), keepdim=True, unbiased=False) + self.eps)\n        batch_ch_std = batch_ch_std.to(device)\n\n        self.running_avg_std = self.running_avg_std.to(device)\n        self.running_avg_mean = self.running_avg_mean.to(device)\n        self.momentum = self.momentum.to(device)\n\n        if self.training:\n            r = torch.clamp(batch_ch_std / self.running_avg_std, 1.0 /\n                            self.r_max, self.r_max).to(device).data.to(device)\n            d = torch.clamp((batch_ch_mean - self.running_avg_mean) /\n                            self.running_avg_std, -self.d_max, self.d_max)\\\n                .to(device).data.to(device)\n\n            x = ((x - batch_ch_mean) * r) / batch_ch_std + d\n            x = self.gamma * x + self.beta\n\n            if self.r_max < self.max_r_max:\n                self.r_max += self.r_max_inc_step * x.shape[0]\n\n            if self.d_max < self.max_d_max:\n                self.d_max += self.d_max_inc_step * x.shape[0]\n\n            self.running_avg_mean = self.running_avg_mean + self.momentum * \\\n                (batch_ch_mean.data.to(device) - self.running_avg_mean)\n            self.running_avg_std = self.running_avg_std + self.momentum * \\\n                (batch_ch_std.data.to(device) - self.running_avg_std)\n\n        else:\n\n            x = (x - self.running_avg_mean) / self.running_avg_std\n            x = self.gamma * x + self.beta\n\n        return x",
  "def __init__(self, num_features, gamma=None, beta=None,\n                 running_mean=None, running_var=None, eps=1e-05,\n                 momentum=0.01, r_d_max_inc_step=0.0001, r_max=1.0,\n                 d_max=0.0, max_r_max=3.0, max_d_max=5.0):\n        super(BatchRenorm2D, self).__init__()\n\n        self.eps = eps\n        self.num_features = num_features\n        self.momentum = torch.tensor(momentum, requires_grad=False)\n\n        if gamma is None:\n            self.gamma = torch.nn.Parameter(\n                torch.ones((1, num_features, 1, 1)), requires_grad=True)\n        else:\n            self.gamma = torch.nn.Parameter(gamma.view(1, -1, 1, 1))\n        if beta is None:\n            self.beta = torch.nn.Parameter(\n                torch.zeros((1, num_features, 1, 1)), requires_grad=True)\n        else:\n            self.beta = torch.nn.Parameter(beta.view(1, -1, 1, 1))\n\n        if running_mean is None:\n            self.running_avg_mean = torch.ones(\n                (1, num_features, 1, 1), requires_grad=False)\n            self.running_avg_std = torch.zeros(\n                (1, num_features, 1, 1), requires_grad=False)\n        else:\n            self.running_avg_mean = running_mean.view(1, -1, 1, 1)\n            self.running_avg_std = torch.sqrt(running_var.view(1, -1, 1, 1))\n\n        self.max_r_max = max_r_max\n        self.max_d_max = max_d_max\n\n        self.r_max_inc_step = r_d_max_inc_step\n        self.d_max_inc_step = r_d_max_inc_step\n\n        self.r_max = r_max\n        self.d_max = d_max",
  "def forward(self, x):\n\n        device = self.gamma.device\n\n        batch_ch_mean = torch.mean(x, dim=(0, 2, 3), keepdim=True).to(device)\n        batch_ch_std = torch.sqrt(torch.var(\n            x, dim=(0, 2, 3), keepdim=True, unbiased=False) + self.eps)\n        batch_ch_std = batch_ch_std.to(device)\n\n        self.running_avg_std = self.running_avg_std.to(device)\n        self.running_avg_mean = self.running_avg_mean.to(device)\n        self.momentum = self.momentum.to(device)\n\n        if self.training:\n            r = torch.clamp(batch_ch_std / self.running_avg_std, 1.0 /\n                            self.r_max, self.r_max).to(device).data.to(device)\n            d = torch.clamp((batch_ch_mean - self.running_avg_mean) /\n                            self.running_avg_std, -self.d_max, self.d_max)\\\n                .to(device).data.to(device)\n\n            x = ((x - batch_ch_mean) * r) / batch_ch_std + d\n            x = self.gamma * x + self.beta\n\n            if self.r_max < self.max_r_max:\n                self.r_max += self.r_max_inc_step * x.shape[0]\n\n            if self.d_max < self.max_d_max:\n                self.d_max += self.d_max_inc_step * x.shape[0]\n\n            self.running_avg_mean = self.running_avg_mean + self.momentum * \\\n                (batch_ch_mean.data.to(device) - self.running_avg_mean)\n            self.running_avg_std = self.running_avg_std + self.momentum * \\\n                (batch_ch_std.data.to(device) - self.running_avg_std)\n\n        else:\n\n            x = (x - self.running_avg_mean) / self.running_avg_std\n            x = self.gamma * x + self.beta\n\n        return x",
  "class NCMClassifier(nn.Module):\n    \"\"\"\n        NCM Classifier.\n        NCMClassifier performs nearest class mean classification\n        measuring the distance between the input tensor and the\n        ones stored in 'self.class_means'.\n    \"\"\"\n    def __init__(self, class_mean=None):\n        \"\"\"\n        :param class_mean: tensor of dimension (num_classes x feature_size)\n            used to classify input patterns.\n        \"\"\"\n        super().__init__()\n        self.class_means = class_mean\n\n    def forward(self, x):\n        pred_inter = (x.T / torch.norm(x.T, dim=0)).T\n        sqd = torch.cdist(self.class_means[:, :].T, pred_inter)\n        return (-sqd).T",
  "def __init__(self, class_mean=None):\n        \"\"\"\n        :param class_mean: tensor of dimension (num_classes x feature_size)\n            used to classify input patterns.\n        \"\"\"\n        super().__init__()\n        self.class_means = class_mean",
  "def forward(self, x):\n        pred_inter = (x.T / torch.norm(x.T, dim=0)).T\n        sqd = torch.cdist(self.class_means[:, :].T, pred_inter)\n        return (-sqd).T",
  "class SimpleCNN(nn.Module):\n\n    def __init__(self, num_classes=10):\n        super(SimpleCNN, self).__init__()\n\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 32, kernel_size=3, padding=0),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Dropout(p=0.25),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=0),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Dropout(p=0.25),\n            nn.Conv2d(64, 64, kernel_size=1, padding=0),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveMaxPool2d(1),\n            nn.Dropout(p=0.25)\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(64, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x",
  "class MTSimpleCNN(SimpleCNN, MultiTaskModule):\n\n    def __init__(self):\n        \"\"\"\n            Multi-task CNN with multi-head classifier.\n        \"\"\"\n        super().__init__()\n        self.classifier = MultiHeadClassifier(64)\n\n    def forward(self, x, task_labels):\n        x = self.features(x)\n        x = x.squeeze()\n        x = self.classifier(x, task_labels)\n        return x",
  "def __init__(self, num_classes=10):\n        super(SimpleCNN, self).__init__()\n\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 32, kernel_size=3, padding=0),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Dropout(p=0.25),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=0),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Dropout(p=0.25),\n            nn.Conv2d(64, 64, kernel_size=1, padding=0),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveMaxPool2d(1),\n            nn.Dropout(p=0.25)\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(64, num_classes)\n        )",
  "def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x",
  "def __init__(self):\n        \"\"\"\n            Multi-task CNN with multi-head classifier.\n        \"\"\"\n        super().__init__()\n        self.classifier = MultiHeadClassifier(64)",
  "def forward(self, x, task_labels):\n        x = self.features(x)\n        x = x.squeeze()\n        x = self.classifier(x, task_labels)\n        return x",
  "def avalanche_forward(model, x, task_labels):\n    if isinstance(model, MultiTaskModule):\n        return model(x, task_labels)\n    else:  # no task labels\n        return model(x)",
  "class FeatureExtractorBackbone(nn.Module):\n    \"\"\"\n    This PyTorch module allows us to extract features from a backbone network\n    given a layer name.\n    \"\"\"\n\n    def __init__(self, model, output_layer_name):\n        super(FeatureExtractorBackbone, self).__init__()\n        self.model = model\n        self.output_layer_name = output_layer_name\n        self.output = None  # this will store the layer output\n        self.add_hooks(self.model)\n\n    def forward(self, x):\n        self.model(x)\n        return self.output\n\n    def get_name_to_module(self, model):\n        name_to_module = {}\n        for m in model.named_modules():\n            name_to_module[m[0]] = m[1]\n        return name_to_module\n\n    def get_activation(self):\n        def hook(model, input, output):\n            self.output = output.detach()\n\n        return hook\n\n    def add_hooks(self, model):\n        \"\"\"\n        :param model:\n        :param outputs: Outputs from layers specified in `output_layer_names`\n        will be stored in `output` variable\n        :param output_layer_names:\n        :return:\n        \"\"\"\n        name_to_module = self.get_name_to_module(model)\n        name_to_module[self.output_layer_name].register_forward_hook(\n            self.get_activation())",
  "def __init__(self, model, output_layer_name):\n        super(FeatureExtractorBackbone, self).__init__()\n        self.model = model\n        self.output_layer_name = output_layer_name\n        self.output = None  # this will store the layer output\n        self.add_hooks(self.model)",
  "def forward(self, x):\n        self.model(x)\n        return self.output",
  "def get_name_to_module(self, model):\n        name_to_module = {}\n        for m in model.named_modules():\n            name_to_module[m[0]] = m[1]\n        return name_to_module",
  "def get_activation(self):\n        def hook(model, input, output):\n            self.output = output.detach()\n\n        return hook",
  "def add_hooks(self, model):\n        \"\"\"\n        :param model:\n        :param outputs: Outputs from layers specified in `output_layer_names`\n        will be stored in `output` variable\n        :param output_layer_names:\n        :return:\n        \"\"\"\n        name_to_module = self.get_name_to_module(model)\n        name_to_module[self.output_layer_name].register_forward_hook(\n            self.get_activation())",
  "def hook(model, input, output):\n            self.output = output.detach()",
  "def remove_sequential(network, all_layers):\n\n    for layer in network.children():\n        # if sequential layer, apply recursively to layers in sequential layer\n        if isinstance(layer, nn.Sequential):\n            # print(layer)\n            remove_sequential(layer, all_layers)\n        else:  # if leaf node, add it to list\n            # print(layer)\n            all_layers.append(layer)",
  "def remove_DwsConvBlock(cur_layers):\n\n    all_layers = []\n    for layer in cur_layers:\n        if isinstance(layer, DwsConvBlock):\n            # print(\"helloooo: \", layer)\n            for ch in layer.children():\n                all_layers.append(ch)\n        else:\n            all_layers.append(layer)\n    return all_layers",
  "class MobilenetV1(nn.Module):\n    def __init__(self, pretrained=True, latent_layer_num=20):\n        super().__init__()\n\n        model = mobilenet_w1(pretrained=pretrained)\n        model.features.final_pool = nn.AvgPool2d(4)\n\n        all_layers = []\n        remove_sequential(model, all_layers)\n        all_layers = remove_DwsConvBlock(all_layers)\n\n        lat_list = []\n        end_list = []\n\n        for i, layer in enumerate(all_layers[:-1]):\n            if i <= latent_layer_num:\n                lat_list.append(layer)\n            else:\n                end_list.append(layer)\n\n        self.lat_features = nn.Sequential(*lat_list)\n        self.end_features = nn.Sequential(*end_list)\n\n        self.output = nn.Linear(1024, 50, bias=False)\n\n    def forward(self, x, latent_input=None,\n                return_lat_acts=False):\n\n        if latent_input is not None:\n            with torch.no_grad():\n                orig_acts = self.lat_features(x)\n            lat_acts = torch.cat((orig_acts, latent_input), 0)\n        else:\n            orig_acts = self.lat_features(x)\n            lat_acts = orig_acts\n\n        x = self.end_features(lat_acts)\n        x = x.view(x.size(0), -1)\n        logits = self.output(x)\n\n        if return_lat_acts:\n            return logits, orig_acts\n        else:\n            return logits",
  "def __init__(self, pretrained=True, latent_layer_num=20):\n        super().__init__()\n\n        model = mobilenet_w1(pretrained=pretrained)\n        model.features.final_pool = nn.AvgPool2d(4)\n\n        all_layers = []\n        remove_sequential(model, all_layers)\n        all_layers = remove_DwsConvBlock(all_layers)\n\n        lat_list = []\n        end_list = []\n\n        for i, layer in enumerate(all_layers[:-1]):\n            if i <= latent_layer_num:\n                lat_list.append(layer)\n            else:\n                end_list.append(layer)\n\n        self.lat_features = nn.Sequential(*lat_list)\n        self.end_features = nn.Sequential(*end_list)\n\n        self.output = nn.Linear(1024, 50, bias=False)",
  "def forward(self, x, latent_input=None,\n                return_lat_acts=False):\n\n        if latent_input is not None:\n            with torch.no_grad():\n                orig_acts = self.lat_features(x)\n            lat_acts = torch.cat((orig_acts, latent_input), 0)\n        else:\n            orig_acts = self.lat_features(x)\n            lat_acts = orig_acts\n\n        x = self.end_features(lat_acts)\n        x = x.view(x.size(0), -1)\n        logits = self.output(x)\n\n        if return_lat_acts:\n            return logits, orig_acts\n        else:\n            return logits",
  "class SimpleMLP(nn.Module):\n    def __init__(self, num_classes=10, input_size=28 * 28,\n                 hidden_size=512, hidden_layers=1, drop_rate=0.5):\n        super().__init__()\n\n        layers = nn.Sequential(*(nn.Linear(input_size, hidden_size),\n                                 nn.ReLU(inplace=True),\n                                 nn.Dropout(p=drop_rate)))\n        for layer_idx in range(hidden_layers - 1):\n            layers.add_module(\n                f\"fc{layer_idx + 1}\", nn.Sequential(\n                    *(nn.Linear(hidden_size, hidden_size),\n                      nn.ReLU(inplace=True),\n                      nn.Dropout())))\n\n        self.features = nn.Sequential(*layers)\n        self.classifier = nn.Linear(hidden_size, num_classes)\n        self._input_size = input_size\n\n    def forward(self, x):\n        x = x.contiguous()\n        x = x.view(x.size(0), self._input_size)\n        x = self.features(x)\n        x = self.classifier(x)\n        return x",
  "class MTSimpleMLP(MultiTaskModule):\n    def __init__(self, input_size=28 * 28, hidden_size=512):\n        \"\"\"\n            Multi-task MLP with multi-head classifier.\n        \"\"\"\n        super().__init__()\n\n        self.features = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n        )\n        self.classifier = MultiHeadClassifier(hidden_size)\n        self._input_size = input_size\n\n    def forward(self, x, task_labels):\n        x = x.contiguous()\n        x = x.view(x.size(0), self._input_size)\n        x = self.features(x)\n        x = self.classifier(x, task_labels)\n        return x",
  "def __init__(self, num_classes=10, input_size=28 * 28,\n                 hidden_size=512, hidden_layers=1, drop_rate=0.5):\n        super().__init__()\n\n        layers = nn.Sequential(*(nn.Linear(input_size, hidden_size),\n                                 nn.ReLU(inplace=True),\n                                 nn.Dropout(p=drop_rate)))\n        for layer_idx in range(hidden_layers - 1):\n            layers.add_module(\n                f\"fc{layer_idx + 1}\", nn.Sequential(\n                    *(nn.Linear(hidden_size, hidden_size),\n                      nn.ReLU(inplace=True),\n                      nn.Dropout())))\n\n        self.features = nn.Sequential(*layers)\n        self.classifier = nn.Linear(hidden_size, num_classes)\n        self._input_size = input_size",
  "def forward(self, x):\n        x = x.contiguous()\n        x = x.view(x.size(0), self._input_size)\n        x = self.features(x)\n        x = self.classifier(x)\n        return x",
  "def __init__(self, input_size=28 * 28, hidden_size=512):\n        \"\"\"\n            Multi-task MLP with multi-head classifier.\n        \"\"\"\n        super().__init__()\n\n        self.features = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n        )\n        self.classifier = MultiHeadClassifier(hidden_size)\n        self._input_size = input_size",
  "def forward(self, x, task_labels):\n        x = x.contiguous()\n        x = x.view(x.size(0), self._input_size)\n        x = self.features(x)\n        x = self.classifier(x, task_labels)\n        return x",
  "class LinearAdapter(nn.Module):\n    def __init__(self, in_features, out_features_per_column, num_prev_modules):\n        \"\"\" Linear adapter for Progressive Neural Networks.\n\n        :param in_features: size of each input sample\n        :param out_features_per_column: size of each output sample\n        :param num_prev_modules: number of previous modules\n        \"\"\"\n        super().__init__()\n        # Eq. 1 - lateral connections\n        # one layer for each previous column. Empty for the first task.\n        self.lat_layers = nn.ModuleList([])\n        for _ in range(num_prev_modules):\n            m = nn.Linear(in_features, out_features_per_column)\n            self.lat_layers.append(m)\n\n    def forward(self, x):\n        assert len(x) == self.num_prev_modules\n        hs = []\n        for ii, lat in enumerate(self.lat_layers):\n            hs.append(lat(x[ii]))\n        return sum(hs)",
  "class MLPAdapter(nn.Module):\n    def __init__(self, in_features, out_features_per_column, num_prev_modules,\n                 activation=F.relu):\n        \"\"\" MLP adapter for Progressive Neural Networks.\n\n        :param in_features: size of each input sample\n        :param out_features_per_column: size of each output sample\n        :param num_prev_modules: number of previous modules\n        :param activation: activation function (default=ReLU)\n        \"\"\"\n        super().__init__()\n        self.num_prev_modules = num_prev_modules\n        self.activation = activation\n\n        if num_prev_modules == 0:\n            return  # first adapter is empty\n\n        # Eq. 2 - MLP adapter. Not needed for the first task.\n        self.V = nn.Linear(in_features * num_prev_modules,\n                           out_features_per_column)\n        self.alphas = nn.Parameter(torch.randn(num_prev_modules))\n        self.U = nn.Linear(out_features_per_column, out_features_per_column)\n\n    def forward(self, x):\n        if self.num_prev_modules == 0:\n            return 0  # first adapter is empty\n\n        assert len(x) == self.num_prev_modules\n        assert len(x[0].shape) == 2, \\\n            \"Inputs to MLPAdapter should have two dimensions: \" \\\n            \"<batch_size, num_features>.\"\n        for i, el in enumerate(x):\n            x[i] = self.alphas[i] * el\n        x = torch.cat(x, dim=1)\n        x = self.U(self.activation(self.V(x)))\n        return x",
  "class PNNColumn(nn.Module):\n    def __init__(self, in_features, out_features_per_column, num_prev_modules,\n                 adapter='mlp'):\n        \"\"\" Progressive Neural Network column.\n\n        :param in_features: size of each input sample\n        :param out_features_per_column:\n            size of each output sample (single column)\n        :param num_prev_modules: number of previous columns\n        :param adapter: adapter type. One of {'linear', 'mlp'} (default='mlp')\n        \"\"\"\n        super().__init__()\n        self.in_features = in_features\n        self.out_features_per_column = out_features_per_column\n        self.num_prev_modules = num_prev_modules\n\n        self.itoh = nn.Linear(in_features, out_features_per_column)\n        if adapter == 'linear':\n            self.adapter = LinearAdapter(in_features, out_features_per_column,\n                                         num_prev_modules)\n        elif adapter == 'mlp':\n            self.adapter = MLPAdapter(in_features, out_features_per_column,\n                                      num_prev_modules)\n        else:\n            raise ValueError(\"`adapter` must be one of: {'mlp', `linear'}.\")\n\n    def freeze(self):\n        for param in self.parameters():\n            param.requires_grad = False\n\n    def forward(self, x):\n        prev_xs, last_x = x[:-1], x[-1]\n        hs = self.adapter(prev_xs)\n        hs += self.itoh(last_x)\n        return hs",
  "class PNNLayer(MultiTaskModule, DynamicModule):\n    def __init__(self, in_features, out_features_per_column, adapter='mlp'):\n        \"\"\" Progressive Neural Network layer.\n\n        The adaptation phase assumes that each experience is a separate task.\n        Multiple experiences with the same task label or multiple task labels\n        within the same experience will result in a runtime error.\n\n        :param in_features: size of each input sample\n        :param out_features_per_column:\n            size of each output sample (single column)\n        :param adapter: adapter type. One of {'linear', 'mlp'} (default='mlp')\n        \"\"\"\n        super().__init__()\n        self.in_features = in_features\n        self.out_features_per_column = out_features_per_column\n        self.adapter = adapter\n\n        # convert from task label to module list order\n        self.task_to_module_idx = {}\n        first_col = PNNColumn(in_features, out_features_per_column,\n                              0, adapter=adapter)\n        self.columns = nn.ModuleList([first_col])\n\n    @property\n    def num_columns(self):\n        return len(self.columns)\n\n    def train_adaptation(self, dataset: AvalancheDataset):\n        \"\"\" Training adaptation for PNN layer.\n\n        Adds an additional column to the layer.\n\n        :param dataset:\n        :return:\n        \"\"\"\n        task_labels = dataset.targets_task_labels\n        if isinstance(task_labels, ConstantSequence):\n            # task label is unique. Don't check duplicates.\n            task_labels = [task_labels[0]]\n        else:\n            task_labels = set(task_labels)\n        assert len(task_labels) == 1, \\\n            \"PNN assumes a single task for each experience. Please use a \" \\\n            \"compatible benchmark.\"\n        # extract task label from set\n        task_label = next(iter(task_labels))\n        assert task_label not in self.task_to_module_idx, \\\n            \"A new experience is using a previously seen task label. This is \" \\\n            \"not compatible with PNN, which assumes different task labels for\" \\\n            \" each training experience.\"\n\n        if len(self.task_to_module_idx) == 0:\n            # we have already initialized the first column.\n            # No need to call add_column here.\n            self.task_to_module_idx[task_label] = 0\n        else:\n            self.task_to_module_idx[task_label] = self.num_columns\n            self._add_column()\n\n    def _add_column(self):\n        \"\"\" Add a new column. \"\"\"\n        # Freeze old parameters\n        for param in self.parameters():\n            param.requires_grad = False\n        self.columns.append(PNNColumn(self.in_features,\n                                      self.out_features_per_column,\n                                      self.num_columns,\n                                      adapter=self.adapter))\n\n    def forward_single_task(self, x, task_label):\n        \"\"\" Forward.\n\n        :param x: list of inputs.\n        :param task_label:\n        :return:\n        \"\"\"\n        col_idx = self.task_to_module_idx[task_label]\n        hs = []\n        for ii in range(col_idx + 1):\n            hs.append(self.columns[ii](x[:ii+1]))\n        return hs",
  "class PNN(MultiTaskModule):\n    def __init__(self, num_layers=1, in_features=784,\n                 hidden_features_per_column=100, adapter='mlp'):\n        \"\"\" Progressive Neural Network.\n\n        The model assumes that each experience is a separate task.\n        Multiple experiences with the same task label or multiple task labels\n        within the same experience will result in a runtime error.\n\n        :param num_layers: number of layers (default=1)\n        :param in_features: size of each input sample\n        :param hidden_features_per_column:\n            number of hidden units for each column\n        :param adapter: adapter type. One of {'linear', 'mlp'} (default='mlp')\n        \"\"\"\n        super().__init__()\n        assert num_layers >= 1\n        self.num_layers = num_layers\n        self.in_features = in_features\n        self.out_features_per_columns = hidden_features_per_column\n\n        self.layers = nn.ModuleList()\n        self.layers.append(PNNLayer(in_features, hidden_features_per_column))\n        for _ in range(num_layers - 1):\n            lay = PNNLayer(hidden_features_per_column,\n                           hidden_features_per_column,\n                           adapter=adapter)\n            self.layers.append(lay)\n        self.classifier = MultiHeadClassifier(hidden_features_per_column)\n\n    def forward_single_task(self, x, task_label):\n        \"\"\" Forward.\n\n        :param x:\n        :param task_label:\n        :return:\n        \"\"\"\n        x = x.contiguous()\n        x = x.view(x.size(0), self.in_features)\n\n        num_columns = self.layers[0].num_columns\n        col_idx = self.layers[-1].task_to_module_idx[task_label]\n\n        x = [x for _ in range(num_columns)]\n        for lay in self.layers:\n            x = [F.relu(el) for el in lay(x, task_label)]\n        return self.classifier(x[col_idx], task_label)",
  "def __init__(self, in_features, out_features_per_column, num_prev_modules):\n        \"\"\" Linear adapter for Progressive Neural Networks.\n\n        :param in_features: size of each input sample\n        :param out_features_per_column: size of each output sample\n        :param num_prev_modules: number of previous modules\n        \"\"\"\n        super().__init__()\n        # Eq. 1 - lateral connections\n        # one layer for each previous column. Empty for the first task.\n        self.lat_layers = nn.ModuleList([])\n        for _ in range(num_prev_modules):\n            m = nn.Linear(in_features, out_features_per_column)\n            self.lat_layers.append(m)",
  "def forward(self, x):\n        assert len(x) == self.num_prev_modules\n        hs = []\n        for ii, lat in enumerate(self.lat_layers):\n            hs.append(lat(x[ii]))\n        return sum(hs)",
  "def __init__(self, in_features, out_features_per_column, num_prev_modules,\n                 activation=F.relu):\n        \"\"\" MLP adapter for Progressive Neural Networks.\n\n        :param in_features: size of each input sample\n        :param out_features_per_column: size of each output sample\n        :param num_prev_modules: number of previous modules\n        :param activation: activation function (default=ReLU)\n        \"\"\"\n        super().__init__()\n        self.num_prev_modules = num_prev_modules\n        self.activation = activation\n\n        if num_prev_modules == 0:\n            return  # first adapter is empty\n\n        # Eq. 2 - MLP adapter. Not needed for the first task.\n        self.V = nn.Linear(in_features * num_prev_modules,\n                           out_features_per_column)\n        self.alphas = nn.Parameter(torch.randn(num_prev_modules))\n        self.U = nn.Linear(out_features_per_column, out_features_per_column)",
  "def forward(self, x):\n        if self.num_prev_modules == 0:\n            return 0  # first adapter is empty\n\n        assert len(x) == self.num_prev_modules\n        assert len(x[0].shape) == 2, \\\n            \"Inputs to MLPAdapter should have two dimensions: \" \\\n            \"<batch_size, num_features>.\"\n        for i, el in enumerate(x):\n            x[i] = self.alphas[i] * el\n        x = torch.cat(x, dim=1)\n        x = self.U(self.activation(self.V(x)))\n        return x",
  "def __init__(self, in_features, out_features_per_column, num_prev_modules,\n                 adapter='mlp'):\n        \"\"\" Progressive Neural Network column.\n\n        :param in_features: size of each input sample\n        :param out_features_per_column:\n            size of each output sample (single column)\n        :param num_prev_modules: number of previous columns\n        :param adapter: adapter type. One of {'linear', 'mlp'} (default='mlp')\n        \"\"\"\n        super().__init__()\n        self.in_features = in_features\n        self.out_features_per_column = out_features_per_column\n        self.num_prev_modules = num_prev_modules\n\n        self.itoh = nn.Linear(in_features, out_features_per_column)\n        if adapter == 'linear':\n            self.adapter = LinearAdapter(in_features, out_features_per_column,\n                                         num_prev_modules)\n        elif adapter == 'mlp':\n            self.adapter = MLPAdapter(in_features, out_features_per_column,\n                                      num_prev_modules)\n        else:\n            raise ValueError(\"`adapter` must be one of: {'mlp', `linear'}.\")",
  "def freeze(self):\n        for param in self.parameters():\n            param.requires_grad = False",
  "def forward(self, x):\n        prev_xs, last_x = x[:-1], x[-1]\n        hs = self.adapter(prev_xs)\n        hs += self.itoh(last_x)\n        return hs",
  "def __init__(self, in_features, out_features_per_column, adapter='mlp'):\n        \"\"\" Progressive Neural Network layer.\n\n        The adaptation phase assumes that each experience is a separate task.\n        Multiple experiences with the same task label or multiple task labels\n        within the same experience will result in a runtime error.\n\n        :param in_features: size of each input sample\n        :param out_features_per_column:\n            size of each output sample (single column)\n        :param adapter: adapter type. One of {'linear', 'mlp'} (default='mlp')\n        \"\"\"\n        super().__init__()\n        self.in_features = in_features\n        self.out_features_per_column = out_features_per_column\n        self.adapter = adapter\n\n        # convert from task label to module list order\n        self.task_to_module_idx = {}\n        first_col = PNNColumn(in_features, out_features_per_column,\n                              0, adapter=adapter)\n        self.columns = nn.ModuleList([first_col])",
  "def num_columns(self):\n        return len(self.columns)",
  "def train_adaptation(self, dataset: AvalancheDataset):\n        \"\"\" Training adaptation for PNN layer.\n\n        Adds an additional column to the layer.\n\n        :param dataset:\n        :return:\n        \"\"\"\n        task_labels = dataset.targets_task_labels\n        if isinstance(task_labels, ConstantSequence):\n            # task label is unique. Don't check duplicates.\n            task_labels = [task_labels[0]]\n        else:\n            task_labels = set(task_labels)\n        assert len(task_labels) == 1, \\\n            \"PNN assumes a single task for each experience. Please use a \" \\\n            \"compatible benchmark.\"\n        # extract task label from set\n        task_label = next(iter(task_labels))\n        assert task_label not in self.task_to_module_idx, \\\n            \"A new experience is using a previously seen task label. This is \" \\\n            \"not compatible with PNN, which assumes different task labels for\" \\\n            \" each training experience.\"\n\n        if len(self.task_to_module_idx) == 0:\n            # we have already initialized the first column.\n            # No need to call add_column here.\n            self.task_to_module_idx[task_label] = 0\n        else:\n            self.task_to_module_idx[task_label] = self.num_columns\n            self._add_column()",
  "def _add_column(self):\n        \"\"\" Add a new column. \"\"\"\n        # Freeze old parameters\n        for param in self.parameters():\n            param.requires_grad = False\n        self.columns.append(PNNColumn(self.in_features,\n                                      self.out_features_per_column,\n                                      self.num_columns,\n                                      adapter=self.adapter))",
  "def forward_single_task(self, x, task_label):\n        \"\"\" Forward.\n\n        :param x: list of inputs.\n        :param task_label:\n        :return:\n        \"\"\"\n        col_idx = self.task_to_module_idx[task_label]\n        hs = []\n        for ii in range(col_idx + 1):\n            hs.append(self.columns[ii](x[:ii+1]))\n        return hs",
  "def __init__(self, num_layers=1, in_features=784,\n                 hidden_features_per_column=100, adapter='mlp'):\n        \"\"\" Progressive Neural Network.\n\n        The model assumes that each experience is a separate task.\n        Multiple experiences with the same task label or multiple task labels\n        within the same experience will result in a runtime error.\n\n        :param num_layers: number of layers (default=1)\n        :param in_features: size of each input sample\n        :param hidden_features_per_column:\n            number of hidden units for each column\n        :param adapter: adapter type. One of {'linear', 'mlp'} (default='mlp')\n        \"\"\"\n        super().__init__()\n        assert num_layers >= 1\n        self.num_layers = num_layers\n        self.in_features = in_features\n        self.out_features_per_columns = hidden_features_per_column\n\n        self.layers = nn.ModuleList()\n        self.layers.append(PNNLayer(in_features, hidden_features_per_column))\n        for _ in range(num_layers - 1):\n            lay = PNNLayer(hidden_features_per_column,\n                           hidden_features_per_column,\n                           adapter=adapter)\n            self.layers.append(lay)\n        self.classifier = MultiHeadClassifier(hidden_features_per_column)",
  "def forward_single_task(self, x, task_label):\n        \"\"\" Forward.\n\n        :param x:\n        :param task_label:\n        :return:\n        \"\"\"\n        x = x.contiguous()\n        x = x.view(x.size(0), self.in_features)\n\n        num_columns = self.layers[0].num_columns\n        col_idx = self.layers[-1].task_to_module_idx[task_label]\n\n        x = [x for _ in range(num_columns)]\n        for lay in self.layers:\n            x = [F.relu(el) for el in lay(x, task_label)]\n        return self.classifier(x[col_idx], task_label)",
  "def vgg(depth: int, batch_normalization=False, pretrained=False) -> Module:\n    \"\"\"\n    Wrapper for VGG net of verious depths availble in the pytorchcv package.\n    VGG is only availabe for imagenet.\n\n    :param depth: Depth of the model, one of (11, 13, 16, 19)\n    :param batch_normalization: include batch normalizaion layers\n    :param pretrained: loads model pretrained on imagnet\n    \"\"\"\n    available_depths = [11, 13, 16, 19]\n    if depth not in available_depths:\n        raise ValueError(f\"Depth {depth} not available, \"\n                         f\"availble depths are {available_depths}\")\n\n    name = f\"vgg{depth}\"\n    if batch_normalization:\n        name = f\"bn_{name}\"\n\n    return ptcv_get_model(name, pretrained=pretrained)",
  "def resnet(dataset: str, depth: int, pretrained=False) -> Module:\n    \"\"\"\n    Wrapper for (basic) renset available in the pytorchcv package. More variants\n    are availble through the general wrapper.\n\n    :param dataset: One of cifar10, cifar100, svhn, imagenet.\n    :param depth: depth of the architecture, one of (10, 12, 14, 16, 18, 26, 34,\n                  50, 101, 152, 200) for imagenet,\n                  (20, 56, 110, 1001, 1202) for the other datasets.\n    :param pretrained: loads model pretrained on `dataset`.\n    \"\"\"\n\n    if dataset in [\"cifar10\", \"cifar100\", \"svhn\"]:\n        available_depths = [20, 56, 110, 1001, 1202]\n        model_name = f\"resnet{depth}_{dataset}\"\n    elif dataset == \"imagenet\":\n        available_depths = [10, 12, 14, 16, 18, 26, 34, 50, 101, 152, 200]\n        model_name = f\"resnet{depth}\"\n    else:\n        raise ValueError(f\"Unrecognized dataset {dataset}\")\n\n    if depth not in available_depths:\n        raise ValueError(f\"Depth {depth} not available for dataset {dataset}, \"\n                         f\"availble depths are {available_depths}\")\n\n    model = ptcv_get_model(model_name, pretrained=pretrained)\n    return model",
  "def densenet(dataset: str, depth: int, pretrained=False) -> Module:\n    \"\"\"\n    Wrapper for densenet available in the pytorchcv package.\n\n    :param dataset: One of cifar10, cifar100, svhn, imagenet.\n    :param depth: The depth of the densnet. For imagenet depths\n                  (121, 161, 169, 201) are supported. The other datasets\n                   support dephts (40, 100).\n    :param pretrained: load model pretrained on `dataset`..\n    \"\"\"\n    if dataset in [\"cifar10\", \"cifar100\", \"svhn\"]:\n        available_depths = [40, 100]\n        # other growth rates are available through the general method.\n        growth_rate = 12\n        model_name = f\"densenet{depth}_k{growth_rate}_{dataset}\"\n    elif dataset == \"imagenet\":\n        available_depths = [121, 161, 169, 201]\n        model_name = f\"densenet{depth}\"\n    else:\n        raise ValueError(f\"Unrecognized dataset {dataset}\")\n\n    if depth not in available_depths:\n        raise ValueError(f\"Depth {depth} not available for dataset {dataset}, \"\n                         f\"availble depths are {available_depths}\")\n\n    model = ptcv_get_model(model_name, pretrained=pretrained)\n    return model",
  "def pyramidnet(dataset: str, depth: int, pretrained=False) -> Module:\n    \"\"\"\n    Wrapper for pyramidnet available in the pytorchcv package.\n\n    :param dataset: One of cifar10, cifar100, svhn, imagenet.\n    :param depth: The depth of the pyramidnet. For imagenet 101 is supported.\n                  The other datasets support dephts (110, 164, 200, 236, 272).\n    :param pretrained: load model pretrained on `dataset`..\n    \"\"\"\n    if dataset in [\"cifar10\", \"cifar100\", \"svhn\"]:\n        available_depths = [110, 164, 200, 236, 272]\n        alpha = {110: 48, 164: 270, 200: 240, 236: 220, 272: 200}.get(depth)\n        if depth < 200:\n            model_name = f\"pyramidnet{depth}_a{alpha}_{dataset}\"\n        else:\n            # These models have batch normalization\n            model_name = f\"pyramidnet{depth}_a{alpha}_bn_{dataset}\"\n    elif dataset == \"imagenet\":\n        available_depths = [101]\n        alpha = 360\n        model_name = f\"pyramidnet{depth}_a{alpha}\"\n    else:\n        raise ValueError(f\"Unrecognized dataset {dataset}\")\n\n    if depth not in available_depths:\n        raise ValueError(f\"Depth {depth} not available for dataset {dataset}, \"\n                         f\"availble depths are {available_depths}\")\n\n    model = ptcv_get_model(model_name, pretrained=pretrained)\n    return model",
  "def get_model(name: str, pretrained=False):\n    \"\"\"\n    This a direct wrapper to the model getter of `pytorchcv`. For available\n    models see: https://github.com/osmr/imgclsmob\n    \"\"\"\n    return ptcv_get_model(name, pretrained=pretrained)",
  "class SimpleMLP_TinyImageNet(nn.Module):\n\n    def __init__(self, num_classes=200, num_channels=3):\n        super(SimpleMLP_TinyImageNet, self).__init__()\n\n        self.features = nn.Sequential(\n            nn.Linear(num_channels*64*64, 1024),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n        )\n        self.classifier = nn.Linear(1024, num_classes)\n\n    def forward(self, x):\n        x = x.contiguous()\n        x = x.view(x.size(0), -1)\n        x = self.features(x)\n        x = self.classifier(x)\n        return x",
  "def __init__(self, num_classes=200, num_channels=3):\n        super(SimpleMLP_TinyImageNet, self).__init__()\n\n        self.features = nn.Sequential(\n            nn.Linear(num_channels*64*64, 1024),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n        )\n        self.classifier = nn.Linear(1024, num_classes)",
  "def forward(self, x):\n        x = x.contiguous()\n        x = x.view(x.size(0), -1)\n        x = self.features(x)\n        x = self.classifier(x)\n        return x",
  "class DynamicModule(Module):\n    \"\"\"\n        Dynamic Modules are Avalanche modules that can be incrementally\n        expanded to allow architectural modifications (multi-head\n        classifiers, progressive networks, ...).\n\n        Compared to pytoch Modules, they provide an additional method,\n        `model_adaptation`, which adapts the model given data from the\n        current experience.\n    \"\"\"\n\n    def adaptation(self, dataset: AvalancheDataset = None):\n        \"\"\" Adapt the module (freeze units, add units...) using the current\n        data. Optimizers must be updated after the model adaptation.\n\n        Avalanche strategies call this method to adapt the architecture\n        *before* processing each experience. Strategies also update the\n        optimizer automatically.\n\n        .. warning::\n            As a general rule, you should NOT use this method to train the\n            model. The dataset should be used only to check conditions which\n            require the model's adaptation, such as the discovery of new\n            classes or tasks.\n\n        :param dataset: data from the current experience.\n        :return:\n        \"\"\"\n        if self.training:\n            self.train_adaptation(dataset)\n        else:\n            self.eval_adaptation(dataset)\n\n    def train_adaptation(self, dataset: AvalancheDataset):\n        \"\"\" Module's adaptation at training time.\n\n        Avalanche strategies automatically call this method *before* training\n        on each experience.\n        \"\"\"\n        pass\n\n    def eval_adaptation(self, dataset: AvalancheDataset):\n        \"\"\" Module's adaptation at evaluation time.\n\n        Avalanche strategies automatically call this method *before* evaluating\n        on each experience.\n\n        .. warning::\n            This method receives the experience's data at evaluation time\n            because some dynamic models need it for adaptation. For example,\n            an incremental classifier needs to be expanded even at evaluation\n            time if new classes are available. However, you should **never**\n            use this data to **train** the module's parameters.\n        \"\"\"\n        pass",
  "class MultiTaskModule(Module):\n    \"\"\"\n        Multi-task modules are `torch.nn.Modules`s for multi-task\n        scenarios. The `forward` method accepts task labels, one for\n        each sample in the mini-batch.\n\n        By default the `forward` method splits the mini-batch by task\n        and calls `forward_single_task`. Subclasses must implement\n        `forward_single_task` or override `forward.\n    \"\"\"\n\n    def forward(self, x: torch.Tensor, task_labels: torch.Tensor)\\\n            -> torch.Tensor:\n        \"\"\" compute the output given the input `x` and task labels.\n\n        :param x:\n        :param task_labels: task labels for each sample.\n        :return:\n        \"\"\"\n        if isinstance(task_labels, int):\n            # fast path. mini-batch is single task.\n            return self.forward_single_task(x, task_labels)\n        else:\n            unique_tasks = torch.unique(task_labels)\n\n        out = None\n        for task in unique_tasks:\n            task_mask = task_labels == task\n            x_task = x[task_mask]\n            out_task = self.forward_single_task(x_task, task.item())\n\n            if out is None:\n                out = torch.empty(x.shape[0], *out_task.shape[1:],\n                                  device=out_task.device)\n            out[task_mask] = out_task\n        return out\n\n    def forward_single_task(self, x: torch.Tensor, task_label: int)\\\n            -> torch.Tensor:\n        \"\"\" compute the output given the input `x` and task label.\n\n        :param x:\n        :param task_label: a single task label.\n        :return:\n        \"\"\"\n        raise NotImplementedError()",
  "class IncrementalClassifier(DynamicModule):\n    def __init__(self, in_features, initial_out_features=2):\n        \"\"\" Output layer that incrementally adds units whenever new classes are\n        encountered.\n\n        Typically used in class-incremental benchmarks where the number of\n        classes grows over time.\n\n        :param in_features: number of input features.\n        :param initial_out_features: initial number of classes (can be\n            dynamically expanded).\n        \"\"\"\n        super().__init__()\n        self.classifier = torch.nn.Linear(in_features, initial_out_features)\n\n    @torch.no_grad()\n    def adaptation(self, dataset: AvalancheDataset):\n        \"\"\" If `dataset` contains unseen classes the classifier is expanded.\n\n        :param dataset: data from the current experience.\n        :return:\n        \"\"\"\n        in_features = self.classifier.in_features\n        old_nclasses = self.classifier.out_features\n        new_nclasses = max(self.classifier.out_features,\n                           max(dataset.targets) + 1)\n\n        if old_nclasses == new_nclasses:\n            return\n        old_w, old_b = self.classifier.weight, self.classifier.bias\n        self.classifier = torch.nn.Linear(in_features, new_nclasses)\n        self.classifier.weight[:old_nclasses] = old_w\n        self.classifier.bias[:old_nclasses] = old_b\n\n    def forward(self, x, **kwargs):\n        \"\"\" compute the output given the input `x`. This module does not use\n        the task label.\n\n        :param x:\n        :return:\n        \"\"\"\n        return self.classifier(x)",
  "class MultiHeadClassifier(MultiTaskModule, DynamicModule):\n    def __init__(self, in_features, initial_out_features=2):\n        \"\"\" Multi-head classifier with separate heads for each task.\n\n        Typically used in task-incremental benchmarks where task labels are\n        available and provided to the model.\n\n        .. note::\n            Each output head may have a different shape, and the number of\n            classes can be determined automatically.\n\n            However, since pytorch doest not support jagged tensors, when you\n            compute a minibatch's output you must ensure that each sample\n            has the same output size, otherwise the model will fail to\n            concatenate the samples together.\n\n            These can be easily ensured in two possible ways:\n            - each minibatch contains a single task, which is the case in most\n                common benchmarks in Avalanche. Some exceptions to this setting\n                are multi-task replay or cumulative strategies.\n            - each head has the same size, which can be enforced by setting a\n                large enough `initial_out_features`.\n\n        :param in_features: number of input features.\n        :param initial_out_features: initial number of classes (can be\n            dynamically expanded).\n        \"\"\"\n        super().__init__()\n        self.in_features = in_features\n        self.starting_out_features = initial_out_features\n        self.classifiers = torch.nn.ModuleDict()\n\n        # needs to create the first head because pytorch optimizers\n        # fail when model.parameters() is empty.\n        first_head = IncrementalClassifier(self.in_features,\n                                           self.starting_out_features)\n        self.classifiers['0'] = first_head\n\n    def adaptation(self, dataset: AvalancheDataset):\n        \"\"\" If `dataset` contains new tasks, a new head is initialized.\n\n        :param dataset: data from the current experience.\n        :return:\n        \"\"\"\n        task_labels = dataset.targets_task_labels\n        if isinstance(task_labels, ConstantSequence):\n            # task label is unique. Don't check duplicates.\n            task_labels = [task_labels[0]]\n\n        for tid in set(task_labels):\n            tid = str(tid)  # need str keys\n            if tid not in self.classifiers:\n                new_head = IncrementalClassifier(self.in_features,\n                                                 self.starting_out_features)\n                new_head.adaptation(dataset)\n                self.classifiers[tid] = new_head\n\n    def forward_single_task(self, x, task_label):\n        \"\"\" compute the output given the input `x`. This module uses the task\n        label to activate the correct head.\n\n        :param x:\n        :param task_label:\n        :return:\n        \"\"\"\n        return self.classifiers[str(task_label)](x)",
  "class TrainEvalModel(DynamicModule):\n    \"\"\"\n        TrainEvalModel.\n        This module allows to wrap together a common feature extractor and\n        two classifiers: one used during training time and another\n        used at test time. The classifier is switched when `self.adaptation()`\n        is called.\n    \"\"\"\n    def __init__(self, feature_extractor, train_classifier, eval_classifier):\n        \"\"\"\n        :param feature_extractor: a differentiable feature extractor\n        :param train_classifier: a differentiable classifier used\n            during training\n        :param eval_classifier: a classifier used during testing.\n            Doesn't have to be differentiable.\n        \"\"\"\n        super().__init__()\n        self.feature_extractor = feature_extractor\n        self.train_classifier = train_classifier\n        self.eval_classifier = eval_classifier\n\n        self.classifier = train_classifier\n\n    def forward(self, x):\n        x = self.feature_extractor(x)\n        return self.classifier(x)\n\n    def train_adaptation(self, dataset: AvalancheDataset = None):\n        self.classifier = self.train_classifier\n\n    def eval_adaptation(self, dataset: AvalancheDataset = None):\n        self.classifier = self.eval_classifier",
  "def adaptation(self, dataset: AvalancheDataset = None):\n        \"\"\" Adapt the module (freeze units, add units...) using the current\n        data. Optimizers must be updated after the model adaptation.\n\n        Avalanche strategies call this method to adapt the architecture\n        *before* processing each experience. Strategies also update the\n        optimizer automatically.\n\n        .. warning::\n            As a general rule, you should NOT use this method to train the\n            model. The dataset should be used only to check conditions which\n            require the model's adaptation, such as the discovery of new\n            classes or tasks.\n\n        :param dataset: data from the current experience.\n        :return:\n        \"\"\"\n        if self.training:\n            self.train_adaptation(dataset)\n        else:\n            self.eval_adaptation(dataset)",
  "def train_adaptation(self, dataset: AvalancheDataset):\n        \"\"\" Module's adaptation at training time.\n\n        Avalanche strategies automatically call this method *before* training\n        on each experience.\n        \"\"\"\n        pass",
  "def eval_adaptation(self, dataset: AvalancheDataset):\n        \"\"\" Module's adaptation at evaluation time.\n\n        Avalanche strategies automatically call this method *before* evaluating\n        on each experience.\n\n        .. warning::\n            This method receives the experience's data at evaluation time\n            because some dynamic models need it for adaptation. For example,\n            an incremental classifier needs to be expanded even at evaluation\n            time if new classes are available. However, you should **never**\n            use this data to **train** the module's parameters.\n        \"\"\"\n        pass",
  "def forward(self, x: torch.Tensor, task_labels: torch.Tensor)\\\n            -> torch.Tensor:\n        \"\"\" compute the output given the input `x` and task labels.\n\n        :param x:\n        :param task_labels: task labels for each sample.\n        :return:\n        \"\"\"\n        if isinstance(task_labels, int):\n            # fast path. mini-batch is single task.\n            return self.forward_single_task(x, task_labels)\n        else:\n            unique_tasks = torch.unique(task_labels)\n\n        out = None\n        for task in unique_tasks:\n            task_mask = task_labels == task\n            x_task = x[task_mask]\n            out_task = self.forward_single_task(x_task, task.item())\n\n            if out is None:\n                out = torch.empty(x.shape[0], *out_task.shape[1:],\n                                  device=out_task.device)\n            out[task_mask] = out_task\n        return out",
  "def forward_single_task(self, x: torch.Tensor, task_label: int)\\\n            -> torch.Tensor:\n        \"\"\" compute the output given the input `x` and task label.\n\n        :param x:\n        :param task_label: a single task label.\n        :return:\n        \"\"\"\n        raise NotImplementedError()",
  "def __init__(self, in_features, initial_out_features=2):\n        \"\"\" Output layer that incrementally adds units whenever new classes are\n        encountered.\n\n        Typically used in class-incremental benchmarks where the number of\n        classes grows over time.\n\n        :param in_features: number of input features.\n        :param initial_out_features: initial number of classes (can be\n            dynamically expanded).\n        \"\"\"\n        super().__init__()\n        self.classifier = torch.nn.Linear(in_features, initial_out_features)",
  "def adaptation(self, dataset: AvalancheDataset):\n        \"\"\" If `dataset` contains unseen classes the classifier is expanded.\n\n        :param dataset: data from the current experience.\n        :return:\n        \"\"\"\n        in_features = self.classifier.in_features\n        old_nclasses = self.classifier.out_features\n        new_nclasses = max(self.classifier.out_features,\n                           max(dataset.targets) + 1)\n\n        if old_nclasses == new_nclasses:\n            return\n        old_w, old_b = self.classifier.weight, self.classifier.bias\n        self.classifier = torch.nn.Linear(in_features, new_nclasses)\n        self.classifier.weight[:old_nclasses] = old_w\n        self.classifier.bias[:old_nclasses] = old_b",
  "def forward(self, x, **kwargs):\n        \"\"\" compute the output given the input `x`. This module does not use\n        the task label.\n\n        :param x:\n        :return:\n        \"\"\"\n        return self.classifier(x)",
  "def __init__(self, in_features, initial_out_features=2):\n        \"\"\" Multi-head classifier with separate heads for each task.\n\n        Typically used in task-incremental benchmarks where task labels are\n        available and provided to the model.\n\n        .. note::\n            Each output head may have a different shape, and the number of\n            classes can be determined automatically.\n\n            However, since pytorch doest not support jagged tensors, when you\n            compute a minibatch's output you must ensure that each sample\n            has the same output size, otherwise the model will fail to\n            concatenate the samples together.\n\n            These can be easily ensured in two possible ways:\n            - each minibatch contains a single task, which is the case in most\n                common benchmarks in Avalanche. Some exceptions to this setting\n                are multi-task replay or cumulative strategies.\n            - each head has the same size, which can be enforced by setting a\n                large enough `initial_out_features`.\n\n        :param in_features: number of input features.\n        :param initial_out_features: initial number of classes (can be\n            dynamically expanded).\n        \"\"\"\n        super().__init__()\n        self.in_features = in_features\n        self.starting_out_features = initial_out_features\n        self.classifiers = torch.nn.ModuleDict()\n\n        # needs to create the first head because pytorch optimizers\n        # fail when model.parameters() is empty.\n        first_head = IncrementalClassifier(self.in_features,\n                                           self.starting_out_features)\n        self.classifiers['0'] = first_head",
  "def adaptation(self, dataset: AvalancheDataset):\n        \"\"\" If `dataset` contains new tasks, a new head is initialized.\n\n        :param dataset: data from the current experience.\n        :return:\n        \"\"\"\n        task_labels = dataset.targets_task_labels\n        if isinstance(task_labels, ConstantSequence):\n            # task label is unique. Don't check duplicates.\n            task_labels = [task_labels[0]]\n\n        for tid in set(task_labels):\n            tid = str(tid)  # need str keys\n            if tid not in self.classifiers:\n                new_head = IncrementalClassifier(self.in_features,\n                                                 self.starting_out_features)\n                new_head.adaptation(dataset)\n                self.classifiers[tid] = new_head",
  "def forward_single_task(self, x, task_label):\n        \"\"\" compute the output given the input `x`. This module uses the task\n        label to activate the correct head.\n\n        :param x:\n        :param task_label:\n        :return:\n        \"\"\"\n        return self.classifiers[str(task_label)](x)",
  "def __init__(self, feature_extractor, train_classifier, eval_classifier):\n        \"\"\"\n        :param feature_extractor: a differentiable feature extractor\n        :param train_classifier: a differentiable classifier used\n            during training\n        :param eval_classifier: a classifier used during testing.\n            Doesn't have to be differentiable.\n        \"\"\"\n        super().__init__()\n        self.feature_extractor = feature_extractor\n        self.train_classifier = train_classifier\n        self.eval_classifier = eval_classifier\n\n        self.classifier = train_classifier",
  "def forward(self, x):\n        x = self.feature_extractor(x)\n        return self.classifier(x)",
  "def train_adaptation(self, dataset: AvalancheDataset = None):\n        self.classifier = self.train_classifier",
  "def eval_adaptation(self, dataset: AvalancheDataset = None):\n        self.classifier = self.eval_classifier",
  "def reset_optimizer(optimizer, model):\n    \"\"\" Reset the optimizer to update the list of learnable parameters.\n\n    .. warning::\n        This function fails if the optimizer uses multiple parameter groups.\n\n    :param optimizer:\n    :param model:\n    :return:\n    \"\"\"\n    assert len(optimizer.param_groups) == 1\n    optimizer.state = defaultdict(dict)\n    optimizer.param_groups[0]['params'] = list(model.parameters())",
  "def update_optimizer(optimizer, old_params, new_params, reset_state=True):\n    \"\"\" Update the optimizer by substituting old_params with new_params.\n\n    :param old_params: List of old trainable parameters.\n    :param new_params: List of new trainable parameters.\n    :param reset_state: Wheter to reset the optimizer's state.\n        Defaults to True.\n    :return:\n    \"\"\"\n    for old_p, new_p in zip(old_params, new_params):\n        found = False\n        # iterate over group and params for each group.\n        for group in optimizer.param_groups:\n            for i, curr_p in enumerate(group['params']):\n                if hash(curr_p) == hash(old_p):\n                    # update parameter reference\n                    group['params'][i] = new_p\n                    found = True\n                    break\n            if found:\n                break\n        if not found:\n            raise Exception(f\"Parameter {old_params} not found in the \"\n                            f\"current optimizer.\")\n    if reset_state:\n        # State contains parameter-specific information.\n        # We reset it because the model is (probably) changed.\n        optimizer.state = defaultdict(dict)",
  "def add_new_params_to_optimizer(optimizer, new_params):\n    \"\"\" Add new parameters to the trainable parameters.\n\n    :param new_params: list of trainable parameters\n    \"\"\"\n    optimizer.add_param_group({'params': new_params})",
  "class IdentityShortcut(Module):\n    def __init__(self, transform_function: Callable[[Tensor], Tensor]):\n        super(IdentityShortcut, self).__init__()\n        self.transform_function = transform_function\n\n    def forward(self, x: Tensor) -> Tensor:\n        return self.transform_function(x)",
  "def conv3x3(in_planes: int, out_planes: int,\n            stride: Union[int, Sequence[int]] = 1):\n    return Conv2d(in_planes, out_planes,\n                  kernel_size=3, stride=stride, padding=1, bias=False)",
  "def batch_norm(num_channels: int) -> BatchNorm2d:\n    return BatchNorm2d(num_channels)",
  "class ResidualBlock(Module):\n\n    def __init__(self, input_num_filters: int,\n                 increase_dim: bool = False,\n                 projection: bool = False,\n                 last: bool = False):\n        super().__init__()\n        self.last: bool = last\n\n        if increase_dim:\n            first_stride = (2, 2)\n            out_num_filters = input_num_filters * 2\n        else:\n            first_stride = (1, 1)\n            out_num_filters = input_num_filters\n\n        self.direct = Sequential(\n            conv3x3(input_num_filters, out_num_filters, stride=first_stride),\n            batch_norm(out_num_filters),\n            ReLU(True),\n            conv3x3(out_num_filters, out_num_filters, stride=(1, 1)),\n            batch_norm(out_num_filters),\n        )\n\n        self.shortcut: Module\n\n        # add shortcut connections\n        if increase_dim:\n            if projection:\n                # projection shortcut, as option B in paper\n                self.shortcut = Sequential(\n                    Conv2d(input_num_filters, out_num_filters,\n                           kernel_size=(1, 1), stride=(2, 2), bias=False),\n                    batch_norm(out_num_filters)\n                )\n            else:\n                # identity shortcut, as option A in paper\n                self.shortcut = Sequential(\n                    IdentityShortcut(lambda x: x[:, :, ::2, ::2]),\n                    ConstantPad3d((0, 0, 0, 0,\n                                   out_num_filters // 4,\n                                   out_num_filters // 4), 0.0)\n                )\n        else:\n            self.shortcut = Identity()\n\n    def forward(self, x):\n        if self.last:\n            return self.direct(x) + self.shortcut(x)\n        else:\n            return torch.relu(self.direct(x) + self.shortcut(x))",
  "class IcarlNet(Module):\n    def __init__(self, num_classes: int, n=5, c=3):\n        super().__init__()\n\n        self.is_train = True\n        input_dims = c\n        output_dims = 16\n\n        first_conv = Sequential(\n            conv3x3(input_dims, output_dims, stride=(1, 1)),\n            batch_norm(16),\n            ReLU(True)\n        )\n\n        input_dims = output_dims\n        output_dims = 16\n\n        # first stack of residual blocks, output is 16 x 32 x 32\n        layers_list = []\n        for _ in range(n):\n            layers_list.append(ResidualBlock(input_dims))\n        first_block = Sequential(*layers_list)\n\n        input_dims = output_dims\n        output_dims = 32\n\n        # second stack of residual blocks, output is 32 x 16 x 16\n        layers_list = [ResidualBlock(input_dims, increase_dim=True)]\n        for _ in range(1, n):\n            layers_list.append(ResidualBlock(output_dims))\n        second_block = Sequential(*layers_list)\n\n        input_dims = output_dims\n        output_dims = 64\n\n        # third stack of residual blocks, output is 64 x 8 x 8\n        layers_list = [ResidualBlock(input_dims, increase_dim=True)]\n        for _ in range(1, n-1):\n            layers_list.append(ResidualBlock(output_dims))\n        layers_list.append(ResidualBlock(output_dims, last=True))\n        third_block = Sequential(*layers_list)\n        final_pool = AdaptiveAvgPool2d(output_size=(1, 1))\n\n        self.feature_extractor = Sequential(\n            first_conv, first_block,\n            second_block, third_block,\n            final_pool, Flatten())\n\n        input_dims = output_dims\n        output_dims = num_classes\n\n        self.classifier = Linear(input_dims, output_dims)\n\n    def forward(self, x):\n        x = self.feature_extractor(x)  # Already flattened\n        x = self.classifier(x)\n        return x",
  "def make_icarl_net(num_classes: int, n=5, c=3) -> IcarlNet:\n    return IcarlNet(num_classes, n=n, c=c)",
  "def initialize_icarl_net(m: Module):\n    if isinstance(m, Conv2d):\n        kaiming_normal_(m.weight.data, mode='fan_in', nonlinearity='relu')\n        if m.bias is not None:\n            zeros_(m.bias.data)\n\n    elif isinstance(m, Linear):\n        kaiming_normal_(m.weight.data, mode='fan_in', nonlinearity='sigmoid')\n        if m.bias is not None:\n            zeros_(m.bias.data)",
  "def __init__(self, transform_function: Callable[[Tensor], Tensor]):\n        super(IdentityShortcut, self).__init__()\n        self.transform_function = transform_function",
  "def forward(self, x: Tensor) -> Tensor:\n        return self.transform_function(x)",
  "def __init__(self, input_num_filters: int,\n                 increase_dim: bool = False,\n                 projection: bool = False,\n                 last: bool = False):\n        super().__init__()\n        self.last: bool = last\n\n        if increase_dim:\n            first_stride = (2, 2)\n            out_num_filters = input_num_filters * 2\n        else:\n            first_stride = (1, 1)\n            out_num_filters = input_num_filters\n\n        self.direct = Sequential(\n            conv3x3(input_num_filters, out_num_filters, stride=first_stride),\n            batch_norm(out_num_filters),\n            ReLU(True),\n            conv3x3(out_num_filters, out_num_filters, stride=(1, 1)),\n            batch_norm(out_num_filters),\n        )\n\n        self.shortcut: Module\n\n        # add shortcut connections\n        if increase_dim:\n            if projection:\n                # projection shortcut, as option B in paper\n                self.shortcut = Sequential(\n                    Conv2d(input_num_filters, out_num_filters,\n                           kernel_size=(1, 1), stride=(2, 2), bias=False),\n                    batch_norm(out_num_filters)\n                )\n            else:\n                # identity shortcut, as option A in paper\n                self.shortcut = Sequential(\n                    IdentityShortcut(lambda x: x[:, :, ::2, ::2]),\n                    ConstantPad3d((0, 0, 0, 0,\n                                   out_num_filters // 4,\n                                   out_num_filters // 4), 0.0)\n                )\n        else:\n            self.shortcut = Identity()",
  "def forward(self, x):\n        if self.last:\n            return self.direct(x) + self.shortcut(x)\n        else:\n            return torch.relu(self.direct(x) + self.shortcut(x))",
  "def __init__(self, num_classes: int, n=5, c=3):\n        super().__init__()\n\n        self.is_train = True\n        input_dims = c\n        output_dims = 16\n\n        first_conv = Sequential(\n            conv3x3(input_dims, output_dims, stride=(1, 1)),\n            batch_norm(16),\n            ReLU(True)\n        )\n\n        input_dims = output_dims\n        output_dims = 16\n\n        # first stack of residual blocks, output is 16 x 32 x 32\n        layers_list = []\n        for _ in range(n):\n            layers_list.append(ResidualBlock(input_dims))\n        first_block = Sequential(*layers_list)\n\n        input_dims = output_dims\n        output_dims = 32\n\n        # second stack of residual blocks, output is 32 x 16 x 16\n        layers_list = [ResidualBlock(input_dims, increase_dim=True)]\n        for _ in range(1, n):\n            layers_list.append(ResidualBlock(output_dims))\n        second_block = Sequential(*layers_list)\n\n        input_dims = output_dims\n        output_dims = 64\n\n        # third stack of residual blocks, output is 64 x 8 x 8\n        layers_list = [ResidualBlock(input_dims, increase_dim=True)]\n        for _ in range(1, n-1):\n            layers_list.append(ResidualBlock(output_dims))\n        layers_list.append(ResidualBlock(output_dims, last=True))\n        third_block = Sequential(*layers_list)\n        final_pool = AdaptiveAvgPool2d(output_size=(1, 1))\n\n        self.feature_extractor = Sequential(\n            first_conv, first_block,\n            second_block, third_block,\n            final_pool, Flatten())\n\n        input_dims = output_dims\n        output_dims = num_classes\n\n        self.classifier = Linear(input_dims, output_dims)",
  "def forward(self, x):\n        x = self.feature_extractor(x)  # Already flattened\n        x = self.classifier(x)\n        return x",
  "class SLDAResNetModel(nn.Module):\n    \"\"\"\n    This is a model wrapper to reproduce experiments from the original\n    paper of Deep Streaming Linear Discriminant Analysis by using\n    a pretrained ResNet model.\n    \"\"\"\n\n    def __init__(self, arch='resnet18', output_layer_name='layer4.1',\n                 imagenet_pretrained=True, device='cpu'):\n        \"\"\"\n        :param arch: backbone architecture (default is resnet-18, but others\n        can be used by modifying layer for\n        feature extraction in `self.feature_extraction_wrapper'\n        :param imagenet_pretrained: True if initializing backbone with imagenet\n        pre-trained weights else False\n        :param output_layer_name: name of the layer from feature extractor\n        :param device: cpu, gpu or other device\n        \"\"\"\n\n        super(SLDAResNetModel, self).__init__()\n\n        feat_extractor = models.__dict__[arch](\n            pretrained=imagenet_pretrained).to(device).eval()\n        self.feature_extraction_wrapper = FeatureExtractorBackbone(\n            feat_extractor, output_layer_name).eval()\n\n    @staticmethod\n    def pool_feat(features):\n        feat_size = features.shape[-1]\n        num_channels = features.shape[1]\n        features2 = features.permute(0, 2, 3,\n                                     1)  # 1 x feat_size x feat_size x\n        # num_channels\n        features3 = torch.reshape(features2, (\n            features.shape[0], feat_size * feat_size, num_channels))\n        feat = features3.mean(1)  # mb x num_channels\n        return feat\n\n    def forward(self, x):\n        \"\"\"\n        :param x: raw x data\n        \"\"\"\n        feat = self.feature_extraction_wrapper(x)\n        feat = SLDAResNetModel.pool_feat(feat)\n        return feat",
  "def __init__(self, arch='resnet18', output_layer_name='layer4.1',\n                 imagenet_pretrained=True, device='cpu'):\n        \"\"\"\n        :param arch: backbone architecture (default is resnet-18, but others\n        can be used by modifying layer for\n        feature extraction in `self.feature_extraction_wrapper'\n        :param imagenet_pretrained: True if initializing backbone with imagenet\n        pre-trained weights else False\n        :param output_layer_name: name of the layer from feature extractor\n        :param device: cpu, gpu or other device\n        \"\"\"\n\n        super(SLDAResNetModel, self).__init__()\n\n        feat_extractor = models.__dict__[arch](\n            pretrained=imagenet_pretrained).to(device).eval()\n        self.feature_extraction_wrapper = FeatureExtractorBackbone(\n            feat_extractor, output_layer_name).eval()",
  "def pool_feat(features):\n        feat_size = features.shape[-1]\n        num_channels = features.shape[1]\n        features2 = features.permute(0, 2, 3,\n                                     1)  # 1 x feat_size x feat_size x\n        # num_channels\n        features3 = torch.reshape(features2, (\n            features.shape[0], feat_size * feat_size, num_channels))\n        feat = features3.mean(1)  # mb x num_channels\n        return feat",
  "def forward(self, x):\n        \"\"\"\n        :param x: raw x data\n        \"\"\"\n        feat = self.feature_extraction_wrapper(x)\n        feat = SLDAResNetModel.pool_feat(feat)\n        return feat",
  "class ICaRLLossPlugin(StrategyPlugin):\n    \"\"\"\n    ICaRLLossPlugin\n    Similar to the Knowledge Distillation Loss. Works as follows:\n        The target is constructed by taking the one-hot vector target for the\n        current sample and assigning to the position corresponding to the\n        past classes the output of the old model on the current sample.\n        Doesn't work if classes observed in previous experiences might be\n        observed again in future training experiences.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.criterion = BCELoss()\n\n        self.old_classes = []\n        self.old_model = None\n        self.old_logits = None\n\n    def before_forward(self, strategy, **kwargs):\n        if self.old_model is not None:\n            with torch.no_grad():\n                self.old_logits = self.old_model(strategy.mb_x)\n\n    def __call__(self, logits, targets):\n        predictions = torch.sigmoid(logits)\n\n        one_hot = torch.zeros(targets.shape[0], logits.shape[1],\n                              dtype=torch.float, device=logits.device)\n        one_hot[range(len(targets)), targets.long()] = 1\n\n        if self.old_logits is not None:\n            old_predictions = torch.sigmoid(self.old_logits)\n            one_hot[:, self.old_classes] = old_predictions[:, self.old_classes]\n            self.old_logits = None\n\n        return self.criterion(predictions, one_hot)\n\n    def after_training_exp(self, strategy, **kwargs):\n        if self.old_model is None:\n            old_model = copy.deepcopy(strategy.model)\n            old_model.eval()\n            self.old_model = old_model.to(strategy.device)\n\n        self.old_model.load_state_dict(strategy.model.state_dict())\n\n        self.old_classes += np.unique(\n            strategy.experience.dataset.targets).tolist()",
  "def __init__(self):\n        super().__init__()\n        self.criterion = BCELoss()\n\n        self.old_classes = []\n        self.old_model = None\n        self.old_logits = None",
  "def before_forward(self, strategy, **kwargs):\n        if self.old_model is not None:\n            with torch.no_grad():\n                self.old_logits = self.old_model(strategy.mb_x)",
  "def __call__(self, logits, targets):\n        predictions = torch.sigmoid(logits)\n\n        one_hot = torch.zeros(targets.shape[0], logits.shape[1],\n                              dtype=torch.float, device=logits.device)\n        one_hot[range(len(targets)), targets.long()] = 1\n\n        if self.old_logits is not None:\n            old_predictions = torch.sigmoid(self.old_logits)\n            one_hot[:, self.old_classes] = old_predictions[:, self.old_classes]\n            self.old_logits = None\n\n        return self.criterion(predictions, one_hot)",
  "def after_training_exp(self, strategy, **kwargs):\n        if self.old_model is None:\n            old_model = copy.deepcopy(strategy.model)\n            old_model.eval()\n            self.old_model = old_model.to(strategy.device)\n\n        self.old_model.load_state_dict(strategy.model.state_dict())\n\n        self.old_classes += np.unique(\n            strategy.experience.dataset.targets).tolist()",
  "def load_all_dataset(dataset: Dataset, num_workers: int = 0):\n    \"\"\"\n    Retrieves the contents of a whole dataset by using a DataLoader\n\n    :param dataset: The dataset\n    :param num_workers: The number of workers the DataLoader should use.\n        Defaults to 0.\n    :return: The content of the whole Dataset\n    \"\"\"\n    # DataLoader parallelism is batch-based. By using \"len(dataset)/num_workers\"\n    # as the batch size, num_workers [+1] batches will be loaded thus\n    # using the required number of workers.\n    if num_workers > 0:\n        batch_size = max(1, len(dataset) // num_workers)\n    else:\n        batch_size = len(dataset)\n    loader = DataLoader(dataset, batch_size=batch_size, drop_last=False,\n                        num_workers=num_workers)\n    has_task_labels = False\n    batches_x = []\n    batches_y = []\n    batches_t = []\n    for loaded_element in loader:\n        batches_x.append(loaded_element[0])\n        batches_y.append(loaded_element[1])\n        if len(loaded_element) > 2:\n            has_task_labels = True\n            batches_t.append(loaded_element[2])\n\n    x, y = torch.cat(batches_x), torch.cat(batches_y)\n\n    if has_task_labels:\n        t = torch.cat(batches_t)\n        return x, y, t\n    else:\n        return x, y",
  "def zerolike_params_dict(model):\n    \"\"\"\n    Create a list of (name, parameter), where parameter is initalized to zero.\n    The list has as many parameters as model, with the same size.\n\n    :param model: a pytorch model\n    \"\"\"\n\n    return [(k, torch.zeros_like(p).to(p.device))\n            for k, p in model.named_parameters()]",
  "def copy_params_dict(model, copy_grad=False):\n    \"\"\"\n    Create a list of (name, parameter), where parameter is copied from model.\n    The list has as many parameters as model, with the same size.\n\n    :param model: a pytorch model\n    :param copy_grad: if True returns gradients instead of parameter values\n    \"\"\"\n\n    if copy_grad:\n        return [(k, p.grad.data.clone()) for k, p in model.named_parameters()]\n    else:\n        return [(k, p.data.clone()) for k, p in model.named_parameters()]",
  "class LayerAndParameter(NamedTuple):\n    layer_name: str\n    layer: Module\n    parameter_name: str\n    parameter: Tensor",
  "def get_layers_and_params(model: Module, prefix='') -> List[LayerAndParameter]:\n    result: List[LayerAndParameter] = []\n    for param_name, param in model.named_parameters(recurse=False):\n        result.append(LayerAndParameter(\n            prefix[:-1], model, prefix + param_name, param))\n\n    layer_name: str\n    layer: Module\n    for layer_name, layer in model.named_modules():\n        if layer == model:\n            continue\n\n        layer_complete_name = prefix + layer_name + '.'\n\n        result += get_layers_and_params(layer, prefix=layer_complete_name)\n\n    return result",
  "def get_layer_by_name(model: Module, layer_name: str) -> Optional[Module]:\n    for layer_param in get_layers_and_params(model):\n        if layer_param.layer_name == layer_name:\n            return layer_param.layer\n    return None",
  "def get_last_fc_layer(model: Module) -> Optional[Tuple[str, Linear]]:\n    last_fc = None\n    for layer_name, layer in model.named_modules():\n        if isinstance(layer, Linear):\n            last_fc = (layer_name, layer)\n\n    return last_fc",
  "def swap_last_fc_layer(model: Module, new_layer: Module) -> None:\n    last_fc_name, last_fc_layer = get_last_fc_layer(model)\n    setattr(model, last_fc_name, new_layer)",
  "def adapt_classification_layer(model: Module, num_classes: int,\n                               bias: bool = None) -> Tuple[str, Linear]:\n    last_fc_layer: Linear\n    last_fc_name, last_fc_layer = get_last_fc_layer(model)\n\n    if bias is not None:\n        use_bias = bias\n    else:\n        use_bias = last_fc_layer.bias is not None\n\n    new_fc = Linear(last_fc_layer.in_features, num_classes, bias=use_bias)\n    swap_last_fc_layer(model, new_fc)\n    return last_fc_name, new_fc",
  "def replace_bn_with_brn(m: Module, momentum=0.1, r_d_max_inc_step=0.0001,\n                        r_max=1.0, d_max=0.0, max_r_max=3.0, max_d_max=5.0):\n    for attr_str in dir(m):\n        target_attr = getattr(m, attr_str)\n        if type(target_attr) == torch.nn.BatchNorm2d:\n            # print('replaced: ', name, attr_str)\n            setattr(m, attr_str,\n                    BatchRenorm2D(\n                        target_attr.num_features,\n                        gamma=target_attr.weight,\n                        beta=target_attr.bias,\n                        running_mean=target_attr.running_mean,\n                        running_var=target_attr.running_var,\n                        eps=target_attr.eps,\n                        momentum=momentum,\n                        r_d_max_inc_step=r_d_max_inc_step,\n                        r_max=r_max,\n                        d_max=d_max,\n                        max_r_max=max_r_max,\n                        max_d_max=max_d_max\n                    )\n                    )\n    for n, ch in m.named_children():\n        replace_bn_with_brn(ch, momentum, r_d_max_inc_step, r_max, d_max,\n                            max_r_max, max_d_max)",
  "def change_brn_pars(\n        m: Module, momentum=0.1, r_d_max_inc_step=0.0001, r_max=1.0,\n        d_max=0.0):\n    for attr_str in dir(m):\n        target_attr = getattr(m, attr_str)\n        if type(target_attr) == BatchRenorm2D:\n            target_attr.momentum = torch.tensor((momentum), requires_grad=False)\n            target_attr.r_max = torch.tensor(r_max, requires_grad=False)\n            target_attr.d_max = torch.tensor(d_max, requires_grad=False)\n            target_attr.r_d_max_inc_step = r_d_max_inc_step\n\n    for n, ch in m.named_children():\n        change_brn_pars(ch, momentum, r_d_max_inc_step, r_max, d_max)",
  "def freeze_everything(model: Module, set_eval_mode: bool = True):\n    if set_eval_mode:\n        model.eval()\n\n    for layer_param in get_layers_and_params(model):\n        layer_param.parameter.requires_grad = False",
  "def unfreeze_everything(model: Module, set_train_mode: bool = True):\n    if set_train_mode:\n        model.train()\n\n    for layer_param in get_layers_and_params(model):\n        layer_param.parameter.requires_grad = True",
  "def freeze_up_to(model: Module,\n                 freeze_until_layer: str = None,\n                 set_eval_mode: bool = True,\n                 set_requires_grad_false: bool = True,\n                 layer_filter: Callable[[LayerAndParameter], bool] = None,\n                 module_prefix: str = ''):\n    \"\"\"\n    A simple utility that can be used to freeze a model.\n\n    :param model: The model.\n    :param freeze_until_layer: If not None, the freezing algorithm will continue\n        (proceeding from the input towards the output) until the specified layer\n        is encountered. The given layer is excluded from the freezing procedure.\n    :param set_eval_mode: If True, the frozen layers will be set in eval mode.\n        Defaults to True.\n    :param set_requires_grad_false: If True, the autograd engine will be\n        disabled for frozen parameters. Defaults to True.\n    :param layer_filter: A function that, given a :class:`LayerParameter`,\n        returns `True` if the parameter must be frozen. If all parameters of\n        a layer are frozen, then the layer will be set in eval mode (according\n        to the `set_eval_mode` parameter. Defaults to None, which means that all\n        parameters will be frozen.\n    :param module_prefix: The model prefix. Do not use if non strictly\n        necessary.\n    :return:\n    \"\"\"\n\n    frozen_layers = set()\n    frozen_parameters = set()\n\n    to_freeze_layers = dict()\n    for param_def in get_layers_and_params(model, prefix=module_prefix):\n        if freeze_until_layer is not None and \\\n                freeze_until_layer == param_def.layer_name:\n            break\n\n        freeze_param = layer_filter is None or layer_filter(param_def)\n        if freeze_param:\n            if set_requires_grad_false:\n                param_def.parameter.requires_grad = False\n                frozen_parameters.add(param_def.parameter_name)\n\n            if param_def.layer_name not in to_freeze_layers:\n                to_freeze_layers[param_def.layer_name] = (True, param_def.layer)\n        else:\n            # Don't freeze this parameter -> do not set eval on the layer\n            to_freeze_layers[param_def.layer_name] = (False, None)\n\n    if set_eval_mode:\n        for layer_name, layer_result in to_freeze_layers.items():\n            if layer_result[0]:\n                layer_result[1].eval()\n                frozen_layers.add(layer_name)\n\n    return frozen_layers, frozen_parameters",
  "def examples_per_class(targets):\n    result = defaultdict(int)\n\n    unique_classes, examples_count = torch.unique(\n        torch.as_tensor(targets), return_counts=True)\n    for unique_idx in range(len(unique_classes)):\n        result[int(unique_classes[unique_idx])] = \\\n            int(examples_count[unique_idx])\n\n    return result",
  "class BaseStrategy:\n    DISABLED_CALLBACKS: Sequence[str] = ()\n\n    def __init__(self, model: Module, optimizer: Optimizer,\n                 criterion=CrossEntropyLoss(),\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = 1, device='cpu',\n                 plugins: Optional[Sequence['StrategyPlugin']] = None,\n                 evaluator=default_logger, eval_every=-1):\n        \"\"\"\n        BaseStrategy is the super class of all task-based continual learning\n        strategies. It implements a basic training loop and callback system\n        that allows to execute code at each experience of the training loop.\n        Plugins can be used to implement callbacks to augment the training\n        loop with additional behavior (e.g. a memory buffer for replay).\n\n        **Scenarios**\n        This strategy supports several continual learning scenarios:\n\n        * class-incremental scenarios (no task labels)\n        * multi-task scenarios, where task labels are provided)\n        * multi-incremental scenarios, where the same task may be revisited\n\n        The exact scenario depends on the data stream and whether it provides\n        the task labels.\n\n        **Training loop**\n        The training loop is organized as follows::\n            train\n                train_exp  # for each experience\n                    adapt_train_dataset\n                    train_dataset_adaptation\n                    make_train_dataloader\n                    train_epoch  # for each epoch\n                        # forward\n                        # backward\n                        # model update\n\n        **Evaluation loop**\n        The evaluation loop is organized as follows::\n            eval\n                eval_exp  # for each experience\n                    adapt_eval_dataset\n                    eval_dataset_adaptation\n                    make_eval_dataloader\n                    eval_epoch  # for each epoch\n                        # forward\n                        # backward\n                        # model update\n\n        :param model: PyTorch model.\n        :param optimizer: PyTorch optimizer.\n        :param criterion: loss function.\n        :param train_mb_size: mini-batch size for training.\n        :param train_epochs: number of training epochs.\n        :param eval_mb_size: mini-batch size for eval.\n        :param device: PyTorch device where the model will be allocated.\n        :param plugins: (optional) list of StrategyPlugins.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations. None to remove logging.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience and before training on the first experience.\n                if >0: calls `eval` every `eval_every` epochs, at the end\n                    of all the epochs for a single experience and before\n                    training on the first experience.\n        \"\"\"\n        self._criterion = criterion\n\n        self.model: Module = model\n        \"\"\" PyTorch model. \"\"\"\n\n        self.optimizer = optimizer\n        \"\"\" PyTorch optimizer. \"\"\"\n\n        self.train_epochs: int = train_epochs\n        \"\"\" Number of training epochs. \"\"\"\n\n        self.train_mb_size: int = train_mb_size\n        \"\"\" Training mini-batch size. \"\"\"\n\n        self.eval_mb_size: int = train_mb_size if eval_mb_size is None \\\n            else eval_mb_size\n        \"\"\" Eval mini-batch size. \"\"\"\n\n        self.device = device\n        \"\"\" PyTorch device where the model will be allocated. \"\"\"\n\n        self.plugins = [] if plugins is None else plugins\n        \"\"\" List of `StrategyPlugin`s. \"\"\"\n\n        if evaluator is None:\n            evaluator = EvaluationPlugin()\n        self.plugins.append(evaluator)\n        self.evaluator = evaluator\n        \"\"\" EvaluationPlugin used for logging and metric computations. \"\"\"\n\n        self.eval_every = eval_every\n        \"\"\" Frequency of the evaluation during training. \"\"\"\n\n        ###################################################################\n        # State variables. These are updated during the train/eval loops. #\n        ###################################################################\n        self.training_exp_counter = 0\n        \"\"\" Counts the number of training steps. +1 at the end of each \n        experience. \"\"\"\n\n        self.epoch: Optional[int] = None\n        \"\"\" Epoch counter. \"\"\"\n\n        self.experience = None\n        \"\"\" Current experience. \"\"\"\n\n        self.adapted_dataset = None\n        \"\"\" Data used to train. It may be modified by plugins. Plugins can \n        append data to it (e.g. for replay). \n         \n        .. note:: \n            This dataset may contain samples from different experiences. If you \n            want the original data for the current experience  \n            use :attr:`.BaseStrategy.experience`.\n        \"\"\"\n\n        self.dataloader = None\n        \"\"\" Dataloader. \"\"\"\n\n        self.mb_it = None\n        \"\"\" Iteration counter. Reset at the start of a new epoch. \"\"\"\n\n        self.mbatch = None\n        \"\"\" Current mini-batch. \"\"\"\n\n        self.mb_output = None\n        \"\"\" Model's output computed on the current mini-batch. \"\"\"\n\n        self.loss = None\n        \"\"\" Loss of the current mini-batch. \"\"\"\n\n        self.is_training: bool = False\n        \"\"\" True if the strategy is in training mode. \"\"\"\n\n        self.current_eval_stream = None\n        \"\"\"User-provided evaluation stream on `eval` call\"\"\"\n\n        self._stop_training = False\n\n        self._warn_for_disabled_plugins_callbacks()\n        self._warn_for_disabled_metrics_callbacks()\n\n    @property\n    def is_eval(self):\n        \"\"\" True if the strategy is in evaluation mode. \"\"\"\n        return not self.is_training\n\n    @property\n    def mb_x(self):\n        \"\"\" Current mini-batch input. \"\"\"\n        return self.mbatch[0]\n\n    @property\n    def mb_y(self):\n        \"\"\" Current mini-batch target. \"\"\"\n        return self.mbatch[1]\n\n    @property\n    def mb_task_id(self):\n        assert len(self.mbatch) >= 3\n        return self.mbatch[-1]\n\n    def criterion(self):\n        \"\"\" Loss function. \"\"\"\n        return self._criterion(self.mb_output, self.mb_y)\n\n    def train(self, experiences: Union[Experience, Sequence[Experience]],\n              eval_streams: Optional[Sequence[Union[Experience,\n                                                    Sequence[\n                                                        Experience]]]] = None,\n              **kwargs):\n        \"\"\" Training loop. if experiences is a single element trains on it.\n        If it is a sequence, trains the model on each experience in order.\n        This is different from joint training on the entire stream.\n        It returns a dictionary with last recorded value for each metric.\n\n        :param experiences: single Experience or sequence.\n        :param eval_streams: list of streams for evaluation.\n            If None: use training experiences for evaluation.\n            Use [] if you do not want to evaluate during training.\n\n        :return: dictionary containing last recorded value for\n            each metric name.\n        \"\"\"\n        self.is_training = True\n        self._stop_training = False\n\n        self.model.train()\n        self.model.to(self.device)\n\n        # Normalize training and eval data.\n        if not isinstance(experiences, Sequence):\n            experiences = [experiences]\n        if eval_streams is None:\n            eval_streams = [experiences]\n\n        self.before_training(**kwargs)\n\n        self._periodic_eval(eval_streams, do_final=False, do_initial=True)\n\n        for self.experience in experiences:\n            self.train_exp(self.experience, eval_streams, **kwargs)\n        self.after_training(**kwargs)\n\n        res = self.evaluator.get_last_metrics()\n        return res\n\n    def train_exp(self, experience: Experience, eval_streams=None, **kwargs):\n        \"\"\"\n        Training loop over a single Experience object.\n\n        :param experience: CL experience information.\n        :param eval_streams: list of streams for evaluation.\n            If None: use the training experience for evaluation.\n            Use [] if you do not want to evaluate during training.\n        :param kwargs: custom arguments.\n        \"\"\"\n        self.experience = experience\n        self.model.train()\n\n        if eval_streams is None:\n            eval_streams = [experience]\n        for i, exp in enumerate(eval_streams):\n            if not isinstance(exp, Sequence):\n                eval_streams[i] = [exp]\n\n        # Data Adaptation (e.g. add new samples/data augmentation)\n        self.before_train_dataset_adaptation(**kwargs)\n        self.train_dataset_adaptation(**kwargs)\n        self.after_train_dataset_adaptation(**kwargs)\n        self.make_train_dataloader(**kwargs)\n\n        # Model Adaptation (e.g. freeze/add new units)\n        self.model_adaptation()\n        self.make_optimizer()\n\n        self.before_training_exp(**kwargs)\n        \n        do_final = True\n        if self.eval_every > 0 and \\\n                (self.train_epochs - 1) % self.eval_every == 0:\n            do_final = False\n\n        self.epoch = 0\n        for self.epoch in range(self.train_epochs):\n            self.before_training_epoch(**kwargs)\n\n            if self._stop_training:  # Early stopping\n                self._stop_training = False\n                break\n\n            self.training_epoch(**kwargs)\n            self.after_training_epoch(**kwargs)\n            self._periodic_eval(eval_streams, do_final=False)\n\n        # Final evaluation\n        self._periodic_eval(eval_streams, do_final=do_final)\n        self.after_training_exp(**kwargs)\n\n    def _periodic_eval(self, eval_streams, do_final, do_initial=False):\n        \"\"\" Periodic eval controlled by `self.eval_every`. \"\"\"\n        # Since we are switching from train to eval model inside the training\n        # loop, we need to save the training state, and restore it after the\n        # eval is done.\n        _prev_state = (\n            self.epoch,\n            self.experience,\n            self.adapted_dataset,\n            self.dataloader,\n            self.is_training)\n\n        if (self.eval_every == 0 and (do_final or do_initial)) or \\\n           (self.eval_every > 0 and do_initial) or \\\n                (self.eval_every > 0 and self.epoch % self.eval_every == 0):\n            # in the first case we are outside epoch loop\n            # in the second case we are within epoch loop\n            for exp in eval_streams:\n                self.eval(exp)\n\n        # restore train-state variables and training mode.\n        self.epoch, self.experience, self.adapted_dataset = _prev_state[:3]\n        self.dataloader = _prev_state[3]\n        self.is_training = _prev_state[4]\n        self.model.train()\n\n    def stop_training(self):\n        \"\"\" Signals to stop training at the next iteration. \"\"\"\n        self._stop_training = True\n\n    def train_dataset_adaptation(self, **kwargs):\n        \"\"\" Initialize `self.adapted_dataset`. \"\"\"\n        self.adapted_dataset = self.experience.dataset\n        self.adapted_dataset = self.adapted_dataset.train()\n\n    @torch.no_grad()\n    def eval(self,\n             exp_list: Union[Experience, Sequence[Experience]],\n             **kwargs):\n        \"\"\"\n        Evaluate the current model on a series of experiences and\n        returns the last recorded value for each metric.\n\n        :param exp_list: CL experience information.\n        :param kwargs: custom arguments.\n\n        :return: dictionary containing last recorded value for\n            each metric name\n        \"\"\"\n        self.is_training = False\n        self.model.eval()\n\n        if not isinstance(exp_list, Sequence):\n            exp_list = [exp_list]\n        self.current_eval_stream = exp_list\n\n        self.before_eval(**kwargs)\n        for self.experience in exp_list:\n            # Data Adaptation\n            self.before_eval_dataset_adaptation(**kwargs)\n            self.eval_dataset_adaptation(**kwargs)\n            self.after_eval_dataset_adaptation(**kwargs)\n            self.make_eval_dataloader(**kwargs)\n\n            # Model Adaptation (e.g. freeze/add new units)\n            self.model_adaptation()\n\n            self.before_eval_exp(**kwargs)\n            self.eval_epoch(**kwargs)\n            self.after_eval_exp(**kwargs)\n\n        self.after_eval(**kwargs)\n\n        res = self.evaluator.get_last_metrics()\n\n        return res\n\n    def before_training_exp(self, **kwargs):\n        \"\"\"\n        Called  after the dataset and data loader creation and\n        before the training loop.\n        \"\"\"\n        for p in self.plugins:\n            p.before_training_exp(self, **kwargs)\n\n    def make_train_dataloader(self, num_workers=0, shuffle=True,\n                              pin_memory=True, **kwargs):\n        \"\"\"\n        Called after the dataset adaptation. Initializes the data loader.\n        :param num_workers: number of thread workers for the data loading.\n        :param shuffle: True if the data should be shuffled, False otherwise.\n        :param pin_memory: If True, the data loader will copy Tensors into CUDA\n            pinned memory before returning them. Defaults to True.\n        \"\"\"\n        self.dataloader = TaskBalancedDataLoader(\n            self.adapted_dataset,\n            oversample_small_groups=True,\n            num_workers=num_workers,\n            batch_size=self.train_mb_size,\n            shuffle=shuffle,\n            pin_memory=pin_memory)\n\n    def make_eval_dataloader(self, num_workers=0, pin_memory=True,\n                             **kwargs):\n        \"\"\"\n        Initializes the eval data loader.\n        :param num_workers: How many subprocesses to use for data loading.\n            0 means that the data will be loaded in the main process.\n            (default: 0).\n        :param pin_memory: If True, the data loader will copy Tensors into CUDA\n            pinned memory before returning them. Defaults to True.\n        :param kwargs:\n        :return:\n        \"\"\"\n        self.dataloader = DataLoader(\n            self.adapted_dataset,\n            num_workers=num_workers,\n            batch_size=self.eval_mb_size,\n            pin_memory=pin_memory)\n\n    def after_train_dataset_adaptation(self, **kwargs):\n        \"\"\"\n        Called after the dataset adaptation and before the\n        dataloader initialization. Allows to customize the dataset.\n        :param kwargs:\n        :return:\n        \"\"\"\n        for p in self.plugins:\n            p.after_train_dataset_adaptation(self, **kwargs)\n\n    def before_training_epoch(self, **kwargs):\n        \"\"\"\n        Called at the beginning of a new training epoch.\n        :param kwargs:\n        :return:\n        \"\"\"\n        for p in self.plugins:\n            p.before_training_epoch(self, **kwargs)\n\n    def training_epoch(self, **kwargs):\n        \"\"\"\n        Training epoch.\n        :param kwargs:\n        :return:\n        \"\"\"\n        for self.mb_it, self.mbatch in enumerate(self.dataloader):\n            if self._stop_training:\n                break\n\n            self._unpack_minibatch()\n            self.before_training_iteration(**kwargs)\n\n            self.optimizer.zero_grad()\n            self.loss = 0\n\n            # Forward\n            self.before_forward(**kwargs)\n            self.mb_output = self.forward()\n            self.after_forward(**kwargs)\n\n            # Loss & Backward\n            self.loss += self.criterion()\n\n            self.before_backward(**kwargs)\n            self.loss.backward()\n            self.after_backward(**kwargs)\n\n            # Optimization step\n            self.before_update(**kwargs)\n            self.optimizer.step()\n            self.after_update(**kwargs)\n\n            self.after_training_iteration(**kwargs)\n\n    def _unpack_minibatch(self):\n        \"\"\" We assume mini-batches have the form <x, y, ..., t>.\n        This allows for arbitrary tensors between y and t.\n        Keep in mind that in the most general case mb_task_id is a tensor\n        which may contain different labels for each sample.\n        \"\"\"\n        assert len(self.mbatch) >= 3\n        for i in range(len(self.mbatch)):\n            self.mbatch[i] = self.mbatch[i].to(self.device)\n\n    def before_training(self, **kwargs):\n        for p in self.plugins:\n            p.before_training(self, **kwargs)\n\n    def after_training(self, **kwargs):\n        for p in self.plugins:\n            p.after_training(self, **kwargs)\n\n    def before_training_iteration(self, **kwargs):\n        for p in self.plugins:\n            p.before_training_iteration(self, **kwargs)\n\n    def before_forward(self, **kwargs):\n        for p in self.plugins:\n            p.before_forward(self, **kwargs)\n\n    def after_forward(self, **kwargs):\n        for p in self.plugins:\n            p.after_forward(self, **kwargs)\n\n    def before_backward(self, **kwargs):\n        for p in self.plugins:\n            p.before_backward(self, **kwargs)\n\n    def after_backward(self, **kwargs):\n        for p in self.plugins:\n            p.after_backward(self, **kwargs)\n\n    def after_training_iteration(self, **kwargs):\n        for p in self.plugins:\n            p.after_training_iteration(self, **kwargs)\n\n    def before_update(self, **kwargs):\n        for p in self.plugins:\n            p.before_update(self, **kwargs)\n\n    def after_update(self, **kwargs):\n        for p in self.plugins:\n            p.after_update(self, **kwargs)\n\n    def after_training_epoch(self, **kwargs):\n        for p in self.plugins:\n            p.after_training_epoch(self, **kwargs)\n\n    def after_training_exp(self, **kwargs):\n        for p in self.plugins:\n            p.after_training_exp(self, **kwargs)\n        self.training_exp_counter += 1\n\n    def before_eval(self, **kwargs):\n        for p in self.plugins:\n            p.before_eval(self, **kwargs)\n\n    def before_eval_exp(self, **kwargs):\n        for p in self.plugins:\n            p.before_eval_exp(self, **kwargs)\n\n    def eval_dataset_adaptation(self, **kwargs):\n        \"\"\" Initialize `self.adapted_dataset`. \"\"\"\n        self.adapted_dataset = self.experience.dataset\n        self.adapted_dataset = self.adapted_dataset.eval()\n\n    def before_eval_dataset_adaptation(self, **kwargs):\n        for p in self.plugins:\n            p.before_eval_dataset_adaptation(self, **kwargs)\n\n    def after_eval_dataset_adaptation(self, **kwargs):\n        for p in self.plugins:\n            p.after_eval_dataset_adaptation(self, **kwargs)\n\n    def eval_epoch(self, **kwargs):\n        for self.mb_it, self.mbatch in \\\n                enumerate(self.dataloader):\n            self._unpack_minibatch()\n            self.before_eval_iteration(**kwargs)\n\n            self.before_eval_forward(**kwargs)\n            self.mb_output = self.forward()\n            self.after_eval_forward(**kwargs)\n            self.loss = self.criterion()\n\n            self.after_eval_iteration(**kwargs)\n\n    def after_eval_exp(self, **kwargs):\n        for p in self.plugins:\n            p.after_eval_exp(self, **kwargs)\n\n    def after_eval(self, **kwargs):\n        for p in self.plugins:\n            p.after_eval(self, **kwargs)\n\n    def before_eval_iteration(self, **kwargs):\n        for p in self.plugins:\n            p.before_eval_iteration(self, **kwargs)\n\n    def before_eval_forward(self, **kwargs):\n        for p in self.plugins:\n            p.before_eval_forward(self, **kwargs)\n\n    def after_eval_forward(self, **kwargs):\n        for p in self.plugins:\n            p.after_eval_forward(self, **kwargs)\n\n    def after_eval_iteration(self, **kwargs):\n        for p in self.plugins:\n            p.after_eval_iteration(self, **kwargs)\n\n    def before_train_dataset_adaptation(self, **kwargs):\n        for p in self.plugins:\n            p.before_train_dataset_adaptation(self, **kwargs)\n\n    def model_adaptation(self):\n        for module in self.model.modules():\n            if isinstance(module, DynamicModule):\n                module.adaptation(self.experience.dataset)\n        self.model = self.model.to(self.device)\n\n    def forward(self):\n        return avalanche_forward(self.model, self.mb_x, self.mb_task_id)\n\n    def make_optimizer(self):\n        # we reset the optimizer's state after each experience.\n        # This allows to add new parameters (new heads) and\n        # freezing old units during the model's adaptation phase.\n        reset_optimizer(self.optimizer, self.model)\n\n    def _warn_for_disabled_plugins_callbacks(self):\n        self._warn_for_disabled_callbacks(self.plugins)\n\n    def _warn_for_disabled_metrics_callbacks(self):\n        self._warn_for_disabled_callbacks(self.evaluator.metrics)\n\n    def _warn_for_disabled_callbacks(\n            self,\n            plugins: List[\"StrategyCallbacks\"]\n    ):\n        \"\"\"\n        Will log some warnings in case some plugins appear to be using callbacks\n        that have been de-activated by the strategy class.\n        \"\"\"\n        for disabled_callback_name in self.DISABLED_CALLBACKS:\n            for plugin in plugins:\n                callback = getattr(plugin, disabled_callback_name)\n                callback_class = callback.__qualname__.split('.')[0]\n                if callback_class not in (\n                    \"StrategyPlugin\",\n                    \"PluginMetric\",\n                    \"EvaluationPlugin\",\n                    \"GenericPluginMetric\",\n                ):\n                    logger.warning(\n                        f\"{plugin.__class__.__name__} seems to use \"\n                        f\"the callback {disabled_callback_name} \"\n                        f\"which is disabled by {self.__class__.__name__}\"\n                    )",
  "def __init__(self, model: Module, optimizer: Optimizer,\n                 criterion=CrossEntropyLoss(),\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = 1, device='cpu',\n                 plugins: Optional[Sequence['StrategyPlugin']] = None,\n                 evaluator=default_logger, eval_every=-1):\n        \"\"\"\n        BaseStrategy is the super class of all task-based continual learning\n        strategies. It implements a basic training loop and callback system\n        that allows to execute code at each experience of the training loop.\n        Plugins can be used to implement callbacks to augment the training\n        loop with additional behavior (e.g. a memory buffer for replay).\n\n        **Scenarios**\n        This strategy supports several continual learning scenarios:\n\n        * class-incremental scenarios (no task labels)\n        * multi-task scenarios, where task labels are provided)\n        * multi-incremental scenarios, where the same task may be revisited\n\n        The exact scenario depends on the data stream and whether it provides\n        the task labels.\n\n        **Training loop**\n        The training loop is organized as follows::\n            train\n                train_exp  # for each experience\n                    adapt_train_dataset\n                    train_dataset_adaptation\n                    make_train_dataloader\n                    train_epoch  # for each epoch\n                        # forward\n                        # backward\n                        # model update\n\n        **Evaluation loop**\n        The evaluation loop is organized as follows::\n            eval\n                eval_exp  # for each experience\n                    adapt_eval_dataset\n                    eval_dataset_adaptation\n                    make_eval_dataloader\n                    eval_epoch  # for each epoch\n                        # forward\n                        # backward\n                        # model update\n\n        :param model: PyTorch model.\n        :param optimizer: PyTorch optimizer.\n        :param criterion: loss function.\n        :param train_mb_size: mini-batch size for training.\n        :param train_epochs: number of training epochs.\n        :param eval_mb_size: mini-batch size for eval.\n        :param device: PyTorch device where the model will be allocated.\n        :param plugins: (optional) list of StrategyPlugins.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations. None to remove logging.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience and before training on the first experience.\n                if >0: calls `eval` every `eval_every` epochs, at the end\n                    of all the epochs for a single experience and before\n                    training on the first experience.\n        \"\"\"\n        self._criterion = criterion\n\n        self.model: Module = model\n        \"\"\" PyTorch model. \"\"\"\n\n        self.optimizer = optimizer\n        \"\"\" PyTorch optimizer. \"\"\"\n\n        self.train_epochs: int = train_epochs\n        \"\"\" Number of training epochs. \"\"\"\n\n        self.train_mb_size: int = train_mb_size\n        \"\"\" Training mini-batch size. \"\"\"\n\n        self.eval_mb_size: int = train_mb_size if eval_mb_size is None \\\n            else eval_mb_size\n        \"\"\" Eval mini-batch size. \"\"\"\n\n        self.device = device\n        \"\"\" PyTorch device where the model will be allocated. \"\"\"\n\n        self.plugins = [] if plugins is None else plugins\n        \"\"\" List of `StrategyPlugin`s. \"\"\"\n\n        if evaluator is None:\n            evaluator = EvaluationPlugin()\n        self.plugins.append(evaluator)\n        self.evaluator = evaluator\n        \"\"\" EvaluationPlugin used for logging and metric computations. \"\"\"\n\n        self.eval_every = eval_every\n        \"\"\" Frequency of the evaluation during training. \"\"\"\n\n        ###################################################################\n        # State variables. These are updated during the train/eval loops. #\n        ###################################################################\n        self.training_exp_counter = 0\n        \"\"\" Counts the number of training steps. +1 at the end of each \n        experience. \"\"\"\n\n        self.epoch: Optional[int] = None\n        \"\"\" Epoch counter. \"\"\"\n\n        self.experience = None\n        \"\"\" Current experience. \"\"\"\n\n        self.adapted_dataset = None\n        \"\"\" Data used to train. It may be modified by plugins. Plugins can \n        append data to it (e.g. for replay). \n         \n        .. note:: \n            This dataset may contain samples from different experiences. If you \n            want the original data for the current experience  \n            use :attr:`.BaseStrategy.experience`.\n        \"\"\"\n\n        self.dataloader = None\n        \"\"\" Dataloader. \"\"\"\n\n        self.mb_it = None\n        \"\"\" Iteration counter. Reset at the start of a new epoch. \"\"\"\n\n        self.mbatch = None\n        \"\"\" Current mini-batch. \"\"\"\n\n        self.mb_output = None\n        \"\"\" Model's output computed on the current mini-batch. \"\"\"\n\n        self.loss = None\n        \"\"\" Loss of the current mini-batch. \"\"\"\n\n        self.is_training: bool = False\n        \"\"\" True if the strategy is in training mode. \"\"\"\n\n        self.current_eval_stream = None\n        \"\"\"User-provided evaluation stream on `eval` call\"\"\"\n\n        self._stop_training = False\n\n        self._warn_for_disabled_plugins_callbacks()\n        self._warn_for_disabled_metrics_callbacks()",
  "def is_eval(self):\n        \"\"\" True if the strategy is in evaluation mode. \"\"\"\n        return not self.is_training",
  "def mb_x(self):\n        \"\"\" Current mini-batch input. \"\"\"\n        return self.mbatch[0]",
  "def mb_y(self):\n        \"\"\" Current mini-batch target. \"\"\"\n        return self.mbatch[1]",
  "def mb_task_id(self):\n        assert len(self.mbatch) >= 3\n        return self.mbatch[-1]",
  "def criterion(self):\n        \"\"\" Loss function. \"\"\"\n        return self._criterion(self.mb_output, self.mb_y)",
  "def train(self, experiences: Union[Experience, Sequence[Experience]],\n              eval_streams: Optional[Sequence[Union[Experience,\n                                                    Sequence[\n                                                        Experience]]]] = None,\n              **kwargs):\n        \"\"\" Training loop. if experiences is a single element trains on it.\n        If it is a sequence, trains the model on each experience in order.\n        This is different from joint training on the entire stream.\n        It returns a dictionary with last recorded value for each metric.\n\n        :param experiences: single Experience or sequence.\n        :param eval_streams: list of streams for evaluation.\n            If None: use training experiences for evaluation.\n            Use [] if you do not want to evaluate during training.\n\n        :return: dictionary containing last recorded value for\n            each metric name.\n        \"\"\"\n        self.is_training = True\n        self._stop_training = False\n\n        self.model.train()\n        self.model.to(self.device)\n\n        # Normalize training and eval data.\n        if not isinstance(experiences, Sequence):\n            experiences = [experiences]\n        if eval_streams is None:\n            eval_streams = [experiences]\n\n        self.before_training(**kwargs)\n\n        self._periodic_eval(eval_streams, do_final=False, do_initial=True)\n\n        for self.experience in experiences:\n            self.train_exp(self.experience, eval_streams, **kwargs)\n        self.after_training(**kwargs)\n\n        res = self.evaluator.get_last_metrics()\n        return res",
  "def train_exp(self, experience: Experience, eval_streams=None, **kwargs):\n        \"\"\"\n        Training loop over a single Experience object.\n\n        :param experience: CL experience information.\n        :param eval_streams: list of streams for evaluation.\n            If None: use the training experience for evaluation.\n            Use [] if you do not want to evaluate during training.\n        :param kwargs: custom arguments.\n        \"\"\"\n        self.experience = experience\n        self.model.train()\n\n        if eval_streams is None:\n            eval_streams = [experience]\n        for i, exp in enumerate(eval_streams):\n            if not isinstance(exp, Sequence):\n                eval_streams[i] = [exp]\n\n        # Data Adaptation (e.g. add new samples/data augmentation)\n        self.before_train_dataset_adaptation(**kwargs)\n        self.train_dataset_adaptation(**kwargs)\n        self.after_train_dataset_adaptation(**kwargs)\n        self.make_train_dataloader(**kwargs)\n\n        # Model Adaptation (e.g. freeze/add new units)\n        self.model_adaptation()\n        self.make_optimizer()\n\n        self.before_training_exp(**kwargs)\n        \n        do_final = True\n        if self.eval_every > 0 and \\\n                (self.train_epochs - 1) % self.eval_every == 0:\n            do_final = False\n\n        self.epoch = 0\n        for self.epoch in range(self.train_epochs):\n            self.before_training_epoch(**kwargs)\n\n            if self._stop_training:  # Early stopping\n                self._stop_training = False\n                break\n\n            self.training_epoch(**kwargs)\n            self.after_training_epoch(**kwargs)\n            self._periodic_eval(eval_streams, do_final=False)\n\n        # Final evaluation\n        self._periodic_eval(eval_streams, do_final=do_final)\n        self.after_training_exp(**kwargs)",
  "def _periodic_eval(self, eval_streams, do_final, do_initial=False):\n        \"\"\" Periodic eval controlled by `self.eval_every`. \"\"\"\n        # Since we are switching from train to eval model inside the training\n        # loop, we need to save the training state, and restore it after the\n        # eval is done.\n        _prev_state = (\n            self.epoch,\n            self.experience,\n            self.adapted_dataset,\n            self.dataloader,\n            self.is_training)\n\n        if (self.eval_every == 0 and (do_final or do_initial)) or \\\n           (self.eval_every > 0 and do_initial) or \\\n                (self.eval_every > 0 and self.epoch % self.eval_every == 0):\n            # in the first case we are outside epoch loop\n            # in the second case we are within epoch loop\n            for exp in eval_streams:\n                self.eval(exp)\n\n        # restore train-state variables and training mode.\n        self.epoch, self.experience, self.adapted_dataset = _prev_state[:3]\n        self.dataloader = _prev_state[3]\n        self.is_training = _prev_state[4]\n        self.model.train()",
  "def stop_training(self):\n        \"\"\" Signals to stop training at the next iteration. \"\"\"\n        self._stop_training = True",
  "def train_dataset_adaptation(self, **kwargs):\n        \"\"\" Initialize `self.adapted_dataset`. \"\"\"\n        self.adapted_dataset = self.experience.dataset\n        self.adapted_dataset = self.adapted_dataset.train()",
  "def eval(self,\n             exp_list: Union[Experience, Sequence[Experience]],\n             **kwargs):\n        \"\"\"\n        Evaluate the current model on a series of experiences and\n        returns the last recorded value for each metric.\n\n        :param exp_list: CL experience information.\n        :param kwargs: custom arguments.\n\n        :return: dictionary containing last recorded value for\n            each metric name\n        \"\"\"\n        self.is_training = False\n        self.model.eval()\n\n        if not isinstance(exp_list, Sequence):\n            exp_list = [exp_list]\n        self.current_eval_stream = exp_list\n\n        self.before_eval(**kwargs)\n        for self.experience in exp_list:\n            # Data Adaptation\n            self.before_eval_dataset_adaptation(**kwargs)\n            self.eval_dataset_adaptation(**kwargs)\n            self.after_eval_dataset_adaptation(**kwargs)\n            self.make_eval_dataloader(**kwargs)\n\n            # Model Adaptation (e.g. freeze/add new units)\n            self.model_adaptation()\n\n            self.before_eval_exp(**kwargs)\n            self.eval_epoch(**kwargs)\n            self.after_eval_exp(**kwargs)\n\n        self.after_eval(**kwargs)\n\n        res = self.evaluator.get_last_metrics()\n\n        return res",
  "def before_training_exp(self, **kwargs):\n        \"\"\"\n        Called  after the dataset and data loader creation and\n        before the training loop.\n        \"\"\"\n        for p in self.plugins:\n            p.before_training_exp(self, **kwargs)",
  "def make_train_dataloader(self, num_workers=0, shuffle=True,\n                              pin_memory=True, **kwargs):\n        \"\"\"\n        Called after the dataset adaptation. Initializes the data loader.\n        :param num_workers: number of thread workers for the data loading.\n        :param shuffle: True if the data should be shuffled, False otherwise.\n        :param pin_memory: If True, the data loader will copy Tensors into CUDA\n            pinned memory before returning them. Defaults to True.\n        \"\"\"\n        self.dataloader = TaskBalancedDataLoader(\n            self.adapted_dataset,\n            oversample_small_groups=True,\n            num_workers=num_workers,\n            batch_size=self.train_mb_size,\n            shuffle=shuffle,\n            pin_memory=pin_memory)",
  "def make_eval_dataloader(self, num_workers=0, pin_memory=True,\n                             **kwargs):\n        \"\"\"\n        Initializes the eval data loader.\n        :param num_workers: How many subprocesses to use for data loading.\n            0 means that the data will be loaded in the main process.\n            (default: 0).\n        :param pin_memory: If True, the data loader will copy Tensors into CUDA\n            pinned memory before returning them. Defaults to True.\n        :param kwargs:\n        :return:\n        \"\"\"\n        self.dataloader = DataLoader(\n            self.adapted_dataset,\n            num_workers=num_workers,\n            batch_size=self.eval_mb_size,\n            pin_memory=pin_memory)",
  "def after_train_dataset_adaptation(self, **kwargs):\n        \"\"\"\n        Called after the dataset adaptation and before the\n        dataloader initialization. Allows to customize the dataset.\n        :param kwargs:\n        :return:\n        \"\"\"\n        for p in self.plugins:\n            p.after_train_dataset_adaptation(self, **kwargs)",
  "def before_training_epoch(self, **kwargs):\n        \"\"\"\n        Called at the beginning of a new training epoch.\n        :param kwargs:\n        :return:\n        \"\"\"\n        for p in self.plugins:\n            p.before_training_epoch(self, **kwargs)",
  "def training_epoch(self, **kwargs):\n        \"\"\"\n        Training epoch.\n        :param kwargs:\n        :return:\n        \"\"\"\n        for self.mb_it, self.mbatch in enumerate(self.dataloader):\n            if self._stop_training:\n                break\n\n            self._unpack_minibatch()\n            self.before_training_iteration(**kwargs)\n\n            self.optimizer.zero_grad()\n            self.loss = 0\n\n            # Forward\n            self.before_forward(**kwargs)\n            self.mb_output = self.forward()\n            self.after_forward(**kwargs)\n\n            # Loss & Backward\n            self.loss += self.criterion()\n\n            self.before_backward(**kwargs)\n            self.loss.backward()\n            self.after_backward(**kwargs)\n\n            # Optimization step\n            self.before_update(**kwargs)\n            self.optimizer.step()\n            self.after_update(**kwargs)\n\n            self.after_training_iteration(**kwargs)",
  "def _unpack_minibatch(self):\n        \"\"\" We assume mini-batches have the form <x, y, ..., t>.\n        This allows for arbitrary tensors between y and t.\n        Keep in mind that in the most general case mb_task_id is a tensor\n        which may contain different labels for each sample.\n        \"\"\"\n        assert len(self.mbatch) >= 3\n        for i in range(len(self.mbatch)):\n            self.mbatch[i] = self.mbatch[i].to(self.device)",
  "def before_training(self, **kwargs):\n        for p in self.plugins:\n            p.before_training(self, **kwargs)",
  "def after_training(self, **kwargs):\n        for p in self.plugins:\n            p.after_training(self, **kwargs)",
  "def before_training_iteration(self, **kwargs):\n        for p in self.plugins:\n            p.before_training_iteration(self, **kwargs)",
  "def before_forward(self, **kwargs):\n        for p in self.plugins:\n            p.before_forward(self, **kwargs)",
  "def after_forward(self, **kwargs):\n        for p in self.plugins:\n            p.after_forward(self, **kwargs)",
  "def before_backward(self, **kwargs):\n        for p in self.plugins:\n            p.before_backward(self, **kwargs)",
  "def after_backward(self, **kwargs):\n        for p in self.plugins:\n            p.after_backward(self, **kwargs)",
  "def after_training_iteration(self, **kwargs):\n        for p in self.plugins:\n            p.after_training_iteration(self, **kwargs)",
  "def before_update(self, **kwargs):\n        for p in self.plugins:\n            p.before_update(self, **kwargs)",
  "def after_update(self, **kwargs):\n        for p in self.plugins:\n            p.after_update(self, **kwargs)",
  "def after_training_epoch(self, **kwargs):\n        for p in self.plugins:\n            p.after_training_epoch(self, **kwargs)",
  "def after_training_exp(self, **kwargs):\n        for p in self.plugins:\n            p.after_training_exp(self, **kwargs)\n        self.training_exp_counter += 1",
  "def before_eval(self, **kwargs):\n        for p in self.plugins:\n            p.before_eval(self, **kwargs)",
  "def before_eval_exp(self, **kwargs):\n        for p in self.plugins:\n            p.before_eval_exp(self, **kwargs)",
  "def eval_dataset_adaptation(self, **kwargs):\n        \"\"\" Initialize `self.adapted_dataset`. \"\"\"\n        self.adapted_dataset = self.experience.dataset\n        self.adapted_dataset = self.adapted_dataset.eval()",
  "def before_eval_dataset_adaptation(self, **kwargs):\n        for p in self.plugins:\n            p.before_eval_dataset_adaptation(self, **kwargs)",
  "def after_eval_dataset_adaptation(self, **kwargs):\n        for p in self.plugins:\n            p.after_eval_dataset_adaptation(self, **kwargs)",
  "def eval_epoch(self, **kwargs):\n        for self.mb_it, self.mbatch in \\\n                enumerate(self.dataloader):\n            self._unpack_minibatch()\n            self.before_eval_iteration(**kwargs)\n\n            self.before_eval_forward(**kwargs)\n            self.mb_output = self.forward()\n            self.after_eval_forward(**kwargs)\n            self.loss = self.criterion()\n\n            self.after_eval_iteration(**kwargs)",
  "def after_eval_exp(self, **kwargs):\n        for p in self.plugins:\n            p.after_eval_exp(self, **kwargs)",
  "def after_eval(self, **kwargs):\n        for p in self.plugins:\n            p.after_eval(self, **kwargs)",
  "def before_eval_iteration(self, **kwargs):\n        for p in self.plugins:\n            p.before_eval_iteration(self, **kwargs)",
  "def before_eval_forward(self, **kwargs):\n        for p in self.plugins:\n            p.before_eval_forward(self, **kwargs)",
  "def after_eval_forward(self, **kwargs):\n        for p in self.plugins:\n            p.after_eval_forward(self, **kwargs)",
  "def after_eval_iteration(self, **kwargs):\n        for p in self.plugins:\n            p.after_eval_iteration(self, **kwargs)",
  "def before_train_dataset_adaptation(self, **kwargs):\n        for p in self.plugins:\n            p.before_train_dataset_adaptation(self, **kwargs)",
  "def model_adaptation(self):\n        for module in self.model.modules():\n            if isinstance(module, DynamicModule):\n                module.adaptation(self.experience.dataset)\n        self.model = self.model.to(self.device)",
  "def forward(self):\n        return avalanche_forward(self.model, self.mb_x, self.mb_task_id)",
  "def make_optimizer(self):\n        # we reset the optimizer's state after each experience.\n        # This allows to add new parameters (new heads) and\n        # freezing old units during the model's adaptation phase.\n        reset_optimizer(self.optimizer, self.model)",
  "def _warn_for_disabled_plugins_callbacks(self):\n        self._warn_for_disabled_callbacks(self.plugins)",
  "def _warn_for_disabled_metrics_callbacks(self):\n        self._warn_for_disabled_callbacks(self.evaluator.metrics)",
  "def _warn_for_disabled_callbacks(\n            self,\n            plugins: List[\"StrategyCallbacks\"]\n    ):\n        \"\"\"\n        Will log some warnings in case some plugins appear to be using callbacks\n        that have been de-activated by the strategy class.\n        \"\"\"\n        for disabled_callback_name in self.DISABLED_CALLBACKS:\n            for plugin in plugins:\n                callback = getattr(plugin, disabled_callback_name)\n                callback_class = callback.__qualname__.split('.')[0]\n                if callback_class not in (\n                    \"StrategyPlugin\",\n                    \"PluginMetric\",\n                    \"EvaluationPlugin\",\n                    \"GenericPluginMetric\",\n                ):\n                    logger.warning(\n                        f\"{plugin.__class__.__name__} seems to use \"\n                        f\"the callback {disabled_callback_name} \"\n                        f\"which is disabled by {self.__class__.__name__}\"\n                    )",
  "class Naive(BaseStrategy):\n    \"\"\"\n    The simplest (and least effective) Continual Learning strategy. Naive just\n    incrementally fine tunes a single model without employing any method\n    to contrast the catastrophic forgetting of previous knowledge.\n    This strategy does not use task identities.\n\n    Naive is easy to set up and its results are commonly used to show the worst\n    performing baseline.\n    \"\"\"\n\n    def __init__(self, model: Module, optimizer: Optimizer,\n                 criterion=CrossEntropyLoss(),\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = None, device=None,\n                 plugins: Optional[List[StrategyPlugin]] = None,\n                 evaluator: EvaluationPlugin = default_logger, eval_every=-1):\n        \"\"\"\n        Creates an instance of the Naive strategy.\n\n        :param model: The model.\n        :param optimizer: The optimizer to use.\n        :param criterion: The loss criterion to use.\n        :param train_mb_size: The train minibatch size. Defaults to 1.\n        :param train_epochs: The number of training epochs. Defaults to 1.\n        :param eval_mb_size: The eval minibatch size. Defaults to 1.\n        :param device: The device to use. Defaults to None (cpu).\n        :param plugins: Plugins to be added. Defaults to None.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n        super().__init__(\n            model, optimizer, criterion,\n            train_mb_size=train_mb_size, train_epochs=train_epochs,\n            eval_mb_size=eval_mb_size, device=device, plugins=plugins,\n            evaluator=evaluator, eval_every=eval_every)",
  "class PNNStrategy(BaseStrategy):\n    \"\"\"\n    The simplest (and least effective) Continual Learning strategy. Naive just\n    incrementally fine tunes a single model without employing any method\n    to contrast the catastrophic forgetting of previous knowledge.\n    This strategy does not use task identities.\n\n    Naive is easy to set up and its results are commonly used to show the worst\n    performing baseline.\n    \"\"\"\n\n    def __init__(self, num_layers: int, in_features: int,\n                 hidden_features_per_column: int,\n                 lr: float, momentum=0, dampening=0,\n                 weight_decay=0, nesterov=False, adapter='mlp',\n                 criterion=CrossEntropyLoss(),\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = None, device=None,\n                 plugins: Optional[List[StrategyPlugin]] = None,\n                 evaluator: EvaluationPlugin = default_logger, eval_every=-1):\n        \"\"\"\n        Creates an instance of the Naive strategy.\n\n        :param num_layers: Number of layers for the PNN architecture.\n        :param in_features: Number of input features.\n        :param hidden_features_per_column: Number of hidden units for\n            each column of the PNN architecture.\n        :param lr: learning rate\n        :param momentum: momentum factor (default: 0)\n        :param weight_decay: weight decay (L2 penalty) (default: 0)\n        :param dampening: dampening for momentum (default: 0)\n        :param nesterov: enables Nesterov momentum (default: False)\n        :param adapter: adapter type. One of {'linear', 'mlp'} (default='mlp')\n        :param criterion: The loss criterion to use.\n        :param train_mb_size: The train minibatch size. Defaults to 1.\n        :param train_epochs: The number of training epochs. Defaults to 1.\n        :param eval_mb_size: The eval minibatch size. Defaults to 1.\n        :param device: The device to use. Defaults to None (cpu).\n        :param plugins: Plugins to be added. Defaults to None.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n        model = PNN(\n            num_layers=num_layers,\n            in_features=in_features,\n            hidden_features_per_column=hidden_features_per_column,\n            adapter=adapter\n        )\n        optimizer = SGD(model.parameters(), lr=lr, momentum=momentum,\n                        weight_decay=weight_decay, dampening=dampening,\n                        nesterov=nesterov)\n        super().__init__(\n            model, optimizer, criterion,\n            train_mb_size=train_mb_size, train_epochs=train_epochs,\n            eval_mb_size=eval_mb_size, device=device, plugins=plugins,\n            evaluator=evaluator, eval_every=eval_every)",
  "class CWRStar(BaseStrategy):\n\n    def __init__(self, model: Module, optimizer: Optimizer, criterion,\n                 cwr_layer_name: str, train_mb_size: int = 1,\n                 train_epochs: int = 1, eval_mb_size: int = None, device=None,\n                 plugins: Optional[List[StrategyPlugin]] = None,\n                 evaluator: EvaluationPlugin = default_logger, eval_every=-1):\n        \"\"\" CWR* Strategy.\n        This strategy does not use task identities.\n\n        :param model: The model.\n        :param optimizer: The optimizer to use.\n        :param criterion: The loss criterion to use.\n        :param cwr_layer_name: name of the CWR layer. Defaults to None, which\n            means that the last fully connected layer will be used.\n        :param train_mb_size: The train minibatch size. Defaults to 1.\n        :param train_epochs: The number of training epochs. Defaults to 1.\n        :param eval_mb_size: The eval minibatch size. Defaults to 1.\n        :param device: The device to use. Defaults to None (cpu).\n        :param plugins: Plugins to be added. Defaults to None.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n        cwsp = CWRStarPlugin(model, cwr_layer_name, freeze_remaining_model=True)\n        if plugins is None:\n            plugins = [cwsp]\n        else:\n            plugins.append(cwsp)\n        super().__init__(\n            model, optimizer, criterion,\n            train_mb_size=train_mb_size, train_epochs=train_epochs,\n            eval_mb_size=eval_mb_size, device=device, plugins=plugins,\n            evaluator=evaluator, eval_every=eval_every)",
  "class Replay(BaseStrategy):\n\n    def __init__(self, model: Module, optimizer: Optimizer, criterion,\n                 mem_size: int = 200,\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = None, device=None,\n                 plugins: Optional[List[StrategyPlugin]] = None,\n                 evaluator: EvaluationPlugin = default_logger, eval_every=-1):\n        \"\"\" Experience replay strategy. See ReplayPlugin for more details.\n        This strategy does not use task identities.\n\n        :param model: The model.\n        :param optimizer: The optimizer to use.\n        :param criterion: The loss criterion to use.\n        :param mem_size: replay buffer size.\n        :param train_mb_size: The train minibatch size. Defaults to 1.\n        :param train_epochs: The number of training epochs. Defaults to 1.\n        :param eval_mb_size: The eval minibatch size. Defaults to 1.\n        :param device: The device to use. Defaults to None (cpu).\n        :param plugins: Plugins to be added. Defaults to None.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n        rp = ReplayPlugin(mem_size)\n        if plugins is None:\n            plugins = [rp]\n        else:\n            plugins.append(rp)\n        super().__init__(\n            model, optimizer, criterion,\n            train_mb_size=train_mb_size, train_epochs=train_epochs,\n            eval_mb_size=eval_mb_size, device=device, plugins=plugins,\n            evaluator=evaluator, eval_every=eval_every)",
  "class GDumb(BaseStrategy):\n\n    def __init__(self, model: Module, optimizer: Optimizer, criterion,\n                 mem_size: int = 200,\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = None, device=None,\n                 plugins: Optional[List[StrategyPlugin]] = None,\n                 evaluator: EvaluationPlugin = default_logger, eval_every=-1):\n        \"\"\" GDumb strategy. See GDumbPlugin for more details.\n        This strategy does not use task identities.\n\n        :param model: The model.\n        :param optimizer: The optimizer to use.\n        :param criterion: The loss criterion to use.\n        :param mem_size: replay buffer size.\n        :param train_mb_size: The train minibatch size. Defaults to 1.\n        :param train_epochs: The number of training epochs. Defaults to 1.\n        :param eval_mb_size: The eval minibatch size. Defaults to 1.\n        :param device: The device to use. Defaults to None (cpu).\n        :param plugins: Plugins to be added. Defaults to None.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n\n        gdumb = GDumbPlugin(mem_size)\n        if plugins is None:\n            plugins = [gdumb]\n        else:\n            plugins.append(gdumb)\n\n        super().__init__(\n            model, optimizer, criterion,\n            train_mb_size=train_mb_size, train_epochs=train_epochs,\n            eval_mb_size=eval_mb_size, device=device, plugins=plugins,\n            evaluator=evaluator, eval_every=eval_every)",
  "class LwF(BaseStrategy):\n\n    def __init__(self, model: Module, optimizer: Optimizer, criterion,\n                 alpha: Union[float, Sequence[float]], temperature: float,\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = None, device=None,\n                 plugins: Optional[List[StrategyPlugin]] = None,\n                 evaluator: EvaluationPlugin = default_logger, eval_every=-1):\n        \"\"\" Learning without Forgetting strategy.\n            See LwF plugin for details.\n            This strategy does not use task identities.\n\n        :param model: The model.\n        :param optimizer: The optimizer to use.\n        :param criterion: The loss criterion to use.\n        :param alpha: distillation hyperparameter. It can be either a float\n                number or a list containing alpha for each experience.\n        :param temperature: softmax temperature for distillation\n        :param train_mb_size: The train minibatch size. Defaults to 1.\n        :param train_epochs: The number of training epochs. Defaults to 1.\n        :param eval_mb_size: The eval minibatch size. Defaults to 1.\n        :param device: The device to use. Defaults to None (cpu).\n        :param plugins: Plugins to be added. Defaults to None.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n\n        lwf = LwFPlugin(alpha, temperature)\n        if plugins is None:\n            plugins = [lwf]\n        else:\n            plugins.append(lwf)\n\n        super().__init__(\n            model, optimizer, criterion,\n            train_mb_size=train_mb_size, train_epochs=train_epochs,\n            eval_mb_size=eval_mb_size, device=device, plugins=plugins,\n            evaluator=evaluator, eval_every=eval_every)",
  "class AGEM(BaseStrategy):\n\n    def __init__(self, model: Module, optimizer: Optimizer, criterion,\n                 patterns_per_exp: int, sample_size: int = 64,\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = None, device=None,\n                 plugins: Optional[List[StrategyPlugin]] = None,\n                 evaluator: EvaluationPlugin = default_logger, eval_every=-1):\n        \"\"\" Average Gradient Episodic Memory (A-GEM) strategy.\n            See AGEM plugin for details.\n            This strategy does not use task identities.\n\n        :param model: The model.\n        :param optimizer: The optimizer to use.\n        :param criterion: The loss criterion to use.\n        :param patterns_per_exp: number of patterns per experience in the memory\n        :param sample_size: number of patterns in memory sample when computing\n            reference gradient.\n        :param train_mb_size: The train minibatch size. Defaults to 1.\n        :param train_epochs: The number of training epochs. Defaults to 1.\n        :param eval_mb_size: The eval minibatch size. Defaults to 1.\n        :param device: The device to use. Defaults to None (cpu).\n        :param plugins: Plugins to be added. Defaults to None.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n\n        agem = AGEMPlugin(patterns_per_exp, sample_size)\n        if plugins is None:\n            plugins = [agem]\n        else:\n            plugins.append(agem)\n\n        super().__init__(\n            model, optimizer, criterion,\n            train_mb_size=train_mb_size, train_epochs=train_epochs,\n            eval_mb_size=eval_mb_size, device=device, plugins=plugins,\n            evaluator=evaluator, eval_every=eval_every)",
  "class GEM(BaseStrategy):\n\n    def __init__(self, model: Module, optimizer: Optimizer, criterion,\n                 patterns_per_exp: int, memory_strength: float = 0.5,\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = None, device=None,\n                 plugins: Optional[List[StrategyPlugin]] = None,\n                 evaluator: EvaluationPlugin = default_logger, eval_every=-1):\n        \"\"\" Gradient Episodic Memory (GEM) strategy.\n            See GEM plugin for details.\n            This strategy does not use task identities.\n\n        :param model: The model.\n        :param optimizer: The optimizer to use.\n        :param criterion: The loss criterion to use.\n        :param patterns_per_exp: number of patterns per experience in the memory\n        :param memory_strength: offset to add to the projection direction\n            in order to favour backward transfer (gamma in original paper).\n        :param train_mb_size: The train minibatch size. Defaults to 1.\n        :param train_epochs: The number of training epochs. Defaults to 1.\n        :param eval_mb_size: The eval minibatch size. Defaults to 1.\n        :param device: The device to use. Defaults to None (cpu).\n        :param plugins: Plugins to be added. Defaults to None.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n\n        gem = GEMPlugin(patterns_per_exp, memory_strength)\n        if plugins is None:\n            plugins = [gem]\n        else:\n            plugins.append(gem)\n\n        super().__init__(\n            model, optimizer, criterion,\n            train_mb_size=train_mb_size, train_epochs=train_epochs,\n            eval_mb_size=eval_mb_size, device=device, plugins=plugins,\n            evaluator=evaluator, eval_every=eval_every)",
  "class EWC(BaseStrategy):\n\n    def __init__(self, model: Module, optimizer: Optimizer, criterion,\n                 ewc_lambda: float, mode: str = 'separate',\n                 decay_factor: Optional[float] = None,\n                 keep_importance_data: bool = False,\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = None, device=None,\n                 plugins: Optional[List[StrategyPlugin]] = None,\n                 evaluator: EvaluationPlugin = default_logger, eval_every=-1):\n        \"\"\" Elastic Weight Consolidation (EWC) strategy.\n            See EWC plugin for details.\n            This strategy does not use task identities.\n\n        :param model: The model.\n        :param optimizer: The optimizer to use.\n        :param criterion: The loss criterion to use.\n        :param ewc_lambda: hyperparameter to weigh the penalty inside the total\n               loss. The larger the lambda, the larger the regularization.\n        :param mode: `separate` to keep a separate penalty for each previous\n               experience. `onlinesum` to keep a single penalty summed over all\n               previous tasks. `onlineweightedsum` to keep a single penalty\n               summed with a decay factor over all previous tasks.\n        :param decay_factor: used only if mode is `onlineweightedsum`.\n               It specify the decay term of the importance matrix.\n        :param keep_importance_data: if True, keep in memory both parameter\n                values and importances for all previous task, for all modes.\n                If False, keep only last parameter values and importances.\n                If mode is `separate`, the value of `keep_importance_data` is\n                set to be True.\n        :param train_mb_size: The train minibatch size. Defaults to 1.\n        :param train_epochs: The number of training epochs. Defaults to 1.\n        :param eval_mb_size: The eval minibatch size. Defaults to 1.\n        :param device: The device to use. Defaults to None (cpu).\n        :param plugins: Plugins to be added. Defaults to None.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n\n        ewc = EWCPlugin(ewc_lambda, mode, decay_factor, keep_importance_data)\n        if plugins is None:\n            plugins = [ewc]\n        else:\n            plugins.append(ewc)\n\n        super().__init__(\n            model, optimizer, criterion,\n            train_mb_size=train_mb_size, train_epochs=train_epochs,\n            eval_mb_size=eval_mb_size, device=device, plugins=plugins,\n            evaluator=evaluator, eval_every=eval_every)",
  "class SynapticIntelligence(BaseStrategy):\n    \"\"\"\n    The Synaptic Intelligence strategy.\n\n    This is the Synaptic Intelligence PyTorch implementation of the\n    algorithm described in the paper\n    \"Continuous Learning in Single-Incremental-Task Scenarios\"\n    (https://arxiv.org/abs/1806.08568)\n\n    The original implementation has been proposed in the paper\n    \"Continual Learning Through Synaptic Intelligence\"\n    (https://arxiv.org/abs/1703.04200).\n\n    The Synaptic Intelligence regularization can also be used in a different\n    strategy by applying the :class:`SynapticIntelligencePlugin` plugin.\n    \"\"\"\n\n    def __init__(self, model: Module, optimizer: Optimizer, criterion,\n                 si_lambda: float, train_mb_size: int = 1,\n                 train_epochs: int = 1, eval_mb_size: int = 1, device='cpu',\n                 plugins: Optional[Sequence['StrategyPlugin']] = None,\n                 evaluator=default_logger, eval_every=-1):\n        \"\"\"\n        Creates an instance of the Synaptic Intelligence strategy.\n\n        :param model: PyTorch model.\n        :param optimizer: PyTorch optimizer.\n        :param criterion: loss function.\n        :param si_lambda: Synaptic Intelligence lambda term.\n        :param train_mb_size: mini-batch size for training.\n        :param train_epochs: number of training epochs.\n        :param eval_mb_size: mini-batch size for eval.\n        :param device: PyTorch device to run the model.\n        :param plugins: (optional) list of StrategyPlugins.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n        if plugins is None:\n            plugins = []\n\n        # This implementation relies on the S.I. Plugin, which contains the\n        # entire implementation of the strategy!\n        plugins.append(SynapticIntelligencePlugin(si_lambda))\n\n        super(SynapticIntelligence, self).__init__(\n            model, optimizer, criterion, train_mb_size, train_epochs,\n            eval_mb_size, device=device, plugins=plugins, evaluator=evaluator,\n            eval_every=eval_every\n        )",
  "class CoPE(BaseStrategy):\n\n    def __init__(self, model: Module, optimizer: Optimizer, criterion,\n                 mem_size: int = 200, n_classes: int = 10, p_size: int = 100,\n                 alpha: float = 0.99, T: float = 0.1,\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = None, device=None,\n                 plugins: Optional[List[StrategyPlugin]] = None,\n                 evaluator: EvaluationPlugin = default_logger,\n                 eval_every=-1):\n        \"\"\" Continual Prototype Evolution strategy.\n        See CoPEPlugin for more details.\n        This strategy does not use task identities during training.\n\n        :param model: The model.\n        :param optimizer: The optimizer to use.\n        :param criterion: Loss criterion to use. Standard overwritten by\n        PPPloss (see CoPEPlugin).\n        :param mem_size: replay buffer size.\n        :param n_classes: total number of classes that will be encountered. This\n        is used to output predictions for all classes, with zero probability\n        for unseen classes.\n        :param p_size: The prototype size, which equals the feature size of the\n        last layer.\n        :param alpha: The momentum for the exponentially moving average of the\n        prototypes.\n        :param T: The softmax temperature, used as a concentration parameter.\n        :param train_mb_size: The train minibatch size. Defaults to 1.\n        :param train_epochs: The number of training epochs. Defaults to 1.\n        :param eval_mb_size: The eval minibatch size. Defaults to 1.\n        :param device: The device to use. Defaults to None (cpu).\n        :param plugins: Plugins to be added. Defaults to None.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n        copep = CoPEPlugin(mem_size, n_classes, p_size, alpha, T)\n        if plugins is None:\n            plugins = [copep]\n        else:\n            plugins.append(copep)\n        super().__init__(\n            model, optimizer, criterion,\n            train_mb_size=train_mb_size, train_epochs=train_epochs,\n            eval_mb_size=eval_mb_size, device=device, plugins=plugins,\n            evaluator=evaluator, eval_every=eval_every)",
  "def __init__(self, model: Module, optimizer: Optimizer,\n                 criterion=CrossEntropyLoss(),\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = None, device=None,\n                 plugins: Optional[List[StrategyPlugin]] = None,\n                 evaluator: EvaluationPlugin = default_logger, eval_every=-1):\n        \"\"\"\n        Creates an instance of the Naive strategy.\n\n        :param model: The model.\n        :param optimizer: The optimizer to use.\n        :param criterion: The loss criterion to use.\n        :param train_mb_size: The train minibatch size. Defaults to 1.\n        :param train_epochs: The number of training epochs. Defaults to 1.\n        :param eval_mb_size: The eval minibatch size. Defaults to 1.\n        :param device: The device to use. Defaults to None (cpu).\n        :param plugins: Plugins to be added. Defaults to None.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n        super().__init__(\n            model, optimizer, criterion,\n            train_mb_size=train_mb_size, train_epochs=train_epochs,\n            eval_mb_size=eval_mb_size, device=device, plugins=plugins,\n            evaluator=evaluator, eval_every=eval_every)",
  "def __init__(self, num_layers: int, in_features: int,\n                 hidden_features_per_column: int,\n                 lr: float, momentum=0, dampening=0,\n                 weight_decay=0, nesterov=False, adapter='mlp',\n                 criterion=CrossEntropyLoss(),\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = None, device=None,\n                 plugins: Optional[List[StrategyPlugin]] = None,\n                 evaluator: EvaluationPlugin = default_logger, eval_every=-1):\n        \"\"\"\n        Creates an instance of the Naive strategy.\n\n        :param num_layers: Number of layers for the PNN architecture.\n        :param in_features: Number of input features.\n        :param hidden_features_per_column: Number of hidden units for\n            each column of the PNN architecture.\n        :param lr: learning rate\n        :param momentum: momentum factor (default: 0)\n        :param weight_decay: weight decay (L2 penalty) (default: 0)\n        :param dampening: dampening for momentum (default: 0)\n        :param nesterov: enables Nesterov momentum (default: False)\n        :param adapter: adapter type. One of {'linear', 'mlp'} (default='mlp')\n        :param criterion: The loss criterion to use.\n        :param train_mb_size: The train minibatch size. Defaults to 1.\n        :param train_epochs: The number of training epochs. Defaults to 1.\n        :param eval_mb_size: The eval minibatch size. Defaults to 1.\n        :param device: The device to use. Defaults to None (cpu).\n        :param plugins: Plugins to be added. Defaults to None.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n        model = PNN(\n            num_layers=num_layers,\n            in_features=in_features,\n            hidden_features_per_column=hidden_features_per_column,\n            adapter=adapter\n        )\n        optimizer = SGD(model.parameters(), lr=lr, momentum=momentum,\n                        weight_decay=weight_decay, dampening=dampening,\n                        nesterov=nesterov)\n        super().__init__(\n            model, optimizer, criterion,\n            train_mb_size=train_mb_size, train_epochs=train_epochs,\n            eval_mb_size=eval_mb_size, device=device, plugins=plugins,\n            evaluator=evaluator, eval_every=eval_every)",
  "def __init__(self, model: Module, optimizer: Optimizer, criterion,\n                 cwr_layer_name: str, train_mb_size: int = 1,\n                 train_epochs: int = 1, eval_mb_size: int = None, device=None,\n                 plugins: Optional[List[StrategyPlugin]] = None,\n                 evaluator: EvaluationPlugin = default_logger, eval_every=-1):\n        \"\"\" CWR* Strategy.\n        This strategy does not use task identities.\n\n        :param model: The model.\n        :param optimizer: The optimizer to use.\n        :param criterion: The loss criterion to use.\n        :param cwr_layer_name: name of the CWR layer. Defaults to None, which\n            means that the last fully connected layer will be used.\n        :param train_mb_size: The train minibatch size. Defaults to 1.\n        :param train_epochs: The number of training epochs. Defaults to 1.\n        :param eval_mb_size: The eval minibatch size. Defaults to 1.\n        :param device: The device to use. Defaults to None (cpu).\n        :param plugins: Plugins to be added. Defaults to None.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n        cwsp = CWRStarPlugin(model, cwr_layer_name, freeze_remaining_model=True)\n        if plugins is None:\n            plugins = [cwsp]\n        else:\n            plugins.append(cwsp)\n        super().__init__(\n            model, optimizer, criterion,\n            train_mb_size=train_mb_size, train_epochs=train_epochs,\n            eval_mb_size=eval_mb_size, device=device, plugins=plugins,\n            evaluator=evaluator, eval_every=eval_every)",
  "def __init__(self, model: Module, optimizer: Optimizer, criterion,\n                 mem_size: int = 200,\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = None, device=None,\n                 plugins: Optional[List[StrategyPlugin]] = None,\n                 evaluator: EvaluationPlugin = default_logger, eval_every=-1):\n        \"\"\" Experience replay strategy. See ReplayPlugin for more details.\n        This strategy does not use task identities.\n\n        :param model: The model.\n        :param optimizer: The optimizer to use.\n        :param criterion: The loss criterion to use.\n        :param mem_size: replay buffer size.\n        :param train_mb_size: The train minibatch size. Defaults to 1.\n        :param train_epochs: The number of training epochs. Defaults to 1.\n        :param eval_mb_size: The eval minibatch size. Defaults to 1.\n        :param device: The device to use. Defaults to None (cpu).\n        :param plugins: Plugins to be added. Defaults to None.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n        rp = ReplayPlugin(mem_size)\n        if plugins is None:\n            plugins = [rp]\n        else:\n            plugins.append(rp)\n        super().__init__(\n            model, optimizer, criterion,\n            train_mb_size=train_mb_size, train_epochs=train_epochs,\n            eval_mb_size=eval_mb_size, device=device, plugins=plugins,\n            evaluator=evaluator, eval_every=eval_every)",
  "def __init__(self, model: Module, optimizer: Optimizer, criterion,\n                 mem_size: int = 200,\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = None, device=None,\n                 plugins: Optional[List[StrategyPlugin]] = None,\n                 evaluator: EvaluationPlugin = default_logger, eval_every=-1):\n        \"\"\" GDumb strategy. See GDumbPlugin for more details.\n        This strategy does not use task identities.\n\n        :param model: The model.\n        :param optimizer: The optimizer to use.\n        :param criterion: The loss criterion to use.\n        :param mem_size: replay buffer size.\n        :param train_mb_size: The train minibatch size. Defaults to 1.\n        :param train_epochs: The number of training epochs. Defaults to 1.\n        :param eval_mb_size: The eval minibatch size. Defaults to 1.\n        :param device: The device to use. Defaults to None (cpu).\n        :param plugins: Plugins to be added. Defaults to None.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n\n        gdumb = GDumbPlugin(mem_size)\n        if plugins is None:\n            plugins = [gdumb]\n        else:\n            plugins.append(gdumb)\n\n        super().__init__(\n            model, optimizer, criterion,\n            train_mb_size=train_mb_size, train_epochs=train_epochs,\n            eval_mb_size=eval_mb_size, device=device, plugins=plugins,\n            evaluator=evaluator, eval_every=eval_every)",
  "def __init__(self, model: Module, optimizer: Optimizer, criterion,\n                 alpha: Union[float, Sequence[float]], temperature: float,\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = None, device=None,\n                 plugins: Optional[List[StrategyPlugin]] = None,\n                 evaluator: EvaluationPlugin = default_logger, eval_every=-1):\n        \"\"\" Learning without Forgetting strategy.\n            See LwF plugin for details.\n            This strategy does not use task identities.\n\n        :param model: The model.\n        :param optimizer: The optimizer to use.\n        :param criterion: The loss criterion to use.\n        :param alpha: distillation hyperparameter. It can be either a float\n                number or a list containing alpha for each experience.\n        :param temperature: softmax temperature for distillation\n        :param train_mb_size: The train minibatch size. Defaults to 1.\n        :param train_epochs: The number of training epochs. Defaults to 1.\n        :param eval_mb_size: The eval minibatch size. Defaults to 1.\n        :param device: The device to use. Defaults to None (cpu).\n        :param plugins: Plugins to be added. Defaults to None.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n\n        lwf = LwFPlugin(alpha, temperature)\n        if plugins is None:\n            plugins = [lwf]\n        else:\n            plugins.append(lwf)\n\n        super().__init__(\n            model, optimizer, criterion,\n            train_mb_size=train_mb_size, train_epochs=train_epochs,\n            eval_mb_size=eval_mb_size, device=device, plugins=plugins,\n            evaluator=evaluator, eval_every=eval_every)",
  "def __init__(self, model: Module, optimizer: Optimizer, criterion,\n                 patterns_per_exp: int, sample_size: int = 64,\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = None, device=None,\n                 plugins: Optional[List[StrategyPlugin]] = None,\n                 evaluator: EvaluationPlugin = default_logger, eval_every=-1):\n        \"\"\" Average Gradient Episodic Memory (A-GEM) strategy.\n            See AGEM plugin for details.\n            This strategy does not use task identities.\n\n        :param model: The model.\n        :param optimizer: The optimizer to use.\n        :param criterion: The loss criterion to use.\n        :param patterns_per_exp: number of patterns per experience in the memory\n        :param sample_size: number of patterns in memory sample when computing\n            reference gradient.\n        :param train_mb_size: The train minibatch size. Defaults to 1.\n        :param train_epochs: The number of training epochs. Defaults to 1.\n        :param eval_mb_size: The eval minibatch size. Defaults to 1.\n        :param device: The device to use. Defaults to None (cpu).\n        :param plugins: Plugins to be added. Defaults to None.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n\n        agem = AGEMPlugin(patterns_per_exp, sample_size)\n        if plugins is None:\n            plugins = [agem]\n        else:\n            plugins.append(agem)\n\n        super().__init__(\n            model, optimizer, criterion,\n            train_mb_size=train_mb_size, train_epochs=train_epochs,\n            eval_mb_size=eval_mb_size, device=device, plugins=plugins,\n            evaluator=evaluator, eval_every=eval_every)",
  "def __init__(self, model: Module, optimizer: Optimizer, criterion,\n                 patterns_per_exp: int, memory_strength: float = 0.5,\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = None, device=None,\n                 plugins: Optional[List[StrategyPlugin]] = None,\n                 evaluator: EvaluationPlugin = default_logger, eval_every=-1):\n        \"\"\" Gradient Episodic Memory (GEM) strategy.\n            See GEM plugin for details.\n            This strategy does not use task identities.\n\n        :param model: The model.\n        :param optimizer: The optimizer to use.\n        :param criterion: The loss criterion to use.\n        :param patterns_per_exp: number of patterns per experience in the memory\n        :param memory_strength: offset to add to the projection direction\n            in order to favour backward transfer (gamma in original paper).\n        :param train_mb_size: The train minibatch size. Defaults to 1.\n        :param train_epochs: The number of training epochs. Defaults to 1.\n        :param eval_mb_size: The eval minibatch size. Defaults to 1.\n        :param device: The device to use. Defaults to None (cpu).\n        :param plugins: Plugins to be added. Defaults to None.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n\n        gem = GEMPlugin(patterns_per_exp, memory_strength)\n        if plugins is None:\n            plugins = [gem]\n        else:\n            plugins.append(gem)\n\n        super().__init__(\n            model, optimizer, criterion,\n            train_mb_size=train_mb_size, train_epochs=train_epochs,\n            eval_mb_size=eval_mb_size, device=device, plugins=plugins,\n            evaluator=evaluator, eval_every=eval_every)",
  "def __init__(self, model: Module, optimizer: Optimizer, criterion,\n                 ewc_lambda: float, mode: str = 'separate',\n                 decay_factor: Optional[float] = None,\n                 keep_importance_data: bool = False,\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = None, device=None,\n                 plugins: Optional[List[StrategyPlugin]] = None,\n                 evaluator: EvaluationPlugin = default_logger, eval_every=-1):\n        \"\"\" Elastic Weight Consolidation (EWC) strategy.\n            See EWC plugin for details.\n            This strategy does not use task identities.\n\n        :param model: The model.\n        :param optimizer: The optimizer to use.\n        :param criterion: The loss criterion to use.\n        :param ewc_lambda: hyperparameter to weigh the penalty inside the total\n               loss. The larger the lambda, the larger the regularization.\n        :param mode: `separate` to keep a separate penalty for each previous\n               experience. `onlinesum` to keep a single penalty summed over all\n               previous tasks. `onlineweightedsum` to keep a single penalty\n               summed with a decay factor over all previous tasks.\n        :param decay_factor: used only if mode is `onlineweightedsum`.\n               It specify the decay term of the importance matrix.\n        :param keep_importance_data: if True, keep in memory both parameter\n                values and importances for all previous task, for all modes.\n                If False, keep only last parameter values and importances.\n                If mode is `separate`, the value of `keep_importance_data` is\n                set to be True.\n        :param train_mb_size: The train minibatch size. Defaults to 1.\n        :param train_epochs: The number of training epochs. Defaults to 1.\n        :param eval_mb_size: The eval minibatch size. Defaults to 1.\n        :param device: The device to use. Defaults to None (cpu).\n        :param plugins: Plugins to be added. Defaults to None.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n\n        ewc = EWCPlugin(ewc_lambda, mode, decay_factor, keep_importance_data)\n        if plugins is None:\n            plugins = [ewc]\n        else:\n            plugins.append(ewc)\n\n        super().__init__(\n            model, optimizer, criterion,\n            train_mb_size=train_mb_size, train_epochs=train_epochs,\n            eval_mb_size=eval_mb_size, device=device, plugins=plugins,\n            evaluator=evaluator, eval_every=eval_every)",
  "def __init__(self, model: Module, optimizer: Optimizer, criterion,\n                 si_lambda: float, train_mb_size: int = 1,\n                 train_epochs: int = 1, eval_mb_size: int = 1, device='cpu',\n                 plugins: Optional[Sequence['StrategyPlugin']] = None,\n                 evaluator=default_logger, eval_every=-1):\n        \"\"\"\n        Creates an instance of the Synaptic Intelligence strategy.\n\n        :param model: PyTorch model.\n        :param optimizer: PyTorch optimizer.\n        :param criterion: loss function.\n        :param si_lambda: Synaptic Intelligence lambda term.\n        :param train_mb_size: mini-batch size for training.\n        :param train_epochs: number of training epochs.\n        :param eval_mb_size: mini-batch size for eval.\n        :param device: PyTorch device to run the model.\n        :param plugins: (optional) list of StrategyPlugins.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n        if plugins is None:\n            plugins = []\n\n        # This implementation relies on the S.I. Plugin, which contains the\n        # entire implementation of the strategy!\n        plugins.append(SynapticIntelligencePlugin(si_lambda))\n\n        super(SynapticIntelligence, self).__init__(\n            model, optimizer, criterion, train_mb_size, train_epochs,\n            eval_mb_size, device=device, plugins=plugins, evaluator=evaluator,\n            eval_every=eval_every\n        )",
  "def __init__(self, model: Module, optimizer: Optimizer, criterion,\n                 mem_size: int = 200, n_classes: int = 10, p_size: int = 100,\n                 alpha: float = 0.99, T: float = 0.1,\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = None, device=None,\n                 plugins: Optional[List[StrategyPlugin]] = None,\n                 evaluator: EvaluationPlugin = default_logger,\n                 eval_every=-1):\n        \"\"\" Continual Prototype Evolution strategy.\n        See CoPEPlugin for more details.\n        This strategy does not use task identities during training.\n\n        :param model: The model.\n        :param optimizer: The optimizer to use.\n        :param criterion: Loss criterion to use. Standard overwritten by\n        PPPloss (see CoPEPlugin).\n        :param mem_size: replay buffer size.\n        :param n_classes: total number of classes that will be encountered. This\n        is used to output predictions for all classes, with zero probability\n        for unseen classes.\n        :param p_size: The prototype size, which equals the feature size of the\n        last layer.\n        :param alpha: The momentum for the exponentially moving average of the\n        prototypes.\n        :param T: The softmax temperature, used as a concentration parameter.\n        :param train_mb_size: The train minibatch size. Defaults to 1.\n        :param train_epochs: The number of training epochs. Defaults to 1.\n        :param eval_mb_size: The eval minibatch size. Defaults to 1.\n        :param device: The device to use. Defaults to None (cpu).\n        :param plugins: Plugins to be added. Defaults to None.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n        copep = CoPEPlugin(mem_size, n_classes, p_size, alpha, T)\n        if plugins is None:\n            plugins = [copep]\n        else:\n            plugins.append(copep)\n        super().__init__(\n            model, optimizer, criterion,\n            train_mb_size=train_mb_size, train_epochs=train_epochs,\n            eval_mb_size=eval_mb_size, device=device, plugins=plugins,\n            evaluator=evaluator, eval_every=eval_every)",
  "class AlreadyTrainedError(Exception):\n    pass",
  "class JointTraining(BaseStrategy):\n    def __init__(self, model: Module, optimizer: Optimizer, criterion,\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = 1, device='cpu',\n                 plugins: Optional[Sequence['StrategyPlugin']] = None,\n                 evaluator=default_logger):\n        \"\"\"\n        JointTraining performs joint training (also called offline training) on\n        the entire stream of data. This means that it is not a continual\n        learning strategy but it can be used as an \"offline\" upper bound for\n        them.\n\n        .. warnings also::\n            Currently :py:class:`JointTraining` adapts its own dataset.\n            Please check that the plugins you are using do not implement\n            :py:meth:`adapt_trainin_dataset`. Otherwise, they are incompatible\n            with :py:class:`JointTraining`.\n\n        :param model: PyTorch model.\n        :param optimizer: PyTorch optimizer.\n        :param criterion: loss function.\n        :param train_mb_size: mini-batch size for training.\n        :param train_epochs: number of training epochs.\n        :param eval_mb_size: mini-batch size for eval.\n        :param device: PyTorch device to run the model.\n        :param plugins: (optional) list of StrategyPlugins.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations. None to remove logging.\n        \"\"\"\n        super().__init__(model, optimizer, criterion, train_mb_size,\n                         train_epochs, eval_mb_size, device, plugins, evaluator)\n        # JointTraining can be trained only once.\n        self._is_fitted = False\n\n    def train(self, experiences: Union[Experience, Sequence[Experience]],\n              eval_streams: Optional[Sequence[Union[Experience,\n                                                    Sequence[\n                                                        Experience]]]] = None,\n              **kwargs):\n        \"\"\" Training loop. if experiences is a single element trains on it.\n        If it is a sequence, trains the model on each experience in order.\n        This is different from joint training on the entire stream.\n        It returns a dictionary with last recorded value for each metric.\n\n        :param experiences: single Experience or sequence.\n        :param eval_streams: list of streams for evaluation.\n            If None: use training experiences for evaluation.\n            Use [] if you do not want to evaluate during training.\n\n        :return: dictionary containing last recorded value for\n            each metric name.\n        \"\"\"\n        self.is_training = True\n        self.model.train()\n        self.model.to(self.device)\n\n        if self._is_fitted:\n            raise AlreadyTrainedError(\n                \"JointTraining can be trained only once. \"\n                \"Please call the train method once on the entire stream.\"\n            )\n\n        # Normalize training and eval data.\n        if isinstance(experiences, Experience):\n            experiences = [experiences]\n        if eval_streams is None:\n            eval_streams = [experiences]\n        for i, exp in enumerate(eval_streams):\n            if isinstance(exp, Experience):\n                eval_streams[i] = [exp]\n\n        self._experiences = experiences\n        self.before_training(**kwargs)\n        for exp in experiences:\n            self.train_exp(exp, eval_streams, **kwargs)\n            # Joint training only needs a single step because\n            # it concatenates all the data at once.\n            break\n        self.after_training(**kwargs)\n\n        res = self.evaluator.get_last_metrics()\n        self._is_fitted = True\n        return res\n\n    def train_dataset_adaptation(self, **kwargs):\n        \"\"\" Concatenates all the datastream. \"\"\"\n        self.adapted_dataset = self._experiences[0].dataset\n        for exp in self._experiences[1:]:\n            cat_data = AvalancheConcatDataset([self.adapted_dataset,\n                                               exp.dataset])\n            self.adapted_dataset = cat_data\n        self.adapted_dataset = self.adapted_dataset.train()",
  "def __init__(self, model: Module, optimizer: Optimizer, criterion,\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = 1, device='cpu',\n                 plugins: Optional[Sequence['StrategyPlugin']] = None,\n                 evaluator=default_logger):\n        \"\"\"\n        JointTraining performs joint training (also called offline training) on\n        the entire stream of data. This means that it is not a continual\n        learning strategy but it can be used as an \"offline\" upper bound for\n        them.\n\n        .. warnings also::\n            Currently :py:class:`JointTraining` adapts its own dataset.\n            Please check that the plugins you are using do not implement\n            :py:meth:`adapt_trainin_dataset`. Otherwise, they are incompatible\n            with :py:class:`JointTraining`.\n\n        :param model: PyTorch model.\n        :param optimizer: PyTorch optimizer.\n        :param criterion: loss function.\n        :param train_mb_size: mini-batch size for training.\n        :param train_epochs: number of training epochs.\n        :param eval_mb_size: mini-batch size for eval.\n        :param device: PyTorch device to run the model.\n        :param plugins: (optional) list of StrategyPlugins.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations. None to remove logging.\n        \"\"\"\n        super().__init__(model, optimizer, criterion, train_mb_size,\n                         train_epochs, eval_mb_size, device, plugins, evaluator)\n        # JointTraining can be trained only once.\n        self._is_fitted = False",
  "def train(self, experiences: Union[Experience, Sequence[Experience]],\n              eval_streams: Optional[Sequence[Union[Experience,\n                                                    Sequence[\n                                                        Experience]]]] = None,\n              **kwargs):\n        \"\"\" Training loop. if experiences is a single element trains on it.\n        If it is a sequence, trains the model on each experience in order.\n        This is different from joint training on the entire stream.\n        It returns a dictionary with last recorded value for each metric.\n\n        :param experiences: single Experience or sequence.\n        :param eval_streams: list of streams for evaluation.\n            If None: use training experiences for evaluation.\n            Use [] if you do not want to evaluate during training.\n\n        :return: dictionary containing last recorded value for\n            each metric name.\n        \"\"\"\n        self.is_training = True\n        self.model.train()\n        self.model.to(self.device)\n\n        if self._is_fitted:\n            raise AlreadyTrainedError(\n                \"JointTraining can be trained only once. \"\n                \"Please call the train method once on the entire stream.\"\n            )\n\n        # Normalize training and eval data.\n        if isinstance(experiences, Experience):\n            experiences = [experiences]\n        if eval_streams is None:\n            eval_streams = [experiences]\n        for i, exp in enumerate(eval_streams):\n            if isinstance(exp, Experience):\n                eval_streams[i] = [exp]\n\n        self._experiences = experiences\n        self.before_training(**kwargs)\n        for exp in experiences:\n            self.train_exp(exp, eval_streams, **kwargs)\n            # Joint training only needs a single step because\n            # it concatenates all the data at once.\n            break\n        self.after_training(**kwargs)\n\n        res = self.evaluator.get_last_metrics()\n        self._is_fitted = True\n        return res",
  "def train_dataset_adaptation(self, **kwargs):\n        \"\"\" Concatenates all the datastream. \"\"\"\n        self.adapted_dataset = self._experiences[0].dataset\n        for exp in self._experiences[1:]:\n            cat_data = AvalancheConcatDataset([self.adapted_dataset,\n                                               exp.dataset])\n            self.adapted_dataset = cat_data\n        self.adapted_dataset = self.adapted_dataset.train()",
  "class AR1(BaseStrategy):\n    \"\"\"\n    The AR1 strategy with Latent Replay.\n\n    This implementations allows for the use of both Synaptic Intelligence and\n    Latent Replay to protect the lower level of the model from forgetting.\n\n    While the original papers show how to use those two techniques in a mutual\n    exclusive way, this implementation allows for the use of both of them\n    concurrently. This behaviour is controlled by passing proper constructor\n    arguments).\n    \"\"\"\n\n    def __init__(self, criterion=None, lr: float = 0.001, momentum=0.9,\n                 l2=0.0005, train_epochs: int = 4,\n                 init_update_rate: float = 0.01,\n                 inc_update_rate=0.00005,\n                 max_r_max=1.25, max_d_max=0.5, inc_step=4.1e-05,\n                 rm_sz: int = 1500,\n                 freeze_below_layer: str = \"lat_features.19.bn.beta\",\n                 latent_layer_num: int = 19, ewc_lambda: float = 0,\n                 train_mb_size: int = 128, eval_mb_size: int = 128, device=None,\n                 plugins: Optional[Sequence[StrategyPlugin]] = None,\n                 evaluator: EvaluationPlugin = default_logger, eval_every=-1):\n        \"\"\"\n        Creates an instance of the AR1 strategy.\n\n        :param criterion: The loss criterion to use. Defaults to None, in which\n            case the cross entropy loss is used.\n        :param lr: The learning rate (SGD optimizer).\n        :param momentum: The momentum (SGD optimizer).\n        :param l2: The L2 penalty used for weight decay.\n        :param train_epochs: The number of training epochs. Defaults to 4.\n        :param init_update_rate: The initial update rate of BatchReNorm layers.\n        :param inc_update_rate: The incremental update rate of BatchReNorm\n            layers.\n        :param max_r_max: The maximum r value of BatchReNorm layers.\n        :param max_d_max: The maximum d value of BatchReNorm layers.\n        :param inc_step: The incremental step of r and d values of BatchReNorm\n            layers.\n        :param rm_sz: The size of the replay buffer. The replay buffer is shared\n            across classes. Defaults to 1500.\n        :param freeze_below_layer: A string describing the name of the layer\n            to use while freezing the lower (nearest to the input) part of the\n            model. The given layer is not frozen (exclusive).\n        :param latent_layer_num: The number of the layer to use as the Latent\n            Replay Layer. Usually this is the same of `freeze_below_layer`.\n        :param ewc_lambda: The Synaptic Intelligence lambda term. Defaults to\n            0, which means that the Synaptic Intelligence regularization\n            will not be applied.\n        :param train_mb_size: The train minibatch size. Defaults to 128.\n        :param eval_mb_size: The eval minibatch size. Defaults to 128.\n        :param device: The device to use. Defaults to None (cpu).\n        :param plugins: (optional) list of StrategyPlugins.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n\n        warnings.warn(\"The AR1 strategy implementation is in an alpha stage \"\n                      \"and is not perfectly aligned with the paper \"\n                      \"implementation. Please use at your own risk!\")\n\n        if plugins is None:\n            plugins = []\n\n        # Model setup\n        model = MobilenetV1(pretrained=True, latent_layer_num=latent_layer_num)\n        replace_bn_with_brn(\n            model, momentum=init_update_rate, r_d_max_inc_step=inc_step,\n            max_r_max=max_r_max, max_d_max=max_d_max)\n\n        fc_name, fc_layer = get_last_fc_layer(model)\n\n        if ewc_lambda != 0:\n            # Synaptic Intelligence is not applied to the last fully\n            # connected layer (and implicitly to \"freeze below\" ones.\n            plugins.append(SynapticIntelligencePlugin(\n                ewc_lambda, excluded_parameters=[fc_name]))\n\n        self.cwr_plugin = CWRStarPlugin(model, cwr_layer_name=fc_name,\n                                        freeze_remaining_model=False)\n        plugins.append(self.cwr_plugin)\n\n        optimizer = SGD(model.parameters(), lr=lr, momentum=momentum,\n                        weight_decay=l2)\n\n        if criterion is None:\n            criterion = CrossEntropyLoss()\n\n        self.ewc_lambda = ewc_lambda\n        self.freeze_below_layer = freeze_below_layer\n        self.rm_sz = rm_sz\n        self.inc_update_rate = inc_update_rate\n        self.max_r_max = max_r_max\n        self.max_d_max = max_d_max\n        self.lr = lr\n        self.momentum = momentum\n        self.l2 = l2\n        self.rm = None\n        self.cur_acts: Optional[Tensor] = None\n        self.replay_mb_size = 0\n\n        super().__init__(\n            model, optimizer, criterion,\n            train_mb_size=train_mb_size, train_epochs=train_epochs,\n            eval_mb_size=eval_mb_size, device=device, plugins=plugins,\n            evaluator=evaluator, eval_every=eval_every)\n\n    def before_training_exp(self, **kwargs):\n        self.model.eval()\n        self.model.end_features.train()\n        self.model.output.train()\n\n        if self.training_exp_counter > 0:\n            # In AR1 batch 0 is treated differently as the feature extractor is\n            # left more free to learn.\n            # This if is executed for batch > 0, in which we freeze layers\n            # below \"self.freeze_below_layer\" (which usually is the latent\n            # replay layer!) and we also change the parameters of BatchReNorm\n            # layers to a more conservative configuration.\n\n            # \"freeze_up_to\" will freeze layers below \"freeze_below_layer\"\n            # Beware that Batch ReNorm layers are not frozen!\n            freeze_up_to(self.model, freeze_until_layer=self.freeze_below_layer,\n                         layer_filter=AR1.filter_bn_and_brn)\n\n            # Adapt the parameters of BatchReNorm layers\n            change_brn_pars(self.model, momentum=self.inc_update_rate,\n                            r_d_max_inc_step=0, r_max=self.max_r_max,\n                            d_max=self.max_d_max)\n\n            # Adapt the model and optimizer\n            self.model = self.model.to(self.device)\n            self.optimizer = SGD(\n                self.model.parameters(), lr=self.lr, momentum=self.momentum,\n                weight_decay=self.l2)\n\n        # super()... will run S.I. and CWR* plugin callbacks\n        super().before_training_exp(**kwargs)\n\n        # Update cur_j of CWR* to consider latent patterns\n        if self.training_exp_counter > 0:\n            for class_id, count in examples_per_class(self.rm[1]).items():\n                self.model.cur_j[class_id] += count\n            self.cwr_plugin.cur_class = [\n                cls for cls in set(self.model.cur_j.keys())\n                if self.model.cur_j[cls] > 0]\n            self.cwr_plugin.reset_weights(self.cwr_plugin.cur_class)\n\n    def make_train_dataloader(self, num_workers=0, shuffle=True, **kwargs):\n        \"\"\"\n        Called after the dataset instantiation. Initialize the data loader.\n\n        For AR1 a \"custom\" dataloader is used: instead of using\n        `self.train_mb_size` as the batch size, the data loader batch size will\n        be computed ad `self.train_mb_size - latent_mb_size`. `latent_mb_size`\n        is in turn computed as:\n\n        `\n        len(train_dataset) // ((len(train_dataset) + len(replay_buffer)\n        // self.train_mb_size)\n        `\n\n        so that the number of iterations required to run an epoch on the current\n        batch is equal to the number of iterations required to run an epoch\n        on the replay buffer.\n\n        :param num_workers: number of thread workers for the data loading.\n        :param shuffle: True if the data should be shuffled, False otherwise.\n        \"\"\"\n\n        current_batch_mb_size = self.train_mb_size\n\n        if self.training_exp_counter > 0:\n            train_patterns = len(self.adapted_dataset)\n            current_batch_mb_size = train_patterns // (\n                    (train_patterns + self.rm_sz) // self.train_mb_size)\n\n        current_batch_mb_size = max(1, current_batch_mb_size)\n        self.replay_mb_size = max(0, self.train_mb_size - current_batch_mb_size)\n\n        # AR1 only supports SIT scenarios (no task labels).\n        self.dataloader = DataLoader(\n            self.adapted_dataset, num_workers=num_workers,\n            batch_size=current_batch_mb_size, shuffle=shuffle)\n\n    def training_epoch(self, **kwargs):\n        for self.mb_it, self.mbatch in \\\n                enumerate(self.dataloader):\n            self.before_training_iteration(**kwargs)\n\n            self.optimizer.zero_grad()\n            if self.training_exp_counter > 0:\n                lat_mb_x = self.rm[0][self.mb_it * self.replay_mb_size:\n                                      (self.mb_it + 1) * self.replay_mb_size]\n                lat_mb_x = lat_mb_x.to(self.device)\n                lat_mb_y = self.rm[1][self.mb_it * self.replay_mb_size:\n                                      (self.mb_it + 1) * self.replay_mb_size]\n                lat_mb_y = lat_mb_y.to(self.device)\n                self.mbatch[1] = torch.cat((self.mb_y, lat_mb_y), 0)\n            else:\n                lat_mb_x = None\n\n            # Forward pass. Here we are injecting latent patterns lat_mb_x.\n            # lat_mb_x will be None for the very first batch (batch 0), which\n            # means that lat_acts.shape[0] == self.mb_x[0].\n            self.before_forward(**kwargs)\n            self.mb_output, lat_acts = self.model(\n                self.mb_x, latent_input=lat_mb_x, return_lat_acts=True)\n\n            if self.epoch == 0:\n                # On the first epoch only: store latent activations. Those\n                # activations will be used to update the replay buffer.\n                lat_acts = lat_acts.detach().clone().cpu()\n                if self.mb_it == 0:\n                    self.cur_acts = lat_acts\n                else:\n                    self.cur_acts = torch.cat((self.cur_acts, lat_acts), 0)\n            self.after_forward(**kwargs)\n\n            # Loss & Backward\n            # We don't need to handle latent replay, as self.mb_y already\n            # contains both current and replay labels.\n            self.loss = self._criterion(self.mb_output, self.mb_y)\n            self.before_backward(**kwargs)\n            self.loss.backward()\n            self.after_backward(**kwargs)\n\n            # Optimization step\n            self.before_update(**kwargs)\n            self.optimizer.step()\n            self.after_update(**kwargs)\n\n            self.after_training_iteration(**kwargs)\n\n    def after_training_exp(self, **kwargs):\n        h = min(self.rm_sz // (self.training_exp_counter + 1),\n                self.cur_acts.size(0))\n\n        curr_data = self.experience.dataset\n        idxs_cur = torch.randperm(self.cur_acts.size(0))[:h]\n        rm_add_y = torch.tensor(\n            [curr_data.targets[idx_cur] for idx_cur in idxs_cur])\n\n        rm_add = [self.cur_acts[idxs_cur], rm_add_y]\n\n        # replace patterns in random memory\n        if self.training_exp_counter == 0:\n            self.rm = rm_add\n        else:\n            idxs_2_replace = torch.randperm(self.rm[0].size(0))[:h]\n            for j, idx in enumerate(idxs_2_replace):\n                idx = int(idx)\n                self.rm[0][idx] = rm_add[0][j]\n                self.rm[1][idx] = rm_add[1][j]\n\n        self.cur_acts = None\n\n        # Runs S.I. and CWR* plugin callbacks\n        super().after_training_exp(**kwargs)\n\n    @staticmethod\n    def filter_bn_and_brn(param_def: LayerAndParameter):\n        return not isinstance(param_def.layer, (_NormBase, BatchRenorm2D))",
  "def __init__(self, criterion=None, lr: float = 0.001, momentum=0.9,\n                 l2=0.0005, train_epochs: int = 4,\n                 init_update_rate: float = 0.01,\n                 inc_update_rate=0.00005,\n                 max_r_max=1.25, max_d_max=0.5, inc_step=4.1e-05,\n                 rm_sz: int = 1500,\n                 freeze_below_layer: str = \"lat_features.19.bn.beta\",\n                 latent_layer_num: int = 19, ewc_lambda: float = 0,\n                 train_mb_size: int = 128, eval_mb_size: int = 128, device=None,\n                 plugins: Optional[Sequence[StrategyPlugin]] = None,\n                 evaluator: EvaluationPlugin = default_logger, eval_every=-1):\n        \"\"\"\n        Creates an instance of the AR1 strategy.\n\n        :param criterion: The loss criterion to use. Defaults to None, in which\n            case the cross entropy loss is used.\n        :param lr: The learning rate (SGD optimizer).\n        :param momentum: The momentum (SGD optimizer).\n        :param l2: The L2 penalty used for weight decay.\n        :param train_epochs: The number of training epochs. Defaults to 4.\n        :param init_update_rate: The initial update rate of BatchReNorm layers.\n        :param inc_update_rate: The incremental update rate of BatchReNorm\n            layers.\n        :param max_r_max: The maximum r value of BatchReNorm layers.\n        :param max_d_max: The maximum d value of BatchReNorm layers.\n        :param inc_step: The incremental step of r and d values of BatchReNorm\n            layers.\n        :param rm_sz: The size of the replay buffer. The replay buffer is shared\n            across classes. Defaults to 1500.\n        :param freeze_below_layer: A string describing the name of the layer\n            to use while freezing the lower (nearest to the input) part of the\n            model. The given layer is not frozen (exclusive).\n        :param latent_layer_num: The number of the layer to use as the Latent\n            Replay Layer. Usually this is the same of `freeze_below_layer`.\n        :param ewc_lambda: The Synaptic Intelligence lambda term. Defaults to\n            0, which means that the Synaptic Intelligence regularization\n            will not be applied.\n        :param train_mb_size: The train minibatch size. Defaults to 128.\n        :param eval_mb_size: The eval minibatch size. Defaults to 128.\n        :param device: The device to use. Defaults to None (cpu).\n        :param plugins: (optional) list of StrategyPlugins.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n\n        warnings.warn(\"The AR1 strategy implementation is in an alpha stage \"\n                      \"and is not perfectly aligned with the paper \"\n                      \"implementation. Please use at your own risk!\")\n\n        if plugins is None:\n            plugins = []\n\n        # Model setup\n        model = MobilenetV1(pretrained=True, latent_layer_num=latent_layer_num)\n        replace_bn_with_brn(\n            model, momentum=init_update_rate, r_d_max_inc_step=inc_step,\n            max_r_max=max_r_max, max_d_max=max_d_max)\n\n        fc_name, fc_layer = get_last_fc_layer(model)\n\n        if ewc_lambda != 0:\n            # Synaptic Intelligence is not applied to the last fully\n            # connected layer (and implicitly to \"freeze below\" ones.\n            plugins.append(SynapticIntelligencePlugin(\n                ewc_lambda, excluded_parameters=[fc_name]))\n\n        self.cwr_plugin = CWRStarPlugin(model, cwr_layer_name=fc_name,\n                                        freeze_remaining_model=False)\n        plugins.append(self.cwr_plugin)\n\n        optimizer = SGD(model.parameters(), lr=lr, momentum=momentum,\n                        weight_decay=l2)\n\n        if criterion is None:\n            criterion = CrossEntropyLoss()\n\n        self.ewc_lambda = ewc_lambda\n        self.freeze_below_layer = freeze_below_layer\n        self.rm_sz = rm_sz\n        self.inc_update_rate = inc_update_rate\n        self.max_r_max = max_r_max\n        self.max_d_max = max_d_max\n        self.lr = lr\n        self.momentum = momentum\n        self.l2 = l2\n        self.rm = None\n        self.cur_acts: Optional[Tensor] = None\n        self.replay_mb_size = 0\n\n        super().__init__(\n            model, optimizer, criterion,\n            train_mb_size=train_mb_size, train_epochs=train_epochs,\n            eval_mb_size=eval_mb_size, device=device, plugins=plugins,\n            evaluator=evaluator, eval_every=eval_every)",
  "def before_training_exp(self, **kwargs):\n        self.model.eval()\n        self.model.end_features.train()\n        self.model.output.train()\n\n        if self.training_exp_counter > 0:\n            # In AR1 batch 0 is treated differently as the feature extractor is\n            # left more free to learn.\n            # This if is executed for batch > 0, in which we freeze layers\n            # below \"self.freeze_below_layer\" (which usually is the latent\n            # replay layer!) and we also change the parameters of BatchReNorm\n            # layers to a more conservative configuration.\n\n            # \"freeze_up_to\" will freeze layers below \"freeze_below_layer\"\n            # Beware that Batch ReNorm layers are not frozen!\n            freeze_up_to(self.model, freeze_until_layer=self.freeze_below_layer,\n                         layer_filter=AR1.filter_bn_and_brn)\n\n            # Adapt the parameters of BatchReNorm layers\n            change_brn_pars(self.model, momentum=self.inc_update_rate,\n                            r_d_max_inc_step=0, r_max=self.max_r_max,\n                            d_max=self.max_d_max)\n\n            # Adapt the model and optimizer\n            self.model = self.model.to(self.device)\n            self.optimizer = SGD(\n                self.model.parameters(), lr=self.lr, momentum=self.momentum,\n                weight_decay=self.l2)\n\n        # super()... will run S.I. and CWR* plugin callbacks\n        super().before_training_exp(**kwargs)\n\n        # Update cur_j of CWR* to consider latent patterns\n        if self.training_exp_counter > 0:\n            for class_id, count in examples_per_class(self.rm[1]).items():\n                self.model.cur_j[class_id] += count\n            self.cwr_plugin.cur_class = [\n                cls for cls in set(self.model.cur_j.keys())\n                if self.model.cur_j[cls] > 0]\n            self.cwr_plugin.reset_weights(self.cwr_plugin.cur_class)",
  "def make_train_dataloader(self, num_workers=0, shuffle=True, **kwargs):\n        \"\"\"\n        Called after the dataset instantiation. Initialize the data loader.\n\n        For AR1 a \"custom\" dataloader is used: instead of using\n        `self.train_mb_size` as the batch size, the data loader batch size will\n        be computed ad `self.train_mb_size - latent_mb_size`. `latent_mb_size`\n        is in turn computed as:\n\n        `\n        len(train_dataset) // ((len(train_dataset) + len(replay_buffer)\n        // self.train_mb_size)\n        `\n\n        so that the number of iterations required to run an epoch on the current\n        batch is equal to the number of iterations required to run an epoch\n        on the replay buffer.\n\n        :param num_workers: number of thread workers for the data loading.\n        :param shuffle: True if the data should be shuffled, False otherwise.\n        \"\"\"\n\n        current_batch_mb_size = self.train_mb_size\n\n        if self.training_exp_counter > 0:\n            train_patterns = len(self.adapted_dataset)\n            current_batch_mb_size = train_patterns // (\n                    (train_patterns + self.rm_sz) // self.train_mb_size)\n\n        current_batch_mb_size = max(1, current_batch_mb_size)\n        self.replay_mb_size = max(0, self.train_mb_size - current_batch_mb_size)\n\n        # AR1 only supports SIT scenarios (no task labels).\n        self.dataloader = DataLoader(\n            self.adapted_dataset, num_workers=num_workers,\n            batch_size=current_batch_mb_size, shuffle=shuffle)",
  "def training_epoch(self, **kwargs):\n        for self.mb_it, self.mbatch in \\\n                enumerate(self.dataloader):\n            self.before_training_iteration(**kwargs)\n\n            self.optimizer.zero_grad()\n            if self.training_exp_counter > 0:\n                lat_mb_x = self.rm[0][self.mb_it * self.replay_mb_size:\n                                      (self.mb_it + 1) * self.replay_mb_size]\n                lat_mb_x = lat_mb_x.to(self.device)\n                lat_mb_y = self.rm[1][self.mb_it * self.replay_mb_size:\n                                      (self.mb_it + 1) * self.replay_mb_size]\n                lat_mb_y = lat_mb_y.to(self.device)\n                self.mbatch[1] = torch.cat((self.mb_y, lat_mb_y), 0)\n            else:\n                lat_mb_x = None\n\n            # Forward pass. Here we are injecting latent patterns lat_mb_x.\n            # lat_mb_x will be None for the very first batch (batch 0), which\n            # means that lat_acts.shape[0] == self.mb_x[0].\n            self.before_forward(**kwargs)\n            self.mb_output, lat_acts = self.model(\n                self.mb_x, latent_input=lat_mb_x, return_lat_acts=True)\n\n            if self.epoch == 0:\n                # On the first epoch only: store latent activations. Those\n                # activations will be used to update the replay buffer.\n                lat_acts = lat_acts.detach().clone().cpu()\n                if self.mb_it == 0:\n                    self.cur_acts = lat_acts\n                else:\n                    self.cur_acts = torch.cat((self.cur_acts, lat_acts), 0)\n            self.after_forward(**kwargs)\n\n            # Loss & Backward\n            # We don't need to handle latent replay, as self.mb_y already\n            # contains both current and replay labels.\n            self.loss = self._criterion(self.mb_output, self.mb_y)\n            self.before_backward(**kwargs)\n            self.loss.backward()\n            self.after_backward(**kwargs)\n\n            # Optimization step\n            self.before_update(**kwargs)\n            self.optimizer.step()\n            self.after_update(**kwargs)\n\n            self.after_training_iteration(**kwargs)",
  "def after_training_exp(self, **kwargs):\n        h = min(self.rm_sz // (self.training_exp_counter + 1),\n                self.cur_acts.size(0))\n\n        curr_data = self.experience.dataset\n        idxs_cur = torch.randperm(self.cur_acts.size(0))[:h]\n        rm_add_y = torch.tensor(\n            [curr_data.targets[idx_cur] for idx_cur in idxs_cur])\n\n        rm_add = [self.cur_acts[idxs_cur], rm_add_y]\n\n        # replace patterns in random memory\n        if self.training_exp_counter == 0:\n            self.rm = rm_add\n        else:\n            idxs_2_replace = torch.randperm(self.rm[0].size(0))[:h]\n            for j, idx in enumerate(idxs_2_replace):\n                idx = int(idx)\n                self.rm[0][idx] = rm_add[0][j]\n                self.rm[1][idx] = rm_add[1][j]\n\n        self.cur_acts = None\n\n        # Runs S.I. and CWR* plugin callbacks\n        super().after_training_exp(**kwargs)",
  "def filter_bn_and_brn(param_def: LayerAndParameter):\n        return not isinstance(param_def.layer, (_NormBase, BatchRenorm2D))",
  "class Cumulative(BaseStrategy):\n\n    def __init__(self, model: Module, optimizer: Optimizer, criterion,\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = None, device=None,\n                 plugins: Optional[List[StrategyPlugin]] = None,\n                 evaluator: EvaluationPlugin = default_logger, eval_every=-1):\n        \"\"\" Cumulative strategy. At each experience,\n            train model with data from all previous experiences and current\n            experience.\n\n        :param model: The model.\n        :param optimizer: The optimizer to use.\n        :param criterion: The loss criterion to use.\n        :param train_mb_size: The train minibatch size. Defaults to 1.\n        :param train_epochs: The number of training epochs. Defaults to 1.\n        :param eval_mb_size: The eval minibatch size. Defaults to 1.\n        :param device: The device to use. Defaults to None (cpu).\n        :param plugins: Plugins to be added. Defaults to None.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n\n        super().__init__(\n            model, optimizer, criterion,\n            train_mb_size=train_mb_size, train_epochs=train_epochs,\n            eval_mb_size=eval_mb_size, device=device, plugins=plugins,\n            evaluator=evaluator, eval_every=eval_every)\n\n        self.dataset = None  # cumulative dataset\n\n    def train_dataset_adaptation(self, **kwargs):\n        \"\"\"\n            Concatenates all the previous experiences.\n        \"\"\"\n        if self.dataset is None:\n            self.dataset = self.experience.dataset\n        else:\n            self.dataset = AvalancheConcatDataset(\n                [self.dataset, self.experience.dataset])\n        self.adapted_dataset = self.dataset",
  "def __init__(self, model: Module, optimizer: Optimizer, criterion,\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = None, device=None,\n                 plugins: Optional[List[StrategyPlugin]] = None,\n                 evaluator: EvaluationPlugin = default_logger, eval_every=-1):\n        \"\"\" Cumulative strategy. At each experience,\n            train model with data from all previous experiences and current\n            experience.\n\n        :param model: The model.\n        :param optimizer: The optimizer to use.\n        :param criterion: The loss criterion to use.\n        :param train_mb_size: The train minibatch size. Defaults to 1.\n        :param train_epochs: The number of training epochs. Defaults to 1.\n        :param eval_mb_size: The eval minibatch size. Defaults to 1.\n        :param device: The device to use. Defaults to None (cpu).\n        :param plugins: Plugins to be added. Defaults to None.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n\n        super().__init__(\n            model, optimizer, criterion,\n            train_mb_size=train_mb_size, train_epochs=train_epochs,\n            eval_mb_size=eval_mb_size, device=device, plugins=plugins,\n            evaluator=evaluator, eval_every=eval_every)\n\n        self.dataset = None",
  "def train_dataset_adaptation(self, **kwargs):\n        \"\"\"\n            Concatenates all the previous experiences.\n        \"\"\"\n        if self.dataset is None:\n            self.dataset = self.experience.dataset\n        else:\n            self.dataset = AvalancheConcatDataset(\n                [self.dataset, self.experience.dataset])\n        self.adapted_dataset = self.dataset",
  "class ICaRL(BaseStrategy):\n    def __init__(self, feature_extractor: Module, classifier: Module,\n                 optimizer: Optimizer, memory_size, buffer_transform,\n                 fixed_memory, criterion=ICaRLLossPlugin(),\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = None, device=None,\n                 plugins: Optional[List[StrategyPlugin]] = None,\n                 evaluator: EvaluationPlugin = default_logger, eval_every=-1):\n        \"\"\" iCaRL Strategy.\n        This strategy does not use task identities.\n\n        :param feature_extractor: The feature extractor.\n        :param classifier: The differentiable classifier that takes as input\n            the output of the feature extractor.\n        :param optimizer: The optimizer to use.\n        :param memory_size: The nuber of patterns saved in the memory.\n        :param buffer_transform: transform applied on buffer elements already\n            modified by test_transform (if specified) before being used for\n             replay\n        :param fixed_memory: If True a memory of size memory_size is\n            allocated and partitioned between samples from the observed\n            experiences. If False every time a new class is observed\n            memory_size samples of that class are added to the memory.\n        :param train_mb_size: The train minibatch size. Defaults to 1.\n        :param train_epochs: The number of training epochs. Defaults to 1.\n        :param eval_mb_size: The eval minibatch size. Defaults to 1.\n        :param device: The device to use. Defaults to None (cpu).\n        :param plugins: Plugins to be added. Defaults to None.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n        model = TrainEvalModel(feature_extractor,\n                               train_classifier=classifier,\n                               eval_classifier=NCMClassifier())\n\n        icarl = _ICaRLPlugin(memory_size, buffer_transform, fixed_memory)\n\n        if plugins is None:\n            plugins = [icarl]\n        else:\n            plugins += [icarl]\n\n        if isinstance(criterion, StrategyPlugin):\n            plugins += [criterion]\n\n        super().__init__(\n            model, optimizer, criterion=criterion,\n            train_mb_size=train_mb_size, train_epochs=train_epochs,\n            eval_mb_size=eval_mb_size, device=device, plugins=plugins,\n            evaluator=evaluator, eval_every=eval_every)",
  "class _ICaRLPlugin(StrategyPlugin):\n    \"\"\"\n        iCaRL Plugin.\n        iCaRL uses nearest class exemplar classification to prevent\n        forgetting to occur at the classification layer. The feature extractor\n        is continually learned using replay and distillation. The exemplars\n        used for replay and classification are selected through herding.\n        This plugin does not use task identities.\n        \"\"\"\n\n    def __init__(self, memory_size, buffer_transform=None, fixed_memory=True):\n        \"\"\"\n        :param memory_size: amount of patterns saved in the memory.\n        :param buffer_transform: transform applied on buffer elements already\n            modified by test_transform (if specified) before being used for\n             replay\n        :param fixed_memory: If True a memory of size memory_size is\n            allocated and partitioned between samples from the observed\n            experiences. If False every time a new class is observed\n            memory_size samples of that class are added to the memory.\n        \"\"\"\n        super().__init__()\n\n        self.memory_size = memory_size\n        self.buffer_transform = buffer_transform\n        self.fixed_memory = fixed_memory\n\n        self.x_memory = []\n        self.y_memory = []\n        self.order = []\n\n        self.old_model = None\n        self.observed_classes = []\n        self.class_means = None\n        self.embedding_size = None\n        self.output_size = None\n        self.input_size = None\n\n    def after_train_dataset_adaptation(self, strategy: 'BaseStrategy',\n                                       **kwargs):\n        if strategy.training_exp_counter != 0:\n            memory = AvalancheTensorDataset(\n                torch.cat(self.x_memory).cpu(),\n                list(itertools.chain.from_iterable(self.y_memory)),\n                transform=self.buffer_transform, target_transform=None)\n\n            strategy.adapted_dataset = \\\n                AvalancheConcatDataset((strategy.adapted_dataset, memory))\n\n    def before_training_exp(self, strategy: 'BaseStrategy', **kwargs):\n        tid = strategy.training_exp_counter\n        benchmark = strategy.experience.benchmark\n        nb_cl = benchmark.n_classes_per_exp[tid]\n\n        self.observed_classes.extend(\n            benchmark.classes_order[tid * nb_cl:(tid + 1) * nb_cl])\n\n    def before_forward(self, strategy: 'BaseStrategy', **kwargs):\n        if self.input_size is None:\n            with torch.no_grad():\n                self.input_size = strategy.mb_x.shape[1:]\n                self.output_size = strategy.model(strategy.mb_x).shape[1]\n                self.embedding_size = strategy.model.feature_extractor(\n                    strategy.mb_x).shape[1]\n\n    def after_training_exp(self, strategy: 'BaseStrategy', **kwargs):\n        strategy.model.eval()\n\n        self.construct_exemplar_set(strategy)\n        self.reduce_exemplar_set(strategy)\n        self.compute_class_means(strategy)\n\n    def compute_class_means(self, strategy):\n        if self.class_means is None:\n            n_classes = sum(strategy.experience.benchmark.n_classes_per_exp)\n            self.class_means = torch.zeros(\n                (self.embedding_size, n_classes)).to(strategy.device)\n\n        for i, class_samples in enumerate(self.x_memory):\n            label = self.y_memory[i][0]\n            class_samples = class_samples.to(strategy.device)\n\n            with torch.no_grad():\n                mapped_prototypes = strategy.model.feature_extractor(\n                    class_samples).detach()\n            D = mapped_prototypes.T\n            D = D / torch.norm(D, dim=0)\n\n            if len(class_samples.shape) == 4:\n                class_samples = torch.flip(class_samples, [3])\n\n            with torch.no_grad():\n                mapped_prototypes2 = strategy.model.feature_extractor(\n                    class_samples).detach()\n\n            D2 = mapped_prototypes2.T\n            D2 = D2 / torch.norm(D2, dim=0)\n\n            div = torch.ones(class_samples.shape[0], device=strategy.device)\n            div = div / class_samples.shape[0]\n\n            m1 = torch.mm(D, div.unsqueeze(1)).squeeze(1)\n            m2 = torch.mm(D2, div.unsqueeze(1)).squeeze(1)\n            self.class_means[:, label] = (m1 + m2) / 2\n            self.class_means[:, label] /= torch.norm(self.class_means[:, label])\n\n            strategy.model.eval_classifier.class_means = self.class_means\n\n    def construct_exemplar_set(self, strategy):\n        tid = strategy.training_exp_counter\n        nb_cl = strategy.experience.benchmark.n_classes_per_exp\n\n        if self.fixed_memory:\n            nb_protos_cl = int(ceil(\n                self.memory_size / len(self.observed_classes)))\n        else:\n            nb_protos_cl = self.memory_size\n        new_classes = self.observed_classes[tid * nb_cl[tid]:\n                                            (tid + 1) * nb_cl[tid]]\n\n        dataset = strategy.experience.dataset\n        targets = torch.tensor(dataset.targets)\n        for iter_dico in range(nb_cl[tid]):\n            cd = AvalancheSubset(dataset,\n                                 torch.where(targets == new_classes[iter_dico])\n                                 [0])\n\n            class_patterns, _, _ = next(iter(\n                DataLoader(cd.eval(), batch_size=len(cd))))\n            class_patterns = class_patterns.to(strategy.device)\n\n            with torch.no_grad():\n                mapped_prototypes = strategy.model.feature_extractor(\n                    class_patterns).detach()\n            D = mapped_prototypes.T\n            D = D / torch.norm(D, dim=0)\n\n            mu = torch.mean(D, dim=1)\n            order = torch.zeros(class_patterns.shape[0])\n            w_t = mu\n\n            i, added, selected = 0, 0, []\n            while not added == nb_protos_cl and i < 1000:\n                tmp_t = torch.mm(w_t.unsqueeze(0), D)\n                ind_max = torch.argmax(tmp_t)\n\n                if ind_max not in selected:\n                    order[ind_max] = 1 + added\n                    added += 1\n                    selected.append(ind_max.item())\n\n                w_t = w_t + mu - D[:, ind_max]\n                i += 1\n\n            pick = (order > 0) * (order < nb_protos_cl + 1) * 1.\n            self.x_memory.append(class_patterns[torch.where(pick == 1)[0]])\n            self.y_memory.append(\n                [new_classes[iter_dico]] * len(torch.where(pick == 1)[0]))\n            self.order.append(order[torch.where(pick == 1)[0]])\n\n    def reduce_exemplar_set(self, strategy):\n        tid = strategy.training_exp_counter\n        nb_cl = strategy.experience.benchmark.n_classes_per_exp\n\n        if self.fixed_memory:\n            nb_protos_cl = int(ceil(\n                self.memory_size / len(self.observed_classes)))\n        else:\n            nb_protos_cl = self.memory_size\n\n        for i in range(len(self.x_memory) - nb_cl[tid]):\n            pick = (self.order[i] < nb_protos_cl + 1) * 1.\n            self.x_memory[i] = self.x_memory[i][torch.where(pick == 1)[0]]\n            self.y_memory[i] = self.y_memory[i][:len(torch.where(pick == 1)[0])]\n            self.order[i] = self.order[i][torch.where(pick == 1)[0]]",
  "def __init__(self, feature_extractor: Module, classifier: Module,\n                 optimizer: Optimizer, memory_size, buffer_transform,\n                 fixed_memory, criterion=ICaRLLossPlugin(),\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = None, device=None,\n                 plugins: Optional[List[StrategyPlugin]] = None,\n                 evaluator: EvaluationPlugin = default_logger, eval_every=-1):\n        \"\"\" iCaRL Strategy.\n        This strategy does not use task identities.\n\n        :param feature_extractor: The feature extractor.\n        :param classifier: The differentiable classifier that takes as input\n            the output of the feature extractor.\n        :param optimizer: The optimizer to use.\n        :param memory_size: The nuber of patterns saved in the memory.\n        :param buffer_transform: transform applied on buffer elements already\n            modified by test_transform (if specified) before being used for\n             replay\n        :param fixed_memory: If True a memory of size memory_size is\n            allocated and partitioned between samples from the observed\n            experiences. If False every time a new class is observed\n            memory_size samples of that class are added to the memory.\n        :param train_mb_size: The train minibatch size. Defaults to 1.\n        :param train_epochs: The number of training epochs. Defaults to 1.\n        :param eval_mb_size: The eval minibatch size. Defaults to 1.\n        :param device: The device to use. Defaults to None (cpu).\n        :param plugins: Plugins to be added. Defaults to None.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience.\n                if >0: calls `eval` every `eval_every` epochs and at the end\n                    of all the epochs for a single experience.\n        \"\"\"\n        model = TrainEvalModel(feature_extractor,\n                               train_classifier=classifier,\n                               eval_classifier=NCMClassifier())\n\n        icarl = _ICaRLPlugin(memory_size, buffer_transform, fixed_memory)\n\n        if plugins is None:\n            plugins = [icarl]\n        else:\n            plugins += [icarl]\n\n        if isinstance(criterion, StrategyPlugin):\n            plugins += [criterion]\n\n        super().__init__(\n            model, optimizer, criterion=criterion,\n            train_mb_size=train_mb_size, train_epochs=train_epochs,\n            eval_mb_size=eval_mb_size, device=device, plugins=plugins,\n            evaluator=evaluator, eval_every=eval_every)",
  "def __init__(self, memory_size, buffer_transform=None, fixed_memory=True):\n        \"\"\"\n        :param memory_size: amount of patterns saved in the memory.\n        :param buffer_transform: transform applied on buffer elements already\n            modified by test_transform (if specified) before being used for\n             replay\n        :param fixed_memory: If True a memory of size memory_size is\n            allocated and partitioned between samples from the observed\n            experiences. If False every time a new class is observed\n            memory_size samples of that class are added to the memory.\n        \"\"\"\n        super().__init__()\n\n        self.memory_size = memory_size\n        self.buffer_transform = buffer_transform\n        self.fixed_memory = fixed_memory\n\n        self.x_memory = []\n        self.y_memory = []\n        self.order = []\n\n        self.old_model = None\n        self.observed_classes = []\n        self.class_means = None\n        self.embedding_size = None\n        self.output_size = None\n        self.input_size = None",
  "def after_train_dataset_adaptation(self, strategy: 'BaseStrategy',\n                                       **kwargs):\n        if strategy.training_exp_counter != 0:\n            memory = AvalancheTensorDataset(\n                torch.cat(self.x_memory).cpu(),\n                list(itertools.chain.from_iterable(self.y_memory)),\n                transform=self.buffer_transform, target_transform=None)\n\n            strategy.adapted_dataset = \\\n                AvalancheConcatDataset((strategy.adapted_dataset, memory))",
  "def before_training_exp(self, strategy: 'BaseStrategy', **kwargs):\n        tid = strategy.training_exp_counter\n        benchmark = strategy.experience.benchmark\n        nb_cl = benchmark.n_classes_per_exp[tid]\n\n        self.observed_classes.extend(\n            benchmark.classes_order[tid * nb_cl:(tid + 1) * nb_cl])",
  "def before_forward(self, strategy: 'BaseStrategy', **kwargs):\n        if self.input_size is None:\n            with torch.no_grad():\n                self.input_size = strategy.mb_x.shape[1:]\n                self.output_size = strategy.model(strategy.mb_x).shape[1]\n                self.embedding_size = strategy.model.feature_extractor(\n                    strategy.mb_x).shape[1]",
  "def after_training_exp(self, strategy: 'BaseStrategy', **kwargs):\n        strategy.model.eval()\n\n        self.construct_exemplar_set(strategy)\n        self.reduce_exemplar_set(strategy)\n        self.compute_class_means(strategy)",
  "def compute_class_means(self, strategy):\n        if self.class_means is None:\n            n_classes = sum(strategy.experience.benchmark.n_classes_per_exp)\n            self.class_means = torch.zeros(\n                (self.embedding_size, n_classes)).to(strategy.device)\n\n        for i, class_samples in enumerate(self.x_memory):\n            label = self.y_memory[i][0]\n            class_samples = class_samples.to(strategy.device)\n\n            with torch.no_grad():\n                mapped_prototypes = strategy.model.feature_extractor(\n                    class_samples).detach()\n            D = mapped_prototypes.T\n            D = D / torch.norm(D, dim=0)\n\n            if len(class_samples.shape) == 4:\n                class_samples = torch.flip(class_samples, [3])\n\n            with torch.no_grad():\n                mapped_prototypes2 = strategy.model.feature_extractor(\n                    class_samples).detach()\n\n            D2 = mapped_prototypes2.T\n            D2 = D2 / torch.norm(D2, dim=0)\n\n            div = torch.ones(class_samples.shape[0], device=strategy.device)\n            div = div / class_samples.shape[0]\n\n            m1 = torch.mm(D, div.unsqueeze(1)).squeeze(1)\n            m2 = torch.mm(D2, div.unsqueeze(1)).squeeze(1)\n            self.class_means[:, label] = (m1 + m2) / 2\n            self.class_means[:, label] /= torch.norm(self.class_means[:, label])\n\n            strategy.model.eval_classifier.class_means = self.class_means",
  "def construct_exemplar_set(self, strategy):\n        tid = strategy.training_exp_counter\n        nb_cl = strategy.experience.benchmark.n_classes_per_exp\n\n        if self.fixed_memory:\n            nb_protos_cl = int(ceil(\n                self.memory_size / len(self.observed_classes)))\n        else:\n            nb_protos_cl = self.memory_size\n        new_classes = self.observed_classes[tid * nb_cl[tid]:\n                                            (tid + 1) * nb_cl[tid]]\n\n        dataset = strategy.experience.dataset\n        targets = torch.tensor(dataset.targets)\n        for iter_dico in range(nb_cl[tid]):\n            cd = AvalancheSubset(dataset,\n                                 torch.where(targets == new_classes[iter_dico])\n                                 [0])\n\n            class_patterns, _, _ = next(iter(\n                DataLoader(cd.eval(), batch_size=len(cd))))\n            class_patterns = class_patterns.to(strategy.device)\n\n            with torch.no_grad():\n                mapped_prototypes = strategy.model.feature_extractor(\n                    class_patterns).detach()\n            D = mapped_prototypes.T\n            D = D / torch.norm(D, dim=0)\n\n            mu = torch.mean(D, dim=1)\n            order = torch.zeros(class_patterns.shape[0])\n            w_t = mu\n\n            i, added, selected = 0, 0, []\n            while not added == nb_protos_cl and i < 1000:\n                tmp_t = torch.mm(w_t.unsqueeze(0), D)\n                ind_max = torch.argmax(tmp_t)\n\n                if ind_max not in selected:\n                    order[ind_max] = 1 + added\n                    added += 1\n                    selected.append(ind_max.item())\n\n                w_t = w_t + mu - D[:, ind_max]\n                i += 1\n\n            pick = (order > 0) * (order < nb_protos_cl + 1) * 1.\n            self.x_memory.append(class_patterns[torch.where(pick == 1)[0]])\n            self.y_memory.append(\n                [new_classes[iter_dico]] * len(torch.where(pick == 1)[0]))\n            self.order.append(order[torch.where(pick == 1)[0]])",
  "def reduce_exemplar_set(self, strategy):\n        tid = strategy.training_exp_counter\n        nb_cl = strategy.experience.benchmark.n_classes_per_exp\n\n        if self.fixed_memory:\n            nb_protos_cl = int(ceil(\n                self.memory_size / len(self.observed_classes)))\n        else:\n            nb_protos_cl = self.memory_size\n\n        for i in range(len(self.x_memory) - nb_cl[tid]):\n            pick = (self.order[i] < nb_protos_cl + 1) * 1.\n            self.x_memory[i] = self.x_memory[i][torch.where(pick == 1)[0]]\n            self.y_memory[i] = self.y_memory[i][:len(torch.where(pick == 1)[0])]\n            self.order[i] = self.order[i][torch.where(pick == 1)[0]]",
  "class StreamingLDA(BaseStrategy):\n    DISABLED_CALLBACKS = (\"before_backward\", \"after_backward\")\n\n    \"\"\"\n    Deep Streaming Linear Discriminant Analysis.\n    This strategy does not use backpropagation.\n    Minibatches are first passed to the pretrained feature extractor.\n    The result is processed one element at a time to fit the\n    LDA.\n    Original paper:\n    \"Hayes et. al., Lifelong Machine Learning with Deep Streaming Linear\n    Discriminant Analysis, CVPR Workshop, 2020\"\n    https://openaccess.thecvf.com/content_CVPRW_2020/papers/w15/Hayes_Lifelong_Machine_Learning_With_Deep_Streaming_Linear_Discriminant_Analysis_CVPRW_2020_paper.pdf\n    \"\"\"\n    def __init__(self, slda_model, criterion,\n                 input_size, num_classes, output_layer_name=None,\n                 shrinkage_param=1e-4, streaming_update_sigma=True,\n                 train_epochs: int = 1, train_mb_size: int = 1,\n                 eval_mb_size: int = 1, device='cpu',\n                 plugins: Optional[Sequence['StrategyPlugin']] = None,\n                 evaluator=default_logger, eval_every=-1):\n        \"\"\"\n        Init function for the SLDA model.\n        :param slda_model: a PyTorch model\n        :param criterion: loss function\n        :param output_layer_name: if not None, wrap model to retrieve\n            only the `output_layer_name` output. If None, the strategy\n            assumes that the model already produces a valid output.\n            You can use `FeatureExtractorBackbone` class to create your custom\n            SLDA-compatible model.\n        :param input_size: feature dimension\n        :param num_classes: number of total classes in stream\n        :param train_mb_size: batch size for feature extractor during\n            training. Fit will be called on a single pattern at a time.\n        :param eval_mb_size: batch size for inference\n        :param shrinkage_param: value of the shrinkage parameter\n        :param streaming_update_sigma: True if sigma is plastic else False\n        feature extraction in `self.feature_extraction_wrapper'\n        :param plugins: list of StrategyPlugins\n        :param evaluator: Evaluation Plugin instance\n        :param eval_every: run eval every `eval_every` epochs.\n            See `BaseStrategy` for details.\n        \"\"\"\n\n        if plugins is None:\n            plugins = []\n\n        slda_model = slda_model.eval()\n        if output_layer_name is not None:\n            slda_model = FeatureExtractorBackbone(slda_model.to(device),\n                                                  output_layer_name).eval()\n\n        super(StreamingLDA, self).__init__(\n            slda_model, None, criterion, train_mb_size, train_epochs,\n            eval_mb_size, device=device, plugins=plugins, evaluator=evaluator,\n            eval_every=eval_every)\n\n        # SLDA parameters\n        self.input_size = input_size\n        self.shrinkage_param = shrinkage_param\n        self.streaming_update_sigma = streaming_update_sigma\n\n        # setup weights for SLDA\n        self.muK = torch.zeros((num_classes, input_size)).to(self.device)\n        self.cK = torch.zeros(num_classes).to(self.device)\n        self.Sigma = torch.ones((input_size, input_size)).to(self.device)\n        self.num_updates = 0\n        self.Lambda = torch.zeros_like(self.Sigma).to(self.device)\n        self.prev_num_updates = -1\n\n    def forward(self, return_features=False):\n        self.model.eval()\n        if isinstance(self.model, MultiTaskModule):\n            feat = self.model(self.mb_x, self.mb_task_id)\n        else:  # no task labels\n            feat = self.model(self.mb_x)\n        out = self.predict(feat)\n        if return_features:\n            return out, feat\n        else:\n            return out\n\n    def training_epoch(self, **kwargs):\n        \"\"\"\n        Training epoch.\n        :param kwargs:\n        :return:\n        \"\"\"\n        for self.mb_it, self.mbatch in enumerate(self.dataloader):\n            self._unpack_minibatch()\n            self.before_training_iteration(**kwargs)\n\n            self.loss = 0\n\n            # Forward\n            self.before_forward(**kwargs)\n            # compute output on entire minibatch\n            self.mb_output, feats = self.forward(return_features=True)\n            self.after_forward(**kwargs)\n\n            # Loss & Backward\n            self.loss += self.criterion()\n\n            # Optimization step\n            self.before_update(**kwargs)\n            # process one element at a time\n            for f, y in zip(feats, self.mb_y):\n                self.fit(f.unsqueeze(0), y.unsqueeze(0))\n            self.after_update(**kwargs)\n\n            self.after_training_iteration(**kwargs)\n\n    def make_optimizer(self):\n        pass\n\n    @torch.no_grad()\n    def fit(self, x, y):\n        \"\"\"\n        Fit the SLDA model to a new sample (x,y).\n        :param x: a torch tensor of the input data (must be a vector)\n        :param y: a torch tensor of the input label\n        :return: None\n        \"\"\"\n\n        # covariance updates\n        if self.streaming_update_sigma:\n            x_minus_mu = (x - self.muK[y])\n            mult = torch.matmul(x_minus_mu.transpose(1, 0), x_minus_mu)\n            delta = mult * self.num_updates / (self.num_updates + 1)\n            self.Sigma = (self.num_updates * self.Sigma + delta) / (\n                    self.num_updates + 1)\n\n        # update class means\n        self.muK[y, :] += (x - self.muK[y, :]) / (self.cK[y] + 1).unsqueeze(1)\n        self.cK[y] += 1\n        self.num_updates += 1\n\n    @torch.no_grad()\n    def predict(self, X):\n        \"\"\"\n        Make predictions on test data X.\n        :param X: a torch tensor that contains N data samples (N x d)\n        :param return_probas: True if the user would like probabilities instead\n        of predictions returned\n        :return: the test predictions or probabilities\n        \"\"\"\n\n        # compute/load Lambda matrix\n        if self.prev_num_updates != self.num_updates:\n            # there have been updates to the model, compute Lambda\n            self.Lambda = torch.pinverse(\n                (\n                        1 - self.shrinkage_param) * self.Sigma +\n                self.shrinkage_param * torch.eye(\n                    self.input_size, device=self.device))\n            self.prev_num_updates = self.num_updates\n\n        # parameters for predictions\n        M = self.muK.transpose(1, 0)\n        W = torch.matmul(self.Lambda, M)\n        c = 0.5 * torch.sum(M * W, dim=0)\n\n        scores = torch.matmul(X, W) - c\n\n        # return predictions or probabilities\n        return scores\n\n    def fit_base(self, X, y):\n        \"\"\"\n        Fit the SLDA model to the base data.\n        :param X: an Nxd torch tensor of base initialization data\n        :param y: an Nx1-dimensional torch tensor of the associated labels for X\n        :return: None\n        \"\"\"\n        print('\\nFitting Base...')\n\n        # update class means\n        for k in torch.unique(y):\n            self.muK[k] = X[y == k].mean(0)\n            self.cK[k] = X[y == k].shape[0]\n        self.num_updates = X.shape[0]\n\n        print('\\nEstimating initial covariance matrix...')\n        from sklearn.covariance import OAS\n        cov_estimator = OAS(assume_centered=True)\n        cov_estimator.fit((X - self.muK[y]).cpu().numpy())\n        self.Sigma = torch.from_numpy(cov_estimator.covariance_).float().to(\n            self.device)\n\n    def save_model(self, save_path, save_name):\n        \"\"\"\n        Save the model parameters to a torch file.\n        :param save_path: the path where the model will be saved\n        :param save_name: the name for the saved file\n        :return:\n        \"\"\"\n        # grab parameters for saving\n        d = dict()\n        d['muK'] = self.muK.cpu()\n        d['cK'] = self.cK.cpu()\n        d['Sigma'] = self.Sigma.cpu()\n        d['num_updates'] = self.num_updates\n\n        # save model out\n        torch.save(d, os.path.join(save_path, save_name + '.pth'))\n\n    def load_model(self, save_path, save_name):\n        \"\"\"\n        Load the model parameters into StreamingLDA object.\n        :param save_path: the path where the model is saved\n        :param save_name: the name of the saved file\n        :return:\n        \"\"\"\n        # load parameters\n        d = torch.load(os.path.join(save_path, save_name + '.pth'))\n        self.muK = d['muK'].to(self.device)\n        self.cK = d['cK'].to(self.device)\n        self.Sigma = d['Sigma'].to(self.device)\n        self.num_updates = d['num_updates']",
  "def __init__(self, slda_model, criterion,\n                 input_size, num_classes, output_layer_name=None,\n                 shrinkage_param=1e-4, streaming_update_sigma=True,\n                 train_epochs: int = 1, train_mb_size: int = 1,\n                 eval_mb_size: int = 1, device='cpu',\n                 plugins: Optional[Sequence['StrategyPlugin']] = None,\n                 evaluator=default_logger, eval_every=-1):\n        \"\"\"\n        Init function for the SLDA model.\n        :param slda_model: a PyTorch model\n        :param criterion: loss function\n        :param output_layer_name: if not None, wrap model to retrieve\n            only the `output_layer_name` output. If None, the strategy\n            assumes that the model already produces a valid output.\n            You can use `FeatureExtractorBackbone` class to create your custom\n            SLDA-compatible model.\n        :param input_size: feature dimension\n        :param num_classes: number of total classes in stream\n        :param train_mb_size: batch size for feature extractor during\n            training. Fit will be called on a single pattern at a time.\n        :param eval_mb_size: batch size for inference\n        :param shrinkage_param: value of the shrinkage parameter\n        :param streaming_update_sigma: True if sigma is plastic else False\n        feature extraction in `self.feature_extraction_wrapper'\n        :param plugins: list of StrategyPlugins\n        :param evaluator: Evaluation Plugin instance\n        :param eval_every: run eval every `eval_every` epochs.\n            See `BaseStrategy` for details.\n        \"\"\"\n\n        if plugins is None:\n            plugins = []\n\n        slda_model = slda_model.eval()\n        if output_layer_name is not None:\n            slda_model = FeatureExtractorBackbone(slda_model.to(device),\n                                                  output_layer_name).eval()\n\n        super(StreamingLDA, self).__init__(\n            slda_model, None, criterion, train_mb_size, train_epochs,\n            eval_mb_size, device=device, plugins=plugins, evaluator=evaluator,\n            eval_every=eval_every)\n\n        # SLDA parameters\n        self.input_size = input_size\n        self.shrinkage_param = shrinkage_param\n        self.streaming_update_sigma = streaming_update_sigma\n\n        # setup weights for SLDA\n        self.muK = torch.zeros((num_classes, input_size)).to(self.device)\n        self.cK = torch.zeros(num_classes).to(self.device)\n        self.Sigma = torch.ones((input_size, input_size)).to(self.device)\n        self.num_updates = 0\n        self.Lambda = torch.zeros_like(self.Sigma).to(self.device)\n        self.prev_num_updates = -1",
  "def forward(self, return_features=False):\n        self.model.eval()\n        if isinstance(self.model, MultiTaskModule):\n            feat = self.model(self.mb_x, self.mb_task_id)\n        else:  # no task labels\n            feat = self.model(self.mb_x)\n        out = self.predict(feat)\n        if return_features:\n            return out, feat\n        else:\n            return out",
  "def training_epoch(self, **kwargs):\n        \"\"\"\n        Training epoch.\n        :param kwargs:\n        :return:\n        \"\"\"\n        for self.mb_it, self.mbatch in enumerate(self.dataloader):\n            self._unpack_minibatch()\n            self.before_training_iteration(**kwargs)\n\n            self.loss = 0\n\n            # Forward\n            self.before_forward(**kwargs)\n            # compute output on entire minibatch\n            self.mb_output, feats = self.forward(return_features=True)\n            self.after_forward(**kwargs)\n\n            # Loss & Backward\n            self.loss += self.criterion()\n\n            # Optimization step\n            self.before_update(**kwargs)\n            # process one element at a time\n            for f, y in zip(feats, self.mb_y):\n                self.fit(f.unsqueeze(0), y.unsqueeze(0))\n            self.after_update(**kwargs)\n\n            self.after_training_iteration(**kwargs)",
  "def make_optimizer(self):\n        pass",
  "def fit(self, x, y):\n        \"\"\"\n        Fit the SLDA model to a new sample (x,y).\n        :param x: a torch tensor of the input data (must be a vector)\n        :param y: a torch tensor of the input label\n        :return: None\n        \"\"\"\n\n        # covariance updates\n        if self.streaming_update_sigma:\n            x_minus_mu = (x - self.muK[y])\n            mult = torch.matmul(x_minus_mu.transpose(1, 0), x_minus_mu)\n            delta = mult * self.num_updates / (self.num_updates + 1)\n            self.Sigma = (self.num_updates * self.Sigma + delta) / (\n                    self.num_updates + 1)\n\n        # update class means\n        self.muK[y, :] += (x - self.muK[y, :]) / (self.cK[y] + 1).unsqueeze(1)\n        self.cK[y] += 1\n        self.num_updates += 1",
  "def predict(self, X):\n        \"\"\"\n        Make predictions on test data X.\n        :param X: a torch tensor that contains N data samples (N x d)\n        :param return_probas: True if the user would like probabilities instead\n        of predictions returned\n        :return: the test predictions or probabilities\n        \"\"\"\n\n        # compute/load Lambda matrix\n        if self.prev_num_updates != self.num_updates:\n            # there have been updates to the model, compute Lambda\n            self.Lambda = torch.pinverse(\n                (\n                        1 - self.shrinkage_param) * self.Sigma +\n                self.shrinkage_param * torch.eye(\n                    self.input_size, device=self.device))\n            self.prev_num_updates = self.num_updates\n\n        # parameters for predictions\n        M = self.muK.transpose(1, 0)\n        W = torch.matmul(self.Lambda, M)\n        c = 0.5 * torch.sum(M * W, dim=0)\n\n        scores = torch.matmul(X, W) - c\n\n        # return predictions or probabilities\n        return scores",
  "def fit_base(self, X, y):\n        \"\"\"\n        Fit the SLDA model to the base data.\n        :param X: an Nxd torch tensor of base initialization data\n        :param y: an Nx1-dimensional torch tensor of the associated labels for X\n        :return: None\n        \"\"\"\n        print('\\nFitting Base...')\n\n        # update class means\n        for k in torch.unique(y):\n            self.muK[k] = X[y == k].mean(0)\n            self.cK[k] = X[y == k].shape[0]\n        self.num_updates = X.shape[0]\n\n        print('\\nEstimating initial covariance matrix...')\n        from sklearn.covariance import OAS\n        cov_estimator = OAS(assume_centered=True)\n        cov_estimator.fit((X - self.muK[y]).cpu().numpy())\n        self.Sigma = torch.from_numpy(cov_estimator.covariance_).float().to(\n            self.device)",
  "def save_model(self, save_path, save_name):\n        \"\"\"\n        Save the model parameters to a torch file.\n        :param save_path: the path where the model will be saved\n        :param save_name: the name for the saved file\n        :return:\n        \"\"\"\n        # grab parameters for saving\n        d = dict()\n        d['muK'] = self.muK.cpu()\n        d['cK'] = self.cK.cpu()\n        d['Sigma'] = self.Sigma.cpu()\n        d['num_updates'] = self.num_updates\n\n        # save model out\n        torch.save(d, os.path.join(save_path, save_name + '.pth'))",
  "def load_model(self, save_path, save_name):\n        \"\"\"\n        Load the model parameters into StreamingLDA object.\n        :param save_path: the path where the model is saved\n        :param save_name: the name of the saved file\n        :return:\n        \"\"\"\n        # load parameters\n        d = torch.load(os.path.join(save_path, save_name + '.pth'))\n        self.muK = d['muK'].to(self.device)\n        self.cK = d['cK'].to(self.device)\n        self.Sigma = d['Sigma'].to(self.device)\n        self.num_updates = d['num_updates']",
  "class LRSchedulerPlugin(StrategyPlugin):\n    \"\"\"\n    Learning Rate Scheduler Plugin\n    This plugin manages learning rate scheduling inside of a strategy using the\n    PyTorch scheduler passed to the constructor. The step() method of the\n    scheduler is called after each training epoch.\n    \"\"\"\n\n    def __init__(self, scheduler, reset_scheduler=True, reset_lr=True):\n        \"\"\"\n        Creates a ``LRSchedulerPlugin`` instance.\n\n        :param scheduler: a learning rate scheduler that can be updated through\n            a step() method and can be reset by setting last_epoch=0\n        :param reset_scheduler: If True, the scheduler is reset at the end of\n            the experience.\n            Defaults to True.\n        :param reset_lr: If True, the optimizer learning rate is reset to its\n            original value.\n            Default to True.\n        \"\"\"\n        super().__init__()\n        self.scheduler = scheduler\n        self.reset_scheduler = reset_scheduler\n        self.reset_lr = reset_lr\n\n    def after_training_epoch(self, strategy, **kwargs):\n        self.scheduler.step()\n\n    def after_training_exp(self, strategy, **kwargs):\n        param_groups = strategy.optimizer.param_groups\n        base_lrs = self.scheduler.base_lrs\n\n        if self.reset_lr:\n            for group, lr in zip(param_groups, base_lrs):\n                group['lr'] = lr\n\n        if self.reset_scheduler:\n            self.scheduler.last_epoch = 0",
  "def __init__(self, scheduler, reset_scheduler=True, reset_lr=True):\n        \"\"\"\n        Creates a ``LRSchedulerPlugin`` instance.\n\n        :param scheduler: a learning rate scheduler that can be updated through\n            a step() method and can be reset by setting last_epoch=0\n        :param reset_scheduler: If True, the scheduler is reset at the end of\n            the experience.\n            Defaults to True.\n        :param reset_lr: If True, the optimizer learning rate is reset to its\n            original value.\n            Default to True.\n        \"\"\"\n        super().__init__()\n        self.scheduler = scheduler\n        self.reset_scheduler = reset_scheduler\n        self.reset_lr = reset_lr",
  "def after_training_epoch(self, strategy, **kwargs):\n        self.scheduler.step()",
  "def after_training_exp(self, strategy, **kwargs):\n        param_groups = strategy.optimizer.param_groups\n        base_lrs = self.scheduler.base_lrs\n\n        if self.reset_lr:\n            for group, lr in zip(param_groups, base_lrs):\n                group['lr'] = lr\n\n        if self.reset_scheduler:\n            self.scheduler.last_epoch = 0",
  "class AGEMPlugin(StrategyPlugin):\n    \"\"\"\n    Average Gradient Episodic Memory Plugin.\n    AGEM projects the gradient on the current minibatch by using an external\n    episodic memory of patterns from previous experiences. If the dot product\n    between the current gradient and the (average) gradient of a randomly\n    sampled set of memory examples is negative, the gradient is projected.\n    This plugin does not use task identities.\n    \"\"\"\n\n    def __init__(self, patterns_per_experience: int, sample_size: int):\n        \"\"\"\n        :param patterns_per_experience: number of patterns per experience in the\n            memory.\n        :param sample_size: number of patterns in memory sample when computing\n            reference gradient.\n        \"\"\"\n\n        super().__init__()\n\n        self.patterns_per_experience = int(patterns_per_experience)\n        self.sample_size = int(sample_size)\n\n        self.buffers = []  # one AvalancheDataset for each experience.\n        self.buffer_dataloader = None\n        self.buffer_dliter = None\n\n        self.reference_gradients = None\n        self.memory_x, self.memory_y = None, None\n\n    def before_training_iteration(self, strategy, **kwargs):\n        \"\"\"\n        Compute reference gradient on memory sample.\n        \"\"\"\n        if len(self.buffers) > 0:\n            strategy.model.train()\n            strategy.optimizer.zero_grad()\n            mb = self.sample_from_memory()\n            xref, yref, tid = mb[0], mb[1], mb[-1]\n            xref, yref = xref.to(strategy.device), yref.to(strategy.device)\n\n            out = avalanche_forward(strategy.model, xref, tid)\n            loss = strategy._criterion(out, yref)\n            loss.backward()\n            self.reference_gradients = [\n                p.grad.view(-1) for n, p\n                in strategy.model.named_parameters() if p.requires_grad]\n            self.reference_gradients = torch.cat(self.reference_gradients)\n            strategy.optimizer.zero_grad()\n\n    @torch.no_grad()\n    def after_backward(self, strategy, **kwargs):\n        \"\"\"\n        Project gradient based on reference gradients\n        \"\"\"\n        if len(self.buffers) > 0:\n            current_gradients = [\n                p.grad.view(-1)\n                for n, p in strategy.model.named_parameters()\n                if p.requires_grad]\n            current_gradients = torch.cat(current_gradients)\n\n            assert current_gradients.shape == self.reference_gradients.shape, \\\n                \"Different model parameters in AGEM projection\"\n\n            dotg = torch.dot(current_gradients, self.reference_gradients)\n            if dotg < 0:\n                alpha2 = dotg / torch.dot(self.reference_gradients,\n                                          self.reference_gradients)\n                grad_proj = current_gradients - \\\n                    self.reference_gradients * alpha2\n                \n                count = 0 \n                for n, p in strategy.model.named_parameters():\n                    if p.requires_grad:\n                        n_param = p.numel()      \n                        p.grad.copy_(grad_proj[count:count+n_param].view_as(p))\n                        count += n_param\n\n    def after_training_exp(self, strategy, **kwargs):\n        \"\"\" Update replay memory with patterns from current experience. \"\"\"\n        self.update_memory(strategy.experience.dataset)\n\n    def sample_from_memory(self):\n        \"\"\"\n        Sample a minibatch from memory.\n        Return a tuple of patterns (tensor), targets (tensor).\n        \"\"\"\n        return next(self.buffer_dliter)\n\n    @torch.no_grad()\n    def update_memory(self, dataset):\n        \"\"\"\n        Update replay memory with patterns from current experience.\n        \"\"\"\n        removed_els = len(dataset) - self.patterns_per_experience\n        if removed_els > 0:\n            dataset, _ = random_split(dataset,\n                                      [self.patterns_per_experience,\n                                       removed_els])\n        self.buffers.append(dataset)\n        self.buffer_dataloader = GroupBalancedInfiniteDataLoader(\n            self.buffers,\n            batch_size=self.sample_size // len(self.buffers),\n            num_workers=4,\n            pin_memory=True,\n            persistent_workers=True)\n        self.buffer_dliter = iter(self.buffer_dataloader)",
  "def __init__(self, patterns_per_experience: int, sample_size: int):\n        \"\"\"\n        :param patterns_per_experience: number of patterns per experience in the\n            memory.\n        :param sample_size: number of patterns in memory sample when computing\n            reference gradient.\n        \"\"\"\n\n        super().__init__()\n\n        self.patterns_per_experience = int(patterns_per_experience)\n        self.sample_size = int(sample_size)\n\n        self.buffers = []  # one AvalancheDataset for each experience.\n        self.buffer_dataloader = None\n        self.buffer_dliter = None\n\n        self.reference_gradients = None\n        self.memory_x, self.memory_y = None, None",
  "def before_training_iteration(self, strategy, **kwargs):\n        \"\"\"\n        Compute reference gradient on memory sample.\n        \"\"\"\n        if len(self.buffers) > 0:\n            strategy.model.train()\n            strategy.optimizer.zero_grad()\n            mb = self.sample_from_memory()\n            xref, yref, tid = mb[0], mb[1], mb[-1]\n            xref, yref = xref.to(strategy.device), yref.to(strategy.device)\n\n            out = avalanche_forward(strategy.model, xref, tid)\n            loss = strategy._criterion(out, yref)\n            loss.backward()\n            self.reference_gradients = [\n                p.grad.view(-1) for n, p\n                in strategy.model.named_parameters() if p.requires_grad]\n            self.reference_gradients = torch.cat(self.reference_gradients)\n            strategy.optimizer.zero_grad()",
  "def after_backward(self, strategy, **kwargs):\n        \"\"\"\n        Project gradient based on reference gradients\n        \"\"\"\n        if len(self.buffers) > 0:\n            current_gradients = [\n                p.grad.view(-1)\n                for n, p in strategy.model.named_parameters()\n                if p.requires_grad]\n            current_gradients = torch.cat(current_gradients)\n\n            assert current_gradients.shape == self.reference_gradients.shape, \\\n                \"Different model parameters in AGEM projection\"\n\n            dotg = torch.dot(current_gradients, self.reference_gradients)\n            if dotg < 0:\n                alpha2 = dotg / torch.dot(self.reference_gradients,\n                                          self.reference_gradients)\n                grad_proj = current_gradients - \\\n                    self.reference_gradients * alpha2\n                \n                count = 0 \n                for n, p in strategy.model.named_parameters():\n                    if p.requires_grad:\n                        n_param = p.numel()      \n                        p.grad.copy_(grad_proj[count:count+n_param].view_as(p))\n                        count += n_param",
  "def after_training_exp(self, strategy, **kwargs):\n        \"\"\" Update replay memory with patterns from current experience. \"\"\"\n        self.update_memory(strategy.experience.dataset)",
  "def sample_from_memory(self):\n        \"\"\"\n        Sample a minibatch from memory.\n        Return a tuple of patterns (tensor), targets (tensor).\n        \"\"\"\n        return next(self.buffer_dliter)",
  "def update_memory(self, dataset):\n        \"\"\"\n        Update replay memory with patterns from current experience.\n        \"\"\"\n        removed_els = len(dataset) - self.patterns_per_experience\n        if removed_els > 0:\n            dataset, _ = random_split(dataset,\n                                      [self.patterns_per_experience,\n                                       removed_els])\n        self.buffers.append(dataset)\n        self.buffer_dataloader = GroupBalancedInfiniteDataLoader(\n            self.buffers,\n            batch_size=self.sample_size // len(self.buffers),\n            num_workers=4,\n            pin_memory=True,\n            persistent_workers=True)\n        self.buffer_dliter = iter(self.buffer_dataloader)",
  "class GDumbPlugin(StrategyPlugin):\n    \"\"\"\n    A GDumb plugin. At each experience the model\n    is trained with all and only the data of the external memory.\n    The memory is updated at the end of each experience to add new classes or\n    new examples of already encountered classes.\n    In multitask scenarios, mem_size is the memory size for each task.\n    This plugin can be combined with a Naive strategy to obtain the\n    standard GDumb strategy.\n    https://www.robots.ox.ac.uk/~tvg/publications/2020/gdumb.pdf\n    \"\"\"\n\n    def __init__(self, mem_size: int = 200):\n        super().__init__()\n        self.mem_size = mem_size\n        self.ext_mem: Dict[Any, Tuple[List[Tensor], List[Tensor]]] = {}\n        # count occurrences for each class\n        self.counter: Dict[Any, Dict[Any, int]] = {}\n\n    def after_train_dataset_adaptation(\n        self, strategy: \"BaseStrategy\", **kwargs\n    ):\n        \"\"\"Before training we make sure to organize the memory following\n        GDumb approach and updating the dataset accordingly.\n        \"\"\"\n\n        # for each pattern, add it to the memory or not\n        assert strategy.experience is not None\n        dataset = strategy.experience.dataset\n        pbar = tqdm.tqdm(\n            dataset, desc=\"Exhausting dataset to create GDumb buffer\"\n        )\n        for pattern, target, task_id in pbar:\n            target = torch.as_tensor(target)\n            target_value = target.item()\n\n            if len(pattern.size()) == 1:\n                pattern = pattern.unsqueeze(0)\n\n            current_counter = self.counter.setdefault(task_id, defaultdict(int))\n            current_mem = self.ext_mem.setdefault(task_id, ([], []))\n\n            if current_counter == {}:\n                # any positive (>0) number is ok\n                patterns_per_class = 1\n            else:\n                patterns_per_class = int(\n                    self.mem_size / len(current_counter.keys())\n                )\n\n            if (\n                target_value not in current_counter\n                or current_counter[target_value] < patterns_per_class\n            ):\n                # add new pattern into memory\n                if sum(current_counter.values()) >= self.mem_size:\n                    # full memory: replace item from most represented class\n                    # with current pattern\n                    to_remove = max(current_counter, key=current_counter.get)\n\n                    dataset_size = len(current_mem[0])\n                    for j in range(dataset_size):\n                        if current_mem[1][j].item() == to_remove:\n                            current_mem[0][j] = pattern\n                            current_mem[1][j] = target\n                            break\n                    current_counter[to_remove] -= 1\n                else:\n                    # memory not full: add new pattern\n                    current_mem[0].append(pattern)\n                    current_mem[1].append(target)\n\n                # Indicate that we've changed the number of stored instances of\n                # this class.\n                current_counter[target_value] += 1\n\n        task_datasets: Dict[Any, TensorDataset] = {}\n        for task_id, task_mem_tuple in self.ext_mem.items():\n            patterns, targets = task_mem_tuple\n            task_dataset = TensorDataset(\n                torch.stack(patterns, dim=0), torch.stack(targets, dim=0)\n            )\n            task_datasets[task_id] = task_dataset\n\n        adapted_dataset = AvalancheConcatDataset(task_datasets.values())\n        strategy.adapted_dataset = adapted_dataset",
  "def __init__(self, mem_size: int = 200):\n        super().__init__()\n        self.mem_size = mem_size\n        self.ext_mem: Dict[Any, Tuple[List[Tensor], List[Tensor]]] = {}\n        # count occurrences for each class\n        self.counter: Dict[Any, Dict[Any, int]] = {}",
  "def after_train_dataset_adaptation(\n        self, strategy: \"BaseStrategy\", **kwargs\n    ):\n        \"\"\"Before training we make sure to organize the memory following\n        GDumb approach and updating the dataset accordingly.\n        \"\"\"\n\n        # for each pattern, add it to the memory or not\n        assert strategy.experience is not None\n        dataset = strategy.experience.dataset\n        pbar = tqdm.tqdm(\n            dataset, desc=\"Exhausting dataset to create GDumb buffer\"\n        )\n        for pattern, target, task_id in pbar:\n            target = torch.as_tensor(target)\n            target_value = target.item()\n\n            if len(pattern.size()) == 1:\n                pattern = pattern.unsqueeze(0)\n\n            current_counter = self.counter.setdefault(task_id, defaultdict(int))\n            current_mem = self.ext_mem.setdefault(task_id, ([], []))\n\n            if current_counter == {}:\n                # any positive (>0) number is ok\n                patterns_per_class = 1\n            else:\n                patterns_per_class = int(\n                    self.mem_size / len(current_counter.keys())\n                )\n\n            if (\n                target_value not in current_counter\n                or current_counter[target_value] < patterns_per_class\n            ):\n                # add new pattern into memory\n                if sum(current_counter.values()) >= self.mem_size:\n                    # full memory: replace item from most represented class\n                    # with current pattern\n                    to_remove = max(current_counter, key=current_counter.get)\n\n                    dataset_size = len(current_mem[0])\n                    for j in range(dataset_size):\n                        if current_mem[1][j].item() == to_remove:\n                            current_mem[0][j] = pattern\n                            current_mem[1][j] = target\n                            break\n                    current_counter[to_remove] -= 1\n                else:\n                    # memory not full: add new pattern\n                    current_mem[0].append(pattern)\n                    current_mem[1].append(target)\n\n                # Indicate that we've changed the number of stored instances of\n                # this class.\n                current_counter[target_value] += 1\n\n        task_datasets: Dict[Any, TensorDataset] = {}\n        for task_id, task_mem_tuple in self.ext_mem.items():\n            patterns, targets = task_mem_tuple\n            task_dataset = TensorDataset(\n                torch.stack(patterns, dim=0), torch.stack(targets, dim=0)\n            )\n            task_datasets[task_id] = task_dataset\n\n        adapted_dataset = AvalancheConcatDataset(task_datasets.values())\n        strategy.adapted_dataset = adapted_dataset",
  "class ReplayPlugin(StrategyPlugin):\n    \"\"\"\n    Experience replay plugin.\n\n    Handles an external memory filled with randomly selected\n    patterns and implementing `before_training_exp` and `after_training_exp`\n    callbacks. \n    The `before_training_exp` callback is implemented in order to use the\n    dataloader that creates mini-batches with examples from both training\n    data and external memory. The examples in the mini-batch is balanced \n    such that there are the same number of examples for each experience.    \n    \n    The `after_training_exp` callback is implemented in order to add new \n    patterns to the external memory.\n\n    The :mem_size: attribute controls the total number of patterns to be stored \n    in the external memory.\n    :param storage_policy: The policy that controls how to add new exemplars\n                           in memory\n    \"\"\"\n\n    def __init__(self, mem_size: int = 200,\n                 storage_policy: Optional[\"StoragePolicy\"] = None):\n        super().__init__()\n        self.mem_size = mem_size\n\n        if storage_policy is not None:  # Use other storage policy\n            self.storage_policy = storage_policy\n            self.ext_mem = storage_policy.ext_mem  # Keep ref\n            assert storage_policy.mem_size == self.mem_size\n\n        else:  # Default\n            self.ext_mem = {}  # a Dict<task_id, Dataset>\n            self.storage_policy = ExperienceBalancedStoragePolicy(\n                ext_mem=self.ext_mem,\n                mem_size=self.mem_size,\n                adaptive_size=True)\n\n    def before_training_exp(self, strategy: \"BaseStrategy\",\n                            num_workers: int = 0, shuffle: bool = True,\n                            **kwargs):\n        \"\"\"\n        Dataloader to build batches containing examples from both memories and\n        the training dataset\n        \"\"\"\n        if len(self.ext_mem) == 0:\n            return\n        strategy.dataloader = ReplayDataLoader(\n            strategy.adapted_dataset,\n            AvalancheConcatDataset(self.ext_mem.values()),\n            oversample_small_tasks=True,\n            num_workers=num_workers,\n            batch_size=strategy.train_mb_size,\n            shuffle=shuffle)\n\n    def after_training_exp(self, strategy: \"BaseStrategy\", **kwargs):\n        self.storage_policy(strategy, **kwargs)",
  "class StoragePolicy(ABC):\n    \"\"\"\n    A policy to store exemplars in a replay memory.\n\n    :param ext_mem: The replay memory dictionary to store samples.\n    :param mem_size: max number of total input samples in the replay memory.\n    \"\"\"\n    def __init__(self, ext_mem: Dict[int, AvalancheDataset], mem_size: int):\n        self.ext_mem = ext_mem\n        self.mem_size = mem_size\n\n    @abstractmethod\n    def __call__(self, data_source: AvalancheDataset, **kwargs):\n        \"\"\"Store exemplars in the replay memory\"\"\"",
  "class ExperienceBalancedStoragePolicy(StoragePolicy):\n    def __init__(self, ext_mem: Dict, mem_size: int, adaptive_size: bool = True,\n                 num_experiences=-1):\n        \"\"\"\n        Stores samples for replay, equally divided over experiences.\n        Because it is conditioned on the experience, it should be called in\n        the 'after_training_exp' phase.\n\n        The number of experiences can be fixed up front or adaptive, based on\n        the 'adaptive_size' attribute. When adaptive, the memory is equally\n        divided over all the unique observed experiences so far.\n\n        :param ext_mem: The replay memory dictionary to store samples.\n        :param mem_size: max number of total input samples in the replay memory.\n        :param adaptive_size: True if mem_size is divided equally over all\n                              observed experiences (keys in replay_mem).\n        :param num_experiences: If adaptive size is False, the fixed number\n                                of experiences to divide capacity over.\n        \"\"\"\n        super().__init__(ext_mem, mem_size)\n        self.adaptive_size = adaptive_size\n        self.num_experiences = num_experiences\n\n        if not self.adaptive_size:\n            assert self.num_experiences > 0, \\\n                \"\"\"When fixed exp mem size, num_experiences should be > 0.\"\"\"\n\n    def subsample_single(self, data: AvalancheDataset, new_size: int):\n        \"\"\" Subsample `data` to match length `new_size`. \"\"\"\n        removed_els = len(data) - new_size\n        if removed_els > 0:\n            data, _ = random_split(data, [new_size, removed_els])\n        return data\n\n    def subsample_all_groups(self, new_size: int):\n        \"\"\" Subsample all groups equally to match total buffer size\n        `new_size`. \"\"\"\n        groups = list(self.ext_mem.keys())\n        if len(groups) == 0:\n            return  # buffer is empty.\n\n        num_groups = len(groups) if self.adaptive_size else self.num_experiences\n        group_size = new_size // num_groups\n        last_group_size = group_size + (new_size % num_groups)\n\n        for g in groups[:-1]:\n            self.ext_mem[g] = self.subsample_single(self.ext_mem[g], group_size)\n        # last group may be bigger\n        last = self.ext_mem[groups[-1]]\n        self.ext_mem[groups[-1]] = self.subsample_single(last, last_group_size)\n\n    def __call__(self, strategy: \"BaseStrategy\", **kwargs):\n        num_exps = strategy.training_exp_counter + 1\n        num_exps = num_exps if self.adaptive_size else self.num_experiences\n        curr_data = strategy.experience.dataset\n\n        # new group may be bigger because of the remainder.\n        group_size = self.mem_size // num_exps\n        new_group_size = group_size + (self.mem_size % num_exps)\n\n        self.subsample_all_groups(group_size * (num_exps - 1))\n        curr_data = self.subsample_single(curr_data, new_group_size)\n        self.ext_mem[strategy.training_exp_counter + 1] = curr_data\n\n        # buffer size should always equal self.mem_size\n        len_tot = sum(len(el) for el in self.ext_mem.values())\n        assert len_tot == self.mem_size",
  "class ClassBalancedStoragePolicy(StoragePolicy):\n    def __init__(self, ext_mem: Dict, mem_size: int, adaptive_size: bool = True,\n                 total_num_classes: int = -1, selection_strategy:\n                 Optional[\"ClassExemplarsSelectionStrategy\"] = None):\n        \"\"\"\n        Stores samples for replay, equally divided over classes.\n        It should be called in the 'after_training_exp' phase (see\n        ExperienceBalancedStoragePolicy).\n        The number of classes can be fixed up front or adaptive, based on\n        the 'adaptive_size' attribute. When adaptive, the memory is equally\n        divided over all the unique observed classes so far.\n        :param ext_mem: The replay memory dictionary to store samples in.\n        :param mem_size: The max capacity of the replay memory.\n        :param adaptive_size: True if mem_size is divided equally over all\n                            observed experiences (keys in replay_mem).\n        :param total_num_classes: If adaptive size is False, the fixed number\n                                  of classes to divide capacity over.\n        :param selection_strategy: The strategy used to select exemplars to \n                                   keep in memory when cutting it off\n        \"\"\"\n        super().__init__(ext_mem, mem_size)\n        self.selection_strategy = selection_strategy or \\\n            RandomExemplarsSelectionStrategy()\n        self.adaptive_size = adaptive_size\n        self.total_num_classes = total_num_classes\n        self.seen_classes = set()\n\n        if not self.adaptive_size:\n            assert self.total_num_classes > 0, \\\n                \"\"\"When fixed exp mem size, total_num_classes should be > 0.\"\"\"\n\n    def __call__(self, strategy: \"BaseStrategy\", **kwargs):\n        new_data = strategy.experience.dataset\n\n        # Get sample idxs per class\n        cl_idxs = {}\n        for idx, target in enumerate(new_data.targets):\n            if target not in cl_idxs:\n                cl_idxs[target] = []\n            cl_idxs[target].append(idx)\n\n        # Make AvalancheSubset per class\n        cl_datasets = {}\n        for c, c_idxs in cl_idxs.items():\n            cl_datasets[c] = AvalancheSubset(new_data, indices=c_idxs)\n\n        # Update seen classes\n        self.seen_classes.update(cl_datasets.keys())\n\n        # how many experiences to divide the memory over\n        div_cnt = len(self.seen_classes) if self.adaptive_size \\\n            else self.total_num_classes\n        class_mem_size = self.mem_size // div_cnt\n\n        # Add current classes data to memory\n        for c, c_mem in cl_datasets.items():\n            if c in self.ext_mem:  # Merge data with previous seen data\n                c_mem = AvalancheConcatDataset((c_mem, self.ext_mem[c]))\n            sorted_indices = self.selection_strategy.make_sorted_indices(\n                strategy, c_mem)\n            self.ext_mem[c] = AvalancheSubset(c_mem, sorted_indices)\n\n        # Distribute remaining samples using counts\n        cutoff_per_exp = self.divide_remaining_samples(class_mem_size, div_cnt)\n\n        # Use counts to remove samples from memory\n        self.cutoff_memory(cutoff_per_exp)\n\n    def divide_remaining_samples(self, exp_mem_size: int, div_cnt: int) -> \\\n            Dict[int, int]:\n        # Find number of remaining samples\n        samples_per_exp = {exp: len(mem) for exp, mem in\n                           self.ext_mem.items()}\n        rem_from_exps = {exp: exp_mem_size - memsize for exp, memsize in\n                         samples_per_exp.items() if\n                         exp_mem_size - memsize > 0}\n        rem_from_div = self.mem_size % div_cnt\n        free_mem = sum(rem_from_exps.values()) + rem_from_div\n\n        # Divide the remaining samples randomly over the experiences\n        cutoff_per_exp = {exp: min(exp_mem_size, len(m))\n                          for exp, m in self.ext_mem.items()}\n\n        # Find remaining data samples to divide\n        rem_samples_exp = {exp: memsize - exp_mem_size for exp, memsize in\n                           samples_per_exp.items()\n                           if memsize - exp_mem_size > 0}\n\n        while len(rem_samples_exp) > 0 and free_mem > 0:\n            exp = random.choice(list(rem_samples_exp.keys()))\n            cutoff_per_exp[exp] += 1\n            free_mem -= 1\n            rem_samples_exp[exp] -= 1\n            if rem_samples_exp[exp] <= 0:\n                del rem_samples_exp[exp]\n\n        return cutoff_per_exp\n\n    def cutoff_memory(self, cutoff_per_exp: Dict[int, int]):\n        # No need to reselect at this point, we expect the first selection to\n        # have sorted the exemplars\n        for exp, cutoff in cutoff_per_exp.items():\n            self.ext_mem[exp] = AvalancheSubset(self.ext_mem[exp],\n                                                list(range(cutoff)))",
  "class ClassExemplarsSelectionStrategy(ABC):\n    \"\"\"\n    Base class to define how to select class exemplars from a dataset\n    \"\"\"\n    @abstractmethod\n    def make_sorted_indices(self, strategy: \"BaseStrategy\",\n                            data: AvalancheDataset) -> List[int]:\n        \"\"\"\n        Should return the sorted list of indices to keep as exemplars.\n\n        The last indices will be the first to be removed when cutoff memory.\n        \"\"\"",
  "class RandomExemplarsSelectionStrategy(ClassExemplarsSelectionStrategy):\n    \"\"\"Select the exemplars at random in the dataset\"\"\"\n\n    def make_sorted_indices(self, strategy: \"BaseStrategy\",\n                            data: AvalancheDataset) -> List[int]:\n        indices = list(range(len(data)))\n        random.shuffle(indices)\n        return indices",
  "class FeatureBasedExemplarsSelectionStrategy(ClassExemplarsSelectionStrategy,\n                                             ABC):\n    \"\"\"Base class to select exemplars from their features\"\"\"\n    def __init__(self, model: Module, layer_name: str):\n        self.feature_extractor = FeatureExtractorBackbone(model, layer_name)\n\n    @torch.no_grad()\n    def make_sorted_indices(self, strategy: \"BaseStrategy\",\n                            data: AvalancheDataset) -> List[int]:\n        self.feature_extractor.eval()\n        features = cat(\n            [\n                self.feature_extractor(x.to(strategy.device))\n                for x, *_ in DataLoader(data, batch_size=strategy.eval_mb_size)\n            ]\n        )\n        return self.make_sorted_indices_from_features(features)\n\n    @abstractmethod\n    def make_sorted_indices_from_features(self, features: Tensor\n                                          ) -> List[int]:\n        \"\"\"\n        Should return the sorted list of indices to keep as exemplars.\n\n        The last indices will be the first to be removed when cutoff memory.\n        \"\"\"",
  "class HerdingSelectionStrategy(FeatureBasedExemplarsSelectionStrategy):\n    def make_sorted_indices_from_features(self, features: Tensor\n                                          ) -> List[int]:\n        \"\"\"\n        The herding strategy as described in iCaRL\n\n        It is a greedy algorithm, that select the remaining exemplar that get\n        the center of already selected exemplars as close as possible as the\n        center of all elements (in the feature space).\n        \"\"\"\n        selected_indices = []\n\n        center = features.mean(dim=0)\n        current_center = center * 0\n\n        for i in range(len(features)):\n            # Compute distances with real center\n            candidate_centers = current_center * i / (i + 1) + features / (i\n                                                                           + 1)\n            distances = pow(candidate_centers - center, 2).sum(dim=1)\n            distances[selected_indices] = inf\n\n            # Select best candidate\n            new_index = distances.argmin().tolist()\n            selected_indices.append(new_index)\n            current_center = candidate_centers[new_index]\n\n        return selected_indices",
  "class ClosestToCenterSelectionStrategy(FeatureBasedExemplarsSelectionStrategy):\n    def make_sorted_indices_from_features(self, features: Tensor\n                                          ) -> List[int]:\n        \"\"\"\n        A greedy algorithm that select the remaining exemplar that is the\n        closest to the center of all elements (in feature space)\n        \"\"\"\n        center = features.mean(dim=0)\n        distances = pow(features - center, 2).sum(dim=1)\n        return distances.argsort()",
  "def __init__(self, mem_size: int = 200,\n                 storage_policy: Optional[\"StoragePolicy\"] = None):\n        super().__init__()\n        self.mem_size = mem_size\n\n        if storage_policy is not None:  # Use other storage policy\n            self.storage_policy = storage_policy\n            self.ext_mem = storage_policy.ext_mem  # Keep ref\n            assert storage_policy.mem_size == self.mem_size\n\n        else:  # Default\n            self.ext_mem = {}  # a Dict<task_id, Dataset>\n            self.storage_policy = ExperienceBalancedStoragePolicy(\n                ext_mem=self.ext_mem,\n                mem_size=self.mem_size,\n                adaptive_size=True)",
  "def before_training_exp(self, strategy: \"BaseStrategy\",\n                            num_workers: int = 0, shuffle: bool = True,\n                            **kwargs):\n        \"\"\"\n        Dataloader to build batches containing examples from both memories and\n        the training dataset\n        \"\"\"\n        if len(self.ext_mem) == 0:\n            return\n        strategy.dataloader = ReplayDataLoader(\n            strategy.adapted_dataset,\n            AvalancheConcatDataset(self.ext_mem.values()),\n            oversample_small_tasks=True,\n            num_workers=num_workers,\n            batch_size=strategy.train_mb_size,\n            shuffle=shuffle)",
  "def after_training_exp(self, strategy: \"BaseStrategy\", **kwargs):\n        self.storage_policy(strategy, **kwargs)",
  "def __init__(self, ext_mem: Dict[int, AvalancheDataset], mem_size: int):\n        self.ext_mem = ext_mem\n        self.mem_size = mem_size",
  "def __call__(self, data_source: AvalancheDataset, **kwargs):\n        \"\"\"Store exemplars in the replay memory\"\"\"",
  "def __init__(self, ext_mem: Dict, mem_size: int, adaptive_size: bool = True,\n                 num_experiences=-1):\n        \"\"\"\n        Stores samples for replay, equally divided over experiences.\n        Because it is conditioned on the experience, it should be called in\n        the 'after_training_exp' phase.\n\n        The number of experiences can be fixed up front or adaptive, based on\n        the 'adaptive_size' attribute. When adaptive, the memory is equally\n        divided over all the unique observed experiences so far.\n\n        :param ext_mem: The replay memory dictionary to store samples.\n        :param mem_size: max number of total input samples in the replay memory.\n        :param adaptive_size: True if mem_size is divided equally over all\n                              observed experiences (keys in replay_mem).\n        :param num_experiences: If adaptive size is False, the fixed number\n                                of experiences to divide capacity over.\n        \"\"\"\n        super().__init__(ext_mem, mem_size)\n        self.adaptive_size = adaptive_size\n        self.num_experiences = num_experiences\n\n        if not self.adaptive_size:\n            assert self.num_experiences > 0, \\\n                \"\"\"When fixed exp mem size, num_experiences should be > 0.\"\"\"",
  "def subsample_single(self, data: AvalancheDataset, new_size: int):\n        \"\"\" Subsample `data` to match length `new_size`. \"\"\"\n        removed_els = len(data) - new_size\n        if removed_els > 0:\n            data, _ = random_split(data, [new_size, removed_els])\n        return data",
  "def subsample_all_groups(self, new_size: int):\n        \"\"\" Subsample all groups equally to match total buffer size\n        `new_size`. \"\"\"\n        groups = list(self.ext_mem.keys())\n        if len(groups) == 0:\n            return  # buffer is empty.\n\n        num_groups = len(groups) if self.adaptive_size else self.num_experiences\n        group_size = new_size // num_groups\n        last_group_size = group_size + (new_size % num_groups)\n\n        for g in groups[:-1]:\n            self.ext_mem[g] = self.subsample_single(self.ext_mem[g], group_size)\n        # last group may be bigger\n        last = self.ext_mem[groups[-1]]\n        self.ext_mem[groups[-1]] = self.subsample_single(last, last_group_size)",
  "def __call__(self, strategy: \"BaseStrategy\", **kwargs):\n        num_exps = strategy.training_exp_counter + 1\n        num_exps = num_exps if self.adaptive_size else self.num_experiences\n        curr_data = strategy.experience.dataset\n\n        # new group may be bigger because of the remainder.\n        group_size = self.mem_size // num_exps\n        new_group_size = group_size + (self.mem_size % num_exps)\n\n        self.subsample_all_groups(group_size * (num_exps - 1))\n        curr_data = self.subsample_single(curr_data, new_group_size)\n        self.ext_mem[strategy.training_exp_counter + 1] = curr_data\n\n        # buffer size should always equal self.mem_size\n        len_tot = sum(len(el) for el in self.ext_mem.values())\n        assert len_tot == self.mem_size",
  "def __init__(self, ext_mem: Dict, mem_size: int, adaptive_size: bool = True,\n                 total_num_classes: int = -1, selection_strategy:\n                 Optional[\"ClassExemplarsSelectionStrategy\"] = None):\n        \"\"\"\n        Stores samples for replay, equally divided over classes.\n        It should be called in the 'after_training_exp' phase (see\n        ExperienceBalancedStoragePolicy).\n        The number of classes can be fixed up front or adaptive, based on\n        the 'adaptive_size' attribute. When adaptive, the memory is equally\n        divided over all the unique observed classes so far.\n        :param ext_mem: The replay memory dictionary to store samples in.\n        :param mem_size: The max capacity of the replay memory.\n        :param adaptive_size: True if mem_size is divided equally over all\n                            observed experiences (keys in replay_mem).\n        :param total_num_classes: If adaptive size is False, the fixed number\n                                  of classes to divide capacity over.\n        :param selection_strategy: The strategy used to select exemplars to \n                                   keep in memory when cutting it off\n        \"\"\"\n        super().__init__(ext_mem, mem_size)\n        self.selection_strategy = selection_strategy or \\\n            RandomExemplarsSelectionStrategy()\n        self.adaptive_size = adaptive_size\n        self.total_num_classes = total_num_classes\n        self.seen_classes = set()\n\n        if not self.adaptive_size:\n            assert self.total_num_classes > 0, \\\n                \"\"\"When fixed exp mem size, total_num_classes should be > 0.\"\"\"",
  "def __call__(self, strategy: \"BaseStrategy\", **kwargs):\n        new_data = strategy.experience.dataset\n\n        # Get sample idxs per class\n        cl_idxs = {}\n        for idx, target in enumerate(new_data.targets):\n            if target not in cl_idxs:\n                cl_idxs[target] = []\n            cl_idxs[target].append(idx)\n\n        # Make AvalancheSubset per class\n        cl_datasets = {}\n        for c, c_idxs in cl_idxs.items():\n            cl_datasets[c] = AvalancheSubset(new_data, indices=c_idxs)\n\n        # Update seen classes\n        self.seen_classes.update(cl_datasets.keys())\n\n        # how many experiences to divide the memory over\n        div_cnt = len(self.seen_classes) if self.adaptive_size \\\n            else self.total_num_classes\n        class_mem_size = self.mem_size // div_cnt\n\n        # Add current classes data to memory\n        for c, c_mem in cl_datasets.items():\n            if c in self.ext_mem:  # Merge data with previous seen data\n                c_mem = AvalancheConcatDataset((c_mem, self.ext_mem[c]))\n            sorted_indices = self.selection_strategy.make_sorted_indices(\n                strategy, c_mem)\n            self.ext_mem[c] = AvalancheSubset(c_mem, sorted_indices)\n\n        # Distribute remaining samples using counts\n        cutoff_per_exp = self.divide_remaining_samples(class_mem_size, div_cnt)\n\n        # Use counts to remove samples from memory\n        self.cutoff_memory(cutoff_per_exp)",
  "def divide_remaining_samples(self, exp_mem_size: int, div_cnt: int) -> \\\n            Dict[int, int]:\n        # Find number of remaining samples\n        samples_per_exp = {exp: len(mem) for exp, mem in\n                           self.ext_mem.items()}\n        rem_from_exps = {exp: exp_mem_size - memsize for exp, memsize in\n                         samples_per_exp.items() if\n                         exp_mem_size - memsize > 0}\n        rem_from_div = self.mem_size % div_cnt\n        free_mem = sum(rem_from_exps.values()) + rem_from_div\n\n        # Divide the remaining samples randomly over the experiences\n        cutoff_per_exp = {exp: min(exp_mem_size, len(m))\n                          for exp, m in self.ext_mem.items()}\n\n        # Find remaining data samples to divide\n        rem_samples_exp = {exp: memsize - exp_mem_size for exp, memsize in\n                           samples_per_exp.items()\n                           if memsize - exp_mem_size > 0}\n\n        while len(rem_samples_exp) > 0 and free_mem > 0:\n            exp = random.choice(list(rem_samples_exp.keys()))\n            cutoff_per_exp[exp] += 1\n            free_mem -= 1\n            rem_samples_exp[exp] -= 1\n            if rem_samples_exp[exp] <= 0:\n                del rem_samples_exp[exp]\n\n        return cutoff_per_exp",
  "def cutoff_memory(self, cutoff_per_exp: Dict[int, int]):\n        # No need to reselect at this point, we expect the first selection to\n        # have sorted the exemplars\n        for exp, cutoff in cutoff_per_exp.items():\n            self.ext_mem[exp] = AvalancheSubset(self.ext_mem[exp],\n                                                list(range(cutoff)))",
  "def make_sorted_indices(self, strategy: \"BaseStrategy\",\n                            data: AvalancheDataset) -> List[int]:\n        \"\"\"\n        Should return the sorted list of indices to keep as exemplars.\n\n        The last indices will be the first to be removed when cutoff memory.\n        \"\"\"",
  "def make_sorted_indices(self, strategy: \"BaseStrategy\",\n                            data: AvalancheDataset) -> List[int]:\n        indices = list(range(len(data)))\n        random.shuffle(indices)\n        return indices",
  "def __init__(self, model: Module, layer_name: str):\n        self.feature_extractor = FeatureExtractorBackbone(model, layer_name)",
  "def make_sorted_indices(self, strategy: \"BaseStrategy\",\n                            data: AvalancheDataset) -> List[int]:\n        self.feature_extractor.eval()\n        features = cat(\n            [\n                self.feature_extractor(x.to(strategy.device))\n                for x, *_ in DataLoader(data, batch_size=strategy.eval_mb_size)\n            ]\n        )\n        return self.make_sorted_indices_from_features(features)",
  "def make_sorted_indices_from_features(self, features: Tensor\n                                          ) -> List[int]:\n        \"\"\"\n        Should return the sorted list of indices to keep as exemplars.\n\n        The last indices will be the first to be removed when cutoff memory.\n        \"\"\"",
  "def make_sorted_indices_from_features(self, features: Tensor\n                                          ) -> List[int]:\n        \"\"\"\n        The herding strategy as described in iCaRL\n\n        It is a greedy algorithm, that select the remaining exemplar that get\n        the center of already selected exemplars as close as possible as the\n        center of all elements (in the feature space).\n        \"\"\"\n        selected_indices = []\n\n        center = features.mean(dim=0)\n        current_center = center * 0\n\n        for i in range(len(features)):\n            # Compute distances with real center\n            candidate_centers = current_center * i / (i + 1) + features / (i\n                                                                           + 1)\n            distances = pow(candidate_centers - center, 2).sum(dim=1)\n            distances[selected_indices] = inf\n\n            # Select best candidate\n            new_index = distances.argmin().tolist()\n            selected_indices.append(new_index)\n            current_center = candidate_centers[new_index]\n\n        return selected_indices",
  "def make_sorted_indices_from_features(self, features: Tensor\n                                          ) -> List[int]:\n        \"\"\"\n        A greedy algorithm that select the remaining exemplar that is the\n        closest to the center of all elements (in feature space)\n        \"\"\"\n        center = features.mean(dim=0)\n        distances = pow(features - center, 2).sum(dim=1)\n        return distances.argsort()",
  "class EWCPlugin(StrategyPlugin):\n    \"\"\"\n    Elastic Weight Consolidation (EWC) plugin.\n    EWC computes importance of each weight at the end of training on current\n    experience. During training on each minibatch, the loss is augmented\n    with a penalty which keeps the value of the current weights close to the\n    value they had on previous experiences in proportion to their importance\n    on that experience. Importances are computed with an additional pass on the\n    training set. This plugin does not use task identities.\n    \"\"\"\n\n    def __init__(self, ewc_lambda, mode='separate', decay_factor=None,\n                 keep_importance_data=False):\n        \"\"\"\n        :param ewc_lambda: hyperparameter to weigh the penalty inside the total\n               loss. The larger the lambda, the larger the regularization.\n        :param mode: `separate` to keep a separate penalty for each previous\n               experience.\n               `online` to keep a single penalty summed with a decay factor\n               over all previous tasks.\n        :param decay_factor: used only if mode is `online`.\n               It specifies the decay term of the importance matrix.\n        :param keep_importance_data: if True, keep in memory both parameter\n                values and importances for all previous task, for all modes.\n                If False, keep only last parameter values and importances.\n                If mode is `separate`, the value of `keep_importance_data` is\n                set to be True.\n        \"\"\"\n\n        super().__init__()\n\n        assert mode == 'separate' or mode == 'online', \\\n            'Mode must be separate or online.'\n\n        self.ewc_lambda = ewc_lambda\n        self.mode = mode\n        self.decay_factor = decay_factor\n\n        if self.mode == 'separate':\n            self.keep_importance_data = True\n        else:\n            self.keep_importance_data = keep_importance_data\n\n        self.saved_params = defaultdict(list)\n        self.importances = defaultdict(list)\n\n    def before_backward(self, strategy, **kwargs):\n        \"\"\"\n        Compute EWC penalty and add it to the loss.\n        \"\"\"\n\n        if strategy.training_exp_counter == 0:\n            return\n\n        penalty = torch.tensor(0).float().to(strategy.device)\n\n        if self.mode == 'separate':\n            for experience in range(strategy.training_exp_counter):\n                for (_, cur_param), (_, saved_param), (_, imp) in zip(\n                        strategy.model.named_parameters(),\n                        self.saved_params[experience],\n                        self.importances[experience]):\n                    penalty += (imp * (cur_param - saved_param).pow(2)).sum()\n        elif self.mode == 'online':\n            for (_, cur_param), (_, saved_param), (_, imp) in zip(\n                    strategy.model.named_parameters(),\n                    self.saved_params[strategy.training_exp_counter],\n                    self.importances[strategy.training_exp_counter]):\n                penalty += (imp * (cur_param - saved_param).pow(2)).sum()\n        else:\n            raise ValueError('Wrong EWC mode.')\n\n        strategy.loss += self.ewc_lambda * penalty\n\n    def after_training_exp(self, strategy, **kwargs):\n        \"\"\"\n        Compute importances of parameters after each experience.\n        \"\"\"\n\n        importances = self.compute_importances(strategy.model,\n                                               strategy._criterion,\n                                               strategy.optimizer,\n                                               strategy.experience.dataset,\n                                               strategy.device,\n                                               strategy.train_mb_size)\n        self.update_importances(importances, strategy.training_exp_counter)\n        self.saved_params[strategy.training_exp_counter] = \\\n            copy_params_dict(strategy.model)\n        # clear previuos parameter values\n        if strategy.training_exp_counter > 0 and \\\n                (not self.keep_importance_data):\n            del self.saved_params[strategy.training_exp_counter - 1]\n\n    def compute_importances(self, model, criterion, optimizer,\n                            dataset, device, batch_size):\n        \"\"\"\n        Compute EWC importance matrix for each parameter\n        \"\"\"\n\n        model.train()\n\n        # list of list\n        importances = zerolike_params_dict(model)\n        dataloader = DataLoader(dataset, batch_size=batch_size)\n        for i, (x, y, task_labels) in enumerate(dataloader):\n            x, y = x.to(device), y.to(device)\n\n            optimizer.zero_grad()\n            out = avalanche_forward(model, x, task_labels)\n            loss = criterion(out, y)\n            loss.backward()\n\n            for (k1, p), (k2, imp) in zip(model.named_parameters(),\n                                          importances):\n                assert (k1 == k2)\n                if p.grad is not None:\n                    imp += p.grad.data.clone().pow(2)\n\n        # average over mini batch length\n        for _, imp in importances:\n            imp /= float(len(dataloader))\n\n        return importances\n\n    @torch.no_grad()\n    def update_importances(self, importances, t):\n        \"\"\"\n        Update importance for each parameter based on the currently computed\n        importances.\n        \"\"\"\n\n        if self.mode == 'separate' or t == 0:\n            self.importances[t] = importances\n        elif self.mode == 'online':\n            for (k1, old_imp), (k2, curr_imp) in \\\n                    zip(self.importances[t - 1], importances):\n                assert k1 == k2, 'Error in importance computation.'\n                self.importances[t].append(\n                    (k1, (self.decay_factor * old_imp + curr_imp)))\n\n            # clear previous parameter importances\n            if t > 0 and (not self.keep_importance_data):\n                del self.importances[t - 1]\n\n        else:\n            raise ValueError(\"Wrong EWC mode.\")",
  "def __init__(self, ewc_lambda, mode='separate', decay_factor=None,\n                 keep_importance_data=False):\n        \"\"\"\n        :param ewc_lambda: hyperparameter to weigh the penalty inside the total\n               loss. The larger the lambda, the larger the regularization.\n        :param mode: `separate` to keep a separate penalty for each previous\n               experience.\n               `online` to keep a single penalty summed with a decay factor\n               over all previous tasks.\n        :param decay_factor: used only if mode is `online`.\n               It specifies the decay term of the importance matrix.\n        :param keep_importance_data: if True, keep in memory both parameter\n                values and importances for all previous task, for all modes.\n                If False, keep only last parameter values and importances.\n                If mode is `separate`, the value of `keep_importance_data` is\n                set to be True.\n        \"\"\"\n\n        super().__init__()\n\n        assert mode == 'separate' or mode == 'online', \\\n            'Mode must be separate or online.'\n\n        self.ewc_lambda = ewc_lambda\n        self.mode = mode\n        self.decay_factor = decay_factor\n\n        if self.mode == 'separate':\n            self.keep_importance_data = True\n        else:\n            self.keep_importance_data = keep_importance_data\n\n        self.saved_params = defaultdict(list)\n        self.importances = defaultdict(list)",
  "def before_backward(self, strategy, **kwargs):\n        \"\"\"\n        Compute EWC penalty and add it to the loss.\n        \"\"\"\n\n        if strategy.training_exp_counter == 0:\n            return\n\n        penalty = torch.tensor(0).float().to(strategy.device)\n\n        if self.mode == 'separate':\n            for experience in range(strategy.training_exp_counter):\n                for (_, cur_param), (_, saved_param), (_, imp) in zip(\n                        strategy.model.named_parameters(),\n                        self.saved_params[experience],\n                        self.importances[experience]):\n                    penalty += (imp * (cur_param - saved_param).pow(2)).sum()\n        elif self.mode == 'online':\n            for (_, cur_param), (_, saved_param), (_, imp) in zip(\n                    strategy.model.named_parameters(),\n                    self.saved_params[strategy.training_exp_counter],\n                    self.importances[strategy.training_exp_counter]):\n                penalty += (imp * (cur_param - saved_param).pow(2)).sum()\n        else:\n            raise ValueError('Wrong EWC mode.')\n\n        strategy.loss += self.ewc_lambda * penalty",
  "def after_training_exp(self, strategy, **kwargs):\n        \"\"\"\n        Compute importances of parameters after each experience.\n        \"\"\"\n\n        importances = self.compute_importances(strategy.model,\n                                               strategy._criterion,\n                                               strategy.optimizer,\n                                               strategy.experience.dataset,\n                                               strategy.device,\n                                               strategy.train_mb_size)\n        self.update_importances(importances, strategy.training_exp_counter)\n        self.saved_params[strategy.training_exp_counter] = \\\n            copy_params_dict(strategy.model)\n        # clear previuos parameter values\n        if strategy.training_exp_counter > 0 and \\\n                (not self.keep_importance_data):\n            del self.saved_params[strategy.training_exp_counter - 1]",
  "def compute_importances(self, model, criterion, optimizer,\n                            dataset, device, batch_size):\n        \"\"\"\n        Compute EWC importance matrix for each parameter\n        \"\"\"\n\n        model.train()\n\n        # list of list\n        importances = zerolike_params_dict(model)\n        dataloader = DataLoader(dataset, batch_size=batch_size)\n        for i, (x, y, task_labels) in enumerate(dataloader):\n            x, y = x.to(device), y.to(device)\n\n            optimizer.zero_grad()\n            out = avalanche_forward(model, x, task_labels)\n            loss = criterion(out, y)\n            loss.backward()\n\n            for (k1, p), (k2, imp) in zip(model.named_parameters(),\n                                          importances):\n                assert (k1 == k2)\n                if p.grad is not None:\n                    imp += p.grad.data.clone().pow(2)\n\n        # average over mini batch length\n        for _, imp in importances:\n            imp /= float(len(dataloader))\n\n        return importances",
  "def update_importances(self, importances, t):\n        \"\"\"\n        Update importance for each parameter based on the currently computed\n        importances.\n        \"\"\"\n\n        if self.mode == 'separate' or t == 0:\n            self.importances[t] = importances\n        elif self.mode == 'online':\n            for (k1, old_imp), (k2, curr_imp) in \\\n                    zip(self.importances[t - 1], importances):\n                assert k1 == k2, 'Error in importance computation.'\n                self.importances[t].append(\n                    (k1, (self.decay_factor * old_imp + curr_imp)))\n\n            # clear previous parameter importances\n            if t > 0 and (not self.keep_importance_data):\n                del self.importances[t - 1]\n\n        else:\n            raise ValueError(\"Wrong EWC mode.\")",
  "class CWRStarPlugin(StrategyPlugin):\n\n    def __init__(self, model, cwr_layer_name=None, freeze_remaining_model=True):\n        \"\"\"\n        CWR* Strategy.\n        This plugin does not use task identities.\n\n        :param model: the model.\n        :param cwr_layer_name: name of the last fully connected layer. Defaults\n            to None, which means that the plugin will attempt an automatic\n            detection.\n        :param freeze_remaining_model: If True, the plugin will freeze (set\n            layers in eval mode and disable autograd for parameters) all the\n            model except the cwr layer. Defaults to True.\n        \"\"\"\n        super().__init__()\n        self.log = logging.getLogger(\"avalanche\")\n        self.model = model\n        self.cwr_layer_name = cwr_layer_name\n        self.freeze_remaining_model = freeze_remaining_model\n\n        # Model setup\n        self.model.saved_weights = {}\n        self.model.past_j = defaultdict(int)\n        self.model.cur_j = defaultdict(int)\n\n        # to be updated\n        self.cur_class = None\n\n    def after_training_exp(self, strategy, **kwargs):\n        self.consolidate_weights()\n        self.set_consolidate_weights()\n\n    def before_training_exp(self, strategy, **kwargs):\n        if self.freeze_remaining_model and strategy.training_exp_counter > 0:\n            self.freeze_other_layers()\n\n        # Count current classes and number of samples for each of them.\n        data = strategy.experience.dataset\n        self.model.cur_j = examples_per_class(data.targets)\n        self.cur_class = [cls for cls in set(self.model.cur_j.keys()) if\n                          self.model.cur_j[cls] > 0]\n\n        self.reset_weights(self.cur_class)\n\n    def consolidate_weights(self):\n        \"\"\" Mean-shift for the target layer weights\"\"\"\n\n        with torch.no_grad():\n            cwr_layer = self.get_cwr_layer()\n            globavg = np.average(cwr_layer.weight.detach()\n                                 .cpu().numpy()[self.cur_class])\n            for c in self.cur_class:\n                w = cwr_layer.weight.detach().cpu().numpy()[c]\n\n                if c in self.cur_class:\n                    new_w = w - globavg\n                    if c in self.model.saved_weights.keys():\n                        wpast_j = np.sqrt(self.model.past_j[c] /\n                                          self.model.cur_j[c])\n                        # wpast_j = model.past_j[c] / model.cur_j[c]\n                        self.model.saved_weights[c] = \\\n                            (self.model.saved_weights[c] * wpast_j + new_w) / \\\n                            (wpast_j + 1)\n                    else:\n                        self.model.saved_weights[c] = new_w\n\n    def set_consolidate_weights(self):\n        \"\"\" set trained weights \"\"\"\n\n        with torch.no_grad():\n            cwr_layer = self.get_cwr_layer()\n            for c, w in self.model.saved_weights.items():\n                cwr_layer.weight[c].copy_(\n                    torch.from_numpy(self.model.saved_weights[c])\n                )\n\n    def reset_weights(self, cur_clas):\n        \"\"\" reset weights\"\"\"\n        with torch.no_grad():\n            cwr_layer = self.get_cwr_layer()\n            cwr_layer.weight.fill_(0.0)\n            for c, w in self.model.saved_weights.items():\n                if c in cur_clas:\n                    cwr_layer.weight[c].copy_(\n                        torch.from_numpy(self.model.saved_weights[c])\n                    )\n\n    def get_cwr_layer(self) -> Optional[Linear]:\n        result = None\n        if self.cwr_layer_name is None:\n            last_fc = get_last_fc_layer(self.model)\n            if last_fc is not None:\n                result = last_fc[1]\n        else:\n            result = get_layer_by_name(self.model, self.cwr_layer_name)\n\n        return result\n\n    def freeze_other_layers(self):\n        cwr_layer = self.get_cwr_layer()\n        if cwr_layer is None:\n            raise RuntimeError('Can\\'t find a the Linear layer')\n        freeze_everything(self.model)\n        unfreeze_everything(cwr_layer)",
  "def __init__(self, model, cwr_layer_name=None, freeze_remaining_model=True):\n        \"\"\"\n        CWR* Strategy.\n        This plugin does not use task identities.\n\n        :param model: the model.\n        :param cwr_layer_name: name of the last fully connected layer. Defaults\n            to None, which means that the plugin will attempt an automatic\n            detection.\n        :param freeze_remaining_model: If True, the plugin will freeze (set\n            layers in eval mode and disable autograd for parameters) all the\n            model except the cwr layer. Defaults to True.\n        \"\"\"\n        super().__init__()\n        self.log = logging.getLogger(\"avalanche\")\n        self.model = model\n        self.cwr_layer_name = cwr_layer_name\n        self.freeze_remaining_model = freeze_remaining_model\n\n        # Model setup\n        self.model.saved_weights = {}\n        self.model.past_j = defaultdict(int)\n        self.model.cur_j = defaultdict(int)\n\n        # to be updated\n        self.cur_class = None",
  "def after_training_exp(self, strategy, **kwargs):\n        self.consolidate_weights()\n        self.set_consolidate_weights()",
  "def before_training_exp(self, strategy, **kwargs):\n        if self.freeze_remaining_model and strategy.training_exp_counter > 0:\n            self.freeze_other_layers()\n\n        # Count current classes and number of samples for each of them.\n        data = strategy.experience.dataset\n        self.model.cur_j = examples_per_class(data.targets)\n        self.cur_class = [cls for cls in set(self.model.cur_j.keys()) if\n                          self.model.cur_j[cls] > 0]\n\n        self.reset_weights(self.cur_class)",
  "def consolidate_weights(self):\n        \"\"\" Mean-shift for the target layer weights\"\"\"\n\n        with torch.no_grad():\n            cwr_layer = self.get_cwr_layer()\n            globavg = np.average(cwr_layer.weight.detach()\n                                 .cpu().numpy()[self.cur_class])\n            for c in self.cur_class:\n                w = cwr_layer.weight.detach().cpu().numpy()[c]\n\n                if c in self.cur_class:\n                    new_w = w - globavg\n                    if c in self.model.saved_weights.keys():\n                        wpast_j = np.sqrt(self.model.past_j[c] /\n                                          self.model.cur_j[c])\n                        # wpast_j = model.past_j[c] / model.cur_j[c]\n                        self.model.saved_weights[c] = \\\n                            (self.model.saved_weights[c] * wpast_j + new_w) / \\\n                            (wpast_j + 1)\n                    else:\n                        self.model.saved_weights[c] = new_w",
  "def set_consolidate_weights(self):\n        \"\"\" set trained weights \"\"\"\n\n        with torch.no_grad():\n            cwr_layer = self.get_cwr_layer()\n            for c, w in self.model.saved_weights.items():\n                cwr_layer.weight[c].copy_(\n                    torch.from_numpy(self.model.saved_weights[c])\n                )",
  "def reset_weights(self, cur_clas):\n        \"\"\" reset weights\"\"\"\n        with torch.no_grad():\n            cwr_layer = self.get_cwr_layer()\n            cwr_layer.weight.fill_(0.0)\n            for c, w in self.model.saved_weights.items():\n                if c in cur_clas:\n                    cwr_layer.weight[c].copy_(\n                        torch.from_numpy(self.model.saved_weights[c])\n                    )",
  "def get_cwr_layer(self) -> Optional[Linear]:\n        result = None\n        if self.cwr_layer_name is None:\n            last_fc = get_last_fc_layer(self.model)\n            if last_fc is not None:\n                result = last_fc[1]\n        else:\n            result = get_layer_by_name(self.model, self.cwr_layer_name)\n\n        return result",
  "def freeze_other_layers(self):\n        cwr_layer = self.get_cwr_layer()\n        if cwr_layer is None:\n            raise RuntimeError('Can\\'t find a the Linear layer')\n        freeze_everything(self.model)\n        unfreeze_everything(cwr_layer)",
  "class SynapticIntelligencePlugin(StrategyPlugin):\n    \"\"\"\n    The Synaptic Intelligence plugin.\n\n    This is the Synaptic Intelligence PyTorch implementation of the\n    algorithm described in the paper\n    \"Continuous Learning in Single-Incremental-Task Scenarios\"\n    (https://arxiv.org/abs/1806.08568)\n\n    The original implementation has been proposed in the paper\n    \"Continual Learning Through Synaptic Intelligence\"\n    (https://arxiv.org/abs/1703.04200).\n\n    This plugin can be attached to existing strategies to achieve a\n    regularization effect.\n\n    This plugin will require the strategy `loss` field to be set before the\n    `before_backward` callback is invoked. The loss Tensor will be updated to\n    achieve the S.I. regularization effect.\n    \"\"\"\n\n    def __init__(self, si_lambda: float,\n                 excluded_parameters: Sequence['str'] = None,\n                 device: Any = 'as_strategy'):\n        \"\"\"\n        Creates an instance of the Synaptic Intelligence plugin.\n\n        :param si_lambda: Synaptic Intelligence lambda term.\n        :param device: The device to use to run the S.I. experiences.\n            Defaults to \"as_strategy\", which means that the `device` field of\n            the strategy will be used. Using a different device may lead to a\n            performance drop due to the required data transfer.\n        \"\"\"\n\n        super().__init__()\n\n        warnings.warn(\"The Synaptic Intelligence plugin is in an alpha stage \"\n                      \"and is not perfectly aligned with the paper \"\n                      \"implementation. Please use at your own risk!\")\n\n        if excluded_parameters is None:\n            excluded_parameters = []\n        self.si_lambda: float = si_lambda\n        self.excluded_parameters: Set[str] = set(excluded_parameters)\n        self.ewc_data: EwcDataType = (dict(), dict())\n        \"\"\"\n        The first dictionary contains the params at loss minimum while the \n        second one contains the parameter importance.\n        \"\"\"\n\n        self.syn_data: SynDataType = {\n            'old_theta': dict(),\n            'new_theta': dict(),\n            'grad': dict(),\n            'trajectory': dict(),\n            'cum_trajectory': dict()}\n\n        self._device = device\n\n    def before_training_exp(self, strategy: 'BaseStrategy', **kwargs):\n        super().before_training_exp(strategy, **kwargs)\n        SynapticIntelligencePlugin.create_syn_data(\n            strategy.model, self.ewc_data, self.syn_data,\n            self.excluded_parameters)\n\n        SynapticIntelligencePlugin.init_batch(\n            strategy.model, self.ewc_data, self.syn_data,\n            self.excluded_parameters)\n\n    def after_backward(self, strategy: 'BaseStrategy', **kwargs):\n        super().after_backward(strategy, **kwargs)\n        syn_loss = SynapticIntelligencePlugin.compute_ewc_loss(\n            strategy.model, self.ewc_data, self.excluded_parameters,\n            lambd=self.si_lambda, device=self.device(strategy))\n\n        if syn_loss is not None:\n            strategy.loss += syn_loss.to(strategy.device)\n\n    def before_training_iteration(self, strategy: 'BaseStrategy', **kwargs):\n        super().before_training_iteration(strategy, **kwargs)\n        SynapticIntelligencePlugin.pre_update(strategy.model, self.syn_data,\n                                              self.excluded_parameters)\n\n    def after_training_iteration(self, strategy: 'BaseStrategy', **kwargs):\n        super().after_training_iteration(strategy, **kwargs)\n        SynapticIntelligencePlugin.post_update(strategy.model, self.syn_data,\n                                               self.excluded_parameters)\n\n    def after_training_exp(self, strategy: 'BaseStrategy', **kwargs):\n        super().after_training_exp(strategy, **kwargs)\n        SynapticIntelligencePlugin.update_ewc_data(\n            strategy.model, self.ewc_data, self.syn_data, 0.001,\n            self.excluded_parameters, 1)\n\n    def device(self, strategy: 'BaseStrategy'):\n        if self._device == 'as_strategy':\n            return strategy.device\n\n        return self._device\n\n    @staticmethod\n    @torch.no_grad()\n    def create_syn_data(model: Module, ewc_data: EwcDataType,\n                        syn_data: SynDataType, excluded_parameters: Set[str]):\n        params = SynapticIntelligencePlugin.allowed_parameters(\n            model, excluded_parameters)\n\n        for param_name, param in params:\n            if param_name in ewc_data[0]:\n                continue\n\n            # Handles added parameters (doesn't manage parameter expansion!)\n            ewc_data[0][param_name] = SynapticIntelligencePlugin._zero(param)\n            ewc_data[1][param_name] = SynapticIntelligencePlugin._zero(param)\n\n            syn_data['old_theta'][param_name] = \\\n                SynapticIntelligencePlugin._zero(param)\n            syn_data['new_theta'][param_name] = \\\n                SynapticIntelligencePlugin._zero(param)\n            syn_data['grad'][param_name] = \\\n                SynapticIntelligencePlugin._zero(param)\n            syn_data['trajectory'][param_name] = \\\n                SynapticIntelligencePlugin._zero(param)\n            syn_data['cum_trajectory'][param_name] = \\\n                SynapticIntelligencePlugin._zero(param)\n\n    @staticmethod\n    @torch.no_grad()\n    def _zero(param: Tensor):\n        return torch.zeros(param.numel(), dtype=param.dtype)\n\n    @staticmethod\n    @torch.no_grad()\n    def extract_weights(model: Module, target: ParamDict,\n                        excluded_parameters: Set[str]):\n        params = SynapticIntelligencePlugin.allowed_parameters(\n            model, excluded_parameters)\n\n        for name, param in params:\n            target[name][...] = param.detach().cpu().flatten()\n\n    @staticmethod\n    @torch.no_grad()\n    def extract_grad(model, target: ParamDict, excluded_parameters: Set[str]):\n        params = SynapticIntelligencePlugin.allowed_parameters(\n            model, excluded_parameters)\n\n        # Store the gradients into target\n        for name, param in params:\n            target[name][...] = param.grad.detach().cpu().flatten()\n\n    @staticmethod\n    @torch.no_grad()\n    def init_batch(model, ewc_data: EwcDataType, syn_data: SynDataType,\n                   excluded_parameters: Set[str]):\n        # Keep initial weights\n        SynapticIntelligencePlugin. \\\n            extract_weights(model, ewc_data[0], excluded_parameters)\n        for param_name, param_trajectory in syn_data['trajectory'].items():\n            param_trajectory.fill_(0.0)\n\n    @staticmethod\n    @torch.no_grad()\n    def pre_update(model, syn_data: SynDataType,\n                   excluded_parameters: Set[str]):\n        SynapticIntelligencePlugin.extract_weights(model, syn_data['old_theta'],\n                                                   excluded_parameters)\n\n    @staticmethod\n    @torch.no_grad()\n    def post_update(model, syn_data: SynDataType,\n                    excluded_parameters: Set[str]):\n        SynapticIntelligencePlugin.extract_weights(model, syn_data['new_theta'],\n                                                   excluded_parameters)\n        SynapticIntelligencePlugin.extract_grad(model, syn_data['grad'],\n                                                excluded_parameters)\n\n        for param_name in syn_data['trajectory']:\n            syn_data['trajectory'][param_name] += \\\n                syn_data['grad'][param_name] * (\n                        syn_data['new_theta'][param_name] -\n                        syn_data['old_theta'][param_name])\n\n    @staticmethod\n    def compute_ewc_loss(model, ewc_data: EwcDataType,\n                         excluded_parameters: Set[str], device, lambd=0.0):\n        params = SynapticIntelligencePlugin.allowed_parameters(\n            model, excluded_parameters)\n\n        loss = None\n        for name, param in params:\n            weights = param.to(device).flatten()  # Flat, not detached\n            param_ewc_data_0 = ewc_data[0][name].to(device)  # Flat, detached\n            param_ewc_data_1 = ewc_data[1][name].to(device)  # Flat, detached\n\n            syn_loss: Tensor = torch.dot(\n                param_ewc_data_1,\n                (weights - param_ewc_data_0) ** 2) * (lambd / 2)\n\n            if loss is None:\n                loss = syn_loss\n            else:\n                loss += syn_loss\n\n        return loss\n\n    @staticmethod\n    @torch.no_grad()\n    def update_ewc_data(net, ewc_data: EwcDataType, syn_data: SynDataType,\n                        clip_to: float, excluded_parameters: Set[str],\n                        c=0.0015):\n        SynapticIntelligencePlugin.extract_weights(net, syn_data['new_theta'],\n                                                   excluded_parameters)\n        eps = 0.0000001  # 0.001 in few task - 0.1 used in a more complex setup\n\n        for param_name in syn_data['cum_trajectory']:\n            syn_data['cum_trajectory'][param_name] += \\\n                c * syn_data['trajectory'][param_name] / (\n                        np.square(syn_data['new_theta'][param_name] -\n                                  ewc_data[0][param_name]) + eps)\n\n        for param_name in syn_data['cum_trajectory']:\n            ewc_data[1][param_name] = torch.empty_like(\n                syn_data['cum_trajectory'][param_name]).copy_(\n                -syn_data['cum_trajectory'][param_name])\n\n        # change sign here because the Ewc regularization\n        # in Caffe (theta - thetaold) is inverted w.r.t. syn equation [4]\n        # (thetaold - theta)\n        for param_name in ewc_data[1]:\n            ewc_data[1][param_name] = torch.clamp(\n                ewc_data[1][param_name], max=clip_to)\n            ewc_data[0][param_name] = \\\n                syn_data['new_theta'][param_name].clone()\n\n    @staticmethod\n    def explode_excluded_parameters(excluded: Set[str]) -> Set[str]:\n        \"\"\"\n        Explodes a list of excluded parameters by adding a generic final \".*\"\n        wildcard at its end.\n\n        :param excluded: The original set of excluded parameters.\n\n        :return: The set of excluded parameters in which \".*\" patterns have been\n            added.\n        \"\"\"\n        result = set()\n        for x in excluded:\n            result.add(x)\n            if not x.endswith('*'):\n                result.add(x + '.*')\n        return result\n\n    @staticmethod\n    def not_excluded_parameters(model: Module, excluded_parameters: Set[str]) \\\n            -> List[Tuple[str, Tensor]]:\n        # Add wildcards \".*\" to all excluded parameter names\n        result = []\n        excluded_parameters = SynapticIntelligencePlugin. \\\n            explode_excluded_parameters(excluded_parameters)\n        layers_params = get_layers_and_params(model)\n\n        for lp in layers_params:\n            if isinstance(lp.layer, _NormBase):\n                # Exclude batch norm parameters\n                excluded_parameters.add(lp.parameter_name)\n\n        for name, param in model.named_parameters():\n            accepted = True\n            for exclusion_pattern in excluded_parameters:\n                if fnmatch(name, exclusion_pattern):\n                    accepted = False\n                    break\n\n            if accepted:\n                result.append((name, param))\n\n        return result\n\n    @staticmethod\n    def allowed_parameters(model: Module, excluded_parameters: Set[str]) \\\n            -> List[Tuple[str, Tensor]]:\n\n        allow_list = SynapticIntelligencePlugin.not_excluded_parameters(\n            model, excluded_parameters)\n\n        result = []\n        for name, param in allow_list:\n            if param.requires_grad:\n                result.append((name, param))\n\n        return result",
  "def __init__(self, si_lambda: float,\n                 excluded_parameters: Sequence['str'] = None,\n                 device: Any = 'as_strategy'):\n        \"\"\"\n        Creates an instance of the Synaptic Intelligence plugin.\n\n        :param si_lambda: Synaptic Intelligence lambda term.\n        :param device: The device to use to run the S.I. experiences.\n            Defaults to \"as_strategy\", which means that the `device` field of\n            the strategy will be used. Using a different device may lead to a\n            performance drop due to the required data transfer.\n        \"\"\"\n\n        super().__init__()\n\n        warnings.warn(\"The Synaptic Intelligence plugin is in an alpha stage \"\n                      \"and is not perfectly aligned with the paper \"\n                      \"implementation. Please use at your own risk!\")\n\n        if excluded_parameters is None:\n            excluded_parameters = []\n        self.si_lambda: float = si_lambda\n        self.excluded_parameters: Set[str] = set(excluded_parameters)\n        self.ewc_data: EwcDataType = (dict(), dict())\n        \"\"\"\n        The first dictionary contains the params at loss minimum while the \n        second one contains the parameter importance.\n        \"\"\"\n\n        self.syn_data: SynDataType = {\n            'old_theta': dict(),\n            'new_theta': dict(),\n            'grad': dict(),\n            'trajectory': dict(),\n            'cum_trajectory': dict()}\n\n        self._device = device",
  "def before_training_exp(self, strategy: 'BaseStrategy', **kwargs):\n        super().before_training_exp(strategy, **kwargs)\n        SynapticIntelligencePlugin.create_syn_data(\n            strategy.model, self.ewc_data, self.syn_data,\n            self.excluded_parameters)\n\n        SynapticIntelligencePlugin.init_batch(\n            strategy.model, self.ewc_data, self.syn_data,\n            self.excluded_parameters)",
  "def after_backward(self, strategy: 'BaseStrategy', **kwargs):\n        super().after_backward(strategy, **kwargs)\n        syn_loss = SynapticIntelligencePlugin.compute_ewc_loss(\n            strategy.model, self.ewc_data, self.excluded_parameters,\n            lambd=self.si_lambda, device=self.device(strategy))\n\n        if syn_loss is not None:\n            strategy.loss += syn_loss.to(strategy.device)",
  "def before_training_iteration(self, strategy: 'BaseStrategy', **kwargs):\n        super().before_training_iteration(strategy, **kwargs)\n        SynapticIntelligencePlugin.pre_update(strategy.model, self.syn_data,\n                                              self.excluded_parameters)",
  "def after_training_iteration(self, strategy: 'BaseStrategy', **kwargs):\n        super().after_training_iteration(strategy, **kwargs)\n        SynapticIntelligencePlugin.post_update(strategy.model, self.syn_data,\n                                               self.excluded_parameters)",
  "def after_training_exp(self, strategy: 'BaseStrategy', **kwargs):\n        super().after_training_exp(strategy, **kwargs)\n        SynapticIntelligencePlugin.update_ewc_data(\n            strategy.model, self.ewc_data, self.syn_data, 0.001,\n            self.excluded_parameters, 1)",
  "def device(self, strategy: 'BaseStrategy'):\n        if self._device == 'as_strategy':\n            return strategy.device\n\n        return self._device",
  "def create_syn_data(model: Module, ewc_data: EwcDataType,\n                        syn_data: SynDataType, excluded_parameters: Set[str]):\n        params = SynapticIntelligencePlugin.allowed_parameters(\n            model, excluded_parameters)\n\n        for param_name, param in params:\n            if param_name in ewc_data[0]:\n                continue\n\n            # Handles added parameters (doesn't manage parameter expansion!)\n            ewc_data[0][param_name] = SynapticIntelligencePlugin._zero(param)\n            ewc_data[1][param_name] = SynapticIntelligencePlugin._zero(param)\n\n            syn_data['old_theta'][param_name] = \\\n                SynapticIntelligencePlugin._zero(param)\n            syn_data['new_theta'][param_name] = \\\n                SynapticIntelligencePlugin._zero(param)\n            syn_data['grad'][param_name] = \\\n                SynapticIntelligencePlugin._zero(param)\n            syn_data['trajectory'][param_name] = \\\n                SynapticIntelligencePlugin._zero(param)\n            syn_data['cum_trajectory'][param_name] = \\\n                SynapticIntelligencePlugin._zero(param)",
  "def _zero(param: Tensor):\n        return torch.zeros(param.numel(), dtype=param.dtype)",
  "def extract_weights(model: Module, target: ParamDict,\n                        excluded_parameters: Set[str]):\n        params = SynapticIntelligencePlugin.allowed_parameters(\n            model, excluded_parameters)\n\n        for name, param in params:\n            target[name][...] = param.detach().cpu().flatten()",
  "def extract_grad(model, target: ParamDict, excluded_parameters: Set[str]):\n        params = SynapticIntelligencePlugin.allowed_parameters(\n            model, excluded_parameters)\n\n        # Store the gradients into target\n        for name, param in params:\n            target[name][...] = param.grad.detach().cpu().flatten()",
  "def init_batch(model, ewc_data: EwcDataType, syn_data: SynDataType,\n                   excluded_parameters: Set[str]):\n        # Keep initial weights\n        SynapticIntelligencePlugin. \\\n            extract_weights(model, ewc_data[0], excluded_parameters)\n        for param_name, param_trajectory in syn_data['trajectory'].items():\n            param_trajectory.fill_(0.0)",
  "def pre_update(model, syn_data: SynDataType,\n                   excluded_parameters: Set[str]):\n        SynapticIntelligencePlugin.extract_weights(model, syn_data['old_theta'],\n                                                   excluded_parameters)",
  "def post_update(model, syn_data: SynDataType,\n                    excluded_parameters: Set[str]):\n        SynapticIntelligencePlugin.extract_weights(model, syn_data['new_theta'],\n                                                   excluded_parameters)\n        SynapticIntelligencePlugin.extract_grad(model, syn_data['grad'],\n                                                excluded_parameters)\n\n        for param_name in syn_data['trajectory']:\n            syn_data['trajectory'][param_name] += \\\n                syn_data['grad'][param_name] * (\n                        syn_data['new_theta'][param_name] -\n                        syn_data['old_theta'][param_name])",
  "def compute_ewc_loss(model, ewc_data: EwcDataType,\n                         excluded_parameters: Set[str], device, lambd=0.0):\n        params = SynapticIntelligencePlugin.allowed_parameters(\n            model, excluded_parameters)\n\n        loss = None\n        for name, param in params:\n            weights = param.to(device).flatten()  # Flat, not detached\n            param_ewc_data_0 = ewc_data[0][name].to(device)  # Flat, detached\n            param_ewc_data_1 = ewc_data[1][name].to(device)  # Flat, detached\n\n            syn_loss: Tensor = torch.dot(\n                param_ewc_data_1,\n                (weights - param_ewc_data_0) ** 2) * (lambd / 2)\n\n            if loss is None:\n                loss = syn_loss\n            else:\n                loss += syn_loss\n\n        return loss",
  "def update_ewc_data(net, ewc_data: EwcDataType, syn_data: SynDataType,\n                        clip_to: float, excluded_parameters: Set[str],\n                        c=0.0015):\n        SynapticIntelligencePlugin.extract_weights(net, syn_data['new_theta'],\n                                                   excluded_parameters)\n        eps = 0.0000001  # 0.001 in few task - 0.1 used in a more complex setup\n\n        for param_name in syn_data['cum_trajectory']:\n            syn_data['cum_trajectory'][param_name] += \\\n                c * syn_data['trajectory'][param_name] / (\n                        np.square(syn_data['new_theta'][param_name] -\n                                  ewc_data[0][param_name]) + eps)\n\n        for param_name in syn_data['cum_trajectory']:\n            ewc_data[1][param_name] = torch.empty_like(\n                syn_data['cum_trajectory'][param_name]).copy_(\n                -syn_data['cum_trajectory'][param_name])\n\n        # change sign here because the Ewc regularization\n        # in Caffe (theta - thetaold) is inverted w.r.t. syn equation [4]\n        # (thetaold - theta)\n        for param_name in ewc_data[1]:\n            ewc_data[1][param_name] = torch.clamp(\n                ewc_data[1][param_name], max=clip_to)\n            ewc_data[0][param_name] = \\\n                syn_data['new_theta'][param_name].clone()",
  "def explode_excluded_parameters(excluded: Set[str]) -> Set[str]:\n        \"\"\"\n        Explodes a list of excluded parameters by adding a generic final \".*\"\n        wildcard at its end.\n\n        :param excluded: The original set of excluded parameters.\n\n        :return: The set of excluded parameters in which \".*\" patterns have been\n            added.\n        \"\"\"\n        result = set()\n        for x in excluded:\n            result.add(x)\n            if not x.endswith('*'):\n                result.add(x + '.*')\n        return result",
  "def not_excluded_parameters(model: Module, excluded_parameters: Set[str]) \\\n            -> List[Tuple[str, Tensor]]:\n        # Add wildcards \".*\" to all excluded parameter names\n        result = []\n        excluded_parameters = SynapticIntelligencePlugin. \\\n            explode_excluded_parameters(excluded_parameters)\n        layers_params = get_layers_and_params(model)\n\n        for lp in layers_params:\n            if isinstance(lp.layer, _NormBase):\n                # Exclude batch norm parameters\n                excluded_parameters.add(lp.parameter_name)\n\n        for name, param in model.named_parameters():\n            accepted = True\n            for exclusion_pattern in excluded_parameters:\n                if fnmatch(name, exclusion_pattern):\n                    accepted = False\n                    break\n\n            if accepted:\n                result.append((name, param))\n\n        return result",
  "def allowed_parameters(model: Module, excluded_parameters: Set[str]) \\\n            -> List[Tuple[str, Tensor]]:\n\n        allow_list = SynapticIntelligencePlugin.not_excluded_parameters(\n            model, excluded_parameters)\n\n        result = []\n        for name, param in allow_list:\n            if param.requires_grad:\n                result.append((name, param))\n\n        return result",
  "class CoPEPlugin(StrategyPlugin):\n    \"\"\"\n    Continual Prototype Evolution plugin.\n    Each class has a prototype for nearest-neighbor classification.\n    The prototypes are updated continually with an exponentially moving average,\n    using class-balanced replay to keep the prototypes up-to-date.\n    The embedding space is optimized using the PseudoPrototypicalProxy-loss,\n    exploiting both prototypes and batch information.\n\n    This plugin doesn't use task identities in training or eval\n    (data incremental) and is designed for online learning (1 epoch per task).\n    \"\"\"\n\n    def __init__(self, mem_size=200, n_classes=10, p_size=100, alpha=0.99,\n                 T=0.1, max_it_cnt=1):\n        \"\"\"\n        :param mem_size: max number of input samples in the replay memory.\n        :param n_classes: total number of classes that will be encountered. This\n        is used to output predictions for all classes, with zero probability\n        for unseen classes.\n        :param p_size: The prototype size, which equals the feature size of the\n        last layer.\n        :param alpha: The momentum for the exponentially moving average of the\n        prototypes.\n        :param T: The softmax temperature, used as a concentration parameter.\n        :param max_it_cnt: How many processing iterations per batch (experience)\n        \"\"\"\n        super().__init__()\n        self.n_classes = n_classes\n        self.it_cnt = 0\n        self.max_it_cnt = max_it_cnt\n\n        # Operational memory: replay memory\n        self.replay_mem = {}\n        self.mem_size = mem_size  # replay memory size\n        self.storage_policy = ClassBalancedStoragePolicy(\n            ext_mem=self.replay_mem,\n            mem_size=self.mem_size,\n            adaptive_size=True)\n\n        # Operational memory: Prototypical memory\n        self.p_mem = {}  # Scales with nb classes * feature size\n        self.p_size = p_size  # Prototype size determined on runtime\n        self.tmp_p_mem = {}  # Intermediate to process batch for multiple times\n        self.alpha = alpha\n        self.p_init_adaptive = False  # Only create proto when class seen\n\n        # PPP-loss\n        self.T = T\n        self.ppp_loss = PPPloss(self.p_mem, T=self.T)\n\n        self.initialized = False\n\n    def before_training(self, strategy, **kwargs):\n        \"\"\" Enforce using the PPP-loss and add a NN-classifier.\"\"\"\n        if not self.initialized:\n            strategy._criterion = self.ppp_loss\n            print(\"Using the Pseudo-Prototypical-Proxy loss for CoPE.\")\n\n            # Normalize representation of last layer\n            swap_last_fc_layer(strategy.model,\n                               torch.nn.Sequential(\n                                   get_last_fc_layer(strategy.model)[1],\n                                   L2Normalization())\n                               )\n\n            # Static prototype init\n            # Create prototypes for all classes at once\n            if not self.p_init_adaptive and len(self.p_mem) == 0:\n                self._init_new_prototypes(\n                    torch.arange(0, self.n_classes).to(strategy.device))\n\n            self.initialized = True\n\n    def before_training_exp(self, strategy, num_workers=0, shuffle=True,\n                            **kwargs):\n        \"\"\"\n        Random retrieval from a class-balanced memory.\n        Dataloader builds batches containing examples from both memories and\n        the training dataset.\n        This implementation requires the use of early stopping, otherwise the\n        entire memory will be iterated.\n        \"\"\"\n        if len(self.replay_mem) == 0:\n            return\n        self.it_cnt = 0\n        strategy.dataloader = ReplayDataLoader(\n            strategy.adapted_dataset,\n            AvalancheConcatDataset(self.replay_mem.values()),\n            oversample_small_tasks=False,\n            num_workers=num_workers,\n            batch_size=strategy.train_mb_size * 2,\n            force_data_batch_size=strategy.train_mb_size,\n            shuffle=shuffle)\n\n    def after_training_iteration(self, strategy, **kwargs):\n        \"\"\"\n        Implements early stopping, determining how many subsequent times a\n        batch can be used for updates. The dataloader contains only data for\n        the current experience (batch) and the entire memory.\n        Multiple iterations will hence result in the original batch with new\n        exemplars sampled from the memory for each iteration.\n        \"\"\"\n        self.it_cnt += 1\n        if self.it_cnt == self.max_it_cnt:\n            strategy.stop_training()  # Stop processing the new-data batch\n\n    def after_forward(self, strategy, **kwargs):\n        \"\"\"\n        After the forward we can use the representations to update our running\n        avg of the prototypes. This is in case we do multiple iterations of\n        processing on the same batch.\n\n        New prototypes are initialized for previously unseen classes.\n        \"\"\"\n\n        if self.p_init_adaptive:  # Init prototypes for unseen classes in batch\n            self._init_new_prototypes(strategy.mb_y)\n\n        # Update batch info (when multiple iterations on same batch)\n        self._update_running_prototypes(strategy)\n\n    @torch.no_grad()\n    def _init_new_prototypes(self, targets: Tensor):\n        \"\"\"Initialize prototypes for previously unseen classes.\n        :param targets: The targets Tensor to make prototypes for.\n        \"\"\"\n        y_unique = torch.unique(targets).squeeze().view(-1)\n        for idx in range(y_unique.size(0)):\n            c = y_unique[idx].item()\n            if c not in self.p_mem:  # Init new prototype\n                self.p_mem[c] = normalize(\n                    torch.empty((1, self.p_size)).uniform_(-1, 1), p=2,\n                    dim=1).detach().to(targets.device)\n\n    @torch.no_grad()\n    def _update_running_prototypes(self, strategy):\n        \"\"\" Accumulate seen outputs of the network and keep counts. \"\"\"\n        y_unique = torch.unique(strategy.mb_y).squeeze().view(-1)\n        for idx in range(y_unique.size(0)):\n            c = y_unique[idx].item()\n            idxs = torch.nonzero(strategy.mb_y == c).squeeze(1)\n            p_tmp_batch = strategy.mb_output[idxs].sum(dim=0).unsqueeze(0).to(\n                strategy.device)\n\n            p_init, cnt_init = self.tmp_p_mem[c] \\\n                if c in self.tmp_p_mem else (0, 0)\n            self.tmp_p_mem[c] = (p_init + p_tmp_batch, cnt_init + len(idxs))\n\n    def after_training_exp(self, strategy, **kwargs):\n        \"\"\" After the current experience (batch), update prototypes and\n        store observed samples for replay.\n        \"\"\"\n        self._update_prototypes()  # Update prototypes\n        self.storage_policy(strategy)  # Update memory\n\n    @torch.no_grad()\n    def _update_prototypes(self):\n        \"\"\" Update the prototypes based on the running averages. \"\"\"\n        for c, (p_sum, p_cnt) in self.tmp_p_mem.items():\n            incr_p = normalize(p_sum / p_cnt, p=2, dim=1)  # L2 normalized\n            old_p = self.p_mem[c].clone()\n            new_p_momentum = self.alpha * old_p + (\n                    1 - self.alpha) * incr_p  # Momentum update\n            self.p_mem[c] = normalize(new_p_momentum, p=2, dim=1).detach()\n        self.tmp_p_mem = {}\n\n    def after_eval_iteration(self, strategy, **kwargs):\n        \"\"\" Convert output scores to probabilities for other metrics like\n        accuracy and forgetting. We only do it at this point because before\n        this,we still need the embedding outputs to obtain the PPP-loss.\"\"\"\n        strategy.mb_output = self._get_nearest_neigbor_distr(strategy.mb_output)\n\n    def _get_nearest_neigbor_distr(self, x: Tensor) -> Tensor:\n        \"\"\"\n        Find closest prototype for output samples in batch x.\n        :param x: Batch of network logits.\n        :return: one-hot representation of the predicted class.\n        \"\"\"\n        ns = x.size(0)\n        nd = x.view(ns, -1).shape[-1]\n\n        # Get prototypes\n        seen_c = len(self.p_mem.keys())\n        if seen_c == 0:  # no prototypes yet, output uniform distr. all classes\n            return torch.Tensor(ns, self.n_classes\n                                ).fill_(1.0 / self.n_classes).to(x.device)\n        means = torch.ones(seen_c, nd).to(x.device) * float('inf')\n        for c, c_proto in self.p_mem.items():\n            means[c] = c_proto  # Class idx gets allocated its prototype\n\n        # Predict nearest mean\n        classpred = torch.LongTensor(ns)\n        for s_idx in range(ns):  # Per sample\n            dist = - torch.mm(means, x[s_idx].unsqueeze(-1))  # Dot product\n            _, ii = dist.min(0)  # Min dist (no proto = inf)\n            ii = ii.squeeze()\n            classpred[s_idx] = ii.item()  # Allocate class idx\n\n        # Convert to 1-hot\n        out = torch.zeros(ns, self.n_classes).to(x.device)\n        for s_idx in range(ns):\n            out[s_idx, classpred[s_idx]] = 1\n        return out",
  "class L2Normalization(Module):\n    \"\"\"Module to L2-normalize the input. Typically used in last layer to\n    normalize the embedding.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x: Tensor) -> Tensor:\n        return torch.nn.functional.normalize(x, p=2, dim=1)",
  "class PPPloss(object):\n    \"\"\" Pseudo-Prototypical Proxy loss (PPP-loss).\n        This is a contrastive loss using prototypes and representations of the\n        samples in the batch to optimize the embedding space.\n    \"\"\"\n\n    def __init__(self, p_mem: Dict, T=0.1):\n        \"\"\"\n        :param p_mem: dictionary with keys the prototype identifier and\n                      values the prototype tensors.\n        :param T: temperature of the softmax, serving as concentration\n                  density parameter.\n        \"\"\"\n        self.T = T\n        self.p_mem = p_mem\n\n    def __call__(self, x, y):\n        \"\"\"\n        The loss is calculated with one-vs-rest batches Bc and Bk,\n        split into the attractor and repellor loss terms.\n        We iterate over the possible batches while accumulating the losses per\n        class c vs other-classes k.\n        \"\"\"\n        loss = None\n        bs = x.size(0)\n        x = x.view(bs, -1)  # Batch x feature size\n        y_unique = torch.unique(y).squeeze().view(-1)\n        include_repellor = len(y_unique.size()) <= 1  # When at least 2 classes\n\n        # All prototypes\n        p_y = torch.tensor(\n            [c for c in self.p_mem.keys()]).to(x.device).detach()\n        p_x = torch.cat(\n            [self.p_mem[c.item()] for c in p_y]).to(x.device).detach()\n\n        for label_idx in range(y_unique.size(0)):  # Per-class operation\n            c = y_unique[label_idx]\n\n            # Make all-vs-rest batches per class (Bc=attractor, Bk=repellor set)\n            Bc = x.index_select(0, torch.nonzero(y == c).squeeze(dim=1))\n            Bk = x.index_select(0, torch.nonzero(y != c).squeeze(dim=1))\n\n            p_idx = torch.nonzero(p_y == c).squeeze(dim=1)  # Prototypes\n            pc = p_x[p_idx]  # Class proto\n            pk = torch.cat([p_x[:p_idx], p_x[p_idx + 1:]]).clone().detach()\n\n            # Accumulate loss for instances of class c\n            sum_logLc = self.attractor(pc, pk, Bc)\n            sum_logLk = self.repellor(pc, pk, Bc, Bk) if include_repellor else 0\n            Loss_c = -sum_logLc - sum_logLk  # attractor + repellor for class c\n            loss = Loss_c if loss is None else loss + Loss_c  # Update loss\n        return loss / bs  # Make independent batch size\n\n    def attractor(self, pc, pk, Bc):\n        \"\"\"\n        Get the attractor loss terms for all instances in xc.\n        :param pc: Prototype of the same class c.\n        :param pk: Prototoypes of the other classes.\n        :param Bc: Batch of instances of the same class c.\n        :return: Sum_{i, the part of same class c} log P(c|x_i^c)\n        \"\"\"\n        m = torch.cat([Bc.clone(), pc, pk]).detach()  # Incl other-class proto\n        pk_idx = m.shape[0] - pk.shape[0]  # from when starts p_k\n\n        # Resulting distance columns are per-instance loss terms\n        D = torch.mm(m, Bc.t()).div_(self.T).exp_()  # Distance matrix exp terms\n        mask = torch.eye(*D.shape).bool().to(Bc.device)  # Exclude self-product\n        Dm = D.masked_fill(mask, 0)  # Masked out products with self\n\n        Lc_n, Lk_d = Dm[:pk_idx], Dm[pk_idx:].sum(dim=0)  # Num/denominator\n        Pci = Lc_n / (Lc_n + Lk_d)  # Get probabilities per instance\n        E_Pc = Pci.sum(0) / Bc.shape[0]  # Expectation over pseudo-prototypes\n        return E_Pc.log_().sum()  # sum over all instances (sum i)\n\n    def repellor(self, pc, pk, Bc, Bk):\n        \"\"\"\n        Get the repellor loss terms for all pseudo-prototype instances in Bc.\n        :param pc: Actual prototype of the same class c.\n        :param pk: Prototoypes of the other classes (k).\n        :param Bc: Batch of instances of the same class c. Acting as\n        pseudo-prototypes.\n        :param Bk: Batch of instances of other-than-c classes (k).\n        :return: Sum_{i, part of same class c} Sum_{x_j^k} log 1 - P(c|x_j^k)\n        \"\"\"\n        union_ck = torch.cat([Bc.clone(), pc, pk]).detach()\n        pk_idx = union_ck.shape[0] - pk.shape[0]\n\n        # Distance other-class-k to prototypes (pc/pk) and pseudo-prototype (xc)\n        D = torch.mm(union_ck, Bk.t()).div_(self.T).exp_()\n\n        Lk_d = D[pk_idx:].sum(dim=0).unsqueeze(0)  # Numerator/denominator terms\n        Lc_n = D[:pk_idx]\n        Pki = Lc_n / (Lc_n + Lk_d)  # probability\n\n        E_Pk = (Pki[:-1] + Pki[-1].unsqueeze(0)) / 2  # Exp. pseudo/prototype\n        inv_E_Pk = E_Pk.mul_(-1).add_(1).log_()  # log( (1 - Pk))\n        return inv_E_Pk.sum()",
  "def __init__(self, mem_size=200, n_classes=10, p_size=100, alpha=0.99,\n                 T=0.1, max_it_cnt=1):\n        \"\"\"\n        :param mem_size: max number of input samples in the replay memory.\n        :param n_classes: total number of classes that will be encountered. This\n        is used to output predictions for all classes, with zero probability\n        for unseen classes.\n        :param p_size: The prototype size, which equals the feature size of the\n        last layer.\n        :param alpha: The momentum for the exponentially moving average of the\n        prototypes.\n        :param T: The softmax temperature, used as a concentration parameter.\n        :param max_it_cnt: How many processing iterations per batch (experience)\n        \"\"\"\n        super().__init__()\n        self.n_classes = n_classes\n        self.it_cnt = 0\n        self.max_it_cnt = max_it_cnt\n\n        # Operational memory: replay memory\n        self.replay_mem = {}\n        self.mem_size = mem_size  # replay memory size\n        self.storage_policy = ClassBalancedStoragePolicy(\n            ext_mem=self.replay_mem,\n            mem_size=self.mem_size,\n            adaptive_size=True)\n\n        # Operational memory: Prototypical memory\n        self.p_mem = {}  # Scales with nb classes * feature size\n        self.p_size = p_size  # Prototype size determined on runtime\n        self.tmp_p_mem = {}  # Intermediate to process batch for multiple times\n        self.alpha = alpha\n        self.p_init_adaptive = False  # Only create proto when class seen\n\n        # PPP-loss\n        self.T = T\n        self.ppp_loss = PPPloss(self.p_mem, T=self.T)\n\n        self.initialized = False",
  "def before_training(self, strategy, **kwargs):\n        \"\"\" Enforce using the PPP-loss and add a NN-classifier.\"\"\"\n        if not self.initialized:\n            strategy._criterion = self.ppp_loss\n            print(\"Using the Pseudo-Prototypical-Proxy loss for CoPE.\")\n\n            # Normalize representation of last layer\n            swap_last_fc_layer(strategy.model,\n                               torch.nn.Sequential(\n                                   get_last_fc_layer(strategy.model)[1],\n                                   L2Normalization())\n                               )\n\n            # Static prototype init\n            # Create prototypes for all classes at once\n            if not self.p_init_adaptive and len(self.p_mem) == 0:\n                self._init_new_prototypes(\n                    torch.arange(0, self.n_classes).to(strategy.device))\n\n            self.initialized = True",
  "def before_training_exp(self, strategy, num_workers=0, shuffle=True,\n                            **kwargs):\n        \"\"\"\n        Random retrieval from a class-balanced memory.\n        Dataloader builds batches containing examples from both memories and\n        the training dataset.\n        This implementation requires the use of early stopping, otherwise the\n        entire memory will be iterated.\n        \"\"\"\n        if len(self.replay_mem) == 0:\n            return\n        self.it_cnt = 0\n        strategy.dataloader = ReplayDataLoader(\n            strategy.adapted_dataset,\n            AvalancheConcatDataset(self.replay_mem.values()),\n            oversample_small_tasks=False,\n            num_workers=num_workers,\n            batch_size=strategy.train_mb_size * 2,\n            force_data_batch_size=strategy.train_mb_size,\n            shuffle=shuffle)",
  "def after_training_iteration(self, strategy, **kwargs):\n        \"\"\"\n        Implements early stopping, determining how many subsequent times a\n        batch can be used for updates. The dataloader contains only data for\n        the current experience (batch) and the entire memory.\n        Multiple iterations will hence result in the original batch with new\n        exemplars sampled from the memory for each iteration.\n        \"\"\"\n        self.it_cnt += 1\n        if self.it_cnt == self.max_it_cnt:\n            strategy.stop_training()",
  "def after_forward(self, strategy, **kwargs):\n        \"\"\"\n        After the forward we can use the representations to update our running\n        avg of the prototypes. This is in case we do multiple iterations of\n        processing on the same batch.\n\n        New prototypes are initialized for previously unseen classes.\n        \"\"\"\n\n        if self.p_init_adaptive:  # Init prototypes for unseen classes in batch\n            self._init_new_prototypes(strategy.mb_y)\n\n        # Update batch info (when multiple iterations on same batch)\n        self._update_running_prototypes(strategy)",
  "def _init_new_prototypes(self, targets: Tensor):\n        \"\"\"Initialize prototypes for previously unseen classes.\n        :param targets: The targets Tensor to make prototypes for.\n        \"\"\"\n        y_unique = torch.unique(targets).squeeze().view(-1)\n        for idx in range(y_unique.size(0)):\n            c = y_unique[idx].item()\n            if c not in self.p_mem:  # Init new prototype\n                self.p_mem[c] = normalize(\n                    torch.empty((1, self.p_size)).uniform_(-1, 1), p=2,\n                    dim=1).detach().to(targets.device)",
  "def _update_running_prototypes(self, strategy):\n        \"\"\" Accumulate seen outputs of the network and keep counts. \"\"\"\n        y_unique = torch.unique(strategy.mb_y).squeeze().view(-1)\n        for idx in range(y_unique.size(0)):\n            c = y_unique[idx].item()\n            idxs = torch.nonzero(strategy.mb_y == c).squeeze(1)\n            p_tmp_batch = strategy.mb_output[idxs].sum(dim=0).unsqueeze(0).to(\n                strategy.device)\n\n            p_init, cnt_init = self.tmp_p_mem[c] \\\n                if c in self.tmp_p_mem else (0, 0)\n            self.tmp_p_mem[c] = (p_init + p_tmp_batch, cnt_init + len(idxs))",
  "def after_training_exp(self, strategy, **kwargs):\n        \"\"\" After the current experience (batch), update prototypes and\n        store observed samples for replay.\n        \"\"\"\n        self._update_prototypes()  # Update prototypes\n        self.storage_policy(strategy)",
  "def _update_prototypes(self):\n        \"\"\" Update the prototypes based on the running averages. \"\"\"\n        for c, (p_sum, p_cnt) in self.tmp_p_mem.items():\n            incr_p = normalize(p_sum / p_cnt, p=2, dim=1)  # L2 normalized\n            old_p = self.p_mem[c].clone()\n            new_p_momentum = self.alpha * old_p + (\n                    1 - self.alpha) * incr_p  # Momentum update\n            self.p_mem[c] = normalize(new_p_momentum, p=2, dim=1).detach()\n        self.tmp_p_mem = {}",
  "def after_eval_iteration(self, strategy, **kwargs):\n        \"\"\" Convert output scores to probabilities for other metrics like\n        accuracy and forgetting. We only do it at this point because before\n        this,we still need the embedding outputs to obtain the PPP-loss.\"\"\"\n        strategy.mb_output = self._get_nearest_neigbor_distr(strategy.mb_output)",
  "def _get_nearest_neigbor_distr(self, x: Tensor) -> Tensor:\n        \"\"\"\n        Find closest prototype for output samples in batch x.\n        :param x: Batch of network logits.\n        :return: one-hot representation of the predicted class.\n        \"\"\"\n        ns = x.size(0)\n        nd = x.view(ns, -1).shape[-1]\n\n        # Get prototypes\n        seen_c = len(self.p_mem.keys())\n        if seen_c == 0:  # no prototypes yet, output uniform distr. all classes\n            return torch.Tensor(ns, self.n_classes\n                                ).fill_(1.0 / self.n_classes).to(x.device)\n        means = torch.ones(seen_c, nd).to(x.device) * float('inf')\n        for c, c_proto in self.p_mem.items():\n            means[c] = c_proto  # Class idx gets allocated its prototype\n\n        # Predict nearest mean\n        classpred = torch.LongTensor(ns)\n        for s_idx in range(ns):  # Per sample\n            dist = - torch.mm(means, x[s_idx].unsqueeze(-1))  # Dot product\n            _, ii = dist.min(0)  # Min dist (no proto = inf)\n            ii = ii.squeeze()\n            classpred[s_idx] = ii.item()  # Allocate class idx\n\n        # Convert to 1-hot\n        out = torch.zeros(ns, self.n_classes).to(x.device)\n        for s_idx in range(ns):\n            out[s_idx, classpred[s_idx]] = 1\n        return out",
  "def __init__(self):\n        super().__init__()",
  "def forward(self, x: Tensor) -> Tensor:\n        return torch.nn.functional.normalize(x, p=2, dim=1)",
  "def __init__(self, p_mem: Dict, T=0.1):\n        \"\"\"\n        :param p_mem: dictionary with keys the prototype identifier and\n                      values the prototype tensors.\n        :param T: temperature of the softmax, serving as concentration\n                  density parameter.\n        \"\"\"\n        self.T = T\n        self.p_mem = p_mem",
  "def __call__(self, x, y):\n        \"\"\"\n        The loss is calculated with one-vs-rest batches Bc and Bk,\n        split into the attractor and repellor loss terms.\n        We iterate over the possible batches while accumulating the losses per\n        class c vs other-classes k.\n        \"\"\"\n        loss = None\n        bs = x.size(0)\n        x = x.view(bs, -1)  # Batch x feature size\n        y_unique = torch.unique(y).squeeze().view(-1)\n        include_repellor = len(y_unique.size()) <= 1  # When at least 2 classes\n\n        # All prototypes\n        p_y = torch.tensor(\n            [c for c in self.p_mem.keys()]).to(x.device).detach()\n        p_x = torch.cat(\n            [self.p_mem[c.item()] for c in p_y]).to(x.device).detach()\n\n        for label_idx in range(y_unique.size(0)):  # Per-class operation\n            c = y_unique[label_idx]\n\n            # Make all-vs-rest batches per class (Bc=attractor, Bk=repellor set)\n            Bc = x.index_select(0, torch.nonzero(y == c).squeeze(dim=1))\n            Bk = x.index_select(0, torch.nonzero(y != c).squeeze(dim=1))\n\n            p_idx = torch.nonzero(p_y == c).squeeze(dim=1)  # Prototypes\n            pc = p_x[p_idx]  # Class proto\n            pk = torch.cat([p_x[:p_idx], p_x[p_idx + 1:]]).clone().detach()\n\n            # Accumulate loss for instances of class c\n            sum_logLc = self.attractor(pc, pk, Bc)\n            sum_logLk = self.repellor(pc, pk, Bc, Bk) if include_repellor else 0\n            Loss_c = -sum_logLc - sum_logLk  # attractor + repellor for class c\n            loss = Loss_c if loss is None else loss + Loss_c  # Update loss\n        return loss / bs",
  "def attractor(self, pc, pk, Bc):\n        \"\"\"\n        Get the attractor loss terms for all instances in xc.\n        :param pc: Prototype of the same class c.\n        :param pk: Prototoypes of the other classes.\n        :param Bc: Batch of instances of the same class c.\n        :return: Sum_{i, the part of same class c} log P(c|x_i^c)\n        \"\"\"\n        m = torch.cat([Bc.clone(), pc, pk]).detach()  # Incl other-class proto\n        pk_idx = m.shape[0] - pk.shape[0]  # from when starts p_k\n\n        # Resulting distance columns are per-instance loss terms\n        D = torch.mm(m, Bc.t()).div_(self.T).exp_()  # Distance matrix exp terms\n        mask = torch.eye(*D.shape).bool().to(Bc.device)  # Exclude self-product\n        Dm = D.masked_fill(mask, 0)  # Masked out products with self\n\n        Lc_n, Lk_d = Dm[:pk_idx], Dm[pk_idx:].sum(dim=0)  # Num/denominator\n        Pci = Lc_n / (Lc_n + Lk_d)  # Get probabilities per instance\n        E_Pc = Pci.sum(0) / Bc.shape[0]  # Expectation over pseudo-prototypes\n        return E_Pc.log_().sum()",
  "def repellor(self, pc, pk, Bc, Bk):\n        \"\"\"\n        Get the repellor loss terms for all pseudo-prototype instances in Bc.\n        :param pc: Actual prototype of the same class c.\n        :param pk: Prototoypes of the other classes (k).\n        :param Bc: Batch of instances of the same class c. Acting as\n        pseudo-prototypes.\n        :param Bk: Batch of instances of other-than-c classes (k).\n        :return: Sum_{i, part of same class c} Sum_{x_j^k} log 1 - P(c|x_j^k)\n        \"\"\"\n        union_ck = torch.cat([Bc.clone(), pc, pk]).detach()\n        pk_idx = union_ck.shape[0] - pk.shape[0]\n\n        # Distance other-class-k to prototypes (pc/pk) and pseudo-prototype (xc)\n        D = torch.mm(union_ck, Bk.t()).div_(self.T).exp_()\n\n        Lk_d = D[pk_idx:].sum(dim=0).unsqueeze(0)  # Numerator/denominator terms\n        Lc_n = D[:pk_idx]\n        Pki = Lc_n / (Lc_n + Lk_d)  # probability\n\n        E_Pk = (Pki[:-1] + Pki[-1].unsqueeze(0)) / 2  # Exp. pseudo/prototype\n        inv_E_Pk = E_Pk.mul_(-1).add_(1).log_()  # log( (1 - Pk))\n        return inv_E_Pk.sum()",
  "class LwFPlugin(StrategyPlugin):\n    \"\"\"\n    A Learning without Forgetting plugin.\n    LwF uses distillation to regularize the current loss with soft targets\n    taken from a previous version of the model.\n    This plugin does not use task identities.\n    \"\"\"\n\n    def __init__(self, alpha=1, temperature=2):\n        \"\"\"\n        :param alpha: distillation hyperparameter. It can be either a float\n                number or a list containing alpha for each experience.\n        :param temperature: softmax temperature for distillation\n        \"\"\"\n\n        super().__init__()\n\n        self.alpha = alpha\n        self.temperature = temperature\n        self.prev_model = None\n\n    def _distillation_loss(self, out, prev_out):\n        \"\"\"\n        Compute distillation loss between output of the current model and\n        and output of the previous (saved) model.\n        \"\"\"\n\n        log_p = torch.log_softmax(out / self.temperature, dim=1)\n        q = torch.softmax(prev_out / self.temperature, dim=1)\n        res = torch.nn.functional.kl_div(log_p, q, reduction='batchmean')\n        return res\n\n    def penalty(self, out, x, alpha):\n        \"\"\"\n        Compute weighted distillation loss.\n        \"\"\"\n\n        if self.prev_model is None:\n            return 0\n        else:\n            y_prev = self.prev_model(x).detach()\n            dist_loss = self._distillation_loss(out, y_prev)\n            return alpha * dist_loss\n\n    def before_backward(self, strategy, **kwargs):\n        \"\"\"\n        Add distillation loss\n        \"\"\"\n        alpha = self.alpha[strategy.training_exp_counter] \\\n            if isinstance(self.alpha, (list, tuple)) else self.alpha\n        penalty = self.penalty(strategy.mb_output, strategy.mb_x, alpha)\n        strategy.loss += penalty\n\n    def after_training_exp(self, strategy, **kwargs):\n        \"\"\"\n        Save a copy of the model after each experience\n        \"\"\"\n\n        self.prev_model = copy.deepcopy(strategy.model)",
  "def __init__(self, alpha=1, temperature=2):\n        \"\"\"\n        :param alpha: distillation hyperparameter. It can be either a float\n                number or a list containing alpha for each experience.\n        :param temperature: softmax temperature for distillation\n        \"\"\"\n\n        super().__init__()\n\n        self.alpha = alpha\n        self.temperature = temperature\n        self.prev_model = None",
  "def _distillation_loss(self, out, prev_out):\n        \"\"\"\n        Compute distillation loss between output of the current model and\n        and output of the previous (saved) model.\n        \"\"\"\n\n        log_p = torch.log_softmax(out / self.temperature, dim=1)\n        q = torch.softmax(prev_out / self.temperature, dim=1)\n        res = torch.nn.functional.kl_div(log_p, q, reduction='batchmean')\n        return res",
  "def penalty(self, out, x, alpha):\n        \"\"\"\n        Compute weighted distillation loss.\n        \"\"\"\n\n        if self.prev_model is None:\n            return 0\n        else:\n            y_prev = self.prev_model(x).detach()\n            dist_loss = self._distillation_loss(out, y_prev)\n            return alpha * dist_loss",
  "def before_backward(self, strategy, **kwargs):\n        \"\"\"\n        Add distillation loss\n        \"\"\"\n        alpha = self.alpha[strategy.training_exp_counter] \\\n            if isinstance(self.alpha, (list, tuple)) else self.alpha\n        penalty = self.penalty(strategy.mb_output, strategy.mb_x, alpha)\n        strategy.loss += penalty",
  "def after_training_exp(self, strategy, **kwargs):\n        \"\"\"\n        Save a copy of the model after each experience\n        \"\"\"\n\n        self.prev_model = copy.deepcopy(strategy.model)",
  "class GEMPlugin(StrategyPlugin):\n    \"\"\"\n    Gradient Episodic Memory Plugin.\n    GEM projects the gradient on the current minibatch by using an external\n    episodic memory of patterns from previous experiences. The gradient on\n    the current minibatch is projected so that the dot product with all the\n    reference gradients of previous tasks remains positive.\n    This plugin does not use task identities.\n    \"\"\"\n\n    def __init__(self, patterns_per_experience: int, memory_strength: float):\n        \"\"\"\n        :param patterns_per_experience: number of patterns per experience in the\n            memory.\n        :param memory_strength: offset to add to the projection direction\n            in order to favour backward transfer (gamma in original paper).\n        \"\"\"\n\n        super().__init__()\n\n        self.patterns_per_experience = int(patterns_per_experience)\n        self.memory_strength = memory_strength\n\n        self.memory_x, self.memory_y = {}, {}\n\n        self.G = None\n\n    def before_training_iteration(self, strategy, **kwargs):\n        \"\"\"\n        Compute gradient constraints on previous memory samples from all\n        experiences.\n        \"\"\"\n\n        if strategy.training_exp_counter > 0:\n            G = []\n            strategy.model.train()\n            for t in range(strategy.training_exp_counter):\n                strategy.optimizer.zero_grad()\n                xref = self.memory_x[t].to(strategy.device)\n                yref = self.memory_y[t].to(strategy.device)\n                out = strategy.model(xref)\n                loss = strategy._criterion(out, yref)\n                loss.backward()\n\n                G.append(torch.cat([p.grad.flatten()\n                                    for p in strategy.model.parameters()\n                                    if p.grad is not None], dim=0))\n\n            self.G = torch.stack(G)  # (experiences, parameters)\n\n    @torch.no_grad()\n    def after_backward(self, strategy, **kwargs):\n        \"\"\"\n        Project gradient based on reference gradients\n        \"\"\"\n\n        if strategy.training_exp_counter > 0:\n            g = torch.cat([p.grad.flatten()\n                           for p in strategy.model.parameters()\n                           if p.grad is not None], dim=0)\n\n            to_project = (torch.mv(self.G, g) < 0).any()\n        else:\n            to_project = False\n\n        if to_project:\n            v_star = self.solve_quadprog(g).to(strategy.device)\n\n            num_pars = 0  # reshape v_star into the parameter matrices\n            for p in strategy.model.parameters():\n\n                curr_pars = p.numel()\n\n                if p.grad is None:\n                    continue\n\n                p.grad.copy_(\n                    v_star[num_pars:num_pars + curr_pars].view(p.size()))\n                num_pars += curr_pars\n\n            assert num_pars == v_star.numel(), \"Error in projecting gradient\"\n\n    def after_training_exp(self, strategy, **kwargs):\n        \"\"\"\n        Save a copy of the model after each experience\n        \"\"\"\n\n        self.update_memory(strategy.experience.dataset,\n                           strategy.training_exp_counter,\n                           strategy.train_mb_size)\n\n    @torch.no_grad()\n    def update_memory(self, dataset, t, batch_size):\n        \"\"\"\n        Update replay memory with patterns from current experience.\n        \"\"\"\n        dataloader = DataLoader(dataset, batch_size=batch_size)\n        tot = 0\n        for x, y, _ in dataloader:\n            if tot + x.size(0) <= self.patterns_per_experience:\n                if t not in self.memory_x:\n                    self.memory_x[t] = x.clone()\n                    self.memory_y[t] = y.clone()\n                else:\n                    self.memory_x[t] = torch.cat((self.memory_x[t], x), dim=0)\n                    self.memory_y[t] = torch.cat((self.memory_y[t], y), dim=0)\n            else:\n                diff = self.patterns_per_experience - tot\n                if t not in self.memory_x:\n                    self.memory_x[t] = x[:diff].clone()\n                    self.memory_y[t] = y[:diff].clone()\n                else:\n                    self.memory_x[t] = torch.cat((self.memory_x[t], x[:diff]),\n                                                 dim=0)\n                    self.memory_y[t] = torch.cat((self.memory_y[t], y[:diff]),\n                                                 dim=0)\n                break\n            tot += x.size(0)\n\n    def solve_quadprog(self, g):\n        \"\"\"\n        Solve quadratic programming with current gradient g and\n        gradients matrix on previous tasks G.\n        Taken from original code:\n        https://github.com/facebookresearch/GradientEpisodicMemory/blob/master/model/gem.py\n        \"\"\"\n\n        memories_np = self.G.cpu().double().numpy()\n        gradient_np = g.cpu().contiguous().view(-1).double().numpy()\n        t = memories_np.shape[0]\n        P = np.dot(memories_np, memories_np.transpose())\n        P = 0.5 * (P + P.transpose()) + np.eye(t) * 1e-3\n        q = np.dot(memories_np, gradient_np) * -1\n        G = np.eye(t)\n        h = np.zeros(t) + self.memory_strength\n        v = quadprog.solve_qp(P, q, G, h)[0]\n        v_star = np.dot(v, memories_np) + gradient_np\n\n        return torch.from_numpy(v_star).float()",
  "def __init__(self, patterns_per_experience: int, memory_strength: float):\n        \"\"\"\n        :param patterns_per_experience: number of patterns per experience in the\n            memory.\n        :param memory_strength: offset to add to the projection direction\n            in order to favour backward transfer (gamma in original paper).\n        \"\"\"\n\n        super().__init__()\n\n        self.patterns_per_experience = int(patterns_per_experience)\n        self.memory_strength = memory_strength\n\n        self.memory_x, self.memory_y = {}, {}\n\n        self.G = None",
  "def before_training_iteration(self, strategy, **kwargs):\n        \"\"\"\n        Compute gradient constraints on previous memory samples from all\n        experiences.\n        \"\"\"\n\n        if strategy.training_exp_counter > 0:\n            G = []\n            strategy.model.train()\n            for t in range(strategy.training_exp_counter):\n                strategy.optimizer.zero_grad()\n                xref = self.memory_x[t].to(strategy.device)\n                yref = self.memory_y[t].to(strategy.device)\n                out = strategy.model(xref)\n                loss = strategy._criterion(out, yref)\n                loss.backward()\n\n                G.append(torch.cat([p.grad.flatten()\n                                    for p in strategy.model.parameters()\n                                    if p.grad is not None], dim=0))\n\n            self.G = torch.stack(G)",
  "def after_backward(self, strategy, **kwargs):\n        \"\"\"\n        Project gradient based on reference gradients\n        \"\"\"\n\n        if strategy.training_exp_counter > 0:\n            g = torch.cat([p.grad.flatten()\n                           for p in strategy.model.parameters()\n                           if p.grad is not None], dim=0)\n\n            to_project = (torch.mv(self.G, g) < 0).any()\n        else:\n            to_project = False\n\n        if to_project:\n            v_star = self.solve_quadprog(g).to(strategy.device)\n\n            num_pars = 0  # reshape v_star into the parameter matrices\n            for p in strategy.model.parameters():\n\n                curr_pars = p.numel()\n\n                if p.grad is None:\n                    continue\n\n                p.grad.copy_(\n                    v_star[num_pars:num_pars + curr_pars].view(p.size()))\n                num_pars += curr_pars\n\n            assert num_pars == v_star.numel(), \"Error in projecting gradient\"",
  "def after_training_exp(self, strategy, **kwargs):\n        \"\"\"\n        Save a copy of the model after each experience\n        \"\"\"\n\n        self.update_memory(strategy.experience.dataset,\n                           strategy.training_exp_counter,\n                           strategy.train_mb_size)",
  "def update_memory(self, dataset, t, batch_size):\n        \"\"\"\n        Update replay memory with patterns from current experience.\n        \"\"\"\n        dataloader = DataLoader(dataset, batch_size=batch_size)\n        tot = 0\n        for x, y, _ in dataloader:\n            if tot + x.size(0) <= self.patterns_per_experience:\n                if t not in self.memory_x:\n                    self.memory_x[t] = x.clone()\n                    self.memory_y[t] = y.clone()\n                else:\n                    self.memory_x[t] = torch.cat((self.memory_x[t], x), dim=0)\n                    self.memory_y[t] = torch.cat((self.memory_y[t], y), dim=0)\n            else:\n                diff = self.patterns_per_experience - tot\n                if t not in self.memory_x:\n                    self.memory_x[t] = x[:diff].clone()\n                    self.memory_y[t] = y[:diff].clone()\n                else:\n                    self.memory_x[t] = torch.cat((self.memory_x[t], x[:diff]),\n                                                 dim=0)\n                    self.memory_y[t] = torch.cat((self.memory_y[t], y[:diff]),\n                                                 dim=0)\n                break\n            tot += x.size(0)",
  "def solve_quadprog(self, g):\n        \"\"\"\n        Solve quadratic programming with current gradient g and\n        gradients matrix on previous tasks G.\n        Taken from original code:\n        https://github.com/facebookresearch/GradientEpisodicMemory/blob/master/model/gem.py\n        \"\"\"\n\n        memories_np = self.G.cpu().double().numpy()\n        gradient_np = g.cpu().contiguous().view(-1).double().numpy()\n        t = memories_np.shape[0]\n        P = np.dot(memories_np, memories_np.transpose())\n        P = 0.5 * (P + P.transpose()) + np.eye(t) * 1e-3\n        q = np.dot(memories_np, gradient_np) * -1\n        G = np.eye(t)\n        h = np.zeros(t) + self.memory_strength\n        v = quadprog.solve_qp(P, q, G, h)[0]\n        v_star = np.dot(v, memories_np) + gradient_np\n\n        return torch.from_numpy(v_star).float()",
  "class EarlyStoppingPlugin(StrategyPlugin):\n    def __init__(self, patience: int, val_stream_name: str,\n                 metric_name: str = 'Top1_Acc_Stream', mode: str = 'max'):\n        \"\"\"\n        Simple plugin stopping the training when the accuracy on the\n        corresponding validation metric stopped progressing for a few epochs.\n        The state of the best model is saved after each improvement on the\n        given metric and is loaded back into the model before stopping the\n        training procedure.\n\n        :param patience: Number of epochs to wait before stopping the training.\n        :param val_stream_name: Name of the validation stream to search in the\n        metrics. The corresponding stream will be used to keep track of the\n        evolution of the performance of a model.\n        :param metric_name: The name of the metric to watch as it will be\n        reported in the evaluator.\n        :param mode: Must be \"max\" or \"min\". max (resp. min) means that the\n        given metric should me maximized (resp. minimized).\n        \"\"\"\n        super().__init__()\n        self.val_stream_name = val_stream_name\n        self.patience = patience\n        self.metric_name = metric_name\n        self.metric_key = f'{self.metric_name}/eval_phase/' \\\n                          f'{self.val_stream_name}'\n        if mode not in ('max', 'min'):\n            raise ValueError(f'Mode must be \"max\" or \"min\", got {mode}.')\n        self.operator = operator.gt if mode == 'max' else operator.lt\n\n        self.best_state = None  # Contains the best parameters\n        self.best_val = None\n        self.best_epoch = None\n\n    def before_training(self, strategy, **kwargs):\n        self.best_state = None\n        self.best_val = None\n        self.best_epoch = None\n\n    def before_training_epoch(self, strategy, **kwargs):\n        self._update_best(strategy)\n        if strategy.epoch - self.best_epoch >= self.patience:\n            strategy.model.load_state_dict(self.best_state)\n            strategy.stop_training()\n\n    def _update_best(self, strategy):\n        res = strategy.evaluator.get_last_metrics()\n        val_acc = res.get(self.metric_key)\n        if self.best_val is None or self.operator(val_acc, self.best_val):\n            self.best_state = deepcopy(strategy.model.state_dict())\n            self.best_val = val_acc\n            self.best_epoch = strategy.epoch",
  "def __init__(self, patience: int, val_stream_name: str,\n                 metric_name: str = 'Top1_Acc_Stream', mode: str = 'max'):\n        \"\"\"\n        Simple plugin stopping the training when the accuracy on the\n        corresponding validation metric stopped progressing for a few epochs.\n        The state of the best model is saved after each improvement on the\n        given metric and is loaded back into the model before stopping the\n        training procedure.\n\n        :param patience: Number of epochs to wait before stopping the training.\n        :param val_stream_name: Name of the validation stream to search in the\n        metrics. The corresponding stream will be used to keep track of the\n        evolution of the performance of a model.\n        :param metric_name: The name of the metric to watch as it will be\n        reported in the evaluator.\n        :param mode: Must be \"max\" or \"min\". max (resp. min) means that the\n        given metric should me maximized (resp. minimized).\n        \"\"\"\n        super().__init__()\n        self.val_stream_name = val_stream_name\n        self.patience = patience\n        self.metric_name = metric_name\n        self.metric_key = f'{self.metric_name}/eval_phase/' \\\n                          f'{self.val_stream_name}'\n        if mode not in ('max', 'min'):\n            raise ValueError(f'Mode must be \"max\" or \"min\", got {mode}.')\n        self.operator = operator.gt if mode == 'max' else operator.lt\n\n        self.best_state = None  # Contains the best parameters\n        self.best_val = None\n        self.best_epoch = None",
  "def before_training(self, strategy, **kwargs):\n        self.best_state = None\n        self.best_val = None\n        self.best_epoch = None",
  "def before_training_epoch(self, strategy, **kwargs):\n        self._update_best(strategy)\n        if strategy.epoch - self.best_epoch >= self.patience:\n            strategy.model.load_state_dict(self.best_state)\n            strategy.stop_training()",
  "def _update_best(self, strategy):\n        res = strategy.evaluator.get_last_metrics()\n        val_acc = res.get(self.metric_key)\n        if self.best_val is None or self.operator(val_acc, self.best_val):\n            self.best_state = deepcopy(strategy.model.state_dict())\n            self.best_val = val_acc\n            self.best_epoch = strategy.epoch",
  "class EvaluationPlugin(StrategyPlugin):\n    \"\"\"\n    An evaluation plugin that obtains relevant data from the\n    training and eval loops of the strategy through callbacks.\n    The plugin keeps a dictionary with the last recorded value for each metric.\n    The dictionary will be returned by the `train` and `eval` methods of the\n    strategies.\n    It is also possible to keep a dictionary with all recorded metrics by\n    specifying `collect_all=True`. The dictionary can be retrieved via\n    the `get_all_metrics` method.\n\n    This plugin also logs metrics using the provided loggers.\n    \"\"\"\n\n    def __init__(self,\n                 *metrics: Union['PluginMetric', Sequence['PluginMetric']],\n                 loggers: Union['StrategyLogger',\n                                Sequence['StrategyLogger']] = None,\n                 collect_all=True,\n                 benchmark=None,\n                 strict_checks=False,\n                 suppress_warnings=False):\n        \"\"\"\n        Creates an instance of the evaluation plugin.\n\n        :param metrics: The metrics to compute.\n        :param loggers: The loggers to be used to log the metric values.\n        :param collect_all: if True, collect in a separate dictionary all\n            metric curves values. This dictionary is accessible with\n            `get_all_metrics` method.\n        :param benchmark: continual learning benchmark needed to check stream\n            completeness during evaluation or other kind of properties. If\n            None, no check will be conducted and the plugin will emit a\n            warning to signal this fact.\n        :param strict_checks: if True, `benchmark` has to be provided.\n            In this case, only full evaluation streams are admitted when\n            calling `eval`. An error will be raised otherwise. When False,\n            `benchmark` can be `None` and only warnings will be raised.\n        :param suppress_warnings: if True, warnings and errors will never be\n            raised from the plugin.\n            If False, warnings and errors will be raised following\n            `benchmark` and `strict_checks` behavior.\n        \"\"\"\n        super().__init__()\n        self.collect_all = collect_all\n        self.benchmark = benchmark\n        self.strict_checks = strict_checks\n        self.suppress_warnings = suppress_warnings\n        flat_metrics_list = []\n        for metric in metrics:\n            if isinstance(metric, Sequence):\n                flat_metrics_list += list(metric)\n            else:\n                flat_metrics_list.append(metric)\n        self.metrics = flat_metrics_list\n\n        if loggers is None:\n            loggers = []\n        elif not isinstance(loggers, Sequence):\n            loggers = [loggers]\n\n        if benchmark is None:\n            if not suppress_warnings:\n                if strict_checks:\n                    raise ValueError(\"Benchmark cannot be None \"\n                                     \"in strict mode.\")\n                else:\n                    warnings.warn(\n                        \"No benchmark provided to the evaluation plugin. \"\n                        \"Metrics may be computed on inconsistent portion \"\n                        \"of streams, use at your own risk.\")\n        else:\n            self.complete_test_stream = benchmark.test_stream\n\n        self.loggers: Sequence['StrategyLogger'] = loggers\n\n        if len(self.loggers) == 0:\n            warnings.warn('No loggers specified, metrics will not be logged')\n\n        if self.collect_all:\n            # for each curve collect all emitted values.\n            # dictionary key is full metric name.\n            # Dictionary value is a tuple of two lists.\n            # first list gathers x values (indices representing\n            # time steps at which the corresponding metric value\n            # has been emitted)\n            # second list gathers metric values\n            self.all_metric_results = defaultdict(lambda: ([], []))\n        # Dictionary of last values emitted. Dictionary key\n        # is the full metric name, while dictionary value is\n        # metric value.\n        self.last_metric_results = {}\n\n        self._active = True\n        \"\"\"If True, no metrics will be collected.\"\"\"\n\n    @property\n    def active(self):\n        return self._active\n\n    @active.setter\n    def active(self, value):\n        assert value is True or value is False, \\\n            \"Active must be set as either True or False\"\n        self._active = value\n\n    def _update_metrics(self, strategy: 'BaseStrategy', callback: str):\n        if not self._active:\n            return []\n\n        metric_values = []\n        for metric in self.metrics:\n            metric_result = getattr(metric, callback)(strategy)\n            if isinstance(metric_result, Sequence):\n                metric_values += list(metric_result)\n            elif metric_result is not None:\n                metric_values.append(metric_result)\n\n        for metric_value in metric_values:\n            name = metric_value.name\n            x = metric_value.x_plot\n            val = metric_value.value\n            if self.collect_all:\n                self.all_metric_results[name][0].append(x)\n                self.all_metric_results[name][1].append(val)\n\n            self.last_metric_results[name] = val\n\n        for logger in self.loggers:\n            getattr(logger, callback)(strategy, metric_values)\n        return metric_values\n\n    def get_last_metrics(self):\n        \"\"\"\n        Return a shallow copy of dictionary with metric names\n        as keys and last metrics value as values.\n\n        :return: a dictionary with full metric\n            names as keys and last metric value as value.\n        \"\"\"\n        return copy(self.last_metric_results)\n\n    def get_all_metrics(self):\n        \"\"\"\n        Return the dictionary of all collected metrics.\n        This method should be called only when `collect_all` is set to True.\n\n        :return: if `collect_all` is True, returns a dictionary\n            with full metric names as keys and a tuple of two lists\n            as value. The first list gathers x values (indices\n            representing time steps at which the corresponding\n            metric value has been emitted). The second list\n            gathers metric values. a dictionary. If `collect_all`\n            is False return an empty dictionary\n        \"\"\"\n        if self.collect_all:\n            return self.all_metric_results\n        else:\n            return {}\n\n    def reset_last_metrics(self):\n        \"\"\"\n        Set the dictionary storing last value for each metric to be\n        empty dict.\n        \"\"\"\n        self.last_metric_results = {}\n\n    def before_training(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'before_training')\n\n    def before_training_exp(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'before_training_exp')\n\n    def before_train_dataset_adaptation(self, strategy: 'BaseStrategy',\n                                        **kwargs):\n        self._update_metrics(strategy, 'before_train_dataset_adaptation')\n\n    def after_train_dataset_adaptation(self, strategy: 'BaseStrategy',\n                                       **kwargs):\n        self._update_metrics(strategy, 'after_train_dataset_adaptation')\n\n    def before_training_epoch(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'before_training_epoch')\n\n    def before_training_iteration(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'before_training_iteration')\n\n    def before_forward(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'before_forward')\n\n    def after_forward(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'after_forward')\n\n    def before_backward(self, strategy: 'BaseStrategy', **kwargs):\n        self.update_metrics = self._update_metrics(strategy, 'before_backward')\n\n    def after_backward(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'after_backward')\n\n    def after_training_iteration(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'after_training_iteration')\n\n    def before_update(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'before_update')\n\n    def after_update(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'after_update')\n\n    def after_training_epoch(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'after_training_epoch')\n\n    def after_training_exp(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'after_training_exp')\n\n    def after_training(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'after_training')\n\n    def before_eval(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'before_eval')\n        msgw = \"Evaluation stream is not equal to the complete test stream. \" \\\n               \"This may result in inconsistent metrics. Use at your own risk.\"\n        msge = \"Stream provided to `eval` must be the same of the entire \" \\\n               \"evaluation stream.\"\n        if self.benchmark is not None:\n            for i, exp in enumerate(self.complete_test_stream):\n                try:\n                    current_exp = strategy.current_eval_stream[i]\n                    if exp.current_experience != current_exp.current_experience:\n                        if not self.suppress_warnings:\n                            if self.strict_checks:\n                                raise ValueError(msge)\n                            else:\n                                warnings.warn(msgw)\n                except IndexError:\n                    if self.strict_checks:\n                        raise ValueError(msge)\n                    else:\n                        warnings.warn(msgw)\n\n    def before_eval_dataset_adaptation(self, strategy: 'BaseStrategy',\n                                       **kwargs):\n        self._update_metrics(strategy, 'before_eval_dataset_adaptation')\n\n    def after_eval_dataset_adaptation(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'after_eval_dataset_adaptation')\n\n    def before_eval_exp(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'before_eval_exp')\n\n    def after_eval_exp(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'after_eval_exp')\n\n    def after_eval(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'after_eval')\n\n    def before_eval_iteration(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'before_eval_iteration')\n\n    def before_eval_forward(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'before_eval_forward')\n\n    def after_eval_forward(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'after_eval_forward')\n\n    def after_eval_iteration(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'after_eval_iteration')",
  "def __init__(self,\n                 *metrics: Union['PluginMetric', Sequence['PluginMetric']],\n                 loggers: Union['StrategyLogger',\n                                Sequence['StrategyLogger']] = None,\n                 collect_all=True,\n                 benchmark=None,\n                 strict_checks=False,\n                 suppress_warnings=False):\n        \"\"\"\n        Creates an instance of the evaluation plugin.\n\n        :param metrics: The metrics to compute.\n        :param loggers: The loggers to be used to log the metric values.\n        :param collect_all: if True, collect in a separate dictionary all\n            metric curves values. This dictionary is accessible with\n            `get_all_metrics` method.\n        :param benchmark: continual learning benchmark needed to check stream\n            completeness during evaluation or other kind of properties. If\n            None, no check will be conducted and the plugin will emit a\n            warning to signal this fact.\n        :param strict_checks: if True, `benchmark` has to be provided.\n            In this case, only full evaluation streams are admitted when\n            calling `eval`. An error will be raised otherwise. When False,\n            `benchmark` can be `None` and only warnings will be raised.\n        :param suppress_warnings: if True, warnings and errors will never be\n            raised from the plugin.\n            If False, warnings and errors will be raised following\n            `benchmark` and `strict_checks` behavior.\n        \"\"\"\n        super().__init__()\n        self.collect_all = collect_all\n        self.benchmark = benchmark\n        self.strict_checks = strict_checks\n        self.suppress_warnings = suppress_warnings\n        flat_metrics_list = []\n        for metric in metrics:\n            if isinstance(metric, Sequence):\n                flat_metrics_list += list(metric)\n            else:\n                flat_metrics_list.append(metric)\n        self.metrics = flat_metrics_list\n\n        if loggers is None:\n            loggers = []\n        elif not isinstance(loggers, Sequence):\n            loggers = [loggers]\n\n        if benchmark is None:\n            if not suppress_warnings:\n                if strict_checks:\n                    raise ValueError(\"Benchmark cannot be None \"\n                                     \"in strict mode.\")\n                else:\n                    warnings.warn(\n                        \"No benchmark provided to the evaluation plugin. \"\n                        \"Metrics may be computed on inconsistent portion \"\n                        \"of streams, use at your own risk.\")\n        else:\n            self.complete_test_stream = benchmark.test_stream\n\n        self.loggers: Sequence['StrategyLogger'] = loggers\n\n        if len(self.loggers) == 0:\n            warnings.warn('No loggers specified, metrics will not be logged')\n\n        if self.collect_all:\n            # for each curve collect all emitted values.\n            # dictionary key is full metric name.\n            # Dictionary value is a tuple of two lists.\n            # first list gathers x values (indices representing\n            # time steps at which the corresponding metric value\n            # has been emitted)\n            # second list gathers metric values\n            self.all_metric_results = defaultdict(lambda: ([], []))\n        # Dictionary of last values emitted. Dictionary key\n        # is the full metric name, while dictionary value is\n        # metric value.\n        self.last_metric_results = {}\n\n        self._active = True\n        \"\"\"If True, no metrics will be collected.\"\"\"",
  "def active(self):\n        return self._active",
  "def active(self, value):\n        assert value is True or value is False, \\\n            \"Active must be set as either True or False\"\n        self._active = value",
  "def _update_metrics(self, strategy: 'BaseStrategy', callback: str):\n        if not self._active:\n            return []\n\n        metric_values = []\n        for metric in self.metrics:\n            metric_result = getattr(metric, callback)(strategy)\n            if isinstance(metric_result, Sequence):\n                metric_values += list(metric_result)\n            elif metric_result is not None:\n                metric_values.append(metric_result)\n\n        for metric_value in metric_values:\n            name = metric_value.name\n            x = metric_value.x_plot\n            val = metric_value.value\n            if self.collect_all:\n                self.all_metric_results[name][0].append(x)\n                self.all_metric_results[name][1].append(val)\n\n            self.last_metric_results[name] = val\n\n        for logger in self.loggers:\n            getattr(logger, callback)(strategy, metric_values)\n        return metric_values",
  "def get_last_metrics(self):\n        \"\"\"\n        Return a shallow copy of dictionary with metric names\n        as keys and last metrics value as values.\n\n        :return: a dictionary with full metric\n            names as keys and last metric value as value.\n        \"\"\"\n        return copy(self.last_metric_results)",
  "def get_all_metrics(self):\n        \"\"\"\n        Return the dictionary of all collected metrics.\n        This method should be called only when `collect_all` is set to True.\n\n        :return: if `collect_all` is True, returns a dictionary\n            with full metric names as keys and a tuple of two lists\n            as value. The first list gathers x values (indices\n            representing time steps at which the corresponding\n            metric value has been emitted). The second list\n            gathers metric values. a dictionary. If `collect_all`\n            is False return an empty dictionary\n        \"\"\"\n        if self.collect_all:\n            return self.all_metric_results\n        else:\n            return {}",
  "def reset_last_metrics(self):\n        \"\"\"\n        Set the dictionary storing last value for each metric to be\n        empty dict.\n        \"\"\"\n        self.last_metric_results = {}",
  "def before_training(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'before_training')",
  "def before_training_exp(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'before_training_exp')",
  "def before_train_dataset_adaptation(self, strategy: 'BaseStrategy',\n                                        **kwargs):\n        self._update_metrics(strategy, 'before_train_dataset_adaptation')",
  "def after_train_dataset_adaptation(self, strategy: 'BaseStrategy',\n                                       **kwargs):\n        self._update_metrics(strategy, 'after_train_dataset_adaptation')",
  "def before_training_epoch(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'before_training_epoch')",
  "def before_training_iteration(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'before_training_iteration')",
  "def before_forward(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'before_forward')",
  "def after_forward(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'after_forward')",
  "def before_backward(self, strategy: 'BaseStrategy', **kwargs):\n        self.update_metrics = self._update_metrics(strategy, 'before_backward')",
  "def after_backward(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'after_backward')",
  "def after_training_iteration(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'after_training_iteration')",
  "def before_update(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'before_update')",
  "def after_update(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'after_update')",
  "def after_training_epoch(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'after_training_epoch')",
  "def after_training_exp(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'after_training_exp')",
  "def after_training(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'after_training')",
  "def before_eval(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'before_eval')\n        msgw = \"Evaluation stream is not equal to the complete test stream. \" \\\n               \"This may result in inconsistent metrics. Use at your own risk.\"\n        msge = \"Stream provided to `eval` must be the same of the entire \" \\\n               \"evaluation stream.\"\n        if self.benchmark is not None:\n            for i, exp in enumerate(self.complete_test_stream):\n                try:\n                    current_exp = strategy.current_eval_stream[i]\n                    if exp.current_experience != current_exp.current_experience:\n                        if not self.suppress_warnings:\n                            if self.strict_checks:\n                                raise ValueError(msge)\n                            else:\n                                warnings.warn(msgw)\n                except IndexError:\n                    if self.strict_checks:\n                        raise ValueError(msge)\n                    else:\n                        warnings.warn(msgw)",
  "def before_eval_dataset_adaptation(self, strategy: 'BaseStrategy',\n                                       **kwargs):\n        self._update_metrics(strategy, 'before_eval_dataset_adaptation')",
  "def after_eval_dataset_adaptation(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'after_eval_dataset_adaptation')",
  "def before_eval_exp(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'before_eval_exp')",
  "def after_eval_exp(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'after_eval_exp')",
  "def after_eval(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'after_eval')",
  "def before_eval_iteration(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'before_eval_iteration')",
  "def before_eval_forward(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'before_eval_forward')",
  "def after_eval_forward(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'after_eval_forward')",
  "def after_eval_iteration(self, strategy: 'BaseStrategy', **kwargs):\n        self._update_metrics(strategy, 'after_eval_iteration')",
  "class StrategyPlugin(StrategyCallbacks[Any]):\n    \"\"\"\n    Base class for strategy plugins. Implements all the callbacks required\n    by the BaseStrategy with an empty function. Subclasses must override\n    the callbacks.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        pass\n\n    def before_training(self, strategy: 'BaseStrategy', **kwargs):\n        pass\n\n    def before_training_exp(self, strategy: 'BaseStrategy', **kwargs):\n        pass\n\n    def before_train_dataset_adaptation(self, strategy: 'BaseStrategy',\n                                        **kwargs):\n        pass\n\n    def after_train_dataset_adaptation(self, strategy: 'BaseStrategy',\n                                       **kwargs):\n        pass\n\n    def before_training_epoch(self, strategy: 'BaseStrategy', **kwargs):\n        pass\n\n    def before_training_iteration(self, strategy: 'BaseStrategy', **kwargs):\n        pass\n\n    def before_forward(self, strategy: 'BaseStrategy', **kwargs):\n        pass\n\n    def after_forward(self, strategy: 'BaseStrategy', **kwargs):\n        pass\n\n    def before_backward(self, strategy: 'BaseStrategy', **kwargs):\n        pass\n\n    def after_backward(self, strategy: 'BaseStrategy', **kwargs):\n        pass\n\n    def after_training_iteration(self, strategy: 'BaseStrategy', **kwargs):\n        pass\n\n    def before_update(self, strategy: 'BaseStrategy', **kwargs):\n        pass\n\n    def after_update(self, strategy: 'BaseStrategy', **kwargs):\n        pass\n\n    def after_training_epoch(self, strategy: 'BaseStrategy', **kwargs):\n        pass\n\n    def after_training_exp(self, strategy: 'BaseStrategy', **kwargs):\n        pass\n\n    def after_training(self, strategy: 'BaseStrategy', **kwargs):\n        pass\n\n    def before_eval(self, strategy: 'BaseStrategy', **kwargs):\n        pass\n\n    def before_eval_dataset_adaptation(self, strategy: 'BaseStrategy',\n                                       **kwargs):\n        pass\n\n    def after_eval_dataset_adaptation(self, strategy: 'BaseStrategy', **kwargs):\n        pass\n\n    def before_eval_exp(self, strategy: 'BaseStrategy', **kwargs):\n        pass\n\n    def after_eval_exp(self, strategy: 'BaseStrategy', **kwargs):\n        pass\n\n    def after_eval(self, strategy: 'BaseStrategy', **kwargs):\n        pass\n\n    def before_eval_iteration(self, strategy: 'BaseStrategy', **kwargs):\n        pass\n\n    def before_eval_forward(self, strategy: 'BaseStrategy', **kwargs):\n        pass\n\n    def after_eval_forward(self, strategy: 'BaseStrategy', **kwargs):\n        pass\n\n    def after_eval_iteration(self, strategy: 'BaseStrategy', **kwargs):\n        pass",
  "def __init__(self):\n        super().__init__()\n        pass",
  "def before_training(self, strategy: 'BaseStrategy', **kwargs):\n        pass",
  "def before_training_exp(self, strategy: 'BaseStrategy', **kwargs):\n        pass",
  "def before_train_dataset_adaptation(self, strategy: 'BaseStrategy',\n                                        **kwargs):\n        pass",
  "def after_train_dataset_adaptation(self, strategy: 'BaseStrategy',\n                                       **kwargs):\n        pass",
  "def before_training_epoch(self, strategy: 'BaseStrategy', **kwargs):\n        pass",
  "def before_training_iteration(self, strategy: 'BaseStrategy', **kwargs):\n        pass",
  "def before_forward(self, strategy: 'BaseStrategy', **kwargs):\n        pass",
  "def after_forward(self, strategy: 'BaseStrategy', **kwargs):\n        pass",
  "def before_backward(self, strategy: 'BaseStrategy', **kwargs):\n        pass",
  "def after_backward(self, strategy: 'BaseStrategy', **kwargs):\n        pass",
  "def after_training_iteration(self, strategy: 'BaseStrategy', **kwargs):\n        pass",
  "def before_update(self, strategy: 'BaseStrategy', **kwargs):\n        pass",
  "def after_update(self, strategy: 'BaseStrategy', **kwargs):\n        pass",
  "def after_training_epoch(self, strategy: 'BaseStrategy', **kwargs):\n        pass",
  "def after_training_exp(self, strategy: 'BaseStrategy', **kwargs):\n        pass",
  "def after_training(self, strategy: 'BaseStrategy', **kwargs):\n        pass",
  "def before_eval(self, strategy: 'BaseStrategy', **kwargs):\n        pass",
  "def before_eval_dataset_adaptation(self, strategy: 'BaseStrategy',\n                                       **kwargs):\n        pass",
  "def after_eval_dataset_adaptation(self, strategy: 'BaseStrategy', **kwargs):\n        pass",
  "def before_eval_exp(self, strategy: 'BaseStrategy', **kwargs):\n        pass",
  "def after_eval_exp(self, strategy: 'BaseStrategy', **kwargs):\n        pass",
  "def after_eval(self, strategy: 'BaseStrategy', **kwargs):\n        pass",
  "def before_eval_iteration(self, strategy: 'BaseStrategy', **kwargs):\n        pass",
  "def before_eval_forward(self, strategy: 'BaseStrategy', **kwargs):\n        pass",
  "def after_eval_forward(self, strategy: 'BaseStrategy', **kwargs):\n        pass",
  "def after_eval_iteration(self, strategy: 'BaseStrategy', **kwargs):\n        pass"
]