[
  "def read(fname):\n        return open(os.path.join(os.path.dirname(__file__), fname)).read()",
  "class build_ext(_build_ext):\n            def finalize_options(self):\n                _build_ext.finalize_options(self)\n                # Prevent numpy from thinking it is still in its setup process:\n                __builtins__.__NUMPY_SETUP__ = False\n                import numpy\n                self.include_dirs.append(numpy.get_include())",
  "def finalize_options(self):\n                _build_ext.finalize_options(self)\n                # Prevent numpy from thinking it is still in its setup process:\n                __builtins__.__NUMPY_SETUP__ = False\n                import numpy\n                self.include_dirs.append(numpy.get_include())",
  "def coarsen(g, method, **method_kwargs):\n    \"\"\" Create a coarse grid from a given grid. If a grid bucket is passed the\n    procedure is applied to the higher in dimension.\n    Note: the grid is modified in place.\n    Note: do not call compute_geometry afterward.\n    Parameters:\n        g: the grid or grid bucket\n        method: string which define the method to coarse. Current options:\n            'by_volume' (the coarsening is based on the cell volumes) or 'by_tpfa'\n            (using the algebraic multigrid method coarse/fine-splittings based on\n            direct couplings)\n        method_kwargs: the arguments for each method\n\n    \"\"\"\n\n    if method.lower() == 'by_volume':\n        partition = create_aggregations(g, **method_kwargs)\n\n    elif method.lower() == 'by_tpfa':\n        seeds = np.empty(0, dtype=np.int)\n        if method_kwargs.get('if_seeds', False):\n            seeds = generate_seeds(g)\n        matrix = tpfa_matrix(g)\n        partition = create_partition(matrix, seeds=seeds, **method_kwargs)\n\n    else:\n        raise ValueError(\"Undefined coarsening algorithm\")\n\n    generate_coarse_grid(g, partition)",
  "def generate_coarse_grid(g, subdiv):\n    \"\"\" Generate a coarse grid clustering the cells according to the flags\n    given by subdiv. Subdiv should be long as the number of cells in the\n    original grid, it contains integers (possibly not continuous) which\n    represent the cells in the final mesh. If a grid bucket is given the\n    coarsening is applied to the higher dimensional grid.\n\n    The values computed in \"compute_geometry\" are not preserved and they should\n    be computed out from this function.\n\n    Note: there is no check for disconnected cells in the final grid.\n\n    Parameters:\n        g: the grid or grid bucket\n        subdiv: a list of flags, one for each cell of the original grid\n\n    Return:\n        grid: if a grid is given as input, its coarser version is returned.\n        If a grid bucket is given as input, the grid is updated in place.\n\n    How to use:\n    subdiv = np.array([0,0,1,1,1,1,3,4,6,4,6,4])\n    g = generate_coarse_grid(g, subdiv)\n\n    or with a grid bucket:\n    subdiv = np.array([0,0,1,1,1,1,3,4,6,4,6,4])\n    generate_coarse_grid(gb, subdiv)\n\n    \"\"\"\n    if isinstance(g, grid.Grid):\n        generate_coarse_grid_single(g, subdiv, False)\n\n    if isinstance(g, grid_bucket.GridBucket):\n        generate_coarse_grid_gb(g, subdiv)",
  "def reorder_partition(subdiv):\n    \"\"\"\n    Re-order the partition id in case to obtain contiguous numbers.\n    Parameters:\n        subdiv: array where for each cell one id\n    Return:\n        the subdivision written in a contiguous way\n    \"\"\"\n    if isinstance(subdiv, dict):\n        for _, partition in subdiv.items():\n            old_ids = np.unique(partition)\n            for new_id, old_id in enumerate(old_ids):\n                partition[partition == old_id] = new_id\n    else:\n        old_ids = np.unique(subdiv)\n        for new_id, old_id in enumerate(old_ids):\n            subdiv[subdiv == old_id] = new_id\n\n    return subdiv",
  "def generate_coarse_grid_single(g, subdiv, face_map):\n    \"\"\"\n    Specific function for a single grid. Use the common interface instead.\n    \"\"\"\n\n    subdiv = np.asarray(subdiv)\n    assert subdiv.size == g.num_cells\n\n    # declare the storage array to build the cell_faces map\n    cell_faces = np.empty(0, dtype=g.cell_faces.indptr.dtype)\n    cells = np.empty(0, dtype=cell_faces.dtype)\n    orient = np.empty(0, dtype=g.cell_faces.data.dtype)\n\n    # declare the storage array to build the face_nodes map\n    face_nodes = np.empty(0, dtype=g.face_nodes.indptr.dtype)\n    nodes = np.empty(0, dtype=face_nodes.dtype)\n    visit = np.zeros(g.num_faces, dtype=np.bool)\n\n    # compute the face_node indexes\n    num_nodes_per_face = g.face_nodes.indptr[1:] - g.face_nodes.indptr[:-1]\n    face_node_ind = matrix_compression.rldecode(np.arange(g.num_faces), \\\n                                                num_nodes_per_face)\n\n    cells_list = np.unique(subdiv)\n    cell_volumes = np.zeros(cells_list.size)\n    cell_centers = np.zeros((3, cells_list.size))\n\n    for cellId, cell in enumerate(cells_list):\n        # extract the cells of the original mesh associated to a specific label\n        cells_old = np.where(subdiv == cell)[0]\n\n        # compute the volume\n        cell_volumes[cellId] = np.sum(g.cell_volumes[cells_old])\n        cell_centers[:, cellId] = np.average(g.cell_centers[:, cells_old], axis=1)\n\n        # reconstruct the cell_faces mapping\n        faces_old, _, orient_old = sps.find(g.cell_faces[:, cells_old])\n        mask = np.ones(faces_old.size, dtype=np.bool)\n        mask[np.unique(faces_old, return_index=True )[1]] = False\n        # extract the indexes of the internal edges, to be discared\n        index = np.array([ np.where( faces_old == f )[0] \\\n                                for f in faces_old[mask]], dtype=np.int).ravel()\n        faces_new = np.delete(faces_old, index)\n        cell_faces = np.r_[cell_faces, faces_new]\n        cells = np.r_[cells, np.repeat(cellId, faces_new.shape[0])]\n        orient = np.r_[orient, np.delete(orient_old, index)]\n\n        # reconstruct the face_nodes mapping\n        # consider only the unvisited faces\n        not_visit = ~visit[faces_new]\n        if not_visit.size == 0 or np.all(~not_visit):\n            continue\n        # mask to consider only the external faces\n        mask = np.atleast_1d(np.sum([face_node_ind == f \\\n                                     for f in faces_new[not_visit]], \\\n                                    axis=0, dtype=np.bool))\n        face_nodes = np.r_[face_nodes, face_node_ind[mask]]\n\n        nodes_new = g.face_nodes.indices[mask]\n        nodes = np.r_[nodes, nodes_new]\n        visit[faces_new] = True\n\n    # Rename the faces\n    cell_faces_unique = np.unique(cell_faces)\n    cell_faces_id = np.arange(cell_faces_unique.size, dtype=cell_faces.dtype)\n    cell_faces = np.array([cell_faces_id[np.where( cell_faces_unique == f )[0]]\\\n                                                   for f in cell_faces]).ravel()\n    shape = (cell_faces_unique.size, cells_list.size)\n    cell_faces =  sps.csc_matrix((orient, (cell_faces, cells)), shape = shape)\n\n    # Rename the nodes\n    face_nodes = np.array([cell_faces_id[np.where( cell_faces_unique == f )[0]]\\\n                                                   for f in face_nodes]).ravel()\n    nodes_list = np.unique(nodes)\n    nodes_id = np.arange(nodes_list.size, dtype=nodes.dtype)\n    nodes = np.array([nodes_id[np.where( nodes_list == n )[0]] \\\n                                                        for n in nodes]).ravel()\n\n    # sort the nodes\n    nodes = nodes[np.argsort(face_nodes, kind='mergesort')]\n    data = np.ones(nodes.size, dtype=g.face_nodes.data.dtype)\n    indptr = np.r_[0, np.cumsum(np.bincount(face_nodes))]\n    face_nodes =  sps.csc_matrix((data, nodes, indptr))\n\n    # store again the data in the same grid\n    g.name.append(\"coarse\")\n\n    g.nodes = g.nodes[:, nodes_list]\n    g.num_nodes = g.nodes.shape[1]\n\n    g.face_nodes = face_nodes\n    g.num_faces = g.face_nodes.shape[1]\n    g.face_areas = g.face_areas[cell_faces_unique]\n    g.tags = tags.extract(g.tags, cell_faces_unique, tags.standard_face_tags())\n    g.face_normals = g.face_normals[:, cell_faces_unique]\n    g.face_centers = g.face_centers[:, cell_faces_unique]\n\n    g.cell_faces = cell_faces\n    g.num_cells = g.cell_faces.shape[1]\n    g.cell_volumes = cell_volumes\n    g.cell_centers = half_space.star_shape_cell_centers(g)\n    is_nan = np.isnan(g.cell_centers[0, :])\n    g.cell_centers[:, is_nan] = cell_centers[:, is_nan]\n\n    if face_map:\n        return np.array([cell_faces_unique, cell_faces_id])",
  "def generate_coarse_grid_gb(gb, subdiv):\n    \"\"\"\n    Specific function for a grid bucket. Use the common interface instead.\n    \"\"\"\n\n    if not isinstance(subdiv, dict):\n        g = gb.get_grids(lambda g: g.dim==gb.dim_max())[0]\n        subdiv = {g: subdiv}\n\n    for g, partition in subdiv.items():\n\n        # Construct the coarse grids\n        face_map = generate_coarse_grid_single(g, partition, True)\n\n        # Update all the face_cells for all the 'edges' connected to the grid\n        for e, d in gb.edges_props_of_node(g):\n            # The indices that need to be mapped to the new grid\n            face_cells = d['face_cells'].tocsr()\n            indices = face_cells.indices\n            # Map indices\n            mask = np.argsort(indices)\n            indices = np.in1d(face_map[0, :], indices[mask]).nonzero()[0]\n            # Reverse the ordering\n            face_cells.indices = indices[np.argsort(mask)]\n            d['face_cells'] = face_cells.tocsc()",
  "def tpfa_matrix(g, perm=None):\n    \"\"\"\n    Compute a two-point flux approximation matrix useful related to a call of\n    create_partition.\n\n    Parameters\n    ----------\n    g: the grid\n    perm: (optional) permeability, the it is not given unitary tensor is assumed\n\n    Returns\n    -------\n    out: sparse matrix\n        Two-point flux approximation matrix\n\n    \"\"\"\n    if isinstance(g, grid_bucket.GridBucket):\n       g = g.get_grids(lambda g_: g_.dim == g.dim_max())[0]\n\n    if perm is None:\n        perm = tensor.SecondOrder(g.dim,np.ones(g.num_cells))\n\n    solver = tpfa.Tpfa()\n    param = Parameters(g)\n    param.set_tensor(solver, perm)\n    param.set_bc(solver, BoundaryCondition(g, np.empty(0), ''))\n    return solver.matrix_rhs(g, {'param': param})[0]",
  "def generate_seeds(gb):\n    \"\"\"\n    Giving the higher dimensional grid in a grid bucket, generate the seed for\n    the tip of lower\n    \"\"\"\n    seeds = np.empty(0, dtype=np.int)\n\n    if isinstance(gb, grid.Grid):\n        return seeds\n\n    # Extract the higher dimensional grid\n    g_h = gb.get_grids(lambda g: g.dim == gb.dim_max())[0]\n    g_h_faces, g_h_cells, _ = sps.find(g_h.cell_faces)\n\n    # Extract the 1-codimensional grids\n    gs = gb.get_grids(lambda g: g.dim == gb.dim_max()-1)\n\n    for g in gs:\n        tips = np.where(g.tags['tip_faces'])[0]\n        faces, cells, _ = sps.find(g.cell_faces)\n        index = np.in1d(faces, tips).nonzero()[0]\n        cells = np.unique(cells[index])\n\n        face_cells = gb.graph.edge[g][g_h]['face_cells']\n        interf_cells, interf_faces, _ = sps.find(face_cells)\n        index = np.in1d(interf_cells, cells).nonzero()[0]\n\n        index = np.in1d(g_h_faces, interf_faces[index]).nonzero()[0]\n        seeds = np.concatenate((seeds, g_h_cells[index]))\n\n    return seeds",
  "def create_aggregations(g, **kwargs):\n    \"\"\" Create a cell partition based on their volumes.\n\n    Parameter:\n        g: grid or grid bucket\n\n    Return:\n        partition: partition of the cells for the coarsening algorithm\n\n    \"\"\"\n\n    # Extract the higher dimensional grids\n    if isinstance(g, grid_bucket.GridBucket):\n        g = g.get_grids(lambda g_: g_.dim == g.dim_max())\n\n    g_list = np.atleast_1d(g)\n    partition = dict()\n\n    for g in g_list:\n        partition_local = -np.ones(g.num_cells, dtype=np.int)\n\n        volumes = g.cell_volumes.copy()\n        volumes_checked = volumes.copy()\n        c2c = g.cell_connection_map()\n\n        # Compute the inverse of the harminc mean\n        weight = kwargs.get('weight', 1.)\n        mean = weight/stats.hmean(1./volumes)\n\n        new_id = 1\n        while np.any(partition_local < 0):\n            # Consider the smallest element to be aggregated\n            cell_id = np.argmin(volumes_checked)\n\n            # If the smallest fulfil the condition, stop the loop\n            if volumes[cell_id] > mean:\n                break\n\n            do_it = True\n            old_cluster = np.array([cell_id])\n            while do_it:\n                cluster = __get_neigh(old_cluster, c2c, partition_local)\n                volume = np.sum(volumes[cluster])\n                if volume > mean or np.array_equal(old_cluster, cluster):\n                    do_it = False\n                else:\n                    old_cluster = cluster\n\n            # If one of the element in the cluster has already a partition id,\n            # we uniform the ids\n            partition_cluster = partition_local[cluster]\n            has_coarse_id = partition_cluster > 0\n            if np.any(has_coarse_id):\n                # For each coarse id in the cluster, rename the coarse ids in the\n                # partition_local\n                for partition_id in partition_cluster[has_coarse_id]:\n                    which_partition_id = partition_local == partition_id\n                    partition_local[which_partition_id] = new_id\n                    volumes[which_partition_id] = volume\n                    volumes_checked[which_partition_id] = volume\n\n            # Update the data for the cluster\n            partition_local[cluster] = new_id\n            volumes[cluster] = volume\n            new_id += 1\n\n            volumes_checked[cluster] = np.inf\n\n        volumes_checked = volumes.copy()\n        which_cell = volumes_checked < mean\n        volumes_checked[np.logical_not(which_cell)] = np.inf\n\n        while np.any(which_cell):\n            cell_id = np.argmin(volumes_checked)\n            part_cell = partition_local[cell_id]\n            # Extract the neighbors of the current cell\n            loc = slice(c2c.indptr[cell_id], c2c.indptr[cell_id + 1])\n            neighbors = np.setdiff1d(c2c.indices[loc], np.asarray(cell_id))\n            part_neighbors = partition_local[neighbors]\n            neighbors = neighbors[part_neighbors != part_cell]\n            if neighbors.size == 0:\n                volumes_checked = np.inf\n                which_cell = volumes_checked < mean\n                continue\n            smallest = np.argmin(volumes[neighbors])\n            mask = partition_local == part_cell\n            partition_local[mask] = partition_local[neighbors[smallest]]\n            volumes[mask] = volumes[smallest] + volumes[cell_id]\n            volumes_checked[mask] = volumes[smallest] + volumes[cell_id]\n            which_cell = volumes_checked < mean\n\n        # Fill up the cells which are left\n        has_not_coarse_id = partition_local < 0\n        partition_local[has_not_coarse_id] = new_id + \\\n                                            np.arange(np.sum(has_not_coarse_id))\n\n        partition[g] = partition_local\n\n    return partition",
  "def __get_neigh(cells_id, c2c, partition):\n    \"\"\" Support function for create_aggregations\n    \"\"\"\n    neighbors = np.empty(0, dtype=np.int)\n\n    for cell_id in np.atleast_1d(cells_id):\n        # Extract the neighbors of the current cell\n        loc = slice(c2c.indptr[cell_id], c2c.indptr[cell_id + 1])\n        neighbors = np.hstack((neighbors, c2c.indices[loc]))\n\n    neighbors = np.unique(neighbors)\n    partition_neighbors = partition[neighbors]\n\n    # Check if some neighbor has already a coarse id\n    return np.sort(neighbors[partition_neighbors < 0])",
  "def create_partition(A, seeds=None, **kwargs):\n    \"\"\"\n    Create the partition based on an input matrix using the algebraic multigrid\n    method coarse/fine-splittings based on direct couplings. The standard values\n    for cdepth and epsilon are taken from the following reference.\n\n    For more information see: U. Trottenberg, C. W. Oosterlee, and A. Schuller.\n    Multigrid. Academic press, 2000.\n\n    Parameters\n    ----------\n    A: sparse matrix used for the agglomeration\n    cdepth: the greather is the more intense the aggregation will be, e.g. less\n        cells if it is used combined with generate_coarse_grid\n    epsilon: weight for the off-diagonal entries to define the \"strong\n        negatively cupling\"\n    seeds: (optional) to define a-priori coarse cells\n\n    Returns\n    -------\n    out: agglomeration indices\n\n    How to use\n    ----------\n    part = create_partition(tpfa_matrix(g))\n    g = generate_coarse_grid(g, part)\n\n    \"\"\"\n\n    cdepth = int(kwargs.get('cdepth', 2))\n    epsilon = kwargs.get('epsilon', 0.25)\n\n    if A.size == 0:\n        return np.zeros(1)\n    Nc = A.shape[0]\n\n    # For each node, which other nodes are strongly connected to it\n    ST = sps.lil_matrix((Nc,Nc),dtype=np.bool)\n\n    # In the first instance, all cells are strongly connected to each other\n    At = A.T\n\n    for i in np.arange(Nc):\n        loc = slice(At.indptr[i], At.indptr[i+1])\n        ci, vals = At.indices[loc], At.data[loc]\n        neg = vals < 0.\n        nvals = vals[neg]\n        nci = ci[neg]\n        minId = np.argmin(nvals)\n        ind = -nvals >= epsilon * np.abs(nvals[minId])\n        ST[nci[ind], i] = True\n\n    # Temporary field, will store connections of depth 1\n    for _ in np.arange(2, cdepth+1):\n        STold = ST.copy()\n        for j in np.arange(Nc):\n            rowj = np.array(STold.rows[j])\n            if rowj.size == 0:\n                continue\n            row = np.hstack([STold.rows[r] for r in rowj])\n            ST[j, np.concatenate((rowj, row))] = True\n\n    del STold\n\n    ST.setdiag(False)\n    lmbda = np.array([len(s) for s in ST.rows])\n\n    # Define coarse nodes\n    candidate = np.ones(Nc, dtype=np.bool)\n    is_fine = np.zeros(Nc, dtype=np.bool)\n    is_coarse = np.zeros(Nc, dtype=np.bool)\n\n    # cells that are not important for any other cells are on the fine scale.\n    for row_id, row in enumerate(ST.rows):\n        if not row:\n            is_fine[row_id] = True\n            candidate[row_id] = False\n\n    ST = ST.tocsr()\n    it = 0\n    while np.any(candidate):\n        i = np.argmax(lmbda)\n        is_coarse[i] = True\n        j = ST.indices[ST.indptr[i]:ST.indptr[i+1]]\n        jf = j[candidate[j]]\n        is_fine[jf] = True\n        candidate[np.r_[i, jf]] = False\n        loop = ST.indices[ mcolon.mcolon(ST.indptr[jf], ST.indptr[jf+1]) ]\n        for row in np.unique(loop):\n            s = ST.indices[ST.indptr[row]:ST.indptr[row+1]]\n            lmbda[row] = s[candidate[s]].size + 2*s[is_fine[s]].size\n        lmbda[np.logical_not(candidate)]= -1\n        it = it + 1\n\n        # Something went wrong during aggregation\n        assert it <= Nc\n\n    del lmbda, ST\n\n    if seeds is not None:\n        is_coarse[seeds] = True\n        is_fine[seeds] = False\n\n    # If two neighbors are coarse, eliminate one of them without touching the\n    # seeds\n    c2c = np.abs(A) > 0\n    c2c_rows, _, _ = sps.find(c2c)\n\n    pairs = np.empty((0,2), dtype=np.int)\n    for idx, it in enumerate(np.where(is_coarse)[0]):\n        loc = slice(c2c.indptr[it], c2c.indptr[it+1])\n        ind = np.setdiff1d(c2c_rows[loc], it)\n        cind = ind[is_coarse[ind]]\n        new_pair = np.stack((np.repeat(it, cind.size), cind), axis=-1)\n        pairs = np.append(pairs, new_pair, axis=0)\n\n    # Remove one of the neighbors cells\n    if pairs.size:\n        pairs = setmembership.unique_rows(np.sort(pairs, axis=1))[0]\n        for ij in pairs:\n            A_val = np.array(A[ij, ij]).ravel()\n            ids = ij[np.argsort(A_val)]\n            ids = np.setdiff1d(ids, seeds, assume_unique=True)\n            if ids.size:\n                is_coarse[ids[0]] = False\n                is_fine[ids[0]] = True\n\n    coarse = np.where(is_coarse)[0]\n\n    # Primal grid\n    NC = coarse.size\n    primal = sps.lil_matrix((NC,Nc),dtype=np.bool)\n    primal[np.arange(NC), coarse[np.arange(NC)]] = True\n\n    connection = sps.lil_matrix((Nc,Nc),dtype=np.double)\n    for it in np.arange(Nc):\n        n = np.setdiff1d(c2c_rows[c2c.indptr[it]:c2c.indptr[it+1]], it)\n        loc = slice(A.indptr[it], A.indptr[it+1])\n        A_idx, A_row = A.indices[loc], A.data[loc]\n        mask = A_idx != it\n        connection[it, n] = np.abs( A_row[mask] / A_row[np.logical_not(mask)] )\n\n    connection = connection.tocsr()\n\n    candidates_rep = np.ediff1d(connection.indptr)\n    candidates_idx = np.repeat(is_coarse, candidates_rep)\n    candidates = np.stack((connection.indices[candidates_idx],\n                           np.repeat(np.arange(NC), candidates_rep[is_coarse])),\n                           axis=-1)\n\n    connection_idx = mcolon.mcolon(connection.indptr[coarse],\n                                   connection.indptr[coarse+1])\n    vals = sps.csr_matrix(accumarray.accum(candidates,\n                                           connection.data[connection_idx],\n                                           size=[Nc,NC]))\n    del candidates_rep, candidates_idx, connection_idx\n\n    it = NC\n    not_found = np.logical_not(is_coarse)\n    # Process the strongest connection globally\n    while np.any(not_found):\n\n        np.argmax(vals.data)\n        vals.argmax(axis=0)\n        mcind = np.atleast_1d(np.squeeze(np.asarray(vals.argmax(axis=0))))\n        mcval = -np.inf*np.ones(mcind.size)\n        for c, r in enumerate(mcind):\n            loc = slice(vals.indptr[r], vals.indptr[r+1])\n            vals_idx, vals_data = vals.indices[loc], vals.data[loc]\n            mask = vals_idx == c\n            if vals_idx.size == 0 or not np.any(mask):\n                continue\n            mcval[c] = vals_data[mask]\n\n        mi = np.argmax(mcval)\n        nadd = mcind[mi]\n\n        primal[mi, nadd] = True\n        it = it + 1\n        if it > Nc + 5:\n            break\n\n        not_found[nadd] = False\n        vals.data[vals.indptr[nadd]:vals.indptr[nadd+1]] = 0\n\n        loc = slice(connection.indptr[nadd], connection.indptr[nadd+1])\n        nc = connection.indices[loc]\n        af = not_found[nc]\n        nc = nc[af]\n        nv = mcval[mi] * connection[nadd, :]\n        nv = nv.data[af]\n        if len(nc) > 0:\n            vals += sps.csr_matrix((nv,(nc, np.repeat(mi,len(nc)))),\n                                          shape=(Nc,NC))\n\n    coarse, fine = primal.tocsr().nonzero()\n    return coarse[np.argsort(fine)]",
  "class PointGrid(Grid):\n\n#------------------------------------------------------------------------------#\n\n    def __init__(self, pt, name=None):\n        \"\"\"\n        Constructor for 0D grid\n\n        Parameters\n            pt (np.array): Point which represent the grid\n            name (str): Name of grid, passed to super constructor\n        \"\"\"\n\n        name = 'PointGrid' if name is None else name\n\n        face_nodes = sps.identity(1, np.int, \"csr\")\n        cell_faces = sps.identity(1, np.int, \"csr\")\n        pt = np.asarray(pt).reshape((3,1))\n\n        super(PointGrid, self).__init__(0, pt, face_nodes, cell_faces, name)",
  "def __init__(self, pt, name=None):\n        \"\"\"\n        Constructor for 0D grid\n\n        Parameters\n            pt (np.array): Point which represent the grid\n            name (str): Name of grid, passed to super constructor\n        \"\"\"\n\n        name = 'PointGrid' if name is None else name\n\n        face_nodes = sps.identity(1, np.int, \"csr\")\n        cell_faces = sps.identity(1, np.int, \"csr\")\n        pt = np.asarray(pt).reshape((3,1))\n\n        super(PointGrid, self).__init__(0, pt, face_nodes, cell_faces, name)",
  "class GridBucket(object):\n    \"\"\"\n    Container for the hiererchy of grids formed by fractures and their\n    intersection.\n\n    The information is stored in a graph borrowed from networkx, and the\n    GridBucket is to a large degree a wrapper around the graph. Each grid\n    defines a node in the graph, while edges are defined by grid pairs that\n    have a connection.\n\n    To all nodes and vertexes, there is associated a dictionary (courtesy\n    networkx) that can store any type of data. Thus the GridBucket can double\n    as a data storage and management tool.\n\n    Attributes:\n        graph (networkx.Graph): The of the grid. See above for further\n            description.\n\n    \"\"\"\n\n    def __init__(self):\n        self.graph = networkx.Graph(directed=False)\n        self.name = \"grid bucket\"\n\n#------------------------------------------------------------------------------#\n\n    def __iter__(self):\n        \"\"\"\n        Iterator over the grid bucket.\n\n        Yields:\n            core.grid.nodes: The grid associated with a node.\n            data: The dictionary storing all information in this node.\n\n        \"\"\"\n        for g in self.graph:\n            data = self.graph.node[g]\n            yield g, data\n\n#------------------------------------------------------------------------------#\n\n    def size(self):\n        \"\"\"\n        Returns:\n            int: Number of nodes in the grid.\n\n        \"\"\"\n        return self.graph.number_of_nodes()\n\n#------------------------------------------------------------------------------#\n\n    def dim_max(self):\n        \"\"\"\n        Returns:\n            int: Maximum dimension of the grids present in the hierarchy.\n\n        \"\"\"\n        return np.amax([g.dim for g, _ in self])\n\n#------------------------------------------------------------------------------#\n\n    def dim_min(self):\n        \"\"\"\n        Returns:\n            int: Minimum dimension of the grids present in the hierarchy.\n\n        \"\"\"\n        return np.amin([g.dim for g, _ in self])\n\n#------------------------------------------------------------------------------#\n\n    def nodes(self):\n        \"\"\"\n        Yields:\n            An iterator over the graph nodes.\n\n        \"\"\"\n        return self.graph.nodes_iter()\n\n#------------------------------------------------------------------------------#\n\n    def edges(self):\n        \"\"\"\n        Yields:\n            An iterator over the graph edges.\n\n        \"\"\"\n        return self.graph.edges_iter()\n\n#------------------------------------------------------------------------------#\n\n    def edges_props_of_node(self, n):\n        \"\"\"\n        Iterator over the edges of the specific node.\n\n        Parameters:\n            n: A node in the graph.\n\n        Yields:\n            core.grid.edges: The edge (pair of grids) associated with an edge.\n            data: The dictionary storing all information in this edge.\n\n        \"\"\"\n        for e in self.graph.edges([n]):\n            yield e, self.edge_props(e)\n\n#------------------------------------------------------------------------------#\n\n    def sorted_nodes_of_edge(self, e):\n        \"\"\"\n        Obtain the vertices of an edge, in ascending order with respect their\n        dimension. If the edge is between grids of the same dimension, the node\n        ordering (as defined by assign_node_ordering()) is used. If no ordering\n        of nodes exists, assign_node_ordering() will be called by this method.\n\n        Parameters:\n            e: An edge in the graph.\n\n        Returns:\n            dictionary: The first vertex of the edge.\n            dictionary: The second vertex of the edge.\n\n        \"\"\"\n\n        if e[0].dim == e[1].dim:\n            if not self.has_nodes_prop(e, 'node_number'):\n                self.assign_node_ordering()\n\n            node_indexes = self.nodes_prop(e, 'node_number')\n            if node_indexes[0] < node_indexes[1]:\n                return e[0], e[1]\n            return e[1], e[0]\n\n        elif e[0].dim < e[1].dim:\n            return e[0], e[1]\n        else:\n            return e[1], e[0]\n\n#------------------------------------------------------------------------------#\n\n    def sort_multiple_nodes(self, nodes):\n        \"\"\"\n        Sort all the nodes according to node number.\n\n        Parameters:\n            nodes: List of graph nodes.\n\n        Returns:\n            sorted_nodes: The same nodes, sorted.\n\n        \"\"\"\n        assert self.has_nodes_prop(nodes,'node_number')\n\n        return sorted(nodes, key = lambda n: self.node_prop( n, 'node_number'))\n\n#------------------------------------------------------------------------------#\n\n    def nodes_of_edge(self, e):\n        \"\"\"\n        Obtain the vertices of an edge.\n\n        Parameters:\n            e: An edge in the graph.\n\n        Returns:\n            dictionary: The first vertex of the edge.\n            dictionary: The second vertex of the edge.\n\n        \"\"\"\n        return e[0], e[1]\n\n#------------------------------------------------------------------------------#\n\n    def get_grids(self, cond):\n        \"\"\"\n        Obtain the grids subject to a condition.\n\n        Parameters:\n            cond: Predicate to select a grid.\n\n        Returns:\n            grids: np.array of the grids.\n\n        \"\"\"\n        return np.array([g for g, _ in self if cond(g)])\n\n#------------------------------------------------------------------------------#\n\n    def update_nodes(self, new, old):\n        \"\"\"\n        Update the grids givin the old and new values. The edges are updated\n        accordingly.\n\n        Parameters:\n            new: List of the new grids, it can be a single element\n            old: List of the old grids, it can be a single element\n        \"\"\"\n        new = np.atleast_1d(new)\n        old = np.atleast_1d(old)\n        assert new.size == old.size\n\n        networkx.relabel_nodes(self.graph, dict(zip(new, old)), False)\n\n#------------------------------------------------------------------------------#\n\n    def node_neighbors(self, node, only_higher=False, only_lower=False):\n        \"\"\"\n        Parameters:\n            node: node in the graph, grid\n            only_higher: consider only the higher dimensional neighbors\n            only_lower: consider only the lower dimensional neighbors\n        Return:\n            list of networkx.node: Neighbors of node 'node'\n\n        \"\"\"\n        neigh = np.array(self.graph.neighbors(node))\n\n        if not only_higher and not only_lower:\n            return neigh\n        elif only_higher and only_lower:\n            return np.empty(0)\n        elif only_higher:\n            # Find the neighbours that are higher dimensional\n            is_high = np.array([w.dim > node.dim for w in neigh])\n            return neigh[is_high]\n        else:\n            # Find the neighbours that are higher dimensional\n            is_low = np.array([w.dim < node.dim for w in neigh])\n            return neigh[is_low]\n\n#------------------------------------------------------------------------------#\n\n    def add_node_prop(self, key, g=None, prop=None):\n        \"\"\"\n        Add a new property to existing nodes in the graph.\n\n        Properties can be added either to all nodes, or to selected nodes as\n        specified by their grid. In the former case, all nodes will be assigned\n        the property, with None as the default option.\n\n        No tests are done on whether the key already exist, values are simply\n        overwritten.\n\n        Parameters:\n            prop (optional, defaults to None): Property to be added.\n            key (object): Key to the property to be handled.\n            g (list of core.grids.grid, optional): Nodes to be assigned values.\n                Defaults to None, in which case all nodes are assigned the same\n                value.\n            prop (list, or single object, optional): Value to be assigned.\n                Should be either a list with the same length as g, or a single\n                item which is assigned to all nodes. Defaults to None.\n\n        Raises:\n            ValueError if the key is 'node_number', this is reserved for other\n                purposes. See self.assign_node_ordering() for details.\n\n        \"\"\"\n\n        # Check that the key is not 'node_number' - this is reserved\n        if key == 'node_number':\n            raise ValueError('Node number is a reserved key, stay away')\n\n        # Do some checks of parameters first\n        if g is None:\n            assert prop is None or not isinstance(prop, list)\n        else:\n            assert len(g) == len(prop)\n\n        # Set default value.\n        networkx.set_node_attributes(self.graph, key, None)\n\n        if prop is not None:\n            if g is None:\n                for _, n in self:\n                    n[key] = prop\n            else:\n                for v, p in zip(g, prop):\n                    self.graph.node[v][key] = p\n\n#------------------------------------------------------------------------------#\n\n    def add_node_props(self, keys):\n        \"\"\"\n        Add new properties to existing nodes in the graph.\n\n        Properties are be added to all nodes.\n\n        No tests are done on whether the key already exist, values are simply\n        overwritten.\n\n        Parameters:\n            keys (object): Keys to the properties to be handled.\n\n        Raises:\n            ValueError if the key is 'node_number', this is reserved for other\n                purposes. See self.assign_node_ordering() for details.\n\n        \"\"\"\n        [self.add_node_prop(key) for key in np.atleast_1d(keys)]\n\n#------------------------------------------------------------------------------#\n\n    def add_edge_prop(self, key, grid_pairs=None, prop=None):\n        \"\"\"\n        Associate a property with an edge.\n\n        Properties can be added either to all edges, or to selected edges as\n        specified by their grid pair. In the former case, all edges will be\n        assigned the property, with None as the default option.\n\n        No tests are done on whether the key already exist, values are simply\n        overwritten.\n\n        Parameters:\n            prop (optional, defaults to None): Property to be added.\n            key (object): Key to the property to be handled.\n            g (list of list of core.grids.grid, optional): Grid pairs defining\n                the edges to be assigned. values. Defaults to None, in which\n                case all edges are assigned the same value.\n            prop (list, or single object, optional): Value to be assigned.\n                Should be either a list with the same length as grid_pairs, or\n                a single item which is assigned to all nodes. Defaults to\n                None.\n\n        Raises:\n            KeyError if a grid pair is not an existing edge in the grid.\n\n        \"\"\"\n\n        # Do some checks of parameters first\n        if grid_pairs is None:\n            assert prop is None or not isinstance(prop, list)\n        else:\n            assert len(grid_pairs) == len(prop)\n\n        # networkx.set_edge_attributes(self.graph, key, None)\n\n        if prop is not None:\n            for gp, p in zip(grid_pairs, prop):\n                if tuple(gp) in self.graph.edges():\n                    self.graph.edge[gp[0]][gp[1]][key] = p\n                elif tuple(gp[::-1]) in self.graph.edges():\n                    self.graph.edge[gp[1]][gp[0]][key] = p\n                else:\n                    raise KeyError('Cannot assign property to undefined\\\n                                     edge')\n\n#------------------------------------------------------------------------------#\n\n    def add_edge_props(self, keys):\n        \"\"\"\n        Add new propertiy to existing edges in the graph.\n\n        Properties can be added either to all edges.\n\n        No tests are done on whether the key already exist, values are simply\n        overwritten.\n\n        Parameters:\n            keys (object): Keys to the properties to be handled.\n\n        Raises:\n            KeyError if a grid pair is not an existing edge in the grid.\n\n        \"\"\"\n        [self.add_edge_prop(key) for key in np.atleast_1d(keys)]\n\n#------------------------------------------------------------------------------#\n\n    def node_prop(self, g, key):\n        \"\"\"\n        Getter for a node property of the bucket.\n\n        Parameters:\n            grid (core.grids.grid): The grid associated with the node.\n            key (object): Key for the property to be retrieved.\n\n        Returns:\n            object: The property.\n        \"\"\"\n        return self.graph.node[g][key]\n\n#------------------------------------------------------------------------------#\n\n    def has_nodes_prop(self, gs, key):\n        \"\"\"\n        Test if a key exists for a node property of the bucket, for several nodes.\n        Note: the property may contain None but the outcome of the test is\n        still true.\n\n        Parameters:\n            grids (core.grids.grid): The grids associated with the nodes.\n            key (object): Key for the property to be tested.\n\n        Returns:\n            object: The tested property.\n\n        \"\"\"\n        return tuple([key in self.graph.node[g] for g in gs])\n\n\n#------------------------------------------------------------------------------#\n\n    def nodes_prop(self, gs, key):\n        \"\"\"\n        Getter for a node property of the bucket, for several nodes.\n\n        Parameters:\n            grids (core.grids.grid): The grids associated with the nodes.\n            key (object): Key for the property to be retrieved.\n\n        Returns:\n            object: The property.\n\n        \"\"\"\n        return tuple([self.graph.node[g][key] for g in gs])\n\n#------------------------------------------------------------------------------#\n\n    def node_props(self, g):\n        \"\"\"\n        Getter for a node property of the bucket.\n\n        Parameters:\n            grid (core.grids.grid): The grid associated with the node.\n\n        Returns:\n            object: A dictionary with keys and properties.\n\n        \"\"\"\n        return self.graph.node[g]\n\n#------------------------------------------------------------------------------#\n\n    def node_props_of_keys(self, g, keys):\n        \"\"\"\n        Getter for a node property of the bucket.\n\n        Parameters:\n            grid (core.grids.grid): The grid associated with the node.\n            keys (object): Key for the property to be retrieved.\n\n        Returns:\n            object: A dictionary with key and property.\n\n        \"\"\"\n        return {key: self.graph.node[g][key] for key in keys}\n\n#------------------------------------------------------------------------------#\n\n    def edge_prop(self, grid_pairs, key):\n        \"\"\"\n        Getter for an edge property of the bucket.\n\n        Parameters:\n            grid_pairs (list of core.grids.grid): The two grids making up the\n                edge.\n            key (object): Key for the property to be retrieved.\n\n        Returns:\n            object: The property.\n\n        Raises:\n            KeyError if the two grids do not form an edge.\n\n        \"\"\"\n        prop_list = []\n        for gp in np.atleast_2d(grid_pairs):\n            if tuple(gp) in self.graph.edges():\n                prop_list.append(self.graph.edge[gp[0]][gp[1]][key])\n            elif tuple(gp[::-1]) in self.graph.edges():\n                prop_list.append(self.graph.edge[gp[1]][gp[0]][key])\n            else:\n                raise KeyError('Unknown edge')\n        return np.array(prop_list)\n\n#------------------------------------------------------------------------------#\n\n    def edge_props(self, gp):\n        \"\"\"\n        Getter for an edge properties of the bucket.\n\n        Parameters:\n            grids_pair (list of core.grids.grid): The two grids making up the\n                edge.\n            key (object): Keys for the properties to be retrieved.\n\n        Returns:\n            object: The properties.\n\n        Raises:\n            KeyError if the two grids do not form an edge.\n\n        \"\"\"\n        if tuple(gp) in self.graph.edges():\n            return self.graph.edge[gp[0]][gp[1]]\n        elif tuple(gp[::-1]) in self.graph.edges():\n            return self.graph.edge[gp[1]][gp[0]]\n        else:\n            raise KeyError('Unknown edge')\n\n#------------------------------------------------------------------------------#\n\n    def edges_props(self):\n        \"\"\"\n        Iterator over the edges of the grid bucket.\n\n        Yields:\n            core.grid.edges: The edge (pair of grids) associated with an edge.\n            data: The dictionary storing all information in this edge.\n\n        \"\"\"\n        for e in self.graph.edges():\n            yield e, self.edge_props(e)\n\n#------------------------------------------------------------------------------#\n\n    def add_nodes(self, new_grids):\n        \"\"\"\n        Add grids to the hierarchy.\n\n        The grids are added to self.grids.\n\n        Parameters:\n            grids (iterable, list?): The grids to be added. None of these\n                should have been added previously.\n\n        Returns:\n            np.ndarray, dtype object: Numpy array of vertexes, that are\n                identified with the grid hierarchy. Same order as input grid.\n\n        \"\"\"\n        new_grids = np.atleast_1d(new_grids)\n        assert not np.any([i is j for i in new_grids for j in self.graph])\n        [self.graph.add_node(g) for g in new_grids]\n\n#------------------------------------------------------------------------------#\n\n    def remove_node(self, node):\n        \"\"\"\n        Remove node, and related edges, from the grid bucket.\n        Parameters:\n           node : the node to be removed\n\n        \"\"\"\n\n        self.graph.remove_node(node)\n\n#------------------------------------------------------------------------------#\n\n    def remove_nodes(self, cond):\n        \"\"\"\n        Remove nodes, and related edges, from the grid bucket subject to a\n        conditions. The latter takes as input a grid.\n\n        Parameters:\n            cond: predicate to select the grids to remove.\n\n        \"\"\"\n\n        self.graph.remove_nodes_from([g for g in self.graph if cond(g)])\n\n#------------------------------------------------------------------------------#\n\n    def add_edge(self, grids, face_cells):\n        \"\"\"\n        Add an edge in the graph.\n\n        If the grids have different dimensions, the coupling will be added with\n        the higher-dimensional grid as the first node. For equal dimensions,\n        the ordering of the nodes is the same as in input grids.\n\n        Parameters:\n            grids (list, len==2). Grids to be connected. Order is arbitrary.\n            face_cells (object): Identity mapping between cells in the\n                higher-dimensional grid and faces in the lower-dimensional\n                grid. No assumptions are made on the type of the object at this\n                stage. In the grids[0].dim = grids[1].dim case, the mapping is\n                from faces of the first grid to faces of the second one.\n\n        Raises:\n            ValueError if the two grids are not one dimension apart\n\n        \"\"\"\n        assert np.asarray(grids).size == 2\n        # Check that the connection does not already exist\n        assert not (tuple(grids) in self.graph.edges()\n                    or tuple(grids[::-1]) in self.graph.edges())\n\n        # The higher-dimensional grid is the first node of the edge.\n        if grids[0].dim - 1 == grids[1].dim:\n            self.graph.add_edge(*grids, face_cells=face_cells)\n        elif grids[0].dim == grids[1].dim - 1:\n            self.graph.add_edge(*grids[::-1], face_cells=face_cells)\n        elif grids[0].dim == grids[1].dim:\n            self.graph.add_edge(*grids, face_cells=face_cells)\n        else:\n            raise ValueError('Grid dimension mismatch')\n\n#------------------------------------------------------------------------------#\n\n    def grids_of_dimension(self, dim):\n        \"\"\"\n        Get all grids in the bucket of a specific dimension.\n\n        Returns:\n            list: Of grids of the specified dimension\n\n        \"\"\"\n        return [g for g in self.graph.nodes() if g.dim == dim]\n\n#------------------------------------------------------------------------------#\n\n    def assign_node_ordering(self, overwrite_existing=True):\n        \"\"\"\n        Assign an ordering of the nodes in the graph, stored as the attribute\n        'node_number'.\n\n        The intended use is to define the block structure of a discretization\n        on the grid hierarchy.\n\n        The ordering starts with grids of highest dimension. The ordering\n        within each dimension is determined by an iterator over the graph, and\n        can in principle change between two calls to this function.\n\n        If an ordering covering all nodes in the graph already exist, but does\n        not coincide with the new ordering, a warning is issued. If the optional\n        parameter overwrite_existing is set to False, no update is performed if\n        an node ordering already exists.\n        \"\"\"\n\n        # Check whether 'node_number' is defined for the grids already.\n        ordering_exists = True\n        for _, n in self:\n            if not 'node_number' in n.keys():\n                ordering_exists = False\n        if ordering_exists and not overwrite_existing:\n            return\n\n        counter = 0\n        # Loop over grids in decreasing dimensions\n        for dim in range(self.dim_max(), self.dim_min() - 1, -1):\n            for g in self.grids_of_dimension(dim):\n                n = self.graph.node[g]\n                # Get old value, issue warning if not equal to the new one.\n                num = n.get('node_number', -1)\n                if ordering_exists and num != counter:\n                    warnings.warn('Order of graph nodes has changed')\n                # Assign new value\n                n['node_number'] = counter\n                counter += 1\n\n#------------------------------------------------------------------------------#\n\n    def update_node_ordering(self, removed_number):\n        \"\"\"\n        Uppdate an existing ordering of the nodes in the graph, stored as the attribute\n        'node_number'.\n        Intended for keeping the node ordering after removing a node from the bucket. In\n        this way, the edge sorting will not be disturbed by the removal, but no gaps in are\n        created.\n\n        Parameter:\n            removed_number: node_number of the removed grid.\n\n        \"\"\"\n\n        # Loop over grids in decreasing dimensions\n        for dim in range(self.dim_max(), self.dim_min() - 1, -1):\n            for g in self.grids_of_dimension(dim):\n                if not self.has_nodes_prop([g], 'node_number'):\n                    # It is not clear how severe this case is. For the moment,\n                    # we give a warning, and hope the user knows what to do\n                    warnings.warn(\n                        'Tried to update node ordering where none exists')\n                    # No point in continuing with this node.\n                    continue\n\n                # Obtain the old node number\n                n = self.graph.node[g]\n                old_number = n.get('node_number', -1)\n                # And replace it if it is higher than the removed one\n                if old_number > removed_number:\n                    n['node_number'] = old_number - 1\n\n\n#------------------------------------------------------------------------------#\n\n    def target_2_source_nodes(self, g_src, g_trg):\n        \"\"\"\n        Find the local node mapping from a source grid to a target grid.\n\n        target_2_source_nodes(..) returns the mapping from g_src -> g_trg such\n        that g_trg.nodes[:, map] == g_src.nodes. E.g., if the target grid is the\n        highest dim grid, target_2_source_nodes will equal the global node\n        numbering.\n\n        \"\"\"\n        node_source = np.atleast_2d(g_src.global_point_ind)\n        node_target = np.atleast_2d(g_trg.global_point_ind)\n        _, trg_2_src_nodes = setmembership.ismember_rows(\n            node_source.astype(np.int32), node_target.astype(np.int32))\n        return trg_2_src_nodes\n\n#------------------------------------------------------------------------------#\n\n    def compute_geometry(self, is_embedded=True):\n        \"\"\"Compute geometric quantities for the grids.\n\n        Note: the flag \"is_embedded\" is True by default.\n        \"\"\"\n\n        [g.compute_geometry(is_embedded=is_embedded) for g, _ in self]\n\n#------------------------------------------------------------------------------#\n\n    def copy(self):\n        \"\"\"Make a copy of the grid bucket utilizing the built-in copy function\n        of networkx.\n\n        \"\"\"\n        gb_copy = GridBucket()\n        gb_copy.graph = self.graph.copy()\n        return gb_copy\n\n#------------------------------------------------------------------------------#\n\n    def duplicate_without_dimension(self, dim):\n        \"\"\"\n        Remove all the nodes of dimension dim and add new edges between their\n        neighbors by calls to remove_node.\n\n        \"\"\"\n\n        gb_copy = self.copy()\n        grids_of_dim = gb_copy.grids_of_dimension(dim)\n        grids_of_dim_old = self.grids_of_dimension(dim)\n        # The node numbers are copied for each grid, so they can be used to\n        # make sure we use the same grids (g and g_old) below.\n        nn_new = gb_copy.nodes_prop(grids_of_dim, 'node_number')\n        nn_old = self.nodes_prop(grids_of_dim_old, 'node_number')\n        _, old_in_new = setmembership.ismember_rows(np.array(nn_new),\n                                                   np.array(nn_old), sort=False)\n        neighbours_dict = {}\n        neighbours_dict_old = {}\n        eliminated_nodes =  {}\n        for i, g in enumerate(grids_of_dim):\n            # Eliminate the node and add new gb edges:\n            neighbours =  gb_copy.eliminate_node(g)\n            # Keep track of which nodes were connected to each of the eliminated\n            # nodes. Note that the key is the node number in the old gb, whereas\n            # the neighbours lists refer to the copy grids.\n            g_old = grids_of_dim_old[old_in_new[i]]\n            neighbours_dict[i] = neighbours\n            neighbours_dict_old[i] = self.node_neighbors(g_old)\n            eliminated_nodes[i] = g_old\n\n        elimination_data = {'neighbours':neighbours_dict,\n                            'neighbours_old':neighbours_dict_old,\n                            'eliminated_nodes':eliminated_nodes}\n\n        return gb_copy, elimination_data\n\n#------------------------------------------------------------------------------#\n\n    def find_shared_face(self, g0, g1, g_l):\n        \"\"\"\n        Given two nd grids meeting at a (n-1)d node (to be removed), find which two\n        faces meet at the intersection (one from each grid) and build the connection\n        matrix cell_cells.\n        In face_cells, the cells of the lower dimensional grid correspond to the first\n        axis and the faces of the higher dimensional grid to the second axis.\n        Furthermore, grids are sorted with the lower-dimensional first and the\n        higher-dimesnional last. To be consistent with this, the grid corresponding\n        to the first axis of cell_cells is the first grid of the node sorting.\n\n        Parameters:\n            g0 and g1: The two nd grids.\n            g_l: The (n-1)d grid to be removed.\n        Returns: The np array cell_cells (g0.num_cells x g1.num_cells), the nd-nd\n            equivalent of the face_cells matrix. Cell_cells identifies connections\n            (\"faces\") between all permutations of cells of the two grids which were\n            initially connected to the lower_dim_node.\n            Example: g_l is connected to cells 0 and 1 of g0 (vertical)\n            and 1 and 2 of g1 (horizontal) as follows:\n\n                |\n            _ _ . _ _\n                |\n\n            Cell_cells: [ [ 0, 1, 1, 0],\n                          [ 0, 1, 1, 0] ]\n            connects both cell 0 and 1 of g0 (read along first dimension) to\n            cells 1 and 2 of g1 (second dimension of the array).\n        \"\"\"\n        # Sort nodes according to node_number\n        g0, g1 = self.sorted_nodes_of_edge([g0, g1])\n\n        # Identify the faces connecting the neighbors to the grid to be removed\n        fc1 = self.edge_props([g0, g_l])\n        fc2 = self.edge_props([g1, g_l])\n        _, faces_1 = fc1['face_cells'].nonzero()\n        _, faces_2 = fc2['face_cells'].nonzero()\n        # Extract the corresponding cells\n        _,cells_1 = g0.cell_faces[faces_1].nonzero()\n        _,cells_2 = g1.cell_faces[faces_2].nonzero()\n\n        # Connect the two remaining grid through the cell_cells matrix,\n        # to be placed as a face_cells substitute. The ordering is consistent\n        # with the face_cells wrt the sorting of the nodes.\n        cell_cells = np.zeros((g0.num_cells, g1.num_cells), dtype=bool)\n        rows = np.tile(cells_1,(cells_2.size,1))\n        cols = np.tile(cells_2,(cells_1.size,1)).T\n        cell_cells[rows,cols] = True\n\n        return cell_cells\n\n#------------------------------------------------------------------------------#\n\n    def eliminate_node(self, node):\n        \"\"\"\n        Remove the node (and the edges it partakes in) and add new direct\n        connections (gb edges) between each of the neighbor pairs. A node with\n        n_neighbors neighbours gives rise to 1 + 2 + ... + n_neighbors-1 new edges.\n\n        \"\"\"\n        # Identify neighbors\n        neighbors = self.sort_multiple_nodes( self.node_neighbors(node) )\n\n        n_neighbors = len(neighbors)\n\n        # Add an edge between each neighbor pair\n        for i in range(n_neighbors - 1):\n            g0 = neighbors[i]\n            for j in range(i + 1, n_neighbors):\n                g1 = neighbors[j]\n                cell_cells = self.find_shared_face(g0, g1, node)\n                self.add_edge([g0, g1], cell_cells)\n\n        # Remove the node and update the ordering of the remaining nodes\n        node_number = self.node_prop(node, 'node_number')\n        self.remove_node(node)\n        self.update_node_ordering(node_number)\n\n        return neighbors\n\n#------------------------------------------------------------------------------#\n\n    def apply_function_to_nodes(self, fct):\n        \"\"\"\n        Loop on all the nodes and evaluate a function on each of them.\n\n        Parameter:\n            fct: function to evaluate. It takes a grid and the related data and\n                returns a scalar.\n\n        Returns:\n            values: vector containing the function evaluated on each node,\n                ordered by 'node_number'.\n\n        \"\"\"\n        values = np.empty(self.size())\n        for g, d in self:\n            values[d['node_number']] = fct(g, d)\n        return values\n\n#------------------------------------------------------------------------------#\n\n    def apply_function_to_edges(self, fct):\n        \"\"\"\n        Loop on all the edges and evaluate a function on each of them.\n\n        Parameter:\n            fct: function to evaluate. It returns a scalar and takes: the higher\n                and lower dimensional grids, the higher and lower dimensional\n                data, the global data.\n\n        Returns:\n            matrix: sparse strict upper triangular matrix containing the function\n                evaluated on each edge (pair of nodes), ordered by their\n                relative 'node_number'.\n\n        \"\"\"\n        i = np.zeros(self.graph.number_of_edges(), dtype=int)\n        j = np.zeros(i.size, dtype=int)\n        values = np.zeros(i.size)\n\n        # Loop over the edges of the graph (pair of connected nodes)\n        idx = 0\n        for e, data in self.edges_props():\n            g_l, g_h = self.sorted_nodes_of_edge(e)\n            data_l, data_h = self.node_props(g_l), self.node_props(g_h)\n\n            i[idx], j[idx] = self.nodes_prop([g_l, g_h], 'node_number')\n            values[idx] = fct(g_h, g_l, data_h, data_l, data)\n            idx += 1\n\n        # Upper triangular matrix\n        return sps.coo_matrix((values, (i, j)), (self.size(), self.size()))\n\n#------------------------------------------------------------------------------#\n\n    def apply_function(self, fct_nodes, fct_edges):\n        \"\"\"\n        Loop on all the nodes and edges and evaluate a function on each of them.\n\n        Parameter:\n            fct_nodes: function to evaluate. It takes a grid and the related data\n                and returns a scalar.\n\n            fct_edges: function to evaluate. It returns a scalar and takes: the\n                higher and lower dimensional grids, the higher and lower\n                dimensional data, the global data.\n\n        Returns:\n            matrix: sparse triangular matrix containing the function\n                evaluated on each edge (pair of nodes) and node, ordered by their\n                relative 'node_number'. The diagonal contains the node\n                evaluation.\n\n        \"\"\"\n        matrix = self.apply_function_to_edges(fct_edges)\n        matrix.setdiag(self.apply_function_to_nodes(fct_nodes))\n        return matrix\n\n#------------------------------------------------------------------------------#\n\n    def diameter(self, cond=None):\n        \"\"\"\n        Compute the grid bucket diameter (mesh size), considering a loop on all\n        the grids.  It is possible to specify a condition based on the grid to\n        select some of them.\n\n        Parameter:\n            cond: optional, predicate with a grid as input.\n\n        Return:\n            diameter: the diameter of the grid bucket.\n        \"\"\"\n        if cond is None:\n            cond = lambda _: True\n        diam = [np.amax(g.cell_diameters()) for g in self.graph if cond(g)]\n        return np.amax(diam)\n\n#------------------------------------------------------------------------------#\n\n    def bounding_box(self, as_dict=False):\n        \"\"\"\n        Return the bounding box of the grid bucket.\n        \"\"\"\n        c_0s = np.empty((3, self.size()))\n        c_1s = np.empty((3, self.size()))\n\n        for i, g in enumerate(self.graph):\n            c_0s[:, i], c_1s[:, i] = g.bounding_box()\n\n        min_vals = np.amin(c_0s, axis=1)\n        max_vals = np.amax(c_1s, axis=1)\n\n        if as_dict:\n            return {'xmin': min_vals[0], 'xmax': max_vals[0],\n                    'ymin': min_vals[1], 'ymax': max_vals[1],\n                    'zmin': min_vals[2], 'zmax': max_vals[2]}\n        else:\n            return min_vals, max_vals\n\n#------------------------------------------------------------------------------#\n\n    def num_cells(self, cond=None):\n        \"\"\"\n        Compute the total number of cells of the grid bucket, considering a loop\n        on all the grids.  It is possible to specify a condition based on the\n        grid to select some of them.\n\n        Parameter:\n            cond: optional, predicate with a grid as input.\n\n        Return:\n            num_cells: the total number of cells of the grid bucket.\n        \"\"\"\n        if cond is None:\n            cond = lambda _: True\n        return np.sum([g.num_cells for g in self.graph if cond(g)])\n\n#------------------------------------------------------------------------------#\n\n    def num_faces(self, cond=None):\n        \"\"\"\n        Compute the total number of faces of the grid bucket, considering a loop\n        on all the grids.  It is possible to specify a condition based on the\n        grid to select some of them.\n\n        Parameter:\n            cond: optional, predicate with a grid as input.\n\n        Return:\n            num_faces: the total number of faces of the grid bucket.\n        \"\"\"\n        if cond is None:\n            cond = lambda _: True\n        return np.sum([g.num_faces for g in self.graph if cond(g)])\n\n#------------------------------------------------------------------------------#\n\n    def num_nodes(self, cond=None):\n        \"\"\"\n        Compute the total number of nodes of the grid bucket, considering a loop\n        on all the grids.  It is possible to specify a condition based on the\n        grid to select some of them.\n\n        Parameter:\n            cond: optional, predicate with a grid as input.\n\n        Return:\n            num_nodes: the total number of nodes of the grid bucket.\n        \"\"\"\n        if cond is None:\n            cond = lambda _: True\n        return np.sum([g.num_nodes for g in self.graph if cond(g)])\n\n#------------------------------------------------------------------------------#\n\n    def __str__(self):\n        max_dim = self.grids_of_dimension(self.dim_max())\n        num_nodes = 0\n        num_cells = 0\n        for g in max_dim:\n            num_nodes += g.num_nodes\n            num_cells += g.num_cells\n        s = 'Mixed dimensional grid. \\n'\n        s += 'Maximum dimension ' + str(self.dim_max()) + '\\n'\n        s += 'Minimum dimension ' + str(self.dim_min()) + '\\n'\n        s += 'Size of highest dimensional grid: Cells: ' + str(num_cells)\n        s += '. Nodes: ' + str(num_nodes) + '\\n'\n        s += 'In lower dimensions: \\n'\n        for dim in range(self.dim_max() - 1, self.dim_min() - 1, -1):\n            gl = self.grids_of_dimension(dim)\n            s += str(len(gl)) + ' grids of dimension ' + str(dim) + '\\n'\n        return s\n\n#------------------------------------------------------------------------------#\n\n    def __repr__(self):\n        s = 'Grid bucket containing ' + str(self.size()) + ' grids:\\n'\n        num = 0\n        for dim in range(self.dim_max(), self.dim_min() - 1, -1):\n            gl = self.grids_of_dimension(dim)\n            s += str(len(gl)) + ' grids of dimension ' + str(dim) + '\\n'\n        return s",
  "def __init__(self):\n        self.graph = networkx.Graph(directed=False)\n        self.name = \"grid bucket\"",
  "def __iter__(self):\n        \"\"\"\n        Iterator over the grid bucket.\n\n        Yields:\n            core.grid.nodes: The grid associated with a node.\n            data: The dictionary storing all information in this node.\n\n        \"\"\"\n        for g in self.graph:\n            data = self.graph.node[g]\n            yield g, data",
  "def size(self):\n        \"\"\"\n        Returns:\n            int: Number of nodes in the grid.\n\n        \"\"\"\n        return self.graph.number_of_nodes()",
  "def dim_max(self):\n        \"\"\"\n        Returns:\n            int: Maximum dimension of the grids present in the hierarchy.\n\n        \"\"\"\n        return np.amax([g.dim for g, _ in self])",
  "def dim_min(self):\n        \"\"\"\n        Returns:\n            int: Minimum dimension of the grids present in the hierarchy.\n\n        \"\"\"\n        return np.amin([g.dim for g, _ in self])",
  "def nodes(self):\n        \"\"\"\n        Yields:\n            An iterator over the graph nodes.\n\n        \"\"\"\n        return self.graph.nodes_iter()",
  "def edges(self):\n        \"\"\"\n        Yields:\n            An iterator over the graph edges.\n\n        \"\"\"\n        return self.graph.edges_iter()",
  "def edges_props_of_node(self, n):\n        \"\"\"\n        Iterator over the edges of the specific node.\n\n        Parameters:\n            n: A node in the graph.\n\n        Yields:\n            core.grid.edges: The edge (pair of grids) associated with an edge.\n            data: The dictionary storing all information in this edge.\n\n        \"\"\"\n        for e in self.graph.edges([n]):\n            yield e, self.edge_props(e)",
  "def sorted_nodes_of_edge(self, e):\n        \"\"\"\n        Obtain the vertices of an edge, in ascending order with respect their\n        dimension. If the edge is between grids of the same dimension, the node\n        ordering (as defined by assign_node_ordering()) is used. If no ordering\n        of nodes exists, assign_node_ordering() will be called by this method.\n\n        Parameters:\n            e: An edge in the graph.\n\n        Returns:\n            dictionary: The first vertex of the edge.\n            dictionary: The second vertex of the edge.\n\n        \"\"\"\n\n        if e[0].dim == e[1].dim:\n            if not self.has_nodes_prop(e, 'node_number'):\n                self.assign_node_ordering()\n\n            node_indexes = self.nodes_prop(e, 'node_number')\n            if node_indexes[0] < node_indexes[1]:\n                return e[0], e[1]\n            return e[1], e[0]\n\n        elif e[0].dim < e[1].dim:\n            return e[0], e[1]\n        else:\n            return e[1], e[0]",
  "def sort_multiple_nodes(self, nodes):\n        \"\"\"\n        Sort all the nodes according to node number.\n\n        Parameters:\n            nodes: List of graph nodes.\n\n        Returns:\n            sorted_nodes: The same nodes, sorted.\n\n        \"\"\"\n        assert self.has_nodes_prop(nodes,'node_number')\n\n        return sorted(nodes, key = lambda n: self.node_prop( n, 'node_number'))",
  "def nodes_of_edge(self, e):\n        \"\"\"\n        Obtain the vertices of an edge.\n\n        Parameters:\n            e: An edge in the graph.\n\n        Returns:\n            dictionary: The first vertex of the edge.\n            dictionary: The second vertex of the edge.\n\n        \"\"\"\n        return e[0], e[1]",
  "def get_grids(self, cond):\n        \"\"\"\n        Obtain the grids subject to a condition.\n\n        Parameters:\n            cond: Predicate to select a grid.\n\n        Returns:\n            grids: np.array of the grids.\n\n        \"\"\"\n        return np.array([g for g, _ in self if cond(g)])",
  "def update_nodes(self, new, old):\n        \"\"\"\n        Update the grids givin the old and new values. The edges are updated\n        accordingly.\n\n        Parameters:\n            new: List of the new grids, it can be a single element\n            old: List of the old grids, it can be a single element\n        \"\"\"\n        new = np.atleast_1d(new)\n        old = np.atleast_1d(old)\n        assert new.size == old.size\n\n        networkx.relabel_nodes(self.graph, dict(zip(new, old)), False)",
  "def node_neighbors(self, node, only_higher=False, only_lower=False):\n        \"\"\"\n        Parameters:\n            node: node in the graph, grid\n            only_higher: consider only the higher dimensional neighbors\n            only_lower: consider only the lower dimensional neighbors\n        Return:\n            list of networkx.node: Neighbors of node 'node'\n\n        \"\"\"\n        neigh = np.array(self.graph.neighbors(node))\n\n        if not only_higher and not only_lower:\n            return neigh\n        elif only_higher and only_lower:\n            return np.empty(0)\n        elif only_higher:\n            # Find the neighbours that are higher dimensional\n            is_high = np.array([w.dim > node.dim for w in neigh])\n            return neigh[is_high]\n        else:\n            # Find the neighbours that are higher dimensional\n            is_low = np.array([w.dim < node.dim for w in neigh])\n            return neigh[is_low]",
  "def add_node_prop(self, key, g=None, prop=None):\n        \"\"\"\n        Add a new property to existing nodes in the graph.\n\n        Properties can be added either to all nodes, or to selected nodes as\n        specified by their grid. In the former case, all nodes will be assigned\n        the property, with None as the default option.\n\n        No tests are done on whether the key already exist, values are simply\n        overwritten.\n\n        Parameters:\n            prop (optional, defaults to None): Property to be added.\n            key (object): Key to the property to be handled.\n            g (list of core.grids.grid, optional): Nodes to be assigned values.\n                Defaults to None, in which case all nodes are assigned the same\n                value.\n            prop (list, or single object, optional): Value to be assigned.\n                Should be either a list with the same length as g, or a single\n                item which is assigned to all nodes. Defaults to None.\n\n        Raises:\n            ValueError if the key is 'node_number', this is reserved for other\n                purposes. See self.assign_node_ordering() for details.\n\n        \"\"\"\n\n        # Check that the key is not 'node_number' - this is reserved\n        if key == 'node_number':\n            raise ValueError('Node number is a reserved key, stay away')\n\n        # Do some checks of parameters first\n        if g is None:\n            assert prop is None or not isinstance(prop, list)\n        else:\n            assert len(g) == len(prop)\n\n        # Set default value.\n        networkx.set_node_attributes(self.graph, key, None)\n\n        if prop is not None:\n            if g is None:\n                for _, n in self:\n                    n[key] = prop\n            else:\n                for v, p in zip(g, prop):\n                    self.graph.node[v][key] = p",
  "def add_node_props(self, keys):\n        \"\"\"\n        Add new properties to existing nodes in the graph.\n\n        Properties are be added to all nodes.\n\n        No tests are done on whether the key already exist, values are simply\n        overwritten.\n\n        Parameters:\n            keys (object): Keys to the properties to be handled.\n\n        Raises:\n            ValueError if the key is 'node_number', this is reserved for other\n                purposes. See self.assign_node_ordering() for details.\n\n        \"\"\"\n        [self.add_node_prop(key) for key in np.atleast_1d(keys)]",
  "def add_edge_prop(self, key, grid_pairs=None, prop=None):\n        \"\"\"\n        Associate a property with an edge.\n\n        Properties can be added either to all edges, or to selected edges as\n        specified by their grid pair. In the former case, all edges will be\n        assigned the property, with None as the default option.\n\n        No tests are done on whether the key already exist, values are simply\n        overwritten.\n\n        Parameters:\n            prop (optional, defaults to None): Property to be added.\n            key (object): Key to the property to be handled.\n            g (list of list of core.grids.grid, optional): Grid pairs defining\n                the edges to be assigned. values. Defaults to None, in which\n                case all edges are assigned the same value.\n            prop (list, or single object, optional): Value to be assigned.\n                Should be either a list with the same length as grid_pairs, or\n                a single item which is assigned to all nodes. Defaults to\n                None.\n\n        Raises:\n            KeyError if a grid pair is not an existing edge in the grid.\n\n        \"\"\"\n\n        # Do some checks of parameters first\n        if grid_pairs is None:\n            assert prop is None or not isinstance(prop, list)\n        else:\n            assert len(grid_pairs) == len(prop)\n\n        # networkx.set_edge_attributes(self.graph, key, None)\n\n        if prop is not None:\n            for gp, p in zip(grid_pairs, prop):\n                if tuple(gp) in self.graph.edges():\n                    self.graph.edge[gp[0]][gp[1]][key] = p\n                elif tuple(gp[::-1]) in self.graph.edges():\n                    self.graph.edge[gp[1]][gp[0]][key] = p\n                else:\n                    raise KeyError('Cannot assign property to undefined\\\n                                     edge')",
  "def add_edge_props(self, keys):\n        \"\"\"\n        Add new propertiy to existing edges in the graph.\n\n        Properties can be added either to all edges.\n\n        No tests are done on whether the key already exist, values are simply\n        overwritten.\n\n        Parameters:\n            keys (object): Keys to the properties to be handled.\n\n        Raises:\n            KeyError if a grid pair is not an existing edge in the grid.\n\n        \"\"\"\n        [self.add_edge_prop(key) for key in np.atleast_1d(keys)]",
  "def node_prop(self, g, key):\n        \"\"\"\n        Getter for a node property of the bucket.\n\n        Parameters:\n            grid (core.grids.grid): The grid associated with the node.\n            key (object): Key for the property to be retrieved.\n\n        Returns:\n            object: The property.\n        \"\"\"\n        return self.graph.node[g][key]",
  "def has_nodes_prop(self, gs, key):\n        \"\"\"\n        Test if a key exists for a node property of the bucket, for several nodes.\n        Note: the property may contain None but the outcome of the test is\n        still true.\n\n        Parameters:\n            grids (core.grids.grid): The grids associated with the nodes.\n            key (object): Key for the property to be tested.\n\n        Returns:\n            object: The tested property.\n\n        \"\"\"\n        return tuple([key in self.graph.node[g] for g in gs])",
  "def nodes_prop(self, gs, key):\n        \"\"\"\n        Getter for a node property of the bucket, for several nodes.\n\n        Parameters:\n            grids (core.grids.grid): The grids associated with the nodes.\n            key (object): Key for the property to be retrieved.\n\n        Returns:\n            object: The property.\n\n        \"\"\"\n        return tuple([self.graph.node[g][key] for g in gs])",
  "def node_props(self, g):\n        \"\"\"\n        Getter for a node property of the bucket.\n\n        Parameters:\n            grid (core.grids.grid): The grid associated with the node.\n\n        Returns:\n            object: A dictionary with keys and properties.\n\n        \"\"\"\n        return self.graph.node[g]",
  "def node_props_of_keys(self, g, keys):\n        \"\"\"\n        Getter for a node property of the bucket.\n\n        Parameters:\n            grid (core.grids.grid): The grid associated with the node.\n            keys (object): Key for the property to be retrieved.\n\n        Returns:\n            object: A dictionary with key and property.\n\n        \"\"\"\n        return {key: self.graph.node[g][key] for key in keys}",
  "def edge_prop(self, grid_pairs, key):\n        \"\"\"\n        Getter for an edge property of the bucket.\n\n        Parameters:\n            grid_pairs (list of core.grids.grid): The two grids making up the\n                edge.\n            key (object): Key for the property to be retrieved.\n\n        Returns:\n            object: The property.\n\n        Raises:\n            KeyError if the two grids do not form an edge.\n\n        \"\"\"\n        prop_list = []\n        for gp in np.atleast_2d(grid_pairs):\n            if tuple(gp) in self.graph.edges():\n                prop_list.append(self.graph.edge[gp[0]][gp[1]][key])\n            elif tuple(gp[::-1]) in self.graph.edges():\n                prop_list.append(self.graph.edge[gp[1]][gp[0]][key])\n            else:\n                raise KeyError('Unknown edge')\n        return np.array(prop_list)",
  "def edge_props(self, gp):\n        \"\"\"\n        Getter for an edge properties of the bucket.\n\n        Parameters:\n            grids_pair (list of core.grids.grid): The two grids making up the\n                edge.\n            key (object): Keys for the properties to be retrieved.\n\n        Returns:\n            object: The properties.\n\n        Raises:\n            KeyError if the two grids do not form an edge.\n\n        \"\"\"\n        if tuple(gp) in self.graph.edges():\n            return self.graph.edge[gp[0]][gp[1]]\n        elif tuple(gp[::-1]) in self.graph.edges():\n            return self.graph.edge[gp[1]][gp[0]]\n        else:\n            raise KeyError('Unknown edge')",
  "def edges_props(self):\n        \"\"\"\n        Iterator over the edges of the grid bucket.\n\n        Yields:\n            core.grid.edges: The edge (pair of grids) associated with an edge.\n            data: The dictionary storing all information in this edge.\n\n        \"\"\"\n        for e in self.graph.edges():\n            yield e, self.edge_props(e)",
  "def add_nodes(self, new_grids):\n        \"\"\"\n        Add grids to the hierarchy.\n\n        The grids are added to self.grids.\n\n        Parameters:\n            grids (iterable, list?): The grids to be added. None of these\n                should have been added previously.\n\n        Returns:\n            np.ndarray, dtype object: Numpy array of vertexes, that are\n                identified with the grid hierarchy. Same order as input grid.\n\n        \"\"\"\n        new_grids = np.atleast_1d(new_grids)\n        assert not np.any([i is j for i in new_grids for j in self.graph])\n        [self.graph.add_node(g) for g in new_grids]",
  "def remove_node(self, node):\n        \"\"\"\n        Remove node, and related edges, from the grid bucket.\n        Parameters:\n           node : the node to be removed\n\n        \"\"\"\n\n        self.graph.remove_node(node)",
  "def remove_nodes(self, cond):\n        \"\"\"\n        Remove nodes, and related edges, from the grid bucket subject to a\n        conditions. The latter takes as input a grid.\n\n        Parameters:\n            cond: predicate to select the grids to remove.\n\n        \"\"\"\n\n        self.graph.remove_nodes_from([g for g in self.graph if cond(g)])",
  "def add_edge(self, grids, face_cells):\n        \"\"\"\n        Add an edge in the graph.\n\n        If the grids have different dimensions, the coupling will be added with\n        the higher-dimensional grid as the first node. For equal dimensions,\n        the ordering of the nodes is the same as in input grids.\n\n        Parameters:\n            grids (list, len==2). Grids to be connected. Order is arbitrary.\n            face_cells (object): Identity mapping between cells in the\n                higher-dimensional grid and faces in the lower-dimensional\n                grid. No assumptions are made on the type of the object at this\n                stage. In the grids[0].dim = grids[1].dim case, the mapping is\n                from faces of the first grid to faces of the second one.\n\n        Raises:\n            ValueError if the two grids are not one dimension apart\n\n        \"\"\"\n        assert np.asarray(grids).size == 2\n        # Check that the connection does not already exist\n        assert not (tuple(grids) in self.graph.edges()\n                    or tuple(grids[::-1]) in self.graph.edges())\n\n        # The higher-dimensional grid is the first node of the edge.\n        if grids[0].dim - 1 == grids[1].dim:\n            self.graph.add_edge(*grids, face_cells=face_cells)\n        elif grids[0].dim == grids[1].dim - 1:\n            self.graph.add_edge(*grids[::-1], face_cells=face_cells)\n        elif grids[0].dim == grids[1].dim:\n            self.graph.add_edge(*grids, face_cells=face_cells)\n        else:\n            raise ValueError('Grid dimension mismatch')",
  "def grids_of_dimension(self, dim):\n        \"\"\"\n        Get all grids in the bucket of a specific dimension.\n\n        Returns:\n            list: Of grids of the specified dimension\n\n        \"\"\"\n        return [g for g in self.graph.nodes() if g.dim == dim]",
  "def assign_node_ordering(self, overwrite_existing=True):\n        \"\"\"\n        Assign an ordering of the nodes in the graph, stored as the attribute\n        'node_number'.\n\n        The intended use is to define the block structure of a discretization\n        on the grid hierarchy.\n\n        The ordering starts with grids of highest dimension. The ordering\n        within each dimension is determined by an iterator over the graph, and\n        can in principle change between two calls to this function.\n\n        If an ordering covering all nodes in the graph already exist, but does\n        not coincide with the new ordering, a warning is issued. If the optional\n        parameter overwrite_existing is set to False, no update is performed if\n        an node ordering already exists.\n        \"\"\"\n\n        # Check whether 'node_number' is defined for the grids already.\n        ordering_exists = True\n        for _, n in self:\n            if not 'node_number' in n.keys():\n                ordering_exists = False\n        if ordering_exists and not overwrite_existing:\n            return\n\n        counter = 0\n        # Loop over grids in decreasing dimensions\n        for dim in range(self.dim_max(), self.dim_min() - 1, -1):\n            for g in self.grids_of_dimension(dim):\n                n = self.graph.node[g]\n                # Get old value, issue warning if not equal to the new one.\n                num = n.get('node_number', -1)\n                if ordering_exists and num != counter:\n                    warnings.warn('Order of graph nodes has changed')\n                # Assign new value\n                n['node_number'] = counter\n                counter += 1",
  "def update_node_ordering(self, removed_number):\n        \"\"\"\n        Uppdate an existing ordering of the nodes in the graph, stored as the attribute\n        'node_number'.\n        Intended for keeping the node ordering after removing a node from the bucket. In\n        this way, the edge sorting will not be disturbed by the removal, but no gaps in are\n        created.\n\n        Parameter:\n            removed_number: node_number of the removed grid.\n\n        \"\"\"\n\n        # Loop over grids in decreasing dimensions\n        for dim in range(self.dim_max(), self.dim_min() - 1, -1):\n            for g in self.grids_of_dimension(dim):\n                if not self.has_nodes_prop([g], 'node_number'):\n                    # It is not clear how severe this case is. For the moment,\n                    # we give a warning, and hope the user knows what to do\n                    warnings.warn(\n                        'Tried to update node ordering where none exists')\n                    # No point in continuing with this node.\n                    continue\n\n                # Obtain the old node number\n                n = self.graph.node[g]\n                old_number = n.get('node_number', -1)\n                # And replace it if it is higher than the removed one\n                if old_number > removed_number:\n                    n['node_number'] = old_number - 1",
  "def target_2_source_nodes(self, g_src, g_trg):\n        \"\"\"\n        Find the local node mapping from a source grid to a target grid.\n\n        target_2_source_nodes(..) returns the mapping from g_src -> g_trg such\n        that g_trg.nodes[:, map] == g_src.nodes. E.g., if the target grid is the\n        highest dim grid, target_2_source_nodes will equal the global node\n        numbering.\n\n        \"\"\"\n        node_source = np.atleast_2d(g_src.global_point_ind)\n        node_target = np.atleast_2d(g_trg.global_point_ind)\n        _, trg_2_src_nodes = setmembership.ismember_rows(\n            node_source.astype(np.int32), node_target.astype(np.int32))\n        return trg_2_src_nodes",
  "def compute_geometry(self, is_embedded=True):\n        \"\"\"Compute geometric quantities for the grids.\n\n        Note: the flag \"is_embedded\" is True by default.\n        \"\"\"\n\n        [g.compute_geometry(is_embedded=is_embedded) for g, _ in self]",
  "def copy(self):\n        \"\"\"Make a copy of the grid bucket utilizing the built-in copy function\n        of networkx.\n\n        \"\"\"\n        gb_copy = GridBucket()\n        gb_copy.graph = self.graph.copy()\n        return gb_copy",
  "def duplicate_without_dimension(self, dim):\n        \"\"\"\n        Remove all the nodes of dimension dim and add new edges between their\n        neighbors by calls to remove_node.\n\n        \"\"\"\n\n        gb_copy = self.copy()\n        grids_of_dim = gb_copy.grids_of_dimension(dim)\n        grids_of_dim_old = self.grids_of_dimension(dim)\n        # The node numbers are copied for each grid, so they can be used to\n        # make sure we use the same grids (g and g_old) below.\n        nn_new = gb_copy.nodes_prop(grids_of_dim, 'node_number')\n        nn_old = self.nodes_prop(grids_of_dim_old, 'node_number')\n        _, old_in_new = setmembership.ismember_rows(np.array(nn_new),\n                                                   np.array(nn_old), sort=False)\n        neighbours_dict = {}\n        neighbours_dict_old = {}\n        eliminated_nodes =  {}\n        for i, g in enumerate(grids_of_dim):\n            # Eliminate the node and add new gb edges:\n            neighbours =  gb_copy.eliminate_node(g)\n            # Keep track of which nodes were connected to each of the eliminated\n            # nodes. Note that the key is the node number in the old gb, whereas\n            # the neighbours lists refer to the copy grids.\n            g_old = grids_of_dim_old[old_in_new[i]]\n            neighbours_dict[i] = neighbours\n            neighbours_dict_old[i] = self.node_neighbors(g_old)\n            eliminated_nodes[i] = g_old\n\n        elimination_data = {'neighbours':neighbours_dict,\n                            'neighbours_old':neighbours_dict_old,\n                            'eliminated_nodes':eliminated_nodes}\n\n        return gb_copy, elimination_data",
  "def find_shared_face(self, g0, g1, g_l):\n        \"\"\"\n        Given two nd grids meeting at a (n-1)d node (to be removed), find which two\n        faces meet at the intersection (one from each grid) and build the connection\n        matrix cell_cells.\n        In face_cells, the cells of the lower dimensional grid correspond to the first\n        axis and the faces of the higher dimensional grid to the second axis.\n        Furthermore, grids are sorted with the lower-dimensional first and the\n        higher-dimesnional last. To be consistent with this, the grid corresponding\n        to the first axis of cell_cells is the first grid of the node sorting.\n\n        Parameters:\n            g0 and g1: The two nd grids.\n            g_l: The (n-1)d grid to be removed.\n        Returns: The np array cell_cells (g0.num_cells x g1.num_cells), the nd-nd\n            equivalent of the face_cells matrix. Cell_cells identifies connections\n            (\"faces\") between all permutations of cells of the two grids which were\n            initially connected to the lower_dim_node.\n            Example: g_l is connected to cells 0 and 1 of g0 (vertical)\n            and 1 and 2 of g1 (horizontal) as follows:\n\n                |\n            _ _ . _ _\n                |\n\n            Cell_cells: [ [ 0, 1, 1, 0],\n                          [ 0, 1, 1, 0] ]\n            connects both cell 0 and 1 of g0 (read along first dimension) to\n            cells 1 and 2 of g1 (second dimension of the array).\n        \"\"\"\n        # Sort nodes according to node_number\n        g0, g1 = self.sorted_nodes_of_edge([g0, g1])\n\n        # Identify the faces connecting the neighbors to the grid to be removed\n        fc1 = self.edge_props([g0, g_l])\n        fc2 = self.edge_props([g1, g_l])\n        _, faces_1 = fc1['face_cells'].nonzero()\n        _, faces_2 = fc2['face_cells'].nonzero()\n        # Extract the corresponding cells\n        _,cells_1 = g0.cell_faces[faces_1].nonzero()\n        _,cells_2 = g1.cell_faces[faces_2].nonzero()\n\n        # Connect the two remaining grid through the cell_cells matrix,\n        # to be placed as a face_cells substitute. The ordering is consistent\n        # with the face_cells wrt the sorting of the nodes.\n        cell_cells = np.zeros((g0.num_cells, g1.num_cells), dtype=bool)\n        rows = np.tile(cells_1,(cells_2.size,1))\n        cols = np.tile(cells_2,(cells_1.size,1)).T\n        cell_cells[rows,cols] = True\n\n        return cell_cells",
  "def eliminate_node(self, node):\n        \"\"\"\n        Remove the node (and the edges it partakes in) and add new direct\n        connections (gb edges) between each of the neighbor pairs. A node with\n        n_neighbors neighbours gives rise to 1 + 2 + ... + n_neighbors-1 new edges.\n\n        \"\"\"\n        # Identify neighbors\n        neighbors = self.sort_multiple_nodes( self.node_neighbors(node) )\n\n        n_neighbors = len(neighbors)\n\n        # Add an edge between each neighbor pair\n        for i in range(n_neighbors - 1):\n            g0 = neighbors[i]\n            for j in range(i + 1, n_neighbors):\n                g1 = neighbors[j]\n                cell_cells = self.find_shared_face(g0, g1, node)\n                self.add_edge([g0, g1], cell_cells)\n\n        # Remove the node and update the ordering of the remaining nodes\n        node_number = self.node_prop(node, 'node_number')\n        self.remove_node(node)\n        self.update_node_ordering(node_number)\n\n        return neighbors",
  "def apply_function_to_nodes(self, fct):\n        \"\"\"\n        Loop on all the nodes and evaluate a function on each of them.\n\n        Parameter:\n            fct: function to evaluate. It takes a grid and the related data and\n                returns a scalar.\n\n        Returns:\n            values: vector containing the function evaluated on each node,\n                ordered by 'node_number'.\n\n        \"\"\"\n        values = np.empty(self.size())\n        for g, d in self:\n            values[d['node_number']] = fct(g, d)\n        return values",
  "def apply_function_to_edges(self, fct):\n        \"\"\"\n        Loop on all the edges and evaluate a function on each of them.\n\n        Parameter:\n            fct: function to evaluate. It returns a scalar and takes: the higher\n                and lower dimensional grids, the higher and lower dimensional\n                data, the global data.\n\n        Returns:\n            matrix: sparse strict upper triangular matrix containing the function\n                evaluated on each edge (pair of nodes), ordered by their\n                relative 'node_number'.\n\n        \"\"\"\n        i = np.zeros(self.graph.number_of_edges(), dtype=int)\n        j = np.zeros(i.size, dtype=int)\n        values = np.zeros(i.size)\n\n        # Loop over the edges of the graph (pair of connected nodes)\n        idx = 0\n        for e, data in self.edges_props():\n            g_l, g_h = self.sorted_nodes_of_edge(e)\n            data_l, data_h = self.node_props(g_l), self.node_props(g_h)\n\n            i[idx], j[idx] = self.nodes_prop([g_l, g_h], 'node_number')\n            values[idx] = fct(g_h, g_l, data_h, data_l, data)\n            idx += 1\n\n        # Upper triangular matrix\n        return sps.coo_matrix((values, (i, j)), (self.size(), self.size()))",
  "def apply_function(self, fct_nodes, fct_edges):\n        \"\"\"\n        Loop on all the nodes and edges and evaluate a function on each of them.\n\n        Parameter:\n            fct_nodes: function to evaluate. It takes a grid and the related data\n                and returns a scalar.\n\n            fct_edges: function to evaluate. It returns a scalar and takes: the\n                higher and lower dimensional grids, the higher and lower\n                dimensional data, the global data.\n\n        Returns:\n            matrix: sparse triangular matrix containing the function\n                evaluated on each edge (pair of nodes) and node, ordered by their\n                relative 'node_number'. The diagonal contains the node\n                evaluation.\n\n        \"\"\"\n        matrix = self.apply_function_to_edges(fct_edges)\n        matrix.setdiag(self.apply_function_to_nodes(fct_nodes))\n        return matrix",
  "def diameter(self, cond=None):\n        \"\"\"\n        Compute the grid bucket diameter (mesh size), considering a loop on all\n        the grids.  It is possible to specify a condition based on the grid to\n        select some of them.\n\n        Parameter:\n            cond: optional, predicate with a grid as input.\n\n        Return:\n            diameter: the diameter of the grid bucket.\n        \"\"\"\n        if cond is None:\n            cond = lambda _: True\n        diam = [np.amax(g.cell_diameters()) for g in self.graph if cond(g)]\n        return np.amax(diam)",
  "def bounding_box(self, as_dict=False):\n        \"\"\"\n        Return the bounding box of the grid bucket.\n        \"\"\"\n        c_0s = np.empty((3, self.size()))\n        c_1s = np.empty((3, self.size()))\n\n        for i, g in enumerate(self.graph):\n            c_0s[:, i], c_1s[:, i] = g.bounding_box()\n\n        min_vals = np.amin(c_0s, axis=1)\n        max_vals = np.amax(c_1s, axis=1)\n\n        if as_dict:\n            return {'xmin': min_vals[0], 'xmax': max_vals[0],\n                    'ymin': min_vals[1], 'ymax': max_vals[1],\n                    'zmin': min_vals[2], 'zmax': max_vals[2]}\n        else:\n            return min_vals, max_vals",
  "def num_cells(self, cond=None):\n        \"\"\"\n        Compute the total number of cells of the grid bucket, considering a loop\n        on all the grids.  It is possible to specify a condition based on the\n        grid to select some of them.\n\n        Parameter:\n            cond: optional, predicate with a grid as input.\n\n        Return:\n            num_cells: the total number of cells of the grid bucket.\n        \"\"\"\n        if cond is None:\n            cond = lambda _: True\n        return np.sum([g.num_cells for g in self.graph if cond(g)])",
  "def num_faces(self, cond=None):\n        \"\"\"\n        Compute the total number of faces of the grid bucket, considering a loop\n        on all the grids.  It is possible to specify a condition based on the\n        grid to select some of them.\n\n        Parameter:\n            cond: optional, predicate with a grid as input.\n\n        Return:\n            num_faces: the total number of faces of the grid bucket.\n        \"\"\"\n        if cond is None:\n            cond = lambda _: True\n        return np.sum([g.num_faces for g in self.graph if cond(g)])",
  "def num_nodes(self, cond=None):\n        \"\"\"\n        Compute the total number of nodes of the grid bucket, considering a loop\n        on all the grids.  It is possible to specify a condition based on the\n        grid to select some of them.\n\n        Parameter:\n            cond: optional, predicate with a grid as input.\n\n        Return:\n            num_nodes: the total number of nodes of the grid bucket.\n        \"\"\"\n        if cond is None:\n            cond = lambda _: True\n        return np.sum([g.num_nodes for g in self.graph if cond(g)])",
  "def __str__(self):\n        max_dim = self.grids_of_dimension(self.dim_max())\n        num_nodes = 0\n        num_cells = 0\n        for g in max_dim:\n            num_nodes += g.num_nodes\n            num_cells += g.num_cells\n        s = 'Mixed dimensional grid. \\n'\n        s += 'Maximum dimension ' + str(self.dim_max()) + '\\n'\n        s += 'Minimum dimension ' + str(self.dim_min()) + '\\n'\n        s += 'Size of highest dimensional grid: Cells: ' + str(num_cells)\n        s += '. Nodes: ' + str(num_nodes) + '\\n'\n        s += 'In lower dimensions: \\n'\n        for dim in range(self.dim_max() - 1, self.dim_min() - 1, -1):\n            gl = self.grids_of_dimension(dim)\n            s += str(len(gl)) + ' grids of dimension ' + str(dim) + '\\n'\n        return s",
  "def __repr__(self):\n        s = 'Grid bucket containing ' + str(self.size()) + ' grids:\\n'\n        num = 0\n        for dim in range(self.dim_max(), self.dim_min() - 1, -1):\n            gl = self.grids_of_dimension(dim)\n            s += str(len(gl)) + ' grids of dimension ' + str(dim) + '\\n'\n        return s",
  "class TriangleGrid(Grid):\n    \"\"\" Class representation of a general triangular grid.\n\n    For information on attributes and methods, see the documentation of the\n    parent Grid class.\n\n    \"\"\"\n    def __init__(self, p, tri=None, name=None):\n        \"\"\"\n        Create triangular grid from point cloud.\n\n        If no triangulation is provided, Delaunay will be applied.\n\n        Examples:\n        >>> p = np.random.rand(2, 10)\n        >>> tri = scipy.spatial.Delaunay(p.transpose()).simplices\n        >>> g = TriangleGrid(p, tri.transpose())\n\n        Parameters\n        ----------\n        p (np.ndarray, 2 x num_nodes): Point coordinates\n        tri (np.ndarray, 3 x num_cells): Cell-node connections. If not\n        provided, a Delaunay triangulation will be applied\n        name (str, optional): Name of grid type. Defaults to None, in which\n            case  'TriangleGrid' will be assigned.\n        \"\"\"\n\n        self.dim = 2\n\n        if tri is None:\n            tri = scipy.spatial.Delaunay(p.transpose())\n            tri = tri.simplices\n            tri = tri.transpose()\n\n        if name is None:\n            name = 'TriangleGrid'\n\n        num_nodes = p.shape[1]\n\n        # Add a zero z-coordinate\n        if p.shape[0] == 2:\n            nodes = np.vstack((p, np.zeros(num_nodes)))\n        else:\n            nodes = p\n\n        assert num_nodes > 2   # Check of transposes of point array\n\n        # Face node relations\n        face_nodes = np.hstack((tri[[0, 1]],\n                                tri[[1, 2]],\n                                tri[[2, 0]])).transpose()\n        face_nodes.sort(axis=1)\n        face_nodes, _, cell_faces = setmembership.unique_rows(face_nodes)\n\n        num_faces = face_nodes.shape[0]\n        num_cells = tri.shape[1]\n\n        num_nodes_per_face = 2\n        face_nodes = face_nodes.ravel('C')\n        indptr = np.hstack((np.arange(0, num_nodes_per_face * num_faces,\n                                      num_nodes_per_face),\n                            num_nodes_per_face * num_faces))\n        data = np.ones(face_nodes.shape, dtype=bool)\n        face_nodes = sps.csc_matrix((data, face_nodes, indptr),\n                                    shape=(num_nodes, num_faces))\n\n        # Cell face relation\n        num_faces_per_cell = 3\n        cell_faces = cell_faces.reshape(num_faces_per_cell,\n                                        num_cells).ravel('F')\n        indptr = np.hstack((np.arange(0, num_faces_per_cell*num_cells,\n                                      num_faces_per_cell),\n                            num_faces_per_cell * num_cells))\n        data = -np.ones(cell_faces.shape)\n        _, sgns = np.unique(cell_faces, return_index=True)\n        data[sgns] = 1\n        cell_faces = sps.csc_matrix((data, cell_faces, indptr),\n                                    shape=(num_faces, num_cells))\n\n        super(TriangleGrid, self).__init__(2, nodes, face_nodes, cell_faces,\n                                           name)\n\n    def cell_node_matrix(self):\n        \"\"\" Get cell-node relations in a Nc x 3 matrix\n        Perhaps move this method to a superclass when tet-grids are implemented\n        \"\"\"\n\n        # Absolute value needed since cellFaces can be negative\n        cn = self.face_nodes * np.abs(self.cell_faces) \\\n             * sps.eye(self.num_cells)\n        row, col = cn.nonzero()\n        scol = np.argsort(col)\n\n        # Consistency check\n        assert np.all(accumarray.accum(col, np.ones(col.size)) ==\n                      (self.dim + 1))\n\n        return row[scol].reshape(self.num_cells, 3)",
  "class StructuredTriangleGrid(TriangleGrid):\n    \"\"\" Class for a structured triangular grids, composed of squares divided\n    into two.\n\n    For information on attributes and methods, see the documentation of the\n    parent Grid class.\n\n    \"\"\"\n\n    def __init__(self, nx, physdims=None):\n        \"\"\"\n        Construct a triangular grid by splitting Cartesian cells in two.\n\n        Examples:\n        Grid on the unit cube\n        >>> nx = np.array([2, 3])\n        >>> physdims = np.ones(2)\n        >>> g = simplex.StructuredTriangleGrid(nx, physdims)\n\n        Parameters\n        ----------\n        nx (np.ndarray, size 2): number of cells in each direction of\n        underlying Cartesian grid\n        physdims (np.ndarray, size 2): domain size. Defaults to nx,\n        thus Cartesian cells are unit squares\n        \"\"\"\n        nx = np.asarray(nx)\n        if physdims is None:\n            physdims = nx\n        else:\n            physdims = np.asarray(physdims)\n\n        x = np.linspace(0, physdims[0], nx[0] + 1)\n        y = np.linspace(0, physdims[1], nx[1] + 1)\n\n        # Node coordinates\n        x_coord, y_coord = np.meshgrid(x, y)\n        p = np.vstack((x_coord.ravel(order='C'), y_coord.ravel(order='C')))\n\n        # Define nodes of the first row of cells.\n        tmp_ind = np.arange(0, nx[0])\n        ind_1 = tmp_ind  # Lower left node in quad\n        ind_2 = tmp_ind + 1  # Lower right node\n        ind_3 = nx[0] + 2 + tmp_ind  # Upper left node\n        ind_4 = nx[0] + 1 + tmp_ind  # Upper right node\n\n        # The first triangle is defined by (i1, i2, i3), the next by\n        # (i1, i3, i4). Stack these vertically, and reshape so that the\n        # first quad is split into cells 0 and 1 and so on\n        tri_base = np.vstack((ind_1, ind_2, ind_3, ind_1, ind_3,\n                              ind_4)).reshape((3, -1), order='F')\n        # Initialize array of triangles. For the moment, we will append the\n        # cells here, but we do know how many cells there are in advance,\n        # so pre-allocation is possible if this turns out to be a bottleneck\n        tri = tri_base\n\n        # Loop over all remaining rows in the y-direction.\n        for iter1 in range(nx[1].astype('int64') - 1):\n            # The node numbers are increased by nx[0] + 1 for each row\n            tri = np.hstack((tri, tri_base + (iter1 + 1) * (nx[0] + 1)))\n\n        super(self.__class__, self).__init__(p, tri,\n                                             name='StructuredTriangleGrid')",
  "class TetrahedralGrid(Grid):\n    \"\"\" Class for Tetrahedral grids.\n\n    For information on attributes and methods, see the documentation of the\n    parent Grid class.\n\n    \"\"\"\n\n    def __init__(self, p, tet=None, name=None):\n        \"\"\"\n        Create a tetrahedral grid from a set of point and cells.\n\n        If the cells are not provided a Delaunay tessalation will be\n        constructed.\n\n        Parameters:\n            p (np.array, 3xn_pt): Coordinates of vertices\n            tet (np.array, 4xn_tet, optional): Cell vertices. If none is\n                provided, a Delaunay triangulation will be performed.\n\n        \"\"\"\n        # The method below is to a large degree translated from MRST.\n\n        self.dim = 3\n\n        # Transform points to column vector if necessary (scipy.Delaunay\n        # requires this format)\n        if tet is None:\n            tet = scipy.spatial.Delaunay(p.transpose())\n            tet = tet.simplices\n            tet = tet.transpose()\n\n        if name is None:\n            name = 'TetrahedralGrid'\n\n        num_nodes = p.shape[1]\n\n        nodes = p\n        assert num_nodes > 3   # Check of transposes of point array\n\n        num_cells = tet.shape[1]\n        tet = self.__permute_nodes(p, tet)\n\n        # Define face-nodes so that the first column contains fn of cell 0,\n        # etc.\n        face_nodes = np.vstack((tet[[1, 0, 2]],\n                                tet[[0, 1, 3]],\n                                tet[[2, 0, 3]],\n                                tet[[1, 2, 3]]))\n        # Reshape face-nodes into a 3x 4*num_cells-matrix, with the four first\n        # columns belonging to cell 0.\n        face_nodes = face_nodes.reshape((3, 4*num_cells), order='F')\n        sort_ind = np.squeeze(np.argsort(face_nodes, axis=0))\n        face_nodes_sorted = np.sort(face_nodes, axis=0)\n        face_nodes, _, cell_faces = \\\n            setmembership.unique_columns_tol(face_nodes_sorted)\n\n        num_faces = face_nodes.shape[1]\n\n        num_nodes_per_face = 3\n        face_nodes = face_nodes.ravel(order='F')\n        indptr = np.hstack((np.arange(0, num_nodes_per_face * num_faces,\n                                      num_nodes_per_face),\n                            num_nodes_per_face * num_faces))\n        data = np.ones(face_nodes.shape, dtype=bool)\n        face_nodes = sps.csc_matrix((data, face_nodes, indptr),\n                                    shape=(num_nodes, num_faces))\n\n        # Cell face relation\n        num_faces_per_cell = 4\n        indptr = np.hstack((np.arange(0, num_faces_per_cell*num_cells,\n                                      num_faces_per_cell),\n                            num_faces_per_cell * num_cells))\n        data = np.ones(cell_faces.shape)\n        sgn_change = np.where(np.any(np.diff(sort_ind, axis=0) == 1, axis=0))[0]\n        data[sgn_change] = -1\n        cell_faces = sps.csc_matrix((data, cell_faces, indptr),\n                                    shape=(num_faces, num_cells))\n\n        super(TetrahedralGrid, self).__init__(3, nodes, face_nodes, cell_faces,\n                                              'TetrahedralGrid')\n\n    def __permute_nodes(self, p, t):\n        v = self.__triple_product(p, t)\n        permute = np.where(v > 0)[0]\n        if t.ndim == 1:\n            if permute[0]:\n                t[:2] = t[1::-1]\n        else:\n            t[:2, permute] = t[1::-1, permute]\n        return t\n\n    def __triple_product(self, p, t):\n        px = p[0]\n        py = p[1]\n        pz = p[2]\n\n        x = px[t]\n        y = py[t]\n        z = pz[t]\n\n        dx = x[1:] - x[0]\n        dy = y[1:] - y[0]\n        dz = z[1:] - z[0]\n\n        cross_x = dy[0] * dz[1] - dy[1] * dz[0]\n        cross_y = dz[0] * dx[1] - dz[1] * dx[0]\n        cross_z = dx[0] * dy[1] - dx[1] * dy[0]\n\n        return dx[2] * cross_x + dy[2] * cross_y + dz[2] * cross_z",
  "class StructuredTetrahedralGrid(TetrahedralGrid):\n    \"\"\" Class for a structured triangular grids, composed of squares divided\n    into two.\n\n    For information on attributes and methods, see the documentation of the\n    parent Grid class.\n\n    \"\"\"\n\n    def __init__(self, nx, physdims=None):\n        \"\"\"\n        Construct a triangular grid by splitting Cartesian cells in two.\n\n        Examples:\n        Grid on the unit cube\n        >>> nx = np.array([2, 3])\n        >>> physdims = np.ones(2)\n        >>> g = simplex.StructuredTriangleGrid(nx, physdims)\n\n        Parameters\n        ----------\n        nx (np.ndarray, size 2): number of cells in each direction of\n        underlying Cartesian grid\n        physdims (np.ndarray, size 2): domain size. Defaults to nx,\n        thus Cartesian cells are unit squares\n        \"\"\"\n        nx = np.asarray(nx)\n        if physdims is None:\n            physdims = nx\n        else:\n            physdims = np.asarray(physdims)\n\n        x = np.linspace(0, physdims[0], nx[0] + 1)\n        y = np.linspace(0, physdims[1], nx[1] + 1)\n        z = np.linspace(0, physdims[2], nx[2] + 1)\n\n        # Node coordinates\n        y_coord, x_coord, z_coord = np.meshgrid(y, x, z)\n        p = np.vstack((x_coord.ravel(order='F'), y_coord.ravel(order='F'),\n                       z_coord.ravel(order='F')))\n\n        # Define nodes of the first row of cells.\n        tmp_ind = np.arange(0, nx[0])\n        ind_1 = tmp_ind  # Lower left node in quad\n        ind_2 = tmp_ind + 1  # Lower right node\n        ind_3 = nx[0] + 1 + tmp_ind  # Upper left node\n        ind_4 = nx[0] + 2 + tmp_ind  # Upper right node\n\n        nxy = (nx[0] + 1) * (nx[1] + 1)\n        ind_5 = ind_1 + nxy\n        ind_6 = ind_2 + nxy\n        ind_7 = ind_3 + nxy\n        ind_8 = ind_4 + nxy\n\n        tet_base = np.vstack((ind_1, ind_2, ind_3, ind_5,\n                              ind_2, ind_3, ind_5, ind_7,\n                              ind_2, ind_5, ind_6, ind_7,\n                              ind_2, ind_3, ind_4, ind_7,\n                              ind_2, ind_4, ind_6, ind_7,\n                              ind_4, ind_6, ind_7, ind_8)).reshape((4, -1), order='F')\n        # Initialize array of triangles. For the moment, we will append the\n        # cells here, but we do know how many cells there are in advance,\n        # so pre-allocation is possible if this turns out to be a bottleneck\n\n        # Loop over all remaining rows in the y-direction.\n        for iter2 in range(nx[2].astype('int64')):\n            for iter1 in range(nx[1].astype('int64')):\n                increment = iter2 * nxy + iter1 * (nx[0]+1)\n                if iter2 == 0 and iter1 == 0:\n                    tet = tet_base + increment\n                else:\n                    tet = np.hstack((tet, tet_base + increment))\n\n        super(self.__class__, self).__init__(p, tet=tet,\n                                             name='StructuredTetrahedralGrid')",
  "def __init__(self, p, tri=None, name=None):\n        \"\"\"\n        Create triangular grid from point cloud.\n\n        If no triangulation is provided, Delaunay will be applied.\n\n        Examples:\n        >>> p = np.random.rand(2, 10)\n        >>> tri = scipy.spatial.Delaunay(p.transpose()).simplices\n        >>> g = TriangleGrid(p, tri.transpose())\n\n        Parameters\n        ----------\n        p (np.ndarray, 2 x num_nodes): Point coordinates\n        tri (np.ndarray, 3 x num_cells): Cell-node connections. If not\n        provided, a Delaunay triangulation will be applied\n        name (str, optional): Name of grid type. Defaults to None, in which\n            case  'TriangleGrid' will be assigned.\n        \"\"\"\n\n        self.dim = 2\n\n        if tri is None:\n            tri = scipy.spatial.Delaunay(p.transpose())\n            tri = tri.simplices\n            tri = tri.transpose()\n\n        if name is None:\n            name = 'TriangleGrid'\n\n        num_nodes = p.shape[1]\n\n        # Add a zero z-coordinate\n        if p.shape[0] == 2:\n            nodes = np.vstack((p, np.zeros(num_nodes)))\n        else:\n            nodes = p\n\n        assert num_nodes > 2   # Check of transposes of point array\n\n        # Face node relations\n        face_nodes = np.hstack((tri[[0, 1]],\n                                tri[[1, 2]],\n                                tri[[2, 0]])).transpose()\n        face_nodes.sort(axis=1)\n        face_nodes, _, cell_faces = setmembership.unique_rows(face_nodes)\n\n        num_faces = face_nodes.shape[0]\n        num_cells = tri.shape[1]\n\n        num_nodes_per_face = 2\n        face_nodes = face_nodes.ravel('C')\n        indptr = np.hstack((np.arange(0, num_nodes_per_face * num_faces,\n                                      num_nodes_per_face),\n                            num_nodes_per_face * num_faces))\n        data = np.ones(face_nodes.shape, dtype=bool)\n        face_nodes = sps.csc_matrix((data, face_nodes, indptr),\n                                    shape=(num_nodes, num_faces))\n\n        # Cell face relation\n        num_faces_per_cell = 3\n        cell_faces = cell_faces.reshape(num_faces_per_cell,\n                                        num_cells).ravel('F')\n        indptr = np.hstack((np.arange(0, num_faces_per_cell*num_cells,\n                                      num_faces_per_cell),\n                            num_faces_per_cell * num_cells))\n        data = -np.ones(cell_faces.shape)\n        _, sgns = np.unique(cell_faces, return_index=True)\n        data[sgns] = 1\n        cell_faces = sps.csc_matrix((data, cell_faces, indptr),\n                                    shape=(num_faces, num_cells))\n\n        super(TriangleGrid, self).__init__(2, nodes, face_nodes, cell_faces,\n                                           name)",
  "def cell_node_matrix(self):\n        \"\"\" Get cell-node relations in a Nc x 3 matrix\n        Perhaps move this method to a superclass when tet-grids are implemented\n        \"\"\"\n\n        # Absolute value needed since cellFaces can be negative\n        cn = self.face_nodes * np.abs(self.cell_faces) \\\n             * sps.eye(self.num_cells)\n        row, col = cn.nonzero()\n        scol = np.argsort(col)\n\n        # Consistency check\n        assert np.all(accumarray.accum(col, np.ones(col.size)) ==\n                      (self.dim + 1))\n\n        return row[scol].reshape(self.num_cells, 3)",
  "def __init__(self, nx, physdims=None):\n        \"\"\"\n        Construct a triangular grid by splitting Cartesian cells in two.\n\n        Examples:\n        Grid on the unit cube\n        >>> nx = np.array([2, 3])\n        >>> physdims = np.ones(2)\n        >>> g = simplex.StructuredTriangleGrid(nx, physdims)\n\n        Parameters\n        ----------\n        nx (np.ndarray, size 2): number of cells in each direction of\n        underlying Cartesian grid\n        physdims (np.ndarray, size 2): domain size. Defaults to nx,\n        thus Cartesian cells are unit squares\n        \"\"\"\n        nx = np.asarray(nx)\n        if physdims is None:\n            physdims = nx\n        else:\n            physdims = np.asarray(physdims)\n\n        x = np.linspace(0, physdims[0], nx[0] + 1)\n        y = np.linspace(0, physdims[1], nx[1] + 1)\n\n        # Node coordinates\n        x_coord, y_coord = np.meshgrid(x, y)\n        p = np.vstack((x_coord.ravel(order='C'), y_coord.ravel(order='C')))\n\n        # Define nodes of the first row of cells.\n        tmp_ind = np.arange(0, nx[0])\n        ind_1 = tmp_ind  # Lower left node in quad\n        ind_2 = tmp_ind + 1  # Lower right node\n        ind_3 = nx[0] + 2 + tmp_ind  # Upper left node\n        ind_4 = nx[0] + 1 + tmp_ind  # Upper right node\n\n        # The first triangle is defined by (i1, i2, i3), the next by\n        # (i1, i3, i4). Stack these vertically, and reshape so that the\n        # first quad is split into cells 0 and 1 and so on\n        tri_base = np.vstack((ind_1, ind_2, ind_3, ind_1, ind_3,\n                              ind_4)).reshape((3, -1), order='F')\n        # Initialize array of triangles. For the moment, we will append the\n        # cells here, but we do know how many cells there are in advance,\n        # so pre-allocation is possible if this turns out to be a bottleneck\n        tri = tri_base\n\n        # Loop over all remaining rows in the y-direction.\n        for iter1 in range(nx[1].astype('int64') - 1):\n            # The node numbers are increased by nx[0] + 1 for each row\n            tri = np.hstack((tri, tri_base + (iter1 + 1) * (nx[0] + 1)))\n\n        super(self.__class__, self).__init__(p, tri,\n                                             name='StructuredTriangleGrid')",
  "def __init__(self, p, tet=None, name=None):\n        \"\"\"\n        Create a tetrahedral grid from a set of point and cells.\n\n        If the cells are not provided a Delaunay tessalation will be\n        constructed.\n\n        Parameters:\n            p (np.array, 3xn_pt): Coordinates of vertices\n            tet (np.array, 4xn_tet, optional): Cell vertices. If none is\n                provided, a Delaunay triangulation will be performed.\n\n        \"\"\"\n        # The method below is to a large degree translated from MRST.\n\n        self.dim = 3\n\n        # Transform points to column vector if necessary (scipy.Delaunay\n        # requires this format)\n        if tet is None:\n            tet = scipy.spatial.Delaunay(p.transpose())\n            tet = tet.simplices\n            tet = tet.transpose()\n\n        if name is None:\n            name = 'TetrahedralGrid'\n\n        num_nodes = p.shape[1]\n\n        nodes = p\n        assert num_nodes > 3   # Check of transposes of point array\n\n        num_cells = tet.shape[1]\n        tet = self.__permute_nodes(p, tet)\n\n        # Define face-nodes so that the first column contains fn of cell 0,\n        # etc.\n        face_nodes = np.vstack((tet[[1, 0, 2]],\n                                tet[[0, 1, 3]],\n                                tet[[2, 0, 3]],\n                                tet[[1, 2, 3]]))\n        # Reshape face-nodes into a 3x 4*num_cells-matrix, with the four first\n        # columns belonging to cell 0.\n        face_nodes = face_nodes.reshape((3, 4*num_cells), order='F')\n        sort_ind = np.squeeze(np.argsort(face_nodes, axis=0))\n        face_nodes_sorted = np.sort(face_nodes, axis=0)\n        face_nodes, _, cell_faces = \\\n            setmembership.unique_columns_tol(face_nodes_sorted)\n\n        num_faces = face_nodes.shape[1]\n\n        num_nodes_per_face = 3\n        face_nodes = face_nodes.ravel(order='F')\n        indptr = np.hstack((np.arange(0, num_nodes_per_face * num_faces,\n                                      num_nodes_per_face),\n                            num_nodes_per_face * num_faces))\n        data = np.ones(face_nodes.shape, dtype=bool)\n        face_nodes = sps.csc_matrix((data, face_nodes, indptr),\n                                    shape=(num_nodes, num_faces))\n\n        # Cell face relation\n        num_faces_per_cell = 4\n        indptr = np.hstack((np.arange(0, num_faces_per_cell*num_cells,\n                                      num_faces_per_cell),\n                            num_faces_per_cell * num_cells))\n        data = np.ones(cell_faces.shape)\n        sgn_change = np.where(np.any(np.diff(sort_ind, axis=0) == 1, axis=0))[0]\n        data[sgn_change] = -1\n        cell_faces = sps.csc_matrix((data, cell_faces, indptr),\n                                    shape=(num_faces, num_cells))\n\n        super(TetrahedralGrid, self).__init__(3, nodes, face_nodes, cell_faces,\n                                              'TetrahedralGrid')",
  "def __permute_nodes(self, p, t):\n        v = self.__triple_product(p, t)\n        permute = np.where(v > 0)[0]\n        if t.ndim == 1:\n            if permute[0]:\n                t[:2] = t[1::-1]\n        else:\n            t[:2, permute] = t[1::-1, permute]\n        return t",
  "def __triple_product(self, p, t):\n        px = p[0]\n        py = p[1]\n        pz = p[2]\n\n        x = px[t]\n        y = py[t]\n        z = pz[t]\n\n        dx = x[1:] - x[0]\n        dy = y[1:] - y[0]\n        dz = z[1:] - z[0]\n\n        cross_x = dy[0] * dz[1] - dy[1] * dz[0]\n        cross_y = dz[0] * dx[1] - dz[1] * dx[0]\n        cross_z = dx[0] * dy[1] - dx[1] * dy[0]\n\n        return dx[2] * cross_x + dy[2] * cross_y + dz[2] * cross_z",
  "def __init__(self, nx, physdims=None):\n        \"\"\"\n        Construct a triangular grid by splitting Cartesian cells in two.\n\n        Examples:\n        Grid on the unit cube\n        >>> nx = np.array([2, 3])\n        >>> physdims = np.ones(2)\n        >>> g = simplex.StructuredTriangleGrid(nx, physdims)\n\n        Parameters\n        ----------\n        nx (np.ndarray, size 2): number of cells in each direction of\n        underlying Cartesian grid\n        physdims (np.ndarray, size 2): domain size. Defaults to nx,\n        thus Cartesian cells are unit squares\n        \"\"\"\n        nx = np.asarray(nx)\n        if physdims is None:\n            physdims = nx\n        else:\n            physdims = np.asarray(physdims)\n\n        x = np.linspace(0, physdims[0], nx[0] + 1)\n        y = np.linspace(0, physdims[1], nx[1] + 1)\n        z = np.linspace(0, physdims[2], nx[2] + 1)\n\n        # Node coordinates\n        y_coord, x_coord, z_coord = np.meshgrid(y, x, z)\n        p = np.vstack((x_coord.ravel(order='F'), y_coord.ravel(order='F'),\n                       z_coord.ravel(order='F')))\n\n        # Define nodes of the first row of cells.\n        tmp_ind = np.arange(0, nx[0])\n        ind_1 = tmp_ind  # Lower left node in quad\n        ind_2 = tmp_ind + 1  # Lower right node\n        ind_3 = nx[0] + 1 + tmp_ind  # Upper left node\n        ind_4 = nx[0] + 2 + tmp_ind  # Upper right node\n\n        nxy = (nx[0] + 1) * (nx[1] + 1)\n        ind_5 = ind_1 + nxy\n        ind_6 = ind_2 + nxy\n        ind_7 = ind_3 + nxy\n        ind_8 = ind_4 + nxy\n\n        tet_base = np.vstack((ind_1, ind_2, ind_3, ind_5,\n                              ind_2, ind_3, ind_5, ind_7,\n                              ind_2, ind_5, ind_6, ind_7,\n                              ind_2, ind_3, ind_4, ind_7,\n                              ind_2, ind_4, ind_6, ind_7,\n                              ind_4, ind_6, ind_7, ind_8)).reshape((4, -1), order='F')\n        # Initialize array of triangles. For the moment, we will append the\n        # cells here, but we do know how many cells there are in advance,\n        # so pre-allocation is possible if this turns out to be a bottleneck\n\n        # Loop over all remaining rows in the y-direction.\n        for iter2 in range(nx[2].astype('int64')):\n            for iter1 in range(nx[1].astype('int64')):\n                increment = iter2 * nxy + iter1 * (nx[0]+1)\n                if iter2 == 0 and iter1 == 0:\n                    tet = tet_base + increment\n                else:\n                    tet = np.hstack((tet, tet_base + increment))\n\n        super(self.__class__, self).__init__(p, tet=tet,\n                                             name='StructuredTetrahedralGrid')",
  "class GmshConstants(object):\n    \"\"\"\n    This class is a container for storing constant values that are used in\n    the meshing algorithm. The intention is to make them available to all\n    functions and modules.\n\n    This may not be the most pythonic way of doing this, but it works.\n    \"\"\"\n    def __init__(self):\n        self.DOMAIN_BOUNDARY_TAG = 1\n        self.COMPARTMENT_BOUNDARY_TAG = 2\n        self.FRACTURE_TAG = 3\n        # Tag for objects on the fracture tip\n        self.FRACTURE_TIP_TAG = 4\n        # Tag for objcets on the intersection between two fractures\n        # (co-dimension n-2)\n        self.FRACTURE_INTERSECTION_LINE_TAG = 5\n        # Tag for objects on the intersection between three fractures\n        # (co-dimension n-3)\n        self.FRACTURE_INTERSECTION_POINT_TAG = 6\n        # General auxiliary tag\n        self.AUXILIARY_TAG = 7\n\n        self.PHYSICAL_NAME_DOMAIN = 'DOMAIN'\n        self.PHYSICAL_NAME_FRACTURES = 'FRACTURE_'\n        self.PHYSICAL_NAME_AUXILIARY = 'AUXILIARY_'\n        \n        # Physical name for fracture tips\n        self.PHYSICAL_NAME_FRACTURE_TIP ='FRACTURE_TIP_'\n        self.PHYSICAL_NAME_FRACTURE_LINE = 'FRACTURE_LINE_'\n        self.PHYSICAL_NAME_AUXILIARY_LINE = 'AUXILIARY_LINE_'\n        self.PHYSICAL_NAME_FRACTURE_POINT = 'FRACTURE_POINT_'",
  "def __init__(self):\n        self.DOMAIN_BOUNDARY_TAG = 1\n        self.COMPARTMENT_BOUNDARY_TAG = 2\n        self.FRACTURE_TAG = 3\n        # Tag for objects on the fracture tip\n        self.FRACTURE_TIP_TAG = 4\n        # Tag for objcets on the intersection between two fractures\n        # (co-dimension n-2)\n        self.FRACTURE_INTERSECTION_LINE_TAG = 5\n        # Tag for objects on the intersection between three fractures\n        # (co-dimension n-3)\n        self.FRACTURE_INTERSECTION_POINT_TAG = 6\n        # General auxiliary tag\n        self.AUXILIARY_TAG = 7\n\n        self.PHYSICAL_NAME_DOMAIN = 'DOMAIN'\n        self.PHYSICAL_NAME_FRACTURES = 'FRACTURE_'\n        self.PHYSICAL_NAME_AUXILIARY = 'AUXILIARY_'\n        \n        # Physical name for fracture tips\n        self.PHYSICAL_NAME_FRACTURE_TIP ='FRACTURE_TIP_'\n        self.PHYSICAL_NAME_FRACTURE_LINE = 'FRACTURE_LINE_'\n        self.PHYSICAL_NAME_AUXILIARY_LINE = 'AUXILIARY_LINE_'\n        self.PHYSICAL_NAME_FRACTURE_POINT = 'FRACTURE_POINT_'",
  "def partition_metis(g, num_part):\n    \"\"\"\n    Partition a grid using metis.\n\n    This function requires that pymetis is installed, as can be done by\n\n        pip install pymetis\n\n    This will install metis itself in addition to the python bindings. There\n    are other python bindings for metis as well, but pymetis has behaved well\n    so far.\n\n    Parameters:\n        g: core.grids.grid: To be partitioned. Only the cell_face attribute is\n            used\n        num_part (int): Number of partitions.\n\n    Returns:\n        np.array (size:g.num_cells): Partition vector, one number in\n            [0, num_part) for each cell.\n\n    \"\"\"\n\n    # Connection map between cells\n    c2c = g.cell_connection_map()\n\n    # Convert the cells into the format required by pymetis\n    adjacency_list = [c2c.getrow(i).indices for i in range(c2c.shape[0])]\n    # Call pymetis\n    part = pymetis.part_graph(num_part, adjacency=adjacency_list)\n\n    # The meaning of the first number returned by pymetis is not clear (poor\n    # documentation), only return the partitioning.\n    return np.array(part[1])",
  "def partition_structured(g, coarse_dims=None, num_part=None):\n    \"\"\"\n    Define a partitioning of a grid based on logical Cartesian indexing.\n\n    The grid should have a field cart_dims, describing the Cartesian dimensions\n    of the grid.\n\n    The coarse grid can be specified either by its Cartesian dimensions\n    (parameter coarse_dims), or by its total number of partitions (num_part).\n    In the latter case, a partitioning will be inferred from the fine-scale\n    Cartesian dimensions, in a way that gives roughly the same number of cells\n    in each direction.\n\n    Parameters:\n        g: core.grids.grid: To be partitioned. Only the cell_face attribute is\n            used\n        coarse_dims (np.array): Cartesian dimensions of the coarse grids.\n        num_part (int): Number of partitions.\n\n    Returns:\n        np.array (size:g.num_cells): Partition vector, one number in\n            [0, num_part) for each cell.\n\n    Raises:\n        Value error if both coarse_dims and num_part are None.\n\n    \"\"\"\n\n\n    if (coarse_dims is None) and (num_part is None):\n        raise ValueError('Either coarse dimensions or number of coarse cells \\\n                         must be specified')\n\n    nd = g.dim\n    fine_dims = g.cart_dims\n\n    if coarse_dims is None:\n        coarse_dims = determine_coarse_dimensions(num_part, fine_dims)\n\n    # Number of fine cells per coarse cell\n    fine_per_coarse = np.floor(fine_dims / coarse_dims)\n\n    # First define the coarse index for the individual dimensions.\n    ind = []\n    for i in range(nd):\n        # Fine indexes where the coarse index will increase\n        incr_ind = np.arange(0, fine_dims[i], fine_per_coarse[i], dtype='i')\n\n        # If the coarse dimension is not an exact multiple of the fine, there\n        # will be an extra cell in this dimension. Remove this.\n        if incr_ind.size > coarse_dims[i]:\n            incr_ind = incr_ind[:-1]\n\n        # Array for coarse index of fine cells\n        loc_ind = np.zeros(fine_dims[i])\n        # The index will increase by one\n        loc_ind[incr_ind] += 1\n        # A cumulative sum now gives the index, but subtract by one to be\n        # 0-offset\n        ind.append(np.cumsum(loc_ind) - 1)\n\n    # Then combine the indexes. In 2D meshgrid does the job, in 3D it turned\n    # out that some acrobatics was necessary to get the right ordering of the\n    # cells.\n    if nd == 2:\n        xi, yi = np.meshgrid(ind[0], ind[1])\n        # y-index jumps in steps of the number of coarse x-cells\n        glob_dims = (xi + yi * coarse_dims[0]).ravel('C')\n    elif nd == 3:\n        xi, yi, zi = np.meshgrid(ind[0], ind[1], ind[2])\n        # Combine indices, with appropriate jumps in y and z counting\n        glob_dims = (xi + yi * coarse_dims[0] + zi * np.prod(coarse_dims[:2]))\n        # This just happened to work, may be logical, but the documentanion of\n        # np.meshgrid was hard to comprehend.\n        glob_dims = np.swapaxes(np.swapaxes(glob_dims, 1, 2), 0, 1).ravel('C')\n\n    # Return an int\n    return glob_dims.astype('int')",
  "def partition_coordinates(g, num_coarse, check_connectivity=True):\n    \"\"\"\"\n    Brute force partitioning of a grid based on cell center coordinates.\n\n    The intention at the time of implementation is to provide a partitioning\n    for general grids that does not rely on METIS being available. However, if\n    METIS is available, partition_metis should be prefered.\n\n    The idea is to divide the domain into a coarse Cartesian grid, and then\n    assign a coarse partitioning based on the cell center coordinates.\n\n    It is not clear that this will create a connected coarse partitioning for\n    all grid cells (it may be possible to construct pathological examples,\n    probably involving non-convex cells). We optionally check for connectivity\n    and raise an error if this is not fulfilled.\n\n    The method assumes that the cells have a center, that is,\n    g.compute_geometry() has been called. If g does not have a field\n    cell_centers, compute_geometry() will be called.\n\n    Parameters:\n        g (core.grids.grid): Grid to be partitioned.\n        num_coarse (int): Target number of coarse cells. The real number of\n            coarse cells will be close, but not necessarily equal.\n        check_connectivity (boolean, optional): Check if the partitioning form\n            connected coarse grids. Defaults to True.\n\n    Returns:\n        np.array, size g.num_cells: Partition vector.\n\n    Raises:\n        ValueError if the partitioning is found to not form connected subgrids.\n\n    \"\"\"\n\n    # Compute geometry if necessary\n    if not hasattr(g, 'cell_centers'):\n        g.compute_geometry()\n\n    # Rough computation of the size of the Cartesian coarse grid: Determine the\n    # extension of the domain in each direction, transform into integer sizes,\n    # and use function to determine coarse dimensions.\n\n    # Use node coordinates to define the boxes\n    min_coord = np.min(g.nodes, axis=1)\n    max_coord = np.max(g.nodes, axis=1)\n\n    # Drop unused dimensions for 2d (and 1d) grids\n    min_coord = min_coord[:g.dim]\n    max_coord = max_coord[:g.dim]\n    # Cell centers, with the right number of dimensions\n    cc = g.cell_centers[:g.dim]\n\n    delta = max_coord - min_coord\n\n    # Estimate of the number of coarse Cartesian cells in each dimensions:\n    # Distribute the target number over all dimensions. Then multiply with\n    # relative distances.\n    # Use ceil to round up, and thus avoid zeros. This may not be perfect, but\n    # it should be robust.\n    delta_int = np.ceil(np.power(num_coarse, 1/g.dim) * delta\\\n                        / np.min(delta)).astype('int')\n\n    coarse_dims = determine_coarse_dimensions(num_coarse, delta_int)\n\n    # Effective number of coarse cells, should be close to num_coarse\n    nc = coarse_dims.prod()\n\n    # Initialize partition vector\n    partition = -np.ones(g.num_cells)\n\n    # Grid resolution of coarse grid (roughly)\n    dx = delta / coarse_dims\n\n    #  Loop over all coarse cells, pick out cells that lies within the coarse\n    #  box\n    for i in range(nc):\n        ind = np.array(np.unravel_index(i, coarse_dims))\n        # Bounding coordinates\n        lower_coord = min_coord + dx * ind\n        upper_coord = min_coord + dx * (ind+1)\n        # Find cell centers within the box\n        hit = np.logical_and(cc >= lower_coord.reshape((-1, 1)),\n                             cc < upper_coord.reshape((-1, 1)))\n        # We have a hit if all coordinates are within the box\n        hit_ind = np.argwhere(np.all(hit, axis=0)).ravel(order='C')\n        partition[hit_ind] = i\n\n    # Sanity check, all cells should have received a coarse index\n    assert partition.min() >= 0\n\n    if check_connectivity:\n        for p in np.unique(partition):\n            p_ind = np.squeeze(np.argwhere(p == partition))\n            if not grid_is_connected(g, p_ind):\n                raise ValueError('Partitioning led to unconnected subgrids')\n\n    return partition",
  "def partition(g, num_coarse):\n    \"\"\"\n    Wrapper for partition methods, tries to apply best possible algorithm.\n\n    The method will first try to use METIS; if this is not available (or fails\n    otherwise), the partition_structured will be applied if the grid is\n    Cartesian. The last resort is partitioning based on coordinates.\n\n    See the methods partition_metis(), partition_structured() and\n    partition_coordinates() for further details.\n\n    Parameters:\n        g (core.grids.grid): Grid to be partitioned.\n        num_coarse (int): Target number of coarse cells.\n\n    Returns:\n        np.ndarray (int), size g.num_cells: Partition vector.\n\n    \"\"\"\n    try:\n        # Apparently, this will throw a KeyError unless pymetis has been\n        # successfully imported. This is does not look elegant, but it should\n        # work.\n        sys.modules['pymetis']\n        # If we have made it this far, we can run pymetis.\n        return partition_metis(g, num_coarse)\n    except KeyError:\n        if isinstance(g, structured.TensorGrid):\n            return partition_structured(g, num_part=num_coarse)\n        else:\n            return partition_coordinates(g, num_coarse)",
  "def determine_coarse_dimensions(target, fine_size):\n    \"\"\"\n    For a logically Cartesian grid determine a coarse partitioning based on a\n    target number of coarse cells.\n\n    The target size in general will not be a product of the possible grid\n    dimensions (it may be a prime, or it may be outside the bounds [1,\n    fine_size]. For concreteness, we seek to have roughly the same number of\n    cells in each directions (given by the Nd-root of the target). If this\n    requires more coarse cells in a dimension than there are fine cells there,\n    the coarse size is set equal to the fine, and the remaining cells are\n    distributed to the other dimensions.\n\n    Parameters:\n        target (int): Target number of coarse cells.\n        fine_size (np.ndarray): Number of fine-scale cell in each dimension\n\n    Returns:\n        np.ndarray: Coarse dimension sizes.\n\n    Raises:\n        ValueError if the while-loop runs more iterations than the number of\n            dimensions. This should not happen, in practice it means there is\n            bug.\n\n    \"\"\"\n\n    # The algorithm may be unstable for values outside the relevant bounds\n    target = np.maximum(1, np.minimum(target, fine_size.prod()))\n\n    nd = fine_size.size\n\n    # Array to store optimal values. Set the default value to one, this avoids\n    # interfering with target_now below.\n    optimum = np.ones(nd)\n    found = np.zeros(nd, dtype=np.bool)\n\n    # Counter for number of iterations. Should be unnecessary, remove when the\n    # code is trusted.\n    it_counter = 0\n\n    # Loop until all dimensions have been assigned a number of cells.\n    while not np.all(found) and it_counter <= nd:\n\n        it_counter += 1\n\n        # Remaining cells to deal with\n        target_now = target / optimum.prod()\n\n        # The simplest option is to take the Nd-root of the target number. This\n        # will generally not give integers, and we will therefore settle for the\n        # combination of rounding up and down which brings us closest to the\n        # target.\n        # There should be at least one coarse cell in each dimension, and at\n        # maximum as many cells as on the fine scale.\n        s_num = np.power(target_now, 1/(nd - found.sum()))\n        s_low = np.maximum(np.ones(nd), np.floor(s_num))\n        s_high = np.minimum(fine_size, np.ceil(s_num))\n\n        # Find dimensions where we have hit the ceiling\n        hit_ceil = np.squeeze(np.argwhere(np.logical_and(s_high == fine_size,\n                                                         ~found)))\n        # These have a bound, and will have their leeway removed\n        optimum[hit_ceil] = s_high[hit_ceil]\n        found[hit_ceil] = True\n\n        # If the ceiling was hit in some dimension, we have to start over\n        # again.\n        if np.any(hit_ceil):\n            continue\n\n        # There is no room for variations in found cells\n        s_low[found] = optimum[found]\n        s_high[found] = optimum[found]\n\n        # Array for storing the combinations.\n        coarse_size = np.vstack((s_low, s_high))\n        # The closest we've been to hit the target size. Set this to an\n        # unrealistically high number\n        dist = fine_size.prod()\n\n        # Loop over all combinations of rounding up and down, and test if we\n        # are closer to the target number.\n        for perm in permutations.multinary_permutations(2, nd):\n            size_now = np.zeros(nd)\n            for i, bit in enumerate(perm):\n                size_now[i] = coarse_size[bit, i]\n            if np.abs(target - size_now.prod()) < dist:\n                dist = target - size_now.prod()\n                optimum = size_now\n\n        # All dimensions that may hit the ceiling have been found, and we have\n        # the optimum solution. Declare victory and return home.\n        found[:] = True\n\n    if it_counter > nd:\n        raise ValueError('Maximum number of iterations exceeded. There is a \\\n                         bug somewhere.')\n\n    return optimum.astype('int')",
  "def extract_subgrid(g, c, sort=True):\n    \"\"\"\n    Extract a subgrid based on cell indices.\n\n    For simplicity the cell indices will be sorted before the subgrid is\n    extracted.\n\n    If the parent grid has geometry attributes (cell centers etc.) these are\n    copied to the child.\n\n    No checks are done on whether the cells form a connected area. The method\n    should work in theory for non-connected cells, the user will then have to\n    decide what to do with the resulting grid. This option has however not been\n    tested.\n\n    Parameters:\n        g (core.grids.Grid): Grid object, parent\n        c (np.array, dtype=int): Indices of cells to be extracted\n\n    Returns:\n        core.grids.Grid: Extracted subgrid. Will share (note, *not* copy)\n            geometric fileds with the parent grid. Also has an additional\n            field parent_cell_ind giving correspondance between parent and\n            child cells.\n        np.ndarray, dtype=int: Index of the extracted faces, ordered so that\n            element i is the global index of face i in the subgrid.\n        np.ndarray, dtype=int: Index of the extracted nodes, ordered so that\n            element i is the global index of node i in the subgrid.\n\n    \"\"\"\n    if sort:\n        c = np.sort(c)\n\n    # Local cell-face and face-node maps.\n    cf_sub, unique_faces = __extract_submatrix(g.cell_faces, c)\n    fn_sub, unique_nodes = __extract_submatrix(g.face_nodes, unique_faces)\n\n    # Append information on subgrid extraction to the new grid's history\n    name = list(g.name)\n    name.append('Extract subgrid')\n\n    # Construct new grid.\n    h = Grid(g.dim, g.nodes[:, unique_nodes], fn_sub, cf_sub, name)\n\n    # Copy geometric information if any\n    if hasattr(g, 'cell_centers'):\n        h.cell_centers = g.cell_centers[:, c]\n    if hasattr(g, 'cell_volumes'):\n        h.cell_volumes = g.cell_volumes[c]\n    if hasattr(g, 'face_centers'):\n        h.face_centers = g.face_centers[:, unique_faces]\n    if hasattr(g, 'face_normals'):\n        h.face_normals = g.face_normals[:, unique_faces]\n    if hasattr(g, 'face_areas'):\n        h.face_areas = g.face_areas[unique_faces]\n\n    h.parent_cell_ind = c\n\n    return h, unique_faces, unique_nodes",
  "def __extract_submatrix(mat, ind):\n    \"\"\" From a matrix, extract the column specified by ind. All zero columns\n    are stripped from the sub-matrix. Mappings from global to local row numbers\n    are also returned.\n    \"\"\"\n    sub_mat = mat[:, ind]\n    cols = sub_mat.indptr\n    data = sub_mat.data\n    unique_rows, rows_sub = np.unique(sub_mat.indices,\n                                      return_inverse=True)\n    return sps.csc_matrix((data, rows_sub, cols)), unique_rows",
  "def partition_grid(g, ind):\n    \"\"\"\n    Partition a grid into multiple subgrids based on an index set.\n\n    No tests are made on whether the resulting grids are connected.\n\n    Example:\n        >>> g = structured.CartGrid(np.array([10, 10]))\n        >>> p = partition_structured(g, num_part=4)\n        >>> subg, face_map, node_map = partition_grid(g, p)\n\n    Parameters:\n        g (core.grids.grid): Global grid to be partitioned\n        ind (np.array): Partition vector, one per cell. Should be 0-offset.\n\n    Returns:\n        list: List of grids, each element representing a grid.\n        list of np.arrays: Each element contains the global indices of the\n            local faces.\n        list of np.arrays: Each element contains the global indices of the\n            local nodes.\n    \"\"\"\n\n    sub_grid = []\n    face_map_list = []\n    node_map_list = []\n    for i in np.unique(ind):\n        ci = np.squeeze(np.argwhere(ind == i))\n        sg, fm, nm = extract_subgrid(g, ci)\n        sub_grid.append(sg)\n        face_map_list.append(fm)\n        node_map_list.append(nm)\n\n    return sub_grid, face_map_list, node_map_list",
  "def overlap(g, cell_ind, num_layers, criterion='node'):\n    \"\"\"\n    From a set of cell indices, find an extended set of cells that form an\n    overlap (in the domain decomposition sense).\n\n    The cell set is increased by including all cells that share at least one\n    node with the existing set. When multiple layers are asked for, this\n    process is repeated.\n\n    The definition of neighborship is specified by criterion. Possible options\n    are 'face' (each layer will add cells that share a face with the active\n    face set), or 'node' (each layer will add cells sharing a vertex with the\n    active set).\n\n    Parameters:\n        g (core.grids.grid): The grid; the cell-node relation will be used to\n            extend the cell set.\n        cell_ind (np.array): Cell indices, the initial cell set.\n        num_layers (int): Number of overlap layers.\n        criterion (str, optional): Which definition of neighborship to apply.\n            Should be either 'face' or 'node'. Default is 'node'.\n\n    Returns:\n        np.array: Indices of the extended cell set.\n\n    Examples:\n        >>> g = structured.CartGrid([5, 5])\n        >>> ci = np.array([0, 1, 5, 6])\n        >>> overlap(g, ci, 1)\n        array([ 0,  1,  2,  5,  6,  7, 10, 11, 12])\n\n    \"\"\"\n\n    # Boolean storage of cells in the active set; these are the ones that will\n    # be in the overlap\n    active_cells = np.zeros(g.num_cells, dtype=np.bool)\n    # Initialize by the specified cells\n    active_cells[cell_ind] = 1\n\n    if criterion.lower().strip() == 'node':\n        # Construct cell-node map, its transpose will be a node-cell map\n        cn = g.cell_nodes()\n\n        # Also introduce active nodes\n        active_nodes = np.zeros(g.num_nodes, dtype=np.bool)\n\n        # Gradually increase the size of the cell set\n        for _ in range(num_layers):\n            # Nodes are found via the mapping\n            active_nodes[np.squeeze(np.where((cn * active_cells) > 0))] = 1\n            # Map back to new cells\n            ci_new = np.squeeze(np.where((cn.transpose() * active_nodes) > 0))\n            # Activate new cells.\n            active_cells[ci_new] = 1\n\n    elif criterion().lower().strip() == 'face':\n        # Create a version of g.cell_faces with only positive values for\n        # connections, e.g. let go of the divergence property\n        cf = g.cell_faces\n        # This avoids overwriting data in cell_faces.\n        data = np.ones_like(cf.data)\n        cf = sps.csc_matrix((data, cf.indices, cf.indptr))\n\n        active_faces = np.zeros(g.num_faces, dtype=np.bool)\n\n        # Gradually increase the size of the cell set\n        for _ in range(num_layers):\n            # All faces adjacent to an active cell\n            active_faces[np.squeeze(np.where((cf * active_cells) > 0))] = 1\n            # Map back to active cells, including that on the other side of the\n            # newly found faces\n            ci_new = np.squeeze(np.where((cf.transpose() * active_faces) > 0))\n            # Activate new cells\n            active_cells[ci_new] = 1\n\n    # Sort the output, this should not be a disadvantage\n    return np.sort(np.squeeze(np.argwhere(active_cells > 0)))",
  "def grid_is_connected(g, cell_ind=None):\n    \"\"\"\n    Check if a grid is fully connected, as defined by its cell_connection_map().\n\n    The function is intended used in one of two ways:\n        1) To test if a subgrid will be connected before it is extracted. In\n        this case, the cells to be tested is specified by cell_ind.\n        2) To check if an existing grid is composed of a single component. In\n        this case, all cells are should be included in the analyzis.\n\n    Parameters:\n        g (core.grids.grid): Grid to be tested. Only its cell_faces map is\n            used.\n        cell_ind (np.array): Index of cells to be included when looking for\n            connections. Defaults to all cells in the grid.\n\n    Returns:\n        boolean: True if the grid is connected.\n        list of np.arrays: Each list item contains a np.array with cell indices\n            of a connected component.\n\n    Examples:\n        >>> g = structured.CartGrid(np.array([2, 2]))\n        >>> p = np.array([0, 1])\n        >>> is_con, l = grid_is_connected(g, p)\n        >>> is_con\n        True\n\n        >>> g = structured.CartGrid(np.array([2, 2]))\n        >>> p = np.array([0, 3])\n        >>> is_con, l = grid_is_connected(g, p)\n        >>> is_con\n        False\n\n    \"\"\"\n\n    # If no cell indices are specified, we use them all.\n    if cell_ind is None:\n        cell_ind = np.arange(g.num_cells)\n\n    # Get connection map for the full grid\n    c2c = g.cell_connection_map()\n\n    # Extract submatrix of the active cell set.\n    # To slice the sparse matrix, we first convert to row storage, slice rows,\n    # and then convert to columns and slice those as well.\n    c2c = c2c.tocsr()[cell_ind, :].tocsc()[:, cell_ind]\n\n    # Represent the connections as a networkx graph and check for connectivity\n    graph = networkx.from_scipy_sparse_matrix(c2c)\n    is_connected = networkx.is_connected(graph)\n\n    # Get the connected components of the network.\n    # networkx gives an generator that produce sets of node indices. Use this\n    # to define a list of numpy arrays.\n    component_generator = networkx.connected_components(graph)\n    components = [np.array(list(i)) for i in component_generator]\n\n    return is_connected, components",
  "def duplicate_without_dimension(gb, dim):\n    \"\"\"\n    Remove all the nodes of dimension dim and add new edges between their\n    neighbors by calls to remove_node.\n\n    \"\"\"\n    gb1 = gb.copy()\n    for g in gb1.grids_of_dimension(dim):\n        remove_node(gb1, g)\n    return gb1",
  "def remove_node(gb, node):\n    \"\"\"\n    Remove the node (and the edges it partakes in) and add new direct\n    connections (gb edges) between each of the neighbor pairs. A 0d node\n    with n_neighbors gives rise to 1 + 2 + ... + n_neighbors-1 new edges.\n\n    \"\"\"\n    neighbors = gb.node_neighbors(node)\n    n_neighbors = len(neighbors)\n    for i in range(n_neighbors - 1):\n        n1 = neighbors[i]\n        for j in range(i + 1, n_neighbors):\n            n2 = neighbors[j]\n            face_faces = find_shared_face(n1, n2, node, gb)\n\n            gb.add_edge([n1, n2], face_faces)\n\n    # Remove the node and update the ordering of the remaining nodes\n    node_number = gb.node_prop(node, 'node_number')\n    gb.remove_node(node)\n    gb.update_node_ordering(node_number)",
  "def find_shared_face(n1, n2, node, gb):\n    \"\"\"\n    Given two 1d grids meeting at a 0d node (to be removed), find which two\n    faces meet at the intersection (one from each grid). Returns the sparse\n    matrix face_faces, the 1d-1d equivalent of the face_cells matrix.\n    \"\"\"\n    # Sort nodes according to node_number\n    n1, n2 = gb.sorted_nodes_of_edge([n1, n2])\n\n    # Identify the faces connecting the neighbors to the grid to be removed\n    fc1 = gb.edge_props([n1, node])\n    fc2 = gb.edge_props([n2, node])\n    _, face_number_1, _ = sps.find(fc1['face_cells'])\n    _, face_number_2, _ = sps.find(fc2['face_cells'])\n\n    # The lower dim. node (corresponding to the first dimension, cells,\n    # in face_cells) is first in gb.sorted_nodes_of_edge. To be consistent\n    # with this, the grid corresponding to the first dimension of face_faces\n    # should be the first grid of the node sorting. Connect the two remaining\n    # grids through the face_faces matrix, to be placed as a face_cells\n    # substitute.\n    face_faces = sps.csc_matrix(\n        (np.array([True]), (face_number_1, face_number_2)),\n        (n1.num_faces, n2.num_faces))\n\n    return face_faces",
  "class TensorGrid(Grid):\n    \"\"\"Representation of grid formed by a tensor product of line point\n    distributions.\n\n    For information on attributes and methods, see the documentation of the\n    parent Grid class.\n\n    \"\"\"\n\n    def __init__(self, x, y=None, z=None, name=None):\n        \"\"\"\n        Constructor for 1D or 2D or 3D tensor grid\n\n        The resulting grid is 1D or 2D or 3D, depending of the number of\n        coordinate lines are provided\n\n        Parameters\n            x (np.ndarray): Node coordinates in x-direction\n            y (np.ndarray): Node coordinates in y-direction. Defaults to\n                None, in which case the grid is 1D.\n            z (np.ndarray): Node coordinates in z-direction. Defaults to\n                None, in which case the grid is 2D.\n            name (str): Name of grid, passed to super constructor\n        \"\"\"\n        if name is None:\n            name = 'TensorGrid'\n\n        if y is None:\n            nodes, face_nodes, cell_faces = self._create_1d_grid(x)\n            self.cart_dims = np.array([x.size - 1])\n            super(TensorGrid, self).__init__(1, nodes, face_nodes,\n                                             cell_faces, name)\n        elif z is None:\n            nodes, face_nodes, cell_faces = self._create_2d_grid(x, y)\n            self.cart_dims = np.array([x.size, y.size]) - 1\n            super(TensorGrid, self).__init__(2, nodes, face_nodes,\n                                             cell_faces, name)\n        else:\n            nodes, face_nodes, cell_faces = self._create_3d_grid(x, y, z)\n            self.cart_dims = np.array([x.size, y.size, z.size]) - 1\n            super(TensorGrid, self).__init__(3, nodes, face_nodes,\n                                             cell_faces, name)\n\n    def _create_1d_grid(self, nodes_x):\n        \"\"\"\n        Compute grid topology for 1D grids.\n\n        This is really a part of the constructor, but put it here to improve\n        readability. Not sure if that is the right choice..\n\n        \"\"\"\n\n        num_x = nodes_x.size - 1\n\n        num_cells = num_x\n        num_nodes = num_x + 1\n        num_faces = num_x + 1\n\n        nodes = np.vstack((nodes_x, np.zeros(nodes_x.size),\n                           np.zeros(nodes_x.size)))\n\n        # Face nodes\n        indptr = np.arange(num_faces+1)\n        face_nodes = np.arange(num_faces)\n        data = np.ones(face_nodes.shape, dtype=bool)\n        face_nodes = sps.csc_matrix((data, face_nodes, indptr),\n                                    shape=(num_nodes, num_faces))\n\n        # Cell faces\n        face_array = np.arange(num_faces)\n        cell_faces = np.vstack((face_array[:-1],\n                                face_array[1:])).ravel(order='F')\n\n        num_faces_per_cell = 2\n        indptr = np.append(np.arange(0, num_faces_per_cell*num_cells,\n                                     num_faces_per_cell),\n                           num_faces_per_cell * num_cells)\n        data = np.empty(cell_faces.size)\n        data[::2] = -1\n        data[1::2] = 1\n\n        cell_faces = sps.csc_matrix((data, cell_faces, indptr),\n                                    shape=(num_faces, num_cells))\n        return nodes, face_nodes, cell_faces\n\n    def _create_2d_grid(self, nodes_x, nodes_y):\n        \"\"\"\n        Compute grid topology for 2D grids.\n\n        This is really a part of the constructor, but put it here to improve\n        readability. Not sure if that is the right choice..\n\n        \"\"\"\n\n        num_x = nodes_x.size - 1\n        num_y = nodes_y.size - 1\n\n        num_cells = num_x * num_y\n        num_nodes = (num_x + 1) * (num_y + 1)\n        num_faces_x = (num_x + 1) * num_y\n        num_faces_y = num_x * (num_y + 1)\n        num_faces = num_faces_x + num_faces_y\n\n        num_cells = num_cells\n        num_faces = num_faces\n        num_nodes = num_nodes\n\n        x_coord, y_coord = sp.meshgrid(nodes_x, nodes_y)\n\n        nodes = np.vstack((x_coord.flatten(), y_coord.flatten(),\n                           np.zeros(x_coord.size)))\n\n        # Face nodes\n        node_array = np.arange(0, num_nodes).reshape(num_y+1, num_x+1)\n        fn1 = node_array[:-1, ::].ravel(order='C')\n        fn2 = node_array[1:, ::].ravel(order='C')\n        face_nodes_x = np.vstack((fn1, fn2)).ravel(order='F')\n\n        fn1 = node_array[::, :-1].ravel(order='C')\n        fn2 = node_array[::, 1:].ravel(order='C')\n        face_nodes_y = np.vstack((fn1, fn2)).ravel(order='F')\n\n        num_nodes_per_face = 2\n        indptr = np.append(np.arange(0, num_nodes_per_face*num_faces,\n                                     num_nodes_per_face),\n                           num_nodes_per_face * num_faces)\n        face_nodes = np.hstack((face_nodes_x, face_nodes_y))\n        data = np.ones(face_nodes.shape, dtype=bool)\n        face_nodes = sps.csc_matrix((data, face_nodes, indptr),\n                                    shape=(num_nodes, num_faces))\n\n        # Cell faces\n        face_x = np.arange(num_faces_x).reshape(num_y, num_x+1)\n        face_y = num_faces_x + np.arange(num_faces_y).reshape(num_y+1, num_x)\n\n        face_west = face_x[::, :-1].ravel(order='C')\n        face_east = face_x[::, 1:].ravel(order='C')\n        face_south = face_y[:-1, ::].ravel(order='C')\n        face_north = face_y[1:, ::].ravel(order='C')\n\n        cell_faces = np.vstack((face_west, face_east,\n                                face_south, face_north)).ravel(order='F')\n\n        num_faces_per_cell = 4\n        indptr = np.append(np.arange(0, num_faces_per_cell*num_cells,\n                                     num_faces_per_cell),\n                           num_faces_per_cell * num_cells)\n        data = np.vstack((-np.ones(face_west.size), np.ones(face_east.size),\n                          -np.ones(face_south.size),\n                          np.ones(face_north.size))).ravel(order='F')\n        cell_faces = sps.csc_matrix((data, cell_faces, indptr),\n                                    shape=(num_faces, num_cells))\n        return nodes, face_nodes, cell_faces\n\n    def _create_3d_grid(self, nodes_x, nodes_y, nodes_z):\n\n        num_x = nodes_x.size - 1\n        num_y = nodes_y.size - 1\n        num_z = nodes_z.size - 1\n\n        num_cells = num_x * num_y * num_z\n        num_nodes = (num_x + 1) * (num_y + 1) * (num_z + 1)\n        num_faces_x = (num_x + 1) * num_y * num_z\n        num_faces_y = num_x * (num_y + 1) * num_z\n        num_faces_z = num_x * num_y * (num_z + 1)\n        num_faces = num_faces_x + num_faces_y + num_faces_z\n\n        num_cells = num_cells\n        num_faces = num_faces\n        num_nodes = num_nodes\n\n        x_coord, y_coord, z_coord = np.meshgrid(nodes_x, nodes_y, nodes_z)\n        # This rearangement turned out to work. Not the first thing I tried..\n        x_coord = np.swapaxes(x_coord, 1, 0).ravel(order='F')\n        y_coord = np.swapaxes(y_coord, 1, 0).ravel(order='F')\n        z_coord = np.swapaxes(z_coord, 1, 0).ravel(order='F')\n\n        nodes = np.vstack((x_coord, y_coord, z_coord))\n\n        # Face nodes\n        node_array = np.arange(num_nodes).reshape(num_x + 1, num_y + 1,\n                                                  num_z + 1, order='F')\n\n        # Define face-node relations for all x-faces.\n        # The code here is a bit different from the corresponding part in\n        # 2d, I did learn some tricks in python the past month\n        fn1 = node_array[:, :-1, :-1].ravel(order='F')\n        fn2 = node_array[:, 1:, :-1].ravel(order='F')\n        fn3 = node_array[:, 1:, 1:].ravel(order='F')\n        fn4 = node_array[:, :-1, 1:].ravel(order='F')\n        face_nodes_x = np.vstack((fn1, fn2, fn3, fn4)).ravel(order='F')\n\n        # Define face-node relations for all y-faces\n        fn1 = node_array[:-1:, :, :-1].ravel(order='F')\n        fn2 = node_array[:-1, :, 1:].ravel(order='F')\n        fn3 = node_array[1:, :, 1:].ravel(order='F')\n        fn4 = node_array[1:, :, :-1].ravel(order='F')\n        face_nodes_y = np.vstack((fn1, fn2, fn3, fn4)).ravel(order='F')\n\n        # Define face-node relations for all y-faces\n        fn1 = node_array[:-1:, :-1, :].ravel(order='F')\n        fn2 = node_array[1:, :-1, :].ravel(order='F')\n        fn3 = node_array[1:, 1:, :].ravel(order='F')\n        fn4 = node_array[:-1, 1:, :].ravel(order='F')\n        face_nodes_z = np.vstack((fn1, fn2, fn3, fn4)).ravel(order='F')\n\n        num_nodes_per_face = 4\n        indptr = np.append(np.arange(0, num_nodes_per_face * num_faces,\n                                     num_nodes_per_face),\n                           num_nodes_per_face * num_faces)\n        face_nodes = np.hstack((face_nodes_x, face_nodes_y, face_nodes_z))\n        data = np.ones(face_nodes.shape, dtype=bool)\n        face_nodes = sps.csc_matrix((data, face_nodes, indptr),\n                                    shape=(num_nodes, num_faces))\n\n        # Cell faces\n        face_x = np.arange(num_faces_x).reshape(num_x + 1, num_y, num_z,\n                                                order='F')\n        face_y = num_faces_x + np.arange(num_faces_y).reshape(num_x, num_y + 1,\n                                                              num_z, order='F')\n        face_z = num_faces_x + num_faces_y + \\\n                 np.arange(num_faces_z).reshape(num_x, num_y, num_z + 1,\n                                                order='F')\n\n        face_west = face_x[:-1, :, :].ravel(order='F')\n        face_east = face_x[1:, :, :].ravel(order='F')\n        face_south = face_y[:, :-1, :].ravel(order='F')\n        face_north = face_y[:, 1:, :].ravel(order='F')\n        face_top = face_z[:, :, :-1].ravel(order='F')\n        face_bottom = face_z[:, :, 1:].ravel(order='F')\n\n        cell_faces = np.vstack((face_west, face_east,\n                                face_south, face_north, face_top,\n                                face_bottom)).ravel(order='F')\n\n        num_faces_per_cell = 6\n        indptr = np.append(np.arange(0, num_faces_per_cell * num_cells,\n                                     num_faces_per_cell),\n                           num_faces_per_cell * num_cells)\n        data = np.vstack((-np.ones(num_cells), np.ones(num_cells),\n                          -np.ones(num_cells), np.ones(num_cells),\n                          -np.ones(num_cells),\n                          np.ones(num_cells))).ravel(order='F')\n        cell_faces = sps.csc_matrix((data, cell_faces, indptr),\n                                    shape=(num_faces, num_cells))\n        return nodes, face_nodes, cell_faces",
  "class CartGrid(TensorGrid):\n    \"\"\"Representation of a 2D or 3D Cartesian grid.\n\n    For information on attributes and methods, see the documentation of the\n    parent Grid class.\n\n    \"\"\"\n\n    def __init__(self, nx, physdims=None):\n        \"\"\"\n        Constructor for Cartesian grid\n\n        Parameters\n        ----------\n        nx (np.ndarray): Number of cells in each direction. Should be 2D or 3D\n        physdims (np.ndarray): Physical dimensions in each direction.\n            Defaults to same as nx, that is, cells of unit size.\n        \"\"\"\n\n#        nx = nx.astype(np.int)\n\n        if physdims is None:\n            physdims = nx\n\n        dims = np.asarray(nx).shape\n        assert dims == np.asarray(physdims).shape\n        name = 'CartGrid'\n\n        # Create point distribution, and then leave construction to\n        # TensorGrid constructor\n        if dims is (): # dirty trick\n            nodes_x = np.linspace(0, physdims, nx+1)\n            super(self.__class__, self).__init__(nodes_x, name=name)\n        elif dims[0] == 2:\n            nodes_x = np.linspace(0, physdims[0], nx[0]+1)\n            nodes_y = np.linspace(0, physdims[1], nx[1]+1)\n            super(self.__class__, self).__init__(nodes_x, nodes_y, name=name)\n        elif dims[0] == 3:\n            nodes_x = np.linspace(0, physdims[0], nx[0] + 1)\n            nodes_y = np.linspace(0, physdims[1], nx[1] + 1)\n            nodes_z = np.linspace(0, physdims[2], nx[2] + 1)\n            super(self.__class__, self).__init__(nodes_x, nodes_y, nodes_z,\n                                                 name=name)\n        else:\n            raise ValueError('Cartesian grid only implemented for up to three \\\n            dimensions')",
  "def __init__(self, x, y=None, z=None, name=None):\n        \"\"\"\n        Constructor for 1D or 2D or 3D tensor grid\n\n        The resulting grid is 1D or 2D or 3D, depending of the number of\n        coordinate lines are provided\n\n        Parameters\n            x (np.ndarray): Node coordinates in x-direction\n            y (np.ndarray): Node coordinates in y-direction. Defaults to\n                None, in which case the grid is 1D.\n            z (np.ndarray): Node coordinates in z-direction. Defaults to\n                None, in which case the grid is 2D.\n            name (str): Name of grid, passed to super constructor\n        \"\"\"\n        if name is None:\n            name = 'TensorGrid'\n\n        if y is None:\n            nodes, face_nodes, cell_faces = self._create_1d_grid(x)\n            self.cart_dims = np.array([x.size - 1])\n            super(TensorGrid, self).__init__(1, nodes, face_nodes,\n                                             cell_faces, name)\n        elif z is None:\n            nodes, face_nodes, cell_faces = self._create_2d_grid(x, y)\n            self.cart_dims = np.array([x.size, y.size]) - 1\n            super(TensorGrid, self).__init__(2, nodes, face_nodes,\n                                             cell_faces, name)\n        else:\n            nodes, face_nodes, cell_faces = self._create_3d_grid(x, y, z)\n            self.cart_dims = np.array([x.size, y.size, z.size]) - 1\n            super(TensorGrid, self).__init__(3, nodes, face_nodes,\n                                             cell_faces, name)",
  "def _create_1d_grid(self, nodes_x):\n        \"\"\"\n        Compute grid topology for 1D grids.\n\n        This is really a part of the constructor, but put it here to improve\n        readability. Not sure if that is the right choice..\n\n        \"\"\"\n\n        num_x = nodes_x.size - 1\n\n        num_cells = num_x\n        num_nodes = num_x + 1\n        num_faces = num_x + 1\n\n        nodes = np.vstack((nodes_x, np.zeros(nodes_x.size),\n                           np.zeros(nodes_x.size)))\n\n        # Face nodes\n        indptr = np.arange(num_faces+1)\n        face_nodes = np.arange(num_faces)\n        data = np.ones(face_nodes.shape, dtype=bool)\n        face_nodes = sps.csc_matrix((data, face_nodes, indptr),\n                                    shape=(num_nodes, num_faces))\n\n        # Cell faces\n        face_array = np.arange(num_faces)\n        cell_faces = np.vstack((face_array[:-1],\n                                face_array[1:])).ravel(order='F')\n\n        num_faces_per_cell = 2\n        indptr = np.append(np.arange(0, num_faces_per_cell*num_cells,\n                                     num_faces_per_cell),\n                           num_faces_per_cell * num_cells)\n        data = np.empty(cell_faces.size)\n        data[::2] = -1\n        data[1::2] = 1\n\n        cell_faces = sps.csc_matrix((data, cell_faces, indptr),\n                                    shape=(num_faces, num_cells))\n        return nodes, face_nodes, cell_faces",
  "def _create_2d_grid(self, nodes_x, nodes_y):\n        \"\"\"\n        Compute grid topology for 2D grids.\n\n        This is really a part of the constructor, but put it here to improve\n        readability. Not sure if that is the right choice..\n\n        \"\"\"\n\n        num_x = nodes_x.size - 1\n        num_y = nodes_y.size - 1\n\n        num_cells = num_x * num_y\n        num_nodes = (num_x + 1) * (num_y + 1)\n        num_faces_x = (num_x + 1) * num_y\n        num_faces_y = num_x * (num_y + 1)\n        num_faces = num_faces_x + num_faces_y\n\n        num_cells = num_cells\n        num_faces = num_faces\n        num_nodes = num_nodes\n\n        x_coord, y_coord = sp.meshgrid(nodes_x, nodes_y)\n\n        nodes = np.vstack((x_coord.flatten(), y_coord.flatten(),\n                           np.zeros(x_coord.size)))\n\n        # Face nodes\n        node_array = np.arange(0, num_nodes).reshape(num_y+1, num_x+1)\n        fn1 = node_array[:-1, ::].ravel(order='C')\n        fn2 = node_array[1:, ::].ravel(order='C')\n        face_nodes_x = np.vstack((fn1, fn2)).ravel(order='F')\n\n        fn1 = node_array[::, :-1].ravel(order='C')\n        fn2 = node_array[::, 1:].ravel(order='C')\n        face_nodes_y = np.vstack((fn1, fn2)).ravel(order='F')\n\n        num_nodes_per_face = 2\n        indptr = np.append(np.arange(0, num_nodes_per_face*num_faces,\n                                     num_nodes_per_face),\n                           num_nodes_per_face * num_faces)\n        face_nodes = np.hstack((face_nodes_x, face_nodes_y))\n        data = np.ones(face_nodes.shape, dtype=bool)\n        face_nodes = sps.csc_matrix((data, face_nodes, indptr),\n                                    shape=(num_nodes, num_faces))\n\n        # Cell faces\n        face_x = np.arange(num_faces_x).reshape(num_y, num_x+1)\n        face_y = num_faces_x + np.arange(num_faces_y).reshape(num_y+1, num_x)\n\n        face_west = face_x[::, :-1].ravel(order='C')\n        face_east = face_x[::, 1:].ravel(order='C')\n        face_south = face_y[:-1, ::].ravel(order='C')\n        face_north = face_y[1:, ::].ravel(order='C')\n\n        cell_faces = np.vstack((face_west, face_east,\n                                face_south, face_north)).ravel(order='F')\n\n        num_faces_per_cell = 4\n        indptr = np.append(np.arange(0, num_faces_per_cell*num_cells,\n                                     num_faces_per_cell),\n                           num_faces_per_cell * num_cells)\n        data = np.vstack((-np.ones(face_west.size), np.ones(face_east.size),\n                          -np.ones(face_south.size),\n                          np.ones(face_north.size))).ravel(order='F')\n        cell_faces = sps.csc_matrix((data, cell_faces, indptr),\n                                    shape=(num_faces, num_cells))\n        return nodes, face_nodes, cell_faces",
  "def _create_3d_grid(self, nodes_x, nodes_y, nodes_z):\n\n        num_x = nodes_x.size - 1\n        num_y = nodes_y.size - 1\n        num_z = nodes_z.size - 1\n\n        num_cells = num_x * num_y * num_z\n        num_nodes = (num_x + 1) * (num_y + 1) * (num_z + 1)\n        num_faces_x = (num_x + 1) * num_y * num_z\n        num_faces_y = num_x * (num_y + 1) * num_z\n        num_faces_z = num_x * num_y * (num_z + 1)\n        num_faces = num_faces_x + num_faces_y + num_faces_z\n\n        num_cells = num_cells\n        num_faces = num_faces\n        num_nodes = num_nodes\n\n        x_coord, y_coord, z_coord = np.meshgrid(nodes_x, nodes_y, nodes_z)\n        # This rearangement turned out to work. Not the first thing I tried..\n        x_coord = np.swapaxes(x_coord, 1, 0).ravel(order='F')\n        y_coord = np.swapaxes(y_coord, 1, 0).ravel(order='F')\n        z_coord = np.swapaxes(z_coord, 1, 0).ravel(order='F')\n\n        nodes = np.vstack((x_coord, y_coord, z_coord))\n\n        # Face nodes\n        node_array = np.arange(num_nodes).reshape(num_x + 1, num_y + 1,\n                                                  num_z + 1, order='F')\n\n        # Define face-node relations for all x-faces.\n        # The code here is a bit different from the corresponding part in\n        # 2d, I did learn some tricks in python the past month\n        fn1 = node_array[:, :-1, :-1].ravel(order='F')\n        fn2 = node_array[:, 1:, :-1].ravel(order='F')\n        fn3 = node_array[:, 1:, 1:].ravel(order='F')\n        fn4 = node_array[:, :-1, 1:].ravel(order='F')\n        face_nodes_x = np.vstack((fn1, fn2, fn3, fn4)).ravel(order='F')\n\n        # Define face-node relations for all y-faces\n        fn1 = node_array[:-1:, :, :-1].ravel(order='F')\n        fn2 = node_array[:-1, :, 1:].ravel(order='F')\n        fn3 = node_array[1:, :, 1:].ravel(order='F')\n        fn4 = node_array[1:, :, :-1].ravel(order='F')\n        face_nodes_y = np.vstack((fn1, fn2, fn3, fn4)).ravel(order='F')\n\n        # Define face-node relations for all y-faces\n        fn1 = node_array[:-1:, :-1, :].ravel(order='F')\n        fn2 = node_array[1:, :-1, :].ravel(order='F')\n        fn3 = node_array[1:, 1:, :].ravel(order='F')\n        fn4 = node_array[:-1, 1:, :].ravel(order='F')\n        face_nodes_z = np.vstack((fn1, fn2, fn3, fn4)).ravel(order='F')\n\n        num_nodes_per_face = 4\n        indptr = np.append(np.arange(0, num_nodes_per_face * num_faces,\n                                     num_nodes_per_face),\n                           num_nodes_per_face * num_faces)\n        face_nodes = np.hstack((face_nodes_x, face_nodes_y, face_nodes_z))\n        data = np.ones(face_nodes.shape, dtype=bool)\n        face_nodes = sps.csc_matrix((data, face_nodes, indptr),\n                                    shape=(num_nodes, num_faces))\n\n        # Cell faces\n        face_x = np.arange(num_faces_x).reshape(num_x + 1, num_y, num_z,\n                                                order='F')\n        face_y = num_faces_x + np.arange(num_faces_y).reshape(num_x, num_y + 1,\n                                                              num_z, order='F')\n        face_z = num_faces_x + num_faces_y + \\\n                 np.arange(num_faces_z).reshape(num_x, num_y, num_z + 1,\n                                                order='F')\n\n        face_west = face_x[:-1, :, :].ravel(order='F')\n        face_east = face_x[1:, :, :].ravel(order='F')\n        face_south = face_y[:, :-1, :].ravel(order='F')\n        face_north = face_y[:, 1:, :].ravel(order='F')\n        face_top = face_z[:, :, :-1].ravel(order='F')\n        face_bottom = face_z[:, :, 1:].ravel(order='F')\n\n        cell_faces = np.vstack((face_west, face_east,\n                                face_south, face_north, face_top,\n                                face_bottom)).ravel(order='F')\n\n        num_faces_per_cell = 6\n        indptr = np.append(np.arange(0, num_faces_per_cell * num_cells,\n                                     num_faces_per_cell),\n                           num_faces_per_cell * num_cells)\n        data = np.vstack((-np.ones(num_cells), np.ones(num_cells),\n                          -np.ones(num_cells), np.ones(num_cells),\n                          -np.ones(num_cells),\n                          np.ones(num_cells))).ravel(order='F')\n        cell_faces = sps.csc_matrix((data, cell_faces, indptr),\n                                    shape=(num_faces, num_cells))\n        return nodes, face_nodes, cell_faces",
  "def __init__(self, nx, physdims=None):\n        \"\"\"\n        Constructor for Cartesian grid\n\n        Parameters\n        ----------\n        nx (np.ndarray): Number of cells in each direction. Should be 2D or 3D\n        physdims (np.ndarray): Physical dimensions in each direction.\n            Defaults to same as nx, that is, cells of unit size.\n        \"\"\"\n\n#        nx = nx.astype(np.int)\n\n        if physdims is None:\n            physdims = nx\n\n        dims = np.asarray(nx).shape\n        assert dims == np.asarray(physdims).shape\n        name = 'CartGrid'\n\n        # Create point distribution, and then leave construction to\n        # TensorGrid constructor\n        if dims is (): # dirty trick\n            nodes_x = np.linspace(0, physdims, nx+1)\n            super(self.__class__, self).__init__(nodes_x, name=name)\n        elif dims[0] == 2:\n            nodes_x = np.linspace(0, physdims[0], nx[0]+1)\n            nodes_y = np.linspace(0, physdims[1], nx[1]+1)\n            super(self.__class__, self).__init__(nodes_x, nodes_y, name=name)\n        elif dims[0] == 3:\n            nodes_x = np.linspace(0, physdims[0], nx[0] + 1)\n            nodes_y = np.linspace(0, physdims[1], nx[1] + 1)\n            nodes_z = np.linspace(0, physdims[2], nx[2] + 1)\n            super(self.__class__, self).__init__(nodes_x, nodes_y, nodes_z,\n                                                 name=name)\n        else:\n            raise ValueError('Cartesian grid only implemented for up to three \\\n            dimensions')",
  "def grid(g):\n    \"\"\" Sanity check for the grid. General method which apply the following:\n    - check if the face normals are actually normal to the faces\n    - check if a bidimensional grid is planar\n\n    Args:\n        g (grid): Grid, or a subclass, with geometry fields computed.\n\n    How to use:\n        import core.grids.check as check\n        check.grid(g)\n    \"\"\"\n\n    if g.dim == 1:\n        assert cg.is_collinear(g.nodes)\n        face_normals_1d(g)\n\n    if g.dim == 2:\n        assert cg.is_planar(g.nodes)\n\n    if g.dim != 1:\n        face_normals(g)",
  "def face_normals(g):\n    \"\"\" Check if the face normals are actually normal to the faces.\n\n    Args:\n        g (grid): Grid, or a subclass, with geometry fields computed.\n    \"\"\"\n\n    nodes, faces, _ = sps.find(g.face_nodes)\n\n    for f in np.arange(g.num_faces):\n        loc = slice(g.face_nodes.indptr[f], g.face_nodes.indptr[f+1])\n        normal = g.face_normals[:, f]\n        tangent = cg.compute_tangent(g.nodes[:, nodes[loc]])\n        assert np.isclose(np.dot(normal, tangent), 0)",
  "def face_normals_1d(g):\n    \"\"\" Check if the face normals are actually normal to the faces, 1d case.\n\n    Args:\n        g (grid): 1D grid, or a subclass, with geometry fields computed.\n    \"\"\"\n\n    assert g.dim == 1\n    a_normal = cg.compute_a_normal_1d(g.nodes)\n    for f in np.arange(g.num_faces):\n        assert np.isclose(np.dot(g.face_normals[:, f], a_normal), 0)",
  "class Grid(object):\n    \"\"\"\n    Parent class for all grids.\n\n    The grid stores topological information, as well as geometric\n    information (after a call to self.compute_geometry().\n\n    As of yet, there is no structure for tags (face or cell) is the grid.\n    This will be introduced later.\n\n    Attributes:\n        Comes in two classes. Topologogical information, defined at\n        construction time:\n\n        dim (int): dimension. Should be 0 or 1 or 2 or 3\n        nodes (np.ndarray): node coordinates. size: dim x num_nodes\n        face_nodes (sps.csc-matrix): Face-node relationships. Matrix size:\n            num_faces x num_cells. To use compute_geometry() later, he field\n            face_nodes.indices should store the nodes of each face sorted.\n            For more information, see information on compute_geometry()\n            below.\n        cell_faces (sps.csc-matrix): Cell-face relationships. Matrix size:\n            num_faces x num_cells. Matrix elements have value +-1, where +\n            corresponds to the face normal vector being outwards.\n        name (list): Information on the formation of the grid, such as the\n            constructor, computations of geometry etc.\n        num_nodes (int): Number of nodes in the grid\n        num_faces (int): Number of faces in the grid\n        num_cells (int): Number of cells in the grid\n\n        ---\n        compute_geometry():\n        Assumes the nodes of each face is ordered according to the right\n        hand rule.\n        face_nodes.indices[face_nodes.indptr[i]:face_nodes.indptr[i+1]]\n        are the nodes of face i, which should be ordered counter-clockwise.\n        By counter-clockwise we mean as seen from cell cell_faces[i,:]==1.\n        Equivalently the nodes will be clockwise as seen from cell\n        cell_faces[i,:] == -1. Note that operations on the face_nodes matrix\n        (such as converting it to a csr-matrix) may change the ordering of\n        the nodes (face_nodes.indices), which will break compute_geometry().\n        Geometric information, available after compute_geometry() has been\n        called on the object:\n\n        face_areas (np.ndarray): Areas of all faces\n        face_centers (np.ndarray): Centers of all faces. Dimensions dim x\n            num_faces\n        face_normals (np.ndarray): Normal vectors of all faces. Dimensions\n            dim x num_faces. See also cell_faces.\n        cell_centers (np.ndarray): Centers of all cells. Dimensions dim x\n            num_cells\n        cell_volumes (np.ndarray): Volumes of all cells\n\n    \"\"\"\n\n    def __init__(self, dim, nodes, face_nodes, cell_faces, name):\n        \"\"\"Initialize the grid\n\n        See class documentation for further description of parameters.\n\n        Parameters\n        ----------\n        dim (int): grid dimension\n        nodes (np.ndarray): node coordinates.\n        face_nodes (sps.csc_matrix): Face-node relations.\n        cell_faces (sps.csc_matrix): Cell-face relations\n        name (str): Name of grid\n        \"\"\"\n        assert dim >= 0 and dim <= 3\n        self.dim = dim\n        self.nodes = nodes\n        self.cell_faces = cell_faces\n        self.face_nodes = face_nodes\n\n        if isinstance(name, list):\n            self.name = name\n        else:\n            self.name = [name]\n\n        # Infer bookkeeping from size of parameters\n        self.num_nodes = nodes.shape[1]\n        self.num_faces = face_nodes.shape[1]\n        self.num_cells = cell_faces.shape[1]\n\n        # Add tag for the boundary faces\n        self.tags = {}\n        self.initiate_face_tags()\n        self.update_boundary_face_tag()\n\n    def copy(self):\n        \"\"\"\n        Create a deep copy of the grid.\n\n        Returns:\n            grid: A deep copy of self. All attributes will also be copied.\n\n        \"\"\"\n        h = Grid(self.dim, self.nodes.copy(), self.face_nodes.copy(),\n                 self.cell_faces.copy(), self.name)\n        if hasattr(self, 'cell_volumes'):\n            h.cell_volumes = self.cell_volumes.copy()\n        if hasattr(self, 'cell_centers'):\n            h.cell_centers = self.cell_centers.copy()\n        if hasattr(self, 'face_centers'):\n            h.face_centers = self.face_centers.copy()\n        if hasattr(self, 'face_normals'):\n            h.face_normals = self.face_normals.copy()\n        if hasattr(self, 'face_areas'):\n            h.face_areas = self.face_areas.copy()\n        if hasattr(self, 'tags'):\n            h.tags = self.tags.copy()\n        return h\n\n    def __repr__(self):\n        \"\"\"\n        Implementation of __repr__\n\n        \"\"\"\n        s = 'Grid with history ' + ', '.join(self.name) + '\\n'\n        s = s + 'Number of cells ' + str(self.num_cells) + '\\n'\n        s = s + 'Number of faces ' + str(self.num_faces) + '\\n'\n        s = s + 'Number of nodes ' + str(self.num_nodes) + '\\n'\n        s += 'Dimension ' + str(self.dim)\n        return s\n\n    def __str__(self):\n        \"\"\" Implementation of __str__\n        \"\"\"\n        s = str()\n\n        # Special treatment of point grids.\n        if 'PointGrid' in self.name:\n            s = 'Point grid.\\n'\n            n = self.nodes\n            s += 'Coordinate: (' + str(n[0]) + ', ' + str(n[1])\n            s += ', ' + str(n[2]) + ')\\n'\n            return s\n\n        # More or less uniform treatment of the types of grids.\n        if 'CartGrid' in self.name:\n            s = 'Cartesian grid in ' + str(self.dim) + ' dimensions.\\n'\n        elif 'TensorGrid' in self.name:\n            s = 'Tensor grid in ' + str(self.dim) + ' dimensions.\\n'\n        elif 'StructuredTriangleGrid' in self.name:\n            s = 'Structured triangular grid.\\n'\n        elif 'TriangleGrid' in self.name:\n            s = 'Triangular grid. \\n'\n        elif 'StructuredTetrahedralGrid' in self.name:\n            s = 'Structured tetrahedral grid.\\n'\n        elif 'TetrahedralGrid' in self.name:\n            s = 'Tetrahedral grid.\\n'\n        s = s + 'Number of cells ' + str(self.num_cells) + '\\n'\n        s = s + 'Number of faces ' + str(self.num_faces) + '\\n'\n        s = s + 'Number of nodes ' + str(self.num_nodes) + '\\n'\n\n        return s\n\n    def compute_geometry(self, is_embedded=False):\n        \"\"\"Compute geometric quantities for the grid.\n\n        This method initializes class variables describing the grid\n        geometry, see class documentation for details.\n\n        The method could have been called from the constructor, however,\n        in cases where the grid is modified after the initial construction (\n        say, grid refinement), this may lead to costly, unnecessary\n        computations.\n        \"\"\"\n\n        self.name.append('Compute geometry')\n\n        if self.dim == 0:\n            self.__compute_geometry_0d()\n        elif self.dim == 1:\n            self.__compute_geometry_1d()\n        elif self.dim == 2:\n            self.__compute_geometry_2d(is_embedded)\n        else:\n            self.__compute_geometry_3d()\n\n    def __compute_geometry_0d(self):\n        \"Compute 0D geometry\"\n        self.face_areas = np.ones(1)\n        self.face_centers = self.nodes\n        self.face_normals = np.zeros((3, 1))  # not well-defined\n\n        self.cell_volumes = np.ones(1)\n        self.cell_centers = self.nodes\n\n    def __compute_geometry_1d(self):\n        \"Compute 1D geometry\"\n\n        self.face_areas = np.ones(self.num_faces)\n\n        fn = self.face_nodes.indices\n        n = fn.size\n        self.face_centers = self.nodes[:, fn]\n\n        self.face_normals = np.tile(cg.compute_tangent(self.nodes), (n, 1)).T\n\n        cf = self.cell_faces.indices\n        xf1 = self.face_centers[:, cf[::2]]\n        xf2 = self.face_centers[:, cf[1::2]]\n\n        self.cell_volumes = np.linalg.norm(xf1 - xf2, axis=0)\n        self.cell_centers = 0.5 * (xf1 + xf2)\n\n        # Ensure that normal vector direction corresponds with sign convention\n        # in self.cellFaces\n        def nrm(u):\n            return np.sqrt(u[0] * u[0] + u[1] * u[1] + u[2] * u[2])\n\n        [fi, ci, val] = sps.find(self.cell_faces)\n        _, idx = np.unique(fi, return_index=True)\n        sgn = val[idx]\n        fc = self.face_centers[:, fi[idx]]\n        cc = self.cell_centers[:, ci[idx]]\n        v = fc - cc\n        # Prolong the vector from cell to face center in the direction of the\n        # normal vector. If the prolonged vector is shorter, the normal should\n        # flipped\n        vn = v + nrm(v) * self.face_normals[:, fi[idx]] * 0.001\n        flip = np.logical_or(np.logical_and(nrm(v) > nrm(vn), sgn > 0),\n                             np.logical_and(nrm(v) < nrm(vn), sgn < 0))\n        self.face_normals[:, flip] *= -1\n\n    def __compute_geometry_2d(self, is_embedded):\n        \"Compute 2D geometry, with method motivated by similar MRST function\"\n\n        if is_embedded:\n            R = cg.project_plane_matrix(self.nodes, check_planar=False)\n            self.nodes = np.dot(R, self.nodes)\n\n        fn = self.face_nodes.indices\n        edge1 = fn[::2]\n        edge2 = fn[1::2]\n\n        xe1 = self.nodes[:, edge1]\n        xe2 = self.nodes[:, edge2]\n\n        edge_length_x = xe2[0] - xe1[0]\n        edge_length_y = xe2[1] - xe1[1]\n        edge_length_z = xe2[2] - xe1[2]\n        self.face_areas = np.sqrt(np.power(edge_length_x, 2) +\n                                  np.power(edge_length_y, 2) +\n                                  np.power(edge_length_z, 2))\n        self.face_centers = 0.5 * (xe1 + xe2)\n        n = edge_length_z.shape[0]\n        self.face_normals = np.vstack(\n            (edge_length_y, -edge_length_x, np.zeros(n)))\n\n        cell_faces, cellno = self.cell_faces.nonzero()\n        cx = np.bincount(cellno, weights=self.face_centers[0, cell_faces])\n        cy = np.bincount(cellno, weights=self.face_centers[1, cell_faces])\n        cz = np.bincount(cellno, weights=self.face_centers[2, cell_faces])\n        self.cell_centers = np.vstack((cx, cy, cz)) / np.bincount(cellno)\n\n        a = xe1[:, cell_faces] - self.cell_centers[:, cellno]\n        b = xe2[:, cell_faces] - self.cell_centers[:, cellno]\n\n        sub_volumes = 0.5 * np.abs(a[0] * b[1] - a[1] * b[0])\n        self.cell_volumes = np.bincount(cellno, weights=sub_volumes)\n\n        sub_centroids = (self.cell_centers[:, cellno] + 2 *\n                         self.face_centers[:, cell_faces]) / 3\n\n        ccx = np.bincount(cellno, weights=sub_volumes * sub_centroids[0])\n        ccy = np.bincount(cellno, weights=sub_volumes * sub_centroids[1])\n        ccz = np.bincount(cellno, weights=sub_volumes * sub_centroids[2])\n\n        self.cell_centers = np.vstack((ccx, ccy, ccz)) / self.cell_volumes\n\n        # Ensure that normal vector direction corresponds with sign convention\n        # in self.cellFaces\n        def nrm(u):\n            return np.sqrt(u[0] * u[0] + u[1] * u[1] + u[2] * u[2])\n\n        [fi, ci, val] = sps.find(self.cell_faces)\n        _, idx = np.unique(fi, return_index=True)\n        sgn = val[idx]\n        fc = self.face_centers[:, fi[idx]]\n        cc = self.cell_centers[:, ci[idx]]\n        v = fc - cc\n        # Prolong the vector from cell to face center in the direction of the\n        # normal vector. If the prolonged vector is shorter, the normal should\n        # flipped\n        vn = v + nrm(v) * self.face_normals[:, fi[idx]] * 0.001\n        flip = np.logical_or(np.logical_and(nrm(v) > nrm(vn), sgn > 0),\n                             np.logical_and(nrm(v) < nrm(vn), sgn < 0))\n        self.face_normals[:, flip] *= -1\n\n        if is_embedded:\n            self.nodes = np.dot(R.T, self.nodes)\n            self.face_normals = np.dot(R.T, self.face_normals)\n            self.face_centers = np.dot(R.T, self.face_centers)\n            self.cell_centers = np.dot(R.T, self.cell_centers)\n\n    def __compute_geometry_3d(self):\n        \"\"\"\n        Helper function to compute geometry for 3D grids\n\n        The implementation is motivated by the similar MRST function.\n\n        NOTE: The function is very long, and could have been broken up into\n        parts (face and cell computations are an obvious solution).\n\n        \"\"\"\n        num_face_nodes = self.face_nodes.nnz\n        face_node_ptr = self.face_nodes.indptr\n\n        num_nodes_per_face = face_node_ptr[1:] - face_node_ptr[:-1]\n\n        # Face-node relationships. Note that the elements here will also\n        # serve as a representation of an edge along the face (face_nodes[i]\n        #  represents the edge running from face_nodes[i] to face_nodes[i+1])\n        face_nodes = self.face_nodes.indices\n        # For each node, index of its parent face\n        face_node_ind = matrix_compression.rldecode(np.arange(\n            self.num_faces), num_nodes_per_face)\n\n        # Index of next node on the edge list. Note that this assumes the\n        # elements in face_nodes is stored in an ordered fasion\n        next_node = np.arange(num_face_nodes) + 1\n        # Close loops, for face i, the next node is the first of face i\n        next_node[face_node_ptr[1:] - 1] = face_node_ptr[:-1]\n\n        # Mapping from cells to faces\n        edge_2_face = sps.coo_matrix((np.ones(num_face_nodes),\n                                      (np.arange(num_face_nodes),\n                                       face_node_ind))).tocsc()\n\n        # Define temporary face center as the mean of the face nodes\n        tmp_face_center = self.nodes[:, face_nodes] * \\\n            edge_2_face / num_nodes_per_face\n        # Associate this value with all the edge of this face\n        tmp_face_center = edge_2_face * tmp_face_center.transpose()\n\n        # Vector along each edge\n        along_edge = self.nodes[:, face_nodes[next_node]] - \\\n            self.nodes[:, face_nodes]\n        # Vector from face center to start node of each edge\n        face_2_node = tmp_face_center.transpose() - self.nodes[:, face_nodes]\n\n        # Assign a normal vector with this edge, by taking the cross product\n        # between along_edge and face_2_node\n        # Divide by two to ensure that the normal vector has length equal to\n        # the area of the face triangle (by properties of cross product)\n        sub_normals = np.vstack((along_edge[1] * face_2_node[2] -\n                                 along_edge[2] * face_2_node[1],\n                                 along_edge[2] * face_2_node[0] -\n                                 along_edge[0] * face_2_node[2],\n                                 along_edge[0] * face_2_node[1] -\n                                 along_edge[1] * face_2_node[0])) / 2\n\n        def nrm(v):\n            return np.sqrt(np.sum(v * v, axis=0))\n\n        # Calculate area of sub-face associated with each edge - note that\n        # the sub-normals are area weighted\n        sub_areas = nrm(sub_normals)\n\n        # Centers of sub-faces are given by the centroid coordinates,\n        # e.g. the mean coordinate of the edge endpoints and the temporary\n        # face center\n        sub_centroids = (self.nodes[:, face_nodes] +\n                         self.nodes[:, face_nodes[next_node]]\n                         + tmp_face_center.transpose()) / 3\n\n        # Face normals are given as the sum of the sub-components\n        face_normals = sub_normals * edge_2_face\n        # Similar with face areas\n        face_areas = edge_2_face.transpose() * sub_areas\n\n        # Test whether the sub-normals are pointing in the same direction as\n        # the main normal: Distribute the main normal onto the edges,\n        # and take scalar product by element-wise multiplication with\n        # sub-normals, and sum over the components (axis=0).\n        # NOTE: There should be a built-in function for this in numpy?\n        sub_normals_sign = np.sign(np.sum(sub_normals * (edge_2_face *\n                                                         face_normals.transpose()).transpose(),\n                                          axis=0))\n\n        # Finally, face centers are the area weighted means of centroids of\n        # the sub-faces\n        face_centers = sub_areas * sub_centroids * edge_2_face / face_areas\n\n        # .. and we're done with the faces. Store information\n        self.face_centers = face_centers\n        self.face_normals = face_normals\n        self.face_areas = face_areas\n\n        # Cells\n\n        # Temporary cell center coordinates as the mean of the face center\n        # coordinates. The cells are divided into sub-tetrahedra (\n        # corresponding to triangular sub-faces above), with the temporary\n        # cell center as the final node\n\n        # Mapping from edges to cells. Take absolute value of cell_faces,\n        # since the elements are signed (contains the divergence).\n        # Note that edge_2_cell will contain more elements than edge_2_face,\n        # since the former will count internal faces twice (one for each\n        # adjacent cell)\n        edge_2_cell = edge_2_face * np.abs(self.cell_faces)\n        # Sort indices to avoid messing up the mappings later\n        edge_2_cell.sort_indices()\n\n        # Obtain relations between edges, faces and cells, in the form of\n        # index lists. Each element in the list corresponds to an edge seen\n        # from a cell (e.g. edges on internal faces are seen twice).\n\n        # Cell numbers are obtained from the columns in edge_2_cell.\n        cell_numbers = matrix_compression.rldecode(np.arange(self.num_cells),\n                                                   np.diff(edge_2_cell.indptr))\n        # Edge numbers from the rows. Here it is crucial that the indices\n        # are sorted\n        edge_numbers = edge_2_cell.indices\n        # Face numbers are obtained from the face-node relations (with the\n        # nodes doubling as representation of edges)\n        face_numbers = face_node_ind[edge_numbers]\n\n        # Number of edges per cell\n        num_cell_edges = edge_2_cell.indptr[1:] - edge_2_cell.indptr[:-1]\n\n        def bincount_nd(arr, weights):\n            \"\"\" Utility function to sum vector quantities by np.bincount. We\n            could probably have used np.apply_along_axis, but I could not\n            make it work.\n\n            Intended use: Map sub-cell centroids to a quantity for the cell.\n            \"\"\"\n            dim = weights.shape[0]\n            sz = arr.max() + 1\n\n            count = np.zeros((dim, sz))\n            for iter1 in range(dim):\n                count[iter1] = np.bincount(arr, weights=weights[iter1],\n                                           minlength=sz)\n            return count\n\n        # First estimate of cell centers as the mean of its faces' centers\n        # Divide by num_cell_edges here since all edges bring in their faces\n        tmp_cell_centers = bincount_nd(cell_numbers,\n                                       face_centers[:, face_numbers]\n                                       / num_cell_edges[cell_numbers])\n\n        # Distance from the temporary cell center to the sub-centroids (of\n        # the tetrahedra associated with each edge)\n        dist_cellcenter_subface = sub_centroids[:, edge_numbers] \\\n            - tmp_cell_centers[:, cell_numbers]\n\n        # Get sign of normal vectors, seen from all faces.\n        # Make sure we get a numpy ndarray, and not a matrix (.A), and that\n        # the array is 1D (squeeze)\n        orientation = np.squeeze(self.cell_faces[face_numbers, cell_numbers].A)\n\n        # Get outwards pointing sub-normals for all sub-faces: We need to\n        # account for both the orientation of the face, and the orientation\n        # of sub-faces relative to faces.\n        outer_normals = sub_normals[:, edge_numbers] \\\n            * orientation * sub_normals_sign[edge_numbers]\n\n        # Volumes of tetrahedra are now given by the dot product between the\n        #  outer normal (which is area weighted, and thus represent the base\n        #  of the tet), with the distancance from temporary cell center (the\n        # dot product gives the hight).\n        tet_volumes = np.sum(dist_cellcenter_subface * outer_normals,\n                             axis=0) / 3\n\n        # Sometimes the sub-tet volumes can have a volume of numerical zero.\n        # Why this is so is not clear, but for the moment, we allow for a\n        # slightly negative value.\n        assert np.all(tet_volumes > -1e-12)  # On the fly test\n\n        # The cell volumes are now found by summing sub-tetrahedra\n        cell_volumes = np.bincount(cell_numbers, weights=tet_volumes)\n        tri_centroids = 3 / 4 * dist_cellcenter_subface\n\n        # Compute a correction to the temporary cell center, by a volume\n        # weighted sum of the sub-tetrahedra\n        rel_centroid = bincount_nd(cell_numbers, tet_volumes * tri_centroids) \\\n            / cell_volumes\n        cell_centers = tmp_cell_centers + rel_centroid\n\n        # ... and we're done\n        self.cell_centers = cell_centers\n        self.cell_volumes = cell_volumes\n\n    def cell_nodes(self):\n        \"\"\"\n        Obtain mapping between cells and nodes.\n\n        Returns:\n            sps.csc_matrix, size num_nodes x num_cells: Value 1 indicates a\n                connection between cell and node.\n\n        \"\"\"\n        # Local version of cell-face map, using absolute value to avoid\n        # artifacts from +- in the original version.\n        cf_loc = sps.csc_matrix((np.abs(self.cell_faces.data),\n                                 self.cell_faces.indices,\n                                 self.cell_faces.indptr))\n        mat = (self.face_nodes * cf_loc) > 0\n        return mat\n\n    def num_cell_nodes(self):\n        \"\"\" Number of nodes per cell.\n\n        Returns:\n            np.ndarray, size num_cells: Number of nodes per cell.\n\n        \"\"\"\n        return self.cell_nodes().sum(axis=0).A.ravel('F')\n\n    def get_internal_nodes(self):\n        \"\"\"\n        Get internal nodes id of the grid.\n\n        Returns:\n            np.ndarray (1D), index of internal nodes.\n\n        \"\"\"\n        internal_nodes = np.setdiff1d(np.arange(self.num_nodes),\n                                      self.get_boundary_nodes(),\n                                      assume_unique=True)\n        return internal_nodes\n\n    def get_all_boundary_faces(self):\n        \"\"\"\n        Get indices of all faces tagged as either fractures, domain boundary or\n        tip.\n        \"\"\"\n        all_tags = tags.all_face_tags(self.tags)\n        return self.__indices(all_tags)\n\n    def get_internal_faces(self):\n        \"\"\"\n        Get internal faces id of the grid\n\n        Returns:\n            np.ndarray (1d), index of internal faces.\n\n        \"\"\"\n        return self.__indices(np.logical_not(self.get_all_boundary_faces()))\n\n    def get_boundary_nodes(self):\n        \"\"\"\n        Get nodes on the boundary\n\n        Returns:\n            np.ndarray (1d), index of nodes on the boundary\n\n        \"\"\"\n        b_faces = self.get_all_boundary_faces()\n        first = self.face_nodes.indptr[b_faces]\n        second = self.face_nodes.indptr[b_faces + 1]\n        return np.unique(self.face_nodes.indices[mcolon.mcolon(first, second)])\n\n    def update_boundary_face_tag(self):\n        \"\"\" Tag faces on the boundary of the grid with boundary tag.\n\n        \"\"\"\n        bd_faces = np.argwhere(np.abs(self.cell_faces).sum(axis=1).A.ravel('F')\n                               == 1).ravel('F')\n        self.tags['domain_boundary_faces'][bd_faces] = True\n\n    def cell_diameters(self, cn=None):\n        \"\"\"\n        Compute the cell diameters.\n\n        Parameters:\n            cn (optional): cell nodes map, previously already computed.\n            Otherwise a call to self.cell_nodes is provided.\n\n        Returns:\n            np.array, num_cells: values of the cell diameter for each cell\n\n        \"\"\"\n\n        def comb(n): return np.fromiter(itertools.chain.from_iterable(\n            itertools.combinations(n, 2)), n.dtype).reshape((2, -1),\n                                                            order='F')\n\n        def diam(n): return np.amax(np.linalg.norm(self.nodes[:, n[0, :]] -\n                                                   self.nodes[:, n[1, :]],\n                                                   axis=0))\n\n        if cn is None:\n            cn = self.cell_nodes()\n        return np.array([diam(comb(cn.indices[cn.indptr[c]:cn.indptr[c + 1]]))\n                         for c in np.arange(self.num_cells)])\n\n    def cell_face_as_dense(self):\n        \"\"\"\n        Obtain the cell-face relation in the from of two rows, rather than a\n        sparse matrix. This alterative format can be useful in some cases.\n\n        Each column in the array corresponds to a face, and the elements in\n        that column refers to cell indices. The value -1 signifies a boundary.\n        The normal vector of the face points from the first to the second row.\n\n        Returns:\n            np.ndarray, 2 x num_faces: Array representation of face-cell\n                relations\n        \"\"\"\n        n = self.cell_faces.tocsr()\n        d = np.diff(n.indptr)\n        rows = matrix_compression.rldecode(np.arange(d.size), d)\n        # Increase the data by one to distinguish cell indices from boundary\n        # cells\n        data = n.indices + 1\n        cols = ((n.data + 1) / 2).astype('i')\n        neighs = sps.coo_matrix((data, (rows, cols))).todense()\n        # Subtract 1 to get back to real cell indices\n        neighs -= 1\n        neighs = neighs.transpose().A.astype('int')\n        # Finally, we need to switch order of rows to get normal vectors\n        # pointing from first to second row.\n        return neighs[::-1]\n\n    def cell_connection_map(self):\n        \"\"\"\n        Get a matrix representation of cell-cell connections, as defined by\n        two cells sharing a face.\n\n        Returns:\n            scipy.sparse.csr_matrix, size num_cells * num_cells: Boolean\n                matrix, element (i,j) is true if cells i and j share a face.\n                The matrix is thus symmetric.\n        \"\"\"\n\n        # Create a copy of the cell-face relation, so that we can modify it at\n        # will\n        cell_faces = self.cell_faces.copy()\n\n        # Direction of normal vector does not matter here, only 0s and 1s\n        cell_faces.data = np.abs(cell_faces.data)\n\n        # Find connection between cells via the cell-face map\n        c2c = cell_faces.transpose() * cell_faces\n        # Only care about absolute values\n        c2c.data = np.clip(c2c.data, 0, 1).astype('bool')\n\n        return c2c\n\n    def bounding_box(self):\n        \"\"\"\n        Return the bounding box of the grid.\n        \"\"\"\n        return np.amin(self.nodes, axis=1), np.amax(self.nodes, axis=1)\n\n    def closest_cell(self, p):\n        \"\"\" For a set of points, find closest cell by cell center.\n\n        Parameters:\n            p (np.ndarray, 3xn): Point coordinates. If p.shape[0] < 3,\n                additional points will be treated as zeros.\n\n        Returns:\n            np.ndarray of ints: For each point, index of the cell with center\n                closest to the point.\n        \"\"\"\n        dim_p = p.shape[0]\n        if p.shape[0] < 3:\n            z = np.zeros((3 - p.shape[0], p.shape[1]))\n            p = np.vstack((p, z))\n\n        def min_dist(pts):\n            c = self.cell_centers\n            d = np.sum(np.power(c - pts, 2), axis=0)\n            return np.argmin(d)\n\n        ci = np.empty(p.shape[1], dtype=np.int)\n        for i in range(p.shape[1]):\n            ci[i] = min_dist(p[:, i].reshape((3, -1)))\n        return ci\n\n    def initiate_face_tags(self):\n        keys = tags.standard_face_tags()\n        values = [np.zeros(self.num_faces, dtype=bool)\n                  for _ in range(len(keys))]\n        tags.add_tags(self, dict(zip(keys, values)))\n\n    def __indices(self, true_false):\n        \"\"\" Shorthand for np.argwhere.\n        \"\"\"\n        return np.argwhere(true_false).ravel('F')",
  "def __init__(self, dim, nodes, face_nodes, cell_faces, name):\n        \"\"\"Initialize the grid\n\n        See class documentation for further description of parameters.\n\n        Parameters\n        ----------\n        dim (int): grid dimension\n        nodes (np.ndarray): node coordinates.\n        face_nodes (sps.csc_matrix): Face-node relations.\n        cell_faces (sps.csc_matrix): Cell-face relations\n        name (str): Name of grid\n        \"\"\"\n        assert dim >= 0 and dim <= 3\n        self.dim = dim\n        self.nodes = nodes\n        self.cell_faces = cell_faces\n        self.face_nodes = face_nodes\n\n        if isinstance(name, list):\n            self.name = name\n        else:\n            self.name = [name]\n\n        # Infer bookkeeping from size of parameters\n        self.num_nodes = nodes.shape[1]\n        self.num_faces = face_nodes.shape[1]\n        self.num_cells = cell_faces.shape[1]\n\n        # Add tag for the boundary faces\n        self.tags = {}\n        self.initiate_face_tags()\n        self.update_boundary_face_tag()",
  "def copy(self):\n        \"\"\"\n        Create a deep copy of the grid.\n\n        Returns:\n            grid: A deep copy of self. All attributes will also be copied.\n\n        \"\"\"\n        h = Grid(self.dim, self.nodes.copy(), self.face_nodes.copy(),\n                 self.cell_faces.copy(), self.name)\n        if hasattr(self, 'cell_volumes'):\n            h.cell_volumes = self.cell_volumes.copy()\n        if hasattr(self, 'cell_centers'):\n            h.cell_centers = self.cell_centers.copy()\n        if hasattr(self, 'face_centers'):\n            h.face_centers = self.face_centers.copy()\n        if hasattr(self, 'face_normals'):\n            h.face_normals = self.face_normals.copy()\n        if hasattr(self, 'face_areas'):\n            h.face_areas = self.face_areas.copy()\n        if hasattr(self, 'tags'):\n            h.tags = self.tags.copy()\n        return h",
  "def __repr__(self):\n        \"\"\"\n        Implementation of __repr__\n\n        \"\"\"\n        s = 'Grid with history ' + ', '.join(self.name) + '\\n'\n        s = s + 'Number of cells ' + str(self.num_cells) + '\\n'\n        s = s + 'Number of faces ' + str(self.num_faces) + '\\n'\n        s = s + 'Number of nodes ' + str(self.num_nodes) + '\\n'\n        s += 'Dimension ' + str(self.dim)\n        return s",
  "def __str__(self):\n        \"\"\" Implementation of __str__\n        \"\"\"\n        s = str()\n\n        # Special treatment of point grids.\n        if 'PointGrid' in self.name:\n            s = 'Point grid.\\n'\n            n = self.nodes\n            s += 'Coordinate: (' + str(n[0]) + ', ' + str(n[1])\n            s += ', ' + str(n[2]) + ')\\n'\n            return s\n\n        # More or less uniform treatment of the types of grids.\n        if 'CartGrid' in self.name:\n            s = 'Cartesian grid in ' + str(self.dim) + ' dimensions.\\n'\n        elif 'TensorGrid' in self.name:\n            s = 'Tensor grid in ' + str(self.dim) + ' dimensions.\\n'\n        elif 'StructuredTriangleGrid' in self.name:\n            s = 'Structured triangular grid.\\n'\n        elif 'TriangleGrid' in self.name:\n            s = 'Triangular grid. \\n'\n        elif 'StructuredTetrahedralGrid' in self.name:\n            s = 'Structured tetrahedral grid.\\n'\n        elif 'TetrahedralGrid' in self.name:\n            s = 'Tetrahedral grid.\\n'\n        s = s + 'Number of cells ' + str(self.num_cells) + '\\n'\n        s = s + 'Number of faces ' + str(self.num_faces) + '\\n'\n        s = s + 'Number of nodes ' + str(self.num_nodes) + '\\n'\n\n        return s",
  "def compute_geometry(self, is_embedded=False):\n        \"\"\"Compute geometric quantities for the grid.\n\n        This method initializes class variables describing the grid\n        geometry, see class documentation for details.\n\n        The method could have been called from the constructor, however,\n        in cases where the grid is modified after the initial construction (\n        say, grid refinement), this may lead to costly, unnecessary\n        computations.\n        \"\"\"\n\n        self.name.append('Compute geometry')\n\n        if self.dim == 0:\n            self.__compute_geometry_0d()\n        elif self.dim == 1:\n            self.__compute_geometry_1d()\n        elif self.dim == 2:\n            self.__compute_geometry_2d(is_embedded)\n        else:\n            self.__compute_geometry_3d()",
  "def __compute_geometry_0d(self):\n        \"Compute 0D geometry\"\n        self.face_areas = np.ones(1)\n        self.face_centers = self.nodes\n        self.face_normals = np.zeros((3, 1))  # not well-defined\n\n        self.cell_volumes = np.ones(1)\n        self.cell_centers = self.nodes",
  "def __compute_geometry_1d(self):\n        \"Compute 1D geometry\"\n\n        self.face_areas = np.ones(self.num_faces)\n\n        fn = self.face_nodes.indices\n        n = fn.size\n        self.face_centers = self.nodes[:, fn]\n\n        self.face_normals = np.tile(cg.compute_tangent(self.nodes), (n, 1)).T\n\n        cf = self.cell_faces.indices\n        xf1 = self.face_centers[:, cf[::2]]\n        xf2 = self.face_centers[:, cf[1::2]]\n\n        self.cell_volumes = np.linalg.norm(xf1 - xf2, axis=0)\n        self.cell_centers = 0.5 * (xf1 + xf2)\n\n        # Ensure that normal vector direction corresponds with sign convention\n        # in self.cellFaces\n        def nrm(u):\n            return np.sqrt(u[0] * u[0] + u[1] * u[1] + u[2] * u[2])\n\n        [fi, ci, val] = sps.find(self.cell_faces)\n        _, idx = np.unique(fi, return_index=True)\n        sgn = val[idx]\n        fc = self.face_centers[:, fi[idx]]\n        cc = self.cell_centers[:, ci[idx]]\n        v = fc - cc\n        # Prolong the vector from cell to face center in the direction of the\n        # normal vector. If the prolonged vector is shorter, the normal should\n        # flipped\n        vn = v + nrm(v) * self.face_normals[:, fi[idx]] * 0.001\n        flip = np.logical_or(np.logical_and(nrm(v) > nrm(vn), sgn > 0),\n                             np.logical_and(nrm(v) < nrm(vn), sgn < 0))\n        self.face_normals[:, flip] *= -1",
  "def __compute_geometry_2d(self, is_embedded):\n        \"Compute 2D geometry, with method motivated by similar MRST function\"\n\n        if is_embedded:\n            R = cg.project_plane_matrix(self.nodes, check_planar=False)\n            self.nodes = np.dot(R, self.nodes)\n\n        fn = self.face_nodes.indices\n        edge1 = fn[::2]\n        edge2 = fn[1::2]\n\n        xe1 = self.nodes[:, edge1]\n        xe2 = self.nodes[:, edge2]\n\n        edge_length_x = xe2[0] - xe1[0]\n        edge_length_y = xe2[1] - xe1[1]\n        edge_length_z = xe2[2] - xe1[2]\n        self.face_areas = np.sqrt(np.power(edge_length_x, 2) +\n                                  np.power(edge_length_y, 2) +\n                                  np.power(edge_length_z, 2))\n        self.face_centers = 0.5 * (xe1 + xe2)\n        n = edge_length_z.shape[0]\n        self.face_normals = np.vstack(\n            (edge_length_y, -edge_length_x, np.zeros(n)))\n\n        cell_faces, cellno = self.cell_faces.nonzero()\n        cx = np.bincount(cellno, weights=self.face_centers[0, cell_faces])\n        cy = np.bincount(cellno, weights=self.face_centers[1, cell_faces])\n        cz = np.bincount(cellno, weights=self.face_centers[2, cell_faces])\n        self.cell_centers = np.vstack((cx, cy, cz)) / np.bincount(cellno)\n\n        a = xe1[:, cell_faces] - self.cell_centers[:, cellno]\n        b = xe2[:, cell_faces] - self.cell_centers[:, cellno]\n\n        sub_volumes = 0.5 * np.abs(a[0] * b[1] - a[1] * b[0])\n        self.cell_volumes = np.bincount(cellno, weights=sub_volumes)\n\n        sub_centroids = (self.cell_centers[:, cellno] + 2 *\n                         self.face_centers[:, cell_faces]) / 3\n\n        ccx = np.bincount(cellno, weights=sub_volumes * sub_centroids[0])\n        ccy = np.bincount(cellno, weights=sub_volumes * sub_centroids[1])\n        ccz = np.bincount(cellno, weights=sub_volumes * sub_centroids[2])\n\n        self.cell_centers = np.vstack((ccx, ccy, ccz)) / self.cell_volumes\n\n        # Ensure that normal vector direction corresponds with sign convention\n        # in self.cellFaces\n        def nrm(u):\n            return np.sqrt(u[0] * u[0] + u[1] * u[1] + u[2] * u[2])\n\n        [fi, ci, val] = sps.find(self.cell_faces)\n        _, idx = np.unique(fi, return_index=True)\n        sgn = val[idx]\n        fc = self.face_centers[:, fi[idx]]\n        cc = self.cell_centers[:, ci[idx]]\n        v = fc - cc\n        # Prolong the vector from cell to face center in the direction of the\n        # normal vector. If the prolonged vector is shorter, the normal should\n        # flipped\n        vn = v + nrm(v) * self.face_normals[:, fi[idx]] * 0.001\n        flip = np.logical_or(np.logical_and(nrm(v) > nrm(vn), sgn > 0),\n                             np.logical_and(nrm(v) < nrm(vn), sgn < 0))\n        self.face_normals[:, flip] *= -1\n\n        if is_embedded:\n            self.nodes = np.dot(R.T, self.nodes)\n            self.face_normals = np.dot(R.T, self.face_normals)\n            self.face_centers = np.dot(R.T, self.face_centers)\n            self.cell_centers = np.dot(R.T, self.cell_centers)",
  "def __compute_geometry_3d(self):\n        \"\"\"\n        Helper function to compute geometry for 3D grids\n\n        The implementation is motivated by the similar MRST function.\n\n        NOTE: The function is very long, and could have been broken up into\n        parts (face and cell computations are an obvious solution).\n\n        \"\"\"\n        num_face_nodes = self.face_nodes.nnz\n        face_node_ptr = self.face_nodes.indptr\n\n        num_nodes_per_face = face_node_ptr[1:] - face_node_ptr[:-1]\n\n        # Face-node relationships. Note that the elements here will also\n        # serve as a representation of an edge along the face (face_nodes[i]\n        #  represents the edge running from face_nodes[i] to face_nodes[i+1])\n        face_nodes = self.face_nodes.indices\n        # For each node, index of its parent face\n        face_node_ind = matrix_compression.rldecode(np.arange(\n            self.num_faces), num_nodes_per_face)\n\n        # Index of next node on the edge list. Note that this assumes the\n        # elements in face_nodes is stored in an ordered fasion\n        next_node = np.arange(num_face_nodes) + 1\n        # Close loops, for face i, the next node is the first of face i\n        next_node[face_node_ptr[1:] - 1] = face_node_ptr[:-1]\n\n        # Mapping from cells to faces\n        edge_2_face = sps.coo_matrix((np.ones(num_face_nodes),\n                                      (np.arange(num_face_nodes),\n                                       face_node_ind))).tocsc()\n\n        # Define temporary face center as the mean of the face nodes\n        tmp_face_center = self.nodes[:, face_nodes] * \\\n            edge_2_face / num_nodes_per_face\n        # Associate this value with all the edge of this face\n        tmp_face_center = edge_2_face * tmp_face_center.transpose()\n\n        # Vector along each edge\n        along_edge = self.nodes[:, face_nodes[next_node]] - \\\n            self.nodes[:, face_nodes]\n        # Vector from face center to start node of each edge\n        face_2_node = tmp_face_center.transpose() - self.nodes[:, face_nodes]\n\n        # Assign a normal vector with this edge, by taking the cross product\n        # between along_edge and face_2_node\n        # Divide by two to ensure that the normal vector has length equal to\n        # the area of the face triangle (by properties of cross product)\n        sub_normals = np.vstack((along_edge[1] * face_2_node[2] -\n                                 along_edge[2] * face_2_node[1],\n                                 along_edge[2] * face_2_node[0] -\n                                 along_edge[0] * face_2_node[2],\n                                 along_edge[0] * face_2_node[1] -\n                                 along_edge[1] * face_2_node[0])) / 2\n\n        def nrm(v):\n            return np.sqrt(np.sum(v * v, axis=0))\n\n        # Calculate area of sub-face associated with each edge - note that\n        # the sub-normals are area weighted\n        sub_areas = nrm(sub_normals)\n\n        # Centers of sub-faces are given by the centroid coordinates,\n        # e.g. the mean coordinate of the edge endpoints and the temporary\n        # face center\n        sub_centroids = (self.nodes[:, face_nodes] +\n                         self.nodes[:, face_nodes[next_node]]\n                         + tmp_face_center.transpose()) / 3\n\n        # Face normals are given as the sum of the sub-components\n        face_normals = sub_normals * edge_2_face\n        # Similar with face areas\n        face_areas = edge_2_face.transpose() * sub_areas\n\n        # Test whether the sub-normals are pointing in the same direction as\n        # the main normal: Distribute the main normal onto the edges,\n        # and take scalar product by element-wise multiplication with\n        # sub-normals, and sum over the components (axis=0).\n        # NOTE: There should be a built-in function for this in numpy?\n        sub_normals_sign = np.sign(np.sum(sub_normals * (edge_2_face *\n                                                         face_normals.transpose()).transpose(),\n                                          axis=0))\n\n        # Finally, face centers are the area weighted means of centroids of\n        # the sub-faces\n        face_centers = sub_areas * sub_centroids * edge_2_face / face_areas\n\n        # .. and we're done with the faces. Store information\n        self.face_centers = face_centers\n        self.face_normals = face_normals\n        self.face_areas = face_areas\n\n        # Cells\n\n        # Temporary cell center coordinates as the mean of the face center\n        # coordinates. The cells are divided into sub-tetrahedra (\n        # corresponding to triangular sub-faces above), with the temporary\n        # cell center as the final node\n\n        # Mapping from edges to cells. Take absolute value of cell_faces,\n        # since the elements are signed (contains the divergence).\n        # Note that edge_2_cell will contain more elements than edge_2_face,\n        # since the former will count internal faces twice (one for each\n        # adjacent cell)\n        edge_2_cell = edge_2_face * np.abs(self.cell_faces)\n        # Sort indices to avoid messing up the mappings later\n        edge_2_cell.sort_indices()\n\n        # Obtain relations between edges, faces and cells, in the form of\n        # index lists. Each element in the list corresponds to an edge seen\n        # from a cell (e.g. edges on internal faces are seen twice).\n\n        # Cell numbers are obtained from the columns in edge_2_cell.\n        cell_numbers = matrix_compression.rldecode(np.arange(self.num_cells),\n                                                   np.diff(edge_2_cell.indptr))\n        # Edge numbers from the rows. Here it is crucial that the indices\n        # are sorted\n        edge_numbers = edge_2_cell.indices\n        # Face numbers are obtained from the face-node relations (with the\n        # nodes doubling as representation of edges)\n        face_numbers = face_node_ind[edge_numbers]\n\n        # Number of edges per cell\n        num_cell_edges = edge_2_cell.indptr[1:] - edge_2_cell.indptr[:-1]\n\n        def bincount_nd(arr, weights):\n            \"\"\" Utility function to sum vector quantities by np.bincount. We\n            could probably have used np.apply_along_axis, but I could not\n            make it work.\n\n            Intended use: Map sub-cell centroids to a quantity for the cell.\n            \"\"\"\n            dim = weights.shape[0]\n            sz = arr.max() + 1\n\n            count = np.zeros((dim, sz))\n            for iter1 in range(dim):\n                count[iter1] = np.bincount(arr, weights=weights[iter1],\n                                           minlength=sz)\n            return count\n\n        # First estimate of cell centers as the mean of its faces' centers\n        # Divide by num_cell_edges here since all edges bring in their faces\n        tmp_cell_centers = bincount_nd(cell_numbers,\n                                       face_centers[:, face_numbers]\n                                       / num_cell_edges[cell_numbers])\n\n        # Distance from the temporary cell center to the sub-centroids (of\n        # the tetrahedra associated with each edge)\n        dist_cellcenter_subface = sub_centroids[:, edge_numbers] \\\n            - tmp_cell_centers[:, cell_numbers]\n\n        # Get sign of normal vectors, seen from all faces.\n        # Make sure we get a numpy ndarray, and not a matrix (.A), and that\n        # the array is 1D (squeeze)\n        orientation = np.squeeze(self.cell_faces[face_numbers, cell_numbers].A)\n\n        # Get outwards pointing sub-normals for all sub-faces: We need to\n        # account for both the orientation of the face, and the orientation\n        # of sub-faces relative to faces.\n        outer_normals = sub_normals[:, edge_numbers] \\\n            * orientation * sub_normals_sign[edge_numbers]\n\n        # Volumes of tetrahedra are now given by the dot product between the\n        #  outer normal (which is area weighted, and thus represent the base\n        #  of the tet), with the distancance from temporary cell center (the\n        # dot product gives the hight).\n        tet_volumes = np.sum(dist_cellcenter_subface * outer_normals,\n                             axis=0) / 3\n\n        # Sometimes the sub-tet volumes can have a volume of numerical zero.\n        # Why this is so is not clear, but for the moment, we allow for a\n        # slightly negative value.\n        assert np.all(tet_volumes > -1e-12)  # On the fly test\n\n        # The cell volumes are now found by summing sub-tetrahedra\n        cell_volumes = np.bincount(cell_numbers, weights=tet_volumes)\n        tri_centroids = 3 / 4 * dist_cellcenter_subface\n\n        # Compute a correction to the temporary cell center, by a volume\n        # weighted sum of the sub-tetrahedra\n        rel_centroid = bincount_nd(cell_numbers, tet_volumes * tri_centroids) \\\n            / cell_volumes\n        cell_centers = tmp_cell_centers + rel_centroid\n\n        # ... and we're done\n        self.cell_centers = cell_centers\n        self.cell_volumes = cell_volumes",
  "def cell_nodes(self):\n        \"\"\"\n        Obtain mapping between cells and nodes.\n\n        Returns:\n            sps.csc_matrix, size num_nodes x num_cells: Value 1 indicates a\n                connection between cell and node.\n\n        \"\"\"\n        # Local version of cell-face map, using absolute value to avoid\n        # artifacts from +- in the original version.\n        cf_loc = sps.csc_matrix((np.abs(self.cell_faces.data),\n                                 self.cell_faces.indices,\n                                 self.cell_faces.indptr))\n        mat = (self.face_nodes * cf_loc) > 0\n        return mat",
  "def num_cell_nodes(self):\n        \"\"\" Number of nodes per cell.\n\n        Returns:\n            np.ndarray, size num_cells: Number of nodes per cell.\n\n        \"\"\"\n        return self.cell_nodes().sum(axis=0).A.ravel('F')",
  "def get_internal_nodes(self):\n        \"\"\"\n        Get internal nodes id of the grid.\n\n        Returns:\n            np.ndarray (1D), index of internal nodes.\n\n        \"\"\"\n        internal_nodes = np.setdiff1d(np.arange(self.num_nodes),\n                                      self.get_boundary_nodes(),\n                                      assume_unique=True)\n        return internal_nodes",
  "def get_all_boundary_faces(self):\n        \"\"\"\n        Get indices of all faces tagged as either fractures, domain boundary or\n        tip.\n        \"\"\"\n        all_tags = tags.all_face_tags(self.tags)\n        return self.__indices(all_tags)",
  "def get_internal_faces(self):\n        \"\"\"\n        Get internal faces id of the grid\n\n        Returns:\n            np.ndarray (1d), index of internal faces.\n\n        \"\"\"\n        return self.__indices(np.logical_not(self.get_all_boundary_faces()))",
  "def get_boundary_nodes(self):\n        \"\"\"\n        Get nodes on the boundary\n\n        Returns:\n            np.ndarray (1d), index of nodes on the boundary\n\n        \"\"\"\n        b_faces = self.get_all_boundary_faces()\n        first = self.face_nodes.indptr[b_faces]\n        second = self.face_nodes.indptr[b_faces + 1]\n        return np.unique(self.face_nodes.indices[mcolon.mcolon(first, second)])",
  "def update_boundary_face_tag(self):\n        \"\"\" Tag faces on the boundary of the grid with boundary tag.\n\n        \"\"\"\n        bd_faces = np.argwhere(np.abs(self.cell_faces).sum(axis=1).A.ravel('F')\n                               == 1).ravel('F')\n        self.tags['domain_boundary_faces'][bd_faces] = True",
  "def cell_diameters(self, cn=None):\n        \"\"\"\n        Compute the cell diameters.\n\n        Parameters:\n            cn (optional): cell nodes map, previously already computed.\n            Otherwise a call to self.cell_nodes is provided.\n\n        Returns:\n            np.array, num_cells: values of the cell diameter for each cell\n\n        \"\"\"\n\n        def comb(n): return np.fromiter(itertools.chain.from_iterable(\n            itertools.combinations(n, 2)), n.dtype).reshape((2, -1),\n                                                            order='F')\n\n        def diam(n): return np.amax(np.linalg.norm(self.nodes[:, n[0, :]] -\n                                                   self.nodes[:, n[1, :]],\n                                                   axis=0))\n\n        if cn is None:\n            cn = self.cell_nodes()\n        return np.array([diam(comb(cn.indices[cn.indptr[c]:cn.indptr[c + 1]]))\n                         for c in np.arange(self.num_cells)])",
  "def cell_face_as_dense(self):\n        \"\"\"\n        Obtain the cell-face relation in the from of two rows, rather than a\n        sparse matrix. This alterative format can be useful in some cases.\n\n        Each column in the array corresponds to a face, and the elements in\n        that column refers to cell indices. The value -1 signifies a boundary.\n        The normal vector of the face points from the first to the second row.\n\n        Returns:\n            np.ndarray, 2 x num_faces: Array representation of face-cell\n                relations\n        \"\"\"\n        n = self.cell_faces.tocsr()\n        d = np.diff(n.indptr)\n        rows = matrix_compression.rldecode(np.arange(d.size), d)\n        # Increase the data by one to distinguish cell indices from boundary\n        # cells\n        data = n.indices + 1\n        cols = ((n.data + 1) / 2).astype('i')\n        neighs = sps.coo_matrix((data, (rows, cols))).todense()\n        # Subtract 1 to get back to real cell indices\n        neighs -= 1\n        neighs = neighs.transpose().A.astype('int')\n        # Finally, we need to switch order of rows to get normal vectors\n        # pointing from first to second row.\n        return neighs[::-1]",
  "def cell_connection_map(self):\n        \"\"\"\n        Get a matrix representation of cell-cell connections, as defined by\n        two cells sharing a face.\n\n        Returns:\n            scipy.sparse.csr_matrix, size num_cells * num_cells: Boolean\n                matrix, element (i,j) is true if cells i and j share a face.\n                The matrix is thus symmetric.\n        \"\"\"\n\n        # Create a copy of the cell-face relation, so that we can modify it at\n        # will\n        cell_faces = self.cell_faces.copy()\n\n        # Direction of normal vector does not matter here, only 0s and 1s\n        cell_faces.data = np.abs(cell_faces.data)\n\n        # Find connection between cells via the cell-face map\n        c2c = cell_faces.transpose() * cell_faces\n        # Only care about absolute values\n        c2c.data = np.clip(c2c.data, 0, 1).astype('bool')\n\n        return c2c",
  "def bounding_box(self):\n        \"\"\"\n        Return the bounding box of the grid.\n        \"\"\"\n        return np.amin(self.nodes, axis=1), np.amax(self.nodes, axis=1)",
  "def closest_cell(self, p):\n        \"\"\" For a set of points, find closest cell by cell center.\n\n        Parameters:\n            p (np.ndarray, 3xn): Point coordinates. If p.shape[0] < 3,\n                additional points will be treated as zeros.\n\n        Returns:\n            np.ndarray of ints: For each point, index of the cell with center\n                closest to the point.\n        \"\"\"\n        dim_p = p.shape[0]\n        if p.shape[0] < 3:\n            z = np.zeros((3 - p.shape[0], p.shape[1]))\n            p = np.vstack((p, z))\n\n        def min_dist(pts):\n            c = self.cell_centers\n            d = np.sum(np.power(c - pts, 2), axis=0)\n            return np.argmin(d)\n\n        ci = np.empty(p.shape[1], dtype=np.int)\n        for i in range(p.shape[1]):\n            ci[i] = min_dist(p[:, i].reshape((3, -1)))\n        return ci",
  "def initiate_face_tags(self):\n        keys = tags.standard_face_tags()\n        values = [np.zeros(self.num_faces, dtype=bool)\n                  for _ in range(len(keys))]\n        tags.add_tags(self, dict(zip(keys, values)))",
  "def __indices(self, true_false):\n        \"\"\" Shorthand for np.argwhere.\n        \"\"\"\n        return np.argwhere(true_false).ravel('F')",
  "def nrm(u):\n            return np.sqrt(u[0] * u[0] + u[1] * u[1] + u[2] * u[2])",
  "def nrm(u):\n            return np.sqrt(u[0] * u[0] + u[1] * u[1] + u[2] * u[2])",
  "def nrm(v):\n            return np.sqrt(np.sum(v * v, axis=0))",
  "def bincount_nd(arr, weights):\n            \"\"\" Utility function to sum vector quantities by np.bincount. We\n            could probably have used np.apply_along_axis, but I could not\n            make it work.\n\n            Intended use: Map sub-cell centroids to a quantity for the cell.\n            \"\"\"\n            dim = weights.shape[0]\n            sz = arr.max() + 1\n\n            count = np.zeros((dim, sz))\n            for iter1 in range(dim):\n                count[iter1] = np.bincount(arr, weights=weights[iter1],\n                                           minlength=sz)\n            return count",
  "def comb(n): return np.fromiter(itertools.chain.from_iterable(\n            itertools.combinations(n, 2)), n.dtype).reshape((2, -1),\n                                                            order='F')",
  "def diam(n): return np.amax(np.linalg.norm(self.nodes[:, n[0, :]] -\n                                                   self.nodes[:, n[1, :]],\n                                                   axis=0))",
  "def min_dist(pts):\n            c = self.cell_centers\n            d = np.sum(np.power(c - pts, 2), axis=0)\n            return np.argmin(d)",
  "def create_3d_grids(pts, cells):\n    tet_cells = cells['tetra']\n    g_3d = simplex.TetrahedralGrid(pts.transpose(), tet_cells.transpose())\n\n    # Create mapping to global numbering (will be a unit mapping, but is\n    # crucial for consistency with lower dimensions)\n    g_3d.global_point_ind = np.arange(pts.shape[0])\n\n    # Convert to list to be consistent with lower dimensions\n    # This may also become useful in the future if we ever implement domain\n    # decomposition approaches based on gmsh.\n    g_3d = [g_3d]\n    return g_3d",
  "def create_2d_grids(pts, cells, **kwargs):\n    # List of 2D grids, one for each surface\n    g_2d = []\n    is_embedded = kwargs.get('is_embedded', False)\n    if is_embedded:\n        phys_names = kwargs.get('phys_names', False)\n        cell_info = kwargs.get('cell_info', False)\n        network = kwargs.get('network', False)\n\n        # Check input\n        if not phys_names:\n            raise TypeError('Need to specify phys_names for embedded grids')\n        if not cell_info:\n            raise TypeError('Need to specify cell_info for embedded grids')\n        if not network:\n            raise TypeError('Need to specify network for embedded grids')\n\n        # Special treatment of the case with no fractures\n        if not 'triangle' in cells:\n            return g_2d\n\n        # Recover cells on fracture surfaces, and create grids\n        tri_cells = cells['triangle']\n\n        # Map from split polygons and fractures, as defined by the network\n        # decomposition\n        poly_2_frac = network.decomposition['polygon_frac']\n\n        num_tri = tri_cells.shape[0]\n\n        phys_name_ind_tri = np.unique(cell_info['triangle']['physical'])\n\n        # Index of the physical name tag assigned by gmsh to each fracture\n        gmsh_num = np.zeros(phys_name_ind_tri.size, dtype='int')\n        # Index of the corresponding name used in the input to gmsh (on the\n        # from FRACTURE_{0, 1} etc.\n        frac_num = np.zeros(phys_name_ind_tri.size, dtype='int')\n\n        for i, pn_ind in enumerate(phys_name_ind_tri):\n            pn = phys_names[pn_ind]\n            offset = pn.rfind('_')\n            frac_num[i] = poly_2_frac[int(pn[offset + 1:])]\n            gmsh_num[i] = pn_ind\n\n        # Counter for boundary and auxiliary planes\n        count_bound_and_aux = 0\n        for fi in np.unique(frac_num):\n            # This loop should only produce grids on surfaces that are actually fractures\n            # Fractures are identified with physical names\n            # a) Either give seperate physical name to non-fractures (e.g. AUX_POLYGON)\n            # b) OR: Pass a list of which fractures (numbers) are really not fractures\n            # b) is simpler, a) is better (long term)\n            # If a) is chosen, you may need to be careful with\n\n            pn = phys_names[phys_name_ind_tri[fi]]\n            plane_type = pn[:pn.rfind('_')]\n\n            if plane_type != 'FRACTURE':\n                count_bound_and_aux += 1\n                continue\n\n            loc_num = np.where(frac_num == fi-count_bound_and_aux)[0]\n            loc_gmsh_num = gmsh_num[loc_num]\n\n            loc_tri_glob_ind = np.empty((0, 3))\n            for ti in loc_gmsh_num:\n                # It seems the gmsh numbering corresponding to the physical tags\n                # (as found in physnames) is stored in the first column of info\n                gmsh_ind = np.where(cell_info['triangle']['physical'] == ti)[0]\n                loc_tri_glob_ind = np.vstack((loc_tri_glob_ind,\n                                              tri_cells[gmsh_ind, :]))\n\n            loc_tri_glob_ind = loc_tri_glob_ind.astype('int')\n            pind_loc, p_map = np.unique(loc_tri_glob_ind, return_inverse=True)\n            loc_tri_ind = p_map.reshape((-1, 3))\n            g = simplex.TriangleGrid(pts[pind_loc, :].transpose(),\n                                     loc_tri_ind.transpose())\n            # Add mapping to global point numbers\n            g.global_point_ind = pind_loc\n\n            # Associate a fracture id (corresponding to the ordering of the\n            # frature planes in the original fracture list provided by the\n            # user)\n            g.frac_num = fi - count_bound_and_aux\n\n            # Append to list of 2d grids\n            g_2d.append(g)\n\n    else:\n        triangles = cells['triangle'].transpose()\n        # Construct grid\n        g_2d = simplex.TriangleGrid(pts.transpose(), triangles)\n\n        # Create mapping to global numbering (will be a unit mapping, but is\n        # crucial for consistency with lower dimensions)\n        g_2d.global_point_ind = np.arange(pts.shape[0])\n\n        # Convert to list to be consistent with lower dimensions\n        # This may also become useful in the future if we ever implement domain\n        # decomposition approaches based on gmsh.\n        g_2d = [g_2d]\n    return g_2d",
  "def create_1d_grids(pts, cells, phys_names, cell_info,\n                    line_tag=constants.GmshConstants().PHYSICAL_NAME_FRACTURE_LINE):\n    # Recover lines\n    # There will be up to three types of physical lines: intersections (between\n    # fractures), fracture tips, and auxiliary lines (to be disregarded)\n\n    # All intersection lines and points on boundaries are non-physical in 3d.\n    # I.e., they are assigned boundary conditions, but are not gridded.\n\n    g_1d = []\n\n    # If there are no fracture intersections, we return empty lists\n    if not 'line' in cells:\n        return g_1d, np.empty(0)\n\n    gmsh_const = constants.GmshConstants()\n\n    line_tags = cell_info['line']['physical']\n    line_cells = cells['line']\n\n    gmsh_tip_num = []\n    tip_pts = np.empty(0)\n\n    for i, pn_ind in enumerate(np.unique(line_tags)):\n        # Index of the final underscore in the physical name. Chars before this\n        # will identify the line type, the one after will give index\n        pn = phys_names[pn_ind]\n        offset_index = pn.rfind('_')\n        loc_line_cell_num = np.where(line_tags == pn_ind)[0]\n        loc_line_pts = line_cells[loc_line_cell_num, :]\n\n        assert loc_line_pts.size > 1\n\n        line_type = pn[:offset_index]\n\n        if line_type == gmsh_const.PHYSICAL_NAME_FRACTURE_TIP[:-1]:\n            gmsh_tip_num.append(i)\n\n            # We need not know which fracture the line is on the tip of (do\n            # we?)\n            tip_pts = np.append(tip_pts, np.unique(loc_line_pts))\n\n        elif line_type == line_tag[:-1]:\n            loc_pts_1d = np.unique(loc_line_pts)  # .flatten()\n            loc_coord = pts[loc_pts_1d, :].transpose()\n            g = create_embedded_line_grid(loc_coord, loc_pts_1d)\n            frac_num = pn[:offset_index+1:]\n            g.frac_num = frac_num\n            g_1d.append(g)\n\n        else:  # Auxiliary line\n            pass\n    return g_1d, tip_pts",
  "def create_0d_grids(pts, cells):\n    # Find 0-d grids (points)\n    # We know the points are 1d, so squeeze the superflous dimension\n\n    # All intersection lines and points on boundaries are non-physical in 3d.\n    # I.e., they are assigned boundary conditions, but are not gridded.\n    g_0d = []\n    if 'vertex' in cells:\n        point_cells = cells['vertex'].ravel()\n        for pi in point_cells:\n            g = point_grid.PointGrid(pts[pi])\n            g.global_point_ind = np.atleast_1d(np.asarray(pi))\n            g_0d.append(g)\n    return g_0d",
  "def create_embedded_line_grid(loc_coord, glob_id, atol=1e-4):\n    loc_center = np.mean(loc_coord, axis=1).reshape((-1, 1))\n    loc_coord -= loc_center\n    # Check that the points indeed form a line\n    assert cg.is_collinear(loc_coord, atol)\n    # Find the tangent of the line\n    tangent = cg.compute_tangent(loc_coord)\n    # Projection matrix\n    rot = cg.project_line_matrix(loc_coord, tangent)\n\n    loc_coord_1d = rot.dot(loc_coord)\n    # The points are now 1d along one of the coordinate axis, but we\n    # don't know which yet. Find this.\n\n    sum_coord = np.sum(np.abs(loc_coord_1d), axis=1)\n    sum_coord /= np.amax(sum_coord)\n    active_dimension = np.logical_not(np.isclose(sum_coord, 0, atol=atol, rtol=0))\n    # Check that we are indeed in 1d\n    assert np.sum(active_dimension) == 1\n    # Sort nodes, and create grid\n    coord_1d = loc_coord_1d[active_dimension]\n    sort_ind = np.argsort(coord_1d)[0]\n    sorted_coord = coord_1d[0, sort_ind]\n    g = structured.TensorGrid(sorted_coord)\n\n    # Project back to active dimension\n    nodes = np.zeros(g.nodes.shape)\n    nodes[active_dimension] = g.nodes[0]\n    g.nodes = nodes\n\n    # Project back again to 3d coordinates\n\n    irot = rot.transpose()\n    g.nodes = irot.dot(g.nodes)\n    g.nodes += loc_center\n\n    # Add mapping to global point numbers\n    g.global_point_ind = glob_id[sort_ind]\n    return g",
  "class GmshWriter(object):\n    \"\"\"\n     Write a gmsh.geo file for a fractured 2D domains, possibly including\n     compartments\n    \"\"\"\n\n    def __init__(self, pts, lines, polygons=None, domain=None, nd=None,\n                 mesh_size=None, mesh_size_bound=None, line_type=None,\n                 intersection_points=None, tolerance=None, edges_2_frac=None,\n                 meshing_algorithm=None, fracture_tags=None):\n        \"\"\"\n\n        :param pts: np.ndarary, Points\n        :param lines: np.ndarray. Non-intersecting lines in the geometry.\n        :param nd: Dimension. Inferred from points if not provided\n        \"\"\"\n        self.pts = pts\n        self.lines = lines\n        self.polygons = polygons\n        if nd is None:\n            if pts.shape[0] == 2:\n                self.nd = 2\n            elif pts.shape[0] == 3:\n                self.nd = 3\n        else:\n            self.nd = nd\n\n        self.lchar = mesh_size\n        self.lchar_bound = mesh_size_bound\n\n        self.domain = domain\n\n        if fracture_tags is not None:\n            self.polygon_tags = fracture_tags\n\n        self.mesh_size = mesh_size\n        self.mesh_size_bound = mesh_size_bound\n\n        # Points that should be decleared physical (intersections between 3\n        # fractures)\n        self.intersection_points = intersection_points\n        self.tolerance = tolerance\n        self.e2f = edges_2_frac\n\n        self.meshing_algorithm = meshing_algorithm\n\n    def write_geo(self, file_name):\n\n        if self.tolerance is not None:\n            s = 'Geometry.Tolerance = ' + str(self.tolerance) + ';\\n'\n        else:\n            s = '\\n'\n        s += self.__write_points()\n\n        if self.nd == 2:\n            if self.domain is not None:\n                s += self.__write_boundary_2d()\n            s += self.__write_fractures_compartments_2d()\n        elif self.nd == 3:\n            s += self.__write_lines()\n            if self.domain is not None:\n                s += self.__write_boundary_3d()\n            s += self.__write_polygons()\n\n        s += self.__write_physical_points()\n        s += self.__write_meshing_algorithm()\n\n        with open(file_name, 'w') as f:\n            f.write(s)\n\n    def __write_fractures_compartments_2d(self):\n        # Both fractures and compartments are\n        constants = gridding_constants.GmshConstants()\n\n        frac_ind = np.argwhere(np.logical_or(\n            self.lines[2] == constants.COMPARTMENT_BOUNDARY_TAG,\n            self.lines[2] == constants.FRACTURE_TAG)).ravel()\n        frac_lines = self.lines[:, frac_ind]\n\n        frac_id = frac_lines[3, :]\n        if frac_id.size == 0:\n            return str()\n        range_id = np.arange(np.amin(frac_id), np.amax(frac_id)+1)\n\n        s = '// Start specification of fractures\\n'\n        seg_id = 0\n        for i in range_id:\n            local_seg_id = str()\n            for mask in np.flatnonzero(frac_id == i):\n                s += 'frac_line_' + str(seg_id) + ' = newl; ' + \\\n                     'Line(frac_line_' + str(seg_id) + ') = {' + \\\n                     'p' + str(int(frac_lines[0, mask])) + \\\n                     ', p' + str(int(frac_lines[1, mask])) + \\\n                     '};\\n' + \\\n                     'Line{ frac_line_' + str(seg_id) + \\\n                     '} In Surface{domain_surf};\\n'\n                local_seg_id += 'frac_line_' + str(seg_id) + ', '\n                seg_id += 1\n\n            local_seg_id = local_seg_id[:-2]\n            s += 'Physical Line(\\\"' + constants.PHYSICAL_NAME_FRACTURES \\\n                 + str(i) + '\\\") = { ' + local_seg_id + ' };\\n'\n            s += '\\n'\n\n        s += '// End of fracture specification\\n\\n'\n        return s\n\n    def __write_boundary_2d(self):\n        constants = gridding_constants.GmshConstants()\n        bound_line_ind = np.argwhere(self.lines[2] ==\n                                     constants.DOMAIN_BOUNDARY_TAG).ravel()\n        bound_line = self.lines[:2, bound_line_ind]\n        bound_line = sort_points.sort_point_pairs(bound_line,\n                                                  check_circular=True)\n\n        s = '// Start of specification of domain'\n        s += '// Define lines that make up the domain boundary\\n'\n\n        loop_str = '{'\n        for i in range(bound_line.shape[1]):\n            s += 'bound_line_' + str(i) + ' = newl; Line(bound_line_'\\\n                 + str(i) + ') ={'\n            s += 'p' + str(int(bound_line[0, i])) + ', p' + \\\n                 str(int(bound_line[1, i])) + '};\\n'\n            loop_str += 'bound_line_' + str(i) + ', '\n\n        s += '\\n'\n        loop_str = loop_str[:-2]  # Remove last comma\n        loop_str += '};\\n'\n        s += '// Line loop that makes the domain boundary\\n'\n        s += 'Domain_loop = newll;\\n'\n        s += 'Line Loop(Domain_loop) = ' + loop_str\n        s += 'domain_surf = news;\\n'\n        s += 'Plane Surface(domain_surf) = {Domain_loop};\\n'\n        s += 'Physical Surface(\\\"' + constants.PHYSICAL_NAME_DOMAIN + \\\n             '\\\") = {domain_surf};\\n'\n        s += '// End of domain specification\\n\\n'\n        return s\n\n    def __write_boundary_3d(self):\n        ls = '\\n'\n        s = '// Start domain specification'+ls\n        # Write surfaces:\n        s += self.__write_polygons(boundary=True)\n\n        # Make a box out of them\n        s += 'domain_loop = newsl;' + ls\n        s += 'Surface Loop(domain_loop) = {'\n        for pi in range(len(self.polygons[0])):\n            if self.polygon_tags['boundary'][pi]:\n                s += 'auxiliary_' + str(pi) + ','\n        s = s[:-1]\n        s += '};' + ls\n        s += 'Volume(1) = {domain_loop};' + ls\n        s += 'Physical Volume(\\\"' + \\\n             gridding_constants.GmshConstants().PHYSICAL_NAME_DOMAIN + \\\n             '\\\") = {1};' + ls\n\n        s += '// End of domain specification\\n\\n'\n        return s\n\n\n    def __write_points(self, boundary=False):\n        p = self.pts\n        num_p = p.shape[1]\n        if p.shape[0] == 2:\n            p = np.vstack((p, np.zeros(num_p)))\n        s = '// Define points\\n'\n        for i in range(self.pts.shape[1]):\n            s += 'p' + str(i) + ' = newp; Point(p' + str(i) + ') = '\n            s += '{' + str(p[0, i]) + ', ' + str(p[1, i]) + ', '\\\n                 + str(p[2, i])\n            if self.mesh_size is not None:\n                s += ', ' + str(self.mesh_size[i]) + ' };\\n'\n            else:\n                s += '};\\n'\n\n        s += '// End of point specification\\n\\n'\n        return s\n\n    def __write_lines(self, embed_in=None):\n        l = self.lines\n        num_lines = l.shape[1]\n        ls = '\\n'\n        s = '// Define lines ' + ls\n        constants = gridding_constants.GmshConstants()\n        if l.shape[0] > 2:\n            lt = l[2]\n            has_tags = True\n        else:\n            has_tags = False\n        for i in range(num_lines):\n            si = str(i)\n            s += 'frac_line_' + si + '= newl; Line(frac_line_' + si \\\n                + ') = {p' + str(l[0, i]) + ', p' + str(l[1, i]) \\\n                + '};' + ls\n            if has_tags:\n                s += 'Physical Line(\\\"'\n                if l[2, i] == constants.FRACTURE_TIP_TAG:\n                    s += constants.PHYSICAL_NAME_FRACTURE_TIP\n                elif l[2, i] == constants.FRACTURE_INTERSECTION_LINE_TAG:\n                    s += constants.PHYSICAL_NAME_FRACTURE_LINE\n                else:\n                    # This is a line that need not be physical (recognized by\n                    # the parser of output from gmsh). Applies to boundary and\n                    # subdomain boundary lines.\n                    s += constants.PHYSICAL_NAME_AUXILIARY_LINE\n\n                s += si + '\\\") = {frac_line_' + si + '};' + ls\n\n            s += ls\n        s += '// End of line specification ' + ls + ls\n        return s\n\n    def __write_polygons(self, boundary=False):\n        \"\"\"\n        Writes either all fractures or all boundary planes.\n        \"\"\"\n        constants = gridding_constants.GmshConstants()\n        bound_tags = self.polygon_tags.get('boundary',\n                                       [False]*len(self.polygons[0]))\n        subd_tags = self.polygon_tags.get('subdomain',\n                                       [False]*len(self.polygons[0]))\n\n        ls = '\\n'\n        # Name boundary or fracture\n        f_or_b = 'auxiliary' if boundary else 'fracture'\n        if not boundary:\n            s = '// Start fracture specification' + ls\n        else:\n            s = ''\n        for pi in range(len(self.polygons[0])):\n            if bound_tags[pi] != boundary:\n                continue\n            # Check if the polygon is a subdomain boundary, i.e., auxiliary\n            # polygon.\n            auxiliary = subd_tags[pi]\n            if auxiliary:\n                # Keep track of \"fake fractures\", i.e., subdomain\n                # boundaries.\n                f_or_b = 'auxiliary'\n            p = self.polygons[0][pi].astype('int')\n            reverse = self.polygons[1][pi]\n            # First define line loop\n            s += 'frac_loop_' + str(pi) + ' = newll; ' + ls\n            s += 'Line Loop(frac_loop_' + str(pi) + ') = { '\n            for i, li in enumerate(p):\n                if reverse[i]:\n                    s += '-'\n                s += 'frac_line_' + str(li)\n                if i < p.size - 1:\n                    s += ', '\n\n            s += '};' + ls\n\n            n = f_or_b + '_'\n            # Then the surface\n            s += n + str(pi) + ' = news; '\n            s += 'Plane Surface(' + n + str(pi) + ') = {frac_loop_' \\\n                + str(pi) + '};' + ls\n\n            if bound_tags[pi] or auxiliary:\n                # Domain boundary or \"fake fracture\" = subdomain boundary\n                s += 'Physical Surface(\\\"' + constants.PHYSICAL_NAME_AUXILIARY \\\n                      + str(pi) + '\\\") = {auxiliary_' + str(pi) + '};' + ls\n\n            else:\n                # Normal fracture\n                s += 'Physical Surface(\\\"' + constants.PHYSICAL_NAME_FRACTURES \\\n                     + str(pi) + '\\\") = {fracture_' + str(pi) + '};' + ls\n                if self.domain is not None:\n                    s += 'Surface{' + n + str(pi) + '} In Volume{1};' + ls + ls\n\n            for li in self.e2f[pi]:\n                s += 'Line{frac_line_' + str(li) + '} In Surface{' + n\n                s += str(pi) + '};' + ls\n            s += ls\n\n        if not boundary:\n            s += '// End of fracture specification' + ls + ls\n\n        return s\n\n    def __write_physical_points(self):\n        ls = '\\n'\n        s = '// Start physical point specification' + ls\n\n        constants = gridding_constants.GmshConstants()\n\n        for i, p in enumerate(self.intersection_points):\n            s += 'Physical Point(\\\"' + constants.PHYSICAL_NAME_FRACTURE_POINT \\\n                + str(i) + '\\\") = {p' + str(p) + '};' + ls\n        s += '// End of physical point specification' + ls + ls\n        return s\n\n    def __write_meshing_algorithm(self):\n        # See: http://www.manpagez.com/info/gmsh/gmsh-2.4.0/gmsh_76.php\n        if self.meshing_algorithm is None:\n            return \"\"\n        else:\n            return \"\\nMesh.Algorithm = \" + str(self.meshing_algorithm) + \";\"",
  "class GmshGridBucketWriter(object):\n    \"\"\"\n    Dump a grid bucket to a gmsh .msh file, to be read by other software.\n\n    The function assumes that the grid consists of simplices, and error\n    messages will be raised if otherwise. The extension should not be\n    difficult, but the need has not been there yet.\n\n    All grids in all dimensions will have a separate physical name (in the gmsh\n    sense), on the format GRID_#ID_DIM_#DIMENSION. Here #ID is the index of\n    the corresponding node in the grid bucket, as defined by\n    gb.assign_node_ordering. #DIMENSION is the dimension of the grid.\n\n    \"\"\"\n\n    def __init__(self, gb):\n        \"\"\"\n        Parameters:\n            gb (gridding.grid_bucket): Grid bucket to be dumped.\n\n        \"\"\"\n        self.gb = gb\n\n        # Assign ordering of the nodes in gb - used for unique identification\n        # of each grid\n        gb.assign_node_ordering()\n\n        # Compute number of grids in each dimension of the gb\n        self._num_grids()\n\n    def write(self, file_name):\n        \"\"\"\n        Write the whole bucket to a .msh file.\n\n        Parameters:\n            file_name (str): Name of dump file.\n\n        \"\"\"\n        s = self._preamble()\n        s += self._physical_names()\n        s += self._points()\n        s += self._elements()\n\n        with open(file_name, 'w') as f:\n            f.write(s)\n\n    def _preamble(self):\n        # Write the preamble (mesh Format) section\n        s_preamble = '$MeshFormat\\n'\n        s_preamble += '2.2 0 8\\n'\n        s_preamble += '$EndMeshFormat\\n'\n        return s_preamble\n\n    def _num_grids(self):\n        # Find number of grids in each dimension\n        max_dim = 3\n        num_grids = np.zeros(max_dim + 1, dtype='int')\n        for dim in range(max_dim + 1):\n            num_grids[dim] = len(self.gb.grids_of_dimension(dim))\n\n            # Identify the highest dimension\n        while num_grids[-1] == 0:\n            num_grids = num_grids[:-1]\n\n            # We will pick the global point set from the highest dimensional\n            # grid. The current implementation assumes there is a single grid\n            # in that dimension. Taking care of multiple grids should not be\n            # difficult, but it has not been necessary up to know.\n            if num_grids[-1] != 1:\n                raise NotImplementedError('Have not considered several grids\\\n                                          in the highest dimension')\n        self.num_grids = num_grids\n\n    def _points(self):\n        # The global point set\n        p = self.gb.grids_of_dimension(len(self.num_grids)-1)[0].nodes\n\n        ls = '\\n'\n        s = '$Nodes' + ls\n        s += str(p.shape[1]) + ls\n        for i in range(p.shape[1]):\n            s += str(i+1) + ' ' + str(p[0, i]) + ' ' + str(p[1, i]) + \\\n                    ' ' + str(p[2, i]) + ls\n        s += '$EndNodes' + ls\n        return s\n\n    def _physical_names(self):\n        ls = '\\n'\n        s = '$PhysicalNames' + ls\n\n        # Use one physical name for each grid (including highest dimensional\n        # one)\n        s += str(self.gb.size()) + ls\n        for i, g in enumerate(self.gb):\n            dim = g[0].dim\n            s += str(dim) + ' ' + str(i+1) + ' ' + 'GRID_' + \\\n                    str(g[1]['node_number']) + '_DIM_' + str(dim) + ls\n        s += '$EndPhysicalNames' + ls\n        return s\n\n    def _elements(self):\n        ls = '\\n'\n        s = '$Elements' + ls\n\n        num_cells = 0\n        for g, _ in self.gb:\n            num_cells += g.num_cells\n        s += str(num_cells) + ls\n\n        # Element types (as specified by the gmsh .msh format), index by\n        # dimensions. This assumes all cells are simplices.\n        elem_type = [15, 1, 2, 4]\n        for i, gr in enumerate(self.gb):\n            g = gr[0]\n            gn = str(gr[1]['node_number'])\n\n            # Sanity check - all cells should be simplices\n            assert np.all(np.diff(g.cell_nodes().indptr) == g.dim+1)\n\n            # Write the cell-node relation as an num_cells x dim+1 array\n            cn = g.cell_nodes().indices.reshape((g.num_cells, g.dim+1))\n\n            et = str(elem_type[g.dim])\n            for ci in range(g.num_cells):\n                s += str(ci+1) + ' ' + et + ' ' + str(1) + ' ' + gn + ' '\n                # There may be underlaying assumptions in gmsh on the ordering\n                # of nodes.\n                for d in range(g.dim+1):\n                    # Increase vertex offset by 1\n                    s += str(cn[ci, d] + 1) + ' '\n                s += ls\n\n        s += '$EndElements' + ls\n        return s",
  "def run_gmsh(in_file, out_file, dims, **kwargs):\n    \"\"\"\n    Convenience function to run gmsh.\n\n    Parameters:\n        in_file (str): Name of gmsh configuration file (.geo)\n        out_file (str): Name of output file for gmsh (.msh)\n        dims (int): Number of dimensions gmsh should grid. If dims is less than\n            the geometry dimensions, gmsh will grid all lower-dimensional\n            objcets described in in_file (e.g. all surfaces embeded in a 3D\n            geometry).\n        **kwargs: Options passed on to gmsh. See gmsh documentation for\n            possible values.\n\n    Returns:\n        double: Status of the generation, as returned by os.system. 0 means the\n            simulation completed successfully, >0 signifies problems.\n\n    \"\"\"\n    # Import config file to get location of gmsh executable.\n    config = read_config.read()\n    path_to_gmsh = config['gmsh_path']\n\n    opts = ' '\n    for key, val in kwargs.items():\n        # Gmsh keywords are specified with prefix '-'\n        if key[0] != '-':\n            key = '-' + key\n        opts += key + ' ' + str(val) + ' '\n\n    if dims == 2:\n        cmd = path_to_gmsh + ' -2 ' + in_file + ' -o ' + out_file + opts\n    else:\n        cmd = path_to_gmsh + ' -3 ' + in_file + ' -o ' + out_file + opts\n\n    status = os.system(cmd)\n\n    return status",
  "def __init__(self, pts, lines, polygons=None, domain=None, nd=None,\n                 mesh_size=None, mesh_size_bound=None, line_type=None,\n                 intersection_points=None, tolerance=None, edges_2_frac=None,\n                 meshing_algorithm=None, fracture_tags=None):\n        \"\"\"\n\n        :param pts: np.ndarary, Points\n        :param lines: np.ndarray. Non-intersecting lines in the geometry.\n        :param nd: Dimension. Inferred from points if not provided\n        \"\"\"\n        self.pts = pts\n        self.lines = lines\n        self.polygons = polygons\n        if nd is None:\n            if pts.shape[0] == 2:\n                self.nd = 2\n            elif pts.shape[0] == 3:\n                self.nd = 3\n        else:\n            self.nd = nd\n\n        self.lchar = mesh_size\n        self.lchar_bound = mesh_size_bound\n\n        self.domain = domain\n\n        if fracture_tags is not None:\n            self.polygon_tags = fracture_tags\n\n        self.mesh_size = mesh_size\n        self.mesh_size_bound = mesh_size_bound\n\n        # Points that should be decleared physical (intersections between 3\n        # fractures)\n        self.intersection_points = intersection_points\n        self.tolerance = tolerance\n        self.e2f = edges_2_frac\n\n        self.meshing_algorithm = meshing_algorithm",
  "def write_geo(self, file_name):\n\n        if self.tolerance is not None:\n            s = 'Geometry.Tolerance = ' + str(self.tolerance) + ';\\n'\n        else:\n            s = '\\n'\n        s += self.__write_points()\n\n        if self.nd == 2:\n            if self.domain is not None:\n                s += self.__write_boundary_2d()\n            s += self.__write_fractures_compartments_2d()\n        elif self.nd == 3:\n            s += self.__write_lines()\n            if self.domain is not None:\n                s += self.__write_boundary_3d()\n            s += self.__write_polygons()\n\n        s += self.__write_physical_points()\n        s += self.__write_meshing_algorithm()\n\n        with open(file_name, 'w') as f:\n            f.write(s)",
  "def __write_fractures_compartments_2d(self):\n        # Both fractures and compartments are\n        constants = gridding_constants.GmshConstants()\n\n        frac_ind = np.argwhere(np.logical_or(\n            self.lines[2] == constants.COMPARTMENT_BOUNDARY_TAG,\n            self.lines[2] == constants.FRACTURE_TAG)).ravel()\n        frac_lines = self.lines[:, frac_ind]\n\n        frac_id = frac_lines[3, :]\n        if frac_id.size == 0:\n            return str()\n        range_id = np.arange(np.amin(frac_id), np.amax(frac_id)+1)\n\n        s = '// Start specification of fractures\\n'\n        seg_id = 0\n        for i in range_id:\n            local_seg_id = str()\n            for mask in np.flatnonzero(frac_id == i):\n                s += 'frac_line_' + str(seg_id) + ' = newl; ' + \\\n                     'Line(frac_line_' + str(seg_id) + ') = {' + \\\n                     'p' + str(int(frac_lines[0, mask])) + \\\n                     ', p' + str(int(frac_lines[1, mask])) + \\\n                     '};\\n' + \\\n                     'Line{ frac_line_' + str(seg_id) + \\\n                     '} In Surface{domain_surf};\\n'\n                local_seg_id += 'frac_line_' + str(seg_id) + ', '\n                seg_id += 1\n\n            local_seg_id = local_seg_id[:-2]\n            s += 'Physical Line(\\\"' + constants.PHYSICAL_NAME_FRACTURES \\\n                 + str(i) + '\\\") = { ' + local_seg_id + ' };\\n'\n            s += '\\n'\n\n        s += '// End of fracture specification\\n\\n'\n        return s",
  "def __write_boundary_2d(self):\n        constants = gridding_constants.GmshConstants()\n        bound_line_ind = np.argwhere(self.lines[2] ==\n                                     constants.DOMAIN_BOUNDARY_TAG).ravel()\n        bound_line = self.lines[:2, bound_line_ind]\n        bound_line = sort_points.sort_point_pairs(bound_line,\n                                                  check_circular=True)\n\n        s = '// Start of specification of domain'\n        s += '// Define lines that make up the domain boundary\\n'\n\n        loop_str = '{'\n        for i in range(bound_line.shape[1]):\n            s += 'bound_line_' + str(i) + ' = newl; Line(bound_line_'\\\n                 + str(i) + ') ={'\n            s += 'p' + str(int(bound_line[0, i])) + ', p' + \\\n                 str(int(bound_line[1, i])) + '};\\n'\n            loop_str += 'bound_line_' + str(i) + ', '\n\n        s += '\\n'\n        loop_str = loop_str[:-2]  # Remove last comma\n        loop_str += '};\\n'\n        s += '// Line loop that makes the domain boundary\\n'\n        s += 'Domain_loop = newll;\\n'\n        s += 'Line Loop(Domain_loop) = ' + loop_str\n        s += 'domain_surf = news;\\n'\n        s += 'Plane Surface(domain_surf) = {Domain_loop};\\n'\n        s += 'Physical Surface(\\\"' + constants.PHYSICAL_NAME_DOMAIN + \\\n             '\\\") = {domain_surf};\\n'\n        s += '// End of domain specification\\n\\n'\n        return s",
  "def __write_boundary_3d(self):\n        ls = '\\n'\n        s = '// Start domain specification'+ls\n        # Write surfaces:\n        s += self.__write_polygons(boundary=True)\n\n        # Make a box out of them\n        s += 'domain_loop = newsl;' + ls\n        s += 'Surface Loop(domain_loop) = {'\n        for pi in range(len(self.polygons[0])):\n            if self.polygon_tags['boundary'][pi]:\n                s += 'auxiliary_' + str(pi) + ','\n        s = s[:-1]\n        s += '};' + ls\n        s += 'Volume(1) = {domain_loop};' + ls\n        s += 'Physical Volume(\\\"' + \\\n             gridding_constants.GmshConstants().PHYSICAL_NAME_DOMAIN + \\\n             '\\\") = {1};' + ls\n\n        s += '// End of domain specification\\n\\n'\n        return s",
  "def __write_points(self, boundary=False):\n        p = self.pts\n        num_p = p.shape[1]\n        if p.shape[0] == 2:\n            p = np.vstack((p, np.zeros(num_p)))\n        s = '// Define points\\n'\n        for i in range(self.pts.shape[1]):\n            s += 'p' + str(i) + ' = newp; Point(p' + str(i) + ') = '\n            s += '{' + str(p[0, i]) + ', ' + str(p[1, i]) + ', '\\\n                 + str(p[2, i])\n            if self.mesh_size is not None:\n                s += ', ' + str(self.mesh_size[i]) + ' };\\n'\n            else:\n                s += '};\\n'\n\n        s += '// End of point specification\\n\\n'\n        return s",
  "def __write_lines(self, embed_in=None):\n        l = self.lines\n        num_lines = l.shape[1]\n        ls = '\\n'\n        s = '// Define lines ' + ls\n        constants = gridding_constants.GmshConstants()\n        if l.shape[0] > 2:\n            lt = l[2]\n            has_tags = True\n        else:\n            has_tags = False\n        for i in range(num_lines):\n            si = str(i)\n            s += 'frac_line_' + si + '= newl; Line(frac_line_' + si \\\n                + ') = {p' + str(l[0, i]) + ', p' + str(l[1, i]) \\\n                + '};' + ls\n            if has_tags:\n                s += 'Physical Line(\\\"'\n                if l[2, i] == constants.FRACTURE_TIP_TAG:\n                    s += constants.PHYSICAL_NAME_FRACTURE_TIP\n                elif l[2, i] == constants.FRACTURE_INTERSECTION_LINE_TAG:\n                    s += constants.PHYSICAL_NAME_FRACTURE_LINE\n                else:\n                    # This is a line that need not be physical (recognized by\n                    # the parser of output from gmsh). Applies to boundary and\n                    # subdomain boundary lines.\n                    s += constants.PHYSICAL_NAME_AUXILIARY_LINE\n\n                s += si + '\\\") = {frac_line_' + si + '};' + ls\n\n            s += ls\n        s += '// End of line specification ' + ls + ls\n        return s",
  "def __write_polygons(self, boundary=False):\n        \"\"\"\n        Writes either all fractures or all boundary planes.\n        \"\"\"\n        constants = gridding_constants.GmshConstants()\n        bound_tags = self.polygon_tags.get('boundary',\n                                       [False]*len(self.polygons[0]))\n        subd_tags = self.polygon_tags.get('subdomain',\n                                       [False]*len(self.polygons[0]))\n\n        ls = '\\n'\n        # Name boundary or fracture\n        f_or_b = 'auxiliary' if boundary else 'fracture'\n        if not boundary:\n            s = '// Start fracture specification' + ls\n        else:\n            s = ''\n        for pi in range(len(self.polygons[0])):\n            if bound_tags[pi] != boundary:\n                continue\n            # Check if the polygon is a subdomain boundary, i.e., auxiliary\n            # polygon.\n            auxiliary = subd_tags[pi]\n            if auxiliary:\n                # Keep track of \"fake fractures\", i.e., subdomain\n                # boundaries.\n                f_or_b = 'auxiliary'\n            p = self.polygons[0][pi].astype('int')\n            reverse = self.polygons[1][pi]\n            # First define line loop\n            s += 'frac_loop_' + str(pi) + ' = newll; ' + ls\n            s += 'Line Loop(frac_loop_' + str(pi) + ') = { '\n            for i, li in enumerate(p):\n                if reverse[i]:\n                    s += '-'\n                s += 'frac_line_' + str(li)\n                if i < p.size - 1:\n                    s += ', '\n\n            s += '};' + ls\n\n            n = f_or_b + '_'\n            # Then the surface\n            s += n + str(pi) + ' = news; '\n            s += 'Plane Surface(' + n + str(pi) + ') = {frac_loop_' \\\n                + str(pi) + '};' + ls\n\n            if bound_tags[pi] or auxiliary:\n                # Domain boundary or \"fake fracture\" = subdomain boundary\n                s += 'Physical Surface(\\\"' + constants.PHYSICAL_NAME_AUXILIARY \\\n                      + str(pi) + '\\\") = {auxiliary_' + str(pi) + '};' + ls\n\n            else:\n                # Normal fracture\n                s += 'Physical Surface(\\\"' + constants.PHYSICAL_NAME_FRACTURES \\\n                     + str(pi) + '\\\") = {fracture_' + str(pi) + '};' + ls\n                if self.domain is not None:\n                    s += 'Surface{' + n + str(pi) + '} In Volume{1};' + ls + ls\n\n            for li in self.e2f[pi]:\n                s += 'Line{frac_line_' + str(li) + '} In Surface{' + n\n                s += str(pi) + '};' + ls\n            s += ls\n\n        if not boundary:\n            s += '// End of fracture specification' + ls + ls\n\n        return s",
  "def __write_physical_points(self):\n        ls = '\\n'\n        s = '// Start physical point specification' + ls\n\n        constants = gridding_constants.GmshConstants()\n\n        for i, p in enumerate(self.intersection_points):\n            s += 'Physical Point(\\\"' + constants.PHYSICAL_NAME_FRACTURE_POINT \\\n                + str(i) + '\\\") = {p' + str(p) + '};' + ls\n        s += '// End of physical point specification' + ls + ls\n        return s",
  "def __write_meshing_algorithm(self):\n        # See: http://www.manpagez.com/info/gmsh/gmsh-2.4.0/gmsh_76.php\n        if self.meshing_algorithm is None:\n            return \"\"\n        else:\n            return \"\\nMesh.Algorithm = \" + str(self.meshing_algorithm) + \";\"",
  "def __init__(self, gb):\n        \"\"\"\n        Parameters:\n            gb (gridding.grid_bucket): Grid bucket to be dumped.\n\n        \"\"\"\n        self.gb = gb\n\n        # Assign ordering of the nodes in gb - used for unique identification\n        # of each grid\n        gb.assign_node_ordering()\n\n        # Compute number of grids in each dimension of the gb\n        self._num_grids()",
  "def write(self, file_name):\n        \"\"\"\n        Write the whole bucket to a .msh file.\n\n        Parameters:\n            file_name (str): Name of dump file.\n\n        \"\"\"\n        s = self._preamble()\n        s += self._physical_names()\n        s += self._points()\n        s += self._elements()\n\n        with open(file_name, 'w') as f:\n            f.write(s)",
  "def _preamble(self):\n        # Write the preamble (mesh Format) section\n        s_preamble = '$MeshFormat\\n'\n        s_preamble += '2.2 0 8\\n'\n        s_preamble += '$EndMeshFormat\\n'\n        return s_preamble",
  "def _num_grids(self):\n        # Find number of grids in each dimension\n        max_dim = 3\n        num_grids = np.zeros(max_dim + 1, dtype='int')\n        for dim in range(max_dim + 1):\n            num_grids[dim] = len(self.gb.grids_of_dimension(dim))\n\n            # Identify the highest dimension\n        while num_grids[-1] == 0:\n            num_grids = num_grids[:-1]\n\n            # We will pick the global point set from the highest dimensional\n            # grid. The current implementation assumes there is a single grid\n            # in that dimension. Taking care of multiple grids should not be\n            # difficult, but it has not been necessary up to know.\n            if num_grids[-1] != 1:\n                raise NotImplementedError('Have not considered several grids\\\n                                          in the highest dimension')\n        self.num_grids = num_grids",
  "def _points(self):\n        # The global point set\n        p = self.gb.grids_of_dimension(len(self.num_grids)-1)[0].nodes\n\n        ls = '\\n'\n        s = '$Nodes' + ls\n        s += str(p.shape[1]) + ls\n        for i in range(p.shape[1]):\n            s += str(i+1) + ' ' + str(p[0, i]) + ' ' + str(p[1, i]) + \\\n                    ' ' + str(p[2, i]) + ls\n        s += '$EndNodes' + ls\n        return s",
  "def _physical_names(self):\n        ls = '\\n'\n        s = '$PhysicalNames' + ls\n\n        # Use one physical name for each grid (including highest dimensional\n        # one)\n        s += str(self.gb.size()) + ls\n        for i, g in enumerate(self.gb):\n            dim = g[0].dim\n            s += str(dim) + ' ' + str(i+1) + ' ' + 'GRID_' + \\\n                    str(g[1]['node_number']) + '_DIM_' + str(dim) + ls\n        s += '$EndPhysicalNames' + ls\n        return s",
  "def _elements(self):\n        ls = '\\n'\n        s = '$Elements' + ls\n\n        num_cells = 0\n        for g, _ in self.gb:\n            num_cells += g.num_cells\n        s += str(num_cells) + ls\n\n        # Element types (as specified by the gmsh .msh format), index by\n        # dimensions. This assumes all cells are simplices.\n        elem_type = [15, 1, 2, 4]\n        for i, gr in enumerate(self.gb):\n            g = gr[0]\n            gn = str(gr[1]['node_number'])\n\n            # Sanity check - all cells should be simplices\n            assert np.all(np.diff(g.cell_nodes().indptr) == g.dim+1)\n\n            # Write the cell-node relation as an num_cells x dim+1 array\n            cn = g.cell_nodes().indices.reshape((g.num_cells, g.dim+1))\n\n            et = str(elem_type[g.dim])\n            for ci in range(g.num_cells):\n                s += str(ci+1) + ' ' + et + ' ' + str(1) + ' ' + gn + ' '\n                # There may be underlaying assumptions in gmsh on the ordering\n                # of nodes.\n                for d in range(g.dim+1):\n                    # Increase vertex offset by 1\n                    s += str(cn[ci, d] + 1) + ' '\n                s += ls\n\n        s += '$EndElements' + ls\n        return s",
  "class DarcyAndTransport():\n    \"\"\"\n    Wrapper for a stationary Darcy problem and a transport problem\n    on the resulting fluxes.\n    The flow and transport inputs should be members of the\n    Darcy and Parabolic classes, respectively.\n    A common application is solving the flow problem once, with subsequent\n    stepping for the transport on a static flow field. We provide the\n    convenient static_IE (implicit Euler for static flow field) option to\n    greatly reduce computational time.\n\n    \"\"\"\n\n    def __init__(self, flow, transport):\n        self.flow = flow\n        self.transport = transport\n        if not hasattr(self.flow, 'el'):\n            self.flow.el = False\n\n    def solve(self):\n        \"\"\"\n        Solve both problems.\n        \"\"\"\n        p = self.flow.step()\n        self.flow.pressure()\n        if self.flow.el:\n            SC.compute_elimination_fluxes(self.flow.full_grid, self.flow.grid(), self.flow.el_data)\n        self.flow.discharge()\n        s = self.transport.solve()\n        return p, s[self.transport.physics]\n\n    def save(self, export_every=1):\n        \"\"\"\n        Save for visualization.\n        \"\"\"\n        self.flow.save(variables=[self.flow.pressure_name])\n        self.transport.save([self.transport.physics], save_every=export_every)",
  "class static_flow_IE_solver(AbstractSolver):\n     \"\"\"\n     Implicit time discretization:\n     (y_k+1 - y_k) / dt = F^k+1\n     No diffusion and static flow field is assumed. This is an adjusted\n     version of the IE_solver in the time stepping module.\n     \"\"\"\n\n     def __init__(self, problem):\n        AbstractSolver.__init__(self, problem)\n\n     def assemble(self):\n        lhs_flux, rhs_flux = self._discretize(self.space_disc)\n        lhs_time, rhs_time = self._discretize(self.time_disc)\n\n        self.lhs = lhs_time + lhs_flux\n        self.lhs_time = lhs_time\n        self.static_rhs = rhs_flux + rhs_time\n        self.rhs = lhs_time * self.p0 + rhs_flux + rhs_time\n\n     def solve(self):\n        \"\"\"\n        Solve problem.\n        \"\"\"\n        nt = np.ceil(self.T / self.dt).astype(np.int)\n        logger.info('Time stepping using ' + str(nt) + ' steps')\n        t = self.dt\n        counter = 1\n        self.assemble()\n        IE_solver = sps.linalg.factorized((self.lhs).tocsc())\n        while t < self.T + 1e-14:\n            logger.info('Step ' + str(counter) + ' out of ' + str(nt))\n            counter += 1\n            self.update(t)\n            self.step(IE_solver)\n            logger.debug('Maximum value ' + str(self.p.max()) +\\\n                         ', minimum value ' + str(self.p.min()))\n            t += self.dt\n        self.update(t)\n        return self.data\n\n     def step(self, IE_solver):\n          self.p = IE_solver(self.lhs_time * self.p0 + self.static_rhs)\n          return self.p\n\n     def update(self, t):\n        \"\"\"\n        update parameters for next time step\n        \"\"\"\n        self.p0 = self.p\n        # Store result\n        if self.parameters['store_results'] == True:\n            self.data[self.problem.physics].append(self.p)\n            self.data['times'].append(t - self.dt)",
  "def __init__(self, flow, transport):\n        self.flow = flow\n        self.transport = transport\n        if not hasattr(self.flow, 'el'):\n            self.flow.el = False",
  "def solve(self):\n        \"\"\"\n        Solve both problems.\n        \"\"\"\n        p = self.flow.step()\n        self.flow.pressure()\n        if self.flow.el:\n            SC.compute_elimination_fluxes(self.flow.full_grid, self.flow.grid(), self.flow.el_data)\n        self.flow.discharge()\n        s = self.transport.solve()\n        return p, s[self.transport.physics]",
  "def save(self, export_every=1):\n        \"\"\"\n        Save for visualization.\n        \"\"\"\n        self.flow.save(variables=[self.flow.pressure_name])\n        self.transport.save([self.transport.physics], save_every=export_every)",
  "def __init__(self, problem):\n        AbstractSolver.__init__(self, problem)",
  "def assemble(self):\n        lhs_flux, rhs_flux = self._discretize(self.space_disc)\n        lhs_time, rhs_time = self._discretize(self.time_disc)\n\n        self.lhs = lhs_time + lhs_flux\n        self.lhs_time = lhs_time\n        self.static_rhs = rhs_flux + rhs_time\n        self.rhs = lhs_time * self.p0 + rhs_flux + rhs_time",
  "def solve(self):\n        \"\"\"\n        Solve problem.\n        \"\"\"\n        nt = np.ceil(self.T / self.dt).astype(np.int)\n        logger.info('Time stepping using ' + str(nt) + ' steps')\n        t = self.dt\n        counter = 1\n        self.assemble()\n        IE_solver = sps.linalg.factorized((self.lhs).tocsc())\n        while t < self.T + 1e-14:\n            logger.info('Step ' + str(counter) + ' out of ' + str(nt))\n            counter += 1\n            self.update(t)\n            self.step(IE_solver)\n            logger.debug('Maximum value ' + str(self.p.max()) +\\\n                         ', minimum value ' + str(self.p.min()))\n            t += self.dt\n        self.update(t)\n        return self.data",
  "def step(self, IE_solver):\n          self.p = IE_solver(self.lhs_time * self.p0 + self.static_rhs)\n          return self.p",
  "def update(self, t):\n        \"\"\"\n        update parameters for next time step\n        \"\"\"\n        self.p0 = self.p\n        # Store result\n        if self.parameters['store_results'] == True:\n            self.data[self.problem.physics].append(self.p)\n            self.data['times'].append(t - self.dt)",
  "class ParabolicModel():\n    '''\n    Base class for solving general pde problems. This class solves equations of\n    the type:\n    dT/dt + v*\\nabla T - \\nabla K \\nabla T = q\n\n    Init:\n    - gb (Grid/GridBucket) Grid or grid bucket for the problem\n    - physics (string) Physics key word. See Parameters class for valid physics\n\n    Functions:\n    data(): returns data dictionary. Is only used for single grids (I.e. not\n            GridBucket)\n    solve(): solve problem\n    step(): take one time step\n    update(t): update parameters to time t\n    reassemble(): reassemble matrices and right hand side\n    solver(): initiate solver (see numerics.pde_solver)\n    advective_disc(): discretization of the advective term\n    diffusive_disc(): discretization of the diffusive term\n    soruce_disc(): discretization of the source term, q\n    space_disc(): returns one or more of the above discretizations. If\n                  advective_disc(), source_disc() are returned we solve\n                  the problem without diffusion\n    time_disc(): returns the time discretization\n    initial_condition(): returns the initial condition for global variable\n    grid(): returns the grid bucket for the problem\n    time_step(): returns time step length\n    end_time(): returns end time\n    save(save_every=1): save solution. Parameter: save_every, save only every\n                                                  save_every time steps\n\n    Example:\n    # We create a problem with default data, neglecting the advective term\n\n    class ExampleProblem(ParabolicProblem):\n        def __init__(self, gb):\n            self._g = gb\n            ParabolicProblem.__init__(self)\n\n        def space_disc(self):\n            return self.source_disc(), self.diffusive_discr()\n    gb = meshing.cart_grid([], [10,10], physdims=[1,1])\n    for g, d in gb:\n        d['problem'] = ParabolicData(g, d)\n    problem = ExampleProblem(gb)\n    problem.solve()\n    '''\n\n    def __init__(self, gb, physics='transport',time_step=1.0, end_time=1.0, **kwargs):\n        self._gb = gb\n        self.is_GridBucket = isinstance(self._gb, GridBucket)\n        self.physics = physics\n        self._data = kwargs.get('data', dict())\n        self._time_step = time_step\n        self._end_time = end_time\n        self._set_data()\n\n        self._solver = self.solver()\n\n        logger.info('Create exporter')\n        tic = time.time()\n        file_name = kwargs.get('file_name', 'solution')\n        folder_name = kwargs.get('folder_name', 'results')\n        self.exporter = Exporter(self._gb, file_name, folder_name)\n        logger.info('Done. Elapsed time: ' + str(time.time() - tic))\n\n        self.x_name = 'solution'\n        self._time_disc = self.time_disc()\n\n    def data(self):\n        'Get data dictionary'\n        return self._data\n\n    def _set_data(self):\n        if self.is_GridBucket:\n            for _, d in self.grid():\n                d['deltaT'] = self.time_step()\n        else:\n            self.data()['deltaT'] = self.time_step()\n\n    def solve(self, save_as=None, save_every=1):\n        '''Solve problem\n\n        Arguments:\n        save_as (string), defaults to None. If a string is given, the solution\n                          variable is saved to a vtk-file as save_as\n        save_every (int), defines which time steps to save. save_every=2 will\n                          store every second time step.\n        '''\n        tic = time.time()\n        logger.info('Solve problem')\n        s = self._solver.solve(save_as, save_every)\n        logger.info('Done. Elapsed time: ' + str(time.time() - tic))\n        return s\n\n    def step(self):\n        'Take one time step'\n        return self._solver.step()\n\n    def update(self, t):\n        'Update parameters to time t'\n        if self.is_GridBucket:\n            for g, d in self.grid():\n                d[self.physics + '_data'].update(t)\n        else:\n            self.data()[self.physics + '_data'].update(t)\n\n    def split(self, x_name='solution'):\n        self.x_name = x_name\n        self._time_disc.split(self.grid(), self.x_name, self._solver.p)\n\n    def reassemble(self):\n        'Reassemble matrices and rhs'\n        return self._solver.reassemble()\n\n    def solver(self):\n        'Initiate solver'\n        return time_stepper.Implicit(self)\n\n    def advective_disc(self):\n        'Discretization of fluid_density*fluid_specific_heat * v * \\nabla T'\n\n        class WeightedUpwindDisc(upwind.Upwind):\n            def __init__(self):\n                self.physics = 'transport'\n\n            def matrix_rhs(self, g, data):\n                lhs, rhs = upwind.Upwind.matrix_rhs(self, g, data)\n                factor = data['param'].fluid_specific_heat\\\n                       * data['param'].fluid_density\n                lhs *= factor\n                rhs *= factor\n                return lhs, rhs\n\n        class WeightedUpwindCoupler(upwind.UpwindCoupling):\n            def __init__(self, discr):\n                self.physics = 'transport'\n                upwind.UpwindCoupling.__init__(self, discr)\n\n            def matrix_rhs(self, g_h, g_l, data_h, data_l, data_edge):\n                cc = upwind.UpwindCoupling.matrix_rhs(self, g_h, g_l, data_h,\n                                                      data_l, data_edge)\n                factor = data_h['param'].fluid_specific_heat \\\n                       * data_h['param'].fluid_density\n                return cc * factor\n\n        class WeightedUpwindMixedDim(upwind.UpwindMixedDim):\n\n            def __init__(self):\n                self.physics = 'transport'\n\n                self.discr = WeightedUpwindDisc()\n                self.discr_ndof = self.discr.ndof\n                self.coupling_conditions = WeightedUpwindCoupler(self.discr)\n\n                self.solver = coupler.Coupler(self.discr,\n                                             self.coupling_conditions)\n\n        if self.is_GridBucket:\n            upwind_discr = WeightedUpwindMixedDim()\n        else:\n            upwind_discr = WeightedUpwindDisc()\n        return upwind_discr\n\n    def diffusive_disc(self):\n        'Discretization of term \\nabla K \\nabla T'\n        if self.is_GridBucket:\n            diffusive_discr = tpfa.TpfaMixedDim(physics=self.physics)\n        else:\n            diffusive_discr = tpfa.Tpfa(physics=self.physics)\n        return diffusive_discr\n\n    def source_disc(self):\n        'Discretization of source term, q'\n        if self.is_GridBucket:\n            return source.IntegralMixedDim(physics=self.physics)\n        else:\n            return source.Integral(physics=self.physics)\n\n    def space_disc(self):\n        '''Space discretization. Returns the discretization terms that should be\n        used in the model'''\n        return self.advective_disc(), self.diffusive_disc(), self.source_disc()\n\n    def time_disc(self):\n        \"\"\"\n        Returns the time discretization.\n        \"\"\"\n        class TimeDisc(mass_matrix.MassMatrix):\n            def __init__(self, deltaT):\n                self.deltaT = deltaT\n\n            def matrix_rhs(self, g, data):\n                ndof = g.num_cells\n                aperture = data['param'].get_aperture()\n                coeff = g.cell_volumes * aperture / self.deltaT\n\n                factor_fluid = data['param'].fluid_specific_heat\\\n                             * data['param'].fluid_density\\\n                             * data['param'].porosity\n                factor_rock = data['param'].rock_specific_heat\\\n                             * data['param'].rock_density\\\n                             * (1 - data['param'].porosity)\n                factor = sps.dia_matrix((factor_fluid + factor_rock, 0),\n                                        shape=(ndof, ndof))\n\n                lhs = sps.dia_matrix((coeff, 0), shape=(ndof, ndof))\n                rhs = np.zeros(ndof)\n                return factor * lhs, factor * rhs\n\n        single_dim_discr = TimeDisc(self.time_step())\n        if self.is_GridBucket:\n            time_discretization = coupler.Coupler(single_dim_discr)\n        else:\n            time_discretization = TimeDisc(self.time_step())\n        return time_discretization\n\n    def initial_condition(self):\n        'Returns initial condition for global variable'\n        if self.is_GridBucket:\n            for _, d in self.grid():\n                d[self.physics] = d[self.physics + '_data'].initial_condition()\n            global_variable = self.time_disc().merge(self.grid(), self.physics)\n        else:\n            global_variable = self._data[self.physics + '_data'].initial_condition()\n        return global_variable\n\n    def grid(self):\n        'Returns grid/grid_bucket'\n        return self._gb\n\n    def time_step(self):\n        'Returns the time step'\n        return self._time_step\n\n    def end_time(self):\n        'Returns the end time'\n        return self._end_time",
  "class ParabolicDataAssigner():\n    '''\n    Base class for assigning valid data to a grid.\n    Init:\n    - g    (Grid) Grid that data should correspond to\n    - d    (dictionary) data dictionary that data will be assigned to\n    - physics (string) Physics key word. See Parameters class for valid physics\n\n    Functions:\n        update(t): Update source and bc term to time t\n        bc: Set boundary condition\n        bc_val(t): boundary condition value at time t\n        initial_condition(): initial condition for problem\n        source(): source term for problem\n        porosity(): porosity of each cell\n        diffusivity(): second order diffusivity tensor\n        aperture(): the aperture of each cell\n        rock_specific_heat(): Specific heat of the rock. Constant.\n        fluid_specific_heat(): Specific heat of the fluid. Constant.\n        rock_density(): Density of the rock. Constant.\n        fluid_density(): Density of the fluid. Constant.\n        data(): returns data dictionary\n        grid(): returns the grid g\n\n    Example:\n    # We set an inflow and outflow boundary condition by overloading the\n    # bc_val term\n    class ExampleData(ParabolicData):\n        def __init__(g, d):\n            ParabolicData.__init__(self, g, d)\n        def bc_val(self):\n            left = self.grid().nodes[0] < 1e-6\n            right = self.grid().nodes[0] > 1 - 1e-6\n            val = np.zeros(g.num_faces)\n            val[left] = 1\n            val[right] = -1\n            return val\n    gb = meshing.cart_grid([], [10,10], physdims=[1,1])\n    for g, d in gb:\n        d['problem'] = ExampleData(g, d)\n    '''\n\n    def __init__(self, g, data, physics='transport'):\n        self._g = g\n        self._data = data\n        self.physics = physics\n        self._set_data()\n\n    def update(self, t):\n        'Update source and bc_val term to time step t'\n        source = self.source(t)\n        bc_val = self.bc_val(t)\n        self.data()['param'].set_source(self.physics, source)\n        self.data()['param'].set_bc_val(self.physics, bc_val)\n\n    def bc(self):\n        'Define boundary condition'\n        dir_bound = np.array([])\n        return bc.BoundaryCondition(self.grid(), dir_bound,\n                                    ['dir'] * dir_bound.size)\n\n    def bc_val(self, t):\n        'Returns boundary condition values at time t'\n        return np.zeros(self.grid().num_faces)\n\n    def initial_condition(self):\n        'Returns initial condition'\n        return np.zeros(self.grid().num_cells)\n\n    def source(self, t):\n        'Returns source term'\n        return np.zeros(self.grid().num_cells)\n\n    def porosity(self):\n        '''Returns apperture of each cell. If None is returned, default\n        Parameter class value is used'''\n        return None\n\n    def diffusivity(self):\n        'Returns diffusivity tensor'\n        kxx = np.ones(self.grid().num_cells)\n        return tensor.SecondOrder(self.grid().dim, kxx)\n\n    def aperture(self):\n        '''Returns apperture of each cell. If None is returned, default\n        Parameter class value is used'''\n        return None\n\n    def rock_specific_heat(self):\n        \"\"\" Returns *constant* specific heat capacity of rock. If None is\n        returned, default Parameter class value is used.\n        \"\"\"\n        return None\n\n    def fluid_specific_heat(self):\n        \"\"\" Returns *constant* specific heat capacity of fluid. If None is\n        returned, default Parameter class value is used.\n        \"\"\"\n        return None\n\n    def rock_density(self):\n        \"\"\" Returns *constant* density of rock. If None is\n        returned, default Parameter class value is used.\n        \"\"\"\n        return None\n\n    def fluid_density(self):\n        \"\"\" Returns *constant* density of fluid. If None is\n        returned, default Parameter class value is used.\n        \"\"\"\n        return None\n\n    def data(self):\n        'Returns data dictionary'\n        return self._data\n\n    def grid(self):\n        'Returns grid'\n        return self._g\n\n    def _set_data(self):\n        '''Create a Parameter object and assign data based on the returned\n        values from the functions (e.g., self.source(t))\n        '''\n        if 'param' not in self._data:\n            self._data['param'] = Parameters(self.grid())\n        self._data['param'].set_tensor(self.physics, self.diffusivity())\n        self._data['param'].set_bc(self.physics, self.bc())\n        self._data['param'].set_bc_val(self.physics, self.bc_val(0.0))\n        self._data['param'].set_source(self.physics, self.source(0.0))\n\n        if self.porosity() is not None:\n            self._data['param'].set_porosity(self.porosity())\n        if self.aperture() is not None:\n            self._data['param'].set_aperture(self.aperture())\n        if self.rock_specific_heat() is not None:\n            self._data['param'].set_rock_specific_heat(self.rock_specific_heat())\n        if self.fluid_specific_heat() is not None:\n            self._data['param'].set_fluid_specific_heat(self.fluid_specific_heat())\n        if self.rock_density() is not None:\n            self._data['param'].set_rock_density(self.rock_density())\n        if self.fluid_density() is not None:\n            self._data['param'].set_fluid_density(self.fluid_density())",
  "def __init__(self, gb, physics='transport',time_step=1.0, end_time=1.0, **kwargs):\n        self._gb = gb\n        self.is_GridBucket = isinstance(self._gb, GridBucket)\n        self.physics = physics\n        self._data = kwargs.get('data', dict())\n        self._time_step = time_step\n        self._end_time = end_time\n        self._set_data()\n\n        self._solver = self.solver()\n\n        logger.info('Create exporter')\n        tic = time.time()\n        file_name = kwargs.get('file_name', 'solution')\n        folder_name = kwargs.get('folder_name', 'results')\n        self.exporter = Exporter(self._gb, file_name, folder_name)\n        logger.info('Done. Elapsed time: ' + str(time.time() - tic))\n\n        self.x_name = 'solution'\n        self._time_disc = self.time_disc()",
  "def data(self):\n        'Get data dictionary'\n        return self._data",
  "def _set_data(self):\n        if self.is_GridBucket:\n            for _, d in self.grid():\n                d['deltaT'] = self.time_step()\n        else:\n            self.data()['deltaT'] = self.time_step()",
  "def solve(self, save_as=None, save_every=1):\n        '''Solve problem\n\n        Arguments:\n        save_as (string), defaults to None. If a string is given, the solution\n                          variable is saved to a vtk-file as save_as\n        save_every (int), defines which time steps to save. save_every=2 will\n                          store every second time step.\n        '''\n        tic = time.time()\n        logger.info('Solve problem')\n        s = self._solver.solve(save_as, save_every)\n        logger.info('Done. Elapsed time: ' + str(time.time() - tic))\n        return s",
  "def step(self):\n        'Take one time step'\n        return self._solver.step()",
  "def update(self, t):\n        'Update parameters to time t'\n        if self.is_GridBucket:\n            for g, d in self.grid():\n                d[self.physics + '_data'].update(t)\n        else:\n            self.data()[self.physics + '_data'].update(t)",
  "def split(self, x_name='solution'):\n        self.x_name = x_name\n        self._time_disc.split(self.grid(), self.x_name, self._solver.p)",
  "def reassemble(self):\n        'Reassemble matrices and rhs'\n        return self._solver.reassemble()",
  "def solver(self):\n        'Initiate solver'\n        return time_stepper.Implicit(self)",
  "def advective_disc(self):\n        'Discretization of fluid_density*fluid_specific_heat * v * \\nabla T'\n\n        class WeightedUpwindDisc(upwind.Upwind):\n            def __init__(self):\n                self.physics = 'transport'\n\n            def matrix_rhs(self, g, data):\n                lhs, rhs = upwind.Upwind.matrix_rhs(self, g, data)\n                factor = data['param'].fluid_specific_heat\\\n                       * data['param'].fluid_density\n                lhs *= factor\n                rhs *= factor\n                return lhs, rhs\n\n        class WeightedUpwindCoupler(upwind.UpwindCoupling):\n            def __init__(self, discr):\n                self.physics = 'transport'\n                upwind.UpwindCoupling.__init__(self, discr)\n\n            def matrix_rhs(self, g_h, g_l, data_h, data_l, data_edge):\n                cc = upwind.UpwindCoupling.matrix_rhs(self, g_h, g_l, data_h,\n                                                      data_l, data_edge)\n                factor = data_h['param'].fluid_specific_heat \\\n                       * data_h['param'].fluid_density\n                return cc * factor\n\n        class WeightedUpwindMixedDim(upwind.UpwindMixedDim):\n\n            def __init__(self):\n                self.physics = 'transport'\n\n                self.discr = WeightedUpwindDisc()\n                self.discr_ndof = self.discr.ndof\n                self.coupling_conditions = WeightedUpwindCoupler(self.discr)\n\n                self.solver = coupler.Coupler(self.discr,\n                                             self.coupling_conditions)\n\n        if self.is_GridBucket:\n            upwind_discr = WeightedUpwindMixedDim()\n        else:\n            upwind_discr = WeightedUpwindDisc()\n        return upwind_discr",
  "def diffusive_disc(self):\n        'Discretization of term \\nabla K \\nabla T'\n        if self.is_GridBucket:\n            diffusive_discr = tpfa.TpfaMixedDim(physics=self.physics)\n        else:\n            diffusive_discr = tpfa.Tpfa(physics=self.physics)\n        return diffusive_discr",
  "def source_disc(self):\n        'Discretization of source term, q'\n        if self.is_GridBucket:\n            return source.IntegralMixedDim(physics=self.physics)\n        else:\n            return source.Integral(physics=self.physics)",
  "def space_disc(self):\n        '''Space discretization. Returns the discretization terms that should be\n        used in the model'''\n        return self.advective_disc(), self.diffusive_disc(), self.source_disc()",
  "def time_disc(self):\n        \"\"\"\n        Returns the time discretization.\n        \"\"\"\n        class TimeDisc(mass_matrix.MassMatrix):\n            def __init__(self, deltaT):\n                self.deltaT = deltaT\n\n            def matrix_rhs(self, g, data):\n                ndof = g.num_cells\n                aperture = data['param'].get_aperture()\n                coeff = g.cell_volumes * aperture / self.deltaT\n\n                factor_fluid = data['param'].fluid_specific_heat\\\n                             * data['param'].fluid_density\\\n                             * data['param'].porosity\n                factor_rock = data['param'].rock_specific_heat\\\n                             * data['param'].rock_density\\\n                             * (1 - data['param'].porosity)\n                factor = sps.dia_matrix((factor_fluid + factor_rock, 0),\n                                        shape=(ndof, ndof))\n\n                lhs = sps.dia_matrix((coeff, 0), shape=(ndof, ndof))\n                rhs = np.zeros(ndof)\n                return factor * lhs, factor * rhs\n\n        single_dim_discr = TimeDisc(self.time_step())\n        if self.is_GridBucket:\n            time_discretization = coupler.Coupler(single_dim_discr)\n        else:\n            time_discretization = TimeDisc(self.time_step())\n        return time_discretization",
  "def initial_condition(self):\n        'Returns initial condition for global variable'\n        if self.is_GridBucket:\n            for _, d in self.grid():\n                d[self.physics] = d[self.physics + '_data'].initial_condition()\n            global_variable = self.time_disc().merge(self.grid(), self.physics)\n        else:\n            global_variable = self._data[self.physics + '_data'].initial_condition()\n        return global_variable",
  "def grid(self):\n        'Returns grid/grid_bucket'\n        return self._gb",
  "def time_step(self):\n        'Returns the time step'\n        return self._time_step",
  "def end_time(self):\n        'Returns the end time'\n        return self._end_time",
  "def __init__(self, g, data, physics='transport'):\n        self._g = g\n        self._data = data\n        self.physics = physics\n        self._set_data()",
  "def update(self, t):\n        'Update source and bc_val term to time step t'\n        source = self.source(t)\n        bc_val = self.bc_val(t)\n        self.data()['param'].set_source(self.physics, source)\n        self.data()['param'].set_bc_val(self.physics, bc_val)",
  "def bc(self):\n        'Define boundary condition'\n        dir_bound = np.array([])\n        return bc.BoundaryCondition(self.grid(), dir_bound,\n                                    ['dir'] * dir_bound.size)",
  "def bc_val(self, t):\n        'Returns boundary condition values at time t'\n        return np.zeros(self.grid().num_faces)",
  "def initial_condition(self):\n        'Returns initial condition'\n        return np.zeros(self.grid().num_cells)",
  "def source(self, t):\n        'Returns source term'\n        return np.zeros(self.grid().num_cells)",
  "def porosity(self):\n        '''Returns apperture of each cell. If None is returned, default\n        Parameter class value is used'''\n        return None",
  "def diffusivity(self):\n        'Returns diffusivity tensor'\n        kxx = np.ones(self.grid().num_cells)\n        return tensor.SecondOrder(self.grid().dim, kxx)",
  "def aperture(self):\n        '''Returns apperture of each cell. If None is returned, default\n        Parameter class value is used'''\n        return None",
  "def rock_specific_heat(self):\n        \"\"\" Returns *constant* specific heat capacity of rock. If None is\n        returned, default Parameter class value is used.\n        \"\"\"\n        return None",
  "def fluid_specific_heat(self):\n        \"\"\" Returns *constant* specific heat capacity of fluid. If None is\n        returned, default Parameter class value is used.\n        \"\"\"\n        return None",
  "def rock_density(self):\n        \"\"\" Returns *constant* density of rock. If None is\n        returned, default Parameter class value is used.\n        \"\"\"\n        return None",
  "def fluid_density(self):\n        \"\"\" Returns *constant* density of fluid. If None is\n        returned, default Parameter class value is used.\n        \"\"\"\n        return None",
  "def data(self):\n        'Returns data dictionary'\n        return self._data",
  "def grid(self):\n        'Returns grid'\n        return self._g",
  "def _set_data(self):\n        '''Create a Parameter object and assign data based on the returned\n        values from the functions (e.g., self.source(t))\n        '''\n        if 'param' not in self._data:\n            self._data['param'] = Parameters(self.grid())\n        self._data['param'].set_tensor(self.physics, self.diffusivity())\n        self._data['param'].set_bc(self.physics, self.bc())\n        self._data['param'].set_bc_val(self.physics, self.bc_val(0.0))\n        self._data['param'].set_source(self.physics, self.source(0.0))\n\n        if self.porosity() is not None:\n            self._data['param'].set_porosity(self.porosity())\n        if self.aperture() is not None:\n            self._data['param'].set_aperture(self.aperture())\n        if self.rock_specific_heat() is not None:\n            self._data['param'].set_rock_specific_heat(self.rock_specific_heat())\n        if self.fluid_specific_heat() is not None:\n            self._data['param'].set_fluid_specific_heat(self.fluid_specific_heat())\n        if self.rock_density() is not None:\n            self._data['param'].set_rock_density(self.rock_density())\n        if self.fluid_density() is not None:\n            self._data['param'].set_fluid_density(self.fluid_density())",
  "class WeightedUpwindDisc(upwind.Upwind):\n            def __init__(self):\n                self.physics = 'transport'\n\n            def matrix_rhs(self, g, data):\n                lhs, rhs = upwind.Upwind.matrix_rhs(self, g, data)\n                factor = data['param'].fluid_specific_heat\\\n                       * data['param'].fluid_density\n                lhs *= factor\n                rhs *= factor\n                return lhs, rhs",
  "class WeightedUpwindCoupler(upwind.UpwindCoupling):\n            def __init__(self, discr):\n                self.physics = 'transport'\n                upwind.UpwindCoupling.__init__(self, discr)\n\n            def matrix_rhs(self, g_h, g_l, data_h, data_l, data_edge):\n                cc = upwind.UpwindCoupling.matrix_rhs(self, g_h, g_l, data_h,\n                                                      data_l, data_edge)\n                factor = data_h['param'].fluid_specific_heat \\\n                       * data_h['param'].fluid_density\n                return cc * factor",
  "class WeightedUpwindMixedDim(upwind.UpwindMixedDim):\n\n            def __init__(self):\n                self.physics = 'transport'\n\n                self.discr = WeightedUpwindDisc()\n                self.discr_ndof = self.discr.ndof\n                self.coupling_conditions = WeightedUpwindCoupler(self.discr)\n\n                self.solver = coupler.Coupler(self.discr,\n                                             self.coupling_conditions)",
  "class TimeDisc(mass_matrix.MassMatrix):\n            def __init__(self, deltaT):\n                self.deltaT = deltaT\n\n            def matrix_rhs(self, g, data):\n                ndof = g.num_cells\n                aperture = data['param'].get_aperture()\n                coeff = g.cell_volumes * aperture / self.deltaT\n\n                factor_fluid = data['param'].fluid_specific_heat\\\n                             * data['param'].fluid_density\\\n                             * data['param'].porosity\n                factor_rock = data['param'].rock_specific_heat\\\n                             * data['param'].rock_density\\\n                             * (1 - data['param'].porosity)\n                factor = sps.dia_matrix((factor_fluid + factor_rock, 0),\n                                        shape=(ndof, ndof))\n\n                lhs = sps.dia_matrix((coeff, 0), shape=(ndof, ndof))\n                rhs = np.zeros(ndof)\n                return factor * lhs, factor * rhs",
  "def __init__(self):\n                self.physics = 'transport'",
  "def matrix_rhs(self, g, data):\n                lhs, rhs = upwind.Upwind.matrix_rhs(self, g, data)\n                factor = data['param'].fluid_specific_heat\\\n                       * data['param'].fluid_density\n                lhs *= factor\n                rhs *= factor\n                return lhs, rhs",
  "def __init__(self, discr):\n                self.physics = 'transport'\n                upwind.UpwindCoupling.__init__(self, discr)",
  "def matrix_rhs(self, g_h, g_l, data_h, data_l, data_edge):\n                cc = upwind.UpwindCoupling.matrix_rhs(self, g_h, g_l, data_h,\n                                                      data_l, data_edge)\n                factor = data_h['param'].fluid_specific_heat \\\n                       * data_h['param'].fluid_density\n                return cc * factor",
  "def __init__(self):\n                self.physics = 'transport'\n\n                self.discr = WeightedUpwindDisc()\n                self.discr_ndof = self.discr.ndof\n                self.coupling_conditions = WeightedUpwindCoupler(self.discr)\n\n                self.solver = coupler.Coupler(self.discr,\n                                             self.coupling_conditions)",
  "def __init__(self, deltaT):\n                self.deltaT = deltaT",
  "def matrix_rhs(self, g, data):\n                ndof = g.num_cells\n                aperture = data['param'].get_aperture()\n                coeff = g.cell_volumes * aperture / self.deltaT\n\n                factor_fluid = data['param'].fluid_specific_heat\\\n                             * data['param'].fluid_density\\\n                             * data['param'].porosity\n                factor_rock = data['param'].rock_specific_heat\\\n                             * data['param'].rock_density\\\n                             * (1 - data['param'].porosity)\n                factor = sps.dia_matrix((factor_fluid + factor_rock, 0),\n                                        shape=(ndof, ndof))\n\n                lhs = sps.dia_matrix((coeff, 0), shape=(ndof, ndof))\n                rhs = np.zeros(ndof)\n                return factor * lhs, factor * rhs",
  "class SlightlyCompressibleModel(ParabolicModel):\n    '''\n    Inherits from ParabolicProblem\n    This class solves equations of the type:\n    phi *c_p dp/dt  - \\nabla K \\nabla p = q\n\n    Init:\n    - gb (Grid/GridBucket) Grid or grid bucket for the problem\n    - physics (string) Physics key word. See Parameters class for valid physics\n\n    functions:\n    discharge(): computes the discharges and saves it in the grid bucket as 'pressure'\n    Also see functions from ParabolicProblem\n\n    Example:\n    # We create a problem with standard data\n\n    gb = meshing.cart_grid([], [10,10], physdims=[1,1])\n    for g, d in gb:\n        d['problem'] = SlightlyCompressibleData(g, d)\n    problem = SlightlyCompressible(gb)\n    problem.solve()\n   '''\n\n    def __init__(self, gb, physics='flow', **kwargs):\n        self.is_GridBucket = isinstance(gb, GridBucket)\n        ParabolicModel.__init__(self, gb, physics, **kwargs)\n\n    def space_disc(self):\n        return self.diffusive_disc(), self.source_disc()\n\n    def time_disc(self):\n        \"\"\"\n        Returns the time discretization.\n        \"\"\"\n        class TimeDisc(mass_matrix.MassMatrix):\n            def __init__(self, deltaT):\n                self.deltaT = deltaT\n\n            def matrix_rhs(self, g, data):\n                lhs, rhs = mass_matrix.MassMatrix.matrix_rhs(self, g, data)\n                return lhs * data['compressibility'], rhs * data['compressibility']\n        time_discretization = TimeDisc(self.time_step())\n        if self.is_GridBucket:\n            time_discretization = Coupler(time_discretization)\n        return time_discretization\n\n    def discharge(self, d_name='discharge', p_name='pressure'):\n        self.pressure(p_name)\n        fvutils.compute_discharges(self.grid(), d_name=d_name, p_name=p_name)\n\n    def pressure(self, pressure_name='pressure'):\n        self.pressure_name = pressure_name\n        if self.is_GridBucket:\n            self.split(self.pressure_name)\n        else:\n            self._data[self.pressure_name] = self._solver.p",
  "class SlightlyCompressibleDataAssigner(ParabolicDataAssigner):\n    '''\n    Inherits from ParabolicData\n    Base class for assigning valid data for a slighly compressible problem.\n    Init:\n    - g    (Grid) Grid that data should correspond to\n    - d    (dictionary) data dictionary that data will be assigned to\n    - physics (string) Physics key word. See Parameters class for valid physics\n\n    Functions:\n        compressibility: (float) the compressibility of the fluid\n        permeability: (tensor.SecondOrder) The permeability tensor for the rock.\n                      Setting the permeability is equivalent to setting\n                      the ParabolicData.diffusivity() function.\n    Example:\n    # We set an inflow and outflow boundary condition by overloading the\n    # bc_val term\n    class ExampleData(SlightlyCompressibleData):\n        def __init__(g, d):\n            SlightlyCompressibleData.__init__(self, g, d)\n        def bc_val(self):\n            left = self.grid().nodes[0] < 1e-6\n            right = self.grid().nodes[0] > 1 - 1e-6\n            val = np.zeros(g.num_faces)\n            val[left] = 1\n            val[right] = -1\n            return val\n    gb = meshing.cart_grid([], [10,10], physdims=[1,1])\n    for g, d in gb:\n        d['problem'] = ExampleData(g, d)\n    '''\n\n    def __init__(self, g, data, physics='flow'):\n        ParabolicDataAssigner.__init__(self, g, data, physics)\n\n    def _set_data(self):\n        ParabolicDataAssigner._set_data(self)\n        self.data()['compressibility'] = self.compressibility()\n\n    def compressibility(self):\n        return 1.0\n\n    def permeability(self):\n        kxx = np.ones(self.grid().num_cells)\n        return tensor.SecondOrder(self.grid().dim, kxx)\n\n    def diffusivity(self):\n        return self.permeability()",
  "def __init__(self, gb, physics='flow', **kwargs):\n        self.is_GridBucket = isinstance(gb, GridBucket)\n        ParabolicModel.__init__(self, gb, physics, **kwargs)",
  "def space_disc(self):\n        return self.diffusive_disc(), self.source_disc()",
  "def time_disc(self):\n        \"\"\"\n        Returns the time discretization.\n        \"\"\"\n        class TimeDisc(mass_matrix.MassMatrix):\n            def __init__(self, deltaT):\n                self.deltaT = deltaT\n\n            def matrix_rhs(self, g, data):\n                lhs, rhs = mass_matrix.MassMatrix.matrix_rhs(self, g, data)\n                return lhs * data['compressibility'], rhs * data['compressibility']\n        time_discretization = TimeDisc(self.time_step())\n        if self.is_GridBucket:\n            time_discretization = Coupler(time_discretization)\n        return time_discretization",
  "def discharge(self, d_name='discharge', p_name='pressure'):\n        self.pressure(p_name)\n        fvutils.compute_discharges(self.grid(), d_name=d_name, p_name=p_name)",
  "def pressure(self, pressure_name='pressure'):\n        self.pressure_name = pressure_name\n        if self.is_GridBucket:\n            self.split(self.pressure_name)\n        else:\n            self._data[self.pressure_name] = self._solver.p",
  "def __init__(self, g, data, physics='flow'):\n        ParabolicDataAssigner.__init__(self, g, data, physics)",
  "def _set_data(self):\n        ParabolicDataAssigner._set_data(self)\n        self.data()['compressibility'] = self.compressibility()",
  "def compressibility(self):\n        return 1.0",
  "def permeability(self):\n        kxx = np.ones(self.grid().num_cells)\n        return tensor.SecondOrder(self.grid().dim, kxx)",
  "def diffusivity(self):\n        return self.permeability()",
  "class TimeDisc(mass_matrix.MassMatrix):\n            def __init__(self, deltaT):\n                self.deltaT = deltaT\n\n            def matrix_rhs(self, g, data):\n                lhs, rhs = mass_matrix.MassMatrix.matrix_rhs(self, g, data)\n                return lhs * data['compressibility'], rhs * data['compressibility']",
  "def __init__(self, deltaT):\n                self.deltaT = deltaT",
  "def matrix_rhs(self, g, data):\n                lhs, rhs = mass_matrix.MassMatrix.matrix_rhs(self, g, data)\n                return lhs * data['compressibility'], rhs * data['compressibility']",
  "class StaticModel():\n    '''\n    Class for solving an static elasticity problem flow problem:\n     \\nabla \\sigma = 0,\n    where nabla is the stress tensor.\n\n    Parameters in Init:\n    gb: (Grid) a grid object.\n    data (dictionary): dictionary of data. Should contain a Parameter class\n                       with the keyword 'Param'\n    physics: (string): defaults to 'mechanics'\n\n    Functions:\n    solve(): Calls reassemble and solves the linear system.\n             Returns: the displacement d.\n             Sets attributes: self.x\n    step(): Same as solve, but without reassemble of the matrices\n    reassemble(): Assembles the lhs matrix and rhs array.\n            Returns: lhs, rhs.\n            Sets attributes: self.lhs, self.rhs\n    stress_disc(): Defines the discretization of the stress term.\n            Returns stress discretization object (E.g., Mpsa)\n    grid(): Returns: the Grid or GridBucket\n    data(): Returns: Data dictionary\n    traction(name='traction'): Calculate the traction over each\n                face in the grid and assigne it to the data dictionary as\n                keyword name.\n    save(): calls split('d'). Then export the pressure to a vtk file to the\n            folder kwargs['folder_name'] with file name\n            kwargs['file_name'], default values are 'results' for the folder and\n            physics for the file name.\n    '''\n\n    def __init__(self, gb, data, physics='mechanics', **kwargs):\n        self.physics = physics\n        self._gb = gb\n        if not isinstance(self._gb, Grid):\n            raise ValueError('StaticModel only defined for Grid class')\n\n        self._data = data\n\n        self.lhs = []\n        self.rhs = []\n        self.x = []\n\n        file_name = kwargs.get('file_name', physics)\n        folder_name = kwargs.get('folder_name', 'results')\n\n        tic = time.time()\n        logger.info('Create exporter')\n        self.exporter = Exporter(self._gb, file_name, folder_name)\n        logger.info('Elapsed time: ' + str(time.time() - tic))\n\n        self._stress_disc = self.stress_disc()\n\n        self.displacement_name = 'displacement'\n        self.frac_displacement_name = 'frac_displacement'\n\n    def solve(self, max_direct=40000, callback=False, **kwargs):\n        \"\"\" Reassemble and solve linear system.\n\n        After the funtion has been called, the attributes lhs and rhs are\n        updated according to the parameter states. Also, the attribute x\n        gives the pressure given the current state.\n\n        The function attempts to set up the best linear solver based on the\n        system size. The setup and parameter choices here are still\n        experimental.\n\n        Parameters:\n            max_direct (int): Maximum number of unknowns where a direct solver\n                is applied. If a direct solver can be applied this is usually\n                the most efficient option. However, if the system size is\n                too large compared to available memory, a direct solver becomes\n                extremely slow.\n            callback (boolean, optional): If True iteration information will be\n                output when an iterative solver is applied (system size larger\n                than max_direct)\n\n        Returns:\n            np.array: Pressure state.\n\n        \"\"\"\n        # Discretize\n        tic = time.time()\n        logger.info('Discretize')\n        self.lhs, self.rhs = self.reassemble(**kwargs)\n        logger.info('Done. Elapsed time ' + str(time.time() - tic))\n\n        # Solve\n        tic = time.time()\n        ls = LSFactory()\n\n        if self.rhs.size <  max_direct:\n            logger.info('Solve linear system using direct solver')\n            self.x = ls.direct(self.lhs,self.rhs)\n        else:\n            logger.info('Solve linear system using GMRES')\n            precond = self._setup_preconditioner()\n#            precond = ls.ilu(self.lhs)\n            slv = ls.gmres(self.lhs)\n            self.x, info = slv(self.rhs, M=precond, callback=callback,\n                               maxiter=10000, restart=1500, tol=1e-8)\n            if info == 0:\n                logger.info('GMRES succeeded.')\n            else:\n                logger.error('GMRES failed with status ' + str(info))\n\n        logger.info('Done. Elapsed time ' + str(time.time() - tic))\n        return self.x\n\n    def step(self, **kwargs):\n        \"\"\"\n        Calls self.solve(**kwargs)\n        \"\"\"\n        return self.solve(**kwargs)\n\n    def reassemble(self, discretize=True):\n        \"\"\"\n        reassemble matrices. This must be called between every time step to\n        update the rhs of the system.\n        \"\"\"\n        self.lhs, self.rhs = self._stress_disc.matrix_rhs(self.grid(), self.data(), discretize)\n        return self.lhs, self.rhs\n\n    def stress_disc(self):\n        \"\"\"\n        Define the stress discretization. \n        Returns:\n            FracturedMpsa (Solver object)\n        \"\"\"\n        return mpsa.FracturedMpsa(physics=self.physics)\n\n    def grid(self):\n        \"\"\"\n        get the model grid\n        Returns:\n            gb (Grid object)\n        \"\"\"\n        return self._gb\n\n    def data(self):\n        \"\"\"\n        get data\n        Returns:\n            data (Dictionary)\n        \"\"\"\n        return self._data\n\n    def displacement(self, displacement_name='displacement'):\n        \"\"\"\n        Save the cell displacement to the data dictionary. The displacement\n        will be saved as a (3,  self.grid().num_cells) array\n        Parameters: \n        -----------\n        displacement_name:    (string) Defaults to 'displacement'. Defines the\n                              keyword for the saved displacement in the data\n                              dictionary\n        Returns:\n        --------\n        d:  (ndarray) the displacement as a (3, self.grid().num_cells) array\n        \"\"\"\n\n        self.displacement_name = displacement_name\n        d = self._stress_disc.extract_u(self.grid(), self.x)\n        d = d.reshape((3, -1),order='F')\n        self._data[self.displacement_name] = d\n        return d\n\n    def frac_displacement(self, frac_displacement_name='frac_displacement'):\n        \"\"\"\n        Save the fracture displacement to the data dictionary. This is the \n        displacement on the fracture facers. The displacement\n        will be saved as a (3,  self.grid().num_cells) array\n        Parameters: \n        -----------\n        frac_displacement_name: \n            (string) Defaults to 'frac_displacement'. Defines the keyword for\n            the saved displacement in the data dictionary\n\n        Returns:\n        --------\n        d:  (ndarray) the displacement of size (3, #number of fracture faces)\n        \"\"\"\n        self.frac_displacement_name =frac_displacement_name\n        d = self._stress_disc.extract_frac_u(self.grid(), self.x)\n        d = d.reshape((3, -1),order='F')\n        self._data[self.frac_displacement_name] = d\n        return d\n\n    def traction(self, traction_name='traction'):\n        \"\"\"\n        Save the  traction on faces to the data dictionary. This is the \n        area scaled traction on the fracture facers. The displacement\n        will be saved as a (3,  self.grid().num_cells) array\n        Parameters: \n        -----------\n        traction_name\n            (string) Defaults to 'traction'. Defines the keyword for the\n            saved traction in the data dictionary\n\n        Returns:\n        --------\n        d:  (ndarray) the traction as a (3, self.grid().num_faces) array\n        \"\"\"\n        T = self._stress_disc.traction(self.grid(),\n                                       self._data,\n                                       self.x)\n        T = T.reshape((self.grid().dim, -1), order='F')\n        T_b = np.zeros(T.shape)\n        sigma = self._data['param'].get_background_stress(self.physics)\n        if np.any(sigma):\n            normals = self.grid().face_normals\n            for i in range(normals.shape[1]):\n                T_b[:, i] = np.dot(normals[:, i], sigma)\n        else:\n            T_b = 0\n        self._data[traction_name] = T + T_b\n\n    def save(self, variables=None, time_step=None):\n        \"\"\"\n        Save the result as vtk. \n\n        Parameters:\n        ----------\n        variables: (list) Optional, defaults to None. If None, only the grid\n            will be exported. A list of strings where each element defines a\n            keyword in the data dictionary to be saved.\n        time_step: (float) optinal, defaults to None. The time step of the\n            variable(s) that is saved\n        \"\"\"\n\n        if variables is None:\n            self.exporter.write_vtk()\n        else: \n            variables = {k: self._data[k] for k in variables \\\n                         if k in self._data}\n            self.exporter.write_vtk(variables, time_step=time_step)\n\n    ### Helper functions for linear solve below\n    def _setup_preconditioner(self):\n        solvers, ind, not_ind = self._assign_solvers()\n\n        def precond(r):\n            x = np.zeros_like(r)\n            for s, i, ni in zip(solvers, ind, not_ind):\n                x[i] += s(r[i])\n            return x\n        def precond_mult(r):\n            x = np.zeros_like(r)\n            A = self.lhs\n            for s, i, ni in zip(solvers, ind, not_ind):\n                r_i = r[i] - A[i, :][:, ni] * x[ni]\n                x[i] += s(r_i)\n            return x\n\n        M = lambda r: precond(r)\n        return spl.LinearOperator(self.lhs.shape, M)\n\n\n    def _assign_solvers(self):\n        mat, ind = self._obtain_submatrix()\n        all_ind = np.arange(self.rhs.size)\n        not_ind = [np.setdiff1d(all_ind, i) for i in ind]\n\n        factory = LSFactory()\n        num_mat = len(mat)\n        solvers = np.empty(num_mat, dtype=np.object)\n        for i, A in enumerate(mat):\n            sz = A.shape[0]\n            if sz < 5000:\n                solvers[i] = factory.direct(A)\n            else:\n                # amg solver is pyamg is installed, if not ilu\n                try:\n                    solvers[i] = factory.amg(A, as_precond=True)\n                except ImportError:\n                    solvers[i] = factory.ilu(A)\n\n        return solvers, ind, not_ind\n\n\n    def _obtain_submatrix(self):\n            return [self.lhs], [np.arange(self.grid().num_cells)]",
  "class StaticDataAssigner():\n    '''\n    Class for setting data to a linear elastic static problem:\n    \\nabla \\sigma = 0,\n    This class creates a Parameter object and assigns the data to this object\n    by calling StaticDataAssigner's functions.\n\n    To change the default values, create a class that inherits from\n    StaticDataAssigner. Then overload the values you whish to change.\n\n    Parameters in Init:\n    gb (Grid): a grid object \n    data (dictionary): Dictionary which Parameter will be added to with keyword\n                       'param'\n    physics: (string): defaults to 'mechanics'\n\n    Functions that assign data to Parameter class:\n        bc(): defaults to neumann boundary condition\n             Returns: (Object) boundary condition\n        bc_val(): defaults to 0\n             returns: (ndarray) boundary condition values\n        stress_tensor(): defaults to 1\n             returns: (tensor.FourthOrder) Stress tensor\n\n    Utility functions:\n        grid(): returns: the grid\n\n    '''\n\n    def __init__(self, g, data, physics='mechanics'):\n        self._g = g\n        self._data = data\n\n        self.physics = physics\n        self._set_data()\n\n    def bc(self):\n        return bc.BoundaryCondition(self.grid())\n\n    def bc_val(self):\n        return np.zeros(self.grid().dim * self.grid().num_faces)\n\n    def background_stress(self):\n        sigma = -np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n        return sigma\n\n    def stress_tensor(self):\n        return None\n\n    def data(self):\n        return self._data\n\n    def grid(self):\n        return self._g\n\n    def _set_data(self):\n        if 'param' not in self._data:\n            self._data['param'] = Parameters(self.grid())\n        self._data['param'].set_bc(self.physics, self.bc())\n        self._data['param'].set_bc_val(self.physics, self.bc_val())\n        self._data['param'].set_background_stress(self.physics, self.background_stress())        \n        if self.stress_tensor() is not None:\n            self._data['param'].set_tensor(self.physics, self.stress_tensor())",
  "def __init__(self, gb, data, physics='mechanics', **kwargs):\n        self.physics = physics\n        self._gb = gb\n        if not isinstance(self._gb, Grid):\n            raise ValueError('StaticModel only defined for Grid class')\n\n        self._data = data\n\n        self.lhs = []\n        self.rhs = []\n        self.x = []\n\n        file_name = kwargs.get('file_name', physics)\n        folder_name = kwargs.get('folder_name', 'results')\n\n        tic = time.time()\n        logger.info('Create exporter')\n        self.exporter = Exporter(self._gb, file_name, folder_name)\n        logger.info('Elapsed time: ' + str(time.time() - tic))\n\n        self._stress_disc = self.stress_disc()\n\n        self.displacement_name = 'displacement'\n        self.frac_displacement_name = 'frac_displacement'",
  "def solve(self, max_direct=40000, callback=False, **kwargs):\n        \"\"\" Reassemble and solve linear system.\n\n        After the funtion has been called, the attributes lhs and rhs are\n        updated according to the parameter states. Also, the attribute x\n        gives the pressure given the current state.\n\n        The function attempts to set up the best linear solver based on the\n        system size. The setup and parameter choices here are still\n        experimental.\n\n        Parameters:\n            max_direct (int): Maximum number of unknowns where a direct solver\n                is applied. If a direct solver can be applied this is usually\n                the most efficient option. However, if the system size is\n                too large compared to available memory, a direct solver becomes\n                extremely slow.\n            callback (boolean, optional): If True iteration information will be\n                output when an iterative solver is applied (system size larger\n                than max_direct)\n\n        Returns:\n            np.array: Pressure state.\n\n        \"\"\"\n        # Discretize\n        tic = time.time()\n        logger.info('Discretize')\n        self.lhs, self.rhs = self.reassemble(**kwargs)\n        logger.info('Done. Elapsed time ' + str(time.time() - tic))\n\n        # Solve\n        tic = time.time()\n        ls = LSFactory()\n\n        if self.rhs.size <  max_direct:\n            logger.info('Solve linear system using direct solver')\n            self.x = ls.direct(self.lhs,self.rhs)\n        else:\n            logger.info('Solve linear system using GMRES')\n            precond = self._setup_preconditioner()\n#            precond = ls.ilu(self.lhs)\n            slv = ls.gmres(self.lhs)\n            self.x, info = slv(self.rhs, M=precond, callback=callback,\n                               maxiter=10000, restart=1500, tol=1e-8)\n            if info == 0:\n                logger.info('GMRES succeeded.')\n            else:\n                logger.error('GMRES failed with status ' + str(info))\n\n        logger.info('Done. Elapsed time ' + str(time.time() - tic))\n        return self.x",
  "def step(self, **kwargs):\n        \"\"\"\n        Calls self.solve(**kwargs)\n        \"\"\"\n        return self.solve(**kwargs)",
  "def reassemble(self, discretize=True):\n        \"\"\"\n        reassemble matrices. This must be called between every time step to\n        update the rhs of the system.\n        \"\"\"\n        self.lhs, self.rhs = self._stress_disc.matrix_rhs(self.grid(), self.data(), discretize)\n        return self.lhs, self.rhs",
  "def stress_disc(self):\n        \"\"\"\n        Define the stress discretization. \n        Returns:\n            FracturedMpsa (Solver object)\n        \"\"\"\n        return mpsa.FracturedMpsa(physics=self.physics)",
  "def grid(self):\n        \"\"\"\n        get the model grid\n        Returns:\n            gb (Grid object)\n        \"\"\"\n        return self._gb",
  "def data(self):\n        \"\"\"\n        get data\n        Returns:\n            data (Dictionary)\n        \"\"\"\n        return self._data",
  "def displacement(self, displacement_name='displacement'):\n        \"\"\"\n        Save the cell displacement to the data dictionary. The displacement\n        will be saved as a (3,  self.grid().num_cells) array\n        Parameters: \n        -----------\n        displacement_name:    (string) Defaults to 'displacement'. Defines the\n                              keyword for the saved displacement in the data\n                              dictionary\n        Returns:\n        --------\n        d:  (ndarray) the displacement as a (3, self.grid().num_cells) array\n        \"\"\"\n\n        self.displacement_name = displacement_name\n        d = self._stress_disc.extract_u(self.grid(), self.x)\n        d = d.reshape((3, -1),order='F')\n        self._data[self.displacement_name] = d\n        return d",
  "def frac_displacement(self, frac_displacement_name='frac_displacement'):\n        \"\"\"\n        Save the fracture displacement to the data dictionary. This is the \n        displacement on the fracture facers. The displacement\n        will be saved as a (3,  self.grid().num_cells) array\n        Parameters: \n        -----------\n        frac_displacement_name: \n            (string) Defaults to 'frac_displacement'. Defines the keyword for\n            the saved displacement in the data dictionary\n\n        Returns:\n        --------\n        d:  (ndarray) the displacement of size (3, #number of fracture faces)\n        \"\"\"\n        self.frac_displacement_name =frac_displacement_name\n        d = self._stress_disc.extract_frac_u(self.grid(), self.x)\n        d = d.reshape((3, -1),order='F')\n        self._data[self.frac_displacement_name] = d\n        return d",
  "def traction(self, traction_name='traction'):\n        \"\"\"\n        Save the  traction on faces to the data dictionary. This is the \n        area scaled traction on the fracture facers. The displacement\n        will be saved as a (3,  self.grid().num_cells) array\n        Parameters: \n        -----------\n        traction_name\n            (string) Defaults to 'traction'. Defines the keyword for the\n            saved traction in the data dictionary\n\n        Returns:\n        --------\n        d:  (ndarray) the traction as a (3, self.grid().num_faces) array\n        \"\"\"\n        T = self._stress_disc.traction(self.grid(),\n                                       self._data,\n                                       self.x)\n        T = T.reshape((self.grid().dim, -1), order='F')\n        T_b = np.zeros(T.shape)\n        sigma = self._data['param'].get_background_stress(self.physics)\n        if np.any(sigma):\n            normals = self.grid().face_normals\n            for i in range(normals.shape[1]):\n                T_b[:, i] = np.dot(normals[:, i], sigma)\n        else:\n            T_b = 0\n        self._data[traction_name] = T + T_b",
  "def save(self, variables=None, time_step=None):\n        \"\"\"\n        Save the result as vtk. \n\n        Parameters:\n        ----------\n        variables: (list) Optional, defaults to None. If None, only the grid\n            will be exported. A list of strings where each element defines a\n            keyword in the data dictionary to be saved.\n        time_step: (float) optinal, defaults to None. The time step of the\n            variable(s) that is saved\n        \"\"\"\n\n        if variables is None:\n            self.exporter.write_vtk()\n        else: \n            variables = {k: self._data[k] for k in variables \\\n                         if k in self._data}\n            self.exporter.write_vtk(variables, time_step=time_step)",
  "def _setup_preconditioner(self):\n        solvers, ind, not_ind = self._assign_solvers()\n\n        def precond(r):\n            x = np.zeros_like(r)\n            for s, i, ni in zip(solvers, ind, not_ind):\n                x[i] += s(r[i])\n            return x\n        def precond_mult(r):\n            x = np.zeros_like(r)\n            A = self.lhs\n            for s, i, ni in zip(solvers, ind, not_ind):\n                r_i = r[i] - A[i, :][:, ni] * x[ni]\n                x[i] += s(r_i)\n            return x\n\n        M = lambda r: precond(r)\n        return spl.LinearOperator(self.lhs.shape, M)",
  "def _assign_solvers(self):\n        mat, ind = self._obtain_submatrix()\n        all_ind = np.arange(self.rhs.size)\n        not_ind = [np.setdiff1d(all_ind, i) for i in ind]\n\n        factory = LSFactory()\n        num_mat = len(mat)\n        solvers = np.empty(num_mat, dtype=np.object)\n        for i, A in enumerate(mat):\n            sz = A.shape[0]\n            if sz < 5000:\n                solvers[i] = factory.direct(A)\n            else:\n                # amg solver is pyamg is installed, if not ilu\n                try:\n                    solvers[i] = factory.amg(A, as_precond=True)\n                except ImportError:\n                    solvers[i] = factory.ilu(A)\n\n        return solvers, ind, not_ind",
  "def _obtain_submatrix(self):\n            return [self.lhs], [np.arange(self.grid().num_cells)]",
  "def __init__(self, g, data, physics='mechanics'):\n        self._g = g\n        self._data = data\n\n        self.physics = physics\n        self._set_data()",
  "def bc(self):\n        return bc.BoundaryCondition(self.grid())",
  "def bc_val(self):\n        return np.zeros(self.grid().dim * self.grid().num_faces)",
  "def background_stress(self):\n        sigma = -np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n        return sigma",
  "def stress_tensor(self):\n        return None",
  "def data(self):\n        return self._data",
  "def grid(self):\n        return self._g",
  "def _set_data(self):\n        if 'param' not in self._data:\n            self._data['param'] = Parameters(self.grid())\n        self._data['param'].set_bc(self.physics, self.bc())\n        self._data['param'].set_bc_val(self.physics, self.bc_val())\n        self._data['param'].set_background_stress(self.physics, self.background_stress())        \n        if self.stress_tensor() is not None:\n            self._data['param'].set_tensor(self.physics, self.stress_tensor())",
  "def precond(r):\n            x = np.zeros_like(r)\n            for s, i, ni in zip(solvers, ind, not_ind):\n                x[i] += s(r[i])\n            return x",
  "def precond_mult(r):\n            x = np.zeros_like(r)\n            A = self.lhs\n            for s, i, ni in zip(solvers, ind, not_ind):\n                r_i = r[i] - A[i, :][:, ni] * x[ni]\n                x[i] += s(r_i)\n            return x",
  "class AbstractSolver(object):\n    \"\"\"\n    Abstract base class for solving a general first order time pde problem.\n    dT/dt + G(T) = 0,\n    where G(T) is a space discretization\n    \"\"\"\n\n    def __init__(self, problem):\n        \"\"\"\n        Parameters:\n        ----------\n        problem: a problem class. Must have the attributes\n            problem.grid()\n            problem.data()\n            problem.space_disc()\n            problem.time_disc()\n            problem.time_step()\n            problem.end_time()\n            problem.initial_pressure()\n        \"\"\"\n        # Get data\n        g = problem.grid()\n\n        data = problem.data()\n        data[problem.physics] = []\n        data['times'] = []\n\n        p0 = problem.initial_condition()\n        p = p0\n\n        self.problem = problem\n        self.g = g\n        self.data = data\n        self.dt = problem.time_step()\n        self.T = problem.end_time()\n        self.space_disc = problem.space_disc()\n        self.time_disc = problem.time_disc()\n        self.p0 = p0\n        self.p = p\n        # First initial empty lhs and rhs, then initialize them through\n        # reassemble\n        self.lhs = []\n        self.rhs = []\n\n    def solve(self, save_as=None, save_every=1):\n        \"\"\"\n        Solve problem.\n        \"\"\"\n        nt = np.ceil(self.T / self.dt).astype(np.int)\n        logger.warning('Time stepping using ' + str(nt) + ' steps')\n        t = self.dt\n        counter = 0\n        if not save_as is None:\n            self.problem.split(save_as)\n            self.problem.exporter.write_vtk([save_as], time_step=counter)\n            times = [0.0]\n\n        while t < self.T *(1 + 1e-14):\n            logger.warning('Step ' + str(counter) + ' out of ' + str(nt))\n            counter += 1\n            self.update(t)\n            self.reassemble()\n            self.step()\n            logger.debug('Maximum value ' + str(self.p.max()) +\\\n                         ', minimum value ' + str(self.p.min()))\n            if not save_as is None and np.mod(counter, save_every)==0:\n                logger.info('Saving solution')\n                self.problem.split(save_as)\n                self.problem.exporter.write_vtk([save_as], time_step=counter)\n                times.append(t)\n                logger.info('Finished saving')\n            t += self.dt\n\n        if not save_as is None:\n            self.problem.exporter.write_pvd(np.asarray(times))\n\n        return self.data\n\n    def step(self):\n        \"\"\"\n        Take one time step\n        \"\"\"\n        ls = LSFactory()\n        self.p = ls.direct(self.lhs, self.rhs)\n        return self.p\n\n    def update(self, t):\n        \"\"\"\n        update parameters for next time step\n        \"\"\"\n        self.problem.update(t)\n        self.p0 = self.p\n\n    def reassemble(self):\n        \"\"\"\n        reassemble matrices. This must be called between every time step to\n        update the rhs of the system.\n        \"\"\"\n        raise NotImplementedError(\n            'subclass must overload function reasemble()')\n\n    def _discretize(self, discs):\n        if isinstance(self.g, GridBucket):\n            if not isinstance(discs, tuple):\n                discs = [discs]\n            lhs, rhs = np.array(discs[0].matrix_rhs(self.g))\n            for disc in discs[1:]:\n                lhs_n, rhs_n = disc.matrix_rhs(self.g)\n                lhs += lhs_n\n                rhs += rhs_n\n        else:\n            if not isinstance(discs, tuple):\n                discs = [discs]\n            lhs, rhs = discs[0].matrix_rhs(self.g, self.data)\n            for disc in discs[1:]:\n                lhs_n, rhs_n = disc.matrix_rhs(self.g, self.data)\n                lhs += lhs_n\n                rhs += rhs_n\n        return lhs, rhs",
  "class Implicit(AbstractSolver):\n    \"\"\"\n    Implicit time discretization:\n    (y_k+1 - y_k) / dt = F^k+1\n    \"\"\"\n\n    def __init__(self, problem):\n        AbstractSolver.__init__(self, problem)\n\n    def reassemble(self):\n        lhs_flux, rhs_flux = self._discretize(self.space_disc)\n        lhs_time, rhs_time = self._discretize(self.time_disc)\n\n        self.lhs = lhs_time + lhs_flux\n        self.rhs = lhs_time * self.p0 + rhs_flux + rhs_time",
  "class BDF2(AbstractSolver):\n    \"\"\"\n    Second order implicit time discretization:\n    (y_k+2 - 4/3 * y_k+1 + 1/3 * y_k) / dt = 2/3 * F^k+2\n    \"\"\"\n\n    def __init__(self, problem):\n        self.flag_first = True\n        AbstractSolver.__init__(self, problem)\n        self.p_1 = self.p0\n\n    def update(self, t):\n        \"\"\"\n        update parameters for next time step\n        \"\"\"\n        if t > self.dt + 1e-6:\n            self.flag_first = False\n        else:\n            self.flag_first = True\n        self.p_1 = self.p0\n        AbstractSolver.update(self, t)\n\n    def reassemble(self):\n\n        lhs_flux, rhs_flux = self._discretize(self.space_disc)\n        lhs_time, rhs_time = self._discretize(self.time_disc)\n\n        if self.flag_first:\n            self.lhs = lhs_time + lhs_flux\n            self.rhs = lhs_time * self.p0 + rhs_flux + rhs_time\n        else:\n            self.lhs = lhs_time + 2. / 3 * lhs_flux\n            bdf2_rhs = 4. / 3 * lhs_time * self.p0 - 1. / 3 * lhs_time * self.p_1\n            self.rhs = bdf2_rhs + 2. / 3 * rhs_flux + rhs_time",
  "class Explicit(AbstractSolver):\n    \"\"\"\n    Explicit time discretization:\n    (y_k - y_k-1)/dt = F^k\n    \"\"\"\n\n    def __init__(self, problem):\n        AbstractSolver.__init__(self, problem)\n\n    def solve(self, save_as=None, save_every=1):\n        \"\"\"\n        Solve problem.\n        \"\"\"\n        t = 0.0\n        counter = 0\n        # Save initial condition\n        if not save_as is None:\n            self.problem.split(save_as)\n            self.problem.exporter.write_vtk([save_as], time_step=counter)\n            times = [0.0]\n\n        while t < self.T - self.dt + 1e-14:\n            self.update(t)\n            self.reassemble()\n            self.step()\n            # Save time step\n            if not save_as is None and np.mod(counter, save_every)==0:\n                logger.info('Saving solution')\n                self.problem.split(save_as)\n                self.problem.exporter.write_vtk([save_as], time_step=counter)\n                times.append(t)\n                logger.info('Finished saving')\n            t += self.dt\n\n        # Write pvd\n        if not save_as is None:\n            self.problem.exporter.write_pvd(np.asarray(times))\n\n        return self.data\n\n    def reassemble(self):\n\n        lhs_flux, rhs_flux = self._discretize(self.space_disc)\n        lhs_time, rhs_time = self._discretize(self.time_disc)\n\n        self.lhs = lhs_time\n        self.rhs = (lhs_time - lhs_flux) * self.p0 + rhs_flux + rhs_time",
  "class CrankNicolson(AbstractSolver):\n    \"\"\"\n    Crank-Nicolson time discretization:\n    (y_k+1 - y_k) / dt = 0.5 * (F^k+1 + F^k)\n    \"\"\"\n\n    def __init__(self, problem):\n        self.g = problem.grid()\n        self.lhs_flux, self.rhs_flux = self._discretize(problem.space_disc())\n        self.lhs_time, self.rhs_time = self._discretize(problem.time_disc())\n        self.lhs_flux_0 = self.lhs_flux\n        self.rhs_flux_0 = self.rhs_flux\n        self.lhs_time_0 = self.lhs_time\n        self.rhs_time_0 = self.rhs_time\n        AbstractSolver.__init__(self, problem)\n\n    def update(self, t):\n        \"\"\"\n        update parameters for next time step\n        \"\"\"\n        AbstractSolver.update(self, t)\n        self.lhs_flux_0 = self.lhs_flux\n        self.rhs_flux_0 = self.rhs_flux\n        self.lhs_time_0 = self.lhs_time\n        self.rhs_time_0 = self.rhs_time\n\n    def reassemble(self):\n        self.lhs_flux, self.rhs_flux = self._discretize(self.space_disc)\n        self.lhs_time, self.rhs_time = self._discretize(self.time_disc)\n\n        rhs1 = 0.5 * (self.rhs_flux + self.rhs_time)\n        rhs0 = 0.5 * (self.rhs_flux_0 + self.rhs_time_0)\n        self.lhs = self.lhs_time + 0.5 * self.lhs_flux\n        self.rhs = (self.lhs_time - 0.5 * self.lhs_flux_0) * \\\n            self.p0 + rhs1 + rhs0",
  "def __init__(self, problem):\n        \"\"\"\n        Parameters:\n        ----------\n        problem: a problem class. Must have the attributes\n            problem.grid()\n            problem.data()\n            problem.space_disc()\n            problem.time_disc()\n            problem.time_step()\n            problem.end_time()\n            problem.initial_pressure()\n        \"\"\"\n        # Get data\n        g = problem.grid()\n\n        data = problem.data()\n        data[problem.physics] = []\n        data['times'] = []\n\n        p0 = problem.initial_condition()\n        p = p0\n\n        self.problem = problem\n        self.g = g\n        self.data = data\n        self.dt = problem.time_step()\n        self.T = problem.end_time()\n        self.space_disc = problem.space_disc()\n        self.time_disc = problem.time_disc()\n        self.p0 = p0\n        self.p = p\n        # First initial empty lhs and rhs, then initialize them through\n        # reassemble\n        self.lhs = []\n        self.rhs = []",
  "def solve(self, save_as=None, save_every=1):\n        \"\"\"\n        Solve problem.\n        \"\"\"\n        nt = np.ceil(self.T / self.dt).astype(np.int)\n        logger.warning('Time stepping using ' + str(nt) + ' steps')\n        t = self.dt\n        counter = 0\n        if not save_as is None:\n            self.problem.split(save_as)\n            self.problem.exporter.write_vtk([save_as], time_step=counter)\n            times = [0.0]\n\n        while t < self.T *(1 + 1e-14):\n            logger.warning('Step ' + str(counter) + ' out of ' + str(nt))\n            counter += 1\n            self.update(t)\n            self.reassemble()\n            self.step()\n            logger.debug('Maximum value ' + str(self.p.max()) +\\\n                         ', minimum value ' + str(self.p.min()))\n            if not save_as is None and np.mod(counter, save_every)==0:\n                logger.info('Saving solution')\n                self.problem.split(save_as)\n                self.problem.exporter.write_vtk([save_as], time_step=counter)\n                times.append(t)\n                logger.info('Finished saving')\n            t += self.dt\n\n        if not save_as is None:\n            self.problem.exporter.write_pvd(np.asarray(times))\n\n        return self.data",
  "def step(self):\n        \"\"\"\n        Take one time step\n        \"\"\"\n        ls = LSFactory()\n        self.p = ls.direct(self.lhs, self.rhs)\n        return self.p",
  "def update(self, t):\n        \"\"\"\n        update parameters for next time step\n        \"\"\"\n        self.problem.update(t)\n        self.p0 = self.p",
  "def reassemble(self):\n        \"\"\"\n        reassemble matrices. This must be called between every time step to\n        update the rhs of the system.\n        \"\"\"\n        raise NotImplementedError(\n            'subclass must overload function reasemble()')",
  "def _discretize(self, discs):\n        if isinstance(self.g, GridBucket):\n            if not isinstance(discs, tuple):\n                discs = [discs]\n            lhs, rhs = np.array(discs[0].matrix_rhs(self.g))\n            for disc in discs[1:]:\n                lhs_n, rhs_n = disc.matrix_rhs(self.g)\n                lhs += lhs_n\n                rhs += rhs_n\n        else:\n            if not isinstance(discs, tuple):\n                discs = [discs]\n            lhs, rhs = discs[0].matrix_rhs(self.g, self.data)\n            for disc in discs[1:]:\n                lhs_n, rhs_n = disc.matrix_rhs(self.g, self.data)\n                lhs += lhs_n\n                rhs += rhs_n\n        return lhs, rhs",
  "def __init__(self, problem):\n        AbstractSolver.__init__(self, problem)",
  "def reassemble(self):\n        lhs_flux, rhs_flux = self._discretize(self.space_disc)\n        lhs_time, rhs_time = self._discretize(self.time_disc)\n\n        self.lhs = lhs_time + lhs_flux\n        self.rhs = lhs_time * self.p0 + rhs_flux + rhs_time",
  "def __init__(self, problem):\n        self.flag_first = True\n        AbstractSolver.__init__(self, problem)\n        self.p_1 = self.p0",
  "def update(self, t):\n        \"\"\"\n        update parameters for next time step\n        \"\"\"\n        if t > self.dt + 1e-6:\n            self.flag_first = False\n        else:\n            self.flag_first = True\n        self.p_1 = self.p0\n        AbstractSolver.update(self, t)",
  "def reassemble(self):\n\n        lhs_flux, rhs_flux = self._discretize(self.space_disc)\n        lhs_time, rhs_time = self._discretize(self.time_disc)\n\n        if self.flag_first:\n            self.lhs = lhs_time + lhs_flux\n            self.rhs = lhs_time * self.p0 + rhs_flux + rhs_time\n        else:\n            self.lhs = lhs_time + 2. / 3 * lhs_flux\n            bdf2_rhs = 4. / 3 * lhs_time * self.p0 - 1. / 3 * lhs_time * self.p_1\n            self.rhs = bdf2_rhs + 2. / 3 * rhs_flux + rhs_time",
  "def __init__(self, problem):\n        AbstractSolver.__init__(self, problem)",
  "def solve(self, save_as=None, save_every=1):\n        \"\"\"\n        Solve problem.\n        \"\"\"\n        t = 0.0\n        counter = 0\n        # Save initial condition\n        if not save_as is None:\n            self.problem.split(save_as)\n            self.problem.exporter.write_vtk([save_as], time_step=counter)\n            times = [0.0]\n\n        while t < self.T - self.dt + 1e-14:\n            self.update(t)\n            self.reassemble()\n            self.step()\n            # Save time step\n            if not save_as is None and np.mod(counter, save_every)==0:\n                logger.info('Saving solution')\n                self.problem.split(save_as)\n                self.problem.exporter.write_vtk([save_as], time_step=counter)\n                times.append(t)\n                logger.info('Finished saving')\n            t += self.dt\n\n        # Write pvd\n        if not save_as is None:\n            self.problem.exporter.write_pvd(np.asarray(times))\n\n        return self.data",
  "def reassemble(self):\n\n        lhs_flux, rhs_flux = self._discretize(self.space_disc)\n        lhs_time, rhs_time = self._discretize(self.time_disc)\n\n        self.lhs = lhs_time\n        self.rhs = (lhs_time - lhs_flux) * self.p0 + rhs_flux + rhs_time",
  "def __init__(self, problem):\n        self.g = problem.grid()\n        self.lhs_flux, self.rhs_flux = self._discretize(problem.space_disc())\n        self.lhs_time, self.rhs_time = self._discretize(problem.time_disc())\n        self.lhs_flux_0 = self.lhs_flux\n        self.rhs_flux_0 = self.rhs_flux\n        self.lhs_time_0 = self.lhs_time\n        self.rhs_time_0 = self.rhs_time\n        AbstractSolver.__init__(self, problem)",
  "def update(self, t):\n        \"\"\"\n        update parameters for next time step\n        \"\"\"\n        AbstractSolver.update(self, t)\n        self.lhs_flux_0 = self.lhs_flux\n        self.rhs_flux_0 = self.rhs_flux\n        self.lhs_time_0 = self.lhs_time\n        self.rhs_time_0 = self.rhs_time",
  "def reassemble(self):\n        self.lhs_flux, self.rhs_flux = self._discretize(self.space_disc)\n        self.lhs_time, self.rhs_time = self._discretize(self.time_disc)\n\n        rhs1 = 0.5 * (self.rhs_flux + self.rhs_time)\n        rhs0 = 0.5 * (self.rhs_flux_0 + self.rhs_time_0)\n        self.lhs = self.lhs_time + 0.5 * self.lhs_flux\n        self.rhs = (self.lhs_time - 0.5 * self.lhs_flux_0) * \\\n            self.p0 + rhs1 + rhs0",
  "class EllipticModel():\n    '''\n    Class for solving an incompressible flow problem:\n    \\nabla K \\nabla p = q,\n    where K is the second order permeability tenser, p the fluid pressure\n    and q sinks and sources.\n\n    Parameters in Init:\n    gb: (Grid /GridBucket) a grid or grid bucket object. If gb = GridBucket\n        a Parameter class should be added to each grid bucket data node with\n        keyword 'param'.\n    data: (dictionary) Defaults to None. Only used if gb is a Grid. Should\n          contain a Parameter class with the keyword 'Param'\n    physics: (string): defaults to 'flow'\n\n    Functions:\n    solve(): Calls reassemble and solves the linear system.\n             Returns: the pressure p.\n             Sets attributes: self.x\n    step(): Same as solve, but without reassemble of the matrices\n    reassemble(): Assembles the lhs matrix and rhs array.\n            Returns: lhs, rhs.\n            Sets attributes: self.lhs, self.rhs\n    source_disc(): Defines the discretization of the source term.\n            Returns Source discretization object\n    flux_disc(): Defines the discretization of the flux term.\n            Returns Flux discretization object (E.g., Tpfa)\n    grid(): Returns: the Grid or GridBucket\n    data(): Returns: Data dictionary\n    split(name): Assignes the solution self.x to the data dictionary at each\n                 node in the GridBucket.\n                 Parameters:\n                    name: (string) The keyword assigned to the pressure\n    discharge(): Calls split('pressure'). Then calculate the discharges over each\n                 face in the grids and between edges in the GridBucket\n    save(): calls split('pressure'). Then export the pressure to a vtk file to the\n            folder kwargs['folder_name'] with file name\n            kwargs['file_name'], default values are 'results' for the folder and\n            physics for the file name.\n    '''\n\n    def __init__(self, gb, data=None, physics='flow', **kwargs):\n        self.physics = physics\n        self._gb = gb\n        self.is_GridBucket = isinstance(self._gb, GridBucket)\n        self._data = data\n\n        self.lhs = []\n        self.rhs = []\n        self.x = []\n\n        file_name = kwargs.get('file_name', physics)\n        folder_name = kwargs.get('folder_name', 'results')\n        mesh_kw = kwargs.get('mesh_kw', {})\n\n        tic = time.time()\n        logger.info('Create exporter')\n        self.exporter = Exporter(self._gb, file_name, folder_name, **mesh_kw)\n        logger.info('Elapsed time: ' + str(time.time() - tic))\n\n        self._flux_disc = self.flux_disc()\n        self._source_disc = self.source_disc()\n\n    def solve(self, max_direct=40000, callback=False, **kwargs):\n        \"\"\" Reassemble and solve linear system.\n\n        After the funtion has been called, the attributes lhs and rhs are\n        updated according to the parameter states. Also, the attribute x\n        gives the pressure given the current state.\n\n        TODO: Provide an option to save solver information if multiple\n        systems are to be solved with the same left hand side.\n\n        The function attempts to set up the best linear solver based on the\n        system size. The setup and parameter choices here are still\n        experimental.\n\n        Parameters:\n            max_direct (int): Maximum number of unknowns where a direct solver\n                is applied. If a direct solver can be applied this is usually\n                the most efficient option. However, if the system size is\n                too large compared to available memory, a direct solver becomes\n                extremely slow.\n            callback (boolean, optional): If True iteration information will be\n                output when an iterative solver is applied (system size larger\n                than max_direct)\n\n        Returns:\n            np.array: Pressure state.\n\n        \"\"\"\n        logger.error('Solve elliptic model')\n        # Discretize\n        tic = time.time()\n        logger.warning('Discretize')\n        self.lhs, self.rhs = self.reassemble()\n        logger.warning('Done. Elapsed time ' + str(time.time() - tic))\n\n        # Solve\n        tic = time.time()\n        ls = LSFactory()\n        if self.rhs.size < max_direct:\n            logger.warning('Solve linear system using direct solver')\n            self.x = ls.direct(self.lhs, self.rhs)\n        else:\n            logger.warning('Solve linear system using GMRES')\n            precond = self._setup_preconditioner()\n#            precond = ls.ilu(self.lhs)\n            slv = ls.gmres(self.lhs)\n            self.x, info = slv(self.rhs, M=precond, callback=callback,\n                               maxiter=10000, restart=1500, tol=1e-8)\n            if info == 0:\n                logger.warning('GMRES succeeded.')\n            else:\n                logger.warning('GMRES failed with status ' + str(info))\n\n        logger.warning('Done. Elapsed time ' + str(time.time() - tic))\n        return self.x\n\n    def step(self):\n        return self.solve()\n\n    def reassemble(self):\n        \"\"\"\n        reassemble matrices. This must be called between every time step to\n        update the rhs of the system.\n        \"\"\"\n        lhs_flux, rhs_flux = self._discretize(self._flux_disc)\n        lhs_source, rhs_source = self._discretize(self._source_disc)\n        assert lhs_source.nnz == 0, 'Source lhs different from zero!'\n        self.lhs = lhs_flux\n        self.rhs = rhs_flux + rhs_source\n        return self.lhs, self.rhs\n\n    def source_disc(self):\n        if self.is_GridBucket:\n            return source.IntegralMixedDim(physics=self.physics)\n        else:\n            return source.Integral(physics=self.physics)\n\n    def flux_disc(self):\n        if self.is_GridBucket:\n            return tpfa.TpfaMixedDim(physics=self.physics)\n        else:\n            return tpfa.Tpfa(physics=self.physics)\n\n    def _discretize(self, discr):\n        if self.is_GridBucket:\n            return discr.matrix_rhs(self.grid())\n        else:\n            return discr.matrix_rhs(self.grid(), self.data())\n\n    def grid(self):\n        return self._gb\n\n    def data(self):\n        return self._data\n\n    def split(self, x_name='solution'):\n        self.x_name = x_name\n        self._flux_disc.split(self.grid(), self.x_name, self.x)\n\n    def pressure(self, pressure_name='pressure'):\n        self.pressure_name = pressure_name\n        if self.is_GridBucket:\n            self.split(self.pressure_name)\n        else:\n            self._data[self.pressure_name] = self.x\n\n    def discharge(self, discharge_name='discharge'):\n        if self.is_GridBucket:\n            fvutils.compute_discharges(self.grid(), self.physics,\n                                       p_name=self.pressure_name)\n        else:\n            fvutils.compute_discharges(self.grid(), self.physics,\n                                       self.pressure_name,\n                                       self._data)\n\n    def permeability(self, perm_names=['kxx', 'kyy', 'kzz']):\n        \"\"\" Assign permeability to self._data, ready for export to vtk.\n\n        For the moment, we only dump the main diagonals of the permeabliity.\n        Extensions should be trivial if needed.\n\n        Parameters:\n            perm_names (list): Which components to export. Defaults to kxx,\n                kyy and xzz.\n\n        \"\"\"\n\n        def get_ind(n):\n            if n == 'kxx':\n                return 0\n            elif n == 'kyy':\n                return 1\n            elif n == 'kzz':\n                return 2\n            else:\n                raise ValueError('Unknown perm keyword ' + n)\n\n        for n in perm_names:\n            ind = get_ind(n)\n            if self.is_GridBucket:\n                for _, d in self.grid():\n                    d[n] = d['param'].get_permeability().perm[ind, ind, :]\n            else:\n                self._data[n] = self._data['param'].get_permeability()\\\n                    .perm[ind, ind, :]\n\n    def porosity(self, poro_name='porosity'):\n        if self.is_GridBucket:\n            for _, d in self.grid():\n                d[poro_name] = d['param'].get_porosity()\n        else:\n            self._data[poro_name] = self._data['param'].get_porosity()\n\n    def save(self, variables=None, save_every=None):\n        if variables is None:\n            self.exporter.write_vtk()\n        else:\n            if not self.is_GridBucket:\n                variables = {k: self._data[k] for k in variables\n                             if k in self._data}\n            self.exporter.write_vtk(variables)\n\n    # Helper functions for linear solve below\n    def _setup_preconditioner(self):\n        solvers, ind, not_ind = self._assign_solvers()\n\n        def precond(r):\n            x = np.zeros_like(r)\n            for s, i, ni in zip(solvers, ind, not_ind):\n                x[i] += s(r[i])\n            return x\n\n        def precond_mult(r):\n            x = np.zeros_like(r)\n            A = self.lhs\n            for s, i, ni in zip(solvers, ind, not_ind):\n                r_i = r[i] - A[i, :][:, ni] * x[ni]\n                x[i] += s(r_i)\n            return x\n\n        def M(r): return precond(r)\n        return spl.LinearOperator(self.lhs.shape, M)\n\n    def _assign_solvers(self):\n        mat, ind = self._obtain_submatrix()\n        all_ind = np.arange(self.rhs.size)\n        not_ind = [np.setdiff1d(all_ind, i) for i in ind]\n\n        factory = LSFactory()\n        num_mat = len(mat)\n        solvers = np.empty(num_mat, dtype=np.object)\n        for i, A in enumerate(mat):\n            sz = A.shape[0]\n            if sz < 5000:\n                solvers[i] = factory.direct(A)\n            else:\n                # amg solver is pyamg is installed, if not ilu\n                try:\n                    solvers[i] = factory.amg(A, as_precond=True)\n                except ImportError:\n                    solvers[i] = factory.ilu(A)\n\n        return solvers, ind, not_ind\n\n    def _obtain_submatrix(self):\n\n        if isinstance(self.grid(), GridBucket):\n            gb = self.grid()\n            fd = self.flux_disc()\n            mat = []\n            sub_ind = []\n            for g, _ in self.grid():\n                ind = fd.solver.dof_of_grid(gb, g)\n                A = self.lhs[ind, :][:, ind]\n                mat.append(A)\n                sub_ind.append(ind)\n            return mat, sub_ind\n        else:\n            return [self.lhs], [np.arange(self.grid().num_cells)]",
  "class DualEllipticModel(EllipticModel):\n\n    def __init__(self, gb, data=None, physics='flow', **kwargs):\n        EllipticModel.__init__(self, gb, data, physics, **kwargs)\n\n        self.discharge_name = str()\n        self.projected_discharge_name = str()\n\n    def source_disc(self):\n        if self.is_GridBucket:\n            return vem_source.IntegralMixedDim(physics=self.physics)\n        else:\n            return vem_source.Integral(physics=self.physics)\n\n    def flux_disc(self):\n        if self.is_GridBucket:\n            return vem_dual.DualVEMMixedDim(physics=self.physics)\n        else:\n            return vem_dual.DualVEM(physics=self.physics)\n\n    def solve(self):\n        \"\"\" Discretize and solve linear system by a direct solver.\n\n        The saddle point structure of the dual discretization implies that\n        other block solvers are needed. TODO.\n\n        \"\"\"\n        ls = LSFactory()\n        self.x = ls.direct(*self.reassemble())\n        return self.x\n\n    def pressure(self, pressure_name='pressure'):\n        self.pressure_name = pressure_name\n        if self.is_GridBucket:\n            self._flux_disc.extract_p(\n                self._gb, self.x_name, self.pressure_name)\n        else:\n            pressure = self._flux_disc.extract_p(self._gb, self.x)\n            self._data[self.pressure_name] = pressure\n\n    def discharge(self, discharge_name=\"discharge\"):\n        self.discharge_name = discharge_name\n        if self.is_GridBucket:\n            self._flux_disc.extract_u(\n                self._gb, self.x_name, self.discharge_name)\n        else:\n            discharge = self._flux_disc.extract_u(self._gb, self.x)\n            self._data[self.discharge_name] = discharge\n\n    def project_discharge(self, projected_discharge_name=\"P0u\"):\n        if self.discharge_name is str():\n            self.discharge()\n        self.projected_discharge_name = projected_discharge_name\n        if self.is_GridBucket:\n            self._flux_disc.project_u(self._gb, self.discharge_name,\n                                      self.projected_discharge_name)\n        else:\n            discharge = self._data[self.discharge_name]\n            projected_discharge = self._flux_disc.project_u(self._gb,\n                                                            discharge, self._data)\n            self._data[self.projected_discharge_name] = projected_discharge",
  "class EllipticDataAssigner():\n    '''\n    Class for setting data to an incompressible flow problem:\n    \\nabla K \\nabla p = q,\n    where K is the second order permeability tenser, p the fluid pressure\n    and q sinks and sources. This class creates a Parameter object and\n    assigns the data to this object by calling EllipticData's functions.\n\n    To change the default values create a class that inherits from EllipticData.\n    Then overload the values you whish to change.\n\n    Parameters in Init:\n    gb: (Grid /GridBucket) a grid or grid bucket object\n    data: (dictionary) Dictionary which Parameter will be added to with keyword\n          'param'\n    physics: (string): defaults to 'flow'\n\n    Functions that assign data to Parameter class:\n        bc(): defaults to neumann boundary condition\n             Returns: (Object) boundary condition\n        bc_val(): defaults to 0\n             returns: (ndarray) boundary condition values\n        porosity(): defaults to 1\n             returns: (ndarray) porosity of each cell\n        apperture(): defaults to 1\n             returns: (ndarray) aperture of each cell\n        permeability(): defaults to 1\n             returns: (tensor.SecondOrder) Permeabillity tensor\n        source(): defaults to 0\n             returns: (ndarray) The source and sinks\n\n    Utility functions:\n        grid(): returns: the grid\n\n    '''\n\n    def __init__(self, g, data, physics='flow'):\n        self._g = g\n        self._data = data\n\n        self.physics = physics\n        self._set_data()\n\n    def bc(self):\n        return bc.BoundaryCondition(self.grid())\n\n    def bc_val(self):\n        return np.zeros(self.grid().num_faces)\n\n    def porosity(self):\n        '''Returns apperture of each cell. If None is returned, default\n        Parameter class value is used'''\n        return None\n\n    def aperture(self):\n        '''Returns apperture of each cell. If None is returned, default\n        Parameter class value is used'''\n        return None\n\n    def permeability(self):\n        kxx = np.ones(self.grid().num_cells)\n        return tensor.SecondOrder(self.grid().dim, kxx)\n\n    def source(self):\n        return np.zeros(self.grid().num_cells)\n\n    def data(self):\n        return self._data\n\n    def grid(self):\n        return self._g\n\n    def _set_data(self):\n        if 'param' not in self._data:\n            self._data['param'] = Parameters(self.grid())\n        self._data['param'].set_tensor(self.physics, self.permeability())\n        self._data['param'].set_bc(self.physics, self.bc())\n        self._data['param'].set_bc_val(self.physics, self.bc_val())\n        self._data['param'].set_source(self.physics, self.source())\n\n        if self.porosity() is not None:\n            self._data['param'].set_porosity(self.porosity())\n        if self.aperture() is not None:\n            self._data['param'].set_aperture(self.aperture())",
  "def __init__(self, gb, data=None, physics='flow', **kwargs):\n        self.physics = physics\n        self._gb = gb\n        self.is_GridBucket = isinstance(self._gb, GridBucket)\n        self._data = data\n\n        self.lhs = []\n        self.rhs = []\n        self.x = []\n\n        file_name = kwargs.get('file_name', physics)\n        folder_name = kwargs.get('folder_name', 'results')\n        mesh_kw = kwargs.get('mesh_kw', {})\n\n        tic = time.time()\n        logger.info('Create exporter')\n        self.exporter = Exporter(self._gb, file_name, folder_name, **mesh_kw)\n        logger.info('Elapsed time: ' + str(time.time() - tic))\n\n        self._flux_disc = self.flux_disc()\n        self._source_disc = self.source_disc()",
  "def solve(self, max_direct=40000, callback=False, **kwargs):\n        \"\"\" Reassemble and solve linear system.\n\n        After the funtion has been called, the attributes lhs and rhs are\n        updated according to the parameter states. Also, the attribute x\n        gives the pressure given the current state.\n\n        TODO: Provide an option to save solver information if multiple\n        systems are to be solved with the same left hand side.\n\n        The function attempts to set up the best linear solver based on the\n        system size. The setup and parameter choices here are still\n        experimental.\n\n        Parameters:\n            max_direct (int): Maximum number of unknowns where a direct solver\n                is applied. If a direct solver can be applied this is usually\n                the most efficient option. However, if the system size is\n                too large compared to available memory, a direct solver becomes\n                extremely slow.\n            callback (boolean, optional): If True iteration information will be\n                output when an iterative solver is applied (system size larger\n                than max_direct)\n\n        Returns:\n            np.array: Pressure state.\n\n        \"\"\"\n        logger.error('Solve elliptic model')\n        # Discretize\n        tic = time.time()\n        logger.warning('Discretize')\n        self.lhs, self.rhs = self.reassemble()\n        logger.warning('Done. Elapsed time ' + str(time.time() - tic))\n\n        # Solve\n        tic = time.time()\n        ls = LSFactory()\n        if self.rhs.size < max_direct:\n            logger.warning('Solve linear system using direct solver')\n            self.x = ls.direct(self.lhs, self.rhs)\n        else:\n            logger.warning('Solve linear system using GMRES')\n            precond = self._setup_preconditioner()\n#            precond = ls.ilu(self.lhs)\n            slv = ls.gmres(self.lhs)\n            self.x, info = slv(self.rhs, M=precond, callback=callback,\n                               maxiter=10000, restart=1500, tol=1e-8)\n            if info == 0:\n                logger.warning('GMRES succeeded.')\n            else:\n                logger.warning('GMRES failed with status ' + str(info))\n\n        logger.warning('Done. Elapsed time ' + str(time.time() - tic))\n        return self.x",
  "def step(self):\n        return self.solve()",
  "def reassemble(self):\n        \"\"\"\n        reassemble matrices. This must be called between every time step to\n        update the rhs of the system.\n        \"\"\"\n        lhs_flux, rhs_flux = self._discretize(self._flux_disc)\n        lhs_source, rhs_source = self._discretize(self._source_disc)\n        assert lhs_source.nnz == 0, 'Source lhs different from zero!'\n        self.lhs = lhs_flux\n        self.rhs = rhs_flux + rhs_source\n        return self.lhs, self.rhs",
  "def source_disc(self):\n        if self.is_GridBucket:\n            return source.IntegralMixedDim(physics=self.physics)\n        else:\n            return source.Integral(physics=self.physics)",
  "def flux_disc(self):\n        if self.is_GridBucket:\n            return tpfa.TpfaMixedDim(physics=self.physics)\n        else:\n            return tpfa.Tpfa(physics=self.physics)",
  "def _discretize(self, discr):\n        if self.is_GridBucket:\n            return discr.matrix_rhs(self.grid())\n        else:\n            return discr.matrix_rhs(self.grid(), self.data())",
  "def grid(self):\n        return self._gb",
  "def data(self):\n        return self._data",
  "def split(self, x_name='solution'):\n        self.x_name = x_name\n        self._flux_disc.split(self.grid(), self.x_name, self.x)",
  "def pressure(self, pressure_name='pressure'):\n        self.pressure_name = pressure_name\n        if self.is_GridBucket:\n            self.split(self.pressure_name)\n        else:\n            self._data[self.pressure_name] = self.x",
  "def discharge(self, discharge_name='discharge'):\n        if self.is_GridBucket:\n            fvutils.compute_discharges(self.grid(), self.physics,\n                                       p_name=self.pressure_name)\n        else:\n            fvutils.compute_discharges(self.grid(), self.physics,\n                                       self.pressure_name,\n                                       self._data)",
  "def permeability(self, perm_names=['kxx', 'kyy', 'kzz']):\n        \"\"\" Assign permeability to self._data, ready for export to vtk.\n\n        For the moment, we only dump the main diagonals of the permeabliity.\n        Extensions should be trivial if needed.\n\n        Parameters:\n            perm_names (list): Which components to export. Defaults to kxx,\n                kyy and xzz.\n\n        \"\"\"\n\n        def get_ind(n):\n            if n == 'kxx':\n                return 0\n            elif n == 'kyy':\n                return 1\n            elif n == 'kzz':\n                return 2\n            else:\n                raise ValueError('Unknown perm keyword ' + n)\n\n        for n in perm_names:\n            ind = get_ind(n)\n            if self.is_GridBucket:\n                for _, d in self.grid():\n                    d[n] = d['param'].get_permeability().perm[ind, ind, :]\n            else:\n                self._data[n] = self._data['param'].get_permeability()\\\n                    .perm[ind, ind, :]",
  "def porosity(self, poro_name='porosity'):\n        if self.is_GridBucket:\n            for _, d in self.grid():\n                d[poro_name] = d['param'].get_porosity()\n        else:\n            self._data[poro_name] = self._data['param'].get_porosity()",
  "def save(self, variables=None, save_every=None):\n        if variables is None:\n            self.exporter.write_vtk()\n        else:\n            if not self.is_GridBucket:\n                variables = {k: self._data[k] for k in variables\n                             if k in self._data}\n            self.exporter.write_vtk(variables)",
  "def _setup_preconditioner(self):\n        solvers, ind, not_ind = self._assign_solvers()\n\n        def precond(r):\n            x = np.zeros_like(r)\n            for s, i, ni in zip(solvers, ind, not_ind):\n                x[i] += s(r[i])\n            return x\n\n        def precond_mult(r):\n            x = np.zeros_like(r)\n            A = self.lhs\n            for s, i, ni in zip(solvers, ind, not_ind):\n                r_i = r[i] - A[i, :][:, ni] * x[ni]\n                x[i] += s(r_i)\n            return x\n\n        def M(r): return precond(r)\n        return spl.LinearOperator(self.lhs.shape, M)",
  "def _assign_solvers(self):\n        mat, ind = self._obtain_submatrix()\n        all_ind = np.arange(self.rhs.size)\n        not_ind = [np.setdiff1d(all_ind, i) for i in ind]\n\n        factory = LSFactory()\n        num_mat = len(mat)\n        solvers = np.empty(num_mat, dtype=np.object)\n        for i, A in enumerate(mat):\n            sz = A.shape[0]\n            if sz < 5000:\n                solvers[i] = factory.direct(A)\n            else:\n                # amg solver is pyamg is installed, if not ilu\n                try:\n                    solvers[i] = factory.amg(A, as_precond=True)\n                except ImportError:\n                    solvers[i] = factory.ilu(A)\n\n        return solvers, ind, not_ind",
  "def _obtain_submatrix(self):\n\n        if isinstance(self.grid(), GridBucket):\n            gb = self.grid()\n            fd = self.flux_disc()\n            mat = []\n            sub_ind = []\n            for g, _ in self.grid():\n                ind = fd.solver.dof_of_grid(gb, g)\n                A = self.lhs[ind, :][:, ind]\n                mat.append(A)\n                sub_ind.append(ind)\n            return mat, sub_ind\n        else:\n            return [self.lhs], [np.arange(self.grid().num_cells)]",
  "def __init__(self, gb, data=None, physics='flow', **kwargs):\n        EllipticModel.__init__(self, gb, data, physics, **kwargs)\n\n        self.discharge_name = str()\n        self.projected_discharge_name = str()",
  "def source_disc(self):\n        if self.is_GridBucket:\n            return vem_source.IntegralMixedDim(physics=self.physics)\n        else:\n            return vem_source.Integral(physics=self.physics)",
  "def flux_disc(self):\n        if self.is_GridBucket:\n            return vem_dual.DualVEMMixedDim(physics=self.physics)\n        else:\n            return vem_dual.DualVEM(physics=self.physics)",
  "def solve(self):\n        \"\"\" Discretize and solve linear system by a direct solver.\n\n        The saddle point structure of the dual discretization implies that\n        other block solvers are needed. TODO.\n\n        \"\"\"\n        ls = LSFactory()\n        self.x = ls.direct(*self.reassemble())\n        return self.x",
  "def pressure(self, pressure_name='pressure'):\n        self.pressure_name = pressure_name\n        if self.is_GridBucket:\n            self._flux_disc.extract_p(\n                self._gb, self.x_name, self.pressure_name)\n        else:\n            pressure = self._flux_disc.extract_p(self._gb, self.x)\n            self._data[self.pressure_name] = pressure",
  "def discharge(self, discharge_name=\"discharge\"):\n        self.discharge_name = discharge_name\n        if self.is_GridBucket:\n            self._flux_disc.extract_u(\n                self._gb, self.x_name, self.discharge_name)\n        else:\n            discharge = self._flux_disc.extract_u(self._gb, self.x)\n            self._data[self.discharge_name] = discharge",
  "def project_discharge(self, projected_discharge_name=\"P0u\"):\n        if self.discharge_name is str():\n            self.discharge()\n        self.projected_discharge_name = projected_discharge_name\n        if self.is_GridBucket:\n            self._flux_disc.project_u(self._gb, self.discharge_name,\n                                      self.projected_discharge_name)\n        else:\n            discharge = self._data[self.discharge_name]\n            projected_discharge = self._flux_disc.project_u(self._gb,\n                                                            discharge, self._data)\n            self._data[self.projected_discharge_name] = projected_discharge",
  "def __init__(self, g, data, physics='flow'):\n        self._g = g\n        self._data = data\n\n        self.physics = physics\n        self._set_data()",
  "def bc(self):\n        return bc.BoundaryCondition(self.grid())",
  "def bc_val(self):\n        return np.zeros(self.grid().num_faces)",
  "def porosity(self):\n        '''Returns apperture of each cell. If None is returned, default\n        Parameter class value is used'''\n        return None",
  "def aperture(self):\n        '''Returns apperture of each cell. If None is returned, default\n        Parameter class value is used'''\n        return None",
  "def permeability(self):\n        kxx = np.ones(self.grid().num_cells)\n        return tensor.SecondOrder(self.grid().dim, kxx)",
  "def source(self):\n        return np.zeros(self.grid().num_cells)",
  "def data(self):\n        return self._data",
  "def grid(self):\n        return self._g",
  "def _set_data(self):\n        if 'param' not in self._data:\n            self._data['param'] = Parameters(self.grid())\n        self._data['param'].set_tensor(self.physics, self.permeability())\n        self._data['param'].set_bc(self.physics, self.bc())\n        self._data['param'].set_bc_val(self.physics, self.bc_val())\n        self._data['param'].set_source(self.physics, self.source())\n\n        if self.porosity() is not None:\n            self._data['param'].set_porosity(self.porosity())\n        if self.aperture() is not None:\n            self._data['param'].set_aperture(self.aperture())",
  "def get_ind(n):\n            if n == 'kxx':\n                return 0\n            elif n == 'kyy':\n                return 1\n            elif n == 'kzz':\n                return 2\n            else:\n                raise ValueError('Unknown perm keyword ' + n)",
  "def precond(r):\n            x = np.zeros_like(r)\n            for s, i, ni in zip(solvers, ind, not_ind):\n                x[i] += s(r[i])\n            return x",
  "def precond_mult(r):\n            x = np.zeros_like(r)\n            A = self.lhs\n            for s, i, ni in zip(solvers, ind, not_ind):\n                r_i = r[i] - A[i, :][:, ni] * x[ni]\n                x[i] += s(r_i)\n            return x",
  "def M(r): return precond(r)",
  "class FrictionSlipModel():\n    '''\n    Class for solving a frictional slip problem: T_s <= mu * (T_n -p)\n    \n\n    Parameters in Init:\n    gb: (Grid) a Grid Object.\n    data: (dictionary) Should contain a Parameter class with the keyword\n        'Param'\n    physics: (string): defaults to 'slip'\n\n    Functions:\n    solve(): Calls reassemble and solves the linear system.\n             Returns: new slip if T_s > mu * (T_n - p)\n             Sets attributes: self.x, self.is_slipping, self.d_n\n    step(): Same as solve\n    normal_shear_traction(): project the traction into the corresponding\n                             normal and shear components\n    grid(): Returns: the Grid or GridBucket\n    data(): Returns: Data dictionary\n    fracture_dilation(slip_distance): Returns: the amount of dilation for given\n                                               slip distance\n    slip_distance(): saves the slip distance to the data dictionary and\n                     returns it\n    aperture_change(): saves the aperture change to the data dictionary and\n                       returns it\n    mu(faces): returns: the coefficient of friction\n    gamma(): returns: the numerical step length parameter\n    save(): calls split('pressure'). Then export the pressure to a vtk file to the\n            folder kwargs['folder_name'] with file name\n            kwargs['file_name'], default values are 'results' for the folder and\n            physics for the file name.\n    '''\n\n    def __init__(self, gb, data, physics='slip', **kwargs):\n        self.physics = physics\n        if isinstance(gb, GridBucket):\n            raise ValueError('FrictionSlip excpected a Grid, not a GridBucket')\n\n        self._gb = gb\n        self._data = data\n\n        file_name = kwargs.get('file_name', physics)\n        folder_name = kwargs.get('folder_name', 'results')\n\n        tic = time.time()\n        logger.info('Create exporter')\n        self.exporter = Exporter(self._gb, file_name, folder_name)\n        logger.info('Elapsed time: ' + str(time.time() - tic))\n\n        self.x = np.zeros((3, gb.num_faces))\n        self.d_n = np.zeros(gb.num_faces)\n\n        self.is_slipping = np.zeros(gb.num_faces, dtype=np.bool)\n\n        self.slip_name = 'slip_distance'\n        self.aperture_name = 'aperture_change'\n\n    def solve(self):\n        \"\"\" Linearize and solve corresponding system\n\n        First, the function calculate if the slip-criterion is satisfied for\n        each face: T_s <= mu * (T_n - p).\n        If this is violated, the fracture is slipping. It estimates the slip\n        in direction of shear traction as:\n        d += T_s - mu(T_n - p) * sqrt(face_area) / G.\n\n        Stores this result in self.x which is a ndarray of dimension\n        (3, number of faces). Also updates the ndarray self.is_slipping to\n        True for any face that violated the slip-criterion.\n\n        Requires the following keywords in the data dictionary:\n        'face_pressure': (ndarray) size equal number of faces in the grid.\n                         Only the pressure on the fracture faces are used, and\n                         should be equivalent to the pressure in the\n                        pressure in the corresponding lower dimensional cells.\n        'traction': (ndarray) size (3, number_of_faces). This should be the \n                    area scaled traction on each face.\n        'rock': a Rock Object with shear stiffness Rock.MU defined.\n\n        Returns:\n            new_slip (bool) returns True if the slip vector was violated for\n                     any faces\n        \"\"\"\n        assert self._gb.dim == 3, 'only support for 3D (yet)'\n\n        frac_faces = self._gb.frac_pairs\n        fi = frac_faces[1]\n        fi_left = frac_faces[0]\n        T_n, T_s, n, t = self.normal_shear_traction(fi)\n        \n        assert np.all(T_s > -1e-10)\n        assert np.all(T_n < 0), 'Must have a normal force on the fracture'\n\n        # we find the effective normal stress on the fracture face.\n        # Here we need to multiply T_n with -1 as we want the absolute value,\n        # and all the normal tractions should be negative.\n        sigma_n = -T_n - self._data['face_pressure'][fi]\n        assert np.all(sigma_n > 0 )\n        new_slip = T_s - \\\n            self.mu(fi, self.is_slipping[fi]) * \\\n            sigma_n > 1e-5 * self._data['rock'].MU\n\n        self.is_slipping[fi] = self.is_slipping[fi] | new_slip\n        excess_shear = np.abs(\n            T_s) - self.mu(fi, self.is_slipping[fi]) * sigma_n\n        shear_stiffness = np.sqrt(\n            self._gb.face_areas[fi]) / (self._data['rock'].MU)\n        slip_d = excess_shear * shear_stiffness * self.gamma() * new_slip\n\n        # We also add the values to the left cells so that when we average the\n        # face values to obtain a cell value, it will equal the face value\n        self.d_n[fi] += self.fracture_dilation(slip_d)\n        self.d_n[fi_left] += self.fracture_dilation(slip_d)\n        assert np.all(self.d_n[fi] >-1e-6)\n        slip_vec =  -t * slip_d - n * self.fracture_dilation(slip_d)\n        \n        self.x[:, fi] += slip_vec\n        self.x[:, fi_left] -= slip_vec\n\n        return new_slip\n\n    def normal_shear_traction(self, faces=None):\n        \"\"\"\n        Project the traction vector into the normal and tangential components\n        as seen from the fractures. \n        Requires that the data dictionary has keyword:\n        traction:  (ndarray) size (3, number of faces). giving the area\n                   weighted traction on each face.\n        \n        Returns:\n        --------\n        T_n:  (ndarray) size (number of fracture_cells) the normal traction on\n              each fracture.\n        T_s:  (ndarray) size (number of fracture_cells) the shear traction on\n              each fracture.\n        normals: (ndarray) size (3, number of fracture_cells) normal vector,\n            i.e., the direction of normal traction\n        tangents: (ndarray) size (3, number of fracture_cells) tangential\n            vector, i.e., the direction of shear traction\n        \"\"\"\n\n        if faces is None:\n            frac_faces = self._gb.frac_pairs\n            fi = frac_faces[1]\n        else:\n            fi = faces\n\n        assert self._gb.dim == 3\n        T = self._data['traction'].copy()\n        T = T / self._gb.face_areas\n\n        sgn = sign_of_faces(self._gb, fi)\n        #sgn_test = g.cell_faces[fi, ci]\n\n        T = sgn * T[:, fi]\n        normals = sgn * self._gb.face_normals[:, fi] / self._gb.face_areas[fi]\n        assert np.allclose(np.sqrt(np.sum(normals**2, axis=0)), 1)\n\n        T_n = np.sum(T * normals, axis=0)\n        tangents = T - T_n * normals\n        T_s = np.sqrt(np.sum(tangents**2, axis=0))\n        tangents = tangents / np.sqrt(np.sum(tangents**2, axis=0))\n        assert np.allclose(np.sqrt(np.sum(tangents**2, axis=0)), 1)\n        assert np.allclose(T, T_n * normals + T_s * tangents)\n        # Sanity check:\n        frac_faces = self._gb.frac_pairs\n        trac = self._data['traction'].copy()\n        fi_left = frac_faces[0]\n        sgn_left = sign_of_faces(self._gb, fi_left)\n        sgn_right = sign_of_faces(self._gb, fi)\n        T_left = sgn_left * \\\n            trac.reshape((3, -1), order='F')[:, fi_left]\n        T_right = sgn_right * \\\n            trac.reshape((3, -1), order='F')[:, fi]\n        if not np.allclose(T_left, -T_right):#, atol=1e-6 * np.max(T_left)):\n            import pdb\n            pdb.set_trace()\n\n        assert np.allclose(T_left, -T_right)\n\n        # TESTING DONE\n\n        return T_n, T_s, normals, tangents\n\n    def fracture_dilation(self, distance):\n        \"\"\"\n        defines the fracture dilation as a function of slip distance\n        Parameters:\n        ----------\n        distance: (ndarray) the slip distances\n\n        Returns:\n        ---------\n        dilation: (ndarray) the corresponding normal displacement of fractures.\n        \"\"\"\n\n        phi = 1 * np.pi / 180\n        return distance * np.tan(phi)\n\n    def mu(self, faces, slip_faces=[]):\n        \"\"\"\n        Coefficient of friction.\n        Parameters:\n        ----------\n        faces: (ndarray) indexes of fracture faces\n        slip_faces: (ndarray) optional, defaults to []. Indexes of faces that\n                    are slipping ( will be given a dynamic friciton).\n        returns:\n        mu: (ndarray) the coefficient of each fracture face.\n        \"\"\"\n        mu_d = 0.55\n        mu_ = 0.6 * np.ones(faces.size)\n        mu_[slip_faces] = mu_d\n        return mu_\n\n    def gamma(self):\n        \"\"\"\n        Numerical step length parameter. Defines of far a fracture violating \n        the slip-condition should slip.\n        \"\"\"\n        return 2\n\n    def step(self):\n        \"\"\"\n        calls self.solve()\n        \"\"\"\n        return self.solve()\n\n    def grid(self):\n        \"\"\"\n        returns model grid\n        \"\"\"\n        return self._gb\n\n    def data(self):\n        \"\"\"\n        returns data\n        \"\"\"\n        return self._data\n\n    def slip_distance(self, slip_name='slip_distance'):\n        \"\"\"\n        Save the slip distance to the data dictionary. The slip distance\n        will be saved as a (3, self.grid().num_faces) array\n        Parameters: \n        -----------\n        slip_name:    (string) Defaults to 'slip_distance'. Defines the\n                               keyword for the saved slip distance in the data\n                               dictionary\n        Returns:\n        --------\n        d:  (ndarray) the slip distance as a (3, self.grid().num_faces) array\n        \"\"\"\n        self.slip_name = slip_name\n        self._data[self.slip_name] = self.x\n        return self.x\n\n    def aperture_change(self, aperture_name='apperture_change'):\n        \"\"\"\n        Save the aperture change to the data dictionary. The aperture change\n        will be saved as a (self.grid().num_faces) array\n        Parameters: \n        -----------\n        slip_name:    (string) Defaults to 'aperture_name'. Defines the\n                               keyword for the saved aperture change in the data\n                               dictionary\n        Returns:\n        --------\n        d:  (ndarray) the change in aperture as a (self.grid().num_faces) array\n        \"\"\"\n        self.aperture_name = aperture_name\n        self._data[self.aperture_name] = self.d_n\n        return self.d_n\n\n    def save(self, variables=None, save_every=None):\n        \"\"\"\n        Save the result as vtk. \n\n        Parameters:\n        ----------\n        variables: (list) Optional, defaults to None. If None, only the grid\n            will be exported. A list of strings where each element defines a\n            keyword in the data dictionary to be saved.\n        time_step: (float) optinal, defaults to None. The time step of the\n            variable(s) that is saved\n        \"\"\"\n        if variables is None:\n            self.exporter.write_vtk()\n        else:\n            variables = {k: self._data[k] for k in variables\n                         if k in self._data}\n            self.exporter.write_vtk(variables)",
  "class FrictionSlipDataAssigner():\n    '''\n    Class for setting data to a slip problem:\n    T_s <= mu (T_n - p)\n    This class creates a Parameter object and assigns the data to this object\n    by calling FricitonSlipDataAssigner's functions.\n\n    To change the default values, create a class that inherits from\n    FrictionSlipDataAssigner. Then overload the values you whish to change.\n\n    Parameters in Init:\n    gb: (Grid) a grid object \n    data: (dictionary) Dictionary which Parameter will be added to with keyword\n          'param'\n    physics: (string): defaults to 'mechanics'\n\n    Functions that assign data to Parameter class:\n        bc(): defaults to neumann boundary condition\n             Returns: (Object) boundary condition\n        bc_val(): defaults to 0\n             returns: (ndarray) boundary condition values\n        stress_tensor(): defaults to 1\n             returns: (tensor.FourthOrder) Stress tensor\n\n    Utility functions:\n        grid(): returns: the grid\n\n    '''\n\n    def __init__(self, g, data, physics='slip'):\n        self._g = g\n        self._data = data\n        self.physics = physics\n        self._set_data()\n\n    def data(self):\n        return self._data\n\n    def grid(self):\n        return self._g\n\n    def _set_data(self):\n        if 'param' not in self._data:\n            self._data['param'] = Parameters(self.grid())",
  "def sign_of_faces(g, faces):\n    \"\"\"\n    returns the sign of faces as defined by g.cell_faces. \n    Parameters:\n    g: (Grid Object)\n    faces: (ndarray) indices of faces that you want to know the sign for. The \n           faces must be boundary faces.\n\n    Returns:\n    sgn: (ndarray) the sign of the faces\n    \"\"\"\n\n    IA = np.argsort(faces)\n    IC = np.argsort(IA)\n\n    fi, _, sgn = sps.find(g.cell_faces[faces[IA], :])\n    assert fi.size == faces.size, 'sign of internal faces does not make sense'\n    I = np.argsort(fi)\n    sgn = sgn[I]\n    sgn = sgn[IC]\n    return sgn",
  "def __init__(self, gb, data, physics='slip', **kwargs):\n        self.physics = physics\n        if isinstance(gb, GridBucket):\n            raise ValueError('FrictionSlip excpected a Grid, not a GridBucket')\n\n        self._gb = gb\n        self._data = data\n\n        file_name = kwargs.get('file_name', physics)\n        folder_name = kwargs.get('folder_name', 'results')\n\n        tic = time.time()\n        logger.info('Create exporter')\n        self.exporter = Exporter(self._gb, file_name, folder_name)\n        logger.info('Elapsed time: ' + str(time.time() - tic))\n\n        self.x = np.zeros((3, gb.num_faces))\n        self.d_n = np.zeros(gb.num_faces)\n\n        self.is_slipping = np.zeros(gb.num_faces, dtype=np.bool)\n\n        self.slip_name = 'slip_distance'\n        self.aperture_name = 'aperture_change'",
  "def solve(self):\n        \"\"\" Linearize and solve corresponding system\n\n        First, the function calculate if the slip-criterion is satisfied for\n        each face: T_s <= mu * (T_n - p).\n        If this is violated, the fracture is slipping. It estimates the slip\n        in direction of shear traction as:\n        d += T_s - mu(T_n - p) * sqrt(face_area) / G.\n\n        Stores this result in self.x which is a ndarray of dimension\n        (3, number of faces). Also updates the ndarray self.is_slipping to\n        True for any face that violated the slip-criterion.\n\n        Requires the following keywords in the data dictionary:\n        'face_pressure': (ndarray) size equal number of faces in the grid.\n                         Only the pressure on the fracture faces are used, and\n                         should be equivalent to the pressure in the\n                        pressure in the corresponding lower dimensional cells.\n        'traction': (ndarray) size (3, number_of_faces). This should be the \n                    area scaled traction on each face.\n        'rock': a Rock Object with shear stiffness Rock.MU defined.\n\n        Returns:\n            new_slip (bool) returns True if the slip vector was violated for\n                     any faces\n        \"\"\"\n        assert self._gb.dim == 3, 'only support for 3D (yet)'\n\n        frac_faces = self._gb.frac_pairs\n        fi = frac_faces[1]\n        fi_left = frac_faces[0]\n        T_n, T_s, n, t = self.normal_shear_traction(fi)\n        \n        assert np.all(T_s > -1e-10)\n        assert np.all(T_n < 0), 'Must have a normal force on the fracture'\n\n        # we find the effective normal stress on the fracture face.\n        # Here we need to multiply T_n with -1 as we want the absolute value,\n        # and all the normal tractions should be negative.\n        sigma_n = -T_n - self._data['face_pressure'][fi]\n        assert np.all(sigma_n > 0 )\n        new_slip = T_s - \\\n            self.mu(fi, self.is_slipping[fi]) * \\\n            sigma_n > 1e-5 * self._data['rock'].MU\n\n        self.is_slipping[fi] = self.is_slipping[fi] | new_slip\n        excess_shear = np.abs(\n            T_s) - self.mu(fi, self.is_slipping[fi]) * sigma_n\n        shear_stiffness = np.sqrt(\n            self._gb.face_areas[fi]) / (self._data['rock'].MU)\n        slip_d = excess_shear * shear_stiffness * self.gamma() * new_slip\n\n        # We also add the values to the left cells so that when we average the\n        # face values to obtain a cell value, it will equal the face value\n        self.d_n[fi] += self.fracture_dilation(slip_d)\n        self.d_n[fi_left] += self.fracture_dilation(slip_d)\n        assert np.all(self.d_n[fi] >-1e-6)\n        slip_vec =  -t * slip_d - n * self.fracture_dilation(slip_d)\n        \n        self.x[:, fi] += slip_vec\n        self.x[:, fi_left] -= slip_vec\n\n        return new_slip",
  "def normal_shear_traction(self, faces=None):\n        \"\"\"\n        Project the traction vector into the normal and tangential components\n        as seen from the fractures. \n        Requires that the data dictionary has keyword:\n        traction:  (ndarray) size (3, number of faces). giving the area\n                   weighted traction on each face.\n        \n        Returns:\n        --------\n        T_n:  (ndarray) size (number of fracture_cells) the normal traction on\n              each fracture.\n        T_s:  (ndarray) size (number of fracture_cells) the shear traction on\n              each fracture.\n        normals: (ndarray) size (3, number of fracture_cells) normal vector,\n            i.e., the direction of normal traction\n        tangents: (ndarray) size (3, number of fracture_cells) tangential\n            vector, i.e., the direction of shear traction\n        \"\"\"\n\n        if faces is None:\n            frac_faces = self._gb.frac_pairs\n            fi = frac_faces[1]\n        else:\n            fi = faces\n\n        assert self._gb.dim == 3\n        T = self._data['traction'].copy()\n        T = T / self._gb.face_areas\n\n        sgn = sign_of_faces(self._gb, fi)\n        #sgn_test = g.cell_faces[fi, ci]\n\n        T = sgn * T[:, fi]\n        normals = sgn * self._gb.face_normals[:, fi] / self._gb.face_areas[fi]\n        assert np.allclose(np.sqrt(np.sum(normals**2, axis=0)), 1)\n\n        T_n = np.sum(T * normals, axis=0)\n        tangents = T - T_n * normals\n        T_s = np.sqrt(np.sum(tangents**2, axis=0))\n        tangents = tangents / np.sqrt(np.sum(tangents**2, axis=0))\n        assert np.allclose(np.sqrt(np.sum(tangents**2, axis=0)), 1)\n        assert np.allclose(T, T_n * normals + T_s * tangents)\n        # Sanity check:\n        frac_faces = self._gb.frac_pairs\n        trac = self._data['traction'].copy()\n        fi_left = frac_faces[0]\n        sgn_left = sign_of_faces(self._gb, fi_left)\n        sgn_right = sign_of_faces(self._gb, fi)\n        T_left = sgn_left * \\\n            trac.reshape((3, -1), order='F')[:, fi_left]\n        T_right = sgn_right * \\\n            trac.reshape((3, -1), order='F')[:, fi]\n        if not np.allclose(T_left, -T_right):#, atol=1e-6 * np.max(T_left)):\n            import pdb\n            pdb.set_trace()\n\n        assert np.allclose(T_left, -T_right)\n\n        # TESTING DONE\n\n        return T_n, T_s, normals, tangents",
  "def fracture_dilation(self, distance):\n        \"\"\"\n        defines the fracture dilation as a function of slip distance\n        Parameters:\n        ----------\n        distance: (ndarray) the slip distances\n\n        Returns:\n        ---------\n        dilation: (ndarray) the corresponding normal displacement of fractures.\n        \"\"\"\n\n        phi = 1 * np.pi / 180\n        return distance * np.tan(phi)",
  "def mu(self, faces, slip_faces=[]):\n        \"\"\"\n        Coefficient of friction.\n        Parameters:\n        ----------\n        faces: (ndarray) indexes of fracture faces\n        slip_faces: (ndarray) optional, defaults to []. Indexes of faces that\n                    are slipping ( will be given a dynamic friciton).\n        returns:\n        mu: (ndarray) the coefficient of each fracture face.\n        \"\"\"\n        mu_d = 0.55\n        mu_ = 0.6 * np.ones(faces.size)\n        mu_[slip_faces] = mu_d\n        return mu_",
  "def gamma(self):\n        \"\"\"\n        Numerical step length parameter. Defines of far a fracture violating \n        the slip-condition should slip.\n        \"\"\"\n        return 2",
  "def step(self):\n        \"\"\"\n        calls self.solve()\n        \"\"\"\n        return self.solve()",
  "def grid(self):\n        \"\"\"\n        returns model grid\n        \"\"\"\n        return self._gb",
  "def data(self):\n        \"\"\"\n        returns data\n        \"\"\"\n        return self._data",
  "def slip_distance(self, slip_name='slip_distance'):\n        \"\"\"\n        Save the slip distance to the data dictionary. The slip distance\n        will be saved as a (3, self.grid().num_faces) array\n        Parameters: \n        -----------\n        slip_name:    (string) Defaults to 'slip_distance'. Defines the\n                               keyword for the saved slip distance in the data\n                               dictionary\n        Returns:\n        --------\n        d:  (ndarray) the slip distance as a (3, self.grid().num_faces) array\n        \"\"\"\n        self.slip_name = slip_name\n        self._data[self.slip_name] = self.x\n        return self.x",
  "def aperture_change(self, aperture_name='apperture_change'):\n        \"\"\"\n        Save the aperture change to the data dictionary. The aperture change\n        will be saved as a (self.grid().num_faces) array\n        Parameters: \n        -----------\n        slip_name:    (string) Defaults to 'aperture_name'. Defines the\n                               keyword for the saved aperture change in the data\n                               dictionary\n        Returns:\n        --------\n        d:  (ndarray) the change in aperture as a (self.grid().num_faces) array\n        \"\"\"\n        self.aperture_name = aperture_name\n        self._data[self.aperture_name] = self.d_n\n        return self.d_n",
  "def save(self, variables=None, save_every=None):\n        \"\"\"\n        Save the result as vtk. \n\n        Parameters:\n        ----------\n        variables: (list) Optional, defaults to None. If None, only the grid\n            will be exported. A list of strings where each element defines a\n            keyword in the data dictionary to be saved.\n        time_step: (float) optinal, defaults to None. The time step of the\n            variable(s) that is saved\n        \"\"\"\n        if variables is None:\n            self.exporter.write_vtk()\n        else:\n            variables = {k: self._data[k] for k in variables\n                         if k in self._data}\n            self.exporter.write_vtk(variables)",
  "def __init__(self, g, data, physics='slip'):\n        self._g = g\n        self._data = data\n        self.physics = physics\n        self._set_data()",
  "def data(self):\n        return self._data",
  "def grid(self):\n        return self._g",
  "def _set_data(self):\n        if 'param' not in self._data:\n            self._data['param'] = Parameters(self.grid())",
  "class MpfaMixedDim(SolverMixedDim):\n    def __init__(self, physics='flow'):\n        self.physics = physics\n\n        self.discr = Mpfa(self.physics)\n        self.discr_ndof = self.discr.ndof\n        self.coupling_conditions = TpfaCoupling(self.discr)\n\n        self.solver = Coupler(self.discr, self.coupling_conditions)",
  "class MpfaDFN(SolverMixedDim):\n\n    def __init__(self, dim_max, physics='flow'):\n        # NOTE: There is no flow along the intersections of the fractures.\n\n        self.physics = physics\n        self.dim_max = dim_max\n\n        self.discr = Mpfa(self.physics)\n        self.coupling_conditions = TpfaCouplingDFN(self.discr)\n\n        kwargs = {\"discr_ndof\": self.discr.ndof,\n                  \"discr_fct\": self.__matrix_rhs__}\n        self.solver = Coupler(coupling=self.coupling_conditions, **kwargs)\n        SolverMixDim.__init__(self)\n\n    def __matrix_rhs__(self, g, data):\n        # The highest dimensional problem compute the matrix and rhs, the lower\n        # dimensional problem and empty matrix. For the latter, the size of the\n        # matrix is the number of cells.\n        if g.dim == self.dim_max:\n            return self.discr.matrix_rhs(g, data)\n        else:\n            ndof = self.discr.ndof(g)\n            return sps.csr_matrix((ndof, ndof)), np.zeros(ndof)",
  "class Mpfa(Solver):\n\n    def __init__(self, physics='flow'):\n        self.physics = physics\n\n    def ndof(self, g):\n        \"\"\"\n        Return the number of degrees of freedom associated to the method.\n        In this case number of cells (pressure dof).\n\n        Parameter\n        ---------\n        g: grid, or a subclass.\n\n        Return\n        ------\n        dof: the number of degrees of freedom.\n\n        \"\"\"\n        return g.num_cells\n\n#------------------------------------------------------------------------------#\n\n    def matrix_rhs(self, g, data, discretize=True):\n        \"\"\"\n        Return the matrix and right-hand side for a discretization of a second\n        order elliptic equation using a FV method with a multi-point flux\n        approximation.\n\n        The name of data in the input dictionary (data) are:\n        k : second_order_tensor\n            Permeability defined cell-wise.\n        bc : boundary conditions (optional)\n        bc_val : dictionary (optional)\n            Values of the boundary conditions. The dictionary has at most the\n            following keys: 'dir' and 'neu', for Dirichlet and Neumann boundary\n            conditions, respectively.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data. For details on necessary keywords,\n            see method discretize()\n        discretize (boolean, optional): Whether to discetize prior to matrix\n            assembly. If False, data should already contain discretization.\n            Defaults to True.\n\n        Return\n        ------\n        matrix: sparse csr (g_num_cells, g_num_cells)\n            Discretization matrix.\n        rhs: array (g_num_cells)\n            Right-hand side which contains the boundary conditions and the scalar\n            source term.\n\n        \"\"\"\n        if discretize:\n            self.discretize(g, data)\n\n        div = fvutils.scalar_divergence(g)\n        flux = data['flux']\n        M = div * flux\n\n        bound_flux = data['bound_flux']\n\n        param = data['param']\n\n        bc_val = param.get_bc_val(self)\n\n        return M, self.rhs(g, bound_flux, bc_val)\n\n#------------------------------------------------------------------------------#\n\n    def rhs(self, g, bound_flux, bc_val):\n        \"\"\"\n        Return the righ-hand side for a discretization of a second order elliptic\n        equation using the MPFA method. See self.matrix_rhs for a detaild\n        description.\n        \"\"\"\n        div = g.cell_faces.T\n\n        return -div * bound_flux * bc_val\n\n#------------------------------------------------------------------------------#\n\n    def discretize(self, g, data):\n        \"\"\"\n        The name of data in the input dictionary (data) are:\n        k : second_order_tensor\n            Permeability defined cell-wise. If not given a identity permeability\n            is assumed and a warning arised.\n        bc : boundary conditions (optional)\n        bc_val : dictionary (optional)\n            Values of the boundary conditions. The dictionary has at most the\n            following keys: 'dir' and 'neu', for Dirichlet and Neumann boundary\n            conditions, respectively.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data.\n\n        \"\"\"\n        param = data['param']\n        k = param.get_tensor(self)\n        bnd = param.get_bc(self)\n        a = param.aperture\n\n        trm, bound_flux = mpfa(g, k, bnd, apertures=a)\n        data['flux'] = trm\n        data['bound_flux'] = bound_flux",
  "def mpfa(g, k, bnd, eta=None, inverter=None, apertures=None, max_memory=None,\n         **kwargs):\n    \"\"\"\n    Discretize the scalar elliptic equation by the multi-point flux\n    approximation method.\n\n    The method computes fluxes over faces in terms of pressures in adjacent\n    cells (defined as all cells sharing at least one vertex with the face).\n    This corresponds to the MPFA-O method, see\n\n    Aavatsmark (2002): An introduction to the MPFA-O method on\n            quadrilateral grids, Comp. Geosci. for details.\n\n\n    Implementation needs:\n        1) The local linear systems should be scaled with the permeability and\n        the local grid size, so that we avoid rounding errors accumulating\n        under grid refinement / convergence tests.\n        2) It probably makes sense to create a wrapper class to store the\n        discretization, interface to linear solvers etc.\n    Right now, there are concrete plans for 2).\n\n    Parameters:\n        g (core.grids.grid): grid to be discretized\n        k (core.constit.second_order_tensor) permeability tensor\n        bnd (core.bc.bc) class for boundary values\n        eta Location of pressure continuity point. Defaults to 1/3 for simplex\n            grids, 0 otherwise. On boundary faces with Dirichlet conditions,\n            eta=0 will be enforced.\n        inverter (string) Block inverter to be used, either numba (default),\n            cython or python. See fvutils.invert_diagonal_blocks for details.\n        apertures (np.ndarray) apertures of the cells for scaling of the face\n            normals.\n        max_memory (double): Threshold for peak memory during discretization.\n            If the **estimated** memory need is larger than the provided\n            threshold, the discretization will be split into an appropriate\n            number of sub-calculations, using mpfa_partial().\n\n    Returns:\n        scipy.sparse.csr_matrix (shape num_faces, num_cells): flux\n            discretization, in the form of mapping from cell pressures to face\n            fluxes.\n        scipy.sparse.csr_matrix (shape num_faces, num_faces): discretization of\n            boundary conditions. Interpreted as fluxes induced by the boundary\n            condition (both Dirichlet and Neumann). For Neumann, this will be\n            the prescribed flux over the boundary face, and possibly fluxes\n            over faces having nodes on the boundary. For Dirichlet, the values\n            will be fluxes induced by the prescribed pressure. Incorporation as\n            a right hand side in linear system by multiplication with\n            divergence operator.\n\n    Example:\n        # Set up a Cartesian grid\n        g = structured.CartGrid([5, 5])\n        k = tensor.SecondOrder(g.dim, np.ones(g.num_cells))\n        g.compute_geometry()\n        # Dirirchlet boundary conditions\n        bound_faces = g.tags['domain_boundary_faces'].ravel()\n        bnd = bc.BoundaryCondition(g, bound_faces, ['dir'] * bound_faces.size)\n        # Discretization\n        flux, bound_flux = mpfa(g, k, bnd)\n        # Source in the middle of the domain\n        q = np.zeros(g.num_cells)\n        q[12] = 1\n        # Divergence operator for the grid\n        div = fvutils.scalar_divergence(g)\n        # Discretization matrix\n        A = div * flux\n        # Assign boundary values to all faces on the bounary\n        bound_vals = np.zeros(g.num_faces)\n        bound_vals[bound_faces] = np.arange(bound_faces.size)\n        # Assemble the right hand side and solve\n        rhs = q + div * bound_flux * bound_vals\n        x = sps.linalg.spsolve(A, rhs)\n        f = flux * x - bound_flux * bound_vals\n\n    \"\"\"\n\n    if max_memory is None:\n        # For the moment nothing to do here, just call main mpfa method for the\n        # entire grid.\n        # TODO: We may want to estimate the memory need, and give a warning if\n        # this seems excessive\n        flux, bound_flux = _mpfa_local(\n            g, k, bnd, eta=eta, inverter=inverter, apertures=apertures)\n    else:\n        # Estimate number of partitions necessary based on prescribed memory\n        # usage\n        peak_mem = _estimate_peak_memory(g)\n        num_part = np.ceil(peak_mem / max_memory)\n\n        # Let partitioning module apply the best available method\n        part = partition.partition(g, num_part)\n\n        # Boundary faces on the main grid\n        glob_bound_face = g.get_all_boundary_faces()\n\n        # Empty fields for flux and bound_flux. Will be expanded as we go.\n        # Implementation note: It should be relatively straightforward to\n        # estimate the memory need of flux (face_nodes -> node_cells ->\n        # unique).\n        flux = sps.csr_matrix(g.num_faces, g.num_cells)\n        bound_flux = sps.csr_matrix(g.num_faces, g.num_faces)\n\n        cn = g.cell_nodes()\n\n        face_covered = np.zeros(g.num_faces, dtype=np.bool)\n\n        for p in range(part.max()):\n            # Cells in this partitioning\n            cell_ind = np.argwhere(part == p).ravel('F')\n            # To discretize with as little overlap as possible, we use the\n            # keyword nodes to specify the update stencil. Find nodes of the\n            # local cells.\n            active_cells = np.zeros(g.num_cells, dtype=np.bool)\n            active_cells[cell_ind] = 1\n            active_nodes = np.squeeze(np.where((cn * active_cells) > 0))\n\n            # Perform local discretization.\n            loc_flux, loc_bound_flux, loc_faces \\\n                = mpfa_partial(g, k, bnd, eta=eta, inverter=inverter,\n                               nodes=active_nodes)\n\n            # Eliminate contribution from faces already covered\n            loc_flux[face_covered, :] *= 0\n            loc_bound_flux[face_covered, :] *= 0\n\n            face_covered[loc_faces] = 1\n\n            flux += loc_flux\n            bound_flux += loc_bound_flux\n\n    return flux, bound_flux",
  "def mpfa_partial(g, k, bnd, eta=0, inverter='numba', cells=None, faces=None,\n                 nodes=None, apertures=None):\n    \"\"\"\n    Run an MPFA discretization on subgrid, and return discretization in terms\n    of global variable numbers.\n\n    Scenarios where the method will be used include updates of permeability,\n    and the introduction of an internal boundary (e.g. fracture growth).\n\n    The subgrid can be specified in terms of cells, faces and nodes to be\n    updated. For details on the implementation, see\n    fv_utils.cell_ind_for_partial_update()\n\n    Parameters:\n        g (porepy.grids.grid.Grid): grid to be discretized\n        k (porepy.params.tensor.SecondOrder) permeability tensor\n        bnd (porepy.params.bc.BoundarCondition) class for boundary conditions\n        faces (np.ndarray) faces to be considered. Intended for partial\n            discretization, may change in the future\n        eta Location of pressure continuity point. Should be 1/3 for simplex\n            grids, 0 otherwise. On boundary faces with Dirichlet conditions,\n            eta=0 will be enforced.\n        inverter (string) Block inverter to be used, either numba (default),\n            cython or python. See fvutils.invert_diagonal_blocks for details.\n        cells (np.array, int, optional): Index of cells on which to base the\n            subgrid computation. Defaults to None.\n        faces (np.array, int, optional): Index of faces on which to base the\n            subgrid computation. Defaults to None.\n        nodes (np.array, int, optional): Index of nodes on which to base the\n            subgrid computation. Defaults to None.\n        apertures (np.ndarray, float, optional) apertures of the cells for scaling of the face\n            normals. Defaults to None.\n\n        Note that if all of {cells, faces, nodes} are None, empty matrices will\n        be returned.\n\n    Returns:\n        sps.csr_matrix (g.num_faces x g.num_cells): Flux discretization,\n            computed on a subgrid.\n        sps.csr_matrix (g,num_faces x g.num_faces): Boundary flux\n            discretization, computed on a subgrid\n        np.array (int): Global of the faces where the flux discretization is\n            computed.\n\n    \"\"\"\n    if cells is not None:\n        warnings.warn('Cells keyword for partial mpfa has not been tested')\n    if faces is not None:\n        warnings.warn('Faces keyword for partial mpfa has not been tested')\n\n    # Find computational stencil, based on specified cells, faces and nodes.\n    ind, active_faces = fvutils.cell_ind_for_partial_update(g, cells=cells,\n                                                            faces=faces,\n                                                            nodes=nodes)\n\n    # Extract subgrid, together with mappings between local and global\n    # cells\n    sub_g, l2g_faces, _ = partition.extract_subgrid(g, ind)\n    l2g_cells = sub_g.parent_cell_ind\n\n    # Local parameter fields\n    # Copy permeability field, and restrict to local cells\n    loc_k = k.copy()\n    loc_k.perm = loc_k.perm[::, ::, l2g_cells]\n\n    glob_bound_face = g.get_all_boundary_faces()\n\n    # Boundary conditions are slightly more complex. Find local faces\n    # that are on the global boundary.\n    loc_bound_ind = np.argwhere(np.in1d(l2g_faces, glob_bound_face)).ravel('F')\n    loc_cond = np.array(loc_bound_ind.size * ['neu'])\n    # Then pick boundary condition on those faces.\n    if loc_bound_ind.size > 0:\n        # We could have avoided to explicitly define Neumann conditions,\n        # since these are default.\n        # For primal-like discretizations like the MPFA, internal boundaries\n        # are handled by assigning Neumann conditions.\n        is_dir = np.logical_and(bnd.is_dir, np.logical_not(bnd.is_internal))\n        is_neu = np.logical_or(bnd.is_neu, bnd.is_internal)\n\n        is_dir = is_dir[l2g_faces[loc_bound_ind]]\n        is_neu = is_neu[l2g_faces[loc_bound_ind]]\n\n        loc_cond[is_dir] = 'dir'\n    loc_bnd = bc.BoundaryCondition(sub_g, faces=loc_bound_ind, cond=loc_cond)\n\n    # Discretization of sub-problem\n    flux_loc, bound_flux_loc = _mpfa_local(sub_g, loc_k, loc_bnd,\n                                           eta=eta, inverter=inverter, apertures=apertures)\n\n    # Map to global indices\n    face_map, cell_map = fvutils.map_subgrid_to_grid(g, l2g_faces, l2g_cells,\n                                                     is_vector=False)\n    flux_glob = face_map * flux_loc * cell_map\n    bound_flux_glob = face_map * bound_flux_loc * face_map.transpose()\n\n    # By design of mpfa, and the subgrids, the discretization will update faces\n    # outside the active faces. Kill these.\n    outside = np.setdiff1d(np.arange(g.num_faces), active_faces,\n                           assume_unique=True)\n    flux_glob[outside, :] = 0\n    bound_flux_glob[outside, :] = 0\n\n    return flux_glob, bound_flux_glob, active_faces",
  "def _mpfa_local(g, k, bnd, eta=None, inverter='numba', apertures=None):\n    \"\"\"\n    Actual implementation of the MPFA O-method. To calculate MPFA on a grid\n    directly, either call this method, or, to respect the privacy of this\n    method, the main mpfa method with no memory constraints.\n\n    Method properties and implementation details.\n    The pressure is discretized as a linear function on sub-cells (see\n    reference paper). In this implementation, the pressure is represented by\n    its cell center value and the sub-cell gradients (this is in contrast to\n    most papers, which use auxiliary pressures on the faces; the current\n    formulation is equivalent, but somewhat easier to implement).\n    The method will give continuous fluxes over the faces, and pressure\n    continuity for certain points (controlled by the parameter eta). This can\n    be expressed as a linear system on the form\n        (i)   A * grad_p            = 0\n        (ii)  B * grad_p + C * p_cc = 0\n        (iii) 0            D * p_cc = I\n    Here, the first equation represents flux continuity, and involves only the\n    pressure gradients (grad_p). The second equation gives pressure continuity\n    over cell faces, thus B will contain distances between cell centers and the\n    face continuity points, while C consists of +- 1 (depending on which side\n    the cell is relative to the face normal vector). The third equation\n    enforces the pressure to be unity in one cell at a time. Thus (i)-(iii) can\n    be inverted to express the pressure gradients as in terms of the cell\n    center variables, that is, we can compute the basis functions on the\n    sub-cells. Because of the method construction (again see reference paper),\n    the basis function of a cell c will be non-zero on all sub-cells sharing\n    a vertex with c. Finally, the fluxes as functions of cell center values are\n    computed by insertion into Darcy's law (which is essentially half of A from\n    (i), that is, only consider contribution from one side of the face.\n    Boundary values can be incorporated with appropriate modifications -\n    Neumann conditions will have a non-zero right hand side for (i), while\n    Dirichlet gives a right hand side for (ii).\n\n    \"\"\"\n    if eta is None:\n        eta = fvutils.determine_eta(g)\n\n    # The method reduces to the more efficient TPFA in one dimension, so that\n    # method may be called. In 0D, there is no internal discretization to be\n    # done.\n    if g.dim == 1:\n        discr = tpfa.Tpfa()\n        params = data.Parameters(g)\n        params.set_bc('flow', bnd)\n        params.set_aperture(apertures)\n        params.set_tensor('flow', k)\n        d = {'param': params}\n        discr.discretize(g, d)\n        return d['flux'], d['bound_flux']\n    elif g.dim == 0:\n        return sps.csr_matrix([0]), 0\n\n    # The grid coordinates are always three-dimensional, even if the grid is\n    # really 2D. This means that there is not a 1-1 relation between the number\n    # of coordinates of a point / vector and the real dimension. This again\n    # violates some assumptions tacitly made in the discretization (in\n    # particular that the number of faces of a cell that meets in a vertex\n    # equals the grid dimension, and that this can be used to construct an\n    # index of local variables in the discretization). These issues should be\n    # possible to overcome, but for the moment, we simply force 2D grids to be\n    # proper 2D.\n\n    if g.dim == 2:\n        # Rotate the grid into the xy plane and delete third dimension. First\n        # make a copy to avoid alterations to the input grid\n        g = g.copy()\n        cell_centers, face_normals, face_centers, R, _, nodes = cg.map_grid(\n            g)\n        g.cell_centers = cell_centers\n        g.face_normals = face_normals\n        g.face_centers = face_centers\n        g.nodes = nodes\n\n        # Rotate the permeability tensor and delete last dimension\n        k = k.copy()\n        k.perm = np.tensordot(R.T, np.tensordot(R, k.perm, (1, 0)), (0, 1))\n        k.perm = np.delete(k.perm, (2), axis=0)\n        k.perm = np.delete(k.perm, (2), axis=1)\n\n    # Define subcell topology, that is, the local numbering of faces, subfaces,\n    # sub-cells and nodes. This numbering is used throughout the\n    # discretization.\n    subcell_topology = fvutils.SubcellTopology(g)\n\n    # Obtain normal_vector * k, pairings of cells and nodes (which together\n    # uniquely define sub-cells, and thus index for gradients.\n    nk_grad, cell_node_blocks, \\\n        sub_cell_index = _tensor_vector_prod(g, k, subcell_topology, apertures)\n\n    # Distance from cell centers to face centers, this will be the\n    # contribution from gradient unknown to equations for pressure continuity\n    pr_cont_grad = fvutils.compute_dist_face_cell(g, subcell_topology, eta)\n\n    # Darcy's law\n    darcy = -nk_grad[subcell_topology.unique_subfno]\n\n    # Pair fluxes over subfaces, that is, enforce conservation\n    nk_grad = subcell_topology.pair_over_subfaces(nk_grad)\n\n    # Contribution from cell center potentials to local systems\n    # For pressure continuity, +-1 (Depending on whether the cell is on the\n    # positive or negative side of the face.\n    # The .A suffix is necessary to get a numpy array, instead of a scipy\n    # matrix.\n    sgn = g.cell_faces[subcell_topology.fno, subcell_topology.cno].A\n    pr_cont_cell = sps.coo_matrix((sgn[0], (subcell_topology.subfno,\n                                            subcell_topology.cno))).tocsr()\n    # The cell centers give zero contribution to flux continuity\n    nk_cell = sps.coo_matrix((np.zeros(1), (np.zeros(1), np.zeros(1))),\n                             shape=(subcell_topology.num_subfno,\n                                    subcell_topology.num_cno)).tocsr()\n    del sgn\n\n    # Mapping from sub-faces to faces\n    hf2f = sps.coo_matrix((np.ones(subcell_topology.unique_subfno.size),\n                           (subcell_topology.fno_unique,\n                            subcell_topology.subfno_unique)))\n\n    # Update signs\n    sgn_unique = g.cell_faces[subcell_topology.fno_unique,\n                              subcell_topology.cno_unique].A.ravel('F')\n\n    # The boundary faces will have either a Dirichlet or Neumann condition, but\n    # not both (Robin is not implemented).\n    # Obtain mappings to exclude boundary faces.\n    bound_exclusion = fvutils.ExcludeBoundaries(subcell_topology, bnd, g.dim)\n\n    # No flux conditions for Dirichlet boundary faces\n    nk_grad = bound_exclusion.exclude_dirichlet(nk_grad)\n    nk_cell = bound_exclusion.exclude_dirichlet(nk_cell)\n    # No pressure condition for Neumann boundary faces\n    pr_cont_grad = bound_exclusion.exclude_neumann(pr_cont_grad)\n    pr_cont_cell = bound_exclusion.exclude_neumann(pr_cont_cell)\n\n    # So far, the local numbering has been based on the numbering scheme\n    # implemented in SubcellTopology (which treats one cell at a time). For\n    # efficient inversion (below), it is desirable to get the system over to a\n    # block-diagonal structure, with one block centered around each vertex.\n    # Obtain the necessary mappings.\n    rows2blk_diag, cols2blk_diag, size_of_blocks = _block_diagonal_structure(\n        sub_cell_index, cell_node_blocks, subcell_topology.nno_unique,\n        bound_exclusion)\n\n    del cell_node_blocks, sub_cell_index\n\n    # System of equations for the subcell gradient variables. On block diagonal\n    # form.\n    grad_eqs = sps.vstack([nk_grad, pr_cont_grad])\n\n    num_nk_cell = nk_cell.shape[0]\n    num_pr_cont_grad = pr_cont_grad.shape[0]\n    del nk_grad, pr_cont_grad\n\n    grad = rows2blk_diag * grad_eqs * cols2blk_diag\n\n    del grad_eqs\n    darcy_igrad = darcy * cols2blk_diag * fvutils.invert_diagonal_blocks(grad,\n                                                                         size_of_blocks,\n                                                                         method=inverter) \\\n        * rows2blk_diag\n\n    del grad, cols2blk_diag, rows2blk_diag, darcy\n\n    flux = hf2f * darcy_igrad * (-sps.vstack([nk_cell, pr_cont_cell]))\n\n    del nk_cell, pr_cont_cell\n    ####\n    # Boundary conditions\n    rhs_bound = _create_bound_rhs(bnd, bound_exclusion,\n                                  subcell_topology, sgn_unique, g,\n                                  num_nk_cell, num_pr_cont_grad)\n    # Discretization of boundary values\n    bound_flux = hf2f * darcy_igrad * rhs_bound\n\n    return flux, bound_flux",
  "def _estimate_peak_memory(g):\n    \"\"\"\n    Rough estimate of peak memory need\n    \"\"\"\n    nd = g.dim\n    num_cell_nodes = g.cell_nodes().toarray().sum(axis=1)\n\n    # Number of unknowns around a vertex: nd per cell that share the vertex for\n    # pressure gradients, and one per cell (cell center pressure)\n    num_grad_unknowns = nd * num_cell_nodes\n\n    # The most expensive field is the storage of igrad, which is block diagonal\n    # with num_grad_unknowns sized blocks\n    igrad_size = num_grad_unknowns.sum()\n\n    # The discretization of Darcy's law will require nd (that is, a gradient)\n    # per sub-face.\n    num_sub_face = g.face_nodes.toarray().sum()\n    darcy_size = nd * num_sub_face\n\n    # Balancing of fluxes will require 2*nd (gradient on both sides) fields per\n    # sub-face\n    nk_grad_size = 2 * nd * num_sub_face\n    # Similarly, pressure continuity requires 2 * (nd+1) (gradient on both\n    # sides, and cell center pressures) numbers\n    pr_cont_size = 2 * (nd + 1) * num_sub_face\n\n    total_size = igrad_size + darcy_size + nk_grad_size + pr_cont_size\n\n    # Not covered yet is various fields on subcell topology, mapping matrices\n    # between local and block ordering etc.\n    return total_size",
  "def _tensor_vector_prod(g, k, subcell_topology, apertures=None):\n    \"\"\"\n    Compute product of normal vectors and tensors on a sub-cell level.\n\n    This is essentially defining Darcy's law for each sub-face in terms of\n    sub-cell gradients. Thus, we also implicitly define the global ordering\n    of sub-cell gradient variables (via the interpretation of the columns in\n    nk).\n\n    NOTE: In the local numbering below, in particular in the variables i and j,\n    it is tacitly assumed that g.dim == g.nodes.shape[0] ==\n    g.face_normals.shape[0] etc. See implementation note in main method.\n\n    Parameters:\n        g (core.grids.grid): Discretization grid\n        k (core.constit.second_order_tensor): The permeability tensor\n        subcell_topology (fvutils.SubcellTopology): Wrapper class containing\n            subcell numbering.\n\n    Returns:\n        nk: sub-face wise product of normal vector and permeability tensor.\n        cell_node_blocks pairings of node and cell indices, which together\n            define a sub-cell.\n        sub_cell_ind: index of all subcells\n\n    \"\"\"\n\n    # Stack cell and nodes, and remove duplicate rows. Since subcell_mapping\n    # defines cno and nno (and others) working cell-wise, this will\n    # correspond to a unique rows (Matlab-style) from what I understand.\n    # This also means that the pairs in cell_node_blocks uniquely defines\n    # subcells, and can be used to index gradients etc.\n    cell_node_blocks, blocksz = matrix_compression.rlencode(np.vstack((\n        subcell_topology.cno, subcell_topology.nno)))\n\n    nd = g.dim\n\n    # Duplicates in [cno, nno] corresponds to different faces meeting at the\n    # same node. There should be exactly nd of these. This test will fail\n    # for pyramids in 3D\n    assert np.all(blocksz == nd)\n\n    # Define row and column indices to be used for normal_vectors * perm.\n    # Rows are based on sub-face numbers.\n    # Columns have nd elements for each sub-cell (to store a gradient) and\n    # is adjusted according to block sizes\n    _, j = np.meshgrid(subcell_topology.subhfno, np.arange(nd))\n    sum_blocksz = np.cumsum(blocksz)\n    j += matrix_compression.rldecode(sum_blocksz - blocksz[0], blocksz)\n\n    # Distribute faces equally on the sub-faces\n    num_nodes = np.diff(g.face_nodes.indptr)\n    normals = g.face_normals[:, subcell_topology.fno] / num_nodes[\n        subcell_topology.fno]\n    if apertures is not None:\n        normals = normals * apertures[subcell_topology.cno]\n\n    # Represent normals and permeability on matrix form\n    ind_ptr = np.hstack((np.arange(0, j.size, nd), j.size))\n    normals_mat = sps.csr_matrix((normals.ravel('F'), j.ravel('F'), ind_ptr))\n    k_mat = sps.csr_matrix((k.perm[::, ::, cell_node_blocks[0]].ravel('F'),\n                            j.ravel('F'), ind_ptr))\n\n    nk = normals_mat * k_mat\n\n    # Unique sub-cell indexes are pulled from column indices, we only need\n    # every nd column (since nd faces of the cell meet at each vertex)\n    sub_cell_ind = j[::, 0::nd]\n    return nk, cell_node_blocks, sub_cell_ind",
  "def _block_diagonal_structure(sub_cell_index, cell_node_blocks, nno,\n                              bound_exclusion):\n    \"\"\" Define matrices to turn linear system into block-diagonal form\n\n    Parameters\n    ----------\n    sub_cell_index\n    cell_node_blocks: pairs of cell and node pairs, which defines sub-cells\n    nno node numbers associated with balance equations\n    exclude_dirichlet mapping to remove rows associated with flux boundary\n    exclude_neumann mapping to remove rows associated with pressure boundary\n\n    Returns\n    -------\n    rows2blk_diag transform rows of linear system to block-diagonal form\n    cols2blk_diag transform columns of linear system to block-diagonal form\n    size_of_blocks number of equations in each block\n    \"\"\"\n\n    # Stack node numbers of equations on top of each other, and sort them to\n    # get block-structure. First eliminate node numbers at the boundary, where\n    # the equations are either of flux or pressure continuity (not both)\n    nno_flux = bound_exclusion.exclude_dirichlet(nno)\n    nno_pressure = bound_exclusion.exclude_neumann(nno)\n    node_occ = np.hstack((nno_flux, nno_pressure))\n    sorted_ind = np.argsort(node_occ)\n    sorted_nodes_rows = node_occ[sorted_ind]\n    # Size of block systems\n    size_of_blocks = np.bincount(sorted_nodes_rows.astype('int64'))\n    rows2blk_diag = sps.coo_matrix((np.ones(sorted_nodes_rows.size),\n                                    (np.arange(sorted_ind.size),\n                                     sorted_ind))).tocsr()\n\n    # cell_node_blocks[1] contains the node numbers associated with each\n    # sub-cell gradient (and so column of the local linear systems). A sort\n    # of these will give a block-diagonal structure\n    sorted_nodes_cols = np.argsort(cell_node_blocks[1])\n    subcind_nodes = sub_cell_index[::, sorted_nodes_cols].ravel('F')\n    cols2blk_diag = sps.coo_matrix((np.ones(sub_cell_index.size),\n                                    (subcind_nodes,\n                                     np.arange(sub_cell_index.size)))).tocsr()\n    return rows2blk_diag, cols2blk_diag, size_of_blocks",
  "def _create_bound_rhs(bnd, bound_exclusion,\n                      subcell_topology, sgn, g, num_flux, num_pr):\n    \"\"\"\n    Define rhs matrix to get basis functions for incorporates boundary\n    conditions\n\n    Parameters\n    ----------\n    bnd\n    exclude_dirichlet\n    exclude_neumann\n    fno\n    sgn : +-1, defining here and there of the faces\n    g : grid\n    num_flux : number of equations for flux continuity\n    num_pr: number of equations for pressure continuity\n\n    Returns\n    -------\n    rhs_bound: Matrix that can be multiplied with inverse block matrix to get\n               basis functions for boundary values\n    \"\"\"\n    # For primal-like discretizations like the MPFA, internal boundaries\n    # are handled by assigning Neumann conditions.\n    is_dir = np.logical_and(bnd.is_dir, np.logical_not(bnd.is_internal))\n    is_neu = np.logical_or(bnd.is_neu, bnd.is_internal)\n\n    fno = subcell_topology.fno_unique\n    num_neu = np.sum(is_neu[fno])\n    num_dir = np.sum(is_dir[fno])\n    num_bound = num_neu + num_dir\n\n    # Neumann boundary conditions\n    # Find Neumann faces, exclude Dirichlet faces (since these are excluded\n    # from the right hand side linear system), and do necessary formating.\n    neu_ind = np.argwhere(bound_exclusion.exclude_dirichlet(\n        is_neu[fno].astype('int64'))).ravel('F')\n    # We also need to map the respective Neumann and Dirichlet half-faces to\n    # the global half-face numbering (also interior faces). The latter should\n    # not have Dirichlet and Neumann excluded (respectively), and thus we need\n    # new fields\n    neu_ind_all = np.argwhere(is_neu[fno].astype('int')).ravel('F')\n    dir_ind_all = np.argwhere(is_dir[fno].astype('int')).ravel('F')\n    num_face_nodes = g.face_nodes.sum(axis=0).A.ravel(order='F')\n\n    # For the Neumann boundary conditions, we define the value as seen from\n    # the innside of the domain. E.g. outflow is defined to be positive. We\n    # therefore set the matrix indices to -1. We also have to scale it with\n    # the number of nodes per face because the flux of face is the sum of its\n    # half-faces.\n    scaled_sgn = - 1 / num_face_nodes[fno[neu_ind_all]]\n    if neu_ind.size > 0:\n        neu_cell = sps.coo_matrix((scaled_sgn,\n                                   (neu_ind, np.arange(neu_ind.size))),\n                                  shape=(num_flux, num_bound))\n    else:\n        # Special handling when no elements are found. Not sure if this is\n        # necessary, or if it is me being stupid\n        neu_cell = sps.coo_matrix((num_flux, num_bound))\n\n    # Dirichlet boundary conditions\n    dir_ind = np.argwhere(bound_exclusion.exclude_neumann(\n        is_dir[fno].astype('int64'))).ravel('F')\n    if dir_ind.size > 0:\n        dir_cell = sps.coo_matrix((sgn[dir_ind_all], (dir_ind, num_neu +\n                                                      np.arange(dir_ind.size))),\n                                  shape=(num_pr, num_bound))\n    else:\n        # Special handling when no elements are found. Not sure if this is\n        # necessary, or if it is me being stupid\n        dir_cell = sps.coo_matrix((num_pr, num_bound))\n\n    # Number of elements in neu_ind and neu_ind_all are equal, we can test with\n    # any of them. Same with dir.\n    if neu_ind.size > 0 and dir_ind.size > 0:\n        neu_dir_ind = np.hstack([neu_ind_all, dir_ind_all]).ravel('F')\n    elif neu_ind.size > 0:\n        neu_dir_ind = neu_ind_all\n    elif dir_ind.size > 0:\n        neu_dir_ind = dir_ind_all\n    else:\n        raise ValueError(\"Boundary values should be either Dirichlet or \"\n                         \"Neumann\")\n\n    num_subfno = subcell_topology.num_subfno_unique\n\n    # The columns in neu_cell, dir_cell are ordered from 0 to num_bound-1.\n    # Map these to all half-face indices\n    bnd_2_all_hf = sps.coo_matrix((np.ones(num_bound),\n                                   (np.arange(num_bound), neu_dir_ind)),\n                                  shape=(num_bound, num_subfno))\n    # The user of the discretization should now nothing about half faces,\n    # thus map from half face to face indices.\n    hf_2_f = sps.coo_matrix((np.ones(subcell_topology.subfno_unique.size),\n                             (subcell_topology.subfno_unique,\n                              subcell_topology.fno_unique)),\n                            shape=(num_subfno, g.num_faces))\n    rhs_bound = sps.vstack([neu_cell, dir_cell]) * bnd_2_all_hf * hf_2_f\n    return rhs_bound",
  "def __init__(self, physics='flow'):\n        self.physics = physics\n\n        self.discr = Mpfa(self.physics)\n        self.discr_ndof = self.discr.ndof\n        self.coupling_conditions = TpfaCoupling(self.discr)\n\n        self.solver = Coupler(self.discr, self.coupling_conditions)",
  "def __init__(self, dim_max, physics='flow'):\n        # NOTE: There is no flow along the intersections of the fractures.\n\n        self.physics = physics\n        self.dim_max = dim_max\n\n        self.discr = Mpfa(self.physics)\n        self.coupling_conditions = TpfaCouplingDFN(self.discr)\n\n        kwargs = {\"discr_ndof\": self.discr.ndof,\n                  \"discr_fct\": self.__matrix_rhs__}\n        self.solver = Coupler(coupling=self.coupling_conditions, **kwargs)\n        SolverMixDim.__init__(self)",
  "def __matrix_rhs__(self, g, data):\n        # The highest dimensional problem compute the matrix and rhs, the lower\n        # dimensional problem and empty matrix. For the latter, the size of the\n        # matrix is the number of cells.\n        if g.dim == self.dim_max:\n            return self.discr.matrix_rhs(g, data)\n        else:\n            ndof = self.discr.ndof(g)\n            return sps.csr_matrix((ndof, ndof)), np.zeros(ndof)",
  "def __init__(self, physics='flow'):\n        self.physics = physics",
  "def ndof(self, g):\n        \"\"\"\n        Return the number of degrees of freedom associated to the method.\n        In this case number of cells (pressure dof).\n\n        Parameter\n        ---------\n        g: grid, or a subclass.\n\n        Return\n        ------\n        dof: the number of degrees of freedom.\n\n        \"\"\"\n        return g.num_cells",
  "def matrix_rhs(self, g, data, discretize=True):\n        \"\"\"\n        Return the matrix and right-hand side for a discretization of a second\n        order elliptic equation using a FV method with a multi-point flux\n        approximation.\n\n        The name of data in the input dictionary (data) are:\n        k : second_order_tensor\n            Permeability defined cell-wise.\n        bc : boundary conditions (optional)\n        bc_val : dictionary (optional)\n            Values of the boundary conditions. The dictionary has at most the\n            following keys: 'dir' and 'neu', for Dirichlet and Neumann boundary\n            conditions, respectively.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data. For details on necessary keywords,\n            see method discretize()\n        discretize (boolean, optional): Whether to discetize prior to matrix\n            assembly. If False, data should already contain discretization.\n            Defaults to True.\n\n        Return\n        ------\n        matrix: sparse csr (g_num_cells, g_num_cells)\n            Discretization matrix.\n        rhs: array (g_num_cells)\n            Right-hand side which contains the boundary conditions and the scalar\n            source term.\n\n        \"\"\"\n        if discretize:\n            self.discretize(g, data)\n\n        div = fvutils.scalar_divergence(g)\n        flux = data['flux']\n        M = div * flux\n\n        bound_flux = data['bound_flux']\n\n        param = data['param']\n\n        bc_val = param.get_bc_val(self)\n\n        return M, self.rhs(g, bound_flux, bc_val)",
  "def rhs(self, g, bound_flux, bc_val):\n        \"\"\"\n        Return the righ-hand side for a discretization of a second order elliptic\n        equation using the MPFA method. See self.matrix_rhs for a detaild\n        description.\n        \"\"\"\n        div = g.cell_faces.T\n\n        return -div * bound_flux * bc_val",
  "def discretize(self, g, data):\n        \"\"\"\n        The name of data in the input dictionary (data) are:\n        k : second_order_tensor\n            Permeability defined cell-wise. If not given a identity permeability\n            is assumed and a warning arised.\n        bc : boundary conditions (optional)\n        bc_val : dictionary (optional)\n            Values of the boundary conditions. The dictionary has at most the\n            following keys: 'dir' and 'neu', for Dirichlet and Neumann boundary\n            conditions, respectively.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data.\n\n        \"\"\"\n        param = data['param']\n        k = param.get_tensor(self)\n        bnd = param.get_bc(self)\n        a = param.aperture\n\n        trm, bound_flux = mpfa(g, k, bnd, apertures=a)\n        data['flux'] = trm\n        data['bound_flux'] = bound_flux",
  "class Biot(Solver):\n\n    def __init__(self, eta=None):\n        \"\"\" Set default values for some parameters used in discretization.\n\n        \"\"\"\n        defaults = {'fluid_compr': 0,\n                    'fluid_viscosity': 1,\n                    'biot_alpha': 1\n                    }\n        self.defaults = defaults\n\n    def ndof(self, g):\n        \"\"\" Return the number of degrees of freedom associated wiht the method.\n\n        In this case, each cell has nd displacement variables, as well as a\n        pressure variable.\n\n        Parameters:\n            g: grid, or a subclass.\n\n        Returns:\n            int: Number of degrees of freedom in the grid.\n\n        \"\"\"\n        return g.num_cells * (1 + g.dim)\n\n    def matrix_rhs(self, g, data, discretize=True):\n        if discretize:\n            self.discretize(g, data)\n\n        A_biot = self.assemble_matrix(g, data)\n        rhs_bound = self.rhs(g, data)\n        return A_biot, rhs_bound\n\n#--------------------------- Helper methods for discretization ----------\n\n    def rhs(self, g, data):\n        bnd = self.rhs_bound(g, data)\n        tm = self.rhs_time(g, data)\n#        src = data['source']\n        return bnd + tm\n\n    def rhs_bound(self, g, data):\n        \"\"\" Boundary component of the right hand side.\n\n        TODO: Boundary effects of coupling terms.\n\n        Parameters:\n            g: grid, or subclass, with geometry fields computed.\n            data: dictionary to store the data terms. Must have been through a\n                call to discretize() to discretization of right hand side.\n            state: np.ndarray, solution vector from previous time step.\n\n        Returns:\n            np.ndarray: Contribution to right hand side given the current\n            state.\n\n        \"\"\"\n        d = data['param'].get_bc_val('mechanics')\n        p = data['param'].get_bc_val('flow')\n\n        div_flow = fvutils.scalar_divergence(g)\n        div_mech = fvutils.vector_divergence(g)\n\n        p_bound = -div_flow * data['bound_flux'] * p\\\n                  - data['bound_div_d'] * d\n        s_bound = -div_mech * data['bound_stress'] * d\n        return np.hstack((s_bound, p_bound))\n\n    def rhs_time(self, g, data):\n        \"\"\" Time component of the right hand side (dependency on previous time\n        step).\n\n        TODO: 1) Generalize this to allow other methods than Euler backwards?\n              2) How about time dependent boundary conditions.\n\n        Parameters:\n            g: grid, or subclass, with geometry fields computed.\n            data: dictionary to store the data terms. Must have been through a\n                call to discretize() to discretization of right hand side.\n            state: np.ndarray optional, solution vector from previous time\n                step. Defaults to zero.\n\n        Returns:\n            np.ndarray: Contribution to right hand side given the current\n            state.\n\n        \"\"\"\n        state = data.get('state', None)\n        if state is None:\n            state = np.zeros((g.dim + 1) * g.num_cells)\n\n        d = self.extractD(g, state, as_vector=True)\n        p = self.extractP(g, state)\n\n        d_scaling = data.get('displacement_scaling', 1)\n\n        div_d = np.squeeze(data['param'].biot_alpha *\n                           data['div_d'] * d * d_scaling)\n        p_cmpr = data['compr_discr'] * p\n\n        mech_rhs = np.zeros(g.dim * g.num_cells)\n\n        return np.hstack((mech_rhs, div_d + p_cmpr))\n\n    def discretize(self, g, data):\n        \"\"\" Discretize flow and mechanics equations using FV methods.\n\n        The parameters needed for the discretization are stored in the\n        dictionary data, which should contain the following mandatory keywords:\n\n            Related to flow equation:\n                perm: Second order tensor representing permeability\n                bound_flow: BoundaryCondition object for flow equation. Used in\n                    mpfa.\n\n            Related to mechanics equation:\n                stiffness: Fourth order tensor representing elastic moduli.\n                bound_mech: BoundaryCondition object for mechanics equation.\n                    Used in mpsa.\n\n        In addition, the following parameters are optional:\n\n            Related to flow equation:\n                fluid_viscosity (double). Defaults to 1.\n                fluid_compr (double): Fluid compressibility. Defaults to 0.\n\n            Related to coupling terms:\n                biot_alpha (double between 0 and 1): Biot's coefficient.\n                    Defaults to 1.\n\n            Related to numerics:\n                inverter (str): Which method to use for block inversion. See\n                    fvutils.invert_diagonal_blocks for detail, and for default\n                    options.\n                eta (double): Location of continuity point in MPSA and MPFA.\n                    Defaults to 1/3 for simplex grids, 0 otherwise.\n\n        The discretization is stored in the data dictionary, in the form of\n        several matrices representing different coupling terms. For details,\n        and how to combine these, see self.assemble_matrix()\n\n        Parameters:\n            g (grid): Grid to be discretized.\n            data (dictionary): Containing data for discretization. See above\n                for specification.\n\n        \"\"\"\n        # Discretization of elasticity / poro-mechanics\n        self._discretize_flow(g, data)\n        self._discretize_mech(g, data)\n        self._discretize_compr(g, data)\n\n    def assemble_matrix(self, g, data):\n        \"\"\" Assemble the poro-elastic system matrix.\n\n        The discretization is presumed stored in the data dictionary.\n\n        Parameters:\n            g (grid): Grid for disrcetization\n            data (dictionary): Data for discretization, as well as matrices\n                with discretization of the sub-parts of the system.\n\n        Returns:\n            scipy.sparse.bmat: Block matrix with the combined MPSA/MPFA\n                discretization.\n\n        \"\"\"\n        div_flow = fvutils.scalar_divergence(g)\n        div_mech = fvutils.vector_divergence(g)\n        param = data['param']\n\n        fluid_viscosity = param.fluid_viscosity\n        biot_alpha = param.biot_alpha\n\n        # Put together linear system\n        A_flow = div_flow * data['flux'] / fluid_viscosity\n        A_mech = div_mech * data['stress']\n\n        # Time step size\n        dt = data['dt']\n\n        d_scaling = data.get('displacement_scaling', 1)\n        # Matrix for left hand side\n        A_biot = sps.bmat([[A_mech,\n                            data['grad_p'] * biot_alpha],\n                           [data['div_d'] * biot_alpha * d_scaling,\n                            data['compr_discr']\n                            + dt * A_flow + data['stabilization']]]).tocsr()\n\n        return A_biot\n\n    def _discretize_flow(self, g, data):\n\n        # Discretiztaion using MPFA\n        md = mpfa.Mpfa(physics='flow')\n\n        md.discretize(g, data)\n        data['flux'] = data['flux']\n        data['bound_flux'] = data['bound_flux']\n\n    def _discretize_compr(self, g, data):\n        param = data['param']\n        compr = param.fluid_compr\n        poro = param.porosity\n        data['compr_discr'] = sps.dia_matrix((g.cell_volumes * compr * poro, 0),\n                                             shape=(g.num_cells, g.num_cells))\n\n    def _discretize_mech(self, g, data):\n        \"\"\"\n        Discretization of poro-elasticity by the MPSA-W method.\n\n        Implementation needs (in addition to those mentioned in mpsa function):\n            1) Fields for non-zero boundary conditions. Should be simple.\n            2) Split return value grad_p into forces and a divergence operator,\n            so that we can compute Biot forces on a face.\n\n        Parameters:\n            g (core.grids.grid): grid to be discretized\n            k (core.constit.second_order_tensor) permeability tensor\n            bound_mech: Boundary condition object for mechancis\n            bound_flow: Boundary condition object for flow.\n            constit (porepy.bc.bc.BoundaryCondition) class for boundary values\n            faces (np.ndarray) faces to be considered. Intended for partial\n                discretization, may change in the future\n            eta Location of pressure continuity point. Should be 1/3 for simplex\n                grids, 0 otherwise. On boundary faces with Dirichlet conditions,\n                eta=0 will be enforced.\n            inverter (string) Block inverter to be used, either numba (default),\n                cython or python. See fvutils.invert_diagonal_blocks for details.\n\n        Returns:\n            scipy.sparse.csr_matrix (shape num_faces * dim, num_cells * dim): stres\n                discretization, in the form of mapping from cell displacement to\n                face stresses.\n            scipy.sparse.csr_matrix (shape num_faces * dim, num_faces * dim):\n                discretization of boundary conditions. Interpreted as istresses\n                induced by the boundary condition (both Dirichlet and Neumann). For\n                Neumann, this will be the prescribed stress over the boundary face,\n                and possibly stress on faces having nodes on the boundary. For\n                Dirichlet, the values will be stresses induced by the prescribed\n                displacement.  Incorporation as a right hand side in linear system\n                by multiplication with divergence operator.\n            scipy.sparse.csr_matrix (shape num_cells * dim, num_cells): Forces from\n                the pressure gradient (I*p-term), represented as body forces.\n                TODO: Should rather be represented as forces on faces.\n            scipy.sparse.csr_matrix (shape num_cells, num_cells * dim): Trace of\n                strain matrix, cell-wise.\n            scipy.sparse.csr_matrix (shape num_cells x num_cells): Stabilization\n                term.\n\n        Example:\n            # Set up a Cartesian grid\n            g = structured.CartGrid([5, 5])\n            c = tensor.FourthOrder(g.dim, np.ones(g.num_cells))\n            k = tensor.SecondOrder(g.dim, np.ones(g.num_cells))\n\n            # Dirirchlet boundary conditions for mechanics\n            bound_faces = g.get_all_boundary_faces().ravel()\n            bnd = bc.BoundaryCondition(g, bound_faces, ['dir'] * bound_faces.size)\n\n            # Use no boundary conditions for flow, will default to homogeneous\n            # Neumann.\n\n            # Discretization\n            stress, bound_stress, grad_p, div_d, stabilization = biot(g, c, bnd)\n            flux, bound_flux = mpfa(g, k, None)\n\n            # Source in the middle of the domain\n            q_mech = np.zeros(g.num_cells * g.dim)\n\n            # Divergence operator for the grid\n            div_mech = fvutils.vector_divergence(g)\n            div_flow = fvutils.scalar_divergence(g)\n            a_mech = div_mech * stress\n            a_flow = div_flow * flux\n\n            a_biot = sps.bmat([[a_mech, grad_p], [div_d, a_flow +\n                                                           stabilization]])\n\n            # Zero boundary conditions by default.\n\n            # Injection in the middle of the domain\n            rhs = np.zeros(g.num_cells * (g.dim + 1))\n            rhs[g.num_cells * g.dim + np.ceil(g.num_cells / 2)] = 1\n            x = sps.linalg.spsolve(A, rhs)\n\n            u_x = x[0:g.num_cells * g.dim: g.dim]\n            u_y = x[1:g.num_cells * g.dim: g.dim]\n            p = x[g.num_cells * gdim:]\n\n        \"\"\"\n        param = data['param']\n        bound_mech = param.get_bc('mechanics')\n        bound_flow = param.get_bc('flow')\n        constit = param.get_tensor('mechanics')\n\n        eta = data.get('eta', 0)\n        inverter = data.get('inverter', None)\n\n        # The grid coordinates are always three-dimensional, even if the grid\n        # is really 2D. This means that there is not a 1-1 relation between the\n        # number of coordinates of a point / vector and the real dimension.\n        # This again violates some assumptions tacitly made in the\n        # discretization (in particular that the number of faces of a cell that\n        # meets in a vertex equals the grid dimension, and that this can be\n        # used to construct an index of local variables in the discretization).\n        # These issues should be possible to overcome, but for the moment, we\n        # simply force 2D grids to be proper 2D.\n        if g.dim == 2:\n            g = g.copy()\n            g.cell_centers = np.delete(g.cell_centers, (2), axis=0)\n            g.face_centers = np.delete(g.face_centers, (2), axis=0)\n            g.face_normals = np.delete(g.face_normals, (2), axis=0)\n            g.nodes = np.delete(g.nodes, (2), axis=0)\n\n            constit.c = np.delete(constit.c, (2, 5, 6, 7, 8), axis=0)\n            constit.c = np.delete(constit.c, (2, 5, 6, 7, 8), axis=1)\n        nd = g.dim\n\n        # Define subcell topology\n        subcell_topology = fvutils.SubcellTopology(g)\n        # Obtain mappings to exclude boundary faces for mechanics\n        bound_exclusion_mech = fvutils.ExcludeBoundaries(subcell_topology,\n                                                         bound_mech, nd)\n        # ... and flow\n        bound_exclusion_flow = fvutils.ExcludeBoundaries(subcell_topology,\n                                                         bound_flow, nd)\n\n        num_subhfno = subcell_topology.subhfno.size\n\n        num_nodes = np.diff(g.face_nodes.indptr)\n        sgn = g.cell_faces[subcell_topology.fno, subcell_topology.cno].A\n\n        # The pressure gradient term in the mechanics equation is discretized\n        # as a force on the faces. The right hand side is thus formed of the\n        # normal vectors.\n        def build_rhs_normals_single_dimension(dim):\n            val = g.face_normals[dim, subcell_topology.fno] \\\n                * sgn / num_nodes[subcell_topology.fno]\n            mat = sps.coo_matrix((val.squeeze(), (subcell_topology.subfno,\n                                                  subcell_topology.cno)),\n                                 shape=(subcell_topology.num_subfno,\n                                        subcell_topology.num_cno))\n            return mat\n\n        rhs_normals = build_rhs_normals_single_dimension(0)\n        for iter1 in range(1, nd):\n            this_dim = build_rhs_normals_single_dimension(iter1)\n            rhs_normals = sps.vstack([rhs_normals, this_dim])\n\n        rhs_normals = bound_exclusion_mech.exclude_dirichlet_nd(rhs_normals)\n\n        num_dir_subface = (bound_exclusion_mech.exclude_neu.shape[1] -\n                           bound_exclusion_mech.exclude_neu.shape[0]) * nd\n        # No right hand side for cell displacement equations.\n        rhs_normals_displ_var = sps.coo_matrix((nd * subcell_topology.num_subfno\n                                                - num_dir_subface,\n                                                subcell_topology.num_cno))\n\n        # Why minus?\n        rhs_normals = -sps.vstack([rhs_normals, rhs_normals_displ_var])\n        del rhs_normals_displ_var\n\n        # Call core part of MPSA\n        hook, igrad, rhs_cells, cell_node_blocks, hook_normal \\\n            = mpsa.mpsa_elasticity(g, constit, subcell_topology,\n                                   bound_exclusion_mech, eta, inverter)\n\n        # Output should be on face-level (not sub-face)\n        hf2f = fvutils.map_hf_2_f(subcell_topology.fno_unique,\n                                  subcell_topology.subfno_unique, nd)\n\n        # Stress discretization\n        stress = hf2f * hook * igrad * rhs_cells\n\n        # Right hand side for boundary discretization\n        rhs_bound = mpsa.create_bound_rhs(bound_mech, bound_exclusion_mech,\n                                          subcell_topology, g)\n        # Discretization of boundary values\n        bound_stress = hf2f * hook * igrad * rhs_bound\n\n        # Face-wise gradient operator. Used for the term grad_p in Biot's\n        # equations.\n        rows = fvutils.expand_indices_nd(subcell_topology.cno, nd)\n        cols = np.arange(num_subhfno * nd)\n        vals = np.tile(sgn, (nd, 1)).ravel('F')\n        div_gradp = sps.coo_matrix((vals, (rows, cols)),\n                                   shape=(subcell_topology.num_cno * nd,\n                                          num_subhfno * nd)).tocsr()\n\n#        del hook, rhs_bound\n        del rows, cols, vals\n\n        grad_p = div_gradp * hook_normal * igrad * rhs_normals\n        # assert np.allclose(grad_p.sum(axis=0), np.zeros(g.num_cells))\n\n        del hook_normal, div_gradp\n\n        div = self._subcell_gradient_to_cell_scalar(g, cell_node_blocks)\n\n        div_d = div * igrad * rhs_cells\n\n        # The boundary discretization of the div_d term is represented directly\n        # on the cells, instead of going via the faces.\n        bound_div_d = div * igrad * rhs_bound\n        del rhs_cells\n\n        stabilization = div * igrad * rhs_normals\n\n        data['stress'] = stress\n        data['bound_stress'] = bound_stress\n        data['grad_p'] = grad_p\n        data['div_d'] = div_d\n        data['stabilization'] = stabilization\n        data['bound_div_d'] = bound_div_d\n\n    def _face_vector_to_scalar(self, nf, nd):\n        \"\"\" Create a mapping from vector quantities on faces (stresses) to\n        scalar quantities. The mapping is intended for the boundary\n        discretization of the term div(u) (coupling term in the flow equation).\n\n        Parameters:\n            nf (int): Number of faces in the grid\n        \"\"\"\n        rows = np.tile(np.arange(nf), ((nd, 1))).reshape((1, nd * nf),\n                                                         order='F')[0]\n\n        cols = fvutils.expand_indices_nd(np.arange(nf), nd)\n        vals = np.ones(nf * nd)\n        return sps.coo_matrix((vals, (rows, cols))).tocsr()\n\n    def _subcell_gradient_to_cell_scalar(self, g, cell_node_blocks):\n        \"\"\" Create a mapping from sub-cell gradients to cell-wise traces of the\n        gradient operator. The mapping is intended for the discretization of\n        the term div(u) (coupling term in flow equation).\n        \"\"\"\n        # To pick out the trace of the strain tensor, we access elements\n        #   (2d): 0 (u_x) and 3 (u_y)\n        #   (3d): 0 (u_x), 4 (u_y), 8 (u_z)\n        nd = g.dim\n        if nd == 2:\n            trace = np.array([0, 3])\n        elif nd == 3:\n            trace = np.array([0, 4, 8])\n\n        # Sub-cell wise trace of strain tensor: One row per sub-cell\n        row, col = np.meshgrid(np.arange(cell_node_blocks.shape[1]), trace)\n        # Adjust the columns to hit each sub-cell\n        incr = np.cumsum(nd**2 * np.ones(cell_node_blocks.shape[1])) - nd**2\n        col += incr.astype('int32')\n\n        # Integrate the trace over the sub-cell, that is, distribute the cell\n        # volumes equally over the sub-cells\n        num_cell_nodes = g.num_cell_nodes()\n        cell_vol = g.cell_volumes / num_cell_nodes\n        val = np.tile(cell_vol[cell_node_blocks[0]], (nd, 1))\n        # and we have our mapping from vector to scalar values on sub-cells\n        vector_2_scalar = sps.coo_matrix((val.ravel('F'),\n                                          (row.ravel('F'),\n                                           col.ravel('F')))).tocsr()\n\n        # Mapping from sub-cells to cells\n        div_op = sps.coo_matrix((np.ones(cell_node_blocks.shape[1]),\n                                 (cell_node_blocks[0], np.arange(\n                                     cell_node_blocks.shape[1])))).tocsr()\n        # and the composed map\n        div = div_op * vector_2_scalar\n        return div\n\n#----------------------- Linear solvers -------------------------------------\n\n    def solve(self, A, solver='direct', **kwargs):\n\n        solver = solver.strip().lower()\n        if solver == 'direct':\n            def slv(b):\n                x = la.spsolve(A, b)\n                return x\n        elif solver == 'factorized':\n            slv = la.factorized(A.tocsc())\n\n        else:\n            raise ValueError('Unknown solver ' + solver)\n\n        return slv\n\n\n#----------------------- Methods for post processing -------------------------\n    def extractD(self, g, u, dims=None, as_vector=False):\n        \"\"\" Extract displacement field from solution.\n\n        Parameters:\n            g: grid, or a subclass.\n            u (np.ndarray): Solution variable, representing displacements and\n                pressure.\n            dim (list of int, optional): Which dimension to extract. If None,\n                all dimensions are returned.\n        Returns:\n            list of np.ndarray: Displacement variables in the specified\n                dimensions.\n\n        \"\"\"\n        if dims is None:\n            dims = np.arange(g.dim)\n        vals = []\n\n        inds = np.arange(0, g.num_cells * g.dim, g.dim)\n\n        for d in dims:\n            vals.append(u[d + inds])\n        if as_vector:\n            vals = np.asarray(vals).reshape((-1, 1), order='F')\n            return vals\n        else:\n            return vals\n\n    def extractP(self, g, u):\n        \"\"\" Extract pressure field from solution.\n\n        Parameters:\n            g: grid, or a subclass.\n            u (np.ndarray): Solution variable, representing displacements and\n                pressure.\n\n        Returns:\n            np.ndarray: Pressure part of solution vector.\n\n        \"\"\"\n        return u[g.dim * g.num_cells:]\n\n    def compute_flux(self, g, u, data):\n        \"\"\" Compute flux field corresponding to a solution.\n\n        Parameters:\n            g: grid, or a subclass.\n            u (np.ndarray): Solution variable, representing displacements and\n                pressure.\n            bc_flow (np.ndarray): Flux boundary values.\n            data (dictionary): Dictionary related to grid and problem. Should\n                contain boundary discretization.\n\n        Returns:\n            np.ndarray: Flux over all faces\n\n        \"\"\"\n        flux_discr = data['flux']\n        bound_flux = data['bound_flux']\n        bound_val = data['bc_val_flow']\n        p = self.extractP(g, u)\n        flux = flux_discr * p + bound_flux * bound_val\n        return flux\n\n    def compute_stress(self, g, u, data):\n        \"\"\" Compute stress field corresponding to a solution.\n\n        Parameters:\n            g: grid, or a subclass.\n            u (np.ndarray): Solution variable, representing displacements and\n                pressure.\n            bc_flow (np.ndarray): Flux boundary values.\n            data (dictionary): Dictionary related to grid and problem. Should\n                contain boundary discretization.\n\n        Returns:\n            np.ndarray, g.dim * g.num_faces: Stress over all faces. Stored as\n                all stress values on the first face, then the second etc.\n\n        \"\"\"\n        param = data['param']\n        stress_discr = data['stress']\n        bound_stress = data['bound_stress']\n        bound_val = param.get_bc_val_mechanics()\n        d = self.extractD(g, u, as_vector=True)\n        stress = np.squeeze(stress_discr * d) + (bound_stress * bound_val)\n        return stress",
  "def __init__(self, eta=None):\n        \"\"\" Set default values for some parameters used in discretization.\n\n        \"\"\"\n        defaults = {'fluid_compr': 0,\n                    'fluid_viscosity': 1,\n                    'biot_alpha': 1\n                    }\n        self.defaults = defaults",
  "def ndof(self, g):\n        \"\"\" Return the number of degrees of freedom associated wiht the method.\n\n        In this case, each cell has nd displacement variables, as well as a\n        pressure variable.\n\n        Parameters:\n            g: grid, or a subclass.\n\n        Returns:\n            int: Number of degrees of freedom in the grid.\n\n        \"\"\"\n        return g.num_cells * (1 + g.dim)",
  "def matrix_rhs(self, g, data, discretize=True):\n        if discretize:\n            self.discretize(g, data)\n\n        A_biot = self.assemble_matrix(g, data)\n        rhs_bound = self.rhs(g, data)\n        return A_biot, rhs_bound",
  "def rhs(self, g, data):\n        bnd = self.rhs_bound(g, data)\n        tm = self.rhs_time(g, data)\n#        src = data['source']\n        return bnd + tm",
  "def rhs_bound(self, g, data):\n        \"\"\" Boundary component of the right hand side.\n\n        TODO: Boundary effects of coupling terms.\n\n        Parameters:\n            g: grid, or subclass, with geometry fields computed.\n            data: dictionary to store the data terms. Must have been through a\n                call to discretize() to discretization of right hand side.\n            state: np.ndarray, solution vector from previous time step.\n\n        Returns:\n            np.ndarray: Contribution to right hand side given the current\n            state.\n\n        \"\"\"\n        d = data['param'].get_bc_val('mechanics')\n        p = data['param'].get_bc_val('flow')\n\n        div_flow = fvutils.scalar_divergence(g)\n        div_mech = fvutils.vector_divergence(g)\n\n        p_bound = -div_flow * data['bound_flux'] * p\\\n                  - data['bound_div_d'] * d\n        s_bound = -div_mech * data['bound_stress'] * d\n        return np.hstack((s_bound, p_bound))",
  "def rhs_time(self, g, data):\n        \"\"\" Time component of the right hand side (dependency on previous time\n        step).\n\n        TODO: 1) Generalize this to allow other methods than Euler backwards?\n              2) How about time dependent boundary conditions.\n\n        Parameters:\n            g: grid, or subclass, with geometry fields computed.\n            data: dictionary to store the data terms. Must have been through a\n                call to discretize() to discretization of right hand side.\n            state: np.ndarray optional, solution vector from previous time\n                step. Defaults to zero.\n\n        Returns:\n            np.ndarray: Contribution to right hand side given the current\n            state.\n\n        \"\"\"\n        state = data.get('state', None)\n        if state is None:\n            state = np.zeros((g.dim + 1) * g.num_cells)\n\n        d = self.extractD(g, state, as_vector=True)\n        p = self.extractP(g, state)\n\n        d_scaling = data.get('displacement_scaling', 1)\n\n        div_d = np.squeeze(data['param'].biot_alpha *\n                           data['div_d'] * d * d_scaling)\n        p_cmpr = data['compr_discr'] * p\n\n        mech_rhs = np.zeros(g.dim * g.num_cells)\n\n        return np.hstack((mech_rhs, div_d + p_cmpr))",
  "def discretize(self, g, data):\n        \"\"\" Discretize flow and mechanics equations using FV methods.\n\n        The parameters needed for the discretization are stored in the\n        dictionary data, which should contain the following mandatory keywords:\n\n            Related to flow equation:\n                perm: Second order tensor representing permeability\n                bound_flow: BoundaryCondition object for flow equation. Used in\n                    mpfa.\n\n            Related to mechanics equation:\n                stiffness: Fourth order tensor representing elastic moduli.\n                bound_mech: BoundaryCondition object for mechanics equation.\n                    Used in mpsa.\n\n        In addition, the following parameters are optional:\n\n            Related to flow equation:\n                fluid_viscosity (double). Defaults to 1.\n                fluid_compr (double): Fluid compressibility. Defaults to 0.\n\n            Related to coupling terms:\n                biot_alpha (double between 0 and 1): Biot's coefficient.\n                    Defaults to 1.\n\n            Related to numerics:\n                inverter (str): Which method to use for block inversion. See\n                    fvutils.invert_diagonal_blocks for detail, and for default\n                    options.\n                eta (double): Location of continuity point in MPSA and MPFA.\n                    Defaults to 1/3 for simplex grids, 0 otherwise.\n\n        The discretization is stored in the data dictionary, in the form of\n        several matrices representing different coupling terms. For details,\n        and how to combine these, see self.assemble_matrix()\n\n        Parameters:\n            g (grid): Grid to be discretized.\n            data (dictionary): Containing data for discretization. See above\n                for specification.\n\n        \"\"\"\n        # Discretization of elasticity / poro-mechanics\n        self._discretize_flow(g, data)\n        self._discretize_mech(g, data)\n        self._discretize_compr(g, data)",
  "def assemble_matrix(self, g, data):\n        \"\"\" Assemble the poro-elastic system matrix.\n\n        The discretization is presumed stored in the data dictionary.\n\n        Parameters:\n            g (grid): Grid for disrcetization\n            data (dictionary): Data for discretization, as well as matrices\n                with discretization of the sub-parts of the system.\n\n        Returns:\n            scipy.sparse.bmat: Block matrix with the combined MPSA/MPFA\n                discretization.\n\n        \"\"\"\n        div_flow = fvutils.scalar_divergence(g)\n        div_mech = fvutils.vector_divergence(g)\n        param = data['param']\n\n        fluid_viscosity = param.fluid_viscosity\n        biot_alpha = param.biot_alpha\n\n        # Put together linear system\n        A_flow = div_flow * data['flux'] / fluid_viscosity\n        A_mech = div_mech * data['stress']\n\n        # Time step size\n        dt = data['dt']\n\n        d_scaling = data.get('displacement_scaling', 1)\n        # Matrix for left hand side\n        A_biot = sps.bmat([[A_mech,\n                            data['grad_p'] * biot_alpha],\n                           [data['div_d'] * biot_alpha * d_scaling,\n                            data['compr_discr']\n                            + dt * A_flow + data['stabilization']]]).tocsr()\n\n        return A_biot",
  "def _discretize_flow(self, g, data):\n\n        # Discretiztaion using MPFA\n        md = mpfa.Mpfa(physics='flow')\n\n        md.discretize(g, data)\n        data['flux'] = data['flux']\n        data['bound_flux'] = data['bound_flux']",
  "def _discretize_compr(self, g, data):\n        param = data['param']\n        compr = param.fluid_compr\n        poro = param.porosity\n        data['compr_discr'] = sps.dia_matrix((g.cell_volumes * compr * poro, 0),\n                                             shape=(g.num_cells, g.num_cells))",
  "def _discretize_mech(self, g, data):\n        \"\"\"\n        Discretization of poro-elasticity by the MPSA-W method.\n\n        Implementation needs (in addition to those mentioned in mpsa function):\n            1) Fields for non-zero boundary conditions. Should be simple.\n            2) Split return value grad_p into forces and a divergence operator,\n            so that we can compute Biot forces on a face.\n\n        Parameters:\n            g (core.grids.grid): grid to be discretized\n            k (core.constit.second_order_tensor) permeability tensor\n            bound_mech: Boundary condition object for mechancis\n            bound_flow: Boundary condition object for flow.\n            constit (porepy.bc.bc.BoundaryCondition) class for boundary values\n            faces (np.ndarray) faces to be considered. Intended for partial\n                discretization, may change in the future\n            eta Location of pressure continuity point. Should be 1/3 for simplex\n                grids, 0 otherwise. On boundary faces with Dirichlet conditions,\n                eta=0 will be enforced.\n            inverter (string) Block inverter to be used, either numba (default),\n                cython or python. See fvutils.invert_diagonal_blocks for details.\n\n        Returns:\n            scipy.sparse.csr_matrix (shape num_faces * dim, num_cells * dim): stres\n                discretization, in the form of mapping from cell displacement to\n                face stresses.\n            scipy.sparse.csr_matrix (shape num_faces * dim, num_faces * dim):\n                discretization of boundary conditions. Interpreted as istresses\n                induced by the boundary condition (both Dirichlet and Neumann). For\n                Neumann, this will be the prescribed stress over the boundary face,\n                and possibly stress on faces having nodes on the boundary. For\n                Dirichlet, the values will be stresses induced by the prescribed\n                displacement.  Incorporation as a right hand side in linear system\n                by multiplication with divergence operator.\n            scipy.sparse.csr_matrix (shape num_cells * dim, num_cells): Forces from\n                the pressure gradient (I*p-term), represented as body forces.\n                TODO: Should rather be represented as forces on faces.\n            scipy.sparse.csr_matrix (shape num_cells, num_cells * dim): Trace of\n                strain matrix, cell-wise.\n            scipy.sparse.csr_matrix (shape num_cells x num_cells): Stabilization\n                term.\n\n        Example:\n            # Set up a Cartesian grid\n            g = structured.CartGrid([5, 5])\n            c = tensor.FourthOrder(g.dim, np.ones(g.num_cells))\n            k = tensor.SecondOrder(g.dim, np.ones(g.num_cells))\n\n            # Dirirchlet boundary conditions for mechanics\n            bound_faces = g.get_all_boundary_faces().ravel()\n            bnd = bc.BoundaryCondition(g, bound_faces, ['dir'] * bound_faces.size)\n\n            # Use no boundary conditions for flow, will default to homogeneous\n            # Neumann.\n\n            # Discretization\n            stress, bound_stress, grad_p, div_d, stabilization = biot(g, c, bnd)\n            flux, bound_flux = mpfa(g, k, None)\n\n            # Source in the middle of the domain\n            q_mech = np.zeros(g.num_cells * g.dim)\n\n            # Divergence operator for the grid\n            div_mech = fvutils.vector_divergence(g)\n            div_flow = fvutils.scalar_divergence(g)\n            a_mech = div_mech * stress\n            a_flow = div_flow * flux\n\n            a_biot = sps.bmat([[a_mech, grad_p], [div_d, a_flow +\n                                                           stabilization]])\n\n            # Zero boundary conditions by default.\n\n            # Injection in the middle of the domain\n            rhs = np.zeros(g.num_cells * (g.dim + 1))\n            rhs[g.num_cells * g.dim + np.ceil(g.num_cells / 2)] = 1\n            x = sps.linalg.spsolve(A, rhs)\n\n            u_x = x[0:g.num_cells * g.dim: g.dim]\n            u_y = x[1:g.num_cells * g.dim: g.dim]\n            p = x[g.num_cells * gdim:]\n\n        \"\"\"\n        param = data['param']\n        bound_mech = param.get_bc('mechanics')\n        bound_flow = param.get_bc('flow')\n        constit = param.get_tensor('mechanics')\n\n        eta = data.get('eta', 0)\n        inverter = data.get('inverter', None)\n\n        # The grid coordinates are always three-dimensional, even if the grid\n        # is really 2D. This means that there is not a 1-1 relation between the\n        # number of coordinates of a point / vector and the real dimension.\n        # This again violates some assumptions tacitly made in the\n        # discretization (in particular that the number of faces of a cell that\n        # meets in a vertex equals the grid dimension, and that this can be\n        # used to construct an index of local variables in the discretization).\n        # These issues should be possible to overcome, but for the moment, we\n        # simply force 2D grids to be proper 2D.\n        if g.dim == 2:\n            g = g.copy()\n            g.cell_centers = np.delete(g.cell_centers, (2), axis=0)\n            g.face_centers = np.delete(g.face_centers, (2), axis=0)\n            g.face_normals = np.delete(g.face_normals, (2), axis=0)\n            g.nodes = np.delete(g.nodes, (2), axis=0)\n\n            constit.c = np.delete(constit.c, (2, 5, 6, 7, 8), axis=0)\n            constit.c = np.delete(constit.c, (2, 5, 6, 7, 8), axis=1)\n        nd = g.dim\n\n        # Define subcell topology\n        subcell_topology = fvutils.SubcellTopology(g)\n        # Obtain mappings to exclude boundary faces for mechanics\n        bound_exclusion_mech = fvutils.ExcludeBoundaries(subcell_topology,\n                                                         bound_mech, nd)\n        # ... and flow\n        bound_exclusion_flow = fvutils.ExcludeBoundaries(subcell_topology,\n                                                         bound_flow, nd)\n\n        num_subhfno = subcell_topology.subhfno.size\n\n        num_nodes = np.diff(g.face_nodes.indptr)\n        sgn = g.cell_faces[subcell_topology.fno, subcell_topology.cno].A\n\n        # The pressure gradient term in the mechanics equation is discretized\n        # as a force on the faces. The right hand side is thus formed of the\n        # normal vectors.\n        def build_rhs_normals_single_dimension(dim):\n            val = g.face_normals[dim, subcell_topology.fno] \\\n                * sgn / num_nodes[subcell_topology.fno]\n            mat = sps.coo_matrix((val.squeeze(), (subcell_topology.subfno,\n                                                  subcell_topology.cno)),\n                                 shape=(subcell_topology.num_subfno,\n                                        subcell_topology.num_cno))\n            return mat\n\n        rhs_normals = build_rhs_normals_single_dimension(0)\n        for iter1 in range(1, nd):\n            this_dim = build_rhs_normals_single_dimension(iter1)\n            rhs_normals = sps.vstack([rhs_normals, this_dim])\n\n        rhs_normals = bound_exclusion_mech.exclude_dirichlet_nd(rhs_normals)\n\n        num_dir_subface = (bound_exclusion_mech.exclude_neu.shape[1] -\n                           bound_exclusion_mech.exclude_neu.shape[0]) * nd\n        # No right hand side for cell displacement equations.\n        rhs_normals_displ_var = sps.coo_matrix((nd * subcell_topology.num_subfno\n                                                - num_dir_subface,\n                                                subcell_topology.num_cno))\n\n        # Why minus?\n        rhs_normals = -sps.vstack([rhs_normals, rhs_normals_displ_var])\n        del rhs_normals_displ_var\n\n        # Call core part of MPSA\n        hook, igrad, rhs_cells, cell_node_blocks, hook_normal \\\n            = mpsa.mpsa_elasticity(g, constit, subcell_topology,\n                                   bound_exclusion_mech, eta, inverter)\n\n        # Output should be on face-level (not sub-face)\n        hf2f = fvutils.map_hf_2_f(subcell_topology.fno_unique,\n                                  subcell_topology.subfno_unique, nd)\n\n        # Stress discretization\n        stress = hf2f * hook * igrad * rhs_cells\n\n        # Right hand side for boundary discretization\n        rhs_bound = mpsa.create_bound_rhs(bound_mech, bound_exclusion_mech,\n                                          subcell_topology, g)\n        # Discretization of boundary values\n        bound_stress = hf2f * hook * igrad * rhs_bound\n\n        # Face-wise gradient operator. Used for the term grad_p in Biot's\n        # equations.\n        rows = fvutils.expand_indices_nd(subcell_topology.cno, nd)\n        cols = np.arange(num_subhfno * nd)\n        vals = np.tile(sgn, (nd, 1)).ravel('F')\n        div_gradp = sps.coo_matrix((vals, (rows, cols)),\n                                   shape=(subcell_topology.num_cno * nd,\n                                          num_subhfno * nd)).tocsr()\n\n#        del hook, rhs_bound\n        del rows, cols, vals\n\n        grad_p = div_gradp * hook_normal * igrad * rhs_normals\n        # assert np.allclose(grad_p.sum(axis=0), np.zeros(g.num_cells))\n\n        del hook_normal, div_gradp\n\n        div = self._subcell_gradient_to_cell_scalar(g, cell_node_blocks)\n\n        div_d = div * igrad * rhs_cells\n\n        # The boundary discretization of the div_d term is represented directly\n        # on the cells, instead of going via the faces.\n        bound_div_d = div * igrad * rhs_bound\n        del rhs_cells\n\n        stabilization = div * igrad * rhs_normals\n\n        data['stress'] = stress\n        data['bound_stress'] = bound_stress\n        data['grad_p'] = grad_p\n        data['div_d'] = div_d\n        data['stabilization'] = stabilization\n        data['bound_div_d'] = bound_div_d",
  "def _face_vector_to_scalar(self, nf, nd):\n        \"\"\" Create a mapping from vector quantities on faces (stresses) to\n        scalar quantities. The mapping is intended for the boundary\n        discretization of the term div(u) (coupling term in the flow equation).\n\n        Parameters:\n            nf (int): Number of faces in the grid\n        \"\"\"\n        rows = np.tile(np.arange(nf), ((nd, 1))).reshape((1, nd * nf),\n                                                         order='F')[0]\n\n        cols = fvutils.expand_indices_nd(np.arange(nf), nd)\n        vals = np.ones(nf * nd)\n        return sps.coo_matrix((vals, (rows, cols))).tocsr()",
  "def _subcell_gradient_to_cell_scalar(self, g, cell_node_blocks):\n        \"\"\" Create a mapping from sub-cell gradients to cell-wise traces of the\n        gradient operator. The mapping is intended for the discretization of\n        the term div(u) (coupling term in flow equation).\n        \"\"\"\n        # To pick out the trace of the strain tensor, we access elements\n        #   (2d): 0 (u_x) and 3 (u_y)\n        #   (3d): 0 (u_x), 4 (u_y), 8 (u_z)\n        nd = g.dim\n        if nd == 2:\n            trace = np.array([0, 3])\n        elif nd == 3:\n            trace = np.array([0, 4, 8])\n\n        # Sub-cell wise trace of strain tensor: One row per sub-cell\n        row, col = np.meshgrid(np.arange(cell_node_blocks.shape[1]), trace)\n        # Adjust the columns to hit each sub-cell\n        incr = np.cumsum(nd**2 * np.ones(cell_node_blocks.shape[1])) - nd**2\n        col += incr.astype('int32')\n\n        # Integrate the trace over the sub-cell, that is, distribute the cell\n        # volumes equally over the sub-cells\n        num_cell_nodes = g.num_cell_nodes()\n        cell_vol = g.cell_volumes / num_cell_nodes\n        val = np.tile(cell_vol[cell_node_blocks[0]], (nd, 1))\n        # and we have our mapping from vector to scalar values on sub-cells\n        vector_2_scalar = sps.coo_matrix((val.ravel('F'),\n                                          (row.ravel('F'),\n                                           col.ravel('F')))).tocsr()\n\n        # Mapping from sub-cells to cells\n        div_op = sps.coo_matrix((np.ones(cell_node_blocks.shape[1]),\n                                 (cell_node_blocks[0], np.arange(\n                                     cell_node_blocks.shape[1])))).tocsr()\n        # and the composed map\n        div = div_op * vector_2_scalar\n        return div",
  "def solve(self, A, solver='direct', **kwargs):\n\n        solver = solver.strip().lower()\n        if solver == 'direct':\n            def slv(b):\n                x = la.spsolve(A, b)\n                return x\n        elif solver == 'factorized':\n            slv = la.factorized(A.tocsc())\n\n        else:\n            raise ValueError('Unknown solver ' + solver)\n\n        return slv",
  "def extractD(self, g, u, dims=None, as_vector=False):\n        \"\"\" Extract displacement field from solution.\n\n        Parameters:\n            g: grid, or a subclass.\n            u (np.ndarray): Solution variable, representing displacements and\n                pressure.\n            dim (list of int, optional): Which dimension to extract. If None,\n                all dimensions are returned.\n        Returns:\n            list of np.ndarray: Displacement variables in the specified\n                dimensions.\n\n        \"\"\"\n        if dims is None:\n            dims = np.arange(g.dim)\n        vals = []\n\n        inds = np.arange(0, g.num_cells * g.dim, g.dim)\n\n        for d in dims:\n            vals.append(u[d + inds])\n        if as_vector:\n            vals = np.asarray(vals).reshape((-1, 1), order='F')\n            return vals\n        else:\n            return vals",
  "def extractP(self, g, u):\n        \"\"\" Extract pressure field from solution.\n\n        Parameters:\n            g: grid, or a subclass.\n            u (np.ndarray): Solution variable, representing displacements and\n                pressure.\n\n        Returns:\n            np.ndarray: Pressure part of solution vector.\n\n        \"\"\"\n        return u[g.dim * g.num_cells:]",
  "def compute_flux(self, g, u, data):\n        \"\"\" Compute flux field corresponding to a solution.\n\n        Parameters:\n            g: grid, or a subclass.\n            u (np.ndarray): Solution variable, representing displacements and\n                pressure.\n            bc_flow (np.ndarray): Flux boundary values.\n            data (dictionary): Dictionary related to grid and problem. Should\n                contain boundary discretization.\n\n        Returns:\n            np.ndarray: Flux over all faces\n\n        \"\"\"\n        flux_discr = data['flux']\n        bound_flux = data['bound_flux']\n        bound_val = data['bc_val_flow']\n        p = self.extractP(g, u)\n        flux = flux_discr * p + bound_flux * bound_val\n        return flux",
  "def compute_stress(self, g, u, data):\n        \"\"\" Compute stress field corresponding to a solution.\n\n        Parameters:\n            g: grid, or a subclass.\n            u (np.ndarray): Solution variable, representing displacements and\n                pressure.\n            bc_flow (np.ndarray): Flux boundary values.\n            data (dictionary): Dictionary related to grid and problem. Should\n                contain boundary discretization.\n\n        Returns:\n            np.ndarray, g.dim * g.num_faces: Stress over all faces. Stored as\n                all stress values on the first face, then the second etc.\n\n        \"\"\"\n        param = data['param']\n        stress_discr = data['stress']\n        bound_stress = data['bound_stress']\n        bound_val = param.get_bc_val_mechanics()\n        d = self.extractD(g, u, as_vector=True)\n        stress = np.squeeze(stress_discr * d) + (bound_stress * bound_val)\n        return stress",
  "def build_rhs_normals_single_dimension(dim):\n            val = g.face_normals[dim, subcell_topology.fno] \\\n                * sgn / num_nodes[subcell_topology.fno]\n            mat = sps.coo_matrix((val.squeeze(), (subcell_topology.subfno,\n                                                  subcell_topology.cno)),\n                                 shape=(subcell_topology.num_subfno,\n                                        subcell_topology.num_cno))\n            return mat",
  "def slv(b):\n                x = la.spsolve(A, b)\n                return x",
  "class SubcellTopology(object):\n    \"\"\"\n    Class to represent data of subcell topology (interaction regions) for\n    mpsa/mpfa.\n\n    Attributes:\n        g - the grid\n\n        Subcell topology, all cells seen apart:\n        nno - node numbers\n        fno - face numbers\n        cno - cell numbers\n        subfno - subface numbers. Has exactly two duplicates for internal faces\n        subhfno - subface numbers. No duplicates\n        num_cno - cno.max() + 1\n        num_subfno - subfno.max() + 1\n\n        Subcell topology, after cells sharing a face have been joined. Use\n        terminology _unique, since subfno is unique after\n        nno_unique - node numbers after pairing sub-faces\n        fno_unique - face numbers after pairing sub-faces\n        cno_unique - cell numbers after pairing sub-faces\n        subfno_unique - subface numbers  after pairing sub-faces\n        unique_subfno - index of those elements in subfno that are included\n            in subfno_unique, that is subfno_unique = subfno[unique_subfno],\n            and similarly cno_unique = cno[subfno_unique] etc.\n        num_subfno_unique = subfno_unique.max() + 1\n\n    \"\"\"\n\n    def __init__(self, g):\n        \"\"\"\n        Constructor for subcell topology\n\n        Parameters\n        ----------\n        g grid\n        \"\"\"\n        self.g = g\n\n        # Indices of neighboring faces and cells. The indices are sorted to\n        # simplify later treatment\n        g.cell_faces.sort_indices()\n        face_ind, cell_ind = g.cell_faces.nonzero()\n\n        # Number of faces per node\n        num_face_nodes = np.diff(g.face_nodes.indptr)\n\n        # Duplicate cell and face indices, so that they can be matched with\n        # the nodes\n        cells_duplicated = matrix_compression.rldecode(\n            cell_ind, num_face_nodes[face_ind])\n        faces_duplicated = matrix_compression.rldecode(\n            face_ind, num_face_nodes[face_ind])\n        M = sps.coo_matrix((np.ones(face_ind.size),\n                            (face_ind, np.arange(face_ind.size))),\n                           shape=(face_ind.max() + 1, face_ind.size))\n        nodes_duplicated = g.face_nodes * M\n        nodes_duplicated = nodes_duplicated.indices\n\n        face_nodes_indptr = g.face_nodes.indptr\n        face_nodes_indices = g.face_nodes.indices\n        face_nodes_data = np.arange(face_nodes_indices.size) + 1\n        sub_face_mat = sps.csc_matrix((face_nodes_data, face_nodes_indices,\n                                       face_nodes_indptr))\n        sub_faces = sub_face_mat * M\n        sub_faces = sub_faces.data - 1\n\n        # Sort data\n        idx = np.lexsort((sub_faces, faces_duplicated, nodes_duplicated,\n                          cells_duplicated))\n        self.nno = nodes_duplicated[idx]\n        self.cno = cells_duplicated[idx]\n        self.fno = faces_duplicated[idx]\n        self.subfno = sub_faces[idx].astype(int)\n        self.subhfno = np.arange(idx.size, dtype='>i4')\n        self.num_subfno = self.subfno.max() + 1\n        self.num_cno = self.cno.max() + 1\n\n        # Make subface indices unique, that is, pair the indices from the two\n        # adjacent cells\n        _, unique_subfno = np.unique(self.subfno, return_index=True)\n\n        # Reduce topology to one field per subface\n        self.nno_unique = self.nno[unique_subfno]\n        self.fno_unique = self.fno[unique_subfno]\n        self.cno_unique = self.cno[unique_subfno]\n        self.subfno_unique = self.subfno[unique_subfno]\n        self.num_subfno_unique = self.subfno_unique.max() + 1\n        self.unique_subfno = unique_subfno\n\n    def __repr__(self):\n        s = 'Subcell topology with:\\n'\n        s += str(self.num_cno) + ' cells\\n'\n        s += str(self.g.num_nodes) + ' nodes\\n'\n        s += str(self.g.num_faces) + ' faces\\n'\n        s += str(self.num_subfno_unique) + ' unique subfaces\\n'\n        s += str(self.fno.size) + ' subfaces before pairing face neighbors\\n'\n        return s\n\n    def pair_over_subfaces(self, other):\n        \"\"\"\n        Transfer quantities from a cell-face base (cells sharing a face have\n        their own expressions) to a face-based. The quantities should live\n        on sub-faces (not faces)\n\n        The normal vector is honored, so that the here and there side get\n        different signs when paired up.\n\n        The method is intended used for combining forces, fluxes,\n        displacements and pressures, as used in MPSA / MPFA.\n\n        Parameters\n        ----------\n        other: sps.matrix, size (self.subhfno.size x something)\n\n        Returns\n        -------\n        sps.matrix, size (self.subfno_unique.size x something)\n        \"\"\"\n\n        sgn = self.g.cell_faces[self.fno, self.cno].A\n        pair_over_subfaces = sps.coo_matrix((sgn[0], (self.subfno,\n                                                      self.subhfno)))\n        return pair_over_subfaces * other\n\n    def pair_over_subfaces_nd(self, other):\n        \"\"\" nd-version of pair_over_subfaces, see above. \"\"\"\n        nd = self.g.dim\n        # For force balance, displacements and stresses on the two sides of the\n        # matrices must be paired\n        # Operator to create the pairing\n        sgn = self.g.cell_faces[self.fno, self.cno].A\n        pair_over_subfaces = sps.coo_matrix((sgn[0], (self.subfno,\n                                                      self.subhfno)))\n        # vector version, to be used on stresses\n        pair_over_subfaces_nd = sps.kron(sps.eye(nd), pair_over_subfaces)\n        return pair_over_subfaces_nd * other",
  "def compute_dist_face_cell(g, subcell_topology, eta):\n    \"\"\"\n    Compute vectors from cell centers continuity points on each sub-face.\n\n    The location of the continuity point is given by\n\n        x_cp = (1-eta) * x_facecenter + eta * x_vertex\n\n    On the boundary, eta is set to zero, thus the continuity point is at the\n    face center\n\n    Parameters\n    ----------\n    g: Grid\n    subcell_topology: Of class subcell topology in this module\n    eta: [0,1), eta = 0 gives cont. pt. at face midpoint, eta = 1 means at\n    the vertex\n\n    Returns\n    -------\n    sps.csr() matrix representation of vectors. Size g.nf x (g.nc * g.nd)\n    \"\"\"\n    _, blocksz = matrix_compression.rlencode(np.vstack((\n        subcell_topology.cno, subcell_topology.nno)))\n    dims = g.dim\n\n    _, cols = np.meshgrid(subcell_topology.subhfno, np.arange(dims))\n    cols += matrix_compression.rldecode(np.cumsum(blocksz) -\n                                        blocksz[0], blocksz)\n    eta_vec = eta * np.ones(subcell_topology.fno.size)\n    # Set eta values to zero at the boundary\n    bnd = np.in1d(subcell_topology.fno, g.get_all_boundary_faces())\n    eta_vec[bnd] = 0\n    cp = g.face_centers[:, subcell_topology.fno] \\\n        + eta_vec * (g.nodes[:, subcell_topology.nno] -\n                     g.face_centers[:, subcell_topology.fno])\n    dist = cp - g.cell_centers[:, subcell_topology.cno]\n\n    ind_ptr = np.hstack((np.arange(0, cols.size, dims), cols.size))\n    mat = sps.csr_matrix((dist.ravel('F'), cols.ravel('F'), ind_ptr))\n    return subcell_topology.pair_over_subfaces(mat)",
  "def determine_eta(g):\n    \"\"\" Set default value for the location of continuity point eta in MPFA and\n    MSPA.\n\n    The function is intended to give a best estimate of eta in cases where the\n    user has not specified a value.\n\n    Parameters:\n        g: Grid for discretization\n\n    Returns:\n        double. 1/3 if the grid in known to consist of simplicies (it is one of\n           TriangleGrid, TetrahedralGrid, or their structured versions). 0 if\n           not.\n    \"\"\"\n\n    if 'StructuredTriangleGrid' in g.name:\n        return 1 / 3\n    elif 'TriangleGrid' in g.name:\n        return 1 / 3\n    elif 'StructuredTetrahedralGrid' in g.name:\n        return 1 / 3\n    elif 'TetrahedralGrid' in g.name:\n        return 1 / 3\n    else:\n        return 0",
  "def invert_diagonal_blocks(mat, s, method=None):\n    \"\"\"\n    Invert block diagonal matrix.\n\n    Three implementations are available, either pure numpy, or a speedup using\n    numba or cython. If none is specified, the function will try to use numba,\n    then cython. The python option will only be invoked if explicitly asked\n    for; it will be very slow for general problems.\n\n    Parameters\n    ----------\n    mat: sps.csr matrix to be inverted.\n    s: block size. Must be int64 for the numba acceleration to work\n    method: Choice of method. Either numba (default), cython or 'python'.\n        Defaults to None, in which case first numba, then cython is tried.\n\n    Returns\n    -------\n    imat: Inverse matrix\n\n    Raises\n    -------\n    ImportError: If numba or cython implementation is invoked without numba or\n        cython being available on the system.\n\n    \"\"\"\n\n    def invert_diagonal_blocks_python(a, sz):\n        \"\"\"\n        Invert block diagonal matrix using pure python code.\n\n        The implementation is slow for large matrices, consider to use the\n        numba-accelerated method invert_invert_diagagonal_blocks_numba instead\n\n        Parameters\n        ----------\n        A sps.crs-matrix, to be inverted\n        sz - size of the individual blocks\n\n        Returns\n        -------\n        inv_a inverse matrix\n        \"\"\"\n        v = np.zeros(np.sum(np.square(sz)))\n        p1 = 0\n        p2 = 0\n        for b in range(sz.size):\n            n = sz[b]\n            n2 = n * n\n            i = p1 + np.arange(n + 1)\n            # Picking out the sub-matrices here takes a lot of time.\n            v[p2 + np.arange(n2)] = np.linalg.inv(a[i[0]:i[-1],\n                                                    i[0]:i[-1]].A).ravel()\n            p1 = p1 + n\n            p2 = p2 + n2\n        return v\n\n    def invert_diagonal_blocks_cython(a, size):\n        \"\"\" Invert block diagonal matrix using code wrapped with cython.\n        \"\"\"\n        try:\n            import porepy.numerics.fv.cythoninvert as cythoninvert\n        except:\n            ImportError('Compiled Cython module not available. Is cython\\\n            installed?')\n\n        a.sorted_indices()\n        ptr = a.indptr\n        indices = a.indices\n        dat = a.data\n\n        v = cythoninvert.inv_python(ptr, indices, dat, size)\n        return v\n\n    def invert_diagonal_blocks_numba(a, size):\n        \"\"\"\n        Invert block diagonal matrix by invoking numba acceleration of a simple\n        for-loop based algorithm.\n\n        This approach should be more efficient than the related method\n        invert_diagonal_blocks_python for larger problems.\n\n        Parameters\n        ----------\n        a : sps.csr matrix\n        size : Size of individual blocks\n\n        Returns\n        -------\n        ia: inverse of a\n        \"\"\"\n        try:\n            import numba\n        except:\n            raise ImportError('Numba not available on the system')\n\n        # Sort matrix storage before pulling indices and data\n        a.sorted_indices()\n        ptr = a.indptr\n        indices = a.indices\n        dat = a.data\n\n        # Just in time compilation\n        @numba.jit(\"f8[:](i4[:],i4[:],f8[:],i8[:])\", nopython=True,\n                   nogil=False)\n        def inv_python(indptr, ind, data, sz):\n            \"\"\"\n            Invert block matrices by explicitly forming local matrices. The code\n            in itself is not efficient, but it is hopefully well suited for\n            speeding up with numba.\n\n            It may be possible to restruct the code to further help numba,\n            this has not been investigated.\n\n            The computation can easily be parallelized, consider this later.\n            \"\"\"\n\n            # Index of where the rows start for each block.\n            # block_row_starts_ind = np.hstack((np.array([0]),\n            #                                   np.cumsum(sz[:-1])))\n            block_row_starts_ind = np.zeros(sz.size, dtype=np.int32)\n            block_row_starts_ind[1:] = np.cumsum(sz[:-1])\n\n            # Number of columns per row. Will change from one column to the\n            # next\n            num_cols_per_row = indptr[1:] - indptr[0:-1]\n            # Index to where the columns start for each row (NOT blocks)\n            # row_cols_start_ind = np.hstack((np.zeros(1),\n            #                                 np.cumsum(num_cols_per_row)))\n            row_cols_start_ind = np.zeros(num_cols_per_row.size + 1,\n                                          dtype=np.int32)\n            row_cols_start_ind[1:] = np.cumsum(num_cols_per_row)\n\n            # Index to where the (full) data starts. Needed, since the\n            # inverse matrix will generally be full\n            # full_block_starts_ind = np.hstack((np.array([0]),\n            #                                    np.cumsum(np.square(sz))))\n            full_block_starts_ind = np.zeros(sz.size + 1, dtype=np.int32)\n            full_block_starts_ind[1:] = np.cumsum(np.square(sz))\n            # Structure to store the solution\n            inv_vals = np.zeros(np.sum(np.square(sz)))\n\n            # Loop over all blocks\n            for iter1 in range(sz.size):\n                n = sz[iter1]\n                loc_mat = np.zeros((n, n))\n                # Fill in non-zero elements in local matrix\n                for iter2 in range(n):  # Local rows\n                    global_row = block_row_starts_ind[iter1] + iter2\n                    data_counter = row_cols_start_ind[global_row]\n\n                    # Loop over local columns. Getting the number of columns\n                    #  for each row is a bit involved\n                    for _ in range(num_cols_per_row[iter2 +\n                                                    block_row_starts_ind[iter1]]):\n                        loc_col = ind[data_counter] \\\n                            - block_row_starts_ind[iter1]\n                        loc_mat[iter2, loc_col] = data[data_counter]\n                        data_counter += 1\n\n                # Compute inverse. np.linalg.inv is supported by numba (May\n                # 2016), it is not clear if this is the best option. To be\n                # revised\n                inv_mat = np.ravel(np.linalg.inv(loc_mat))\n\n                loc_ind = np.arange(full_block_starts_ind[iter1],\n                                    full_block_starts_ind[iter1 + 1])\n                inv_vals[loc_ind] = inv_mat\n                # Update fields\n            return inv_vals\n\n        v = inv_python(ptr, indices, dat, size)\n        return v\n\n    # Variable to check if we have tried and failed with numba\n    try_cython = False\n    # Do not use numba for small systems due to compiling overhead. If the\n    # application is inversion of transmissibility systems in mpfa,\n    # s.shape[0] = number of nodes of the grid.\n    if method == 'numba' or (method is None and s.shape[0] > 4000):\n        try:\n            inv_vals = invert_diagonal_blocks_numba(mat, s)\n        except:\n            # This went wrong, fall back on cython\n            try_cython = True\n    # Variable to check if we should fall back on python\n    if method == 'cython' or try_cython:\n        try:\n            inv_vals = invert_diagonal_blocks_cython(mat, s)\n        except ImportError as e:\n            raise e\n    elif method == 'python' or (method is None and s.shape[0] <= 4000):\n        inv_vals = invert_diagonal_blocks_python(mat, s)\n\n    ia = block_diag_matrix(inv_vals, s)\n    return ia",
  "def block_diag_matrix(vals, sz):\n    \"\"\"\n    Construct block diagonal matrix based on matrix elements and block sizes.\n\n    Parameters\n    ----------\n    vals: matrix values\n    sz: size of matrix blocks\n\n    Returns\n    -------\n    sps.csr matrix\n    \"\"\"\n    row, _ = block_diag_index(sz)\n    # This line recovers starting indices of the rows.\n    indptr = np.hstack((np.zeros(1),\n                        np.cumsum(matrix_compression\n                                  .rldecode(sz, sz)))).astype('int32')\n    return sps.csr_matrix((vals, row, indptr))",
  "def block_diag_index(m, n=None):\n    \"\"\"\n    Get row and column indices for block diagonal matrix\n\n    This is intended as the equivalent of the corresponding method in MRST.\n\n    Examples:\n    >>> m = np.array([2, 3])\n    >>> n = np.array([1, 2])\n    >>> i, j = block_diag_index(m, n)\n    >>> i, j\n    (array([0, 1, 2, 3, 4, 2, 3, 4]), array([0, 0, 1, 1, 1, 2, 2, 2]))\n    >>> a = np.array([1, 3])\n    >>> i, j = block_diag_index(a)\n    >>> i, j\n    (array([0, 1, 2, 3, 1, 2, 3, 1, 2, 3]), array([0, 1, 1, 1, 2, 2, 2, 3, 3, 3]))\n\n    Parameters:\n        m - ndarray, dimension 1\n        n - ndarray, dimension 1, defaults to m\n\n    \"\"\"\n    if n is None:\n        n = m\n\n    start = np.hstack((np.zeros(1, dtype='int'), m))\n    pos = np.cumsum(start)\n    p1 = pos[0:-1]\n    p2 = pos[1:] - 1\n    p1_full = matrix_compression.rldecode(p1, n)\n    p2_full = matrix_compression.rldecode(p2, n)\n\n    i = mcolon.mcolon(p1_full, p2_full + 1)\n    sumn = np.arange(np.sum(n))\n    m_n_full = matrix_compression.rldecode(m, n)\n    j = matrix_compression.rldecode(sumn, m_n_full)\n    return i, j",
  "def expand_indices_nd(ind, nd, direction=1):\n    \"\"\"\n    Expand indices from scalar to vector form.\n\n    Examples:\n    >>> i = np.array([0, 1, 3])\n    >>> __expand_indices_nd(i, 2)\n    (array([0, 1, 2, 3, 6, 7]))\n\n    >>> __expand_indices_nd(i, 3, 0)\n    (array([0, 3, 9, 1, 4, 10, 2, 5, 11])\n\n    Parameters\n    ----------\n    ind\n    nd\n    direction\n\n    Returns\n    -------\n\n    \"\"\"\n    dim_inds = np.arange(nd)\n    dim_inds = dim_inds[:, np.newaxis]  # Prepare for broadcasting\n    new_ind = nd * ind + dim_inds\n    new_ind = new_ind.ravel(direction)\n    return new_ind",
  "def map_hf_2_f(fno, subfno, nd):\n    \"\"\"\n    Create mapping from half-faces to faces for vector problems.\n\n    Parameters\n    ----------\n    fno face numbering in sub-cell topology based on unique subfno\n    subfno sub-face numbering\n    nd dimension\n\n    Returns\n    -------\n\n    \"\"\"\n\n    hfi = expand_indices_nd(subfno, nd)\n    hf = expand_indices_nd(fno, nd)\n    hf2f = sps.coo_matrix((np.ones(hf.size), (hf, hfi)),\n                          shape=(hf.max() + 1, hfi.max() + 1)).tocsr()\n    return hf2f",
  "def scalar_divergence(g):\n    \"\"\"\n    Get divergence operator for a grid.\n\n    The operator is easily accessible from the grid itself, so we keep it\n    here for completeness.\n\n    See also vector_divergence(g)\n\n    Parameters\n    ----------\n    g grid\n\n    Returns\n    -------\n    divergence operator\n    \"\"\"\n    return g.cell_faces.T",
  "def vector_divergence(g):\n    \"\"\"\n    Get vector divergence operator for a grid g\n\n    It is assumed that the first column corresponds to the x-equation of face\n    0, second column is y-equation etc. (and so on in nd>2). The next column is\n    then the x-equation for face 1. Correspondingly, the first row\n    represents x-component in first cell etc.\n\n    Parameters\n    ----------\n    g grid\n\n    Returns\n    -------\n    vector_div (sparse csr matrix), dimensions: nd * (num_cells, num_faces)\n    \"\"\"\n    # Scalar divergence\n    scalar_div = g.cell_faces\n\n    # Vector extension, convert to coo-format to avoid odd errors when one\n    # grid dimension is 1 (this may return a bsr matrix)\n    # The order of arguments to sps.kron is important.\n    block_div = sps.kron(scalar_div, sps.eye(g.dim)).tocsr()\n\n    return block_div.transpose()",
  "def zero_out_sparse_rows(A, rows, diag=None):\n    \"\"\"\n    zeros out given rows from sparse csr matrix. Optionally also set values on\n    the diagonal.\n\n    Parameters:\n        A: Sparse matrix\n        rows (np.ndarray of int): Indices of rows to be eliminated.\n        diag (np.ndarray, double, optional): Values to be set to the diagonal\n            on the eliminated rows.\n\n    \"\"\"\n\n    # If matrix is not csr, it will be converted to csr, then the rows will be\n    # zeroed, and the matrix converted back.\n    flag = False\n    if not A.getformat() == 'csr':\n        mat_format = A.getformat()\n        A = A.tocsr()\n        flag = True\n\n    ip = A.indptr\n    row_indices = mcolon.mcolon(ip[rows], ip[rows + 1])\n    A.data[row_indices] = 0\n    if diag is not None:\n        # now we set the diagonal\n        diag_vals = np.zeros(A.shape[1])\n        diag_vals[rows] = diag\n        A += sps.dia_matrix((diag_vals, 0), shape=A.shape)\n\n    if flag:\n        # Convert matrix back\n        A = A.astype(mat_format)\n\n    return A",
  "class ExcludeBoundaries(object):\n    \"\"\" Wrapper class to store mapping for exclusion of equations that are\n    redundant due to the presence of boundary conditions.\n\n    The systems being set up in mpfa (and mpsa) describe continuity of flux and\n    potential (respectively stress and displacement) on all sub-faces. For\n    boundary faces, one of the two should be excluded (e.g. for a Dirichlet\n    boundary condition, there is no concept of continuity of flux/stresses).\n    The class contains mappings to eliminate the necessary fields.\n\n    \"\"\"\n\n    def __init__(self, subcell_topology, bound, nd):\n        \"\"\"\n        Define mappings to exclude boundary faces with dirichlet and neumann\n        conditions\n\n        Parameters\n        ----------\n        subcell_topology\n        bound\n\n        Returns\n        -------\n        exclude_neumann: Matrix, mapping from all faces to those having flux\n                         continuity\n        exclude_dirichlet: Matrix, mapping from all faces to those having pressure\n                           continuity\n        \"\"\"\n        self.nd = nd\n\n        # Short hand notation\n        fno = subcell_topology.fno_unique\n        num_subfno = subcell_topology.num_subfno_unique\n\n        # Define mappings to exclude boundary values\n        col_neu = np.argwhere([not it for it in bound.is_neu[fno]])\n        row_neu = np.arange(col_neu.size)\n        self.exclude_neu = sps.coo_matrix((np.ones(row_neu.size),\n                                           (row_neu, col_neu.ravel('C'))),\n                                          shape=(row_neu.size,\n                                                 num_subfno)).tocsr()\n        col_dir = np.argwhere([not it for it in bound.is_dir[fno]])\n        row_dir = np.arange(col_dir.size)\n        self.exclude_dir = sps.coo_matrix((np.ones(row_dir.size),\n                                           (row_dir, col_dir.ravel('C'))),\n                                          shape=(row_dir.size,\n                                                 num_subfno)).tocsr()\n\n    def exclude_dirichlet(self, other):\n        \"\"\" Mapping to exclude faces with Dirichlet boundary conditions from\n        local systems.\n\n        Parameters:\n            other (scipy.sparse matrix): Matrix of local equations for\n                continuity of flux and pressure.\n\n        Returns:\n            scipy.sparse matrix, with rows corresponding to faces with\n                Dirichlet conditions eliminated.\n\n        \"\"\"\n        return self.exclude_dir * other\n\n    def exclude_neumann(self, other):\n        \"\"\" Mapping to exclude faces with Neumann boundary conditions from\n        local systems.\n\n        Parameters:\n            other (scipy.sparse matrix): Matrix of local equations for\n                continuity of flux and pressure.\n\n        Returns:\n            scipy.sparse matrix, with rows corresponding to faces with\n                Neumann conditions eliminated.\n\n        \"\"\"\n        return self.exclude_neu * other\n\n    def exclude_dirichlet_nd(self, other):\n        \"\"\" Exclusion of Dirichlet conditions for vector equations (elasticity).\n        See above method without _nd suffix for description.\n\n        \"\"\"\n        exclude_dirichlet_nd = sps.kron(sps.eye(self.nd),\n                                        self.exclude_dir)\n        return exclude_dirichlet_nd * other\n\n    def exclude_neumann_nd(self, other):\n        \"\"\" Exclusion of Neumann conditions for vector equations (elasticity).\n        See above method without _nd suffix for description.\n\n        \"\"\"\n        exclude_neumann_nd = sps.kron(sps.eye(self.nd), self.exclude_neu)\n        return exclude_neumann_nd * other",
  "def cell_ind_for_partial_update(g, cells=None, faces=None, nodes=None):\n    \"\"\" Obtain indices of cells and faces needed for a partial update of the\n    discretization stencil.\n\n    Implementation note: This function should really be split into three parts,\n    one for each of the modes (cell, face, node).\n\n    The subgrid can be specified in terms of cells, faces and nodes to be\n    updated. The method will then define a sufficiently large subgrid to\n    account for changes in the flux discretization. The idea is that cells are\n    used to account for updates in material parameters (or geometry), faces\n    when faces are split (fracture growth), while the parameter nodes is mainly\n    aimed at a gradual build of the discretization of the entire grid (for\n    memory conservation, see comments in mpfa.mpfa()). For more details, see\n    the implementations and comments below.\n\n    Cautionary note: The option to combine cells, faces and nodes in one go has\n    not been tested. Problems may arise for grid configurations where separate\n    regions are close to touching. This is however speculation at the time of\n    writing.\n\n    Parameters:\n        g (core.grids.grid): grid to be discretized\n        cells (np.array, int, optional): Index of cells on which to base the\n            subgrid computation. Defaults to None.\n        faces (np.array, int, optional): Index of faces on which to base the\n            subgrid computation. Defaults to None.\n        nodes (np.array, int, optional): Index of faces on which to base the\n            subgrid computation. Defaults to None.\n\n    Returns:\n        np.array, int: Cell indexes of the subgrid. No guarantee that they form\n            a connected grid.\n        np.array, int: Indexes of faces to have their discretization updated.\n\n    \"\"\"\n\n    # Faces that are active, and should have their discretization stencil\n    # updated / returned.\n    active_faces = np.zeros(g.num_faces, dtype=np.bool)\n\n    # Index of cells to include in the subgrid.\n    cell_ind = np.empty(0)\n\n    if cells is not None:\n        # To understand the update stencil for a cell-based update, consider\n        # the Cartesian 2d configuration below.\n        #\n        #    _ s s s _\n        #    s o o o s\n        #    s o x o s\n        #    s o o o s\n        #    - s s s -\n        #\n        # The central cell (x) is to be updated. The support of MPFA basis\n        # functions dictates that the stencil between the central cell and its\n        # primary neighbors (o) must be updated, as must the stencil for the\n        # sub-faces between o-cells that shares a vertex with x. Since the\n        # flux information is stored face-wise (not sub-face), the whole o-o\n        # faces must be recomputed, and this involves the secondary neighbors\n        # of x (denoted s). This is most easily realized by defining an overlap\n        # of 2. This will also involve some cells and nodes not needed;\n        # specifically those marked by -. This requires quite a song and dance,\n        # see below; but most of this is necessary to get hold of the active\n        # faces anyhow.\n        #\n        # Note that a simpler option, with a somewhat higher computational cost,\n        # would be to define\n        #   cell_overlap = partition.overlap(g, cells, num_layers=2)\n        # This would however include more cells (all marked - in the\n        # illustration, and potentially significantly many more in 3d, in\n        # particular for unstructured grids).\n\n        cn = g.cell_nodes()\n\n        # The active faces (to be updated; (o-x and o-o above) are those that\n        # share at least one vertex with cells in ind.\n        prim_cells = np.zeros(g.num_cells, dtype=np.bool)\n        prim_cells[cells] = 1\n        # Vertexes of the cells\n        active_vertexes = np.zeros(g.num_nodes, dtype=np.bool)\n        active_vertexes[np.squeeze(np.where(cn * prim_cells > 0))] = 1\n\n        # Faces of the vertexes, these will be the active faces.\n        active_face_ind = np.squeeze(np.where(g.face_nodes.transpose()\n                                              * active_vertexes > 0))\n        active_faces[active_face_ind] = 1\n\n        # Secondary vertexes, involved in at least one of the active faces,\n        # that is, the faces to be updated. Corresponds to vertexes between o-o\n        # above.\n        active_vertexes[np.squeeze(np.where(g.face_nodes\n                                            * active_faces > 0))] = 1\n\n        # Finally, get hold of all cells that shares one of the secondary\n        # vertexes.\n        cells_overlap = np.squeeze(np.where((cn.transpose()\n                                             * active_vertexes) > 0))\n        # And we have our overlap!\n        cell_ind = np.hstack((cell_ind, cells_overlap))\n\n    if faces is not None:\n        # The faces argument is intended used when the configuration of the\n        # specified faces has changed, e.g. due to the introduction of an\n        # external boundary. This requires the recomputation of all faces that\n        # share nodes with the specified faces. Since data is not stored on\n        # sub-faces. This further requires the inclusion of all cells that\n        # share a node with a secondary face.\n        #\n        #      o o o\n        #    o o x o o\n        #    o o x o o\n        #      o o o\n        #\n        # To illustrate for the Cartesian configuration above: The face\n        # between the two x-cells are specified, and this requires the\n        # inclusion of all o-cells.\n        #\n\n        cf = g.cell_faces\n        # This avoids overwriting data in cell_faces.\n        data = np.ones_like(cf.data)\n        cf = sps.csc_matrix((data, cf.indices, cf.indptr))\n\n        primary_faces = np.zeros(g.num_faces, dtype=np.bool)\n        primary_faces[faces] = 1\n\n        # The active faces are those sharing a vertex with the primary faces\n        primary_vertex = np.zeros(g.num_nodes, dtype=np.bool)\n        primary_vertex[np.squeeze(np.where((g.face_nodes\n                                            * primary_faces) > 0))] = 1\n        active_face_ind = np.squeeze(np.where((g.face_nodes.transpose()\n                                               * primary_vertex) > 0))\n        active_faces[active_face_ind] = 1\n\n        # Find vertexes of the active faces\n        active_nodes = np.zeros(g.num_nodes, dtype=np.bool)\n        active_nodes[np.squeeze(np.where((g.face_nodes\n                                          * active_faces) > 0))] = 1\n\n        active_cells = np.zeros(g.num_cells, dtype=np.bool)\n        # Primary cells, those that have the faces as a boundary\n        cells_overlap = np.squeeze(np.where((g.cell_nodes().transpose()\n                                             * active_nodes) > 0))\n        cell_ind = np.hstack((cell_ind, cells_overlap))\n\n    if nodes is not None:\n        # Pick out all cells that have the specified nodes as a vertex.\n        # The active faces will be those that have all their vertexes included\n        # in nodes.\n        cn = g.cell_nodes()\n        # Introduce active nodes, and make the input nodes active\n        # The data type of active_vertex is int (in contrast to similar cases\n        # in other parts of this function), since we will use it to count the\n        # number of active face_nodes below.\n        active_vertexes = np.zeros(g.num_nodes, dtype=np.int)\n        active_vertexes[nodes] = 1\n\n        # Find cells that share these nodes\n        active_cells = np.squeeze(np.where((cn.transpose()\n                                            * active_vertexes) > 0))\n        # Append the newly found active cells\n        cell_ind = np.hstack((cell_ind, active_cells))\n\n        # Multiply face_nodes.transpose() (e.g. node-faces) with the active\n        # vertexes to get the number of active nodes perm face\n        num_active_face_nodes = np.array(g.face_nodes.transpose()\n                                         * active_vertexes)\n        # Total number of nodes per face\n        num_face_nodes = np.array(g.face_nodes.sum(axis=0))\n        # Active faces are those where all nodes are active.\n        active_face_ind = np.squeeze(np.argwhere((num_active_face_nodes ==\n                                                  num_face_nodes).ravel('F')))\n        active_faces[active_face_ind] = 1\n\n    face_ind = np.squeeze(np.where(active_faces))\n\n    # Do a sort of the indexes to be returned.\n    cell_ind.sort()\n    face_ind.sort()\n    # Return, with data type int\n    return cell_ind.astype('int'), face_ind.astype('int')",
  "def map_subgrid_to_grid(g, loc_faces, loc_cells, is_vector):\n\n    num_faces_loc = loc_faces.size\n    num_cells_loc = loc_cells.size\n\n    nd = g.dim\n    if is_vector:\n        face_map = sps.csr_matrix((np.ones(num_faces_loc * nd),\n                                   (expand_indices_nd(loc_faces, nd),\n                                    np.arange(num_faces_loc * nd))),\n                                  shape=(g.num_faces * nd,\n                                         num_faces_loc * nd))\n\n        cell_map = sps.csr_matrix((np.ones(num_cells_loc * nd),\n                                   (np.arange(num_cells_loc * nd),\n                                    expand_indices_nd(loc_cells, nd))),\n                                  shape=(num_cells_loc * nd,\n                                         g.num_cells * nd))\n    else:\n        face_map = sps.csr_matrix((np.ones(num_faces_loc),\n                                   (loc_faces, np.arange(num_faces_loc))),\n                                  shape=(g.num_faces, num_faces_loc))\n        cell_map = sps.csr_matrix((np.ones(num_cells_loc),\n                                   (np.arange(num_cells_loc), loc_cells)),\n                                  shape=(num_cells_loc, g.num_cells))\n    return face_map, cell_map",
  "def compute_discharges(gb, physics='flow', d_name='discharge',\n                       p_name='pressure', data=None):\n    \"\"\"\n    Computes discharges over all faces in the entire grid /grid bucket given\n    pressures for all nodes, provided as node properties.\n\n    Parameter:\n    gb: grid bucket with the following data fields for all nodes/grids:\n        'flux': Internal discretization of fluxes.\n        'bound_flux': Discretization of boundary fluxes.\n        'pressure': Pressure values for each cell of the grid (overwritten by p_name).\n        'bc_val': Boundary condition values.\n            and the following edge property field for all connected grids:\n        'coupling_flux': Discretization of the coupling fluxes.\n    physics (string): defaults to 'flow'. The physic regime\n    d_name (string): defaults to 'discharge'. The keyword which the computed\n                     discharge will be stored by in the dictionary.\n    p_name (string): defaults to 'pressure'. The keyword that the pressure\n                     field is stored by in the dictionary\n    data (dictionary): defaults to None. If gb is mono-dimensional grid the\n                       data dictionary must be given. If gb is a\n                       multi-dimensional grid, this variable has no effect\n                \n    Returns:\n        gb, the same grid bucket with the added field 'discharge'(overwritten\n        by d_name) added to all node data fields. Note that the fluxes between\n        grids will be added only at the gb edge, not at the node fields. The\n        sign of the discharges correspond to the directions of the normals, in\n        the edge/coupling case those of the higher grid. For edges beteween\n        grids of equal dimension, there is an implicit assumption that all\n        normals point from the second to the first of the sorted grids\n        (gb.sorted_nodes_of_edge(e)).\n    \"\"\"\n    if not isinstance(gb, GridBucket):\n        pa = data['param']\n        if data.get('flux') is not None:\n            dis = data['flux'] * data[p_name] + data['bound_flux'] \\\n                               * pa.get_bc_val(physics)\n        else:\n            dis = np.zeros(g.num_faces)\n        data[d_name] = dis\n        return\n\n    for g, d in gb:\n        if g.dim > 0:\n            pa = d['param']\n            if d.get('flux') is not None:\n                dis = d['flux'] * d[p_name] + d['bound_flux'] \\\n                    * pa.get_bc_val(physics)\n            else:\n                dis = np.zeros(g.num_faces)\n            d[d_name] = dis\n\n    for e, d in gb.edges_props():\n        # According to the sorting convention, g2 is the higher dimensional grid,\n        # the one to who's faces the fluxes correspond\n        g1, g2 = gb.sorted_nodes_of_edge(e)\n        try:\n            pa = d['param']\n        except KeyError:\n            pa = Parameters(g2)\n            d['param'] = pa\n\n        if g1.dim != g2.dim and d['face_cells'] is not None:\n            coupling_flux = gb.edge_prop(e, 'coupling_flux')[0]\n            pressures = gb.nodes_prop([g2, g1], p_name)\n            dis = coupling_flux * np.concatenate(pressures)\n            d[d_name] = dis\n\n        elif g1.dim == g2.dim and d['face_cells'] is not None:\n            # g2 is now only the \"higher\", but still the one defining the faces\n            # (cell-cells connections) in the sense that the normals are assumed\n            # outward from g2, \"pointing towards the g1 cells\". Note that in\n            # general, there are g2.num_cells x g1.num_cells connections/\"faces\".\n            cc = d['face_cells']\n            cells_1, cells_2 = cc.nonzero()\n            coupling_flux = gb.edge_prop(e, 'coupling_flux')[0]\n\n            pressures = gb.nodes_prop([g2, g1], p_name)\n            p2 = pressures[0][cells_2]\n            p1 = pressures[1][cells_1]\n            contribution_2 = np.multiply(coupling_flux[cc], p2)\n            contribution_1 = np.multiply(coupling_flux[cc], p1)\n            dis = contribution_2 - contribution_1\n            # Store flux at the edge only. This means that the flux will remain\n            # zero in the data of both g1 and g2\n            d[d_name] = np.ravel(dis)",
  "def __init__(self, g):\n        \"\"\"\n        Constructor for subcell topology\n\n        Parameters\n        ----------\n        g grid\n        \"\"\"\n        self.g = g\n\n        # Indices of neighboring faces and cells. The indices are sorted to\n        # simplify later treatment\n        g.cell_faces.sort_indices()\n        face_ind, cell_ind = g.cell_faces.nonzero()\n\n        # Number of faces per node\n        num_face_nodes = np.diff(g.face_nodes.indptr)\n\n        # Duplicate cell and face indices, so that they can be matched with\n        # the nodes\n        cells_duplicated = matrix_compression.rldecode(\n            cell_ind, num_face_nodes[face_ind])\n        faces_duplicated = matrix_compression.rldecode(\n            face_ind, num_face_nodes[face_ind])\n        M = sps.coo_matrix((np.ones(face_ind.size),\n                            (face_ind, np.arange(face_ind.size))),\n                           shape=(face_ind.max() + 1, face_ind.size))\n        nodes_duplicated = g.face_nodes * M\n        nodes_duplicated = nodes_duplicated.indices\n\n        face_nodes_indptr = g.face_nodes.indptr\n        face_nodes_indices = g.face_nodes.indices\n        face_nodes_data = np.arange(face_nodes_indices.size) + 1\n        sub_face_mat = sps.csc_matrix((face_nodes_data, face_nodes_indices,\n                                       face_nodes_indptr))\n        sub_faces = sub_face_mat * M\n        sub_faces = sub_faces.data - 1\n\n        # Sort data\n        idx = np.lexsort((sub_faces, faces_duplicated, nodes_duplicated,\n                          cells_duplicated))\n        self.nno = nodes_duplicated[idx]\n        self.cno = cells_duplicated[idx]\n        self.fno = faces_duplicated[idx]\n        self.subfno = sub_faces[idx].astype(int)\n        self.subhfno = np.arange(idx.size, dtype='>i4')\n        self.num_subfno = self.subfno.max() + 1\n        self.num_cno = self.cno.max() + 1\n\n        # Make subface indices unique, that is, pair the indices from the two\n        # adjacent cells\n        _, unique_subfno = np.unique(self.subfno, return_index=True)\n\n        # Reduce topology to one field per subface\n        self.nno_unique = self.nno[unique_subfno]\n        self.fno_unique = self.fno[unique_subfno]\n        self.cno_unique = self.cno[unique_subfno]\n        self.subfno_unique = self.subfno[unique_subfno]\n        self.num_subfno_unique = self.subfno_unique.max() + 1\n        self.unique_subfno = unique_subfno",
  "def __repr__(self):\n        s = 'Subcell topology with:\\n'\n        s += str(self.num_cno) + ' cells\\n'\n        s += str(self.g.num_nodes) + ' nodes\\n'\n        s += str(self.g.num_faces) + ' faces\\n'\n        s += str(self.num_subfno_unique) + ' unique subfaces\\n'\n        s += str(self.fno.size) + ' subfaces before pairing face neighbors\\n'\n        return s",
  "def pair_over_subfaces(self, other):\n        \"\"\"\n        Transfer quantities from a cell-face base (cells sharing a face have\n        their own expressions) to a face-based. The quantities should live\n        on sub-faces (not faces)\n\n        The normal vector is honored, so that the here and there side get\n        different signs when paired up.\n\n        The method is intended used for combining forces, fluxes,\n        displacements and pressures, as used in MPSA / MPFA.\n\n        Parameters\n        ----------\n        other: sps.matrix, size (self.subhfno.size x something)\n\n        Returns\n        -------\n        sps.matrix, size (self.subfno_unique.size x something)\n        \"\"\"\n\n        sgn = self.g.cell_faces[self.fno, self.cno].A\n        pair_over_subfaces = sps.coo_matrix((sgn[0], (self.subfno,\n                                                      self.subhfno)))\n        return pair_over_subfaces * other",
  "def pair_over_subfaces_nd(self, other):\n        \"\"\" nd-version of pair_over_subfaces, see above. \"\"\"\n        nd = self.g.dim\n        # For force balance, displacements and stresses on the two sides of the\n        # matrices must be paired\n        # Operator to create the pairing\n        sgn = self.g.cell_faces[self.fno, self.cno].A\n        pair_over_subfaces = sps.coo_matrix((sgn[0], (self.subfno,\n                                                      self.subhfno)))\n        # vector version, to be used on stresses\n        pair_over_subfaces_nd = sps.kron(sps.eye(nd), pair_over_subfaces)\n        return pair_over_subfaces_nd * other",
  "def invert_diagonal_blocks_python(a, sz):\n        \"\"\"\n        Invert block diagonal matrix using pure python code.\n\n        The implementation is slow for large matrices, consider to use the\n        numba-accelerated method invert_invert_diagagonal_blocks_numba instead\n\n        Parameters\n        ----------\n        A sps.crs-matrix, to be inverted\n        sz - size of the individual blocks\n\n        Returns\n        -------\n        inv_a inverse matrix\n        \"\"\"\n        v = np.zeros(np.sum(np.square(sz)))\n        p1 = 0\n        p2 = 0\n        for b in range(sz.size):\n            n = sz[b]\n            n2 = n * n\n            i = p1 + np.arange(n + 1)\n            # Picking out the sub-matrices here takes a lot of time.\n            v[p2 + np.arange(n2)] = np.linalg.inv(a[i[0]:i[-1],\n                                                    i[0]:i[-1]].A).ravel()\n            p1 = p1 + n\n            p2 = p2 + n2\n        return v",
  "def invert_diagonal_blocks_cython(a, size):\n        \"\"\" Invert block diagonal matrix using code wrapped with cython.\n        \"\"\"\n        try:\n            import porepy.numerics.fv.cythoninvert as cythoninvert\n        except:\n            ImportError('Compiled Cython module not available. Is cython\\\n            installed?')\n\n        a.sorted_indices()\n        ptr = a.indptr\n        indices = a.indices\n        dat = a.data\n\n        v = cythoninvert.inv_python(ptr, indices, dat, size)\n        return v",
  "def invert_diagonal_blocks_numba(a, size):\n        \"\"\"\n        Invert block diagonal matrix by invoking numba acceleration of a simple\n        for-loop based algorithm.\n\n        This approach should be more efficient than the related method\n        invert_diagonal_blocks_python for larger problems.\n\n        Parameters\n        ----------\n        a : sps.csr matrix\n        size : Size of individual blocks\n\n        Returns\n        -------\n        ia: inverse of a\n        \"\"\"\n        try:\n            import numba\n        except:\n            raise ImportError('Numba not available on the system')\n\n        # Sort matrix storage before pulling indices and data\n        a.sorted_indices()\n        ptr = a.indptr\n        indices = a.indices\n        dat = a.data\n\n        # Just in time compilation\n        @numba.jit(\"f8[:](i4[:],i4[:],f8[:],i8[:])\", nopython=True,\n                   nogil=False)\n        def inv_python(indptr, ind, data, sz):\n            \"\"\"\n            Invert block matrices by explicitly forming local matrices. The code\n            in itself is not efficient, but it is hopefully well suited for\n            speeding up with numba.\n\n            It may be possible to restruct the code to further help numba,\n            this has not been investigated.\n\n            The computation can easily be parallelized, consider this later.\n            \"\"\"\n\n            # Index of where the rows start for each block.\n            # block_row_starts_ind = np.hstack((np.array([0]),\n            #                                   np.cumsum(sz[:-1])))\n            block_row_starts_ind = np.zeros(sz.size, dtype=np.int32)\n            block_row_starts_ind[1:] = np.cumsum(sz[:-1])\n\n            # Number of columns per row. Will change from one column to the\n            # next\n            num_cols_per_row = indptr[1:] - indptr[0:-1]\n            # Index to where the columns start for each row (NOT blocks)\n            # row_cols_start_ind = np.hstack((np.zeros(1),\n            #                                 np.cumsum(num_cols_per_row)))\n            row_cols_start_ind = np.zeros(num_cols_per_row.size + 1,\n                                          dtype=np.int32)\n            row_cols_start_ind[1:] = np.cumsum(num_cols_per_row)\n\n            # Index to where the (full) data starts. Needed, since the\n            # inverse matrix will generally be full\n            # full_block_starts_ind = np.hstack((np.array([0]),\n            #                                    np.cumsum(np.square(sz))))\n            full_block_starts_ind = np.zeros(sz.size + 1, dtype=np.int32)\n            full_block_starts_ind[1:] = np.cumsum(np.square(sz))\n            # Structure to store the solution\n            inv_vals = np.zeros(np.sum(np.square(sz)))\n\n            # Loop over all blocks\n            for iter1 in range(sz.size):\n                n = sz[iter1]\n                loc_mat = np.zeros((n, n))\n                # Fill in non-zero elements in local matrix\n                for iter2 in range(n):  # Local rows\n                    global_row = block_row_starts_ind[iter1] + iter2\n                    data_counter = row_cols_start_ind[global_row]\n\n                    # Loop over local columns. Getting the number of columns\n                    #  for each row is a bit involved\n                    for _ in range(num_cols_per_row[iter2 +\n                                                    block_row_starts_ind[iter1]]):\n                        loc_col = ind[data_counter] \\\n                            - block_row_starts_ind[iter1]\n                        loc_mat[iter2, loc_col] = data[data_counter]\n                        data_counter += 1\n\n                # Compute inverse. np.linalg.inv is supported by numba (May\n                # 2016), it is not clear if this is the best option. To be\n                # revised\n                inv_mat = np.ravel(np.linalg.inv(loc_mat))\n\n                loc_ind = np.arange(full_block_starts_ind[iter1],\n                                    full_block_starts_ind[iter1 + 1])\n                inv_vals[loc_ind] = inv_mat\n                # Update fields\n            return inv_vals\n\n        v = inv_python(ptr, indices, dat, size)\n        return v",
  "def __init__(self, subcell_topology, bound, nd):\n        \"\"\"\n        Define mappings to exclude boundary faces with dirichlet and neumann\n        conditions\n\n        Parameters\n        ----------\n        subcell_topology\n        bound\n\n        Returns\n        -------\n        exclude_neumann: Matrix, mapping from all faces to those having flux\n                         continuity\n        exclude_dirichlet: Matrix, mapping from all faces to those having pressure\n                           continuity\n        \"\"\"\n        self.nd = nd\n\n        # Short hand notation\n        fno = subcell_topology.fno_unique\n        num_subfno = subcell_topology.num_subfno_unique\n\n        # Define mappings to exclude boundary values\n        col_neu = np.argwhere([not it for it in bound.is_neu[fno]])\n        row_neu = np.arange(col_neu.size)\n        self.exclude_neu = sps.coo_matrix((np.ones(row_neu.size),\n                                           (row_neu, col_neu.ravel('C'))),\n                                          shape=(row_neu.size,\n                                                 num_subfno)).tocsr()\n        col_dir = np.argwhere([not it for it in bound.is_dir[fno]])\n        row_dir = np.arange(col_dir.size)\n        self.exclude_dir = sps.coo_matrix((np.ones(row_dir.size),\n                                           (row_dir, col_dir.ravel('C'))),\n                                          shape=(row_dir.size,\n                                                 num_subfno)).tocsr()",
  "def exclude_dirichlet(self, other):\n        \"\"\" Mapping to exclude faces with Dirichlet boundary conditions from\n        local systems.\n\n        Parameters:\n            other (scipy.sparse matrix): Matrix of local equations for\n                continuity of flux and pressure.\n\n        Returns:\n            scipy.sparse matrix, with rows corresponding to faces with\n                Dirichlet conditions eliminated.\n\n        \"\"\"\n        return self.exclude_dir * other",
  "def exclude_neumann(self, other):\n        \"\"\" Mapping to exclude faces with Neumann boundary conditions from\n        local systems.\n\n        Parameters:\n            other (scipy.sparse matrix): Matrix of local equations for\n                continuity of flux and pressure.\n\n        Returns:\n            scipy.sparse matrix, with rows corresponding to faces with\n                Neumann conditions eliminated.\n\n        \"\"\"\n        return self.exclude_neu * other",
  "def exclude_dirichlet_nd(self, other):\n        \"\"\" Exclusion of Dirichlet conditions for vector equations (elasticity).\n        See above method without _nd suffix for description.\n\n        \"\"\"\n        exclude_dirichlet_nd = sps.kron(sps.eye(self.nd),\n                                        self.exclude_dir)\n        return exclude_dirichlet_nd * other",
  "def exclude_neumann_nd(self, other):\n        \"\"\" Exclusion of Neumann conditions for vector equations (elasticity).\n        See above method without _nd suffix for description.\n\n        \"\"\"\n        exclude_neumann_nd = sps.kron(sps.eye(self.nd), self.exclude_neu)\n        return exclude_neumann_nd * other",
  "def inv_python(indptr, ind, data, sz):\n            \"\"\"\n            Invert block matrices by explicitly forming local matrices. The code\n            in itself is not efficient, but it is hopefully well suited for\n            speeding up with numba.\n\n            It may be possible to restruct the code to further help numba,\n            this has not been investigated.\n\n            The computation can easily be parallelized, consider this later.\n            \"\"\"\n\n            # Index of where the rows start for each block.\n            # block_row_starts_ind = np.hstack((np.array([0]),\n            #                                   np.cumsum(sz[:-1])))\n            block_row_starts_ind = np.zeros(sz.size, dtype=np.int32)\n            block_row_starts_ind[1:] = np.cumsum(sz[:-1])\n\n            # Number of columns per row. Will change from one column to the\n            # next\n            num_cols_per_row = indptr[1:] - indptr[0:-1]\n            # Index to where the columns start for each row (NOT blocks)\n            # row_cols_start_ind = np.hstack((np.zeros(1),\n            #                                 np.cumsum(num_cols_per_row)))\n            row_cols_start_ind = np.zeros(num_cols_per_row.size + 1,\n                                          dtype=np.int32)\n            row_cols_start_ind[1:] = np.cumsum(num_cols_per_row)\n\n            # Index to where the (full) data starts. Needed, since the\n            # inverse matrix will generally be full\n            # full_block_starts_ind = np.hstack((np.array([0]),\n            #                                    np.cumsum(np.square(sz))))\n            full_block_starts_ind = np.zeros(sz.size + 1, dtype=np.int32)\n            full_block_starts_ind[1:] = np.cumsum(np.square(sz))\n            # Structure to store the solution\n            inv_vals = np.zeros(np.sum(np.square(sz)))\n\n            # Loop over all blocks\n            for iter1 in range(sz.size):\n                n = sz[iter1]\n                loc_mat = np.zeros((n, n))\n                # Fill in non-zero elements in local matrix\n                for iter2 in range(n):  # Local rows\n                    global_row = block_row_starts_ind[iter1] + iter2\n                    data_counter = row_cols_start_ind[global_row]\n\n                    # Loop over local columns. Getting the number of columns\n                    #  for each row is a bit involved\n                    for _ in range(num_cols_per_row[iter2 +\n                                                    block_row_starts_ind[iter1]]):\n                        loc_col = ind[data_counter] \\\n                            - block_row_starts_ind[iter1]\n                        loc_mat[iter2, loc_col] = data[data_counter]\n                        data_counter += 1\n\n                # Compute inverse. np.linalg.inv is supported by numba (May\n                # 2016), it is not clear if this is the best option. To be\n                # revised\n                inv_mat = np.ravel(np.linalg.inv(loc_mat))\n\n                loc_ind = np.arange(full_block_starts_ind[iter1],\n                                    full_block_starts_ind[iter1 + 1])\n                inv_vals[loc_ind] = inv_mat\n                # Update fields\n            return inv_vals",
  "def compute_tof(g, flux, poro, q):\n    \"\"\"\n    Compute time of flight using a upstream weighted finite volume method.\n\n    The function is a translation of the corresponding MRST function, albeit\n    without the more advanced options.\n\n    Parameters:\n        g (core.grids.grid): Grid structure.\n        flux (np.array, size num_faces): Flow field used in the computation.\n        poro (np.array, size num_cells): Cell-wise porosity\n        q (np.array, size num_cells): Combined source terms and flux\n        contribution from boundary conditions.\n\n    Returns:\n        np.array, size num_cells: Cell-wise time of flight\n\n    \"\"\"\n\n    # Get neighbors on a dense form (array of two rows)\n    neighs = g.cell_face_as_dense()\n    # We're only interested in internal faces, boundaries are hanled below\n    is_int = np.all(neighs >=0, axis=0)\n    int_neigh = neighs[:, is_int]\n\n    # Outflow fluxes are non-positive\n    out_flow = np.minimum(flux[is_int], 0)\n    # Inflow fluxes are non-negative\n    in_flow = np.maximum(flux[is_int], 0)\n\n    # Find accumulation in each cell.\n    # A positive flow is from neigh[0] to neigh[1], so in_flow will\n    # add to neigh[1]. Conversely, the accumulation in neigh[0] is\n    # the negative of outflow\n    accum = np.bincount(np.hstack((int_neigh[1], int_neigh[0])),\n                                            weights=np.hstack((in_flow,\n                                                               -out_flow)))\n\n    # To consider flow from sources/boundaries to cells, we only need to\n    # consider positive sources\n    np.clip(q, 0, np.inf, out=q)\n\n    # The average TOF for cells with sources are set to twice the time it takes\n    # to fill the cell. Achieve this by adding twice the sources.\n    sources = accum + 2 * q\n\n    nc = g.num_cells\n\n    # Upstream weighting of fluxes taken out of cells\n    A = sps.coo_matrix((-in_flow, (int_neigh[1], int_neigh[0])), shape=(nc,\n                                                                        nc)) \\\n                + sps.coo_matrix((out_flow, (int_neigh[0], int_neigh[1])),\n                                 shape=(nc, nc))\n\n    # Add accumulation in each cell\n    A += sps.dia_matrix((sources, 0), shape=(nc, nc))\n\n    # Pore volume equals porosity times cell volume\n    pv = g.cell_volumes * poro\n\n    tof = spsolve(A, pv)\n    return tof",
  "class Integral(Solver):\n    '''\n    Discretization of the integrated source term\n    int q * dx\n    over each grid cell.\n\n    All this function does is returning a zero lhs and\n    rhs = param.get_source.physics.\n    '''\n\n    def __init__(self, physics='flow'):\n        self.physics = physics\n        Solver.__init__(self)\n\n    def ndof(self, g):\n        if self.physics is 'flow':\n            return g.num_cells\n\n        if self.physics is 'transport':\n            return g.num_cells\n\n        if self.physics is 'mechanics':\n            return g.num_cells * g.dim\n\n    def matrix_rhs(self, g, data):\n        param = data['param']\n        sources = param.get_source(self)\n        lhs = sps.csc_matrix((self.ndof(g), self.ndof(g)))\n        assert sources.size == self.ndof(g), \\\n                                 'There should be one soure value for each cell'\n        return lhs, sources",
  "class IntegralMixedDim(SolverMixedDim):\n    def __init__(self, physics='flow'):\n        self.physics = physics\n\n        self.discr = Integral(self.physics)\n        self.discr_ndof = self.discr.ndof\n        self.coupling_conditions = None\n\n        self.solver = Coupler(self.discr)\n        SolverMixedDim.__init__(self)",
  "class IntegralDFN(SolverMixedDim):\n    def __init__(self, dim_max, physics='flow'):\n        # NOTE: There is no flow along the intersections of the fractures.\n\n        self.physics = physics\n        self.dim_max = dim_max\n\n        self.discr = Integral(self.physics)\n        self.coupling_conditions = None\n\n        kwargs = {\"discr_ndof\": self.discr.ndof,\n                  \"discr_fct\": self.__matrix_rhs__}\n        self.solver = Coupler(coupling = None, **kwargs)\n        SolverMixedDim.__init__(self)\n\n    def __matrix_rhs__(self, g, data):\n        # The highest dimensional problem compute the matrix and rhs, the lower\n        # dimensional problem and empty matrix. For the latter, the size of the\n        # matrix is the number of cells.\n        if g.dim == self.dim_max:\n            return self.discr.matrix_rhs(g, data)\n        else:\n            ndof = self.discr.ndof(g)\n            return sps.csr_matrix((ndof, ndof)), np.zeros(ndof)",
  "def __init__(self, physics='flow'):\n        self.physics = physics\n        Solver.__init__(self)",
  "def ndof(self, g):\n        if self.physics is 'flow':\n            return g.num_cells\n\n        if self.physics is 'transport':\n            return g.num_cells\n\n        if self.physics is 'mechanics':\n            return g.num_cells * g.dim",
  "def matrix_rhs(self, g, data):\n        param = data['param']\n        sources = param.get_source(self)\n        lhs = sps.csc_matrix((self.ndof(g), self.ndof(g)))\n        assert sources.size == self.ndof(g), \\\n                                 'There should be one soure value for each cell'\n        return lhs, sources",
  "def __init__(self, physics='flow'):\n        self.physics = physics\n\n        self.discr = Integral(self.physics)\n        self.discr_ndof = self.discr.ndof\n        self.coupling_conditions = None\n\n        self.solver = Coupler(self.discr)\n        SolverMixedDim.__init__(self)",
  "def __init__(self, dim_max, physics='flow'):\n        # NOTE: There is no flow along the intersections of the fractures.\n\n        self.physics = physics\n        self.dim_max = dim_max\n\n        self.discr = Integral(self.physics)\n        self.coupling_conditions = None\n\n        kwargs = {\"discr_ndof\": self.discr.ndof,\n                  \"discr_fct\": self.__matrix_rhs__}\n        self.solver = Coupler(coupling = None, **kwargs)\n        SolverMixedDim.__init__(self)",
  "def __matrix_rhs__(self, g, data):\n        # The highest dimensional problem compute the matrix and rhs, the lower\n        # dimensional problem and empty matrix. For the latter, the size of the\n        # matrix is the number of cells.\n        if g.dim == self.dim_max:\n            return self.discr.matrix_rhs(g, data)\n        else:\n            ndof = self.discr.ndof(g)\n            return sps.csr_matrix((ndof, ndof)), np.zeros(ndof)",
  "class TpfaMixedDim(SolverMixedDim):\n    def __init__(self, physics='flow'):\n        self.physics = physics\n\n        self.discr = Tpfa(self.physics)\n        self.discr_ndof = self.discr.ndof\n        self.coupling_conditions = TpfaCoupling(self.discr)\n\n        self.solver = Coupler(self.discr, self.coupling_conditions)",
  "class TpfaDFN(SolverMixedDim):\n\n    def __init__(self, dim_max, physics='flow'):\n        # NOTE: There is no flow along the intersections of the fractures.\n\n        self.physics = physics\n        self.dim_max = dim_max\n\n        self.discr = Tpfa(self.physics)\n        self.coupling_conditions = TpfaCouplingDFN(self.discr)\n\n        kwargs = {\"discr_ndof\": self.discr.ndof,\n                  \"discr_fct\": self.__matrix_rhs__}\n        self.solver = Coupler(coupling=self.coupling_conditions, **kwargs)\n        SolverMixedDim.__init__(self)\n\n    def __matrix_rhs__(self, g, data):\n        # The highest dimensional problem compute the matrix and rhs, the lower\n        # dimensional problem and empty matrix. For the latter, the size of the\n        # matrix is the number of cells.\n        if g.dim == self.dim_max:\n            return self.discr.matrix_rhs(g, data)\n        else:\n            ndof = self.discr.ndof(g)\n            return sps.csr_matrix((ndof, ndof)), np.zeros(ndof)",
  "class Tpfa(Solver):\n    \"\"\" Discretize elliptic equations by a two-point flux approximation.\n\n    Attributes:\n\n    physics : str\n        Which physics is the solver intended flow. Will determine which data\n        will be accessed (e.g. flow specific, or conductivity / heat-related).\n        See Data class for more details. Defaults to flow.\n\n    \"\"\"\n\n    def __init__(self, physics='flow'):\n        self.physics = physics\n\n    def ndof(self, g):\n        \"\"\"\n        Return the number of degrees of freedom associated to the method.\n        In this case number of cells (pressure dof).\n\n        Parameter\n        ---------\n        g: grid, or a subclass.\n\n        Return\n        ------\n        dof: the number of degrees of freedom.\n\n        \"\"\"\n        return g.num_cells\n\n#------------------------------------------------------------------------------#\n\n    def matrix_rhs(self, g, data, faces=None, discretize=True):\n        \"\"\"\n        Return the matrix and right-hand side for a discretization of a second\n        order elliptic equation using a FV method with a two-point flux approximation.\n\n        To set a source see the source.Integral discretization class\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data. For details on necessary keywords,\n            see method discretize()\n        discretize (boolean, optional): Whether to discetize prior to matrix\n            assembly. If False, data should already contain discretization.\n            Defaults to True.\n\n        Return\n        ------\n        matrix: sparse csr (g_num_cells, g_num_cells)\n            Discretization matrix.\n        rhs: array (g_num_cells)\n            Right-hand side which contains the boundary conditions and the scalar\n            source term.\n\n        \"\"\"\n        div = fvutils.scalar_divergence(g)\n        if discretize:\n            self.discretize(g, data)\n        flux = data['flux']\n        M = div * flux\n\n        bound_flux = data['bound_flux']\n        param = data['param']\n        bc_val = param.get_bc_val(self)\n\n        return M, self.rhs(g, bound_flux, bc_val)\n\n#------------------------------------------------------------------------------#\n\n    def rhs(self, g, bound_flux, bc_val):\n        \"\"\"\n        Return the righ-hand side for a discretization of a second order elliptic\n        equation using the TPFA method. See self.matrix_rhs for a detaild\n        description.\n        \"\"\"\n        div = g.cell_faces.T\n        return -div * bound_flux * bc_val\n\n#------------------------------------------------------------------------------#\n\n    def discretize(self, g, data, faces=None):\n        \"\"\"\n        Discretize the second order elliptic equation using two-point flux\n\n        The method computes fluxes over faces in terms of pressures in adjacent\n        cells (defined as the two cells sharing the face).\n\n        The name of data in the input dictionary (data) are:\n        param : Parameter(Class). Contains the following parameters:\n            tensor : second_order_tensor\n                Permeability defined cell-wise. If not given a identity permeability\n                is assumed and a warning arised.\n            bc : boundary conditions (optional)\n            bc_val : dictionary (optional)\n                Values of the boundary conditions. The dictionary has at most the\n                following keys: 'dir' and 'neu', for Dirichlet and Neumann boundary\n                conditions, respectively.\n            apertures : (np.ndarray) (optional) apertures of the cells for scaling of\n                the face normals.\n\n        Hidden option (intended as \"advanced\" option that one should normally not\n        care about):\n            Half transmissibility calculation according to Ivar Aavatsmark, see\n            folk.uib.no/fciia/elliptisk.pdf. Activated by adding the entry\n            Aavatsmark_transmissibilities: True   to the data dictionary.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data.\n        \"\"\"\n        param = data['param']\n        k = param.get_tensor(self)\n        bnd = param.get_bc(self)\n        aperture = param.get_aperture()\n\n        if g.dim == 0:\n            data['flux'] = sps.csr_matrix([0])\n            data['bound_flux'] = 0\n            return None\n        if faces is None:\n            is_not_active = np.zeros(g.num_faces, dtype=np.bool)\n        else:\n            is_active = np.zeros(g.num_faces, dtype=np.bool)\n            is_active[faces] = True\n\n            is_not_active = np.logical_not(is_active)\n\n        fi, ci, sgn = sps.find(g.cell_faces)\n\n        # Normal vectors and permeability for each face (here and there side)\n        if aperture is None:\n            n = g.face_normals[:, fi]\n        else:\n            n = g.face_normals[:, fi] * aperture[ci]\n        n *= sgn\n        perm = k.perm[::, ::, ci]\n\n        # Distance from face center to cell center\n        fc_cc = g.face_centers[::, fi] - g.cell_centers[::, ci]\n\n        # Transpose normal vectors to match the shape of K and multiply the two\n        nk = perm * n\n        nk = nk.sum(axis=1)\n\n        if data.get('Aavatsmark_transmissibilities', False):\n            # These work better in some cases (possibly if the problem is grid\n            # quality rather than anisotropy?). To be explored (with care) or\n            # ignored.\n            dist_face_cell = np.linalg.norm(fc_cc, 2, axis=0)\n            t_face = np.linalg.norm(nk, 2, axis=0)\n        else:\n            nk *= fc_cc\n            t_face = nk.sum(axis=0)\n            dist_face_cell = np.power(fc_cc, 2).sum(axis=0)\n\n        t_face = np.divide(t_face, dist_face_cell)\n\n        # Return harmonic average\n        t = 1 / np.bincount(fi, weights=1 / t_face)\n\n        # For primal-like discretizations like the TPFA, internal boundaries\n        # are handled by assigning Neumann conditions.\n        is_dir = np.logical_and(bnd.is_dir, np.logical_not(bnd.is_internal))\n        is_neu = np.logical_or(bnd.is_neu, bnd.is_internal)\n        # Move Neumann faces to Neumann transmissibility\n        bndr_ind = g.get_all_boundary_faces()\n        t_b = np.zeros(g.num_faces)\n        t_b[is_dir] = -t[is_dir]\n        t_b[is_neu] = 1\n        t_b = t_b[bndr_ind]\n        t[np.logical_or(is_neu, is_not_active)] = 0\n        # Create flux matrix\n        flux = sps.coo_matrix((t[fi] * sgn, (fi, ci)))\n\n        # Create boundary flux matrix\n        bndr_sgn = (g.cell_faces[bndr_ind, :]).data\n        sort_id = np.argsort(g.cell_faces[bndr_ind, :].indices)\n        bndr_sgn = bndr_sgn[sort_id]\n        bound_flux = sps.coo_matrix((t_b * bndr_sgn, (bndr_ind, bndr_ind)),\n                                    (g.num_faces, g.num_faces))\n\n        data['flux'] = flux\n        data['bound_flux'] = bound_flux",
  "class TpfaCoupling(AbstractCoupling):\n\n    def __init__(self, solver):\n        self.solver = solver\n\n    def matrix_rhs(self, g_h, g_l, data_h, data_l, data_edge):\n        \"\"\"\n        Computes the coupling terms for the faces between cells in g_h and g_l\n        using the two-point flux approximation.\n\n        Parameters:\n            g_h and g_l: grid structures of the higher and lower dimensional\n                subdomains, respectively.\n            data_h and data_l: the corresponding data dictionaries. Assumed\n                to contain both permeability values ('perm') and apertures\n                ('apertures') for each of the cells in the grids.\n\n        Two hidden options (intended as \"advanced\" options that one should\n        normally not care about):\n            Half transmissibility calculation according to Ivar Aavatsmark, see\n            folk.uib.no/fciia/elliptisk.pdf. Activated by adding the entry\n            'Aavatsmark_transmissibilities': True   to the edge data.\n\n            Aperture correction. The face centre is moved half an aperture\n            away from the fracture for the matrix side transmissibility\n            calculation. Activated by adding the entry\n            'aperture_correction': True   to the edge data.\n\n        Returns:\n            cc: Discretization matrices for the coupling terms assembled\n                in a csc.sparse matrix.\n        \"\"\"\n\n        k_l = data_l['param'].get_tensor(self.solver)\n        k_h = data_h['param'].get_tensor(self.solver)\n        a_l = data_l['param'].get_aperture()\n        a_h = data_h['param'].get_aperture()\n\n        dof = np.array([self.solver.ndof(g_h), self.solver.ndof(g_l)])\n\n        # Obtain the cells and face signs of the higher dimensional grid\n        cells_l, faces_h, _ = sps.find(data_edge['face_cells'])\n        faces, cells_h, sgn_h = sps.find(g_h.cell_faces)\n        ind = np.unique(faces, return_index=True)[1]\n        sgn_h = sgn_h[ind]\n        cells_h = cells_h[ind]\n\n        cells_h, sgn_h = cells_h[faces_h], sgn_h[faces_h]\n\n        # The procedure for obtaining the face transmissibilities of the higher\n        # grid is in the main analougous to the one used in the discretize function\n        # of the Tpfa class.\n        n = g_h.face_normals[:, faces_h]\n        n *= sgn_h\n        perm_h = k_h.perm[:, :, cells_h]\n\n        # Compute the distance between face center and cell center. If specified\n        # (edgewise), the face centroid is shifted half an aperture in the normal\n        # direction of the fracture. Unless matrix cell size approaches the\n        # aperture, this has minimal impact.\n        if data_edge.get('aperture_correction', False):\n            apt_dim = np.divide(a_l[cells_l], a_h[cells_h])\n            fc_corrected = g_h.face_centers[::,\n                                            faces_h].copy() - apt_dim / 2 * n\n            fc_cc_h = fc_corrected - g_h.cell_centers[::, cells_h]\n        else:\n            fc_cc_h = g_h.face_centers[::, faces_h] - \\\n                g_h.cell_centers[::, cells_h]\n\n        nk_h = perm_h * n\n        nk_h = nk_h.sum(axis=1)\n        if data_edge.get('Aavatsmark_transmissibilities', False):\n            dist_face_cell_h = np.linalg.norm(fc_cc_h, 2, axis=0)\n            t_face_h = np.linalg.norm(nk_h, 2, axis=0)\n        else:\n            nk_h *= fc_cc_h\n            t_face_h = nk_h.sum(axis=0)\n            dist_face_cell_h = np.power(fc_cc_h, 2).sum(axis=0)\n\n        # Account for the apertures\n        t_face_h = t_face_h * a_h[cells_h]\n        # and compute the matrix side half transmissibilities\n        t_face_h = np.divide(t_face_h, dist_face_cell_h)\n\n        # For the lower dimension some simplifications can be made, due to the\n        # alignment of the face normals and (normal) permeabilities of the\n        # cells. First, the normal component of the permeability of the lower\n        # dimensional cells must be found. While not provided in g_l, the\n        # normal of these faces is the same as that of the corresponding higher\n        # dimensional face, up to a sign.\n        n1 = n[np.newaxis, :, :]\n        n2 = n[:, np.newaxis, :]\n        n1n2 = n1 * n2\n\n        normal_perm = np.einsum(\n            'ij...,ij...', n1n2, k_l.perm[:, :, cells_l])\n        # The area has been multiplied in twice, not once as above, through n1\n        # and n2\n        normal_perm = np.divide(normal_perm, g_h.face_areas[faces_h])\n\n        # Account for aperture contribution to face area\n        t_face_l = a_h[cells_h] * normal_perm\n\n        # And use it for face-center cell-center distance\n        t_face_l = np.divide(\n            t_face_l, 0.5 * np.divide(a_l[cells_l], a_h[cells_h]))\n\n        # Assemble face transmissibilities for the two dimensions and compute\n        # harmonic average\n        t_face = np.array([t_face_h, t_face_l])\n        t = t_face.prod(axis=0) / t_face.sum(axis=0)\n\n        # Create the block matrix for the contributions\n        cc = np.array([sps.coo_matrix((i, j)) for i in dof for j in dof]\n                      ).reshape((2, 2))\n\n        # Compute the off-diagonal terms\n        dataIJ, I, J = -t, cells_l, cells_h\n        cc[1, 0] = sps.csr_matrix((dataIJ, (I, J)), (dof[1], dof[0]))\n        cc[0, 1] = cc[1, 0].T\n\n        # Compute the diagonal terms\n        dataIJ, I, J = t, cells_h, cells_h\n        cc[0, 0] = sps.csr_matrix((dataIJ, (I, J)), (dof[0], dof[0]))\n        I, J = cells_l, cells_l\n        cc[1, 1] = sps.csr_matrix((dataIJ, (I, J)), (dof[1], dof[1]))\n\n        # Save the flux discretization for back-computation of fluxes\n        cells2faces = sps.csr_matrix((sgn_h, (faces_h, cells_h)),\n                                     (g_h.num_faces, g_h.num_cells))\n\n        data_edge['coupling_flux'] = sps.hstack([cells2faces * cc[0, 0],\n                                                 cells2faces * cc[0, 1]])\n        data_edge['coupling_discretization'] = cc\n\n        return cc",
  "class TpfaCouplingDFN(AbstractCoupling):\n\n    def __init__(self, solver):\n        self.solver = solver\n\n    def matrix_rhs(self, g_h, g_l, data_h, data_l, data_edge):\n        \"\"\"\n        Computes the coupling terms for the faces between cells in g_h and g_l\n        using the two-point flux approximation.\n\n        Parameters:\n            g_h and g_l: grid structures of the higher and lower dimensional\n                subdomains, respectively.\n            data_h and data_l: the corresponding data dictionaries. Assumed\n                to contain both permeability values ('perm') and apertures\n                ('apertures') for each of the cells in the grids.\n\n        Returns:\n            cc: Discretization matrices for the coupling terms assembled\n                in a csc.sparse matrix.\n        \"\"\"\n\n        k_h = data_h['param'].get_tensor(self.solver)\n        a_h = data_h['param'].get_aperture()\n\n        dof = np.array([self.solver.ndof(g_h), self.solver.ndof(g_l)])\n\n        # Obtain the cells and face signs of the higher dimensional grid\n        cells_l, faces_h, _ = sps.find(data_edge['face_cells'])\n        faces, cells_h, sgn_h = sps.find(g_h.cell_faces)\n        ind = np.unique(faces, return_index=True)[1]\n        sgn_h = sgn_h[ind]\n        cells_h = cells_h[ind]\n\n        cells_h, sgn_h = cells_h[faces_h], sgn_h[faces_h]\n\n        # The procedure for obtaining the face transmissibilities of the higher\n        # grid is analougous to the one used in the discretize function of the\n        # Tpfa class.\n        n = g_h.face_normals[:, faces_h]\n        n *= sgn_h\n        perm_h = k_h.perm[:, :, cells_h]\n        fc_cc_h = g_h.face_centers[::, faces_h] - g_h.cell_centers[::, cells_h]\n\n        nk_h = perm_h * n\n        nk_h = nk_h.sum(axis=1)\n        if data_edge.get('Aavatsmark_transmissibilities', False):\n            dist_face_cell_h = np.linalg.norm(fc_cc_h, 2, axis=0)\n            t_face_h = np.linalg.norm(nk_h, 2, axis=0)\n        else:\n            nk_h *= fc_cc_h\n            t_face_h = nk_h.sum(axis=0)\n            dist_face_cell_h = np.power(fc_cc_h, 2).sum(axis=0)\n\n        # Account for the apertures\n        t_face_h = t_face_h * a_h[cells_h]\n        t = np.divide(t_face_h, dist_face_cell_h)\n\n        # Create the block matrix for the contributions\n        cc = np.array([sps.coo_matrix((i, j)) for i in dof for j in dof]\n                      ).reshape((2, 2))\n\n        # Compute the off-diagonal terms\n        dataIJ, I, J = -t, cells_l, cells_h\n        cc[1, 0] = sps.csr_matrix((dataIJ, (I, J)), (dof[1], dof[0]))\n        cc[0, 1] = cc[1, 0].T\n\n        # Compute the diagonal terms\n        dataIJ, I, J = t, cells_h, cells_h\n        cc[0, 0] = sps.csr_matrix((dataIJ, (I, J)), (dof[0], dof[0]))\n        I, J = cells_l, cells_l\n        cc[1, 1] = sps.csr_matrix((dataIJ, (I, J)), (dof[1], dof[1]))\n\n        # Save the flux discretization for back-computation of fluxes\n        cells2faces = sps.csr_matrix((sgn_h, (faces_h, cells_h)),\n                                     (g_h.num_faces, g_h.num_cells))\n\n        data_edge['coupling_flux'] = sps.hstack([cells2faces * cc[0, 0],\n                                                 cells2faces * cc[0, 1]])\n        data_edge['coupling_discretization'] = cc\n\n        return cc",
  "def __init__(self, physics='flow'):\n        self.physics = physics\n\n        self.discr = Tpfa(self.physics)\n        self.discr_ndof = self.discr.ndof\n        self.coupling_conditions = TpfaCoupling(self.discr)\n\n        self.solver = Coupler(self.discr, self.coupling_conditions)",
  "def __init__(self, dim_max, physics='flow'):\n        # NOTE: There is no flow along the intersections of the fractures.\n\n        self.physics = physics\n        self.dim_max = dim_max\n\n        self.discr = Tpfa(self.physics)\n        self.coupling_conditions = TpfaCouplingDFN(self.discr)\n\n        kwargs = {\"discr_ndof\": self.discr.ndof,\n                  \"discr_fct\": self.__matrix_rhs__}\n        self.solver = Coupler(coupling=self.coupling_conditions, **kwargs)\n        SolverMixedDim.__init__(self)",
  "def __matrix_rhs__(self, g, data):\n        # The highest dimensional problem compute the matrix and rhs, the lower\n        # dimensional problem and empty matrix. For the latter, the size of the\n        # matrix is the number of cells.\n        if g.dim == self.dim_max:\n            return self.discr.matrix_rhs(g, data)\n        else:\n            ndof = self.discr.ndof(g)\n            return sps.csr_matrix((ndof, ndof)), np.zeros(ndof)",
  "def __init__(self, physics='flow'):\n        self.physics = physics",
  "def ndof(self, g):\n        \"\"\"\n        Return the number of degrees of freedom associated to the method.\n        In this case number of cells (pressure dof).\n\n        Parameter\n        ---------\n        g: grid, or a subclass.\n\n        Return\n        ------\n        dof: the number of degrees of freedom.\n\n        \"\"\"\n        return g.num_cells",
  "def matrix_rhs(self, g, data, faces=None, discretize=True):\n        \"\"\"\n        Return the matrix and right-hand side for a discretization of a second\n        order elliptic equation using a FV method with a two-point flux approximation.\n\n        To set a source see the source.Integral discretization class\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data. For details on necessary keywords,\n            see method discretize()\n        discretize (boolean, optional): Whether to discetize prior to matrix\n            assembly. If False, data should already contain discretization.\n            Defaults to True.\n\n        Return\n        ------\n        matrix: sparse csr (g_num_cells, g_num_cells)\n            Discretization matrix.\n        rhs: array (g_num_cells)\n            Right-hand side which contains the boundary conditions and the scalar\n            source term.\n\n        \"\"\"\n        div = fvutils.scalar_divergence(g)\n        if discretize:\n            self.discretize(g, data)\n        flux = data['flux']\n        M = div * flux\n\n        bound_flux = data['bound_flux']\n        param = data['param']\n        bc_val = param.get_bc_val(self)\n\n        return M, self.rhs(g, bound_flux, bc_val)",
  "def rhs(self, g, bound_flux, bc_val):\n        \"\"\"\n        Return the righ-hand side for a discretization of a second order elliptic\n        equation using the TPFA method. See self.matrix_rhs for a detaild\n        description.\n        \"\"\"\n        div = g.cell_faces.T\n        return -div * bound_flux * bc_val",
  "def discretize(self, g, data, faces=None):\n        \"\"\"\n        Discretize the second order elliptic equation using two-point flux\n\n        The method computes fluxes over faces in terms of pressures in adjacent\n        cells (defined as the two cells sharing the face).\n\n        The name of data in the input dictionary (data) are:\n        param : Parameter(Class). Contains the following parameters:\n            tensor : second_order_tensor\n                Permeability defined cell-wise. If not given a identity permeability\n                is assumed and a warning arised.\n            bc : boundary conditions (optional)\n            bc_val : dictionary (optional)\n                Values of the boundary conditions. The dictionary has at most the\n                following keys: 'dir' and 'neu', for Dirichlet and Neumann boundary\n                conditions, respectively.\n            apertures : (np.ndarray) (optional) apertures of the cells for scaling of\n                the face normals.\n\n        Hidden option (intended as \"advanced\" option that one should normally not\n        care about):\n            Half transmissibility calculation according to Ivar Aavatsmark, see\n            folk.uib.no/fciia/elliptisk.pdf. Activated by adding the entry\n            Aavatsmark_transmissibilities: True   to the data dictionary.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data.\n        \"\"\"\n        param = data['param']\n        k = param.get_tensor(self)\n        bnd = param.get_bc(self)\n        aperture = param.get_aperture()\n\n        if g.dim == 0:\n            data['flux'] = sps.csr_matrix([0])\n            data['bound_flux'] = 0\n            return None\n        if faces is None:\n            is_not_active = np.zeros(g.num_faces, dtype=np.bool)\n        else:\n            is_active = np.zeros(g.num_faces, dtype=np.bool)\n            is_active[faces] = True\n\n            is_not_active = np.logical_not(is_active)\n\n        fi, ci, sgn = sps.find(g.cell_faces)\n\n        # Normal vectors and permeability for each face (here and there side)\n        if aperture is None:\n            n = g.face_normals[:, fi]\n        else:\n            n = g.face_normals[:, fi] * aperture[ci]\n        n *= sgn\n        perm = k.perm[::, ::, ci]\n\n        # Distance from face center to cell center\n        fc_cc = g.face_centers[::, fi] - g.cell_centers[::, ci]\n\n        # Transpose normal vectors to match the shape of K and multiply the two\n        nk = perm * n\n        nk = nk.sum(axis=1)\n\n        if data.get('Aavatsmark_transmissibilities', False):\n            # These work better in some cases (possibly if the problem is grid\n            # quality rather than anisotropy?). To be explored (with care) or\n            # ignored.\n            dist_face_cell = np.linalg.norm(fc_cc, 2, axis=0)\n            t_face = np.linalg.norm(nk, 2, axis=0)\n        else:\n            nk *= fc_cc\n            t_face = nk.sum(axis=0)\n            dist_face_cell = np.power(fc_cc, 2).sum(axis=0)\n\n        t_face = np.divide(t_face, dist_face_cell)\n\n        # Return harmonic average\n        t = 1 / np.bincount(fi, weights=1 / t_face)\n\n        # For primal-like discretizations like the TPFA, internal boundaries\n        # are handled by assigning Neumann conditions.\n        is_dir = np.logical_and(bnd.is_dir, np.logical_not(bnd.is_internal))\n        is_neu = np.logical_or(bnd.is_neu, bnd.is_internal)\n        # Move Neumann faces to Neumann transmissibility\n        bndr_ind = g.get_all_boundary_faces()\n        t_b = np.zeros(g.num_faces)\n        t_b[is_dir] = -t[is_dir]\n        t_b[is_neu] = 1\n        t_b = t_b[bndr_ind]\n        t[np.logical_or(is_neu, is_not_active)] = 0\n        # Create flux matrix\n        flux = sps.coo_matrix((t[fi] * sgn, (fi, ci)))\n\n        # Create boundary flux matrix\n        bndr_sgn = (g.cell_faces[bndr_ind, :]).data\n        sort_id = np.argsort(g.cell_faces[bndr_ind, :].indices)\n        bndr_sgn = bndr_sgn[sort_id]\n        bound_flux = sps.coo_matrix((t_b * bndr_sgn, (bndr_ind, bndr_ind)),\n                                    (g.num_faces, g.num_faces))\n\n        data['flux'] = flux\n        data['bound_flux'] = bound_flux",
  "def __init__(self, solver):\n        self.solver = solver",
  "def matrix_rhs(self, g_h, g_l, data_h, data_l, data_edge):\n        \"\"\"\n        Computes the coupling terms for the faces between cells in g_h and g_l\n        using the two-point flux approximation.\n\n        Parameters:\n            g_h and g_l: grid structures of the higher and lower dimensional\n                subdomains, respectively.\n            data_h and data_l: the corresponding data dictionaries. Assumed\n                to contain both permeability values ('perm') and apertures\n                ('apertures') for each of the cells in the grids.\n\n        Two hidden options (intended as \"advanced\" options that one should\n        normally not care about):\n            Half transmissibility calculation according to Ivar Aavatsmark, see\n            folk.uib.no/fciia/elliptisk.pdf. Activated by adding the entry\n            'Aavatsmark_transmissibilities': True   to the edge data.\n\n            Aperture correction. The face centre is moved half an aperture\n            away from the fracture for the matrix side transmissibility\n            calculation. Activated by adding the entry\n            'aperture_correction': True   to the edge data.\n\n        Returns:\n            cc: Discretization matrices for the coupling terms assembled\n                in a csc.sparse matrix.\n        \"\"\"\n\n        k_l = data_l['param'].get_tensor(self.solver)\n        k_h = data_h['param'].get_tensor(self.solver)\n        a_l = data_l['param'].get_aperture()\n        a_h = data_h['param'].get_aperture()\n\n        dof = np.array([self.solver.ndof(g_h), self.solver.ndof(g_l)])\n\n        # Obtain the cells and face signs of the higher dimensional grid\n        cells_l, faces_h, _ = sps.find(data_edge['face_cells'])\n        faces, cells_h, sgn_h = sps.find(g_h.cell_faces)\n        ind = np.unique(faces, return_index=True)[1]\n        sgn_h = sgn_h[ind]\n        cells_h = cells_h[ind]\n\n        cells_h, sgn_h = cells_h[faces_h], sgn_h[faces_h]\n\n        # The procedure for obtaining the face transmissibilities of the higher\n        # grid is in the main analougous to the one used in the discretize function\n        # of the Tpfa class.\n        n = g_h.face_normals[:, faces_h]\n        n *= sgn_h\n        perm_h = k_h.perm[:, :, cells_h]\n\n        # Compute the distance between face center and cell center. If specified\n        # (edgewise), the face centroid is shifted half an aperture in the normal\n        # direction of the fracture. Unless matrix cell size approaches the\n        # aperture, this has minimal impact.\n        if data_edge.get('aperture_correction', False):\n            apt_dim = np.divide(a_l[cells_l], a_h[cells_h])\n            fc_corrected = g_h.face_centers[::,\n                                            faces_h].copy() - apt_dim / 2 * n\n            fc_cc_h = fc_corrected - g_h.cell_centers[::, cells_h]\n        else:\n            fc_cc_h = g_h.face_centers[::, faces_h] - \\\n                g_h.cell_centers[::, cells_h]\n\n        nk_h = perm_h * n\n        nk_h = nk_h.sum(axis=1)\n        if data_edge.get('Aavatsmark_transmissibilities', False):\n            dist_face_cell_h = np.linalg.norm(fc_cc_h, 2, axis=0)\n            t_face_h = np.linalg.norm(nk_h, 2, axis=0)\n        else:\n            nk_h *= fc_cc_h\n            t_face_h = nk_h.sum(axis=0)\n            dist_face_cell_h = np.power(fc_cc_h, 2).sum(axis=0)\n\n        # Account for the apertures\n        t_face_h = t_face_h * a_h[cells_h]\n        # and compute the matrix side half transmissibilities\n        t_face_h = np.divide(t_face_h, dist_face_cell_h)\n\n        # For the lower dimension some simplifications can be made, due to the\n        # alignment of the face normals and (normal) permeabilities of the\n        # cells. First, the normal component of the permeability of the lower\n        # dimensional cells must be found. While not provided in g_l, the\n        # normal of these faces is the same as that of the corresponding higher\n        # dimensional face, up to a sign.\n        n1 = n[np.newaxis, :, :]\n        n2 = n[:, np.newaxis, :]\n        n1n2 = n1 * n2\n\n        normal_perm = np.einsum(\n            'ij...,ij...', n1n2, k_l.perm[:, :, cells_l])\n        # The area has been multiplied in twice, not once as above, through n1\n        # and n2\n        normal_perm = np.divide(normal_perm, g_h.face_areas[faces_h])\n\n        # Account for aperture contribution to face area\n        t_face_l = a_h[cells_h] * normal_perm\n\n        # And use it for face-center cell-center distance\n        t_face_l = np.divide(\n            t_face_l, 0.5 * np.divide(a_l[cells_l], a_h[cells_h]))\n\n        # Assemble face transmissibilities for the two dimensions and compute\n        # harmonic average\n        t_face = np.array([t_face_h, t_face_l])\n        t = t_face.prod(axis=0) / t_face.sum(axis=0)\n\n        # Create the block matrix for the contributions\n        cc = np.array([sps.coo_matrix((i, j)) for i in dof for j in dof]\n                      ).reshape((2, 2))\n\n        # Compute the off-diagonal terms\n        dataIJ, I, J = -t, cells_l, cells_h\n        cc[1, 0] = sps.csr_matrix((dataIJ, (I, J)), (dof[1], dof[0]))\n        cc[0, 1] = cc[1, 0].T\n\n        # Compute the diagonal terms\n        dataIJ, I, J = t, cells_h, cells_h\n        cc[0, 0] = sps.csr_matrix((dataIJ, (I, J)), (dof[0], dof[0]))\n        I, J = cells_l, cells_l\n        cc[1, 1] = sps.csr_matrix((dataIJ, (I, J)), (dof[1], dof[1]))\n\n        # Save the flux discretization for back-computation of fluxes\n        cells2faces = sps.csr_matrix((sgn_h, (faces_h, cells_h)),\n                                     (g_h.num_faces, g_h.num_cells))\n\n        data_edge['coupling_flux'] = sps.hstack([cells2faces * cc[0, 0],\n                                                 cells2faces * cc[0, 1]])\n        data_edge['coupling_discretization'] = cc\n\n        return cc",
  "def __init__(self, solver):\n        self.solver = solver",
  "def matrix_rhs(self, g_h, g_l, data_h, data_l, data_edge):\n        \"\"\"\n        Computes the coupling terms for the faces between cells in g_h and g_l\n        using the two-point flux approximation.\n\n        Parameters:\n            g_h and g_l: grid structures of the higher and lower dimensional\n                subdomains, respectively.\n            data_h and data_l: the corresponding data dictionaries. Assumed\n                to contain both permeability values ('perm') and apertures\n                ('apertures') for each of the cells in the grids.\n\n        Returns:\n            cc: Discretization matrices for the coupling terms assembled\n                in a csc.sparse matrix.\n        \"\"\"\n\n        k_h = data_h['param'].get_tensor(self.solver)\n        a_h = data_h['param'].get_aperture()\n\n        dof = np.array([self.solver.ndof(g_h), self.solver.ndof(g_l)])\n\n        # Obtain the cells and face signs of the higher dimensional grid\n        cells_l, faces_h, _ = sps.find(data_edge['face_cells'])\n        faces, cells_h, sgn_h = sps.find(g_h.cell_faces)\n        ind = np.unique(faces, return_index=True)[1]\n        sgn_h = sgn_h[ind]\n        cells_h = cells_h[ind]\n\n        cells_h, sgn_h = cells_h[faces_h], sgn_h[faces_h]\n\n        # The procedure for obtaining the face transmissibilities of the higher\n        # grid is analougous to the one used in the discretize function of the\n        # Tpfa class.\n        n = g_h.face_normals[:, faces_h]\n        n *= sgn_h\n        perm_h = k_h.perm[:, :, cells_h]\n        fc_cc_h = g_h.face_centers[::, faces_h] - g_h.cell_centers[::, cells_h]\n\n        nk_h = perm_h * n\n        nk_h = nk_h.sum(axis=1)\n        if data_edge.get('Aavatsmark_transmissibilities', False):\n            dist_face_cell_h = np.linalg.norm(fc_cc_h, 2, axis=0)\n            t_face_h = np.linalg.norm(nk_h, 2, axis=0)\n        else:\n            nk_h *= fc_cc_h\n            t_face_h = nk_h.sum(axis=0)\n            dist_face_cell_h = np.power(fc_cc_h, 2).sum(axis=0)\n\n        # Account for the apertures\n        t_face_h = t_face_h * a_h[cells_h]\n        t = np.divide(t_face_h, dist_face_cell_h)\n\n        # Create the block matrix for the contributions\n        cc = np.array([sps.coo_matrix((i, j)) for i in dof for j in dof]\n                      ).reshape((2, 2))\n\n        # Compute the off-diagonal terms\n        dataIJ, I, J = -t, cells_l, cells_h\n        cc[1, 0] = sps.csr_matrix((dataIJ, (I, J)), (dof[1], dof[0]))\n        cc[0, 1] = cc[1, 0].T\n\n        # Compute the diagonal terms\n        dataIJ, I, J = t, cells_h, cells_h\n        cc[0, 0] = sps.csr_matrix((dataIJ, (I, J)), (dof[0], dof[0]))\n        I, J = cells_l, cells_l\n        cc[1, 1] = sps.csr_matrix((dataIJ, (I, J)), (dof[1], dof[1]))\n\n        # Save the flux discretization for back-computation of fluxes\n        cells2faces = sps.csr_matrix((sgn_h, (faces_h, cells_h)),\n                                     (g_h.num_faces, g_h.num_cells))\n\n        data_edge['coupling_flux'] = sps.hstack([cells2faces * cc[0, 0],\n                                                 cells2faces * cc[0, 1]])\n        data_edge['coupling_discretization'] = cc\n\n        return cc",
  "class MassMatrixMixedDim(SolverMixedDim):\n\n    def __init__(self, physics='flow'):\n        self.physics = physics\n\n        self.discr = MassMatrix(self.physics)\n\n        self.solver = Coupler(self.discr)",
  "class MassMatrix(Solver):\n\n#------------------------------------------------------------------------------#\n\n    def __init__(self, physics='flow'):\n        self.physics = physics\n\n#------------------------------------------------------------------------------#\n\n    def ndof(self, g):\n        \"\"\"\n        Return the number of degrees of freedom associated to the method.\n        In this case number of cells.\n\n        Parameter\n        ---------\n        g: grid, or a subclass.\n\n        Return\n        ------\n        dof: the number of degrees of freedom.\n\n        \"\"\"\n        return g.num_cells\n\n#------------------------------------------------------------------------------#\n\n    def matrix_rhs(self, g, data):\n        \"\"\"\n        Return the matrix and righ-hand side (null) for a discretization of a\n        L2-mass bilinear form with constant test and trial functions.\n\n        The name of data in the input dictionary (data) are:\n        phi: array (self.g.num_cells)\n            Scalar values which represent the porosity.\n            If not given assumed unitary.\n        deltaT: Time step for a possible temporal discretization scheme.\n            If not given assumed unitary.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data.\n\n        Return\n        ------\n        matrix: sparse dia (g.num_cells, g_num_cells)\n            Mass matrix obtained from the discretization.\n        rhs: array (g_num_cells)\n            Null right-hand side.\n\n        Examples\n        --------\n        data = {'deltaT': 1e-2, 'phi': 0.3*np.ones(g.num_cells)}\n        M, _ = mass.MassMatrix().matrix_rhs(g, data)\n\n        \"\"\"\n        ndof = self.ndof(g)\n        phi = data['param'].get_porosity()\n        aperture = data['param'].get_aperture()\n        coeff = g.cell_volumes * phi / data['deltaT'] * aperture\n\n        return sps.dia_matrix((coeff, 0), shape=(ndof, ndof)), np.zeros(ndof)",
  "class InvMassMatrixMixDim(SolverMixedDim):\n\n    def __init__(self, physics='flow'):\n        self.physics = physics\n\n        self.discr = InvMassMatrix(self.physics)\n\n        self.solver = Coupler(self.discr)",
  "class InvMassMatrix(Solver):\n\n#------------------------------------------------------------------------------#\n\n    def __init__(self, physics='flow'):\n        self.physics = physics\n\n#------------------------------------------------------------------------------#\n\n    def ndof(self, g):\n        \"\"\"\n        Return the number of degrees of freedom associated to the method.\n        In this case number of cells.\n\n        Parameter\n        ---------\n        g: grid, or a subclass.\n\n        Return\n        ------\n        dof: the number of degrees of freedom.\n\n        \"\"\"\n        return g.num_cells\n\n#------------------------------------------------------------------------------#\n\n    def matrix_rhs(self, g, data):\n        \"\"\"\n        Return the inverse of the matrix and righ-hand side (null) for a\n        discretization of a L2-mass bilinear form with constant test and trial\n        functions.\n\n        The name of data in the input dictionary (data) are:\n        phi: array (self.g.num_cells)\n            Scalar values which represent the porosity.\n            If not given assumed unitary.\n        deltaT: Time step for a possible temporal discretization scheme.\n            If not given assumed unitary.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data.\n\n        Return\n        ------\n        matrix: sparse dia (g.num_cells, g_num_cells)\n            Inverse of mass matrix obtained from the discretization.\n        rhs: array (g_num_cells)\n            Null right-hand side.\n\n        Examples\n        --------\n        data = {'deltaT': 1e-2, 'phi': 0.3*np.ones(g.num_cells)}\n        M, _ = mass.InvMassMatrix().matrix_rhs(g, data)\n\n        \"\"\"\n        M, rhs = MassMatrix(physics=self.physics).matrix_rhs(g, data)\n        return sps.dia_matrix((1. / M.diagonal(), 0), shape=M.shape), rhs",
  "def __init__(self, physics='flow'):\n        self.physics = physics\n\n        self.discr = MassMatrix(self.physics)\n\n        self.solver = Coupler(self.discr)",
  "def __init__(self, physics='flow'):\n        self.physics = physics",
  "def ndof(self, g):\n        \"\"\"\n        Return the number of degrees of freedom associated to the method.\n        In this case number of cells.\n\n        Parameter\n        ---------\n        g: grid, or a subclass.\n\n        Return\n        ------\n        dof: the number of degrees of freedom.\n\n        \"\"\"\n        return g.num_cells",
  "def matrix_rhs(self, g, data):\n        \"\"\"\n        Return the matrix and righ-hand side (null) for a discretization of a\n        L2-mass bilinear form with constant test and trial functions.\n\n        The name of data in the input dictionary (data) are:\n        phi: array (self.g.num_cells)\n            Scalar values which represent the porosity.\n            If not given assumed unitary.\n        deltaT: Time step for a possible temporal discretization scheme.\n            If not given assumed unitary.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data.\n\n        Return\n        ------\n        matrix: sparse dia (g.num_cells, g_num_cells)\n            Mass matrix obtained from the discretization.\n        rhs: array (g_num_cells)\n            Null right-hand side.\n\n        Examples\n        --------\n        data = {'deltaT': 1e-2, 'phi': 0.3*np.ones(g.num_cells)}\n        M, _ = mass.MassMatrix().matrix_rhs(g, data)\n\n        \"\"\"\n        ndof = self.ndof(g)\n        phi = data['param'].get_porosity()\n        aperture = data['param'].get_aperture()\n        coeff = g.cell_volumes * phi / data['deltaT'] * aperture\n\n        return sps.dia_matrix((coeff, 0), shape=(ndof, ndof)), np.zeros(ndof)",
  "def __init__(self, physics='flow'):\n        self.physics = physics\n\n        self.discr = InvMassMatrix(self.physics)\n\n        self.solver = Coupler(self.discr)",
  "def __init__(self, physics='flow'):\n        self.physics = physics",
  "def ndof(self, g):\n        \"\"\"\n        Return the number of degrees of freedom associated to the method.\n        In this case number of cells.\n\n        Parameter\n        ---------\n        g: grid, or a subclass.\n\n        Return\n        ------\n        dof: the number of degrees of freedom.\n\n        \"\"\"\n        return g.num_cells",
  "def matrix_rhs(self, g, data):\n        \"\"\"\n        Return the inverse of the matrix and righ-hand side (null) for a\n        discretization of a L2-mass bilinear form with constant test and trial\n        functions.\n\n        The name of data in the input dictionary (data) are:\n        phi: array (self.g.num_cells)\n            Scalar values which represent the porosity.\n            If not given assumed unitary.\n        deltaT: Time step for a possible temporal discretization scheme.\n            If not given assumed unitary.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data.\n\n        Return\n        ------\n        matrix: sparse dia (g.num_cells, g_num_cells)\n            Inverse of mass matrix obtained from the discretization.\n        rhs: array (g_num_cells)\n            Null right-hand side.\n\n        Examples\n        --------\n        data = {'deltaT': 1e-2, 'phi': 0.3*np.ones(g.num_cells)}\n        M, _ = mass.InvMassMatrix().matrix_rhs(g, data)\n\n        \"\"\"\n        M, rhs = MassMatrix(physics=self.physics).matrix_rhs(g, data)\n        return sps.dia_matrix((1. / M.diagonal(), 0), shape=M.shape), rhs",
  "class Mpsa(Solver):\n\n    def __init__(self, physics='mechanics'):\n        self.physics = physics\n\n    def ndof(self, g):\n        \"\"\"\n        Return the number of degrees of freedom associated to the method.\n        In this case number of cells times dimension (stress dof).\n\n        Parameter\n        ---------\n        g: grid, or a subclass.\n\n        Return\n        ------\n        dof: the number of degrees of freedom.\n\n        \"\"\"\n        return g.dim * g.num_cells\n\n#------------------------------------------------------------------------------#\n\n    def matrix_rhs(self, g, data, discretize=True):\n        \"\"\"\n        Return the matrix and right-hand side for a discretization of a second\n        order elliptic equation using a FV method with a multi-point stress\n        approximation.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data. For details on necessary keywords,\n            see method discretize()\n        discretize (boolean, optional): default True. Whether to discetize\n            prior to matrix assembly. If False, data should already contain\n            discretization.\n\n        Return\n        ------\n        matrix: sparse csr (g.dim * g_num_cells, g.dim * g_num_cells)\n            Discretization matrix.\n        rhs: array (g.dim * g_num_cells)\n            Right-hand side which contains the boundary conditions and the scalar\n            source term.\n        \"\"\"\n        param = data['param']\n\n        if discretize:\n            self.discretize(g, data)\n        div = fvutils.vector_divergence(g)\n        stress = data['stress']\n        bound_stress = data['bound_stress']\n        M = div * stress\n\n        f = data['param'].get_source(self)\n        bc_val = data['param'].get_bc_val(self)\n\n        return M, self.rhs(g, bound_stress, bc_val, f)\n\n#------------------------------------------------------------------------------#\n\n    def discretize(self, g, data):\n        \"\"\"\n        Discretize the vector elliptic equation by the multi-point stress\n\n        The method computes fluxes over faces in terms of displacements in\n        adjacent cells (defined as the two cells sharing the face).\n\n        The name of data in the input dictionary (data) are:\n        param : Parameter(Class). Contains the following parameters:\n            tensor : fourth_order_tensor\n                Permeability defined cell-wise. If not given a identity permeability\n                is assumed and a warning arised.\n            bc : boundary conditions (optional)\n            bc_val : dictionary (optional)\n                Values of the boundary conditions. The dictionary has at most the\n                following keys: 'dir' and 'neu', for Dirichlet and Neumann boundary\n                conditions, respectively.\n            apertures : (np.ndarray) (optional) apertures of the cells for scaling of\n                the face normals.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data.\n        \"\"\"\n\n        c = data['param'].get_tensor(self)\n        bnd = data['param'].get_bc(self)\n\n        stress, bound_stress = mpsa(g, c, bnd)\n\n        data['stress'] = stress\n        data['bound_stress'] = bound_stress\n\n#------------------------------------------------------------------------------#\n\n    def rhs(self, g, bound_stress, bc_val, f):\n        \"\"\"\n        Return the righ-hand side for a discretization of a second order elliptic\n        equation using the MPSA method. See self.matrix_rhs for a detailed\n        description.\n        \"\"\"\n        div = fvutils.vector_divergence(g)\n\n        return -div * bound_stress * bc_val - f",
  "class FracturedMpsa(Mpsa):\n    \"\"\"\n    Subclass of MPSA for discretizing a fractured domain. Adds DOFs on each\n    fracture face which describe the fracture deformation.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        Mpsa.__init__(self, **kwargs)\n        assert hasattr(self, 'physics'), 'Mpsa must assign physics'\n\n    def ndof(self, g):\n        \"\"\"\n        Return the number of degrees of freedom associated to the method.\n        In this case number of cells times dimension (stress dof).\n\n        Parameter\n        ---------\n        g: grid, or a subclass.\n\n        Return\n        ------\n        dof: the number of degrees of freedom.\n\n        \"\"\"\n        num_fracs = np.sum(g.tags['fracture_faces'])\n        return g.dim * (g.num_cells + num_fracs)\n\n    def matrix_rhs(self, g, data, discretize=True):\n        \"\"\"\n        Return the matrix and right-hand side for a discretization of a second\n        order elliptic equation using a FV method with a multi-point stress\n        approximation with dofs added on the fracture interfaces.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data. For details on necessary keywords,\n            see method discretize()\n        discretize (boolean, optional): default True. Whether to discetize\n            prior to matrix assembly. If False, data should already contain\n            discretization.\n\n        Return\n        ------\n        matrix: sparse csr (g.dim * g_num_cells + 2 * {#of fracture faces},\n                            2 * {#of fracture faces})\n            Discretization matrix.\n        rhs: array (g.dim * g_num_cells  + g.dim * num_frac_faces)\n            Right-hand side which contains the boundary conditions and the scalar\n            source term.\n        \"\"\"\n        if discretize:\n            self.discretize_fractures(g, data)\n\n        stress = data['stress']\n        bound_stress = data['bound_stress']\n        b_e = data['b_e']\n        A_e = data['A_e']\n\n        L, b_l = self.given_slip_distance(g, stress, bound_stress)\n\n        bc_val = data['param'].get_bc_val(self)\n\n        frac_faces = np.matlib.repmat(g.tags['fracture_faces'], 3, 1)\n        assert np.all(bc_val[frac_faces.ravel('F')] == 0), \\\n            '''Fracture should have zero boundary condition. Set slip by\n               Parameters.set_slip_distance'''\n\n        slip_distance = data['param'].get_slip_distance()\n\n        A = sps.vstack((A_e, L), format='csr')\n        rhs = np.hstack((b_e * bc_val, b_l * slip_distance))\n\n        return A, rhs\n\n    def traction(self, g, data, sol):\n        \"\"\"\n        Extract the traction on the faces from fractured fv solution.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        sol : array (g.dim * (g.num_cells + {#of fracture faces}))\n            Solution, stored as [cell_disp, fracture_disp]\n\n        Return\n        ------\n        T : array (g.dim * g.num_faces)\n            traction on each face\n\n        \"\"\"\n        bc_val = data['param'].get_bc_val(self.physics).copy()\n        frac_disp = self.extract_frac_u(g, sol)\n        cell_disp = self.extract_u(g, sol)\n\n        frac_faces = (g.frac_pairs).ravel('C')\n        frac_ind = mcolon.mcolon(\n            g.dim * frac_faces, g.dim * frac_faces + g.dim)\n\n        bc_val[frac_ind] = frac_disp\n\n        T = data['stress'] * cell_disp + data['bound_stress'] * bc_val\n        return T\n\n    def extract_u(self, g, sol):\n        \"\"\"  Extract the cell displacement from fractured fv solution.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        sol : array (g.dim * (g.num_cells + {#of fracture faces}))\n            Solution, stored as [cell_disp, fracture_disp]\n\n        Return\n        ------\n        u : array (g.dim * g.num_cells)\n            displacement at each cell\n\n        \"\"\"\n        # pylint: disable=invalid-name\n        return sol[:g.dim * g.num_cells]\n\n    def extract_frac_u(self, g, sol):\n        \"\"\"  Extract the fracture displacement from fractured fv solution.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        sol : array (g.dim * (g.num_cells + {#of fracture faces}))\n            Solution, stored as [cell_disp, fracture_disp]\n\n        Return\n        ------\n        u : array (g.dim *{#of fracture faces})\n            displacement at each fracture face\n\n        \"\"\"\n        # pylint: disable=invalid-name\n        return sol[g.dim * g.num_cells:]\n\n    def discretize_fractures(self, g, data, faces=None, **kwargs):\n        \"\"\"\n        Discretize the vector elliptic equation by the multi-point stress and added \n        degrees of freedom on the fracture faces\n\n        The method computes fluxes over faces in terms of displacements in\n        adjacent cells (defined as the two cells sharing the face).\n\n        The name of data in the input dictionary (data) are:\n        param : Parameter(Class). Contains the following parameters:\n            tensor : fourth_order_tensor\n                Permeability defined cell-wise. If not given a identity permeability\n                is assumed and a warning arised.\n            bc : boundary conditions (optional)\n            bc_val : dictionary (optional)\n                Values of the boundary conditions. The dictionary has at most the\n                following keys: 'dir' and 'neu', for Dirichlet and Neumann boundary\n                conditions, respectively.\n            apertures : (np.ndarray) (optional) apertures of the cells for scaling of\n                the face normals.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data.\n        \"\"\"\n\n        #    dir_bound = g.get_all_boundary_faces()\n        #    bound = bc.BoundaryCondition(g, dir_bound, ['dir'] * dir_bound.size)\n\n        frac_faces = g.tags['fracture_faces']\n\n        bound = data['param'].get_bc(self)\n        is_dir = bound.is_dir\n        if not np.all(is_dir[frac_faces]):\n            is_dir[frac_faces] = True\n\n        bound = bc.BoundaryCondition(g, is_dir, 'dir')\n\n        assert np.all(bound.is_dir[frac_faces]), \\\n            'fractures must be set as dirichlet boundary faces'\n\n        # Discretize with normal mpsa\n        self.discretize(g, data, **kwargs)\n        stress, bound_stress = data['stress'], data['bound_stress']\n        # Create A and rhs\n        div = fvutils.vector_divergence(g)\n        a = div * stress\n        b = div * bound_stress\n\n        # we find the matrix indices of the fracture\n        if faces is None:\n            frac_faces = g.frac_pairs\n            frac_faces_left = frac_faces[0]\n            frac_faces_right = frac_faces[1]\n        else:\n            raise NotImplementedError('not implemented given faces')\n\n        int_b_left = mcolon.mcolon(\n            g.dim * frac_faces_left, g.dim * frac_faces_left + g.dim)\n        int_b_right = mcolon.mcolon(\n            g.dim * frac_faces_right, g.dim * frac_faces_right + g.dim)\n        int_b_ind = np.ravel((int_b_left, int_b_right), 'C')\n\n        # We find the sign of the left and right faces.\n        sgn_left = _sign_matrix(g, frac_faces_left)\n        sgn_right = _sign_matrix(g, frac_faces_right)\n        # The displacement on the internal boundary face are considered unknowns,\n        # so we move them over to the lhs. The rhs now only consists of the\n        # external boundary faces\n        b_internal = b[:, int_b_ind]\n        b_external = b.copy()\n        sparse_mat.zero_columns(b_external, int_b_ind)\n\n        bound_stress_external = bound_stress.copy().tocsc()\n        sparse_mat.zero_columns(bound_stress_external, int_b_ind)\n        # We assume that the traction on the left hand side is equal but\n        # opisite\n\n        frac_stress_diff = (sgn_left * bound_stress[int_b_left, :] +\n                            sgn_right * bound_stress[int_b_right, :])[:, int_b_ind]\n        internal_stress = sps.hstack(\n            (sgn_left * stress[int_b_left, :] + sgn_right * stress[int_b_right, :],\n             frac_stress_diff))\n\n        A = sps.vstack((sps.hstack((a, b_internal)),\n                        internal_stress), format='csr')\n        # negative sign since we have moved b_external from lhs to rhs\n        d_b = -b_external\n        # sps.csr_matrix((int_b_left.size, g.num_faces * g.dim))\n        d_t = -sgn_left * bound_stress_external[int_b_left] \\\n            - sgn_right * bound_stress_external[int_b_right]\n\n        b_matrix = sps.vstack((d_b, d_t), format='csr')\n\n        data['b_e'] = b_matrix\n        data['A_e'] = A\n\n    def given_traction(self, g, stress, bound_stress, faces=None, **kwargs):\n        # we find the matrix indices of the fracture\n        if faces is None:\n            frac_faces = g.frac_pairs\n            frac_faces_left = frac_faces[0]\n            frac_faces_right = frac_faces[1]\n        else:\n            raise NotImplementedError('not implemented given faces')\n\n        int_b_left = mcolon.mcolon(\n            g.dim * frac_faces_left, g.dim * frac_faces_left + g.dim)\n        int_b_right = mcolon.mcolon(\n            g.dim * frac_faces_right, g.dim * frac_faces_right + g.dim)\n        int_b_ind = np.ravel((int_b_left, int_b_right), 'C')\n\n        # We find the sign of the left and right faces.\n        sgn_left = _sign_matrix(g, frac_faces_left)\n        sgn_right = _sign_matrix(g, frac_faces_right)\n\n        # We obtain the stress from boundary conditions on the domain boundary\n        bound_stress_external = bound_stress.copy()\n        bound_stress_external[:, int_b_ind] = 0\n\n        # We construct the L matrix, i.e., we set the traction on the left\n        # fracture side\n        frac_stress = (sgn_left * bound_stress[int_b_left, :])[:, int_b_ind]\n\n        L = sps.hstack((sgn_left * stress[int_b_left, :], frac_stress))\n\n        # negative sign since we have moved b_external from lhs to rhs\n        d_f = sps.csr_matrix((np.ones(int_b_left.size),\n                              (np.arange(int_b_left.size), int_b_left)),\n                             (int_b_left.size, g.num_faces * g.dim))\n        d_t = sgn_left * bound_stress_external[int_b_left]  # \\\n    #        + sgn_right * bound_stress_external[int_b_right]\n\n        return L, d_t\n\n    def given_slip_distance(self, g, stress, bound_stress, faces=None):\n        # we find the matrix indices of the fracture\n        if faces is None:\n            frac_faces = g.frac_pairs\n            frac_faces_left = frac_faces[0]\n            frac_faces_right = frac_faces[1]\n        else:\n            raise NotImplementedError('not implemented given faces')\n\n        int_b_left = mcolon.mcolon(\n            g.dim * frac_faces_left, g.dim * frac_faces_left + g.dim)\n        int_b_right = mcolon.mcolon(\n            g.dim * frac_faces_right, g.dim * frac_faces_right + g.dim)\n        int_b_ind = np.ravel((int_b_left, int_b_right), 'C')\n\n        # We construct the L matrix, by assuming that the relative displacement\n        # is given\n        L = sps.hstack((sps.csr_matrix((int_b_left.size, g.dim * g.num_cells)),\n                        sps.identity(int_b_left.size),\n                        -sps.identity(int_b_right.size)))\n\n        d_f = sps.csr_matrix((np.ones(int_b_left.size),\n                              (np.arange(int_b_left.size), int_b_left)),\n                             (int_b_left.size, g.num_faces * g.dim))\n\n        return L, d_f",
  "def mpsa(g, constit, bound, eta=None, inverter=None, max_memory=None,\n         **kwargs):\n    \"\"\"\n    Discretize the vector elliptic equation by the multi-point stress\n    approximation method, specifically the weakly symmetric MPSA-W method.\n\n    The method computes stresses over faces in terms of displacments in\n    adjacent cells (defined as all cells sharing at least one vertex with the\n    face).  This corresponds to the MPSA-W method, see\n\n    Keilegavlen, Nordbotten: Finite volume methods for elasticity with weak\n        symmetry. Int J Num. Meth. Eng. doi: 10.1002/nme.5538.\n\n    Implementation needs:\n        1) The local linear systems should be scaled with the elastic moduli\n        and the local grid size, so that we avoid rounding errors accumulating\n        under grid refinement / convergence tests.\n        2) It should be possible to do a partial update of the discretization\n        stensil (say, if we introduce an internal boundary, or modify the\n        permeability field).\n        3) For large grids, the current implementation will run into memory\n        issues, due to the construction of a block diagonal matrix. This can be\n        overcome by splitting the discretization into several partial updates.\n        4) It probably makes sense to create a wrapper class to store the\n        discretization, interface to linear solvers etc.\n    Right now, there are concrete plans for 2) - 4).\n\n    Parameters:\n        g (core.grids.grid): grid to be discretized\n        constit (core.bc.bc) class for boundary values\n        eta Location of pressure continuity point. Should be 1/3 for simplex\n            grids, 0 otherwise. On boundary faces with Dirichlet conditions,\n            eta=0 will be enforced.\n        inverter (string) Block inverter to be used, either numba (default),\n            cython or python. See fvutils.invert_diagonal_blocks for details.\n        max_memory (double): Threshold for peak memory during discretization.\n            If the **estimated** memory need is larger than the provided\n            threshold, the discretization will be split into an appropriate\n            number of sub-calculations, using mpsa_partial().\n\n    Returns:\n        scipy.sparse.csr_matrix (shape num_faces, num_cells): stress\n            discretization, in the form of mapping from cell displacement to\n            face stresses.\n            NOTE: The cell displacements are ordered cellwise (first u_x_1,\n            u_y_1, u_x_2 etc)\n        scipy.sparse.csr_matrix (shape num_faces, num_faces): discretization of\n            boundary conditions. Interpreted as istresses induced by the boundary\n            condition (both Dirichlet and Neumann). For Neumann, this will be\n            the prescribed stress over the boundary face, and possibly stress\n            on faces having nodes on the boundary. For Dirichlet, the values\n            will be stresses induced by the prescribed displacement.\n            Incorporation as a right hand side in linear system by\n            multiplication with divergence operator.\n            NOTE: The stresses are ordered facewise (first s_x_1, s_y_1 etc)\n\n    Example:\n        # Set up a Cartesian grid\n        g = structured.CartGrid([5, 5])\n        c =tensor.FourthOrder(g.dim, np.ones(g.num_cells))\n\n        # Dirirchlet boundary conditions\n        bound_faces = g.get_all_boundary_faces().ravel()\n        bnd = bc.BoundaryCondition(g, bound_faces, ['dir'] * bound_faces.size)\n\n        # Discretization\n        stress, bound_stress = mpsa(g, c, bnd)\n\n        # Source in the middle of the domain\n        q = np.zeros(g.num_cells * g.dim)\n        q[12 * g.dim] = 1\n\n        # Divergence operator for the grid\n        div = fvutils.vector_divergence(g)\n\n        # Discretization matrix\n        A = div * stress\n\n        # Assign boundary values to all faces on the bounary\n        bound_vals = np.zeros(g.num_faces * g.dim)\n        bound_vals[bound_faces] = np.arange(bound_faces.size * g.dim)\n\n        # Assemble the right hand side and solve\n        rhs = -q - div * bound_stress * bound_vals\n        x = sps.linalg.spsolve(A, rhs)\n        s = stress * x + bound_stress * bound_vals\n\n    \"\"\"\n    if eta is None:\n        eta = fvutils.determine_eta(g)\n\n    if max_memory is None:\n        # For the moment nothing to do here, just call main mpfa method for the\n        # entire grid.\n        # TODO: We may want to estimate the memory need, and give a warning if\n        # this seems excessive\n        stress, bound_stress = _mpsa_local(\n            g, constit, bound, eta=eta, inverter=inverter)\n    else:\n        # Estimate number of partitions necessary based on prescribed memory\n        # usage\n        peak_mem = _estimate_peak_memory_mpsa(g)\n        num_part = np.ceil(peak_mem / max_memory)\n\n        print('Split MPSA discretization into ' + str(num_part) + ' parts')\n\n        # Let partitioning module apply the best available method\n        part = partition.partition(g, num_part)\n\n        # Empty fields for stress and bound_stress. Will be expanded as we go.\n        # Implementation note: It should be relatively straightforward to\n        # estimate the memory need of stress (face_nodes -> node_cells ->\n        # unique).\n        stress = sps.csr_matrix((g.num_faces * g.dim, g.num_cells * g.dim))\n        bound_stress = sps.csr_matrix((g.num_faces * g.dim,\n                                       g.num_faces * g.dim))\n\n        cn = g.cell_nodes()\n\n        face_covered = np.zeros(g.num_faces, dtype=np.bool)\n\n        for p in np.unique(part):\n            # Cells in this partitioning\n            cell_ind = np.argwhere(part == p).ravel('F')\n            # To discretize with as little overlap as possible, we use the\n            # keyword nodes to specify the update stencil. Find nodes of the\n            # local cells.\n            active_cells = np.zeros(g.num_cells, dtype=np.bool)\n            active_cells[cell_ind] = 1\n            active_nodes = np.squeeze(np.where((cn * active_cells) > 0))\n\n            # Perform local discretization.\n            loc_stress, loc_bound_stress, loc_faces \\\n                = mpsa_partial(g, constit, bound, eta=eta, inverter=inverter,\n                               nodes=active_nodes)\n\n            # Eliminate contribution from faces already covered\n            eliminate_ind = fvutils.expand_indices_nd(face_covered, g.dim)\n            fvutils.zero_out_sparse_rows(loc_stress, eliminate_ind)\n            fvutils.zero_out_sparse_rows(loc_bound_stress, eliminate_ind)\n\n            face_covered[loc_faces] = 1\n\n            stress += loc_stress\n            bound_stress += loc_bound_stress\n\n    return stress, bound_stress",
  "def mpsa_partial(g, constit, bound, eta=0, inverter='numba', cells=None,\n                 faces=None, nodes=None):\n    \"\"\"\n    Run an MPFA discretization on subgrid, and return discretization in terms\n    of global variable numbers.\n\n    Scenarios where the method will be used include updates of permeability,\n    and the introduction of an internal boundary (e.g. fracture growth).\n\n    The subgrid can be specified in terms of cells, faces and nodes to be\n    updated. For details on the implementation, see\n    fv_utils.cell_ind_for_partial_update()\n\n    Parameters:\n        g (porepy.grids.grid.Grid): grid to be discretized\n        constit (porepy.params.tensor.SecondOrder) permeability tensor\n        bnd (porepy.params.bc.BoundaryCondition) class for boundary conditions\n        faces (np.ndarray) faces to be considered. Intended for partial\n            discretization, may change in the future\n        eta Location of pressure continuity point. Should be 1/3 for simplex\n            grids, 0 otherwise. On boundary faces with Dirichlet conditions,\n            eta=0 will be enforced.\n        inverter (string) Block inverter to be used, either numba (default),\n            cython or python. See fvutils.invert_diagonal_blocks for details.\n        cells (np.array, int, optional): Index of cells on which to base the\n            subgrid computation. Defaults to None.\n        faces (np.array, int, optional): Index of faces on which to base the\n            subgrid computation. Defaults to None.\n        nodes (np.array, int, optional): Index of nodes on which to base the\n            subgrid computation. Defaults to None.\n\n        Note that if all of {cells, faces, nodes} are None, empty matrices will\n        be returned.\n\n    Returns:\n        sps.csr_matrix (g.num_faces x g.num_cells): Stress discretization,\n            computed on a subgrid.\n        sps.csr_matrix (g,num_faces x g.num_faces): Boundary stress\n            discretization, computed on a subgrid\n        np.array (int): Global of the faces where the stress discretization is\n            computed.\n\n    \"\"\"\n    if cells is not None:\n        warnings.warn('Cells keyword for partial mpfa has not been tested')\n    if faces is not None:\n        warnings.warn('Faces keyword for partial mpfa has not been tested')\n\n    # Find computational stencil, based on specified cells, faces and nodes.\n    ind, active_faces = fvutils.cell_ind_for_partial_update(g, cells=cells,\n                                                            faces=faces,\n                                                            nodes=nodes)\n\n    # Extract subgrid, together with mappings between local and global\n    # cells\n    sub_g, l2g_faces, _ = partition.extract_subgrid(g, ind)\n    l2g_cells = sub_g.parent_cell_ind\n\n    # Copy stiffness tensor, and restrict to local cells\n    loc_c = constit.copy()\n    loc_c.c = loc_c.c[::, ::, l2g_cells]\n    # Also restrict the lambda and mu fields; we will copy the stiffness\n    # tensors later.\n    loc_c.lmbda = loc_c.lmbda[l2g_cells]\n    loc_c.mu = loc_c.mu[l2g_cells]\n\n    glob_bound_face = g.get_all_boundary_faces()\n\n    # Boundary conditions are slightly more complex. Find local faces\n    # that are on the global boundary.\n    loc_bound_ind = np.argwhere(np.in1d(l2g_faces, glob_bound_face)).ravel('F')\n\n    # Then transfer boundary condition on those faces.\n    loc_cond = np.array(loc_bound_ind.size * ['neu'])\n    if loc_bound_ind.size > 0:\n        # Neumann condition is default, so only Dirichlet needs to be set\n        is_dir = bound.is_dir[l2g_faces[loc_bound_ind]]\n        loc_cond[is_dir] = 'dir'\n\n    loc_bnd = bc.BoundaryCondition(sub_g, faces=loc_bound_ind, cond=loc_cond)\n\n    # Discretization of sub-problem\n    stress_loc, bound_stress_loc = _mpsa_local(sub_g, loc_c, loc_bnd,\n                                               eta=eta, inverter=inverter)\n\n    face_map, cell_map = fvutils.map_subgrid_to_grid(g, l2g_faces, l2g_cells,\n                                                     is_vector=True)\n\n    # Update global face fields.\n    stress_glob = face_map * stress_loc * cell_map\n    bound_stress_glob = face_map * bound_stress_loc * face_map.transpose()\n\n    # By design of mpfa, and the subgrids, the discretization will update faces\n    # outside the active faces. Kill these.\n    outside = np.setdiff1d(np.arange(g.num_faces), active_faces,\n                           assume_unique=True)\n    eliminate_ind = fvutils.expand_indices_nd(outside, g.dim)\n    fvutils.zero_out_sparse_rows(stress_glob, eliminate_ind)\n    fvutils.zero_out_sparse_rows(bound_stress_glob, eliminate_ind)\n\n    return stress_glob, bound_stress_glob, active_faces",
  "def _mpsa_local(g, constit, bound, eta=0, inverter='numba'):\n    \"\"\"\n    Actual implementation of the MPSA W-method. To calculate the MPSA\n    discretization on a grid, either call this method, or, to respect the\n    privacy of this method, call the main mpsa method with no memory\n    constraints.\n\n    Implementation details:\n\n    The displacement is discretized as a linear function on sub-cells (see\n    reference paper). In this implementation, the displacement is represented by\n    its cell center value and the sub-cell gradients.\n\n    The method will give continuous stresses over the faces, and displacement\n    continuity for certain points (controlled by the parameter eta). This can\n    be expressed as a linear system on the form\n\n        (i)   A * grad_u            = 0\n        (ii)  B * grad_u + C * u_cc = 0\n        (iii) 0            D * u_cc = I\n\n    Here, the first equation represents stress continuity, and involves only\n    the displacement gradients (grad_u). The second equation gives displacement\n    continuity over cell faces, thus B will contain distances between cell\n    centers and the face continuity points, while C consists of +- 1 (depending\n    on which side the cell is relative to the face normal vector). The third\n    equation enforces the displacement to be unity in one cell at a time. Thus\n    (i)-(iii) can be inverted to express the displacement gradients as in terms\n    of the cell center variables, that is, we can compute the basis functions\n    on the sub-cells. Because of the method construction (again see reference\n    paper), the basis function of a cell c will be non-zero on all sub-cells\n    sharing a vertex with c. Finally, the fluxes as functions of cell center\n    values are computed by insertion into Hook's law (which is essentially half\n    of A from (i), that is, only consider contribution from one side of the\n    face.\n\n    Boundary values can be incorporated with appropriate modifications -\n    Neumann conditions will have a non-zero right hand side for (i), while\n    Dirichlet gives a right hand side for (ii).\n\n    \"\"\"\n\n    # The grid coordinates are always three-dimensional, even if the grid is\n    # really 2D. This means that there is not a 1-1 relation between the number\n    # of coordinates of a point / vector and the real dimension. This again\n    # violates some assumptions tacitly made in the discretization (in\n    # particular that the number of faces of a cell that meets in a vertex\n    # equals the grid dimension, and that this can be used to construct an\n    # index of local variables in the discretization). These issues should be\n    # possible to overcome, but for the moment, we simply force 2D grids to be\n    # proper 2D.\n    if g.dim == 2:\n        g = g.copy()\n        g.cell_centers = np.delete(g.cell_centers, (2), axis=0)\n        g.face_centers = np.delete(g.face_centers, (2), axis=0)\n        g.face_normals = np.delete(g.face_normals, (2), axis=0)\n        g.nodes = np.delete(g.nodes, (2), axis=0)\n\n        constit = constit.copy()\n        constit.c = np.delete(constit.c, (2, 5, 6, 7, 8), axis=0)\n        constit.c = np.delete(constit.c, (2, 5, 6, 7, 8), axis=1)\n\n    nd = g.dim\n\n    # Define subcell topology\n    subcell_topology = fvutils.SubcellTopology(g)\n    # Obtain mappings to exclude boundary faces\n    bound_exclusion = fvutils.ExcludeBoundaries(subcell_topology, bound, nd)\n    # Most of the work is done by submethod for elasticity (which is common for\n    # elasticity and poro-elasticity).\n    hook, igrad, rhs_cells, _, _ = mpsa_elasticity(g, constit,\n                                                   subcell_topology,\n                                                   bound_exclusion, eta,\n                                                   inverter)\n\n    hook_igrad = hook * igrad\n    # NOTE: This is the point where we expect to reach peak memory need.\n    del hook, igrad\n\n    # Output should be on face-level (not sub-face)\n    hf2f = fvutils.map_hf_2_f(subcell_topology.fno_unique,\n                              subcell_topology.subfno_unique, nd)\n\n    # Stress discretization\n    stress = hf2f * hook_igrad * rhs_cells\n\n    # Right hand side for boundary discretization\n    rhs_bound = create_bound_rhs(bound, bound_exclusion, subcell_topology, g)\n    # Discretization of boundary values\n    bound_stress = hf2f * hook_igrad * rhs_bound\n    stress, bound_stress = _zero_neu_rows(g, stress, bound_stress, bound)\n\n    return stress, bound_stress",
  "def mpsa_elasticity(g, constit, subcell_topology, bound_exclusion, eta,\n                    inverter):\n    \"\"\"\n    This is the function where the real discretization takes place. It contains\n    the parts that are common for elasticity and poro-elasticity, and was thus\n    separated out as a helper function.\n\n    The steps in the discretization are the same as in mpfa (although with\n    everything being somewhat more complex since this is a vector equation).\n    The mpfa function is currently more clean, so confer that for additional\n    comments.\n\n    Parameters:\n        g: Grid\n        constit: Constitutive law\n        subcell_topology: Wrapper class for numbering of subcell faces, cells\n            etc.\n        bound_exclusion: Object that can eliminate faces related to boundary\n            conditions.\n        eta: Parameter determining the continuity point\n        inverter: Parameter determining which method to use for inverting the\n            local systems\n\n    Returns:\n        hook: Hooks law, ready to be multiplied with inverse gradients\n        igrad: Inverse gradients\n        rhs_cells: Right hand side used to get basis functions in terms of cell\n            center displacements\n        cell_node_blocks: Relation between cells and vertexes, used to group\n            equations in linear system.\n        hook_normal: Hooks law for the term div(I*p) in poro-elasticity\n    \"\"\"\n\n    nd = g.dim\n\n    # Compute product between normal vectors and stiffness matrices\n    ncsym, ncasym, cell_node_blocks, \\\n        sub_cell_index = _tensor_vector_prod(g, constit, subcell_topology)\n\n    # Prepare for computation of forces due to cell center pressures (the term\n    # div(I*p) in poro-elasticity equations. hook_normal will be used as a right\n    # hand side by the biot disretization, but needs to be computed here, since\n    # this is where we have access to the relevant data.\n    ind_f = np.argsort(np.tile(subcell_topology.subhfno, nd), kind='mergesort')\n    hook_normal = sps.coo_matrix((np.ones(ind_f.size),\n                                  (np.arange(ind_f.size), ind_f)),\n                                 shape=(ind_f.size, ind_f.size)) * (ncsym\n                                                                    + ncasym)\n\n    del ind_f\n    # The final expression of Hook's law will involve deformation gradients\n    # on one side of the faces only; eliminate the other one.\n    # Note that this must be done before we can pair forces from the two\n    # sides of the faces.\n    hook = __unique_hooks_law(ncsym, ncasym, subcell_topology, nd)\n\n    del ncasym\n\n    # Pair the forces from each side\n    ncsym = subcell_topology.pair_over_subfaces_nd(ncsym)\n    ncsym = bound_exclusion.exclude_dirichlet_nd(ncsym)\n    num_subfno = subcell_topology.subfno.max() + 1\n    hook_cell = sps.coo_matrix((np.zeros(1), (np.zeros(1), np.zeros(1))),\n                               shape=(num_subfno * nd,\n                                      (np.max(subcell_topology.cno) + 1) *\n                                      nd)).tocsr()\n    hook_cell = bound_exclusion.exclude_dirichlet_nd(hook_cell)\n\n    # Book keeping\n    num_sub_cells = cell_node_blocks[0].size\n\n    d_cont_grad, d_cont_cell = __get_displacement_submatrices(g,\n                                                              subcell_topology,\n                                                              eta,\n                                                              num_sub_cells,\n                                                              bound_exclusion)\n\n    grad_eqs = sps.vstack([ncsym, d_cont_grad])\n    del ncsym, d_cont_grad\n\n    igrad = _inverse_gradient(grad_eqs, sub_cell_index, cell_node_blocks,\n                              subcell_topology.nno_unique, bound_exclusion,\n                              nd, inverter)\n\n    # Right hand side for cell center variables\n    rhs_cells = -sps.vstack([hook_cell, d_cont_cell])\n    return hook, igrad, rhs_cells, cell_node_blocks, hook_normal",
  "def _estimate_peak_memory_mpsa(g):\n    \"\"\" Rough estimate of peak memory need for mpsa discretization.\n    \"\"\"\n    nd = g.dim\n    num_cell_nodes = g.cell_nodes().sum(axis=1).A\n\n    # Number of unknowns around a vertex: nd^2 per cell that share the vertex\n    # for pressure gradients, and one per cell (cell center pressure)\n    num_grad_unknowns = nd**2 * num_cell_nodes\n\n    # The most expensive field is the storage of igrad, which is block diagonal\n    # with num_grad_unknowns sized blocks. The number of elements is the square\n    # of the local system size. The factor 2 accounts for matrix storage in\n    # sparse format (rows and data; ignore columns since this is in compressed\n    # format)\n    igrad_size = np.power(num_grad_unknowns, 2).sum() * 2\n\n    # The discretization of Hook's law will require nd^2 (that is, a gradient)\n    # per sub-face per dimension\n    num_sub_face = g.face_nodes.sum()\n    hook_size = nd * num_sub_face * nd**2\n\n    # Balancing of stresses will require 2*nd**2 (gradient on both sides)\n    # fields per sub-face per dimension\n    nk_grad_size = 2 * nd * num_sub_face * nd**2\n    # Similarly, pressure continuity requires 2 * (nd+1) (gradient on both\n    # sides, and cell center pressures) numbers\n    pr_cont_size = 2 * (nd**2 + 1) * num_sub_face * nd\n\n    total_size = igrad_size + hook_size + nk_grad_size + pr_cont_size\n\n    # Not covered yet is various fields on subcell topology, mapping matrices\n    # between local and block ordering etc.\n    return total_size",
  "def __get_displacement_submatrices(g, subcell_topology, eta, num_sub_cells,\n                                   bound_exclusion):\n    nd = g.dim\n    # Distance from cell centers to face centers, this will be the\n    # contribution from gradient unknown to equations for displacement\n    # continuity\n    d_cont_grad = fvutils.compute_dist_face_cell(g, subcell_topology, eta)\n\n    # For force balance, displacements and stresses on the two sides of the\n    # matrices must be paired\n    d_cont_grad = sps.kron(sps.eye(nd), d_cont_grad)\n\n    # Contribution from cell center potentials to local systems\n    d_cont_cell = __cell_variable_contribution(g, subcell_topology)\n\n    # Expand equations for displacement balance, and eliminate rows\n    # associated with neumann boundary conditions\n    d_cont_grad = bound_exclusion.exclude_neumann_nd(d_cont_grad)\n    d_cont_cell = bound_exclusion.exclude_neumann_nd(d_cont_cell)\n\n    # The column ordering of the displacement equilibrium equations are\n    # formed as a Kronecker product of scalar equations. Bring them to the\n    # same form as that applied in the force balance equations\n    d_cont_grad, d_cont_cell = __rearange_columns_displacement_eqs(\n        d_cont_grad, d_cont_cell, num_sub_cells, nd)\n\n    return d_cont_grad, d_cont_cell",
  "def _split_stiffness_matrix(constit):\n    \"\"\"\n    Split the stiffness matrix into symmetric and asymetric part\n\n    Parameters\n    ----------\n    constit stiffness tensor\n\n    Returns\n    -------\n    csym part of stiffness tensor that enters the local calculation\n    casym part of stiffness matrix not included in local calculation\n    \"\"\"\n    dim = np.sqrt(constit.c.shape[0])\n\n    # We do not know how constit is used outside the discretization,\n    # so create deep copies to avoid overwriting. Not really sure if this is\n    # necessary\n    csym = 0 * constit.copy().c\n    casym = constit.copy().c\n\n    # The copy constructor for the stiffness matrix will represent all\n    # dimensions as 3d. If dim==2, delete the redundant rows and columns\n    if dim == 2 and csym.shape[0] == 9:\n        csym = np.delete(csym, (2, 5, 6, 7, 8), axis=0)\n        csym = np.delete(csym, (2, 5, 6, 7, 8), axis=1)\n        casym = np.delete(casym, (2, 5, 6, 7, 8), axis=0)\n        casym = np.delete(casym, (2, 5, 6, 7, 8), axis=1)\n\n    # The splitting is hard coded based on the ordering of elements in the\n    # stiffness matrix\n    if dim == 2:\n        csym[0, 0] = casym[0, 0]\n        csym[1, 1] = casym[1, 1]\n        csym[2, 2] = casym[2, 2]\n        csym[3, 0] = casym[3, 0]\n        csym[0, 3] = casym[0, 3]\n        csym[3, 3] = casym[3, 3]\n    else:  # dim == 3\n        csym[0, 0] = casym[0, 0]\n        csym[1, 1] = casym[1, 1]\n        csym[2, 2] = casym[2, 2]\n        csym[3, 3] = casym[3, 3]\n        csym[4, 4] = casym[4, 4]\n        csym[5, 5] = casym[5, 5]\n        csym[6, 6] = casym[6, 6]\n        csym[7, 7] = casym[7, 7]\n        csym[8, 8] = casym[8, 8]\n\n        csym[4, 0] = casym[4, 0]\n        csym[8, 0] = casym[8, 0]\n        csym[0, 4] = casym[0, 4]\n        csym[8, 4] = casym[8, 4]\n        csym[0, 8] = casym[0, 8]\n        csym[4, 8] = casym[4, 8]\n    # The asymmetric part is whatever is not in the symmetric part\n    casym -= csym\n    return csym, casym",
  "def _tensor_vector_prod(g, constit, subcell_topology):\n    \"\"\" Compute product between stiffness tensor and face normals.\n\n    The method splits the stiffness matrix into a symmetric and asymmetric\n    part, and computes the products with normal vectors for each. The method\n    also provides a unique identification of sub-cells (in the form of pairs of\n    cells and nodes), and a global numbering of subcell gradients.\n\n    Parameters:\n        g: grid\n        constit: Stiffness matrix, in the form of a fourth order tensor.\n        subcell_topology: Numberings of subcell quantities etc.\n\n    Returns:\n        ncsym, ncasym: Product with face normals for symmetric and asymmetric\n            part of stiffness tensors. On the subcell level. In effect, these\n            will be stresses on subfaces, as functions of the subcell gradients\n            (to be computed somewhere else). The rows first represent stresses\n            in the x-direction for all faces, then y direction etc.\n        cell_nodes_blocks: Unique pairing of cell and node numbers for\n            subcells. First row: Cell numbers, second node numbers. np.ndarray.\n        grad_ind: Numbering scheme for subcell gradients - gives a global\n            numbering for the gradients. One column per subcell, the rows gives\n            the index for the individual components of the gradients.\n\n    \"\"\"\n\n    # Stack cells and nodes, and remove duplicate rows. Since subcell_mapping\n    # defines cno and nno (and others) working cell-wise, this will\n    # correspond to a unique rows (Matlab-style) from what I understand.\n    # This also means that the pairs in cell_node_blocks uniquely defines\n    # subcells, and can be used to index gradients etc.\n    cell_node_blocks, blocksz = matrix_compression.rlencode(np.vstack((\n        subcell_topology.cno, subcell_topology.nno)))\n\n    nd = g.dim\n\n    # Duplicates in [cno, nno] corresponds to different faces meeting at the\n    # same node. There should be exactly nd of these. This test will fail\n    # for pyramids in 3D\n    assert np.all(blocksz == nd)\n\n    # Define row and column indices to be used for normal vector matrix\n    # Rows are based on sub-face numbers.\n    # Columns have nd elements for each sub-cell (to store a vector) and\n    # is adjusted according to block sizes\n    _, cn = np.meshgrid(subcell_topology.subhfno, np.arange(nd))\n    sum_blocksz = np.cumsum(blocksz)\n    cn += matrix_compression.rldecode(sum_blocksz - blocksz[0], blocksz)\n    ind_ptr_n = np.hstack((np.arange(0, cn.size, nd), cn.size))\n\n    # Distribute faces equally on the sub-faces, and store in a matrix\n    num_nodes = np.diff(g.face_nodes.indptr)\n    normals = g.face_normals[:, subcell_topology.fno] / num_nodes[\n        subcell_topology.fno]\n    normals_mat = sps.csr_matrix((normals.ravel('F'), cn.ravel('F'),\n                                  ind_ptr_n))\n\n    # Then row and columns for stiffness matrix. There are nd^2 elements in\n    # the gradient operator, and so the structure is somewhat different from\n    # the normal vectors\n    _, cc = np.meshgrid(subcell_topology.subhfno, np.arange(nd**2))\n    sum_blocksz = np.cumsum(blocksz**2)\n    cc += matrix_compression.rldecode(sum_blocksz - blocksz[0]**2, blocksz)\n    ind_ptr_c = np.hstack((np.arange(0, cc.size, nd**2), cc.size))\n\n    # Splitt stiffness matrix into symmetric and anti-symmatric part\n    sym_tensor, asym_tensor = _split_stiffness_matrix(constit)\n\n    # Getting the right elements out of the constitutive laws was a bit\n    # tricky, but the following code turned out to do the trick\n    sym_tensor_swp = np.swapaxes(sym_tensor, 2, 0)\n    asym_tensor_swp = np.swapaxes(asym_tensor, 2, 0)\n\n    # The first dimension in csym and casym represent the contribution from\n    # all dimensions to the stress in one dimension (in 2D, csym[0:2,:,\n    # :] together gives stress in the x-direction etc.\n    # Define index vector to access the right rows\n    rind = np.arange(nd)\n\n    # Empty matrices to initialize matrix-tensor products. Will be expanded\n    # as we move on\n    zr = np.zeros(0)\n    ncsym = sps.coo_matrix((zr, (zr,\n                                 zr)), shape=(0, cc.max() + 1)).tocsr()\n    ncasym = sps.coo_matrix((zr, (zr, zr)), shape=(0, cc.max() + 1)).tocsr()\n\n    # For the asymmetric part of the tensor, we will apply volume averaging.\n    # Associate a volume with each sub-cell, and a node-volume as the sum of\n    # all surrounding sub-cells\n    num_cell_nodes = g.num_cell_nodes()\n    cell_vol = g.cell_volumes / num_cell_nodes\n    node_vol = np.bincount(subcell_topology.nno, weights=cell_vol[\n        subcell_topology.cno]) / g.dim\n\n    num_elem = cell_node_blocks.shape[1]\n    map_mat = sps.coo_matrix((np.ones(num_elem),\n                              (np.arange(num_elem), cell_node_blocks[1])))\n    weight_mat = sps.coo_matrix((cell_vol[cell_node_blocks[0]] / node_vol[\n        cell_node_blocks[1]], (cell_node_blocks[1], np.arange(num_elem))))\n    # Operator for carying out the average\n    average = sps.kron(map_mat * weight_mat, sps.identity(nd)).tocsr()\n\n    for iter1 in range(nd):\n        # Pick out part of Hook's law associated with this dimension\n        # The code here looks nasty, it should be possible to get the right\n        # format of the submatrices in a simpler way, but I couldn't do it.\n        sym_dim = np.hstack(sym_tensor_swp[:, :, rind]).transpose()\n        asym_dim = np.hstack(asym_tensor_swp[:, :, rind]).transpose()\n\n        # Distribute (relevant parts of) Hook's law on subcells\n        # This will be nd rows, thus cell ci is associated with indices\n        # ci*nd+np.arange(nd)\n        sub_cell_ind = fvutils.expand_indices_nd(cell_node_blocks[0], nd)\n        sym_vals = sym_dim[sub_cell_ind]\n        asym_vals = asym_dim[sub_cell_ind]\n\n        # Represent this part of the stiffness matrix in matrix form\n        csym_mat = sps.csr_matrix((sym_vals.ravel('C'),\n                                   cc.ravel('F'), ind_ptr_c))\n        casym_mat = sps.csr_matrix((asym_vals.ravel('C'),\n                                    cc.ravel('F'), ind_ptr_c))\n\n        # Compute average around vertexes\n        casym_mat = average * casym_mat\n\n        # Compute products of normal vectors and stiffness tensors,\n        # and stack dimensions vertically\n        ncsym = sps.vstack((ncsym, normals_mat * csym_mat))\n        ncasym = sps.vstack((ncasym, normals_mat * casym_mat))\n\n        # Increase index vector, so that we get rows contributing to forces\n        # in the next dimension\n        rind += nd\n\n    grad_ind = cc[:, ::nd]\n\n    return ncsym, ncasym, cell_node_blocks, grad_ind",
  "def _inverse_gradient(grad_eqs, sub_cell_index, cell_node_blocks,\n                      nno_unique, bound_exclusion, nd, inverter):\n\n    # Mappings to convert linear system to block diagonal form\n    rows2blk_diag, cols2blk_diag, size_of_blocks = _block_diagonal_structure(\n        sub_cell_index, cell_node_blocks, nno_unique, bound_exclusion, nd)\n\n    grad = rows2blk_diag * grad_eqs * cols2blk_diag\n\n    # Compute inverse gradient operator, and map back again\n    igrad = cols2blk_diag * fvutils.invert_diagonal_blocks(grad,\n                                                           size_of_blocks,\n                                                           method=inverter) \\\n        * rows2blk_diag\n    return igrad",
  "def _block_diagonal_structure(sub_cell_index, cell_node_blocks, nno,\n                              bound_exclusion, nd):\n    \"\"\"\n    Define matrices to turn linear system into block-diagonal form.\n\n    Parameters\n    ----------\n    sub_cell_index\n    cell_node_blocks: pairs of cell and node pairs, which defines sub-cells\n    nno node numbers associated with balance equations\n    exclude_dirichlet mapping to remove rows associated with stress boundary\n    exclude_neumann mapping to remove rows associated with displacement boundary\n\n    Returns\n    -------\n    rows2blk_diag transform rows of linear system to block-diagonal form\n    cols2blk_diag transform columns of linear system to block-diagonal form\n    size_of_blocks number of equations in each block\n    \"\"\"\n\n    # Stack node numbers of equations on top of each other, and sort them to\n    # get block-structure. First eliminate node numbers at the boundary, where\n    # the equations are either of flux or pressure continuity (not both)\n    nno_stress = bound_exclusion.exclude_dirichlet(nno)\n    nno_displacement = bound_exclusion.exclude_neumann(nno)\n    node_occ = np.hstack((np.tile(nno_stress, nd),\n                          np.tile(nno_displacement, nd)))\n    sorted_ind = np.argsort(node_occ, kind='mergesort')\n    rows2blk_diag = sps.coo_matrix((np.ones(sorted_ind.size),\n                                    (np.arange(sorted_ind.size),\n                                     sorted_ind))).tocsr()\n    # Size of block systems\n    sorted_nodes_rows = node_occ[sorted_ind]\n    size_of_blocks = np.bincount(sorted_nodes_rows.astype('int64'))\n\n    # cell_node_blocks[1] contains the node numbers associated with each\n    # sub-cell gradient (and so column of the local linear systems). A sort\n    # of these will give a block-diagonal structure\n    sorted_nodes_cols = np.argsort(cell_node_blocks[1], kind='mergesort')\n    subcind_nodes = sub_cell_index[::, sorted_nodes_cols].ravel('F')\n    cols2blk_diag = sps.coo_matrix((np.ones(sub_cell_index.size),\n                                    (subcind_nodes,\n                                     np.arange(sub_cell_index.size)))).tocsr()\n    return rows2blk_diag, cols2blk_diag, size_of_blocks",
  "def create_bound_rhs(bound, bound_exclusion, subcell_topology, g):\n    \"\"\"\n    Define rhs matrix to get basis functions for incorporates boundary\n    conditions\n\n    Parameters\n    ----------\n    bound\n    bound_exclusion\n    fno\n    sgn : +-1, defining here and there of the faces\n    g : grid\n    num_stress : number of equations for flux continuity\n    num_displ: number of equations for pressure continuity\n\n    Returns\n    -------\n    rhs_bound: Matrix that can be multiplied with inverse block matrix to get\n               basis functions for boundary values\n    \"\"\"\n    nd = g.dim\n    num_stress = bound_exclusion.exclude_dir.shape[0] * nd\n    num_displ = bound_exclusion.exclude_neu.shape[0] * nd\n    fno = subcell_topology.fno_unique\n    subfno = subcell_topology.subfno_unique\n    sgn = g.cell_faces[subcell_topology.fno_unique,\n                       subcell_topology.cno_unique].A.ravel('F')\n    num_neu = sum(bound.is_neu[fno]) * nd\n    num_dir = sum(bound.is_dir[fno]) * nd\n    num_bound = num_neu + num_dir\n\n    # Convenience method for duplicating a list, with a certain increment\n    def expand_ind(ind, dim, increment):\n        # Duplicate rows\n        ind_nd = np.tile(ind, (dim, 1))\n        # Add same increment to each row (0*incr, 1*incr etc.)\n        ind_incr = ind_nd + increment * np.array([np.arange(dim)]).transpose()\n        # Back to row vector\n        ind_new = ind_incr.reshape(-1, order='F')\n        return ind_new\n\n    # Define right hand side for Neumann boundary conditions\n    # First row indices in rhs matrix\n    is_neu = bound_exclusion.exclude_dirichlet(bound.is_neu[fno].astype(\n        'int64'))\n    neu_ind_single = np.argwhere(is_neu).ravel('F')\n\n    # There are is_neu.size Neumann conditions per dimension\n    neu_ind = expand_ind(neu_ind_single, nd, is_neu.size)\n\n    # We also need to account for all half faces, that is, do not exclude\n    # Dirichlet and Neumann boundaries.\n    neu_ind_single_all = np.argwhere(bound.is_neu[fno].astype('int'))\\\n        .ravel('F')\n    dir_ind_single_all = np.argwhere(bound.is_dir[fno].astype('int'))\\\n        .ravel('F')\n\n    neu_ind_all = np.tile(neu_ind_single_all, nd)\n    # Some care is needed to compute coefficients in Neumann matrix: sgn is\n    # already defined according to the subcell topology [fno], while areas\n    # must be drawn from the grid structure, and thus go through fno\n\n    fno_ext = np.tile(fno, nd)\n    num_face_nodes = g.face_nodes.sum(axis=0).A.ravel('F')\n\n    # Coefficients in the matrix. For the Neumann boundary faces we set the\n    # value as seen from the outside of the domain. Note that they do not\n    # have to do\n    # so, and we will flip the sign later. This means that a stress [1,1] on a\n    # boundary face pushes(or pulls) the face to the top right corner.\n    neu_val = 1 / num_face_nodes[fno_ext[neu_ind_all]]\n    # The columns will be 0:neu_ind.size\n    if neu_ind.size > 0:\n        neu_cell = sps.coo_matrix((neu_val.ravel('F'),\n                                   (neu_ind, np.arange(neu_ind.size))),\n                                  shape=(num_stress, num_bound)).tocsr()\n    else:\n        # Special handling when no elements are found. Not sure if this is\n        # necessary, or if it is me being stupid\n        neu_cell = sps.coo_matrix((num_stress, num_bound)).tocsr()\n\n    # Dirichlet boundary conditions, procedure is similar to that for Neumann\n    is_dir = bound_exclusion.exclude_neumann(bound.is_dir[fno].astype(\n        'int64'))\n    dir_ind_single = np.argwhere(is_dir).ravel('F')\n    dir_ind = expand_ind(dir_ind_single, nd, is_dir.size)\n    # The coefficients in the matrix should be duplicated the same way as\n    # the row indices, but with no increment\n    dir_val = expand_ind(sgn[dir_ind_single_all], nd, 0)\n    # Column numbering starts right after the last Neumann column. dir_val\n    # is ordered [u_x_1, u_y_1, u_x_2, u_y_2, ...], and dir_ind shuffles this\n    # ordering. The final matrix will first have the x-coponent of the displacement\n    # for each face, then the y-component, etc.\n    if dir_ind.size > 0:\n        dir_cell = sps.coo_matrix((dir_val, (dir_ind, num_neu +\n                                             np.arange(dir_ind.size))),\n                                  shape=(num_displ, num_bound)).tocsr()\n    else:\n        # Special handling when no elements are found. Not sure if this is\n        # necessary, or if it is me being stupid\n        dir_cell = sps.coo_matrix((num_displ, num_bound)).tocsr()\n\n    num_subfno = np.max(subfno) + 1\n\n    # The columns in neu_cell, dir_cell are ordered from 0 to num_bound-1.\n    # Map these to all half-face indices\n\n    is_bnd = np.hstack((neu_ind_single_all, dir_ind_single_all))\n    bnd_ind = fvutils.expand_indices_nd(is_bnd, nd)\n    bnd_2_all_hf = sps.coo_matrix((np.ones(num_bound),\n                                   (np.arange(num_bound), bnd_ind)),\n                                  shape=(num_bound, num_subfno * nd))\n    # The user of the discretization should now nothing about half faces,\n    # thus map from half face to face indices.\n    hf_2_f = fvutils.map_hf_2_f(fno, subfno, nd).transpose()\n    # the rows of rhs_bound will be ordered with first the x-component of all\n    # neumann faces, then the y-component of all neumann faces, then the\n    # z-component of all neumann faces. Then we will have the equivalent for\n    # the dirichlet faces.\n    rhs_bound = sps.vstack([neu_cell, dir_cell]) * bnd_2_all_hf * hf_2_f\n\n    return rhs_bound",
  "def __unique_hooks_law(csym, casym, subcell_topology, nd):\n    \"\"\"\n    Go from products of normal vectors with stiffness matrices (symmetric\n    and asymmetric), covering both sides of faces, to a discrete Hook's law,\n    that, when multiplied with sub-cell gradients, will give face stresses\n\n    Parameters\n    ----------\n    csym\n    casym\n    unique_sub_fno\n    subfno\n    nd\n\n    Returns\n    -------\n    hook (sps.csr) nd * (nsubfno, ncells)\n    \"\"\"\n    # unique_sub_fno covers scalar equations only. Extend indices to cover\n    # multiple dimensions\n    num_eqs = csym.shape[0] / nd\n    ind_single = np.tile(subcell_topology.unique_subfno, (nd, 1))\n    increments = np.arange(nd) * num_eqs\n    ind_all = np.reshape(ind_single + increments[:, np.newaxis], -1)\n\n    # Unique part of symmetric and asymmetric products\n    hook_sym = csym[ind_all, ::]\n    hook_asym = casym[ind_all, ::]\n\n    # Hook's law, as it comes out of the normal-vector * stiffness matrix is\n    # sorted with x-component balances first, then y-, etc. Sort this to a\n    # face-wise ordering\n    comp2face_ind = np.argsort(np.tile(subcell_topology.subfno_unique, nd),\n                               kind='mergesort')\n    comp2face = sps.coo_matrix((np.ones(comp2face_ind.size),\n                                (np.arange(comp2face_ind.size),\n                                 comp2face_ind)),\n                               shape=(comp2face_ind.size, comp2face_ind.size))\n    hook = comp2face * (hook_sym + hook_asym)\n\n    return hook",
  "def __cell_variable_contribution(g, subcell_topology):\n    \"\"\"\n    Construct contribution from cell center variables to local systems.\n    For stress equations, these are zero, while for cell centers it is +- 1\n    Parameters\n    ----------\n    g\n    fno\n    cno\n    subfno\n\n    Returns\n    -------\n\n    \"\"\"\n    nd = g.dim\n    sgn = g.cell_faces[subcell_topology.fno, subcell_topology.cno].A\n\n    # Contribution from cell center potentials to local systems\n    # For pressure continuity, +-1\n    d_cont_cell = sps.coo_matrix((sgn[0], (subcell_topology.subfno,\n                                           subcell_topology.cno))).tocsr()\n    d_cont_cell = sps.kron(sps.eye(nd), d_cont_cell)\n    # Zero contribution to stress continuity\n\n    return d_cont_cell",
  "def __rearange_columns_displacement_eqs(d_cont_grad, d_cont_cell,\n                                        num_sub_cells, nd):\n    \"\"\" Transform columns of displacement balance from increasing cell\n    ordering (first x-variables of all cells, then y) to increasing\n    variables (first all variables of the first cells, then...)\n\n    Parameters\n    ----------\n    d_cont_grad\n    d_cont_cell\n    num_sub_cells\n    nd\n    cno\n\n    Returns\n    -------\n\n    \"\"\"\n    # Repeat sub-cell indices nd times. Fortran ordering (column major)\n    # gives same ordering of indices as used for the scalar equation (where\n    # there are nd gradient variables for each sub-cell), and thus the\n    # format of each block in d_cont_grad\n    rep_ci_single_blk = np.tile(np.arange(num_sub_cells),\n                                (nd, 1)).reshape(-1, order='F')\n    # Then repeat the single-block indices nd times (corresponding to the\n    # way d_cont_grad is constructed by Kronecker product), and find the\n    # sorting indices\n    d_cont_grad_map = np.argsort(np.tile(rep_ci_single_blk, nd),\n                                 kind='mergesort')\n    # Use sorting indices to bring d_cont_grad to the same order as that\n    # used for the columns in the stress continuity equations\n    d_cont_grad = d_cont_grad[:, d_cont_grad_map]\n\n    # For the cell displacement variables, we only need a single expansion (\n    # corresponding to the second step for the gradient unknowns)\n    num_cells = d_cont_cell.shape[1] / nd\n    d_cont_cell_map = np.argsort(np.tile(np.arange(num_cells), nd),\n                                 kind='mergesort')\n    d_cont_cell = d_cont_cell[:, d_cont_cell_map]\n    return d_cont_grad, d_cont_cell",
  "def _neu_face_sgn(g, neu_ind):\n    neu_sgn = (g.cell_faces[neu_ind, :]).data\n    assert neu_sgn.size == neu_ind.size, \\\n        'A normal sign is only well defined for a boundary face'\n    sort_id = np.argsort(g.cell_faces[neu_ind, :].indices)\n    return neu_sgn[sort_id]",
  "def _zero_neu_rows(g, stress, bound_stress, bnd):\n    \"\"\"\n    We zero out all none-diagonal elements for the neumann boundary faces.\n    \"\"\"\n    neu_face_x = g.dim * np.ravel(np.argwhere(bnd.is_neu))\n    if g.dim == 1:\n        neu_face_ind = neu_face_x\n    elif g.dim == 2:\n        neu_face_y = neu_face_x + 1\n        neu_face_ind = np.ravel((neu_face_x, neu_face_y), 'F')\n    elif g.dim == 3:\n        neu_face_y = neu_face_x + 1\n        neu_face_z = neu_face_x + 2\n        neu_face_ind = np.ravel((neu_face_x, neu_face_y, neu_face_z), 'F')\n    else:\n        raise ValueError('Only support for dimension 1, 2, or 3')\n    num_neu = neu_face_ind.size\n\n    if not num_neu:\n        return stress, bound_stress\n    # Frist we zero out the boundary stress. We keep the sign of the diagonal\n    # element, however we discard its value (e.g. set it to +-1). The sign\n    # should be negative if the nomral vector points outwards and positive if\n    # the normal vector points inwards. I'm not sure if this is correct (that\n    # is, zeroing out none-diagonal elements and putting the diagonal elements\n    # to +-1), but it seems to give satisfactory results.\n    sgn = np.sign(np.ravel(bound_stress[neu_face_ind, neu_face_ind]))\n    # Set all neumann rows to zero\n    bound_stress = fvutils.zero_out_sparse_rows(bound_stress, neu_face_ind,\n                                                sgn)\n    # For the stress matrix we zero out any rows corresponding to the Neumann\n    # boundary faces (these have been moved over to the bound_stress matrix).\n    stress = fvutils.zero_out_sparse_rows(stress, neu_face_ind)\n\n    return stress, bound_stress",
  "def _sign_matrix(g, faces):\n    # We find the sign of the given faces\n    IA = np.argsort(faces)\n    IC = np.argsort(IA)\n\n    fi, _, sgn_d = sps.find(g.cell_faces[faces[IA], :])\n    I = np.argsort(fi)\n    sgn_d = sgn_d[I]\n    sgn_d = sgn_d[IC]\n    sgn_d = np.ravel([sgn_d] * g.dim, 'F')\n\n    sgn = sps.diags(sgn_d, 0)\n\n    return sgn",
  "def __init__(self, physics='mechanics'):\n        self.physics = physics",
  "def ndof(self, g):\n        \"\"\"\n        Return the number of degrees of freedom associated to the method.\n        In this case number of cells times dimension (stress dof).\n\n        Parameter\n        ---------\n        g: grid, or a subclass.\n\n        Return\n        ------\n        dof: the number of degrees of freedom.\n\n        \"\"\"\n        return g.dim * g.num_cells",
  "def matrix_rhs(self, g, data, discretize=True):\n        \"\"\"\n        Return the matrix and right-hand side for a discretization of a second\n        order elliptic equation using a FV method with a multi-point stress\n        approximation.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data. For details on necessary keywords,\n            see method discretize()\n        discretize (boolean, optional): default True. Whether to discetize\n            prior to matrix assembly. If False, data should already contain\n            discretization.\n\n        Return\n        ------\n        matrix: sparse csr (g.dim * g_num_cells, g.dim * g_num_cells)\n            Discretization matrix.\n        rhs: array (g.dim * g_num_cells)\n            Right-hand side which contains the boundary conditions and the scalar\n            source term.\n        \"\"\"\n        param = data['param']\n\n        if discretize:\n            self.discretize(g, data)\n        div = fvutils.vector_divergence(g)\n        stress = data['stress']\n        bound_stress = data['bound_stress']\n        M = div * stress\n\n        f = data['param'].get_source(self)\n        bc_val = data['param'].get_bc_val(self)\n\n        return M, self.rhs(g, bound_stress, bc_val, f)",
  "def discretize(self, g, data):\n        \"\"\"\n        Discretize the vector elliptic equation by the multi-point stress\n\n        The method computes fluxes over faces in terms of displacements in\n        adjacent cells (defined as the two cells sharing the face).\n\n        The name of data in the input dictionary (data) are:\n        param : Parameter(Class). Contains the following parameters:\n            tensor : fourth_order_tensor\n                Permeability defined cell-wise. If not given a identity permeability\n                is assumed and a warning arised.\n            bc : boundary conditions (optional)\n            bc_val : dictionary (optional)\n                Values of the boundary conditions. The dictionary has at most the\n                following keys: 'dir' and 'neu', for Dirichlet and Neumann boundary\n                conditions, respectively.\n            apertures : (np.ndarray) (optional) apertures of the cells for scaling of\n                the face normals.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data.\n        \"\"\"\n\n        c = data['param'].get_tensor(self)\n        bnd = data['param'].get_bc(self)\n\n        stress, bound_stress = mpsa(g, c, bnd)\n\n        data['stress'] = stress\n        data['bound_stress'] = bound_stress",
  "def rhs(self, g, bound_stress, bc_val, f):\n        \"\"\"\n        Return the righ-hand side for a discretization of a second order elliptic\n        equation using the MPSA method. See self.matrix_rhs for a detailed\n        description.\n        \"\"\"\n        div = fvutils.vector_divergence(g)\n\n        return -div * bound_stress * bc_val - f",
  "def __init__(self, **kwargs):\n        Mpsa.__init__(self, **kwargs)\n        assert hasattr(self, 'physics'), 'Mpsa must assign physics'",
  "def ndof(self, g):\n        \"\"\"\n        Return the number of degrees of freedom associated to the method.\n        In this case number of cells times dimension (stress dof).\n\n        Parameter\n        ---------\n        g: grid, or a subclass.\n\n        Return\n        ------\n        dof: the number of degrees of freedom.\n\n        \"\"\"\n        num_fracs = np.sum(g.tags['fracture_faces'])\n        return g.dim * (g.num_cells + num_fracs)",
  "def matrix_rhs(self, g, data, discretize=True):\n        \"\"\"\n        Return the matrix and right-hand side for a discretization of a second\n        order elliptic equation using a FV method with a multi-point stress\n        approximation with dofs added on the fracture interfaces.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data. For details on necessary keywords,\n            see method discretize()\n        discretize (boolean, optional): default True. Whether to discetize\n            prior to matrix assembly. If False, data should already contain\n            discretization.\n\n        Return\n        ------\n        matrix: sparse csr (g.dim * g_num_cells + 2 * {#of fracture faces},\n                            2 * {#of fracture faces})\n            Discretization matrix.\n        rhs: array (g.dim * g_num_cells  + g.dim * num_frac_faces)\n            Right-hand side which contains the boundary conditions and the scalar\n            source term.\n        \"\"\"\n        if discretize:\n            self.discretize_fractures(g, data)\n\n        stress = data['stress']\n        bound_stress = data['bound_stress']\n        b_e = data['b_e']\n        A_e = data['A_e']\n\n        L, b_l = self.given_slip_distance(g, stress, bound_stress)\n\n        bc_val = data['param'].get_bc_val(self)\n\n        frac_faces = np.matlib.repmat(g.tags['fracture_faces'], 3, 1)\n        assert np.all(bc_val[frac_faces.ravel('F')] == 0), \\\n            '''Fracture should have zero boundary condition. Set slip by\n               Parameters.set_slip_distance'''\n\n        slip_distance = data['param'].get_slip_distance()\n\n        A = sps.vstack((A_e, L), format='csr')\n        rhs = np.hstack((b_e * bc_val, b_l * slip_distance))\n\n        return A, rhs",
  "def traction(self, g, data, sol):\n        \"\"\"\n        Extract the traction on the faces from fractured fv solution.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        sol : array (g.dim * (g.num_cells + {#of fracture faces}))\n            Solution, stored as [cell_disp, fracture_disp]\n\n        Return\n        ------\n        T : array (g.dim * g.num_faces)\n            traction on each face\n\n        \"\"\"\n        bc_val = data['param'].get_bc_val(self.physics).copy()\n        frac_disp = self.extract_frac_u(g, sol)\n        cell_disp = self.extract_u(g, sol)\n\n        frac_faces = (g.frac_pairs).ravel('C')\n        frac_ind = mcolon.mcolon(\n            g.dim * frac_faces, g.dim * frac_faces + g.dim)\n\n        bc_val[frac_ind] = frac_disp\n\n        T = data['stress'] * cell_disp + data['bound_stress'] * bc_val\n        return T",
  "def extract_u(self, g, sol):\n        \"\"\"  Extract the cell displacement from fractured fv solution.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        sol : array (g.dim * (g.num_cells + {#of fracture faces}))\n            Solution, stored as [cell_disp, fracture_disp]\n\n        Return\n        ------\n        u : array (g.dim * g.num_cells)\n            displacement at each cell\n\n        \"\"\"\n        # pylint: disable=invalid-name\n        return sol[:g.dim * g.num_cells]",
  "def extract_frac_u(self, g, sol):\n        \"\"\"  Extract the fracture displacement from fractured fv solution.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        sol : array (g.dim * (g.num_cells + {#of fracture faces}))\n            Solution, stored as [cell_disp, fracture_disp]\n\n        Return\n        ------\n        u : array (g.dim *{#of fracture faces})\n            displacement at each fracture face\n\n        \"\"\"\n        # pylint: disable=invalid-name\n        return sol[g.dim * g.num_cells:]",
  "def discretize_fractures(self, g, data, faces=None, **kwargs):\n        \"\"\"\n        Discretize the vector elliptic equation by the multi-point stress and added \n        degrees of freedom on the fracture faces\n\n        The method computes fluxes over faces in terms of displacements in\n        adjacent cells (defined as the two cells sharing the face).\n\n        The name of data in the input dictionary (data) are:\n        param : Parameter(Class). Contains the following parameters:\n            tensor : fourth_order_tensor\n                Permeability defined cell-wise. If not given a identity permeability\n                is assumed and a warning arised.\n            bc : boundary conditions (optional)\n            bc_val : dictionary (optional)\n                Values of the boundary conditions. The dictionary has at most the\n                following keys: 'dir' and 'neu', for Dirichlet and Neumann boundary\n                conditions, respectively.\n            apertures : (np.ndarray) (optional) apertures of the cells for scaling of\n                the face normals.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data.\n        \"\"\"\n\n        #    dir_bound = g.get_all_boundary_faces()\n        #    bound = bc.BoundaryCondition(g, dir_bound, ['dir'] * dir_bound.size)\n\n        frac_faces = g.tags['fracture_faces']\n\n        bound = data['param'].get_bc(self)\n        is_dir = bound.is_dir\n        if not np.all(is_dir[frac_faces]):\n            is_dir[frac_faces] = True\n\n        bound = bc.BoundaryCondition(g, is_dir, 'dir')\n\n        assert np.all(bound.is_dir[frac_faces]), \\\n            'fractures must be set as dirichlet boundary faces'\n\n        # Discretize with normal mpsa\n        self.discretize(g, data, **kwargs)\n        stress, bound_stress = data['stress'], data['bound_stress']\n        # Create A and rhs\n        div = fvutils.vector_divergence(g)\n        a = div * stress\n        b = div * bound_stress\n\n        # we find the matrix indices of the fracture\n        if faces is None:\n            frac_faces = g.frac_pairs\n            frac_faces_left = frac_faces[0]\n            frac_faces_right = frac_faces[1]\n        else:\n            raise NotImplementedError('not implemented given faces')\n\n        int_b_left = mcolon.mcolon(\n            g.dim * frac_faces_left, g.dim * frac_faces_left + g.dim)\n        int_b_right = mcolon.mcolon(\n            g.dim * frac_faces_right, g.dim * frac_faces_right + g.dim)\n        int_b_ind = np.ravel((int_b_left, int_b_right), 'C')\n\n        # We find the sign of the left and right faces.\n        sgn_left = _sign_matrix(g, frac_faces_left)\n        sgn_right = _sign_matrix(g, frac_faces_right)\n        # The displacement on the internal boundary face are considered unknowns,\n        # so we move them over to the lhs. The rhs now only consists of the\n        # external boundary faces\n        b_internal = b[:, int_b_ind]\n        b_external = b.copy()\n        sparse_mat.zero_columns(b_external, int_b_ind)\n\n        bound_stress_external = bound_stress.copy().tocsc()\n        sparse_mat.zero_columns(bound_stress_external, int_b_ind)\n        # We assume that the traction on the left hand side is equal but\n        # opisite\n\n        frac_stress_diff = (sgn_left * bound_stress[int_b_left, :] +\n                            sgn_right * bound_stress[int_b_right, :])[:, int_b_ind]\n        internal_stress = sps.hstack(\n            (sgn_left * stress[int_b_left, :] + sgn_right * stress[int_b_right, :],\n             frac_stress_diff))\n\n        A = sps.vstack((sps.hstack((a, b_internal)),\n                        internal_stress), format='csr')\n        # negative sign since we have moved b_external from lhs to rhs\n        d_b = -b_external\n        # sps.csr_matrix((int_b_left.size, g.num_faces * g.dim))\n        d_t = -sgn_left * bound_stress_external[int_b_left] \\\n            - sgn_right * bound_stress_external[int_b_right]\n\n        b_matrix = sps.vstack((d_b, d_t), format='csr')\n\n        data['b_e'] = b_matrix\n        data['A_e'] = A",
  "def given_traction(self, g, stress, bound_stress, faces=None, **kwargs):\n        # we find the matrix indices of the fracture\n        if faces is None:\n            frac_faces = g.frac_pairs\n            frac_faces_left = frac_faces[0]\n            frac_faces_right = frac_faces[1]\n        else:\n            raise NotImplementedError('not implemented given faces')\n\n        int_b_left = mcolon.mcolon(\n            g.dim * frac_faces_left, g.dim * frac_faces_left + g.dim)\n        int_b_right = mcolon.mcolon(\n            g.dim * frac_faces_right, g.dim * frac_faces_right + g.dim)\n        int_b_ind = np.ravel((int_b_left, int_b_right), 'C')\n\n        # We find the sign of the left and right faces.\n        sgn_left = _sign_matrix(g, frac_faces_left)\n        sgn_right = _sign_matrix(g, frac_faces_right)\n\n        # We obtain the stress from boundary conditions on the domain boundary\n        bound_stress_external = bound_stress.copy()\n        bound_stress_external[:, int_b_ind] = 0\n\n        # We construct the L matrix, i.e., we set the traction on the left\n        # fracture side\n        frac_stress = (sgn_left * bound_stress[int_b_left, :])[:, int_b_ind]\n\n        L = sps.hstack((sgn_left * stress[int_b_left, :], frac_stress))\n\n        # negative sign since we have moved b_external from lhs to rhs\n        d_f = sps.csr_matrix((np.ones(int_b_left.size),\n                              (np.arange(int_b_left.size), int_b_left)),\n                             (int_b_left.size, g.num_faces * g.dim))\n        d_t = sgn_left * bound_stress_external[int_b_left]  # \\\n    #        + sgn_right * bound_stress_external[int_b_right]\n\n        return L, d_t",
  "def given_slip_distance(self, g, stress, bound_stress, faces=None):\n        # we find the matrix indices of the fracture\n        if faces is None:\n            frac_faces = g.frac_pairs\n            frac_faces_left = frac_faces[0]\n            frac_faces_right = frac_faces[1]\n        else:\n            raise NotImplementedError('not implemented given faces')\n\n        int_b_left = mcolon.mcolon(\n            g.dim * frac_faces_left, g.dim * frac_faces_left + g.dim)\n        int_b_right = mcolon.mcolon(\n            g.dim * frac_faces_right, g.dim * frac_faces_right + g.dim)\n        int_b_ind = np.ravel((int_b_left, int_b_right), 'C')\n\n        # We construct the L matrix, by assuming that the relative displacement\n        # is given\n        L = sps.hstack((sps.csr_matrix((int_b_left.size, g.dim * g.num_cells)),\n                        sps.identity(int_b_left.size),\n                        -sps.identity(int_b_right.size)))\n\n        d_f = sps.csr_matrix((np.ones(int_b_left.size),\n                              (np.arange(int_b_left.size), int_b_left)),\n                             (int_b_left.size, g.num_faces * g.dim))\n\n        return L, d_f",
  "def expand_ind(ind, dim, increment):\n        # Duplicate rows\n        ind_nd = np.tile(ind, (dim, 1))\n        # Add same increment to each row (0*incr, 1*incr etc.)\n        ind_incr = ind_nd + increment * np.array([np.arange(dim)]).transpose()\n        # Back to row vector\n        ind_new = ind_incr.reshape(-1, order='F')\n        return ind_new",
  "class UpwindMixedDim(SolverMixedDim):\n\n    def __init__(self, physics='transport'):\n        self.physics = physics\n\n        self.discr = Upwind(self.physics)\n        self.discr_ndof = self.discr.ndof\n        self.coupling_conditions = UpwindCoupling(self.discr)\n\n        self.solver = Coupler(self.discr, self.coupling_conditions)\n\n    def cfl(self, gb):\n        deltaT = gb.apply_function(self.discr.cfl,\n                                   self.coupling_conditions.cfl).data\n        return np.amin(deltaT)\n\n    def outflow(self, gb):\n        def bind(g, d):\n            return self.discr.outflow(g, d), np.zeros(g.num_cells)\n        return Coupler(self.discr, solver_fct=bind).matrix_rhs(gb)[0]",
  "class Upwind(Solver):\n    \"\"\"\n    Discretize a hyperbolic transport equation using a single point upstream\n    weighting scheme.\n\n\n    \"\"\"\n#------------------------------------------------------------------------------#\n\n    def __init__(self, physics='transport'):\n        self.physics = physics\n\n#------------------------------------------------------------------------------#\n\n    def ndof(self, g):\n        \"\"\"\n        Return the number of degrees of freedom associated to the method.\n        In this case number of cells (concentration dof).\n\n        Parameter\n        ---------\n        g: grid, or a subclass.\n\n        Return\n        ------\n        dof: the number of degrees of freedom.\n\n        \"\"\"\n        return g.num_cells\n\n#------------------------------------------------------------------------------#\n\n    def matrix_rhs(self, g, data, d_name='discharge'):\n        \"\"\"\n        Return the matrix and righ-hand side for a discretization of a scalar\n        linear transport problem using the upwind scheme.\n        Note: the vector field is assumed to be given as the normal velocity,\n        weighted with the face area, at each face.\n        Note: if not specified the inflow boundary conditions are no-flow, while\n        the outflow boundary conditions are open.\n\n        The name of data in the input dictionary (data) are:\n        discharge : array (g.num_faces)\n            Normal velocity at each face, weighted by the face area.\n        bc : boundary conditions (optional)\n        bc_val : dictionary (optional)\n            Values of the boundary conditions. The dictionary has at most the\n            following keys: 'dir' and 'neu', for Dirichlet and Neumann boundary\n            conditions, respectively.\n        source : array (g.num_cells) of source (positive) or sink (negative) terms.\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data.\n        d_name: (string) keyword for data field in data containing the dischages\n\n        Return\n        ------\n        matrix: sparse csr (g.num_cells, g_num_cells)\n            Upwind matrix obtained from the discretization.\n        rhs: array (g_num_cells)\n            Right-hand side which contains the boundary conditions.\n\n        Examples\n        --------\n        data = {'discharge': u, 'bc': bnd, 'bc_val': bnd_val}\n        advect = upwind.Upwind()\n        U, rhs = advect.matrix_rhs(g, data)\n\n        data = {'deltaT': advect.cfl(g, data)}\n        M, _ = mass.MassMatrix().matrix_rhs(g, data)\n\n        M_minus_U = M - U\n        invM = mass.MassMatrix().inv(M)\n\n        # Loop over the time\n        for i in np.arange( N ):\n            conc = invM.dot((M_minus_U).dot(conc) + rhs)\n\n        \"\"\"\n        if g.dim == 0:\n            return sps.csr_matrix([0]), [0]\n\n        param = data['param']\n        discharge = data[d_name]\n        bc = param.get_bc(self)\n        bc_val = param.get_bc_val(self)\n\n        has_bc = not(bc is None or bc_val is None)\n\n        # Compute the face flux respect to the real direction of the normals\n        indices = g.cell_faces.indices\n        flow_faces = g.cell_faces.copy()\n        flow_faces.data *= discharge[indices]\n\n        # Retrieve the faces boundary and their numeration in the flow_faces\n        # We need to impose no-flow for the inflow faces without boundary\n        # condition\n        mask = np.unique(indices, return_index=True)[1]\n        bc_neu = g.get_all_boundary_faces()\n\n        if has_bc:\n            # If boundary conditions are imposed remove the faces from this\n            # procedure.\n            # For primal-like discretizations, internal boundaries\n            # are handled by assigning Neumann conditions.\n            is_dir = np.logical_and(bc.is_dir, np.logical_not(bc.is_internal))\n            bc_dir = np.where(is_dir)[0]\n            bc_neu = np.setdiff1d(bc_neu, bc_dir, assume_unique=True)\n            bc_dir = mask[bc_dir]\n\n            # Remove Dirichlet inflow\n            inflow = flow_faces.copy()\n\n            inflow.data[bc_dir] = inflow.data[bc_dir].clip(max=0)\n            flow_faces.data[bc_dir] = flow_faces.data[bc_dir].clip(min=0)\n\n        # Remove all Neumann\n        bc_neu = mask[bc_neu]\n        flow_faces.data[bc_neu] = 0\n\n        # Determine the outflow faces\n        if_faces = flow_faces.copy()\n        if_faces.data = np.sign(if_faces.data)\n\n        # Compute the inflow/outflow related to the cells of the problem\n        flow_faces.data = flow_faces.data.clip(min=0)\n        flow_cells = if_faces.transpose() * flow_faces\n        flow_cells.tocsr()\n\n        if not has_bc:\n            return flow_cells, np.zeros(g.num_cells)\n\n        # Impose the boundary conditions\n        bc_val_dir = np.zeros(g.num_faces)\n        if np.any(bc.is_dir):\n            is_dir = np.where(bc.is_dir)[0]\n            bc_val_dir[is_dir] = bc_val[is_dir]\n\n        # We assume that for Neumann boundary condition a positive 'bc_val'\n        # represents an outflow for the domain. A negative 'bc_val' represents\n        # an inflow for the domain.\n        bc_val_neu = np.zeros(g.num_faces)\n        if np.any(bc.is_neu):\n            is_neu = np.where(bc.is_neu)[0]\n            bc_val_neu[is_neu] = bc_val[is_neu]\n\n        return flow_cells, - inflow.transpose() * bc_val_dir \\\n            - np.abs(g.cell_faces.transpose()) * bc_val_neu\n\n#------------------------------------------------------------------------------#\n\n    def cfl(self, g, data, d_name='discharge'):\n        \"\"\"\n        Return the time step according to the CFL condition.\n        Note: the vector field is assumed to be given as the normal velocity,\n        weighted with the face area, at each face.\n\n        The name of data in the input dictionary (data) are:\n        discharge : array (g.num_faces)\n            Normal velocity at each face, weighted by the face area.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data.\n        d_name: (string) keyword for dischagre file in data dictionary\n\n        Return\n        ------\n        deltaT: time step according to CFL condition.\n\n        \"\"\"\n        if g.dim == 0:\n            return np.inf\n        # Retrieve the data, only \"discharge\" is mandatory\n        param = data['param']\n        discharge = data[d_name]\n        aperture = param.get_aperture()\n        phi = param.get_porosity()\n\n        faces, cells, _ = sps.find(g.cell_faces)\n\n        # Detect and remove the faces which have zero in discharge\n        not_zero = ~np.isclose(np.zeros(faces.size), discharge[faces], atol=0)\n        if not np.any(not_zero):\n            return np.inf\n\n        cells = cells[not_zero]\n        faces = faces[not_zero]\n\n        # Compute discrete distance cell to face centers\n        dist_vector = g.face_centers[:, faces] - g.cell_centers[:, cells]\n        # Element-wise scalar products between the distance vectors and the\n        # normals\n        dist = np.einsum('ij,ij->j', dist_vector, g.face_normals[:, faces])\n        # Since discharge is multiplied by the aperture, we get rid of it!!!!\n        # Additionally we consider the phi (porosity) and the cell-mapping\n        coeff = (aperture * phi)[cells]\n        # deltaT is deltaX/discharge with coefficient\n        return np.amin(np.abs(np.divide(dist, discharge[faces])) * coeff)\n\n#------------------------------------------------------------------------------#\n\n    def discharge(self, g, beta, cell_apertures=None):\n        \"\"\"\n        Return the normal component of the velocity, for each face, weighted by\n        the face area and aperture.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        beta: (3x1) array which represents the constant velocity.\n        cell_apertures: (g.num_faces) array of apertures\n\n        Return\n        ------\n        discharge : array (g.num_faces)\n            Normal velocity at each face, weighted by the face area.\n\n        \"\"\"\n        if cell_apertures is None:\n            face_apertures = np.ones(g.num_faces)\n        else:\n            face_apertures = abs(g.cell_faces) * cell_apertures\n            r, _, _ = sps.find(g.cell_faces)\n            face_apertures = face_apertures / np.bincount(r)\n\n        beta = np.asarray(beta)\n        assert beta.size == 3\n\n        if g.dim == 0:\n            dot_prod = np.dot(g.face_normals.ravel('F'), face_apertures * beta)\n            return np.atleast_1d(dot_prod)\n\n        return np.array([np.dot(n, a * beta)\n                         for n, a in zip(g.face_normals.T, face_apertures)])\n\n#------------------------------------------------------------------------------#\n\n    def outflow(self, g, data, d_name='discharge'):\n        if g.dim == 0:\n            return sps.csr_matrix([0])\n\n        param = data['param']\n        discharge = data[d_name]\n        bc = param.get_bc(self)\n        bc_val = param.get_bc_val(self)\n\n        has_bc = not(bc is None or bc_val is None)\n\n        # Compute the face flux respect to the real direction of the normals\n        indices = g.cell_faces.indices\n        flow_faces = g.cell_faces.copy()\n        flow_faces.data *= discharge[indices]\n\n        # Retrieve the faces boundary and their numeration in the flow_faces\n        # We need to impose no-flow for the inflow faces without boundary\n        # condition\n        mask = np.unique(indices, return_index=True)[1]\n        bc_neu = g.tags['domain_boundary_faces'].nonzero()[0]\n\n        if has_bc:\n            # If boundary conditions are imposed remove the faces from this\n            # procedure.\n            bc_dir = np.where(bc.is_dir)[0]\n            bc_neu = np.setdiff1d(bc_neu, bc_dir, assume_unique=True)\n            bc_dir = mask[bc_dir]\n\n            # Remove Dirichlet inflow\n            inflow = flow_faces.copy()\n\n            inflow.data[bc_dir] = inflow.data[bc_dir].clip(max=0)\n            flow_faces.data[bc_dir] = flow_faces.data[bc_dir].clip(min=0)\n\n        # Remove all Neumann\n        bc_neu = mask[bc_neu]\n        flow_faces.data[bc_neu] = 0\n\n        # Determine the outflow faces\n        if_faces = flow_faces.copy()\n        if_faces.data = np.sign(if_faces.data)\n\n        outflow_faces = if_faces.indices[if_faces.data > 0]\n        domain_boundary_faces = g.tags['domain_boundary_faces'].nonzero()[0]\n        outflow_faces = np.intersect1d(outflow_faces,\n                                       domain_boundary_faces,\n                                       assume_unique=True)\n\n        # va tutto bene se ho neumann omogeneo\n        # gli outflow sono positivi\n\n        if_outflow_faces = if_faces.copy()\n        if_outflow_faces.data[:] = 0\n        if_outflow_faces.data[np.in1d(if_faces.indices, outflow_faces)] = 1\n\n        if_outflow_cells = if_outflow_faces.transpose() * flow_faces\n        if_outflow_cells.tocsr()\n\n        return if_outflow_cells",
  "class UpwindCoupling(AbstractCoupling):\n\n    #------------------------------------------------------------------------------#\n\n    def __init__(self, discr):\n        self.discr_ndof = discr.ndof\n\n#------------------------------------------------------------------------------#\n\n    def matrix_rhs(self, g_h, g_l, data_h, data_l, data_edge, d_name='discharge'):\n        \"\"\"\n        Construct the matrix (and right-hand side) for the coupling conditions.\n        Note: the right-hand side is not implemented now.\n\n        Parameters:\n            g_h: grid of higher dimension\n            g_l: grid of lower dimension\n            data_h: dictionary which stores the data for the higher dimensional\n                grid\n            data_l: dictionary which stores the data for the lower dimensional\n                grid\n            data: dictionary which stores the data for the edges of the grid\n                bucket\n\n        Returns:\n            cc: block matrix which store the contribution of the coupling\n                condition. See the abstract coupling class for a more detailed\n                description.\n        \"\"\"\n\n        # Normal component of the velocity from the higher dimensional grid\n        discharge = data_edge[d_name]\n\n        # Retrieve the number of degrees of both grids\n        # Create the block matrix for the contributions\n        dof, cc = self.create_block_matrix(g_h, g_l)\n\n        # 1d-1d\n        if g_h.dim == g_l.dim:\n            # Remember that face_cells are really cell-cell connections\n            # in this case\n            cells_l, cells_h = data_edge['face_cells'].nonzero()\n            diag_cc11 = np.zeros(g_l.num_cells)\n            diag_cc00 = np.zeros(g_h.num_cells)\n            d_00 = np.bincount(cells_h, np.sign(discharge.clip(min=0)) * discharge,\n                               minlength=g_h.num_cells)\n            d_11 = np.bincount(cells_l, np.sign(discharge.clip(max=0)) * discharge,\n                               minlength=g_l.num_cells)\n            np.add.at(diag_cc00, range(g_h.num_cells), d_00)\n            np.add.at(diag_cc11, range(g_l.num_cells), d_11)\n        else:\n            # Recover the information for the grid-grid mapping\n            cells_l, faces_h, _ = sps.find(data_edge['face_cells'])\n\n            # Recover the correct sign for the velocity\n            faces, _, sgn = sps.find(g_h.cell_faces)\n            sgn = sgn[np.unique(faces, return_index=True)[1]]\n            discharge = sgn[faces_h] * discharge[faces_h]\n\n            # Determine which are the corresponding cells of the faces_h\n            cell_faces_h = g_h.cell_faces.tocsr()[faces_h, :]\n            cells_h = cell_faces_h.nonzero()[1]\n\n            diag_cc11 = np.zeros(g_l.num_cells)\n            np.add.at(diag_cc11, cells_l, np.sign(\n                discharge.clip(max=0)) * discharge)\n\n            diag_cc00 = np.zeros(g_h.num_cells)\n            np.add.at(diag_cc00, cells_h, np.sign(\n                discharge.clip(min=0)) * discharge)\n        # Compute the outflow from the higher to the lower dimensional grid\n        cc[1, 0] = sps.coo_matrix((-discharge.clip(min=0), (cells_l, cells_h)),\n                                  shape=(dof[1], dof[0]))\n\n        # Compute the inflow from the higher to the lower dimensional grid\n        cc[0, 1] = sps.coo_matrix((discharge.clip(max=0), (cells_h, cells_l)),\n                                  shape=(dof[0], dof[1]))\n\n        cc[1, 1] = sps.dia_matrix((diag_cc11, 0), shape=(dof[1], dof[1]))\n\n        cc[0, 0] = sps.dia_matrix((diag_cc00, 0), shape=(dof[0], dof[0]))\n\n        if data_h['node_number'] == data_l['node_number']:\n            # All contributions to be returned to the same block of the\n            # global matrix in this case\n            cc = np.array([np.sum(cc, axis=(0, 1))])\n        return cc\n\n#------------------------------------------------------------------------------#\n\n    def cfl(self, g_h, g_l, data_h, data_l, data_edge, d_name='discharge'):\n        \"\"\"\n        Return the time step according to the CFL condition.\n        Note: the vector field is assumed to be given as the normal velocity,\n        weighted with the face area, at each face.\n\n        The name of data in the input dictionary (data) are:\n        discharge : array (g.num_faces)\n            Normal velocity at each face, weighted by the face area.\n\n        Parameters:\n            g_h: grid of higher dimension\n            g_l: grid of lower dimension\n            data_h: dictionary which stores the data for the higher dimensional\n                grid\n            data_l: dictionary which stores the data for the lower dimensional\n                grid\n            data: dictionary which stores the data for the edges of the grid\n                bucket\n\n        Return:\n            deltaT: time step according to CFL condition.\n\n        \"\"\"\n        # Retrieve the discharge, which is mandatory\n        discharge = data_edge[d_name]\n        aperture_h = data_h['param'].get_aperture()\n        aperture_l = data_l['param'].get_aperture()\n        phi_l = data_l['param'].get_porosity()\n        if g_h.dim == g_l.dim:\n            # More or less same as below, except we have cell_cells in the place\n            # of face_cells (see grid_bucket.duplicate_without_dimension).\n            phi_h = data_h['param'].get_porosity()\n            cells_l, cells_h = data_edge['face_cells'].nonzero()\n            not_zero = ~np.isclose(\n                np.zeros(discharge.shape), discharge, atol=0)\n            if not np.any(not_zero):\n                return np.Inf\n\n            diff = g_h.cell_centers[:, cells_h] - g_l.cell_centers[:, cells_l]\n            dist = np.linalg.norm(diff, 2, axis=0)\n\n            # Use minimum of cell values for convenience\n            phi_l = phi_l[cells_l]\n            phi_h = phi_h[cells_h]\n            apt_h = aperture_h[cells_h]\n            apt_l = aperture_l[cells_l]\n            coeff = np.minimum(phi_h, phi_l) * np.minimum(apt_h, apt_l)\n            return np.amin(np.abs(np.divide(dist, discharge)) * coeff)\n\n        # Recover the information for the grid-grid mapping\n        cells_l, faces_h, _ = sps.find(data_edge['face_cells'])\n\n        # Detect and remove the faces which have zero in \"discharge\"\n        not_zero = ~np.isclose(np.zeros(faces_h.size),\n                               discharge[faces_h], atol=0)\n        if not np.any(not_zero):\n            return np.inf\n\n        cells_l = cells_l[not_zero]\n        faces_h = faces_h[not_zero]\n        # Mapping from faces_h to cell_h\n        cell_faces_h = g_h.cell_faces.tocsr()[faces_h, :]\n        cells_h = cell_faces_h.nonzero()[1][not_zero]\n        # Retrieve and map additional data\n        aperture_h = aperture_h[cells_h]\n        aperture_l = aperture_l[cells_l]\n        phi_l = phi_l[cells_l]\n        # Compute discrete distance cell to face centers for the lower\n        # dimensional grid\n        dist = 0.5 * np.divide(aperture_l, aperture_h)\n        # Since discharge is multiplied by the aperture, we get rid of it!!!!\n        discharge = np.divide(discharge[faces_h],\n                              g_h.face_areas[faces_h] * aperture_h)\n        # deltaT is deltaX/discharge with coefficient\n        return np.amin(np.abs(np.divide(dist, discharge)) * phi_l)",
  "def __init__(self, physics='transport'):\n        self.physics = physics\n\n        self.discr = Upwind(self.physics)\n        self.discr_ndof = self.discr.ndof\n        self.coupling_conditions = UpwindCoupling(self.discr)\n\n        self.solver = Coupler(self.discr, self.coupling_conditions)",
  "def cfl(self, gb):\n        deltaT = gb.apply_function(self.discr.cfl,\n                                   self.coupling_conditions.cfl).data\n        return np.amin(deltaT)",
  "def outflow(self, gb):\n        def bind(g, d):\n            return self.discr.outflow(g, d), np.zeros(g.num_cells)\n        return Coupler(self.discr, solver_fct=bind).matrix_rhs(gb)[0]",
  "def __init__(self, physics='transport'):\n        self.physics = physics",
  "def ndof(self, g):\n        \"\"\"\n        Return the number of degrees of freedom associated to the method.\n        In this case number of cells (concentration dof).\n\n        Parameter\n        ---------\n        g: grid, or a subclass.\n\n        Return\n        ------\n        dof: the number of degrees of freedom.\n\n        \"\"\"\n        return g.num_cells",
  "def matrix_rhs(self, g, data, d_name='discharge'):\n        \"\"\"\n        Return the matrix and righ-hand side for a discretization of a scalar\n        linear transport problem using the upwind scheme.\n        Note: the vector field is assumed to be given as the normal velocity,\n        weighted with the face area, at each face.\n        Note: if not specified the inflow boundary conditions are no-flow, while\n        the outflow boundary conditions are open.\n\n        The name of data in the input dictionary (data) are:\n        discharge : array (g.num_faces)\n            Normal velocity at each face, weighted by the face area.\n        bc : boundary conditions (optional)\n        bc_val : dictionary (optional)\n            Values of the boundary conditions. The dictionary has at most the\n            following keys: 'dir' and 'neu', for Dirichlet and Neumann boundary\n            conditions, respectively.\n        source : array (g.num_cells) of source (positive) or sink (negative) terms.\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data.\n        d_name: (string) keyword for data field in data containing the dischages\n\n        Return\n        ------\n        matrix: sparse csr (g.num_cells, g_num_cells)\n            Upwind matrix obtained from the discretization.\n        rhs: array (g_num_cells)\n            Right-hand side which contains the boundary conditions.\n\n        Examples\n        --------\n        data = {'discharge': u, 'bc': bnd, 'bc_val': bnd_val}\n        advect = upwind.Upwind()\n        U, rhs = advect.matrix_rhs(g, data)\n\n        data = {'deltaT': advect.cfl(g, data)}\n        M, _ = mass.MassMatrix().matrix_rhs(g, data)\n\n        M_minus_U = M - U\n        invM = mass.MassMatrix().inv(M)\n\n        # Loop over the time\n        for i in np.arange( N ):\n            conc = invM.dot((M_minus_U).dot(conc) + rhs)\n\n        \"\"\"\n        if g.dim == 0:\n            return sps.csr_matrix([0]), [0]\n\n        param = data['param']\n        discharge = data[d_name]\n        bc = param.get_bc(self)\n        bc_val = param.get_bc_val(self)\n\n        has_bc = not(bc is None or bc_val is None)\n\n        # Compute the face flux respect to the real direction of the normals\n        indices = g.cell_faces.indices\n        flow_faces = g.cell_faces.copy()\n        flow_faces.data *= discharge[indices]\n\n        # Retrieve the faces boundary and their numeration in the flow_faces\n        # We need to impose no-flow for the inflow faces without boundary\n        # condition\n        mask = np.unique(indices, return_index=True)[1]\n        bc_neu = g.get_all_boundary_faces()\n\n        if has_bc:\n            # If boundary conditions are imposed remove the faces from this\n            # procedure.\n            # For primal-like discretizations, internal boundaries\n            # are handled by assigning Neumann conditions.\n            is_dir = np.logical_and(bc.is_dir, np.logical_not(bc.is_internal))\n            bc_dir = np.where(is_dir)[0]\n            bc_neu = np.setdiff1d(bc_neu, bc_dir, assume_unique=True)\n            bc_dir = mask[bc_dir]\n\n            # Remove Dirichlet inflow\n            inflow = flow_faces.copy()\n\n            inflow.data[bc_dir] = inflow.data[bc_dir].clip(max=0)\n            flow_faces.data[bc_dir] = flow_faces.data[bc_dir].clip(min=0)\n\n        # Remove all Neumann\n        bc_neu = mask[bc_neu]\n        flow_faces.data[bc_neu] = 0\n\n        # Determine the outflow faces\n        if_faces = flow_faces.copy()\n        if_faces.data = np.sign(if_faces.data)\n\n        # Compute the inflow/outflow related to the cells of the problem\n        flow_faces.data = flow_faces.data.clip(min=0)\n        flow_cells = if_faces.transpose() * flow_faces\n        flow_cells.tocsr()\n\n        if not has_bc:\n            return flow_cells, np.zeros(g.num_cells)\n\n        # Impose the boundary conditions\n        bc_val_dir = np.zeros(g.num_faces)\n        if np.any(bc.is_dir):\n            is_dir = np.where(bc.is_dir)[0]\n            bc_val_dir[is_dir] = bc_val[is_dir]\n\n        # We assume that for Neumann boundary condition a positive 'bc_val'\n        # represents an outflow for the domain. A negative 'bc_val' represents\n        # an inflow for the domain.\n        bc_val_neu = np.zeros(g.num_faces)\n        if np.any(bc.is_neu):\n            is_neu = np.where(bc.is_neu)[0]\n            bc_val_neu[is_neu] = bc_val[is_neu]\n\n        return flow_cells, - inflow.transpose() * bc_val_dir \\\n            - np.abs(g.cell_faces.transpose()) * bc_val_neu",
  "def cfl(self, g, data, d_name='discharge'):\n        \"\"\"\n        Return the time step according to the CFL condition.\n        Note: the vector field is assumed to be given as the normal velocity,\n        weighted with the face area, at each face.\n\n        The name of data in the input dictionary (data) are:\n        discharge : array (g.num_faces)\n            Normal velocity at each face, weighted by the face area.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data.\n        d_name: (string) keyword for dischagre file in data dictionary\n\n        Return\n        ------\n        deltaT: time step according to CFL condition.\n\n        \"\"\"\n        if g.dim == 0:\n            return np.inf\n        # Retrieve the data, only \"discharge\" is mandatory\n        param = data['param']\n        discharge = data[d_name]\n        aperture = param.get_aperture()\n        phi = param.get_porosity()\n\n        faces, cells, _ = sps.find(g.cell_faces)\n\n        # Detect and remove the faces which have zero in discharge\n        not_zero = ~np.isclose(np.zeros(faces.size), discharge[faces], atol=0)\n        if not np.any(not_zero):\n            return np.inf\n\n        cells = cells[not_zero]\n        faces = faces[not_zero]\n\n        # Compute discrete distance cell to face centers\n        dist_vector = g.face_centers[:, faces] - g.cell_centers[:, cells]\n        # Element-wise scalar products between the distance vectors and the\n        # normals\n        dist = np.einsum('ij,ij->j', dist_vector, g.face_normals[:, faces])\n        # Since discharge is multiplied by the aperture, we get rid of it!!!!\n        # Additionally we consider the phi (porosity) and the cell-mapping\n        coeff = (aperture * phi)[cells]\n        # deltaT is deltaX/discharge with coefficient\n        return np.amin(np.abs(np.divide(dist, discharge[faces])) * coeff)",
  "def discharge(self, g, beta, cell_apertures=None):\n        \"\"\"\n        Return the normal component of the velocity, for each face, weighted by\n        the face area and aperture.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        beta: (3x1) array which represents the constant velocity.\n        cell_apertures: (g.num_faces) array of apertures\n\n        Return\n        ------\n        discharge : array (g.num_faces)\n            Normal velocity at each face, weighted by the face area.\n\n        \"\"\"\n        if cell_apertures is None:\n            face_apertures = np.ones(g.num_faces)\n        else:\n            face_apertures = abs(g.cell_faces) * cell_apertures\n            r, _, _ = sps.find(g.cell_faces)\n            face_apertures = face_apertures / np.bincount(r)\n\n        beta = np.asarray(beta)\n        assert beta.size == 3\n\n        if g.dim == 0:\n            dot_prod = np.dot(g.face_normals.ravel('F'), face_apertures * beta)\n            return np.atleast_1d(dot_prod)\n\n        return np.array([np.dot(n, a * beta)\n                         for n, a in zip(g.face_normals.T, face_apertures)])",
  "def outflow(self, g, data, d_name='discharge'):\n        if g.dim == 0:\n            return sps.csr_matrix([0])\n\n        param = data['param']\n        discharge = data[d_name]\n        bc = param.get_bc(self)\n        bc_val = param.get_bc_val(self)\n\n        has_bc = not(bc is None or bc_val is None)\n\n        # Compute the face flux respect to the real direction of the normals\n        indices = g.cell_faces.indices\n        flow_faces = g.cell_faces.copy()\n        flow_faces.data *= discharge[indices]\n\n        # Retrieve the faces boundary and their numeration in the flow_faces\n        # We need to impose no-flow for the inflow faces without boundary\n        # condition\n        mask = np.unique(indices, return_index=True)[1]\n        bc_neu = g.tags['domain_boundary_faces'].nonzero()[0]\n\n        if has_bc:\n            # If boundary conditions are imposed remove the faces from this\n            # procedure.\n            bc_dir = np.where(bc.is_dir)[0]\n            bc_neu = np.setdiff1d(bc_neu, bc_dir, assume_unique=True)\n            bc_dir = mask[bc_dir]\n\n            # Remove Dirichlet inflow\n            inflow = flow_faces.copy()\n\n            inflow.data[bc_dir] = inflow.data[bc_dir].clip(max=0)\n            flow_faces.data[bc_dir] = flow_faces.data[bc_dir].clip(min=0)\n\n        # Remove all Neumann\n        bc_neu = mask[bc_neu]\n        flow_faces.data[bc_neu] = 0\n\n        # Determine the outflow faces\n        if_faces = flow_faces.copy()\n        if_faces.data = np.sign(if_faces.data)\n\n        outflow_faces = if_faces.indices[if_faces.data > 0]\n        domain_boundary_faces = g.tags['domain_boundary_faces'].nonzero()[0]\n        outflow_faces = np.intersect1d(outflow_faces,\n                                       domain_boundary_faces,\n                                       assume_unique=True)\n\n        # va tutto bene se ho neumann omogeneo\n        # gli outflow sono positivi\n\n        if_outflow_faces = if_faces.copy()\n        if_outflow_faces.data[:] = 0\n        if_outflow_faces.data[np.in1d(if_faces.indices, outflow_faces)] = 1\n\n        if_outflow_cells = if_outflow_faces.transpose() * flow_faces\n        if_outflow_cells.tocsr()\n\n        return if_outflow_cells",
  "def __init__(self, discr):\n        self.discr_ndof = discr.ndof",
  "def matrix_rhs(self, g_h, g_l, data_h, data_l, data_edge, d_name='discharge'):\n        \"\"\"\n        Construct the matrix (and right-hand side) for the coupling conditions.\n        Note: the right-hand side is not implemented now.\n\n        Parameters:\n            g_h: grid of higher dimension\n            g_l: grid of lower dimension\n            data_h: dictionary which stores the data for the higher dimensional\n                grid\n            data_l: dictionary which stores the data for the lower dimensional\n                grid\n            data: dictionary which stores the data for the edges of the grid\n                bucket\n\n        Returns:\n            cc: block matrix which store the contribution of the coupling\n                condition. See the abstract coupling class for a more detailed\n                description.\n        \"\"\"\n\n        # Normal component of the velocity from the higher dimensional grid\n        discharge = data_edge[d_name]\n\n        # Retrieve the number of degrees of both grids\n        # Create the block matrix for the contributions\n        dof, cc = self.create_block_matrix(g_h, g_l)\n\n        # 1d-1d\n        if g_h.dim == g_l.dim:\n            # Remember that face_cells are really cell-cell connections\n            # in this case\n            cells_l, cells_h = data_edge['face_cells'].nonzero()\n            diag_cc11 = np.zeros(g_l.num_cells)\n            diag_cc00 = np.zeros(g_h.num_cells)\n            d_00 = np.bincount(cells_h, np.sign(discharge.clip(min=0)) * discharge,\n                               minlength=g_h.num_cells)\n            d_11 = np.bincount(cells_l, np.sign(discharge.clip(max=0)) * discharge,\n                               minlength=g_l.num_cells)\n            np.add.at(diag_cc00, range(g_h.num_cells), d_00)\n            np.add.at(diag_cc11, range(g_l.num_cells), d_11)\n        else:\n            # Recover the information for the grid-grid mapping\n            cells_l, faces_h, _ = sps.find(data_edge['face_cells'])\n\n            # Recover the correct sign for the velocity\n            faces, _, sgn = sps.find(g_h.cell_faces)\n            sgn = sgn[np.unique(faces, return_index=True)[1]]\n            discharge = sgn[faces_h] * discharge[faces_h]\n\n            # Determine which are the corresponding cells of the faces_h\n            cell_faces_h = g_h.cell_faces.tocsr()[faces_h, :]\n            cells_h = cell_faces_h.nonzero()[1]\n\n            diag_cc11 = np.zeros(g_l.num_cells)\n            np.add.at(diag_cc11, cells_l, np.sign(\n                discharge.clip(max=0)) * discharge)\n\n            diag_cc00 = np.zeros(g_h.num_cells)\n            np.add.at(diag_cc00, cells_h, np.sign(\n                discharge.clip(min=0)) * discharge)\n        # Compute the outflow from the higher to the lower dimensional grid\n        cc[1, 0] = sps.coo_matrix((-discharge.clip(min=0), (cells_l, cells_h)),\n                                  shape=(dof[1], dof[0]))\n\n        # Compute the inflow from the higher to the lower dimensional grid\n        cc[0, 1] = sps.coo_matrix((discharge.clip(max=0), (cells_h, cells_l)),\n                                  shape=(dof[0], dof[1]))\n\n        cc[1, 1] = sps.dia_matrix((diag_cc11, 0), shape=(dof[1], dof[1]))\n\n        cc[0, 0] = sps.dia_matrix((diag_cc00, 0), shape=(dof[0], dof[0]))\n\n        if data_h['node_number'] == data_l['node_number']:\n            # All contributions to be returned to the same block of the\n            # global matrix in this case\n            cc = np.array([np.sum(cc, axis=(0, 1))])\n        return cc",
  "def cfl(self, g_h, g_l, data_h, data_l, data_edge, d_name='discharge'):\n        \"\"\"\n        Return the time step according to the CFL condition.\n        Note: the vector field is assumed to be given as the normal velocity,\n        weighted with the face area, at each face.\n\n        The name of data in the input dictionary (data) are:\n        discharge : array (g.num_faces)\n            Normal velocity at each face, weighted by the face area.\n\n        Parameters:\n            g_h: grid of higher dimension\n            g_l: grid of lower dimension\n            data_h: dictionary which stores the data for the higher dimensional\n                grid\n            data_l: dictionary which stores the data for the lower dimensional\n                grid\n            data: dictionary which stores the data for the edges of the grid\n                bucket\n\n        Return:\n            deltaT: time step according to CFL condition.\n\n        \"\"\"\n        # Retrieve the discharge, which is mandatory\n        discharge = data_edge[d_name]\n        aperture_h = data_h['param'].get_aperture()\n        aperture_l = data_l['param'].get_aperture()\n        phi_l = data_l['param'].get_porosity()\n        if g_h.dim == g_l.dim:\n            # More or less same as below, except we have cell_cells in the place\n            # of face_cells (see grid_bucket.duplicate_without_dimension).\n            phi_h = data_h['param'].get_porosity()\n            cells_l, cells_h = data_edge['face_cells'].nonzero()\n            not_zero = ~np.isclose(\n                np.zeros(discharge.shape), discharge, atol=0)\n            if not np.any(not_zero):\n                return np.Inf\n\n            diff = g_h.cell_centers[:, cells_h] - g_l.cell_centers[:, cells_l]\n            dist = np.linalg.norm(diff, 2, axis=0)\n\n            # Use minimum of cell values for convenience\n            phi_l = phi_l[cells_l]\n            phi_h = phi_h[cells_h]\n            apt_h = aperture_h[cells_h]\n            apt_l = aperture_l[cells_l]\n            coeff = np.minimum(phi_h, phi_l) * np.minimum(apt_h, apt_l)\n            return np.amin(np.abs(np.divide(dist, discharge)) * coeff)\n\n        # Recover the information for the grid-grid mapping\n        cells_l, faces_h, _ = sps.find(data_edge['face_cells'])\n\n        # Detect and remove the faces which have zero in \"discharge\"\n        not_zero = ~np.isclose(np.zeros(faces_h.size),\n                               discharge[faces_h], atol=0)\n        if not np.any(not_zero):\n            return np.inf\n\n        cells_l = cells_l[not_zero]\n        faces_h = faces_h[not_zero]\n        # Mapping from faces_h to cell_h\n        cell_faces_h = g_h.cell_faces.tocsr()[faces_h, :]\n        cells_h = cell_faces_h.nonzero()[1][not_zero]\n        # Retrieve and map additional data\n        aperture_h = aperture_h[cells_h]\n        aperture_l = aperture_l[cells_l]\n        phi_l = phi_l[cells_l]\n        # Compute discrete distance cell to face centers for the lower\n        # dimensional grid\n        dist = 0.5 * np.divide(aperture_l, aperture_h)\n        # Since discharge is multiplied by the aperture, we get rid of it!!!!\n        discharge = np.divide(discharge[faces_h],\n                              g_h.face_areas[faces_h] * aperture_h)\n        # deltaT is deltaX/discharge with coefficient\n        return np.amin(np.abs(np.divide(dist, discharge)) * phi_l)",
  "def bind(g, d):\n            return self.discr.outflow(g, d), np.zeros(g.num_cells)",
  "class IterCounter(object):\n    \"\"\" Simple callback function for iterative solvers.\n\n    Taken from https://stackoverflow.com/questions/33512081/getting-the-number-of-iterations-of-scipys-gmres-iterative-method\n    \"\"\"\n    def __init__(self, disp=True):\n        self._disp = disp\n        self.niter = 0\n\n    def __call__(self, rk=None):\n        self.niter += 1\n        if self._disp:\n            logger.info('iter %3i\\trk = %s' % (self.niter, str(rk)))",
  "class Factory():\n    \"\"\" Factory class for linear solver functionality. The intention is to\n    provide a single entry point for all relevant linear solvers. Hopefully,\n    this will pay off as the number of options expands.\n\n    Currently supported backends are standard scipy.sparse solvers, and pyamg.\n    For information on parameters etc, confer the wrapped functions and\n    libraries.\n\n    \"\"\"\n\n    def ilu(self, A, **kwargs):\n        \"\"\" Wrapper around ILU function from scipy.sparse.linalg.\n        Confer that function for documetnation.\n\n        Parameters:\n            A: Matrix to be factorized\n            **kwargs: Parameters passed on to scipy.sparse\n\n        Returns:\n            scipy.sparse.LinearOperator: Ready to be used as a preconditioner.\n\n        \"\"\"\n        opts = self.__extract_spilu_args(**kwargs)\n        iA = spl.spilu(A, **opts)\n        iA_x = lambda x: iA.solve(x)\n        return spl.LinearOperator(A.shape, iA_x)\n\n\n    def lu(self, A, **kwargs):\n        \"\"\" Wrapper around LU function from scipy.sparse.linalg.\n        Confer that function for documetnation.\n\n        Parameters:\n            A: Matrix to be factorized\n            **kwargs: Parameters passed on to scipy.sparse\n\n        Returns:\n            Solver function from LU.\n\n        \"\"\"\n        opts = self.__extract_splu_args(**kwargs)\n        iA = spl.splu(A, **opts)\n        return iA.solve\n\n\n    def direct(self, A, rhs=None):\n        \"\"\" Wrapper around spsolve from scipy.sparse.linalg.\n        Confer that function for documetnation.\n\n        Parameters:\n            A: Matrix to be factorized\n            rhs (optional): Right hand side vector. If not provided, a funciton\n                to solve with the given A is returned instead.\n\n        Returns:\n            Either scipy.sparse.LinearOperator: Ready to be used as a\n                preconditioner, or the solution A^-1 b\n\n        \"\"\"\n        def solve(b):\n            return spl.spsolve(A, b)\n\n        if rhs is None:\n            return solve\n        else:\n            return solve(rhs)\n\n\n    def gmres(self, A):\n        \"\"\" Wrapper around gmres function from scipy.sparse.linalg.\n        Confer that function for documetnation.\n\n        Parameters:\n            A (Matrix): Left hand side matrix\n\n        Returns:\n            A function that wraps gmres. The function needs a right hand side\n                vector of appropriate size. Also accepts further keyword\n                arguments that are passed on to scipy.\n\n        \"\"\"\n        def solve(b, **kwargs):\n            opt = self.__extract_gmres_args(**kwargs)\n            return spl.gmres(A, b, **opt)\n        return solve\n\n\n    def cg(self, A):\n        \"\"\" Wrapper around cg function from scipy.sparse.linalg.\n        Confer that function for documetnation.\n\n        Parameters:\n            A (Matrix): Left hand side matrix\n\n        Returns:\n            A function that wraps cg. The function needs a right hand side\n                vector of appropriate size. Also accepts further keyword\n                arguments that are passed on to scipy.\n\n        \"\"\"\n        def solve(b, **kwargs):\n            opt = self.__extract_krylov_args(**kwargs)\n            return spl.cg(A, b, **opt)\n        return solve\n\n    def bicgstab(self, A, **kwargs):\n        \"\"\" Wrapper around bicgstab function from scipy.sparse.linalg.\n        Confer that function for documetnation.\n\n        Parameters:\n            A (Matrix): Left hand side matrix\n\n        Returns:\n            A function that wraps bicgstab. The function needs a right hand\n                side vector of appropriate size. Also accepts further keyword\n                arguments that are passed on to scipy.\n\n        \"\"\"\n        def solve(b, **kwargs):\n            opt = self.__extract_krylov_args(**kwargs)\n            return spl.bicgstab(A, b, **opt)\n        return solve\n\n    def amg(self, A, null_space=None, as_precond=True, **kwargs):\n        \"\"\" Wrapper around the pyamg solver by Bell, Olson and Schroder.\n\n        For the moment, the method creates a smoothed aggregation amg solver.\n        More elaborate options may be added in the future if the need arises.\n        If you need other types of solvers or functionality, access pyamg\n        directly.\n\n        For documentation of pyamg, including parameters options, confer\n        https://github.com/pyamg/pyamg.\n\n        This wrapper can either produce a solver or a preconditioner\n        (LinearOperator). For the moment we provide limited parsing of options,\n        the solver will be a GMRES accelerated V-cycle, while the\n        preconditioner is simply a V-cycle. Expanding this is not difficult,\n        but tedious.\n\n        Parameters:\n            A (Matrix): To be factorized.\n            null_space (optional): Null space of the matrix. Accurate\n                information here is essential to creating a good coarse space\n                hierarchy. Defaults to vector of ones, which is the correct\n                choice for standard elliptic equations.\n            as_precond (optional, defaults to True): Whether to return a solver\n                or a preconditioner function.\n            **kwargs: For the moment not in use.\n\n        Returns:\n            Function: Either a LinearOperator to be used as preconditioner,\n                or a solver.\n\n        \"\"\"\n        \n\n        if null_space is None:\n            null_space = np.ones(A.shape[0])\n        try:\n            ml = pyamg.smoothed_aggregation_solver(A, B=null_space)\n        except NameError:\n            raise ImportError('Using amg needs requires the pyamg package. pyamg was not imported')\n        \n\n        def solve(b, res=None, **kwargs):\n            if res is None:\n                return ml.solve(b, accel='gmres', cycle='V')\n            else:\n                return ml.solve(b, residuals=res, accel='gmres', cycle='V')\n\n        if as_precond:\n            M_x = lambda x: ml.solve(x, tol=1e-20, maxiter=10, cycle='W')\n            return spl.LinearOperator(A.shape, M_x)\n        else:\n            return solve\n\n    #### Helper functions below\n\n    def __extract_krylov_args(self, **kwargs):\n        d = {}\n        d['x0'] = kwargs.get('x0', None)\n        d['tol'] = kwargs.get('tol', 1e-5)\n        d['maxiter'] = kwargs.get('maxiter', None)\n        d['M'] = kwargs.get('M', None)\n        cb = kwargs.get('callback', None)\n        if cb is not None and not isinstance(cb, IterCounter):\n            cb = IterCounter()\n        d['callback'] = cb\n        return d\n\n    def __extract_gmres_args(self, **kwargs):\n        d = self.__extract_krylov_args(**kwargs)\n        d['restart'] = kwargs.get('restart', None)\n        return d\n\n    def __extract_splu_args(self, **kwargs):\n        d = {}\n        d['permc_spec'] = kwargs.get('permc_spec', None)\n        d['diag_pivot_thresh'] = kwargs.get('diag_pivot_thresh', None)\n        d['drop_tol'] = kwargs.get('drop_tol', None)\n        d['relax'] = kwargs.get('relax', None)\n        d['panel_size'] = kwargs.get('panel_size', None)\n        return d\n\n    def __extract_spilu_args(self, **kwargs):\n        d = self.__extract_splu_args(**kwargs)\n        d['drop_tol'] = kwargs.get('drop_tol', None)\n        d['fill_factor'] = kwargs.get('fill_factor', None)\n        d['drop_rule'] = kwargs.get('drop_rule', None)\n        return d\n\n    def __as_linear_operator(self, mat, sz=None):\n        if sz is None:\n            sz = mat.shape\n        def mv(v):\n            mat.solve(v)\n        return spl.LinearOperator(sz, matvec = mv)",
  "def __init__(self, disp=True):\n        self._disp = disp\n        self.niter = 0",
  "def __call__(self, rk=None):\n        self.niter += 1\n        if self._disp:\n            logger.info('iter %3i\\trk = %s' % (self.niter, str(rk)))",
  "def ilu(self, A, **kwargs):\n        \"\"\" Wrapper around ILU function from scipy.sparse.linalg.\n        Confer that function for documetnation.\n\n        Parameters:\n            A: Matrix to be factorized\n            **kwargs: Parameters passed on to scipy.sparse\n\n        Returns:\n            scipy.sparse.LinearOperator: Ready to be used as a preconditioner.\n\n        \"\"\"\n        opts = self.__extract_spilu_args(**kwargs)\n        iA = spl.spilu(A, **opts)\n        iA_x = lambda x: iA.solve(x)\n        return spl.LinearOperator(A.shape, iA_x)",
  "def lu(self, A, **kwargs):\n        \"\"\" Wrapper around LU function from scipy.sparse.linalg.\n        Confer that function for documetnation.\n\n        Parameters:\n            A: Matrix to be factorized\n            **kwargs: Parameters passed on to scipy.sparse\n\n        Returns:\n            Solver function from LU.\n\n        \"\"\"\n        opts = self.__extract_splu_args(**kwargs)\n        iA = spl.splu(A, **opts)\n        return iA.solve",
  "def direct(self, A, rhs=None):\n        \"\"\" Wrapper around spsolve from scipy.sparse.linalg.\n        Confer that function for documetnation.\n\n        Parameters:\n            A: Matrix to be factorized\n            rhs (optional): Right hand side vector. If not provided, a funciton\n                to solve with the given A is returned instead.\n\n        Returns:\n            Either scipy.sparse.LinearOperator: Ready to be used as a\n                preconditioner, or the solution A^-1 b\n\n        \"\"\"\n        def solve(b):\n            return spl.spsolve(A, b)\n\n        if rhs is None:\n            return solve\n        else:\n            return solve(rhs)",
  "def gmres(self, A):\n        \"\"\" Wrapper around gmres function from scipy.sparse.linalg.\n        Confer that function for documetnation.\n\n        Parameters:\n            A (Matrix): Left hand side matrix\n\n        Returns:\n            A function that wraps gmres. The function needs a right hand side\n                vector of appropriate size. Also accepts further keyword\n                arguments that are passed on to scipy.\n\n        \"\"\"\n        def solve(b, **kwargs):\n            opt = self.__extract_gmres_args(**kwargs)\n            return spl.gmres(A, b, **opt)\n        return solve",
  "def cg(self, A):\n        \"\"\" Wrapper around cg function from scipy.sparse.linalg.\n        Confer that function for documetnation.\n\n        Parameters:\n            A (Matrix): Left hand side matrix\n\n        Returns:\n            A function that wraps cg. The function needs a right hand side\n                vector of appropriate size. Also accepts further keyword\n                arguments that are passed on to scipy.\n\n        \"\"\"\n        def solve(b, **kwargs):\n            opt = self.__extract_krylov_args(**kwargs)\n            return spl.cg(A, b, **opt)\n        return solve",
  "def bicgstab(self, A, **kwargs):\n        \"\"\" Wrapper around bicgstab function from scipy.sparse.linalg.\n        Confer that function for documetnation.\n\n        Parameters:\n            A (Matrix): Left hand side matrix\n\n        Returns:\n            A function that wraps bicgstab. The function needs a right hand\n                side vector of appropriate size. Also accepts further keyword\n                arguments that are passed on to scipy.\n\n        \"\"\"\n        def solve(b, **kwargs):\n            opt = self.__extract_krylov_args(**kwargs)\n            return spl.bicgstab(A, b, **opt)\n        return solve",
  "def amg(self, A, null_space=None, as_precond=True, **kwargs):\n        \"\"\" Wrapper around the pyamg solver by Bell, Olson and Schroder.\n\n        For the moment, the method creates a smoothed aggregation amg solver.\n        More elaborate options may be added in the future if the need arises.\n        If you need other types of solvers or functionality, access pyamg\n        directly.\n\n        For documentation of pyamg, including parameters options, confer\n        https://github.com/pyamg/pyamg.\n\n        This wrapper can either produce a solver or a preconditioner\n        (LinearOperator). For the moment we provide limited parsing of options,\n        the solver will be a GMRES accelerated V-cycle, while the\n        preconditioner is simply a V-cycle. Expanding this is not difficult,\n        but tedious.\n\n        Parameters:\n            A (Matrix): To be factorized.\n            null_space (optional): Null space of the matrix. Accurate\n                information here is essential to creating a good coarse space\n                hierarchy. Defaults to vector of ones, which is the correct\n                choice for standard elliptic equations.\n            as_precond (optional, defaults to True): Whether to return a solver\n                or a preconditioner function.\n            **kwargs: For the moment not in use.\n\n        Returns:\n            Function: Either a LinearOperator to be used as preconditioner,\n                or a solver.\n\n        \"\"\"\n        \n\n        if null_space is None:\n            null_space = np.ones(A.shape[0])\n        try:\n            ml = pyamg.smoothed_aggregation_solver(A, B=null_space)\n        except NameError:\n            raise ImportError('Using amg needs requires the pyamg package. pyamg was not imported')\n        \n\n        def solve(b, res=None, **kwargs):\n            if res is None:\n                return ml.solve(b, accel='gmres', cycle='V')\n            else:\n                return ml.solve(b, residuals=res, accel='gmres', cycle='V')\n\n        if as_precond:\n            M_x = lambda x: ml.solve(x, tol=1e-20, maxiter=10, cycle='W')\n            return spl.LinearOperator(A.shape, M_x)\n        else:\n            return solve",
  "def __extract_krylov_args(self, **kwargs):\n        d = {}\n        d['x0'] = kwargs.get('x0', None)\n        d['tol'] = kwargs.get('tol', 1e-5)\n        d['maxiter'] = kwargs.get('maxiter', None)\n        d['M'] = kwargs.get('M', None)\n        cb = kwargs.get('callback', None)\n        if cb is not None and not isinstance(cb, IterCounter):\n            cb = IterCounter()\n        d['callback'] = cb\n        return d",
  "def __extract_gmres_args(self, **kwargs):\n        d = self.__extract_krylov_args(**kwargs)\n        d['restart'] = kwargs.get('restart', None)\n        return d",
  "def __extract_splu_args(self, **kwargs):\n        d = {}\n        d['permc_spec'] = kwargs.get('permc_spec', None)\n        d['diag_pivot_thresh'] = kwargs.get('diag_pivot_thresh', None)\n        d['drop_tol'] = kwargs.get('drop_tol', None)\n        d['relax'] = kwargs.get('relax', None)\n        d['panel_size'] = kwargs.get('panel_size', None)\n        return d",
  "def __extract_spilu_args(self, **kwargs):\n        d = self.__extract_splu_args(**kwargs)\n        d['drop_tol'] = kwargs.get('drop_tol', None)\n        d['fill_factor'] = kwargs.get('fill_factor', None)\n        d['drop_rule'] = kwargs.get('drop_rule', None)\n        return d",
  "def __as_linear_operator(self, mat, sz=None):\n        if sz is None:\n            sz = mat.shape\n        def mv(v):\n            mat.solve(v)\n        return spl.LinearOperator(sz, matvec = mv)",
  "def solve(b):\n            return spl.spsolve(A, b)",
  "def solve(b, **kwargs):\n            opt = self.__extract_gmres_args(**kwargs)\n            return spl.gmres(A, b, **opt)",
  "def solve(b, **kwargs):\n            opt = self.__extract_krylov_args(**kwargs)\n            return spl.cg(A, b, **opt)",
  "def solve(b, **kwargs):\n            opt = self.__extract_krylov_args(**kwargs)\n            return spl.bicgstab(A, b, **opt)",
  "def solve(b, res=None, **kwargs):\n            if res is None:\n                return ml.solve(b, accel='gmres', cycle='V')\n            else:\n                return ml.solve(b, residuals=res, accel='gmres', cycle='V')",
  "def mv(v):\n            mat.solve(v)",
  "def solve_static_condensation(a, rhs, gb, dim=0,\n                              condensation_inverter=sps.linalg.inv,\n                              system_inverter=sps.linalg.spsolve):\n    \"\"\"\n    A call to this function uses a static condensation to solve a linear\n    problem without the degrees of freedom related to grids of dimension dim.\n\n    Input:\n        A (sps.csr_matrix): Original matrix and right hand side of the problem\n        to be solved.\n        rhs (np.array): Original matrix and right hand side of the problem to\n        be solved.\n        dim: The dimension one wishes to get rid of. No tests for dim>0.\n        condensation_inverter: The inverter of the (small) system solved\n            to perform the static condensation.\n        system_inverter: Inverter for solving the problem after static\n            condensation has been performed.\n    Returns:\n        x: The solution vector corresponding to the initial system, i.e.,\n            with all dofs.\n        x_reduced: The solution vector for the reduced system, i.e.,\n            corresponding to the master dofs only.\n        original_to_kept_dofs: Mapping from the full to the reduced set of\n            degrees of freedom.\n        eliminated_dofs: Mapping from the full to the removed set of degrees of\n            freedom (i.e., which of the initial correspond to grids of\n            dimension dim).\n    \"\"\"\n    to_be_eliminated = dofs_of_dimension(gb, a, dim)\n\n    a_reduced, rhs_reduced, condensation_matrix, original_to_kept_dofs, a_ss_inv \\\n        = eliminate_dofs(a, rhs, to_be_eliminated, condensation_inverter)\n\n    eliminated_dofs = np.nonzero(to_be_eliminated)[0]\n    x_reduced = system_inverter(a_reduced, rhs_reduced)\n    x = np.zeros(a.shape[0])\n    x[original_to_kept_dofs] = x_reduced\n    x[to_be_eliminated] = condensation_matrix * \\\n        x_reduced + a_ss_inv * rhs[to_be_eliminated]\n\n    return x, x_reduced, original_to_kept_dofs, eliminated_dofs",
  "def dofs_of_dimension(gb, a, dim=0):\n    \"\"\"\n    Extracts the global dof numbers corresponding to a given dimension.\n    Returns a boolean mask extracting the dofs to be eliminated.\n    \"\"\"\n    original_ndof = a.shape[1]  # bytt ut med info fra gb\n    dofs = np.empty(gb.size(), dtype=int)\n    for _, d in gb:\n        dofs[d['node_number']] = d['dof']\n    dofs = np.r_[0, np.cumsum(dofs)]\n\n    to_be_eliminated = np.zeros(original_ndof, dtype=bool)\n\n    for g, d in gb:\n        i = d['node_number']\n        if g.dim == dim:\n            to_be_eliminated[slice(dofs[i], dofs[i + 1])] = \\\n                np.ones(d['dof'], dtype=bool)\n\n    return to_be_eliminated",
  "def eliminate_dofs(A, rhs, to_be_eliminated, inverter=sps.linalg.inv):\n    \"\"\"\n    Splits the system matrix A into four blocks according to which dofs\n    are to be eliminated (the \"slaves\"). The right hand side is split\n    into two parts. For the computation of the blocks, the diagonal\n    part corresponding to the slaves has to be inverted, hence the option\n    to choose inverter. This system will usually be quite small.\n\n    Input:\n    A, rhs: original matrix and right hand side of the problem to be\n        solved.\n        to_be_eliminated: boolean mask specifying which degrees of freedom\n        should be eliminated from the system.\n\n    Returns:\n        A_reduced (scipy.sparse.csr_matrix): The system matrix for the reduced\n        system, i.e., corresponding to the master dofs only. It is computed as\n        the \"master\" part of A minus the slave-master contribution:\n                A_reduced = A_mm - A_ms inv(A_ss) A_sm\n        rhs_reduced: The right hand side for the reduced system.\n        Condensation_matrix: The matrix used for back-computation of the\n            unknowns of the slaves once the reduced system has been solved.\n        to_be_kept: Indices of the masters.\n\n    \"\"\"\n    to_be_kept = np.invert(to_be_eliminated)\n    # Get indexes of the masters:\n    to_be_kept = np.nonzero(to_be_kept)[0]\n    # and slaves:\n    to_be_eliminated = np.nonzero(to_be_eliminated)[0]\n\n    # Masters and slaves:\n    A_mm = A[to_be_kept][:, to_be_kept]\n    A_ms = A[to_be_kept][:, to_be_eliminated]\n    A_sm = A[to_be_eliminated][:, to_be_kept]\n    A_ss = A[to_be_eliminated][:, to_be_eliminated]\n    A_ss_inv = inverter(A_ss)\n    A_ms_A_ss_inv = A_ms * sps.csr_matrix(A_ss_inv)\n    sparse_product = A_ms_A_ss_inv * A_sm\n\n    A_reduced = A_mm - sparse_product\n\n    rhs_reduced = rhs[to_be_kept][:, np.newaxis] - \\\n        A_ms_A_ss_inv * rhs[to_be_eliminated, np.newaxis]\n    condensation_matrix = sps.csc_matrix(- A_ss_inv * A_sm)\n\n    return A_reduced, rhs_reduced, condensation_matrix, to_be_kept, A_ss_inv",
  "def new_coupling_fluxes(gb_old, gb_el, neighbours_old, neighbours_el, node_old):\n    \"\"\"\n    Adds new coupling_flux data fields to the new gb edges arising through\n    the removal of one node.\n    Idea: set up a condensation for the local system of old coupling_fluxes\n    for the removed nodes and its n_neighbours neighbour nodes/grids.\n    \"\"\"\n    neighbours_old = gb_old.sort_multiple_nodes(neighbours_old)\n    neighbours_el = gb_el.sort_multiple_nodes(neighbours_el)\n    n_cells_l = node_old.num_cells\n    n_neighbours = len(neighbours_old)\n\n    # Initialize coupling matrix (see coupler.py, matrix_rhs)\n    all_cc = np.empty((n_neighbours + 1, n_neighbours + 1), dtype=np.object)\n    pos_i = 0\n    for g_i in neighbours_old:\n        pos_j = 0\n        for g_j in neighbours_old:\n            all_cc[pos_i, pos_j] = sps.coo_matrix(\n                (g_i.num_cells, g_j.num_cells))\n            pos_j += 1\n\n        all_cc[pos_i, n_neighbours] = sps.coo_matrix(\n            (g_i.num_cells, node_old.num_cells))\n        all_cc[n_neighbours, pos_i] = sps.coo_matrix(\n            (node_old.num_cells, g_i.num_cells))\n        pos_i += 1\n\n    all_cc[n_neighbours, n_neighbours] = sps.coo_matrix((node_old.num_cells,\n                                                         node_old.num_cells))\n    dofs = np.zeros(n_neighbours)\n\n    # Assemble original system:\n    for i in range(n_neighbours):\n        cc = gb_old.edge_prop(\n            (neighbours_old[i], node_old), 'coupling_discretization')\n        idx = np.ix_([i, n_neighbours], [i, n_neighbours])\n        all_cc[idx] += cc[0]\n        dofs[i] = cc[0][0][0].shape[0]\n    global_idx = np.r_[0, np.cumsum(dofs)].astype(int)\n    all_cc = sps.bmat(all_cc, 'csr')\n\n    # Eliminate \"node\"\n    n_dof = all_cc.shape[0]\n    to_be_eliminated = np.zeros(n_dof, dtype=bool)\n    to_be_eliminated[range(n_dof - n_cells_l, n_dof)] = True\n    all_cc, _, _, _, _ = eliminate_dofs(\n        all_cc, np.zeros(n_dof), to_be_eliminated)\n\n    # Extract the new coupling fluxes from the eliminated system and map to\n    # faces of the first grid\n    for i, g_0 in enumerate(neighbours_el):\n        id_0 = slice(global_idx[i], global_idx[i + 1])\n        # Get the internal contribution (grids that have an internal hole after\n        # the elimination), to be added to the node g_0. This contribution\n        # is found at the off-diagonal part of the diagonal blocks\n        cc_00 = all_cc.tocsr()[id_0, :].tocsc()[:, id_0]\n\n        # Keep only one connection, the one from the \"first/higher\" cell(s) to\n        # the \"second/lower\". Fluxes from higher to lower, so entries should be\n        # positive (coming from off-diagonal, they are now negative)\n        c_f = -np.triu(cc_00.todense(), k=1)\n\n        # Check whether there is an internal hole in the grid. If so, add\n        # connections between the cells on either side\n        if not np.allclose(c_f, 0, 1e-10, 1e-12):\n            cell_cells = np.array(c_f > 0, dtype=bool)\n            # The fluxes c_f*p go from cells_1 to cells_2:\n            # c_1, c_2, _ = sparse.find(cell_cells)\n            gb_el.add_edge([g_0, g_0], cell_cells)\n            d_edge = gb_el.edge_props([g_0, g_0])\n            d_edge['coupling_flux'] = sps.csr_matrix(c_f)\n            d_edge['param'] = Parameters(g_0)\n\n        # Get the contribution between different grids\n        for j in range(i + 1, n_neighbours):\n            g_1 = neighbours_el[j]\n            id_1 = slice(global_idx[j], global_idx[j + 1])\n            cc_01 = all_cc.tocsr()[id_0, :].tocsc()[:, id_1]\n            gb_el.add_edge_prop('coupling_flux', [[g_0, g_1]], [-cc_01])",
  "def compute_elimination_fluxes(gb, gb_el, elimination_data):\n    neighbours_dict = elimination_data['neighbours']\n    neighbours_dict_old = elimination_data['neighbours_old']\n    eliminated_nodes = elimination_data['eliminated_nodes']\n    for i, neighbours in neighbours_dict.items():\n        new_coupling_fluxes(gb, gb_el, neighbours_dict_old[i], neighbours,\n                            eliminated_nodes[i])",
  "def sparse_condition_number(A):\n    \"\"\"\n    Convenience function to efficiently estimate condition number of large\n    sparse matrices. See sps.linalg.eigsh for assumptions on the matrix.\n    \"\"\"\n    eval_large, _ = sps.linalg.eigsh(A, k=1, which='LM')\n    eval_small, _ = sps.linalg.eigsh(A, 1, sigma=0, which='LM')\n    return eval_large / eval_small",
  "class Coupler(object):\n\n    #------------------------------------------------------------------------------#\n\n    def __init__(self, discr=None, coupling=None, **kwargs):\n\n        # Consider the dofs\n        discr_ndof = kwargs.get(\"discr_ndof\")\n        if discr_ndof is None:\n            self.discr_ndof = discr.ndof\n        else:\n            self.discr_ndof = discr_ndof\n\n        # Consider the solver for each dimension\n        discr_fct = kwargs.get(\"discr_fct\")\n        if discr_fct is None:\n            self.discr_fct = discr.matrix_rhs\n        else:\n            self.discr_fct = discr_fct\n\n        # Consider the coupling between dimensions\n        coupling_fct = kwargs.get(\"coupling_fct\")\n        if coupling is None and coupling_fct is None:\n            self.coupling_fct = None\n        elif coupling_fct is not None:\n            self.coupling_fct = coupling_fct\n        else:\n            self.coupling_fct = coupling.matrix_rhs\n\n#------------------------------------------------------------------------------#\n\n    def ndof(self, gb):\n        \"\"\"\n        Store in the grid bucket the number of degrees of freedom associated to\n        the method.\n        It requires the key \"dof\" in the grid bucket as reserved.\n\n        Parameter\n        ---------\n        gb: grid bucket.\n\n        \"\"\"\n        gb.add_node_prop('dof')\n        for g, d in gb:\n            d['dof'] = self.discr_ndof(g)\n\n#------------------------------------------------------------------------------#\n\n    def matrix_rhs(self, gb, matrix_format=\"csr\"):\n        \"\"\"\n        Return the matrix and righ-hand side for a suitable discretization, where\n        a hierarchy of grids are considered. The matrices are stored in the\n        global matrix and right-hand side according to the numeration given by\n        \"node_number\".\n        It requires the key \"dof\" in the grid bucket as reserved.\n        It requires the key \"node_number\" be present in the grid bucket, see\n        GridBucket.assign_node_ordering().\n\n        Parameters\n        ----------\n        gb : grid bucket with geometry fields computed.\n        matrix_format: (optional, default is csr) format of the sparse matrix.\n\n        Return\n        ------\n        matrix: sparse matrix from the discretization.\n        rhs: array right-hand side of the problem.\n        \"\"\"\n        self.ndof(gb)\n\n        # Initialize the global matrix and rhs to store the local problems\n        matrix = np.empty((gb.size(), gb.size()), dtype=np.object)\n        rhs = np.empty(gb.size(), dtype=np.object)\n        for g_i, d_i in gb:\n            pos_i = d_i['node_number']\n            rhs[pos_i] = np.empty(d_i['dof'])\n            for g_j, d_j in gb:\n                pos_j = d_j['node_number']\n                matrix[pos_i, pos_j] = sps.coo_matrix((d_i['dof'], d_j['dof']))\n\n        # Loop over the grids and compute the problem matrix\n        for g, data in gb:\n            pos = data['node_number']\n            matrix[pos, pos], rhs[pos] = self.discr_fct(g, data)\n\n        # Handle special case of 1-element grids, that give 0-d arrays\n        rhs = np.array([np.atleast_1d(a) for a in tuple(rhs)])\n\n        # if the coupling conditions are not given fill only the diagonal part\n        if self.coupling_fct is None:\n            return sps.bmat(matrix, matrix_format), np.concatenate(tuple(rhs))\n\n        # Loop over the edges of the graph (pair of connected grids) to compute\n        # the coupling conditions\n        for e, data in gb.edges_props():\n            g_l, g_h = gb.sorted_nodes_of_edge(e)\n            pos_l, pos_h = gb.nodes_prop([g_l, g_h], 'node_number')\n            idx = np.ix_([pos_h, pos_l], [pos_h, pos_l])\n\n            data_l, data_h = gb.node_props(g_l), gb.node_props(g_h)\n            matrix[idx] += self.coupling_fct(g_h, g_l, data_h, data_l, data)\n\n        return sps.bmat(matrix, matrix_format), np.concatenate(tuple(rhs))\n\n#------------------------------------------------------------------------------#\n\n    def split(self, gb, key, values):\n        \"\"\"\n        Store in the grid bucket the vector, split in the function, solution of\n        the problem. The values are extracted from the global solution vector\n        according to the numeration given by \"node_number\".\n\n        Parameters\n        ----------\n        gb : grid bucket with geometry fields computed.\n        key: new name of the solution to be stored in the grid bucket.\n        values: array, global solution.\n\n        \"\"\"\n        dofs = self._dof_start_of_grids(gb)\n\n        gb.add_node_prop(key)\n        for g, d in gb:\n            i = d['node_number']\n            d[key] = values[slice(dofs[i], dofs[i + 1])]\n\n#------------------------------------------------------------------------------#\n    def merge(self, gb, key):\n        \"\"\"\n        Merge the stored split function stored in the grid bucket to a vector.\n        The values are put into the global  vector according to the numeration\n        given by \"node_number\".\n\n        Parameters\n        ----------\n        gb : grid bucket with geometry fields computed.\n        key: new name of the solution to be stored in the grid bucket.\n\n        Returns\n        -------\n        values: (ndarray) the values stored in the bucket as an array\n        \"\"\"\n\n        dofs = self._dof_start_of_grids(gb)\n        values = np.zeros(dofs[-1])\n\n        for g, d in gb:\n            i = d['node_number']\n            values[slice(dofs[i], dofs[i + 1])] = d[key]\n\n        return values\n#------------------------------------------------------------------------------#\n\n    def _dof_start_of_grids(self ,gb):\n        \" Helper method to get first global dof for all grids. \"\n        self.ndof(gb)\n        dofs = np.empty(gb.size(), dtype=int)\n        for _, d in gb:\n            dofs[d['node_number']] = d['dof']\n        return np.r_[0, np.cumsum(dofs)]\n#------------------------------------------------------------------------------#\n\n    def dof_of_grid(self, gb, g):\n        \"\"\" Obtain global indices of dof associated with a given grid.\n\n        Parameters:\n            gb: Grid_bucket representation of mixed-dimensional data.\n            g: Grid, one member of gb.\n\n        Returns:\n            np.array of ints: Indices of all dof for the given grid\n\n        \"\"\"\n        dof_list = self._dof_start_of_grids(gb)\n        nn = gb.node_props(g)['node_number']\n        return np.arange(dof_list[nn], dof_list[nn+1])",
  "def __init__(self, discr=None, coupling=None, **kwargs):\n\n        # Consider the dofs\n        discr_ndof = kwargs.get(\"discr_ndof\")\n        if discr_ndof is None:\n            self.discr_ndof = discr.ndof\n        else:\n            self.discr_ndof = discr_ndof\n\n        # Consider the solver for each dimension\n        discr_fct = kwargs.get(\"discr_fct\")\n        if discr_fct is None:\n            self.discr_fct = discr.matrix_rhs\n        else:\n            self.discr_fct = discr_fct\n\n        # Consider the coupling between dimensions\n        coupling_fct = kwargs.get(\"coupling_fct\")\n        if coupling is None and coupling_fct is None:\n            self.coupling_fct = None\n        elif coupling_fct is not None:\n            self.coupling_fct = coupling_fct\n        else:\n            self.coupling_fct = coupling.matrix_rhs",
  "def ndof(self, gb):\n        \"\"\"\n        Store in the grid bucket the number of degrees of freedom associated to\n        the method.\n        It requires the key \"dof\" in the grid bucket as reserved.\n\n        Parameter\n        ---------\n        gb: grid bucket.\n\n        \"\"\"\n        gb.add_node_prop('dof')\n        for g, d in gb:\n            d['dof'] = self.discr_ndof(g)",
  "def matrix_rhs(self, gb, matrix_format=\"csr\"):\n        \"\"\"\n        Return the matrix and righ-hand side for a suitable discretization, where\n        a hierarchy of grids are considered. The matrices are stored in the\n        global matrix and right-hand side according to the numeration given by\n        \"node_number\".\n        It requires the key \"dof\" in the grid bucket as reserved.\n        It requires the key \"node_number\" be present in the grid bucket, see\n        GridBucket.assign_node_ordering().\n\n        Parameters\n        ----------\n        gb : grid bucket with geometry fields computed.\n        matrix_format: (optional, default is csr) format of the sparse matrix.\n\n        Return\n        ------\n        matrix: sparse matrix from the discretization.\n        rhs: array right-hand side of the problem.\n        \"\"\"\n        self.ndof(gb)\n\n        # Initialize the global matrix and rhs to store the local problems\n        matrix = np.empty((gb.size(), gb.size()), dtype=np.object)\n        rhs = np.empty(gb.size(), dtype=np.object)\n        for g_i, d_i in gb:\n            pos_i = d_i['node_number']\n            rhs[pos_i] = np.empty(d_i['dof'])\n            for g_j, d_j in gb:\n                pos_j = d_j['node_number']\n                matrix[pos_i, pos_j] = sps.coo_matrix((d_i['dof'], d_j['dof']))\n\n        # Loop over the grids and compute the problem matrix\n        for g, data in gb:\n            pos = data['node_number']\n            matrix[pos, pos], rhs[pos] = self.discr_fct(g, data)\n\n        # Handle special case of 1-element grids, that give 0-d arrays\n        rhs = np.array([np.atleast_1d(a) for a in tuple(rhs)])\n\n        # if the coupling conditions are not given fill only the diagonal part\n        if self.coupling_fct is None:\n            return sps.bmat(matrix, matrix_format), np.concatenate(tuple(rhs))\n\n        # Loop over the edges of the graph (pair of connected grids) to compute\n        # the coupling conditions\n        for e, data in gb.edges_props():\n            g_l, g_h = gb.sorted_nodes_of_edge(e)\n            pos_l, pos_h = gb.nodes_prop([g_l, g_h], 'node_number')\n            idx = np.ix_([pos_h, pos_l], [pos_h, pos_l])\n\n            data_l, data_h = gb.node_props(g_l), gb.node_props(g_h)\n            matrix[idx] += self.coupling_fct(g_h, g_l, data_h, data_l, data)\n\n        return sps.bmat(matrix, matrix_format), np.concatenate(tuple(rhs))",
  "def split(self, gb, key, values):\n        \"\"\"\n        Store in the grid bucket the vector, split in the function, solution of\n        the problem. The values are extracted from the global solution vector\n        according to the numeration given by \"node_number\".\n\n        Parameters\n        ----------\n        gb : grid bucket with geometry fields computed.\n        key: new name of the solution to be stored in the grid bucket.\n        values: array, global solution.\n\n        \"\"\"\n        dofs = self._dof_start_of_grids(gb)\n\n        gb.add_node_prop(key)\n        for g, d in gb:\n            i = d['node_number']\n            d[key] = values[slice(dofs[i], dofs[i + 1])]",
  "def merge(self, gb, key):\n        \"\"\"\n        Merge the stored split function stored in the grid bucket to a vector.\n        The values are put into the global  vector according to the numeration\n        given by \"node_number\".\n\n        Parameters\n        ----------\n        gb : grid bucket with geometry fields computed.\n        key: new name of the solution to be stored in the grid bucket.\n\n        Returns\n        -------\n        values: (ndarray) the values stored in the bucket as an array\n        \"\"\"\n\n        dofs = self._dof_start_of_grids(gb)\n        values = np.zeros(dofs[-1])\n\n        for g, d in gb:\n            i = d['node_number']\n            values[slice(dofs[i], dofs[i + 1])] = d[key]\n\n        return values",
  "def _dof_start_of_grids(self ,gb):\n        \" Helper method to get first global dof for all grids. \"\n        self.ndof(gb)\n        dofs = np.empty(gb.size(), dtype=int)\n        for _, d in gb:\n            dofs[d['node_number']] = d['dof']\n        return np.r_[0, np.cumsum(dofs)]",
  "def dof_of_grid(self, gb, g):\n        \"\"\" Obtain global indices of dof associated with a given grid.\n\n        Parameters:\n            gb: Grid_bucket representation of mixed-dimensional data.\n            g: Grid, one member of gb.\n\n        Returns:\n            np.array of ints: Indices of all dof for the given grid\n\n        \"\"\"\n        dof_list = self._dof_start_of_grids(gb)\n        nn = gb.node_props(g)['node_number']\n        return np.arange(dof_list[nn], dof_list[nn+1])",
  "class AbstractCoupling(object):\n\n    #------------------------------------------------------------------------------#\n\n    def __init__(self, discr=None, discr_ndof=None):\n\n        if discr_ndof is None:\n            self.discr_ndof = discr.ndof\n        else:\n            self.discr_ndof = discr_ndof\n\n#------------------------------------------------------------------------------#\n\n    def matrix_rhs(self, g_h, g_l, data_h, data_l, data_edge):\n        \"\"\"\n        Abstract method.\n        Return the matrix and righ-hand side for a suitable discretization.\n\n        Parameters:\n        -----------\n        g_h: grid of higher dimension\n        g_l: grid of lower dimension\n        data_h: dictionary which stores the data for the higher dimensional\n            grid\n        data_l: dictionary which stores the data for the lower dimensional\n            grid\n        data: dictionary which stores the data for the edges of the grid\n            bucket\n\n        Returns:\n        --------\n        cc: block matrix which store the contribution of the coupling\n            condition in the following order:\n            [ cc_hh  cc_hl ]\n            [ cc_lh  cc_ll ]\n            where:\n            - cc_hh is the contribution to be added to the global block\n              matrix in related to the grid of higher dimension (g_h).\n            - cc_ll is the contribution to be added to the global block\n              matrix in related to the grid of lower dimension (g_l).\n              In this case the term is null.\n            - cc_hl is the contribution to be added to the global block\n              matrix in related to the coupling between grid of higher\n              dimension (g_h) and the grid of lower dimension (g_l).\n            - cc_lh is the contribution to be added to the global block\n              matrix in related to the coupling between grid of lower\n              dimension (g_l) and the grid of higher dimension (g_h).\n              In this case cc_lh is the transpose of cc_hl.\n\n        \"\"\"\n        raise NotImplementedError(\"Method not implemented\")\n\n#------------------------------------------------------------------------------#\n\n    def create_block_matrix(self, g_h, g_l):\n        \"\"\"\n        Create the block matrix structure descibed in self.matrix_rhs\n\n        Parameters\n        ----------\n        g_h: grid of higher dimension\n        g_l: grid of lower dimension\n\n        Return\n        ------\n        dof: array containing the number of dofs for the higher and lower\n            dimensional grids, respectively.\n        matrix: sparse empty block matrix from the discretization.\n        \"\"\"\n\n        # Retrieve the number of degrees of both grids\n        dof = np.array([self.discr_ndof(g_h), self.discr_ndof(g_l)])\n\n        # Create the block matrix for the contributions\n        cc = np.array([sps.coo_matrix((i, j)) for i in dof for j in dof])\n\n        return dof, cc.reshape((2, 2))",
  "def __init__(self, discr=None, discr_ndof=None):\n\n        if discr_ndof is None:\n            self.discr_ndof = discr.ndof\n        else:\n            self.discr_ndof = discr_ndof",
  "def matrix_rhs(self, g_h, g_l, data_h, data_l, data_edge):\n        \"\"\"\n        Abstract method.\n        Return the matrix and righ-hand side for a suitable discretization.\n\n        Parameters:\n        -----------\n        g_h: grid of higher dimension\n        g_l: grid of lower dimension\n        data_h: dictionary which stores the data for the higher dimensional\n            grid\n        data_l: dictionary which stores the data for the lower dimensional\n            grid\n        data: dictionary which stores the data for the edges of the grid\n            bucket\n\n        Returns:\n        --------\n        cc: block matrix which store the contribution of the coupling\n            condition in the following order:\n            [ cc_hh  cc_hl ]\n            [ cc_lh  cc_ll ]\n            where:\n            - cc_hh is the contribution to be added to the global block\n              matrix in related to the grid of higher dimension (g_h).\n            - cc_ll is the contribution to be added to the global block\n              matrix in related to the grid of lower dimension (g_l).\n              In this case the term is null.\n            - cc_hl is the contribution to be added to the global block\n              matrix in related to the coupling between grid of higher\n              dimension (g_h) and the grid of lower dimension (g_l).\n            - cc_lh is the contribution to be added to the global block\n              matrix in related to the coupling between grid of lower\n              dimension (g_l) and the grid of higher dimension (g_h).\n              In this case cc_lh is the transpose of cc_hl.\n\n        \"\"\"\n        raise NotImplementedError(\"Method not implemented\")",
  "def create_block_matrix(self, g_h, g_l):\n        \"\"\"\n        Create the block matrix structure descibed in self.matrix_rhs\n\n        Parameters\n        ----------\n        g_h: grid of higher dimension\n        g_l: grid of lower dimension\n\n        Return\n        ------\n        dof: array containing the number of dofs for the higher and lower\n            dimensional grids, respectively.\n        matrix: sparse empty block matrix from the discretization.\n        \"\"\"\n\n        # Retrieve the number of degrees of both grids\n        dof = np.array([self.discr_ndof(g_h), self.discr_ndof(g_l)])\n\n        # Create the block matrix for the contributions\n        cc = np.array([sps.coo_matrix((i, j)) for i in dof for j in dof])\n\n        return dof, cc.reshape((2, 2))",
  "class Solver(object):\n\n#------------------------------------------------------------------------------#\n\n    def ndof(self, g):\n        \"\"\"\n        Abstract method.\n        Return the number of degrees of freedom associated to the method.\n\n        Parameter\n        ---------\n        g: grid, or a subclass.\n\n        Return\n        ------\n        dof: the number of degrees of freedom.\n\n        \"\"\"\n        raise NotImplementedError(\"Method not implemented\")\n\n#------------------------------------------------------------------------------#\n\n    def matrix_rhs(self, g, data):\n        \"\"\"\n        Abstract method.\n        Return the matrix and righ-hand side for a suitable discretization.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data.\n\n        Return\n        ------\n        matrix: sparse matrix (self.ndof x self.ndof) from the discretization.\n        rhs: array (self.ndof)\n            Right-hand side of the problem.\n        \"\"\"\n        raise NotImplementedError(\"Method not implemented\")",
  "class SolverMixedDim():\n\n    def __init__(self):\n        pass\n\n    def matrix_rhs(self, gb):\n        return self.solver.matrix_rhs(gb)\n\n    def split(self, gb, key, values):\n        return self.solver.split(gb, key, values)\n\n    def ndof(self, gb):\n        return np.sum([self.discr_ndof(g) for g,_ in gb])",
  "def ndof(self, g):\n        \"\"\"\n        Abstract method.\n        Return the number of degrees of freedom associated to the method.\n\n        Parameter\n        ---------\n        g: grid, or a subclass.\n\n        Return\n        ------\n        dof: the number of degrees of freedom.\n\n        \"\"\"\n        raise NotImplementedError(\"Method not implemented\")",
  "def matrix_rhs(self, g, data):\n        \"\"\"\n        Abstract method.\n        Return the matrix and righ-hand side for a suitable discretization.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data.\n\n        Return\n        ------\n        matrix: sparse matrix (self.ndof x self.ndof) from the discretization.\n        rhs: array (self.ndof)\n            Right-hand side of the problem.\n        \"\"\"\n        raise NotImplementedError(\"Method not implemented\")",
  "def __init__(self):\n        pass",
  "def matrix_rhs(self, gb):\n        return self.solver.matrix_rhs(gb)",
  "def split(self, gb, key, values):\n        return self.solver.split(gb, key, values)",
  "def ndof(self, gb):\n        return np.sum([self.discr_ndof(g) for g,_ in gb])",
  "class HybridDualVEM(Solver):\n\n#------------------------------------------------------------------------------#\n\n    def __init__(self, physics='flow'):\n        self.physics = physics\n\n    def ndof(self, g):\n        \"\"\"\n        Return the number of degrees of freedom associated to the method.\n        In this case number of faces (hybrid dofs).\n\n        Parameter\n        ---------\n        g: grid, or a subclass.\n\n        Return\n        ------\n        dof: the number of degrees of freedom.\n\n        \"\"\"\n        return g.num_faces\n\n#------------------------------------------------------------------------------#\n\n    def matrix_rhs(self, g, data):\n        \"\"\"\n        Return the matrix and righ-hand side for a discretization of a second\n        order elliptic equation using hybrid dual virtual element method.\n        The name of data in the input dictionary (data) are:\n        perm : tensor.SecondOrder\n            Permeability defined cell-wise. If not given a identity permeability\n            is assumed and a warning arised.\n        source : array (self.g.num_cells)\n            Scalar source term defined cell-wise. If not given a zero source\n            term is assumed and a warning arised.\n        bc : boundary conditions (optional)\n        bc_val : dictionary (optional)\n            Values of the boundary conditions. The dictionary has at most the\n            following keys: 'dir' and 'neu', for Dirichlet and Neumann boundary\n            conditions, respectively.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data.\n\n        Return\n        ------\n        matrix: sparse csr (g.num_faces+g_num_cells, g.num_faces+g_num_cells)\n            Saddle point matrix obtained from the discretization.\n        rhs: array (g.num_faces+g_num_cells)\n            Right-hand side which contains the boundary conditions and the scalar\n            source term.\n\n        Examples\n        --------\n        b_faces_neu = ... # id of the Neumann faces\n        b_faces_dir = ... # id of the Dirichlet faces\n        bnd = bc.BoundaryCondition(g, np.hstack((b_faces_dir, b_faces_neu)),\n                                ['dir']*b_faces_dir.size + ['neu']*b_faces_neu.size)\n        bnd_val = {'dir': fun_dir(g.face_centers[:, b_faces_dir]),\n                   'neu': fun_neu(f.face_centers[:, b_faces_neu])}\n\n        data = {'perm': perm, 'source': f, 'bc': bnd, 'bc_val': bnd_val}\n\n        H, rhs = hybrid.matrix_rhs(g, data)\n        l = sps.linalg.spsolve(H, rhs)\n        u, p = hybrid.compute_up(g, l, data)\n        P0u = dual.project_u(g, perm, u)\n\n        \"\"\"\n        # pylint: disable=invalid-name\n\n        # If a 0-d grid is given then we return an identity matrix\n        if g.dim == 0:\n            return sps.identity(self.ndof(g), format=\"csr\"), np.zeros(1)\n\n        param = data['param']\n        k = param.get_tensor(self)\n        f = param.get_source(self)\n        bc = param.get_bc(self)\n        bc_val = param.get_bc_val(self)\n        a = param.aperture\n\n        faces, _, sgn = sps.find(g.cell_faces)\n\n        # Map the domain to a reference geometry (i.e. equivalent to compute\n        # surface coordinates in 1d and 2d)\n        c_centers, f_normals, f_centers, _, _, _ = cg.map_grid(g)\n\n        # Weight for the stabilization term\n        diams = g.cell_diameters()\n        weight = np.power(diams, 2-g.dim)\n\n        # Allocate the data to store matrix entries, that's the most efficient\n        # way to create a sparse matrix.\n        size = np.sum(np.square(g.cell_faces.indptr[1:] - \\\n                                g.cell_faces.indptr[:-1]))\n        I = np.empty(size, dtype=np.int)\n        J = np.empty(size, dtype=np.int)\n        data = np.empty(size)\n        rhs = np.zeros(g.num_faces)\n\n        idx = 0\n        massHdiv = dual.DualVEM().massHdiv\n\n        for c in np.arange(g.num_cells):\n            # For the current cell retrieve its faces\n            loc = slice(g.cell_faces.indptr[c], g.cell_faces.indptr[c+1])\n            faces_loc = faces[loc]\n            ndof = faces_loc.size\n\n            # Retrieve permeability and normals assumed outward to the cell.\n            sgn_loc = sgn[loc].reshape((-1, 1))\n            normals = np.multiply(np.tile(sgn_loc.T, (g.dim, 1)),\n                                  f_normals[:, faces_loc])\n\n            # Compute the H_div-mass local matrix\n            A = massHdiv(k.perm[0:g.dim, 0:g.dim, c], c_centers[:, c],\n                         a[c]*g.cell_volumes[c], f_centers[:, faces_loc],\n                         a[c]*normals, np.ones(ndof), diams[c], weight[c])[0]\n            # Compute the Div local matrix\n            B = -np.ones((ndof, 1))\n            # Compute the hybrid local matrix\n            C = np.eye(ndof, ndof)\n\n            # Perform the static condensation to compute the hybrid local matrix\n            invA = np.linalg.inv(A)\n            S = 1/np.dot(B.T, np.dot(invA, B))\n            L = np.dot(np.dot(invA, np.dot(B, np.dot(S, B.T))), invA)\n            L = np.dot(np.dot(C.T, L - invA), C)\n\n            # Compute the local hybrid right using the static condensation\n            rhs[faces_loc] += np.dot(C.T,\n                                     np.dot(invA,\n                                            np.dot(B, np.dot(S, f[c]))))[:, 0]\n\n            # Save values for hybrid matrix\n            cols = np.tile(faces_loc, (faces_loc.size, 1))\n            loc_idx = slice(idx, idx+cols.size)\n            I[loc_idx] = cols.T.ravel()\n            J[loc_idx] = cols.ravel()\n            data[loc_idx] = L.ravel()\n            idx += cols.size\n\n        # construct the global matrices\n        H = sps.coo_matrix((data, (I, J))).tocsr()\n\n        # Apply the boundary conditions\n        if bc is not None:\n\n            if np.any(bc.is_dir):\n                norm = sps.linalg.norm(H, np.inf)\n                is_dir = np.where(bc.is_dir)[0]\n\n                H[is_dir, :] *= 0\n                H[is_dir, is_dir] = norm\n                rhs[is_dir] = norm*bc_val[is_dir]\n\n            if np.any(bc.is_neu):\n                faces, _, sgn = sps.find(g.cell_faces)\n                sgn = sgn[np.unique(faces, return_index=True)[1]]\n\n                is_neu = np.where(bc.is_neu)[0]\n                rhs[is_neu] += sgn[is_neu]*bc_val[is_neu]*g.face_areas[is_neu]\n\n        return H, rhs\n\n#------------------------------------------------------------------------------#\n\n    def compute_up(self, g, l, data):\n        \"\"\"\n        Return the velocity and pressure computed from the hybrid variables.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        l : array (g.num_faces) Hybrid solution of the system.\n        data: dictionary to store the data. See self.matrix_rhs for a detaild\n            description.\n\n        Return\n        ------\n        u : array (g.num_faces) Velocity at each face.\n        p : array (g.num_cells) Pressure at each cell.\n\n        \"\"\"\n        # pylint: disable=invalid-name\n\n        if g.dim == 0:\n            return 0, l[0]\n\n        param = data['param']\n        k = param.get_tensor(self)\n        f = param.get_source(self)\n        a = param.aperture\n\n        faces, _, sgn = sps.find(g.cell_faces)\n\n        # Map the domain to a reference geometry (i.e. equivalent to compute\n        # surface coordinates in 1d and 2d)\n        c_centers, f_normals, f_centers, _, _, _ = cg.map_grid(g)\n\n        # Weight for the stabilization term\n        diams = g.cell_diameters()\n        weight = np.power(diams, 2-g.dim)\n\n        # Allocation of the pressure and velocity vectors\n        p = np.zeros(g.num_cells)\n        u = np.zeros(g.num_faces)\n        massHdiv = dual.DualVEM().massHdiv\n\n        for c in np.arange(g.num_cells):\n            # For the current cell retrieve its faces\n            loc = slice(g.cell_faces.indptr[c], g.cell_faces.indptr[c+1])\n            faces_loc = faces[loc]\n            ndof = faces_loc.size\n\n            # Retrieve permeability and normals assumed outward to the cell.\n            sgn_loc = sgn[loc].reshape((-1, 1))\n            normals = np.multiply(np.tile(sgn_loc.T, (g.dim, 1)),\n                                  f_normals[:, faces_loc])\n\n            # Compute the H_div-mass local matrix\n            A = massHdiv(k.perm[0:g.dim, 0:g.dim, c], c_centers[:, c],\n                         a[c]*g.cell_volumes[c], f_centers[:, faces_loc],\n                         a[c]*normals, np.ones(ndof), diams[c], weight[c])[0]\n            # Compute the Div local matrix\n            B = -np.ones((ndof, 1))\n            # Compute the hybrid local matrix\n            C = np.eye(ndof, ndof)\n\n            # Perform the static condensation to compute the pressure and velocity\n            S = 1/np.dot(B.T, solve(A, B))\n            l_loc = l[faces_loc].reshape((-1, 1))\n\n            p[c] = np.dot(S, f[c] - np.dot(B.T, solve(A, np.dot(C, l_loc))))\n            u[faces_loc] = -np.multiply(sgn_loc, solve(A, np.dot(B, p[c]) + \\\n                                                       np.dot(C, l_loc)))\n\n        return u, p",
  "def __init__(self, physics='flow'):\n        self.physics = physics",
  "def ndof(self, g):\n        \"\"\"\n        Return the number of degrees of freedom associated to the method.\n        In this case number of faces (hybrid dofs).\n\n        Parameter\n        ---------\n        g: grid, or a subclass.\n\n        Return\n        ------\n        dof: the number of degrees of freedom.\n\n        \"\"\"\n        return g.num_faces",
  "def matrix_rhs(self, g, data):\n        \"\"\"\n        Return the matrix and righ-hand side for a discretization of a second\n        order elliptic equation using hybrid dual virtual element method.\n        The name of data in the input dictionary (data) are:\n        perm : tensor.SecondOrder\n            Permeability defined cell-wise. If not given a identity permeability\n            is assumed and a warning arised.\n        source : array (self.g.num_cells)\n            Scalar source term defined cell-wise. If not given a zero source\n            term is assumed and a warning arised.\n        bc : boundary conditions (optional)\n        bc_val : dictionary (optional)\n            Values of the boundary conditions. The dictionary has at most the\n            following keys: 'dir' and 'neu', for Dirichlet and Neumann boundary\n            conditions, respectively.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data.\n\n        Return\n        ------\n        matrix: sparse csr (g.num_faces+g_num_cells, g.num_faces+g_num_cells)\n            Saddle point matrix obtained from the discretization.\n        rhs: array (g.num_faces+g_num_cells)\n            Right-hand side which contains the boundary conditions and the scalar\n            source term.\n\n        Examples\n        --------\n        b_faces_neu = ... # id of the Neumann faces\n        b_faces_dir = ... # id of the Dirichlet faces\n        bnd = bc.BoundaryCondition(g, np.hstack((b_faces_dir, b_faces_neu)),\n                                ['dir']*b_faces_dir.size + ['neu']*b_faces_neu.size)\n        bnd_val = {'dir': fun_dir(g.face_centers[:, b_faces_dir]),\n                   'neu': fun_neu(f.face_centers[:, b_faces_neu])}\n\n        data = {'perm': perm, 'source': f, 'bc': bnd, 'bc_val': bnd_val}\n\n        H, rhs = hybrid.matrix_rhs(g, data)\n        l = sps.linalg.spsolve(H, rhs)\n        u, p = hybrid.compute_up(g, l, data)\n        P0u = dual.project_u(g, perm, u)\n\n        \"\"\"\n        # pylint: disable=invalid-name\n\n        # If a 0-d grid is given then we return an identity matrix\n        if g.dim == 0:\n            return sps.identity(self.ndof(g), format=\"csr\"), np.zeros(1)\n\n        param = data['param']\n        k = param.get_tensor(self)\n        f = param.get_source(self)\n        bc = param.get_bc(self)\n        bc_val = param.get_bc_val(self)\n        a = param.aperture\n\n        faces, _, sgn = sps.find(g.cell_faces)\n\n        # Map the domain to a reference geometry (i.e. equivalent to compute\n        # surface coordinates in 1d and 2d)\n        c_centers, f_normals, f_centers, _, _, _ = cg.map_grid(g)\n\n        # Weight for the stabilization term\n        diams = g.cell_diameters()\n        weight = np.power(diams, 2-g.dim)\n\n        # Allocate the data to store matrix entries, that's the most efficient\n        # way to create a sparse matrix.\n        size = np.sum(np.square(g.cell_faces.indptr[1:] - \\\n                                g.cell_faces.indptr[:-1]))\n        I = np.empty(size, dtype=np.int)\n        J = np.empty(size, dtype=np.int)\n        data = np.empty(size)\n        rhs = np.zeros(g.num_faces)\n\n        idx = 0\n        massHdiv = dual.DualVEM().massHdiv\n\n        for c in np.arange(g.num_cells):\n            # For the current cell retrieve its faces\n            loc = slice(g.cell_faces.indptr[c], g.cell_faces.indptr[c+1])\n            faces_loc = faces[loc]\n            ndof = faces_loc.size\n\n            # Retrieve permeability and normals assumed outward to the cell.\n            sgn_loc = sgn[loc].reshape((-1, 1))\n            normals = np.multiply(np.tile(sgn_loc.T, (g.dim, 1)),\n                                  f_normals[:, faces_loc])\n\n            # Compute the H_div-mass local matrix\n            A = massHdiv(k.perm[0:g.dim, 0:g.dim, c], c_centers[:, c],\n                         a[c]*g.cell_volumes[c], f_centers[:, faces_loc],\n                         a[c]*normals, np.ones(ndof), diams[c], weight[c])[0]\n            # Compute the Div local matrix\n            B = -np.ones((ndof, 1))\n            # Compute the hybrid local matrix\n            C = np.eye(ndof, ndof)\n\n            # Perform the static condensation to compute the hybrid local matrix\n            invA = np.linalg.inv(A)\n            S = 1/np.dot(B.T, np.dot(invA, B))\n            L = np.dot(np.dot(invA, np.dot(B, np.dot(S, B.T))), invA)\n            L = np.dot(np.dot(C.T, L - invA), C)\n\n            # Compute the local hybrid right using the static condensation\n            rhs[faces_loc] += np.dot(C.T,\n                                     np.dot(invA,\n                                            np.dot(B, np.dot(S, f[c]))))[:, 0]\n\n            # Save values for hybrid matrix\n            cols = np.tile(faces_loc, (faces_loc.size, 1))\n            loc_idx = slice(idx, idx+cols.size)\n            I[loc_idx] = cols.T.ravel()\n            J[loc_idx] = cols.ravel()\n            data[loc_idx] = L.ravel()\n            idx += cols.size\n\n        # construct the global matrices\n        H = sps.coo_matrix((data, (I, J))).tocsr()\n\n        # Apply the boundary conditions\n        if bc is not None:\n\n            if np.any(bc.is_dir):\n                norm = sps.linalg.norm(H, np.inf)\n                is_dir = np.where(bc.is_dir)[0]\n\n                H[is_dir, :] *= 0\n                H[is_dir, is_dir] = norm\n                rhs[is_dir] = norm*bc_val[is_dir]\n\n            if np.any(bc.is_neu):\n                faces, _, sgn = sps.find(g.cell_faces)\n                sgn = sgn[np.unique(faces, return_index=True)[1]]\n\n                is_neu = np.where(bc.is_neu)[0]\n                rhs[is_neu] += sgn[is_neu]*bc_val[is_neu]*g.face_areas[is_neu]\n\n        return H, rhs",
  "def compute_up(self, g, l, data):\n        \"\"\"\n        Return the velocity and pressure computed from the hybrid variables.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        l : array (g.num_faces) Hybrid solution of the system.\n        data: dictionary to store the data. See self.matrix_rhs for a detaild\n            description.\n\n        Return\n        ------\n        u : array (g.num_faces) Velocity at each face.\n        p : array (g.num_cells) Pressure at each cell.\n\n        \"\"\"\n        # pylint: disable=invalid-name\n\n        if g.dim == 0:\n            return 0, l[0]\n\n        param = data['param']\n        k = param.get_tensor(self)\n        f = param.get_source(self)\n        a = param.aperture\n\n        faces, _, sgn = sps.find(g.cell_faces)\n\n        # Map the domain to a reference geometry (i.e. equivalent to compute\n        # surface coordinates in 1d and 2d)\n        c_centers, f_normals, f_centers, _, _, _ = cg.map_grid(g)\n\n        # Weight for the stabilization term\n        diams = g.cell_diameters()\n        weight = np.power(diams, 2-g.dim)\n\n        # Allocation of the pressure and velocity vectors\n        p = np.zeros(g.num_cells)\n        u = np.zeros(g.num_faces)\n        massHdiv = dual.DualVEM().massHdiv\n\n        for c in np.arange(g.num_cells):\n            # For the current cell retrieve its faces\n            loc = slice(g.cell_faces.indptr[c], g.cell_faces.indptr[c+1])\n            faces_loc = faces[loc]\n            ndof = faces_loc.size\n\n            # Retrieve permeability and normals assumed outward to the cell.\n            sgn_loc = sgn[loc].reshape((-1, 1))\n            normals = np.multiply(np.tile(sgn_loc.T, (g.dim, 1)),\n                                  f_normals[:, faces_loc])\n\n            # Compute the H_div-mass local matrix\n            A = massHdiv(k.perm[0:g.dim, 0:g.dim, c], c_centers[:, c],\n                         a[c]*g.cell_volumes[c], f_centers[:, faces_loc],\n                         a[c]*normals, np.ones(ndof), diams[c], weight[c])[0]\n            # Compute the Div local matrix\n            B = -np.ones((ndof, 1))\n            # Compute the hybrid local matrix\n            C = np.eye(ndof, ndof)\n\n            # Perform the static condensation to compute the pressure and velocity\n            S = 1/np.dot(B.T, solve(A, B))\n            l_loc = l[faces_loc].reshape((-1, 1))\n\n            p[c] = np.dot(S, f[c] - np.dot(B.T, solve(A, np.dot(C, l_loc))))\n            u[faces_loc] = -np.multiply(sgn_loc, solve(A, np.dot(B, p[c]) + \\\n                                                       np.dot(C, l_loc)))\n\n        return u, p",
  "class Integral(Solver):\n    '''\n    Discretization of the integrated source term\n    int q * dx\n    over each grid cell.\n\n    All this function does is returning a zero lhs and\n    rhs = - param.get_source.physics in a saddle point fashion.\n    '''\n\n    def __init__(self, physics='flow'):\n        self.physics = physics\n        Solver.__init__(self)\n\n    def ndof(self, g):\n        return g.num_cells + g.num_faces\n\n    def matrix_rhs(self, g, data):\n        param = data['param']\n        sources = param.get_source(self)\n        lhs = sps.csc_matrix((self.ndof(g), self.ndof(g)))\n        assert sources.size == g.num_cells, \\\n                                 'There should be one soure value for each cell'\n\n        rhs = np.zeros(self.ndof(g))\n        is_p = np.hstack((np.zeros(g.num_faces, dtype=np.bool),\n                          np.ones(g.num_cells, dtype=np.bool)))\n\n        rhs[is_p] = -sources\n\n        return lhs, rhs",
  "class IntegralMixedDim(SolverMixedDim):\n    def __init__(self, physics='flow'):\n        self.physics = physics\n\n        self.discr = Integral(self.physics)\n        self.discr_ndof = self.discr.ndof\n        self.coupling_conditions = None\n\n        self.solver = Coupler(self.discr)\n        SolverMixedDim.__init__(self)",
  "class IntegralDFN(SolverMixedDim):\n    def __init__(self, dim_max, physics='flow'):\n        # NOTE: There is no flow along the intersections of the fractures.\n        # In this case a mixed solver is considered. We assume only two\n        # (contiguous) dimensions active. In the higher dimensional grid the\n        # physical problem is discretise with a vem solver and in the lower\n        # dimensional grid Lagrange multiplier are used to \"glue\" the velocity\n        # dof at the interface between the fractures.\n        # For this reason the matrix_rhs and ndof need to be carefully taking\n        # care.\n\n        self.physics = physics\n        self.dim_max = dim_max\n\n        self.discr = Integral(self.physics)\n        self.coupling_conditions = None\n\n        kwargs = {\"discr_ndof\": self.__ndof__,\n                  \"discr_fct\": self.__matrix_rhs__}\n        self.solver = Coupler(coupling = None, **kwargs)\n        SolverMixedDim.__init__(self)\n\n    def __ndof__(self, g):\n        # The highest dimensional problem has the standard number of dof\n        # associated with the solver. For the lower dimensional problems, the\n        # number of dof is the number of cells.\n        if g.dim == self.dim_max:\n            return self.discr.ndof(g)\n        else:\n            return g.num_cells\n\n    def __matrix_rhs__(self, g, data):\n        # The highest dimensional problem compute the matrix and rhs, the lower\n        # dimensional problem and empty matrix. For the latter, the size of the\n        # matrix is the number of cells.\n        if g.dim == self.dim_max:\n            return self.discr.matrix_rhs(g, data)\n        else:\n            ndof = self.__ndof__(g)\n            return sps.csr_matrix((ndof, ndof)), np.zeros(ndof)",
  "def __init__(self, physics='flow'):\n        self.physics = physics\n        Solver.__init__(self)",
  "def ndof(self, g):\n        return g.num_cells + g.num_faces",
  "def matrix_rhs(self, g, data):\n        param = data['param']\n        sources = param.get_source(self)\n        lhs = sps.csc_matrix((self.ndof(g), self.ndof(g)))\n        assert sources.size == g.num_cells, \\\n                                 'There should be one soure value for each cell'\n\n        rhs = np.zeros(self.ndof(g))\n        is_p = np.hstack((np.zeros(g.num_faces, dtype=np.bool),\n                          np.ones(g.num_cells, dtype=np.bool)))\n\n        rhs[is_p] = -sources\n\n        return lhs, rhs",
  "def __init__(self, physics='flow'):\n        self.physics = physics\n\n        self.discr = Integral(self.physics)\n        self.discr_ndof = self.discr.ndof\n        self.coupling_conditions = None\n\n        self.solver = Coupler(self.discr)\n        SolverMixedDim.__init__(self)",
  "def __init__(self, dim_max, physics='flow'):\n        # NOTE: There is no flow along the intersections of the fractures.\n        # In this case a mixed solver is considered. We assume only two\n        # (contiguous) dimensions active. In the higher dimensional grid the\n        # physical problem is discretise with a vem solver and in the lower\n        # dimensional grid Lagrange multiplier are used to \"glue\" the velocity\n        # dof at the interface between the fractures.\n        # For this reason the matrix_rhs and ndof need to be carefully taking\n        # care.\n\n        self.physics = physics\n        self.dim_max = dim_max\n\n        self.discr = Integral(self.physics)\n        self.coupling_conditions = None\n\n        kwargs = {\"discr_ndof\": self.__ndof__,\n                  \"discr_fct\": self.__matrix_rhs__}\n        self.solver = Coupler(coupling = None, **kwargs)\n        SolverMixedDim.__init__(self)",
  "def __ndof__(self, g):\n        # The highest dimensional problem has the standard number of dof\n        # associated with the solver. For the lower dimensional problems, the\n        # number of dof is the number of cells.\n        if g.dim == self.dim_max:\n            return self.discr.ndof(g)\n        else:\n            return g.num_cells",
  "def __matrix_rhs__(self, g, data):\n        # The highest dimensional problem compute the matrix and rhs, the lower\n        # dimensional problem and empty matrix. For the latter, the size of the\n        # matrix is the number of cells.\n        if g.dim == self.dim_max:\n            return self.discr.matrix_rhs(g, data)\n        else:\n            ndof = self.__ndof__(g)\n            return sps.csr_matrix((ndof, ndof)), np.zeros(ndof)",
  "class DualVEMMixedDim(SolverMixedDim):\n\n    def __init__(self, physics='flow'):\n        self.physics = physics\n\n        self.discr = DualVEM(self.physics)\n        self.discr_ndof = self.discr.ndof\n        self.coupling_conditions = DualCoupling(self.discr)\n\n        self.solver = Coupler(self.discr, self.coupling_conditions)\n\n    def extract_u(self, gb, up, u):\n        gb.add_node_props([u])\n        for g, d in gb:\n            d[u] = self.discr.extract_u(g, d[up])\n\n    def extract_p(self, gb, up, p):\n        gb.add_node_props([p])\n        for g, d in gb:\n            d[p] = self.discr.extract_p(g, d[up])\n\n    def project_u(self, gb, u, P0u):\n        gb.add_node_props([P0u])\n        for g, d in gb:\n            d[P0u] = self.discr.project_u(g, d[u], d)\n\n    def check_conservation(self, gb, u, conservation):\n        \"\"\"\n        Assert if the local conservation of mass is preserved for the grid\n        bucket.\n        Parameters\n        ----------\n        gb: grid bucket, or a subclass.\n        u : string name of the velocity in the data associated to gb.\n        conservation: string name for the conservation of mass.\n        \"\"\"\n        for g, d in gb:\n            d[conservation] = self.discr.check_conservation(g, d[u])\n\n        # add to the lower dimensional grids the contribution from the higher\n        # dimensional grids\n        for e, data in gb.edges_props():\n            g_l, g_h = gb.sorted_nodes_of_edge(e)\n\n            cells_l, faces_h, _ = sps.find(data['face_cells'])\n            faces, cells_h, sign = sps.find(g_h.cell_faces)\n            ind = np.unique(faces, return_index=True)[1]\n            sign = sign[ind][faces_h]\n\n            conservation_l = gb.node_prop(g_l, conservation)\n            u_h = sign*gb.node_prop(g_h, u)[faces_h]\n\n            for c_l, u_f in zip(cells_l, u_h):\n                conservation_l[c_l] -= u_f\n\n        for g, d in gb:\n            print(np.amax(np.abs(d[conservation])))",
  "class DualVEMDFN(SolverMixedDim):\n\n    def __init__(self, dim_max, physics='flow'):\n        # NOTE: There is no flow along the intersections of the fractures.\n\n        self.physics = physics\n        self.dim_max = dim_max\n        self.discr = DualVEM(self.physics)\n\n        self.coupling_conditions = DualCouplingDFN(self.__ndof__)\n\n        kwargs = {\"discr_ndof\": self.__ndof__,\n                  \"discr_fct\": self.__matrix_rhs__}\n        self.solver = Coupler(coupling = self.coupling_conditions, **kwargs)\n        SolverMixedDim.__init__(self)\n\n    def extract_u(self, gb, up, u):\n        for g, d in gb:\n            if g.dim == self.dim_max:\n                d[u] = self.discr.extract_u(g, d[up])\n            else:\n                d[u] = np.zeros(g.num_faces)\n\n    def extract_p(self, gb, up, p):\n        for g, d in gb:\n            if g.dim == self.dim_max:\n                d[p] = self.discr.extract_p(g, d[up])\n            else:\n                d[p] = d[up]\n\n    def project_u(self, gb, u, P0u):\n        for g, d in gb:\n            if g.dim == self.dim_max:\n                d[P0u] = self.discr.project_u(g, d[u], d)\n            else:\n                d[P0u] = np.zeros((3, g.num_cells))\n\n    def __ndof__(self, g):\n        # The highest dimensional problem has the standard number of dof\n        # associated with the solver. For the lower dimensional problems, the\n        # number of dof is the number of cells.\n        if g.dim == self.dim_max:\n            return self.discr.ndof(g)\n        else:\n            return g.num_cells\n\n    def __matrix_rhs__(self, g, data):\n        # The highest dimensional problem compute the matrix and rhs, the lower\n        # dimensional problem and empty matrix. For the latter, the size of the\n        # matrix is the number of cells.\n        if g.dim == self.dim_max:\n            return self.discr.matrix_rhs(g, data)\n        else:\n            ndof = self.__ndof__(g)\n            return sps.csr_matrix((ndof, ndof)), np.zeros(ndof)",
  "class DualVEM(Solver):\n\n#------------------------------------------------------------------------------#\n\n    def __init__(self, physics='flow'):\n        self.physics = physics\n\n#------------------------------------------------------------------------------#\n\n    def ndof(self, g):\n        \"\"\"\n        Return the number of degrees of freedom associated to the method.\n        In this case number of faces (velocity dofs) plus the number of cells\n        (pressure dof).\n\n        Parameter\n        ---------\n        g: grid, or a subclass.\n\n        Return\n        ------\n        dof: the number of degrees of freedom.\n\n        \"\"\"\n        return g.num_cells + g.num_faces\n\n#------------------------------------------------------------------------------#\n\n    def matrix_rhs(self, g, data):\n        \"\"\"\n        Return the matrix and righ-hand side for a discretization of a second\n        order elliptic equation using dual virtual element method.\n        The name of data in the input dictionary (data) are:\n        perm : second_order_tensor\n            Permeability defined cell-wise.\n        source : array (self.g.num_cells)\n            Scalar source term defined cell-wise. If not given a zero source\n            term is assumed and a warning arised.\n        bc : boundary conditions (optional)\n        bc_val : dictionary (optional)\n            Values of the boundary conditions. The dictionary has at most the\n            following keys: 'dir' and 'neu', for Dirichlet and Neumann boundary\n            conditions, respectively.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data.\n\n        Return\n        ------\n        matrix: sparse csr (g.num_faces+g_num_cells, g.num_faces+g_num_cells)\n            Saddle point matrix obtained from the discretization.\n        rhs: array (g.num_faces+g_num_cells)\n            Right-hand side which contains the boundary conditions and the scalar\n            source term.\n\n        Examples\n        --------\n        b_faces_neu = ... # id of the Neumann faces\n        b_faces_dir = ... # id of the Dirichlet faces\n        bnd = bc.BoundaryCondition(g, np.hstack((b_faces_dir, b_faces_neu)),\n                                ['dir']*b_faces_dir.size + ['neu']*b_faces_neu.size)\n        bnd_val = {'dir': fun_dir(g.face_centers[:, b_faces_dir]),\n                   'neu': fun_neu(f.face_centers[:, b_faces_neu])}\n\n        data = {'perm': perm, 'source': f, 'bc': bnd, 'bc_val': bnd_val}\n\n        D, rhs = dual.matrix_rhs(g, data)\n        up = sps.linalg.spsolve(D, rhs)\n        u = dual.extract_u(g, up)\n        p = dual.extract_p(g, up)\n        P0u = dual.project_u(g, u, perm)\n\n        \"\"\"\n        M, bc_weight = self.matrix(g, data, bc_weight=True)\n        return M, self.rhs(g, data, bc_weight)\n\n#------------------------------------------------------------------------------#\n\n    def matrix(self, g, data, bc_weight=False):\n        \"\"\"\n        Return the matrix for a discretization of a second order elliptic equation\n        using dual virtual element method. See self.matrix_rhs for a detaild\n        description.\n\n        Additional parameter:\n        --------------------\n        bc_weight: to compute the infinity norm of the matrix and use it as a\n            weight to impose the boundary conditions. Default True.\n\n        Additional return:\n        weight: if bc_weight is True return the weight computed.\n\n        \"\"\"\n        # Allow short variable names in backend function\n        # pylint: disable=invalid-name\n\n        # If a 0-d grid is given then we return an identity matrix\n        if g.dim == 0:\n            M = sps.dia_matrix(([1, 0], 0), (self.ndof(g), self.ndof(g)))\n            if bc_weight:\n                return M, 1\n            return M\n\n        # Retrieve the permeability, boundary conditions, and aperture\n        # The aperture is needed in the hybrid-dimensional case, otherwise is\n        # assumed unitary\n        param = data['param']\n        k = param.get_tensor(self)\n        bc = param.get_bc(self)\n        a = param.get_aperture()\n\n        faces, cells, sign = sps.find(g.cell_faces)\n        index = np.argsort(cells)\n        faces, sign = faces[index], sign[index]\n\n        # Map the domain to a reference geometry (i.e. equivalent to compute\n        # surface coordinates in 1d and 2d)\n        c_centers, f_normals, f_centers, R, dim, _ = cg.map_grid(g)\n\n        if not data.get('is_tangential', False):\n                # Rotate the permeability tensor and delete last dimension\n                if g.dim < 3:\n                    k = k.copy()\n                    k.rotate(R)\n                    remove_dim = np.where(np.logical_not(dim))[0]\n                    k.perm = np.delete(k.perm, (remove_dim), axis=0)\n                    k.perm = np.delete(k.perm, (remove_dim), axis=1)\n\n        # In the virtual cell approach the cell diameters should involve the\n        # apertures, however to keep consistency with the hybrid-dimensional\n        # approach and with the related hypotheses we avoid.\n        diams = g.cell_diameters()\n        # Weight for the stabilization term\n        weight = np.power(diams, 2-g.dim)\n\n        # Allocate the data to store matrix entries, that's the most efficient\n        # way to create a sparse matrix.\n        size = np.sum(np.square(g.cell_faces.indptr[1:]-\\\n                                g.cell_faces.indptr[:-1]))\n        I = np.empty(size, dtype=np.int)\n        J = np.empty(size, dtype=np.int)\n        dataIJ = np.empty(size)\n        idx = 0\n\n        for c in np.arange(g.num_cells):\n            # For the current cell retrieve its faces\n            loc = slice(g.cell_faces.indptr[c], g.cell_faces.indptr[c+1])\n            faces_loc = faces[loc]\n\n            # Compute the H_div-mass local matrix\n            A = self.massHdiv(a[c]*k.perm[0:g.dim, 0:g.dim, c], c_centers[:, c],\n                              g.cell_volumes[c], f_centers[:, faces_loc],\n                              f_normals[:, faces_loc], sign[loc],\n                              diams[c], weight[c])[0]\n\n            # Save values for Hdiv-mass local matrix in the global structure\n            cols = np.tile(faces_loc, (faces_loc.size, 1))\n            loc_idx = slice(idx, idx+cols.size)\n            I[loc_idx] = cols.T.ravel()\n            J[loc_idx] = cols.ravel()\n            dataIJ[loc_idx] = A.ravel()\n            idx += cols.size\n\n        # Construct the global matrices\n        mass = sps.coo_matrix((dataIJ, (I, J)))\n        div = -g.cell_faces.T\n        M = sps.bmat([[mass, div.T],\n                      [ div,  None]], format='csr')\n\n        norm = sps.linalg.norm(mass, np.inf) if bc_weight else 1\n\n        # assign the Neumann boundary conditions\n        # For dual discretizations, internal boundaries\n        # are handled by assigning Dirichlet conditions. THus, we remove them\n        # from the is_neu (where they belong by default) and add them in\n        # is_dir.\n        is_neu = np.logical_and(bc.is_neu, np.logical_not(bc.is_internal))\n        if bc and np.any(is_neu):\n            is_neu = np.hstack((is_neu,\n                                np.zeros(g.num_cells, dtype=np.bool)))\n            M[is_neu, :] *= 0\n            M[is_neu, is_neu] = norm\n\n        if bc_weight:\n            return M, norm\n        return M\n\n#------------------------------------------------------------------------------#\n\n    def rhs(self, g, data, bc_weight=1):\n        \"\"\"\n        Return the righ-hand side for a discretization of a second order elliptic\n        equation using dual virtual element method. See self.matrix_rhs for a detaild\n        description.\n\n        Additional parameter:\n        --------------------\n        bc_weight: to use the infinity norm of the matrix to impose the\n            boundary conditions. Default 1.\n\n        \"\"\"\n        # Allow short variable names in backend function\n        # pylint: disable=invalid-name\n\n        param = data['param']\n        f = param.get_source(self)\n\n        if g.dim == 0:\n            return np.hstack(([0], f))\n\n        bc = param.get_bc(self)\n        bc_val = param.get_bc_val(self)\n\n        assert not bool(bc is None) != bool(bc_val is None)\n\n        rhs = np.zeros(self.ndof(g))\n        if bc is None:\n            return rhs\n\n        is_p = np.hstack((np.zeros(g.num_faces, dtype=np.bool),\n                          np.ones(g.num_cells, dtype=np.bool)))\n        # For dual discretizations, internal boundaries\n        # are handled by assigning Dirichlet conditions. Thus, we remove them\n        # from the is_neu (where they belong by default). As the dirichlet\n        # values are simply added to the rhs, and the internal Dirichlet\n        # conditions on the fractures SHOULD be homogeneous, we exclude them\n        # from the dirichlet condition as well.\n        is_neu = np.logical_and(bc.is_neu, np.logical_not(bc.is_internal))\n        is_dir = np.logical_and(bc.is_dir, np.logical_not(bc.is_internal))\n        if np.any(is_dir):\n            is_dir = np.where(is_dir)[0]\n            faces, _, sign = sps.find(g.cell_faces)\n            sign = sign[np.unique(faces, return_index=True)[1]]\n            rhs[is_dir] += -sign[is_dir] * bc_val[is_dir]\n\n        if np.any(is_neu):\n            is_neu = np.where(is_neu)[0]\n            rhs[is_neu] = bc_weight * bc_val[is_neu]\n\n        return rhs\n\n#------------------------------------------------------------------------------#\n\n    def extract_u(self, g, up):\n        \"\"\"  Extract the velocity from a dual virtual element solution.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        up : array (g.num_faces+g.num_cells)\n            Solution, stored as [velocity,pressure]\n\n        Return\n        ------\n        u : array (g.num_faces)\n            Velocity at each face.\n\n        \"\"\"\n        # pylint: disable=invalid-name\n        return up[:g.num_faces]\n\n#------------------------------------------------------------------------------#\n\n    def extract_p(self, g, up):\n        \"\"\"  Extract the pressure from a dual virtual element solution.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        up : array (g.num_faces+g.num_cells)\n            Solution, stored as [velocity,pressure]\n\n        Return\n        ------\n        p : array (g.num_cells)\n            Pressure at each cell.\n\n        \"\"\"\n        # pylint: disable=invalid-name\n        return up[g.num_faces:]\n\n#------------------------------------------------------------------------------#\n\n    def project_u(self, g, u, data):\n        \"\"\"  Project the velocity computed with a dual vem solver to obtain a\n        piecewise constant vector field, one triplet for each cell.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        u : array (g.num_faces) Velocity at each face.\n\n        Return\n        ------\n        P0u : ndarray (3, g.num_faces) Velocity at each cell.\n\n        \"\"\"\n        # Allow short variable names in backend function\n        # pylint: disable=invalid-name\n\n        if g.dim == 0:\n            return np.zeros(3).reshape((3, 1))\n\n        # The velocity field already has permeability effects incorporated,\n        # thus we assign a unit permeability to be passed to self.massHdiv\n        k = tensor.SecondOrder(g.dim, kxx=np.ones(g.num_cells))\n        param = data['param']\n        a = param.get_aperture()\n\n        faces, cells, sign = sps.find(g.cell_faces)\n        index = np.argsort(cells)\n        faces, sign = faces[index], sign[index]\n\n        c_centers, f_normals, f_centers, R, dim, _ = cg.map_grid(g)\n\n        # In the virtual cell approach the cell diameters should involve the\n        # apertures, however to keep consistency with the hybrid-dimensional\n        # approach and with the related hypotheses we avoid.\n        diams = g.cell_diameters()\n\n        P0u = np.zeros((3, g.num_cells))\n\n        for c in np.arange(g.num_cells):\n            loc = slice(g.cell_faces.indptr[c], g.cell_faces.indptr[c+1])\n            faces_loc = faces[loc]\n\n            Pi_s = self.massHdiv(a[c]*k.perm[0:g.dim, 0:g.dim, c], c_centers[:, c],\n                                 g.cell_volumes[c], f_centers[:, faces_loc],\n                                 f_normals[:, faces_loc], sign[loc],\n                                 diams[c])[1]\n\n            # extract the velocity for the current cell\n            P0u[dim, c] = np.dot(Pi_s, u[faces_loc]) / diams[c] * a[c]\n            P0u[:, c] = np.dot(R.T, P0u[:, c])\n\n        return P0u\n\n#------------------------------------------------------------------------------#\n\n    def check_conservation(self, g, u):\n        \"\"\"\n        Return the local conservation of mass in the cells.\n        Parameters\n        ----------\n        g: grid, or a subclass.\n        u : array (g.num_faces) velocity at each face.\n        \"\"\"\n        faces, cells, sign = sps.find(g.cell_faces)\n        index = np.argsort(cells)\n        faces, sign = faces[index], sign[index]\n\n        conservation = np.empty(g.num_cells)\n        for c in np.arange(g.num_cells):\n            # For the current cell retrieve its faces\n            loc = slice(g.cell_faces.indptr[c], g.cell_faces.indptr[c+1])\n            conservation[c] = np.sum(u[faces[loc]]*sign[loc])\n\n        return conservation\n\n#------------------------------------------------------------------------------#\n\n    def massHdiv(self, K, c_center, c_volume, f_centers, normals, sign, diam,\n                 weight=0):\n        \"\"\" Compute the local mass Hdiv matrix using the mixed vem approach.\n\n        Parameters\n        ----------\n        K : ndarray (g.dim, g.dim)\n            Permeability of the cell.\n        c_center : array (g.dim)\n            Cell center.\n        c_volume : scalar\n            Cell volume.\n        f_centers : ndarray (g.dim, num_faces_of_cell)\n            Center of the cell faces.\n        normals : ndarray (g.dim, num_faces_of_cell)\n            Normal of the cell faces weighted by the face areas.\n        sign : array (num_faces_of_cell)\n            +1 or -1 if the normal is inward or outward to the cell.\n        diam : scalar\n            Diameter of the cell.\n        weight : scalar\n            weight for the stabilization term. Optional, default = 0.\n\n        Return\n        ------\n        out: ndarray (num_faces_of_cell, num_faces_of_cell)\n            Local mass Hdiv matrix.\n        \"\"\"\n        # Allow short variable names in this function\n        # pylint: disable=invalid-name\n\n        dim = K.shape[0]\n        mono = np.array([lambda pt, i=i: (pt[i] - c_center[i])/diam \\\n                                                       for i in np.arange(dim)])\n        grad = np.eye(dim)/diam\n\n        # local matrix D\n        D = np.array([np.dot(normals.T, np.dot(K, g)) for g in grad]).T\n\n        # local matrix G\n        G = np.dot(grad, np.dot(K, grad.T))*c_volume\n\n        # local matrix F\n        F = np.array([s*m(f) for m in mono \\\n                      for s, f in zip(sign, f_centers.T)]).reshape((dim, -1))\n\n        assert np.allclose(G, np.dot(F, D)), \"G \"+str(G)+\" F*D \"+str(np.dot(F,D))\n\n        # local matrix Pi_s\n        Pi_s = np.linalg.solve(G, F)\n        I_Pi = np.eye(f_centers.shape[1]) - np.dot(D, Pi_s)\n\n        # local Hdiv-mass matrix\n        w = weight * np.linalg.norm(np.linalg.inv(K), np.inf)\n        A = np.dot(Pi_s.T, np.dot(G, Pi_s)) + w * np.dot(I_Pi.T, I_Pi)\n\n        return A, Pi_s",
  "class DualCoupling(AbstractCoupling):\n\n#------------------------------------------------------------------------------#\n\n    def __init__(self, discr):\n        self.discr_ndof = discr.ndof\n\n#------------------------------------------------------------------------------#\n\n    def matrix_rhs(self, g_h, g_l, data_h, data_l, data_edge):\n        \"\"\"\n        Construct the matrix (and right-hand side) for the coupling conditions.\n        Note: the right-hand side is not implemented now.\n\n        Parameters:\n            g_h: grid of higher dimension\n            g_l: grid of lower dimension\n            data_h: dictionary which stores the data for the higher dimensional\n                grid\n            data_l: dictionary which stores the data for the lower dimensional\n                grid\n            data: dictionary which stores the data for the edges of the grid\n                bucket\n\n        Returns:\n            cc: block matrix which store the contribution of the coupling\n                condition. See the abstract coupling class for a more detailed\n                description.\n        \"\"\"\n        # pylint: disable=invalid-name\n\n        # Normal permeability and aperture of the intersection\n        k = 2*data_edge['kn'] # TODO: need to be handled in a different way\n        aperture_h = data_h['param'].get_aperture()\n\n        # Retrieve the number of degrees of both grids\n        # Create the block matrix for the contributions\n        dof, cc = self.create_block_matrix(g_h, g_l)\n\n        # Recover the information for the grid-grid mapping\n        cells_l, faces_h, _ = sps.find(data_edge['face_cells'])\n        faces, cells_h, sign = sps.find(g_h.cell_faces)\n        ind = np.unique(faces, return_index=True)[1]\n        sign = sign[ind][faces_h]\n        cells_h = cells_h[ind][faces_h]\n\n        # Compute the off-diagonal terms\n        dataIJ, I, J = sign, g_l.num_faces+cells_l, faces_h\n        cc[1, 0] = sps.csr_matrix((dataIJ, (I, J)), (dof[1], dof[0]))\n        cc[0, 1] = cc[1, 0].T\n\n        # Compute the diagonal terms\n        dataIJ = 1./(g_h.face_areas[faces_h] * aperture_h[cells_h] * k[cells_l])\n        I, J = faces_h, faces_h\n        cc[0, 0] = sps.csr_matrix((dataIJ, (I, J)), (dof[0], dof[0]))\n\n        return cc",
  "class DualCouplingDFN(AbstractCoupling):\n\n#------------------------------------------------------------------------------#\n\n    def __init__(self, discr_ndof):\n\n        self.discr_ndof = discr_ndof\n\n#------------------------------------------------------------------------------#\n\n    def matrix_rhs(self, g_h, g_l, data_h, data_l, data_edge):\n        \"\"\"\n        Construct the matrix (and right-hand side) for the coupling conditions\n        of a DFN. We use the Lagrange multiplier to impose continuity of the\n        normal fluxes at the intersections.\n        Note: the right-hand side is not implemented now.\n\n        Parameters:\n            g_h: grid of higher dimension\n            g_l: grid of lower dimension\n            data_h: Not used but kept for consistency\n            data_l: Not used but kept for consistency\n            data: Not used but kept for consistency\n\n        Returns:\n            cc: block matrix which store the contribution of the coupling\n                condition. See the abstract coupling class for a more detailed\n                description.\n        \"\"\"\n        # pylint: disable=invalid-name\n\n        # Retrieve the number of degrees of both grids\n        # Create the block matrix for the contributions\n        dof, cc = self.create_block_matrix(g_h, g_l)\n\n        # Recover the information for the grid-grid mapping\n        cells_l, faces_h, _ = sps.find(data_edge['face_cells'])\n        faces, cells_h, sign = sps.find(g_h.cell_faces)\n        ind = np.unique(faces, return_index=True)[1]\n        sign = sign[ind][faces_h]\n\n        # Compute the off-diagonal terms\n        dataIJ, I, J = sign, cells_l, faces_h\n        cc[1, 0] = sps.csr_matrix((dataIJ, (I, J)), (dof[1], dof[0]))\n        cc[0, 1] = cc[1, 0].T\n\n        return cc",
  "def __init__(self, physics='flow'):\n        self.physics = physics\n\n        self.discr = DualVEM(self.physics)\n        self.discr_ndof = self.discr.ndof\n        self.coupling_conditions = DualCoupling(self.discr)\n\n        self.solver = Coupler(self.discr, self.coupling_conditions)",
  "def extract_u(self, gb, up, u):\n        gb.add_node_props([u])\n        for g, d in gb:\n            d[u] = self.discr.extract_u(g, d[up])",
  "def extract_p(self, gb, up, p):\n        gb.add_node_props([p])\n        for g, d in gb:\n            d[p] = self.discr.extract_p(g, d[up])",
  "def project_u(self, gb, u, P0u):\n        gb.add_node_props([P0u])\n        for g, d in gb:\n            d[P0u] = self.discr.project_u(g, d[u], d)",
  "def check_conservation(self, gb, u, conservation):\n        \"\"\"\n        Assert if the local conservation of mass is preserved for the grid\n        bucket.\n        Parameters\n        ----------\n        gb: grid bucket, or a subclass.\n        u : string name of the velocity in the data associated to gb.\n        conservation: string name for the conservation of mass.\n        \"\"\"\n        for g, d in gb:\n            d[conservation] = self.discr.check_conservation(g, d[u])\n\n        # add to the lower dimensional grids the contribution from the higher\n        # dimensional grids\n        for e, data in gb.edges_props():\n            g_l, g_h = gb.sorted_nodes_of_edge(e)\n\n            cells_l, faces_h, _ = sps.find(data['face_cells'])\n            faces, cells_h, sign = sps.find(g_h.cell_faces)\n            ind = np.unique(faces, return_index=True)[1]\n            sign = sign[ind][faces_h]\n\n            conservation_l = gb.node_prop(g_l, conservation)\n            u_h = sign*gb.node_prop(g_h, u)[faces_h]\n\n            for c_l, u_f in zip(cells_l, u_h):\n                conservation_l[c_l] -= u_f\n\n        for g, d in gb:\n            print(np.amax(np.abs(d[conservation])))",
  "def __init__(self, dim_max, physics='flow'):\n        # NOTE: There is no flow along the intersections of the fractures.\n\n        self.physics = physics\n        self.dim_max = dim_max\n        self.discr = DualVEM(self.physics)\n\n        self.coupling_conditions = DualCouplingDFN(self.__ndof__)\n\n        kwargs = {\"discr_ndof\": self.__ndof__,\n                  \"discr_fct\": self.__matrix_rhs__}\n        self.solver = Coupler(coupling = self.coupling_conditions, **kwargs)\n        SolverMixedDim.__init__(self)",
  "def extract_u(self, gb, up, u):\n        for g, d in gb:\n            if g.dim == self.dim_max:\n                d[u] = self.discr.extract_u(g, d[up])\n            else:\n                d[u] = np.zeros(g.num_faces)",
  "def extract_p(self, gb, up, p):\n        for g, d in gb:\n            if g.dim == self.dim_max:\n                d[p] = self.discr.extract_p(g, d[up])\n            else:\n                d[p] = d[up]",
  "def project_u(self, gb, u, P0u):\n        for g, d in gb:\n            if g.dim == self.dim_max:\n                d[P0u] = self.discr.project_u(g, d[u], d)\n            else:\n                d[P0u] = np.zeros((3, g.num_cells))",
  "def __ndof__(self, g):\n        # The highest dimensional problem has the standard number of dof\n        # associated with the solver. For the lower dimensional problems, the\n        # number of dof is the number of cells.\n        if g.dim == self.dim_max:\n            return self.discr.ndof(g)\n        else:\n            return g.num_cells",
  "def __matrix_rhs__(self, g, data):\n        # The highest dimensional problem compute the matrix and rhs, the lower\n        # dimensional problem and empty matrix. For the latter, the size of the\n        # matrix is the number of cells.\n        if g.dim == self.dim_max:\n            return self.discr.matrix_rhs(g, data)\n        else:\n            ndof = self.__ndof__(g)\n            return sps.csr_matrix((ndof, ndof)), np.zeros(ndof)",
  "def __init__(self, physics='flow'):\n        self.physics = physics",
  "def ndof(self, g):\n        \"\"\"\n        Return the number of degrees of freedom associated to the method.\n        In this case number of faces (velocity dofs) plus the number of cells\n        (pressure dof).\n\n        Parameter\n        ---------\n        g: grid, or a subclass.\n\n        Return\n        ------\n        dof: the number of degrees of freedom.\n\n        \"\"\"\n        return g.num_cells + g.num_faces",
  "def matrix_rhs(self, g, data):\n        \"\"\"\n        Return the matrix and righ-hand side for a discretization of a second\n        order elliptic equation using dual virtual element method.\n        The name of data in the input dictionary (data) are:\n        perm : second_order_tensor\n            Permeability defined cell-wise.\n        source : array (self.g.num_cells)\n            Scalar source term defined cell-wise. If not given a zero source\n            term is assumed and a warning arised.\n        bc : boundary conditions (optional)\n        bc_val : dictionary (optional)\n            Values of the boundary conditions. The dictionary has at most the\n            following keys: 'dir' and 'neu', for Dirichlet and Neumann boundary\n            conditions, respectively.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data.\n\n        Return\n        ------\n        matrix: sparse csr (g.num_faces+g_num_cells, g.num_faces+g_num_cells)\n            Saddle point matrix obtained from the discretization.\n        rhs: array (g.num_faces+g_num_cells)\n            Right-hand side which contains the boundary conditions and the scalar\n            source term.\n\n        Examples\n        --------\n        b_faces_neu = ... # id of the Neumann faces\n        b_faces_dir = ... # id of the Dirichlet faces\n        bnd = bc.BoundaryCondition(g, np.hstack((b_faces_dir, b_faces_neu)),\n                                ['dir']*b_faces_dir.size + ['neu']*b_faces_neu.size)\n        bnd_val = {'dir': fun_dir(g.face_centers[:, b_faces_dir]),\n                   'neu': fun_neu(f.face_centers[:, b_faces_neu])}\n\n        data = {'perm': perm, 'source': f, 'bc': bnd, 'bc_val': bnd_val}\n\n        D, rhs = dual.matrix_rhs(g, data)\n        up = sps.linalg.spsolve(D, rhs)\n        u = dual.extract_u(g, up)\n        p = dual.extract_p(g, up)\n        P0u = dual.project_u(g, u, perm)\n\n        \"\"\"\n        M, bc_weight = self.matrix(g, data, bc_weight=True)\n        return M, self.rhs(g, data, bc_weight)",
  "def matrix(self, g, data, bc_weight=False):\n        \"\"\"\n        Return the matrix for a discretization of a second order elliptic equation\n        using dual virtual element method. See self.matrix_rhs for a detaild\n        description.\n\n        Additional parameter:\n        --------------------\n        bc_weight: to compute the infinity norm of the matrix and use it as a\n            weight to impose the boundary conditions. Default True.\n\n        Additional return:\n        weight: if bc_weight is True return the weight computed.\n\n        \"\"\"\n        # Allow short variable names in backend function\n        # pylint: disable=invalid-name\n\n        # If a 0-d grid is given then we return an identity matrix\n        if g.dim == 0:\n            M = sps.dia_matrix(([1, 0], 0), (self.ndof(g), self.ndof(g)))\n            if bc_weight:\n                return M, 1\n            return M\n\n        # Retrieve the permeability, boundary conditions, and aperture\n        # The aperture is needed in the hybrid-dimensional case, otherwise is\n        # assumed unitary\n        param = data['param']\n        k = param.get_tensor(self)\n        bc = param.get_bc(self)\n        a = param.get_aperture()\n\n        faces, cells, sign = sps.find(g.cell_faces)\n        index = np.argsort(cells)\n        faces, sign = faces[index], sign[index]\n\n        # Map the domain to a reference geometry (i.e. equivalent to compute\n        # surface coordinates in 1d and 2d)\n        c_centers, f_normals, f_centers, R, dim, _ = cg.map_grid(g)\n\n        if not data.get('is_tangential', False):\n                # Rotate the permeability tensor and delete last dimension\n                if g.dim < 3:\n                    k = k.copy()\n                    k.rotate(R)\n                    remove_dim = np.where(np.logical_not(dim))[0]\n                    k.perm = np.delete(k.perm, (remove_dim), axis=0)\n                    k.perm = np.delete(k.perm, (remove_dim), axis=1)\n\n        # In the virtual cell approach the cell diameters should involve the\n        # apertures, however to keep consistency with the hybrid-dimensional\n        # approach and with the related hypotheses we avoid.\n        diams = g.cell_diameters()\n        # Weight for the stabilization term\n        weight = np.power(diams, 2-g.dim)\n\n        # Allocate the data to store matrix entries, that's the most efficient\n        # way to create a sparse matrix.\n        size = np.sum(np.square(g.cell_faces.indptr[1:]-\\\n                                g.cell_faces.indptr[:-1]))\n        I = np.empty(size, dtype=np.int)\n        J = np.empty(size, dtype=np.int)\n        dataIJ = np.empty(size)\n        idx = 0\n\n        for c in np.arange(g.num_cells):\n            # For the current cell retrieve its faces\n            loc = slice(g.cell_faces.indptr[c], g.cell_faces.indptr[c+1])\n            faces_loc = faces[loc]\n\n            # Compute the H_div-mass local matrix\n            A = self.massHdiv(a[c]*k.perm[0:g.dim, 0:g.dim, c], c_centers[:, c],\n                              g.cell_volumes[c], f_centers[:, faces_loc],\n                              f_normals[:, faces_loc], sign[loc],\n                              diams[c], weight[c])[0]\n\n            # Save values for Hdiv-mass local matrix in the global structure\n            cols = np.tile(faces_loc, (faces_loc.size, 1))\n            loc_idx = slice(idx, idx+cols.size)\n            I[loc_idx] = cols.T.ravel()\n            J[loc_idx] = cols.ravel()\n            dataIJ[loc_idx] = A.ravel()\n            idx += cols.size\n\n        # Construct the global matrices\n        mass = sps.coo_matrix((dataIJ, (I, J)))\n        div = -g.cell_faces.T\n        M = sps.bmat([[mass, div.T],\n                      [ div,  None]], format='csr')\n\n        norm = sps.linalg.norm(mass, np.inf) if bc_weight else 1\n\n        # assign the Neumann boundary conditions\n        # For dual discretizations, internal boundaries\n        # are handled by assigning Dirichlet conditions. THus, we remove them\n        # from the is_neu (where they belong by default) and add them in\n        # is_dir.\n        is_neu = np.logical_and(bc.is_neu, np.logical_not(bc.is_internal))\n        if bc and np.any(is_neu):\n            is_neu = np.hstack((is_neu,\n                                np.zeros(g.num_cells, dtype=np.bool)))\n            M[is_neu, :] *= 0\n            M[is_neu, is_neu] = norm\n\n        if bc_weight:\n            return M, norm\n        return M",
  "def rhs(self, g, data, bc_weight=1):\n        \"\"\"\n        Return the righ-hand side for a discretization of a second order elliptic\n        equation using dual virtual element method. See self.matrix_rhs for a detaild\n        description.\n\n        Additional parameter:\n        --------------------\n        bc_weight: to use the infinity norm of the matrix to impose the\n            boundary conditions. Default 1.\n\n        \"\"\"\n        # Allow short variable names in backend function\n        # pylint: disable=invalid-name\n\n        param = data['param']\n        f = param.get_source(self)\n\n        if g.dim == 0:\n            return np.hstack(([0], f))\n\n        bc = param.get_bc(self)\n        bc_val = param.get_bc_val(self)\n\n        assert not bool(bc is None) != bool(bc_val is None)\n\n        rhs = np.zeros(self.ndof(g))\n        if bc is None:\n            return rhs\n\n        is_p = np.hstack((np.zeros(g.num_faces, dtype=np.bool),\n                          np.ones(g.num_cells, dtype=np.bool)))\n        # For dual discretizations, internal boundaries\n        # are handled by assigning Dirichlet conditions. Thus, we remove them\n        # from the is_neu (where they belong by default). As the dirichlet\n        # values are simply added to the rhs, and the internal Dirichlet\n        # conditions on the fractures SHOULD be homogeneous, we exclude them\n        # from the dirichlet condition as well.\n        is_neu = np.logical_and(bc.is_neu, np.logical_not(bc.is_internal))\n        is_dir = np.logical_and(bc.is_dir, np.logical_not(bc.is_internal))\n        if np.any(is_dir):\n            is_dir = np.where(is_dir)[0]\n            faces, _, sign = sps.find(g.cell_faces)\n            sign = sign[np.unique(faces, return_index=True)[1]]\n            rhs[is_dir] += -sign[is_dir] * bc_val[is_dir]\n\n        if np.any(is_neu):\n            is_neu = np.where(is_neu)[0]\n            rhs[is_neu] = bc_weight * bc_val[is_neu]\n\n        return rhs",
  "def extract_u(self, g, up):\n        \"\"\"  Extract the velocity from a dual virtual element solution.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        up : array (g.num_faces+g.num_cells)\n            Solution, stored as [velocity,pressure]\n\n        Return\n        ------\n        u : array (g.num_faces)\n            Velocity at each face.\n\n        \"\"\"\n        # pylint: disable=invalid-name\n        return up[:g.num_faces]",
  "def extract_p(self, g, up):\n        \"\"\"  Extract the pressure from a dual virtual element solution.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        up : array (g.num_faces+g.num_cells)\n            Solution, stored as [velocity,pressure]\n\n        Return\n        ------\n        p : array (g.num_cells)\n            Pressure at each cell.\n\n        \"\"\"\n        # pylint: disable=invalid-name\n        return up[g.num_faces:]",
  "def project_u(self, g, u, data):\n        \"\"\"  Project the velocity computed with a dual vem solver to obtain a\n        piecewise constant vector field, one triplet for each cell.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        u : array (g.num_faces) Velocity at each face.\n\n        Return\n        ------\n        P0u : ndarray (3, g.num_faces) Velocity at each cell.\n\n        \"\"\"\n        # Allow short variable names in backend function\n        # pylint: disable=invalid-name\n\n        if g.dim == 0:\n            return np.zeros(3).reshape((3, 1))\n\n        # The velocity field already has permeability effects incorporated,\n        # thus we assign a unit permeability to be passed to self.massHdiv\n        k = tensor.SecondOrder(g.dim, kxx=np.ones(g.num_cells))\n        param = data['param']\n        a = param.get_aperture()\n\n        faces, cells, sign = sps.find(g.cell_faces)\n        index = np.argsort(cells)\n        faces, sign = faces[index], sign[index]\n\n        c_centers, f_normals, f_centers, R, dim, _ = cg.map_grid(g)\n\n        # In the virtual cell approach the cell diameters should involve the\n        # apertures, however to keep consistency with the hybrid-dimensional\n        # approach and with the related hypotheses we avoid.\n        diams = g.cell_diameters()\n\n        P0u = np.zeros((3, g.num_cells))\n\n        for c in np.arange(g.num_cells):\n            loc = slice(g.cell_faces.indptr[c], g.cell_faces.indptr[c+1])\n            faces_loc = faces[loc]\n\n            Pi_s = self.massHdiv(a[c]*k.perm[0:g.dim, 0:g.dim, c], c_centers[:, c],\n                                 g.cell_volumes[c], f_centers[:, faces_loc],\n                                 f_normals[:, faces_loc], sign[loc],\n                                 diams[c])[1]\n\n            # extract the velocity for the current cell\n            P0u[dim, c] = np.dot(Pi_s, u[faces_loc]) / diams[c] * a[c]\n            P0u[:, c] = np.dot(R.T, P0u[:, c])\n\n        return P0u",
  "def check_conservation(self, g, u):\n        \"\"\"\n        Return the local conservation of mass in the cells.\n        Parameters\n        ----------\n        g: grid, or a subclass.\n        u : array (g.num_faces) velocity at each face.\n        \"\"\"\n        faces, cells, sign = sps.find(g.cell_faces)\n        index = np.argsort(cells)\n        faces, sign = faces[index], sign[index]\n\n        conservation = np.empty(g.num_cells)\n        for c in np.arange(g.num_cells):\n            # For the current cell retrieve its faces\n            loc = slice(g.cell_faces.indptr[c], g.cell_faces.indptr[c+1])\n            conservation[c] = np.sum(u[faces[loc]]*sign[loc])\n\n        return conservation",
  "def massHdiv(self, K, c_center, c_volume, f_centers, normals, sign, diam,\n                 weight=0):\n        \"\"\" Compute the local mass Hdiv matrix using the mixed vem approach.\n\n        Parameters\n        ----------\n        K : ndarray (g.dim, g.dim)\n            Permeability of the cell.\n        c_center : array (g.dim)\n            Cell center.\n        c_volume : scalar\n            Cell volume.\n        f_centers : ndarray (g.dim, num_faces_of_cell)\n            Center of the cell faces.\n        normals : ndarray (g.dim, num_faces_of_cell)\n            Normal of the cell faces weighted by the face areas.\n        sign : array (num_faces_of_cell)\n            +1 or -1 if the normal is inward or outward to the cell.\n        diam : scalar\n            Diameter of the cell.\n        weight : scalar\n            weight for the stabilization term. Optional, default = 0.\n\n        Return\n        ------\n        out: ndarray (num_faces_of_cell, num_faces_of_cell)\n            Local mass Hdiv matrix.\n        \"\"\"\n        # Allow short variable names in this function\n        # pylint: disable=invalid-name\n\n        dim = K.shape[0]\n        mono = np.array([lambda pt, i=i: (pt[i] - c_center[i])/diam \\\n                                                       for i in np.arange(dim)])\n        grad = np.eye(dim)/diam\n\n        # local matrix D\n        D = np.array([np.dot(normals.T, np.dot(K, g)) for g in grad]).T\n\n        # local matrix G\n        G = np.dot(grad, np.dot(K, grad.T))*c_volume\n\n        # local matrix F\n        F = np.array([s*m(f) for m in mono \\\n                      for s, f in zip(sign, f_centers.T)]).reshape((dim, -1))\n\n        assert np.allclose(G, np.dot(F, D)), \"G \"+str(G)+\" F*D \"+str(np.dot(F,D))\n\n        # local matrix Pi_s\n        Pi_s = np.linalg.solve(G, F)\n        I_Pi = np.eye(f_centers.shape[1]) - np.dot(D, Pi_s)\n\n        # local Hdiv-mass matrix\n        w = weight * np.linalg.norm(np.linalg.inv(K), np.inf)\n        A = np.dot(Pi_s.T, np.dot(G, Pi_s)) + w * np.dot(I_Pi.T, I_Pi)\n\n        return A, Pi_s",
  "def __init__(self, discr):\n        self.discr_ndof = discr.ndof",
  "def matrix_rhs(self, g_h, g_l, data_h, data_l, data_edge):\n        \"\"\"\n        Construct the matrix (and right-hand side) for the coupling conditions.\n        Note: the right-hand side is not implemented now.\n\n        Parameters:\n            g_h: grid of higher dimension\n            g_l: grid of lower dimension\n            data_h: dictionary which stores the data for the higher dimensional\n                grid\n            data_l: dictionary which stores the data for the lower dimensional\n                grid\n            data: dictionary which stores the data for the edges of the grid\n                bucket\n\n        Returns:\n            cc: block matrix which store the contribution of the coupling\n                condition. See the abstract coupling class for a more detailed\n                description.\n        \"\"\"\n        # pylint: disable=invalid-name\n\n        # Normal permeability and aperture of the intersection\n        k = 2*data_edge['kn'] # TODO: need to be handled in a different way\n        aperture_h = data_h['param'].get_aperture()\n\n        # Retrieve the number of degrees of both grids\n        # Create the block matrix for the contributions\n        dof, cc = self.create_block_matrix(g_h, g_l)\n\n        # Recover the information for the grid-grid mapping\n        cells_l, faces_h, _ = sps.find(data_edge['face_cells'])\n        faces, cells_h, sign = sps.find(g_h.cell_faces)\n        ind = np.unique(faces, return_index=True)[1]\n        sign = sign[ind][faces_h]\n        cells_h = cells_h[ind][faces_h]\n\n        # Compute the off-diagonal terms\n        dataIJ, I, J = sign, g_l.num_faces+cells_l, faces_h\n        cc[1, 0] = sps.csr_matrix((dataIJ, (I, J)), (dof[1], dof[0]))\n        cc[0, 1] = cc[1, 0].T\n\n        # Compute the diagonal terms\n        dataIJ = 1./(g_h.face_areas[faces_h] * aperture_h[cells_h] * k[cells_l])\n        I, J = faces_h, faces_h\n        cc[0, 0] = sps.csr_matrix((dataIJ, (I, J)), (dof[0], dof[0]))\n\n        return cc",
  "def __init__(self, discr_ndof):\n\n        self.discr_ndof = discr_ndof",
  "def matrix_rhs(self, g_h, g_l, data_h, data_l, data_edge):\n        \"\"\"\n        Construct the matrix (and right-hand side) for the coupling conditions\n        of a DFN. We use the Lagrange multiplier to impose continuity of the\n        normal fluxes at the intersections.\n        Note: the right-hand side is not implemented now.\n\n        Parameters:\n            g_h: grid of higher dimension\n            g_l: grid of lower dimension\n            data_h: Not used but kept for consistency\n            data_l: Not used but kept for consistency\n            data: Not used but kept for consistency\n\n        Returns:\n            cc: block matrix which store the contribution of the coupling\n                condition. See the abstract coupling class for a more detailed\n                description.\n        \"\"\"\n        # pylint: disable=invalid-name\n\n        # Retrieve the number of degrees of both grids\n        # Create the block matrix for the contributions\n        dof, cc = self.create_block_matrix(g_h, g_l)\n\n        # Recover the information for the grid-grid mapping\n        cells_l, faces_h, _ = sps.find(data_edge['face_cells'])\n        faces, cells_h, sign = sps.find(g_h.cell_faces)\n        ind = np.unique(faces, return_index=True)[1]\n        sign = sign[ind][faces_h]\n\n        # Compute the off-diagonal terms\n        dataIJ, I, J = sign, cells_l, faces_h\n        cc[1, 0] = sps.csr_matrix((dataIJ, (I, J)), (dof[1], dof[0]))\n        cc[0, 1] = cc[1, 0].T\n\n        return cc",
  "def plot_grid(g, cell_value=None, vector_value=None, info=None, **kwargs):\n    \"\"\" plot the grid in a 3d framework.\n\n    It is possible to add the cell ids at the cells centers (info option 'c'),\n    the face ids at the face centers (info option 'f'), the node ids at the node\n    (info option 'n'), and the normal at the face (info option 'o'). If info is\n    set to 'all' all the informations are displayed.\n\n    Parameters:\n    g: the grid\n    cell_value: (optional) if g is a single grid then cell scalar field to be\n        represented (only 1d and 2d). If g is a grid bucket the name (key) of the\n        scalar field.\n    vector_value: (optional) if g is a single grid then vector scalar field to be\n        represented (only 1d and 2d). If g is a grid bucket the name (key) of the\n        vector field.\n    info: (optional) add extra information to the plot\n    alpha: (optonal) transparency of cells (2d) and faces (3d)\n\n    How to use:\n    if g is a single grid:\n    cell_id = np.arange(g.num_cells)\n    plot_grid(g, cell_value=cell_id, info=\"ncfo\", alpha=0.75)\n\n    if g is a grid bucket\n    plot_grid(g, cell_value=\"cell_id\", info=\"ncfo\", alpha=0.75)\n\n    \"\"\"\n\n    if isinstance(g, grid.Grid):\n        plot_single(g, cell_value, vector_value, info, **kwargs)\n\n    if isinstance(g, grid_bucket.GridBucket):\n        plot_gb(g, cell_value, vector_value, info, **kwargs)",
  "def save_img(name, g, cell_value=None, vector_value=None, info=None, **kwargs):\n    \"\"\" save the grid in a 3d framework.\n\n    It is possible to add the cell ids at the cells centers (info option 'c'),\n    the face ids at the face centers (info option 'f'), the node ids at the node\n    (info option 'n'), and the normal at the face (info option 'o'). If info is\n    set to 'all' all the informations are displayed.\n\n    Parameters:\n    name: the name of the file\n    g: the grid\n    cell_value: (optional) if g is a single grid then cell scalar field to be\n        represented (only 1d and 2d). If g is a grid bucket the name (key) of the\n        scalar field.\n    vector_value: (optional) if g is a single grid then vector scalar field to be\n        represented (only 1d and 2d). If g is a grid bucket the name (key) of the\n        vector field.\n    info: (optional) add extra information to the plot\n    alpha: (optonal) transparency of cells (2d) and faces (3d)\n\n    How to use:\n    if g is a single grid:\n    cell_id = np.arange(g.num_cells)\n    save_img(g, cell_value=cell_id, info=\"ncfo\", alpha=0.75)\n\n    if g is a grid bucket\n    save_img(g, cell_value=\"cell_id\", info=\"ncfo\", alpha=0.75)\n\n    \"\"\"\n\n    plot_grid(g, cell_value, vector_value, info, **dict(kwargs, if_plot=False))\n    plt.savefig(name, bbox_inches='tight', pad_inches = 0)",
  "class Arrow3D(FancyArrowPatch):\n    def __init__(self, xs, ys, zs, *args, **kwargs):\n        FancyArrowPatch.__init__(self, (0,0), (0,0), *args, **kwargs)\n        self._verts3d = xs, ys, zs\n\n    def draw(self, renderer):\n        xs3d, ys3d, zs3d = self._verts3d\n        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n        self.set_positions((xs[0],ys[0]),(xs[1],ys[1]))\n        FancyArrowPatch.draw(self, renderer)",
  "def quiver(vector_value, ax, g, **kwargs):\n\n    if vector_value.shape[1] == g.num_faces:\n        where = g.face_centers\n    elif vector_value.shape[1] == g.num_cells:\n        where = g.cell_centers\n    else:\n        raise ValueError\n\n    scale = kwargs.get('vector_scale', 0.02)\n    for v in np.arange(vector_value.shape[1]):\n        x = [where[0, v], where[0, v] + scale * vector_value[0, v]]\n        y = [where[1, v], where[1, v] + scale * vector_value[1, v]]\n        z = [where[2, v], where[2, v] + scale * vector_value[2, v]]\n        linewidth = kwargs.get('linewidth', 1)\n        a = Arrow3D(x, y, z, mutation_scale=5, linewidth=linewidth,\n                    arrowstyle=\"-|>\", color=\"k\", zorder=np.inf)\n        ax.add_artist(a)",
  "def plot_single(g, cell_value, vector_value, info, **kwargs):\n\n    figsize = kwargs.get('figsize', None)\n    if figsize is None:\n        fig = plt.figure()\n    else:\n        fig = plt.figure(figsize=figsize)\n\n    ax = fig.add_subplot(111, projection='3d')\n\n    ax.set_title( \" \".join( g.name ) )\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_zlabel('z')\n\n    if cell_value is not None and g.dim !=3:\n        if kwargs.get('color_map'):\n            extr_value = kwargs['color_map']\n        else:\n            extr_value = np.array([np.amin(cell_value), np.amax(cell_value)])\n\t        \n        kwargs['color_map'] = color_map(extr_value)\n\n    plot_grid_xd(g, cell_value, vector_value, ax, **kwargs)\n    x, y, z = lim(ax, g.nodes)\n    if not np.isclose(x[0], x[1]): ax.set_xlim3d( x )\n    if not np.isclose(y[0], y[1]): ax.set_ylim3d( y )\n    if not np.isclose(z[0], z[1]): ax.set_zlim3d( z )\n\n    if info is not None: add_info(g, info, ax, **kwargs)\n\n    if kwargs.get('color_map'): fig.colorbar(kwargs['color_map'])\n\n    plt.draw()\n    if kwargs.get('if_plot', True):\n        plt.show()",
  "def plot_gb(gb, cell_value, vector_value, info, **kwargs):\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    ax.set_title( \" \".join( gb.name ) )\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_zlabel('z')\n\n    if cell_value is not None and gb.dim_max() !=3:\n        if kwargs.get('color_map'):\n            extr_value = kwargs['color_map']\n        else:\n            extr_value = np.array([np.inf, -np.inf])\n            for _, d in gb:\n                extr_value[0] = min(np.amin(d[cell_value]), extr_value[0])\n                extr_value[1] = max(np.amax(d[cell_value]), extr_value[1])\n        kwargs['color_map'] = color_map(extr_value)\n\n    gb.assign_node_ordering()\n    for g, d in gb:\n        kwargs['rgb'] = np.divide(kwargs.get('rgb', [1,0,0]), d['node_number']+1)\n        plot_grid_xd(g, d.get(cell_value), d.get(vector_value), ax, **kwargs)\n\n    val = np.array([lim(ax, g.nodes) for g, _ in gb])\n\n    x = [np.amin(val[:,0,:]), np.amax(val[:,0,:])]\n    y = [np.amin(val[:,1,:]), np.amax(val[:,1,:])]\n    z = [np.amin(val[:,2,:]), np.amax(val[:,2,:])]\n\n    if not np.isclose(x[0], x[1]): ax.set_xlim3d( x )\n    if not np.isclose(y[0], y[1]): ax.set_ylim3d( y )\n    if not np.isclose(z[0], z[1]): ax.set_zlim3d( z )\n\n    if info is not None: [add_info( g, info, ax ) for g, _ in gb]\n\n    if kwargs.get('color_map'): fig.colorbar(kwargs['color_map'])\n\n    plt.draw()\n    if kwargs.get('if_plot', True):\n        plt.show()",
  "def plot_grid_xd(g, cell_value, vector_value, ax, **kwargs):\n    if g.dim == 0:   plot_grid_0d(g, ax, **kwargs)\n    elif g.dim == 1: plot_grid_1d(g, cell_value, ax, **kwargs)\n    elif g.dim == 2: plot_grid_2d(g, cell_value, ax, **kwargs)\n    else:            plot_grid_3d(g, ax, **kwargs)\n\n    if vector_value is not None: quiver(vector_value, ax, g, **kwargs)",
  "def lim(ax, nodes):\n    x = [np.amin(nodes[0,:]), np.amax(nodes[0,:])]\n    y = [np.amin(nodes[1,:]), np.amax(nodes[1,:])]\n    z = [np.amin(nodes[2,:]), np.amax(nodes[2,:])]\n    return x, y, z",
  "def color_map(extr_value, cmap_type='jet'):\n    cmap = plt.get_cmap(cmap_type)\n    scalar_map = mpl.cm.ScalarMappable(cmap=cmap)\n    scalar_map.set_array(extr_value)\n    scalar_map.set_clim(vmin=extr_value[0], vmax=extr_value[1])\n    return scalar_map",
  "def add_info( g, info, ax, **kwargs ):\n\n    def disp( i, p, c, m ): ax.scatter( *p, c=c, marker=m ); ax.text( *p, i )\n    def disp_loop( v, c, m ): [disp( i, ic, c, m ) for i, ic in enumerate(v.T)]\n\n    info = info.upper()\n    info = string.ascii_uppercase if info == \"ALL\" else info\n\n    if \"C\" in info: disp_loop( g.cell_centers, 'r', 'o' )\n    if \"N\" in info: disp_loop( g.nodes, 'b', 's' )\n    if \"F\" in info: disp_loop( g.face_centers, 'y', 'd' )\n\n    if \"O\" in info.upper() and g.dim != 0:\n        normals = np.array( [ n/np.linalg.norm(n) \\\n                                      for n in g.face_normals.T ] ).T\n        quiver(normals, ax, g, **kwargs)",
  "def plot_grid_0d(g, ax, **kwargs):\n    ax.scatter(*g.nodes, color='k', marker='o', s=kwargs.get('pointsize', 1))",
  "def plot_grid_1d(g, cell_value, ax, **kwargs):\n    cell_nodes = g.cell_nodes()\n    nodes, cells, _  = sps.find( cell_nodes )\n\n    if kwargs.get('color_map'):\n        scalar_map = kwargs['color_map']\n        alpha = kwargs.get('alpha', 1)\n        def color_edge(value): return scalar_map.to_rgba(value, alpha)\n    else:\n        cell_value = np.zeros(g.num_cells)\n        def color_edge(value): return 'k'\n\n    for c in np.arange( g.num_cells ):\n        loc = slice(cell_nodes.indptr[c], cell_nodes.indptr[c+1])\n        ptsId = nodes[loc]\n        pts = g.nodes[:, ptsId]\n        poly = Poly3DCollection( [pts.T] )\n        poly.set_edgecolor(color_edge(cell_value[c]))\n        ax.add_collection3d(poly)",
  "def plot_grid_2d(g, cell_value, ax, **kwargs):\n    faces, _, _ = sps.find(g.cell_faces)\n    nodes, _, _ = sps.find(g.face_nodes)\n\n\n    alpha = kwargs.get('alpha', 1)\n    if kwargs.get('color_map'):\n        scalar_map = kwargs['color_map']\n        def color_face(value): return scalar_map.to_rgba(value, alpha)\n    else:\n        cell_value = np.zeros(g.num_cells)\n        rgb = kwargs.get('rgb', [1,0,0])\n        def color_face(value): return np.r_[rgb, alpha]\n\n    for c in np.arange( g.num_cells ):\n        loc_f = slice(g.cell_faces.indptr[c], g.cell_faces.indptr[c + 1])\n        faces_loc = faces[loc_f]\n\n        loc_n = g.face_nodes.indptr[faces_loc]\n        pts_pairs = np.array([nodes[loc_n], nodes[loc_n + 1]])\n        ordering = sort_points.sort_point_pairs(pts_pairs)[0, :]\n\n        pts = g.nodes[:, ordering]\n        linewidth = kwargs.get('linewidth', 1)\n        poly = Poly3DCollection([pts.T], linewidth = linewidth)\n        poly.set_edgecolor('k')\n        poly.set_facecolors(color_face(cell_value[c]))\n        ax.add_collection3d(poly)\n\n    ax.view_init(90, -90)",
  "def plot_grid_3d(g, ax, **kwargs):\n    faces_cells, cells, _ = sps.find( g.cell_faces )\n    nodes_faces, faces, _ = sps.find( g.face_nodes )\n\n    cell_value = np.zeros(g.num_cells)\n    rgb = kwargs.get('rgb', [1,0,0])\n    alpha = kwargs.get('alpha', 1)\n    def color_face(value): return np.r_[rgb, alpha]\n\n    for c in np.arange(g.num_cells):\n        loc_c = slice(g.cell_faces.indptr[c], g.cell_faces.indptr[c+1])\n        fs = faces_cells[loc_c]\n        for f in fs:\n            loc_f = slice(g.face_nodes.indptr[f], g.face_nodes.indptr[f+1])\n            ptsId = nodes_faces[loc_f]\n            mask = sort_points.sort_point_plane( g.nodes[:, ptsId], \\\n                                                 g.face_centers[:, f], \\\n                                                 g.face_normals[:, f] )\n            pts = g.nodes[:, ptsId[mask]]\n            linewidth = kwargs.get('linewidth', 1)\n            poly = Poly3DCollection([pts.T], linewidth=linewidth)\n            poly.set_edgecolor('k')\n            poly.set_facecolors(color_face(cell_value[c]))\n            ax.add_collection3d(poly)",
  "def __init__(self, xs, ys, zs, *args, **kwargs):\n        FancyArrowPatch.__init__(self, (0,0), (0,0), *args, **kwargs)\n        self._verts3d = xs, ys, zs",
  "def draw(self, renderer):\n        xs3d, ys3d, zs3d = self._verts3d\n        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n        self.set_positions((xs[0],ys[0]),(xs[1],ys[1]))\n        FancyArrowPatch.draw(self, renderer)",
  "def disp( i, p, c, m ): ax.scatter( *p, c=c, marker=m ); ax.text( *p, i )",
  "def disp_loop( v, c, m ): [disp( i, ic, c, m ) for i, ic in enumerate(v.T)]",
  "def color_face(value): return np.r_[rgb, alpha]",
  "def color_edge(value): return scalar_map.to_rgba(value, alpha)",
  "def color_edge(value): return 'k'",
  "def color_face(value): return scalar_map.to_rgba(value, alpha)",
  "def color_face(value): return np.r_[rgb, alpha]",
  "class Exporter():\n    def __init__(self, grid, name, folder=None, **kwargs):\n        \"\"\"\n        Parameters:\n        grid: the grid or grid bucket\n        name: the root of file name without any extension.\n        folder: (optional) the folder to save the file. If the folder does not\n            exist it will be created.\n\n        Optional arguments in kwargs:\n        fixed_grid: (optional) in a time dependent simulation specify if the\n            grid changes in time or not. The default is True.\n        binary: export in binary format, default is True.\n\n        How to use:\n        If you need to export a single grid:\n        save = Exporter(g, \"solution\", folder=\"results\")\n        save.write_vtk({\"cells_id\": cells_id, \"pressure\": pressure})\n\n        In a time loop:\n        save = Exporter(gb, \"solution\", folder=\"results\")\n        while time:\n            save.write_vtk({\"conc\": conc}, time_step=i)\n        save.write_pvd(steps*deltaT)\n\n        if you need to export the grid bucket\n        save = Exporter(gb, \"solution\", folder=\"results\")\n        save.write_vtk(gb, [\"cells_id\", \"pressure\"])\n\n        In a time loop:\n        while time:\n            save.write_vtk([\"conc\"], time_step=i)\n        save.write_pvd(steps*deltaT)\n\n        In the case of different physics, change the file name with\n        \"change_name\".\n\n        \"\"\"\n\n        self.gb = grid\n        self.name = name\n        self.folder = folder\n        self.fixed_grid = kwargs.get('fixed_grid', True)\n        self.binary = kwargs.get('binary', True)\n\n        self.is_GridBucket = isinstance(self.gb, grid_bucket.GridBucket)\n        self.is_not_vtk = 'vtk' not in sys.modules\n\n        if self.is_not_vtk:\n            return\n\n        if self.is_GridBucket:\n            self.gb_VTK = np.empty(self.gb.size(), dtype=np.object)\n        else:\n            self.gb_VTK = None\n\n        self.has_numba = 'numba' in sys.modules\n\n        if self.fixed_grid:\n            self._update_gb_VTK()\n\n#------------------------------------------------------------------------------#\n\n    def change_name(self, name):\n        \"\"\"\n        Change the root name of the files, useful when different physics are\n        considered but on the same grid.\n\n        Parameters:\n        name: the new root name of the files.\n        \"\"\"\n        self.name = name\n\n#------------------------------------------------------------------------------#\n\n    def write_vtk(self, data=None, time_step=None, grid=None):\n        \"\"\" Interface function to export in VTK the grid and additional data.\n\n        In 2d the cells are represented as polygon, while in 3d as polyhedra.\n        VTK module need to be installed.\n        In 3d the geometry of the mesh needs to be computed.\n\n        To work with python3, the package vtk should be installed in version 7\n        or higher.\n\n        Parameters:\n        data: if g is a single grid then data is a dictionary (see example)\n              if g is a grid bucket then list of names for optional data,\n              they are the keys in the grid bucket (see example).\n        time_step: (optional) in a time dependent problem defines the full name of\n            the file.\n        grid: (optional) in case of changing grid set a new one.\n\n        \"\"\"\n        if self.is_not_vtk:\n            return\n\n        if self.fixed_grid and grid is not None:\n            raise ValueError(\"Inconsistency in exporter setting\")\n        elif not self.fixed_grid and grid is not None:\n            self.gb = grid\n            self.is_GridBucket = isinstance(self.gb, grid_bucket.GridBucket)\n            self._update_gVTK()\n\n        if self.is_GridBucket:\n            self._export_vtk_gb(data, time_step)\n        else:\n            # No need of special naming, create the folder\n            name = self._make_folder(self.folder, self.name)\n            self._export_vtk_single(data, time_step, self.gb, name)\n\n#------------------------------------------------------------------------------#\n\n    def write_pvd(self, time):\n        \"\"\" Interface function to export in PVD file the time loop informations.\n        The user should open only this file in paraview.\n\n        We assume that the VTU associated files have the same name.\n        We assume that the VTU associated files are in the same folder.\n\n        Parameters:\n        time: vector of times.\n\n        \"\"\"\n        if self.is_not_vtk:\n            return\n\n        o_file = open(self._make_folder(self.folder, self.name)+\".pvd\", 'w')\n        b = 'LittleEndian' if sys.byteorder == 'little' else 'BigEndian'\n        c = ' compressor=\"vtkZLibDataCompressor\"'\n        header = '<?xml version=\"1.0\"?>\\n'+ \\\n                 '<VTKFile type=\"Collection\" version=\"0.1\" ' + \\\n                                              'byte_order=\"%s\"%s>\\n' % (b,c) + \\\n                 '<Collection>\\n'\n        o_file.write(header)\n        fm = '\\t<DataSet group=\"\" part=\"\" timestep=\"%f\" file=\"%s\"/>\\n'\n\n        time_step = np.arange(np.atleast_1d(time).size)\n\n        if self.is_GridBucket:\n            [o_file.write(fm%(time[t],\n                        self._make_file_name(self.name, t, d['node_number']))) \\\n                                         for t in time_step for _, d in self.gb]\n        else:\n            [o_file.write(fm%(time[t], self._make_file_name(self.name, t))) \\\n                                                             for t in time_step]\n\n        o_file.write('</Collection>\\n'+'</VTKFile>')\n        o_file.close()\n\n#------------------------------------------------------------------------------#\n\n    def _export_vtk_single(self, data, time_step, g, name):\n        name = self._make_file_name(name, time_step)\n        self._write_vtk(data, name, self.gb_VTK)\n\n#------------------------------------------------------------------------------#\n\n    def _export_vtk_gb(self, data, time_step):\n        if data is not None:\n            data = np.atleast_1d(data).tolist()\n        assert isinstance(data, list) or data is None\n        data = list() if data is None else data\n        data.append('grid_dim')\n\n        self.gb.assign_node_ordering(overwrite_existing=False)\n        self.gb.add_node_props(['grid_dim', 'file_name'])\n\n        for g, d in self.gb:\n            if g.dim > 0:\n                d['file_name'] = self._make_file_name(self.name, time_step,\n                                                               d['node_number'])\n                file_name = self._make_folder(self.folder, d['file_name'])\n                d['grid_dim'] = np.tile(g.dim, g.num_cells)\n                dic_data = self.gb.node_props_of_keys(g, data)\n                g_VTK = self.gb_VTK[d['node_number']]\n                self._write_vtk(dic_data, file_name, g_VTK)\n\n        name = self._make_folder(self.folder, self.name)+\".pvd\"\n        self._export_pvd_gb(name)\n\n#------------------------------------------------------------------------------#\n\n    def _export_pvd_gb(self, name):\n        o_file = open(name, 'w')\n        b = 'LittleEndian' if sys.byteorder == 'little' else 'BigEndian'\n        c = ' compressor=\"vtkZLibDataCompressor\"'\n        header = '<?xml version=\"1.0\"?>\\n'+ \\\n                 '<VTKFile type=\"Collection\" version=\"0.1\" ' + \\\n                                              'byte_order=\"%s\"%s>\\n' % (b,c) + \\\n                 '<Collection>\\n'\n        o_file.write(header)\n        fm = '\\t<DataSet group=\"\" part=\"\" file=\"%s\"/>\\n'\n        [o_file.write( fm % d['file_name'] ) for g, d in self.gb if g.dim!=0]\n        o_file.write('</Collection>\\n'+'</VTKFile>')\n        o_file.close()\n\n#------------------------------------------------------------------------------#\n\n    def _export_vtk_grid(self, g):\n        if g.dim == 0:\n            return\n        elif g.dim == 1:\n            return self._export_vtk_1d(g)\n        elif g.dim == 2:\n            return self._export_vtk_2d(g)\n        elif g.dim == 3:\n            return self._export_vtk_3d(g)\n\n#------------------------------------------------------------------------------#\n\n    def _export_vtk_1d(self, g):\n        cell_nodes = g.cell_nodes()\n        nodes, cells, _  = sps.find(cell_nodes)\n        gVTK = vtk.vtkUnstructuredGrid()\n\n        for c in np.arange(g.num_cells):\n            loc = slice(cell_nodes.indptr[c], cell_nodes.indptr[c+1])\n            ptsId = nodes[loc]\n            fsVTK = vtk.vtkIdList()\n            [fsVTK.InsertNextId(p) for p in ptsId]\n\n            gVTK.InsertNextCell(vtk.VTK_LINE, fsVTK)\n\n        ptsVTK = vtk.vtkPoints()\n        if g.nodes.shape[0] == 1:\n            [ptsVTK.InsertNextPoint(node, 0., 0.) for node in g.nodes.T]\n        if g.nodes.shape[0] == 2:\n            [ptsVTK.InsertNextPoint(node[0], node[1], 0.) for node in g.nodes.T]\n        else:\n            [ptsVTK.InsertNextPoint(*node) for node in g.nodes.T]\n        gVTK.SetPoints(ptsVTK)\n\n        return gVTK\n\n#------------------------------------------------------------------------------#\n\n    def _export_vtk_2d(self, g):\n        faces_cells, _, _ = sps.find(g.cell_faces)\n        nodes_faces, _, _ = sps.find(g.face_nodes)\n\n        gVTK = vtk.vtkUnstructuredGrid()\n\n        for c in np.arange(g.num_cells):\n            loc = slice(g.cell_faces.indptr[c], g.cell_faces.indptr[c+1])\n            ptsId = np.array([nodes_faces[g.face_nodes.indptr[f]:\\\n                                          g.face_nodes.indptr[f+1]]\n                              for f in faces_cells[loc]]).T\n            ptsId = sort_points.sort_point_pairs(ptsId)[0,:]\n\n            fsVTK = vtk.vtkIdList()\n            [fsVTK.InsertNextId(p) for p in ptsId]\n\n            gVTK.InsertNextCell(vtk.VTK_POLYGON, fsVTK)\n\n        ptsVTK = vtk.vtkPoints()\n        if g.nodes.shape[0] == 2:\n            [ptsVTK.InsertNextPoint(node[0], node[1], 0.) for node in g.nodes.T]\n        else:\n            [ptsVTK.InsertNextPoint(*node ) for node in g.nodes.T]\n        gVTK.SetPoints(ptsVTK)\n\n        return gVTK\n\n#------------------------------------------------------------------------------#\n\n    def _export_vtk_3d(self, g):\n        # This functionality became rather complex, with possible use of numba.\n        # Decided to dump this to a separate file.\n        return self._define_gvtk_3d(g)\n\n#------------------------------------------------------------------------------#\n\n    def _write_vtk(self, data, name, g_VTK):\n        writer = vtk.vtkXMLUnstructuredGridWriter()\n        writer.SetInputData(g_VTK)\n        writer.SetFileName(name)\n\n        if data is not None:\n            for name_field, values_field in data.items():\n                if values_field is None:\n                    continue\n                dataVTK = ns.numpy_to_vtk(values_field.ravel(order='F'),\n                                          deep=True, array_type=vtk.VTK_DOUBLE)\n                dataVTK.SetName(str(name_field))\n                dataVTK.SetNumberOfComponents(1 if values_field.ndim == 1 else 3)\n                g_VTK.GetCellData().AddArray(dataVTK)\n\n        if not self.binary:\n            writer.SetDataModeToAscii()\n        writer.Update()\n\n        if data is not None:\n            for name_field, _ in data.items():\n                cell_data = g_VTK.GetCellData().RemoveArray(str(name_field))\n\n#------------------------------------------------------------------------------#\n\n    def _update_gb_VTK(self):\n        if self.is_GridBucket:\n            for g, d in self.gb:\n                self.gb_VTK[d['node_number']] = self._export_vtk_grid(g)\n        else:\n            self.gb_VTK = self._export_vtk_grid(self.gb)\n\n#------------------------------------------------------------------------------#\n\n    def _make_folder(self, folder, name=None):\n        if folder is None: return name\n\n        if not os.path.exists(folder):\n            os.makedirs(folder)\n        return os.path.join(folder, name)\n\n#------------------------------------------------------------------------------#\n\n    def _make_file_name(self, name, time_step=None, node_number=None):\n\n        extension = \".vtu\"\n        padding = 6\n        if node_number is None: # normal grid\n            if time_step is None:\n                return name + extension\n            else:\n                time = str(time_step).zfill(padding)\n                return name + \"_\" + time + extension\n        else: # part of a grid bucket\n            grid = str(node_number).zfill(padding)\n            if time_step is None:\n                return name + \"_\" + grid + extension\n            else:\n                time = str(time_step).zfill(padding)\n                return name + \"_\" + grid + \"_\" + time + extension\n\n#------------------------------------------------------------------------------#\n\n    def _define_gvtk_3d(self, g):\n        gVTK = vtk.vtkUnstructuredGrid()\n\n        faces_cells, cells, _ = sps.find(g.cell_faces)\n        nodes_faces, faces, _ = sps.find(g.face_nodes)\n\n        cptr = g.cell_faces.indptr\n        fptr = g.face_nodes.indptr\n        face_per_cell = np.diff(cptr)\n        nodes_per_face = np.diff(fptr)\n\n        # Total number of nodes to be written in the face-node relation\n        num_cell_nodes = np.array([nodes_per_face[i] \\\n                                   for i in g.cell_faces.indices])\n\n        n = g.nodes\n        fc = g.face_centers\n        normal_vec = g.face_normals / g.face_areas\n\n        # Use numba if available, unless the problem is very small, in which\n        # case the pure python version probably is faster than combined compile\n        # and runtime for numba\n        # The number 1000 here is somewhat random.\n        if self.has_numba and g.num_cells > 1000:\n            logger.info('Construct 3d grid information using numba')\n            cell_nodes = _point_ind_numba(cptr, fptr, faces_cells, nodes_faces,\n                                          n, fc, normal_vec, num_cell_nodes)\n        else:\n            logger.info('Construct 3d grid information using pure python')\n            cell_nodes = _point_ind(cptr, fptr, faces_cells, nodes_faces, n,\n                                    fc, normal_vec, num_cell_nodes)\n        # implementation note: I did not even try feeding this to numba, my\n        # guess is that it will not like the vtk specific stuff.\n        node_counter = 0\n        face_counter = 0\n        for c in np.arange(g.num_cells):\n            fsVTK = vtk.vtkIdList()\n            fsVTK.InsertNextId(face_per_cell[c]) # Number faces that make up the cell\n            for f in range(face_per_cell[c]):\n                fi = g.cell_faces.indices[face_counter]\n                fsVTK.InsertNextId(nodes_per_face[fi]) # Number of points in face\n                for ni in range(nodes_per_face[fi]):\n                    fsVTK.InsertNextId(cell_nodes[node_counter])\n                    node_counter += 1\n                face_counter += 1\n\n            gVTK.InsertNextCell(vtk.VTK_POLYHEDRON, fsVTK)\n\n        ptsVTK = vtk.vtkPoints()\n        [ptsVTK.InsertNextPoint(*node) for node in g.nodes.T]\n        gVTK.SetPoints(ptsVTK)\n\n        return gVTK",
  "def _point_ind(cell_ptr, face_ptr, face_cells, nodes_faces, nodes,\n               fc, normals, num_cell_nodes):\n    cell_nodes = np.zeros(num_cell_nodes.sum(), dtype=np.int)\n    counter = 0\n    for ci in range(cell_ptr.size - 1):\n        loc_c = slice(cell_ptr[ci], cell_ptr[ci + 1])\n\n        for fi in face_cells[loc_c]:\n            loc_f = slice(face_ptr[fi], face_ptr[fi+1])\n            ptsId = nodes_faces[loc_f]\n            num_p_loc = ptsId.size\n            nodes_loc = nodes[:, ptsId]\n            # Sort points. Cut-down version of\n            # sort_points.sort_points_plane() and subfunctions\n            reference = np.array([0., 0., 1])\n            angle = np.arccos(np.dot(normals[:, fi], reference))\n            vect = np.cross(normals[:, fi], reference)\n            # Cut-down version of cg.rot()\n            W = np.array( [[       0., -vect[2],  vect[1]],\n                           [  vect[2],       0., -vect[0]],\n                           [ -vect[1],  vect[0],       0. ]])\n            R = np.identity(3) + np.sin(angle)*W + \\\n                  (1.-np.cos(angle)) * np.linalg.matrix_power(W, 2)\n            # pts is now a npt x 3 matrix\n            pts = np.array([R.dot(nodes_loc[:, i])\\\n                                  for i in range(nodes_loc.shape[1])])\n            center = R.dot(fc[:, fi])\n            # Distance from projected points to center\n            delta = np.array([pts[i] - center\\\n                              for i in range(pts.shape[0])])[:, :2]\n            nrm = np.sqrt(delta[:, 0]**2 + delta[:, 1]**2)\n            delta = delta / nrm[:, np.newaxis]\n\n            argsort = np.argsort(np.arctan2(delta[:, 0], delta[:, 1]))\n            cell_nodes[counter:(counter+num_p_loc)] = ptsId[argsort]\n            counter += num_p_loc\n\n    return cell_nodes",
  "def __init__(self, grid, name, folder=None, **kwargs):\n        \"\"\"\n        Parameters:\n        grid: the grid or grid bucket\n        name: the root of file name without any extension.\n        folder: (optional) the folder to save the file. If the folder does not\n            exist it will be created.\n\n        Optional arguments in kwargs:\n        fixed_grid: (optional) in a time dependent simulation specify if the\n            grid changes in time or not. The default is True.\n        binary: export in binary format, default is True.\n\n        How to use:\n        If you need to export a single grid:\n        save = Exporter(g, \"solution\", folder=\"results\")\n        save.write_vtk({\"cells_id\": cells_id, \"pressure\": pressure})\n\n        In a time loop:\n        save = Exporter(gb, \"solution\", folder=\"results\")\n        while time:\n            save.write_vtk({\"conc\": conc}, time_step=i)\n        save.write_pvd(steps*deltaT)\n\n        if you need to export the grid bucket\n        save = Exporter(gb, \"solution\", folder=\"results\")\n        save.write_vtk(gb, [\"cells_id\", \"pressure\"])\n\n        In a time loop:\n        while time:\n            save.write_vtk([\"conc\"], time_step=i)\n        save.write_pvd(steps*deltaT)\n\n        In the case of different physics, change the file name with\n        \"change_name\".\n\n        \"\"\"\n\n        self.gb = grid\n        self.name = name\n        self.folder = folder\n        self.fixed_grid = kwargs.get('fixed_grid', True)\n        self.binary = kwargs.get('binary', True)\n\n        self.is_GridBucket = isinstance(self.gb, grid_bucket.GridBucket)\n        self.is_not_vtk = 'vtk' not in sys.modules\n\n        if self.is_not_vtk:\n            return\n\n        if self.is_GridBucket:\n            self.gb_VTK = np.empty(self.gb.size(), dtype=np.object)\n        else:\n            self.gb_VTK = None\n\n        self.has_numba = 'numba' in sys.modules\n\n        if self.fixed_grid:\n            self._update_gb_VTK()",
  "def change_name(self, name):\n        \"\"\"\n        Change the root name of the files, useful when different physics are\n        considered but on the same grid.\n\n        Parameters:\n        name: the new root name of the files.\n        \"\"\"\n        self.name = name",
  "def write_vtk(self, data=None, time_step=None, grid=None):\n        \"\"\" Interface function to export in VTK the grid and additional data.\n\n        In 2d the cells are represented as polygon, while in 3d as polyhedra.\n        VTK module need to be installed.\n        In 3d the geometry of the mesh needs to be computed.\n\n        To work with python3, the package vtk should be installed in version 7\n        or higher.\n\n        Parameters:\n        data: if g is a single grid then data is a dictionary (see example)\n              if g is a grid bucket then list of names for optional data,\n              they are the keys in the grid bucket (see example).\n        time_step: (optional) in a time dependent problem defines the full name of\n            the file.\n        grid: (optional) in case of changing grid set a new one.\n\n        \"\"\"\n        if self.is_not_vtk:\n            return\n\n        if self.fixed_grid and grid is not None:\n            raise ValueError(\"Inconsistency in exporter setting\")\n        elif not self.fixed_grid and grid is not None:\n            self.gb = grid\n            self.is_GridBucket = isinstance(self.gb, grid_bucket.GridBucket)\n            self._update_gVTK()\n\n        if self.is_GridBucket:\n            self._export_vtk_gb(data, time_step)\n        else:\n            # No need of special naming, create the folder\n            name = self._make_folder(self.folder, self.name)\n            self._export_vtk_single(data, time_step, self.gb, name)",
  "def write_pvd(self, time):\n        \"\"\" Interface function to export in PVD file the time loop informations.\n        The user should open only this file in paraview.\n\n        We assume that the VTU associated files have the same name.\n        We assume that the VTU associated files are in the same folder.\n\n        Parameters:\n        time: vector of times.\n\n        \"\"\"\n        if self.is_not_vtk:\n            return\n\n        o_file = open(self._make_folder(self.folder, self.name)+\".pvd\", 'w')\n        b = 'LittleEndian' if sys.byteorder == 'little' else 'BigEndian'\n        c = ' compressor=\"vtkZLibDataCompressor\"'\n        header = '<?xml version=\"1.0\"?>\\n'+ \\\n                 '<VTKFile type=\"Collection\" version=\"0.1\" ' + \\\n                                              'byte_order=\"%s\"%s>\\n' % (b,c) + \\\n                 '<Collection>\\n'\n        o_file.write(header)\n        fm = '\\t<DataSet group=\"\" part=\"\" timestep=\"%f\" file=\"%s\"/>\\n'\n\n        time_step = np.arange(np.atleast_1d(time).size)\n\n        if self.is_GridBucket:\n            [o_file.write(fm%(time[t],\n                        self._make_file_name(self.name, t, d['node_number']))) \\\n                                         for t in time_step for _, d in self.gb]\n        else:\n            [o_file.write(fm%(time[t], self._make_file_name(self.name, t))) \\\n                                                             for t in time_step]\n\n        o_file.write('</Collection>\\n'+'</VTKFile>')\n        o_file.close()",
  "def _export_vtk_single(self, data, time_step, g, name):\n        name = self._make_file_name(name, time_step)\n        self._write_vtk(data, name, self.gb_VTK)",
  "def _export_vtk_gb(self, data, time_step):\n        if data is not None:\n            data = np.atleast_1d(data).tolist()\n        assert isinstance(data, list) or data is None\n        data = list() if data is None else data\n        data.append('grid_dim')\n\n        self.gb.assign_node_ordering(overwrite_existing=False)\n        self.gb.add_node_props(['grid_dim', 'file_name'])\n\n        for g, d in self.gb:\n            if g.dim > 0:\n                d['file_name'] = self._make_file_name(self.name, time_step,\n                                                               d['node_number'])\n                file_name = self._make_folder(self.folder, d['file_name'])\n                d['grid_dim'] = np.tile(g.dim, g.num_cells)\n                dic_data = self.gb.node_props_of_keys(g, data)\n                g_VTK = self.gb_VTK[d['node_number']]\n                self._write_vtk(dic_data, file_name, g_VTK)\n\n        name = self._make_folder(self.folder, self.name)+\".pvd\"\n        self._export_pvd_gb(name)",
  "def _export_pvd_gb(self, name):\n        o_file = open(name, 'w')\n        b = 'LittleEndian' if sys.byteorder == 'little' else 'BigEndian'\n        c = ' compressor=\"vtkZLibDataCompressor\"'\n        header = '<?xml version=\"1.0\"?>\\n'+ \\\n                 '<VTKFile type=\"Collection\" version=\"0.1\" ' + \\\n                                              'byte_order=\"%s\"%s>\\n' % (b,c) + \\\n                 '<Collection>\\n'\n        o_file.write(header)\n        fm = '\\t<DataSet group=\"\" part=\"\" file=\"%s\"/>\\n'\n        [o_file.write( fm % d['file_name'] ) for g, d in self.gb if g.dim!=0]\n        o_file.write('</Collection>\\n'+'</VTKFile>')\n        o_file.close()",
  "def _export_vtk_grid(self, g):\n        if g.dim == 0:\n            return\n        elif g.dim == 1:\n            return self._export_vtk_1d(g)\n        elif g.dim == 2:\n            return self._export_vtk_2d(g)\n        elif g.dim == 3:\n            return self._export_vtk_3d(g)",
  "def _export_vtk_1d(self, g):\n        cell_nodes = g.cell_nodes()\n        nodes, cells, _  = sps.find(cell_nodes)\n        gVTK = vtk.vtkUnstructuredGrid()\n\n        for c in np.arange(g.num_cells):\n            loc = slice(cell_nodes.indptr[c], cell_nodes.indptr[c+1])\n            ptsId = nodes[loc]\n            fsVTK = vtk.vtkIdList()\n            [fsVTK.InsertNextId(p) for p in ptsId]\n\n            gVTK.InsertNextCell(vtk.VTK_LINE, fsVTK)\n\n        ptsVTK = vtk.vtkPoints()\n        if g.nodes.shape[0] == 1:\n            [ptsVTK.InsertNextPoint(node, 0., 0.) for node in g.nodes.T]\n        if g.nodes.shape[0] == 2:\n            [ptsVTK.InsertNextPoint(node[0], node[1], 0.) for node in g.nodes.T]\n        else:\n            [ptsVTK.InsertNextPoint(*node) for node in g.nodes.T]\n        gVTK.SetPoints(ptsVTK)\n\n        return gVTK",
  "def _export_vtk_2d(self, g):\n        faces_cells, _, _ = sps.find(g.cell_faces)\n        nodes_faces, _, _ = sps.find(g.face_nodes)\n\n        gVTK = vtk.vtkUnstructuredGrid()\n\n        for c in np.arange(g.num_cells):\n            loc = slice(g.cell_faces.indptr[c], g.cell_faces.indptr[c+1])\n            ptsId = np.array([nodes_faces[g.face_nodes.indptr[f]:\\\n                                          g.face_nodes.indptr[f+1]]\n                              for f in faces_cells[loc]]).T\n            ptsId = sort_points.sort_point_pairs(ptsId)[0,:]\n\n            fsVTK = vtk.vtkIdList()\n            [fsVTK.InsertNextId(p) for p in ptsId]\n\n            gVTK.InsertNextCell(vtk.VTK_POLYGON, fsVTK)\n\n        ptsVTK = vtk.vtkPoints()\n        if g.nodes.shape[0] == 2:\n            [ptsVTK.InsertNextPoint(node[0], node[1], 0.) for node in g.nodes.T]\n        else:\n            [ptsVTK.InsertNextPoint(*node ) for node in g.nodes.T]\n        gVTK.SetPoints(ptsVTK)\n\n        return gVTK",
  "def _export_vtk_3d(self, g):\n        # This functionality became rather complex, with possible use of numba.\n        # Decided to dump this to a separate file.\n        return self._define_gvtk_3d(g)",
  "def _write_vtk(self, data, name, g_VTK):\n        writer = vtk.vtkXMLUnstructuredGridWriter()\n        writer.SetInputData(g_VTK)\n        writer.SetFileName(name)\n\n        if data is not None:\n            for name_field, values_field in data.items():\n                if values_field is None:\n                    continue\n                dataVTK = ns.numpy_to_vtk(values_field.ravel(order='F'),\n                                          deep=True, array_type=vtk.VTK_DOUBLE)\n                dataVTK.SetName(str(name_field))\n                dataVTK.SetNumberOfComponents(1 if values_field.ndim == 1 else 3)\n                g_VTK.GetCellData().AddArray(dataVTK)\n\n        if not self.binary:\n            writer.SetDataModeToAscii()\n        writer.Update()\n\n        if data is not None:\n            for name_field, _ in data.items():\n                cell_data = g_VTK.GetCellData().RemoveArray(str(name_field))",
  "def _update_gb_VTK(self):\n        if self.is_GridBucket:\n            for g, d in self.gb:\n                self.gb_VTK[d['node_number']] = self._export_vtk_grid(g)\n        else:\n            self.gb_VTK = self._export_vtk_grid(self.gb)",
  "def _make_folder(self, folder, name=None):\n        if folder is None: return name\n\n        if not os.path.exists(folder):\n            os.makedirs(folder)\n        return os.path.join(folder, name)",
  "def _make_file_name(self, name, time_step=None, node_number=None):\n\n        extension = \".vtu\"\n        padding = 6\n        if node_number is None: # normal grid\n            if time_step is None:\n                return name + extension\n            else:\n                time = str(time_step).zfill(padding)\n                return name + \"_\" + time + extension\n        else: # part of a grid bucket\n            grid = str(node_number).zfill(padding)\n            if time_step is None:\n                return name + \"_\" + grid + extension\n            else:\n                time = str(time_step).zfill(padding)\n                return name + \"_\" + grid + \"_\" + time + extension",
  "def _define_gvtk_3d(self, g):\n        gVTK = vtk.vtkUnstructuredGrid()\n\n        faces_cells, cells, _ = sps.find(g.cell_faces)\n        nodes_faces, faces, _ = sps.find(g.face_nodes)\n\n        cptr = g.cell_faces.indptr\n        fptr = g.face_nodes.indptr\n        face_per_cell = np.diff(cptr)\n        nodes_per_face = np.diff(fptr)\n\n        # Total number of nodes to be written in the face-node relation\n        num_cell_nodes = np.array([nodes_per_face[i] \\\n                                   for i in g.cell_faces.indices])\n\n        n = g.nodes\n        fc = g.face_centers\n        normal_vec = g.face_normals / g.face_areas\n\n        # Use numba if available, unless the problem is very small, in which\n        # case the pure python version probably is faster than combined compile\n        # and runtime for numba\n        # The number 1000 here is somewhat random.\n        if self.has_numba and g.num_cells > 1000:\n            logger.info('Construct 3d grid information using numba')\n            cell_nodes = _point_ind_numba(cptr, fptr, faces_cells, nodes_faces,\n                                          n, fc, normal_vec, num_cell_nodes)\n        else:\n            logger.info('Construct 3d grid information using pure python')\n            cell_nodes = _point_ind(cptr, fptr, faces_cells, nodes_faces, n,\n                                    fc, normal_vec, num_cell_nodes)\n        # implementation note: I did not even try feeding this to numba, my\n        # guess is that it will not like the vtk specific stuff.\n        node_counter = 0\n        face_counter = 0\n        for c in np.arange(g.num_cells):\n            fsVTK = vtk.vtkIdList()\n            fsVTK.InsertNextId(face_per_cell[c]) # Number faces that make up the cell\n            for f in range(face_per_cell[c]):\n                fi = g.cell_faces.indices[face_counter]\n                fsVTK.InsertNextId(nodes_per_face[fi]) # Number of points in face\n                for ni in range(nodes_per_face[fi]):\n                    fsVTK.InsertNextId(cell_nodes[node_counter])\n                    node_counter += 1\n                face_counter += 1\n\n            gVTK.InsertNextCell(vtk.VTK_POLYHEDRON, fsVTK)\n\n        ptsVTK = vtk.vtkPoints()\n        [ptsVTK.InsertNextPoint(*node) for node in g.nodes.T]\n        gVTK.SetPoints(ptsVTK)\n\n        return gVTK",
  "def _point_ind_numba(cell_ptr, face_ptr, faces_cells, nodes_faces,\n                         nodes, fc, normals, num_cell_nodes):\n        \"\"\" Implementation note: This turned out to be less than pretty, and quite\n        a bit more explicit than the corresponding pure python implementation.\n        The process was basically to circumvent whatever statements numba did not\n        like. Not sure about why this ended so, but there you go.\n        \"\"\"\n        cell_nodes = np.zeros(num_cell_nodes.sum(), dtype=np.int32)\n        counter = 0\n        fc.astype(np.float64)\n        for ci in range(cell_ptr.size - 1):\n            loc_c = slice(cell_ptr[ci], cell_ptr[ci + 1])\n            for fi in faces_cells[loc_c]:\n                loc_f = np.arange(face_ptr[fi], face_ptr[fi+1])\n                ptsId = nodes_faces[loc_f]\n                num_p_loc = ptsId.size\n                nodes_loc = np.zeros((3, num_p_loc))\n                for iter1 in range(num_p_loc):\n                    nodes_loc[:, iter1] = nodes[:, ptsId[iter1]]\n    #            # Sort points. Cut-down version of\n    #            # sort_points.sort_points_plane() and subfunctions\n                reference = np.array([0., 0., 1])\n                angle = np.arccos(np.dot(normals[:, fi], reference))\n                # Hand code cross product, not supported by current numba version\n                vect = np.array([  normals[1, fi] * reference[2]\\\n                                 - normals[2, fi] * reference[1],\n                                   normals[2, fi] * reference[0]\\\n                                 - normals[0, fi] * reference[2],\n                                   normals[0, fi] * reference[1]\\\n                                 - normals[1, fi] * reference[0]\n                                 ], dtype=np.float64)\n    ##            # Cut-down version of cg.rot()\n                W = np.array( [       0., -vect[2],  vect[1],\n                                 vect[2],       0., -vect[0],\n                                -vect[1],  vect[0],       0. ]).reshape((3, 3))\n                R = np.identity(3) + np.sin(angle)*W.reshape((3, 3)) + \\\n                ((1.-np.cos(angle)) * np.linalg.matrix_power(W, 2).ravel()).reshape((3, 3))\n    ##            # pts is now a npt x 3 matrix\n                num_p = nodes_loc.shape[1]\n                pts = np.zeros((3, num_p))\n                fc_loc = fc[:, fi]\n                center = np.zeros(3)\n                for i in range(3):\n                    center[i] = R[i, 0] * fc_loc[0] + R[i, 1] * fc_loc[1] + \\\n                                R[i, 2] * fc_loc[2]\n                for i in range(num_p):\n                    for j in range(3):\n                        pts[j, i] = R[j, 0] * nodes_loc[0, i]\\\n                                  + R[j, 1] * nodes_loc[1, i] + \\\n                                  + R[j, 2] * nodes_loc[2, i]\n    ##            # Distance from projected points to center\n                delta = 0*pts\n                for i in range(num_p):\n                    delta[:, i] = pts[:, i] - center\n                nrm = np.sqrt(delta[0]**2 + delta[1]**2)\n                for i in range(num_p):\n                    delta[:, i] = delta[:, i] / nrm[i]\n    ##\n                argsort = np.argsort(np.arctan2(delta[0], delta[1]))\n                cell_nodes[counter:(counter+num_p_loc)] = ptsId[argsort]\n                counter += num_p_loc\n\n        return cell_nodes",
  "class BoundaryCondition(object):\n    \"\"\" Class to store information on boundary conditions.\n\n    The BCs are specified by face number, and can have type Dirichlet or\n    Neumann (Robin may be included later). For details on default values etc.,\n    see constructor.\n\n    Attributes:\n        num_faces (int): Number of faces in the grid\n        dim (int): Dimension of the boundary. One less than the dimension of\n            the grid.\n        is_neu (np.ndarray boolean, size g.num_faces): Element i is true if\n            face i has been assigned a Neumann condition. Tacitly assumes that\n            the face is on the boundary. Should be false for internal faces, as\n            well as Dirichlet faces.\n        is_dir (np.ndarary, boolean, size g.num_faces): Element i is true if\n            face i has been assigned a Neumann condition.\n\n    \"\"\"\n\n    def __init__(self, g, faces=None, cond=None):\n        \"\"\"Constructor for BoundaryConditions.\n\n        The conditions are specified by face numbers. Faces that do not get an\n        explicit condition will have Neumann conditions assigned.\n\n        Parameters:\n            g (grid): For which boundary conditions are set.\n            faces (np.ndarray): Faces for which conditions are assigned.\n            cond (list of str): Conditions on the faces, in the same order as\n                used in faces. Should be as long as faces.\n\n        Example:\n            # Assign Dirichlet condititons on the left side of a grid; implicit\n            # Neumann conditions on the rest\n            g = CartGrid([2, 2])\n            west_face = bc.face_on_side(g, 'west')\n            bound_cond = BoundaryCondition(g, faces=west_face, cond=['dir',\n                                                                     'dir'])\n\n        \"\"\"\n\n        self.num_faces = g.num_faces\n        self.dim = g.dim - 1\n\n        # Find indices of boundary faces\n        bf = g.get_all_boundary_faces()\n\n        # Keep track of internal boundaries.\n        self.is_internal = g.tags['fracture_faces']\n\n        self.is_neu = np.zeros(self.num_faces, dtype=bool)\n        self.is_dir = np.zeros(self.num_faces, dtype=bool)\n\n        # By default, all boundary faces are Neumann.\n        self.is_neu[bf] = True\n\n        if faces is not None:\n            # Validate arguments\n            assert cond is not None\n            if faces.dtype == bool:\n                if faces.size != self.num_faces:\n                    raise ValueError('''When giving logical faces, the size of\n                                        array must match number of faces''')\n                faces = np.argwhere(faces)\n            if not np.all(np.in1d(faces, bf)):\n                raise ValueError('Give boundary condition only on the \\\n                                 boundary')\n            domain_boundary_and_tips = np.argwhere(np.logical_or(\n                g.tags['domain_boundary_faces'], g.tags['tip_faces']))\n            if not np.all(np.in1d(faces, domain_boundary_and_tips)):\n                warnings.warn('You are now specifying conditions on internal \\\n                              boundaries. Be very careful!')\n            if isinstance(cond, str):\n                cond = [cond] * faces.size\n            if faces.size != len(cond):\n                raise ValueError('One BC per face')\n\n            for l in np.arange(faces.size):\n                s = cond[l]\n                if s.lower() == 'neu':\n                    pass  # Neumann is already default\n                elif s.lower() == 'dir':\n                    self.is_dir[faces[l]] = True\n                    self.is_neu[faces[l]] = False\n                else:\n                    raise ValueError('Boundary should be Dirichlet or Neumann')",
  "def face_on_side(g, side, tol=1e-8):\n    \"\"\" Find faces on specified sides of a grid.\n\n    It is assumed that the grid forms a box in 2d or 3d.\n\n    The faces are specified by one of two type of keywords: (xmin / west),\n    (xmax / east), (ymin / south), (ymax / north), (zmin, bottom),\n    (zmax / top).\n\n    Parameters:\n        g (grid): For which we want to find faces.\n        side (str, or list of str): Sides for which we want to find the\n            boundary faces.\n        tol (double, optional): Geometric tolerance for deciding whether a face\n            lays on the boundary. Defaults to 1e-8.\n\n    Returns:\n        list of lists: Outer list has one element per element in side (same\n            ordering). Inner list contains global indices of faces laying on\n            that side.\n\n    \"\"\"\n    if isinstance(side, str):\n        side = [side]\n\n    faces = []\n    for s in side:\n        s = s.lower().strip()\n        if s == 'west' or s == 'xmin':\n            xm = g.nodes[0].min()\n            faces.append(np.squeeze(np.where(np.abs(g.face_centers[0] - xm) <\n                                             tol)))\n        elif s == 'east' or s == 'xmax':\n            xm = g.nodes[0].max()\n            faces.append(np.squeeze(np.where(np.abs(g.face_centers[0] - xm) <\n                                             tol)))\n        elif s == 'south' or s == 'ymin':\n            xm = g.nodes[1].min()\n            faces.append(np.squeeze(np.where(np.abs(g.face_centers[1] - xm) <\n                                             tol)))\n        elif s == 'north' or s == 'ymax':\n            xm = g.nodes[1].max()\n            faces.append(np.squeeze(np.where(np.abs(g.face_centers[1] - xm) <\n                                             tol)))\n        elif s == 'bottom' or s == 'bot' or s == 'zmin':\n            xm = g.nodes[2].min()\n            faces.append(np.squeeze(np.where(np.abs(g.face_centers[2] - xm) <\n                                             tol)))\n        elif s == 'top' or s == 'zmax':\n            xm = g.nodes[2].max()\n            faces.append(np.squeeze(np.where(np.abs(g.face_centers[2] - xm) <\n                                             tol)))\n        else:\n            raise ValueError('Unknow face side')\n    return faces",
  "def __init__(self, g, faces=None, cond=None):\n        \"\"\"Constructor for BoundaryConditions.\n\n        The conditions are specified by face numbers. Faces that do not get an\n        explicit condition will have Neumann conditions assigned.\n\n        Parameters:\n            g (grid): For which boundary conditions are set.\n            faces (np.ndarray): Faces for which conditions are assigned.\n            cond (list of str): Conditions on the faces, in the same order as\n                used in faces. Should be as long as faces.\n\n        Example:\n            # Assign Dirichlet condititons on the left side of a grid; implicit\n            # Neumann conditions on the rest\n            g = CartGrid([2, 2])\n            west_face = bc.face_on_side(g, 'west')\n            bound_cond = BoundaryCondition(g, faces=west_face, cond=['dir',\n                                                                     'dir'])\n\n        \"\"\"\n\n        self.num_faces = g.num_faces\n        self.dim = g.dim - 1\n\n        # Find indices of boundary faces\n        bf = g.get_all_boundary_faces()\n\n        # Keep track of internal boundaries.\n        self.is_internal = g.tags['fracture_faces']\n\n        self.is_neu = np.zeros(self.num_faces, dtype=bool)\n        self.is_dir = np.zeros(self.num_faces, dtype=bool)\n\n        # By default, all boundary faces are Neumann.\n        self.is_neu[bf] = True\n\n        if faces is not None:\n            # Validate arguments\n            assert cond is not None\n            if faces.dtype == bool:\n                if faces.size != self.num_faces:\n                    raise ValueError('''When giving logical faces, the size of\n                                        array must match number of faces''')\n                faces = np.argwhere(faces)\n            if not np.all(np.in1d(faces, bf)):\n                raise ValueError('Give boundary condition only on the \\\n                                 boundary')\n            domain_boundary_and_tips = np.argwhere(np.logical_or(\n                g.tags['domain_boundary_faces'], g.tags['tip_faces']))\n            if not np.all(np.in1d(faces, domain_boundary_and_tips)):\n                warnings.warn('You are now specifying conditions on internal \\\n                              boundaries. Be very careful!')\n            if isinstance(cond, str):\n                cond = [cond] * faces.size\n            if faces.size != len(cond):\n                raise ValueError('One BC per face')\n\n            for l in np.arange(faces.size):\n                s = cond[l]\n                if s.lower() == 'neu':\n                    pass  # Neumann is already default\n                elif s.lower() == 'dir':\n                    self.is_dir[faces[l]] = True\n                    self.is_neu[faces[l]] = False\n                else:\n                    raise ValueError('Boundary should be Dirichlet or Neumann')",
  "def poisson_from_lame(mu, lmbda):\n    \"\"\" Compute Poisson's ratio from Lame parameters\n\n    Parameters:\n        mu (double): first Lame parameter\n        lmbda (double): Second Lame parameter\n\n    Returns:\n        double: Poisson's ratio\n\n    \"\"\"\n    return lmbda / (2 * (mu + lmbda))",
  "def lame_from_young_poisson(e, nu):\n    \"\"\" Compute Lame parameters from Young's modulus and Poisson's ratio.\n\n    Parameters:\n        e (double): Young's modulus\n        nu (double): Poisson's ratio\n\n    Returns:\n        double: First Lame parameter\n        double: Second Lame parameter / shear modulus\n\n    \"\"\"\n    lmbda = e * nu / ((1 + nu) * (1 - 2 * nu))\n    mu = e / (2 * (1 + nu))\n\n    return lmbda, mu",
  "class UnitRock(object):\n    \"\"\" Mother of all rocks, all values are unity.\n\n    Attributes:\n        PERMEABILITY:\n        POROSITY:\n        LAMBDA: First Lame parameter\n        MU: Second lame parameter / shear modulus\n        YOUNG_MODULUS: Young's modulus\n        POISSON_RATIO:\n\n    \"\"\"\n\n    \n\n    PERMEABILITY = 1\n    POROSITY = 1\n    MU = 1\n    LAMBDA = 1\n    YOUNG_MODULUS = 1\n    POSSION_RATIO = 1",
  "class SandStone(UnitRock):\n    \"\"\" Generic values for Sandstone.\n\n    Data partially from:\n        http://civilblog.org/2015/02/13/what-are-the-values-of-modulus-of-elasticity-poissons-ratio-for-different-rocks/\n\n    \"\"\"\n    def __init__(self):\n\n        # Fairly permeable rock.\n        self.PERMEABILITY = 1 * units.DARCY\n        self.POROSITY = 0.2\n        # Reported range for Young's modulus is 0.5-8.6\n        self.YOUNG_MODULUS = 5 * units.KILOGRAM / units.CENTI**2 * 1e5\n        # Reported range for Poisson's ratio is 0.066-0.125\n        self.POISSON_RATIO = 0.1\n\n        self.LAMBDA, self.MU = lame_from_young_poisson(self.YOUNG_MODULUS,\n                                                       self.POISSON_RATIO)",
  "class Shale(UnitRock):\n    \"\"\" Generic values for shale.\n\n\n    Data partially from:\n        http://civilblog.org/2015/02/13/what-are-the-values-of-modulus-of-elasticity-poissons-ratio-for-different-rocks/\n\n    \"\"\"\n    def __init__(self):\n        # No source for permeability and porosity.\n        self.PERMEABILITY = 1e-5 * units.DARCY\n        self.POROSITY = 0.01\n        # Reported range for Young's modulus is 0.8-3.0\n        self.YOUNG_MODULUS = 1.5 * units.KILOGRAM / units.CENTI**2 * 1e5\n        # Reported range for Poisson's ratio is 0.11-0.54 (the latter is strange)\n        self.POISSON_RATIO = 0.3\n\n        self.LAMBDA, self.MU = lame_from_young_poisson(self.YOUNG_MODULUS, self.POISSON_RATIO)",
  "class Granite(UnitRock):\n    \"\"\" Generic values for granite.\n\n\n    Data partially from:\n        http://civilblog.org/2015/02/13/what-are-the-values-of-modulus-of-elasticity-poissons-ratio-for-different-rocks/\n\n    \"\"\"\n    def __init__(self):\n        # No source for permeability and porosity\n        self.PERMEABILITY = 1e-8 * units.DARCY\n        self.POROSITY = 0.01\n        # Reported range for Young's modulus is 2.6-7.0\n        self.YOUNG_MODULUS = 5 * units.KILOGRAM / units.CENTI**2 * 1e5\n        # Reported range for Poisson's ratio is 0.125-0.25\n        self.POISSON_RATIO = 0.2\n\n        self.LAMBDA, self.MU = lame_from_young_poisson(self.YOUNG_MODULUS,\n                                                       self.POISSON_RATIO)",
  "def __init__(self):\n\n        # Fairly permeable rock.\n        self.PERMEABILITY = 1 * units.DARCY\n        self.POROSITY = 0.2\n        # Reported range for Young's modulus is 0.5-8.6\n        self.YOUNG_MODULUS = 5 * units.KILOGRAM / units.CENTI**2 * 1e5\n        # Reported range for Poisson's ratio is 0.066-0.125\n        self.POISSON_RATIO = 0.1\n\n        self.LAMBDA, self.MU = lame_from_young_poisson(self.YOUNG_MODULUS,\n                                                       self.POISSON_RATIO)",
  "def __init__(self):\n        # No source for permeability and porosity.\n        self.PERMEABILITY = 1e-5 * units.DARCY\n        self.POROSITY = 0.01\n        # Reported range for Young's modulus is 0.8-3.0\n        self.YOUNG_MODULUS = 1.5 * units.KILOGRAM / units.CENTI**2 * 1e5\n        # Reported range for Poisson's ratio is 0.11-0.54 (the latter is strange)\n        self.POISSON_RATIO = 0.3\n\n        self.LAMBDA, self.MU = lame_from_young_poisson(self.YOUNG_MODULUS, self.POISSON_RATIO)",
  "def __init__(self):\n        # No source for permeability and porosity\n        self.PERMEABILITY = 1e-8 * units.DARCY\n        self.POROSITY = 0.01\n        # Reported range for Young's modulus is 2.6-7.0\n        self.YOUNG_MODULUS = 5 * units.KILOGRAM / units.CENTI**2 * 1e5\n        # Reported range for Poisson's ratio is 0.125-0.25\n        self.POISSON_RATIO = 0.2\n\n        self.LAMBDA, self.MU = lame_from_young_poisson(self.YOUNG_MODULUS,\n                                                       self.POISSON_RATIO)",
  "class Parameters(object):\n    \"\"\" Class to store all physical parameters used by solvers.\n\n    The intention is to provide a unified way of passing around parameters, and\n    also circumvent the issue of using a solver for multiple physical\n    processes (e.g. different types of boundary conditions in multi-physics\n    applications).\n\n    List of possible attributes (implemented as properties, see respective\n    getter methods for further description):\n\n    General quantities (not physics specific)\n\n    Scalar quantities\n        biot_alpha: Biot's coefficient in poro-elasticity. Defaults to 1.\n        fluid_viscosity: In a single phase system. Defaults to 1.\n        fluid_compr: Fluid compressibility in a single phase system. Defaults\n            to 0.\n\n    Cell-wise quantities:\n        aperture (fracture width). Defaults to 1.\n        porosity\n\n    Physic-specific\n        tensor (Returns permeability, conductivtiy or stiffness)\n        bc (BoundaryCondition object)\n        bc_val (Boundary values\n        sources (flow and transport)\n\n    Solvers will access data as needed. If a solver inquires for unassigned\n    data, this will result in a runtime error.\n\n    Attributes (in addition to parameters described above):\n\n    known_physics : list\n        List of keyword signifying physical processes. There are at least one\n        Solver that uses each of the keywords.\n\n    \"\"\"\n\n    def __init__(self, g):\n        \"\"\" Initialize Data object.\n\n        Parameters:\n\n        g - grid:\n            Grid where the data is valid. Currently, only number of cells and\n            faces are accessed.\n\n        \"\"\"\n        self._num_cells = g.num_cells\n        self._num_faces = g.num_faces\n        self.dim = g.dim\n        self.g = g\n\n        self.known_physics = ['flow', 'transport', 'mechanics']\n\n    def __repr__(self):\n        s = 'Data object for grid with ' + str(self._num_cells)\n        s += ' cells and ' + str(self._num_faces) + ' faces \\n'\n        s += 'Assigned attributes / properties: \\n'\n        s += str(list(self.__dict__.keys()))\n        return s\n\n    def _get_physics(self, obj):\n        if isinstance(obj, Solver):\n            if not hasattr(obj, 'physics'):\n                raise AttributeError(\n                    'Solver object should have attribute physics')\n            s = obj.physics.strip().lower()\n        elif isinstance(obj, str):\n            s = obj.strip().lower()\n        else:\n            raise ValueError('Expected str or Solver object')\n\n        if not s in self.known_physics:\n            # Give a warning if the physics keyword is unknown\n            warnings.warn('Unknown physics ' + s)\n        return s\n\n#------------- Start of definition of parameters -------------------------\n\n#------------- Constants\n\n#--------------- Biot Alpha ---------------------------------------\n    def get_biot_alpha(self):\n        if hasattr(self, '_biot_alpha'):\n            return self._biot_alpha\n        else:\n            return 1\n\n    def set_biot_alpha(self, val):\n        if val < 0 or val > 1:\n            raise ValueError('Biot\\'s constant should be between 0 and 1')\n        self._biot_alpha = val\n    biot_alpha = property(get_biot_alpha, set_biot_alpha)\n\n#--------------- Fluid viscosity ------------------------------------\n    def get_fluid_viscosity(self):\n        if hasattr(self, '_fluid_viscosity'):\n            return self._fluid_viscosity\n        else:\n            return 1\n\n    def set_fluid_viscosity(self, val):\n        if val <= 0:\n            raise ValueError('Fluid viscosity should be positive')\n        self._fluid_viscosity = val\n    fluid_viscosity = property(get_fluid_viscosity, set_fluid_viscosity)\n\n#----------- Fluid compressibility\n    def get_fluid_compr(self):\n        if hasattr(self, '_fluid_compr'):\n            return self._fluid_compr\n        else:\n            return 0.\n\n    def set_fluid_compr(self, val):\n        if val < 0:\n            raise ValueError('Fluid compressibility should be non-negative')\n        self._fluid_compr = val\n    fluid_compr = property(get_fluid_compr, set_fluid_compr)\n\n#----------- Fluid density - treated as constant for now\n    def get_fluid_density(self):\n        if hasattr(self, '_fluid_density'):\n            return self._fluid_density\n        else:\n            return 1\n    def set_fluid_density(self, val):\n        if val < 0:\n            raise ValueError('Fluid density should be non-negative')\n        self._fluid_density = val\n    fluid_density = property(get_fluid_density, set_fluid_density)\n\n#----------- Fluid specific heat - treated as constant for now\n    def get_fluid_specific_heat(self):\n        if hasattr(self, '_fluid_specific_heat'):\n            return self._fluid_specific_heat\n        else:\n            return 1\n    def set_fluid_specific_heat(self, val):\n        if val < 0:\n            raise ValueError('Fluid specific heat should be non-negative')\n        self._fluid_specific_heat = val\n    fluid_specific_heat = property(get_fluid_specific_heat,\n                                   set_fluid_specific_heat)\n\n#----------- rock density - treated as constant for now\n    def get_rock_density(self):\n        if hasattr(self, '_rock_density'):\n            return self._rock_density\n        else:\n            return 1\n    def set_rock_density(self, val):\n        if val < 0:\n            raise ValueError('rock density should be non-negative')\n        self._rock_density = val\n    rock_density = property(get_rock_density, set_rock_density)\n\n#----------- rock specific heat - treated as constant for now\n    def get_rock_specific_heat(self):\n        if hasattr(self, '_rock_specific_heat'):\n            return self._rock_specific_heat\n        else:\n            return 1\n    def set_rock_specific_heat(self, val):\n        if val < 0:\n            raise ValueError('rock specific heat should be non-negative')\n        self._rock_specific_heat = val\n    rock_specific_heat = property(get_rock_specific_heat,\n                                   set_rock_specific_heat)\n\n#-------------------- Cell-wise quantities below here --------------------\n\n#------------------ Aperture -----------------\n\n    def get_aperture(self, default=1):\n        \"\"\" double or array_like\n        Cell-wise quantity representing fracture aperture (really, height of\n        surpressed dimensions). Set as either a np.ndarray, or a scalar\n        (uniform) value. Always returned as np.ndarray.\n        \"\"\"\n        if not hasattr(self, '_apertures'):\n            return default * np.ones(self._num_cells)\n\n        if isinstance(self._apertures, np.ndarray):\n            # Hope that the user did not initialize as array with wrong size\n            return self._apertures\n        else:\n            return self._apertures * np.ones(self._num_cells)\n\n    def set_aperture(self, val):\n        if np.any(val < 0):\n            raise ValueError('Negative aperture')\n        self._apertures = val\n\n    aperture = property(get_aperture, set_aperture)\n\n#---------------- Porosity -------------------------------------------------\n\n    def get_porosity(self, default=1):\n        \"\"\" double or array-like\n        Cell-wise representation of porosity. Set as either a np.ndarary, or a\n        scalar (uniform) value. Always returned as np.ndarray.\n        \"\"\"\n        if not hasattr(self, '_porosity'):\n            return default * np.ones(self._num_cells)\n\n        if isinstance(self._porosity, np.ndarray):\n            # Hope that the user did not initialize as array with wrong size\n            return self._porosity\n        else:\n            return self._porosity * np.ones(self._num_cells)\n\n    def set_porosity(self, val):\n        if isinstance(val, np.ndarray):\n            if np.any(val < 0) or np.any(val > 1):\n                raise ValueError('Porosity outside unit interval')\n        else:\n            if val < 0 or val > 1:\n                raise ValueError('Porosity outside unit interval')\n        self._porosity = val\n\n    porosity = property(get_porosity, set_porosity)\n\n#-------------------- Face-wise quantities below here --------------------\n\n#------------------ slip_distance -----------------\n\n#------------------------------------\n    def get_slip_distance(self, default=0):\n        \"\"\" double or array-like\n        face-wise representation of slip distance. Set as either a np.ndarary, or a\n        scalar (uniform) value. Always returned as np.ndarray.\n        \"\"\"\n        if not hasattr(self, '_slip_distance'):\n            return default * np.ones(self._num_faces * self.dim)\n\n        if isinstance(self._slip_distance, np.ndarray):\n            # Hope that the user did not initialize as array with wrong size\n            return self._slip_distance\n        else:\n            return self._slip_distance * np.ones(self._num_faces * self.dim)\n\n\n    def set_slip_distance(self, val):\n        \"\"\" Set physics-specific slip_distance\n\n        Parameters:\n\n        val : slip distance, representing the difference in slip between left\n              and right fracture faces\n        \"\"\"\n        self._slip_distance = val\n\n    slip_distance = property(get_slip_distance, set_slip_distance)\n\n\n#----------- Multi-physics (solver-/context-dependent) parameters below -----\n\n#------------------- Sources ---------------------------------------------\n\n    def get_source(self, obj):\n        \"\"\" Pick out physics-specific source.\n\n        Discretization methods should access this method.\n\n        Parameters:\n\n        obj : Solver or str\n            Identification of physical regime. Either discretization object\n            with attribute 'physics' or a str.\n\n        Returns:\n\n        np.ndarray\n            Volume source if obj.physics equals 'flow'\n            Heat source if obj.physics equals 'transport'.\n\n        \"\"\"\n        physics = self._get_physics(obj)\n\n        if physics == 'flow':\n            return self.get_source_flow()\n        elif physics == 'transport':\n            return self.get_source_transport()\n        elif physics == 'mechanics':\n            return self.get_source_mechanics()\n        else:\n            raise ValueError('Unknown physics \"%s\".\\n Possible physics are: %s'\n                             % (physics, self.known_physics))\n\n    def set_source(self, obj, val):\n        \"\"\" Set physics-specific source\n\n        Parameters:\n\n        obj: Solver or str\n            Identification of physical regime. Either discretization object\n            with attribute 'physics' or a str.\n\n        val: np.ndarray. Size self._num_cells\n            Source terms in each cell.\n\n        \"\"\"\n        physics = self._get_physics(obj)\n\n        if physics == 'flow':\n            self._source_flow = val\n        elif physics == 'transport':\n            self._source_transport = val\n        elif physics == 'mechanics':\n            self._source_mechanics = val\n        else:\n            raise ValueError('Unknown physics \"%s\".\\n Possible physics are: %s'\n                             % (physics, self.known_physics))\n\n    def get_source_flow(self):\n        \"\"\" array_like\n        Cell-wise quantity representing the volume source in a cell. Represent\n        total in/outflow in the cell (integrated over the cell volume).\n        Sources should be accessed via get_source / set_source\n        \"\"\"\n        if hasattr(self, '_source_flow'):\n            return self._source_flow\n        else:\n            return np.zeros(self._num_cells)\n\n    source_flow = property(get_source_flow)\n\n    def get_source_transport(self):\n        \"\"\" array_like\n        Cell-wise quantity representing the concentration / temperature source\n        in a cell. Represent total in/outflow in the cell (integrated over the\n        cell volume).\n        Sources should be accessed via get_source / set_source\n        \"\"\"\n        if hasattr(self, '_source_transport'):\n            return self._source_transport\n        else:\n            return np.zeros(self._num_cells)\n\n    source_transport = property(get_source_transport)\n\n    def get_source_mechanics(self):\n        if hasattr(self, '_source_mechanics'):\n            return self._source_mechanics\n        else:\n            return np.zeros(self._num_cells * self.dim)\n    source_mechanics = property(get_source_mechanics)\n\n#-------------------- Backgound stress, ---------------------\n    def get_background_stress(self, obj):\n        \"\"\" Pick out physics-specific background_stress.\n\n        Discretization methods should access this method.\n\n        Parameters:\n\n        obj : Solver or str\n            Identification of physical regime. Either discretization object\n            with attribute 'physics' or a str.\n\n        Returns:\n\n        np.ndarray\n            stress matrix size = (g.dim, g.dim)\n\n        \"\"\"\n        physics = self._get_physics(obj)\n\n        if physics == 'flow':\n            return self.get_background_stress_flow()\n        elif physics == 'transport':\n            return self.get_background_stress_transport()\n        elif physics == 'mechanics':\n            return self.get_background_stress_mechanics()\n        else:\n            raise ValueError('Unknown physics \"%s\".\\n Possible physics are: %s'\n                             % (physics, self.known_physics))\n\n    def set_background_stress(self, obj, val):\n        \"\"\" Set physics-specific background_stress\n\n        Parameters:\n\n        obj: Solver or str\n            Identification of physical regime. Either discretization object\n            with attribute 'physics' or a str.\n\n        val: np.ndarray. Size (g.dim, g.dim)\n            background_stress for all cells\n\n        \"\"\"\n        physics = self._get_physics(obj)\n\n        if physics == 'flow':\n            self._background_stress_flow = val\n        elif physics == 'transport':\n            self._background_stress_transport = val\n        elif physics == 'mechanics':\n            self._background_stress_mechanics = val\n        else:\n            raise ValueError('Unknown physics \"%s\".\\n Possible physics are: %s'\n                             % (physics, self.known_physics))\n\n    def get_background_stress_flow(self):\n        \"\"\" array_like\n        Cell-wise quantity representing the volume background_stress in a cell. Represent\n        total in/outflow in the cell (integrated over the cell volume).\n        Background_Stresss should be accessed via get_background_stress / set_background_stress\n        \"\"\"\n        if hasattr(self, '_background_stress_flow'):\n            return self._background_stress_flow\n        else:\n            return np.zeros(self._num_cells)\n\n    background_stress_flow = property(get_background_stress_flow)\n\n    def get_background_stress_transport(self):\n        \"\"\" array_like\n        Cell-wise quantity representing the concentration / temperature background_stress\n        in a cell. Represent total in/outflow in the cell (integrated over the\n        cell volume).\n        Background_Stresss should be accessed via get_background_stress / set_background_stress\n        \"\"\"\n        if hasattr(self, '_background_stress_transport'):\n            return self._background_stress_transport\n        else:\n            return np.zeros(self._num_cells)\n\n    background_stress_transport = property(get_background_stress_transport)\n\n    def get_background_stress_mechanics(self):\n        if hasattr(self, '_background_stress_mechanics'):\n            return self._background_stress_mechanics\n        else:\n            return np.zeros(self._num_cells * self.dim)\n    background_stress_mechanics = property(get_background_stress_mechanics)\n\n\n#-------------------- Permeability, conductivity, ---------------------\n\n    def get_tensor(self, obj):\n        \"\"\" Pick out physics-specific tensor.\n\n        Discretization methods considering second and fourth orrder tensors\n        (e.g. permeability, conductivity, stiffness) should access this method.\n\n        Parameters:\n\n        obj : Solver\n            Discretization object. Should have attribute 'physics'.\n\n        Returns:\n\n        tensor, representing\n            Permeability if obj.physics equals 'flow'\n            conductivity if obj.physics equals 'transport'\n            stiffness if physics equals 'mechanics'\n\n        \"\"\"\n        physics = self._get_physics(obj)\n\n        if physics == 'flow':\n            return self.get_permeability()\n        elif physics == 'transport':\n            return self.get_conductivity()\n        elif physics == 'mechanics':\n            return self.get_stiffness()\n        else:\n            raise ValueError('Unknown physics \"%s\".\\n Possible physics are: %s'\n                             % (physics, self.known_physics))\n\n    def set_tensor(self, obj, val):\n        \"\"\" Set physics-specific tensor\n\n        Parameters:\n\n        obj: Solver or str\n            Identification of physical regime. Either discretization object\n            with attribute 'physics' or a str.\n\n        val : tensor, representing\n            Permeability if obj.physics equals 'flow'\n            conductivity if obj.physics equals 'transport'\n            stiffness if physics equals 'mechanics'\n\n        \"\"\"\n        physics = self._get_physics(obj)\n\n        if physics == 'flow':\n            self._perm = val\n        elif physics == 'transport':\n            self._conductivity = val\n        elif physics == 'mechanics':\n            self._stiffness = val\n        else:\n            raise ValueError('Unknown physics \"%s\".\\n Possible physics are: %s'\n                             % (physics, self.known_physics))\n\n    def get_permeability(self):\n        \"\"\" tensor.SecondOrder\n        Cell wise permeability, represented as a second order tensor.\n        Defaults to a unit tensor.\n        \"\"\"\n        if hasattr(self, '_perm'):\n            return self._perm\n        else:\n            t = SecondOrder(self.dim, np.ones(self._num_cells))\n            return t\n\n    perm = property(get_permeability)\n\n    def get_conductivity(self):\n        \"\"\" tensor.SecondOrder\n        Cell wise conductivity, represented as a second order tensor.\n        Defaults to a unit tensor.\n        \"\"\"\n        if hasattr(self, '_conductivity'):\n            return self._conductivity\n        else:\n            t = SecondOrder(self.dim, np.ones(self._num_cells))\n            return t\n\n    conductivity = property(get_conductivity)\n\n    def get_stiffness(self):\n        \"\"\" Stiffness matrix, defined as fourth order tensor.\n        If not defined, a unit tensor is returned.\n        \"\"\"\n        if hasattr(self, '_stiffness'):\n            return self._stiffness\n        else:\n            t = FourthOrder(self.dim, np.ones(self._num_cells),\n                            np.ones(self._num_cells))\n            return t\n\n    stiffness = property(get_stiffness)\n\n#--------------------- Boundary conditions and values ------------------------\n\n# Boundary condition\n\n    def get_bc(self, obj):\n        \"\"\" Pick out physics-specific boundary condition\n\n        Discretization methods should access this method.\n\n        Parameters:\n\n        obj : Solver\n            Discretization object. Should have attribute 'physics'.\n\n        Returns:\n\n        BoundaryCondition, for\n            flow/pressure equation y if physics equals 'flow'\n            transport equation if physics equals 'transport'\n            elasticity if physics equals 'mechanics'\n\n        \"\"\"\n        physics = self._get_physics(obj)\n\n        if physics == 'flow':\n            return self.get_bc_flow()\n        elif physics == 'transport':\n            return self.get_bc_transport()\n        elif physics == 'mechanics':\n            return self.get_bc_mechanics()\n\n    def set_bc(self, obj, val):\n        \"\"\" Set physics-specific boundary condition\n\n        Parameters:\n\n        obj: Solver or str\n            Identification of physical regime. Either discretization object\n            with attribute 'physics' or a str.\n\n        val : BoundaryCondition, representing\n            flow/pressure equation y if physics equals 'flow'\n            transport equation if physics equals 'transport'\n            elasticity if physics equals 'mechanics'\n\n        \"\"\"\n        physics = self._get_physics(obj)\n\n        if physics == 'flow':\n            self._bc_flow = val\n        elif physics == 'transport':\n            self._bc_transport = val\n        elif physics == 'mechanics':\n            self._bc_mechanics = val\n        else:\n            raise ValueError('Unknown physics \"%s\".\\n Possible physics are: %s'\n                             % (physics, self.known_physics))\n\n    def get_bc_flow(self):\n        \"\"\" BoundaryCondition object\n        Cell wise permeability, represented as a second order tensor.\n        Solvers should rather access get_tensor().\n        \"\"\"\n        if hasattr(self, '_bc_flow'):\n            return self._bc_flow\n        else:\n            return BoundaryCondition(self.g)\n\n    bc_flow = property(get_bc_flow)\n\n    def get_bc_transport(self):\n        \"\"\" bc.BoundaryCondition\n        Cell wise conductivity, represented as a second order tensor.\n        Solvers should rather access tensor().\n        \"\"\"\n        if hasattr(self, '_bc_transport'):\n            return self._bc_transport\n        else:\n            return BoundaryCondition(self.g)\n\n    bc_transport = property(get_bc_transport)\n\n    def get_bc_mechanics(self):\n        \"\"\" Stiffness matrix, defined as fourth order tensor\n        \"\"\"\n        if hasattr(self, '_bc_mechanics'):\n            return self._bc_mechanics\n        else:\n            return BoundaryCondition(self.g)\n\n    bc_mechanics = property(get_bc_mechanics)\n\n\n# Boundary value\n\n    def get_bc_val(self, obj):\n        \"\"\" Pick out physics-specific boundary condition\n\n        Discretization methods should access this method.\n\n        Parameters:\n\n        obj : Solver\n            Discretization object. Should have attribute 'physics'.\n\n        Returns:\n\n        BoundaryCondition, for\n            flow/pressure equation y if physics equals 'flow'\n            transport equation if physics equals 'transport'\n            elasticity if physics equals 'mechanics'\n            If the BoundaryCondition is not specified, Neumann conditions will\n            be assigned.\n        \"\"\"\n        physics = self._get_physics(obj)\n\n        if physics == 'flow':\n            return self.get_bc_val_flow()\n        elif physics == 'transport':\n            return self.get_bc_val_transport()\n        elif physics == 'mechanics':\n            return self.get_bc_val_mechanics()\n        else:\n            raise ValueError('Unknown physics \"%s\".\\n Possible physics are: %s'\n                             % (physics, self.known_physics))\n\n    def set_bc_val(self, obj, val):\n        \"\"\" Set physics-specific boundary condition\n\n        Parameters:\n\n        obj: Solver or str\n            Identification of physical regime. Either discretization object\n            with attribute 'physics' or a str.\n\n        val : BoundaryCondition, representing\n            flow/pressure equation y if physics equals 'flow'\n            transport equation if physics equals 'transport'\n            elasticity if physics equals 'mechanics'\n\n        \"\"\"\n        physics = self._get_physics(obj)\n\n        if physics == 'flow':\n            self._bc_val_flow = val\n        elif physics == 'transport':\n            self._bc_val_transport = val\n        elif physics == 'mechanics':\n            self._bc_val_mechanics = val\n        else:\n            raise ValueError('Unknown physics \"%s\".\\n Possible physics are: %s'\n                             % (physics, self.known_physics))\n\n    def get_bc_val_flow(self):\n        \"\"\" tensor.SecondOrder\n        Cell wise permeability, represented as a second order tensor.\n        Solvers should rather access get_tensor().\n        \"\"\"\n        if hasattr(self, '_bc_val_flow'):\n            return self._bc_val_flow\n        else:\n            return np.zeros(self._num_faces)\n\n    bc_val_flow = property(get_bc_val_flow)\n\n    def get_bc_val_transport(self):\n        \"\"\" tensor.SecondOrder\n        Cell wise conductivity, represented as a second order tensor.\n        Solvers should rather access tensor().\n        \"\"\"\n        if hasattr(self, '_bc_val_transport'):\n            return self._bc_val_transport\n        else:\n            return np.zeros(self._num_faces)\n\n    bc_val_transport = property(get_bc_val_transport)\n\n    def get_bc_val_mechanics(self):\n        \"\"\" tensor.FourthOrder\n        Cell wise conductivity, represented as a fourth order tensor.\n        Solvers should rather access tensor().\n        \"\"\"\n        if hasattr(self, '_bc_val_mechanics'):\n            return self._bc_val_mechanics\n        else:\n            return np.zeros(self._num_faces * self.dim)\n    bc_val_mechanics = property(get_bc_val_mechanics)",
  "def __init__(self, g):\n        \"\"\" Initialize Data object.\n\n        Parameters:\n\n        g - grid:\n            Grid where the data is valid. Currently, only number of cells and\n            faces are accessed.\n\n        \"\"\"\n        self._num_cells = g.num_cells\n        self._num_faces = g.num_faces\n        self.dim = g.dim\n        self.g = g\n\n        self.known_physics = ['flow', 'transport', 'mechanics']",
  "def __repr__(self):\n        s = 'Data object for grid with ' + str(self._num_cells)\n        s += ' cells and ' + str(self._num_faces) + ' faces \\n'\n        s += 'Assigned attributes / properties: \\n'\n        s += str(list(self.__dict__.keys()))\n        return s",
  "def _get_physics(self, obj):\n        if isinstance(obj, Solver):\n            if not hasattr(obj, 'physics'):\n                raise AttributeError(\n                    'Solver object should have attribute physics')\n            s = obj.physics.strip().lower()\n        elif isinstance(obj, str):\n            s = obj.strip().lower()\n        else:\n            raise ValueError('Expected str or Solver object')\n\n        if not s in self.known_physics:\n            # Give a warning if the physics keyword is unknown\n            warnings.warn('Unknown physics ' + s)\n        return s",
  "def get_biot_alpha(self):\n        if hasattr(self, '_biot_alpha'):\n            return self._biot_alpha\n        else:\n            return 1",
  "def set_biot_alpha(self, val):\n        if val < 0 or val > 1:\n            raise ValueError('Biot\\'s constant should be between 0 and 1')\n        self._biot_alpha = val",
  "def get_fluid_viscosity(self):\n        if hasattr(self, '_fluid_viscosity'):\n            return self._fluid_viscosity\n        else:\n            return 1",
  "def set_fluid_viscosity(self, val):\n        if val <= 0:\n            raise ValueError('Fluid viscosity should be positive')\n        self._fluid_viscosity = val",
  "def get_fluid_compr(self):\n        if hasattr(self, '_fluid_compr'):\n            return self._fluid_compr\n        else:\n            return 0.",
  "def set_fluid_compr(self, val):\n        if val < 0:\n            raise ValueError('Fluid compressibility should be non-negative')\n        self._fluid_compr = val",
  "def get_fluid_density(self):\n        if hasattr(self, '_fluid_density'):\n            return self._fluid_density\n        else:\n            return 1",
  "def set_fluid_density(self, val):\n        if val < 0:\n            raise ValueError('Fluid density should be non-negative')\n        self._fluid_density = val",
  "def get_fluid_specific_heat(self):\n        if hasattr(self, '_fluid_specific_heat'):\n            return self._fluid_specific_heat\n        else:\n            return 1",
  "def set_fluid_specific_heat(self, val):\n        if val < 0:\n            raise ValueError('Fluid specific heat should be non-negative')\n        self._fluid_specific_heat = val",
  "def get_rock_density(self):\n        if hasattr(self, '_rock_density'):\n            return self._rock_density\n        else:\n            return 1",
  "def set_rock_density(self, val):\n        if val < 0:\n            raise ValueError('rock density should be non-negative')\n        self._rock_density = val",
  "def get_rock_specific_heat(self):\n        if hasattr(self, '_rock_specific_heat'):\n            return self._rock_specific_heat\n        else:\n            return 1",
  "def set_rock_specific_heat(self, val):\n        if val < 0:\n            raise ValueError('rock specific heat should be non-negative')\n        self._rock_specific_heat = val",
  "def get_aperture(self, default=1):\n        \"\"\" double or array_like\n        Cell-wise quantity representing fracture aperture (really, height of\n        surpressed dimensions). Set as either a np.ndarray, or a scalar\n        (uniform) value. Always returned as np.ndarray.\n        \"\"\"\n        if not hasattr(self, '_apertures'):\n            return default * np.ones(self._num_cells)\n\n        if isinstance(self._apertures, np.ndarray):\n            # Hope that the user did not initialize as array with wrong size\n            return self._apertures\n        else:\n            return self._apertures * np.ones(self._num_cells)",
  "def set_aperture(self, val):\n        if np.any(val < 0):\n            raise ValueError('Negative aperture')\n        self._apertures = val",
  "def get_porosity(self, default=1):\n        \"\"\" double or array-like\n        Cell-wise representation of porosity. Set as either a np.ndarary, or a\n        scalar (uniform) value. Always returned as np.ndarray.\n        \"\"\"\n        if not hasattr(self, '_porosity'):\n            return default * np.ones(self._num_cells)\n\n        if isinstance(self._porosity, np.ndarray):\n            # Hope that the user did not initialize as array with wrong size\n            return self._porosity\n        else:\n            return self._porosity * np.ones(self._num_cells)",
  "def set_porosity(self, val):\n        if isinstance(val, np.ndarray):\n            if np.any(val < 0) or np.any(val > 1):\n                raise ValueError('Porosity outside unit interval')\n        else:\n            if val < 0 or val > 1:\n                raise ValueError('Porosity outside unit interval')\n        self._porosity = val",
  "def get_slip_distance(self, default=0):\n        \"\"\" double or array-like\n        face-wise representation of slip distance. Set as either a np.ndarary, or a\n        scalar (uniform) value. Always returned as np.ndarray.\n        \"\"\"\n        if not hasattr(self, '_slip_distance'):\n            return default * np.ones(self._num_faces * self.dim)\n\n        if isinstance(self._slip_distance, np.ndarray):\n            # Hope that the user did not initialize as array with wrong size\n            return self._slip_distance\n        else:\n            return self._slip_distance * np.ones(self._num_faces * self.dim)",
  "def set_slip_distance(self, val):\n        \"\"\" Set physics-specific slip_distance\n\n        Parameters:\n\n        val : slip distance, representing the difference in slip between left\n              and right fracture faces\n        \"\"\"\n        self._slip_distance = val",
  "def get_source(self, obj):\n        \"\"\" Pick out physics-specific source.\n\n        Discretization methods should access this method.\n\n        Parameters:\n\n        obj : Solver or str\n            Identification of physical regime. Either discretization object\n            with attribute 'physics' or a str.\n\n        Returns:\n\n        np.ndarray\n            Volume source if obj.physics equals 'flow'\n            Heat source if obj.physics equals 'transport'.\n\n        \"\"\"\n        physics = self._get_physics(obj)\n\n        if physics == 'flow':\n            return self.get_source_flow()\n        elif physics == 'transport':\n            return self.get_source_transport()\n        elif physics == 'mechanics':\n            return self.get_source_mechanics()\n        else:\n            raise ValueError('Unknown physics \"%s\".\\n Possible physics are: %s'\n                             % (physics, self.known_physics))",
  "def set_source(self, obj, val):\n        \"\"\" Set physics-specific source\n\n        Parameters:\n\n        obj: Solver or str\n            Identification of physical regime. Either discretization object\n            with attribute 'physics' or a str.\n\n        val: np.ndarray. Size self._num_cells\n            Source terms in each cell.\n\n        \"\"\"\n        physics = self._get_physics(obj)\n\n        if physics == 'flow':\n            self._source_flow = val\n        elif physics == 'transport':\n            self._source_transport = val\n        elif physics == 'mechanics':\n            self._source_mechanics = val\n        else:\n            raise ValueError('Unknown physics \"%s\".\\n Possible physics are: %s'\n                             % (physics, self.known_physics))",
  "def get_source_flow(self):\n        \"\"\" array_like\n        Cell-wise quantity representing the volume source in a cell. Represent\n        total in/outflow in the cell (integrated over the cell volume).\n        Sources should be accessed via get_source / set_source\n        \"\"\"\n        if hasattr(self, '_source_flow'):\n            return self._source_flow\n        else:\n            return np.zeros(self._num_cells)",
  "def get_source_transport(self):\n        \"\"\" array_like\n        Cell-wise quantity representing the concentration / temperature source\n        in a cell. Represent total in/outflow in the cell (integrated over the\n        cell volume).\n        Sources should be accessed via get_source / set_source\n        \"\"\"\n        if hasattr(self, '_source_transport'):\n            return self._source_transport\n        else:\n            return np.zeros(self._num_cells)",
  "def get_source_mechanics(self):\n        if hasattr(self, '_source_mechanics'):\n            return self._source_mechanics\n        else:\n            return np.zeros(self._num_cells * self.dim)",
  "def get_background_stress(self, obj):\n        \"\"\" Pick out physics-specific background_stress.\n\n        Discretization methods should access this method.\n\n        Parameters:\n\n        obj : Solver or str\n            Identification of physical regime. Either discretization object\n            with attribute 'physics' or a str.\n\n        Returns:\n\n        np.ndarray\n            stress matrix size = (g.dim, g.dim)\n\n        \"\"\"\n        physics = self._get_physics(obj)\n\n        if physics == 'flow':\n            return self.get_background_stress_flow()\n        elif physics == 'transport':\n            return self.get_background_stress_transport()\n        elif physics == 'mechanics':\n            return self.get_background_stress_mechanics()\n        else:\n            raise ValueError('Unknown physics \"%s\".\\n Possible physics are: %s'\n                             % (physics, self.known_physics))",
  "def set_background_stress(self, obj, val):\n        \"\"\" Set physics-specific background_stress\n\n        Parameters:\n\n        obj: Solver or str\n            Identification of physical regime. Either discretization object\n            with attribute 'physics' or a str.\n\n        val: np.ndarray. Size (g.dim, g.dim)\n            background_stress for all cells\n\n        \"\"\"\n        physics = self._get_physics(obj)\n\n        if physics == 'flow':\n            self._background_stress_flow = val\n        elif physics == 'transport':\n            self._background_stress_transport = val\n        elif physics == 'mechanics':\n            self._background_stress_mechanics = val\n        else:\n            raise ValueError('Unknown physics \"%s\".\\n Possible physics are: %s'\n                             % (physics, self.known_physics))",
  "def get_background_stress_flow(self):\n        \"\"\" array_like\n        Cell-wise quantity representing the volume background_stress in a cell. Represent\n        total in/outflow in the cell (integrated over the cell volume).\n        Background_Stresss should be accessed via get_background_stress / set_background_stress\n        \"\"\"\n        if hasattr(self, '_background_stress_flow'):\n            return self._background_stress_flow\n        else:\n            return np.zeros(self._num_cells)",
  "def get_background_stress_transport(self):\n        \"\"\" array_like\n        Cell-wise quantity representing the concentration / temperature background_stress\n        in a cell. Represent total in/outflow in the cell (integrated over the\n        cell volume).\n        Background_Stresss should be accessed via get_background_stress / set_background_stress\n        \"\"\"\n        if hasattr(self, '_background_stress_transport'):\n            return self._background_stress_transport\n        else:\n            return np.zeros(self._num_cells)",
  "def get_background_stress_mechanics(self):\n        if hasattr(self, '_background_stress_mechanics'):\n            return self._background_stress_mechanics\n        else:\n            return np.zeros(self._num_cells * self.dim)",
  "def get_tensor(self, obj):\n        \"\"\" Pick out physics-specific tensor.\n\n        Discretization methods considering second and fourth orrder tensors\n        (e.g. permeability, conductivity, stiffness) should access this method.\n\n        Parameters:\n\n        obj : Solver\n            Discretization object. Should have attribute 'physics'.\n\n        Returns:\n\n        tensor, representing\n            Permeability if obj.physics equals 'flow'\n            conductivity if obj.physics equals 'transport'\n            stiffness if physics equals 'mechanics'\n\n        \"\"\"\n        physics = self._get_physics(obj)\n\n        if physics == 'flow':\n            return self.get_permeability()\n        elif physics == 'transport':\n            return self.get_conductivity()\n        elif physics == 'mechanics':\n            return self.get_stiffness()\n        else:\n            raise ValueError('Unknown physics \"%s\".\\n Possible physics are: %s'\n                             % (physics, self.known_physics))",
  "def set_tensor(self, obj, val):\n        \"\"\" Set physics-specific tensor\n\n        Parameters:\n\n        obj: Solver or str\n            Identification of physical regime. Either discretization object\n            with attribute 'physics' or a str.\n\n        val : tensor, representing\n            Permeability if obj.physics equals 'flow'\n            conductivity if obj.physics equals 'transport'\n            stiffness if physics equals 'mechanics'\n\n        \"\"\"\n        physics = self._get_physics(obj)\n\n        if physics == 'flow':\n            self._perm = val\n        elif physics == 'transport':\n            self._conductivity = val\n        elif physics == 'mechanics':\n            self._stiffness = val\n        else:\n            raise ValueError('Unknown physics \"%s\".\\n Possible physics are: %s'\n                             % (physics, self.known_physics))",
  "def get_permeability(self):\n        \"\"\" tensor.SecondOrder\n        Cell wise permeability, represented as a second order tensor.\n        Defaults to a unit tensor.\n        \"\"\"\n        if hasattr(self, '_perm'):\n            return self._perm\n        else:\n            t = SecondOrder(self.dim, np.ones(self._num_cells))\n            return t",
  "def get_conductivity(self):\n        \"\"\" tensor.SecondOrder\n        Cell wise conductivity, represented as a second order tensor.\n        Defaults to a unit tensor.\n        \"\"\"\n        if hasattr(self, '_conductivity'):\n            return self._conductivity\n        else:\n            t = SecondOrder(self.dim, np.ones(self._num_cells))\n            return t",
  "def get_stiffness(self):\n        \"\"\" Stiffness matrix, defined as fourth order tensor.\n        If not defined, a unit tensor is returned.\n        \"\"\"\n        if hasattr(self, '_stiffness'):\n            return self._stiffness\n        else:\n            t = FourthOrder(self.dim, np.ones(self._num_cells),\n                            np.ones(self._num_cells))\n            return t",
  "def get_bc(self, obj):\n        \"\"\" Pick out physics-specific boundary condition\n\n        Discretization methods should access this method.\n\n        Parameters:\n\n        obj : Solver\n            Discretization object. Should have attribute 'physics'.\n\n        Returns:\n\n        BoundaryCondition, for\n            flow/pressure equation y if physics equals 'flow'\n            transport equation if physics equals 'transport'\n            elasticity if physics equals 'mechanics'\n\n        \"\"\"\n        physics = self._get_physics(obj)\n\n        if physics == 'flow':\n            return self.get_bc_flow()\n        elif physics == 'transport':\n            return self.get_bc_transport()\n        elif physics == 'mechanics':\n            return self.get_bc_mechanics()",
  "def set_bc(self, obj, val):\n        \"\"\" Set physics-specific boundary condition\n\n        Parameters:\n\n        obj: Solver or str\n            Identification of physical regime. Either discretization object\n            with attribute 'physics' or a str.\n\n        val : BoundaryCondition, representing\n            flow/pressure equation y if physics equals 'flow'\n            transport equation if physics equals 'transport'\n            elasticity if physics equals 'mechanics'\n\n        \"\"\"\n        physics = self._get_physics(obj)\n\n        if physics == 'flow':\n            self._bc_flow = val\n        elif physics == 'transport':\n            self._bc_transport = val\n        elif physics == 'mechanics':\n            self._bc_mechanics = val\n        else:\n            raise ValueError('Unknown physics \"%s\".\\n Possible physics are: %s'\n                             % (physics, self.known_physics))",
  "def get_bc_flow(self):\n        \"\"\" BoundaryCondition object\n        Cell wise permeability, represented as a second order tensor.\n        Solvers should rather access get_tensor().\n        \"\"\"\n        if hasattr(self, '_bc_flow'):\n            return self._bc_flow\n        else:\n            return BoundaryCondition(self.g)",
  "def get_bc_transport(self):\n        \"\"\" bc.BoundaryCondition\n        Cell wise conductivity, represented as a second order tensor.\n        Solvers should rather access tensor().\n        \"\"\"\n        if hasattr(self, '_bc_transport'):\n            return self._bc_transport\n        else:\n            return BoundaryCondition(self.g)",
  "def get_bc_mechanics(self):\n        \"\"\" Stiffness matrix, defined as fourth order tensor\n        \"\"\"\n        if hasattr(self, '_bc_mechanics'):\n            return self._bc_mechanics\n        else:\n            return BoundaryCondition(self.g)",
  "def get_bc_val(self, obj):\n        \"\"\" Pick out physics-specific boundary condition\n\n        Discretization methods should access this method.\n\n        Parameters:\n\n        obj : Solver\n            Discretization object. Should have attribute 'physics'.\n\n        Returns:\n\n        BoundaryCondition, for\n            flow/pressure equation y if physics equals 'flow'\n            transport equation if physics equals 'transport'\n            elasticity if physics equals 'mechanics'\n            If the BoundaryCondition is not specified, Neumann conditions will\n            be assigned.\n        \"\"\"\n        physics = self._get_physics(obj)\n\n        if physics == 'flow':\n            return self.get_bc_val_flow()\n        elif physics == 'transport':\n            return self.get_bc_val_transport()\n        elif physics == 'mechanics':\n            return self.get_bc_val_mechanics()\n        else:\n            raise ValueError('Unknown physics \"%s\".\\n Possible physics are: %s'\n                             % (physics, self.known_physics))",
  "def set_bc_val(self, obj, val):\n        \"\"\" Set physics-specific boundary condition\n\n        Parameters:\n\n        obj: Solver or str\n            Identification of physical regime. Either discretization object\n            with attribute 'physics' or a str.\n\n        val : BoundaryCondition, representing\n            flow/pressure equation y if physics equals 'flow'\n            transport equation if physics equals 'transport'\n            elasticity if physics equals 'mechanics'\n\n        \"\"\"\n        physics = self._get_physics(obj)\n\n        if physics == 'flow':\n            self._bc_val_flow = val\n        elif physics == 'transport':\n            self._bc_val_transport = val\n        elif physics == 'mechanics':\n            self._bc_val_mechanics = val\n        else:\n            raise ValueError('Unknown physics \"%s\".\\n Possible physics are: %s'\n                             % (physics, self.known_physics))",
  "def get_bc_val_flow(self):\n        \"\"\" tensor.SecondOrder\n        Cell wise permeability, represented as a second order tensor.\n        Solvers should rather access get_tensor().\n        \"\"\"\n        if hasattr(self, '_bc_val_flow'):\n            return self._bc_val_flow\n        else:\n            return np.zeros(self._num_faces)",
  "def get_bc_val_transport(self):\n        \"\"\" tensor.SecondOrder\n        Cell wise conductivity, represented as a second order tensor.\n        Solvers should rather access tensor().\n        \"\"\"\n        if hasattr(self, '_bc_val_transport'):\n            return self._bc_val_transport\n        else:\n            return np.zeros(self._num_faces)",
  "def get_bc_val_mechanics(self):\n        \"\"\" tensor.FourthOrder\n        Cell wise conductivity, represented as a fourth order tensor.\n        Solvers should rather access tensor().\n        \"\"\"\n        if hasattr(self, '_bc_val_mechanics'):\n            return self._bc_val_mechanics\n        else:\n            return np.zeros(self._num_faces * self.dim)",
  "class SecondOrder(object):\n    \"\"\" Cell-wise permeability represented by (3 ,3 ,Nc)-matrix.\n\n    The permeability is always 3-dimensional (since the geometry is always 3D),\n    however, 1D and 2D problems are accomodated by assigning unit values to kzz\n    and kyy, and no cross terms.\n    \"\"\"\n\n    def __init__(self, dim, kxx, kyy=None, kzz=None,\n                 kxy=None, kxz=None, kyz=None):\n        \"\"\" Initialize permeability\n\n        Parameters:\n            dim (int): Dimension, should be between 1 and 3.\n            kxx (double): Nc array, with cell-wise values of kxx permeability.\n            kyy (optional, double): Nc array of kyy. Default equal to kxx.\n            kzz (optional, double): Nc array of kzz. Default equal to kxx.\n                Not used if dim < 3.\n            kxy (optional, double): Nc array of kxy. Defaults to zero.\n            kxz (optional, double): Nc array of kxz. Defaults to zero.\n                Not used if dim < 3.\n            kyz (optional, double): Nc array of kyz. Defaults to zero.\n                Not used if dim < 3.\n\n        Raises:\n            ValueError if the permeability is not positive definite.\n       \"\"\"\n        if dim > 3 or dim < 0:\n            raise ValueError('Dimension should be between 1 and 3')\n\n        self.dim = dim\n\n        Nc = kxx.size\n        perm = np.zeros((3, 3, Nc))\n\n        if not np.all(kxx > 0):\n            raise ValueError('Tensor is not positive definite because of '\n                             'components in x-direction')\n\n        perm[0, 0, ::] = kxx\n\n        # Assign unit values to the diagonal, these are overwritten later if\n        # the specified dimension requires it.\n        perm[1, 1, ::] = 1\n        perm[2, 2, ::] = 1\n\n        if dim > 1:\n            if kyy is None:\n                kyy = kxx\n            if kxy is None:\n                kxy = 0 * kxx\n            # Onsager's principle\n            if not np.all((kxx * kyy - kxy * kxy) > 0):\n                raise ValueError('Tensor is not positive definite because of '\n                                 'components in y-direction')\n\n            perm[1, 0, ::] = kxy\n            perm[0, 1, ::] = kxy\n            perm[1, 1, ::] = kyy\n\n        if dim > 2:\n            if kyy is None:\n                kyy = kxx\n            if kzz is None:\n                kzz = kxx\n            if kxy is None:\n                kxy = 0 * kxx\n            if kxz is None:\n                kxz = 0 * kxx\n            if kyz is None:\n                kyz = 0 * kxx\n            # Onsager's principle\n            if not np.all((kxx * (kyy * kzz - kyz * kyz) -\n                           kxy * (kxy * kzz - kxz * kyz) +\n                           kxz * (kxy * kyz - kxz * kyy)) > 0):\n                raise ValueError('Tensor is not positive definite because of '\n                                 'components in z-direction')\n\n            perm[2, 0, ::] = kxz\n            perm[0, 2, ::] = kxz\n            perm[2, 1, ::] = kyz\n            perm[1, 2, ::] = kyz\n            perm[2, 2, ::] = kzz\n\n        self.perm = perm\n\n    def copy(self):\n        \"\"\"\n        Define a deep copy of the tensor.\n\n        Returns:\n            SecondOrderTensor: New tensor with identical fields, but separate\n                arrays (in the memory sense).\n        \"\"\"\n\n        if self.dim == 2:\n\n            kxx = self.perm[0, 0, :].copy()\n            kxy = self.perm[1, 0, :].copy()\n            kyy = self.perm[1, 1, :].copy()\n\n            return SecondOrder(self.dim, kxx, kxy=kxy, kyy=kyy)\n        else:\n            kxx = self.perm[0, 0, :].copy()\n            kxy = self.perm[1, 0, :].copy()\n            kyy = self.perm[1, 1, :].copy()\n\n            kxz = self.perm[2, 0, :].copy()\n            kyz = self.perm[2, 1, :].copy()\n            kzz = self.perm[2, 2, :].copy()\n\n            return SecondOrder(self.dim, kxx, kxy=kxy, kxz=kxz, kyy=kyy,\n                                     kyz=kyz, kzz=kzz)\n\n\n    def rotate(self, R):\n        \"\"\"\n        Rotate the permeability given a rotation matrix.\n\n        Parameter:\n            R: a rotation matrix 3x3\n        \"\"\"\n        self.perm = np.tensordot(R.T, np.tensordot(R, self.perm, (1, 0)), (0, 1))",
  "class FourthOrder(object):\n    \"\"\" Cell-wise representation of fourth order tensor.\n\n    For each cell, there are dim^4 degrees of freedom, stored in a\n    dim^2 * dim^2 matrix (exactly how to convert between 2D and 4D matrix\n    is not clear to me a the moment, but in pratcise there is sufficient\n    symmetry in the tensors for the question to be irrelevant).\n\n    Since all coordinates are three-dimensional, the tensor is stored as a 3^2\n    x 3^2 x num_cells arrays. If a 2d problem is solved, the third dimension\n    will automatically be disregarded by the numerical method.\n\n    The only constructor available for the moment is based on the lame parameters,\n    e.g. using two degrees of freedom. A third parameter phi is also present,\n    but this has never been used.\n\n    Primary usage for the class is for mpsa discretizations. Other applications\n    have not been tested.\n\n    Attributes:\n        c - numpy.ndarray, dimensions (dim^2,dim^2,nc), cell-wise\n            representation of the stiffness matrix.\n        dim (int): Real dimension of the tensor (as oposed to the 3d\n            representation of the data)\n        lmbda (np.ndarray, size: num_cells): First Lame parameter\n        mu (np.ndarray, size: num_cells): Second Lame parameter\n\n    \"\"\"\n\n    def __init__(self, dim, mu, lmbda, phi=None):\n        \"\"\" Constructor for fourth order tensor on Lame-parameter form\n\n        Parameters\n        ----------\n        dim (int) dimension, should be 2 or 3\n        mu (numpy.ndarray), First lame parameter, 1-D, one value per cell\n        lmbda (numpy.ndarray), Second lame parameter, 1-D, one value per cell\n        phi (Optional numpy.ndarray), 1-D one value per cell, never been used.\n\n        \"\"\"\n\n        # Check arguments\n        if dim > 3 or dim < 2:\n            raise ValueError('Dimension should be between 1 and 3')\n\n        if not isinstance(mu, np.ndarray):\n            raise ValueError(\"Input mu should be a numpy array\")\n        if not isinstance(lmbda, np.ndarray):\n            raise ValueError(\"Input lmbda should be a numpy array\")\n        if not mu.ndim == 1:\n            raise ValueError(\"mu should be 1-D\")\n        if not lmbda.ndim == 1:\n            raise ValueError(\"Lmbda should be 1-D\")\n        if mu.size != lmbda.size:\n            raise ValueError(\"Mu and lmbda should have the same length\")\n\n        if phi is None:\n            phi = 0 * mu  # Default value for phi is zero\n        elif not isinstance(phi, np.ndarray):\n            raise ValueError(\"Phi should be a numpy array\")\n        elif not phi.ndim == 1:\n            raise ValueError(\"Phi should be 1-D\")\n        elif phi.size != lmbda.size:\n            raise ValueError(\"Phi and Lmbda should have the same length\")\n\n        # Save lmbda and mu, can be useful to have in some cases\n        self.lmbda = lmbda\n        self.mu = mu\n        self.dim = dim\n\n        # Basis for the contributions of mu, lmbda and phi is hard-coded\n        if dim == 2:\n            mu_mat = np.array([[2, 0, 0, 0, 0, 0, 0, 0, 0],\n                               [0, 1, 0, 1, 0, 0, 0, 0, 0],\n                               [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                               [0, 1, 0, 1, 0, 0, 0, 0, 0],\n                               [0, 0, 0, 0, 2, 0, 0, 0, 0],\n                               [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                               [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                               [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                               [0, 0, 0, 0, 0, 0, 0, 0, 1]])\n\n            lmbda_mat = np.array([[1, 0, 0, 0, 1, 0, 0, 0, 0],\n                                  [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                  [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                  [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                  [1, 0, 0, 0, 1, 0, 0, 0, 0],\n                                  [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                  [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                  [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                  [0, 0, 0, 0, 0, 0, 0, 0, 1]])\n\n            phi_mat = np.array([[0, 1, 1, 1, 0, 1, 1, 1, 0],\n                                [1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                [1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                [1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                [0, 1, 1, 1, 0, 1, 1, 1, 0],\n                                [1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                [1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                [1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                [0, 1, 1, 1, 0, 1, 1, 1, 0]])\n\n\n        else:  # Dimension is either 2 or 3\n            mu_mat = np.array([[2, 0, 0, 0, 0, 0, 0, 0, 0],\n                               [0, 1, 0, 1, 0, 0, 0, 0, 0],\n                               [0, 0, 1, 0, 0, 0, 1, 0, 0],\n                               [0, 1, 0, 1, 0, 0, 0, 0, 0],\n                               [0, 0, 0, 0, 2, 0, 0, 0, 0],\n                               [0, 0, 0, 0, 0, 1, 0, 1, 0],\n                               [0, 0, 1, 0, 0, 0, 1, 0, 0],\n                               [0, 0, 0, 0, 0, 1, 0, 1, 0],\n                               [0, 0, 0, 0, 0, 0, 0, 0, 2]])\n            lmbda_mat = np.array([[1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                  [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                  [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                  [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                  [1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                  [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                  [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                  [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                  [1, 0, 0, 0, 1, 0, 0, 0, 1]])\n            phi_mat = np.array([[0, 1, 1, 1, 0, 1, 1, 1, 0],\n                                [1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                [1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                [1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                [0, 1, 1, 1, 0, 1, 1, 1, 0],\n                                [1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                [1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                [1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                [0, 1, 1, 1, 0, 1, 1, 1, 0]])\n\n        # Expand dimensions to prepare for cell-wise representation\n        mu_mat = mu_mat[:, :, np.newaxis]\n        lmbda_mat = lmbda_mat[:, :, np.newaxis]\n        phi_mat = phi_mat[:, :, np.newaxis]\n\n        c = mu_mat * mu + lmbda_mat * lmbda + phi_mat * phi\n        self.c = c\n\n    def copy(self):\n        return FourthOrder(self.dim, mu=self.mu, lmbda=self.lmbda)",
  "def __init__(self, dim, kxx, kyy=None, kzz=None,\n                 kxy=None, kxz=None, kyz=None):\n        \"\"\" Initialize permeability\n\n        Parameters:\n            dim (int): Dimension, should be between 1 and 3.\n            kxx (double): Nc array, with cell-wise values of kxx permeability.\n            kyy (optional, double): Nc array of kyy. Default equal to kxx.\n            kzz (optional, double): Nc array of kzz. Default equal to kxx.\n                Not used if dim < 3.\n            kxy (optional, double): Nc array of kxy. Defaults to zero.\n            kxz (optional, double): Nc array of kxz. Defaults to zero.\n                Not used if dim < 3.\n            kyz (optional, double): Nc array of kyz. Defaults to zero.\n                Not used if dim < 3.\n\n        Raises:\n            ValueError if the permeability is not positive definite.\n       \"\"\"\n        if dim > 3 or dim < 0:\n            raise ValueError('Dimension should be between 1 and 3')\n\n        self.dim = dim\n\n        Nc = kxx.size\n        perm = np.zeros((3, 3, Nc))\n\n        if not np.all(kxx > 0):\n            raise ValueError('Tensor is not positive definite because of '\n                             'components in x-direction')\n\n        perm[0, 0, ::] = kxx\n\n        # Assign unit values to the diagonal, these are overwritten later if\n        # the specified dimension requires it.\n        perm[1, 1, ::] = 1\n        perm[2, 2, ::] = 1\n\n        if dim > 1:\n            if kyy is None:\n                kyy = kxx\n            if kxy is None:\n                kxy = 0 * kxx\n            # Onsager's principle\n            if not np.all((kxx * kyy - kxy * kxy) > 0):\n                raise ValueError('Tensor is not positive definite because of '\n                                 'components in y-direction')\n\n            perm[1, 0, ::] = kxy\n            perm[0, 1, ::] = kxy\n            perm[1, 1, ::] = kyy\n\n        if dim > 2:\n            if kyy is None:\n                kyy = kxx\n            if kzz is None:\n                kzz = kxx\n            if kxy is None:\n                kxy = 0 * kxx\n            if kxz is None:\n                kxz = 0 * kxx\n            if kyz is None:\n                kyz = 0 * kxx\n            # Onsager's principle\n            if not np.all((kxx * (kyy * kzz - kyz * kyz) -\n                           kxy * (kxy * kzz - kxz * kyz) +\n                           kxz * (kxy * kyz - kxz * kyy)) > 0):\n                raise ValueError('Tensor is not positive definite because of '\n                                 'components in z-direction')\n\n            perm[2, 0, ::] = kxz\n            perm[0, 2, ::] = kxz\n            perm[2, 1, ::] = kyz\n            perm[1, 2, ::] = kyz\n            perm[2, 2, ::] = kzz\n\n        self.perm = perm",
  "def copy(self):\n        \"\"\"\n        Define a deep copy of the tensor.\n\n        Returns:\n            SecondOrderTensor: New tensor with identical fields, but separate\n                arrays (in the memory sense).\n        \"\"\"\n\n        if self.dim == 2:\n\n            kxx = self.perm[0, 0, :].copy()\n            kxy = self.perm[1, 0, :].copy()\n            kyy = self.perm[1, 1, :].copy()\n\n            return SecondOrder(self.dim, kxx, kxy=kxy, kyy=kyy)\n        else:\n            kxx = self.perm[0, 0, :].copy()\n            kxy = self.perm[1, 0, :].copy()\n            kyy = self.perm[1, 1, :].copy()\n\n            kxz = self.perm[2, 0, :].copy()\n            kyz = self.perm[2, 1, :].copy()\n            kzz = self.perm[2, 2, :].copy()\n\n            return SecondOrder(self.dim, kxx, kxy=kxy, kxz=kxz, kyy=kyy,\n                                     kyz=kyz, kzz=kzz)",
  "def rotate(self, R):\n        \"\"\"\n        Rotate the permeability given a rotation matrix.\n\n        Parameter:\n            R: a rotation matrix 3x3\n        \"\"\"\n        self.perm = np.tensordot(R.T, np.tensordot(R, self.perm, (1, 0)), (0, 1))",
  "def __init__(self, dim, mu, lmbda, phi=None):\n        \"\"\" Constructor for fourth order tensor on Lame-parameter form\n\n        Parameters\n        ----------\n        dim (int) dimension, should be 2 or 3\n        mu (numpy.ndarray), First lame parameter, 1-D, one value per cell\n        lmbda (numpy.ndarray), Second lame parameter, 1-D, one value per cell\n        phi (Optional numpy.ndarray), 1-D one value per cell, never been used.\n\n        \"\"\"\n\n        # Check arguments\n        if dim > 3 or dim < 2:\n            raise ValueError('Dimension should be between 1 and 3')\n\n        if not isinstance(mu, np.ndarray):\n            raise ValueError(\"Input mu should be a numpy array\")\n        if not isinstance(lmbda, np.ndarray):\n            raise ValueError(\"Input lmbda should be a numpy array\")\n        if not mu.ndim == 1:\n            raise ValueError(\"mu should be 1-D\")\n        if not lmbda.ndim == 1:\n            raise ValueError(\"Lmbda should be 1-D\")\n        if mu.size != lmbda.size:\n            raise ValueError(\"Mu and lmbda should have the same length\")\n\n        if phi is None:\n            phi = 0 * mu  # Default value for phi is zero\n        elif not isinstance(phi, np.ndarray):\n            raise ValueError(\"Phi should be a numpy array\")\n        elif not phi.ndim == 1:\n            raise ValueError(\"Phi should be 1-D\")\n        elif phi.size != lmbda.size:\n            raise ValueError(\"Phi and Lmbda should have the same length\")\n\n        # Save lmbda and mu, can be useful to have in some cases\n        self.lmbda = lmbda\n        self.mu = mu\n        self.dim = dim\n\n        # Basis for the contributions of mu, lmbda and phi is hard-coded\n        if dim == 2:\n            mu_mat = np.array([[2, 0, 0, 0, 0, 0, 0, 0, 0],\n                               [0, 1, 0, 1, 0, 0, 0, 0, 0],\n                               [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                               [0, 1, 0, 1, 0, 0, 0, 0, 0],\n                               [0, 0, 0, 0, 2, 0, 0, 0, 0],\n                               [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                               [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                               [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                               [0, 0, 0, 0, 0, 0, 0, 0, 1]])\n\n            lmbda_mat = np.array([[1, 0, 0, 0, 1, 0, 0, 0, 0],\n                                  [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                  [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                  [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                  [1, 0, 0, 0, 1, 0, 0, 0, 0],\n                                  [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                  [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                  [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                  [0, 0, 0, 0, 0, 0, 0, 0, 1]])\n\n            phi_mat = np.array([[0, 1, 1, 1, 0, 1, 1, 1, 0],\n                                [1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                [1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                [1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                [0, 1, 1, 1, 0, 1, 1, 1, 0],\n                                [1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                [1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                [1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                [0, 1, 1, 1, 0, 1, 1, 1, 0]])\n\n\n        else:  # Dimension is either 2 or 3\n            mu_mat = np.array([[2, 0, 0, 0, 0, 0, 0, 0, 0],\n                               [0, 1, 0, 1, 0, 0, 0, 0, 0],\n                               [0, 0, 1, 0, 0, 0, 1, 0, 0],\n                               [0, 1, 0, 1, 0, 0, 0, 0, 0],\n                               [0, 0, 0, 0, 2, 0, 0, 0, 0],\n                               [0, 0, 0, 0, 0, 1, 0, 1, 0],\n                               [0, 0, 1, 0, 0, 0, 1, 0, 0],\n                               [0, 0, 0, 0, 0, 1, 0, 1, 0],\n                               [0, 0, 0, 0, 0, 0, 0, 0, 2]])\n            lmbda_mat = np.array([[1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                  [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                  [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                  [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                  [1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                  [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                  [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                  [0, 0, 0, 0, 0, 0, 0, 0, 0],\n                                  [1, 0, 0, 0, 1, 0, 0, 0, 1]])\n            phi_mat = np.array([[0, 1, 1, 1, 0, 1, 1, 1, 0],\n                                [1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                [1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                [1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                [0, 1, 1, 1, 0, 1, 1, 1, 0],\n                                [1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                [1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                [1, 0, 0, 0, 1, 0, 0, 0, 1],\n                                [0, 1, 1, 1, 0, 1, 1, 1, 0]])\n\n        # Expand dimensions to prepare for cell-wise representation\n        mu_mat = mu_mat[:, :, np.newaxis]\n        lmbda_mat = lmbda_mat[:, :, np.newaxis]\n        phi_mat = phi_mat[:, :, np.newaxis]\n\n        c = mu_mat * mu + lmbda_mat * lmbda + phi_mat * phi\n        self.c = c",
  "def copy(self):\n        return FourthOrder(self.dim, mu=self.mu, lmbda=self.lmbda)",
  "def mcolon(lo, hi):\n    \"\"\" Expansion of np.arange(a, b) for arrays a and b.\n\n    The code is equivalent to the following (less efficient) loop:\n    arr = np.empty(0)\n    for l, h in zip(lo, hi):\n        arr = np.hstack((arr, np.arange(l, h, 1)))\n\n    Parameters:\n        lo (np.ndarray, int): Lower bounds of the arrays to be created.\n        hi (np.ndarray, int): Upper bounds of the arrays to be created. The\n            elements in hi will *not* be included in the resulting array.\n\n        lo and hi should either have 1 or n elements. If their size are both\n        larger than one, they should have the same length.\n\n    Examples:\n        >>> mcolon(np.array([0, 0, 0]), np.array([2, 4, 3]))\n        array([0, 1, 0, 1, 2, 3, 0, 1, 2])\n\n        >>> mcolon(np.array([0, 1]), np.array([2]))\n        array([0, 1, 1])\n\n        >>> mcolon(np.array([0, 1, 1, 1]), np.array([1, 3, 3, 3]))\n        array([0, 1, 2, 1, 2, 1, 2])\n\n    \"\"\"\n    if lo.size == 1:\n        lo = lo * np.ones(hi.size, dtype='int64')\n    if hi.size == 1:\n        hi = hi * np.ones(lo.size, dtype='int64')\n    if lo.size != hi.size:\n        raise ValueError('Low and high should have same number of elements, '\n                         'or a single item ')\n\n    i = hi >= lo + 1\n    if not any(i):\n        return np.array([], dtype=np.int32)\n\n    lo = lo[i]\n    hi = hi[i] - 1\n    d = hi - lo + 1\n    n = np.sum(d)\n\n    x = np.ones(n, dtype='int64')\n    x[0] = lo[0]\n    x[np.cumsum(d[0:-1]).astype('int64')] = lo[1:] - hi[0:-1]\n    return np.cumsum(x).astype('int64')",
  "def read():\n    \"\"\" Read configuration file, located somewhere in the PYTHONPATH.\n\n    Returns \n    -------\n    dictionary\n        See module level comments for details.\n\n    Raises\n    ------\n    ImportError \n        If the file porepy_config is not found in PYTHONPATH\n\n\n    \"\"\"\n    try:\n        import porepy_config\n    except ImportError:\n        s = 'Could not load configuration file for PorePy.\\n'\n        s += 'To see instructions on how to generate this file, confer help\\n'\n        s += 'for this module (for instance, type \\n'\n        s += '  porepy.utils.read_config? \\n'\n        s += 'in ipython)'\n        raise ImportError(s)\n\n    return porepy_config.config",
  "def half_space_int(n, x0, pts):\n    \"\"\"\n    Find the points that lie in the intersection of half spaces (3D)\n\n    Parameters\n    ----------\n    n : ndarray\n        This is the normal vectors of the half planes. The normal\n        vectors is assumed to point out of the half spaces.\n    x0 : ndarray\n        Point on the boundary of the half-spaces. Half space i is given\n        by all points satisfying (x - x0[:,i])*n[:,i]<=0\n    pts : ndarray\n        The points to be tested if they are in the intersection of all\n        half-spaces or not.\n\n    Returns\n    -------\n    out : ndarray\n        A logical array with length equal number of pts.\n\n        out[i] is True if pts[:,i] is in all half-spaces\n\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> n = np.array([[0, 1], [1, 0], [0, 0]])\n    >>> x0 = np.array([[0, -1], [0, 0], [0, 0]])\n    >>> pts = np.array([[-1 ,-1 ,4], [2, -2, -2], [0, 0, 0]])\n    >>> half_space_int(n,x0,pts)\n    array([False,  True, False], dtype=bool)\n    \"\"\"\n    assert n.shape[0] == 3, ' only 3D supported'\n    assert x0.shape[0] == 3, ' only 3D supported'\n    assert pts.shape[0] == 3, ' only 3D supported'\n    assert n.shape[1] == x0.shape[1], 'there must be the same number of normal vectors as points'\n\n    n_pts = pts.shape[1]\n    in_hull = np.zeros(n_pts)\n    x0 = np.repeat(x0[:, :, np.newaxis], n_pts, axis=2)\n    n = np.repeat(n[:, :, np.newaxis], n_pts, axis=2)\n    for i in range(x0.shape[1]):\n        in_hull += np.sum((pts - x0[:, i, :])*n[:, i, :], axis=0) <= 0\n\n    return in_hull == x0.shape[1]",
  "def half_space_pt(n, x0, pts, recompute=True):\n    \"\"\"\n    Find an interior point for the halfspaces.\n\n    Parameters\n    ----------\n    n : ndarray\n        This is the normal vectors of the half planes. The normal\n        vectors is assumed to coherently for all the half spaces\n        (inward or outward).\n    x0 : ndarray\n        Point on the boundary of the half-spaces. Half space i is given\n        by all points satisfying (x - x0[:,i])*n[:,i]<=0\n    pts : ndarray\n        Points which defines a bounds for the algorithm.\n    recompute: bool\n        If the algorithm fails try again with flipped normals.\n\n    Returns\n    -------\n    out: array\n        Interior point of the halfspaces.\n\n    We use linear programming to find one interior point for the half spaces.\n    Assume, n halfspaces defined by: aj*x1+bj*x2+cj*x3+dj<=0, j=1..n.\n    Perform the following linear program:\n    max(x5) aj*x1+bj*x2+cj*x3+dj*x4+x5<=0, j=1..n\n\n    Then, if [x1,x2,x3,x4,x5] is an optimal solution with x4>0 and x5>0 we get:\n    aj*(x1/x4)+bj*(x2/x4)+cj*(x3/x4)+dj<=(-x5/x4) j=1..n and (-x5/x4)<0,\n    and conclude that the point [x1/x4,x2/x4,x3/x4] is in the interior of all\n    the halfspaces. Since x5 is optimal, this point is \"way in\" the interior\n    (good for precision errors).\n    http://www.qhull.org/html/qhalf.htm#notes\n\n    \"\"\"\n    dim = (1, n.shape[1])\n    c = np.array([0, 0, 0, 0, -1])\n    A_ub = np.concatenate((n, [np.sum(-n*x0, axis=0)], np.ones(dim))).T\n    b_ub = np.zeros(dim).T\n    b_min, b_max = np.amin(pts, axis=1), np.amax(pts, axis=1)\n    bounds = ((b_min[0], b_max[0]), (b_min[1], b_max[1]),\n              (b_min[2], b_max[2]), (0, None), (0, None))\n    res = opt.linprog(c, A_ub, b_ub, bounds=bounds)\n\n    if recompute and (not res.success or np.all(np.isclose(res.x[3:], 0))):\n        return half_space_pt(-n, x0, pts, False)\n\n    if res.success and not np.all(np.isclose(res.x[3:], 0)):\n        return np.array(res.x[:3])/res.x[3]\n    else:\n        return np.array([np.nan, np.nan, np.nan])",
  "def star_shape_cell_centers(g):\n\n    if g.dim < 2:\n        return g.cell_centers\n\n    faces, _, sgn = sps.find(g.cell_faces)\n    nodes, _, _ = sps.find(g.face_nodes)\n\n    xn = g.nodes\n    if g.dim == 2:\n        R = cg.project_plane_matrix(xn)\n        xn = np.dot(R, xn)\n\n    cell_centers = np.zeros((3, g.num_cells))\n    for c in np.arange(g.num_cells):\n        loc = slice(g.cell_faces.indptr[c], g.cell_faces.indptr[c + 1])\n        faces_loc = faces[loc]\n        loc_n = g.face_nodes.indptr[faces_loc]\n        normal = np.multiply(sgn[loc], np.divide(g.face_normals[:, faces_loc],\n                                                 g.face_areas[faces_loc]))\n\n        x0, x1 = xn[:, nodes[loc_n]], xn[:, nodes[loc_n + 1]]\n        coords = np.concatenate((x0, x1), axis=1)\n        cell_centers[:, c] = half_space_pt(normal, (x1 + x0)/2., coords)\n\n    if g.dim == 2:\n        cell_centers = np.dot(R.T, cell_centers)\n\n    return cell_centers",
  "class Graph:\n    \"\"\"\n    A graph class.\n\n    The graph class stores the nodes and edges of the graph in a sparse\n    array (equivalently to face_nodes in the Grid class).\n\n    Attributes:\n        node_connections (sps.csc-matrix): Should be given at construction.\n            node_node connections. Matrix size: num_nodes x num_nodes.\n            node_connections[i,j] should be true if there is an edge\n            connecting node i and j.\n        regions (int) the number of regions. A region is a set of nodes\n            that can be reached by traversing the graph. Two nodes are\n            int different regions if they can not be reached by traversing\n            the graph.\n        color (int) the color of each region. Initialized as (NaN). By\n            calling color_nodes() all nodes in a region are given the\n            same colors and nodes in different regions are given different\n            colors.\n    \"\"\"\n\n    def __init__(self, node_connections):\n        if node_connections.getformat() != 'csr':\n            self.node_connections = node_connections.tocsr()\n        else:\n            self.node_connections = node_connections\n        self.regions = 0\n        self.color = np.array([np.NaN] * node_connections.shape[1])\n\n    def color_nodes(self):\n        \"\"\"\n        Color the nodes in each region\n        \"\"\"\n        color = 0\n        while self.regions <= self.node_connections.shape[1]:\n            start = np.ravel(np.argwhere(np.isnan(self.color)))\n            if start.size != 0:\n                self.bfs(start[0], color)\n                color += 1\n                self.regions += 1\n            else:\n                return\n        raise RuntimeWarning('number of regions can not be greater than '\n                             'number of nodes')\n\n    def bfs(self, start, color):\n        \"\"\"\n        Breadth first search\n        \"\"\"\n        visited, queue = [], [start]\n        while queue:\n            node = queue.pop(0)\n            if node not in visited:\n                visited.append(node)\n                neighbours = sparse_mat.slice_indices(\n                    self.node_connections, node)\n                queue.extend(neighbours)\n        self.color[visited] = color",
  "def __init__(self, node_connections):\n        if node_connections.getformat() != 'csr':\n            self.node_connections = node_connections.tocsr()\n        else:\n            self.node_connections = node_connections\n        self.regions = 0\n        self.color = np.array([np.NaN] * node_connections.shape[1])",
  "def color_nodes(self):\n        \"\"\"\n        Color the nodes in each region\n        \"\"\"\n        color = 0\n        while self.regions <= self.node_connections.shape[1]:\n            start = np.ravel(np.argwhere(np.isnan(self.color)))\n            if start.size != 0:\n                self.bfs(start[0], color)\n                color += 1\n                self.regions += 1\n            else:\n                return\n        raise RuntimeWarning('number of regions can not be greater than '\n                             'number of nodes')",
  "def bfs(self, start, color):\n        \"\"\"\n        Breadth first search\n        \"\"\"\n        visited, queue = [], [start]\n        while queue:\n            node = queue.pop(0)\n            if node not in visited:\n                visited.append(node)\n                neighbours = sparse_mat.slice_indices(\n                    self.node_connections, node)\n                queue.extend(neighbours)\n        self.color[visited] = color",
  "def accum(accmap, a, func=None, size=None, fill_value=0, dtype=None):\n    \"\"\"\n    An accumulation function similar to Matlab's `accumarray` function.\n\n    Parameters\n    ----------\n    accmap : ndarray\n        This is the \"accumulation map\".  It maps input (i.e. indices into\n        `a`) to their destination in the output array.  The first `a.ndim`\n        dimensions of `accmap` must be the same as `a.shape`.  That is,\n        `accmap.shape[:a.ndim]` must equal `a.shape`.  For example, if `a`\n        has shape (15,4), then `accmap.shape[:2]` must equal (15,4).  In this\n        case `accmap[i,j]` gives the index into the output array where\n        element (i,j) of `a` is to be accumulated.  If the output is, say,\n        a 2D, then `accmap` must have shape (15,4,2).  The value in the\n        last dimension give indices into the output array. If the output is\n        1D, then the shape of `accmap` can be either (15,4) or (15,4,1)\n    a : ndarray\n        The input data to be accumulated.\n    func : callable or None\n        The accumulation function.  The function will be passed a list\n        of values from `a` to be accumulated.\n        If None, numpy.sum is assumed.\n    size : ndarray or None\n        The size of the output array.  If None, the size will be determined\n        from `accmap`.\n    fill_value : scalar\n        The default value for elements of the output array.\n    dtype : numpy data type, or None\n        The data type of the output array.  If None, the data type of\n        `a` is used.\n\n    Returns\n    -------\n    out : ndarray\n        The accumulated results.\n\n        The shape of `out` is `size` if `size` is given.  Otherwise the\n        shape is determined by the (lexicographically) largest indices of\n        the output found in `accmap`.\n\n\n    Examples\n    --------\n    >>> from numpy import array, prod\n    >>> a = array([[1,2,3],[4,-1,6],[-1,8,9]])\n    >>> a\n    array([[ 1,  2,  3],\n           [ 4, -1,  6],\n           [-1,  8,  9]])\n    >>> # Sum the diagonals.\n    >>> accmap = array([[0,1,2],[2,0,1],[1,2,0]])\n    >>> s = accum(accmap, a)\n    array([9, 7, 15])\n    >>> # A 2D output, from sub-arrays with shapes and positions like this:\n    >>> # [ (2,2) (2,1)]\n    >>> # [ (1,2) (1,1)]\n    >>> accmap = array([\n            [[0,0],[0,0],[0,1]],\n            [[0,0],[0,0],[0,1]],\n            [[1,0],[1,0],[1,1]],\n        ])\n    >>> # Accumulate using a product.\n    >>> accum(accmap, a, func=prod, dtype=float)\n    array([[ -8.,  18.],\n           [ -8.,   9.]])\n    >>> # Same accmap, but create an array of lists of values.\n    >>> accum(accmap, a, func=lambda x: x, dtype='O')\n    array([[[1, 2, 4, -1], [3, 6]],\n           [[-1, 8], [9]]], dtype=object)\n    \"\"\"\n\n    # Check for bad arguments and handle the defaults.\n    if accmap.shape[:a.ndim] != a.shape:\n        raise ValueError(\"The initial dimensions of accmap must be the same as a.shape\")\n    if func is None:\n        func = np.sum\n    if dtype is None:\n        dtype = a.dtype\n    if accmap.shape == a.shape:\n        accmap = np.expand_dims(accmap, -1)\n    adims = tuple(range(a.ndim))\n    if size is None:\n        size = 1 + np.squeeze(np.apply_over_axes(np.max, accmap, axes=adims))\n    size = np.atleast_1d(size)\n\n    # Create an array of python lists of values.\n    vals = np.empty(size, dtype='O')\n    for s in product(*[range(k) for k in size]):\n        vals[s] = []\n    for s in product(*[range(k) for k in a.shape]):\n        indx = tuple(accmap[s])\n        val = a[s]\n        vals[indx].append(val)\n\n    # Create the output array.\n    out = np.empty(size, dtype=dtype)\n    for s in product(*[range(k) for k in size]):\n        if vals[s] == []:\n            out[s] = fill_value\n        else:\n            out[s] = func(vals[s])\n\n    return out",
  "def rldecode(A,n):\n    \"\"\" Decode compressed information. \n        \n        The code is heavily inspired by MRST's function with the same name, \n        however, requirements on the shape of functions are probably somewhat\n        different.\n        \n        >>> rldecode(np.array([1, 2, 3]), np.array([2, 3, 1]))\n        [1, 1, 2, 2, 2, 3]\n        \n        >>> rldecode(np.array([1, 2]), np.array([1, 3]))\n        [1, 2, 2, 2]\n        \n        Args:\n            A (double, m x k), compressed matrix to be recovered. The \n            compression should be along dimension 1\n            n (int): Number of occurences for each element\n    \"\"\"\n    r = n > 0         \n    i = np.cumsum(np.hstack((np.zeros(1), n[r])), dtype='>i4')\n    j = np.zeros(i[-1])\n    j[i[1:-1:]] = 1\n    B = A[np.cumsum(j, dtype='>i4')]\n    return B",
  "def rlencode(A):\n    \"\"\" Compress matrix by looking for identical columns. \"\"\"\n    comp = A[::, 0:-1] != A[::, 1::]\n    i = np.any(comp, axis=0)\n    i = np.hstack((np.argwhere(i).ravel(), (A.shape[1]-1)))\n    \n    num = np.diff(np.hstack((np.array([-1]), i)))\n    \n    return A[::, i], num",
  "def zero_columns(A, cols):\n    '''\n    Function to zero out columns in matrix A. Note that this function does not\n    change the sparcity structure of the matrix, it only changes the column\n    values to 0\n\n    Parameter\n    ---------\n    A (scipy.sparse.spmatrix): A sparce matrix\n    cols (ndarray): A numpy array of columns that should be zeroed\n    Return\n    ------\n    None\n\n\n    '''\n\n    if A.getformat() != 'csc':\n        raise ValueError('Need a csc matrix')\n\n    indptr = A.indptr\n    col_indptr = mcolon(indptr[cols], indptr[cols + 1])\n    A.data[col_indptr] = 0",
  "def merge_matrices(A, B, lines):\n    '''\n    Replace rows/coloms of matrix A with rows/cols of matrix B.\n    If A and B are csc matrices this function is equivalent with\n    A[:, lines] = B\n    If A and B are csr matrices this funciton is equivalent iwth\n    A[lines, :] = B\n\n    Parameter\n    ---------\n    A (scipy.sparse.spmatrix): A sparce matrix\n    B (scipy.sparse.spmatrix): A sparce matrix\n    lines (ndarray): Lines of A to be replaced by B. \n\n    Return\n    ------\n    None\n\n\n    '''\n    if A.getformat() != 'csc' and A.getformat() != 'csr':\n        raise ValueError('Need a csc or csr matrix')\n    elif A.getformat() != B.getformat():\n        raise ValueError('A and B must be of same matrix type')\n    if A.getformat() == 'csc':\n        if A.shape[0] != B.shape[0]:\n            raise ValueError('A.shape[0] must equal B.shape[0]')\n    if A.getformat() == 'csr':\n        if A.shape[1] != B.shape[1]:\n            raise ValueError('A.shape[0] must equal B.shape[0]')\n\n    if B.getformat() == 'csc':\n        if lines.size != B.shape[1]:\n            raise ValueError('B.shape[1] must equal size of lines')\n    if B.getformat() == 'csr':\n        if lines.size != B.shape[0]:\n            raise ValueError('B.shape[0] must equal size of lines')\n\n    if np.unique(lines).shape != lines.shape:\n        raise ValueError('Can only merge unique lines')\n\n    indptr = A.indptr\n    indices = A.indices\n    data = A.data\n\n    ind_ix = mcolon(indptr[lines], indptr[lines + 1])\n\n    # First we remove the old data\n    num_rem = np.zeros(indptr.size, dtype=np.int32)\n    num_rem[lines + 1] = indptr[lines + 1] - indptr[lines]\n    num_rem = np.cumsum(num_rem, dtype=num_rem.dtype)\n\n    indptr = indptr - num_rem\n\n    keep = np.ones(A.data.size, dtype=np.bool)\n    keep[ind_ix] = False\n    indices = indices[keep]\n    data = data[keep]\n\n    # Then we add the new\n    b_indptr = B.indptr\n    b_indices = B.indices\n    b_data = B.data\n\n    num_added = np.zeros(indptr.size,  dtype=np.int32)\n    num_added[lines + 1] = b_indptr[1:] - b_indptr[:-1]\n    num_added = np.cumsum(num_added, dtype=num_added.dtype)\n\n    rep = np.diff(b_indptr)\n    indPos = np.repeat(indptr[lines], rep)\n\n    A.indices = np.insert(indices,  indPos, b_indices)\n    A.data = np.insert(data,  indPos, b_data)\n    A.indptr = indptr + num_added",
  "def stack_mat(A, B):\n    '''\n    Stack matrix B at the end of matrix A.\n    If A and B are csc matrices this function is equivalent to \n    A = scipy.sparse.hstack((A, B))\n    If A and B are csr matrices this function is equivalent to \n    A = scipy.sparse.vstack((A, B))\n\n    Parameters:\n    -----------\n    A (scipy.sparse.spmatrix): A sparce matrix\n    B (scipy.sparse.spmatrix): A sparce matrix\n\n    Return\n    ------\n    None\n\n\n    '''\n    if A.getformat() != 'csc' and A.getformat() != 'csr':\n        raise ValueError('Need a csc or csr matrix')\n    elif A.getformat() != B.getformat():\n        raise ValueError('A and B must be of same matrix type')\n    if A.getformat() == 'csc':\n        if A.shape[0] != B.shape[0]:\n            raise ValueError('A.shape[0] must equal B.shape[0]')\n    if A.getformat() == 'csr':\n        if A.shape[1] != B.shape[1]:\n            raise ValueError('A.shape[0] must equal B.shape[0]')\n\n    if B.indptr.size == 1:\n        return\n\n    A.indptr = np.append(A.indptr, B.indptr[1:] + A.indptr[-1])\n    A.indices = np.append(A.indices, B.indices)\n    A.data = np.append(A.data, B.data)\n\n    if A.getformat() == 'csc':\n        A._shape = (A._shape[0], A._shape[1] + B._shape[1])\n    if A.getformat() == 'csr':\n        A._shape = (A._shape[0] + B._shape[0], A._shape[1])",
  "def slice_indices(A, slice_ind):\n    \"\"\"\n    Function for slicing sparse matrix along rows or columns.\n    If A is a csc_matrix A will be sliced along columns, while if A is a\n    csr_matrix A will be sliced along the rows.\n\n    Parameters\n    ----------\n    A (scipy.sparse.csc/csr_matrix): A sparse matrix.\n    slice_ind (np.array): Array containing indices to be sliced\n\n    Returns\n    -------\n    indices (np.array): If A is csc_matrix:\n                            The nonzero row indices or columns slice_ind\n                        If A is csr_matrix:\n                            The nonzero columns indices or rows slice_ind\n    Examples\n    --------\n    A = sps.csc_matrix(np.eye(10))\n    rows = slice_indices(A, np.array([0,2,3]))\n    \"\"\"\n    assert A.getformat() == 'csc' or A.getformat() == 'csr'\n    if isinstance(slice_ind, int):\n        indices = A.indices[slice(\n            A.indptr[int(slice_ind)], A.indptr[int(slice_ind + 1)])]\n    elif slice_ind.size == 1:\n        indices = A.indices[slice(\n            A.indptr[int(slice_ind)], A.indptr[int(slice_ind + 1)])]\n    else:\n        indices = A.indices[mcolon(\n            A.indptr[slice_ind], A.indptr[slice_ind + 1])]\n    return indices",
  "def slice_mat(A, ind):\n    \"\"\"\n    Function for slicing sparse matrix along rows or columns.\n    If A is a csc_matrix A will be sliced along columns, while if A is a\n    csr_matrix A will be sliced along the rows.\n\n    Parameters\n    ----------\n    A (scipy.sparse.csc/csr_matrix): A sparse matrix.\n    ind (np.array): Array containing indices to be sliced.\n\n    Returns\n    -------\n    A_sliced (scipy.sparse.csc/csr_matrix): The sliced matrix\n        if A is a csc_matrix A_sliced = A[:, ind]\n        if A is a csr_matrix A_slice = A[ind, :]\n\n    Examples\n    --------\n    A = sps.csc_matrix(np.eye(10))\n    rows = slice_mat(A, np.array([0,2,3]))\n    \"\"\"\n    assert A.getformat() == 'csc' or A.getformat() == 'csr'\n\n    if isinstance(ind, int):\n        N = 1\n        indptr = np.zeros(2)\n        ind_slice = slice(\n            A.indptr[int(ind)], A.indptr[int(ind + 1)])\n    elif ind.size == 1:\n        N = 1\n        indptr = np.zeros(2)\n        ind_slice = slice(\n            A.indptr[int(ind)], A.indptr[int(ind + 1)])\n    else:\n        N = ind.size\n        indptr = np.zeros(ind.size + 1)\n        ind_slice = mcolon(\n            A.indptr[ind], A.indptr[ind + 1])\n\n    indices = A.indices[ind_slice]\n    indptr[1:] = np.cumsum(A.indptr[ind + 1] - A.indptr[ind])\n    data = A.data[ind_slice]\n\n    if A.getformat() == 'csc':\n        return sps.csc_matrix((data, indices, indptr), shape=(A.shape[0], N))\n    elif A.getformat() == 'csr':\n        return sps.csr_matrix((data, indices, indptr), shape=(N, A.shape[1]))",
  "def unique_rows(data):\n    \"\"\"\n    Function similar to Matlab's unique(...,'rows')\n\n    See also function unique_columns in this module; this is likely slower, but\n    is understandable, documented, and has a tolerance option.\n\n    Copied from\n    http://stackoverflow.com/questions/16970982/find-unique-rows-in-numpy-array/\n    (summary pretty far down on the page)\n    Note: I have no idea what happens here\n\n    \"\"\"\n    b = np.ascontiguousarray(data).view(np.dtype((np.void,\n                                                  data.dtype.itemsize * data.shape[1])))\n    _, ia = np.unique(b, return_index=True)\n    _, ic = np.unique(b, return_inverse=True)\n#    return np.unique(b).view(data.dtype).reshape(-1, data.shape[1]), ia, ic\n    return data[ia], ia, ic",
  "def _asvoid(arr):\n    \"\"\"\n\n    Taken from\n    http://stackoverflow.com/questions/22699756/python-version-of-ismember-with-rows-and-index\n\n    View the array as dtype np.void (bytes)\n    This views the last axis of ND-arrays as bytes so you can perform\n    comparisons on the entire row.\n    http://stackoverflow.com/a/16840350/190597 (Jaime, 2013-05)\n    Warning: When using asvoid for comparison, note that float zeros may\n    compare UNEQUALLY\n    >>> asvoid([-0.]) == asvoid([0.])\n    array([False], dtype=bool)\n    \"\"\"\n    arr = np.ascontiguousarray(arr)\n    return arr.view(np.dtype((np.void, arr.dtype.itemsize * arr.shape[-1])))",
  "def _find_occ(a, b):\n    \"\"\"\n    Find index of occurences of a in b.\n\n    The function has only been tested on np.arrays, but it should be fairly\n    general (only require iterables?)\n\n    Code snippet found at\n    http://stackoverflow.com/questions/15864082/python-equivalent-of-matlabs-ismember-function?rq=1\n\n    \"\"\"\n    # Base search on a dictionary\n\n    bind = {}\n    # Invert dictionary to create a map from an item in b to the *first*\n    # occurence of the item.\n    # NOTE: If we ever need to give the option of returning last index, it\n    # should require no more than bypassing the if statement.\n    for i, elt in enumerate(b):\n        if elt not in bind:\n            bind[elt] = i\n    # Use inverse mapping to obtain\n    return [bind.get(itm, None) for itm in a]",
  "def ismember_rows(a, b, sort=True, simple_version=False):\n    \"\"\"\n    Find *columns* of a that are also members of *columns* of b.\n\n    The function mimics Matlab's function ismember(..., 'rows').\n\n    TODO: Rename function, this is confusing!\n\n    Parameters:\n        a (np.array): Each column in a will search for an equal in b.\n        b (np.array): Array in which we will look for a twin\n        sort (boolean, optional): If true, the arrays will be sorted before\n            seraching, increasing the chances for a match. Defaults to True.\n        simple_verion (boolean, optional): Use an alternative implementation\n            based on a global for loop. The code is slow for large arrays, but\n            easy to understand. Defaults to False.\n\n    Returns:\n        np.array (boolean): For each column in a, true if there is a\n            corresponding column in b.\n        np.array (int): Indexes so that b[:, ind] is also found in a.\n\n    Examples:\n        >>> a = np.array([[1, 3, 3, 1, 7], [3, 3, 2, 3, 0]])\n        >>> b = np.array([[3, 1, 3, 5, 3], [3, 3, 2, 1, 2]])\n        >>> ismember_rows(a, b)\n        (array([ True,  True,  True,  True, False], dtype=bool), [1, 0, 2, 1])\n\n        >>> a = np.array([[1, 3, 3, 1, 7], [3, 3, 2, 3, 0]])\n        >>> b = np.array([[3, 1, 2, 5, 1], [3, 3, 3, 1, 2]])\n        >>> ismember_rows(a, b, sort=False)\n        (array([ True,  True, False,  True, False], dtype=bool), [1, 0, 1])\n\n    \"\"\"\n\n    # Sort if required, but not if the input is 1d\n    if sort and a.ndim > 1:\n        sa = np.sort(a, axis=0)\n        sb = np.sort(b, axis=0)\n    else:\n        sa = a\n        sb = b\n\n    b = np.atleast_1d(b)\n    a = np.atleast_1d(a)\n    num_a = a.shape[-1]\n\n    if simple_version:\n        # Use straightforward search, based on a for loop. This is slow for\n        # large arrays, but as the alternative implementation is opaque, and\n        # there has been some doubts on its reliability, this version is kept\n        # as a safeguard.\n        ismem_a = np.zeros(num_a, dtype=np.bool)\n        ind_of_a_in_b = np.empty(0)\n        for i in range(num_a):\n            if sa.ndim == 1:\n                diff = np.abs(sb - sa[i])\n            else:\n                diff = np.sum(np.abs(sb - sa[:, i].reshape((-1, 1))), axis=0)\n            if np.any(diff == 0):\n                ismem_a[i] = True\n                hit = np.where(diff == 0)[0]\n                if hit.size > 1:\n                    hit = hit[0]\n                ind_of_a_in_b = np.append(ind_of_a_in_b, hit)\n\n        return ismem_a, ind_of_a_in_b.astype('int')\n\n    else:\n        if a.ndim == 1:\n            # Special treatment of 1d, vstack of voids (below) ran into trouble\n            # here.\n            _, k, count = np.unique(np.hstack((a, b)), return_inverse=True,\n                                    return_counts=True)\n            _, k_a, count_a = np.unique(a, return_inverse=True,\n                                        return_counts=True)\n        else:\n            # Represent the arrays as voids to facilitate quick comparison\n            # Take void type of int64s, or else spurious error messages may\n            # arise. We do this after the transpose (which copies sa and sb) to\n            # avoid disturbing the original fields.\n            voida = _asvoid(sa.transpose().astype('int64'))\n            voidb = _asvoid(sb.transpose().astype('int64'))\n\n            # Use unique to count the number of occurences in a\n            _, _, k, count = np.unique(np.vstack((voida, voidb)),\n                                       return_index=True,\n                                       return_inverse=True,\n                                       return_counts=True)\n            # Also count the number of occurences in voida\n            _, _, k_a, count_a = np.unique(voida, return_index=True,\n                                           return_inverse=True,\n                                           return_counts=True)\n\n        # Index of a and b elements in the combined array\n        ind_a = np.arange(num_a)\n        ind_b = num_a + np.arange(b.shape[-1])\n\n        # Count number of occurences in combine array, and in a only\n        num_occ_a_and_b = count[k[ind_a]]\n        num_occ_a = count_a[k_a[ind_a]]\n\n        # Subtraction gives number of a in b\n        num_occ_a_in_b = num_occ_a_and_b - num_occ_a\n        ismem_a = (num_occ_a_in_b > 0)\n\n        # To get the indices of common elements in a and b, compare the\n        # elements in k (pointers to elements in the unique combined arary)\n        occ_a = k[ind_a[ismem_a]]\n        occ_b = k[ind_b]\n\n        ind_of_a_in_b = _find_occ(occ_a, occ_b)\n        # Remove None types when no hit was found\n        ind_of_a_in_b = [i for i in ind_of_a_in_b if i is not None]\n\n        return ismem_a, np.array(ind_of_a_in_b, dtype='int')",
  "def unique_columns_tol(mat, tol=1e-8, exponent=2):\n    \"\"\"\n    Remove duplicates from a point set, for a given distance traveling.\n\n    Resembles Matlab's uniquetol function, as applied to columns. To rather\n    work at rows, use a transpose.\n\n\n    Parameters:\n        mat (np.ndarray, nd x n_pts): Columns to be uniquified\n        tol (double, optional): Tolerance for when columns are considered equal.\n            Should be seen in connection with distance between the points in\n            the points (due to rounding errors). Defaults to 1e-8.\n        exponent (double, optional): Exponnet in norm used in distance\n            calculation. Defaults to 2.\n\n    Returns:\n        np.ndarray: Unique columns.\n        new_2_old: Index of which points that are preserved\n        old_2_new: Index of the representation of old points in the reduced\n            list.\n\n    Example (won't work as doctest):\n        >>> p_un, n2o, o2n = unique_columns(np.array([[1, 0, 1], [1, 0, 1]]))\n        >>> p_un\n        array([[1, 0], [1, 0]])\n        >>> n2o\n        array([0, 1])\n        >>> o2n\n        array([0, 1, 0])\n\n    \"\"\"\n    # Treat 1d array as 2d\n    mat = np.atleast_2d(mat)\n\n    # Special treatment of the case with an empty array\n    if mat.shape[1] == 0:\n        return mat, np.array([], dtype=int), np.array([], dtype=int)\n\n    # If the matrix is integers, and the tolerance less than 1/2, we can use\n    # the new unique function that ships with numpy 1.13. This comes with a\n    # significant speedup, in particular for large arrays (runtime has gone\n    # from hours to split-seconds - that is, the alternative implementation\n    # below is ineffecient).\n    # If the current numpy version is older, an ugly hack is possible: Download\n    # the file from the numpy repositories, and place it somewhere in\n    # $PYHTONPATH, with the name 'numpy_113_unique'.\n    if issubclass(mat.dtype.type, np.integer) and tol < 0.5:\n        # Obtain version of numpy that was loaded by the import in this module\n        np_version = np.version.version.split('.')\n        # If we are on numpy 2, or 1.13 or higher, we're good.\n        if int(np_version[0]) > 1 or int(np_version[1]) > 12:\n            un_ar, new_2_old, old_2_new \\\n                = np.unique(mat, return_index=True, return_inverse=True,\n                            axis=1)\n            return un_ar, new_2_old, old_2_new\n        else:\n            try:\n                import numpy_113_unique\n                un_ar, new_2_old, old_2_new \\\n                    = numpy_113_unique.unique_np1130(mat, return_index=True,\n                                                     return_inverse=True,\n                                                     axis=1)\n                return un_ar, new_2_old, old_2_new\n            except:\n                pass\n\n    def dist(p, pset):\n        \" Helper function to compute distance \"\n        if p.ndim == 1:\n            pt = p.reshape((-1, 1))\n        else:\n            pt = p\n\n        return np.power(np.sum(np.power(np.abs(pt - pset), exponent),\n                               axis=0), 1 / exponent)\n\n    (nd, l) = mat.shape\n\n    # By default, no columns are kept\n    keep = np.zeros(l, dtype=np.bool)\n\n    # We will however keep the first point\n    keep[0] = True\n    keep_counter = 1\n\n    # Map from old points to the unique subspace. Defaults to map to itself.\n    old_2_new = np.arange(l)\n\n    # Loop over all points, check if it is already represented in the kept list\n    for i in range(1, l):\n        proximate = np.argwhere(\n            dist(mat[:, i], mat[:, keep]) < tol * np.sqrt(nd))\n\n        if proximate.size > 0:\n            # We will not keep this point\n            old_2_new[i] = proximate[0]\n        else:\n            # We have found a new point\n            keep[i] = True\n            old_2_new[i] = keep_counter\n            keep_counter += 1\n    # Finally find which elements we kept\n    new_2_old = np.argwhere(keep).ravel()\n\n    return mat[:, keep], new_2_old, old_2_new",
  "def dist(p, pset):\n        \" Helper function to compute distance \"\n        if p.ndim == 1:\n            pt = p.reshape((-1, 1))\n        else:\n            pt = p\n\n        return np.power(np.sum(np.power(np.abs(pt - pset), exponent),\n                               axis=0), 1 / exponent)",
  "def append_tags(tags, keys, appendices):\n    \"\"\"\n    Append tags of certain keys.\n    tags:       dictionary with existing entries corresponding to\n    keys:       list of keys\n    appendices: list of values to be appended, typicall numpy arrays\n    \"\"\"\n    for i, key in enumerate(keys):\n        tags[key] = np.append(tags[key], appendices[i])",
  "def standard_face_tags():\n    \"\"\"\n    Returns the three standard face tag keys.\n    \"\"\"\n    return ['fracture_faces', 'tip_faces', 'domain_boundary_faces']",
  "def all_face_tags(parent):\n    \"\"\"\n    Return a logical array indicate which of the parent (grid.tags) faces are\n    tagged with any of the standard face tags.\n    \"\"\"\n    ft = standard_face_tags()\n    all_tags = np.logical_or(np.logical_or(parent[ft[0]],\n                                           parent[ft[1]]),\n                            parent[ft[2]])\n    return all_tags",
  "def extract(all_tags, indices, keys=None):\n    \"\"\"\n    Extracts only the values of indices (e.g. a face subset) for the given\n    keys. Any unspecified keys are left untouched (e.g. all node tags). If\n    keys=None, the extraction is performed on all fields.\n    \"\"\"\n    if keys is None:\n        keys = all_tags.keys()\n    new_tags = all_tags.copy()\n    for k in keys:\n        new_tags[k] = all_tags[k][indices]\n    return new_tags",
  "def add_tags(parent, new_tags):\n    \"\"\"\n    Add new tags (as a premade dictionary) to the tags of the parent object\n    (usually a grid). Values corresponding to keys existing in both\n    dictionaries (parent.tags and new_tags) will be decided by those in\n    new_tags.\n    \"\"\"\n    old_tags = getattr(parent, 'tags', {}).copy()\n    nt = dict(old_tags)\n    nt.update(new_tags)\n    parent.tags = nt",
  "def snap_to_grid(pts, tol=1e-3, box=None, **kwargs):\n    \"\"\"\n    Snap points to an underlying Cartesian grid.\n    Used e.g. for avoiding rounding issues when testing for equality between\n    points.\n\n    Anisotropy in the rounding rules can be enforced by the parameter box.\n\n    >>> snap_to_grid([[0.2443], [0.501]])\n    array([[ 0.244],\n           [ 0.501]])\n\n    >>> snap_to_grid([[0.2443], [0.501]], box=[[10], [1]])\n    array([[ 0.24 ],\n           [ 0.501]])\n\n    >>> snap_to_grid([[0.2443], [0.501]], tol=0.01)\n    array([[ 0.24],\n           [ 0.5 ]])\n\n    Parameters:\n        pts (np.ndarray, nd x n_pts): Points to be rounded.\n        box (np.ndarray, nd x 1, optional): Size of the domain, precision will\n            be taken relative to the size. Defaults to unit box.\n        precision (double, optional): Resolution of the underlying grid.\n\n    Returns:\n        np.ndarray, nd x n_pts: Rounded coordinates.\n\n    \"\"\"\n\n    pts = np.asarray(pts)\n\n    nd = pts.shape[0]\n\n    if box is None:\n        box = np.reshape(np.ones(nd), (nd, 1))\n    else:\n        box = np.asarray(box)\n\n    # Precission in each direction\n    delta = box * tol\n    pts = np.rint(pts.astype(np.float64) / delta) * delta\n#    logging.debug('Snapped %i points to grid with tolerance %d', pts.shape[1],\n#                 tol)\n    return pts",
  "def _split_edge(vertices, edges, edge_ind, new_pt, **kwargs):\n    \"\"\"\n    Split a line into two by introcuding a new point.\n    Function is used e.g. for gridding purposes.\n\n    The input can be a set of points, and lines between them, of which one is\n    to be split.\n\n    New lines will be inserted, unless the new points coincide with the\n    start or endpoint of the edge (under the given precision).\n\n    The code is intended for 2D, in 3D use with caution.\n\n    Examples:\n        >>> p = np.array([[0, 0], [0, 1]])\n        >>> edges = np.array([[0], [1]])\n        >>> new_pt = np.array([[0], [0.5]])\n        >>> v, e, nl, _ = _split_edge(p, edges, 0, new_pt, tol=1e-3)\n        >>> e\n        array([[0, 2],\n               [2, 1]])\n\n    Parameters:\n        vertices (np.ndarray, nd x num_pt): Points of the vertex sets.\n        edges (np.ndarray, n x num_edges): Connections between lines. If n>2,\n            the additional rows are treated as tags, that are preserved under\n            splitting.\n        edge_ind (int): index of edges to be split.\n        new_pt (np.ndarray, nd x n): new points to be inserted. Assumed to be\n            on the edge to be split. If more than one point is inserted\n            (segment intersection), it is assumed that new_pt[:, 0] is the one\n            closest to edges[0, edge_ind] (conforming with the output of\n            lines_intersect).\n        **kwargs: Arguments passed to snap_to_grid\n\n    Returns:\n        np.ndarray, nd x n_pt: new point set, possibly with new point inserted.\n        np.ndarray, n x n_con: new edge set, possibly with new lines defined.\n        boolean: True if a new line is created, otherwise false.\n        int: Splitting type, indicating which splitting strategy was used.\n            Intended for debugging.\n\n    \"\"\"\n    logger = logging.getLogger(__name__ + '.split_edge')\n    tol = kwargs['tol']\n\n    # Some back and forth with the index of the edges to be split, depending on\n    # whether it is one or two\n    edge_ind = np.asarray(edge_ind)\n    if edge_ind.size > 1:\n        edge_ind_first = edge_ind[0]\n    else:\n        edge_ind_first = edge_ind\n\n    # Start and end of the first (possibly only) edge\n    start = edges[0, edge_ind_first]\n    end = edges[1, edge_ind_first]\n\n    # Number of points before edges is modified. Used for sanity check below.\n    orig_num_pts = edges[:2].max()\n    orig_num_edges = edges.shape[1]\n\n    # Save tags associated with the edge.\n    # NOTE: For segment intersetions where the edges have different tags, one\n    # of the values will be overridden now. Fix later.\n    tags = edges[2:, edge_ind_first]\n\n    # Try to add new points\n    vertices, pt_ind, _ = _add_point(vertices, new_pt, **kwargs)\n\n    # Sanity check\n    assert len(pt_ind) <= 2, 'Splitting can at most create two new points'\n    # Check for a single intersection point\n    if len(pt_ind) < 2:\n        pi = pt_ind[0]\n        # Intersection at a point.\n        if start == pi or end == pi:\n            # Nothing happens, the intersection between the edges coincides\n            # with a shared vertex for the edges\n            new_line = 0\n            split_type = 0\n            logger.debug('Intersection on shared vertex')\n            return vertices, edges, new_line, split_type\n        else:\n            # We may need to split the edge (start, end) into two\n            new_edges = np.array([[start, pi],\n                                  [pi, end]])\n            # ... however, the new edges may already exist in the set (this\n            # apparently can happen for complex networks with several fractures\n            # sharing a line).\n            # Check if the new candidate edges already are defined in the set\n            # of edges\n            ismem, _ = setmembership.ismember_rows(new_edges, edges[:2])\n            if any(ismem):\n                new_edges = np.delete(new_edges,\n                                      np.squeeze(np.argwhere(ismem)),\n                                      axis=0)\n            if new_edges.shape[0] == 1:\n                new_edges = new_edges.reshape((-1, 1))\n\n        if new_edges.size == 0:\n            new_line = 0\n            split_type = 1\n            logger.debug('Intersection on existing vertex')\n            return vertices, edges, new_line, split_type\n\n        # Add any tags to the new edge.\n        if tags.size > 0:\n            new_edges = np.vstack((new_edges,\n                                   np.tile(tags[:, np.newaxis],\n                                           new_edges.shape[1])))\n        # Insert the new edge in the midle of the set of edges.\n        edges = np.hstack((edges[:, :edge_ind_first], new_edges,\n                           edges[:, edge_ind_first+1:]))\n        # We have added as many new edges as there are columns in new_edges,\n        # minus 1 (which was removed / ignored).\n        new_line = new_edges.shape[1] - 1\n\n        # Sanity check of new edge\n        if np.any(np.diff(edges[:2], axis=0) == 0):\n            raise ValueError('Have created a point edge')\n        edge_copy = np.sort(edges[:2], axis=0)\n        edge_unique, _, _ = setmembership.unique_columns_tol(edge_copy, tol=tol)\n        if edge_unique.shape[1] < edges.shape[1]:\n            raise ValueError('Have created the same edge twice')\n\n        split_type = 2\n        logger.debug('Intersection on new single vertex')\n        return vertices, edges, new_line, split_type\n    else:\n        logger.debug('Splitting handles two points')\n        # Without this, we will delete something we should not delete below.\n        assert edge_ind[0] < edge_ind[1]\n\n        # Intersection along a segment.\n        # No new points should have been made\n        assert pt_ind[0] <= orig_num_pts and pt_ind[1] <= orig_num_pts\n\n        pt_ind = np.reshape(np.array(pt_ind), (-1, 1))\n\n        # There are three (four) possible configurations\n        # a) The intersection is contained within (start, end). edge_ind[0]\n        # should be split into three segments, and edge_ind[1] should be\n        # deleted (it will be identical to the middle of the new segments).\n        # b) The intersection is identical with (start, end). edge_ind[1]\n        # should be split into three segments, and edge_ind[0] is deleted.\n        # c) and d) The intersection consists of one of (start, end), and another\n        # point. Both edge_ind[0] and edge_ind[1] should be split into two\n        # segments.\n\n        i0 = pt_ind[0]\n        i1 = pt_ind[1]\n        if i0 != start and i1 != end:\n            # Delete the second segment\n            edges = np.delete(edges, edge_ind[1], axis=1)\n            if edges.shape[0] == 1:\n                edges = edges.reshape((-1, 1))\n            # We know that i0 will be closest to start, thus (start, i0) is a\n            # pair.\n            # New segments (i0, i1) is identical to the old edge_ind[1]\n            new_edges = np.array([[start, i0, i1],\n                                  [i0, i1, end]])\n            if tags.size > 0:\n                new_edges = np.vstack((new_edges,\n                                       np.tile(tags[:, np.newaxis],\n                                               new_edges.shape[1])))\n            # Combine everything.\n            edges = np.hstack((edges[:, :edge_ind[0]],\n                               new_edges,\n                               edges[:, edge_ind[0]+1:]))\n\n            logger.debug('Second edge split into two new parts')\n            split_type = 4\n        elif i0 == start and i1 == end:\n            # We don't know if i0 is closest to the start or end of edges[:,\n            # edges_ind[1]]. Find the nearest.\n            if dist_point_pointset(vertices[:, i0],\n                                     vertices[:, edges[0, edge_ind[1]]]) < \\\n                dist_point_pointset(vertices[:, i1],\n                                      vertices[:, edges[0, edge_ind[1]]]):\n                other_start = edges[0, edge_ind[1]]\n                other_end = edges[1, edge_ind[1]]\n            else:\n                other_start = edges[1, edge_ind[1]]\n                other_end = edges[0, edge_ind[1]]\n            # New segments (i0, i1) is identical to the old edge_ind[0]\n            new_edges = np.array([[other_start, i0, i1],\n                                  [i0, i1, other_end]])\n            # For some reason we sometimes create point-edges here (start and\n            # end are identical). Delete these if necessary\n            del_ind = np.squeeze(np.where(np.diff(new_edges, axis=0)[0] == 0))\n            new_edges = np.delete(new_edges, del_ind, axis=1)\n            if tags.size > 0:\n                new_edges = np.vstack((new_edges,\n                                       np.tile(tags[:, np.newaxis],\n                                               new_edges.shape[1])))\n            # Combine everything.\n            edges = np.hstack((edges[:, :edge_ind[1]],\n                               new_edges,\n                               edges[:, (edge_ind[1]+1):]))\n            # Delete the second segment. This is most easily handled after\n            # edges is expanded, to avoid accounting for changing edge indices.\n            edges = np.delete(edges, edge_ind[0], axis=1)\n            logger.debug('First edge split into 2 parts')\n            split_type = 5\n\n        # Note that we know that i0 is closest to start, thus no need to test\n        # for i1 == start\n        elif i0 == start and i1 != end:\n            # The intersection starts in start of edge_ind[0], and end before\n            # the end of edge_ind[0] (if not we would have i1==end).\n            # The intersection should be split into intervals (start, i1), (i1,\n            # end) and possibly (edge_ind[1][0 or 1], start); with the latter\n            # representing the part of edge_ind[1] laying on the other side of\n            # start compared than i1. The latter part will should not be\n            # included if start is also a node of edge_ind[1].\n            #\n            # Examples in 1d (really needed to make this concrete right now):\n            #  edge_ind[0] = (0, 2), edge_ind[1] = (-1, 1) is split into\n            #   (0, 1), (1, 2) and (-1, 1) (listed in the same order as above).\n            #\n            # edge_ind[0] = (0, 2), edge_ind[1] = (0, 1) is split into\n            #   (0, 1), (1, 2)\n            if edges[0, edge_ind[1]] == i1:\n                if edges[1, edge_ind[1]] == start:\n                    logger.debug('First edge split into 2')\n                    edges = np.delete(edges, edge_ind[1], axis=1)\n                else:\n                    edges[0, edge_ind[1]] = start\n                    logger.debug('First and second edge split into 2')\n            elif edges[1, edge_ind[1]] == i1:\n                if edges[0, edge_ind[1]] == start:\n                    edges = np.delete(edges, edge_ind[1], axis=1)\n                    logger.debug('First edge split into 2')\n                else:\n                    edges[1, edge_ind[1]] = start\n                    logger.debug('First and second edge split into 2')\n            else:\n                raise ValueError('This should not happen')\n\n            new_edges = np.array([[start, i1],\n                                  [i1, end]])\n            if tags.size > 0:\n                new_edges = np.vstack((new_edges,\n                                       np.tile(tags[:, np.newaxis],\n                                               new_edges.shape[1])))\n\n            edges = np.hstack((edges[:, :edge_ind[0]],\n                               new_edges,\n                               edges[:, (edge_ind[0]+1):]))\n            split_type = 6\n\n        elif i0 != start and i1 == end:\n            # Analogous configuration as the one above, but with i0 replaced by\n            # i1 and start by end.\n            if edges[0, edge_ind[1]] == i0:\n                if edges[1, edge_ind[1]] == end:\n                    edges = np.delete(edges, edge_ind[1], axis=1)\n                    logger.debug('First edge split into 2')\n                else:\n                    edges[0, edge_ind[1]] = end\n                    logger.debug('First and second edge split into 2')\n            elif edges[1, edge_ind[1]] == i0:\n                if edges[0, edge_ind[1]] == end:\n                    edges = np.delete(edges, edge_ind[1], axis=1)\n                    logger.debug('First edge split into 2')\n                else:\n                    edges[1, edge_ind[1]] = end\n                    logger.debug('First and second edge split into 2')\n            else:\n                raise ValueError('This should not happen')\n            new_edges = np.array([[start, i0],\n                                  [i0, end]])\n            if tags.size > 0:\n                new_edges = np.vstack((new_edges,\n                                       np.tile(tags[:, np.newaxis],\n                                               new_edges.shape[1])))\n\n            edges = np.hstack((edges[:, :edge_ind[0]],\n                               new_edges,\n                               edges[:, (edge_ind[0]+1):]))\n            split_type = 7\n\n        else:\n            raise ValueError('How did it come to this')\n\n        # Check validity of the new edge configuration\n        if np.any(np.diff(edges[:2], axis=0) == 0):\n            raise ValueError('Have created a point edge')\n\n        # We may have created an edge that already existed in the grid. Remove\n        # this by uniquifying the edges.\n        # Hopefully, we do not mess up the edges here.\n        edges_copy = np.sort(edges[:2], axis=0)\n        edges_unique, new_2_old, _ \\\n                = setmembership.unique_columns_tol(edges_copy, tol=tol)\n        # Refer to unique edges if necessary\n        if edges_unique.shape[1] < edges.shape[1]:\n            # Copy tags\n            edges = np.vstack((edges_unique, edges[2:, new_2_old]))\n            # Also signify that we have carried out this operation.\n            split_type = [split_type, 8]\n\n        # Number of new lines created\n        new_line = edges.shape[1] - orig_num_edges\n\n        return vertices, edges, new_line, split_type",
  "def _add_point(vertices, pt, tol=1e-3, snap=True, **kwargs):\n    \"\"\"\n    Add a point to a point set, unless the point already exist in the set.\n\n    Point coordinates are compared relative to an underlying Cartesian grid,\n    see snap_to_grid for details.\n\n    The function is created with gridding in mind, but may be useful for other\n    purposes as well.\n\n    Parameters:\n        vertices (np.ndarray, nd x num_pts): existing point set\n        pt (np.ndarray, nd x 1): Point to be added\n        tol (double): Precision of underlying Cartesian grid\n        **kwargs: Arguments passed to snap_to_grid\n\n    Returns:\n        np.ndarray, nd x n_pt: New point set, possibly containing a new point\n        int: Index of the new point added (if any). If not, index of the\n            closest existing point, i.e. the one that made a new point\n            unnecessary.\n        np.ndarray, nd x 1: The new point, or None if no new point was needed.\n\n    \"\"\"\n    if 'tol' not in kwargs:\n        kwargs['tol'] = tol\n\n    nd = vertices.shape[0]\n    # Before comparing coordinates, snap both existing and new point to the\n    # underlying grid\n    if snap:\n        vertices = snap_to_grid(vertices, **kwargs)\n        pt = snap_to_grid(pt, **kwargs)\n\n    new_pt = np.empty((nd, 0))\n    ind = []\n    # Distance\n    for i in range(pt.shape[-1]):\n        dist = dist_point_pointset(pt[:, i], vertices)\n        min_dist = np.min(dist)\n\n        # The tolerance parameter here turns out to be critical in an edge\n        # intersection removal procedure. The scaling factor is somewhat\n        # arbitrary, and should be looked into.\n        if min_dist < tol * np.sqrt(3):\n            # No new point is needed\n            ind.append(np.argmin(dist))\n        else:\n            # Append the new point at the end of the point list\n            ind.append(vertices.shape[1])\n            vertices = np.append(vertices, pt, axis=1)\n            new_pt = np.hstack((new_pt, pt[:, i].reshape((-1, 1))))\n    if new_pt.shape[1] == 0:\n        new_pt = None\n    return vertices, ind, new_pt",
  "def remove_edge_crossings(vertices, edges, tol=1e-3, verbose=0, snap=True,\n                          **kwargs):\n    \"\"\"\n    Process a set of points and connections between them so that the result\n    is an extended point set and new connections that do not intersect.\n\n    The function is written for gridding of fractured domains, but may be\n    of use in other cases as well. The geometry is assumed to be 2D, (the\n    corresponding operation in 3D requires intersection between planes, and\n    is a rather complex, although doable, task).\n\n    The connections are defined by their start and endpoints, and can also\n    have tags assigned. If so, the tags are preserved as connections are split.\n\n    Parameters:\n        vertices (np.ndarray, 2 x n_pt): Coordinates of points to be processed\n        edges (np.ndarray, n x n_con): Connections between lines. n >= 2, row\n            0 and 1 are index of start and endpoints, additional rows are tags\n        tol (double, optional, default=1e-8): Tolerance used for comparing\n            equal points.\n        **kwargs: Arguments passed to snap_to_grid.\n\n    Returns:\n    np.ndarray, (2 x n_pt), array of points, possibly expanded.\n    np.ndarray, (n x n_edges), array of new edges. Non-intersecting.\n\n    Raises:\n    NotImplementedError if a 3D point array is provided.\n\n    \"\"\"\n    # Sanity check of input specification edge endpoints\n    assert np.all(np.diff(edges[:2], axis=0) != 0), 'Found point edge before'\\\n        'removal of intersections'\n\n    # Use a non-standard naming convention for the logger to\n    logger = logging.getLogger(__name__ + '.remove_edge_crossings')\n\n    logger.info('Find intersections between %i edges', edges.shape[1])\n    nd = vertices.shape[0]\n\n    # Only 2D is considered. 3D should not be too dificult, but it is not\n    # clear how relevant it is\n    if nd != 2:\n        raise NotImplementedError('Only 2D so far')\n\n    edge_counter = 0\n\n    # Add tolerance to kwargs, this is later passed to split_edges, and further\n    # on.\n    kwargs['tol'] = tol\n    kwargs['snap'] = snap\n    if snap:\n        vertices = snap_to_grid(vertices, **kwargs)\n\n    # Field used for debugging of edge splits. To see the meaning of the values\n    # of each split, look in the source code of split_edges.\n    split_type = []\n\n    # Loop over all edges, search for intersections. The number of edges can\n    # change due to splitting.\n    while edge_counter < edges.shape[1]:\n        # The direct test of whether two edges intersect is somewhat\n        # expensive, and it is hard to vectorize this part. Therefore,\n        # we first identify edges which crosses the extention of this edge (\n        # intersection of line and line segment). We then go on to test for\n        # real intersections.\n        logger.debug('Remove intersection from edge with indices %i, %i',\n                      edges[0, edge_counter], edges[1, edge_counter])\n        # Find start and stop coordinates for all edges\n        start_x = vertices[0, edges[0]]\n        start_y = vertices[1, edges[0]]\n        end_x = vertices[0, edges[1]]\n        end_y = vertices[1, edges[1]]\n        logger.debug('Start point (%.5f, %.5f), End (%.5f, %.5f)',\n                      start_x[edge_counter], start_y[edge_counter],\n                      end_x[edge_counter], end_y[edge_counter])\n\n        a = end_y - start_y\n        b = -(end_x - start_x)\n\n        # Midpoint of this edge\n        xm = (start_x[edge_counter] + end_x[edge_counter]) / 2.\n        ym = (start_y[edge_counter] + end_y[edge_counter]) / 2.\n\n        # For all lines, find which side of line i it's two endpoints are.\n        # If c1 and c2 have different signs, they will be on different sides\n        # of line i. See\n        #\n        # http://stackoverflow.com/questions/385305/efficient-maths-algorithm-to-calculate-intersections\n        #\n        # answer by PolyThinker and comments by Jason S, for more information.\n        c1 = a[edge_counter] * (start_x - xm) \\\n             + b[edge_counter] * (start_y - ym)\n        c2 = a[edge_counter] * (end_x - xm) + b[edge_counter] * (end_y - ym)\n\n        tol_scaled = tol * max(1, np.max([np.sqrt(np.abs(c1)),\n                                          np.sqrt(np.abs(c2))]))\n\n        # We check for three cases\n        # 1) Lines crossing\n        lines_cross = np.sign(c1) != np.sign(c2)\n        # 2) Lines parallel\n        parallel_lines = np.logical_and(np.abs(c1) < tol_scaled,\n                                        np.abs(c2) < tol_scaled)\n        # 3) One line look to end on the other\n        lines_almost_cross = np.logical_or(np.abs(c1) < tol_scaled,\n                                           np.abs(c2) < tol_scaled)\n        # Any of the three above deserves a closer look\n        line_intersections = np.logical_or(np.logical_or(parallel_lines,\n                                                         lines_cross),\n                                           lines_almost_cross)\n\n        # Find elements which may intersect.\n        intersections = np.argwhere(line_intersections)\n        # np.argwhere returns an array of dimensions (1, dim), so we reduce\n        # this to truly 1D, or simply continue with the next edge if there\n        # are no candidate edges\n        if intersections.size > 0:\n            intersections = intersections.ravel('C')\n            logger.debug('Found %i candidate intersections',\n                          intersections.size)\n        else:\n            # There are no candidates for intersection\n            edge_counter += 1\n            logger.debug('Found no candidate intersections')\n            continue\n\n        size_before_splitting = edges.shape[1]\n\n        int_counter = 0\n        while intersections.size > 0 and int_counter < intersections.size:\n            # Line intersect (inner loop) is an intersection if it crosses\n            # the extension of line edge_counter (outer loop) (ie intsect it\n            #  crosses the infinite line that goes through the endpoints of\n            # edge_counter), but we don't know if it actually crosses the\n            # line segment edge_counter. Now we do a more refined search to\n            # find if the line segments intersects. Note that there is no\n            # help in vectorizing lines_intersect and computing intersection\n            #  points for all lines in intersections, since line i may be\n            # split, and the intersection points must recalculated. It may\n            # be possible to reorganize this while-loop by computing all\n            # intersection points(vectorized), and only recompuing if line\n            # edge_counter is split, but we keep things simple for now.\n            intsect = intersections[int_counter]\n            if intsect <= edge_counter:\n                int_counter += 1\n                continue\n\n            logger.debug('Look for intersection with edge %i', intsect)\n            logger.debug('Outer edge: Start (%.5f, %.5f), End (%.5f, %.5f)',\n                          vertices[0, edges[0, edge_counter]],\n                          vertices[1, edges[0, edge_counter]],\n                          vertices[0, edges[1, edge_counter]],\n                          vertices[1, edges[1, edge_counter]])\n            logger.debug('Inner edge: Start (%.5f, %.5f), End (%.5f, %.5f)',\n                          vertices[0, edges[0, intsect]],\n                          vertices[1, edges[0, intsect]],\n                          vertices[0, edges[1, intsect]],\n                          vertices[1, edges[1, intsect]])\n\n            # Check if this point intersects\n            new_pt = lines_intersect(vertices[:, edges[0, edge_counter]],\n                                     vertices[:, edges[1, edge_counter]],\n                                     vertices[:, edges[0, intsect]],\n                                     vertices[:, edges[1, intsect]],\n                                     tol=tol)\n\n            def __min_dist(p):\n                md = np.inf\n                for pi in [edges[0, edge_counter],\n                           edges[1, edge_counter],\n                           edges[0, intsect], edges[1, intsect]]:\n                    md = min(md, dist_point_pointset(p, vertices[:, pi]))\n                return md\n\n            orig_vertex_num = vertices.shape[1]\n            orig_edge_num = edges.shape[1]\n\n            if new_pt is None:\n                logger.debug('No intersection found')\n            else:\n                if snap:\n                    new_pt = snap_to_grid(new_pt, tol=tol)\n                # The case of segment intersections need special treatment.\n                if new_pt.shape[-1] == 1:\n                    logger.debug('Found intersection (%.5f, %.5f)', new_pt[0],\n                                  new_pt[1])\n\n                    # Split edge edge_counter (outer loop), unless the\n                    # intersection hits an existing point (in practices this\n                    # means the intersection runs through an endpoint of the\n                    # edge in an L-type configuration, in which case no new point\n                    # is needed)\n                    md = __min_dist(new_pt)\n                    vertices, edges, split_outer_edge,\\\n                            split = _split_edge(vertices, edges, edge_counter,\n                                                new_pt, **kwargs)\n                    split_type.append(split)\n                    if split_outer_edge > 0:\n                        logger.debug('Split outer edge')\n\n                    if edges.shape[1] > orig_edge_num + split_outer_edge:\n                        raise ValueError('Have created edge without bookkeeping')\n                    # If the outer edge (represented by edge_counter) was split,\n                    # e.g. inserted into the list of edges we need to increase the\n                    # index of the inner edge\n                    intsect += split_outer_edge\n\n                    # Possibly split the inner edge\n                    vertices, edges, split_inner_edge, \\\n                            split = _split_edge(vertices, edges, intsect,\n                                                new_pt, **kwargs)\n                    if edges.shape[1] > \\\n                       orig_edge_num + split_inner_edge + split_outer_edge:\n                        raise ValueError('Have created edge without bookkeeping')\n\n                    split_type.append(split)\n                    if split_inner_edge > 0:\n                        logger.debug('Split inner edge')\n                    intersections += split_outer_edge + split_inner_edge\n                else:\n                    # We have found an intersection along a line segment\n                    logger.debug('''Found two intersections: (%.5f, %.5f) and\n                                    (%.5f, %.5f)''', new_pt[0, 0],\n                                    new_pt[1, 0], new_pt[0, 1], new_pt[1, 1])\n                    vertices, edges, splits,\\\n                            s_type = _split_edge(vertices, edges,\n                                                 [edge_counter, intsect],\n                                                 new_pt, **kwargs)\n                    split_type.append(s_type)\n                    intersections += splits\n                    logger.debug('Split into %i parts', splits)\n\n            # Sanity checks - turned out to be useful for debugging.\n            if np.any(np.diff(edges[:2], axis=0) == 0):\n                raise ValueError('Have somehow created a point edge')\n            if intersections.max() > edges.shape[1]:\n                raise ValueError('Intersection pointer outside edge array')\n\n            # We're done with this candidate edge. Increase index of inner loop\n            int_counter += 1\n\n        # We're done with all intersections of this loop. increase index of\n        # outer loop\n        edge_counter += 1\n        logger.debug('Edge split into %i new parts', edges.shape[1] -\n                     size_before_splitting)\n\n    if verbose > 1:\n        logger.info('Edge intersection removal complete. Elapsed time: %g',\n                    time.time() - start_time)\n        logger.info('Introduced %i new edges', edges.shape[1] - num_edges_orig)\n\n    return vertices, edges",
  "def is_ccw_polygon(poly):\n    \"\"\"\n    Determine if the vertices of a polygon are sorted counter clockwise.\n\n    The method computes the winding number of the polygon, see\n        http://stackoverflow.com/questions/1165647/how-to-determine-if-a-list-of-polygon-points-are-in-clockwise-order\n    and\n        http://blog.element84.com/polygon-winding.html\n\n    for descriptions of the algorithm.\n\n    The algorithm should work for non-convex polygons. If the polygon is\n    self-intersecting (shaped like the number 8), the number returned will\n    reflect whether the method is 'mostly' cw or ccw.\n\n    Parameters:\n        poly (np.ndarray, 2xn): Polygon vertices.\n\n    Returns:\n        boolean, True if the polygon is ccw.\n\n    See also:\n        is_ccw_polyline\n\n    Examples:\n        >>> is_ccw_polygon(np.array([[0, 1, 0], [0, 0, 1]]))\n        True\n\n        >>> is_ccw_polygon(np.array([[0, 0, 1], [0, 1, 0]]))\n        False\n\n    \"\"\"\n    p_0 = np.append(poly[0], poly[0, 0])\n    p_1 = np.append(poly[1], poly[1, 0])\n\n    num_p = poly.shape[1]\n    value = 0\n    for i in range(num_p):\n        value += (p_1[i+1] + p_1[i]) * (p_0[i+1] - p_0[i])\n    return value < 0",
  "def is_ccw_polyline(p1, p2, p3, tol=0, default=False):\n    \"\"\"\n    Check if the line segments formed by three points is part of a\n    conuter-clockwise circle.\n\n    The test is positiv if p3 lies to left of the line running through p1 and\n    p2.\n\n    The function is intended for 2D points; higher-dimensional coordinates will\n    be ignored.\n\n    Extensions to lines with more than three points should be straightforward,\n    the input points should be merged into a 2d array.\n\n    Parameters:\n        p1 (np.ndarray, length 2): Point on dividing line\n        p2 (np.ndarray, length 2): Point on dividing line\n        p3 (np.ndarray, length 2): Point to be tested\n        tol (double, optional): Tolerance used in the comparison, can be used\n            to account for rounding errors. Defaults to zero.\n        default (boolean, optional): Mode returned if the point is within the\n            tolerance. Should be set according to what is desired behavior of\n            the function (will vary with application). Defaults to False.\n\n    Returns:\n        boolean, true if the points form a ccw polyline.\n\n    See also:\n        is_ccw_polygon\n\n    \"\"\"\n\n    # Compute cross product between p1-p2 and p1-p3. Right hand rule gives that\n    # p3 is to the left if the cross product is positive.\n    cross_product = (p2[0] - p1[0]) * (p3[1] - p1[1])\\\n                   -(p2[1] - p1[1]) * (p3[0] - p1[0])\n\n    # Should there be a scaling of the tolerance relative to the distance\n    # between the points?\n\n    if np.abs(cross_product) <= tol:\n        return default\n    return cross_product > -tol",
  "def is_inside_polygon(poly, p, tol=0, default=False):\n    \"\"\"\n    Check if a set of points are inside a polygon.\n\n    The method assumes that the polygon is convex.\n\n    Paremeters:\n        poly (np.ndarray, 2 x n): vertexes of polygon. The segments are formed by\n            connecting subsequent columns of poly\n        p (np.ndarray, 2 x n2): Points to be tested.\n        tol (double, optional): Tolerance for rounding errors. Defaults to\n            zero.\n        default (boolean, optional): Default behavior if the point is close to\n            the boundary of the polygon. Defaults to False.\n\n    Returns:\n        np.ndarray, boolean: Length equal to p, true if the point is inside the\n            polygon.\n\n    \"\"\"\n    if p.ndim == 1:\n        pt = p.reshape((-1, 1))\n    else:\n        pt = p\n\n    # The test uses is_ccw_polyline, and tacitly assumes that the polygon\n    # vertexes is sorted in a ccw fashion. If this is not the case, flip the\n    # order of the nodes on a copy, and use this for the testing.\n    # Note that if the nodes are not cw nor ccw (e.g. they are crossing), the\n    # test cannot be trusted anyhow.\n    if not is_ccw_polygon(poly):\n        poly = poly.copy()[:, ::-1]\n\n    poly_size = poly.shape[1]\n\n    inside = np.ones(pt.shape[1], dtype=np.bool)\n    for i in range(pt.shape[1]):\n        for j in range(poly.shape[1]):\n            if not is_ccw_polyline(poly[:, j], poly[:, (j+1) % poly_size],\n                                   pt[:, i], tol=tol, default=default):\n                inside[i] = False\n                # No need to check the remaining segments of the polygon.\n                break\n    return inside",
  "def lines_intersect(start_1, end_1, start_2, end_2, tol=1e-8):\n    \"\"\"\n    Check if two line segments defined by their start end endpoints, intersect.\n\n    The lines are assumed to be in 2D.\n\n    Note that, oposed to other functions related to grid generation such as\n    remove_edge_crossings, this function does not use the concept of\n    snap_to_grid. This may cause problems at some point, although no issues\n    have been discovered so far.\n\n    Implementation note:\n        This function can be replaced by a call to segments_intersect_3d. Todo.\n\n    Example:\n        >>> lines_intersect([0, 0], [1, 1], [0, 1], [1, 0])\n        array([[ 0.5],\n           [ 0.5]])\n\n        >>> lines_intersect([0, 0], [1, 0], [0, 1], [1, 1])\n\n    Parameters:\n        start_1 (np.ndarray or list): coordinates of start point for first\n            line.\n        end_1 (np.ndarray or list): coordinates of end point for first line.\n        start_2 (np.ndarray or list): coordinates of start point for first\n            line.\n        end_2 (np.ndarray or list): coordinates of end point for first line.\n\n    Returns:\n        np.ndarray (2 x num_pts): coordinates of intersection point, or the\n            endpoints of the intersection segments if relevant. In the case of\n            a segment, the first point (column) will be closest to start_1.  If\n            the lines do not intersect, None is returned.\n\n    Raises:\n        ValueError if the start and endpoints of a line are the same.\n\n    \"\"\"\n    start_1 = np.asarray(start_1).astype(np.float)\n    end_1 = np.asarray(end_1).astype(np.float)\n    start_2 = np.asarray(start_2).astype(np.float)\n    end_2 = np.asarray(end_2).astype(np.float)\n\n    # Vectors along first and second line\n    d_1 = end_1 - start_1\n    d_2 = end_2 - start_2\n\n    length_1 = np.sqrt(np.sum(d_1 * d_1))\n    length_2 = np.sqrt(np.sum(d_2 * d_2))\n\n    # Vector between the start points\n    d_s = start_2 - start_1\n\n    # An intersection point is characterized by\n    #   start_1 + d_1 * t_1 = start_2 + d_2 * t_2\n    #\n    # which on component form becomes\n    #\n    #   d_1[0] * t_1 - d_2[0] * t_2 = d_s[0]\n    #   d_1[1] * t_1 - d_2[1] * t_2 = d_s[1]\n    #\n    # First check for solvability of the system (e.g. parallel lines) by the\n    # determinant of the matrix.\n\n    discr = d_1[0] *(-d_2[1]) - d_1[1] * (-d_2[0])\n\n    # Check if lines are parallel.\n    # The tolerance should be relative to the length of d_1 and d_2\n    if np.abs(discr) < tol * length_1 * length_2:\n        # The lines are parallel, and will only cross if they are also colinear\n        logger.debug('The segments are parallel')\n        # Cross product between line 1 and line between start points on line\n        start_cross_line = d_s[0] * d_1[1] - d_s[1] * d_1[0]\n        if np.abs(start_cross_line) < tol * max(length_1, length_2):\n            logger.debug('Lines are colinear')\n            # The lines are co-linear\n\n            # Write l1 on the form start_1 + t * d_1, find the parameter value\n            # needed for equality with start_2 and end_2\n            if np.abs(d_1[0]) > tol * length_1:\n                t_start_2 = (start_2[0] - start_1[0])/d_1[0]\n                t_end_2 = (end_2[0] - start_1[0])/d_1[0]\n            elif np.abs(d_1[1]) > tol * length_2:\n                t_start_2 = (start_2[1] - start_1[1])/d_1[1]\n                t_end_2 = (end_2[1] - start_1[1])/d_1[1]\n            else:\n                # d_1 is zero\n                logger.error('Found what must be a point-edge')\n                raise ValueError('Start and endpoint of line should be\\\n                                 different')\n            if t_start_2 < 0 and t_end_2 < 0:\n                logger.debug('Lines are not overlapping')\n                return None\n            elif t_start_2 > 1 and t_end_2 > 1:\n                logger.debug('Lines are not overlapping')\n                return None\n            # We have an overlap, find its parameter values\n            t_min = max(min(t_start_2, t_end_2), 0)\n            t_max = min(max(t_start_2, t_end_2), 1)\n\n            if t_max - t_min < tol:\n                # It seems this can only happen if they are also equal to 0 or\n                # 1, that is, the lines share a single point\n                p_1 = start_1 + d_1 * t_min\n                logger.debug('Colinear lines share a single point')\n                return p_1.reshape((-1, 1))\n\n            logger.debug('Colinear lines intersect along segment')\n            p_1 = start_1 + d_1 * t_min\n            p_2 = start_1 + d_1 * t_max\n            return np.array([[p_1[0], p_2[0]], [p_1[1], p_2[1]]])\n\n        else:\n            logger.debug('Lines are not colinear')\n            # Lines are parallel, but not colinear\n            return None\n    else:\n        # Solve linear system using Cramer's rule\n        t_1 = (d_s[0] * (-d_2[1]) - d_s[1] * (-d_2[0])) / discr\n        t_2 = (d_1[0] * d_s[1] - d_1[1] * d_s[0]) / discr\n\n        isect_1 = start_1 + t_1 * d_1\n        isect_2 = start_2 + t_2 * d_2\n        # Safeguarding\n        assert np.allclose(isect_1, isect_2, tol)\n\n        # The intersection lies on both segments if both t_1 and t_2 are on the\n        # unit interval.\n        # Use tol to allow some approximations\n        if t_1 >= -tol and t_1 <= (1 + tol) and \\\n           t_2 >= -tol and t_2 <= (1 + tol):\n            logger.debug('Segment intersection found in one point')\n            return np.array([[isect_1[0]], [isect_1[1]]])\n\n        return None",
  "def segments_intersect_3d(start_1, end_1, start_2, end_2, tol=1e-8):\n    \"\"\"\n    Find intersection points (or segments) of two 3d lines.\n\n    Note that, oposed to other functions related to grid generation such as\n    remove_edge_crossings, this function does not use the concept of\n    snap_to_grid. This may cause problems at some point, although no issues\n    have been discovered so far.\n\n    Parameters:\n        start_1 (np.ndarray or list): coordinates of start point for first\n            line.\n        end_1 (np.ndarray or list): coordinates of end point for first line.\n        start_2 (np.ndarray or list): coordinates of start point for first\n            line.\n        end_2 (np.ndarray or list): coordinates of end point for first line.\n\n    Returns:\n        np.ndarray, dimension 3xn_pts): coordinates of intersection points\n            (number of columns will be either 1 for a point intersection, or 2\n            for a segment intersection). If the lines do not intersect, None is\n            returned.\n\n    \"\"\"\n\n    # Convert input to numpy if necessary\n    start_1 = np.asarray(start_1).astype(np.float).ravel()\n    end_1 = np.asarray(end_1).astype(np.float).ravel()\n    start_2 = np.asarray(start_2).astype(np.float).ravel()\n    end_2 = np.asarray(end_2).astype(np.float).ravel()\n\n    # Short hand for component of start and end points, as well as vectors\n    # along lines.\n    xs_1 = start_1[0]\n    ys_1 = start_1[1]\n    zs_1 = start_1[2]\n\n    xe_1 = end_1[0]\n    ye_1 = end_1[1]\n    ze_1 = end_1[2]\n\n    dx_1 = xe_1 - xs_1\n    dy_1 = ye_1 - ys_1\n    dz_1 = ze_1 - zs_1\n\n    xs_2 = start_2[0]\n    ys_2 = start_2[1]\n    zs_2 = start_2[2]\n\n    xe_2 = end_2[0]\n    ye_2 = end_2[1]\n    ze_2 = end_2[2]\n\n    dx_2 = xe_2 - xs_2\n    dy_2 = ye_2 - ys_2\n    dz_2 = ze_2 - zs_2\n\n    # The lines are parallel in the x-y plane, but we don't know about the\n    # z-direction. CHeck this\n    deltas_1 = np.array([dx_1, dy_1, dz_1])\n    deltas_2 = np.array([dx_2, dy_2, dz_2])\n\n    # Use masked arrays to avoid divisions by zero\n    mask_1 = np.ma.greater(np.abs(deltas_1), tol)\n    mask_2 = np.ma.greater(np.abs(deltas_2), tol)\n\n    # Check for two dimensions that are not parallel with at least one line\n    mask_sum = mask_1 + mask_2\n    if mask_sum.sum() > 1:\n        in_discr = np.argwhere(mask_sum)[:2]\n    else:\n        # We're going to have a zero discreminant anyhow, just pick some dimensions.\n        in_discr = np.arange(2)\n\n    not_in_discr = np.setdiff1d(np.arange(3), in_discr)[0]\n    discr = deltas_1[in_discr[0]] * deltas_2[in_discr[1]]\\\n            - deltas_1[in_discr[1]] * deltas_2[in_discr[0]]\n\n    # An intersection will be a solution of the linear system\n    #   xs_1 + dx_1 * t_1 = xs_2 + dx_2 * t_2 (1)\n    #   ys_1 + dy_1 * t_1 = ys_2 + dy_2 * t_2 (2)\n    #\n    # In addition, the solution should satisfy\n    #   zs_1 + dz_1 * t_1 = zs_2 + dz_2 * t_2 (3)\n    #\n    # The intersection is on the line segments if 0 <= (t_1, t_2) <= 1\n\n    # Either the lines are parallel in two directions\n    if np.abs(discr) < tol:\n        # If the lines are (almost) parallel, there is no single intersection,\n        # but it may be a segment\n\n        # First check if the third dimension is also parallel, if not, no\n        # intersection\n\n        # A first, simple test\n        if np.any(mask_1 != mask_2):\n            return None\n\n        t = deltas_1[mask_1] / deltas_2[mask_2]\n\n        # Second, test for alignment in all directions\n        if not np.allclose(t, t.mean(), tol):\n            return None\n\n        # If we have made it this far, the lines are indeed parallel. Next,\n        # check that they lay along the same line.\n        diff_start = start_2 - start_1\n\n        dstart_x_delta_x = diff_start[1] * deltas_1[2] -\\\n                           diff_start[2] * deltas_1[1]\n        if np.abs(dstart_x_delta_x) > tol:\n            return None\n        dstart_x_delta_y = diff_start[2] * deltas_1[0] -\\\n                           diff_start[0] * deltas_1[2]\n        if np.abs(dstart_x_delta_y) > tol:\n            return None\n        dstart_x_delta_z = diff_start[0] * deltas_1[1] -\\\n                           diff_start[1] * deltas_1[0]\n        if np.abs(dstart_x_delta_z) > tol:\n            return None\n\n        # For dimensions with an incline, the vector between segment start\n        # points should be parallel to the segments.\n        # Since the masks are equal, we can use any of them.\n        # For dimensions with no incline, the start cooordinates should be the same\n        if not np.allclose(start_1[~mask_1], start_2[~mask_1], tol):\n            return None\n\n        # We have overlapping lines! finally check if segments are overlapping.\n\n        # Since everything is parallel, it suffices to work with a single coordinate\n        s_1 = start_1[mask_1][0]\n        e_1 = end_1[mask_1][0]\n        s_2 = start_2[mask_1][0]\n        e_2 = end_2[mask_1][0]\n\n        max_1 = max(s_1, e_1)\n        min_1 = min(s_1, e_1)\n        max_2 = max(s_2, e_2)\n        min_2 = min(s_2, e_2)\n\n        # Rule out case with non-overlapping segments\n        if max_1 < min_2:\n            return None\n        elif max_2 < min_1:\n            return None\n\n\n        # The lines are overlapping, we need to find their common line\n        lines = np.array([s_1, e_1, s_2, e_2])\n        sort_ind = np.argsort(lines)\n\n        # The overlap will be between the middle two points in the sorted list\n        target = sort_ind[1:3]\n\n        # Array of the full coordinates - same order as lines\n        lines_full = np.vstack((start_1, end_1, start_2, end_2)).transpose()\n        # Our segment consists of the second and third column. We're done!\n        return lines_full[:, target]\n\n    # or we are looking for a point intersection\n    else:\n        # Solve 2x2 system by Cramer's rule\n\n        discr = deltas_1[in_discr[0]] * (-deltas_2[in_discr[1]]) -\\\n                deltas_1[in_discr[1]] * (-deltas_2[in_discr[0]])\n        t_1 = ((start_2[in_discr[0]] - start_1[in_discr[0]]) \\\n               * (-deltas_2[in_discr[1]]) - \\\n               (start_2[in_discr[1]] - start_1[in_discr[1]]) \\\n               * (-deltas_2[in_discr[0]]))/discr\n\n        t_2 = (deltas_1[in_discr[0]] * (start_2[in_discr[1]] -\n                                        start_1[in_discr[1]]) - \\\n               deltas_1[in_discr[1]] * (start_2[in_discr[0]] -\n                                        start_1[in_discr[0]])) / discr\n\n        # Check that we are on line segment\n        if t_1 < 0 or t_1 > 1 or t_2 < 0 or t_2 > 1:\n            return None\n\n        # Compute the z-coordinates of the intersection points\n        z_1_isect = start_1[not_in_discr] + t_1 * deltas_1[not_in_discr]\n        z_2_isect = start_2[not_in_discr] + t_2 * deltas_2[not_in_discr]\n\n        if np.abs(z_1_isect - z_2_isect) < tol:\n            vec = np.zeros(3)\n            vec[in_discr] = start_1[in_discr] + t_1 * deltas_1[in_discr]\n            vec[not_in_discr] = z_1_isect\n            return vec.reshape((-1, 1))\n        else:\n            return None",
  "def _np2p(p):\n    # Convert a numpy point array (3xn) to sympy points\n    if p.ndim == 1:\n        return geom.Point(p[:])\n    else:\n        return [geom.Point(p[:, i]) for i in range(p.shape[1])]",
  "def _p2np(p):\n    # Convert sympy points to numpy format. If more than one point, these should be sent as a list\n    if isinstance(p, list):\n        return np.array(list([i.args for i in p]), dtype='float').transpose()\n    else:\n        return np.array(list(p.args), dtype='float').reshape((-1, 1))",
  "def _to3D(p):\n    # Add a third dimension\n    return np.vstack((p, np.zeros(p.shape[1])))",
  "def polygon_boundaries_intersect(poly_1, poly_2, tol=1e-8):\n    \"\"\"\n    Test for intersection between the bounding segments of two 3D polygons.\n\n    No tests are done for intersections with polygons interiors. The code has\n    only been tested for convex polygons, status for non-convex polygons is\n    unknown.\n\n    A segment can either be intersected by segments of the other polygon as\n     i) a single point\n     ii) two points, hit by different segments (cannot be more than 2 for\n         convex polygons)\n     iii) along a line, if the segments are parallel.\n\n    Note that if the polygons share a vertex, this point will be found multiple\n    times (depending on the configuration of the polygons).\n\n    Each intersection is represented as a list of three items. The first two\n    are the segment index (numbered according to start point) of the\n    intersecting segments. The third is the coordinates of the intersection\n    point, this can either be a single point (3x1 nd.array), or a 3x2 array\n    with columns representing the same (shared vertex) or different (shared\n    segment) points.\n\n    Paremeters:\n        poly_1 (np.array, 3 x n_pt): First polygon, assumed to be\n        non-intersecting. The closing segment is defined by the last and first\n            column.\n        poly_2 (np.array, 3 x n_pt): Second polygon, assumed to be\n        non-intersecting. The closing segment is defined by the last and first\n            column.\n        tol (float, optional): Tolerance used for equality.\n\n    Returns:\n        list: of intersections. See above for description of data format. If no\n        intersections are found, an empty list is returned.\n\n    \"\"\"\n    l_1 = poly_1.shape[1]\n    ind_1 = np.append(np.arange(l_1), 0)\n    l_2 = poly_2.shape[1]\n    ind_2 = np.append(np.arange(l_2), 0)\n\n    isect = []\n\n    for i in range(l_1):\n        p_1_1 = poly_1[:, ind_1[i]]\n        p_1_2 = poly_1[:, ind_1[i+1]]\n\n        for j in range(l_2):\n            p_2_1 = poly_2[:, ind_2[j]]\n            p_2_2 = poly_2[:, ind_2[j+1]]\n            isect_loc = segments_intersect_3d(p_1_1, p_1_2, p_2_1, p_2_2)\n            if isect_loc is not None:\n                isect.append([i, j, isect_loc])\n\n    return isect",
  "def polygon_segment_intersect(poly_1, poly_2, tol=1e-8, include_bound_pt=True):\n    \"\"\"\n    Find intersections between polygons embeded in 3D.\n\n    The intersections are defined as between the interior of the first polygon\n    and the boundary of the second, although intersections on the boundary of\n    both polygons can also be picked up sometimes. If you need to distinguish\n    between the two, the safer option is to also call\n    polygon_boundary_intersect(), and compare the results.\n\n    Parameters:\n        poly_1 (np.ndarray, 3xn1): Vertexes of polygon, assumed ordered as cw or\n            ccw.\n        poly_2 (np.ndarray, 3xn2): Vertexes of second polygon, assumed ordered\n            as cw or ccw.\n        tol (double, optional): Tolerance for when two points are equal.\n            Defaults to 1e-8.\n        include_bound_pt (boolean, optional): Include cases where a segment is\n            in the plane of the first ploygon, and the segment crosses the\n            polygon boundary. Defaults to True.\n\n    Returns:\n        np.ndarray, size 3 x num_isect, coordinates of intersection points; or\n            None if no intersection is found (may change to empty array of size\n            (3, 0)).\n\n    Raises:\n        NotImplementedError if the two polygons overlap in a 2D area. An\n        extension should not be difficult, but the function is not intended for\n        this use.\n\n    \"\"\"\n\n    # First translate the points so that the first plane is located at the origin\n    center_1 = np.mean(poly_1, axis=1).reshape((-1, 1))\n    poly_1 = poly_1 - center_1\n    poly_2 = poly_2 - center_1\n\n    # Obtain the rotation matrix that projects p1 to the xy-plane\n    rot_p_1 = project_plane_matrix(poly_1)\n    irot = rot_p_1.transpose()\n    poly_1_rot = rot_p_1.dot(poly_1)\n\n    # Sanity check: The points should lay on a plane\n    assert np.amax(np.abs(poly_1_rot[2]))/np.amax(np.abs(poly_1_rot[:2])) < tol\n\n    # Drop the z-coordinate\n    poly_1_xy = poly_1_rot[:2]\n\n    # Make sure the xy-polygon is ccw.\n    if not is_ccw_polygon(poly_1_xy):\n        poly_1_xy = poly_1_xy[:, ::-1]\n\n    # Rotate the second polygon with the same rotation matrix\n    poly_2_rot = rot_p_1.dot(poly_2)\n\n    # If the rotation of whole second point cloud lies on the same side of z=0,\n    # there are no intersections.\n    if poly_2_rot[2].min() > tol:\n        return None\n    elif poly_2_rot[2].max() < -tol:\n        return None\n\n    # Check if the second plane is parallel to the first (same xy-plane)\n    dz_2 = poly_2_rot[2].max() - poly_2_rot[2].min()\n    if dz_2 < tol:\n        if poly_2_rot[2].max() < tol:\n            # The polygons are parallel, and in the same plane\n            # Represent second polygon by sympy, and use sympy function to\n            # detect intersection.\n            # Convert the first polygon to sympy format\n            poly_1_sp = geom.Polygon(*_np2p(poly_1_xy))\n            poly_2_sp = geom.Polygon(*_np2p(poly_2_rot[:2]))\n\n            isect = poly_1_sp.intersection(poly_2_sp)\n            if isinstance(isect, list) and len(isect) > 0:\n                # It would have been possible to return the intersecting area,\n                # but this is not the intended behavior of the function.\n                # Instead raise an error, and leave it to the user to deal with\n                # this.\n                raise NotImplementedError\n            else:\n                return None\n        else:\n            # Polygons lies in different parallel planes. No intersection\n            return None\n    else:\n        # Loop over all boundary segments of the second plane. Check if they\n        # intersect with the first polygon.\n        # TODO: Special treatment of the case where one or two vertexes lies in\n        # the plane of the poly_1\n        num_p2 = poly_2.shape[1]\n        # Roling indexing\n        ind = np.append(np.arange(num_p2), np.zeros(1)).astype('int')\n\n        isect = np.empty((3, 0))\n\n        for i in range(num_p2):\n\n            # Coordinates of this segment\n            pt_1 = poly_2_rot[:, ind[i]]\n            pt_2 = poly_2_rot[:, ind[i+1]]\n\n            # Check if segment crosses z=0 in the rotated coordinates\n            if max(pt_1[2], pt_2[2]) < -tol or min(pt_1[2], pt_2[2]) > tol:\n                continue\n\n            dx = pt_2[0] - pt_1[0]\n            dy = pt_2[1] - pt_1[1]\n            dz = pt_2[2] - pt_1[2]\n            if np.abs(dz) > tol:\n                # We are on a plane, and we know that dz_2 is non-zero, so all\n                # individiual segments must have an incline.\n                # Parametrize the line, find parameter value for intersection\n                # with z=0.\n                t = (-pt_1[2] - 0) / dz\n\n                # Sanity check. We have ruled out segments not crossing the\n                # origin above.\n                if t < -tol or t > 1+tol:\n                    continue\n\n                # x and y-coordinate for z=0\n                x0 = pt_1[0] + dx * t\n                y0 = pt_1[1] + dy * t\n                # Representation as point\n                p_00 = np.array([x0, y0]).reshape((-1, 1))\n\n                # Check if the first polygon encloses the point. When applied\n                # to fracture intersections of T-type (segment embedded in the\n                # plane of another fracture), it turned out to be useful to be\n                # somewhat generous with the definition of the intersection.\n                # Therefore, allow for intersections that are slightly outside\n                # the polygon, and use the projection onto the polygon.\n\n                start = np.arange(poly_1_xy.shape[1])\n                end = np.r_[np.arange(1, poly_1_xy.shape[1]), 0]\n\n                poly_1_to3D = _to3D(poly_1_xy)\n                p_00_to3D = _to3D(p_00)\n                dist, cp = dist_points_segments(p_00_to3D,\n                                                poly_1_to3D[:, start],\n                                                poly_1_to3D[:, end])\n                mask = np.where(dist[0] < tol)[0]\n                if mask.size > 0:\n                    cp = cp[0].T[:, mask]\n                    isect = np.hstack((isect, irot.dot(cp) + center_1))\n                else:\n                    dist, cp, ins = dist_points_polygon(p_00_to3D, poly_1_to3D)\n                    if (dist[0] < tol and include_bound_pt) or dist[0] < 1e-12:\n                        isect = np.hstack((isect, irot.dot(cp) + center_1))\n\n            elif np.abs(pt_1[2]) < tol and np.abs(pt_2[2]) < tol:\n                # The segment lies completely within the polygon plane.\n                both_pts = np.vstack((pt_1, pt_2)).T\n                # Find points within tho polygon itself\n                inside = is_inside_polygon(poly_1_xy, both_pts[:2],tol=tol)\n\n                if inside.all():\n                    # Both points are inside, add and go on\n                    isect = np.hstack((isect, irot.dot(both_pts) + center_1))\n                else:\n                    # A single point is inside. Need to find the intersection between this line segment and the polygon\n                    if inside.any():\n                        isect_loc = both_pts[:2, inside].reshape((2, -1))\n                        p1 = both_pts[:, inside]\n                        p2 = both_pts[:, np.logical_not(inside)]\n                    else:\n                        isect_loc = np.empty((2, 0))\n                        p1 = both_pts[:, 0]\n                        p2 = both_pts[:, 1]\n\n                    # If a single internal point is found\n                    if isect_loc.shape[1] == 1 or include_bound_pt:\n                        poly_1_start = poly_1_rot\n                        poly_1_end = np.roll(poly_1_rot, 1, axis=1)\n                        for j in range(poly_1.shape[1]):\n                            ip = segments_intersect_3d(p1, p2,\n                                                       poly_1_start[:, j],\n                                                       poly_1_end[:, j])\n                            if ip is not None:\n                                isect_loc = np.hstack((isect_loc, ip[:2]))\n\n                    isect = np.hstack((isect, irot.dot(_to3D(isect_loc)) + center_1))\n\n        if isect.shape[1] == 0:\n            isect = None\n\n        # For points lying in the plane of poly_1, the same points may be found\n        # several times\n        if isect is not None:\n            isect, _, _ = setmembership.unique_columns_tol(isect, tol=tol)\n        return isect",
  "def is_planar(pts, normal=None, tol=1e-5):\n    \"\"\" Check if the points lie on a plane.\n\n    Parameters:\n    pts (np.ndarray, 3xn): the points.\n    normal: (optional) the normal of the plane, otherwise three points are\n        required.\n\n    Returns:\n    check, bool, if the points lie on a plane or not.\n\n    \"\"\"\n\n    if normal is None:\n        normal = compute_normal(pts)\n    else:\n        normal = normal.flatten() / np.linalg.norm(normal)\n\n    check_all = np.zeros(pts.shape[1]-1, dtype=np.bool)\n\n    for idx, p in enumerate(pts[:, 1:].T):\n        den = np.linalg.norm(pts[:, 0] - p)\n        dotprod = np.dot(normal, (pts[:, 0] - p)/(den if den else 1))\n        check_all[idx] = np.isclose(dotprod, 0, atol=tol, rtol=0)\n\n    return np.all(check_all)",
  "def is_point_in_cell(poly, p, if_make_planar=True):\n    \"\"\"\n    Check whatever a point is inside a cell. Note a similar behaviour could be\n    reached using the function is_inside_polygon, however the current\n    implementation deals with concave cells as well. Not sure which is the best,\n    in term of performances, for convex cells.\n\n    Parameters:\n        poly (np.ndarray, 3xn): vertexes of polygon. The segments are formed by\n            connecting subsequent columns of poly.\n        p (np.array, 3x1): Point to be tested.\n    if_make_planar (optional, default True): The cell needs to lie on (s, t)\n        plane. If not already done, this flag need to be used.\n\n    Return:\n        boolean, if the point is inside the cell. If a point is on the boundary\n        of the cell the result may be either True or False.\n    \"\"\"\n    p.shape = (3, 1)\n    if if_make_planar:\n        R = project_plane_matrix(poly)\n        poly = np.dot(R, poly)\n        p = np.dot(R, p)\n\n    j = poly.shape[1]-1\n    is_odd = False\n\n    for i in np.arange(poly.shape[1]):\n        if (poly[1, i] < p[1] and poly[1, j] >= p[1]) \\\n           or \\\n           (poly[1, j] < p[1] and poly[1, i] >= p[1]):\n            if (poly[0, i]+(p[1]-poly[1, i])/(poly[1, j]-poly[1, i])\\\n                *(poly[0,j]-poly[0, i])) < p[0]:\n                is_odd = not is_odd\n        j = i\n\n    return is_odd",
  "def project_plane_matrix(pts, normal=None, tol=1e-5, reference=[0, 0, 1],\n                         check_planar=True):\n    \"\"\" Project the points on a plane using local coordinates.\n\n    The projected points are computed by a dot product.\n    example: np.dot( R, pts )\n\n    Parameters:\n    pts (np.ndarray, 3xn): the points.\n    normal: (optional) the normal of the plane, otherwise three points are\n        required.\n    tol: (optional, float) tolerance to assert the planarity of the cloud of\n        points. Default value 1e-5.\n    reference: (optional, np.array, 3x1) reference vector to compute the angles.\n        Default value [0, 0, 1].\n\n    Returns:\n    np.ndarray, 3x3, projection matrix.\n\n    \"\"\"\n\n    if normal is None:\n        normal = compute_normal(pts)\n    else:\n        normal = np.asarray(normal)\n        normal = normal.flatten() / np.linalg.norm(normal)\n\n    if check_planar:\n        assert is_planar(pts, normal, tol)\n\n    reference = np.asarray(reference, dtype=np.float)\n    angle = np.arccos(np.dot(normal, reference))\n    vect = np.cross(normal, reference)\n    return rot(angle, vect)",
  "def project_line_matrix(pts, tangent=None, tol=1e-5, reference=[0, 0, 1]):\n    \"\"\" Project the points on a line using local coordinates.\n\n    The projected points are computed by a dot product.\n    example: np.dot( R, pts )\n\n    Parameters:\n    pts (np.ndarray, 3xn): the points.\n    tangent: (optional) the tangent unit vector of the plane, otherwise two\n        points are required.\n\n    Returns:\n    np.ndarray, 3x3, projection matrix.\n\n    \"\"\"\n\n    if tangent is None:\n        tangent = compute_tangent(pts)\n    else:\n        tangent = tangent.flatten() / np.linalg.norm(tangent)\n\n    reference = np.asarray(reference, dtype=np.float)\n    angle = np.arccos(np.dot(tangent, reference))\n    vect = np.cross(tangent, reference)\n    return rot(angle, vect)",
  "def rot(a, vect):\n    \"\"\" Compute the rotation matrix about a vector by an angle using the matrix\n    form of Rodrigues formula.\n\n    Parameters:\n    a: double, the angle.\n    vect: np.array, 1x3, the vector.\n\n    Returns:\n    matrix: np.ndarray, 3x3, the rotation matrix.\n\n    \"\"\"\n    if np.allclose(vect, [0., 0., 0.]):\n        return np.identity(3)\n    vect = vect / np.linalg.norm(vect)\n\n    # Prioritize readability over PEP0008 whitespaces.\n    # pylint: disable=bad-whitespace\n    W = np.array( [[       0., -vect[2],  vect[1]],\n                   [  vect[2],       0., -vect[0]],\n                   [ -vect[1],  vect[0],       0. ]])\n    return np.identity(3) + np.sin(a)*W + \\\n           (1.-np.cos(a)) * np.linalg.matrix_power(W, 2)",
  "def normal_matrix(pts=None, normal=None):\n    \"\"\" Compute the normal projection matrix of a plane.\n\n    The algorithm assume that the points lie on a plane.\n    Three non-aligned points are required.\n\n    Either points or normal are mandatory.\n\n    Parameters:\n    pts (optional): np.ndarray, 3xn, the points. Need n > 2.\n    normal (optional): np.array, 1x3, the normal.\n\n    Returns:\n    normal matrix: np.array, 3x3, the normal matrix.\n\n    \"\"\"\n    if normal is not None:\n        normal = normal / np.linalg.norm(normal)\n    elif pts is not None:\n        normal = compute_normal(pts)\n    else:\n        assert False, \"Points or normal are mandatory\"\n\n    return np.tensordot(normal, normal, axes=0)",
  "def tangent_matrix(pts=None, normal=None):\n    \"\"\" Compute the tangential projection matrix of a plane.\n\n    The algorithm assume that the points lie on a plane.\n    Three non-aligned points are required.\n\n    Either points or normal are mandatory.\n\n    Parameters:\n    pts (optional): np.ndarray, 3xn, the points. Need n > 2.\n    normal (optional): np.array, 1x3, the normal.\n\n    Returns:\n    tangential matrix: np.array, 3x3, the tangential matrix.\n\n    \"\"\"\n    return np.eye(3) - normal_matrix(pts, normal)",
  "def compute_normal(pts):\n    \"\"\" Compute the normal of a set of points.\n\n    The algorithm assume that the points lie on a plane.\n    Three non-aligned points are required.\n\n    Parameters:\n    pts: np.ndarray, 3xn, the points. Need n > 2.\n\n    Returns:\n    normal: np.array, 1x3, the normal.\n\n    \"\"\"\n\n    assert pts.shape[1] > 2\n    normal = np.cross(pts[:, 0] - pts[:, 1], compute_tangent(pts))\n    if np.allclose(normal, np.zeros(3)):\n        return compute_normal(pts[:, 1:])\n    return normal / np.linalg.norm(normal)",
  "def compute_normals_1d(pts):\n    t = compute_tangent(pts)\n    n = np.array([t[1], -t[0], 0]) / np.sqrt(t[0]**2+t[1]**2)\n    return np.r_['1,2,0', n, np.dot(rot(np.pi/2., t), n)]",
  "def compute_tangent(pts):\n    \"\"\" Compute a tangent vector of a set of points.\n\n    The algorithm assume that the points lie on a plane.\n\n    Parameters:\n    pts: np.ndarray, 3xn, the points.\n\n    Returns:\n    tangent: np.array, 1x3, the tangent.\n\n    \"\"\"\n\n    mean_pts = np.mean(pts, axis=1).reshape((-1, 1))\n    # Set of possible tangent vector. We can pick any of these, as long as it\n    # is nonzero\n    tangent = pts - mean_pts\n    # Find the point that is furthest away from the mean point\n    max_ind = np.argmax(np.sum(tangent**2, axis=0))\n    tangent = tangent[:, max_ind]\n    assert not np.allclose(tangent, np.zeros(3))\n    return tangent / np.linalg.norm(tangent)",
  "def is_collinear(pts, tol=1e-5):\n    \"\"\" Check if the points lie on a line.\n\n    Parameters:\n        pts (np.ndarray, 3xn): the points.\n        tol (double, optional): Absolute tolerance used in comparison.\n            Defaults to 1e-5.\n\n    Returns:\n        boolean, True if the points lie on a line.\n\n    \"\"\"\n\n    if pts.shape[1] == 1 or pts.shape[1] == 2:\n        return True\n\n    pt0 = pts[:, 0]\n    pt1 = pts[:, 1]\n\n    dist = 1\n    for i in np.arange(pts.shape[1]):\n        for j in np.arange(i+1, pts.shape[1]):\n            dist = max(dist, np.linalg.norm(pts[:, i] - pts[:, j]))\n\n    coll = np.array([np.linalg.norm(np.cross(p - pt0, pt1 - pt0)) \\\n             for p in pts[:, 1:-1].T])/dist\n    return np.allclose(coll, np.zeros(coll.size), atol=tol, rtol=0)",
  "def make_collinear(pts):\n    \"\"\"\n    Given a set of points, return them aligned on a line.\n    Useful to enforce collinearity for almost collinear points. The order of the\n    points remain the same.\n    NOTE: The first point in the list has to be on the extrema of the line.\n\n    Parameter:\n        pts: (3 x num_pts) the input points.\n\n    Return:\n        pts: (3 x num_pts) the corrected points.\n    \"\"\"\n    assert pts.shape[1] > 1\n\n    delta = pts - np.tile(pts[:, 0], (pts.shape[1], 1)).T\n    dist = np.sqrt(np.einsum('ij,ij->j', delta, delta))\n    end = np.argmax(dist)\n\n    dist /= dist[end]\n\n    return pts[:, 0, np.newaxis]*(1-dist) + pts[:, end, np.newaxis]*dist",
  "def map_grid(g, tol=1e-5):\n    \"\"\" If a 2d or a 1d grid is passed, the function return the cell_centers,\n    face_normals, and face_centers using local coordinates. If a 3d grid is\n    passed nothing is applied. The return vectors have a reduced number of rows.\n\n    Parameters:\n    g (grid): the grid.\n\n    Returns:\n    cell_centers: (g.dim x g.num_cells) the mapped centers of the cells.\n    face_normals: (g.dim x g.num_faces) the mapped normals of the faces.\n    face_centers: (g.dim x g.num_faces) the mapped centers of the faces.\n    R: (3 x 3) the rotation matrix used.\n    dim: indicates which are the dimensions active.\n    nodes: (g.dim x g.num_nodes) the mapped nodes.\n\n    \"\"\"\n    cell_centers = g.cell_centers\n    face_normals = g.face_normals\n    face_centers = g.face_centers\n    nodes = g.nodes\n    R = np.eye(3)\n\n    if g.dim == 0 or g.dim == 3:\n        return cell_centers, face_normals, face_centers, R,\\\n               np.ones(3, dtype=bool), nodes\n\n    if g.dim == 1 or g.dim == 2:\n\n        if g.dim == 2:\n            R = project_plane_matrix(g.nodes)\n        else:\n            R = project_line_matrix(g.nodes)\n\n        face_centers = np.dot(R, face_centers)\n\n        check = np.sum(np.abs(face_centers.T - face_centers[:, 0]), axis=0)\n        check /= np.sum(check)\n        dim = np.logical_not(np.isclose(check, 0, atol=tol, rtol=0))\n        assert g.dim == np.sum(dim)\n        face_centers = face_centers[dim, :]\n        cell_centers = np.dot(R, cell_centers)[dim, :]\n        face_normals = np.dot(R, face_normals)[dim, :]\n        nodes = np.dot(R, nodes)[dim, :]\n\n    return cell_centers, face_normals, face_centers, R, dim, nodes",
  "def dist_segment_set(start, end):\n    \"\"\" Compute distance and closest points between sets of line segments.\n\n    Parameters:\n        start (np.array, nd x num_segments): Start points of segments.\n        end (np.array, nd x num_segments): End points of segments.\n\n    Returns:\n        np.array, num_segments x num_segments: Distances between segments.\n        np.array, num_segments x num_segments x nd: For segment i and j,\n            element [i, j] gives the point on i closest to segment j.\n\n    \"\"\"\n    if start.size < 4:\n        start = start.reshape((-1, 1))\n    if end.size < 4:\n        end = end.reshape((-1, 1))\n\n    nd = start.shape[0]\n    ns = start.shape[1]\n\n    d = np.zeros((ns, ns))\n    cp = np.zeros((ns, ns, nd))\n\n    for i in range(ns):\n        cp[i, i, :] = start[:, i] + 0.5 * (end[:, i] - start[:, i])\n        for j in range(i+1, ns):\n            dl, cpi, cpj = dist_two_segments(start[:, i], end[:, i],\n                                             start[:, j], end[:, j])\n            d[i, j] = dl\n            d[j, i] = dl\n            cp[i, j, :] = cpi\n            cp[j, i, :] = cpj\n\n    return d, cp",
  "def dist_segment_segment_set(start, end, start_set, end_set):\n    \"\"\" Compute distance and closest points between a segment and a set of\n    segments.\n\n    Parameters:\n\n    \"\"\"\n    start = np.squeeze(start)\n    end = np.squeeze(end)\n\n    nd = start.shape[0]\n    ns = start_set.shape[1]\n\n    d = np.zeros( ns)\n    cp_set = np.zeros(( nd, ns))\n    cp = np.zeros((nd, ns))\n\n    for i in range(ns):\n        dl, cpi, cpj = dist_two_segments(start, end, start_set[:, i],\n                                         end_set[:, i])\n        d[i] = dl\n        cp[:, i] = cpi\n        cp_set[:, i] = cpj\n\n    return d, cp, cp_set",
  "def dist_two_segments(s1_start, s1_end, s2_start, s2_end):\n    \"\"\"\n    Compute the distance between two line segments.\n\n    Also find the closest point on each of the two segments. In the case where\n    the closest points are not unique (parallel lines), points somewhere along\n    the segments are returned.\n\n    The implementaion is based on http://geomalgorithms.com/a07-_distance.html\n    (C++ code can be found somewhere on the page). Also confer that page for\n    explanation of the algorithm.\n\n    Implementation note:\n        It should be possible to rewrite the algorithm to allow for one of (or\n        both?) segments to be a set of segments, thus exploiting\n        vectorization.\n\n    Parameters:\n        s1_start (np.array, size nd): Start point for the first segment\n        s1_end (np.array, size nd): End point for the first segment\n        s2_start (np.array, size nd): Start point for the second segment\n        s2_end (np.array, size nd): End point for the second segment\n\n    Returns:\n        double: Minimum distance between the segments\n        np.array (size nd): Closest point on the first segment\n        np.array (size nd): Closest point on the second segment\n\n    \"\"\"\n\n    # Variable used to fine almost parallel lines. Sensitivity to this value has not been tested.\n    SMALL_TOLERANCE = 1e-6\n\n    # For the rest of the algorithm, see the webpage referred to above for details.\n    d1 = s1_end - s1_start\n    d2 = s2_end - s2_start\n    d_starts = s1_start - s2_start\n\n    dot_1_1 = d1.dot(d1)\n    dot_1_2 = d1.dot(d2)\n    dot_2_2 = d2.dot(d2)\n    dot_1_starts = d1.dot(d_starts)\n    dot_2_starts = d2.dot(d_starts)\n    discr = dot_1_1 * dot_2_2 - dot_1_2 ** 2\n    # Sanity check\n    assert discr >= -SMALL_TOLERANCE * dot_1_1 * dot_2_2\n\n    sc = sN = sD = discr\n    tc = tN = tD = discr\n\n    if discr < SMALL_TOLERANCE:\n        sN = 0\n        sD = 1\n        tN = dot_2_starts\n        tD = dot_2_2\n    else:\n        sN = dot_1_2 * dot_2_starts - dot_2_2 * dot_1_starts\n        tN = dot_1_1 * dot_2_starts - dot_1_2 * dot_1_starts\n        if sN < 0.0:        # sc < 0 => the s=0 edge is visible\n            sN = 0.0\n            tN = dot_2_starts\n            tD = dot_2_2\n\n        elif sN > sD:   # sc > 1  => the s=1 edge is visible\n            sN = sD\n            tN = dot_1_2 + dot_2_starts\n            tD = dot_2_2\n\n    if tN < 0.0:            # tc < 0 => the t=0 edge is visible\n        tN = 0.0\n        # recompute sc for this edge\n        if -dot_1_starts < 0.0:\n            sN = 0.0\n        elif -dot_1_starts > dot_1_1:\n            sN = sD\n        else:\n            sN = -dot_1_starts\n            sD = dot_1_1\n    elif tN > tD:       # tc > 1  => the t=1 edge is visible\n        tN = tD\n        # recompute sc for this edge\n        if (-dot_1_starts + dot_1_2) < 0.0:\n            sN = 0\n        elif (-dot_1_starts+ dot_1_2) > dot_1_1:\n            sN = sD\n        else:\n            sN = (-dot_1_starts + dot_1_2)\n            sD = dot_1_1\n\n    # finally do the division to get sc and tc\n    if abs(sN) < SMALL_TOLERANCE:\n        sc = 0.0\n    else:\n        sc = sN / sD\n    if abs(tN) < SMALL_TOLERANCE:\n        tc = 0.0\n    else:\n        tc = tN / tD\n\n    # get the difference of the two closest points\n    dist = d_starts + sc * d1 - tc * d2\n\n    cp1 = s1_start + d1 * sc\n    cp2 = s2_start + d2 * tc\n\n    return np.sqrt(dist.dot(dist)), cp1, cp2",
  "def dist_points_segments(p, start, end):\n    \"\"\" Compute distances between points and line segments.\n\n    Also return closest points on the segments.\n\n    Parameters:\n        p (np.array, ndxn): Individual points\n        start (np.ndarray, nd x n_segments): Start points of segments.\n        end (np.ndarray, nd x n_segments): End point of segments\n\n    Returns:\n        np.array, num_points x num_segments: Distances.\n        np.array, num-points x num_segments x nd: Points on the segments\n            closest to the individual points.\n\n    \"\"\"\n    if start.size < 4:\n        start = start.reshape((-1, 1))\n        end = end.reshape((-1, 1))\n    if p.size < 4:\n        p = p.reshape((-1, 1))\n\n    num_p = p.shape[1]\n    num_l = start.shape[1]\n    d = np.zeros((num_p, num_l))\n\n    line = end - start\n    lengths = np.sqrt(np.sum(line * line, axis=0))\n\n    nd = p.shape[0]\n    # Closest points\n    cp = np.zeros((num_p, num_l, nd))\n\n    for pi in range(num_p):\n        # Project the vectors from start to point onto the line, and compute\n        # relative length\n        v = p[:, pi].reshape((-1, 1)) - start\n        proj = np.sum(v * line, axis=0) / lengths**2\n\n        # Projections with length less than zero have the closest point at\n        # start\n        less = np.ma.less_equal(proj, 0)\n        d[pi, less] = dist_point_pointset(p[:, pi], start[:, less])\n        cp[pi, less, :] = np.swapaxes(start[:, less], 1, 0)\n        # Similarly, above one signifies closest to end\n        above = np.ma.greater_equal(proj, 1)\n        d[pi, above] = dist_point_pointset(p[:, pi], end[:, above])\n        cp[pi, above, :] = np.swapaxes(end[:, above], 1, 0)\n\n        # Points inbetween\n        between = np.logical_not(np.logical_or(less, above).data)\n        proj_p = start[:, between] + proj[between] * line[:, between]\n        d[pi, between] = dist_point_pointset(p[:, pi], proj_p)\n        cp[pi, between, :] = np.swapaxes(proj_p, 1, 0)\n\n    return d, cp",
  "def dist_point_pointset(p, pset, exponent=2):\n    \"\"\"\n    Compute distance between a point and a set of points.\n\n    Parameters:\n        p (np.ndarray): Point from which distances will be computed\n        pset (nd.array): Point cloud to which we compute distances\n        exponent (double, optional): Exponent of the norm used. Defaults to 2.\n\n    Return:\n        np.ndarray: Array of distances.\n\n    \"\"\"\n\n    # If p is 1D, do a reshape to facilitate broadcasting, but on a copy\n    if p.ndim == 1:\n        pt = p.reshape((-1, 1))\n    else:\n        pt = p\n\n    # If the point cloud is a single point, it should still be a ndx1 array.\n    if pset.size == 0:\n        # In case of empty sets, return an empty zero\n        return np.zeros(0)\n    elif pset.size < 4:\n        pset_copy = pset.reshape((-1, 1))\n    else:\n        # Call it a copy, even though it isn't\n        pset_copy = pset\n\n    return np.power(np.sum(np.power(np.abs(pt - pset_copy), exponent),\n                           axis=0), 1/exponent)",
  "def dist_pointset(p, max_diag=False):\n    \"\"\" Compute mutual distance between all points in a point set.\n\n    Parameters:\n        p (np.ndarray, 3xn): Points\n        max_diag (boolean, defaults to True): If True, the diagonal values will\n            are set to a large value, rather than 0.\n\n    Returns:\n        np.array (nxn): Distance between points.\n    \"\"\"\n    if p.size > 3:\n        n = p.shape[1]\n    else:\n        n = 1\n        p = p.reshape((-1, 1))\n\n    d = np.zeros((n, n))\n    for i in range(n):\n        d[i] = dist_point_pointset(p[:, i], p)\n\n    if max_diag:\n        for i in range(n):\n            d[i, i] = 2 * np.max(d[i])\n\n    return d",
  "def dist_points_polygon(p, poly, tol=1e-5):\n    \"\"\" Compute distance from points to a polygon. Also find closest point on\n    the polygon.\n\n    The computation is split into two, the closest point can either be in the\n    interior, or at the boundary of the point.\n\n    Parameters:\n        p (np.array, nd x n_pts): Points for which we will compute distances.\n        poly (np.array, nd x n_vertexes): Vertexes of polygon. Edges are formed\n            by subsequent points.\n\n    Returns:\n        np.array (n_pts): Distance from points to polygon\n        np.array (nd x n_pts): For each point, the closest point on the\n            polygon.\n        np.array (n_pts, bool): True if the point is found in the interior,\n            false if on a bounding segment.\n\n    \"\"\"\n\n\n    if p.size < 4:\n        p.reshape((-1, 1))\n\n    num_p = p.shape[1]\n    num_vert = poly.shape[1]\n    nd = p.shape[0]\n\n    # First translate the points so that the first plane is located at the origin\n    center = np.mean(poly, axis=1).reshape((-1, 1))\n    # Compute copies of polygon and point in new coordinate system\n    orig_poly = poly\n    poly = poly - center\n    orig_p = p\n    p = p - center\n\n    # Obtain the rotation matrix that projects p1 to the xy-plane\n    rot_p = project_plane_matrix(poly)\n    irot = rot_p.transpose()\n    poly_rot = rot_p.dot(poly)\n\n    # Sanity check: The points should lay on a plane\n    assert np.all(np.abs(poly_rot[2]) < tol)\n\n    poly_xy = poly_rot[:2]\n\n    # Make sure the xy-polygon is ccw.\n    if not is_ccw_polygon(poly_xy):\n        poly_1_xy = poly_xy[:, ::-1]\n\n    # Rotate the point set, using the same coordinate system.\n    p = rot_p.dot(p)\n\n    in_poly = is_inside_polygon(poly_xy, p)\n\n    # Distances\n    d = np.zeros(num_p)\n    # Closest points\n    cp = np.zeros((nd, num_p))\n\n    # For points that are located inside the extrusion of the polygon, the\n    # distance is simply the z-coordinate\n    d[in_poly] = np.abs(p[2, in_poly])\n\n    # The corresponding closest points are found by inverse rotation of the\n    # closest point on the polygon\n    cp_inpoly = p[:, in_poly]\n    if cp_inpoly.size == 3:\n        cp_inpoly = cp_inpoly.reshape((-1, 1))\n    cp_inpoly[2, :] = 0\n    cp[:, in_poly] = center + irot.dot(cp_inpoly)\n\n    if np.all(in_poly):\n        return d, cp, in_poly\n\n    # Next, points that are outside the extruded polygons. These will have\n    # their closest point among one of the edges\n    start = orig_poly\n    end = orig_poly[:, (1 + np.arange(num_vert)) % num_vert]\n\n    outside_poly = np.where(np.logical_not(in_poly))[0]\n\n    d_outside, p_outside = dist_points_segments(orig_p[:, outside_poly], start,\n                                                end)\n\n    for i, pi in enumerate(outside_poly):\n        mi = np.argmin(d_outside[i])\n        d[pi] = d_outside[i, mi]\n        cp[:, pi] = p_outside[i, mi, :]\n\n    return d, cp, in_poly",
  "def dist_segments_polygon(start, end, poly, tol=1e-5):\n    \"\"\" Compute the distance from line segments to a polygon.\n\n    Parameters:\n        start (np.array, nd x num_segments): One endpoint of segments\n        end (np.array, nd x num_segments): Other endpoint of segments\n        poly (np.array, nd x n_vert): Vertexes of polygon.\n\n    Returns:\n        np.ndarray, double: Distance from segment to polygon.\n        np.array, nd x num_segments: Closest point.\n    \"\"\"\n    if start.size < 4:\n        start = start.reshape((-1, 1))\n    if end.size < 4:\n        end = end.reshape((-1, 1))\n\n    num_p = start.shape[1]\n    num_vert = poly.shape[1]\n    nd = start.shape[0]\n\n    d = np.zeros(num_p)\n    cp = np.zeros((nd, num_p))\n\n    # First translate the points so that the first plane is located at the origin\n    center = np.mean(poly, axis=1).reshape((-1, 1))\n    # Compute copies of polygon and point in new coordinate system\n    orig_poly = poly\n    orig_start = start\n    orig_end = end\n\n    poly = poly - center\n    start  = start - center\n    end = end - center\n\n    # Obtain the rotation matrix that projects p1 to the xy-plane\n    rot_p = project_plane_matrix(poly)\n    irot = rot_p.transpose()\n    poly_rot = rot_p.dot(poly)\n\n    # Sanity check: The points should lay on a plane\n    assert np.all(np.abs(poly_rot[2]) < tol)\n\n    poly_xy = poly_rot[:2]\n\n    # Make sure the xy-polygon is ccw.\n    if not is_ccw_polygon(poly_xy):\n        poly_1_xy = poly_xy[:, ::-1]\n\n    # Rotate the point set, using the same coordinate system.\n    start = rot_p.dot(start)\n    end = rot_p.dot(end)\n\n    dz = end[2] - start[2]\n    non_zero_incline = np.abs(dz) > tol\n\n    # Parametrization along line of intersection point\n    t = 0 * dz\n\n    # Intersection point for segments with non-zero incline\n    t[non_zero_incline] = -start[2, non_zero_incline]\\\n                          / dz[non_zero_incline]\n    # Segments with z=0 along the segment\n    zero_along_segment = np.logical_and(non_zero_incline,\n                                        np.logical_and(t>=0,\n                                                       t<=1).astype(np.bool))\n\n    x0 = start +  (end - start) * t\n    # Check if zero point is inside the polygon\n    inside = is_inside_polygon(poly_xy, x0[:2])\n    crosses = np.logical_and(inside, zero_along_segment)\n\n    # For points with zero incline, the z-coordinate should be zero for the\n    # point to be inside\n    segment_in_plane = np.logical_and(np.abs(start[2]) < tol,\n                                      np.logical_not(non_zero_incline))\n    # Check if either start or endpoint is inside the polygon. This leaves the\n    # option of the segment crossing the polygon within the plane, but this\n    # will be handled by the crossing of segments below\n    endpoint_in_polygon = np.logical_or(is_inside_polygon(poly_xy, start[:2]),\n                                        is_inside_polygon(poly_xy, end[:2]))\n\n    segment_in_polygon = np.logical_and(segment_in_plane, endpoint_in_polygon)\n\n    intersects = np.logical_or(crosses, segment_in_polygon)\n\n    x0[2, intersects] = 0\n    cp[:, intersects] = center + irot.dot(x0[:, intersects])\n\n    # Check if we're done, or if we should consider proximity to polygon\n    # segments\n    if np.all(intersects):\n        # The distance is known to be zero, so no need to set it\n        return d, cp\n\n    not_found = np.where(np.logical_not(intersects))[0]\n\n    # If we reach this, the minimum is not zero for all segments. The point\n    # with minimum distance is then either 1) one endpoint of the segments\n    # (point-polygon), 2) found as a segment-segment minimum (segment and\n    # boundary of polygon), or 3) anywhere along the segment parallel with\n    # polygon.\n    poly = orig_poly\n    start = orig_start\n    end = orig_end\n\n    # Distance from endpoints to\n    d_start_poly, cp_s_p, s_in_poly = dist_points_polygon(start, poly)\n    d_end_poly, cp_e_p, e_in_poly = dist_points_polygon(end, poly)\n\n    # Loop over all segments that did not cross the polygon. The minimum is\n    # found either by the endpoints, or as between two segments.\n    for si in not_found:\n        md = d_start_poly[si]\n        cp_l = cp_s_p\n\n        if d_end_poly[si] < md:\n            md = d_end_poly\n            cp_l = cp_e_p\n\n        # Loop over polygon segments\n        for poly_i in range(num_vert):\n            ds, cp_s, _ = dist_two_segments(start[:, si], end[:, si],\n                                            poly[:, poly_i],\n                                            poly[:, (poly_i+1) % num_vert])\n            if ds < md:\n                md = ds\n                cp_l = cp_s\n\n        # By now, we have found the minimum\n        d[si] = md\n        cp[:, si] = cp_l.reshape((1, -1))\n\n    return d, cp",
  "def distance_point_segment(pt, start, end):\n    \"\"\"\n    Compute the minimum distance between a point and a segment.\n\n    Parameters:\n        pt: the point\n        start: a point representing one extreme of the segment.\n        end: the second point representing the segment.\n    Returns:\n        distance: the minimum distance between the point and the segment.\n        intersect: point of intersection\n    \"\"\"\n    pt_shift = end - start\n    length = np.dot(pt_shift, pt_shift)\n    u = np.dot(pt - start, pt_shift) / (length if length != 0 else 1)\n    dx = start + np.clip(u, 0., 1.) * pt_shift - pt\n\n    return np.sqrt(np.dot(dx, dx)), dx + pt",
  "def snap_points_to_segments(p_edges, edges, tol, p_to_snap=None):\n    \"\"\"\n    Snap points in the proximity of lines to the lines.\n\n    Note that if two vertices of two edges are close, they may effectively\n    be co-located by the snapping. Thus, the modified point set may have\n    duplicate coordinates.\n\n    Parameters:\n        p_edges (np.ndarray, nd x npt): Points defining endpoints of segments\n        edges (np.ndarray, 2 x nedges): Connection between lines in p_edges.\n            If edges.shape[0] > 2, the extra rows are ignored.\n        tol (double): Tolerance for snapping, points that are closer will be\n            snapped.\n        p_to_snap (np.ndarray, nd x npt_snap, optional): The points to snap. If\n            not provided, p_edges will be snapped, that is, the lines will be\n            modified.\n\n    Returns:\n        np.ndarray (nd x n_pt_snap): A copy of p_to_snap (or p_edges) with\n            modified coordinates.\n\n    \"\"\"\n\n    if p_to_snap is None:\n        p_to_snap = p_edges\n        mod_edges = True\n    else:\n        mod_edges = False\n\n    pn = p_to_snap.copy()\n\n    nl = edges.shape[1]\n    for ei in range(nl):\n\n        # Find start and endpoint of this segment.\n        # If we modify the edges themselves (mod_edges==True), we should use\n        # the updated point coordinates. If not, we risk trouble for almost\n        # coinciding vertexes.\n        if mod_edges:\n            p_start = pn[:, edges[0, ei]].reshape((-1, 1))\n            p_end = pn[:, edges[1, ei]].reshape((-1, 1))\n        else:\n            p_start = p_edges[:, edges[0, ei]].reshape((-1, 1))\n            p_end = p_edges[:, edges[1, ei]].reshape((-1, 1))\n        d_segment, cp = dist_points_segments(pn, p_start, p_end)\n        hit = np.argwhere(d_segment[:, 0] < tol)\n        for i in hit:\n            if mod_edges and (i == edges[0, ei] or i == edges[1, ei]):\n                continue\n            pn[:, i] = cp[i, 0, :].reshape((-1, 1))\n    return pn",
  "def argsort_point_on_line(pts, tol=1e-5):\n    \"\"\"\n    Return the indexes of the point according to their position on a line.\n    The first point in the list has to be on of the extrema of the line.\n\n    Parameters:\n        pts: the list of points\n    Returns:\n        argsort: the indexes of the points\n    \"\"\"\n    if pts.shape[1] == 1:\n        return np.array([0])\n    assert is_collinear(pts, tol)\n    delta = np.tile(pts[:, 0], (pts.shape[1], 1)).T - pts\n    return np.argsort(np.abs(np.einsum('ij,ij->j', delta, delta)))",
  "def bounding_box(pts, overlap=0):\n    \"\"\" Obtain a bounding box for a point cloud.\n\n    Parameters:\n        pts: np.ndarray (nd x npt). Point cloud. nd should be 2 or 3\n        overlap (double, defaults to 0): Extension of the bounding box outside\n            the point cloud. Scaled with extent of the point cloud in the\n            respective dimension.\n\n    Returns:\n        domain (dictionary): Containing keywords xmin, xmax, ymin, ymax, and\n            possibly zmin and zmax (if nd == 3)\n\n    \"\"\"\n    max_coord = pts.max(axis=1)\n    min_coord = pts.min(axis=1)\n    dx = max_coord - min_coord\n    domain = {'xmin': min_coord[0] - dx[0] * overlap,\n              'xmax': max_coord[0] + dx[0] * overlap,\n              'ymin': min_coord[1] - dx[1] * overlap,\n              'ymax': max_coord[1] + dx[1] * overlap}\n\n    if max_coord.size == 3:\n        domain['zmin'] = min_coord[2] - dx[2] * overlap\n        domain['zmax'] = max_coord[2] + dx[2] * overlap\n    return domain",
  "def __min_dist(p):\n                md = np.inf\n                for pi in [edges[0, edge_counter],\n                           edges[1, edge_counter],\n                           edges[0, intsect], edges[1, intsect]]:\n                    md = min(md, dist_point_pointset(p, vertices[:, pi]))\n                return md",
  "def sort_point_pairs(lines, check_circular=True, ordering=False, is_circular=True):\n    \"\"\" Sort pairs of numbers to form a chain.\n\n    The target application is to sort lines, defined by their\n    start end endpoints, so that they form a continuous polyline.\n\n    The algorithm is brute-force, using a double for-loop. This can\n    surely be imporved.\n\n    Parameters:\n    lines: np.ndarray, 2xn, the line pairs.\n    check_circular: Verify that the sorted polyline form a circle.\n                    Defaluts to true.\n    ordering: np.array, return in the original order if a line is flipped or not\n    is_circular: if the lines form a closed set. Default is True.\n\n    Returns:\n    sorted_lines: np.ndarray, 2xn, sorted line pairs.\n\n    \"\"\"\n\n    num_lines = lines.shape[1]\n    sorted_lines = -np.ones((2, num_lines), dtype=lines.dtype)\n\n    # Start with the first line in input\n    sorted_lines[:, 0] = lines[:, 0]\n\n    # In the case of non-circular ordering ensure to start from the correct one\n    if not is_circular:\n        check_circular = False\n        if np.count_nonzero(lines==sorted_lines[0, 0]) > 1:\n            sorted_lines[:, 0] = np.flip(sorted_lines[:, 0], 0)\n\n    # The starting point for the next line\n    prev = sorted_lines[1, 0]\n\n    # Keep track of which lines have been found, which are still candidates\n    found = np.zeros(num_lines, dtype=np.bool)\n    found[0] = True\n\n    # Order of the origin line list, store if they're flip or not to form the chain\n    is_ordered = np.zeros(num_lines, dtype=np.bool)\n    is_ordered[0] = True\n\n    # The sorting algorithm: Loop over all places in sorted_line to be filled,\n    # for each of these, loop over all members in lines, check if the line is still\n    # a candidate, and if one of its points equals the current starting point.\n    # More efficient and more elegant approaches can surely be found, but this\n    # will do for now.\n    for i in range(1, num_lines):  # The first line has already been found\n        for j in range(0, num_lines):\n            if not found[j] and lines[0, j] == prev:\n                sorted_lines[:, i] = lines[:, j]\n                found[j] = True\n                prev = lines[1, j]\n                is_ordered[j] = True\n                break\n            elif not found[j] and lines[1, j] == prev:\n                sorted_lines[:, i] = lines[::-1, j]\n                found[j] = True\n                prev = lines[0, j]\n                break\n    # By now, we should have used all lines\n    assert np.all(found)\n    if check_circular:\n        assert sorted_lines[0, 0] == sorted_lines[1, -1]\n    if ordering:\n        return sorted_lines, is_ordered\n    return sorted_lines",
  "def sort_point_plane(pts, centre, normal=None):\n    \"\"\" Sort the points which lie on a plane.\n\n    The algorithm assumes a star-shaped disposition of the points with respect\n    the centre.\n\n    Parameters:\n    pts: np.ndarray, 3xn, the points.\n    centre: np.ndarray, 3x1, the face centre.\n    normal: (optional) the normal of the plane, otherwise three points are\n    required.\n\n    Returns:\n    map_pts: np.array, 1xn, sorted point ids.\n\n    \"\"\"\n    R = project_plane_matrix(pts, normal)\n    pts = np.array([np.dot(R, p) for p in pts.T]).T\n    centre = np.dot(R, centre)\n    delta = np.array([p - centre for p in pts.T]).T[0:2, :]\n    delta = np.array([d / np.linalg.norm(d) for d in delta.T]).T\n    return np.argsort(np.arctan2(*delta))",
  "def multinary_permutations(base, length):\n    \"\"\"\n    Define a generator over all numbers of a certain length for a number system\n    with a specified base.\n\n    For details on the decomposition into an arbitrary base see\n        http://math.stackexchange.com/questions/111150/changing-a-number-between-arbitrary-bases\n\n    Note that the generator will loop over base**length combinations.\n\n    Examples:\n\n        Construct the numbers [00] to [11] in binary numbers\n        >>> multinary_permutations(2, 2)\n        [array([ 0.,  0.]), array([ 1.,  0.]), array([ 0.,  1.]), array([ 1.,  1.])]\n\n        Construct the numbers from 0 to 99 (permuted) in the decimal number\n        system.\n        >>> it = multinary_permutation(10, 2)\n\n    Parameters:\n        base (int): Base of the number system\n        length (int): Number of digits in the numbers\n\n    Yields:\n        array, size length: Array describing the next number combination.\n\n    \"\"\"\n\n    # There are in total base ** length numbers to be covered, these need to be\n    # rewritten into the base number system\n    for iter1 in range(base **  length):\n\n        # Array to store the multi-d index of the current index\n        bit_val = [0] * length\n        # Number to be decomposed\n        v = iter1\n\n        # Loop over all digits, find the expression of v in that system\n        for iter2 in range(length):\n            bit_val[iter2] = v % base\n            v = v // base\n        # Yield the next value\n        yield bit_val",
  "def interpolate(g, fun):\n    \"\"\"\n    Interpolate a scalar or vector function on the cell centers of the grid.\n\n    Parameters\n    ----------\n    g : grid\n        Grid, or a subclass, with geometry fields computed.\n    fun : function\n        Scalar or vector function.\n\n    Return\n    ------\n    out: np.ndarray (dim of fun, g.num_cells)\n        Function interpolated in the cell centers.\n\n    Examples\n    --------\n    def fun_p(pt): return np.sin(2*np.pi*pt[0])*np.sin(2*np.pi*pt[1])\n    def fun_u(pt): return [\\\n                      -2*np.pi*np.cos(2*np.pi*pt[0])*np.sin(2*np.pi*pt[1]),\n                      -2*np.pi*np.sin(2*np.pi*pt[0])*np.cos(2*np.pi*pt[1])]\n    p_ex = interpolate(g, fun_p)\n    u_ex = interpolate(g, fun_u)\n\n    \"\"\"\n\n    return np.array([fun(pt) for pt in g.cell_centers.T]).T",
  "def norm_L2(g, val):\n    \"\"\"\n    Compute the L2 norm of a scalar or vector field.\n\n    Parameters\n    ----------\n    g : grid\n        Grid, or a subclass, with geometry fields computed.\n    val : np.ndarray (dim of val, g.num_cells)\n        Scalar or vector field.\n\n    Return\n    ------\n    out: double\n        The L2 norm of the input field.\n\n    Examples\n    --------\n    def fun_p(pt): return np.sin(2*np.pi*pt[0])*np.sin(2*np.pi*pt[1])\n    p_ex = interpolate(g, fun_p)\n    norm_ex = norm_L2(g, p_ex)\n\n    \"\"\"\n\n    val = np.asarray(val)\n    norm_sq = lambda v: np.sum(np.multiply(np.square(v), g.cell_volumes))\n    if val.ndim == 1:\n        return np.sqrt(norm_sq(val))\n    return np.sqrt(np.sum([norm_sq(v) for v in val]))",
  "def error_L2(g, val, val_ex, relative=True):\n    \"\"\"\n    Compute the L2 error of a scalar or vector field with respect to a reference\n    field. It is possible to compute the relative error (default) or the\n    absolute error.\n\n    Parameters\n    ----------\n    g : grid\n        Grid, or a subclass, with geometry fields computed.\n    val : np.ndarray (dim of val, g.num_cells)\n        Scalar or vector field.\n    val_ex : np.ndarray (dim of val, g.num_cells)\n        Reference scalar or vector field, i.e. the exact solution\n    relative: bool (True default)\n        Compute the relative error (if True) or the absolute error (if False).\n\n    Return\n    ------\n    out: double\n        The L2 error of the input fields.\n\n    Examples\n    --------\n    p = ...\n    def fun_p(pt): return np.sin(2*np.pi*pt[0])*np.sin(2*np.pi*pt[1])\n    p_ex = interpolate(g, fun_p)\n    err_p = err_L2(g, p, p_ex)\n\n    \"\"\"\n\n    val, val_ex = np.asarray(val), np.asarray(val_ex)\n    err = norm_L2(g, np.subtract(val, val_ex))\n    den = norm_L2(g, val_ex) if relative else 1\n    assert den != 0\n    return err/den",
  "def determine_mesh_size(pts, lines=None, **kwargs):\n    \"\"\"\n    Set the preferred mesh size for geometrical points as specified by\n    gmsh. Currently, two options are available:\n    - specify a single value for all fracture points, and one value for\n      the boundary.\n    - to weighed the mesh size depending of the fracture geometry. Useful for\n      complex network of fractures.\n\n    See the gmsh manual for further details.\n\n    \"\"\"\n    mode = kwargs.get('mode', 'constant')\n\n    if mode == 'weighted':\n        return __weighted_determine_mesh_size(pts, lines, **kwargs)\n    elif mode == 'constant':\n        return __constant_determine_mesh_size(pts, lines, **kwargs)\n    else:\n        raise ValueError('Unknown mesh size mode ' + mode)",
  "def __constant_determine_mesh_size(pts, lines, **kwargs):\n    num_pts = pts.shape[1]\n    val = kwargs.get('value', None)\n    bound_val = kwargs.get('bound_value', None)\n\n    if val is not None:\n        mesh_size = val * np.ones(num_pts)\n    else:\n        mesh_size = None\n\n    if bound_val is not None:\n        mesh_size_bound = bound_val\n    else:\n        mesh_size_bound = None\n\n    if lines is None:\n        return mesh_size, mesh_size_bound\n    else:\n        return mesh_size, mesh_size_bound, pts, lines",
  "def __weighted_determine_mesh_size(pts, lines, **kwargs):\n    num_pts = pts.shape[1]\n    val = kwargs.get('value', 1)\n    bound_val = kwargs.get('bound_value', None)\n    tol = kwargs.get('tol', 1e-5)\n\n    # Compute the lenght of each pair of points (fractures + domain boundary)\n    pts_id = lines[:2, :]\n    dist = np.linalg.norm(pts[:, pts_id[0, :]] - pts[:, pts_id[1, :]],\n                          axis=0)\n    dist_pts = np.tile(np.inf, pts.shape[1])\n\n    # Loop on all the points and consider the minimum between the pairs of points\n    # lengths associated to the single point and the value input by the user\n    for i, pt_id in enumerate(pts_id.T):\n        distances = np.array([dist_pts[pt_id], [dist[i]]*2, [val]*2])\n        dist_pts[pt_id] = np.amin(distances, axis=0)\n\n    num_pts = pts.shape[1]\n    num_lines = lines.shape[1]\n    pts_extra = np.empty((2, 0))\n    dist_extra = np.empty(0)\n    pts_id_extra = np.empty(0, dtype=np.int)\n\n    # For each point we compute the distance between the point and the other\n    # pairs of points. We keep the minimum distance between the previously\n    # computed point distance and the distance among the other pairs of points.\n    # If the latter happens, we introduce a new point (useful to determine the\n    # grid size) on the corresponding pair of points with a corresponding\n    # distance.\n    # Loop on all the original points\n    for pt_id, pt in enumerate(pts.T):\n        # Loop on all the original lines\n        for line in lines.T:\n            start, end = pts[:, line[0]], pts[:, line[1]]\n            # Compute the distance between the point and the current line\n            dist, pt_int = cg.distance_point_segment(pt, start, end)\n            # If the distance is small than the input value we need to consider\n            # it\n            if dist < val and not np.isclose(dist, 0.):\n                dist_pts[pt_id] = min(dist_pts[pt_id], dist)\n\n                dist_start = np.linalg.norm(pt_int - start)\n                dist_end = np.linalg.norm(pt_int - end)\n                # Given the internal point on the line, associated to the\n                # distance with the current point, if its distance with the\n                # endings of the line is greater than the distance computed\n                # then we need to keep the point to balance the grid generation.\n                if dist < dist_start and dist < dist_end:\n                    dist_extra = np.r_[dist_extra, min(dist, val)]\n                    pts_extra = np.c_[pts_extra, pt_int]\n                    pts_id_extra = np.r_[pts_id_extra, line[3]]\n\n    old_lines = lines\n    old_pts = pts\n\n    # Since the computation was done point by point with the lines, we need to\n    # consider all the new points together and remove (from the new points) the\n    # useless ones.\n    extra_ids, inv_index = np.unique(pts_id_extra, return_inverse=True)\n    to_remove = np.empty(0, dtype=np.int)\n    for idx, i in enumerate(extra_ids):\n        mask = np.flatnonzero(inv_index == idx)\n        if mask.size > 1:\n            mesh_matrix = np.tile(dist_extra[mask], (mask.size, 1))\n            pos_matrix = np.zeros((mask.size, mask.size))\n            dist_matrix = np.ones((mask.size, mask.size))*np.inf\n\n            for pt1_id_loc in np.arange(mask.size):\n                for pt2_id_loc in np.arange(pt1_id_loc+1, mask.size):\n                    pt1_id = mask[pt1_id_loc]\n                    pt2_id = mask[pt2_id_loc]\n                    pt1 = pts_extra[:, pt1_id]\n                    pt2 = pts_extra[:, pt2_id]\n                    pts_id = np.array([pt1_id, pt2_id])\n                    pts_id_loc = np.array([pt1_id_loc, pt2_id_loc])\n                    pos_min = np.argmin(dist_extra[pts_id])\n                    pos_max = np.argmax(dist_extra[pts_id])\n                    dist_matrix[pts_id_loc[pos_min], pts_id_loc[pos_max]] = \\\n                                                   np.linalg.norm(pt1 - pt2)\n\n            to_remove_loc = np.any(dist_matrix < mesh_matrix, axis=0)\n            to_remove = np.r_[to_remove, mask[to_remove_loc]]\n\n    # Remove the useless new points\n    pts_extra = np.delete(pts_extra, to_remove, axis=1)\n    dist_extra = np.delete(dist_extra, to_remove)\n    pts_id_extra = np.delete(pts_id_extra, to_remove)\n\n    # Consider all the points\n    pts = np.c_[pts, pts_extra]\n    dist_pts = np.r_[dist_pts, dist_extra]\n\n    # Re-create the lines, considering the new introduced points\n    seg_ids = np.unique(lines[3, :])\n    new_lines = np.empty((4,0), dtype=np.int)\n    for seg_id in seg_ids:\n        mask_bool = lines[3, :] == seg_id\n        extra_mask_bool = pts_id_extra == seg_id\n        if not np.any(extra_mask_bool):\n            # No extra points are considered for the current line\n            new_lines = np.c_[new_lines, lines[:, mask_bool]]\n        else:\n            # New extra point are considered for the current line, they need to\n            # be sorted along the line.\n            pts_frac_id = np.hstack((lines[0:2, mask_bool].ravel(),\n                                 np.flatnonzero(extra_mask_bool) + num_pts))\n            pts_frac_id = np.unique(pts_frac_id)\n            pts_frac = pts[:, pts_frac_id]\n            pts_frac_id = pts_frac_id[cg.argsort_point_on_line(pts_frac, tol)]\n            pts_frac_id = np.vstack((pts_frac_id[:-1], pts_frac_id[1:]))\n            other_info = np.tile(lines[2:, mask_bool][:, 0],\n                                                (pts_frac_id.shape[1], 1)).T\n            new_lines = np.c_[new_lines, np.vstack((pts_frac_id, other_info))]\n\n    # Consider extra points related to the input value, if the fracture is long\n    # and, beacuse of val, needs additional points we increase the number of\n    # lines.\n    relax = kwargs.get('relaxation', 0.8)\n    lines = np.empty((4, 0), dtype=np.int)\n    for seg in new_lines.T:\n        mesh_size_pt1 = dist_pts[seg[0]]\n        mesh_size_pt2 = dist_pts[seg[1]]\n        dist = np.linalg.norm(pts[:, seg[0]] - pts[:, seg[1]])\n        if (mesh_size_pt1 >= relax*val and mesh_size_pt2 >= relax*val) \\\n           or \\\n           (relax*dist <= 2*mesh_size_pt1 and relax*dist <= 2*mesh_size_pt2):\n            lines = np.c_[lines, seg]\n        else:\n            pt_id = pts.shape[1]\n            new_pt = 0.5*(pts[:, seg[0]] + pts[:, seg[1]])\n            pts = np.c_[pts, new_pt]\n            mesh_size = np.amin([val, dist/2.])\n\n            for old_seg in old_lines.T:\n                start, end = old_pts[:, old_seg[0]], old_pts[:, old_seg[1]]\n                # Compute the distance between the point and the current line\n                dist1, pt_int = cg.distance_point_segment(new_pt, start, end)\n                # If the distance is small than the input value we need to consider\n                # it\n                if dist1 < mesh_size and not np.isclose(dist1, 0.):\n                    mesh_size = dist1\n\n            dist_pts = np.r_[dist_pts, mesh_size]\n            lines = np.c_[lines, [seg[0], pt_id, seg[2], seg[3]],\n                                 [pt_id, seg[1], seg[2], seg[3]]]\n\n\n    return dist_pts, bound_val, pts, lines",
  "def obtain_interdim_mappings(lg, fn, n_per_face,\n                             ensure_matching_face_cell=True, **kwargs):\n    \"\"\"\n    Find mappings between faces in higher dimension and cells in the lower\n    dimension\n\n    Parameters:\n        lg: Lower dimensional grid.\n        fn: Higher dimensional face-node relation.\n        n_per_face: Number of nodes per face in the higher-dimensional grid.\n        ensure_matching_face_cell: Boolean, defaults to True. If True, an\n            assertion is made that all lower-dimensional cells corresponds to a\n            higher dimensional cell.\n\n    \"\"\"\n    if lg.dim > 0:\n        cn_loc = lg.cell_nodes().indices.reshape((n_per_face,\n                                                  lg.num_cells),\n                                                 order='F')\n        cn = lg.global_point_ind[cn_loc]\n        cn = np.sort(cn, axis=0)\n    else:\n        cn = np.array([lg.global_point_ind])\n        # We also know that the higher-dimensional grid has faces\n        # of a single node. This sometimes fails, so enforce it.\n        if cn.ndim == 1:\n            fn = fn.ravel()\n    is_mem, cell_2_face = setmembership.ismember_rows(\n        cn.astype(np.int32), fn.astype(np.int32), sort=False)\n    # An element in cell_2_face gives, for all cells in the\n    # lower-dimensional grid, the index of the corresponding face\n    # in the higher-dimensional structure.\n    if not (np.all(is_mem) or np.all(~is_mem)):\n        if ensure_matching_face_cell:\n            raise ValueError(\n                '''Either all cells should have a corresponding face in a higher\n            dim grid or no cells should have a corresponding face in a higher\n            dim grid. This likely is related to gmsh behavior. ''')\n        else:\n            warnings.warn('''Found inconsistency between cells and higher\n                          dimensional faces. Continuing, fingers crossed''')\n    low_dim_cell = np.where(is_mem)[0]\n    return cell_2_face, low_dim_cell",
  "def simplex_grid(fracs=None, domain=None, network=None, subdomains=[], verbose=0, **kwargs):\n    \"\"\"\n    Main function for grid generation. Creates a fractured simiplex grid in 2\n    or 3 dimensions.\n\n    NOTE: For some fracture networks, what appears to be a bug in Gmsh leads to\n    surface grids with cells that does not have a corresponding face in the 3d\n    grid. The problem may have been resolved (at least partly) by newer\n    versions of Gmsh, but can still be an issue for our purposes. If this\n    behavior is detected, an assertion error is raised. To avoid the issue,\n    and go on with a surface mesh that likely is problematic, kwargs should\n    contain a keyword ensure_matching_face_cell=False.\n\n    Parameters\n    ----------\n    fracs (list of np.ndarray): One list item for each fracture. Each item\n        consist of a (nd x n) array describing fracture vertices. The\n        fractures may be intersecting.\n    domain (dict): Domain specification, determined by xmin, xmax, ...\n    subdomains (list of np.ndarray or list of Fractures): One list item\n        for each fracture, same format as fracs. Specifies internal boundaries\n        for the gridding. Only available in 3D.\n    **kwargs: May contain fracture tags, options for gridding, etc.\n\n    Returns\n    -------\n    GridBucket: A complete bucket where all fractures are represented as\n        lower dim grids. The higher dim fracture faces are split in two,\n        and on the edges of the GridBucket graph the mapping from lower dim\n        cells to higher dim faces are stored as 'face_cells'. Each face is\n        given boolean tags depending on the type:\n           domain_boundary_faces: All faces that lie on the domain boundary\n               (i.e. should be given a boundary condition).\n           fracture_faces: All faces that are split (i.e. has a connection to a\n               lower dim grid).\n           tip_faces: A boundary face that is not on the domain boundary, nor\n               coupled to a lower domentional domain.\n        The union of the above three is the tag boundary_faces.\n\n    Examples\n    --------\n    frac1 = np.array([[1,4],[1,4]])\n    frac2 = np.array([[1,4],[4,1]])\n    fracs = [frac1, frac2]\n    domain = {'xmin': 0, 'ymin': 0, 'xmax':5, 'ymax':5}\n    gb = simplex_grid(fracs, domain)\n\n    \"\"\"\n    if domain is None:\n        ndim = fracs[0].shape[0]\n\n    elif 'zmax' in domain:\n        ndim = 3\n    elif 'ymax' in domain:\n        ndim = 2\n    else:\n        raise ValueError('simplex_grid only supported for 2 or 3 dimensions')\n\n    if verbose > 0:\n        print('Construct mesh')\n        tm_msh = time.time()\n        tm_tot = time.time()\n    # Call relevant method, depending on grid dimensions.\n    if ndim == 2:\n        assert fracs is not None, '2d requires definition of fractures'\n        assert domain is not None, '2d requires definition of domain'\n        # Convert the fracture to a fracture dictionary.\n        if len(fracs) == 0:\n            f_lines = np.zeros((2, 0))\n            f_pts = np.zeros((2, 0))\n        else:\n            f_lines = np.reshape(np.arange(2 * len(fracs)), (2, -1), order='F')\n            f_pts = np.hstack(fracs)\n        frac_dic = {'points': f_pts, 'edges': f_lines}\n        grids = simplex.triangle_grid(frac_dic, domain, **kwargs)\n    elif ndim == 3:\n        grids = simplex.tetrahedral_grid(\n            fracs, domain, network, subdomains, **kwargs)\n    else:\n        raise ValueError('Only support for 2 and 3 dimensions')\n\n    if verbose > 0:\n        print('Done. Elapsed time ' + str(time.time() - tm_msh))\n\n    # Tag tip faces\n    tag_faces(grids)\n\n    # Assemble grids in a bucket\n\n    if verbose > 0:\n        print('Assemble in bucket')\n        tm_bucket = time.time()\n    gb = assemble_in_bucket(grids, **kwargs)\n    if verbose > 0:\n        print('Done. Elapsed time ' + str(time.time() - tm_bucket))\n        print('Compute geometry')\n        tm_geom = time.time()\n\n    gb.compute_geometry()\n    # Split the grids.\n    if verbose > 0:\n        print('Done. Elapsed time ' + str(time.time() - tm_geom))\n        print('Split fractures')\n        tm_split = time.time()\n    split_grid.split_fractures(gb, **kwargs)\n    if verbose > 0:\n        print('Done. Elapsed time ' + str(time.time() - tm_split))\n    gb.assign_node_ordering()\n\n    if verbose > 0:\n        print('Mesh construction completed. Total time ' +\n              str(time.time() - tm_tot))\n\n    return gb",
  "def dfn(fracs, conforming, intersections=None, keep_geo=False, tol=1e-4,\n        **kwargs):\n    \"\"\" Create a mesh of a DFN model, that is, only of fractures.\n\n    The mesh can eihter be conforming along fracture intersections, or each\n    fracture is meshed independently. The latter case will typically require\n    some sort of sewing together external to this funciton.\n\n    TODO: What happens if we give in a non-connected network?\n\n    Parameters:\n        fracs (either Fractures, or a FractureNetwork).\n        conforming (boolean): If True, the mesh will be conforming along 1d\n            intersections.\n        intersections (list of lists, optional): Each item corresponds to an\n            intersection between two fractures. In each sublist, the first two\n            indices gives fracture ids (refering to order in fracs). The third\n            item is a numpy array representing intersection coordinates. If no\n            intersections provided, intersections will be detected using\n            function in FractureNetwork.\n        **kwargs: Parameters passed to gmsh.\n\n    Returns:\n        GridBucket (if conforming is True): Mixed-dimensional mesh that\n            represents all fractures, and intersection poitns and line.\n\n    \"\"\"\n\n    if isinstance(fracs, FractureNetwork) \\\n       or isinstance(fracs, FractureNetwork_full):\n        network = fracs\n    else:\n        network = FractureNetwork(fracs)\n\n    # Populate intersections in FractureNetowrk, or find intersections if not\n    # provided.\n\n    if intersections is not None:\n        logger.warn('FractureNetwork use pre-computed intersections')\n        network.intersections = [Intersection(*i) for i in intersections]\n    else:\n        logger.warn('FractureNetwork find intersections in DFN')\n        tic = time.time()\n        network.find_intersections()\n        logger.warn('Done. Elapsed time ' + str(time.time() - tic))\n\n    if conforming:\n        logger.warn('Create conforming mesh for DFN network')\n        grids = simplex.triangle_grid_embedded(network, find_isect=False,\n                                               **kwargs)\n    else:\n        logger.warn('Create non-conforming mesh for DFN network')\n        tic = time.time()\n        grid_list = []\n        neigh_list = []\n\n        for fi in range(len(network._fractures)):\n            logger.info('Meshing of fracture ' + str(fi))\n            # Rotate fracture vertexes and intersection points\n            fp, ip, other_frac, rot, cp = network.fracture_to_plane(fi)\n            frac_i = network[fi]\n\n            f_lines = np.reshape(np.arange(ip.shape[1]), (2, -1), order='F')\n            frac_dict = {'points': ip, 'edges': f_lines}\n            if keep_geo:\n                file_name = 'frac_mesh_' + str(fi)\n                kwargs['file_name'] = file_name\n            # Create mesh on this fracture surface.\n            grids = simplex.triangle_grid(frac_dict, fp, verbose=False,\n                                          **kwargs)\n\n            irot = rot.T\n            # Loop over grids, rotate back again to 3d coordinates\n            for gl in grids:\n                for g in gl:\n                    g.nodes = irot.dot(g.nodes) + cp\n\n            # Nodes of main (fracture) grid, in 3d coordinates1\n            main_nodes = grids[0][0].nodes\n            main_global_point_ind = grids[0][0].global_point_ind\n            # Loop over intersections, check if the intersection is on the\n            # boundary of this fracture.\n            for ind, isect in enumerate(network.intersections_of_fracture(fi)):\n                of = isect.get_other_fracture(frac_i)\n                if isect.on_boundary_of_fracture(frac_i):\n                    dist, _, _ = cg.dist_points_polygon(main_nodes, of.p)\n                    hit = np.argwhere(dist < tol).reshape((1, -1))[0]\n                    nodes_1d = main_nodes[:, hit]\n                    global_point_ind = main_global_point_ind[hit]\n\n                    assert cg.is_collinear(nodes_1d, tol=tol)\n                    sort_ind = cg.argsort_point_on_line(nodes_1d, tol=tol)\n                    g_aux = TensorGrid(np.arange(nodes_1d.shape[1]))\n                    g_aux.nodes = nodes_1d[:, sort_ind]\n                    g_aux.global_point_ind = global_point_ind[sort_ind]\n                    grids[1].insert(ind, g_aux)\n\n            assert len(grids[0]) == 1, 'Fracture should be covered by single'\\\n                'mesh'\n\n            grid_list.append(grids)\n            neigh_list.append(other_frac)\n\n        logger.warn('Finished creating grids. Elapsed time ' +\n                    str(time.time() - tic))\n        logger.warn('Merge grids')\n        tic = time.time()\n        grids = non_conforming.merge_grids(grid_list, neigh_list)\n        logger.warn('Done. Elapsed time ' + str(time.time() - tic))\n\n        print('\\n')\n        for g_set in grids:\n            if len(g_set) > 0:\n                s = 'Created ' + str(len(g_set)) + ' ' + str(g_set[0].dim) + \\\n                    '-d grids with '\n                num = 0\n                for g in g_set:\n                    num += g.num_cells\n                s += str(num) + ' cells'\n                print(s)\n        print('\\n')\n\n    tag_faces(grids, check_highest_dim=False)\n    logger.warn('Assemble in bucket')\n    tic = time.time()\n    gb = assemble_in_bucket(grids)\n    logger.warn('Done. Elapsed time ' + str(time.time() - tic))\n    logger.warn('Compute geometry')\n    tic = time.time()\n    gb.compute_geometry()\n    logger.warn('Done. Elapsed time ' + str(time.time() - tic))\n    logger.warn('Split fractures')\n    tic = time.time()\n    split_grid.split_fractures(gb)\n    logger.warn('Done. Elapsed time ' + str(time.time() - tic))\n    return gb",
  "def from_gmsh(file_name, dim, **kwargs):\n    \"\"\"\n    Import an already generated grid from gmsh.\n    NOTE: Only 2d grid is implemented so far.\n\n    Parameters\n    ----------\n    file_name (string): Gmsh file name.\n    dim (int): Spatial dimension of the grid.\n    **kwargs: May contain fracture tags, options for gridding, etc.\n\n    Returns\n    -------\n    Grid or GridBucket: If no fractures are present in the gmsh file a simple\n        grid is returned. Otherwise, a complete bucket where all fractures are\n        represented as lower dim grids. See the documentation of simplex_grid\n        for further details.\n\n    Examples\n    --------\n    gb = from_gmsh('grid.geo', 2)\n\n    \"\"\"\n    # Call relevant method, depending on grid dimensions.\n    if dim == 2:\n        if file_name.endswith('.geo'):\n            simplex.triangle_grid_run_gmsh(file_name, **kwargs)\n            grids = simplex.triangle_grid_from_gmsh(file_name, **kwargs)\n        elif file_name.endswith('.msh'):\n            grids = simplex.triangle_grid_from_gmsh(file_name, **kwargs)\n\n#    elif dim == 3:\n#        grids = simplex.tetrahedral_grid_from_gmsh(file_name, **kwargs)\n#   NOTE: function simplex.tetrahedral_grid needs to be split as did for\n#   simplex.triangle_grid\n    else:\n        raise ValueError('Only support for 2 dimensions')\n\n    # No fractures are specified, return a simple grid\n    if len(grids[1]) == 0:\n        grids[0][0].compute_geometry()\n        return grids[0][0]\n\n    # Tag tip faces\n    tag_faces(grids)\n\n    # Assemble grids in a bucket\n    gb = assemble_in_bucket(grids)\n    gb.compute_geometry()\n    # Split the grids.\n    split_grid.split_fractures(gb)\n    return gb",
  "def cart_grid(fracs, nx, **kwargs):\n    \"\"\"\n    Creates a cartesian fractured GridBucket in 2- or 3-dimensions.\n\n    Parameters\n    ----------\n    fracs (list of np.ndarray): One list item for each fracture. Each item\n        consist of a (nd x 3) array describing fracture vertices. The\n        fractures has to be rectangles(3D) or straight lines(2D) that\n        alignes with the axis. The fractures may be intersecting.\n        The fractures will snap to closest grid faces.\n    nx (np.ndarray): Number of cells in each direction. Should be 2D or 3D\n    **kwargs:\n        physdims (np.ndarray): Physical dimensions in each direction.\n            Defaults to same as nx, that is, cells of unit size.\n        May also contain fracture tags, options for gridding, etc.\n\n    Returns:\n    -------\n    GridBucket: A complete bucket where all fractures are represented as\n        lower dim grids. The higher dim fracture faces are split in two,\n        and on the edges of the GridBucket graph the mapping from lower dim\n        cells to higher dim faces are stored as 'face_cells'. Each face is\n        given boolean tags depending on the type:\n           domain_boundary_faces: All faces that lie on the domain boundary\n               (i.e. should be given a boundary condition).\n           fracture_faces: All faces that are split (i.e. has a connection to a\n               lower dim grid).\n           tip_faces: A boundary face that is not on the domain boundary, nor\n               coupled to a lower domentional domain.\n        The union of the above three is the tag boundary_faces.\n\n    Examples\n    --------\n    frac1 = np.array([[1,4],[2,2]])\n    frac2 = np.array([[2,2],[4,1]])\n    fracs = [frac1, frac2]\n    gb = cart_grid(fracs, [5,5])\n    \"\"\"\n    ndim = np.asarray(nx).size\n    physdims = kwargs.get('physdims', None)\n\n    if physdims is None:\n        physdims = nx\n    elif np.asarray(physdims).size != ndim:\n        raise ValueError('Physical dimension must equal grid dimension')\n\n    # Call relevant method, depending on grid dimensions\n    if ndim == 2:\n        grids = structured.cart_grid_2d(fracs, nx, physdims=physdims)\n    elif ndim == 3:\n        grids = structured.cart_grid_3d(fracs, nx, physdims=physdims)\n    else:\n        raise ValueError('Only support for 2 and 3 dimensions')\n\n    # Tag tip faces.\n    tag_faces(grids)\n\n    # Asemble in bucket\n    gb = assemble_in_bucket(grids)\n    gb.compute_geometry()\n\n    # Split grid.\n    split_grid.split_fractures(gb, **kwargs)\n    gb.assign_node_ordering()\n    return gb",
  "def tag_faces(grids, check_highest_dim=True):\n    \"\"\"\n    Tag faces of grids. Three different tags are given to different types of\n    faces:\n        NONE: None of the below (i.e. an internal face)\n        DOMAIN_BOUNDARY: All faces that lie on the domain boundary\n            (i.e. should be given a boundary condition).\n        FRACTURE: All faces that are split (i.e. has a connection to a\n            lower dim grid).\n        TIP: A boundary face that is not on the domain boundary, nor\n            coupled to a lower domentional domain.\n\n    Parameters:\n        grids (list): List of grids to be tagged. Sorted per dimension.\n        check_highest_dim (boolean, default=True): If true, we require there is\n            a single mesh in the highest dimension. The test is useful, but\n            should be waived for dfn meshes.\n\n    \"\"\"\n\n    # Assume only one grid of highest dimension\n    if check_highest_dim:\n        assert len(grids[0]) == 1, 'Must be exactly'\\\n            '1 grid of dim: ' + str(len(grids))\n\n    for g_h in np.atleast_1d(grids[0]):\n        bnd_faces = g_h.get_all_boundary_faces()\n        domain_boundary_tags = np.zeros(\n            g_h.num_faces, dtype=bool)\n        domain_boundary_tags[bnd_faces] = True\n        g_h.tags['domain_boundary_faces'] = domain_boundary_tags\n        bnd_nodes, _, _ = sps.find(g_h.face_nodes[:, bnd_faces])\n        bnd_nodes = np.unique(bnd_nodes)\n        for g_dim in grids[1:-1]:\n            for g in g_dim:\n                # We find the global nodes of all boundary faces\n                bnd_faces_l = g.get_all_boundary_faces()\n                indptr = g.face_nodes.indptr\n                fn_loc = mcolon.mcolon(\n                    indptr[bnd_faces_l], indptr[bnd_faces_l + 1])\n                nodes_loc = g.face_nodes.indices[fn_loc]\n                # Convert to global numbering\n                nodes_glb = g.global_point_ind[nodes_loc]\n                # We then tag each node as a tip node if it is not a global\n                # boundary node\n                is_tip = np.in1d(nodes_glb, bnd_nodes, invert=True)\n                # We reshape the nodes such that each column equals the nodes of\n                # one face. If a face only contains global boundary nodes, the\n                # local face is also a boundary face. Otherwise, we add a TIP tag.\n                n_per_face = nodes_per_face(g)\n                is_tip = np.any(is_tip.reshape(\n                    (n_per_face, bnd_faces_l.size), order='F'), axis=0)\n\n                g.tags['tip_faces'][bnd_faces_l[is_tip]] = True\n                domain_boundary_tags = np.zeros(g.num_faces, dtype=bool)\n                domain_boundary_tags[bnd_faces_l[is_tip == False]] = True\n                g.tags['domain_boundary_faces'] = domain_boundary_tags",
  "def nodes_per_face(g):\n    \"\"\"\n    Returns the number of nodes per face for a given grid\n    \"\"\"\n    if ('TensorGrid' in g.name or 'CartGrid' in g.name) and g.dim == 3:\n        n_per_face = 4\n    elif 'TetrahedralGrid' in g.name:\n        n_per_face = 3\n    elif ('TensorGrid' in g.name or 'CartGrid' in g.name) and g.dim == 2:\n        n_per_face = 2\n    elif 'TriangleGrid'in g.name:\n        n_per_face = 2\n    elif ('TensorGrid' in g.name or 'CartGrid' in g.name) and g.dim == 1:\n        n_per_face = 1\n    else:\n        raise ValueError(\n            \"Can not find number of nodes per face for grid: \" + str(g.name))\n    return n_per_face",
  "def assemble_in_bucket(grids, **kwargs):\n    \"\"\"\n    Create a GridBucket from a list of grids.\n    Parameters\n    ----------\n    grids: A list of lists of grids. Each element in the list is a list\n        of all grids of a the same dimension. It is assumed that the\n        grids are sorted from high dimensional grids to low dimensional grids.\n        All grids must also have the mapping g.global_point_ind which maps\n        the local nodes of the grid to the nodes of the highest dimensional\n        grid.\n\n    Returns\n    -------\n    GridBucket: A GridBucket class where the mapping face_cells are given to\n        each edge. face_cells maps from lower-dim cells to higher-dim faces.\n    \"\"\"\n\n    # Create bucket\n    bucket = GridBucket()\n    [bucket.add_nodes(g_d) for g_d in grids]\n\n    # We now find the face_cell mapings.\n    for dim in range(len(grids) - 1):\n        for hg in grids[dim]:\n            # We have to specify the number of nodes per face to generate a\n            # matrix of the nodes of each face.\n            n_per_face = nodes_per_face(hg)\n            fn_loc = hg.face_nodes.indices.reshape((n_per_face, hg.num_faces),\n                                                   order='F')\n            # Convert to global numbering\n            fn = hg.global_point_ind[fn_loc]\n            fn = np.sort(fn, axis=0)\n\n            for lg in grids[dim + 1]:\n                cell_2_face, cell = utils.obtain_interdim_mappings(\n                    lg, fn, n_per_face, **kwargs)\n                face_cells = sps.csc_matrix(\n                    (np.ones(cell.size, dtype=bool), (cell, cell_2_face)),\n                    (lg.num_cells, hg.num_faces))\n\n                # This if may be unnecessary, but better safe than sorry.\n                if face_cells.size > 0:\n                    bucket.add_edge([hg, lg], face_cells)\n\n    return bucket",
  "def tetrahedral_grid(fracs=None, box=None, network=None, subdomains=[], **kwargs):\n    \"\"\"\n    Create grids for a domain with possibly intersecting fractures in 3d.\n    The function can be call through the wrapper function meshing.simplex_grid.\n\n    Based on the specified fractures, the method computes fracture\n    intersections if necessary, creates a gmsh input file, runs gmsh and reads\n    the result, and then constructs grids in 3d (the whole domain), 2d (one for\n    each individual fracture), 1d (along fracture intersections), and 0d\n    (meeting between intersections).\n\n    The fractures can be specified is terms of the keyword 'fracs' (either as\n    numpy arrays or Fractures, see below), or as a ready-made FractureNetwork\n    by the keyword 'network'. For fracs, the boundary of the domain must be\n    specified as well, by 'box'. For a ready network, the boundary will be\n    imposed if provided. For a network will use pre-computed intersection and\n    decomposition if these are available (attributes 'intersections' and\n    'decomposition').\n\n    Parameters\n    ----------\n    fracs (list, optional): List of either pre-defined fractures, or\n        np.ndarrays, (each 3xn) of fracture vertexes.\n    box (dictionary, optional). Domain specification. Should have keywords\n        xmin, xmax, ymin, ymax, zmin, zmax.\n    network (fractures.FractureNetwork, optional): A FractureNetwork\n        containing fractures.\n    subdomain (list, optional): List of planes partitioning the 3d domain\n        into subdomains. The planes are defined in the same way as fracs.\n\n    The fractures should be specified either by a combination of fracs and\n    box, or by network (possibly combined with box). See above.\n\n    **kwargs: To be explored.\n\n    Returns\n    -------\n    list (length 4): For each dimension (3 -> 0), a list of all grids in\n        that dimension.\n\n    Examples\n    --------\n    frac1 = np.array([[1,1,4,4], [1,4,4,1], [2,2,2,2]])\n    frac2 = np.array([[2,2,2,2], [1,1,4,4], [1,4,4,1]])\n    fracs = [frac1, frac2]\n    domain = {'xmin': 0, 'ymin': 0, 'zmin': 0,\n              'xmax': 5, 'ymax': 5, 'zmax': 5,}\n    path_to_gmsh = .... # Set the sytem path to gmsh\n    gb = tetrahedral_grid(fracs, domain, gmsh_path = path_to_gmsh)\n    \"\"\"\n\n    # Verbosity level\n    verbose = kwargs.get('verbose', 1)\n\n    # File name for communication with gmsh\n    file_name = kwargs.pop('file_name', 'gmsh_frac_file')\n\n    if network is None:\n\n        frac_list = []\n        for f in fracs:\n            # Input can be either numpy arrays or predifined fractures. As a\n            # guide, we treat f as a fracture if it has an attribute p which is\n            # a numpy array.\n            # If f turns out not to be a fracture, strange errors will result\n            # as the further program tries to access non-existing methods.\n            # The correct treatment here would be several\n            # isinstance-statements, but that became less than elegant. To\n            # revisit.\n            if hasattr(f, 'p') and isinstance(f.p, np.ndarray):\n                frac_list.append(f)\n            else:\n                # Convert the fractures from numpy representation to our 3D\n                # fracture data structure.\n                frac_list.append(fractures.Fracture(f))\n\n        # Combine the fractures into a network\n        network = fractures.FractureNetwork(frac_list, verbose=verbose,\n                                            tol=kwargs.get('tol', 1e-4))\n    # Add any subdomain boundaries:\n    network.add_subdomain_boundaries(subdomains)\n    # Impose external boundary. If box is None, a domain size somewhat larger\n    # than the network will be assigned.\n    network.impose_external_boundary(box)\n    # Find intersections and split them, preparing the way for dumping the\n    # network to gmsh\n    if not network.has_checked_intersections:\n        network.find_intersections()\n    else:\n        print('Use existing intersections')\n\n    start_time = time.time()\n\n    # If fields h_ideal and h_min are provided, try to estimate mesh sizes.\n    h_ideal = kwargs.get('h_ideal', None)\n    h_min = kwargs.get('h_min', None)\n    if h_ideal is not None and h_min is not None:\n        network.insert_auxiliary_points(h_ideal, h_min)\n        # In this case we need to recompute intersection decomposition anyhow.\n        network.split_intersections()\n\n    if not hasattr(network, 'decomposition'):\n        network.split_intersections()\n    else:\n        print('Use existing decomposition')\n\n    in_file = file_name + '.geo'\n    out_file = file_name + '.msh'\n\n    network.to_gmsh(in_file, **kwargs)\n\n    gmsh_opts = kwargs.get('gmsh_opts', {})\n    gmsh_verbose = kwargs.get('gmsh_verbose', verbose)\n    gmsh_opts['-v'] = gmsh_verbose\n    gmsh_status = gmsh_interface.run_gmsh(in_file, out_file, dims=3,\n                                          **gmsh_opts)\n\n    if verbose > 0:\n        start_time = time.time()\n        if gmsh_status == 0:\n            print('Gmsh processed file successfully')\n        else:\n            print('Gmsh failed with status ' + str(gmsh_status))\n\n    pts, cells, _, cell_info, phys_names = gmsh_io.read(out_file)\n\n    # Invert phys_names dictionary to map from physical tags to corresponding\n    # physical names\n    phys_names = {v[0]: k for k, v in phys_names.items()}\n\n    # Call upon helper functions to create grids in various dimensions.\n    # The constructors require somewhat different information, reflecting the\n    # different nature of the grids.\n    g_3d = mesh_2_grid.create_3d_grids(pts, cells)\n    g_2d = mesh_2_grid.create_2d_grids(\n        pts, cells, is_embedded=True, phys_names=phys_names,\n        cell_info=cell_info, network=network)\n    g_1d, _ = mesh_2_grid.create_1d_grids(pts, cells, phys_names, cell_info)\n    g_0d = mesh_2_grid.create_0d_grids(pts, cells)\n\n    grids = [g_3d, g_2d, g_1d, g_0d]\n\n    if verbose > 0:\n        print('\\n')\n        print('Grid creation completed. Elapsed time ' + str(time.time() -\n                                                             start_time))\n        print('\\n')\n        for g_set in grids:\n            if len(g_set) > 0:\n                s = 'Created ' + str(len(g_set)) + ' ' + str(g_set[0].dim) + \\\n                    '-d grids with '\n                num = 0\n                for g in g_set:\n                    num += g.num_cells\n                s += str(num) + ' cells'\n                print(s)\n        print('\\n')\n\n    return grids",
  "def triangle_grid_embedded(network, find_isect=True, f_name='dfn_network',\n                           h_ideal=None, h_min=None, **kwargs):\n    \"\"\" Create triangular (2D) grid of a domain embedded in 3D space, without\n    meshing the 3D volume.\n\n    The resulting grid can be used in a DFN model. The grid will be fully\n    conforming along intersections between fractures.\n\n    This function produces a set of grids for fractures and lower-dimensional\n    objects, but it does nothing to merge the grids. To create a GridBucket,\n    use the function fracs.meshing.dfn instead, with the option\n    conforming=True.\n\n    To set the mesh size, use parameters h_ideal and h_min, to represent the\n    ideal and minimal mesh size sent to gmsh. For more details, see the gmsh\n    manual on how to set mesh sizes.\n\n    Parameters:\n        network (FractureNetwork): To be meshed.\n        find_isect (boolean, optional): If True (default), the network will\n            search for intersections among fractures. Set False if\n            network.find_intersections() already has been called.\n        f_name (str, optional): Filename for communication with gmsh.\n            The config file for gmsh will be f_name.geo, with the grid output\n            to f_name.msh. Defaults to dfn_network.\n        h_ideal (double, optional): Target mesh size sent to gmsh. If not\n            provided, gmsh will do its best to decide on the mesh size.\n        h_min (double, optional): Minimal mesh size sent to gmsh. If not\n            provided, gmsh will do its best to decide on the mesh size.\n        **kwargs: Arguments sent to gmsh etc.\n\n    Returns:\n        list (length 3): For each dimension (2 -> 0), a list of all grids in\n            that dimension.\n\n    \"\"\"\n\n    verbose = 1\n\n    if find_isect:\n        network.find_intersections()\n\n    # If fields h_ideal and h_min are provided, try to estimate mesh sizes.\n    if h_ideal is not None and h_min is not None:\n        network.insert_auxiliary_points(h_ideal, h_min)\n        # In this case we need to recompute intersection decomposition anyhow.\n        network.split_intersections()\n\n    if not hasattr(network, 'decomposition'):\n        network.split_intersections()\n    else:\n        print('Use existing decomposition')\n\n    pts, cells, cell_info, phys_names = _run_gmsh(f_name, network,\n                                                  in_3d=False, **kwargs)\n    g_2d = mesh_2_grid.create_2d_grids(\n        pts, cells, is_embedded=True, phys_names=phys_names,\n        cell_info=cell_info, network=network)\n    g_1d, _ = mesh_2_grid.create_1d_grids(pts, cells, phys_names, cell_info)\n    g_0d = mesh_2_grid.create_0d_grids(pts, cells)\n\n    grids = [g_2d, g_1d, g_0d]\n\n    if verbose > 0:\n        print('\\n')\n        for g_set in grids:\n            if len(g_set) > 0:\n                s = 'Created ' + str(len(g_set)) + ' ' + str(g_set[0].dim) + \\\n                    '-d grids with '\n                num = 0\n                for g in g_set:\n                    num += g.num_cells\n                s += str(num) + ' cells'\n                print(s)\n        print('\\n')\n\n    return grids",
  "def _run_gmsh(file_name, network, **kwargs):\n\n    verbose = kwargs.get('verbose', 1)\n    if file_name[-4:] == '.geo' or file_name[-4:] == '.msh':\n        file_name = file_name[:-4]\n\n    in_file = file_name + '.geo'\n    out_file = file_name + '.msh'\n\n    if not hasattr(network, 'decomposition'):\n        network.split_intersections()\n    else:\n        print('Use existing decomposition')\n\n    network.to_gmsh(in_file, **kwargs)\n\n    gmsh_opts = kwargs.get('gmsh_opts', {})\n    gmsh_verbose = kwargs.get('gmsh_verbose', verbose)\n    gmsh_opts['-v'] = gmsh_verbose\n    gmsh_status = gmsh_interface.run_gmsh(in_file, out_file, dims=3,\n                                          **gmsh_opts)\n\n    if verbose > 0:\n        if gmsh_status == 0:\n            print('Gmsh processed file successfully')\n        else:\n            print('Gmsh failed with status ' + str(gmsh_status))\n            sys.exit()\n\n    pts, cells, _, cell_info, phys_names = gmsh_io.read(out_file)\n\n    # Invert phys_names dictionary to map from physical tags to corresponding\n    # physical names\n    phys_names = {v[0]: k for k, v in phys_names.items()}\n\n    return pts, cells, cell_info, phys_names",
  "def triangle_grid(fracs, domain, **kwargs):\n    \"\"\"\n    Generate a gmsh grid in a 2D domain with fractures.\n\n    The function uses modified versions of pygmsh and mesh_io,\n    both downloaded from github.\n\n    To be added:\n    Functionality for tuning gmsh, including grid size, refinements, etc.\n\n    Parameters\n    ----------\n    fracs: (dictionary) Two fields: points (2 x num_points) np.ndarray,\n        edges (2 x num_lines) connections between points, defines fractures.\n    box: (dictionary) keys xmin, xmax, ymin, ymax, [together bounding box\n        for the domain]\n    **kwargs: To be explored.\n\n    Returns\n    -------\n    list (length 3): For each dimension (2 -> 0), a list of all grids in\n        that dimension.\n\n    Examples\n    p = np.array([[-1, 1, 0, 0], [0, 0, -1, 1]])\n    lines = np.array([[0, 2], [1, 3]])\n    char_h = 0.5 * np.ones(p.shape[1])\n    tags = np.array([1, 3])\n    fracs = {'points': p, 'edges': lines}\n    box = {'xmin': -2, 'xmax': 2, 'ymin': -2, 'ymax': 2}\n    g = triangle_grid(fracs, box)\n\n    \"\"\"\n    # Verbosity level\n    verbose = kwargs.get('verbose', 1)\n\n    # File name for communication with gmsh\n    file_name = kwargs.get('file_name', 'gmsh_frac_file')\n    kwargs.pop('file_name', str())\n\n    tol = kwargs.get('tol', 1e-4)\n\n    in_file = file_name + '.geo'\n    out_file = file_name + '.msh'\n\n    # Pick out fracture points, and their connections\n    frac_pts = fracs['points']\n    frac_con = fracs['edges']\n\n    # Unified description of points and lines for domain, and fractures\n    pts_all, lines = __merge_domain_fracs_2d(domain, frac_pts, frac_con)\n\n    # Snap to underlying grid before comparing points\n    pts_all = cg.snap_to_grid(pts_all, tol)\n\n    assert np.all(np.diff(lines[:2], axis=0) != 0)\n\n    # Ensure unique description of points\n    pts_all, _, old_2_new = unique_columns_tol(pts_all, tol=tol)\n    lines[:2] = old_2_new[lines[:2]]\n    to_remove = np.where(lines[0, :] == lines[1, :])[0]\n    lines = np.delete(lines, to_remove, axis=1)\n\n    # In some cases the fractures and boundaries impose the same constraint\n    # twice, although it is not clear why. Avoid this by uniquifying the lines.\n    # This may disturb the line tags in lines[2], but we should not be\n    # dependent on those.\n    li = np.sort(lines[:2], axis=0)\n    li_unique, new_2_old, old_2_new = unique_columns_tol(li)\n    lines = lines[:, new_2_old]\n\n    assert np.all(np.diff(lines[:2], axis=0) != 0)\n\n    # We split all fracture intersections so that the new lines do not\n    # intersect, except possible at the end points\n    pts_split, lines_split = cg.remove_edge_crossings(pts_all, lines, tol=tol)\n\n    # Ensure unique description of points\n    pts_split = cg.snap_to_grid(pts_split, tol)\n    pts_split, _, old_2_new = unique_columns_tol(pts_split, tol=tol)\n    lines_split[:2] = old_2_new[lines_split[:2]]\n    to_remove = np.where(lines[0, :] == lines[1, :])[0]\n    lines = np.delete(lines, to_remove, axis=1)\n\n    # Remove lines with the same start and end-point.\n    # This can be caused by L-intersections, or possibly also if the two\n    # endpoints are considered equal under tolerance tol.\n    remove_line_ind = np.where(np.diff(lines_split[:2], axis=0)[0] == 0)[0]\n    lines_split = np.delete(lines_split, remove_line_ind, axis=1)\n\n    # TODO: This operation may leave points that are not referenced by any\n    # lines. We should probably delete these.\n\n    # We find the end points that are shared by more than one intersection\n    intersections = __find_intersection_points(lines_split)\n\n    # Gridding size\n    if 'mesh_size' in kwargs.keys():\n        mesh_size, mesh_size_bound, pts_split, lines_split = \\\n            utils.determine_mesh_size(pts_split, lines_split,\n                                      **kwargs['mesh_size'])\n    else:\n        mesh_size = None\n        mesh_size_bound = None\n\n    # gmsh options\n\n    meshing_algorithm = kwargs.get('meshing_algorithm')\n\n    # Create a writer of gmsh .geo-files\n    gw = gmsh_interface.GmshWriter(\n        pts_split, lines_split, domain=domain, mesh_size=mesh_size,\n        mesh_size_bound=mesh_size_bound, intersection_points=intersections,\n        meshing_algorithm=meshing_algorithm)\n    gw.write_geo(in_file)\n\n    triangle_grid_run_gmsh(file_name, **kwargs)\n    return triangle_grid_from_gmsh(file_name, **kwargs)",
  "def triangle_grid_run_gmsh(file_name, **kwargs):\n\n    if file_name.endswith('.geo'):\n        file_name = file_name[:-4]\n    in_file = file_name + '.geo'\n    out_file = file_name + '.msh'\n\n    # Verbosity level\n    verbose = kwargs.get('verbose', 1)\n    gmsh_verbose = kwargs.get('gmsh_verbose', verbose)\n    gmsh_opts = {'-v': gmsh_verbose}\n\n    # Run gmsh\n    gmsh_status = gmsh_interface.run_gmsh(in_file, out_file, dims=2,\n                                          **gmsh_opts)\n\n    if verbose > 0:\n        if gmsh_status == 0:\n            print('Gmsh processed file successfully')\n        else:\n            print('Gmsh failed with status ' + str(gmsh_status))",
  "def triangle_grid_from_gmsh(file_name, **kwargs):\n\n    start_time = time.time()\n\n    if file_name.endswith('.msh'):\n        file_name = file_name[:-4]\n    out_file = file_name + '.msh'\n\n    # Verbosity level\n    verbose = kwargs.get('verbose', 1)\n\n    pts, cells, _, cell_info, phys_names = gmsh_io.read(out_file)\n\n    # Invert phys_names dictionary to map from physical tags to corresponding\n    # physical names.\n    # As of meshio 1.10, the value of the physical name is defined as a numpy\n    # array, with the first item being the tag, the second the dimension.\n    phys_names = {v[0]: k for k, v in phys_names.items()}\n\n    # Constants used in the gmsh.geo-file\n    const = constants.GmshConstants()\n\n    # Create grids from gmsh mesh.\n    g_2d = mesh_2_grid.create_2d_grids(pts, cells, is_embedded=False)\n    g_1d, _ = mesh_2_grid.create_1d_grids(\n        pts, cells, phys_names, cell_info, line_tag=const.PHYSICAL_NAME_FRACTURES)\n    g_0d = mesh_2_grid.create_0d_grids(pts, cells)\n    grids = [g_2d, g_1d, g_0d]\n\n    if verbose > 0:\n        print('\\n')\n        print('Grid creation completed. Elapsed time ' + str(time.time() -\n                                                             start_time))\n        print('\\n')\n        for g_set in grids:\n            if len(g_set) > 0:\n                s = 'Created ' + str(len(g_set)) + ' ' + str(g_set[0].dim) + \\\n                    '-d grids with '\n                num = 0\n                for g in g_set:\n                    num += g.num_cells\n                s += str(num) + ' cells'\n                print(s)\n        print('\\n')\n\n    return grids",
  "def __merge_domain_fracs_2d(dom, frac_p, frac_l):\n    \"\"\"\n    Merge fractures, domain boundaries and lines for compartments.\n    The unified description is ready for feeding into meshing tools such as\n    gmsh\n\n    Parameters:\n    dom: dictionary defining domain. fields xmin, xmax, ymin, ymax\n    frac_p: np.ndarray. Points used in fracture definition. 2 x num_points.\n    frac_l: np.ndarray. Connection between fracture points. 2 x num_fracs\n\n    returns:\n    p: np.ndarary. Merged list of points for fractures, compartments and domain\n        boundaries.\n    l: np.ndarray. Merged list of line connections (first two rows), tag\n        identifying which type of line this is (third row), and a running index\n        for all lines (fourth row)\n    \"\"\"\n\n    # Use constants set outside. If we ever\n    const = constants.GmshConstants()\n\n    if isinstance(dom, dict):\n        # First create lines that define the domain\n        x_min = dom['xmin']\n        x_max = dom['xmax']\n        y_min = dom['ymin']\n        y_max = dom['ymax']\n        dom_p = np.array([[x_min, x_max, x_max, x_min],\n                          [y_min, y_min, y_max, y_max]])\n        dom_lines = np.array([[0, 1], [1, 2], [2, 3], [3, 0]]).T\n    else:\n        dom_p = dom\n        tmp = np.arange(dom_p.shape[1])\n        dom_lines = np.vstack((tmp, (tmp + 1) % dom_p.shape[1]))\n\n    num_dom_lines = dom_lines.shape[1]  # Should be 4\n\n    # The  lines will have all fracture-related tags set to zero.\n    # The plan is to ignore these tags for the boundary and compartments,\n    # so it should not matter\n    dom_tags = const.DOMAIN_BOUNDARY_TAG * np.ones((1, num_dom_lines))\n    dom_l = np.vstack((dom_lines, dom_tags))\n\n    # Also add a tag to the fractures, signifying that these are fractures\n    frac_l = np.vstack((frac_l,\n                        const.FRACTURE_TAG * np.ones(frac_l.shape[1])))\n\n    # Merge the point arrays, compartment points first\n    p = np.hstack((dom_p, frac_p))\n\n    # Adjust index of fracture points to account for the compartment points\n    frac_l[:2] += dom_p.shape[1]\n\n    l = np.hstack((dom_l, frac_l)).astype('int')\n\n    # Add a second tag as an identifier of each line.\n    l = np.vstack((l, np.arange(l.shape[1])))\n\n    return p, l",
  "def __find_intersection_points(lines):\n    const = constants.GmshConstants()\n    frac_id = np.ravel(lines[:2, lines[2] == const.FRACTURE_TAG])\n    _, ia, count = np.unique(frac_id, True, False, True)\n    return frac_id[ia[count > 1]]",
  "def plot_fractures(d, p, c, colortag=None, **kwargs):\n    \"\"\"\n    Plot 2d fractures as lines in a domain.\n\n    The function is primarily intended for data exploration.\n\n    Parameters:\n        d (dictionary): Domain size. Should contain fields xmin, xmax, ymin,\n            ymax.\n        p (np.ndarray, dims 2 x npt): Coordinates of the fracture endpoints.\n        c (np.ndarray, dims 2 x n_edges): Indices of fracture start and\n            endpoints.\n        colortag (np.ndarray, dim n_edges, optional): Colorcoding for fractures\n            (e.g. by fracture family). If provided, different colors will be\n            asign to the different families. Defaults to all fractures being\n            black.\n        kwargs: Keyword arguments passed on to matplotlib.\n\n    \"\"\"\n\n    # Assign a color to each tag. We define these by RBG-values (simplest\n    # option in pyplot).\n    # For the moment, some RBG values are hard coded, do something more\n    # intelligent if necessary.\n    if colortag is None:\n        tagmap = np.zeros(c.shape[1], dtype='int')\n        col = [(0, 0, 0)];\n    else:\n        utag, tagmap = np.unique(colortag, return_inverse=True)\n        ntag = utag.size\n        if ntag <= 3:\n            col = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]\n        elif ntag < 6:\n            col = [(1, 0, 0), (0, 1, 0), (0, 0, 1),\n                   (1, 1, 0), (1, 0, 1), (0, 0, 1)]\n        else:\n            raise NotImplementedError('Have not thought of more than six colors')\n\n    plt.figure(**kwargs)\n    plt.axis([d['xmin'], d['xmax'], d['ymin'], d['ymax']])\n    plt.plot([d['xmin'], d['xmax'], d['xmax'], d['xmin'], d['xmin']],\n             [d['ymin'], d['ymin'], d['ymax'], d['ymax'], d['ymin']],\n             '-', color='red')\n\n    # Simple for-loop to draw one fracture after another. Not fancy, but it\n    # serves its purpose.\n    for i in range(c.shape[1]):\n        plt.plot([p[0, c[0, i]], p[0, c[1, i]]],\n                 [p[1, c[0, i]], p[1, c[1, i]]], 'o-', color=col[tagmap[i]])\n    # Finally set axis\n    plt.show()",
  "def remove_nodes(g, rem):\n    \"\"\"\n    Remove nodes from grid.\n    g - a valid grid definition\n    rem - a ndarray of indecies of nodes to be removed\n    \"\"\"\n    all_rows = np.arange(g.face_nodes.shape[0])\n    rows_to_keep = np.where(np.logical_not(np.in1d(all_rows, rem)))[0]\n    g.face_nodes = g.face_nodes[rows_to_keep,:]\n    g.nodes = g.nodes[:,rows_to_keep]\n    return g",
  "def split_fractures(bucket, **kwargs):\n    \"\"\"\n    Wrapper function to split all fractures. For each grid in the bucket,\n    we locate the corresponding lower-dimensional grids. The faces and\n    nodes corresponding to these grids are then split, creating internal\n    boundaries.\n\n    Parameters\n    ----------\n    bucket    - A grid bucket\n    **kwargs:\n        offset    - FLOAT, defaults to 0. Will perturb the nodes around the\n                    faces that are split. NOTE: this is only for visualization.\n                    E.g., the face centers are not perturbed.\n\n    Returns\n    -------\n    bucket    - A valid bucket where the faces are split at\n                internal boundaries.\n\n\n    Examples\n    >>> import numpy as np\n    >>> from gridding.fractured import meshing, split_grid\n    >>> from viz.exporter import export_vtk\n    >>>\n    >>> f_1 = np.array([[-1, 1, 1, -1 ], [0, 0, 0, 0], [-1, -1, 1, 1]])\n    >>> f_2 = np.array([[0, 0, 0, 0], [-1, 1, 1, -1 ], [-.7, -.7, .8, .8]])\n    >>> f_set = [f_1, f_2]\n    >>> domain = {'xmin': -2, 'xmax': 2,\n            'ymin': -2, 'ymax': 2, 'zmin': -2, 'zmax': 2}\n    >>> bucket = meshing.create_grid(f_set, domain)\n    >>> [g.compute_geometry(is_embedded=True) for g,_ in bucket]\n    >>>\n    >>> split_grid.split_fractures(bucket, offset=0.1)\n    >>> export_vtk(bucket, \"grid\")\n    \"\"\"\n\n    offset = kwargs.get('offset', 0)\n\n    # For each vertex in the bucket we find the corresponding lower-\n    # dimensional grids.\n    for gh, _ in bucket:\n        # add new field to grid\n        gh.frac_pairs = np.zeros((2, 0), dtype=np.int32)\n        if gh.dim < 1:\n            # Nothing to do. We can not split 0D grids.\n            continue\n        # Find connected vertices and corresponding edges.\n        neigh = np.array(bucket.node_neighbors(gh))\n\n        # Find the neighbours that are lower dimensional\n        is_low_dim_grid = np.where([w.dim < gh.dim\n                                    for w in neigh])\n        edges = [(gh, w) for w in neigh[is_low_dim_grid]]\n        if len(edges) == 0:\n            # No lower dim grid. Nothing to do.\n            continue\n        face_cells = bucket.edge_prop(edges, 'face_cells')\n\n        # We split all the faces that are connected to a lower-dim grid.\n        # The new faces will share the same nodes and properties (normals,\n        # etc.)\n\n        face_cells = split_faces(gh, face_cells)\n        bucket.add_edge_prop('face_cells', edges, face_cells)\n\n        # We now find which lower-dim nodes correspond to which higher-\n        # dim nodes. We split these nodes according to the topology of\n        # the connected higher-dim cells. At a X-intersection we split\n        # the node into four, while at the fracture boundary it is not split.\n\n        gl = [e[1] for e in edges]\n        gl_2_gh_nodes = [bucket.target_2_source_nodes(\n            g, gh) for g in gl]\n\n        split_nodes(gh, gl, gl_2_gh_nodes, offset)\n\n    # Remove zeros from cell_faces\n\n    [g.cell_faces.eliminate_zeros() for g, _ in bucket]\n    return bucket",
  "def split_faces(gh, face_cells):\n    \"\"\"\n    Split faces of the grid along each fracture. This function will\n    add an extra face to each fracture face. Note that the original\n    and new fracture face will share the same nodes. However, the\n    cell_faces connectivity is updated such that the fractures are\n    be internal boundaries (cells on left side of fractures are not\n    connected to cells on right side of fracture and vise versa).\n    The face_cells are updated such that the copy of a face also\n    map to the same lower-dim cell.\n    \"\"\"\n    gh.frac_pairs = np.zeros((2, 0), dtype=np.int32)\n    for i in range(len(face_cells)):\n        # We first we duplicate faces along tagged faces. The duplicate\n        # faces will share the same nodes as the original faces,\n        # however, the new faces are not yet added to the cell_faces map\n        # (to save computation).\n        face_id = duplicate_faces(gh, face_cells[i])\n        face_cells = update_face_cells(face_cells, face_id, i)\n        if face_id.size == 0:\n            continue\n\n        # We now set the cell_faces map based on which side of the\n        # fractures the cells lie. We assume that all fractures are\n        # flat surfaces and pick the normal of the first face as\n        # a normal for the whole fracture.\n        n = np.reshape(gh.face_normals[:, face_id[0]], (3, 1))\n        n = n / np.linalg.norm(n)\n        x0 = np.reshape(gh.face_centers[:, face_id[0]], (3, 1))\n        flag = update_cell_connectivity(gh, face_id, n, x0)\n\n        if flag == 0:\n            # if flag== 0 we added left and right faces (if it is -1 no faces\n            # was added and we don't have left and right face pairs.\n            # we now add the new faces to the frac_pair array.\n            left = face_id\n            right = np.arange(gh.num_faces - face_id.size, gh.num_faces)\n            gh.frac_pairs = np.hstack(\n                (gh.frac_pairs, np.vstack((left, right))))\n\n    return face_cells",
  "def split_nodes(gh, gl, gh_2_gl_nodes, offset=0):\n    \"\"\"\n    Splits the nodes of a grid given a set of lower-dimensional grids\n    and a connection mapping between them.\n    Parameters\n    ----------\n    gh            - Higher-dimension grid.\n    gl            - A list of lower dimensional grids\n    gh_2_gl_nodes - A list of connection arrays. Each array in the\n                    list gives the mapping from the lower-dim nodes\n                    to the higher dim nodes. gh_2_gl_nodes[0][0] is\n                    the higher-dim index of the first node of the\n                    first lower-dim.\n    offset - float\n             Optional, defaults to 0. This gives the offset from the\n             fracture to the new nodes. Note that this is only for\n             visualization, e.g., g.face_centers is not updated.\n    \"\"\"\n    # We find the higher-dim node indices of all lower-dim nodes\n    nodes = np.array([], dtype=int)\n    for i in range(len(gl)):\n        nodes = np.append(nodes, gh_2_gl_nodes[i])\n    nodes = np.unique(nodes)\n\n    # Each of these nodes are duplicated dependig on the cell-\n    # topology of the higher-dim around each node. For a X-intersection\n    # we get four duplications, for a T-intersection we get three\n    # duplications, etc. Each of the duplicates are then attached\n    # to the cells on one side of the fractures.\n    node_count = duplicate_nodes(gh, nodes, offset)\n\n    # We remove the old nodes.\n    #gh = remove_nodes(gh, nodes)\n\n    # Update the number of nodes\n    gh.num_nodes = gh.num_nodes + node_count  # - nodes.size\n\n    return True",
  "def duplicate_faces(gh, face_cells):\n    \"\"\"\n    Duplicate all faces that are connected to a lower-dim cell\n\n    Parameters\n    ----------\n    gh         - Higher-dim grid\n    face_cells - A list of connection matrices. Each matrix gives\n                 the mapping from the cells of a lower-dim grid\n                 to the faces of the higher diim grid.\n    \"\"\"\n    # We find the indices of the higher-dim faces to be duplicated.\n    # Each of these faces are duplicated, and the duplication is\n    # attached to the same nodes. We do not attach the faces to\n    # any cells as this connection will have to be undone later\n    # anyway.\n    frac_id = face_cells.nonzero()[1]\n    frac_id = np.unique(frac_id)\n    rem = tags.all_face_tags(gh.tags)[frac_id]\n    gh.tags['fracture_faces'][frac_id[rem]] = True\n    gh.tags['tip_faces'][frac_id] = False\n\n    frac_id = frac_id[~rem]\n    if frac_id.size == 0:\n        return frac_id\n\n    node_start = gh.face_nodes.indptr[frac_id]\n    node_end = gh.face_nodes.indptr[frac_id + 1]\n    nodes = gh.face_nodes.indices[mcolon(node_start, node_end)]\n    added_node_pos = np.cumsum(node_end - node_start) + \\\n        gh.face_nodes.indptr[-1]\n    assert(added_node_pos.size == frac_id.size)\n    assert(added_node_pos[-1] - gh.face_nodes.indptr[-1] == nodes.size)\n    gh.face_nodes.indices = np.hstack((gh.face_nodes.indices, nodes))\n    gh.face_nodes.indptr = np.hstack((gh.face_nodes.indptr, added_node_pos))\n    gh.face_nodes.data = np.hstack((gh.face_nodes.data,\n                                    np.ones(nodes.size, dtype=bool)))\n    gh.face_nodes._shape = (\n        gh.num_nodes, gh.face_nodes.shape[1] + frac_id.size)\n    assert(gh.face_nodes.indices.size == gh.face_nodes.indptr[-1])\n\n    node_start = gh.face_nodes.indptr[frac_id]\n    node_end = gh.face_nodes.indptr[frac_id + 1]\n\n    #frac_nodes = gh.face_nodes[:, frac_id]\n\n    #gh.face_nodes = sps.hstack((gh.face_nodes, frac_nodes))\n    # We also copy the attributes of the original faces.\n    gh.num_faces += frac_id.size\n    gh.face_normals = np.hstack(\n        (gh.face_normals, gh.face_normals[:, frac_id]))\n    gh.face_areas = np.append(gh.face_areas, gh.face_areas[frac_id])\n    gh.face_centers = np.hstack(\n        (gh.face_centers, gh.face_centers[:, frac_id]))\n\n    # Not sure if this still does the correct thing. Might have to\n    # send in a logical array instead of frac_id.\n    gh.tags['fracture_faces'][frac_id] = True\n    gh.tags['tip_faces'][frac_id] = False\n    update_fields = tags.standard_face_tags()\n    update_values = [[]] * len(update_fields)\n    for i, key in enumerate(update_fields):\n        update_values[i] = gh.tags[key][frac_id]\n    tags.append_tags(gh.tags, update_fields, update_values)\n\n    return frac_id",
  "def update_face_cells(face_cells, face_id, i):\n    \"\"\"\n    Add duplicate faces to connection map between lower-dim grids\n    and higher dim grids. To be run after duplicate_faces.\n    \"\"\"\n    # We duplicated the faces associated with lower-dim grid i.\n    # The duplications should also be associated with grid i.\n    # For the other lower-dim grids we just add zeros to conserve\n    # the right matrix dimensions.\n    for j, f_c in enumerate(face_cells):\n        if j == i:\n            f_c = sps.hstack((f_c, f_c[:, face_id]))\n        else:\n            empty = sps.csc_matrix((f_c.shape[0], face_id.size))\n            f_c = sps.hstack((f_c, empty))\n        face_cells[j] = f_c\n    return face_cells",
  "def update_cell_connectivity(g, face_id, normal, x0):\n    \"\"\"\n    After the faces in a grid is duplicated, we update the cell connectivity list.\n    Cells on the right side of the fracture does not change, but the cells\n    on the left side are attached to the face duplicates. We assume that all\n    faces that have been duplicated lie in the same plane. This plane is\n    described by a normal and a point, x0. We attach cell on the left side of the\n    plane to the duplicate of face_id. The cells on the right side is attached\n    to the face frac_id\n\n    Parameters:\n    ----------\n    g         - The grid for wich the cell_face mapping is uppdated\n    frac_id   - Indices of the faces that have been duplicated\n    normal    - Normal of faces that have been duplicated. Note that we assume\n                that all faces have the same normal\n    x0        - A point in the plane where the faces lie\n    \"\"\"\n\n    # We find the cells attached to the tagged faces.\n    cell_frac = g.cell_faces[face_id, :]\n    cell_face_id = np.argwhere(cell_frac)\n\n    # We devide the cells into the cells on the right side of the fracture\n    # and cells on the left side of the fracture.\n    left_cell = half_space_int(normal, x0,\n                               g.cell_centers[:, cell_face_id[:, 1]])\n\n    if np.all(left_cell) or not np.any(left_cell):\n        # Fracture is on boundary of domain. There is nothing to do.\n        # Remove the extra faces. We have not yet updated cell_faces,\n        # so we should not delete anything from this matrix.\n        rem = np.arange(g.cell_faces.shape[0], g.num_faces)\n        remove_faces(g, rem, rem_cell_faces=False)\n        return -1\n\n    # Assume that fracture is either on boundary (above case) or completely\n    # innside domain. Check that each face added two cells:\n    assert sum(left_cell) * 2 == left_cell.size, 'Fractures must either be' \\\n        'on boundary or completely innside domain'\n\n    # We create a cell_faces mapping for the new faces. This will be added\n    # on the end of the excisting cell_faces mapping. We have here assumed\n    # that we do not add any mapping during the duplication of faces.\n    col = cell_face_id[left_cell, 1]\n    row = cell_face_id[left_cell, 0]\n    data = np.ravel(g.cell_faces[np.ravel(face_id[row]), col])\n    assert data.size == face_id.size\n    cell_frac_left = sps.csc_matrix((data, (row, col)),\n                                    (face_id.size, g.cell_faces.shape[1]))\n\n    # We now update the cell_faces map of the faces on the right side of\n    # the fracture. These faces should only be attached to the right cells.\n    # We therefore remove their connection to the cells on the left side of\n    # the fracture.\n    col = cell_face_id[~left_cell, 1]\n    row = cell_face_id[~left_cell, 0]\n    data = np.ravel(g.cell_faces[np.ravel(face_id[row]), col])\n    cell_frac_right = sps.csc_matrix((data, (row, col)),\n                                     (face_id.size, g.cell_faces.shape[1]))\n    g.cell_faces[face_id, :] = cell_frac_right\n\n    # And then we add the new left-faces to the cell_face map. We do not\n    # change the sign of the matrix since we did not flip the normals.\n    # This means that the normals of right and left cells point in the same\n    # direction, but their cell_faces values have oposite signs.\n    g.cell_faces = sps.vstack((g.cell_faces, cell_frac_left), format='csc')\n\n    return 0",
  "def remove_faces(g, face_id, rem_cell_faces=True):\n    \"\"\"\n    Remove faces from grid.\n\n    PARAMETERS:\n    -----------\n    g              - A grid\n    face_id        - Indices of faces to remove\n    rem_cell_faces - Defaults to True. If set to false, the g.cell_faces matrix\n                     is not changed.\n    \"\"\"\n    # update face info\n    keep = np.array([True] * g.num_faces)\n    keep[face_id] = False\n    g.face_nodes = g.face_nodes[:, keep]\n    g.num_faces -= face_id.size\n    g.face_normals = g.face_normals[:, keep]\n    g.face_areas = g.face_areas[keep]\n    g.face_centers = g.face_centers[:, keep]\n    # Not sure if still works\n    update_fields = tags.standard_face_tags()\n    for key in update_fields:\n        g.tags[key] = g.tags[key][keep]\n\n    if rem_cell_faces:\n        g.cell_faces = g.cell_faces[keep, :]",
  "def duplicate_nodes(g, nodes, offset):\n    \"\"\"\n    Duplicate nodes on a fracture. The number of duplication will depend on\n    the cell topology around the node. If the node is not on a fracture 1\n    duplicate will be added. If the node is on a single fracture 2 duplicates\n    will be added. If the node is on a T-intersection 3 duplicates will be\n    added. If the node is on a X-intersection 4 duplicates will be added.\n    Equivalently for other types of intersections.\n\n    Parameters:\n    ----------\n    g         - The grid for which the nodes are duplicated\n    nodes     - The nodes to be duplicated\n    offset    - How far from the original node the duplications should be\n                placed.\n    \"\"\"\n    node_count = 0\n\n    # We wish to convert the sparse csc matrix to a sparse\n    # csr matrix to easily add rows. However, the convertion sorts the\n    # indices, which will change the node order when we convert back. We\n    # therefore find the inverse sorting of the nodes of each face.\n    # After we have performed the row operations we will map the nodes\n    # back to their original position.\n\n    _, iv = sort_sub_list(g.face_nodes.indices, g.face_nodes.indptr)\n    g.face_nodes = g.face_nodes.tocsr()\n    # Iterate over each internal node and split it according to the graph.\n    # For each cell attached to the node, we check wich color the cell has.\n    # All cells with the same color is then attached to a new copy of the\n    # node.\n    cell_nodes = g.cell_nodes().tocsr()\n    for node in nodes:\n        # t_node takes into account the added nodes.\n        t_node = node + node_count\n        # Find cells connected to node\n\n        cells = sparse_mat.slice_indices(cell_nodes, node)\n#        cell_nodes = g.cell_nodes().tocsr()\n#        ind_ptr = cell_nodes.indptr\n#        cells = cell_nodes.indices[\n#            mcolon(ind_ptr[t_node], ind_ptr[t_node + 1])]\n        cells = np.unique(cells)\n        # Find the color of each cell. A group of cells is given the same color\n        # if they are connected by faces. This means that all cells on one side\n        # of a fracture will have the same color, but a different color than\n        # the cells on the other side of the fracture. Equivalently, the cells\n        # at a X-intersection will be given four different colors\n        colors = find_cell_color(g, cells)\n        # Find which cells share the same color\n        colors, ix = np.unique(colors, return_inverse=True)\n        # copy coordinate of old node\n        new_nodes = np.repeat(g.nodes[:, t_node, None], colors.size, axis=1)\n        faces = np.array([], dtype=int)\n        face_pos = np.array([g.face_nodes.indptr[t_node]])\n        for j in range(colors.size):\n            # For each color we wish to add one node. First we find all faces that\n            # are connected to the fracture node, and have the correct cell\n            # color\n            local_faces = (g.cell_faces[:, cells[ix == j]]).nonzero()[0]\n            local_faces = np.unique(local_faces)\n            con_to_node = np.ravel(g.face_nodes[t_node, local_faces].todense())\n            faces = np.append(faces, local_faces[con_to_node])\n            # These faces is then attached to new node number j.\n            face_pos = np.append(face_pos, face_pos[-1] + np.sum(con_to_node))\n            # If an offset is given, we will change the position of the nodes.\n            # We move the nodes a length of offset away from the fracture(s).\n            if offset > 0 and colors.size > 1:\n                new_nodes[:, j] -= avg_normal(g,\n                                              local_faces[con_to_node]) * offset\n        # The total number of faces should not have changed, only their\n        # connection to nodes. We can therefore just update the indices and\n        # indptr map.\n        g.face_nodes.indices[face_pos[0]:face_pos[-1]] = faces\n        node_count += colors.size - 1\n        g.face_nodes.indptr = np.insert(g.face_nodes.indptr,\n                                        t_node + 1, face_pos[1:-1])\n        g.face_nodes._shape = (g.face_nodes.shape[0] + colors.size - 1,\n                               g.face_nodes._shape[1])\n        # We delete the old node because of the offset. If we do not\n        # have an offset we could keep it and add one less node.\n        g.nodes = np.delete(g.nodes, t_node, axis=1)\n        g.nodes = np.insert(g.nodes, [t_node] * new_nodes.shape[1],\n                            new_nodes, axis=1)\n\n    # Transform back to csc format and fix node ordering.\n    g.face_nodes = g.face_nodes.tocsc()\n    g.face_nodes.indices = g.face_nodes.indices[iv]  # For fast row operation\n\n    return node_count",
  "def sort_sub_list(indices, indptr):\n    ix = np.zeros(indices.size, dtype=int)\n    for i in range(indptr.size - 1):\n        sub_ind = slice(indptr[i], indptr[i + 1])\n        loc_ix = np.argsort(indices[sub_ind])\n        ix[sub_ind] = loc_ix + indptr[i]\n    indices = indices[ix]\n    iv = np.zeros(indices.size, dtype=int)\n    iv[ix] = np.arange(indices.size)\n    return indices, iv",
  "def find_cell_color(g, cells):\n    \"\"\"\n    Color the cells depending on the cell connections. Each group of cells\n    that are connected (either directly by a shared face or through a series\n    of shared faces of many cells) is are given different colors.\n           c_1-c_3     c_4\n         /\n       c_7  |           |\n         \\\n           c_2         c_5\n    In this case, cells c_1, c_2, c_3 and c_7 will be given color 0, while\n    cells c_4 and c_5 will be given color 1.\n\n    Parameters:\n    ----------\n    g        - Grid for which the cells belong\n    cells    - indecies of cells (=np.array([1,2,3,4,5,7]) for case above)\n    \"\"\"\n    c = np.sort(cells)\n    # Local cell-face and face-node maps.\n    cf_sub, _ = __extract_submatrix(g.cell_faces, c)\n    child_cell_ind = np.array([-1] * g.num_cells, dtype=np.int)\n    child_cell_ind[c] = np.arange(cf_sub.shape[1])\n\n    # Create a copy of the cell-face relation, so that we can modify it at\n    # will\n    cell_faces = cf_sub.copy()\n    # Direction of normal vector does not matter here, only 0s and 1s\n    cell_faces.data = np.abs(cell_faces.data)\n\n    # Find connection between cells via the cell-face map\n    c2c = cell_faces.transpose() * cell_faces\n    # Only care about absolute values\n    c2c.data = np.clip(c2c.data, 0, 1).astype('bool')\n\n    graph = Graph(c2c)\n    graph.color_nodes()\n    return graph.color[child_cell_ind[cells]]",
  "def avg_normal(g, faces):\n    \"\"\"\n    Calculates the average face normal of a set of faces. The average normal\n    is only constructed from the boundary faces, that is, a face thatbelongs\n    to exactly one cell. If a face is not a boundary face, it will be ignored.\n    The faces normals are fliped such that they point out of the cells.\n\n    Parameters:\n    ----------\n    g         - Grid\n    faces     - Face indecies of face normals that should be averaged\n    \"\"\"\n    frac_face = np.ravel(\n        np.sum(np.abs(g.cell_faces[faces, :]), axis=1) == 1)\n    f, _, sign = sps.find(g.cell_faces[faces[frac_face], :])\n    n = g.face_normals[:, faces[frac_face]]\n    n = n[:, f] * sign\n    n = np.mean(n, axis=1)\n    n = n / np.linalg.norm(n)\n    return n",
  "def remove_nodes(g, rem):\n    \"\"\"\n    Remove nodes from grid.\n    g - a valid grid definition\n    rem - a ndarray of indecies of nodes to be removed\n    \"\"\"\n    all_rows = np.arange(g.face_nodes.shape[0])\n    rows_to_keep = np.where(np.logical_not(np.in1d(all_rows, rem)))[0]\n    g.face_nodes = g.face_nodes[rows_to_keep, :]\n    g.nodes = g.nodes[:, rows_to_keep]\n    return g",
  "def __extract_submatrix(mat, ind):\n    \"\"\" From a matrix, extract the column specified by ind. All zero columns\n    are stripped from the sub-matrix. Mappings from global to local row numbers\n    are also returned.\n    \"\"\"\n    sub_mat = mat[:, ind]\n    cols = sub_mat.indptr\n    rows = sub_mat.indices\n    data = sub_mat.data\n    unique_rows, rows_sub = np.unique(sub_mat.indices,\n                                      return_inverse=True)\n    return sps.csc_matrix((data, rows_sub, cols)), unique_rows",
  "def merge_grids(grids, intersections, tol=1e-4):\n    \"\"\" Main method of module, merge all grids\n    \"\"\"\n    list_of_grids, global_ind_offset = init_global_ind(grids)\n    grids_1d = process_intersections(grids, intersections, global_ind_offset,\n                                     list_of_grids, tol)\n    grid_list_by_dim = [[], [], []]\n\n    grid_list_by_dim[1] = grids_1d\n\n    for g in grids:\n        grid_list_by_dim[0].append(g[0][0])\n\n    return grid_list_by_dim",
  "def init_global_ind(gl):\n    \"\"\" Initialize a global indexing of nodes to a set of local grids.\n\n    Parameters:\n        gl (triple list): Outer: One per fracture, middle: dimension, inner:\n            grids within the dimension.\n\n    Returns:\n        list (Grid): Single list representation of all grids. Fractures and\n            dimensions will be mixed up.\n        int: Global number of nodes.\n\n    \"\"\"\n\n    list_of_grids = []\n\n    # The global point index is for the moment set within each fracture\n    # (everybody start at 0). Adjust this.\n    global_ind_offset = 0\n    # Loop over fractures\n    for frac_ind, f in enumerate(gl):\n        # The 2d grid is the only item in the middle list\n        f[0][0].frac_num = frac_ind\n\n        # Loop over dimensions within the fracture\n        for gd in f:\n            # Loop over grids within the dimension\n            for g in gd:\n                g.global_point_ind += global_ind_offset\n                list_of_grids.append(g)\n        # Increase the offset with the total number of nodes on this fracture\n        global_ind_offset += f[0][0].num_nodes\n\n    return list_of_grids, global_ind_offset",
  "def process_intersections(grids, intersections, global_ind_offset,\n                          list_of_grids, tol):\n    \"\"\" Loop over all intersections, combined two and two grids.\n    \"\"\"\n\n    # All connections will be hit upon twice, one from each intersecting fracture.\n    # Keep track of which connections have been treated, and can be skipped.\n    num_frac = len(grids)\n    isect_is_processed = sps.lil_matrix((num_frac, num_frac), dtype=np.bool)\n\n    grid_1d_list = []\n\n    # Loop over all fractures\n    for frac_ind, frac in enumerate(grids):\n\n        # Pick out the 2d grid of this fracture\n        g = grids[frac_ind][0][0]\n\n        # Loop over all 1d grids in this fracture\n        for ind_1d, g_1d in enumerate(frac[1]):\n\n            # Find index and grid of the other fracture of this intersection\n            other_frac_ind = intersections[frac_ind][ind_1d]\n            h = grids[other_frac_ind][0][0]\n\n            # Check if we have treated this intersection before\n            row = g.frac_num\n            col = h.frac_num\n            if isect_is_processed[row, col]:\n                continue\n            else:\n                # Mark the connection as treated\n                isect_is_processed[row, col] = True\n                isect_is_processed[col, row] = True\n\n            # Find the 1d grid of this intersection, as created from the other fracture\n            g_in_other = \\\n                np.where(intersections[other_frac_ind] == frac_ind)[0][0]\n            h_1d = grids[other_frac_ind][1][g_in_other]\n            g_1d.compute_geometry()\n            h_1d.compute_geometry()\n            g_new_1d, global_ind_offset = combine_grids(g, g_1d, h, h_1d,\n                                                        global_ind_offset,\n                                                        list_of_grids, tol)\n            # Append the new 1d grid to the general list of grids, so that it\n            # will have its global point indices updated as we go.\n            grid_1d_list.append(g_new_1d)\n    return grid_1d_list",
  "def combine_grids(g, g_1d, h, h_1d, global_ind_offset, list_of_grids, tol):\n\n    combined_1d, global_ind_offset, g_in_combined, h_in_combined,\\\n        g_sort, h_sort = merge_1d_grids(g_1d, h_1d, global_ind_offset, tol)\n\n    # First update fields for first grid\n    fn_orig = np.reshape(g.face_nodes.indices, (2, g.num_faces), order='F')\n    node_coord_orig = g.nodes.copy()\n    new_nodes, delete_faces, global_ind_offset =\\\n        update_nodes(g, g_1d, combined_1d, g_in_combined, g_sort,\n                     global_ind_offset, list_of_grids)\n\n    num_new_faces = combined_1d.num_cells\n    new_nodes_offset = new_nodes[0]\n    new_faces = update_face_nodes(g, delete_faces, num_new_faces,\n                                  new_nodes_offset)\n\n    update_cell_faces(g, delete_faces, new_faces, g_in_combined, fn_orig,\n                      node_coord_orig)\n\n    # Then updates for the second grid\n    fn_orig = np.reshape(h.face_nodes.indices, (2, h.num_faces), order='F')\n    node_coord_orig = h.nodes.copy()\n    new_nodes, delete_faces, global_ind_offset =\\\n        update_nodes(h, h_1d, combined_1d, h_in_combined, h_sort,\n                     global_ind_offset, list_of_grids)\n\n    new_nodes_offset = new_nodes[0]\n    new_faces = update_face_nodes(h, delete_faces, num_new_faces,\n                                  new_nodes_offset)\n\n    update_cell_faces(h, delete_faces, new_faces, h_in_combined, fn_orig,\n                      node_coord_orig)\n\n    return combined_1d, global_ind_offset",
  "def merge_1d_grids(g, h, global_ind_offset=0, tol=1e-4):\n    \"\"\" Merge two 1d grids with non-matching nodes to a single grid.\n\n    The grids should have common start and endpoints. They can be into 3d space\n    in a genreal way.\n\n    The function is primarily intended for merging non-conforming DFN grids.\n\n    Parameters:\n        g: 1d tensor grid.\n        h: 1d tensor grid\n        glob_ind_offset (int, defaults to 0): Off set for the global point\n            index of the new grid.\n        tol (double, defaults to 1e-4): Tolerance for when two nodes are merged\n            into one.\n\n    Returns:\n        TensorGrid: New tensor grid, containing the combined node definition.\n        int: New global ind offset, increased by the number of cells in the\n            combined grid.\n        np.array (int): Indices of common nodes (after sorting) of g and the\n            new grid.\n        np.array (int): Indices of common nodes (after sorting) of h and the\n            new grid.\n        np.array (int): Permutation indices that sort the node coordinates of\n            g. The common indices between g and the new grid are found as\n            new_grid.nodes[:, g_in_combined] = g.nodes[:, sorted]\n        np.array (int): Permutation indices that sort the node coordinates of\n            h. The common indices between h and the new grid are found as\n            new_grid.nodes[:, h_in_combined] = h.nodes[:, sorted]\n\n    \"\"\"\n\n    # Nodes of the two 1d grids, combine them\n    gp = g.nodes\n    hp = h.nodes\n    combined = np.hstack((gp, hp))\n\n    num_g = gp.shape[1]\n    num_h = hp.shape[1]\n\n    # Keep track of where we put the indices of the original grids\n    g_in_full = np.arange(num_g)\n    h_in_full = num_g + np.arange(num_h)\n\n    # The tolerance should not be larger than the smallest distance between\n    # two points on any of the grids.\n    diff_gp = np.min(cg.dist_pointset(gp, True))\n    diff_hp = np.min(cg.dist_pointset(hp, True))\n    min_diff = np.minimum(tol, 0.5 * np.minimum(diff_gp, diff_hp))\n\n    # Uniquify points\n    combined_unique, _, new_2_old = unique_columns_tol(combined, tol=min_diff)\n    # Follow locations of the original grid points\n    g_in_unique = new_2_old[g_in_full]\n    h_in_unique = new_2_old[h_in_full]\n\n    # The combined nodes must be sorted along their natural line.\n    # Find the dimension with the largest spatial extension, and sort those\n    # coordinates\n    max_coord = combined_unique.max(axis=1)\n    min_coord = combined_unique.min(axis=1)\n    dx = max_coord - min_coord\n    sort_dim = np.argmax(dx)\n\n    sort_ind = np.argsort(combined_unique[sort_dim])\n    combined_sorted = combined_unique[:, sort_ind]\n\n    # Follow the position of the orginial nodes through sorting\n    _, g_sorted = ismember_rows(g_in_unique, sort_ind)\n    _, h_sorted = ismember_rows(h_in_unique, sort_ind)\n\n    num_new_grid = combined_sorted.shape[1]\n\n    # Create a new 1d grid.\n    # First use a 1d coordinate to initialize topology\n    new_grid = TensorGrid(np.arange(num_new_grid))\n    # Then set the right, 3d coordinates\n    new_grid.nodes = cg.make_collinear(combined_sorted)\n\n    # Set global point indices\n    new_grid.global_point_ind = global_ind_offset + np.arange(num_new_grid)\n    global_ind_offset += num_new_grid\n\n    return new_grid, global_ind_offset, g_sorted, h_sorted, np.arange(num_g),\\\n        np.arange(num_h)",
  "def update_global_point_ind(grid_list, old_ind, new_ind):\n    \"\"\" Update global point indices in a list of grids.\n\n    The method replaces indices in the attribute global_point_ind in the grid.\n    The update is done in place.\n\n    Parameters:\n        grid_list (list of grids): Grids to be updated.\n        old_ind (np.array): Old global indices, to be replaced.\n        new_ind (np.array): New indices.\n\n    \"\"\"\n    for g in grid_list:\n        ismem, o2n = ismember_rows(old_ind, g.global_point_ind)\n        g.global_point_ind[o2n] = new_ind[ismem]",
  "def update_nodes(g, g_1d, new_grid_1d, this_in_combined, sort_ind,\n                 global_ind_offset, list_of_grids):\n    \"\"\" Update a 2d grid to conform to a new grid along a 1d line.\n\n    Intended use: A 1d mesh that is embedded in a 2d mesh (along a fracture)\n    has been updated / refined. This function then updates the node information\n    in the 2d grid.\n\n    Parameters:\n        g (grid, dim==2): Main grid to update. Has faces along a fracture.\n        g_1d (grid, dim==1): Original line grid along the fracture.\n        new_grid_1d (grid, dim==1): New line grid, formed by merging two\n            coinciding, but non-matching grids along a fracture.\n        this_in_combined (np.ndarray): Which nodes in the new grid are also in\n            the old one, as returned by merge_1d_grids().\n        sort_ind (np.ndarray): Sorting indices of the coordinates of the old\n            grid, as returned by merge_1d_grids().\n        list_of_grids (list): Grids for all dimensions.\n\n    Returns:\n\n\n    \"\"\"\n    nodes_per_face = 2\n    # Face-node relation for the grid, in terms of local and global indices\n    fn = g.face_nodes.indices.reshape((nodes_per_face, g.num_faces), order='F')\n    fn_glob = np.sort(g.global_point_ind[fn], axis=0)\n\n    # Mappings between faces in 2d grid and cells in 1d\n    # 2d faces along the 1d grid will be deleted.\n    delete_faces, cell_1d = fracutils.obtain_interdim_mappings(g_1d, fn_glob,\n                                                               nodes_per_face)\n\n    # All 1d cells should be identified with 2d faces\n    assert cell_1d.size == g_1d.num_cells, \"\"\" Failed to find mapping between\n        1d cells and 2d faces\"\"\"\n\n    # The nodes of identified faces on the 2d grid will be deleted\n    delete_nodes = np.unique(fn[:, delete_faces])\n\n    # Nodes to be added will have indicies towards the end\n    num_nodes_orig = g.num_nodes\n    num_delete_nodes = delete_nodes.size\n    num_nodes_not_on_fracture = num_nodes_orig - num_delete_nodes\n\n    # Define indices of new nodes.\n    new_nodes = num_nodes_orig - delete_nodes.size\\\n        + np.arange(new_grid_1d.num_nodes)\n\n    # Adjust node indices in the face-node relation\n    # First, map nodes between 1d and 2d grids. Use sort_ind here to map\n    # indices of g_1d to the same order as the new grid\n    _, node_map_1d_2d = ismember_rows(g.global_point_ind[delete_nodes],\n                                      g_1d.global_point_ind)\n    tmp = np.arange(g.num_nodes)\n    adjustment = np.zeros_like(tmp)\n    adjustment[delete_nodes] = 1\n    node_adjustment = tmp - np.cumsum(adjustment)\n    # Nodes along the 1d grid are deleted and inserted again. Let the\n    # adjutsment point to the restored nodes.\n    # node_map_1d_2d maps from ordering in delete_nodes to ordering of 1d\n    # points (old_grid). this_in_combined then maps further to the ordering of\n    # the new 1d grid\n    node_adjustment[delete_nodes] =\\\n        g.num_nodes - num_delete_nodes + this_in_combined[node_map_1d_2d]\n\n    g.face_nodes.indices = node_adjustment[g.face_nodes.indices]\n\n    # Update node coordinates and global indices for 2d mesh\n    g.nodes = np.hstack((g.nodes, new_grid_1d.nodes))\n\n    new_global_points = new_grid_1d.global_point_ind\n    g.global_point_ind = np.append(g.global_point_ind, new_global_points)\n\n    # Global index of deleted points\n    old_global_pts = g.global_point_ind[delete_nodes]\n\n    # Update any occurences of the old points in other grids. When sewing\n    # together a DFN grid, this may involve more and more updates as common\n    # nodes are found along intersections.\n\n    # The new grid should also be added to the list, if it is not there before\n    if not new_grid_1d in list_of_grids:\n        list_of_grids.append(new_grid_1d)\n    update_global_point_ind(list_of_grids, old_global_pts,\n                            new_global_points[this_in_combined[node_map_1d_2d]])\n\n    # Delete old nodes\n    g.nodes = np.delete(g.nodes, delete_nodes, axis=1)\n    g.global_point_ind = np.delete(g.global_point_ind, delete_nodes)\n\n    g.num_nodes = g.nodes.shape[1]\n    return new_nodes, delete_faces, global_ind_offset",
  "def update_face_nodes(g, delete_faces, num_new_faces, new_node_offset,\n                      nodes_per_face=None):\n    \"\"\" Update face-node map by deleting and inserting new faces.\n\n    The method deletes specified faces, adds new ones towards the end. It does\n    nothing to adjust the face-node relation for remaining faces.\n\n    The code assumes a constant number of nodes per face.\n\n    Parameters:\n        g (grid): Grid to have its faces modified. Should have fields\n            face_nodes and num_faces.\n        delete_faces (np.array): Index of faces to be deleted.\n        num_new_faces (int): Number of new faces to create.\n        new_node_offset (int): Offset index of the new nodes.\n        nodes_per_face (int, optional): Number of nodes per face, assumed equal\n            for all faces. Defaults to g.dim, that is, simplex grids\n\n    Returns:\n        np.array: Index of the new faces.\n\n    \"\"\"\n\n    if nodes_per_face is None:\n        nodes_per_face = g.dim\n\n    # Indices of new nodes.\n    new_face_nodes = np.tile(np.arange(num_new_faces), (nodes_per_face, 1)) \\\n        + np.arange(nodes_per_face).reshape((nodes_per_face, 1))\n    # Offset the numbering: The new nodes are inserted after all outside nodes\n    new_face_nodes = new_node_offset + new_face_nodes\n    # Number of new faces in mesh\n    ind_new_face = g.num_faces - delete_faces.size + np.arange(num_new_faces)\n\n    # Modify face-node map\n    # First obtain face-node relation as a matrix. Thankfully, we know the\n    # number of nodes per face.\n    fn = g.face_nodes.indices.reshape((nodes_per_face, g.num_faces), order='F')\n    # Delete old faces\n    fn = np.delete(fn, delete_faces, axis=1)\n    # Add new face-nodes\n    fn = np.append(fn, new_face_nodes, axis=1)\n\n    indices = fn.flatten(order='F')\n\n    # Trivial updates of data and indptr. Fortunately, this is 2d\n    data = np.ones(fn.size, dtype=np.bool)\n    indptr = np.arange(0, fn.size + 1, nodes_per_face)\n    g.face_nodes = sps.csc_matrix((data, indices, indptr))\n    g.num_faces = int(fn.size / nodes_per_face)\n    assert g.face_nodes.indices.max() < g.nodes.shape[1]\n\n    return ind_new_face",
  "def update_cell_faces(g, delete_faces, new_faces, in_combined, fn_orig,\n                      node_coord_orig, tol=1e-4):\n    \"\"\" Replace faces in a cell-face map.\n\n    If faces have been refined (or otherwise modified), it is necessary to\n    update the cell-face relation as well. This function does so, while taking\n    care that the (implicit) mapping between cells and nodes is ordered so that\n    geometry computation still works.\n\n    The changes of g.cell_faces are done in-place.\n\n    It is assumed that the new faces that replace an old are ordered along the\n    common line. E.g. if a face with node coordinates (0, 0) and (3, 0) is\n    replaced by three new faces of unit length, they should be ordered as\n    1. (0, 0) - (1, 0)\n    2. (1, 0) - (2, 0)\n    3. (2, 0) - (3, 0)\n    Switching the order into 3, 2, 1 is okay, but, say, 1, 3, 2 will create\n    problems.\n\n    It is also tacitly assumed that each cell has at most one face deleted.\n    Changing this may not be difficult, but has not been prioritized so far.\n\n    The function has been tested in 2d only, reliability in 3d is unknown,\n    but doubtful.\n\n    Parameters:\n        g (grid): To be updated.\n        delete_faces (np.ndarray): Faces to be deleted, as found in\n            g.cell_faces\n        new_faces (np.ndarray): Index of new faces, as found in g.face_nodes\n        in_combined (np.ndarray): Map between old and new faces.\n            delete_faces[i] is replaced by\n            new_faces[in_combined[i]:in_combined[i+1]].\n        fn_orig (np.ndarray): Face-node relation of the orginial grid, before\n            update of faces.\n        node_coord_orig (np.ndarray): Node coordinates of orginal grid,\n            before update of nodes.\n        tol (double, defaults to 1e-4): Small tolerance, used to compare\n            coordinates of points.\n\n    \"\"\"\n\n    #\n\n    nodes_per_face = g.dim\n\n    cell_faces = g.cell_faces\n\n    # Mapping from new\n    deleted_2_new_faces = np.empty(in_combined.size - 1, dtype=object)\n\n    # The nodes in the original 1d grid was sorted either in the same way, or\n    # in the oposite order of the new grid. In the latter case, we need to\n    # reverse the order of in_combined to reconstruct the old face-node\n    # relations\n    if in_combined[0] < in_combined[-1]:\n        for i in range(deleted_2_new_faces.size):\n            if in_combined[i] == in_combined[i + 1]:\n                deleted_2_new_faces[i] = new_faces[in_combined[i]]\n            else:\n                deleted_2_new_faces[i] = new_faces[np.arange(in_combined[i],\n                                                             in_combined[i + 1])]\n#            assert deleted_2_new_faces[i].size > 0, \\\n#                str(i)+\" \"+str(in_combined[i])+\" \"+str(in_combined[i+1])+\\\n#                \" \"+str(np.arange(in_combined[i], in_combined[i+1]))\n    else:\n        for i in range(deleted_2_new_faces.size):\n            if in_combined[i] == in_combined[i + 1]:\n                print(new_faces)\n                deleted_2_new_faces[i] = new_faces[in_combined[i]]\n            else:\n                deleted_2_new_faces[i] = new_faces[np.arange(in_combined[i + 1],\n                                                             in_combined[i])]\n#            assert deleted_2_new_faces[i].size > 0, \\\n#                str(i)+\" \"+str(in_combined[i+1])+\" \"+str(in_combined[i])+\\\n#                \" \"+str(np.arange(in_combined[i+1], in_combined[i]))\n\n    # Now that we have mapping from old to new faces, also update face tags\n    update_face_tags(g, delete_faces, deleted_2_new_faces)\n\n    # The cell-face relations\n    cf = cell_faces.indices\n    indptr = cell_faces.indptr\n\n    # Find elements in the cell-face relation that are also along the\n    # intersection, and should be replaced\n    hit = np.where(np.in1d(cf, delete_faces))[0]\n\n    # Mapping from cell_face of 2d grid to cells in 1d grid. Can be combined\n    # with deleted_2_new_faces to match new and old faces\n    # Safeguarding (or stupidity?): Only faces along 1d grid have non-negative\n    # index, but we should never hit any of the other elements\n    cf_2_f = -np.ones(delete_faces.max() + 1, dtype=np.int)\n    cf_2_f[delete_faces] = np.arange(delete_faces.size)\n\n    # Map from faces, as stored in cell_faces,to the corresponding cells\n    face_2_cell = rldecode(np.arange(indptr.size), np.diff(indptr))\n\n    # The cell-face map will go from 3 faces per cell to an arbitrary number.\n    # Split mapping into list of arrays to prepare for this\n    new_cf = [cf[indptr[i]:indptr[i + 1]] for i in range(g.num_cells)]\n    # Similar treatment of direction of normal vectors\n    new_sgn = [g.cell_faces.data[indptr[i]:indptr[i + 1]]\n               for i in range(g.num_cells)]\n\n    # Create mapping to adjust face indices for deletions\n    tmp = np.arange(cf.max() + 1)\n    adjust_deleted = np.zeros_like(tmp)\n    adjust_deleted[delete_faces] = 1\n    face_adjustment = tmp - np.cumsum(adjust_deleted)\n\n    # Face-node relations as array\n    fn = g.face_nodes.indices.reshape((nodes_per_face, g.num_faces), order='F')\n\n    # Collect indices of cells that have one of their faces on the fracture.\n    hit_cell = []\n\n    for i in hit:\n        # The loop variable refers to indices in the face-cell map. Get cell\n        # number.\n        cell = face_2_cell[i]\n        hit_cell.append(cell)\n        # For this cell, find where in the cell-face map the fracture face is\n        # placed.\n        tr = np.where(new_cf[cell] == cf[i])[0]\n        # There should be only one face on the fracture\n        assert tr.size == 1\n        tr = tr[0]\n\n        # Implementation note: If we ever get negative indices here, something\n        # has gone wrong related to cf_2_f, see above.\n        # Digestion of loop: i (in hit) refers to elements in cell-face\n        # cf[i] is specific face\n        # cf_2_f[cf[i]] maps to deleted face along fracture\n        # outermost is one-to-many map from deleted to new faces.\n        new_faces_loc = deleted_2_new_faces[cf_2_f[cf[i]]]\n\n        # Index of the replaced face\n        ci = cf[i]\n\n        # We need to sort the new face-cell relation so that the edges defined\n        # by cell-face-> face_nodes form a closed, non-intersecting loop. If\n        # this is not the case, geometry computation will go wrong.\n        # By assumption, the new faces are defined so that their nodes are\n        # contiguous along the line of the old face.\n\n        # Coordinates of the nodes of the replaced face.\n        # Note use of original coordinates here.\n        ci_coord = node_coord_orig[:, fn_orig[:, ci]]\n        # Coordinates of the nodes of the first new face\n        fi_coord = g.nodes[:, fn[:, new_faces_loc[0]]]\n\n        # Distance between the new nodes and the first node of the old face.\n        dist = cg.dist_point_pointset(ci_coord[:, 0], fi_coord)\n        # Length of the old face.\n        length_face = cg.dist_point_pointset(ci_coord[:, 0], ci_coord[:, 1])[0]\n        # If the minimum distance is larger than a (scaled) tolerance, the new\n        # faces were defined from the second to the first node. Switch order.\n        # This will create trouble if one of the new faces are very small.\n        if dist.min() > length_face * tol:\n            new_faces_loc = new_faces_loc[::-1]\n\n        # Replace the cell-face relation for this cell.\n        # At the same time (stupid!) also adjust indices of the surviving\n        # faces.\n        new_cf[cell] = np.hstack((face_adjustment[new_cf[cell][:tr].ravel()],\n                                  new_faces_loc,\n                                  face_adjustment[new_cf[cell][tr + 1:].ravel()]))\n        # Also replicate directions of normal vectors\n        new_sgn[cell] = np.hstack((new_sgn[cell][:tr].ravel(),\n                                   np.tile(new_sgn[cell][tr],\n                                           new_faces_loc.size),\n                                   new_sgn[cell][tr + 1:].ravel()))\n\n    # Adjust face index of cells that have no contact with the updated faces\n    for i in np.setdiff1d(np.arange(len(new_cf)), hit_cell):\n        new_cf[i] = face_adjustment[new_cf[i]]\n\n    # New pointer structure for cell-face relations\n    num_cell_face = np.array([new_cf[i].size for i in range(len(new_cf))])\n    indptr_new = np.hstack((0, np.cumsum(num_cell_face)))\n\n    ind = np.concatenate(new_cf)\n    data = np.concatenate(new_sgn)\n    # All faces in the cell-face relation should be referred to by 1 or 2 cells\n    assert np.bincount(ind).max() <= 2\n    assert np.all(np.bincount(ind) > 0)\n\n    g.cell_faces = sps.csc_matrix((data, ind, indptr_new))",
  "def update_face_tags(g, delete_faces, new_faces):\n    \"\"\" Update the face tags of a cell.\n\n    Delete tags for old faces, and add new tags for their replacements.\n\n    If the grid has no face tags, no change is done\n\n    Parameters:\n        g (grid): To be modified\n        delete_faces (np.array or list): Faces to be deleted.\n        new_faces (list of list): For each item in delete_faces, a list of new\n            replacement faces.\n\n    \"\"\"\n    keys = tags.standard_face_tags()\n    for key in keys:\n        if hasattr(g, 'tags'):\n            old_tags = g.tags[key].copy()\n            old_tags = np.delete(old_tags, delete_faces)\n            num_new = np.array([len(new_faces[i])\n                                for i in range(len(new_faces))])\n            new_tags = np.zeros(num_new.sum(), dtype=bool)\n            divides = np.hstack((0, np.cumsum(num_new)))\n            for i, d in enumerate(delete_faces):\n                new_tags[divides[i]: divides[i + 1]] = g.tags[key][d]\n            g.tags[key] = np.hstack((old_tags, new_tags))",
  "def fractures_from_outcrop(pt, edges, ensure_realistic_cuts=True, family=None,\n                           **kwargs):\n    \"\"\" Create a set of fractures compatible with exposed lines in an outcrop.\n\n    See module-level documentation for futher comments.\n\n    Parameters:\n        pt (np.array, 2 x num_pts): Coordinates of start and endpoints of\n            extruded lines.\n        edges (np.array, 2 x num_pts): Connections between points. Should not\n            have their crossings removed before.\n        ensure_realistic_cut (boolean, defaults to True): If True, we ensure\n            that T-intersections do not have cut fractures that extend beyond\n            the confining fracture. May overrule user-supplied controls on\n            fracture sizes.\n        **kwargs: Potentially user defined options. Forwarded to\n            discs_from_exposure() and impose_inclines()\n\n    Returns:\n        list of Fracture: Fracture planes.\n\n    \"\"\"\n    logging.info('Extrusion recieved ' + str(edges.shape[1]) + ' lines')\n    assert edges.shape[0] == 2, 'Edges have two endpoints'\n    edges = np.vstack((edges, np.arange(edges.shape[1], dtype=np.int)))\n\n    # identify crossings\n    logging.info('Identify crossings')\n    split_pt, split_edges = cg.remove_edge_crossings(pt, edges, **kwargs)\n    logging.info('Fractures composed of ' + str(split_edges.shape[0]) + 'branches')\n\n    # Find t-intersections\n    abutment_pts, prim_frac, sec_frac, other_pt = t_intersections(split_edges)\n    logging.info('Found ' + str(prim_frac.size) + ' T-intersections')\n\n    # Calculate fracture lengths\n    lengths = fracture_length(pt, edges)\n\n    # Extrude to fracture discs\n    logging.info('Create discs from exposure')\n    fractures, extrude_ang = discs_from_exposure(pt, edges, **kwargs)\n\n    p0 = pt[:, edges[0]]\n    p1 = pt[:, edges[1]]\n    exposure = p1 - p0\n\n    # Impose incline.\n    logging.info('Impose incline')\n    rot_ang = impose_inlcine(fractures, exposure, p0, frac_family=family,\n                             **kwargs)\n\n    # Cut fractures\n    for prim, sec, p in zip(prim_frac, sec_frac, other_pt):\n        _, radius = cut_fracture_by_plane(fractures[sec], fractures[prim],\n                                          split_pt[:, p], **kwargs)\n        # If specified, ensure that cuts in T-intersections appear realistic.\n        if ensure_realistic_cuts and radius is not None:\n            ang = np.arctan2(0.5*lengths[prim], radius)\n\n            # Ensure the center of both fractures are on the same side of the\n            # exposed plane - if not, the cut will still be bad.\n            if extrude_ang[sec] > np.pi/2 and ang < np.pi/2:\n                ang = np.pi-ang\n            elif extrude_ang[sec] < np.pi/2 and ang > np.pi/2:\n                ang = np.pi-ang\n\n            e0 = p0[:, prim]\n            e1 = p1[:, prim]\n            new_radius, center, _ = disc_radius_center(lengths[prim], e0, e1,\n                                                       theta=ang)\n            strike = np.arctan2(e1[1] - e0[1], e1[0]- e0[0])\n            f = create_fracture(center, new_radius, np.pi/2, strike,\n                                np.vstack((e0, e1)).T)\n            rotate_fracture(f, e1-e0, rot_ang[prim], p0[:, prim])\n            fractures[prim] = f\n\n    return fractures",
  "def _intersection_by_num_node(edges, num):\n    \"\"\" Find all edges involved in intersections with a certain number of\n    intersecting lines.\n\n    Parameters:\n        edges: fractures\n        num: Target number of nodes in intersections\n\n    Returns:\n        crosses: Nodes with the prescribed number of edges meeting.\n        edges_of_crosses (n x num): Each row gives edges meeting in a node.\n\n    \"\"\"\n    num_occ = np.bincount(edges[:2].ravel())\n    crosses = np.where(num_occ == num)[0]\n\n    num_crosses = crosses.size\n\n    edges_of_crosses = np.zeros((num_crosses, num), dtype=np.int)\n    for i, pi in enumerate(crosses):\n        edges_of_crosses[i] = np.where(np.any(edges[:2] == pi, axis=0))[0]\n    return crosses, edges_of_crosses",
  "def t_intersections(edges, remove_three_families=True):\n    \"\"\" Find points involved in T-intersections.\n\n    A t-intersection is defined as a point involved in three fracture segments,\n    of which two belong to the same fracture.\n\n    The fractures should have been split (cg.remove_edge_crossings) before\n    calling this function.\n\n    Parameters:\n        edges (np.array, 3 x n): Fractures. First two rows give indices of\n            start and endpoints. Last row gives index of fracture that the\n            segment belongs to.\n        remove_three_families (boolean, defaults to True): If True,\n            T-intersections with where fractures from three different families\n            meet will be removed.\n\n    Returns:\n        abutments (np.ndarray, int): indices of points that are\n            T-intersections\n        primal_frac (np.ndarray, int): Index of edges that are split by a\n            t-intersection\n        sec_frac (np.ndarray, int): Index of edges that ends in a\n            T-intersection\n        other_point (np.ndarray, int): For the secondary fractures, the end\n            that is not in the t-intersection.\n\n    \"\"\"\n    frac_num = edges[-1]\n    abutments, edges_of_abutments = _intersection_by_num_node(edges, 3)\n\n    # Initialize fields for abutments.\n    num_abut = abutments.size\n    primal_frac = np.zeros(num_abut, dtype=np.int)\n    sec_frac = np.zeros(num_abut, dtype=np.int)\n    other_point = np.zeros(num_abut, dtype=np.int)\n\n    # If fractures meeting in a T-intersection all have different family names,\n    # these will be removed from the list.\n    remove = np.zeros(num_abut, dtype=np.bool)\n\n    for i, (pi, ei) in enumerate(zip(abutments, edges_of_abutments)):\n        # Count number of occurences for each fracture associated with this\n        # intersection.\n        fi_all = frac_num[ei]\n        fi, count = np.unique(fi_all, return_counts=True)\n        if fi.size > 2:\n             remove[i] = 1\n             continue\n        # Find the fracture number associated with main and abutting edge.\n        if count[0] == 1:\n            primal_frac[i] = fi[1]\n            sec_frac[i] = fi[0]\n        else:\n            primal_frac[i] = fi[0]\n            sec_frac[i] = fi[1]\n        # Also find the other point of the abutting edge\n        ind = np.where(fi_all == sec_frac[i])\n        ei_abut = ei[ind]\n        assert ei_abut.size == 1\n        if edges[0, ei_abut] == pi:\n            other_point[i] = edges[1, ei_abut]\n        else:\n            other_point[i] = edges[0, ei_abut]\n\n    # Remove any T-intersections that did not belong to\n    if remove_three_families and remove.any():\n         remove = np.where(remove)[0]\n         abutments = np.delete(abutments, remove)\n         primal_frac = np.delete(primal_frac, remove)\n         sec_frac = np.delete(sec_frac, remove)\n         other_point = np.delete(other_point, remove)\n\n    return abutments, primal_frac, sec_frac, other_point",
  "def x_intersections(edges):\n    \"\"\" Obtain nodes and edges involved in an x-intersection\n\n    A x-intersection is defined as a point involved in four fracture segments,\n    with two pairs belonging to two fractures each.\n\n    The fractures should have been split (cg.remove_edge_crossings) before\n    calling this function.\n\n    Parameters:\n        edges (np.array, 3 x n): Fractures. First two rows give indices of\n            start and endpoints. Last row gives index of fracture that the\n            segment belongs to.\n    Returns:\n        nodes: Index of nodes that form x intersections\n        x_fracs (2xn): Index of fractures crossing in the nodes\n        x_edges (4xn): Index of edges crossing in the nodes\n\n    \"\"\"\n    frac_num = edges[-1]\n    nodes, x_edges = _intersection_by_num_node(edges, 4)\n\n    # Convert from edges (split fractures) to fractures themselves.\n    num_x = nodes.size\n    x_fracs = np.zeros((2, num_x))\n    for i, ei in enumerate(frac_num[x_fracs]):\n        x_fracs[:, i] = np.unique(ei)\n    return nodes, x_fracs, x_edges",
  "def fracture_length(pt, e):\n    \"\"\" Compute length of fracture lines.\n\n    Parameters:\n        pt (np.array, 2xnpt): Coordinates of fracture endpoints\n        e (np.array, 2xn_frac): Index of fracture endpoints.\n\n    Returns:\n        np.array, n_frac: Length of fractures.\n\n    \"\"\"\n    x0 = pt[0, e[0]]\n    x1 = pt[0, e[1]]\n    y0 = pt[1, e[0]]\n    y1 = pt[1, e[1]]\n\n    return np.sqrt(np.power(x1-x0, 2) + np.power(y1-y0, 2))",
  "def disc_radius_center(lengths, p0, p1, theta=None):\n    \"\"\" Compute radius and center of a disc, based on the length of a chord\n    through the disc, and assumptions on the location of the chord.\n\n    The relation between the exposure and the center of the fracture is\n    given by the theta, which gives the angle between a vertical line through\n    the disc center and the line through the disc center and any of the\n    exposure endpoints. If no values are given a random value is assigned,\n    corresponding to an arbitrary portion of the original (disc-shaped)\n    fracture has been eroded.\n\n    Parameters:\n        length (np.array, double, size: num_frac): Of the chords\n        p0 (np.array, 2 x num_frac): One endpoint of fractures.\n        p1 (np.array, 2 x num_frac): Second endpoint of fractures\n        angle (np.array, num_frac, optional): Angle determining disc center,\n            see description above. Defaults to random values.\n\n    Returns:\n        np.array, num_frac: Radius of discs\n        np.array, 3 x num_frac: Center of the discs (assuming vertical disc)\n\n    \"\"\"\n\n    num_frac = lengths.size\n\n    # Restrict the angle in the interval (0.1, 0.9) * pi to avoid excessively\n    # large fractures.\n    if theta is None:\n        rnd = np.random.rand(num_frac)\n        # Angles of pi/2 will read to point contacts that cannot be handled\n        # of the FractureNetwork. Point contacts also make little physical\n        # sense, so we vaoid them.\n        limit = 0.3\n        hit = rnd > 1-limit\n        rnd[hit] -= limit\n        hit = rnd < limit\n        rnd[hit] += limit\n        theta = np.pi * (limit + (1-2*limit) * rnd)\n\n    radius = 0.5 * lengths / np.sin(theta)\n\n    # Midpoint in (x, y)-coordinate given as midpoint of exposed line\n    mid_point = 0.5 * (p0 + p1).reshape((2, -1))\n\n    # z-coordinate from angle\n    depth = radius * np.cos(theta)\n\n    return radius, np.vstack((mid_point, depth)), theta",
  "def discs_from_exposure(pt, edges, exposure_angle=None,\n                        outcrop_consistent=True, **kwargs):\n    \"\"\" Create fracture discs based on exposed lines in an outrcrop.\n\n    The outcrop is assumed to be in the xy-plane. The returned disc will be\n    vertical, and the points on the outcrop will be included in the polygon\n    representation.\n\n    The location of the center is calculated from the angle, see\n    disc_radius_center() for details.\n\n    Parameters:\n        pt (np.array, 2 x num_pts): Coordinates of exposed points.\n        edges (np.array, 2 x num_fracs): Connections between fractures.\n        exposure_angle (np.array, num_fracs, optional): See above, and\n           disc_radius_center() for description. Values very close to pi/2, 0\n           and pi will be modified to avoid unphysical extruded fractures.  If\n           not provided, random values will be drawn. Measured in radians.\n           Should be between 0 and pi.\n        outcrop_consistent (boolean, optional): If True (default), points will\n            be added at the outcrop surface z=0. This is necessary for the\n            3D network to be consistent with the outcrop, but depending on\n            the location of the points of the fracture polygon, it may result\n            in very small edges.\n\n    Returns:\n        list of Fracture: One per fracture trace.\n\n    \"\"\"\n\n    num_fracs = edges.shape[1]\n\n    lengths = fracture_length(pt, edges)\n    p0 = pt[:, edges[0]]\n    p1 = pt[:, edges[1]]\n\n    v = p1 - p0\n    strike_angle = np.arctan2(v[1], v[0])\n\n    if exposure_angle is not None:\n        # Angles of pi/2 will give point contacts\n        hit = np.abs(exposure_angle - np.pi/2) < 0.01\n        exposure_angle[hit] = exposure_angle[hit] + 0.01\n\n        # Angles of 0 and pi give infinite fractures.\n        hit = exposure_angle < 0.2\n        exposure_angle[hit] = 0.2\n        hit = np.pi - exposure_angle < 0.2\n        exposure_angle[hit] = 0.2\n\n    radius, center, ang = disc_radius_center(lengths, p0, p1, exposure_angle)\n\n    fracs = []\n\n    for i in range(num_fracs):\n        z = 2 * center[2, i]\n        if outcrop_consistent:\n            extra_point_depth = np.array([0, 0, z, z])\n            extra_points = np.vstack((np.vstack((p0[:, i], p1[:, i], p0[:, i],\n                                                 p1[:, i])).T,\n                                      extra_point_depth))\n        else:\n            extra_points = np.zeros((3, 0))\n\n        fracs.append(create_fracture(center[:, i], radius[i], np.pi/2,\n                                     strike_angle[i], extra_points))\n    return fracs, ang",
  "def create_fracture(center, radius, dip, strike, extra_points):\n    \"\"\" Create a single circular fracture consistent with a given exposure.\n\n    The exposed points will be added to the fracture description.\n\n    Parameters:\n        center (np.array-like, dim 3): Center of the fracture.\n        radius (double): Fracture radius.\n        dip (double): dip angle of the fracture. See EllipticFracture for\n            details.\n        strike (np.array-like, dim 3): Strike angle for rotation. See\n            EllipticFracture for details.\n        extra_points (np.array, 3xnpt): Extra points to be added to the\n            fracture. The points are assumed to lie on the ellipsis.\n\n    Returns:\n        Fracture: New fracture, according to the specifications.\n\n    \"\"\"\n    if extra_points.shape[0] == 2:\n        extra_points = np.vstack((extra_points,\n                                  np.zeros(extra_points.shape[1])))\n\n    # The simplest way of distributing points along the disc seems to be to\n    # create an elliptic fracture, and pick out the points.\n    f = EllipticFracture(center=center, major_axis=radius, minor_axis=radius,\n                         dip_angle=dip, strike_angle=strike,\n                         major_axis_angle=0)\n    # Add the points on the exposed surface. This creates an unequal\n    # distribution of the points, but it is the only hard information we have\n    # on the fracture\n    f.add_points(extra_points, check_convexity=False, enforce_pt_tol=0.01)\n    # Not sure if f still shoudl be EllipticFracture here, or if we should\n    # create a new fracture with the same point distribution.\n    return f",
  "def rotate_fracture(frac, vec, angle, exposure):\n    \"\"\" Rotate a fracture along a specified strike vector, and centered on a\n    given point on the fracture surface.\n\n    Modification of the fracture coordinates is done in place.\n\n    TODO: Move this to the fracture itself?\n\n    Parameters:\n        frac (Fracture): To be rotated. Points are modified in-place.\n        vec (np.array-like): Rotation will be around this vector.\n        ang (double). Rotation angle. Measured in radians.\n        exposure (np.array-like): Point on the strike vector, rotation will be\n            centered around the line running through this point.\n\n    \"\"\"\n    vec = np.asarray(vec)\n    exposure = np.asarray(exposure)\n\n    if vec.size == 2:\n        vec = np.append(vec, 0)\n    if exposure.size == 2:\n        exposure = np.append(exposure, 0)\n    exposure = exposure.reshape((3, 1))\n\n    rot = cg.rot(angle, vec)\n    p = frac.p\n    frac.p = exposure + rot.dot(p - exposure)\n\n    frac.points_2_ccw()\n    frac.compute_centroid()\n    frac.compute_normal()",
  "def impose_inlcine(fracs, exposure_line, exposure_point, frac_family=None,\n                   family_mean_incline=None, family_std_incline=None,\n                   **kwargs):\n    \"\"\" Impose incline on the fractures from family-based parameters.\n\n    The incline for each family is specified in terms of its mean and standard\n    deviation. A normal distribution in assumed. The rotation is taken around\n    the line of exposure, thus the resulting fractures are consistent with the\n    outcrop.\n\n    Parameters:\n        fracs (list of Frature): Fractures to be inclined.\n        exposure_line (np.array, 3xnum_frac): Exposed line for each fracture\n            (visible in outcrop). Rotation will be around this line.\n        exposure_point (np.array, 3xnum_frac): Point on exposure line. This\n            point will not be rotated, it's a fixture.\n        family (np.array, num_fracs): For each fracture, which family does it\n            belong to. If not provided, all fractures are considered to belong\n            to the same family.\n        family_mean_incline (np.array, num_family): Mean value of incline for each\n            family. In radians. Defaults to zero.\n        family_std_incline (np.array, num_family): Standard deviation of incine for\n            each family. In radians. Defaultst to zero.\n\n        To set value for each fracture, set family = np.arange(len(fracs)),\n        family_mean_incline=prescribed_value, and family_std_incline=None.\n\n    Returns:\n        np.array, size num_frac: Rotation angles.\n\n    \"\"\"\n    if frac_family is None:\n        frac_family = np.zeros(len(fracs), dtype=np.int)\n    if family_mean_incline is None:\n        family_mean_incline = np.zeros(np.unique(frac_family).size)\n    if family_std_incline is None:\n        family_std_incline = np.zeros(np.unique(frac_family).size)\n\n    exposure_line = np.vstack((exposure_line, np.zeros(len(fracs))))\n    all_ang = np.zeros(len(fracs))\n    for fi, f in enumerate(fracs):\n        fam = frac_family[fi]\n        ang = np.random.normal(loc=family_mean_incline[fam],\n                               scale=family_std_incline[fam])\n        rotate_fracture(f, exposure_line[:, fi], ang, exposure_point[:, fi])\n        all_ang[fi] = ang\n\n    return all_ang",
  "def cut_fracture_by_plane(main_frac, other_frac, reference_point, tol=1e-4,\n                          recompute_center=True, **kwargs):\n    \"\"\" Cut a fracture by a plane, and confine it to one side of the plane.\n\n    Intended use is to confine abutting fractures (T-intersections) to one side\n    of the fracture it hits. This is done by deleting points on the abutting\n    fracture.\n\n    Parameters:\n        main_frac (Fracture): The fracture to be cut.\n        other_frac (Fracture): The fracture that defines the confining plane.\n        reference_point (np.array, nd): Point on the main frature that defines\n            which side should be kept. Will typically be the other point of the\n            exposed line.\n\n    Returns:\n        Fracture: A copy of the main fracture, cut by the other fracture.\n        double: In cases where one interseciton point extends beyond the other\n            fracture, this is the distance between the center of the other\n            fracture and the intersection point. If both intersections are\n            within the polygon, None will be returned.\n\n    Raises:\n        ValueError if the points in the other fracture is too close. This could\n        probably be handled by a scaling of coordinates, it is tacitly assumed\n        that we're working in something resembling the unit box.\n\n    \"\"\"\n    reference_point = reference_point.reshape((-1, 1))\n    if reference_point.size == 2:\n        reference_point = np.vstack((reference_point, 0))\n\n    # First determine extent of the main fracture\n    main_min = main_frac.p.min(axis=1)\n    main_max = main_frac.p.max(axis=1)\n\n    # Equation for the plane through the other fracture, on the form\n    #  n_x(x-c_x) + n_y(y-c_y) + n_z(z-c_z) = 0\n    n = cg.compute_normal(other_frac.p).reshape((-1, 1))\n    c = other_frac.center\n\n    # max and min coordinates that extends outside the main fracture\n    main_min -= 1\n    main_max += 1\n\n    # Define points in the plane of the second fracture with min and max\n    # coordinates picked from the main fracture.\n    # The below tricks with indices are needed to find a dimension with a\n    # non-zero gradient of the plane, so that we can divide safely.\n    # Implementation note: It might have been possible to do this with a\n    # rotation to the natural plane of the other fracture, but it is not clear\n    # this will really be simpler.\n\n    # Not sure about the tolerance here\n    # We should perhaps do a scaling of coordinates.\n    non_zero = np.where(np.abs(n) > 1e-8)[0]\n    if non_zero.size == 0:\n        raise ValueError('Could not compute normal vector of other fracture')\n    ind = np.setdiff1d(np.arange(3), non_zero[0])\n    i0 = ind[0]\n    i1 = ind[1]\n    i2 = non_zero[0]\n\n    p = np.zeros((3, 4))\n    # A for-loop might have been possible here.\n    p[i0, 0] = main_min[i0]\n    p[i1, 0] = main_min[i1]\n    p[i2, 0] = c[i2] - (n[i0] * (main_min[i0] - c[i0])\n                      + n[i1] * (main_min[i1] - c[i1])) / n[i2]\n\n    p[i0, 1] = main_max[i0]\n    p[i1, 1] = main_min[i1]\n    p[i2, 1] = c[i2] - (n[i0] * (main_max[i0] - c[i0])\n                      + n[i1] * (main_min[i1] - c[i1])) / n[i2]\n\n    p[i0, 2] = main_max[i0]\n    p[i1, 2] = main_max[i1]\n    p[i2, 2] = c[i2] - (n[i0] * (main_max[i0] - c[i0])\n                      + n[i1] * (main_max[i1] - c[i1])) / n[i2]\n\n    p[i0, 3] = main_min[i0]\n    p[i1, 3] = main_max[i1]\n    p[i2, 3] = c[i2] - (n[i0] * (main_min[i0] - c[i0])\n                      + n[i1] * (main_max[i1] - c[i1])) / n[i2]\n\n    # Create an auxiliary fracture that spans the same plane as the other\n    # fracture, and with a larger extension than the main fracture.\n    aux_frac = Fracture(p, check_convexity=False)\n\n    isect_pt, _, _ = main_frac.intersects(aux_frac, tol)\n\n    # The extension of the abutting fracture will cross the other fracture\n    # with a certain angle to the vertical. If the other fracture is rotated\n    # with a similar angle, point contact results.\n    if isect_pt.size == 0:\n        warnings.warn(\"\"\"No intersection found in cutting of fractures. This is\n                         likely caused by an unfortunate combination of\n                         extrusion and rotation angles, which created fractures\n                         that only intersect in a single point (the outcrop\n                         plane. Will try to continue, but this may cause\n                         trouble for meshing etc.\"\"\")\n        return main_frac, None\n\n    # Next step is to eliminate points in the main fracture that are on the\n    # wrong side of the other fracture.\n    v = main_frac.p - other_frac.center.reshape((-1, 1))\n    sgn = np.sign(np.sum(v * n, axis=0))\n    ref_v = reference_point - other_frac.center.reshape((-1, 1))\n    right_sign = np.sign(np.sum(ref_v * n, axis=0))\n\n    # Eliminate points that are on the other side.\n    eliminate = np.where(sgn * right_sign < 0)[0]\n    main_frac.remove_points(eliminate)\n\n\n    # Add intersection points on the main fracture. One of these may already be\n    # present, as the point of extrusion, but add_point will uniquify the point\n    # cloud.\n    # We add the points after elimination, to ensure that the points on the\n    # plane are present in the final fracture.\n    main_frac.add_points(isect_pt, check_convexity=False)\n\n    if recompute_center:\n        main_frac.compute_centroid()\n\n    # If the main fracture is too large compared to the other, the cut line\n    # will extend beyond the confining plane. In these cases, compute the\n    # distance from the fracture center to the outside intersection point. This\n    # can be used to extend the other fracture so as to avoid such strange\n    # configurations.\n    other_center = other_frac.center.reshape((-1, 1))\n    other_p = other_frac.p\n    rot = cg.project_plane_matrix(other_p - other_center)\n\n    other_rot = rot.dot(other_p - other_center)[:2]\n    isect_rot = rot.dot(isect_pt - other_center)[:2]\n\n    is_inside = cg.is_inside_polygon(other_rot, isect_rot, tol, default=True)\n    # At one point (the exposed point) must be in the polygon of the other\n    # fracture.\n    assert is_inside.any()\n\n    if not is_inside.all():\n        hit = np.logical_not(is_inside)\n        r = np.sqrt(np.sum(isect_pt[:, hit]**2))\n        return main_frac, r\n    else:\n        return main_frac, None",
  "def cart_grid_3d(fracs, nx, physdims=None):\n    \"\"\"\n    Create grids for a domain with possibly intersecting fractures in 3d.\n\n    Based on rectangles describing the individual fractures, the method\n    constructs grids in 3d (the whole domain), 2d (one for each individual\n    fracture), 1d (along fracture intersections), and 0d (meeting between\n    intersections).\n\n    Parameters\n    ----------\n    fracs (list of np.ndarray, each 3x4): Vertexes in the rectangle for each\n        fracture. The vertices must be sorted and aligned to the axis.\n        The fractures will snap to the closest grid faces.\n    nx (np.ndarray): Number of cells in each direction. Should be 3D.\n    physdims (np.ndarray): Physical dimensions in each direction.\n        Defaults to same as nx, that is, cells of unit size.\n\n    Returns\n    -------\n    list (length 4): For each dimension (3 -> 0), a list of all grids in\n        that dimension.\n\n    Examples\n    --------\n    frac1 = np.array([[1,1,4,4], [1,4,4,1], [2,2,2,2]])\n    frac2 = np.array([[2,2,2,2], [1,1,4,4], [1,4,4,1]])\n    fracs = [frac1, frac2]\n    gb = cart_grid_3d(fracs, [5,5,5])\n    \"\"\"\n\n    nx = np.asarray(nx)\n    if physdims is None:\n        physdims = nx\n    elif np.asarray(physdims).size != nx.size:\n        raise ValueError('Physical dimension must equal grid dimension')\n    else:\n        physdims = np.asarray(physdims)\n\n    # We create a 3D cartesian grid. The global node mapping is trivial.\n    g_3d = structured.CartGrid(nx, physdims=physdims)\n    g_3d.global_point_ind = np.arange(g_3d.num_nodes)\n    g_3d.compute_geometry()\n    g_2d = []\n    g_1d = []\n    g_0d = []\n    # We set the tolerance for finding points in a plane. This can be any\n    # small number, that is smaller than .25 of the cell sizes.\n    tol = .1 * physdims / nx\n\n    # Create 2D grids\n    for fi, f in enumerate(fracs):\n        assert np.all(f.shape == (3, 4)), 'fractures must have shape [3,4]'\n        is_xy_frac = np.allclose(f[2, 0], f[2])\n        is_xz_frac = np.allclose(f[1, 0], f[1])\n        is_yz_frac = np.allclose(f[0, 0], f[0])\n        assert is_xy_frac + is_xz_frac + is_yz_frac == 1, \\\n            'Fracture must align to x-, y- or z-axis'\n        # snap to grid\n        f_s = np.round(f * nx[:, np.newaxis] / physdims[:, np.newaxis]\n                       ) * physdims[:, np.newaxis] / nx[:, np.newaxis]\n        if is_xy_frac:\n            flat_dim = [2]\n            active_dim = [0, 1]\n        elif is_xz_frac:\n            flat_dim = [1]\n            active_dim = [0, 2]\n        else:\n            flat_dim = [0]\n            active_dim = [1, 2]\n        # construct normal vectors. If the rectangle is ordered\n        # clockwise we need to flip the normals so they point\n        # outwards.\n        sign = 2 * cg.is_ccw_polygon(f_s[active_dim]) - 1\n        tangent = f_s.take(\n            np.arange(f_s.shape[1]) + 1, axis=1, mode='wrap') - f_s\n        normal = tangent\n        normal[active_dim] = tangent[active_dim[1::-1]]\n        normal[active_dim[1]] = -normal[active_dim[1]]\n        normal = sign * normal\n        # We find all the faces inside the convex hull defined by the\n        # rectangle. To find the faces on the fracture plane, we remove any\n        # faces that are further than tol from the snapped fracture plane.\n        in_hull = half_space.half_space_int(\n            normal, f_s, g_3d.face_centers)\n        f_tag = np.logical_and(\n            in_hull,\n            np.logical_and(f_s[flat_dim, 0] - tol[flat_dim] <=\n                           g_3d.face_centers[flat_dim],\n                           g_3d.face_centers[flat_dim] <\n                           f_s[flat_dim, 0] + tol[flat_dim]))\n        f_tag = f_tag.ravel()\n        nodes = sps.find(g_3d.face_nodes[:, f_tag])[0]\n        nodes = np.unique(nodes)\n        loc_coord = g_3d.nodes[:, nodes]\n        g = _create_embedded_2d_grid(loc_coord, nodes)\n\n        g.frac_num = fi\n        g_2d.append(g)\n\n    # Create 1D grids:\n    # Here we make use of the network class to find the intersection of\n    # fracture planes. We could maybe avoid this by doing something similar\n    # as for the 2D-case, and count the number of faces belonging to each edge,\n    # but we use the FractureNetwork class for now.\n    frac_list = []\n    for f in fracs:\n        frac_list.append(fractures.Fracture(f))\n    # Combine the fractures into a network\n    network = fractures.FractureNetwork(frac_list)\n    # Impose domain boundary. For the moment, the network should be immersed in\n    # the domain, or else gmsh will complain.\n    box = {'xmin': 0, 'ymin': 0, 'zmin': 0,\n           'xmax': physdims[0], 'ymax': physdims[1], 'zmax': physdims[2]}\n    network.impose_external_boundary(box)\n\n    # Find intersections and split them.\n    network.find_intersections()\n    network.split_intersections()\n\n    # Extract geometrical network information.\n    pts = network.decomposition['points']\n    edges = network.decomposition['edges']\n    poly = network._poly_2_segment()\n    # And tags identifying points and edges corresponding to normal\n    # fractures, domain boundaries and subdomain boundaries. Only the\n    # entities corresponding to normal fractures should actually be gridded.\n    edge_tags, intersection_points = network._classify_edges(poly)\n    const = constants.GmshConstants()\n    auxiliary_points, edge_tags = network.on_domain_boundary(edges, edge_tags)\n    bound_and_aux = np.array([const.DOMAIN_BOUNDARY_TAG, const.AUXILIARY_TAG])\n    edges = np.vstack((edges, edge_tags))\n\n    # Loop through the edges to make 1D grids. Ommit the auxiliary edges. \n    for e in np.ravel(np.where(edges[2] == const.FRACTURE_INTERSECTION_LINE_TAG)):\n        # We find the start and end point of each fracture intersection (1D\n        # grid) and then the corresponding global node index.\n        if np.isin(edge_tags[e], bound_and_aux):\n            continue\n        s_pt = pts[:, edges[0, e]]\n        e_pt = pts[:, edges[1, e]]\n        nodes = _find_nodes_on_line(g_3d, nx, s_pt, e_pt)\n        loc_coord = g_3d.nodes[:, nodes]\n        assert loc_coord.shape[1] > 1, '1d grid in intersection should span\\\n            more than one node'\n        g = mesh_2_grid.create_embedded_line_grid(loc_coord, nodes)\n        g_1d.append(g)\n\n    # Create 0D grids\n    # Here we also use the intersection information from the FractureNetwork\n    # class. No grids for auxiliary points.\n    for p in intersection_points:\n        if auxiliary_points[p]:\n            continue\n        node = np.argmin(cg.dist_point_pointset(pts[:, p], g_3d.nodes))\n        assert np.allclose(g_3d.nodes[:, node], pts[:, p])\n        g = point_grid.PointGrid(g_3d.nodes[:, node])\n        g.global_point_ind = np.asarray(node)\n        g_0d.append(g)\n\n    grids = [[g_3d], g_2d, g_1d, g_0d]\n    return grids",
  "def cart_grid_2d(fracs, nx, physdims=None):\n    \"\"\"\n    Create grids for a domain with possibly intersecting fractures in 2d.\n\n    Based on lines describing the individual fractures, the method\n    constructs grids in 2d (whole domain), 1d (individual fracture), and 0d\n    (fracture intersections).\n\n    Parameters\n    ----------\n    fracs (list of np.ndarray, each 2x2): Vertexes of the line for each\n        fracture. The fracture lines must align to the coordinat axis.\n        The fractures will snap to the closest grid nodes.\n    nx (np.ndarray): Number of cells in each direction. Should be 2D.\n    physdims (np.ndarray): Physical dimensions in each direction.\n        Defaults to same as nx, that is, cells of unit size.\n\n    Returns\n    -------\n    list (length 3): For each dimension (2 -> 0), a list of all grids in\n        that dimension.\n\n    Examples\n    --------\n    frac1 = np.array([[1,4],[2,2]])\n    frac2 = np.array([[2,2],[1,4]])\n    fracs = [frac1,frac2]\n    gb = cart_grid_2d(fracs, [5,5])\n    \"\"\"\n    nx = np.asarray(nx)\n    if physdims is None:\n        physdims = nx\n    elif np.asarray(physdims).size != nx.size:\n        raise ValueError('Physical dimension must equal grid dimension')\n    else:\n        physdims = np.asarray(physdims)\n\n    g_2d = structured.CartGrid(nx, physdims=physdims)\n    g_2d.global_point_ind = np.arange(g_2d.num_nodes)\n    g_2d.compute_geometry()\n    g_1d = []\n    g_0d = []\n\n    # 1D grids:\n    shared_nodes = np.zeros(g_2d.num_nodes)\n    for f in fracs:\n        is_x_frac = f[1, 0] == f[1, 1]\n        is_y_frac = f[0, 0] == f[0, 1]\n        assert is_x_frac != is_y_frac, 'Fracture must align to x- or y-axis'\n        if f.shape[0] == 2:\n            f = np.vstack((f, np.zeros(f.shape[1])))\n        nodes = _find_nodes_on_line(g_2d, nx, f[:, 0], f[:, 1])\n        #nodes = np.unique(nodes)\n        loc_coord = g_2d.nodes[:, nodes]\n        g = mesh_2_grid.create_embedded_line_grid(loc_coord, nodes)\n        g_1d.append(g)\n        shared_nodes[nodes] += 1\n\n    # Create 0-D grids\n    if np.any(shared_nodes > 1):\n        for global_node in np.argwhere(shared_nodes > 1).ravel():\n            g = point_grid.PointGrid(g_2d.nodes[:, global_node])\n            g.global_point_ind = np.asarray(global_node)\n            g_0d.append(g)\n\n    grids = [[g_2d], g_1d, g_0d]\n    return grids",
  "def _create_embedded_2d_grid(loc_coord, glob_id):\n    \"\"\"\n    Create a 2d grid that is embedded in a 3d grid.\n    \"\"\"\n    loc_center = np.mean(loc_coord, axis=1).reshape((-1, 1))\n    loc_coord -= loc_center\n    # Check that the points indeed form a line\n    assert cg.is_planar(loc_coord)\n    # Find the tangent of the line\n    # Projection matrix\n    rot = cg.project_plane_matrix(loc_coord)\n    loc_coord_2d = rot.dot(loc_coord)\n    # The points are now 2d along two of the coordinate axis, but we\n    # don't know which yet. Find this.\n    sum_coord = np.sum(np.abs(loc_coord_2d), axis=1)\n    active_dimension = np.logical_not(np.isclose(sum_coord, 0))\n    # Check that we are indeed in 2d\n    assert np.sum(active_dimension) == 2\n    # Sort nodes, and create grid\n    coord_2d = loc_coord_2d[active_dimension]\n    sort_ind = np.lexsort((coord_2d[0], coord_2d[1]))\n    sorted_coord = coord_2d[:, sort_ind]\n    sorted_coord = np.round(sorted_coord * 1e10) / 1e10\n    unique_x = np.unique(sorted_coord[0])\n    unique_y = np.unique(sorted_coord[1])\n    # assert unique_x.size == unique_y.size\n    g = structured.TensorGrid(unique_x, unique_y)\n    assert np.all(g.nodes[0:2] - sorted_coord == 0)\n\n    # Project back to active dimension\n    nodes = np.zeros(g.nodes.shape)\n    nodes[active_dimension] = g.nodes[0:2]\n    g.nodes = nodes\n    # Project back again to 3d coordinates\n\n    irot = rot.transpose()\n    g.nodes = irot.dot(g.nodes)\n    g.nodes += loc_center\n\n    # Add mapping to global point numbers\n    g.global_point_ind = glob_id[sort_ind]\n    return g",
  "def _find_nodes_on_line(g, nx, s_pt, e_pt):\n    \"\"\"\n    We have the start and end point of the fracture. From this we find the \n    start and end node and use the structure of the cartesian grid to find\n    the intermediate nodes.\n    \"\"\"\n    s_node = np.argmin(cg.dist_point_pointset(s_pt, g.nodes))\n    e_node = np.argmin(cg.dist_point_pointset(e_pt, g.nodes))\n\n    # We make sure the nodes are ordered from low to high.\n    if s_node > e_node:\n        tmp = s_node\n        s_node = e_node\n        e_node = tmp\n    # We now find the other grid nodes. We here use the node ordering of\n    # meshgrid (which is used by the TensorGrid class).\n\n    # We find the number of nodes along each dimension. From this we find the\n    # jump in node number between two consecutive nodes.\n\n    if np.all(np.isclose(s_pt[1:], e_pt[1:])):\n        # x-line:\n        nodes = np.arange(s_node, e_node + 1)\n    elif np.all(np.isclose(s_pt[[0, 2]], e_pt[[0, 2]])):\n        # y-line\n        nodes = np.arange(s_node, e_node + 1, nx[0] + 1, dtype=int)\n\n    elif nx.size == 3 and np.all(np.isclose(s_pt[0:2], e_pt[0:2])):\n        # is z-line\n        nodes = np.arange(s_node, e_node + 1,\n                          (nx[0] + 1) * (nx[1] + 1), dtype=int)\n    else:\n        raise RuntimeError(\n            'Something went wrong. Found a diagonal intersection')\n    return nodes",
  "def dfm_3d_from_csv(file_name, tol=1e-4, **mesh_kwargs):\n    \"\"\"\n    Create the grid bucket from a set of 3d fractures stored in a csv file and\n    domain. In the csv file, we assume the following structure\n    - first line describes the domain as a rectangle with\n      X_MIN, Y_MIN, Z_MIN, X_MAX, Y_MAX, Z_MAX\n    - the other lines descibe the N fractures as a list of points\n      P0_X, P0_Y, P0_Z, ...,PN_X, PN_Y, PN_Z\n\n    Parameters:\n        file_name: name of the file\n        tol: (optional) tolerance for the methods\n        mesh_kwargs: kwargs for the gridding, see meshing.simplex_grid\n\n    Return:\n        gb: the grid bucket\n    \"\"\"\n    frac_list, network, domain = network_3d_from_csv(file_name)\n\n    gb = meshing.simplex_grid(domain=domain, network=network, **mesh_kwargs)\n    return gb, domain",
  "def network_3d_from_csv(file_name, has_domain=True, tol=1e-4):\n    \"\"\"\n    Create the fracture network from a set of 3d fractures stored in a csv file and\n    domain. In the csv file, we assume the following structure\n    - first line (optional) describes the domain as a rectangle with\n      X_MIN, Y_MIN, Z_MIN, X_MAX, Y_MAX, Z_MAX\n    - the other lines descibe the N fractures as a list of points\n      P0_X, P0_Y, P0_Z, ...,PN_X, PN_Y, PN_Z\n\n    Lines that start with a # are ignored.\n\n    Parameters:\n        file_name: name of the file\n        has_domain: if the first line in the csv file specify the domain\n        tol: (optional) tolerance for the methods\n\n    Return:\n        frac_list: the list of fractures\n        network: the fracture network\n        domain: (optional, returned if has_domain==True) the domain\n    \"\"\"\n\n    # The first line of the csv file defines the bounding box for the domain\n\n    frac_list = []\n    # Extract the data from the csv file\n    with open(file_name, 'r') as csv_file:\n        spam_reader = csv.reader(csv_file, delimiter=',')\n\n        # Read the domain first\n        if has_domain:\n            domain = np.asarray(next(spam_reader), dtype=np.float)\n            assert domain.size == 6\n            domain = {'xmin': domain[0], 'xmax': domain[3], 'ymin': domain[1],\n                      'ymax': domain[4], 'zmin': domain[2], 'zmax': domain[5]}\n\n        for row in spam_reader:\n            # If the line starts with a '#', we consider this a comment\n            if row[0][0] == '#':\n                continue\n\n            # Read the points\n            pts = np.asarray(row, dtype=np.float)\n            assert pts.size % 3 == 0\n\n            # Skip empty lines. Useful if the file ends with a blank line.\n            if pts.size == 0:\n                continue\n\n            frac_list.append(Fracture(pts.reshape((3, -1), order='F')))\n\n    # Create the network\n    network = FractureNetwork(frac_list, tol=tol)\n\n    if has_domain:\n        return frac_list, network, domain\n    else:\n        return frac_list, network",
  "def dfm_2d_from_csv(f_name, mesh_kwargs, domain=None, return_domain=False,\n                    tol=1e-8, **kwargs):\n    \"\"\"\n    Create the grid bucket from a set of fractures stored in a csv file and a\n    domain. In the csv file, we assume the following structure:\n    FID, START_X, START_Y, END_X, END_Y\n\n    Where FID is the fracture id, START_X and START_Y are the abscissa and\n    coordinate of the starting point, and END_X and END_Y are the abscissa and\n    coordinate of the ending point.\n    Note: the delimiter can be different.\n\n    Parameters:\n        f_name: the file name in CSV format\n        mesh_kwargs: list of additional arguments for the meshing\n        domain: rectangular domain, if not given the bounding-box is computed\n        kwargs: list of argument for the numpy function genfromtxt\n\n    Returns:\n        gb: grid bucket associated to the configuration.\n        domain: if the domain is not given as input parameter, the bounding box\n        is returned.\n\n    \"\"\"\n    pts, edges = lines_from_csv(f_name, tol=tol, **kwargs)\n    f_set = np.array([pts[:, e] for e in edges.T])\n\n    # Define the domain as bounding-box if not defined\n    if domain is None:\n        overlap = kwargs.get('domain_overlap', 0)\n        domain = cg.bounding_box(pts, overlap)\n\n    if return_domain:\n        return meshing.simplex_grid(f_set, domain, **mesh_kwargs), domain\n    else:\n        return meshing.simplex_grid(f_set, domain, **mesh_kwargs)",
  "def lines_from_csv(f_name, tagcols=None, tol=1e-8, **kwargs):\n    \"\"\" Read csv file with fractures to obtain fracture description.\n\n    Create the grid bucket from a set of fractures stored in a csv file and a\n    domain. In the csv file, we assume the following structure:\n    FID, START_X, START_Y, END_X, END_Y\n\n    Where FID is the fracture id, START_X and START_Y are the abscissa and\n    coordinate of the starting point, and END_X and END_Y are the abscissa and\n    coordinate of the ending point.\n\n    To change the delimiter from the default comma, use kwargs passed to\n    np.genfromtxt.\n\n    The csv file is assumed to have a header of 1 line. To change this number,\n    use kwargs skip_header.\n\n    Parameters:\n        f_name (str): Path to csv file\n        tagcols (array-like, int. Optional): Column index where fracture tags\n            are stored. 0-offset. Defaults to no columns.\n        **kwargs: keyword arguments passed on to np.genfromtxt.\n\n    Returns:\n        np.ndarray (2 x num_pts): Point coordinates used in the fracture\n            description.\n        np.ndarray (2+numtags x num_fracs): Fractures, described by their start\n            and endpoints (first and second row). If tags are assigned to the\n            fractures, these are stored in rows 2,...\n\n    \"\"\"\n    npargs = {}\n    # EK: Should these really be explicit keyword arguments?\n    npargs['delimiter'] = kwargs.get('delimiter', ',')\n    npargs['skip_header'] = kwargs.get('skip_header', 1)\n\n    # Extract the data from the csv file\n    data = np.genfromtxt(f_name, **npargs)\n    if data.size == 0:\n        return np.empty((2, 0)), np.empty((2, 0), dtype=np.int)\n    data = np.atleast_2d(data)\n\n    num_fracs = data.shape[0] if data.size > 0 else 0\n    num_data = data.shape[1] if data.size > 0 else 0\n\n    pt_cols = np.arange(1, num_data)\n    if tagcols is not None:\n        pt_cols = np.setdiff1d(pt_cols, tagcols)\n\n    pts = data[:, pt_cols].reshape((-1, 2)).T\n\n    # Let the edges correspond to the ordering of the fractures\n    edges = np.vstack((np.arange(0, 2 * num_fracs, 2),\n                       np.arange(1, 2 * num_fracs, 2)))\n    if tagcols is not None:\n        edges = np.vstack((edges, data[:, tagcols].T))\n\n    pts, _, old_2_new = unique_columns_tol(pts, tol=tol)\n    edges[:2] = old_2_new[edges[:2]]\n\n    to_remove = np.where(edges[0, :] == edges[1, :])[0]\n    edges = np.delete(edges, to_remove, axis=1)\n\n    assert np.all(np.diff(edges[:2], axis=0) != 0)\n\n    return pts, edges.astype(np.int)",
  "def dfn_3d_from_fab(file_name, file_inters=None, conforming=True, tol=None,\n                    vtk_name=None, **kwargs):\n    \"\"\" Read the fractures and (possible) intersection from files.\n    The fractures are in a .fab file, as specified by FracMan.\n    The intersection are specified in the function intersection_dfn_3d.\n\n    Parameters:\n        file_name (str): Path to .fab file.\n        file_intersections (str): Optional path to intersection file.\n        conforming (boolean): If True, the mesh will be conforming along 1d\n            intersections.\n        vtk_name (str): Gives the possibility to export the network in a vtu\n            file. Consider the suffix of the file as \".vtu\".\n        **kwargs: Parameters passed to gmsh.\n\n    Returns:\n        gb (GridBucket): The grid bucket.\n\n    \"\"\"\n    network = network_3d_from_fab(file_name, return_all=False, tol=tol)\n\n    if vtk_name is not None:\n        network.to_vtk(vtk_name)\n\n    if file_inters is None:\n        return meshing.dfn(network, conforming, **kwargs)\n    else:\n        inters = intersection_dfn_3d(file_inters, fractures)\n        return meshing.dfn(network, conforming, inters, **kwargs)",
  "def network_3d_from_fab(f_name, return_all=False, tol=None):\n    \"\"\" Read fractures from a .fab file, as specified by FracMan.\n\n    The filter is based on the .fab-files available at the time of writing, and\n    may not cover all options available.\n\n    Parameters:\n        f_name (str): Path to .fab file.\n\n    Returns:\n        network: the network of fractures\n        tess_fracs (optional returned if return_all==True, list of np.ndarray):\n            Each list element contains fracture\n            cut by the domain boundary, represented by vertexes as a nd x n_pt\n            array.\n        tess_sgn (optional returned if return_all==True, np.ndarray):\n            For each element in tess_frac, a +-1 defining\n            which boundary the fracture is on.\n\n    The function also reads in various other information of unknown usefulness,\n    see implementation for details. This information is currently not returned.\n\n    \"\"\"\n\n    def read_keyword(line):\n        # Read a single keyword, on the form  key = val\n        words = line.split('=')\n        assert len(words) == 2\n        key = words[0].strip()\n        val = words[1].strip()\n        return key, val\n\n    def read_section(f, section_name):\n        # Read a section of the file, surrounded by a BEGIN / END wrapping\n        d = {}\n        for line in f:\n            if line.strip() == 'END ' + section_name.upper().strip():\n                return d\n            k, v = read_keyword(line)\n            d[k] = v\n\n    def read_fractures(f, is_tess=False):\n        # Read the fracture\n        fracs = []\n        fracture_ids = []\n        trans = []\n        nd = 3\n        for line in f:\n            if not is_tess and line.strip() == 'END FRACTURE':\n                return fracs, np.asarray(fracture_ids), np.asarray(trans)\n            elif is_tess and line.strip() == 'END TESSFRACTURE':\n                return fracs, np.asarray(fracture_ids), np.asarray(trans)\n            if is_tess:\n                ids, num_vert = line.split()\n            else:\n                ids, num_vert, t = line.split()[:3]\n\n                trans.append(float(t))\n\n            ids = int(ids)\n            num_vert = int(num_vert)\n            vert = np.zeros((num_vert, nd))\n            for i in range(num_vert):\n                data = f.readline().split()\n                vert[i] = np.asarray(data[1:])\n\n            # Transpose to nd x n_pt format\n            vert = vert.T\n\n            # Read line containing normal vector, but disregard result\n            data = f.readline().split()\n            if is_tess:\n                trans.append(int(data[1]))\n            fracs.append(vert)\n            fracture_ids.append(ids)\n\n    with open(f_name, 'r') as f:\n        for line in f:\n            if line.strip() == 'BEGIN FORMAT':\n                # Read the format section, but disregard the information for\n                # now\n                formats = read_section(f, 'FORMAT')\n            elif line.strip() == 'BEGIN PROPERTIES':\n                # Read in properties section, but disregard information\n                props = read_section(f, 'PROPERTIES')\n            elif line.strip() == 'BEGIN SETS':\n                # Read set section, but disregard information.\n                sets = read_section(f, 'SETS')\n            elif line.strip() == 'BEGIN FRACTURE':\n                # Read fractures\n                fracs, frac_ids, trans = read_fractures(f, is_tess=False)\n            elif line.strip() == 'BEGIN TESSFRACTURE':\n                # Read tess_fractures\n                tess_fracs, tess_frac_ids, tess_sgn = \\\n                    read_fractures(f, is_tess=True)\n            elif line.strip()[:5] == 'BEGIN':\n                # Check for keywords not yet implemented.\n                raise ValueError('Unknown section type ' + line)\n\n    fractures = [Fracture(f) for f in fracs]\n    if tol is not None:\n        network = FractureNetwork(fractures, tol=tol)\n    else:\n        network = FractureNetwork(fractures)\n\n    if return_all:\n        return network, tess_fracs, tess_sgn\n    else:\n        return network",
  "def intersection_dfn_3d(file_name, fractures):\n    \"\"\" Read the fracture intersections from file.\n    NOTE: We assume that the fracture id in the file starts from 1. The file\n    format is as follow:\n    x_0 y_0 x_1 y_1 frac_id0 frac_id1\n\n    Parameters:\n        file_name (str): Path to file.\n        fractures (either Fractures, or a FractureNetwork).\n\n    Returns:\n        intersections (list of lists): Each item corresponds to an\n            intersection between two fractures. In each sublist, the first two\n            indices gives fracture ids (refering to order in fractures). The third\n            item is a numpy array representing intersection coordinates.\n    \"\"\"\n    class DummyFracture(object):\n        def __init__(self, index):\n            self.index = index\n\n    inter_from_file = np.atleast_2d(np.loadtxt(file_name))\n    intersections = np.empty((inter_from_file.shape[0], 3), dtype=np.object)\n\n    for line, intersection in zip(inter_from_file, intersections):\n        intersection[0] = DummyFracture(int(line[6]) - 1)\n        intersection[1] = DummyFracture(int(line[7]) - 1)\n        intersection[2] = np.array(line[:6]).reshape((3, -1), order='F')\n    return intersections",
  "def read_dfn_grid(folder, num_fractures, case_id, **kwargs):\n\n    # TODO: tag tip faces\n\n    offset_name = kwargs.get('offset_name', 1)\n    folder += \"/\"\n    g_2d = np.empty(num_fractures, dtype=np.object)\n    gb = grid_bucket.GridBucket()\n\n    global_node_id = 0\n    for f_id in np.arange(num_fractures):\n        post = \"_F\" + str(f_id + offset_name) + \"_\" + str(case_id) + \".txt\"\n        nodes_2d, face_nodes_2d, cell_faces_2d = _dfn_grid_2d(\n            folder, post, **kwargs)\n        g_2d[f_id] = grid.Grid(2, nodes_2d, face_nodes_2d, cell_faces_2d,\n                               \"fracture_\" + str(f_id) + \"_\" + str(case_id))\n\n        bnd_faces = g_2d[f_id].get_all_boundary_faces()\n        g_2d[f_id].tags['domain_boundary_faces'][bnd_faces] = True\n\n        g_2d[f_id].global_point_ind = np.arange(g_2d[f_id].num_nodes) + \\\n            global_node_id\n        global_node_id += g_2d[f_id].num_nodes\n\n    gb.add_nodes(g_2d)\n\n    for f_id in np.arange(num_fractures):\n        post = \"_F\" + str(f_id + offset_name) + \"_\" + str(case_id) + \".txt\"\n\n        face_name = kwargs.get(\"face_name\", \"Faces\")\n        face_file_name = folder + face_name + post\n\n        with open(face_file_name, 'r') as f:\n            skip_lines = int(f.readline().split()[0]) + 1\n            lines = np.array(f.readlines()[skip_lines:])\n            conn = np.atleast_2d([np.fromstring(l, dtype=np.int, sep=' ')\n                                  for l in lines])\n            # Consider only the new intersections\n            conn = conn[conn[:, 2] > f_id, :]\n\n            for g_id in np.unique(conn[:, 2]):\n                other_f_id = g_id - 1\n                mask = conn[:, 2] == g_id\n\n                nodes_id = _nodes_faces_2d(g_2d[f_id], conn[mask, 0])\n                nodes_1d, face_nodes_1d, cell_faces_1d = _dfn_grid_1d(\n                    g_2d[f_id], nodes_id)\n                g_1d = grid.Grid(1, nodes_1d, face_nodes_1d, cell_faces_1d,\n                                 \"intersection_\" + str(f_id) +\n                                 \"_\" + str(g_id - 1) + \"_\" + str(case_id))\n\n                print(nodes_id)\n                print(g_2d[f_id].global_point_ind[nodes_id])\n                global_point_ind = g_2d[f_id].global_point_ind[nodes_id]\n                g_1d.global_point_ind = global_point_ind\n\n                nodes_id = _nodes_faces_2d(g_2d[other_f_id], conn[mask, 1])\n                for g, _ in gb:  # TODO: better access\n                    if g is g_2d[other_f_id]:\n                        g.global_point_ind[nodes_id] = global_point_ind\n                        break\n\n                gb.add_nodes(g_1d)\n\n                shape = (g_1d.num_cells, g_2d[f_id].num_faces)\n                data = np.ones(g_1d.num_cells, dtype=np.bool)\n                face_cells = sps.csc_matrix((data, (np.arange(g_1d.num_cells),\n                                                    conn[mask, 0])), shape)\n                gb.add_edge([g_2d[f_id], g_1d], face_cells)\n\n                shape = (g_1d.num_cells, g_2d[other_f_id].num_faces)\n                face_cells = sps.csc_matrix((data, (np.arange(g_1d.num_cells),\n                                                    conn[mask, 1])), shape)\n                gb.add_edge([g_2d[other_f_id], g_1d], face_cells)\n\n    gb.compute_geometry()\n    # Split the grids.\n    split_grid.split_fractures(gb, offset=0.1)\n    return gb",
  "def _dfn_grid_2d(folder, post, **kwargs):\n\n    cell_name = kwargs.get(\"cell_name\", \"Cells\")\n    cell_file_name = folder + cell_name + post\n\n    # Read the cells and construct the cell_faces matrix\n    with open(cell_file_name, 'r') as f:\n        num_cells, num_faces_cells = map(int, f.readline().split())\n        cell_faces_data = np.empty(num_faces_cells, dtype=np.int)\n        cell_faces_indptr = np.zeros(num_cells + 1, dtype=np.int)\n        cell_faces_indices = np.empty(num_faces_cells, dtype=np.int)\n\n        lines = list(islice(f, num_cells))\n        pos = 0\n        for cell_id, line in enumerate(lines):\n            data = np.fromstring(line, dtype=np.int, sep=' ')\n            cell_faces_indptr[cell_id + 1] = pos + data.size\n            index = slice(pos, pos + data.size)\n            cell_faces_indices[index] = np.abs(data)\n            cell_faces_data[index] = 2 * (data > 0) - 1\n            pos += data.size\n\n        cell_faces = sps.csc_matrix((cell_faces_data,\n                                     cell_faces_indices,\n                                     cell_faces_indptr))\n        del cell_faces_data, cell_faces_indptr, cell_faces_indices\n\n    face_name = kwargs.get(\"face_name\", \"Faces\")\n    face_file_name = folder + face_name + post\n\n    # Read the faces and construct the face_nodes matrix\n    with open(face_file_name, 'r') as f:\n        num_faces, num_nodes_faces = map(int, f.readline().split())\n        face_nodes_indices = np.empty(num_nodes_faces, dtype=np.int)\n\n        lines = list(islice(f, num_faces))\n        pos = 0\n        for face_id, line in enumerate(lines):\n            data = np.fromstring(line, dtype=np.int, sep=' ')\n            index = slice(pos, pos + data.size)\n            face_nodes_indices[index] = data\n            pos += data.size\n\n        face_nodes_indptr = np.hstack((np.arange(0, 2 * num_faces, 2),\n                                       2 * num_faces))\n        face_nodes = sps.csc_matrix((np.ones(num_nodes_faces, dtype=np.bool),\n                                     face_nodes_indices,\n                                     face_nodes_indptr))\n        del face_nodes_indices, face_nodes_indptr\n\n    node_name = kwargs.get(\"node_name\", \"Vertexes\")\n    node_file_name = folder + node_name + post\n\n    with open(node_file_name, 'r') as f:\n        num_nodes = int(f.readline())\n\n        nodes = np.empty((3, num_nodes))\n        lines = list(islice(f, num_nodes))\n        for node_id, line in enumerate(lines):\n            nodes[:, node_id] = np.fromstring(line, dtype=np.float, sep=' ')\n\n    return nodes, face_nodes, cell_faces",
  "def _nodes_faces_2d(grid_2d, faces_2d_id):\n\n    nodes_id = np.empty((2, faces_2d_id.size), dtype=np.int)\n    for cell_id, face_2d_id in enumerate(faces_2d_id):\n        index = slice(grid_2d.face_nodes.indptr[face_2d_id],\n                      grid_2d.face_nodes.indptr[face_2d_id + 1])\n        nodes_id[:, cell_id] = grid_2d.face_nodes.indices[index]\n\n    nodes_id = sort_point_pairs(nodes_id, is_circular=False)\n    return np.hstack((nodes_id[0, :], nodes_id[1, -1]))",
  "def _dfn_grid_1d(grid_2d, nodes_id):\n\n    num_faces = nodes_id.size\n    num_cells = num_faces - 1\n\n    cell_faces_indptr = 2 * np.arange(num_faces)\n    cell_faces_indices = np.vstack((np.arange(num_cells),\n                                    np.arange(1, num_cells + 1))\n                                   ).ravel('F')\n    cell_faces_data = np.ones(2 * num_cells)\n    cell_faces_data[::2] *= -1\n\n    cell_faces = sps.csc_matrix((cell_faces_data,\n                                 cell_faces_indices,\n                                 cell_faces_indptr))\n    del cell_faces_data, cell_faces_indptr, cell_faces_indices\n\n    face_nodes = sps.csc_matrix((np.ones(num_faces + 1, dtype=np.bool),\n                                 np.arange(num_faces + 1),\n                                 np.arange(num_faces + 1)))\n\n    return grid_2d.nodes[:, nodes_id], face_nodes, cell_faces",
  "def read_keyword(line):\n        # Read a single keyword, on the form  key = val\n        words = line.split('=')\n        assert len(words) == 2\n        key = words[0].strip()\n        val = words[1].strip()\n        return key, val",
  "def read_section(f, section_name):\n        # Read a section of the file, surrounded by a BEGIN / END wrapping\n        d = {}\n        for line in f:\n            if line.strip() == 'END ' + section_name.upper().strip():\n                return d\n            k, v = read_keyword(line)\n            d[k] = v",
  "def read_fractures(f, is_tess=False):\n        # Read the fracture\n        fracs = []\n        fracture_ids = []\n        trans = []\n        nd = 3\n        for line in f:\n            if not is_tess and line.strip() == 'END FRACTURE':\n                return fracs, np.asarray(fracture_ids), np.asarray(trans)\n            elif is_tess and line.strip() == 'END TESSFRACTURE':\n                return fracs, np.asarray(fracture_ids), np.asarray(trans)\n            if is_tess:\n                ids, num_vert = line.split()\n            else:\n                ids, num_vert, t = line.split()[:3]\n\n                trans.append(float(t))\n\n            ids = int(ids)\n            num_vert = int(num_vert)\n            vert = np.zeros((num_vert, nd))\n            for i in range(num_vert):\n                data = f.readline().split()\n                vert[i] = np.asarray(data[1:])\n\n            # Transpose to nd x n_pt format\n            vert = vert.T\n\n            # Read line containing normal vector, but disregard result\n            data = f.readline().split()\n            if is_tess:\n                trans.append(int(data[1]))\n            fracs.append(vert)\n            fracture_ids.append(ids)",
  "class DummyFracture(object):\n        def __init__(self, index):\n            self.index = index",
  "def __init__(self, index):\n            self.index = index",
  "class Fracture(object):\n\n    def __init__(self, points, index=None, check_convexity=True):\n        self.p = np.asarray(points, dtype=np.float)\n        # Ensure the points are ccw\n        self.points_2_ccw()\n        self.compute_centroid()\n        self.compute_normal()\n\n        self.orig_p = self.p.copy()\n\n        self.index = index\n\n        assert self.is_planar(), 'Points define non-planar fracture'\n        if check_convexity:\n            assert self.check_convexity(), 'Points form non-convex polygon'\n\n    def set_index(self, i):\n        self.index = i\n\n    def __eq__(self, other):\n        return self.index == other.index\n\n    def copy(self):\n        \"\"\" Return a deep copy of the fracture.\n\n        \"\"\"\n        p = np.copy(self.p)\n        return Fracture(p)\n\n    def points(self):\n        \"\"\"\n        Iterator over the vexrtexes of the bounding polygon\n\n        Yields:\n            np.array (3 x 1): polygon vertexes\n\n        \"\"\"\n        for i in range(self.p.shape[1]):\n            yield self.p[:, i].reshape((-1, 1))\n\n    def segments(self):\n        \"\"\"\n        Iterator over the segments of the bounding polygon.\n\n        Yields:\n            np.array (3 x 2): polygon segment\n        \"\"\"\n\n        sz = self.p.shape[1]\n        for i in range(sz):\n            yield self.p[:, np.array([i, i + 1]) % sz]\n\n    def is_vertex(self, p, tol=1e-4):\n        \"\"\" Check whether a given point is a vertex of the fracture.\n\n        Parameters:\n            p (np.array): Point to check\n            tol (double): Tolerance of point accuracy.\n\n        Returns:\n            True: if the point is in the vertex set, false if not.\n            int: Index of the identical vertex. None if not a vertex.\n        \"\"\"\n        p = p.reshape((-1, 1))\n        ap = np.hstack((p, self.p))\n        up, _, ind = setmembership.unique_columns_tol(ap, tol=tol*np.sqrt(3))\n        if up.shape[1] == ap.shape[1]:\n            return False, None\n        else:\n            occurences = np.where(ind == ind[0])[0]\n            return True, (occurences[1] - 1)\n\n    def points_2_ccw(self):\n        \"\"\"\n        Ensure that the points are sorted in a counter-clockwise order.\n\n        mplementation note:\n            For now, the ordering of nodes in based on a simple angle argument.\n            This will not be robust for general point clouds, but we expect the\n            fractures to be regularly shaped in this sense. In particular, we\n            will be safe if the cell is convex.\n\n        Returns:\n            np.array (int): The indices corresponding to the sorting.\n\n        \"\"\"\n        # First rotate coordinates to the plane\n        points_2d = self.plane_coordinates()\n        # Center around the 2d origin\n        points_2d -= np.mean(points_2d, axis=1).reshape((-1, 1))\n\n        theta = np.arctan2(points_2d[1], points_2d[0])\n        sort_ind = np.argsort(theta)\n\n        self.p = self.p[:, sort_ind]\n\n        return sort_ind\n\n    def add_points(self, p, check_convexity=True, tol=1e-4, enforce_pt_tol=None):\n        \"\"\"\n        Add a point to the polygon with ccw sorting enforced.\n\n        Always run a test to check that the points are still planar. By\n        default, a check of convexity is also performed, however, this can be\n        turned off to speed up simulations (the test uses sympy, which turns\n        out to be slow in many cases).\n\n        Parameters:\n            p (np.ndarray, 3xn): Points to add\n            check_convexity (boolean, optional): Verify that the polygon is\n                convex. Defaults to true.\n            tol (double): Tolerance used to check if the point already exists.\n\n        Return:\n            boolean, true if the resulting polygon is convex.\n\n        \"\"\"\n\n        to_enforce = np.hstack((np.zeros(self.p.shape[1], dtype=np.bool),\n                                np.ones(p.shape[1], dtype=np.bool)))\n        self.p = np.hstack((self.p, p))\n        self.p, _, _ = setmembership.unique_columns_tol(self.p, tol=tol)\n\n        # Sort points to ccw\n        mask = self.points_2_ccw()\n\n        if enforce_pt_tol is not None:\n            to_enforce = np.where(to_enforce[mask])[0]\n            dist = cg.dist_pointset(self.p)\n            dist /= np.amax(dist)\n            np.fill_diagonal(dist, np.inf)\n            mask = np.where(dist < enforce_pt_tol)[0]\n            mask = np.setdiff1d(mask, to_enforce, assume_unique=True)\n            self.remove_points(mask)\n\n        if check_convexity:\n            return self.check_convexity() and self.is_planar(tol)\n        else:\n            return self.is_planar()\n\n    def remove_points(self, ind, keep_orig=False):\n        \"\"\" Remove points from the fracture definition\n\n        Parameters:\n            ind (np array-like): Indices of points to remove.\n            keep_orig (boolean, optional): Whether to keep the original points\n                in the attribute orig_p. Defaults to False.\n\n        \"\"\"\n        self.p = np.delete(self.p, ind, axis=1)\n        if not keep_orig:\n            self.orig_p = self.p\n\n    def plane_coordinates(self):\n        \"\"\"\n        Represent the vertex coordinates in its natural 2d plane.\n\n        The plane does not necessarily have the third coordinate as zero (no\n        translation to the origin is made)\n\n        Returns:\n            np.array (2xn): The 2d coordinates of the vertexes.\n\n        \"\"\"\n        rotation = cg.project_plane_matrix(self.p)\n        points_2d = rotation.dot(self.p)\n\n        return points_2d[:2]\n\n    def check_convexity(self):\n        \"\"\"\n        Check if the polygon is convex.\n\n        Todo: If a hanging node is inserted at a segment, this may slightly\n            violate convexity due to rounding errors. It should be possible to\n            write an algorithm that accounts for this. First idea: Projcet\n            point onto line between points before and after, if the projection\n            is less than a tolerance, it is okay.\n\n        Returns:\n            boolean, true if the polygon is convex.\n\n        \"\"\"\n        p_2d = self.plane_coordinates()\n        return self.as_sp_polygon(p_2d).is_convex()\n\n    def is_planar(self, tol=1e-4):\n        \"\"\" Check if the points forming this fracture lies in a plane.\n\n        Parameters:\n            tol (double): Tolerance for non-planarity. Treated as an absolute\n                quantity (no scaling with fracture extent)\n\n        Returns:\n            boolean, True if the polygon is planar. False if not.\n        \"\"\"\n        p = self.p - np.mean(self.p, axis=1).reshape((-1, 1))\n        rot = cg.project_plane_matrix(p)\n        p_2d = rot.dot(p)\n        return np.max(np.abs(p_2d[2])) < tol\n\n    def compute_centroid(self):\n        \"\"\"\n        Compute, and redefine, center of the fracture in the form of the\n        centroid.\n\n        The method assumes the polygon is convex.\n\n        \"\"\"\n        # Rotate to 2d coordinates\n        rot = cg.project_plane_matrix(self.p)\n        p = rot.dot(self.p)\n        z = p[2, 0]\n        p = p[:2]\n\n        # Vectors from the first point to all other points. Subsequent pairs of\n        # these will span triangles which, assuming convexity, will cover the\n        # polygon.\n        v = p[:, 1:] - p[:, 0].reshape((-1, 1))\n        # The cell center of the triangles spanned by the subsequent vectors\n        cc = (p[:, 0].reshape((-1, 1)) + p[:, 1:-1] + p[:, 2:])/3\n        # Area of triangles\n        area = 0.5 * np.abs(v[0, :-1] * v[1, 1:] - v[1, :-1] * v[0, 1:])\n\n        # The center is found as the area weighted center\n        center = np.sum(cc * area, axis=1) / np.sum(area)\n\n        # Project back again.\n        self.center = rot.transpose().dot(np.append(center, z)).reshape((3, 1))\n\n    def compute_normal(self):\n        \"\"\" Compute normal to the polygon.\n        \"\"\"\n        self.normal = cg.compute_normal(self.p)[:, None]\n\n    def as_sp_polygon(self, p=None):\n        \"\"\" Represent polygon as a sympy object.\n\n        Parameters:\n            p (np.array, nd x npt, optional): Points for the polygon. Defaults\n                to None, in which case self.p is used.\n\n        Returns:\n            sympy.geometry.Polygon: Representation of the polygon formed by p.\n\n        \"\"\"\n        if p is None:\n            p = self.p\n\n        sp = [sympy.geometry.Point(p[:, i])\n              for i in range(p.shape[1])]\n        return sympy.geometry.Polygon(*sp)\n\n    def intersects(self, other, tol, check_point_contact=True):\n        \"\"\"\n        Find intersections between self and another polygon.\n\n        The function checks for both intersection between the interior of one\n        polygon with the boundary of the other, and pure boundary\n        intersections. Intersection types supported so far are\n            X (full intersection, possibly involving boundaries)\n            L (Two fractures intersect at a boundary)\n            T (Boundary segment of one fracture lies partly or completely in\n                the plane of another)\n        Parameters:\n            other (Fracture): To test intersection with.\n            tol (double): Geometric tolerance\n            check_point_contact (boolean, optional): If True, we check if a\n                single point (e.g. a vertex) of one fracture lies in the plane\n                of the other. This is usually an error, thus the test should be\n                on, but may be allowed when imposing external boundaries.\n\n        \"\"\"\n\n        # Find intersections between self and other. Note that the algorithms\n        # for intersections are not fully reflexive in terms of argument\n        # order, so we need to do two tests.\n\n        # Array for intersections with the interior of one polygon (as opposed\n        # to full boundary intersection, below)\n        int_points = np.empty((3, 0))\n\n        # Keep track of whether the intersection points are on the boundary of\n        # the polygons.\n        on_boundary_self = False\n        on_boundary_other = False\n\n        ####\n        # First compare max/min coordinates. If the bounding boxes of the\n        # fractures do not intersect, there is nothing to do.\n        min_self = self.p.min(axis=1)\n        max_self = self.p.max(axis=1)\n        min_other = other.p.min(axis=1)\n        max_other = other.p.max(axis=1)\n\n        # Account for a tolerance in the following test\n        max_self *= 1+np.sign(max_self)*tol\n        min_self *= 1-np.sign(min_self)*tol\n\n        max_other *= 1+np.sign(max_other)*tol\n        min_other *= 1-np.sign(min_other)*tol\n\n        if np.any(max_self < min_other) or np.any(min_self > max_other):\n            return int_points, on_boundary_self, on_boundary_other\n\n        #####\n        # Next screening: To intersect, both fractures must have vertexes\n        # either on both sides of each others plane, or close to the plane (T,\n        # L, Y)-type intersections.\n\n        # Vectors from centers of the fractures to the vertexes of the other\n        # fratures.\n        s_2_o = other.p - self.center.reshape((-1, 1))\n        o_2_s = self.p - other.center.reshape((-1, 1))\n\n        # Take the dot product of distance and normal vectors. Different signs\n        # for different vertexes signifies fractures that potentially cross.\n        other_from_self = np.sum(s_2_o * self.normal, axis=0)\n        self_from_other = np.sum(o_2_s * other.normal, axis=0)\n\n        # To avoid ruling out vertexes that lie on the plane of another\n        # fracture, we introduce a threshold for almost-zero values.\n        # The scaling factor is somewhat arbitrary here, and probably\n        # safeguards too much, but better safe than sorry. False positives will\n        # be corrected by the more accurate, but costly, computations below.\n        scaled_tol = tol * max(1, max(np.max(np.abs(s_2_o)),\n                                      np.max(np.abs(o_2_s))))\n\n        # We can terminate only if no vertexes are close to the plane of the\n        # other fractures.\n        if np.min(np.abs(other_from_self)) > scaled_tol and \\\n            np.min(np.abs(self_from_other)) > scaled_tol:\n            # If one of the fractures has all of its points on a single side of\n            # the other, there can be no intersections.\n            if np.all(np.sign(other_from_self) == 1) or \\\n                np.all(np.sign(other_from_self) == -1) or \\\n                np.all(np.sign(self_from_other) == 1) or \\\n                np.all(np.sign(self_from_other) == -1):\n                return int_points, on_boundary_self, on_boundary_other\n\n        ####\n        # Check for intersection between interior of one polygon with\n        # segment of the other.\n\n        # Compute intersections, with both polygons as first argument\n        isect_self_other = cg.polygon_segment_intersect(self.p, other.p,\n                                                        tol=tol,\n                                                        include_bound_pt=True)\n        isect_other_self = cg.polygon_segment_intersect(other.p, self.p,\n                                                        tol=tol,\n                                                        include_bound_pt=True)\n\n        # Process data\n        if isect_self_other is not None:\n            int_points = np.hstack((int_points, isect_self_other))\n\n            # An intersection between self and other (in that order) is defined\n            # as being between interior of self and boundary of other. See\n            # polygon_segment_intersect for details.\n            on_boundary_self = False\n            on_boundary_other = False\n\n        if isect_other_self is not None:\n            int_points = np.hstack((int_points, isect_other_self))\n\n            # Interior of other intersected by boundary of self\n            on_boundary_self = False\n            on_boundary_other = False\n\n        if int_points.shape[1] > 1:\n            int_points, _, _ \\\n                = setmembership.unique_columns_tol(int_points, tol=tol)\n\n        # There should be at most two of these points.\n        # In some cases, likely involving extrusion, several segments may lay\n        # essentially in the fracture plane, producing more than two segments.\n        # Thus, if more than two colinear points are found, pick out the first\n        # and last one.\n        if int_points.shape[1] > 2:\n            if cg.is_collinear(int_points, tol):\n                sort_ind = cg.argsort_point_on_line(int_points, tol)\n                int_points = int_points[:, [sort_ind[0], sort_ind[-1]]]\n            else:\n                # This is a bug\n                raise ValueError(''' Found more than two intersection between\n                                 fracture polygons.\n                                 ''')\n\n        ####\n        # Next, check for intersections between the polygon boundaries\n        bound_sect_self_other = cg.polygon_boundaries_intersect(self.p,\n                                                                other.p,\n                                                                tol=tol)\n        bound_sect_other_self = cg.polygon_boundaries_intersect(other.p,\n                                                                self.p,\n                                                                tol=tol)\n\n        def point_on_segment(ip, poly, need_two=True):\n            # Check if a set of points are located on a single segment\n            if need_two and ip.shape[1] < 2 or\\\n                ((not need_two) and ip.shape[1] < 1):\n                return False\n            start = poly\n            end = np.roll(poly, 1, axis=1)\n            for si in range(start.shape[1]):\n                dist, cp = cg.dist_points_segments(ip, start[:, si],\n                                                   end[:, si])\n                if np.all(dist < tol):\n                    return True\n            return False\n        # Short cut: If no boundary intersections, we return the interior\n        # points\n        if len(bound_sect_self_other) == 0 and len(bound_sect_other_self) == 0:\n            if check_point_contact and int_points.shape[1] == 1:\n                #  contacts are not implemented. Give a warning, return no\n                # interseciton, and hope the meshing software is merciful\n                if hasattr(self, 'index') and hasattr(other, 'index'):\n                    logger.warning(\"\"\"Found a point contact between fracture\n                                   %i\n                                   and %i at (%.5f, %.5f, %.5f)\"\"\", self.index,\n                                   other.index, *int_points)\n                else:\n                    logger.warning(\"\"\"Found point contact between fractures\n                                   with no index. Coordinate: (%.5f, %.5f,\n                                   %.5f)\"\"\", *int_points)\n                return np.empty((3, 0)), on_boundary_self, on_boundary_other\n            # None of the intersection points lay on the boundary\n\n\n\n        # The 'interior' points can still be on the boundary (naming\n        # of variables should be updated). The points form a boundary\n        # segment if they all lie on the a single segment of the\n        # fracture.\n        on_boundary_self = point_on_segment(int_points, self.p)\n        on_boundary_other = point_on_segment(int_points, other.p)\n        return int_points, on_boundary_self, on_boundary_other\n\n\n    def impose_boundary(self, box, tol):\n        \"\"\"\n        Impose a boundary on a fracture, defined by a bounding box.\n\n        If the fracture extends outside the box, it will be truncated, and new\n        points are inserted on the intersection with the boundary. It is\n        assumed that the points defining the fracture defines a convex set (in\n        its natural plane) to begin with.\n\n        The attribute self.p will be changed if intersections are found. The\n        original vertexes can still be recovered from self.orig_p.\n\n        The box is specified by its extension in Cartesian coordinates.\n\n        Parameters:\n            box (dicitionary): The bounding box, specified by keywords xmin,\n                xmax, ymin, ymax, zmin, zmax.\n            tol (double): Tolerance, defines when two points are considered\n                equal.\n\n        \"\"\"\n\n        # Maximal extent of the fracture\n        min_coord = self.p.min(axis=1)\n        max_coord = self.p.max(axis=1)\n\n        # Coordinates of the bounding box\n        x0_box = box['xmin']\n        x1_box = box['xmax']\n        y0_box = box['ymin']\n        y1_box = box['ymax']\n        z0_box = box['zmin']\n        z1_box = box['zmax']\n\n        # Gather the box coordinates in an array\n        box_array = np.array([[x0_box, x1_box],\n                              [y0_box, y1_box],\n                              [z0_box, z1_box]])\n\n        # We need to be a bit careful if the fracture extends outside the\n        # bounding box on two (non-oposite) sides. In addition to the insertion\n        # of points along the segments defined by self.p, this will also\n        # require the insertion of points at the meeting of the two sides. To\n        # capture this, we first look for the intersection between the fracture\n        # and planes that extends further than the plane, and then later\n        # move the intersection points to lay at the real bounding box\n        x0 = np.minimum(x0_box, min_coord[0] - 10 * tol)\n        x1 = np.maximum(x1_box, max_coord[0] + 10 * tol)\n        y0 = np.minimum(y0_box, min_coord[1] - 10 * tol)\n        y1 = np.maximum(y1_box, max_coord[1] + 10 * tol)\n        z0 = np.minimum(z0_box, min_coord[2] - 10 * tol)\n        z1 = np.maximum(z1_box, max_coord[2] + 10 * tol)\n\n\n        def outside_box(p, bound_i):\n            # Helper function to test if points are outside the bounding box\n            relative = np.amax(np.linalg.norm(p, axis=0))\n            p = cg.snap_to_grid(p, tol=tol/relative)\n            # snap_to_grid will impose a grid of size self.tol, thus points\n            # that are more than half that distance away from the boundary\n            # are deemed outside.\n            # To reduce if-else on the boundary index, we compute all bounds,\n            # and then return the relevant one, as specified by bound_i\n            west_of = p[0] < x0_box - tol / 2\n            east_of = p[0] > x1_box + tol / 2\n            south_of = p[1] < y0_box - tol / 2\n            north_of = p[1] > y1_box + tol / 2\n            beneath = p[2] < z0_box - tol / 2\n            above = p[2] > z1_box + tol / 2\n            outside = np.vstack((west_of, east_of, south_of, north_of,\n                                 beneath, above))\n            return outside[bound_i]\n\n\n        # Represent the planes of the bounding box as fractures, to allow\n        # us to use the fracture intersection methods.\n        # For each plane, we keep the fixed coordinate to the value specified\n        # by the box, and extend the two others to cover segments that run\n        # through the extension of the plane only.\n\n        # Move these to FractureNetwork.impose_external_boundary.\n        # Doesn't immediately work. Depend on the fracture.\n        west = Fracture(np.array([[x0_box, x0_box, x0_box, x0_box],\n                                  [y0, y1, y1, y0],\n                                  [z0, z0, z1, z1]]),\n                        check_convexity=False)\n        east = Fracture(np.array([[x1_box, x1_box, x1_box, x1_box],\n                                  [y0, y1, y1, y0],\n                                  [z0, z0, z1, z1]]),\n                        check_convexity=False)\n        south = Fracture(np.array([[x0, x1, x1, x0],\n                                   [y0_box, y0_box, y0_box, y0_box],\n                                   [z0, z0, z1, z1]]),\n                         check_convexity=False)\n        north = Fracture(np.array([[x0, x1, x1, x0],\n                                   [y1_box, y1_box, y1_box, y1_box],\n                                   [z0, z0, z1, z1]]),\n                         check_convexity=False)\n        bottom = Fracture(np.array([[x0, x1, x1, x0],\n                                    [y0, y0, y1, y1],\n                                    [z0_box, z0_box, z0_box, z0_box]]),\n                          check_convexity=False)\n        top = Fracture(np.array([[x0, x1, x1, x0],\n                                 [y0, y0, y1, y1],\n                                 [z1_box, z1_box, z1_box, z1_box]]),\n                       check_convexity=False)\n        # Collect in a list to allow iteration\n        bound_planes = [west, east, south, north, bottom, top]\n\n        # Loop over all boundary sides and look for intersections\n        for bi, bf in enumerate(bound_planes):\n            # Dimensions that are not fixed by bf\n            active_dims = np.ones(3, dtype=np.bool)\n            active_dims[np.floor(bi/2).astype('int')] = 0\n            # Convert to indices\n            active_dims = np.squeeze(np.argwhere(active_dims))\n\n            # Find intersection points\n            isect, _, _ = self.intersects(bf, tol, check_point_contact=False)\n            num_isect = isect.shape[1]\n            if len(isect) > 0 and num_isect > 0:\n                num_pts_orig = self.p.shape[1]\n                # Add extra points at the end of the point list.\n                self.p, _, _ \\\n                    = setmembership.unique_columns_tol(np.hstack((self.p,\n                                                                  isect)))\n                num_isect = self.p.shape[1] - num_pts_orig\n                if num_isect == 0:\n                    continue\n\n                # Sort fracture points in a ccw order.\n                # This must be done before sliding the intersection points\n                # (below), since the latter may break convexity of the\n                # fracture, thus violate the (current) implementation of\n                # points_2_ccw()\n                sort_ind = self.points_2_ccw()\n\n                # The next step is to decide whether and how to move the\n                # intersection points.\n                # The intersection points will lay on the bounding box on one\n                # of the dimensions (that of bf, specified by xyz_01_box), but\n                # may be outside the box for the other sides (xyz_01). We thus\n                # need to move the intersection points in the plane of bf and\n                # the fracture plane.\n                # The relevant tangent vector is perpendicular to both the\n                # fracture and bf\n                normal_self = cg.compute_normal(self.p)\n                normal_bf = cg.compute_normal(bf.p)\n                tangent = np.cross(normal_self, normal_bf)\n                # Unit vector\n                tangent *= 1./ np.linalg.norm(tangent)\n\n                isect_ind = np.argwhere(sort_ind >= num_pts_orig).ravel('F')\n                p_isect = self.p[:, isect_ind]\n\n                # Loop over all intersection points and active dimensions. If\n                # the point is outside the bounding box, move it.\n                for pi in range(num_isect):\n                    for dim, other_dim in zip(active_dims, active_dims[::-1]):\n                        # Test against lower boundary\n                        lower_diff = p_isect[dim, pi] - box_array[dim, 0]\n                        # Modify coordinates if necessary. This dimension is\n                        # simply set to the boundary, while the other should\n                        # slide along the tangent vector\n                        if lower_diff < 0:\n                            p_isect[dim, pi] = box_array[dim, 0]\n                            # We know lower_diff != 0, no division by zero.\n                            # We may need some tolerances, though.\n                            t = tangent[dim] / lower_diff\n                            p_isect[other_dim, pi] += t * tangent[other_dim]\n\n                        # Same treatment of the upper boundary\n                        upper_diff = p_isect[dim, pi] - box_array[dim, 1]\n                        # Modify coordinates if necessary. This dimension is\n                        # simply set to the boundary, while the other should\n                        # slide along the tangent vector\n                        if upper_diff > 0:\n                            p_isect[dim, pi] = box_array[dim, 1]\n                            t = tangent[dim] / upper_diff\n                            p_isect[other_dim, pi] -= t * tangent[other_dim]\n\n                # Finally, identify points that are outside the face bf\n                inside = np.logical_not(outside_box(self.p, bi))\n                # Dump points that are outside.\n                self.p = self.p[:, inside]\n                self.p, _, _ = setmembership.unique_columns_tol(self.p, tol=tol)\n                # We have modified the fractures, so re-calculate the centroid\n                self.compute_centroid()\n            else:\n                # No points exists, but the whole point set can still be\n                # outside the relevant boundary face.\n                outside = outside_box(self.p, bi)\n                if np.all(outside):\n                    self.p = np.empty((3, 0))\n            # There should be at least three points in a fracture.\n            if self.p.shape[1] < 3:\n                break\n\n    def __repr__(self):\n        return self.__str__()\n\n    def __str__(self):\n        s = 'Points: \\n'\n        s += str(self.p) + '\\n'\n        s += 'Center: ' + str(self.center)\n        return s\n\n    def plot_frame(self, ax=None):\n\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.gca(projection='3d')\n        x = np.append(self.p[0], self.p[0, 0])\n        y = np.append(self.p[1], self.p[1, 0])\n        z = np.append(self.p[2], self.p[2, 0])\n        ax.plot(x, y, z)\n        return ax",
  "class EllipticFracture(Fracture):\n    \"\"\"\n    Subclass of Fractures, representing an elliptic fracture.\n\n    \"\"\"\n\n    def __init__(self, center, major_axis, minor_axis, major_axis_angle,\n                 strike_angle, dip_angle, num_points=16):\n        \"\"\"\n        Initialize an elliptic shaped fracture, approximated by a polygon.\n\n\n        The rotation of the plane is calculated using three angles. First, the\n        rotation of the major axis from the x-axis. Next, the fracture is\n        inclined by specifying the strike angle (which gives the rotation\n        axis) measured from the x-axis, and the dip angle. All angles are\n        measured in radians.\n\n        Parameters:\n            center (np.ndarray, size 3x1): Center coordinates of fracture.\n            major_axis (double): Length of major axis (radius-like, not\n                diameter).\n            minor_axis (double): Length of minor axis. There are no checks on\n                whether the minor axis is less or equal the major.\n            major_axis_angle (double, radians): Rotation of the major axis from\n                the x-axis. Measured before strike-dip rotation, see above.\n            strike_angle (double, radians): Line of rotation for the dip.\n                Given as angle from the x-direction.\n            dip_angle (double, radians): Dip angle, i.e. rotation around the\n                strike direction.\n            num_points (int, optional): Number of points used to approximate\n                the ellipsis. Defaults to 16.\n\n        Example:\n            Fracture centered at [0, 1, 0], with a ratio of lengths of 2,\n            rotation in xy-plane of 45 degrees, and an incline of 30 degrees\n            rotated around the x-axis.\n            >>> frac = EllipticFracture(np.array([0, 1, 0]), 10, 5, np.pi/4, 0,\n                                        np.pi/6)\n\n        \"\"\"\n        center = np.asarray(center)\n        if center.ndim == 1:\n            center = center.reshape((-1, 1))\n        self.center = center\n\n        # First, populate polygon in the xy-plane\n        angs = np.linspace(0, 2 * np.pi, num_points + 1, endpoint=True)[:-1]\n        x = major_axis * np.cos(angs)\n        y = minor_axis * np.sin(angs)\n        z = np.zeros_like(angs)\n        ref_pts = np.vstack((x, y, z))\n\n        assert cg.is_planar(ref_pts)\n\n        # Rotate reference points so that the major axis has the right\n        # orientation\n        major_axis_rot = cg.rot(major_axis_angle, [0, 0, 1])\n        rot_ref_pts = major_axis_rot.dot(ref_pts)\n\n        assert cg.is_planar(rot_ref_pts)\n\n        # Then the dip\n        # Rotation matrix of the strike angle\n        strike_rot = cg.rot(strike_angle, np.array([0, 0, 1]))\n        # Compute strike direction\n        strike_dir = strike_rot.dot(np.array([1, 0, 0]))\n        dip_rot = cg.rot(dip_angle, strike_dir)\n\n        dip_pts = dip_rot.dot(rot_ref_pts)\n\n        assert cg.is_planar(dip_pts)\n\n        # Set the points, and store them in a backup.\n        self.p = center + dip_pts\n        self.orig_p = self.p.copy()\n\n        # Compute normal vector\n        self.normal = cg.compute_normal(self.p)[:, None]\n\n        assert cg.is_planar(self.orig_p, self.normal)",
  "class Intersection(object):\n\n    def __init__(self, first, second, coord, bound_first=False, bound_second=False):\n        self.first = first\n        self.second = second\n        self.coord = coord\n        # Information on whether the intersection points lies on the boundaries\n        # of the fractures\n        self.bound_first = bound_first\n        self.bound_second = bound_second\n\n    def __repr__(self):\n        s = 'Intersection between fractures ' + str(self.first.index) + ' and ' + \\\n            str(self.second.index) + '\\n'\n        s += 'Intersection points: \\n'\n        for i in range(self.coord.shape[1]):\n            s += '(' + str(self.coord[0, i]) + ', ' + str(self.coord[1, i]) + ', ' + \\\n                 str(self.coord[2, i]) + ') \\n'\n        if self.coord.size > 0:\n            s += 'On boundary of first fracture ' + str(self.bound_first) + '\\n'\n            s += 'On boundary of second fracture ' + str(self.bound_second)\n            s += '\\n'\n        return s\n\n    def get_other_fracture(self, i):\n        if self.first == i:\n            return self.second\n        else:\n            return self.first\n\n    def on_boundary_of_fracture(self, i):\n        if self.first == i:\n            return self.bound_first\n        elif self.second == i:\n            return self.bound_second\n        else:\n            raise ValueError('Fracture ' + str(i) + ' is not in intersection')",
  "class FractureNetwork(object):\n    \"\"\"\n    Collection of Fractures with geometrical information. Facilitates\n    computation of intersections of the fracture. Also incorporates the\n    bounding box as six boundary fractures and the possibility to impose\n    additional gridding constraints through subdomain boundaries. To ensure\n    that all fractures lie within the box, call impose_external_boundary()\n    _after_ all fractures and subdomains have been specified.\n    \"\"\"\n\n    def __init__(self, fractures=None, verbose=0, tol=1e-4):\n\n        self._fractures = fractures\n\n        for i, f in enumerate(self._fractures):\n            f.set_index(i)\n\n        self.intersections = []\n\n        self.has_checked_intersections = False\n        self.tol = tol\n        self.verbose = verbose\n\n        # Initialize with an empty domain. Can be modified later by a call to\n        # 'impose_external_boundary()'\n        self.domain = None\n\n        # Initialize mesh size parameters as empty\n        self.h_min = None\n        self.h_ideal = None\n\n        # Assign an empty tag dictionary\n        self.tags = {}\n\n        # No auxiliary points have been added\n        self.auxiliary_points_added = False\n\n    def add(self, f):\n        # Careful here, if we remove one fracture and then add, we'll have\n        # identical indices.\n        f.set_index(len(self._fractures))\n        self._fractures.append(f)\n\n    def __getitem__(self, position):\n        return self._fractures[position]\n\n    def get_normal(self, frac):\n        return self._fractures[frac].normal\n\n    def get_center(self, frac):\n        return self._fractures[frac].center\n\n    def intersections_of_fracture(self, frac):\n        \"\"\" Get all known intersections for a fracture.\n\n        If called before find_intersections(), the returned list will be empty.\n\n        Paremeters:\n            frac: A fracture in the network\n\n        Returns:\n            np.array (Intersection): Array of intersections\n\n        \"\"\"\n        if isinstance(frac, int):\n            fi = frac\n        else:\n            fi = frac.index\n        frac_arr = []\n        for i in self.intersections:\n            if i.coord.size == 0:\n                continue\n            if i.first.index == fi or i.second.index == fi:\n                frac_arr.append(i)\n        return frac_arr\n\n\n    def find_intersections(self, use_orig_points=False):\n        \"\"\"\n        Find intersections between fractures in terms of coordinates.\n\n        The intersections are stored in the attribute self.Intersections.\n\n        Handling of the intersections (splitting into non-intersecting\n        polygons, paving the way for gridding) is taken care of by the function\n        split_intersections().\n\n        Note that find_intersections() should be invoked after external\n        boundaries are imposed. If the reverse order is applied, intersections\n        outside the domain may be identified, with unknown consequences for the\n        reliability of the methods. If intersections outside the bounding box\n        are of interest, these can be found by setting the parameter\n        use_orig_points to True.\n\n        Parameters:\n            use_orig_points (boolean, optional): Whether to use the original\n                fracture description in the search for intersections. Defaults\n                to False. If True, all fractures will have their attribute p\n                reset to their original value.\n\n        \"\"\"\n        self.has_checked_intersections = True\n        logger.info('Find intersection between fratures')\n        start_time = time.time()\n\n        # If desired, use the original points in the fracture intersection.\n        # This will reset the field self._fractures.p, and thus revoke\n        # modifications due to boundaries etc.\n        if use_orig_points:\n            for f in self._fractures:\n                f.p = f.orig_p\n\n        for i, first in enumerate(self._fractures):\n            for j in range(i + 1, len(self._fractures)):\n                second = self._fractures[j]\n                logger.info('Processing fracture %i and %i', i, j)\n                isect, bound_first, bound_second = first.intersects(second,\n                                                                    self.tol)\n                if np.array(isect).size > 0:\n                    logger.info('Found an intersection between %i and %i', i, j)\n                    # Let the intersection know whether both intersection\n                    # points lies on the boundary of each fracture\n                    self.intersections.append(Intersection(first, second,\n                                                           isect,\n                                                           bound_first=bound_first,\n                                                           bound_second=bound_second))\n\n        logger.info('Found %i intersections. Ellapsed time: %.5f',\n                    len(self.intersections), time.time() - start_time)\n\n\n    def intersection_info(self, frac_num=None):\n        # Number of fractures with some intersection\n        num_intersecting_fracs = 0\n        # Number of intersections in total\n        num_intersections = 0\n\n        if frac_num is None:\n            frac_num = np.arange(len(self._fractures))\n        for f in frac_num:\n            isects = []\n            for i in self.intersections:\n                if i.first.index == f and i.coord.shape[1] > 0:\n                    isects.append(i.second.index)\n                elif i.second.index == f and i.coord.shape[1] > 0:\n                    isects.append(i.first.index)\n            if len(isects) > 0:\n                num_intersecting_fracs += 1\n                num_intersections += len(isects)\n\n                if self.verbose > 1:\n                    print('  Fracture ' + str(f) + ' intersects with'\\\n                          ' fractuer(s) ' + str(isects))\n        # Print aggregate numbers. Note that all intersections are counted\n        # twice (from first and second), thus divide by two.\n        print('In total ' + str(num_intersecting_fracs) + ' fractures '\n              + 'intersect in ' + str(int(num_intersections/2)) \\\n              + ' intersections')\n\n\n    def split_intersections(self):\n        \"\"\"\n        Based on the fracture network, and their known intersections, decompose\n        the fractures into non-intersecting sub-polygons. These can\n        subsequently be exported to gmsh.\n\n        The method will add an atribute decomposition to self.\n\n        \"\"\"\n\n        logger.info('Split intersections')\n        start_time = time.time()\n\n        # First, collate all points and edges used to describe fracture\n        # boundaries and intersections.\n        all_p, edges,\\\n            edges_2_frac, is_boundary_edge = self._point_and_edge_lists()\n\n        if self.verbose > 1:\n            self._verify_fractures_in_plane(all_p, edges, edges_2_frac)\n\n        # By now, all segments in the grid are defined by a unique set of\n        # points and edges. The next task is to identify intersecting edges,\n        # and split them.\n        all_p, edges, edges_2_frac, is_boundary_edge\\\n            = self._remove_edge_intersections(all_p, edges, edges_2_frac,\n                                              is_boundary_edge)\n\n        if self.verbose > 1:\n            self._verify_fractures_in_plane(all_p, edges, edges_2_frac)\n\n        # Store the full decomposition.\n        self.decomposition = {'points': all_p,\n                              'edges': edges.astype('int'),\n                              'is_bound': is_boundary_edge,\n                              'edges_2_frac': edges_2_frac}\n        polygons = []\n        line_in_frac = []\n        for fi, _ in enumerate(self._fractures):\n            ei = []\n            ei_bound = []\n            # Find the edges of this fracture, add to either internal or\n            # external fracture list\n            for i, e in enumerate(zip(edges_2_frac, is_boundary_edge)):\n                # Check that the boundary information matches the fractures\n                assert e[0].size == e[1].size\n                hit = np.where(e[0] == fi)[0]\n                if hit.size == 1:\n                    if e[1][hit]:\n                        ei_bound.append(i)\n                    else:\n                        ei.append(i)\n                elif hit.size > 1:\n                    raise ValueError('Non-unique fracture edge relation')\n                else:\n                    continue\n\n            poly = sort_points.sort_point_pairs(edges[:2, ei_bound])\n            polygons.append(poly)\n            line_in_frac.append(ei)\n\n        self.decomposition['polygons'] = polygons\n        self.decomposition['line_in_frac'] = line_in_frac\n\n#                              'polygons': poly_segments,\n#                             'polygon_frac': poly_2_frac}\n\n        logger.info('Finished fracture splitting after %.5f seconds',\n                    time.time() - start_time)\n\n    def _fracs_2_edges(self, edges_2_frac):\n        \"\"\" Invert the mapping between edges and fractures.\n\n        Returns:\n            List of list: For each fracture, index of all edges that points to\n                it.\n        \"\"\"\n        f2e = []\n        for fi in len(self._fractures):\n            f_l = []\n            for ei, e in enumerate(edges_2_frac):\n                if fi in e:\n                    f_l.append(ei)\n            f2e.append(f_l)\n        return f2e\n\n    def _point_and_edge_lists(self):\n        \"\"\"\n        Obtain lists of all points and connections necessary to describe\n        fractures and their intersections.\n\n        Returns:\n            np.ndarray, 3xn: Unique coordinates of all points used to describe\n            the fracture polygons, and their intersections.\n\n            np.ndarray, 2xn_edge: Connections between points, formed either\n                by a fracture boundary, or a fracture intersection.\n            list: For each edge, index of all fractures that point to the\n                edge.\n            np.ndarray of bool (size=num_edges): A flag telling whether the\n                edge is on the boundary of a fracture.\n\n        \"\"\"\n        logger.info('Compile list of points and edges')\n        start_time = time.time()\n\n        # Field for all points in the fracture description\n        all_p = np.empty((3, 0))\n        # All edges, either as fracture boundary, or fracture intersection\n        edges = np.empty((2, 0))\n        # For each edge, a list of all fractures pointing to the edge.\n        edges_2_frac = []\n\n        # Field to know if an edge is on the boundary of a fracture.\n        # Not sure what to do with a T-type intersection here\n        is_boundary_edge = []\n\n        # First loop over all fractures. All edges are assumed to be new; we\n        # will deal with coinciding points later.\n        for fi, frac in enumerate(self._fractures):\n            num_p = all_p.shape[1]\n            num_p_loc = frac.p.shape[1]\n            all_p = np.hstack((all_p, frac.p))\n\n            loc_e = num_p + np.vstack((np.arange(num_p_loc),\n                                       (np.arange(num_p_loc) + 1) % num_p_loc))\n            edges = np.hstack((edges, loc_e))\n            for i in range(num_p_loc):\n                edges_2_frac.append([fi])\n                is_boundary_edge.append([True])\n\n        # Next, loop over all intersections, and define new points and edges\n        for i in self.intersections:\n            # Only add information if the intersection exists, that is, it has\n            # a coordinate.\n            if i.coord.size > 0:\n                num_p = all_p.shape[1]\n                all_p = np.hstack((all_p, i.coord))\n\n                edges = np.hstack(\n                    (edges, num_p + np.arange(2).reshape((-1, 1))))\n                edges_2_frac.append([i.first.index, i.second.index])\n                # If the intersection points are on the boundary of both\n                # fractures, this is a boundary segment.\n                # This does not cover the case of a T-intersection, that will\n                # have to come later.\n                if i.bound_first and i.bound_second:\n                    is_boundary_edge.append([True, True])\n                elif i.bound_first and not i.bound_second:\n                    is_boundary_edge.append([True, False])\n                elif not i.bound_first and i.bound_second:\n                    is_boundary_edge.append([False, True])\n                else:\n                    is_boundary_edge.append([False, False])\n\n        # Ensure that edges are integers\n        edges = edges.astype('int')\n\n        logger.info('Points and edges done. Elapsed time %.5f', time.time() -\n                    start_time)\n\n        return self._uniquify_points_and_edges(all_p, edges, edges_2_frac,\n                                               is_boundary_edge)\n\n    def _uniquify_points_and_edges(self, all_p, edges, edges_2_frac,\n                                   is_boundary_edge):\n        # Snap the points to an underlying Cartesian grid. This is the basis\n        # for declearing two points equal\n        # NOTE: We need to account for dimensions in the tolerance;\n\n        start_time = time.time()\n        logger.info(\"\"\"Uniquify points and edges, starting with %i points, %i\n                    edges\"\"\", all_p.shape[1], edges.shape[1])\n\n#        all_p = cg.snap_to_grid(all_p, tol=self.tol)\n\n        # We now need to find points that occur in multiple places\n        p_unique, unique_ind_p, \\\n            all_2_unique_p = setmembership.unique_columns_tol(all_p, tol=\n                                                              self.tol *\n                                                              np.sqrt(3))\n\n        # Update edges to work with unique points\n        edges = all_2_unique_p[edges]\n\n        # Look for edges that share both nodes. These will be identical, and\n        # will form either a L/Y-type intersection (shared boundary segment),\n        # or a three fractures meeting in a line.\n        # Do a sort of edges before looking for duplicates.\n        e_unique, unique_ind_e, all_2_unique_e = \\\n            setmembership.unique_columns_tol(np.sort(edges, axis=0))\n\n        # Update the edges_2_frac map to refer to the new edges\n        edges_2_frac_new = e_unique.shape[1] * [np.empty(0)]\n        is_boundary_edge_new = e_unique.shape[1] * [np.empty(0)]\n\n        for old_i, new_i in enumerate(all_2_unique_e):\n            edges_2_frac_new[new_i], ind =\\\n                np.unique(np.hstack((edges_2_frac_new[new_i],\n                                     edges_2_frac[old_i])),\n                          return_index=True)\n            tmp = np.hstack((is_boundary_edge_new[new_i],\n                             is_boundary_edge[old_i]))\n            is_boundary_edge_new[new_i] = tmp[ind]\n\n        edges_2_frac = edges_2_frac_new\n        is_boundary_edge = is_boundary_edge_new\n\n        # Represent edges by unique values\n        edges = e_unique\n\n        # The uniquification of points may lead to edges with identical start\n        # and endpoint. Find and remove these.\n        point_edges = np.where(np.squeeze(np.diff(edges, axis=0)) == 0)[0]\n        edges = np.delete(edges, point_edges, axis=1)\n        unique_ind_e = np.delete(unique_ind_e, point_edges)\n        for ri in point_edges[::-1]:\n            del edges_2_frac[ri]\n            del is_boundary_edge[ri]\n\n        # Ensure that the edge to fracture map to a list of numpy arrays.\n        # Use unique so that the same edge only refers to an edge once.\n        edges_2_frac = [np.unique(np.array(edges_2_frac[i])) for i in\n                        range(len(edges_2_frac))]\n\n        # Sanity check, the fractures should still be defined by points in a\n        # plane.\n        self._verify_fractures_in_plane(p_unique, edges, edges_2_frac)\n\n        logger.info('''Uniquify complete. %i points, %i edges. Ellapsed time\n                    %.5f''', p_unique.shape[1], edges.shape[1], time.time() -\n                    start_time)\n\n        return p_unique, edges, edges_2_frac, is_boundary_edge\n\n    def _remove_edge_intersections(self, all_p, edges, edges_2_frac,\n                                   is_boundary_edge):\n        \"\"\"\n        Remove crossings from the set of fracture intersections.\n\n        Intersecting intersections (yes) are split, and new points are\n        introduced.\n\n        Parameters:\n            all_p (np.ndarray, 3xn): Coordinates of all points used to describe\n                the fracture polygons, and their intersections. Should be\n                unique.\n            edges (np.ndarray, 2xn): Connections between points, formed either\n                by a fracture boundary, or a fracture intersection.\n            edges_2_frac (list): For each edge, index of all fractures that\n                point to the edge.\n            is_boundary_edge (np.ndarray of bool, size=num_edges): A flag\n                telling whether the edge is on the boundary of a fracture.\n\n        Returns:\n            The same fields, but updated so that all edges are\n            non-intersecting.\n\n        \"\"\"\n        logger.info('Remove edge intersections')\n        start_time = time.time()\n\n        # The algorithm loops over all fractures, pulls out edges associated\n        # with the fracture, project to the local 2D plane, and look for\n        # intersections there (direct search in 3D may also work, but this was\n        # a simple option). When intersections are found, the global lists of\n        # points and edges are updated.\n        for fi, frac in enumerate(self._fractures):\n\n            logger.debug('Remove intersections from fracture %i', fi)\n\n            # Identify the edges associated with this fracture\n            # It would have been more convenient to use an inverse\n            # relationship frac_2_edge here, but that would have made the\n            # update for new edges (towards the end of this loop) more\n            # cumbersome.\n            edges_loc_ind = []\n            for ei, e in enumerate(edges_2_frac):\n                if np.any(e == fi):\n                    edges_loc_ind.append(ei)\n\n            edges_loc = np.vstack((edges[:, edges_loc_ind],\n                                   np.array(edges_loc_ind)))\n            p_ind_loc = np.unique(edges_loc[:2])\n            p_loc = all_p[:, p_ind_loc]\n\n            p_2d, edges_2d, p_loc_c, rot = self._points_2_plane(p_loc,\n                                                                edges_loc,\n                                                                p_ind_loc)\n\n            # Add a tag to trace the edges during splitting\n            edges_2d[2] = edges_loc[2]\n\n            # Obtain new points and edges, so that no edges on this fracture\n            # are intersecting.\n            # It seems necessary to increase the tolerance here somewhat to\n            # obtain a more robust algorithm. Not sure about how to do this\n            # consistent.\n            p_new, edges_new = cg.remove_edge_crossings(p_2d, edges_2d,\n                                                        tol=self.tol,\n                                                        verbose=self.verbose,\n                                                        snap=False)\n            # Then, patch things up by converting new points to 3D,\n\n            # From the design of the functions in cg, we know that new points\n            # are attached to the end of the array. A more robust alternative\n            # is to find unique points on a combined array of p_loc and p_new.\n            p_add = p_new[:, p_ind_loc.size:]\n            num_p_add = p_add.shape[1]\n\n            # Add third coordinate, and map back to 3D\n            p_add = np.vstack((p_add, np.zeros(num_p_add)))\n\n            # Inverse of rotation matrix is the transpose, add cloud center\n            # correction\n            p_add_3d = rot.transpose().dot(p_add) + p_loc_c\n\n            # The new points will be added at the end of the global point array\n            # (if not, we would need to renumber all global edges).\n            # Global index of added points\n            ind_p_add = all_p.shape[1] + np.arange(num_p_add)\n            # Global index of local points (new and added)\n            p_ind_exp = np.hstack((p_ind_loc, ind_p_add))\n\n            # Add the new points towards the end of the list.\n            all_p = np.hstack((all_p, p_add_3d))\n\n            new_all_p, _, ia = setmembership.unique_columns_tol(all_p,\n                                                                self.tol)\n\n            # Handle case where the new point is already represented in the\n            # global list of points.\n            if new_all_p.shape[1] < all_p.shape[1]:\n                all_p = new_all_p\n                p_ind_exp = ia[p_ind_exp]\n\n            # The ordering of the global edge list bears no significance. We\n            # therefore plan to delete all edges (new and old), and add new\n            # ones.\n\n            # First add new edges.\n            # All local edges in terms of global point indices\n            edges_new_glob = p_ind_exp[edges_new[:2]]\n            edges = np.hstack((edges, edges_new_glob))\n\n            # Global indices of the local edges\n            edges_loc_ind = np.unique(edges_loc_ind)\n\n            # Append fields for edge-fracture map and boundary tags\n            for ei in range(edges_new.shape[1]):\n                # Find the global edge index. For most edges, this will be\n                # correctly identified by edges_new[2], which tracks the\n                # original edges under splitting. However, in cases of\n                # overlapping segments, in which case the index of the one edge\n                # may completely override the index of the other (this is\n                # caused by the implementation of remove_edge_crossings).\n                # We therefore compare the new edge to the old ones (before\n                # splitting). If found, use the old information; if not, use\n                # index as tracked by splitting.\n                is_old, old_loc_ind =\\\n                    setmembership.ismember_rows(edges_new_glob[:, ei]\n                                                .reshape((-1, 1)),\n                                                edges[:2, edges_loc_ind])\n                if is_old[0]:\n                    glob_ei = edges_loc_ind[old_loc_ind[0]]\n                else:\n                    glob_ei = edges_new[2, ei]\n                # Update edge_2_frac and boundary information.\n                edges_2_frac.append(edges_2_frac[glob_ei])\n                is_boundary_edge.append(is_boundary_edge[glob_ei])\n\n            # Finally, purge the old edges\n            edges = np.delete(edges, edges_loc_ind, axis=1)\n\n            # We cannot delete more than one list element at a time. Delete by\n            # index in decreasing order, so that we do not disturb the index\n            # map.\n            edges_loc_ind.sort()\n            for ei in edges_loc_ind[::-1]:\n                del edges_2_frac[ei]\n                del is_boundary_edge[ei]\n            # And we are done with this fracture. On to the next one.\n\n\n        logger.info('Done with intersection removal. Elapsed time %.5f',\n                    time.time() - start_time)\n        self._verify_fractures_in_plane(all_p, edges, edges_2_frac)\n\n        return self._uniquify_points_and_edges(all_p, edges, edges_2_frac,\n                                               is_boundary_edge)\n\n        # To\n        # define the auxiliary edges, we create a triangulation of the\n        # fractures. We then grow polygons forming part of the fracture in a\n        # way that ensures that no polygon lies on both sides of an\n        # intersection edge.\n\n\n    def report_on_decomposition(self, do_print=True, verbose=None):\n        \"\"\"\n        Compute various statistics on the decomposition.\n\n        The coverage is rudimentary for now, will be expanded when needed.\n\n        Parameters:\n            do_print (boolean, optional): Print information. Defaults to True.\n            verbose (int, optional): Override the verbosity level of the\n                network itself. If not provided, the network value will be\n                used.\n\n        Returns:\n            str: String representation of the statistics\n\n        \"\"\"\n        if verbose is None:\n            verbose = self.verbose\n        d = self.decomposition\n\n        s = str(len(self._fractures)) + ' fractures are split into '\n        s += str(len(d['polygons'])) + ' polygons \\n'\n\n        s += 'Number of points: ' + str(d['points'].shape[1]) + '\\n'\n        s += 'Number of edges: ' + str(d['edges'].shape[1]) + '\\n'\n\n        if verbose > 1:\n            # Compute minimum distance between points in point set\n            dist = np.inf\n            p = d['points']\n            num_points = p.shape[1]\n            hit = np.ones(num_points, dtype=np.bool)\n            for i in range(num_points):\n                hit[i] = False\n                dist_loc = cg.dist_point_pointset(p[:, i], p[:, hit])\n                dist = np.minimum(dist, dist_loc.min())\n                hit[i] = True\n            s += 'Minimal disance between points ' + str(dist) + '\\n'\n\n        if do_print:\n            print(s)\n\n        return s\n\n    def fractures_of_points(self, pts):\n        \"\"\"\n        For a given point, find all fractures that refer to it, either as\n        vertex or as internal.\n\n        Returns:\n            list of np.int: indices of fractures, one list item per point.\n        \"\"\"\n        fracs_of_points = []\n        pts = np.atleast_1d(np.asarray(pts))\n        for i in pts:\n            fracs_loc = []\n\n            # First identify edges that refers to the point\n            edge_ind = np.argwhere(np.any(self.decomposition['edges'][:2]\\\n                                          == i, axis=0)).ravel('F')\n            edges_loc = self.decomposition['edges'][:, edge_ind]\n            # Loop over all polygons. If their edges are found in edges_loc,\n            # store the corresponding fracture index\n            for poly_ind, poly in enumerate(self.decomposition['polygons']):\n                ismem, _ = setmembership.ismember_rows(edges_loc, poly)\n                if any(ismem):\n                    fracs_loc.append(self.decomposition['polygon_frac']\\\n                                     [poly_ind])\n            fracs_of_points.append(list(np.unique(fracs_loc)))\n        return fracs_of_points\n\n    def close_points(self, dist):\n        \"\"\"\n        In the set of points used to describe the fractures (after\n        decomposition), find pairs that are closer than a certain distance.\n\n        Parameters:\n            dist (double): Threshold distance, all points closer than this will\n                be reported.\n\n        Returns:\n            List of tuples: Each tuple contain indices of a set of close\n                points, and the distance between the points. The list is not\n                symmetric; if (a, b) is a member, (b, a) will not be.\n\n        \"\"\"\n        c_points = []\n\n        pt = self.decomposition['points']\n        for pi in range(pt.shape[1]):\n            d = cg.dist_point_pointset(pt[:, pi], pt[:, pi+1:])\n            ind = np.argwhere(d < dist).ravel('F')\n            for i in ind:\n                # Indices of close points, with an offset to compensate for\n                # slicing of the point cloud.\n                c_points.append((pi, i + pi + 1, d[i]))\n\n        return c_points\n\n    def _verify_fractures_in_plane(self, p, edges, edges_2_frac):\n        \"\"\"\n        Essentially a debugging method that verify that the given set of\n        points, edges and edge connections indeed form planes.\n\n        This has turned out to be a common symptom of trouble.\n\n        \"\"\"\n        for fi, _ in enumerate(self._fractures):\n\n            # Identify the edges associated with this fracture\n            edges_loc_ind = []\n            for ei, e in enumerate(edges_2_frac):\n                if np.any(e == fi):\n                    edges_loc_ind.append(ei)\n\n            edges_loc = edges[:, edges_loc_ind]\n            p_ind_loc = np.unique(edges_loc)\n            p_loc = p[:, p_ind_loc]\n\n            # Run through points_2_plane, to check the assertions\n            self._points_2_plane(p_loc, edges_loc, p_ind_loc)\n\n\n    def _points_2_plane(self, p_loc, edges_loc, p_ind_loc):\n        \"\"\"\n        Convenience method for rotating a point cloud into its own 2d-plane.\n        \"\"\"\n\n        # Center point cloud around the origin\n        p_loc_c = np.mean(p_loc, axis=1).reshape((-1, 1))\n        p_loc -= p_loc_c\n\n        # Project the points onto the local plane defined by the fracture\n        rot = cg.project_plane_matrix(p_loc, tol=self.tol)\n        p_2d = rot.dot(p_loc)\n\n        extent = p_2d.max(axis=1) - p_2d.min(axis=1)\n        lateral_extent = np.maximum(np.max(extent[2]), 1)\n        assert extent[2] < lateral_extent * self.tol * 30\n\n        # Dump third coordinate\n        p_2d = p_2d[:2]\n\n        # The edges must also be redefined to account for the (implicit)\n        # local numbering of points\n        edges_2d = np.empty_like(edges_loc)\n        for ei in range(edges_loc.shape[1]):\n            edges_2d[0, ei] = np.argwhere(p_ind_loc == edges_loc[0, ei])\n            edges_2d[1, ei] = np.argwhere(p_ind_loc == edges_loc[1, ei])\n\n        assert edges_2d[:2].max() < p_loc.shape[1]\n\n        return p_2d, edges_2d, p_loc_c, rot\n\n    def change_tolerance(self, new_tol):\n        \"\"\"\n        Redo the whole configuration based on the new tolerance\n        \"\"\"\n        pass\n\n    def __repr__(self):\n        s = 'Fracture set with ' + str(len(self._fractures)) + ' fractures'\n        return s\n\n    def plot_fractures(self, ind=None):\n        if ind is None:\n            ind = np.arange(len(self._fractures))\n        fig = plt.figure()\n        ax = fig.gca(projection='3d')\n        for f in self._fractures:\n            f.plot_frame(ax)\n        return fig\n\n        for i, f_1 in enumerate(self._fractures):\n            for j, f_2 in enumerate(self._fractures[i + 1:]):\n                d = np.Inf\n                for p_1 in f_1.points():\n                    for p_2 in f_2.points():\n                        d = np.minimum(d, cg.dist_point_pointset(p_1, p_2)[0])\n                p_2_p[i, i + j + 1] = d\n\n                d = np.Inf\n                for s_1 in f_1.segments():\n                    for s_2 in f_2.segments():\n                        d = np.minimum(d,\n                                       cg.distance_segment_segment(s_1[:, 0],\n                                                                   s_1[:, 1],\n                                                                   s_2[:, 0],\n                                                                   s_2[:, 1]))\n                s_2_s[i, i + j + 1] = d\n\n        return p_2_p, s_2_s\n\n    def add_subdomain_boundaries(self, vertexes):\n        \"\"\"\n        Adds subdomain boundaries to the fracture network. These are\n        intended to ensure the partitioning of the matrix along the planes\n        they define.\n        Parameters:\n            vertexes: either a Fracture or a\n            np.array([[x0, x1, x2, x3],\n                      [y0, y1, y2, y3],\n                      [z0, z1, z2, z3]])\n        \"\"\"\n        subdomain_tags = self.tags.get('subdomain',\n                                       [False]*len(self._fractures))\n        for f in vertexes:\n            if not hasattr(f, 'p') or not isinstance(f.p, np.ndarray):\n                # np.array to Fracture:\n                f = Fracture(f)\n            # Add the fake fracture and its tag to the network:\n            self.add(f)\n            subdomain_tags.append(True)\n\n        self.tags['subdomain'] = subdomain_tags\n\n    def impose_external_boundary(self, box=None, truncate_fractures=True,\n                                 keep_box=True):\n        \"\"\"\n        Set an external boundary for the fracture set.\n\n        The boundary takes the form of a 3D box, described by its minimum and\n        maximum coordinates. If no bounding box is provided, a box will be\n        fited outside the fracture network.\n\n        If desired, the fratures will be truncated to lay within the bounding\n        box; that is, Fracture.p will be modified. The orginal coordinates of\n        the fracture boundary can still be recovered from the attribute\n        Fracture.orig_points.\n\n        Fractures that are completely outside the bounding box will be deleted\n        from the fracture set.\n\n        Parameters:\n            box (dictionary): Has fields 'xmin', 'xmax', and similar for y and\n                z.\n            truncate_fractures (boolean, optional): If True, fractures outside\n            the bounding box will be disregarded, while fractures crossing the\n            boundary will be truncated.\n\n        \"\"\"\n        if box is None:\n            OVERLAP = 0.15\n            cmin = np.ones((3, 1)) * float('inf')\n            cmax = -np.ones((3, 1)) * float('inf')\n            for f in self._fractures:\n                cmin = np.min(np.hstack((cmin, f.p)), axis=1).reshape((-1, 1))\n                cmax = np.max(np.hstack((cmax, f.p)), axis=1).reshape((-1, 1))\n            cmin = cmin[:, 0]\n            cmax = cmax[:, 0]\n            dx = OVERLAP * (cmax - cmin)\n            box = {'xmin': cmin[0] - dx[0], 'xmax': cmax[0] + dx[0],\n                   'ymin': cmin[1] - dx[1], 'ymax': cmax[1] + dx[1],\n                   'zmin': cmin[2] - dx[2], 'zmax': cmax[2] + dx[2]}\n\n        # Insert boundary in the form of a box, and kick out (parts of)\n        # fractures outside the box\n        self.domain = box\n\n        #Create fractures of box here.\n        #Store them self._fractures so that split_intersections work\n        #keep track of which fractures are really boundaries - perhaps attribute is_proxy?\n\n        if truncate_fractures:\n            # Keep track of fractures that are completely outside the domain.\n            # These will be deleted.\n            delete_frac = []\n\n            # Loop over all fractures, use method in fractures to truncate if\n            # necessary.\n            for i, frac in enumerate(self._fractures):\n                frac.impose_boundary(box, self.tol)\n                if frac.p.shape[1] == 0:\n                    delete_frac.append(i)\n\n            # Delete fractures that have all points outside the bounding box\n            # There may be some uncovered cases here, with a fracture barely\n            # touching the box from the outside, but we leave that for now.\n            for i in np.unique(delete_frac)[::-1]:\n                del self._fractures[i]\n\n            # Final sanity check: All fractures should have at least three\n            # points at the end of the manipulations\n            for f in self._fractures:\n                assert f.p.shape[1] >= 3\n\n\n        self._make_bounding_planes(box, keep_box)\n\n    def _make_bounding_planes(self, box, keep_box=True):\n        \"\"\"\n        Translate the bounding box into fractures. Tag them as boundaries.\n        For now limited to a box consisting of six planes.\n        \"\"\"\n        x0 = box['xmin']\n        x1 = box['xmax']\n        y0 = box['ymin']\n        y1 = box['ymax']\n        z0 = box['zmin']\n        z1 = box['zmax']\n        west = Fracture(np.array([[x0, x0, x0, x0],\n                                  [y0, y1, y1, y0],\n                                  [z0, z0, z1, z1]]),\n                        check_convexity=False)\n        east = Fracture(np.array([[x1, x1, x1, x1],\n                                  [y0, y1, y1, y0],\n                                  [z0, z0, z1, z1]]),\n                        check_convexity=False)\n        south = Fracture(np.array([[x0, x1, x1, x0],\n                                   [y0, y0, y0, y0],\n                                   [z0, z0, z1, z1]]),\n                         check_convexity=False)\n        north = Fracture(np.array([[x0, x1, x1, x0],\n                                   [y1, y1, y1, y1],\n                                   [z0, z0, z1, z1]]),\n                         check_convexity=False)\n        bottom = Fracture(np.array([[x0, x1, x1, x0],\n                                    [y0, y0, y1, y1],\n                                    [z0, z0, z0, z0]]),\n                          check_convexity=False)\n        top = Fracture(np.array([[x0, x1, x1, x0],\n                                 [y0, y0, y1, y1],\n                                 [z1, z1, z1, z1]]),\n                       check_convexity=False)\n        # Collect in a list to allow iteration\n        bound_planes = [west, east, south, north, bottom, top]\n        boundary_tags = self.tags.get('boundary',\n                                       [False]*len(self._fractures))\n\n        # Add the boundaries to the fracture network and tag them.\n        if keep_box:\n            for f in bound_planes:\n                self.add(f)\n                boundary_tags.append(True)\n        self.tags['boundary'] = boundary_tags\n\n\n    def _classify_edges(self, polygon_edges):\n        \"\"\"\n        Classify the edges into fracture boundary, intersection, or auxiliary.\n        Also identify points on intersections between interesctions (fractures\n        of co-dimension 3)\n\n        Parameters:\n            polygon_edges (list of lists): For each polygon the global edge\n                indices that forms the polygon boundary.\n\n        Returns:\n            tag: Tag of the fracture, using the values in GmshConstants. Note\n                that auxiliary points will not be tagged (these are also\n                ignored in gmsh_interface.GmshWriter).\n            is_0d_grid: boolean, one for each point. True if the point is\n                shared by two or more intersection lines.\n\n        \"\"\"\n        edges = self.decomposition['edges']\n        is_bound = self.decomposition['is_bound']\n        num_edges = edges.shape[1]\n\n        poly_2_frac = np.arange(len(self.decomposition['polygons']))\n        self.decomposition['polygon_frac'] = poly_2_frac\n\n        # Construct a map from edges to polygons\n        edge_2_poly = [[] for i in range(num_edges)]\n        for pi, poly in enumerate(polygon_edges[0]):\n            for ei in np.unique(poly):\n                edge_2_poly[ei].append(poly_2_frac[pi])\n\n        # Count the number of referals to the edge from polygons belonging to\n        # different fractures (not polygons)\n        num_referals = np.zeros(num_edges)\n        for ei, ep in enumerate(edge_2_poly):\n            num_referals[ei] = np.unique(np.array(ep)).size\n\n        # A 1-d grid is inserted where there is more than one fracture\n        # referring.\n        has_1d_grid = np.where(num_referals > 1)[0]\n\n        num_constraints = len(is_bound)\n        constants = GmshConstants()\n        tag = np.zeros(num_edges, dtype='int')\n\n        # Find fractures that are tagged as a boundary\n        all_bound = [np.all(is_bound[i]) for i in range(len(is_bound))]\n        bound_ind = np.where(all_bound)[0]\n        # Remove those that are referred to by more than fracture - this takes\n        # care of L-type intersections\n        bound_ind = np.setdiff1d(bound_ind, has_1d_grid)\n\n        # Index of lines that should have a 1-d grid. This are all of the first\n        # num-constraints, minus those on the boundary.\n        # Note that edges with index > num_constraints are known to be of the\n        # auxiliary type. These will have tag zero; and treated in a special\n        # manner by the interface to gmsh.\n        intersection_ind = np.setdiff1d(np.arange(num_constraints), bound_ind)\n        tag[bound_ind] = constants.FRACTURE_TIP_TAG\n        tag[intersection_ind] = constants.FRACTURE_INTERSECTION_LINE_TAG\n\n        # Count the number of times a point is referred to by an intersection\n        # between two fractures. If this is more than one, the point should\n        # have a 0-d grid assigned to it.\n        isect_p = edges[:, intersection_ind].ravel()\n        num_occ_pt = np.bincount(isect_p)\n        is_0d_grid = np.where(num_occ_pt > 1)[0]\n\n        return tag, is_0d_grid\n\n    def _poly_2_segment(self):\n        \"\"\"\n        Represent the polygons by the global edges, and determine if the lines\n        must be reversed (locally) for the polygon to form a closed loop.\n\n        \"\"\"\n        edges = self.decomposition['edges']\n        poly = self.decomposition['polygons']\n\n        poly_2_line = []\n        line_reverse = []\n        for p in poly:\n            hit, ind = setmembership.ismember_rows(p, edges[:2], sort=False)\n            hit_reverse, ind_reverse = setmembership.ismember_rows(\n                p[::-1], edges[:2], sort=False)\n            assert np.all(hit + hit_reverse == 1)\n\n            line_ind = np.zeros(p.shape[1])\n\n            hit_ind = np.where(hit)[0]\n            hit_reverse_ind = np.where(hit_reverse)[0]\n            line_ind[hit_ind] = ind\n            line_ind[hit_reverse_ind] = ind_reverse\n\n            poly_2_line.append(line_ind.astype('int'))\n            line_reverse.append(hit_reverse)\n\n        return poly_2_line, line_reverse\n\n    def _determine_mesh_size(self, **kwargs):\n        \"\"\"\n        Set the preferred mesh size for geometrical points as specified by\n        gmsh.\n\n        Currently, the only option supported is to specify a single value for\n        all fracture points, and one value for the boundary.\n\n        See the gmsh manual for further details.\n\n        \"\"\"\n        mode = kwargs.get('mode', 'distance')\n\n        num_pts = self.decomposition['points'].shape[1]\n\n        if mode == 'constant':\n            val = kwargs.get('value', None)\n            bound_val = kwargs.get('bound_value', None)\n            if val is not None:\n                mesh_size = val * np.ones(num_pts)\n            else:\n                mesh_size = None\n            if bound_val is not None:\n                mesh_size_bound = bound_val\n            else:\n                mesh_size_bound = None\n            return mesh_size, mesh_size_bound\n        elif mode == 'distance' or mode == 'weighted':\n            if self.h_min is None or self.h_ideal is None:\n                print('Found no information on mesh sizes. Returning')\n                return None, None\n\n            p = self.decomposition['points']\n            num_pts = p.shape[1]\n            dist = cg.dist_pointset(p, max_diag=True)\n            mesh_size = np.min(dist, axis=1)\n            print('Minimal distance between points encountered is ' + str(np.min(dist)))\n            mesh_size = np.maximum(mesh_size, self.h_min * np.ones(num_pts))\n            mesh_size = np.minimum(mesh_size, self.h_ideal * np.ones(num_pts))\n\n            mesh_size_bound = self.h_ideal\n            return mesh_size, mesh_size_bound\n        else:\n            raise ValueError('Unknown mesh size mode ' + mode)\n\n    def insert_auxiliary_points(self, h_ideal=None, h_min=None):\n        \"\"\" Insert auxiliary points on fracture edges. Used to guide gmsh mesh\n        size parameters.\n\n        The function should only be called once to avoid insertion of multiple\n        layers of extra points, this will likely kill gmsh.\n\n        The function is motivated by similar functionality for 2d domains, but\n        is considerably less mature.\n\n        The function should be used in conjunction with _determine_mesh_size(),\n        called with mode='distance'. The ultimate goal is to set the mesh size\n        for geometrical points in Gmsh. To that end, this function inserts\n        additional points on the fracture boundaries. The mesh size is then\n        determined as the distance between all points in the fracture\n        description.\n\n        Parameters:\n            h_ideal: Ideal mesh size. Will be added to all points that are\n                sufficiently far away from other points.\n            h_min: Minimal mesh size; we will make no attempts to enforce\n                even smaller mesh sizes upon Gmsh.\n\n        \"\"\"\n\n        if self.auxiliary_points_added:\n            print('Auxiliary points already added. Returning.')\n        else:\n            self.auxiliary_points_added = True\n\n        self.h_ideal = h_ideal\n        self.h_min = h_min\n\n        def dist_p(a, b):\n            a = a.reshape((-1, 1))\n            b = b.reshape((-1, 1))\n            d = b - a\n            return np.sqrt(np.sum(d**2))\n\n        intersecting_fracs = []\n        # Loop over all fractures\n        for f in self._fractures:\n\n            # First compare segments with intersections to this fracture\n            nfp = f.p.shape[1]\n\n            # Keep track of which other fractures are intersecting - will be\n            # needed later on\n            isect_f = []\n            for i in self.intersections_of_fracture(f):\n                if f is i.first:\n                    isect_f.append(i.second.index)\n                else:\n                    isect_f.append(i.first.index)\n\n                # Assuming the fracture is convex, the closest point for\n                dist, cp = cg.dist_points_segments(i.coord, f.p,\n                                                   np.roll(f.p, 1, axis=1))\n                # Insert a (candidate) point only at the segment closest to the\n                # intersection point. If the intersection line runs parallel\n                # with a segment, this may be insufficient, but we will deal\n                # with this if necessary.\n                closest_segment = np.argmin(dist, axis=1)\n                min_dist = dist[np.arange(i.coord.shape[1]), closest_segment]\n\n                for pi, (si, di) in enumerate(zip(closest_segment, min_dist)):\n                    if di < h_ideal:\n                        d_1 = dist_p(cp[pi, si], f.p[:, si])\n                        d_2 = dist_p(cp[pi, si], f.p[:, (si+1)%nfp])\n                        # If the intersection point is not very close to any of\n                        # the points on the segment, we split the segment.\n                        if d_1 > h_min and d_2 > h_min:\n                            np.insert(f.p, (si+1)%nfp, cp[pi, si], axis=1)\n\n            # Take note of the intersecting fractures\n            intersecting_fracs.append(isect_f)\n\n        for fi, f in enumerate(self._fractures):\n            nfp = f.p.shape[1]\n            for of in self._fractures:\n                # Can do some box arguments here to avoid computations\n\n                # First, check if we are intersecting, this is covered already\n                if of.index in intersecting_fracs[fi]:\n                    continue\n\n                f_start = f.p\n                f_end = np.roll(f_start, 1, axis=1)\n                # Then, compare distance between segments\n                # Loop over fracture segments of this fracture\n                for si, fs in enumerate(f.segments()):\n                    of_start = of.p\n                    of_end = np.roll(of_start, 1, axis=1)\n                    # Compute distance to all fractures of the other fracture,\n                    # and find the closest segment on the other fracture\n                    fs = f_start[:, si].squeeze()#.reshape((-1, 1))\n                    fe = f_end[:, si].squeeze()#.reshape((-1, 1))\n                    d, cp_f, _ = cg.dist_segment_segment_set(fs, fe, of_start,\n                                                             of_end)\n                    mi = np.argmin(d)\n                    # If the distance is smaller than ideal length, but the\n                    # closets point is not too close to the segment endpoints,\n                    # we add a new point\n                    if d[mi] < h_ideal:\n                        d_1 = dist_p(cp_f[:, mi], f.p[:, si])\n                        d_2 = dist_p(cp_f[:, mi], f.p[:, (si+1)%nfp])\n                        if d_1 > h_min and d_2 > h_min:\n                            np.insert(f.p, (si+1)%nfp, cp_f[:, mi], axis=1)\n\n\n    def to_vtk(self, file_name, data=None, binary=True):\n        \"\"\"\n        Export the fracture network to vtk.\n\n        The fractures are treated as polygonal cells, with no special treatment\n        of intersections.\n\n        Fracture numbers are always exported (1-offset). In addition, it is\n        possible to export additional data, as specified by the\n        keyword-argument data.\n\n        Parameters:\n            file_name (str): Name of the target file.\n            data (dictionary, optional): Data associated with the fractures.\n                The values in the dictionary should be numpy arrays. 1d and 3d\n                data is supported. Fracture numbers are always exported.\n            binary (boolean, optional): Use binary export format. Defaults to\n                True.\n\n        \"\"\"\n        network_vtk = vtk.vtkUnstructuredGrid()\n\n        point_counter = 0\n        pts_vtk = vtk.vtkPoints()\n        for f in self._fractures:\n            # Add local points\n            [pts_vtk.InsertNextPoint(*p) for p in f.p.T]\n\n            # Indices of local points\n            loc_pt_id = point_counter + np.arange(f.p.shape[1],\n                                                  dtype='int')\n            # Update offset\n            point_counter += f.p.shape[1]\n\n            # Add bounding polygon\n            frac_vtk = vtk.vtkIdList()\n            [frac_vtk.InsertNextId(p) for p in loc_pt_id]\n            # Close polygon\n            frac_vtk.InsertNextId(loc_pt_id[0])\n\n            network_vtk.InsertNextCell(vtk.VTK_POLYGON, frac_vtk)\n\n        # Add the points\n        network_vtk.SetPoints(pts_vtk)\n\n        writer = vtk.vtkXMLUnstructuredGridWriter()\n        writer.SetInputData(network_vtk)\n        writer.SetFileName(file_name)\n\n        if not binary:\n            writer.SetDataModeToAscii()\n\n        # Cell-data to be exported is at least the fracture numbers\n        if data is None:\n            data = {}\n        # Use offset 1 for fracture numbers (should we rather do 0?)\n        data['Fracture_Number'] = 1 + np.arange(len(self._fractures))\n\n        for name, data in data.items():\n            data_vtk = vtk_np.numpy_to_vtk(data.ravel(order='F'), deep=True,\n                                           array_type=vtk.VTK_DOUBLE)\n            data_vtk.SetName(name)\n            data_vtk.SetNumberOfComponents(1 if data.ndim == 1 else 3)\n            network_vtk.GetCellData().AddArray(data_vtk)\n\n        writer.Update()\n\n    def to_gmsh(self, file_name, in_3d=True, **kwargs):\n        \"\"\" Write the fracture network as input for mesh generation by gmsh.\n\n        It is assumed that intersections have been found and processed (e.g. by\n        the methods find_intersection() and split_intersection()).\n\n        Parameters:\n            file_name (str): Path to the .geo file to be written\n            in_3d (boolean, optional): Whether to embed the 2d fracture grids\n               in 3d. If True (default), the mesh will be DFM-style, False will\n               give a DFN-type mesh.\n\n        \"\"\"\n        # Extract geometrical information.\n        p = self.decomposition['points']\n        edges = self.decomposition['edges']\n        poly = self._poly_2_segment()\n        # Obtain tags, and set default values (corresponding to real fractures)\n        # for untagged fractures.\n        frac_tags = self.tags\n        frac_tags['subdomain'] = frac_tags.get('subdomain', []) + \\\n                                [False]*(len(self._fractures)\n                                        -len(frac_tags.get('subdomain', [])))\n        frac_tags['boundary'] = frac_tags.get('boundary', []) + \\\n                                [False]*(len(self._fractures)\n                                        -len(frac_tags.get('boundary', [])))\n\n        edge_tags, intersection_points = self._classify_edges(poly)\n\n        # All intersection lines and points on boundaries are non-physical in 3d.\n        # I.e., they are assigned boundary conditions, but are not gridded. Hence:\n        # Remove the points and edges at the boundary\n        point_tags, edge_tags = self.on_domain_boundary(edges, edge_tags)\n        edges = np.vstack((self.decomposition['edges'], edge_tags))\n        int_pts_on_boundary = np.isin(intersection_points, np.where(point_tags))\n        intersection_points = intersection_points[np.logical_not(int_pts_on_boundary)]\n        self.zero_d_pt = intersection_points\n\n        # Obtain mesh size parameters\n        if 'mesh_size' in kwargs.keys():\n            # Legacy option, this should be removed.\n            print('Using old version of mesh size determination')\n            mesh_size, mesh_size_bound = \\\n                self._determine_mesh_size(**kwargs['mesh_size'])\n        else:\n            mesh_size, mesh_size_bound = self._determine_mesh_size()\n\n        # The tolerance applied in gmsh should be consistent with the tolerance\n        # used in the splitting of the fracture network. The documentation of\n        # gmsh is not clear, but it seems gmsh scales a given tolerance with\n        # the size of the domain - presumably by the largest dimension. To\n        # counteract this, we divide our (absolute) tolerance self.tol with the\n        # domain size.\n        if in_3d:\n            dx = np.array([[self.domain['xmax'] - self.domain['xmin']],\n                           [self.domain['ymax'] - self.domain['ymin']],\n                           [self.domain['zmax'] - self.domain['zmin']]])\n            gmsh_tolerance = self.tol / dx.max()\n        else:\n            gmsh_tolerance = self.tol\n\n        if 'mesh_size' in kwargs.keys():\n            meshing_algorithm = kwargs['mesh_size'].get('meshing_algorithm', None)\n        else:\n            meshing_algorithm = None\n\n        # Initialize and run the gmsh writer:\n        if in_3d:\n            dom = self.domain\n        else:\n            dom = None\n        writer = GmshWriter(p, edges, polygons=poly, domain=dom,\n                            intersection_points=intersection_points,\n                            mesh_size_bound=mesh_size_bound,\n                            mesh_size=mesh_size, tolerance=gmsh_tolerance,\n                            edges_2_frac=self.decomposition['line_in_frac'],\n                            meshing_algorithm=meshing_algorithm,\n                            fracture_tags=frac_tags)\n\n        writer.write_geo(file_name)\n\n    def fracture_to_plane(self, frac_num):\n        \"\"\" Project fracture vertexes and intersection points to the natural\n        plane of the fracture.\n\n        Parameters:\n            frac_num (int): Index of fracture.\n\n        Returns:\n            np.ndarray (2xn_pt): 2d coordinates of the fracture vertexes.\n            np.ndarray (2xn_isect): 2d coordinates of fracture intersection\n                points.\n            np.ndarray: Index of intersecting fractures.\n            np.ndarray, 3x3: Rotation matrix into the natural plane.\n            np.ndarray, 3x1. 3d coordinates of the fracture center.\n\n            The 3d coordinates of the frature can be recovered by\n                p_3d = cp + rot.T.dot(np.vstack((p_2d,\n                                                 np.zeros(p_2d.shape[1]))))\n\n        \"\"\"\n        isect = self.intersections_of_fracture(frac_num)\n\n        frac = self._fractures[frac_num]\n        cp = frac.center.reshape((-1, 1))\n\n        rot = cg.project_plane_matrix(frac.p)\n\n        def rot_translate(pts):\n            # Convenience method to translate and rotate a point.\n            return rot.dot(pts - cp)\n\n        p = rot_translate(frac.p)\n        assert np.max(np.abs(p[2])) < self.tol, str(np.max(np.abs(p[2]))) + \\\n                \" \" + str(self.tol)\n        p_2d = p[:2]\n\n        # Intersection points, in 2d coordinates\n        ip = np.empty((2, 0))\n\n        other_frac = np.empty(0, dtype=np.int)\n\n        for i in isect:\n            if i.first.index == frac_num:\n                other_frac = np.append(other_frac, i.second.index)\n            else:\n                other_frac = np.append(other_frac, i.first.index)\n\n            tmp_p = rot_translate(i.coord)\n            if tmp_p.shape[1] > 0:\n                assert np.max(np.abs(tmp_p[2])) < self.tol\n                ip = np.append(ip, tmp_p[:2], axis=1)\n\n        return p_2d, ip, other_frac, rot, cp\n\n    def on_domain_boundary(self, edges, edge_tags):\n        \"\"\"\n        Finds edges and points on boundary, to avoid that these\n        are gridded. Points  introduced by intersections\n        of subdomain boundaries and real fractures remain physical\n        (to maintain contact between split fracture lines).\n        \"\"\"\n\n        constants = GmshConstants()\n        # Obtain current tags on fractures\n        boundary_polygons = np.where(self.tags.get('boundary',\n                                [False]*len(self._fractures)))[0]\n        subdomain_polygons = np.where(self.tags.get('subdomain'\n                                [False]*len(self._fractures)))[0]\n        # ... on the points...\n        point_tags = np.zeros(self.decomposition['points'].shape[1])\n        # and the mapping between fractures and edges.\n        edges_2_frac = self.decomposition['edges_2_frac']\n\n        # Loop on edges to tag according to following rules:\n        #     Edge is on the boundary:\n        #         Tag it and the points it consists of as boundary entities.\n        #     Edge is caused by the presence of subdomain boundaries\n        #     (Note: one fracture may still be involved):\n        #         Tag it as auxiliary\n        for e, e2f in enumerate(edges_2_frac):\n            if any(np.in1d(e2f, boundary_polygons)):\n                edge_tags[e] = constants.DOMAIN_BOUNDARY_TAG\n                point_tags[edges[:,e]] = constants.DOMAIN_BOUNDARY_TAG\n                continue\n            subdomain_parents = np.in1d(e2f, subdomain_polygons)\n            if any(subdomain_parents) and e2f.size - sum(subdomain_parents) < 2:\n                # Intersection of at most one fracture with one (or more)\n                # subdomain boundaries.\n                edge_tags[e] = constants.AUXILIARY_TAG\n\n        # Tag points caused solely by subdomain plane intersection as\n        # auxiliary. Note that points involving at least one real fracture\n        # are kept as physical fracture points. This is to avoid connectivity\n        # loss along real fracture intersections.\n        for p in np.unique(edges):\n            es = np.where(edges==p)[1]\n            sds = []\n            for e in es:\n                sds.append(edges_2_frac[e])\n            sds = np.unique(np.concatenate(sds))\n            subdomain_parents = np.in1d(sds, subdomain_polygons)\n            if all(subdomain_parents) and sum(subdomain_parents) > 2:\n                point_tags[p] = constants.AUXILIARY_TAG\n\n        return point_tags, edge_tags",
  "def __init__(self, points, index=None, check_convexity=True):\n        self.p = np.asarray(points, dtype=np.float)\n        # Ensure the points are ccw\n        self.points_2_ccw()\n        self.compute_centroid()\n        self.compute_normal()\n\n        self.orig_p = self.p.copy()\n\n        self.index = index\n\n        assert self.is_planar(), 'Points define non-planar fracture'\n        if check_convexity:\n            assert self.check_convexity(), 'Points form non-convex polygon'",
  "def set_index(self, i):\n        self.index = i",
  "def __eq__(self, other):\n        return self.index == other.index",
  "def copy(self):\n        \"\"\" Return a deep copy of the fracture.\n\n        \"\"\"\n        p = np.copy(self.p)\n        return Fracture(p)",
  "def points(self):\n        \"\"\"\n        Iterator over the vexrtexes of the bounding polygon\n\n        Yields:\n            np.array (3 x 1): polygon vertexes\n\n        \"\"\"\n        for i in range(self.p.shape[1]):\n            yield self.p[:, i].reshape((-1, 1))",
  "def segments(self):\n        \"\"\"\n        Iterator over the segments of the bounding polygon.\n\n        Yields:\n            np.array (3 x 2): polygon segment\n        \"\"\"\n\n        sz = self.p.shape[1]\n        for i in range(sz):\n            yield self.p[:, np.array([i, i + 1]) % sz]",
  "def is_vertex(self, p, tol=1e-4):\n        \"\"\" Check whether a given point is a vertex of the fracture.\n\n        Parameters:\n            p (np.array): Point to check\n            tol (double): Tolerance of point accuracy.\n\n        Returns:\n            True: if the point is in the vertex set, false if not.\n            int: Index of the identical vertex. None if not a vertex.\n        \"\"\"\n        p = p.reshape((-1, 1))\n        ap = np.hstack((p, self.p))\n        up, _, ind = setmembership.unique_columns_tol(ap, tol=tol*np.sqrt(3))\n        if up.shape[1] == ap.shape[1]:\n            return False, None\n        else:\n            occurences = np.where(ind == ind[0])[0]\n            return True, (occurences[1] - 1)",
  "def points_2_ccw(self):\n        \"\"\"\n        Ensure that the points are sorted in a counter-clockwise order.\n\n        mplementation note:\n            For now, the ordering of nodes in based on a simple angle argument.\n            This will not be robust for general point clouds, but we expect the\n            fractures to be regularly shaped in this sense. In particular, we\n            will be safe if the cell is convex.\n\n        Returns:\n            np.array (int): The indices corresponding to the sorting.\n\n        \"\"\"\n        # First rotate coordinates to the plane\n        points_2d = self.plane_coordinates()\n        # Center around the 2d origin\n        points_2d -= np.mean(points_2d, axis=1).reshape((-1, 1))\n\n        theta = np.arctan2(points_2d[1], points_2d[0])\n        sort_ind = np.argsort(theta)\n\n        self.p = self.p[:, sort_ind]\n\n        return sort_ind",
  "def add_points(self, p, check_convexity=True, tol=1e-4, enforce_pt_tol=None):\n        \"\"\"\n        Add a point to the polygon with ccw sorting enforced.\n\n        Always run a test to check that the points are still planar. By\n        default, a check of convexity is also performed, however, this can be\n        turned off to speed up simulations (the test uses sympy, which turns\n        out to be slow in many cases).\n\n        Parameters:\n            p (np.ndarray, 3xn): Points to add\n            check_convexity (boolean, optional): Verify that the polygon is\n                convex. Defaults to true.\n            tol (double): Tolerance used to check if the point already exists.\n\n        Return:\n            boolean, true if the resulting polygon is convex.\n\n        \"\"\"\n\n        to_enforce = np.hstack((np.zeros(self.p.shape[1], dtype=np.bool),\n                                np.ones(p.shape[1], dtype=np.bool)))\n        self.p = np.hstack((self.p, p))\n        self.p, _, _ = setmembership.unique_columns_tol(self.p, tol=tol)\n\n        # Sort points to ccw\n        mask = self.points_2_ccw()\n\n        if enforce_pt_tol is not None:\n            to_enforce = np.where(to_enforce[mask])[0]\n            dist = cg.dist_pointset(self.p)\n            dist /= np.amax(dist)\n            np.fill_diagonal(dist, np.inf)\n            mask = np.where(dist < enforce_pt_tol)[0]\n            mask = np.setdiff1d(mask, to_enforce, assume_unique=True)\n            self.remove_points(mask)\n\n        if check_convexity:\n            return self.check_convexity() and self.is_planar(tol)\n        else:\n            return self.is_planar()",
  "def remove_points(self, ind, keep_orig=False):\n        \"\"\" Remove points from the fracture definition\n\n        Parameters:\n            ind (np array-like): Indices of points to remove.\n            keep_orig (boolean, optional): Whether to keep the original points\n                in the attribute orig_p. Defaults to False.\n\n        \"\"\"\n        self.p = np.delete(self.p, ind, axis=1)\n        if not keep_orig:\n            self.orig_p = self.p",
  "def plane_coordinates(self):\n        \"\"\"\n        Represent the vertex coordinates in its natural 2d plane.\n\n        The plane does not necessarily have the third coordinate as zero (no\n        translation to the origin is made)\n\n        Returns:\n            np.array (2xn): The 2d coordinates of the vertexes.\n\n        \"\"\"\n        rotation = cg.project_plane_matrix(self.p)\n        points_2d = rotation.dot(self.p)\n\n        return points_2d[:2]",
  "def check_convexity(self):\n        \"\"\"\n        Check if the polygon is convex.\n\n        Todo: If a hanging node is inserted at a segment, this may slightly\n            violate convexity due to rounding errors. It should be possible to\n            write an algorithm that accounts for this. First idea: Projcet\n            point onto line between points before and after, if the projection\n            is less than a tolerance, it is okay.\n\n        Returns:\n            boolean, true if the polygon is convex.\n\n        \"\"\"\n        p_2d = self.plane_coordinates()\n        return self.as_sp_polygon(p_2d).is_convex()",
  "def is_planar(self, tol=1e-4):\n        \"\"\" Check if the points forming this fracture lies in a plane.\n\n        Parameters:\n            tol (double): Tolerance for non-planarity. Treated as an absolute\n                quantity (no scaling with fracture extent)\n\n        Returns:\n            boolean, True if the polygon is planar. False if not.\n        \"\"\"\n        p = self.p - np.mean(self.p, axis=1).reshape((-1, 1))\n        rot = cg.project_plane_matrix(p)\n        p_2d = rot.dot(p)\n        return np.max(np.abs(p_2d[2])) < tol",
  "def compute_centroid(self):\n        \"\"\"\n        Compute, and redefine, center of the fracture in the form of the\n        centroid.\n\n        The method assumes the polygon is convex.\n\n        \"\"\"\n        # Rotate to 2d coordinates\n        rot = cg.project_plane_matrix(self.p)\n        p = rot.dot(self.p)\n        z = p[2, 0]\n        p = p[:2]\n\n        # Vectors from the first point to all other points. Subsequent pairs of\n        # these will span triangles which, assuming convexity, will cover the\n        # polygon.\n        v = p[:, 1:] - p[:, 0].reshape((-1, 1))\n        # The cell center of the triangles spanned by the subsequent vectors\n        cc = (p[:, 0].reshape((-1, 1)) + p[:, 1:-1] + p[:, 2:])/3\n        # Area of triangles\n        area = 0.5 * np.abs(v[0, :-1] * v[1, 1:] - v[1, :-1] * v[0, 1:])\n\n        # The center is found as the area weighted center\n        center = np.sum(cc * area, axis=1) / np.sum(area)\n\n        # Project back again.\n        self.center = rot.transpose().dot(np.append(center, z)).reshape((3, 1))",
  "def compute_normal(self):\n        \"\"\" Compute normal to the polygon.\n        \"\"\"\n        self.normal = cg.compute_normal(self.p)[:, None]",
  "def as_sp_polygon(self, p=None):\n        \"\"\" Represent polygon as a sympy object.\n\n        Parameters:\n            p (np.array, nd x npt, optional): Points for the polygon. Defaults\n                to None, in which case self.p is used.\n\n        Returns:\n            sympy.geometry.Polygon: Representation of the polygon formed by p.\n\n        \"\"\"\n        if p is None:\n            p = self.p\n\n        sp = [sympy.geometry.Point(p[:, i])\n              for i in range(p.shape[1])]\n        return sympy.geometry.Polygon(*sp)",
  "def intersects(self, other, tol, check_point_contact=True):\n        \"\"\"\n        Find intersections between self and another polygon.\n\n        The function checks for both intersection between the interior of one\n        polygon with the boundary of the other, and pure boundary\n        intersections. Intersection types supported so far are\n            X (full intersection, possibly involving boundaries)\n            L (Two fractures intersect at a boundary)\n            T (Boundary segment of one fracture lies partly or completely in\n                the plane of another)\n        Parameters:\n            other (Fracture): To test intersection with.\n            tol (double): Geometric tolerance\n            check_point_contact (boolean, optional): If True, we check if a\n                single point (e.g. a vertex) of one fracture lies in the plane\n                of the other. This is usually an error, thus the test should be\n                on, but may be allowed when imposing external boundaries.\n\n        \"\"\"\n\n        # Find intersections between self and other. Note that the algorithms\n        # for intersections are not fully reflexive in terms of argument\n        # order, so we need to do two tests.\n\n        # Array for intersections with the interior of one polygon (as opposed\n        # to full boundary intersection, below)\n        int_points = np.empty((3, 0))\n\n        # Keep track of whether the intersection points are on the boundary of\n        # the polygons.\n        on_boundary_self = False\n        on_boundary_other = False\n\n        ####\n        # First compare max/min coordinates. If the bounding boxes of the\n        # fractures do not intersect, there is nothing to do.\n        min_self = self.p.min(axis=1)\n        max_self = self.p.max(axis=1)\n        min_other = other.p.min(axis=1)\n        max_other = other.p.max(axis=1)\n\n        # Account for a tolerance in the following test\n        max_self *= 1+np.sign(max_self)*tol\n        min_self *= 1-np.sign(min_self)*tol\n\n        max_other *= 1+np.sign(max_other)*tol\n        min_other *= 1-np.sign(min_other)*tol\n\n        if np.any(max_self < min_other) or np.any(min_self > max_other):\n            return int_points, on_boundary_self, on_boundary_other\n\n        #####\n        # Next screening: To intersect, both fractures must have vertexes\n        # either on both sides of each others plane, or close to the plane (T,\n        # L, Y)-type intersections.\n\n        # Vectors from centers of the fractures to the vertexes of the other\n        # fratures.\n        s_2_o = other.p - self.center.reshape((-1, 1))\n        o_2_s = self.p - other.center.reshape((-1, 1))\n\n        # Take the dot product of distance and normal vectors. Different signs\n        # for different vertexes signifies fractures that potentially cross.\n        other_from_self = np.sum(s_2_o * self.normal, axis=0)\n        self_from_other = np.sum(o_2_s * other.normal, axis=0)\n\n        # To avoid ruling out vertexes that lie on the plane of another\n        # fracture, we introduce a threshold for almost-zero values.\n        # The scaling factor is somewhat arbitrary here, and probably\n        # safeguards too much, but better safe than sorry. False positives will\n        # be corrected by the more accurate, but costly, computations below.\n        scaled_tol = tol * max(1, max(np.max(np.abs(s_2_o)),\n                                      np.max(np.abs(o_2_s))))\n\n        # We can terminate only if no vertexes are close to the plane of the\n        # other fractures.\n        if np.min(np.abs(other_from_self)) > scaled_tol and \\\n            np.min(np.abs(self_from_other)) > scaled_tol:\n            # If one of the fractures has all of its points on a single side of\n            # the other, there can be no intersections.\n            if np.all(np.sign(other_from_self) == 1) or \\\n                np.all(np.sign(other_from_self) == -1) or \\\n                np.all(np.sign(self_from_other) == 1) or \\\n                np.all(np.sign(self_from_other) == -1):\n                return int_points, on_boundary_self, on_boundary_other\n\n        ####\n        # Check for intersection between interior of one polygon with\n        # segment of the other.\n\n        # Compute intersections, with both polygons as first argument\n        isect_self_other = cg.polygon_segment_intersect(self.p, other.p,\n                                                        tol=tol,\n                                                        include_bound_pt=True)\n        isect_other_self = cg.polygon_segment_intersect(other.p, self.p,\n                                                        tol=tol,\n                                                        include_bound_pt=True)\n\n        # Process data\n        if isect_self_other is not None:\n            int_points = np.hstack((int_points, isect_self_other))\n\n            # An intersection between self and other (in that order) is defined\n            # as being between interior of self and boundary of other. See\n            # polygon_segment_intersect for details.\n            on_boundary_self = False\n            on_boundary_other = False\n\n        if isect_other_self is not None:\n            int_points = np.hstack((int_points, isect_other_self))\n\n            # Interior of other intersected by boundary of self\n            on_boundary_self = False\n            on_boundary_other = False\n\n        if int_points.shape[1] > 1:\n            int_points, _, _ \\\n                = setmembership.unique_columns_tol(int_points, tol=tol)\n\n        # There should be at most two of these points.\n        # In some cases, likely involving extrusion, several segments may lay\n        # essentially in the fracture plane, producing more than two segments.\n        # Thus, if more than two colinear points are found, pick out the first\n        # and last one.\n        if int_points.shape[1] > 2:\n            if cg.is_collinear(int_points, tol):\n                sort_ind = cg.argsort_point_on_line(int_points, tol)\n                int_points = int_points[:, [sort_ind[0], sort_ind[-1]]]\n            else:\n                # This is a bug\n                raise ValueError(''' Found more than two intersection between\n                                 fracture polygons.\n                                 ''')\n\n        ####\n        # Next, check for intersections between the polygon boundaries\n        bound_sect_self_other = cg.polygon_boundaries_intersect(self.p,\n                                                                other.p,\n                                                                tol=tol)\n        bound_sect_other_self = cg.polygon_boundaries_intersect(other.p,\n                                                                self.p,\n                                                                tol=tol)\n\n        def point_on_segment(ip, poly, need_two=True):\n            # Check if a set of points are located on a single segment\n            if need_two and ip.shape[1] < 2 or\\\n                ((not need_two) and ip.shape[1] < 1):\n                return False\n            start = poly\n            end = np.roll(poly, 1, axis=1)\n            for si in range(start.shape[1]):\n                dist, cp = cg.dist_points_segments(ip, start[:, si],\n                                                   end[:, si])\n                if np.all(dist < tol):\n                    return True\n            return False\n        # Short cut: If no boundary intersections, we return the interior\n        # points\n        if len(bound_sect_self_other) == 0 and len(bound_sect_other_self) == 0:\n            if check_point_contact and int_points.shape[1] == 1:\n                #  contacts are not implemented. Give a warning, return no\n                # interseciton, and hope the meshing software is merciful\n                if hasattr(self, 'index') and hasattr(other, 'index'):\n                    logger.warning(\"\"\"Found a point contact between fracture\n                                   %i\n                                   and %i at (%.5f, %.5f, %.5f)\"\"\", self.index,\n                                   other.index, *int_points)\n                else:\n                    logger.warning(\"\"\"Found point contact between fractures\n                                   with no index. Coordinate: (%.5f, %.5f,\n                                   %.5f)\"\"\", *int_points)\n                return np.empty((3, 0)), on_boundary_self, on_boundary_other\n            # None of the intersection points lay on the boundary\n\n\n\n        # The 'interior' points can still be on the boundary (naming\n        # of variables should be updated). The points form a boundary\n        # segment if they all lie on the a single segment of the\n        # fracture.\n        on_boundary_self = point_on_segment(int_points, self.p)\n        on_boundary_other = point_on_segment(int_points, other.p)\n        return int_points, on_boundary_self, on_boundary_other",
  "def impose_boundary(self, box, tol):\n        \"\"\"\n        Impose a boundary on a fracture, defined by a bounding box.\n\n        If the fracture extends outside the box, it will be truncated, and new\n        points are inserted on the intersection with the boundary. It is\n        assumed that the points defining the fracture defines a convex set (in\n        its natural plane) to begin with.\n\n        The attribute self.p will be changed if intersections are found. The\n        original vertexes can still be recovered from self.orig_p.\n\n        The box is specified by its extension in Cartesian coordinates.\n\n        Parameters:\n            box (dicitionary): The bounding box, specified by keywords xmin,\n                xmax, ymin, ymax, zmin, zmax.\n            tol (double): Tolerance, defines when two points are considered\n                equal.\n\n        \"\"\"\n\n        # Maximal extent of the fracture\n        min_coord = self.p.min(axis=1)\n        max_coord = self.p.max(axis=1)\n\n        # Coordinates of the bounding box\n        x0_box = box['xmin']\n        x1_box = box['xmax']\n        y0_box = box['ymin']\n        y1_box = box['ymax']\n        z0_box = box['zmin']\n        z1_box = box['zmax']\n\n        # Gather the box coordinates in an array\n        box_array = np.array([[x0_box, x1_box],\n                              [y0_box, y1_box],\n                              [z0_box, z1_box]])\n\n        # We need to be a bit careful if the fracture extends outside the\n        # bounding box on two (non-oposite) sides. In addition to the insertion\n        # of points along the segments defined by self.p, this will also\n        # require the insertion of points at the meeting of the two sides. To\n        # capture this, we first look for the intersection between the fracture\n        # and planes that extends further than the plane, and then later\n        # move the intersection points to lay at the real bounding box\n        x0 = np.minimum(x0_box, min_coord[0] - 10 * tol)\n        x1 = np.maximum(x1_box, max_coord[0] + 10 * tol)\n        y0 = np.minimum(y0_box, min_coord[1] - 10 * tol)\n        y1 = np.maximum(y1_box, max_coord[1] + 10 * tol)\n        z0 = np.minimum(z0_box, min_coord[2] - 10 * tol)\n        z1 = np.maximum(z1_box, max_coord[2] + 10 * tol)\n\n\n        def outside_box(p, bound_i):\n            # Helper function to test if points are outside the bounding box\n            relative = np.amax(np.linalg.norm(p, axis=0))\n            p = cg.snap_to_grid(p, tol=tol/relative)\n            # snap_to_grid will impose a grid of size self.tol, thus points\n            # that are more than half that distance away from the boundary\n            # are deemed outside.\n            # To reduce if-else on the boundary index, we compute all bounds,\n            # and then return the relevant one, as specified by bound_i\n            west_of = p[0] < x0_box - tol / 2\n            east_of = p[0] > x1_box + tol / 2\n            south_of = p[1] < y0_box - tol / 2\n            north_of = p[1] > y1_box + tol / 2\n            beneath = p[2] < z0_box - tol / 2\n            above = p[2] > z1_box + tol / 2\n            outside = np.vstack((west_of, east_of, south_of, north_of,\n                                 beneath, above))\n            return outside[bound_i]\n\n\n        # Represent the planes of the bounding box as fractures, to allow\n        # us to use the fracture intersection methods.\n        # For each plane, we keep the fixed coordinate to the value specified\n        # by the box, and extend the two others to cover segments that run\n        # through the extension of the plane only.\n\n        # Move these to FractureNetwork.impose_external_boundary.\n        # Doesn't immediately work. Depend on the fracture.\n        west = Fracture(np.array([[x0_box, x0_box, x0_box, x0_box],\n                                  [y0, y1, y1, y0],\n                                  [z0, z0, z1, z1]]),\n                        check_convexity=False)\n        east = Fracture(np.array([[x1_box, x1_box, x1_box, x1_box],\n                                  [y0, y1, y1, y0],\n                                  [z0, z0, z1, z1]]),\n                        check_convexity=False)\n        south = Fracture(np.array([[x0, x1, x1, x0],\n                                   [y0_box, y0_box, y0_box, y0_box],\n                                   [z0, z0, z1, z1]]),\n                         check_convexity=False)\n        north = Fracture(np.array([[x0, x1, x1, x0],\n                                   [y1_box, y1_box, y1_box, y1_box],\n                                   [z0, z0, z1, z1]]),\n                         check_convexity=False)\n        bottom = Fracture(np.array([[x0, x1, x1, x0],\n                                    [y0, y0, y1, y1],\n                                    [z0_box, z0_box, z0_box, z0_box]]),\n                          check_convexity=False)\n        top = Fracture(np.array([[x0, x1, x1, x0],\n                                 [y0, y0, y1, y1],\n                                 [z1_box, z1_box, z1_box, z1_box]]),\n                       check_convexity=False)\n        # Collect in a list to allow iteration\n        bound_planes = [west, east, south, north, bottom, top]\n\n        # Loop over all boundary sides and look for intersections\n        for bi, bf in enumerate(bound_planes):\n            # Dimensions that are not fixed by bf\n            active_dims = np.ones(3, dtype=np.bool)\n            active_dims[np.floor(bi/2).astype('int')] = 0\n            # Convert to indices\n            active_dims = np.squeeze(np.argwhere(active_dims))\n\n            # Find intersection points\n            isect, _, _ = self.intersects(bf, tol, check_point_contact=False)\n            num_isect = isect.shape[1]\n            if len(isect) > 0 and num_isect > 0:\n                num_pts_orig = self.p.shape[1]\n                # Add extra points at the end of the point list.\n                self.p, _, _ \\\n                    = setmembership.unique_columns_tol(np.hstack((self.p,\n                                                                  isect)))\n                num_isect = self.p.shape[1] - num_pts_orig\n                if num_isect == 0:\n                    continue\n\n                # Sort fracture points in a ccw order.\n                # This must be done before sliding the intersection points\n                # (below), since the latter may break convexity of the\n                # fracture, thus violate the (current) implementation of\n                # points_2_ccw()\n                sort_ind = self.points_2_ccw()\n\n                # The next step is to decide whether and how to move the\n                # intersection points.\n                # The intersection points will lay on the bounding box on one\n                # of the dimensions (that of bf, specified by xyz_01_box), but\n                # may be outside the box for the other sides (xyz_01). We thus\n                # need to move the intersection points in the plane of bf and\n                # the fracture plane.\n                # The relevant tangent vector is perpendicular to both the\n                # fracture and bf\n                normal_self = cg.compute_normal(self.p)\n                normal_bf = cg.compute_normal(bf.p)\n                tangent = np.cross(normal_self, normal_bf)\n                # Unit vector\n                tangent *= 1./ np.linalg.norm(tangent)\n\n                isect_ind = np.argwhere(sort_ind >= num_pts_orig).ravel('F')\n                p_isect = self.p[:, isect_ind]\n\n                # Loop over all intersection points and active dimensions. If\n                # the point is outside the bounding box, move it.\n                for pi in range(num_isect):\n                    for dim, other_dim in zip(active_dims, active_dims[::-1]):\n                        # Test against lower boundary\n                        lower_diff = p_isect[dim, pi] - box_array[dim, 0]\n                        # Modify coordinates if necessary. This dimension is\n                        # simply set to the boundary, while the other should\n                        # slide along the tangent vector\n                        if lower_diff < 0:\n                            p_isect[dim, pi] = box_array[dim, 0]\n                            # We know lower_diff != 0, no division by zero.\n                            # We may need some tolerances, though.\n                            t = tangent[dim] / lower_diff\n                            p_isect[other_dim, pi] += t * tangent[other_dim]\n\n                        # Same treatment of the upper boundary\n                        upper_diff = p_isect[dim, pi] - box_array[dim, 1]\n                        # Modify coordinates if necessary. This dimension is\n                        # simply set to the boundary, while the other should\n                        # slide along the tangent vector\n                        if upper_diff > 0:\n                            p_isect[dim, pi] = box_array[dim, 1]\n                            t = tangent[dim] / upper_diff\n                            p_isect[other_dim, pi] -= t * tangent[other_dim]\n\n                # Finally, identify points that are outside the face bf\n                inside = np.logical_not(outside_box(self.p, bi))\n                # Dump points that are outside.\n                self.p = self.p[:, inside]\n                self.p, _, _ = setmembership.unique_columns_tol(self.p, tol=tol)\n                # We have modified the fractures, so re-calculate the centroid\n                self.compute_centroid()\n            else:\n                # No points exists, but the whole point set can still be\n                # outside the relevant boundary face.\n                outside = outside_box(self.p, bi)\n                if np.all(outside):\n                    self.p = np.empty((3, 0))\n            # There should be at least three points in a fracture.\n            if self.p.shape[1] < 3:\n                break",
  "def __repr__(self):\n        return self.__str__()",
  "def __str__(self):\n        s = 'Points: \\n'\n        s += str(self.p) + '\\n'\n        s += 'Center: ' + str(self.center)\n        return s",
  "def plot_frame(self, ax=None):\n\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.gca(projection='3d')\n        x = np.append(self.p[0], self.p[0, 0])\n        y = np.append(self.p[1], self.p[1, 0])\n        z = np.append(self.p[2], self.p[2, 0])\n        ax.plot(x, y, z)\n        return ax",
  "def __init__(self, center, major_axis, minor_axis, major_axis_angle,\n                 strike_angle, dip_angle, num_points=16):\n        \"\"\"\n        Initialize an elliptic shaped fracture, approximated by a polygon.\n\n\n        The rotation of the plane is calculated using three angles. First, the\n        rotation of the major axis from the x-axis. Next, the fracture is\n        inclined by specifying the strike angle (which gives the rotation\n        axis) measured from the x-axis, and the dip angle. All angles are\n        measured in radians.\n\n        Parameters:\n            center (np.ndarray, size 3x1): Center coordinates of fracture.\n            major_axis (double): Length of major axis (radius-like, not\n                diameter).\n            minor_axis (double): Length of minor axis. There are no checks on\n                whether the minor axis is less or equal the major.\n            major_axis_angle (double, radians): Rotation of the major axis from\n                the x-axis. Measured before strike-dip rotation, see above.\n            strike_angle (double, radians): Line of rotation for the dip.\n                Given as angle from the x-direction.\n            dip_angle (double, radians): Dip angle, i.e. rotation around the\n                strike direction.\n            num_points (int, optional): Number of points used to approximate\n                the ellipsis. Defaults to 16.\n\n        Example:\n            Fracture centered at [0, 1, 0], with a ratio of lengths of 2,\n            rotation in xy-plane of 45 degrees, and an incline of 30 degrees\n            rotated around the x-axis.\n            >>> frac = EllipticFracture(np.array([0, 1, 0]), 10, 5, np.pi/4, 0,\n                                        np.pi/6)\n\n        \"\"\"\n        center = np.asarray(center)\n        if center.ndim == 1:\n            center = center.reshape((-1, 1))\n        self.center = center\n\n        # First, populate polygon in the xy-plane\n        angs = np.linspace(0, 2 * np.pi, num_points + 1, endpoint=True)[:-1]\n        x = major_axis * np.cos(angs)\n        y = minor_axis * np.sin(angs)\n        z = np.zeros_like(angs)\n        ref_pts = np.vstack((x, y, z))\n\n        assert cg.is_planar(ref_pts)\n\n        # Rotate reference points so that the major axis has the right\n        # orientation\n        major_axis_rot = cg.rot(major_axis_angle, [0, 0, 1])\n        rot_ref_pts = major_axis_rot.dot(ref_pts)\n\n        assert cg.is_planar(rot_ref_pts)\n\n        # Then the dip\n        # Rotation matrix of the strike angle\n        strike_rot = cg.rot(strike_angle, np.array([0, 0, 1]))\n        # Compute strike direction\n        strike_dir = strike_rot.dot(np.array([1, 0, 0]))\n        dip_rot = cg.rot(dip_angle, strike_dir)\n\n        dip_pts = dip_rot.dot(rot_ref_pts)\n\n        assert cg.is_planar(dip_pts)\n\n        # Set the points, and store them in a backup.\n        self.p = center + dip_pts\n        self.orig_p = self.p.copy()\n\n        # Compute normal vector\n        self.normal = cg.compute_normal(self.p)[:, None]\n\n        assert cg.is_planar(self.orig_p, self.normal)",
  "def __init__(self, first, second, coord, bound_first=False, bound_second=False):\n        self.first = first\n        self.second = second\n        self.coord = coord\n        # Information on whether the intersection points lies on the boundaries\n        # of the fractures\n        self.bound_first = bound_first\n        self.bound_second = bound_second",
  "def __repr__(self):\n        s = 'Intersection between fractures ' + str(self.first.index) + ' and ' + \\\n            str(self.second.index) + '\\n'\n        s += 'Intersection points: \\n'\n        for i in range(self.coord.shape[1]):\n            s += '(' + str(self.coord[0, i]) + ', ' + str(self.coord[1, i]) + ', ' + \\\n                 str(self.coord[2, i]) + ') \\n'\n        if self.coord.size > 0:\n            s += 'On boundary of first fracture ' + str(self.bound_first) + '\\n'\n            s += 'On boundary of second fracture ' + str(self.bound_second)\n            s += '\\n'\n        return s",
  "def get_other_fracture(self, i):\n        if self.first == i:\n            return self.second\n        else:\n            return self.first",
  "def on_boundary_of_fracture(self, i):\n        if self.first == i:\n            return self.bound_first\n        elif self.second == i:\n            return self.bound_second\n        else:\n            raise ValueError('Fracture ' + str(i) + ' is not in intersection')",
  "def __init__(self, fractures=None, verbose=0, tol=1e-4):\n\n        self._fractures = fractures\n\n        for i, f in enumerate(self._fractures):\n            f.set_index(i)\n\n        self.intersections = []\n\n        self.has_checked_intersections = False\n        self.tol = tol\n        self.verbose = verbose\n\n        # Initialize with an empty domain. Can be modified later by a call to\n        # 'impose_external_boundary()'\n        self.domain = None\n\n        # Initialize mesh size parameters as empty\n        self.h_min = None\n        self.h_ideal = None\n\n        # Assign an empty tag dictionary\n        self.tags = {}\n\n        # No auxiliary points have been added\n        self.auxiliary_points_added = False",
  "def add(self, f):\n        # Careful here, if we remove one fracture and then add, we'll have\n        # identical indices.\n        f.set_index(len(self._fractures))\n        self._fractures.append(f)",
  "def __getitem__(self, position):\n        return self._fractures[position]",
  "def get_normal(self, frac):\n        return self._fractures[frac].normal",
  "def get_center(self, frac):\n        return self._fractures[frac].center",
  "def intersections_of_fracture(self, frac):\n        \"\"\" Get all known intersections for a fracture.\n\n        If called before find_intersections(), the returned list will be empty.\n\n        Paremeters:\n            frac: A fracture in the network\n\n        Returns:\n            np.array (Intersection): Array of intersections\n\n        \"\"\"\n        if isinstance(frac, int):\n            fi = frac\n        else:\n            fi = frac.index\n        frac_arr = []\n        for i in self.intersections:\n            if i.coord.size == 0:\n                continue\n            if i.first.index == fi or i.second.index == fi:\n                frac_arr.append(i)\n        return frac_arr",
  "def find_intersections(self, use_orig_points=False):\n        \"\"\"\n        Find intersections between fractures in terms of coordinates.\n\n        The intersections are stored in the attribute self.Intersections.\n\n        Handling of the intersections (splitting into non-intersecting\n        polygons, paving the way for gridding) is taken care of by the function\n        split_intersections().\n\n        Note that find_intersections() should be invoked after external\n        boundaries are imposed. If the reverse order is applied, intersections\n        outside the domain may be identified, with unknown consequences for the\n        reliability of the methods. If intersections outside the bounding box\n        are of interest, these can be found by setting the parameter\n        use_orig_points to True.\n\n        Parameters:\n            use_orig_points (boolean, optional): Whether to use the original\n                fracture description in the search for intersections. Defaults\n                to False. If True, all fractures will have their attribute p\n                reset to their original value.\n\n        \"\"\"\n        self.has_checked_intersections = True\n        logger.info('Find intersection between fratures')\n        start_time = time.time()\n\n        # If desired, use the original points in the fracture intersection.\n        # This will reset the field self._fractures.p, and thus revoke\n        # modifications due to boundaries etc.\n        if use_orig_points:\n            for f in self._fractures:\n                f.p = f.orig_p\n\n        for i, first in enumerate(self._fractures):\n            for j in range(i + 1, len(self._fractures)):\n                second = self._fractures[j]\n                logger.info('Processing fracture %i and %i', i, j)\n                isect, bound_first, bound_second = first.intersects(second,\n                                                                    self.tol)\n                if np.array(isect).size > 0:\n                    logger.info('Found an intersection between %i and %i', i, j)\n                    # Let the intersection know whether both intersection\n                    # points lies on the boundary of each fracture\n                    self.intersections.append(Intersection(first, second,\n                                                           isect,\n                                                           bound_first=bound_first,\n                                                           bound_second=bound_second))\n\n        logger.info('Found %i intersections. Ellapsed time: %.5f',\n                    len(self.intersections), time.time() - start_time)",
  "def intersection_info(self, frac_num=None):\n        # Number of fractures with some intersection\n        num_intersecting_fracs = 0\n        # Number of intersections in total\n        num_intersections = 0\n\n        if frac_num is None:\n            frac_num = np.arange(len(self._fractures))\n        for f in frac_num:\n            isects = []\n            for i in self.intersections:\n                if i.first.index == f and i.coord.shape[1] > 0:\n                    isects.append(i.second.index)\n                elif i.second.index == f and i.coord.shape[1] > 0:\n                    isects.append(i.first.index)\n            if len(isects) > 0:\n                num_intersecting_fracs += 1\n                num_intersections += len(isects)\n\n                if self.verbose > 1:\n                    print('  Fracture ' + str(f) + ' intersects with'\\\n                          ' fractuer(s) ' + str(isects))\n        # Print aggregate numbers. Note that all intersections are counted\n        # twice (from first and second), thus divide by two.\n        print('In total ' + str(num_intersecting_fracs) + ' fractures '\n              + 'intersect in ' + str(int(num_intersections/2)) \\\n              + ' intersections')",
  "def split_intersections(self):\n        \"\"\"\n        Based on the fracture network, and their known intersections, decompose\n        the fractures into non-intersecting sub-polygons. These can\n        subsequently be exported to gmsh.\n\n        The method will add an atribute decomposition to self.\n\n        \"\"\"\n\n        logger.info('Split intersections')\n        start_time = time.time()\n\n        # First, collate all points and edges used to describe fracture\n        # boundaries and intersections.\n        all_p, edges,\\\n            edges_2_frac, is_boundary_edge = self._point_and_edge_lists()\n\n        if self.verbose > 1:\n            self._verify_fractures_in_plane(all_p, edges, edges_2_frac)\n\n        # By now, all segments in the grid are defined by a unique set of\n        # points and edges. The next task is to identify intersecting edges,\n        # and split them.\n        all_p, edges, edges_2_frac, is_boundary_edge\\\n            = self._remove_edge_intersections(all_p, edges, edges_2_frac,\n                                              is_boundary_edge)\n\n        if self.verbose > 1:\n            self._verify_fractures_in_plane(all_p, edges, edges_2_frac)\n\n        # Store the full decomposition.\n        self.decomposition = {'points': all_p,\n                              'edges': edges.astype('int'),\n                              'is_bound': is_boundary_edge,\n                              'edges_2_frac': edges_2_frac}\n        polygons = []\n        line_in_frac = []\n        for fi, _ in enumerate(self._fractures):\n            ei = []\n            ei_bound = []\n            # Find the edges of this fracture, add to either internal or\n            # external fracture list\n            for i, e in enumerate(zip(edges_2_frac, is_boundary_edge)):\n                # Check that the boundary information matches the fractures\n                assert e[0].size == e[1].size\n                hit = np.where(e[0] == fi)[0]\n                if hit.size == 1:\n                    if e[1][hit]:\n                        ei_bound.append(i)\n                    else:\n                        ei.append(i)\n                elif hit.size > 1:\n                    raise ValueError('Non-unique fracture edge relation')\n                else:\n                    continue\n\n            poly = sort_points.sort_point_pairs(edges[:2, ei_bound])\n            polygons.append(poly)\n            line_in_frac.append(ei)\n\n        self.decomposition['polygons'] = polygons\n        self.decomposition['line_in_frac'] = line_in_frac\n\n#                              'polygons': poly_segments,\n#                             'polygon_frac': poly_2_frac}\n\n        logger.info('Finished fracture splitting after %.5f seconds',\n                    time.time() - start_time)",
  "def _fracs_2_edges(self, edges_2_frac):\n        \"\"\" Invert the mapping between edges and fractures.\n\n        Returns:\n            List of list: For each fracture, index of all edges that points to\n                it.\n        \"\"\"\n        f2e = []\n        for fi in len(self._fractures):\n            f_l = []\n            for ei, e in enumerate(edges_2_frac):\n                if fi in e:\n                    f_l.append(ei)\n            f2e.append(f_l)\n        return f2e",
  "def _point_and_edge_lists(self):\n        \"\"\"\n        Obtain lists of all points and connections necessary to describe\n        fractures and their intersections.\n\n        Returns:\n            np.ndarray, 3xn: Unique coordinates of all points used to describe\n            the fracture polygons, and their intersections.\n\n            np.ndarray, 2xn_edge: Connections between points, formed either\n                by a fracture boundary, or a fracture intersection.\n            list: For each edge, index of all fractures that point to the\n                edge.\n            np.ndarray of bool (size=num_edges): A flag telling whether the\n                edge is on the boundary of a fracture.\n\n        \"\"\"\n        logger.info('Compile list of points and edges')\n        start_time = time.time()\n\n        # Field for all points in the fracture description\n        all_p = np.empty((3, 0))\n        # All edges, either as fracture boundary, or fracture intersection\n        edges = np.empty((2, 0))\n        # For each edge, a list of all fractures pointing to the edge.\n        edges_2_frac = []\n\n        # Field to know if an edge is on the boundary of a fracture.\n        # Not sure what to do with a T-type intersection here\n        is_boundary_edge = []\n\n        # First loop over all fractures. All edges are assumed to be new; we\n        # will deal with coinciding points later.\n        for fi, frac in enumerate(self._fractures):\n            num_p = all_p.shape[1]\n            num_p_loc = frac.p.shape[1]\n            all_p = np.hstack((all_p, frac.p))\n\n            loc_e = num_p + np.vstack((np.arange(num_p_loc),\n                                       (np.arange(num_p_loc) + 1) % num_p_loc))\n            edges = np.hstack((edges, loc_e))\n            for i in range(num_p_loc):\n                edges_2_frac.append([fi])\n                is_boundary_edge.append([True])\n\n        # Next, loop over all intersections, and define new points and edges\n        for i in self.intersections:\n            # Only add information if the intersection exists, that is, it has\n            # a coordinate.\n            if i.coord.size > 0:\n                num_p = all_p.shape[1]\n                all_p = np.hstack((all_p, i.coord))\n\n                edges = np.hstack(\n                    (edges, num_p + np.arange(2).reshape((-1, 1))))\n                edges_2_frac.append([i.first.index, i.second.index])\n                # If the intersection points are on the boundary of both\n                # fractures, this is a boundary segment.\n                # This does not cover the case of a T-intersection, that will\n                # have to come later.\n                if i.bound_first and i.bound_second:\n                    is_boundary_edge.append([True, True])\n                elif i.bound_first and not i.bound_second:\n                    is_boundary_edge.append([True, False])\n                elif not i.bound_first and i.bound_second:\n                    is_boundary_edge.append([False, True])\n                else:\n                    is_boundary_edge.append([False, False])\n\n        # Ensure that edges are integers\n        edges = edges.astype('int')\n\n        logger.info('Points and edges done. Elapsed time %.5f', time.time() -\n                    start_time)\n\n        return self._uniquify_points_and_edges(all_p, edges, edges_2_frac,\n                                               is_boundary_edge)",
  "def _uniquify_points_and_edges(self, all_p, edges, edges_2_frac,\n                                   is_boundary_edge):\n        # Snap the points to an underlying Cartesian grid. This is the basis\n        # for declearing two points equal\n        # NOTE: We need to account for dimensions in the tolerance;\n\n        start_time = time.time()\n        logger.info(\"\"\"Uniquify points and edges, starting with %i points, %i\n                    edges\"\"\", all_p.shape[1], edges.shape[1])\n\n#        all_p = cg.snap_to_grid(all_p, tol=self.tol)\n\n        # We now need to find points that occur in multiple places\n        p_unique, unique_ind_p, \\\n            all_2_unique_p = setmembership.unique_columns_tol(all_p, tol=\n                                                              self.tol *\n                                                              np.sqrt(3))\n\n        # Update edges to work with unique points\n        edges = all_2_unique_p[edges]\n\n        # Look for edges that share both nodes. These will be identical, and\n        # will form either a L/Y-type intersection (shared boundary segment),\n        # or a three fractures meeting in a line.\n        # Do a sort of edges before looking for duplicates.\n        e_unique, unique_ind_e, all_2_unique_e = \\\n            setmembership.unique_columns_tol(np.sort(edges, axis=0))\n\n        # Update the edges_2_frac map to refer to the new edges\n        edges_2_frac_new = e_unique.shape[1] * [np.empty(0)]\n        is_boundary_edge_new = e_unique.shape[1] * [np.empty(0)]\n\n        for old_i, new_i in enumerate(all_2_unique_e):\n            edges_2_frac_new[new_i], ind =\\\n                np.unique(np.hstack((edges_2_frac_new[new_i],\n                                     edges_2_frac[old_i])),\n                          return_index=True)\n            tmp = np.hstack((is_boundary_edge_new[new_i],\n                             is_boundary_edge[old_i]))\n            is_boundary_edge_new[new_i] = tmp[ind]\n\n        edges_2_frac = edges_2_frac_new\n        is_boundary_edge = is_boundary_edge_new\n\n        # Represent edges by unique values\n        edges = e_unique\n\n        # The uniquification of points may lead to edges with identical start\n        # and endpoint. Find and remove these.\n        point_edges = np.where(np.squeeze(np.diff(edges, axis=0)) == 0)[0]\n        edges = np.delete(edges, point_edges, axis=1)\n        unique_ind_e = np.delete(unique_ind_e, point_edges)\n        for ri in point_edges[::-1]:\n            del edges_2_frac[ri]\n            del is_boundary_edge[ri]\n\n        # Ensure that the edge to fracture map to a list of numpy arrays.\n        # Use unique so that the same edge only refers to an edge once.\n        edges_2_frac = [np.unique(np.array(edges_2_frac[i])) for i in\n                        range(len(edges_2_frac))]\n\n        # Sanity check, the fractures should still be defined by points in a\n        # plane.\n        self._verify_fractures_in_plane(p_unique, edges, edges_2_frac)\n\n        logger.info('''Uniquify complete. %i points, %i edges. Ellapsed time\n                    %.5f''', p_unique.shape[1], edges.shape[1], time.time() -\n                    start_time)\n\n        return p_unique, edges, edges_2_frac, is_boundary_edge",
  "def _remove_edge_intersections(self, all_p, edges, edges_2_frac,\n                                   is_boundary_edge):\n        \"\"\"\n        Remove crossings from the set of fracture intersections.\n\n        Intersecting intersections (yes) are split, and new points are\n        introduced.\n\n        Parameters:\n            all_p (np.ndarray, 3xn): Coordinates of all points used to describe\n                the fracture polygons, and their intersections. Should be\n                unique.\n            edges (np.ndarray, 2xn): Connections between points, formed either\n                by a fracture boundary, or a fracture intersection.\n            edges_2_frac (list): For each edge, index of all fractures that\n                point to the edge.\n            is_boundary_edge (np.ndarray of bool, size=num_edges): A flag\n                telling whether the edge is on the boundary of a fracture.\n\n        Returns:\n            The same fields, but updated so that all edges are\n            non-intersecting.\n\n        \"\"\"\n        logger.info('Remove edge intersections')\n        start_time = time.time()\n\n        # The algorithm loops over all fractures, pulls out edges associated\n        # with the fracture, project to the local 2D plane, and look for\n        # intersections there (direct search in 3D may also work, but this was\n        # a simple option). When intersections are found, the global lists of\n        # points and edges are updated.\n        for fi, frac in enumerate(self._fractures):\n\n            logger.debug('Remove intersections from fracture %i', fi)\n\n            # Identify the edges associated with this fracture\n            # It would have been more convenient to use an inverse\n            # relationship frac_2_edge here, but that would have made the\n            # update for new edges (towards the end of this loop) more\n            # cumbersome.\n            edges_loc_ind = []\n            for ei, e in enumerate(edges_2_frac):\n                if np.any(e == fi):\n                    edges_loc_ind.append(ei)\n\n            edges_loc = np.vstack((edges[:, edges_loc_ind],\n                                   np.array(edges_loc_ind)))\n            p_ind_loc = np.unique(edges_loc[:2])\n            p_loc = all_p[:, p_ind_loc]\n\n            p_2d, edges_2d, p_loc_c, rot = self._points_2_plane(p_loc,\n                                                                edges_loc,\n                                                                p_ind_loc)\n\n            # Add a tag to trace the edges during splitting\n            edges_2d[2] = edges_loc[2]\n\n            # Obtain new points and edges, so that no edges on this fracture\n            # are intersecting.\n            # It seems necessary to increase the tolerance here somewhat to\n            # obtain a more robust algorithm. Not sure about how to do this\n            # consistent.\n            p_new, edges_new = cg.remove_edge_crossings(p_2d, edges_2d,\n                                                        tol=self.tol,\n                                                        verbose=self.verbose,\n                                                        snap=False)\n            # Then, patch things up by converting new points to 3D,\n\n            # From the design of the functions in cg, we know that new points\n            # are attached to the end of the array. A more robust alternative\n            # is to find unique points on a combined array of p_loc and p_new.\n            p_add = p_new[:, p_ind_loc.size:]\n            num_p_add = p_add.shape[1]\n\n            # Add third coordinate, and map back to 3D\n            p_add = np.vstack((p_add, np.zeros(num_p_add)))\n\n            # Inverse of rotation matrix is the transpose, add cloud center\n            # correction\n            p_add_3d = rot.transpose().dot(p_add) + p_loc_c\n\n            # The new points will be added at the end of the global point array\n            # (if not, we would need to renumber all global edges).\n            # Global index of added points\n            ind_p_add = all_p.shape[1] + np.arange(num_p_add)\n            # Global index of local points (new and added)\n            p_ind_exp = np.hstack((p_ind_loc, ind_p_add))\n\n            # Add the new points towards the end of the list.\n            all_p = np.hstack((all_p, p_add_3d))\n\n            new_all_p, _, ia = setmembership.unique_columns_tol(all_p,\n                                                                self.tol)\n\n            # Handle case where the new point is already represented in the\n            # global list of points.\n            if new_all_p.shape[1] < all_p.shape[1]:\n                all_p = new_all_p\n                p_ind_exp = ia[p_ind_exp]\n\n            # The ordering of the global edge list bears no significance. We\n            # therefore plan to delete all edges (new and old), and add new\n            # ones.\n\n            # First add new edges.\n            # All local edges in terms of global point indices\n            edges_new_glob = p_ind_exp[edges_new[:2]]\n            edges = np.hstack((edges, edges_new_glob))\n\n            # Global indices of the local edges\n            edges_loc_ind = np.unique(edges_loc_ind)\n\n            # Append fields for edge-fracture map and boundary tags\n            for ei in range(edges_new.shape[1]):\n                # Find the global edge index. For most edges, this will be\n                # correctly identified by edges_new[2], which tracks the\n                # original edges under splitting. However, in cases of\n                # overlapping segments, in which case the index of the one edge\n                # may completely override the index of the other (this is\n                # caused by the implementation of remove_edge_crossings).\n                # We therefore compare the new edge to the old ones (before\n                # splitting). If found, use the old information; if not, use\n                # index as tracked by splitting.\n                is_old, old_loc_ind =\\\n                    setmembership.ismember_rows(edges_new_glob[:, ei]\n                                                .reshape((-1, 1)),\n                                                edges[:2, edges_loc_ind])\n                if is_old[0]:\n                    glob_ei = edges_loc_ind[old_loc_ind[0]]\n                else:\n                    glob_ei = edges_new[2, ei]\n                # Update edge_2_frac and boundary information.\n                edges_2_frac.append(edges_2_frac[glob_ei])\n                is_boundary_edge.append(is_boundary_edge[glob_ei])\n\n            # Finally, purge the old edges\n            edges = np.delete(edges, edges_loc_ind, axis=1)\n\n            # We cannot delete more than one list element at a time. Delete by\n            # index in decreasing order, so that we do not disturb the index\n            # map.\n            edges_loc_ind.sort()\n            for ei in edges_loc_ind[::-1]:\n                del edges_2_frac[ei]\n                del is_boundary_edge[ei]\n            # And we are done with this fracture. On to the next one.\n\n\n        logger.info('Done with intersection removal. Elapsed time %.5f',\n                    time.time() - start_time)\n        self._verify_fractures_in_plane(all_p, edges, edges_2_frac)\n\n        return self._uniquify_points_and_edges(all_p, edges, edges_2_frac,\n                                               is_boundary_edge)",
  "def report_on_decomposition(self, do_print=True, verbose=None):\n        \"\"\"\n        Compute various statistics on the decomposition.\n\n        The coverage is rudimentary for now, will be expanded when needed.\n\n        Parameters:\n            do_print (boolean, optional): Print information. Defaults to True.\n            verbose (int, optional): Override the verbosity level of the\n                network itself. If not provided, the network value will be\n                used.\n\n        Returns:\n            str: String representation of the statistics\n\n        \"\"\"\n        if verbose is None:\n            verbose = self.verbose\n        d = self.decomposition\n\n        s = str(len(self._fractures)) + ' fractures are split into '\n        s += str(len(d['polygons'])) + ' polygons \\n'\n\n        s += 'Number of points: ' + str(d['points'].shape[1]) + '\\n'\n        s += 'Number of edges: ' + str(d['edges'].shape[1]) + '\\n'\n\n        if verbose > 1:\n            # Compute minimum distance between points in point set\n            dist = np.inf\n            p = d['points']\n            num_points = p.shape[1]\n            hit = np.ones(num_points, dtype=np.bool)\n            for i in range(num_points):\n                hit[i] = False\n                dist_loc = cg.dist_point_pointset(p[:, i], p[:, hit])\n                dist = np.minimum(dist, dist_loc.min())\n                hit[i] = True\n            s += 'Minimal disance between points ' + str(dist) + '\\n'\n\n        if do_print:\n            print(s)\n\n        return s",
  "def fractures_of_points(self, pts):\n        \"\"\"\n        For a given point, find all fractures that refer to it, either as\n        vertex or as internal.\n\n        Returns:\n            list of np.int: indices of fractures, one list item per point.\n        \"\"\"\n        fracs_of_points = []\n        pts = np.atleast_1d(np.asarray(pts))\n        for i in pts:\n            fracs_loc = []\n\n            # First identify edges that refers to the point\n            edge_ind = np.argwhere(np.any(self.decomposition['edges'][:2]\\\n                                          == i, axis=0)).ravel('F')\n            edges_loc = self.decomposition['edges'][:, edge_ind]\n            # Loop over all polygons. If their edges are found in edges_loc,\n            # store the corresponding fracture index\n            for poly_ind, poly in enumerate(self.decomposition['polygons']):\n                ismem, _ = setmembership.ismember_rows(edges_loc, poly)\n                if any(ismem):\n                    fracs_loc.append(self.decomposition['polygon_frac']\\\n                                     [poly_ind])\n            fracs_of_points.append(list(np.unique(fracs_loc)))\n        return fracs_of_points",
  "def close_points(self, dist):\n        \"\"\"\n        In the set of points used to describe the fractures (after\n        decomposition), find pairs that are closer than a certain distance.\n\n        Parameters:\n            dist (double): Threshold distance, all points closer than this will\n                be reported.\n\n        Returns:\n            List of tuples: Each tuple contain indices of a set of close\n                points, and the distance between the points. The list is not\n                symmetric; if (a, b) is a member, (b, a) will not be.\n\n        \"\"\"\n        c_points = []\n\n        pt = self.decomposition['points']\n        for pi in range(pt.shape[1]):\n            d = cg.dist_point_pointset(pt[:, pi], pt[:, pi+1:])\n            ind = np.argwhere(d < dist).ravel('F')\n            for i in ind:\n                # Indices of close points, with an offset to compensate for\n                # slicing of the point cloud.\n                c_points.append((pi, i + pi + 1, d[i]))\n\n        return c_points",
  "def _verify_fractures_in_plane(self, p, edges, edges_2_frac):\n        \"\"\"\n        Essentially a debugging method that verify that the given set of\n        points, edges and edge connections indeed form planes.\n\n        This has turned out to be a common symptom of trouble.\n\n        \"\"\"\n        for fi, _ in enumerate(self._fractures):\n\n            # Identify the edges associated with this fracture\n            edges_loc_ind = []\n            for ei, e in enumerate(edges_2_frac):\n                if np.any(e == fi):\n                    edges_loc_ind.append(ei)\n\n            edges_loc = edges[:, edges_loc_ind]\n            p_ind_loc = np.unique(edges_loc)\n            p_loc = p[:, p_ind_loc]\n\n            # Run through points_2_plane, to check the assertions\n            self._points_2_plane(p_loc, edges_loc, p_ind_loc)",
  "def _points_2_plane(self, p_loc, edges_loc, p_ind_loc):\n        \"\"\"\n        Convenience method for rotating a point cloud into its own 2d-plane.\n        \"\"\"\n\n        # Center point cloud around the origin\n        p_loc_c = np.mean(p_loc, axis=1).reshape((-1, 1))\n        p_loc -= p_loc_c\n\n        # Project the points onto the local plane defined by the fracture\n        rot = cg.project_plane_matrix(p_loc, tol=self.tol)\n        p_2d = rot.dot(p_loc)\n\n        extent = p_2d.max(axis=1) - p_2d.min(axis=1)\n        lateral_extent = np.maximum(np.max(extent[2]), 1)\n        assert extent[2] < lateral_extent * self.tol * 30\n\n        # Dump third coordinate\n        p_2d = p_2d[:2]\n\n        # The edges must also be redefined to account for the (implicit)\n        # local numbering of points\n        edges_2d = np.empty_like(edges_loc)\n        for ei in range(edges_loc.shape[1]):\n            edges_2d[0, ei] = np.argwhere(p_ind_loc == edges_loc[0, ei])\n            edges_2d[1, ei] = np.argwhere(p_ind_loc == edges_loc[1, ei])\n\n        assert edges_2d[:2].max() < p_loc.shape[1]\n\n        return p_2d, edges_2d, p_loc_c, rot",
  "def change_tolerance(self, new_tol):\n        \"\"\"\n        Redo the whole configuration based on the new tolerance\n        \"\"\"\n        pass",
  "def __repr__(self):\n        s = 'Fracture set with ' + str(len(self._fractures)) + ' fractures'\n        return s",
  "def plot_fractures(self, ind=None):\n        if ind is None:\n            ind = np.arange(len(self._fractures))\n        fig = plt.figure()\n        ax = fig.gca(projection='3d')\n        for f in self._fractures:\n            f.plot_frame(ax)\n        return fig\n\n        for i, f_1 in enumerate(self._fractures):\n            for j, f_2 in enumerate(self._fractures[i + 1:]):\n                d = np.Inf\n                for p_1 in f_1.points():\n                    for p_2 in f_2.points():\n                        d = np.minimum(d, cg.dist_point_pointset(p_1, p_2)[0])\n                p_2_p[i, i + j + 1] = d\n\n                d = np.Inf\n                for s_1 in f_1.segments():\n                    for s_2 in f_2.segments():\n                        d = np.minimum(d,\n                                       cg.distance_segment_segment(s_1[:, 0],\n                                                                   s_1[:, 1],\n                                                                   s_2[:, 0],\n                                                                   s_2[:, 1]))\n                s_2_s[i, i + j + 1] = d\n\n        return p_2_p, s_2_s",
  "def add_subdomain_boundaries(self, vertexes):\n        \"\"\"\n        Adds subdomain boundaries to the fracture network. These are\n        intended to ensure the partitioning of the matrix along the planes\n        they define.\n        Parameters:\n            vertexes: either a Fracture or a\n            np.array([[x0, x1, x2, x3],\n                      [y0, y1, y2, y3],\n                      [z0, z1, z2, z3]])\n        \"\"\"\n        subdomain_tags = self.tags.get('subdomain',\n                                       [False]*len(self._fractures))\n        for f in vertexes:\n            if not hasattr(f, 'p') or not isinstance(f.p, np.ndarray):\n                # np.array to Fracture:\n                f = Fracture(f)\n            # Add the fake fracture and its tag to the network:\n            self.add(f)\n            subdomain_tags.append(True)\n\n        self.tags['subdomain'] = subdomain_tags",
  "def impose_external_boundary(self, box=None, truncate_fractures=True,\n                                 keep_box=True):\n        \"\"\"\n        Set an external boundary for the fracture set.\n\n        The boundary takes the form of a 3D box, described by its minimum and\n        maximum coordinates. If no bounding box is provided, a box will be\n        fited outside the fracture network.\n\n        If desired, the fratures will be truncated to lay within the bounding\n        box; that is, Fracture.p will be modified. The orginal coordinates of\n        the fracture boundary can still be recovered from the attribute\n        Fracture.orig_points.\n\n        Fractures that are completely outside the bounding box will be deleted\n        from the fracture set.\n\n        Parameters:\n            box (dictionary): Has fields 'xmin', 'xmax', and similar for y and\n                z.\n            truncate_fractures (boolean, optional): If True, fractures outside\n            the bounding box will be disregarded, while fractures crossing the\n            boundary will be truncated.\n\n        \"\"\"\n        if box is None:\n            OVERLAP = 0.15\n            cmin = np.ones((3, 1)) * float('inf')\n            cmax = -np.ones((3, 1)) * float('inf')\n            for f in self._fractures:\n                cmin = np.min(np.hstack((cmin, f.p)), axis=1).reshape((-1, 1))\n                cmax = np.max(np.hstack((cmax, f.p)), axis=1).reshape((-1, 1))\n            cmin = cmin[:, 0]\n            cmax = cmax[:, 0]\n            dx = OVERLAP * (cmax - cmin)\n            box = {'xmin': cmin[0] - dx[0], 'xmax': cmax[0] + dx[0],\n                   'ymin': cmin[1] - dx[1], 'ymax': cmax[1] + dx[1],\n                   'zmin': cmin[2] - dx[2], 'zmax': cmax[2] + dx[2]}\n\n        # Insert boundary in the form of a box, and kick out (parts of)\n        # fractures outside the box\n        self.domain = box\n\n        #Create fractures of box here.\n        #Store them self._fractures so that split_intersections work\n        #keep track of which fractures are really boundaries - perhaps attribute is_proxy?\n\n        if truncate_fractures:\n            # Keep track of fractures that are completely outside the domain.\n            # These will be deleted.\n            delete_frac = []\n\n            # Loop over all fractures, use method in fractures to truncate if\n            # necessary.\n            for i, frac in enumerate(self._fractures):\n                frac.impose_boundary(box, self.tol)\n                if frac.p.shape[1] == 0:\n                    delete_frac.append(i)\n\n            # Delete fractures that have all points outside the bounding box\n            # There may be some uncovered cases here, with a fracture barely\n            # touching the box from the outside, but we leave that for now.\n            for i in np.unique(delete_frac)[::-1]:\n                del self._fractures[i]\n\n            # Final sanity check: All fractures should have at least three\n            # points at the end of the manipulations\n            for f in self._fractures:\n                assert f.p.shape[1] >= 3\n\n\n        self._make_bounding_planes(box, keep_box)",
  "def _make_bounding_planes(self, box, keep_box=True):\n        \"\"\"\n        Translate the bounding box into fractures. Tag them as boundaries.\n        For now limited to a box consisting of six planes.\n        \"\"\"\n        x0 = box['xmin']\n        x1 = box['xmax']\n        y0 = box['ymin']\n        y1 = box['ymax']\n        z0 = box['zmin']\n        z1 = box['zmax']\n        west = Fracture(np.array([[x0, x0, x0, x0],\n                                  [y0, y1, y1, y0],\n                                  [z0, z0, z1, z1]]),\n                        check_convexity=False)\n        east = Fracture(np.array([[x1, x1, x1, x1],\n                                  [y0, y1, y1, y0],\n                                  [z0, z0, z1, z1]]),\n                        check_convexity=False)\n        south = Fracture(np.array([[x0, x1, x1, x0],\n                                   [y0, y0, y0, y0],\n                                   [z0, z0, z1, z1]]),\n                         check_convexity=False)\n        north = Fracture(np.array([[x0, x1, x1, x0],\n                                   [y1, y1, y1, y1],\n                                   [z0, z0, z1, z1]]),\n                         check_convexity=False)\n        bottom = Fracture(np.array([[x0, x1, x1, x0],\n                                    [y0, y0, y1, y1],\n                                    [z0, z0, z0, z0]]),\n                          check_convexity=False)\n        top = Fracture(np.array([[x0, x1, x1, x0],\n                                 [y0, y0, y1, y1],\n                                 [z1, z1, z1, z1]]),\n                       check_convexity=False)\n        # Collect in a list to allow iteration\n        bound_planes = [west, east, south, north, bottom, top]\n        boundary_tags = self.tags.get('boundary',\n                                       [False]*len(self._fractures))\n\n        # Add the boundaries to the fracture network and tag them.\n        if keep_box:\n            for f in bound_planes:\n                self.add(f)\n                boundary_tags.append(True)\n        self.tags['boundary'] = boundary_tags",
  "def _classify_edges(self, polygon_edges):\n        \"\"\"\n        Classify the edges into fracture boundary, intersection, or auxiliary.\n        Also identify points on intersections between interesctions (fractures\n        of co-dimension 3)\n\n        Parameters:\n            polygon_edges (list of lists): For each polygon the global edge\n                indices that forms the polygon boundary.\n\n        Returns:\n            tag: Tag of the fracture, using the values in GmshConstants. Note\n                that auxiliary points will not be tagged (these are also\n                ignored in gmsh_interface.GmshWriter).\n            is_0d_grid: boolean, one for each point. True if the point is\n                shared by two or more intersection lines.\n\n        \"\"\"\n        edges = self.decomposition['edges']\n        is_bound = self.decomposition['is_bound']\n        num_edges = edges.shape[1]\n\n        poly_2_frac = np.arange(len(self.decomposition['polygons']))\n        self.decomposition['polygon_frac'] = poly_2_frac\n\n        # Construct a map from edges to polygons\n        edge_2_poly = [[] for i in range(num_edges)]\n        for pi, poly in enumerate(polygon_edges[0]):\n            for ei in np.unique(poly):\n                edge_2_poly[ei].append(poly_2_frac[pi])\n\n        # Count the number of referals to the edge from polygons belonging to\n        # different fractures (not polygons)\n        num_referals = np.zeros(num_edges)\n        for ei, ep in enumerate(edge_2_poly):\n            num_referals[ei] = np.unique(np.array(ep)).size\n\n        # A 1-d grid is inserted where there is more than one fracture\n        # referring.\n        has_1d_grid = np.where(num_referals > 1)[0]\n\n        num_constraints = len(is_bound)\n        constants = GmshConstants()\n        tag = np.zeros(num_edges, dtype='int')\n\n        # Find fractures that are tagged as a boundary\n        all_bound = [np.all(is_bound[i]) for i in range(len(is_bound))]\n        bound_ind = np.where(all_bound)[0]\n        # Remove those that are referred to by more than fracture - this takes\n        # care of L-type intersections\n        bound_ind = np.setdiff1d(bound_ind, has_1d_grid)\n\n        # Index of lines that should have a 1-d grid. This are all of the first\n        # num-constraints, minus those on the boundary.\n        # Note that edges with index > num_constraints are known to be of the\n        # auxiliary type. These will have tag zero; and treated in a special\n        # manner by the interface to gmsh.\n        intersection_ind = np.setdiff1d(np.arange(num_constraints), bound_ind)\n        tag[bound_ind] = constants.FRACTURE_TIP_TAG\n        tag[intersection_ind] = constants.FRACTURE_INTERSECTION_LINE_TAG\n\n        # Count the number of times a point is referred to by an intersection\n        # between two fractures. If this is more than one, the point should\n        # have a 0-d grid assigned to it.\n        isect_p = edges[:, intersection_ind].ravel()\n        num_occ_pt = np.bincount(isect_p)\n        is_0d_grid = np.where(num_occ_pt > 1)[0]\n\n        return tag, is_0d_grid",
  "def _poly_2_segment(self):\n        \"\"\"\n        Represent the polygons by the global edges, and determine if the lines\n        must be reversed (locally) for the polygon to form a closed loop.\n\n        \"\"\"\n        edges = self.decomposition['edges']\n        poly = self.decomposition['polygons']\n\n        poly_2_line = []\n        line_reverse = []\n        for p in poly:\n            hit, ind = setmembership.ismember_rows(p, edges[:2], sort=False)\n            hit_reverse, ind_reverse = setmembership.ismember_rows(\n                p[::-1], edges[:2], sort=False)\n            assert np.all(hit + hit_reverse == 1)\n\n            line_ind = np.zeros(p.shape[1])\n\n            hit_ind = np.where(hit)[0]\n            hit_reverse_ind = np.where(hit_reverse)[0]\n            line_ind[hit_ind] = ind\n            line_ind[hit_reverse_ind] = ind_reverse\n\n            poly_2_line.append(line_ind.astype('int'))\n            line_reverse.append(hit_reverse)\n\n        return poly_2_line, line_reverse",
  "def _determine_mesh_size(self, **kwargs):\n        \"\"\"\n        Set the preferred mesh size for geometrical points as specified by\n        gmsh.\n\n        Currently, the only option supported is to specify a single value for\n        all fracture points, and one value for the boundary.\n\n        See the gmsh manual for further details.\n\n        \"\"\"\n        mode = kwargs.get('mode', 'distance')\n\n        num_pts = self.decomposition['points'].shape[1]\n\n        if mode == 'constant':\n            val = kwargs.get('value', None)\n            bound_val = kwargs.get('bound_value', None)\n            if val is not None:\n                mesh_size = val * np.ones(num_pts)\n            else:\n                mesh_size = None\n            if bound_val is not None:\n                mesh_size_bound = bound_val\n            else:\n                mesh_size_bound = None\n            return mesh_size, mesh_size_bound\n        elif mode == 'distance' or mode == 'weighted':\n            if self.h_min is None or self.h_ideal is None:\n                print('Found no information on mesh sizes. Returning')\n                return None, None\n\n            p = self.decomposition['points']\n            num_pts = p.shape[1]\n            dist = cg.dist_pointset(p, max_diag=True)\n            mesh_size = np.min(dist, axis=1)\n            print('Minimal distance between points encountered is ' + str(np.min(dist)))\n            mesh_size = np.maximum(mesh_size, self.h_min * np.ones(num_pts))\n            mesh_size = np.minimum(mesh_size, self.h_ideal * np.ones(num_pts))\n\n            mesh_size_bound = self.h_ideal\n            return mesh_size, mesh_size_bound\n        else:\n            raise ValueError('Unknown mesh size mode ' + mode)",
  "def insert_auxiliary_points(self, h_ideal=None, h_min=None):\n        \"\"\" Insert auxiliary points on fracture edges. Used to guide gmsh mesh\n        size parameters.\n\n        The function should only be called once to avoid insertion of multiple\n        layers of extra points, this will likely kill gmsh.\n\n        The function is motivated by similar functionality for 2d domains, but\n        is considerably less mature.\n\n        The function should be used in conjunction with _determine_mesh_size(),\n        called with mode='distance'. The ultimate goal is to set the mesh size\n        for geometrical points in Gmsh. To that end, this function inserts\n        additional points on the fracture boundaries. The mesh size is then\n        determined as the distance between all points in the fracture\n        description.\n\n        Parameters:\n            h_ideal: Ideal mesh size. Will be added to all points that are\n                sufficiently far away from other points.\n            h_min: Minimal mesh size; we will make no attempts to enforce\n                even smaller mesh sizes upon Gmsh.\n\n        \"\"\"\n\n        if self.auxiliary_points_added:\n            print('Auxiliary points already added. Returning.')\n        else:\n            self.auxiliary_points_added = True\n\n        self.h_ideal = h_ideal\n        self.h_min = h_min\n\n        def dist_p(a, b):\n            a = a.reshape((-1, 1))\n            b = b.reshape((-1, 1))\n            d = b - a\n            return np.sqrt(np.sum(d**2))\n\n        intersecting_fracs = []\n        # Loop over all fractures\n        for f in self._fractures:\n\n            # First compare segments with intersections to this fracture\n            nfp = f.p.shape[1]\n\n            # Keep track of which other fractures are intersecting - will be\n            # needed later on\n            isect_f = []\n            for i in self.intersections_of_fracture(f):\n                if f is i.first:\n                    isect_f.append(i.second.index)\n                else:\n                    isect_f.append(i.first.index)\n\n                # Assuming the fracture is convex, the closest point for\n                dist, cp = cg.dist_points_segments(i.coord, f.p,\n                                                   np.roll(f.p, 1, axis=1))\n                # Insert a (candidate) point only at the segment closest to the\n                # intersection point. If the intersection line runs parallel\n                # with a segment, this may be insufficient, but we will deal\n                # with this if necessary.\n                closest_segment = np.argmin(dist, axis=1)\n                min_dist = dist[np.arange(i.coord.shape[1]), closest_segment]\n\n                for pi, (si, di) in enumerate(zip(closest_segment, min_dist)):\n                    if di < h_ideal:\n                        d_1 = dist_p(cp[pi, si], f.p[:, si])\n                        d_2 = dist_p(cp[pi, si], f.p[:, (si+1)%nfp])\n                        # If the intersection point is not very close to any of\n                        # the points on the segment, we split the segment.\n                        if d_1 > h_min and d_2 > h_min:\n                            np.insert(f.p, (si+1)%nfp, cp[pi, si], axis=1)\n\n            # Take note of the intersecting fractures\n            intersecting_fracs.append(isect_f)\n\n        for fi, f in enumerate(self._fractures):\n            nfp = f.p.shape[1]\n            for of in self._fractures:\n                # Can do some box arguments here to avoid computations\n\n                # First, check if we are intersecting, this is covered already\n                if of.index in intersecting_fracs[fi]:\n                    continue\n\n                f_start = f.p\n                f_end = np.roll(f_start, 1, axis=1)\n                # Then, compare distance between segments\n                # Loop over fracture segments of this fracture\n                for si, fs in enumerate(f.segments()):\n                    of_start = of.p\n                    of_end = np.roll(of_start, 1, axis=1)\n                    # Compute distance to all fractures of the other fracture,\n                    # and find the closest segment on the other fracture\n                    fs = f_start[:, si].squeeze()#.reshape((-1, 1))\n                    fe = f_end[:, si].squeeze()#.reshape((-1, 1))\n                    d, cp_f, _ = cg.dist_segment_segment_set(fs, fe, of_start,\n                                                             of_end)\n                    mi = np.argmin(d)\n                    # If the distance is smaller than ideal length, but the\n                    # closets point is not too close to the segment endpoints,\n                    # we add a new point\n                    if d[mi] < h_ideal:\n                        d_1 = dist_p(cp_f[:, mi], f.p[:, si])\n                        d_2 = dist_p(cp_f[:, mi], f.p[:, (si+1)%nfp])\n                        if d_1 > h_min and d_2 > h_min:\n                            np.insert(f.p, (si+1)%nfp, cp_f[:, mi], axis=1)",
  "def to_vtk(self, file_name, data=None, binary=True):\n        \"\"\"\n        Export the fracture network to vtk.\n\n        The fractures are treated as polygonal cells, with no special treatment\n        of intersections.\n\n        Fracture numbers are always exported (1-offset). In addition, it is\n        possible to export additional data, as specified by the\n        keyword-argument data.\n\n        Parameters:\n            file_name (str): Name of the target file.\n            data (dictionary, optional): Data associated with the fractures.\n                The values in the dictionary should be numpy arrays. 1d and 3d\n                data is supported. Fracture numbers are always exported.\n            binary (boolean, optional): Use binary export format. Defaults to\n                True.\n\n        \"\"\"\n        network_vtk = vtk.vtkUnstructuredGrid()\n\n        point_counter = 0\n        pts_vtk = vtk.vtkPoints()\n        for f in self._fractures:\n            # Add local points\n            [pts_vtk.InsertNextPoint(*p) for p in f.p.T]\n\n            # Indices of local points\n            loc_pt_id = point_counter + np.arange(f.p.shape[1],\n                                                  dtype='int')\n            # Update offset\n            point_counter += f.p.shape[1]\n\n            # Add bounding polygon\n            frac_vtk = vtk.vtkIdList()\n            [frac_vtk.InsertNextId(p) for p in loc_pt_id]\n            # Close polygon\n            frac_vtk.InsertNextId(loc_pt_id[0])\n\n            network_vtk.InsertNextCell(vtk.VTK_POLYGON, frac_vtk)\n\n        # Add the points\n        network_vtk.SetPoints(pts_vtk)\n\n        writer = vtk.vtkXMLUnstructuredGridWriter()\n        writer.SetInputData(network_vtk)\n        writer.SetFileName(file_name)\n\n        if not binary:\n            writer.SetDataModeToAscii()\n\n        # Cell-data to be exported is at least the fracture numbers\n        if data is None:\n            data = {}\n        # Use offset 1 for fracture numbers (should we rather do 0?)\n        data['Fracture_Number'] = 1 + np.arange(len(self._fractures))\n\n        for name, data in data.items():\n            data_vtk = vtk_np.numpy_to_vtk(data.ravel(order='F'), deep=True,\n                                           array_type=vtk.VTK_DOUBLE)\n            data_vtk.SetName(name)\n            data_vtk.SetNumberOfComponents(1 if data.ndim == 1 else 3)\n            network_vtk.GetCellData().AddArray(data_vtk)\n\n        writer.Update()",
  "def to_gmsh(self, file_name, in_3d=True, **kwargs):\n        \"\"\" Write the fracture network as input for mesh generation by gmsh.\n\n        It is assumed that intersections have been found and processed (e.g. by\n        the methods find_intersection() and split_intersection()).\n\n        Parameters:\n            file_name (str): Path to the .geo file to be written\n            in_3d (boolean, optional): Whether to embed the 2d fracture grids\n               in 3d. If True (default), the mesh will be DFM-style, False will\n               give a DFN-type mesh.\n\n        \"\"\"\n        # Extract geometrical information.\n        p = self.decomposition['points']\n        edges = self.decomposition['edges']\n        poly = self._poly_2_segment()\n        # Obtain tags, and set default values (corresponding to real fractures)\n        # for untagged fractures.\n        frac_tags = self.tags\n        frac_tags['subdomain'] = frac_tags.get('subdomain', []) + \\\n                                [False]*(len(self._fractures)\n                                        -len(frac_tags.get('subdomain', [])))\n        frac_tags['boundary'] = frac_tags.get('boundary', []) + \\\n                                [False]*(len(self._fractures)\n                                        -len(frac_tags.get('boundary', [])))\n\n        edge_tags, intersection_points = self._classify_edges(poly)\n\n        # All intersection lines and points on boundaries are non-physical in 3d.\n        # I.e., they are assigned boundary conditions, but are not gridded. Hence:\n        # Remove the points and edges at the boundary\n        point_tags, edge_tags = self.on_domain_boundary(edges, edge_tags)\n        edges = np.vstack((self.decomposition['edges'], edge_tags))\n        int_pts_on_boundary = np.isin(intersection_points, np.where(point_tags))\n        intersection_points = intersection_points[np.logical_not(int_pts_on_boundary)]\n        self.zero_d_pt = intersection_points\n\n        # Obtain mesh size parameters\n        if 'mesh_size' in kwargs.keys():\n            # Legacy option, this should be removed.\n            print('Using old version of mesh size determination')\n            mesh_size, mesh_size_bound = \\\n                self._determine_mesh_size(**kwargs['mesh_size'])\n        else:\n            mesh_size, mesh_size_bound = self._determine_mesh_size()\n\n        # The tolerance applied in gmsh should be consistent with the tolerance\n        # used in the splitting of the fracture network. The documentation of\n        # gmsh is not clear, but it seems gmsh scales a given tolerance with\n        # the size of the domain - presumably by the largest dimension. To\n        # counteract this, we divide our (absolute) tolerance self.tol with the\n        # domain size.\n        if in_3d:\n            dx = np.array([[self.domain['xmax'] - self.domain['xmin']],\n                           [self.domain['ymax'] - self.domain['ymin']],\n                           [self.domain['zmax'] - self.domain['zmin']]])\n            gmsh_tolerance = self.tol / dx.max()\n        else:\n            gmsh_tolerance = self.tol\n\n        if 'mesh_size' in kwargs.keys():\n            meshing_algorithm = kwargs['mesh_size'].get('meshing_algorithm', None)\n        else:\n            meshing_algorithm = None\n\n        # Initialize and run the gmsh writer:\n        if in_3d:\n            dom = self.domain\n        else:\n            dom = None\n        writer = GmshWriter(p, edges, polygons=poly, domain=dom,\n                            intersection_points=intersection_points,\n                            mesh_size_bound=mesh_size_bound,\n                            mesh_size=mesh_size, tolerance=gmsh_tolerance,\n                            edges_2_frac=self.decomposition['line_in_frac'],\n                            meshing_algorithm=meshing_algorithm,\n                            fracture_tags=frac_tags)\n\n        writer.write_geo(file_name)",
  "def fracture_to_plane(self, frac_num):\n        \"\"\" Project fracture vertexes and intersection points to the natural\n        plane of the fracture.\n\n        Parameters:\n            frac_num (int): Index of fracture.\n\n        Returns:\n            np.ndarray (2xn_pt): 2d coordinates of the fracture vertexes.\n            np.ndarray (2xn_isect): 2d coordinates of fracture intersection\n                points.\n            np.ndarray: Index of intersecting fractures.\n            np.ndarray, 3x3: Rotation matrix into the natural plane.\n            np.ndarray, 3x1. 3d coordinates of the fracture center.\n\n            The 3d coordinates of the frature can be recovered by\n                p_3d = cp + rot.T.dot(np.vstack((p_2d,\n                                                 np.zeros(p_2d.shape[1]))))\n\n        \"\"\"\n        isect = self.intersections_of_fracture(frac_num)\n\n        frac = self._fractures[frac_num]\n        cp = frac.center.reshape((-1, 1))\n\n        rot = cg.project_plane_matrix(frac.p)\n\n        def rot_translate(pts):\n            # Convenience method to translate and rotate a point.\n            return rot.dot(pts - cp)\n\n        p = rot_translate(frac.p)\n        assert np.max(np.abs(p[2])) < self.tol, str(np.max(np.abs(p[2]))) + \\\n                \" \" + str(self.tol)\n        p_2d = p[:2]\n\n        # Intersection points, in 2d coordinates\n        ip = np.empty((2, 0))\n\n        other_frac = np.empty(0, dtype=np.int)\n\n        for i in isect:\n            if i.first.index == frac_num:\n                other_frac = np.append(other_frac, i.second.index)\n            else:\n                other_frac = np.append(other_frac, i.first.index)\n\n            tmp_p = rot_translate(i.coord)\n            if tmp_p.shape[1] > 0:\n                assert np.max(np.abs(tmp_p[2])) < self.tol\n                ip = np.append(ip, tmp_p[:2], axis=1)\n\n        return p_2d, ip, other_frac, rot, cp",
  "def on_domain_boundary(self, edges, edge_tags):\n        \"\"\"\n        Finds edges and points on boundary, to avoid that these\n        are gridded. Points  introduced by intersections\n        of subdomain boundaries and real fractures remain physical\n        (to maintain contact between split fracture lines).\n        \"\"\"\n\n        constants = GmshConstants()\n        # Obtain current tags on fractures\n        boundary_polygons = np.where(self.tags.get('boundary',\n                                [False]*len(self._fractures)))[0]\n        subdomain_polygons = np.where(self.tags.get('subdomain'\n                                [False]*len(self._fractures)))[0]\n        # ... on the points...\n        point_tags = np.zeros(self.decomposition['points'].shape[1])\n        # and the mapping between fractures and edges.\n        edges_2_frac = self.decomposition['edges_2_frac']\n\n        # Loop on edges to tag according to following rules:\n        #     Edge is on the boundary:\n        #         Tag it and the points it consists of as boundary entities.\n        #     Edge is caused by the presence of subdomain boundaries\n        #     (Note: one fracture may still be involved):\n        #         Tag it as auxiliary\n        for e, e2f in enumerate(edges_2_frac):\n            if any(np.in1d(e2f, boundary_polygons)):\n                edge_tags[e] = constants.DOMAIN_BOUNDARY_TAG\n                point_tags[edges[:,e]] = constants.DOMAIN_BOUNDARY_TAG\n                continue\n            subdomain_parents = np.in1d(e2f, subdomain_polygons)\n            if any(subdomain_parents) and e2f.size - sum(subdomain_parents) < 2:\n                # Intersection of at most one fracture with one (or more)\n                # subdomain boundaries.\n                edge_tags[e] = constants.AUXILIARY_TAG\n\n        # Tag points caused solely by subdomain plane intersection as\n        # auxiliary. Note that points involving at least one real fracture\n        # are kept as physical fracture points. This is to avoid connectivity\n        # loss along real fracture intersections.\n        for p in np.unique(edges):\n            es = np.where(edges==p)[1]\n            sds = []\n            for e in es:\n                sds.append(edges_2_frac[e])\n            sds = np.unique(np.concatenate(sds))\n            subdomain_parents = np.in1d(sds, subdomain_polygons)\n            if all(subdomain_parents) and sum(subdomain_parents) > 2:\n                point_tags[p] = constants.AUXILIARY_TAG\n\n        return point_tags, edge_tags",
  "def point_on_segment(ip, poly, need_two=True):\n            # Check if a set of points are located on a single segment\n            if need_two and ip.shape[1] < 2 or\\\n                ((not need_two) and ip.shape[1] < 1):\n                return False\n            start = poly\n            end = np.roll(poly, 1, axis=1)\n            for si in range(start.shape[1]):\n                dist, cp = cg.dist_points_segments(ip, start[:, si],\n                                                   end[:, si])\n                if np.all(dist < tol):\n                    return True\n            return False",
  "def outside_box(p, bound_i):\n            # Helper function to test if points are outside the bounding box\n            relative = np.amax(np.linalg.norm(p, axis=0))\n            p = cg.snap_to_grid(p, tol=tol/relative)\n            # snap_to_grid will impose a grid of size self.tol, thus points\n            # that are more than half that distance away from the boundary\n            # are deemed outside.\n            # To reduce if-else on the boundary index, we compute all bounds,\n            # and then return the relevant one, as specified by bound_i\n            west_of = p[0] < x0_box - tol / 2\n            east_of = p[0] > x1_box + tol / 2\n            south_of = p[1] < y0_box - tol / 2\n            north_of = p[1] > y1_box + tol / 2\n            beneath = p[2] < z0_box - tol / 2\n            above = p[2] > z1_box + tol / 2\n            outside = np.vstack((west_of, east_of, south_of, north_of,\n                                 beneath, above))\n            return outside[bound_i]",
  "def dist_p(a, b):\n            a = a.reshape((-1, 1))\n            b = b.reshape((-1, 1))\n            d = b - a\n            return np.sqrt(np.sum(d**2))",
  "def rot_translate(pts):\n            # Convenience method to translate and rotate a point.\n            return rot.dot(pts - cp)"
]