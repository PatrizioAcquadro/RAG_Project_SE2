[
  "class TestBokehJS(unittest.TestCase):\n\n    def test_bokehjs(self):\n        os.chdir('bokehjs')\n        proc = subprocess.Popen([\"grunt\"])\n        self.assertEqual(proc.wait(), 0)",
  "def test_bokehjs(self):\n        os.chdir('bokehjs')\n        proc = subprocess.Popen([\"grunt\"])\n        self.assertEqual(proc.wait(), 0)",
  "def run_command(args, cwd=None, verbose=False):\n    try:\n        # remember shell=False, so use git.cmd on windows, not just git\n        p = subprocess.Popen(args, stdout=subprocess.PIPE, cwd=cwd)\n    except EnvironmentError:\n        e = sys.exc_info()[1]\n        if verbose:\n            print(\"unable to run %s\" % args[0])\n            print(e)\n        return None\n    stdout = p.communicate()[0].strip()\n    if sys.version >= '3':\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(\"unable to run %s (error)\" % args[0])\n        return None\n    return stdout",
  "def get_expanded_variables(versionfile_source):\n    # the code embedded in _version.py can just fetch the value of these\n    # variables. When used from setup.py, we don't want to import\n    # _version.py, so we do it with a regexp instead. This function is not\n    # used from _version.py.\n    variables = {}\n    try:\n        for line in open(versionfile_source,\"r\").readlines():\n            if line.strip().startswith(\"git_refnames =\"):\n                mo = re.search(r'=\\s*\"(.*)\"', line)\n                if mo:\n                    variables[\"refnames\"] = mo.group(1)\n            if line.strip().startswith(\"git_full =\"):\n                mo = re.search(r'=\\s*\"(.*)\"', line)\n                if mo:\n                    variables[\"full\"] = mo.group(1)\n    except EnvironmentError:\n        pass\n    return variables",
  "def versions_from_expanded_variables(variables, tag_prefix, verbose=False):\n    refnames = variables[\"refnames\"].strip()\n    if refnames.startswith(\"$Format\"):\n        if verbose:\n            print(\"variables are unexpanded, not using\")\n        return {} # unexpanded, so not in an unpacked git-archive tarball\n    refs = set([r.strip() for r in refnames.strip(\"()\").split(\",\")])\n    for ref in list(refs):\n        if not re.search(r'\\d', ref):\n            if verbose:\n                print(\"discarding '%s', no digits\" % ref)\n            refs.discard(ref)\n            # Assume all version tags have a digit. git's %d expansion\n            # behaves like git log --decorate=short and strips out the\n            # refs/heads/ and refs/tags/ prefixes that would let us\n            # distinguish between branches and tags. By ignoring refnames\n            # without digits, we filter out many common branch names like\n            # \"release\" and \"stabilization\", as well as \"HEAD\" and \"master\".\n    if verbose:\n        print(\"remaining refs: %s\" % \",\".join(sorted(refs)))\n    for ref in sorted(refs):\n        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(\"picking %s\" % r)\n            return { \"version\": r,\n                     \"full\": variables[\"full\"].strip() }\n    # no suitable tags, so we use the full revision id\n    if verbose:\n        print(\"no suitable tags, using full revision id\")\n    return { \"version\": variables[\"full\"].strip(),\n             \"full\": variables[\"full\"].strip() }",
  "def versions_from_vcs(tag_prefix, versionfile_source, verbose=False):\n    # this runs 'git' from the root of the source tree. That either means\n    # someone ran a setup.py command (and this code is in versioneer.py, so\n    # IN_LONG_VERSION_PY=False, thus the containing directory is the root of\n    # the source tree), or someone ran a project-specific entry point (and\n    # this code is in _version.py, so IN_LONG_VERSION_PY=True, thus the\n    # containing directory is somewhere deeper in the source tree). This only\n    # gets called if the git-archive 'subst' variables were *not* expanded,\n    # and _version.py hasn't already been rewritten with a short version\n    # string, meaning we're inside a checked out source tree.\n\n    try:\n        here = os.path.realpath(__file__)\n    except NameError:\n        # some py2exe/bbfreeze/non-CPython implementations don't do __file__\n        return {} # not always correct\n\n    # versionfile_source is the relative path from the top of the source tree\n    # (where the .git directory might live) to this file. Invert this to find\n    # the root from __file__.\n    root = here\n    if IN_LONG_VERSION_PY:\n        for i in range(len(versionfile_source.split(\"/\"))):\n            root = os.path.dirname(root)\n    else:\n        root = os.path.dirname(here)\n    if not os.path.exists(os.path.join(root, \".git\")):\n        if verbose:\n            print(\"no .git in %s\" % root)\n        return {}\n\n    stdout = run_command([GIT, \"describe\", \"--tags\", \"--dirty\", \"--always\"],\n                         cwd=root)\n    if stdout is None:\n        return {}\n    if not stdout.startswith(tag_prefix):\n        if verbose:\n            print(\"tag '%s' doesn't start with prefix '%s'\" % (stdout, tag_prefix))\n        return {}\n    tag = stdout[len(tag_prefix):]\n    stdout = run_command([GIT, \"rev-parse\", \"HEAD\"], cwd=root)\n    if stdout is None:\n        return {}\n    full = stdout.strip()\n    if tag.endswith(\"-dirty\"):\n        full += \"-dirty\"\n\n    # accomodate to our devel build process\n    try:\n        from bokeh.__conda_version__ import conda_version\n        tag = conda_version.replace(\"'\",\"\")\n        del conda_version\n    except ImportError:\n        pass\n\n    return {\"version\": tag, \"full\": full}",
  "def versions_from_parentdir(parentdir_prefix, versionfile_source, verbose=False):\n    if IN_LONG_VERSION_PY:\n        # We're running from _version.py. If it's from a source tree\n        # (execute-in-place), we can work upwards to find the root of the\n        # tree, and then check the parent directory for a version string. If\n        # it's in an installed application, there's no hope.\n        try:\n            here = os.path.realpath(__file__)\n        except NameError:\n            # py2exe/bbfreeze/non-CPython don't have __file__\n            return {} # without __file__, we have no hope\n        # versionfile_source is the relative path from the top of the source\n        # tree to _version.py. Invert this to find the root from __file__.\n        root = here\n        for i in range(len(versionfile_source.split(\"/\"))):\n            root = os.path.dirname(root)\n    else:\n        # we're running from versioneer.py, which means we're running from\n        # the setup.py in a source tree. sys.argv[0] is setup.py in the root.\n        here = os.path.realpath(sys.argv[0])\n        root = os.path.dirname(here)\n\n    # Source tarballs conventionally unpack into a directory that includes\n    # both the project name and a version string.\n    dirname = os.path.basename(root)\n    if not dirname.startswith(parentdir_prefix):\n        if verbose:\n            print(\"guessing rootdir is '%s', but '%s' doesn't start with prefix '%s'\" %\n                  (root, dirname, parentdir_prefix))\n        return None\n    return {\"version\": dirname[len(parentdir_prefix):], \"full\": \"\"}",
  "def do_vcs_install(versionfile_source, ipy):\n    run_command([GIT, \"add\", \"versioneer.py\"])\n    run_command([GIT, \"add\", versionfile_source])\n    run_command([GIT, \"add\", ipy])\n    present = False\n    try:\n        f = open(\".gitattributes\", \"r\")\n        for line in f.readlines():\n            if line.strip().startswith(versionfile_source):\n                if \"export-subst\" in line.strip().split()[1:]:\n                    present = True\n        f.close()\n    except EnvironmentError:\n        pass\n    if not present:\n        f = open(\".gitattributes\", \"a+\")\n        f.write(\"%s export-subst\\n\" % versionfile_source)\n        f.close()\n        run_command([GIT, \"add\", \".gitattributes\"])",
  "def versions_from_file(filename):\n    versions = {}\n    try:\n        f = open(filename)\n    except EnvironmentError:\n        return versions\n    for line in f.readlines():\n        mo = re.match(\"version_version = '([^']+)'\", line)\n        if mo:\n            versions[\"version\"] = mo.group(1)\n        mo = re.match(\"version_full = '([^']+)'\", line)\n        if mo:\n            versions[\"full\"] = mo.group(1)\n    return versions",
  "def write_to_version_file(filename, versions):\n    f = open(filename, \"w\")\n    f.write(SHORT_VERSION_PY % versions)\n    f.close()\n    print(\"set %s to '%s'\" % (filename, versions[\"version\"]))",
  "def get_best_versions(versionfile, tag_prefix, parentdir_prefix,\n                      default=DEFAULT, verbose=False):\n    # returns dict with two keys: 'version' and 'full'\n    #\n    # extract version from first of _version.py, 'git describe', parentdir.\n    # This is meant to work for developers using a source checkout, for users\n    # of a tarball created by 'setup.py sdist', and for users of a\n    # tarball/zipball created by 'git archive' or github's download-from-tag\n    # feature.\n\n    variables = get_expanded_variables(versionfile_source)\n    if variables:\n        ver = versions_from_expanded_variables(variables, tag_prefix)\n        if ver:\n            if verbose: print(\"got version from expanded variable %s\" % ver)\n            return ver\n\n    ver = versions_from_file(versionfile)\n    if ver:\n        if verbose: print(\"got version from file %s %s\" % (versionfile, ver))\n        return ver\n\n    ver = versions_from_vcs(tag_prefix, versionfile_source, verbose)\n    if ver:\n        if verbose: print(\"got version from git %s\" % ver)\n        return ver\n\n    ver = versions_from_parentdir(parentdir_prefix, versionfile_source, verbose)\n    if ver:\n        if verbose: print(\"got version from parentdir %s\" % ver)\n        return ver\n\n    if verbose: print(\"got version from default %s\" % ver)\n    return default",
  "def get_versions(default=DEFAULT, verbose=False):\n    assert versionfile_source is not None, \"please set versioneer.versionfile_source\"\n    assert tag_prefix is not None, \"please set versioneer.tag_prefix\"\n    assert parentdir_prefix is not None, \"please set versioneer.parentdir_prefix\"\n    return get_best_versions(versionfile_source, tag_prefix, parentdir_prefix,\n                             default=default, verbose=verbose)",
  "def get_version(verbose=False):\n    return get_versions(verbose=verbose)[\"version\"]",
  "class cmd_version(Command):\n    description = \"report generated version string\"\n    user_options = []\n    boolean_options = []\n    def initialize_options(self):\n        pass\n    def finalize_options(self):\n        pass\n    def run(self):\n        ver = get_version(verbose=True)\n        print(\"Version is currently: %s\" % ver)",
  "class cmd_build(_build):\n    def run(self):\n        versions = get_versions(verbose=True)\n        _build.run(self)\n        # now locate _version.py in the new build/ directory and replace it\n        # with an updated value\n        target_versionfile = os.path.join(self.build_lib, versionfile_build)\n        print(\"UPDATING %s\" % target_versionfile)\n        os.unlink(target_versionfile)\n        f = open(target_versionfile, \"w\")\n        f.write(SHORT_VERSION_PY % versions)\n        f.close()",
  "class cmd_sdist(_sdist):\n    def run(self):\n        versions = get_versions(verbose=True)\n        self._versioneer_generated_versions = versions\n        # unless we update this, the command will keep using the old version\n        self.distribution.metadata.version = versions[\"version\"]\n        return _sdist.run(self)\n\n    def make_release_tree(self, base_dir, files):\n        _sdist.make_release_tree(self, base_dir, files)\n        # now locate _version.py in the new base_dir directory (remembering\n        # that it may be a hardlink) and replace it with an updated value\n        target_versionfile = os.path.join(base_dir, versionfile_source)\n        print(\"UPDATING %s\" % target_versionfile)\n        os.unlink(target_versionfile)\n        f = open(target_versionfile, \"w\")\n        f.write(SHORT_VERSION_PY % self._versioneer_generated_versions)\n        f.close()",
  "class cmd_update_files(Command):\n    description = \"modify __init__.py and create _version.py\"\n    user_options = []\n    boolean_options = []\n    def initialize_options(self):\n        pass\n    def finalize_options(self):\n        pass\n    def run(self):\n        ipy = os.path.join(os.path.dirname(versionfile_source), \"__init__.py\")\n        print(\" creating %s\" % versionfile_source)\n        f = open(versionfile_source, \"w\")\n        f.write(LONG_VERSION_PY % {\"DOLLAR\": \"$\",\n                                   \"TAG_PREFIX\": tag_prefix,\n                                   \"PARENTDIR_PREFIX\": parentdir_prefix,\n                                   \"VERSIONFILE_SOURCE\": versionfile_source,\n                                   })\n        f.close()\n        try:\n            old = open(ipy, \"r\").read()\n        except EnvironmentError:\n            old = \"\"\n        if INIT_PY_SNIPPET not in old:\n            print(\" appending to %s\" % ipy)\n            f = open(ipy, \"a\")\n            f.write(INIT_PY_SNIPPET)\n            f.close()\n        else:\n            print(\" %s unmodified\" % ipy)\n        do_vcs_install(versionfile_source, ipy)",
  "def get_cmdclass():\n    return {'version': cmd_version,\n            'update_files': cmd_update_files,\n            'build': cmd_build,\n            'sdist': cmd_sdist,\n            }",
  "def initialize_options(self):\n        pass",
  "def finalize_options(self):\n        pass",
  "def run(self):\n        ver = get_version(verbose=True)\n        print(\"Version is currently: %s\" % ver)",
  "def run(self):\n        versions = get_versions(verbose=True)\n        _build.run(self)\n        # now locate _version.py in the new build/ directory and replace it\n        # with an updated value\n        target_versionfile = os.path.join(self.build_lib, versionfile_build)\n        print(\"UPDATING %s\" % target_versionfile)\n        os.unlink(target_versionfile)\n        f = open(target_versionfile, \"w\")\n        f.write(SHORT_VERSION_PY % versions)\n        f.close()",
  "def run(self):\n        versions = get_versions(verbose=True)\n        self._versioneer_generated_versions = versions\n        # unless we update this, the command will keep using the old version\n        self.distribution.metadata.version = versions[\"version\"]\n        return _sdist.run(self)",
  "def make_release_tree(self, base_dir, files):\n        _sdist.make_release_tree(self, base_dir, files)\n        # now locate _version.py in the new base_dir directory (remembering\n        # that it may be a hardlink) and replace it with an updated value\n        target_versionfile = os.path.join(base_dir, versionfile_source)\n        print(\"UPDATING %s\" % target_versionfile)\n        os.unlink(target_versionfile)\n        f = open(target_versionfile, \"w\")\n        f.write(SHORT_VERSION_PY % self._versioneer_generated_versions)\n        f.close()",
  "def initialize_options(self):\n        pass",
  "def finalize_options(self):\n        pass",
  "def run(self):\n        ipy = os.path.join(os.path.dirname(versionfile_source), \"__init__.py\")\n        print(\" creating %s\" % versionfile_source)\n        f = open(versionfile_source, \"w\")\n        f.write(LONG_VERSION_PY % {\"DOLLAR\": \"$\",\n                                   \"TAG_PREFIX\": tag_prefix,\n                                   \"PARENTDIR_PREFIX\": parentdir_prefix,\n                                   \"VERSIONFILE_SOURCE\": versionfile_source,\n                                   })\n        f.close()\n        try:\n            old = open(ipy, \"r\").read()\n        except EnvironmentError:\n            old = \"\"\n        if INIT_PY_SNIPPET not in old:\n            print(\" appending to %s\" % ipy)\n            f = open(ipy, \"a\")\n            f.write(INIT_PY_SNIPPET)\n            f.close()\n        else:\n            print(\" %s unmodified\" % ipy)\n        do_vcs_install(versionfile_source, ipy)",
  "def build_parser():\n    parser = argparse.ArgumentParser(description=\"start bokeh websocket\")\n    parser.add_argument(\"--url-prefix\",\n                        help=\"url prefix\",\n                        type=str,\n                        default=None\n                        )\n    parser.add_argument(\"--zmqaddr\",\n                        help=\"zmq url\",\n                        action='append'\n    )\n    parser.add_argument(\"--ws-port\",\n                        help=\"port for websocket worker\",\n                        default=5007,\n                        type=int\n    )\n    return parser",
  "def run_args(args):\n    #dont' know how to do default args with append and argparse\n    if args.zmqaddr is None:\n        args.zmqaddr = [\"tcp://127.0.0.1:5007\"]\n    app = make_app(args.url_prefix, args.zmqaddr, args.ws_port)\n    try:\n        app.start()\n    finally:\n        app.stop()",
  "def package_path(path, filters=()):\n    if not os.path.exists(path):\n        raise RuntimeError(\"packaging non-existent path: %s\" % path)\n    elif os.path.isfile(path):\n        package_data.append(relpath(path, 'bokeh'))\n    else:\n        for path, dirs, files in os.walk(path):\n            path = relpath(path, 'bokeh')\n            for f in files:\n                if not filters or f.endswith(filters):\n                    package_data.append(join(path, f))",
  "def getsitepackages():\n    \"\"\"Returns a list containing all global site-packages directories\n    (and possibly site-python).\"\"\"\n\n    _is_64bit = (getattr(sys, 'maxsize', None) or getattr(sys, 'maxint')) > 2**32\n    _is_pypy = hasattr(sys, 'pypy_version_info')\n    _is_jython = sys.platform[:4] == 'java'\n\n    prefixes = [sys.prefix, sys.exec_prefix]\n\n    sitepackages = []\n    seen = set()\n\n    for prefix in prefixes:\n        if not prefix or prefix in seen:\n            continue\n        seen.add(prefix)\n\n        if sys.platform in ('os2emx', 'riscos') or _is_jython:\n            sitedirs = [os.path.join(prefix, \"Lib\", \"site-packages\")]\n        elif _is_pypy:\n            sitedirs = [os.path.join(prefix, 'site-packages')]\n        elif sys.platform == 'darwin' and prefix == sys.prefix:\n            if prefix.startswith(\"/System/Library/Frameworks/\"): # Apple's Python\n                sitedirs = [os.path.join(\"/Library/Python\", sys.version[:3], \"site-packages\"),\n                            os.path.join(prefix, \"Extras\", \"lib\", \"python\")]\n\n            else:  # any other Python distros on OSX work this way\n                sitedirs = [os.path.join(prefix, \"lib\",\n                            \"python\" + sys.version[:3], \"site-packages\")]\n\n        elif os.sep == '/':\n            sitedirs = [os.path.join(prefix,\n                                     \"lib\",\n                                     \"python\" + sys.version[:3],\n                                     \"site-packages\"),\n                        os.path.join(prefix, \"lib\", \"site-python\"),\n                        os.path.join(prefix, \"python\" + sys.version[:3], \"lib-dynload\")]\n            lib64_dir = os.path.join(prefix, \"lib64\", \"python\" + sys.version[:3], \"site-packages\")\n            if (os.path.exists(lib64_dir) and\n                os.path.realpath(lib64_dir) not in [os.path.realpath(p) for p in sitedirs]):\n                if _is_64bit:\n                    sitedirs.insert(0, lib64_dir)\n                else:\n                    sitedirs.append(lib64_dir)\n            try:\n                # sys.getobjects only available in --with-pydebug build\n                sys.getobjects\n                sitedirs.insert(0, os.path.join(sitedirs[0], 'debug'))\n            except AttributeError:\n                pass\n            # Debian-specific dist-packages directories:\n            if sys.version[0] == '2':\n                sitedirs.append(os.path.join(prefix, \"lib\",\n                                             \"python\" + sys.version[:3],\n                                             \"dist-packages\"))\n            else:\n                sitedirs.append(os.path.join(prefix, \"lib\",\n                                             \"python\" + sys.version[0],\n                                             \"dist-packages\"))\n            sitedirs.append(os.path.join(prefix, \"local/lib\",\n                                         \"python\" + sys.version[:3],\n                                         \"dist-packages\"))\n            sitedirs.append(os.path.join(prefix, \"lib\", \"dist-python\"))\n        else:\n            sitedirs = [prefix, os.path.join(prefix, \"lib\", \"site-packages\")]\n        if sys.platform == 'darwin':\n            # for framework builds *only* we add the standard Apple\n            # locations. Currently only per-user, but /Library and\n            # /Network/Library could be added too\n            if 'Python.framework' in prefix:\n                home = os.environ.get('HOME')\n                if home:\n                    sitedirs.append(\n                        os.path.join(home,\n                                     'Library',\n                                     'Python',\n                                     sys.version[:3],\n                                     'site-packages'))\n        for sitedir in sitedirs:\n            sitepackages.append(os.path.abspath(sitedir))\n    return sitepackages",
  "def check_remove_bokeh_install(site_packages):\n    bokeh_path = join(site_packages, \"bokeh\")\n    if not (exists(bokeh_path) and isdir(bokeh_path)):\n        return\n    prompt = \"Found existing bokeh install: %s\\nRemove it? [y|N] \" % bokeh_path\n    val = input(prompt)\n    if val == \"y\":\n        print (\"Removing old bokeh install...\", end=\" \")\n        try:\n            shutil.rmtree(bokeh_path)\n            print (\"Done\")\n        except (IOError, OSError):\n            print (\"Unable to remove old bokeh at %s, exiting\" % bokeh_path)\n            sys.exit(-1)\n    else:\n        print (\"Not removing old bokeh install\")\n        sys.exit(1)",
  "def remove_bokeh_pth(path_file):\n    if exists(path_file):\n        try:\n            os.remove(path_file)\n        except (IOError, OSError):\n            print (\"Unable to remove old path file at %s, exiting\" % path_file)\n            sys.exit(-1)\n        return True\n    return False",
  "def build_js():\n    print(\"Building BokehJS...\")\n    os.chdir('bokehjs')\n\n    if sys.platform != \"win32\":\n        cmd = [join('node_modules', '.bin', 'grunt'), 'deploy']\n    else:\n        cmd = [join('node_modules', '.bin', 'grunt.cmd'), 'deploy']\n\n    try:\n        proc = subprocess.Popen(cmd)\n    except OSError:\n        print(\"\"\"\nFailed to build BokehJS.\n\nHave you run `npm install` from the bokehjs subdirectory?\n  Dev Guide: http://bokeh.pydata.org/docs/dev_guide.html#bokehjs.\n\"\"\")\n        sys.exit(1)\n    finally:\n        os.chdir('..')\n\n    if proc.wait() != 0:\n        print(\"ERROR: could not build BokehJS\")\n        sys.exit(1)",
  "def install_js():\n    target_jsdir = join(SERVER, 'static', 'js')\n    target_cssdir = join(SERVER, 'static', 'css')\n\n    if ( not exists(join(JS, 'bokeh.js')) or\n         not exists(join(JS, 'bokeh.min.js')) or\n         not exists(join(CSS, 'bokeh.css')) or\n         not exists(join(CSS, 'bokeh.min.css'))):\n        print(\"\"\"\nERROR: Cannot install BokehJS: files missing in `./bokehjs/build`.\n\n\nPlease build BokehJS by running setup.py with the `--build_js` option.\n  Dev Guide: http://bokeh.pydata.org/docs/dev_guide.html#bokehjs.\n\"\"\")\n        sys.exit(1)\n\n    if exists(target_jsdir):\n        shutil.rmtree(target_jsdir)\n    shutil.copytree(JS, target_jsdir)\n\n    if exists(target_cssdir):\n        shutil.rmtree(target_cssdir)\n    shutil.copytree(CSS, target_cssdir)",
  "def clean():\n    print(\"Removing prior-built items...\", end=\" \")\n    dir_util.remove_tree('build/lib/bokeh')\n    print(\"Done\")",
  "def get_user_jsargs():\n    print(\"\"\"\nBokeh includes a JavaScript library (BokehJS) that has its own\nbuild process. How would you like to handle BokehJS:\n\n1) build and install fresh BokehJS\n2) install last built BokehJS from bokeh/bokehjs/build\n\"\"\")\n    mapping = {\"1\": True, \"2\": False}\n    value = input(\"Choice? \")\n    while value not in mapping:\n        print(\"Input '%s' not understood. Valid choices: 1, 2\\n\" % value)\n        value = input(\"Choice? \")\n    return mapping[value]",
  "def parse_jsargs():\n    options = ('install', 'develop', 'sdist', 'egg_info', 'build')\n    installing = any(arg in sys.argv for arg in options)\n\n    if '--build_js' in sys.argv:\n        if not installing:\n            print(\"Error: Option '--build_js' only valid with 'install', 'develop' or 'sdist', exiting.\")\n            sys.exit(1)\n        jsbuild = True\n        sys.argv.remove('--build_js')\n\n    elif '--install_js' in sys.argv:\n        if not installing:\n            print(\"Error: Option '--install_js' only valid with 'install', 'develop' or 'sdist', exiting.\")\n            sys.exit(1)\n        jsbuild = False\n        sys.argv.remove('--install_js')\n\n    else:\n        if installing:\n            jsbuild = get_user_jsargs()\n        else:\n            jsbuild = False\n\n    return jsbuild",
  "def deploy():\n\n    v = conf.version\n\n    # make a backup of the old directory\n    run(\"rm -rf /www/bokeh/en/%s.bak\" % v)\n    run(\"mkdir -p /www/bokeh/en/%s\" % v)\n    run(\"cp -ar /www/bokeh/en/%s /www/bokeh/en/%s.bak\" % (v, v))\n\n    # switch latest symlink to archive docs\n    run(\"rm /www/bokeh/en/latest\")\n    run(\"ln -s /www/bokeh/en/%s.bak /www/bokeh/en/latest\" % v)\n\n    rsync_project(\n        local_dir=\"_build/html/\",\n        remote_dir=\"/www/bokeh/en/%s\" % v,\n        delete=True\n    )\n\n    # set permissions\n    run(\"chmod -R g+w /www/bokeh/en/%s\" % v)\n\n    # switch the current symlink to new docs\n    run(\"rm /www/bokeh/en/latest\")\n    run(\"ln -s /www/bokeh/en/%s /www/bokeh/en/latest\" % v)",
  "def update(v=None):\n\n    # TODO (bev) confirm this version is not the latest\n    if v is None:\n        v = conf.version\n\n    # make a backup of the old directory\n    run(\"rm -rf /www/bokeh/en/%s.bak\" % v)\n    run(\"mkdir -p /www/bokeh/en/%s\" % v)\n    run(\"cp -ar /www/bokeh/en/%s /www/bokeh/en/%s.bak\" % (v, v))\n\n    rsync_project(\n        local_dir=\"_build/html/\",\n        remote_dir=\"/www/bokeh/en/%s\" % v,\n        delete=True\n    )\n\n    # set permissions\n    run(\"chmod -R g+w /www/bokeh/en/%s\" % v)",
  "def welcome():\n    return \"\"\"\n    <h1>Welcome to the Bokeh documentation server</h1>\n    You probably want to go to <a href=\"/en/latest/index.html\"> Index</a>\n    \"\"\"",
  "def send_pic(filename):\n    return flask.send_from_directory(\n        os.path.join(_basedir,\"sphinx/_build/html/\"), filename)",
  "def mandel(x, y, max_iters):\n    \"\"\"\n    Given the real and imaginary parts of a complex number,\n    determine if it is a candidate for membership in the Mandelbrot\n    set given a fixed number of iterations.\n    \"\"\"\n    c = complex(x, y)\n    z = 0.0j\n    for i in range(max_iters):\n        z = z*z + c\n        if (z.real*z.real + z.imag*z.imag) >= 4:\n            return i\n    return max_iters",
  "def create_fractal(min_x, max_x, min_y, max_y, image, iters):\n    height = image.shape[0]\n    width = image.shape[1]\n\n    pixel_size_x = (max_x - min_x) / width\n    pixel_size_y = (max_y - min_y) / height\n\n    for x in range(width):\n        real = min_x + x * pixel_size_x\n        for y in range(height):\n            imag = min_y + y * pixel_size_y\n            color = mandel(real, imag, iters)\n            image[y, x] = color",
  "def stacked(data, categories):\n    ys = []\n    last = np.zeros(len(list(data.values())[0]))\n    for cat in categories:\n        next = last + data[cat]\n        ys.append(np.hstack((last[::-1], next)))\n        last = next\n    return ys",
  "def outliers(group):\n   cat = group.name\n   return group[(group.score > upper.loc[cat][0]) | (group.score < lower.loc[cat][0])]['score']",
  "def outliers(group):\n   cat = group.name\n   return group[(group.score > upper.loc[cat][0]) | (group.score < lower.loc[cat][0])]['score']",
  "def mandel(x, y, max_iters):\n    \"\"\"\n    Given the real and imaginary parts of a complex number,\n    determine if it is a candidate for membership in the Mandelbrot\n    set given a fixed number of iterations.\n    \"\"\"\n    c = complex(x, y)\n    z = 0.0j\n    for i in range(max_iters):\n        z = z*z + c\n        if (z.real*z.real + z.imag*z.imag) >= 4:\n            return i\n    return max_iters",
  "def create_fractal(min_x, max_x, min_y, max_y, image, iters):\n    height = image.shape[0]\n    width = image.shape[1]\n\n    pixel_size_x = (max_x - min_x) / width\n    pixel_size_y = (max_y - min_y) / height\n\n    for x in range(width):\n        real = min_x + x * pixel_size_x\n        for y in range(height):\n            imag = min_y + y * pixel_size_y\n            color = mandel(real, imag, iters)\n            image[y, x] = color",
  "def stacked(data, categories):\n    ys = []\n    last = np.zeros(len(list(data.values())[0]))\n    for cat in categories:\n        next = last + data[cat]\n        ys.append(np.hstack((last[::-1], next)))\n        last = next\n    return ys",
  "def outliers(group):\n   cat = group.name\n   return group[(group.score > upper.loc[cat][0]) | (group.score < lower.loc[cat][0])]['score']",
  "def outliers(group):\n   cat = group.name\n   return group[(group.score > upper.loc[cat][0]) | (group.score < lower.loc[cat][0])]['score']",
  "def call_wrapper(cmd_str, *args, **kw):\n    \"\"\"A wrapper for subprocess.call to support 'dry-run' option\"\"\"\n\n    if OPTIONS[\"dry-run\"]:\n        print(cmd_str)\n        return 0\n    else:\n        return subprocess.call(cmd_str, *args, **kw)",
  "def get_parser():\n    \"\"\"Create the parser that will be used to add arguments to the script.\n    \"\"\"\n\n    parser = argparse.ArgumentParser(description=textwrap.dedent(\"\"\"\n                    Creates and runs tests on conda environments for a given\n                    version of bokeh installed using pip and\n                    conda and including python 2.7 and python 3.4.\n\n                    The --previous ('-p') option takes an earlier version of\n                    bokeh to test against, and for use\n                    in creating environments where bokeh will be updated.\n\n                    The --version (-v) option takes the latest version of bokeh\n                    to test against in enviornments\n                    in which bokeh is updated.\n\n                    By default, all envs created will be deleted when the\n                    script finishes.  You can elect to keep these environments\n                    with the --keep option.\n\n                    Ex: ' python test_matrix.py -v 0.7.1 -p 0.7.0'\n                    \"\"\"), formatter_class=argparse.RawTextHelpFormatter)\n\n    parser.add_argument('-p', '--previous', action='store', default=False,\n                        help='Previous version of bokeh', required=True)\n    parser.add_argument('-v', '--version', action='store', default=False,\n                        help='Version of bokeh to test', required=True)\n    parser.add_argument('--keep', action='store_true', default=False,\n                        help=\"Don't delete conda envs created by this script\")\n    parser.add_argument('--dry-run', action='store_true', default=False,\n                        help=\"\"\"Display commands that will be run in each environment\n                        without executing them.\"\"\")\n    # parser.add_argument('')\n\n    return parser",
  "def cleaner(env_path):\n    \"\"\"Checks that an environment path exists and, if so, deletes it.\n    \"\"\"\n\n    if os.path.exists(env_path):\n        shutil.rmtree(env_path)",
  "def conda_creator(env_name, pkgs):\n    \"\"\"Create a conda environment of a given name containing a given string of pkgs.\n    \"\"\"\n\n    call_wrapper(\"conda create --yes -n %s %s\" % (env_name, pkgs), shell=True)",
  "def bokeh_installer(env_name, install_string):\n    \"\"\"Activate an environment and run its install string to either install or update bokeh using\n    conda or pip.\n    \"\"\"\n\n    command_string = 'source activate %s; %s' % (env_name, install_string)\n\n    result = call_wrapper(command_string, shell=True)\n\n    return result == 0",
  "def version_check(env_name, expected_ver):\n    \"\"\"Check a given environment's version of bokeh.\n    \"\"\"\n\n    command_string = 'source activate %s; python -c \"import sys, bokeh; sys.exit(0 if bokeh.__version__ == %r else 1)\"' % (env_name, expected_ver)\n\n    result = call_wrapper(command_string, shell=True)\n\n    return result == 0",
  "def run_tests(env_name):\n    \"\"\"Run bokeh.test() in a given environment.  Writes results to a tmpfile that will be returned as a string\n    in the event of a failure.\n    \"\"\"\n\n    test_failure = ''\n    file_name  = \"tmpfile.txt\"\n    tmpfile = open(file_name, \"w+\")\n    command_string = 'source activate %s; python -c \"import nose, os, sys, bokeh; bokeh.test(exit=True)\"' % env_name\n\n    result = call_wrapper(command_string, shell=True, stderr=tmpfile)\n\n    tmpfile.close()\n\n    if result != 0:\n        with open(file_name, \"r\") as tmpfile:\n            test_failure = (tmpfile.read())\n\n    os.remove(file_name)\n\n    return result == 0, test_failure",
  "def server_check(env_name):\n    pass",
  "def logger(failure_list):\n    \"\"\"Log items in a list of errors to a logfile.\n    \"\"\"\n\n    logfile = 'logfile'\n    while os.path.exists('%s.txt' % logfile):\n        logfile = logfile.split('-')\n        if len(logfile) == 1:\n            logfile.append('2')\n        else:\n            logfile[1] = str(int(logfile[1]) + 1)\n        logfile = '-'.join(logfile)\n\n    with open('%s.txt' % logfile, 'w') as log:\n        for failure in failure_list:\n            log.write(failure)",
  "def get_parser():\n    \"\"\"Create the parser that will be used to add arguments to the script.\n    \"\"\"\n\n    parser = argparse.ArgumentParser(description=textwrap.dedent(\"\"\"\n                    Tests a selection of .py or .ipynb bokeh example files.\n\n                    The --location option allows you to select a specific examples subdirectory to test all files in,\n                    ignoring __init__.py\n\n                    Location arguments you can choose:\n                        - file\n                        - notebook\n                        - server\n                        - ggplot\n                        - glyphs\n                        - mpl\n                        - pandas\n                        - seaborn\n                    \"\"\"), formatter_class=argparse.RawTextHelpFormatter)\n\n    parser.add_argument('--no-log', action='store_true', dest='nolog', default=False,\n                        help=\"don't save a log of any errors discovered\")\n    parser.add_argument('-l', '--location', action='store', default=False,\n                        help=\"example directory in which you wish to test\")\n\n    return parser",
  "def depend_check(dependency):\n    \"\"\"\n    Make sure a given dependency is installed\n    \"\"\"\n\n    try:\n        importlib.import_module(dependency)\n        found = True\n    except ImportError as e:\n        print(\"%s\\nPlease use conda or pip to install the necessary dependency.\" % (e))\n        found = False\n\n    return found",
  "def main(testing_ground=None):\n    \"\"\"\n    Collect and run .py or .ipynb examples from a set list or given examples directory, ignoring __init__.py\n    User input is collected to determine a properly or improperly displayed page\n\n    \"\"\"\n\n    # Create a testing directory if one does not exist, then cd into it\n\n    testing_directory = 'tmp_test'\n\n    if not os.path.exists(testing_directory):\n        os.mkdir(testing_directory)\n\n    os.chdir(testing_directory)\n\n    if testing_ground:\n        log_name = results.location\n\n        TestFiles = [\n            fileName for fileName in os.listdir('%s/.' % testing_ground)\n            if fileName.endswith(('.py', '.ipynb')) and fileName != '__init__.py'\n        ]\n\n    else:\n        log_name = \"fast\"\n\n        TestFiles = DEFAULT_TEST_FILES\n\n    Log = []\n\n    for index, fileName in enumerate(TestFiles):\n        if testing_ground:\n            fileName = \"%s/%s\" % (testing_ground, fileName)\n        try:\n            command = get_cmd(fileName)\n            opener(fileName, command)\n\n            if results.nolog:\n                # Don't display 'next file' message after opening final file in a dir\n                if index != len(TestFiles)-1:\n                    input(\"\\nPress enter to open next file \")\n            else:\n                ErrorReport = test_status()\n                if ErrorReport:\n                    Log.append(\"\\n\\n%s: \\n %s\" % (fileName, ErrorReport))\n\n        except (KeyboardInterrupt, EOFError):\n            break\n\n    # exit the testing directory and delete it\n\n    os.chdir('../')\n    rmtree(testing_directory)\n\n    if Log:\n        logger(Log, log_name)",
  "def get_cmd(some_file):\n    \"\"\"Determines how to open a file depending\n    on whether it is a .py or a .ipynb file\n    \"\"\"\n\n    if some_file.endswith('.py'):\n        command = \"python\"\n    elif some_file.endswith('.ipynb'):\n        command = \"ipython notebook\"\n\n    return command",
  "def opener(some_file, command):\n    \"\"\"Print to screen what file is being opened and then open the file using\n    the command method provided.\n    \"\"\"\n\n    print(\"\\nOpening %s\\n\" % some_file.strip('../'))\n    os.system(\"%s %s\" % (command, some_file))",
  "def test_status():\n    \"\"\"Collect user input to determine if a file displayed correctly or incorrectly.\n    In the case of incorrectly displayed plots, an 'ErrorReport' string is returned.\n    \"\"\"\n\n    status = input(\"Did the plot(s) display correctly? (y/n) \")\n    while not status.startswith(('y', 'n')):\n        print(\"\")\n        status = input(\"Unexpected answer. Please type y or n. \")\n    if status.startswith('n'):\n        ErrorReport = input(\"Please describe the problem: \")\n        return ErrorReport",
  "def logger(error_array, name):\n    \"\"\"\n    Log errors by appending to a .txt file.  The name and directory the file is saved into\n    is provided by the name and log_dir args.\n    \"\"\"\n\n    logfile = \"%s_examples_testlog.txt\" % name\n    if os.path.exists(logfile):\n        os.remove(logfile)\n\n    with open(logfile, 'a') as f:\n        print(\"\")\n        print(\"\\nWriting error log to %s\" % logfile)\n        for error in error_array:\n            f.write(\"%s\\n\" % error)",
  "def get_version_from_git():\n    cmd = [\"git\", \"describe\", \"--tags\", \"--always\"]\n\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE)\n    code = proc.wait()\n\n    if code != 0:\n        print(\"Failed to run: %s\" % \" \".join(cmd))\n        sys.exit(1)\n\n    version = proc.stdout.read().decode('utf-8').strip()\n\n    try:\n        vers, mod = version.split(\"-\")[:2]\n        if not mod.startswith(('rc', 'dev')):\n            mod = \"\"\n    except ValueError:\n        vers, mod = version, \"\"\n\n    return vers, mod",
  "def exercise_lines(path):\n    \"\"\"Yields lines of the given exercise file which are exercise comments.\"\"\"\n    with open(path) as fin:\n        within_exercise = False\n        for line, line_number in zip(fin, count(1)):\n            line = line.lstrip()\n\n            if within_exercise and line.startswith('#'):\n                yield line_number\n            elif not within_exercise and line.startswith('#') and 'EXERCISE:' in line:\n                within_exercise = True\n                yield line_number\n            else:\n                within_exercise = False",
  "def save_object(filename, obj):\n    \"\"\"Compresses and pickles given object to the given filename.\"\"\"\n    logging.info('saving {}...'.format(filename))\n    try:\n        with gzip.GzipFile(filename, 'wb') as f:\n            f.write(pickle.dumps(obj, 1))\n    except Exception as e:\n        logging.error('save failure: {}'.format(e))\n        raise",
  "def load_object(filename):\n    \"\"\"Unpickles and decompresses the given filename and returns the created object.\"\"\"\n    logging.info('loading {}...'.format(filename))\n    try:\n        with gzip.GzipFile(filename, 'rb') as f:\n            buf = ''\n            while True:\n                data = f.read()\n                if data == '':\n                    break\n                buf += data\n            return pickle.loads(buf)\n    except Exception as e:\n        logging.error('load failure: {}'.format(e))\n        raise",
  "def issue_section_order(issue):\n    \"\"\"Returns the section order for the given issue.\"\"\"\n    try:\n        return LOG_SECTION.values().index(issue_section(issue))\n    except:\n        return -1",
  "def issue_completed(issue):\n    \"\"\"Returns True iff this issue is has been resolved as completed.\"\"\"\n    labels = issue.get('labels', [])\n    return any(label['name'] == 'reso: completed' for label in labels)",
  "def issue_section(issue):\n    \"\"\"Returns the section heading for the issue, or None if this issue should be ignored.\"\"\"\n    labels = issue.get('labels', [])\n    for label in labels:\n        if not label['name'].startswith('type: '):\n            continue\n\n        if label['name'] in LOG_SECTION:\n            return LOG_SECTION[label['name']]\n        elif label['name'] in IGNORE_ISSUE_TYPE:\n            return None\n        else:\n            logging.warn('unknown issue type: \"{}\" for: {}'.format(label['name'], issue_line(issue)))\n\n    return None",
  "def issue_tags(issue):\n    \"\"\"Returns list of tags for this issue.\"\"\"\n    labels = issue.get('labels', [])\n    return [label['name'].replace('tag: ', '') for label in labels if label['name'].startswith('tag: ')]",
  "def closed_issue(issue, after=None):\n    \"\"\"Returns True iff this issue was closed after given date. If after not given, only checks if issue is closed.\"\"\"\n    if issue['state'] == 'closed':\n        if after is None or parse_timestamp(issue['closed_at']) > after:\n            return True\n    return False",
  "def relevent_issue(issue, after):\n    \"\"\"Returns True iff this issue is something we should show in the changelog.\"\"\"\n    return (closed_issue(issue, after) and\n            issue_completed(issue) and\n            issue_section(issue))",
  "def relevant_issues(issues, after):\n    \"\"\"Yields relevant closed issues (closed after a given datetime) given a list of issues.\"\"\"\n    logging.info('finding relevant issues after {}...'.format(after))\n    seen = set()\n    for issue in issues:\n        if relevent_issue(issue, after) and issue['title'] not in seen:\n            seen.add(issue['title'])\n            yield issue",
  "def closed_issues(issues, after):\n    \"\"\"Yields closed issues (closed after a given datetime) given a list of issues.\"\"\"\n    logging.info('finding closed issues after {}...'.format(after))\n    seen = set()\n    for issue in issues:\n        if closed_issue(issue, after) and issue['title'] not in seen:\n            seen.add(issue['title'])\n            yield issue",
  "def all_issues(issues):\n    \"\"\"Yields unique set of issues given a list of issues.\"\"\"\n    logging.info('finding issues...')\n    seen = set()\n    for issue in issues:\n        if issue['title'] not in seen:\n            seen.add(issue['title'])\n            yield issue",
  "def get_labels_url():\n    \"\"\"Returns github API URL for querying labels.\"\"\"\n    return '{base_url}/{owner}/{repo}/labels'.format(**API_PARAMS)",
  "def get_issues_url(page, after):\n    \"\"\"Returns github API URL for querying tags.\"\"\"\n    template = '{base_url}/{owner}/{repo}/issues?state=closed&per_page=100&page={page}&since={after}'\n    return template.format(page=page, after=after.isoformat(), **API_PARAMS)",
  "def get_tags_url():\n    \"\"\"Returns github API URL for querying tags.\"\"\"\n    return '{base_url}/{owner}/{repo}/tags'.format(**API_PARAMS)",
  "def parse_timestamp(timestamp):\n    \"\"\"Parse ISO8601 timestamps given by github API.\"\"\"\n    dt = dateutil.parser.parse(timestamp)\n    return dt.astimezone(dateutil.tz.tzutc())",
  "def read_url(url):\n    \"\"\"Reads given URL as JSON and returns data as loaded python object.\"\"\"\n    logging.debug('reading {url} ...'.format(url=url))\n    r = urllib2.urlopen(url).read()\n    return json.loads(r)",
  "def query_tags():\n    \"\"\"Hits the github API for repository tags and returns the data.\"\"\"\n    return read_url(get_tags_url())",
  "def query_issues(page, after):\n    \"\"\"Hits the github API for a single page of closed issues and returns the data.\"\"\"\n    return read_url(get_issues_url(page, after))",
  "def query_all_issues(after):\n    \"\"\"Hits the github API for all closed issues after the given date, returns the data.\"\"\"\n    page = count(1)\n    data = []\n    while True:\n        page_data = query_issues(next(page), after)\n        if not page_data:\n            break\n        data.extend(page_data)\n    return data",
  "def dateof(tag_name, tags):\n    \"\"\"Given a list of tags, returns the datetime of the tag with the given name; Otherwise None.\"\"\"\n    for tag in tags:\n        if tag['name'] == tag_name:\n            commit = read_url(tag['commit']['url'])\n            return parse_timestamp(commit['commit']['committer']['date'])\n    return None",
  "def get_data(query_func, load_data=False, save_data=False):\n    \"\"\"Gets data from query_func, optionally saving that data to a file; or loads data from a file.\"\"\"\n    if hasattr(query_func, '__name__'):\n        func_name = query_func.__name__\n    elif hasattr(query_func, 'func'):\n        func_name = query_func.func.__name__\n\n    pickle_file = '{}.pickle'.format(func_name)\n\n    if load_data:\n        data = load_object(pickle_file)\n    else:\n        data = query_func()\n        if save_data:\n            save_object(pickle_file, data)\n    return data",
  "def check_issue(issue, after):\n    labels = issue.get('labels', [])\n    if not any(label['name'].startswith('type: ') for label in labels):\n        logging.warn('issue with no type label: {}'.format(issue_line((issue))))\n\n    if closed_issue(issue, after):\n        if not any(label['name'].startswith('reso: ') for label in labels):\n            if not any(label['name'] in IGNORE_ISSUE_TYPE for label in labels):\n                logging.warn('closed issue with no reso label: {}'.format(issue_line((issue))))\n\n    if 'pull_request' in issue:\n        if not any(label['name'].startswith('status: ') for label in labels):\n            logging.warn('pull request without status label: {}'.format(issue_line(issue)))",
  "def check_issues(issues, after=None):\n    \"\"\"Checks issues for BEP 1 compliance.\"\"\"\n    issues = closed_issues(issues, after) if after else all_issues(issues)\n    issues = sorted(issues, key=ISSUES_SORT_KEY)\n\n    for section, issue_group in groupby(issues, key=ISSUES_BY_SECTION):\n        for issue in issue_group:\n            check_issue(issue, after)",
  "def issue_line(issue):\n    \"\"\"Returns log line for given issue.\"\"\"\n    template = '#{number} {tags}{title}'\n    tags = issue_tags(issue)\n    params = {\n        'title': issue['title'].capitalize().rstrip('.'),\n        'number': issue['number'],\n        'tags': ' '.join('[{}]'.format(tag) for tag in tags) + (' ' if tags else '')\n    }\n    return template.format(**params)",
  "def generate_changelog(issues, after, heading):\n    \"\"\"Prints out changelog.\"\"\"\n    relevent = relevant_issues(issues, after)\n    relevent = sorted(relevent, key=ISSUES_SORT_KEY)\n\n    print(heading + '\\n' + '-' * 20)\n    for section, issue_group in groupby(relevent, key=ISSUES_BY_SECTION):\n        print('  * {}:'.format(section))\n        for issue in issue_group:\n            print('    - {}'.format(issue_line(issue)))",
  "def check_input(new_ver):\n    \"\"\" Ensure that user input matches the format X.X.X \"\"\"\n\n    pat = r'\\d+.\\d+.\\d+'\n    if not re.match(pat, new_ver):\n        print(\"The new version must be in the format X.X.X (ex. '0.6.0')\")\n        return True",
  "def version_update(new_ver, file_array):\n    \"\"\" Replace existing version/release number in an array of files\n        with a user-supplied version number (new_ver)\"\"\"\n\n    pat = r\"\"\"(release|version)([\\\" ][:=] [\\\"\\'])(\\d+.\\d+.\\d+)([\\\"\\'])\"\"\"\n\n    # List that will contain any files where the version number was successfully replaced\n    replaced = []\n\n    # Set as false until a match is found and replaced in the loop below\n    early_ver = False\n\n    for ver_file in file_array:\n        f = open(ver_file)\n        text = f.read()\n        matchObj = re.search(pat, text)\n        f.close()\n\n        if matchObj:\n            early_ver = matchObj.group(3)\n            f = open(ver_file, 'w')\n            text = re.sub(pat, r'\\g<1>\\g<2>%s\\g<4>' % new_ver, text)\n            f.write(text)\n            replaced.append(ver_file)\n        else:\n            print(\"Unable to find version number matching expected format 'X.X.X' in %s\" % ver_file)\n\n    if early_ver:\n        print(\"Version number changed from %s to %s in \\n%s\" % (early_ver, new_ver, replaced))",
  "class BokehMagics(Magics):\n    \"\"\"Magic to embed Bokeh into the IPython notebook.\"\"\"\n\n    if IPython.__version__.startswith(\"1\"):\n        is_ipytwo = False\n    else:\n        is_ipytwo = True\n\n    has_run = False\n\n    @skip_doctest\n    @magic_arguments()\n    @argument('-n', '--notebook', action=\"store_true\",\n              help='This option enable the execution of the Bokeh '\n              'output_notebook() funtion.')\n    @argument('-f', '--figure', action=\"store_true\",\n              help='This option enable the execution of the Bokeh figure() '\n              'function at the start of each cell.')\n    @argument('-f-off', '--figure-off', action=\"store_true\",\n              help='This option disable the execution of the Bokeh figure() '\n              'function at the start of each cell.')\n    @argument('-h', '--hold', action=\"store_true\",\n              help='This option enable the execution of the Bokeh hold() '\n              'function at the start of each cell.')\n    @argument('-h-off', '--hold-off', action=\"store_true\",\n              help='This option disable the execution of the Bokeh hold() '\n              'function at the start of each cell.')\n    @argument('-s', '--show', action=\"store_true\",\n              help='This option enable the execution of the Bokeh show() '\n              'function at the end of each cell.')\n    @argument('-s-off', '--show-off', action=\"store_true\",\n              help='This option disable the execution of the Bokeh show() '\n              'function at the end of each cell.')\n    @line_magic\n    def bokeh(self, arg, line=None):\n        \"\"\" Set up Bokeh to work interactively.\n\n        This function lets you activate bokeh interactive support\n        at any point during an IPython session. It does not import any other\n        bokeh objects into the interactive namespace.\n\n        Examples\n        --------\n\n\n            In [1]: %install_ext url_for_bokeh_extension\n\n            In [2]: %load_ext bokeh_magic\n\n        To load it each time IPython starts, list it in your configuration file:\n\n            c.InteractiveShellApp.extensions = ['bokeh_magic']\n\n        To enable bokeh for usage with the IPython Notebook::\n\n            In [3]: %bokeh --notebook [-n]\n\n        Then you can use a several `modes` (show, hold, figure)::\n\n            In [4]: %bokeh --show [-s] # to enable the autoshow function\n\n            In [5]: %bokeh --show-off [-s-off] to disable the autoshow function\n\n        You can add concatenate `modes` as arguments::\n\n            In [6]: %bokeh --notebook [-n] --show-off [-s-off]\n\n        Note: In order to actually use this magic, you need to have\n        get_ipython(), so you need to have a running IPython kernel.\n        \"\"\"\n\n        # Get the current running IPython instance.\n        ip = get_ipython()\n\n        # Parse the arguments.\n        args = parse_argstring(self.bokeh, arg)\n\n        # Activate/deactivate the execution of func accordingly with the args.\n        if args.notebook:\n            # Configuring embedded BokehJS mode.\n            if not self.has_run:\n                self.notebook_output()\n\n        if args.figure:\n            if not self.has_run:\n                self.notebook_output()\n            # Register the figure function.\n            if self.is_ipytwo:\n                ip.events.register('pre_run_cell', figure)\n                print(\"Automatic figure() is enabled.\")\n            else:\n                #ip.set_hook('pre_run_code_hook', figure)  # not working\n                print(\"The --figure mode is not supported for this version of IPython.\")\n        elif args.figure_off:\n            if not self.has_run:\n                self.notebook_output()\n            if self.is_ipytwo:\n                try:\n                    # Unregister a figure function.\n                    ip.events.unregister('pre_run_cell', figure)\n                    print(\"Automatic figure() is disabled.\")\n                except ValueError:\n                    raise UsageError(\"\"\"You have to enable the --figure mode before trying to disable it.\"\"\")\n            else:\n                print(\"The --figure mode is not supported for this version of IPython.\")\n\n        if args.hold:\n            if not self.has_run:\n                self.notebook_output()\n            # Register the hold function.\n            if self.is_ipytwo:\n                try:\n                    ip.events.unregister('pre_run_cell', self._hold_false)\n                    ip.events.register('pre_run_cell', self._hold_true)\n                except ValueError:\n                    ip.events.register('pre_run_cell', self._hold_true)\n                print(\"Automatic hold() is enabled.\")\n            else:\n                ip.set_hook('pre_run_code_hook', hold)\n                print(\"Automatic hold() is irreversible for IPython 1.x. Just restart your kernel to disable.\")\n        elif args.hold_off:\n            if not self.has_run:\n                self.notebook_output()\n            if self.is_ipytwo:\n                try:\n                    # Unregister a figure function.\n                    ip.events.unregister('pre_run_cell', self._hold_true)\n                    ip.events.register('pre_run_cell', self._hold_false)\n                    print(\"Automatic hold() is disabled.\")\n                except ValueError:\n                    raise UsageError(\"\"\"You have to enable the --hold mode before trying to disable it.\"\"\")\n            else:\n                print(\"Automatic hold() can not be disabled for IPython 1.x without restarting your kernel. Did you activate it before?\")\n\n        if args.show:\n            if not self.has_run:\n                self.notebook_output()\n            # Register a function for calling after code execution.\n            if self.is_ipytwo:\n                ip.events.register('post_run_cell', self.notebook_show)\n            else:\n                ip.register_post_execute(self.notebook_show)\n            print(\"Automatic show() is enabled.\")\n        elif args.show_off:\n            if not self.has_run:\n                self.notebook_output()\n            if self.is_ipytwo:\n                try:\n                    # Unregister a function\n                    ip.events.unregister('post_run_cell', self.notebook_show)\n                    print(\"Automatic show() is disabled.\")\n                except ValueError:\n                    raise UsageError(\"\"\"You have to enable the --show mode before trying to disable it.\"\"\")\n            else:\n                try:\n                    # Unregister a function from the _post_execute dict.\n                    del ip._post_execute[self.notebook_show]\n                    print(\"Automatic show() is disabled.\")\n                except KeyError:\n                    raise UsageError(\"\"\"You have to enable the --show mode before trying to disable it.\"\"\")\n\n    def notebook_output(self):\n        \"\"\"Wrapper to execute the open notebook function just once to avoid \n        a javascript annoying bug when it is called multiple times.\"\"\"\n        output_notebook()\n        self.has_run = True\n\n    def notebook_show(self):\n        \"Wrapper to avoid the exception when the cell does not contain a plot.\"\n        try:\n            show()\n        except AttributeError as e:\n            # no plot object in the current cell gives us an AttributeError\n            if str(e) != \"'NoneType' object has no attribute 'get_ref'\":\n                raise\n            else:\n                pass\n\n    def _hold_true(self):\n        \"Wrapper to set up the the hold function to True to avoid toggling.\"\n        hold(True)\n\n    def _hold_false(self):\n        \"Wrapper to set up the the hold function to True to avoid toggling.\"\n        hold(False)",
  "def load_ipython_extension(ip):\n    ip.register_magics(BokehMagics)\n    print(\"Bokeh magic loaded.\")",
  "def bokeh(self, arg, line=None):\n        \"\"\" Set up Bokeh to work interactively.\n\n        This function lets you activate bokeh interactive support\n        at any point during an IPython session. It does not import any other\n        bokeh objects into the interactive namespace.\n\n        Examples\n        --------\n\n\n            In [1]: %install_ext url_for_bokeh_extension\n\n            In [2]: %load_ext bokeh_magic\n\n        To load it each time IPython starts, list it in your configuration file:\n\n            c.InteractiveShellApp.extensions = ['bokeh_magic']\n\n        To enable bokeh for usage with the IPython Notebook::\n\n            In [3]: %bokeh --notebook [-n]\n\n        Then you can use a several `modes` (show, hold, figure)::\n\n            In [4]: %bokeh --show [-s] # to enable the autoshow function\n\n            In [5]: %bokeh --show-off [-s-off] to disable the autoshow function\n\n        You can add concatenate `modes` as arguments::\n\n            In [6]: %bokeh --notebook [-n] --show-off [-s-off]\n\n        Note: In order to actually use this magic, you need to have\n        get_ipython(), so you need to have a running IPython kernel.\n        \"\"\"\n\n        # Get the current running IPython instance.\n        ip = get_ipython()\n\n        # Parse the arguments.\n        args = parse_argstring(self.bokeh, arg)\n\n        # Activate/deactivate the execution of func accordingly with the args.\n        if args.notebook:\n            # Configuring embedded BokehJS mode.\n            if not self.has_run:\n                self.notebook_output()\n\n        if args.figure:\n            if not self.has_run:\n                self.notebook_output()\n            # Register the figure function.\n            if self.is_ipytwo:\n                ip.events.register('pre_run_cell', figure)\n                print(\"Automatic figure() is enabled.\")\n            else:\n                #ip.set_hook('pre_run_code_hook', figure)  # not working\n                print(\"The --figure mode is not supported for this version of IPython.\")\n        elif args.figure_off:\n            if not self.has_run:\n                self.notebook_output()\n            if self.is_ipytwo:\n                try:\n                    # Unregister a figure function.\n                    ip.events.unregister('pre_run_cell', figure)\n                    print(\"Automatic figure() is disabled.\")\n                except ValueError:\n                    raise UsageError(\"\"\"You have to enable the --figure mode before trying to disable it.\"\"\")\n            else:\n                print(\"The --figure mode is not supported for this version of IPython.\")\n\n        if args.hold:\n            if not self.has_run:\n                self.notebook_output()\n            # Register the hold function.\n            if self.is_ipytwo:\n                try:\n                    ip.events.unregister('pre_run_cell', self._hold_false)\n                    ip.events.register('pre_run_cell', self._hold_true)\n                except ValueError:\n                    ip.events.register('pre_run_cell', self._hold_true)\n                print(\"Automatic hold() is enabled.\")\n            else:\n                ip.set_hook('pre_run_code_hook', hold)\n                print(\"Automatic hold() is irreversible for IPython 1.x. Just restart your kernel to disable.\")\n        elif args.hold_off:\n            if not self.has_run:\n                self.notebook_output()\n            if self.is_ipytwo:\n                try:\n                    # Unregister a figure function.\n                    ip.events.unregister('pre_run_cell', self._hold_true)\n                    ip.events.register('pre_run_cell', self._hold_false)\n                    print(\"Automatic hold() is disabled.\")\n                except ValueError:\n                    raise UsageError(\"\"\"You have to enable the --hold mode before trying to disable it.\"\"\")\n            else:\n                print(\"Automatic hold() can not be disabled for IPython 1.x without restarting your kernel. Did you activate it before?\")\n\n        if args.show:\n            if not self.has_run:\n                self.notebook_output()\n            # Register a function for calling after code execution.\n            if self.is_ipytwo:\n                ip.events.register('post_run_cell', self.notebook_show)\n            else:\n                ip.register_post_execute(self.notebook_show)\n            print(\"Automatic show() is enabled.\")\n        elif args.show_off:\n            if not self.has_run:\n                self.notebook_output()\n            if self.is_ipytwo:\n                try:\n                    # Unregister a function\n                    ip.events.unregister('post_run_cell', self.notebook_show)\n                    print(\"Automatic show() is disabled.\")\n                except ValueError:\n                    raise UsageError(\"\"\"You have to enable the --show mode before trying to disable it.\"\"\")\n            else:\n                try:\n                    # Unregister a function from the _post_execute dict.\n                    del ip._post_execute[self.notebook_show]\n                    print(\"Automatic show() is disabled.\")\n                except KeyError:\n                    raise UsageError(\"\"\"You have to enable the --show mode before trying to disable it.\"\"\")",
  "def notebook_output(self):\n        \"\"\"Wrapper to execute the open notebook function just once to avoid \n        a javascript annoying bug when it is called multiple times.\"\"\"\n        output_notebook()\n        self.has_run = True",
  "def notebook_show(self):\n        \"Wrapper to avoid the exception when the cell does not contain a plot.\"\n        try:\n            show()\n        except AttributeError as e:\n            # no plot object in the current cell gives us an AttributeError\n            if str(e) != \"'NoneType' object has no attribute 'get_ref'\":\n                raise\n            else:\n                pass",
  "def _hold_true(self):\n        \"Wrapper to set up the the hold function to True to avoid toggling.\"\n        hold(True)",
  "def _hold_false(self):\n        \"Wrapper to set up the the hold function to True to avoid toggling.\"\n        hold(False)",
  "class Session(object):\n    \"\"\" Encapsulate a connection to a document stored on a Bokeh Server.\n\n    Args:\n        name (str, optional) : name of server\n        root_url (str, optional) : root url of server\n        userapikey (str, optional) : (default: \"nokey\")\n        username (str, optional) : (default: \"defaultuser\")\n        load_from_config (bool, optional) :\n            Whether to load login information from config. (default: True)\n            If False, then we may overwrite the user's config.\n        configdir (str) : location of user configuration information\n\n    Attributes:\n        base_url (str) :\n        configdir (str) :\n        configfile (str) :\n        http_session (requests.session) :\n        userapikey (str) :\n        userinfo (dict) :\n        username (str) :\n\n    \"\"\"\n    def __init__(\n            self,\n            name             = DEFAULT_SERVER_URL,\n            root_url         = DEFAULT_SERVER_URL,\n            userapikey       = \"nokey\",\n            username         = \"defaultuser\",\n            load_from_config = True,\n            configdir        = None,\n        ):\n\n        self.name = name\n\n        if not root_url.endswith(\"/\"):\n            logger.warning(\"root_url should end with a /, adding one\")\n            root_url = root_url + \"/\"\n        self.root_url = root_url\n\n        # single user mode case\n        self.userapikey = userapikey\n        self.username = username\n        self._configdir = None\n\n        if configdir:\n            self.configdir = configdir\n\n        if load_from_config:\n            self.load()\n\n    @property\n    def http_session(self):\n        if hasattr(self, \"_http_session\"):\n            return self._http_session\n        else:\n            import requests\n            self._http_session = requests.session()\n            return self._http_session\n\n    @property\n    def username(self):\n        return self.http_session.headers.get('BOKEHUSER')\n\n    @username.setter\n    def username(self, val):\n        self.http_session.headers.update({'BOKEHUSER': val})\n\n    @property\n    def userapikey(self):\n        return self.http_session.headers.get('BOKEHUSER-API-KEY')\n\n    @userapikey.setter\n    def userapikey(self, val):\n        self.http_session.headers.update({'BOKEHUSER-API-KEY': val})\n\n    @property\n    def configdir(self):\n        \"\"\" filename where our config are stored. \"\"\"\n        if self._configdir:\n            return self._configdir\n        bokehdir = join(expanduser(\"~\"), \".bokeh\")\n        if not exists(bokehdir):\n            makedirs(bokehdir)\n        return bokehdir\n\n    # for testing\n    @configdir.setter\n    def configdir(self, path):\n        self._configdir = path\n\n    @property\n    def configfile(self):\n        return join(self.configdir, \"config.json\")\n\n    def load_dict(self):\n        configfile = self.configfile\n        if not exists(configfile):\n            data = {}\n        else:\n            with open(configfile, \"r\") as f:\n                data = json.load(f)\n        return data\n\n    def load(self):\n        \"\"\" Loads the server configuration information from disk\n\n        Returns:\n            None\n\n        \"\"\"\n        config_info = self.load_dict().get(self.name, {})\n        print(\"Using saved session configuration for %s\" % self.name)\n        print(\"To override, pass 'load_from_config=False' to Session\")\n        self.root_url = config_info.get('root_url', self.root_url)\n        self.userapikey = config_info.get('userapikey', self.userapikey)\n        self.username = config_info.get('username', self.username)\n\n    def save(self):\n        \"\"\" Save the server configuration information to JSON\n\n        Returns:\n            None\n\n        \"\"\"\n        data = self.load_dict()\n        data[self.name] = {'root_url': self.root_url,\n                           'userapikey': self.userapikey,\n                           'username': self.username}\n        configfile = self.configfile\n        with open(configfile, \"w+\") as f:\n            json.dump(data, f)\n\n    def register(self, username, password):\n        ''' Register a new user with a bokeh server.\n\n        .. note::\n            This is useful in multi-user mode.\n\n        Args:\n            username (str) : user name to register\n            password (str) : user password for account\n\n        Returns:\n            None\n\n        '''\n        url = urljoin(self.root_url, \"bokeh/register\")\n        result = self.execute('post', url, data={\n            'username': username,\n            'password': password,\n            'api': 'true'\n        })\n        if result.status_code != 200:\n            raise RuntimeError(\"Unknown Error\")\n        result = utils.get_json(result)\n        if result['status']:\n            self.username = username\n            self.userapikey = result['userapikey']\n            self.save()\n        else:\n            raise RuntimeError(result['error'])\n\n    def login(self, username, password):\n        ''' Log a user into a bokeh server.\n\n        .. note::\n            This is useful in multi-user mode.\n\n        Args:\n            username (str) : user name to log in\n            password (str) : user password\n\n        Returns:\n            None\n\n        '''\n        url = urljoin(self.root_url, \"bokeh/login\")\n        result = self.execute('post', url, data={\n            'username': username,\n            'password': password,\n            'api': 'true'\n        })\n        if result.status_code != 200:\n            raise RuntimeError(\"Unknown Error\")\n        result = utils.get_json(result)\n        if result['status']:\n            self.username = username\n            self.userapikey = result['userapikey']\n            self.save()\n        else:\n            raise RuntimeError(result['error'])\n\n        self.save()\n\n    def browser_login(self):\n        \"\"\" Open a browser with a token that logs the user into a bokeh server.\n\n        .. note::\n            This is useful in multi-user mode.\n\n        Return:\n            None\n\n        \"\"\"\n        controller = browserlib.get_browser_controller()\n        url = urljoin(self.root_url, \"bokeh/loginfromapikey\")\n        url += \"?\" + urlencode({'username': self.username,\n                                'userapikey': self.userapikey})\n        controller.open(url)\n\n    def data_source(self, name, data):\n        \"\"\" Makes and uploads a server data source to the server.\n\n        .. note::\n            The server must be configured with a data directory.\n\n        Args:\n            name (str) : name for the data source object\n            data (pd.DataFrame or np.array) : data to upload\n\n        Returns:\n            source : ServerDataSource\n\n        \"\"\"\n        raise NotImplementedError\n\n    def list_data(self):\n        \"\"\" Return all the data soruces on the server.\n\n        Returns:\n            sources : JSON\n\n        \"\"\"\n        raise NotImplementedError\n\n    def publish(self):\n        url = utils.urljoin(self.root_url, \"/bokeh/%s/publish\" % self.docid)\n        self.post_json(url)\n\n    def execute(self, method, url, headers=None, **kwargs):\n        \"\"\" Execute an HTTP request using the current session.\n\n        Returns the response\n\n        Args:\n            method (string) : 'get' or 'post'\n            url (string) : url\n            headers (dict, optional) : any extra HTTP headers\n\n        Keyword Args:\n            Any extra arguments to pass into the requests library\n\n        Returns:\n            response\n\n        Returns the response\n        \"\"\"\n        import requests\n        import warnings\n        func = getattr(self.http_session, method)\n        try:\n            resp = func(url, headers=headers, **kwargs)\n        except requests.exceptions.ConnectionError as e:\n            warnings.warn(\"You need to start the bokeh-server to see this example.\")\n            raise e\n        if resp.status_code == 409:\n            raise DataIntegrityException\n\n        if resp.status_code == 401:\n            raise Exception('HTTP Unauthorized accessing')\n        return resp\n\n    def execute_json(self, method, url, headers=None, **kwargs):\n        \"\"\" same as execute, except ensure that json content-type is\n        set in headers and interprets and returns the json response\n        \"\"\"\n        if headers is None:\n            headers = {}\n        headers['content-type'] = 'application/json'\n        resp = self.execute(method, url, headers=headers, **kwargs)\n        return utils.get_json(resp)\n\n    def get_json(self, url, headers=None, **kwargs):\n        \"\"\" Return the result of an HTTP 'get'.\n\n        Args:\n            url (str) : the URL for the 'get' request\n            headers (dict, optional) : any extra HTTP headers\n\n        Keyword Args:\n            Any extra arguments to pass into the requests library\n\n        Returns:\n            response: JSON\n\n        \"\"\"\n        return self.execute_json('get', url, headers=headers, **kwargs)\n\n    def post_json(self, url, headers=None, **kwargs):\n        \"\"\" Return the result of an HTTP 'post'\n\n        Args:\n            url (str) : the URL for the 'get' request\n            headers (dict, optional) : any extra HTTP headers\n\n        Keyword Args:\n            Any extra arguments to pass into the requests library\n\n        Returns:\n            response: JSON\n\n        \"\"\"\n        return self.execute_json('post', url, headers=headers, **kwargs)\n\n    @property\n    def userinfo(self):\n        if not hasattr(self, \"_userinfo\"):\n            url = urljoin(self.root_url, 'bokeh/userinfo/')\n            self._userinfo = self.get_json(url)\n        return self._userinfo\n\n    @userinfo.setter\n    def userinfo(self, val):\n        self._userinfo = val\n\n    @property\n    def base_url(self):\n        return urljoin(self.root_url, \"bokeh/bb/\")\n\n    def get_api_key(self, docid):\n        \"\"\" Retrieve the document API key from the server.\n\n        Args:\n            docid (string) : docid of the document to retrive API key for\n\n        Returns:\n            apikey : string\n\n        \"\"\"\n        url = urljoin(self.root_url,\"bokeh/getdocapikey/%s\" % docid)\n        apikey = self.get_json(url)\n        if 'apikey' in apikey:\n            apikey = apikey['apikey']\n            logger.info('got read write apikey')\n        else:\n            apikey = apikey['readonlyapikey']\n            logger.info('got read only apikey')\n        return apikey\n\n    def find_doc(self, name):\n        \"\"\" Return the docid of the document with a title matching ``name``.\n\n        .. note::\n            Creates a new document with the given title if one is not found.\n\n        Args:\n            name (string) : name for the document\n\n        Returns:\n            docid : str\n\n        \"\"\"\n\n        docs = self.userinfo.get('docs')\n\n        matching = [x for x in docs if x.get('title') == name]\n\n        if len(matching) == 0:\n            logger.info(\"No documents found, creating new document '%s'\" % name)\n            self.make_doc(name)\n            return self.find_doc(name)\n\n        elif len(matching) > 1:\n            logger.warning(\"Multiple documents with name '%s'\" % name)\n\n        return matching[0]['docid']\n\n    def use_doc(self, name=None, docid=None):\n        \"\"\" Configure the session to use a given document.\n\n        Args:\n            name (str, optional) : name of the document to use\n            docid (str, optional) : id of the document to use\n\n        .. note::\n            only one of ``name`` or ``docid`` may be supplied.\n\n        Creates a document for with the given name if one is not present on\n        the server.\n\n        Returns:\n            None\n\n        \"\"\"\n        if docid is not None and name is not None:\n            raise ValueError(\"only one of 'name' or 'docid' can be supplied to use_doc(...)\")\n\n        if docid:\n            self.docid = docid\n        else:\n            self.docid = self.find_doc(name)\n        self.apikey = self.get_api_key(self.docid)\n\n    def make_doc(self, title):\n        \"\"\" Makes a new document with the given title on the server\n\n        .. note:: user information is reloaded\n\n        Returns:\n            None\n\n        \"\"\"\n        url = urljoin(self.root_url,\"bokeh/doc/\")\n        data = protocol.serialize_json({'title' : title})\n        self.userinfo = self.post_json(url, data=data)\n\n    def pull(self, typename=None, objid=None):\n        \"\"\" Pull JSON objects from the server.\n\n        Returns a specific object if both ``typename`` and ``objid`` are\n        supplied. Otherwise, returns all objects for the currently configured\n        document.\n\n        This is a low-level function.\n\n        Args:\n            typename (str, optional) : name of the type of object to pull\n            objid (str, optional) : ID of the object to pull\n\n        .. note::\n            you must supply either ``typename`` AND ``objid`` or omit both.\n\n        Returns:\n            attrs : JSON\n\n        \"\"\"\n        if typename is None and objid is None:\n            url = utils.urljoin(self.base_url, self.docid +\"/\")\n            attrs = self.get_json(url)\n\n        elif typename is None or objid is None:\n            raise ValueError(\"typename and objid must both be None, or neither.\")\n\n        else:\n            url = utils.urljoin(\n                self.base_url,\n                self.docid + \"/\" + typename + \"/\" + objid + \"/\"\n            )\n            attr = self.get_json(url)\n            attrs = [{\n                'type': typename,\n                'id': objid,\n                'attributes': attr\n            }]\n        return attrs\n\n    def push(self, *jsonobjs):\n        \"\"\" Push JSON objects to the server.\n\n        This is a low-level function.\n\n        Args:\n            *jsonobjs (JSON) : objects to push to the server\n\n        Returns:\n            None\n\n        \"\"\"\n        data = protocol.serialize_json(jsonobjs)\n        url = utils.urljoin(self.base_url, self.docid + \"/\", \"bulkupsert\")\n        self.post_json(url, data=data)\n\n    def gc(self):\n        url = utils.urljoin(self.base_url, self.docid + \"/\", \"gc\")\n        self.post_json(url)\n\n    # convenience functions to use a session and store/fetch from server\n\n    def load_document(self, doc):\n        \"\"\" Loads data for the session and merge with the given document.\n\n        Args:\n            doc (Document) : document to load data into\n\n        Returns:\n            None\n\n        \"\"\"\n        self.gc()\n        json_objs = self.pull()\n        doc.merge(json_objs)\n        doc.docid = self.docid\n\n    def load_object(self, obj, doc):\n        \"\"\" Update an object in a document with data pulled from the server.\n\n        Args:\n            obj (PlotObject) : object to be updated\n            doc (Document) : the object's document\n\n        Returns:\n            None\n\n        \"\"\"\n        assert obj._id in doc._models\n        attrs = self.pull(typename=obj.__view_model__, objid=obj._id)\n        doc.load(*attrs)\n\n    def store_document(self, doc, dirty_only=True):\n        \"\"\" Store a document on the server.\n\n        Returns the models that were actually pushed.\n\n        Args:\n            doc (Document) : the document to store\n            dirty_only (bool, optional) : whether to store only dirty objects. (default: True)\n\n        Returns:\n            models : list[PlotObject]\n\n        \"\"\"\n        doc._add_all()\n        models = doc._models.values()\n\n        if dirty_only:\n            models = [x for x in models if getattr(x, '_dirty', False)]\n\n        json_objs = doc.dump(*models)\n        self.push(*json_objs)\n\n        for model in models:\n            model._dirty = False\n\n        return models\n\n    def store_objects(self, *objs, **kwargs):\n        \"\"\" Store objects on the server\n\n        Returns the objects that were actually stored.\n\n        Args:\n            *objs (PlotObject) : objects to store\n\n        Keywords Args:\n            dirty_only (bool, optional) : whether to store only dirty objects. (default: True)\n\n        Returns:\n            models : set[PlotObject]\n\n        \"\"\"\n\n        models = set()\n\n        for obj in objs:\n            models.update(obj.references())\n\n        if kwargs.pop('dirty_only', True):\n            models = list(models)\n\n        json_objs = utils.dump(models, self.docid)\n        self.push(*json_objs)\n\n        for model in models:\n            model._dirty = False\n\n        return models\n\n    def object_link(self, obj):\n        \"\"\" Return a URL to a server page that will render the given object.\n\n        Args:\n            obj (PlotObject) : object to render\n\n        Returns:\n            url : str\n\n        \"\"\"\n        link = \"bokeh/doc/%s/%s\" % (self.docid, obj._id)\n        return utils.urljoin(self.root_url, link)\n\n    def show(self, obj):\n        \"\"\" Display an object as HTML in IPython using its display protocol.\n\n        Args:\n            obj (PlotObject) : object to display\n\n        Returns:\n            None\n\n        \"\"\"\n        data = {'text/html': autoload_server(obj, self)}\n        utils.publish_display_data(data)\n\n    def poll_document(self, document, interval=0.5):\n        \"\"\" Periodically ask the server for updates to the `document`. \"\"\"\n        try:\n            while True:\n                self.load_document(document)\n                time.sleep(interval)\n        except KeyboardInterrupt:\n            print()\n        except ConnectionError:\n            print(\"Connection to bokeh-server was terminated\")\n\n    # helper methods\n\n    def _prep_data_source_df(self, name, dataframe):\n        name = tempfile.NamedTemporaryFile(prefix=\"bokeh_data\",\n                                           suffix=\".pandas\").name\n        store = pd.HDFStore(name)\n        store.append(\"__data__\", dataframe, format=\"table\", data_columns=True)\n        store.close()\n        return name\n\n    def _prep_data_source_numpy(self, name, arr):\n        name = tempfile.NamedTemporaryFile(prefix=\"bokeh_data\",\n                                           suffix=\".table\").name\n        store = tables.File(name, 'w')\n        store.createArray(\"/\", \"__data__\", obj=arr)\n        store.close()\n        return name",
  "class TestSession(Session):\n    \"\"\"Currently, register and login do not work, everything else should work\n    in theory, but we'll have to test this as we go along and convert tests\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        if 'load_from_config' not in kwargs:\n            kwargs['load_from_config'] = False\n        self.client = kwargs.pop('client')\n        self.headers = {}\n        super(TestSession, self).__init__(*args, **kwargs)\n\n    @property\n    def username(self):\n        return self.headers.get('BOKEHUSER')\n\n    @username.setter\n    def username(self, val):\n        self.headers.update({'BOKEHUSER': val})\n\n    @property\n    def userapikey(self):\n        return self.headers.get('BOKEHUSER-API-KEY')\n\n    @userapikey.setter\n    def userapikey(self, val):\n        self.headers.update({'BOKEHUSER-API-KEY': val})\n\n    def execute(self, method, url, headers=None, **kwargs):\n        if headers is None:\n            headers = {}\n        func = getattr(self.client, method)\n        resp = func(url, headers=headers, **kwargs)\n        if resp.status_code == 409:\n            raise DataIntegrityException\n        if resp.status_code == 401:\n            raise Exception('HTTP Unauthorized accessing')\n        return resp",
  "def __init__(\n            self,\n            name             = DEFAULT_SERVER_URL,\n            root_url         = DEFAULT_SERVER_URL,\n            userapikey       = \"nokey\",\n            username         = \"defaultuser\",\n            load_from_config = True,\n            configdir        = None,\n        ):\n\n        self.name = name\n\n        if not root_url.endswith(\"/\"):\n            logger.warning(\"root_url should end with a /, adding one\")\n            root_url = root_url + \"/\"\n        self.root_url = root_url\n\n        # single user mode case\n        self.userapikey = userapikey\n        self.username = username\n        self._configdir = None\n\n        if configdir:\n            self.configdir = configdir\n\n        if load_from_config:\n            self.load()",
  "def http_session(self):\n        if hasattr(self, \"_http_session\"):\n            return self._http_session\n        else:\n            import requests\n            self._http_session = requests.session()\n            return self._http_session",
  "def username(self):\n        return self.http_session.headers.get('BOKEHUSER')",
  "def username(self, val):\n        self.http_session.headers.update({'BOKEHUSER': val})",
  "def userapikey(self):\n        return self.http_session.headers.get('BOKEHUSER-API-KEY')",
  "def userapikey(self, val):\n        self.http_session.headers.update({'BOKEHUSER-API-KEY': val})",
  "def configdir(self):\n        \"\"\" filename where our config are stored. \"\"\"\n        if self._configdir:\n            return self._configdir\n        bokehdir = join(expanduser(\"~\"), \".bokeh\")\n        if not exists(bokehdir):\n            makedirs(bokehdir)\n        return bokehdir",
  "def configdir(self, path):\n        self._configdir = path",
  "def configfile(self):\n        return join(self.configdir, \"config.json\")",
  "def load_dict(self):\n        configfile = self.configfile\n        if not exists(configfile):\n            data = {}\n        else:\n            with open(configfile, \"r\") as f:\n                data = json.load(f)\n        return data",
  "def load(self):\n        \"\"\" Loads the server configuration information from disk\n\n        Returns:\n            None\n\n        \"\"\"\n        config_info = self.load_dict().get(self.name, {})\n        print(\"Using saved session configuration for %s\" % self.name)\n        print(\"To override, pass 'load_from_config=False' to Session\")\n        self.root_url = config_info.get('root_url', self.root_url)\n        self.userapikey = config_info.get('userapikey', self.userapikey)\n        self.username = config_info.get('username', self.username)",
  "def save(self):\n        \"\"\" Save the server configuration information to JSON\n\n        Returns:\n            None\n\n        \"\"\"\n        data = self.load_dict()\n        data[self.name] = {'root_url': self.root_url,\n                           'userapikey': self.userapikey,\n                           'username': self.username}\n        configfile = self.configfile\n        with open(configfile, \"w+\") as f:\n            json.dump(data, f)",
  "def register(self, username, password):\n        ''' Register a new user with a bokeh server.\n\n        .. note::\n            This is useful in multi-user mode.\n\n        Args:\n            username (str) : user name to register\n            password (str) : user password for account\n\n        Returns:\n            None\n\n        '''\n        url = urljoin(self.root_url, \"bokeh/register\")\n        result = self.execute('post', url, data={\n            'username': username,\n            'password': password,\n            'api': 'true'\n        })\n        if result.status_code != 200:\n            raise RuntimeError(\"Unknown Error\")\n        result = utils.get_json(result)\n        if result['status']:\n            self.username = username\n            self.userapikey = result['userapikey']\n            self.save()\n        else:\n            raise RuntimeError(result['error'])",
  "def login(self, username, password):\n        ''' Log a user into a bokeh server.\n\n        .. note::\n            This is useful in multi-user mode.\n\n        Args:\n            username (str) : user name to log in\n            password (str) : user password\n\n        Returns:\n            None\n\n        '''\n        url = urljoin(self.root_url, \"bokeh/login\")\n        result = self.execute('post', url, data={\n            'username': username,\n            'password': password,\n            'api': 'true'\n        })\n        if result.status_code != 200:\n            raise RuntimeError(\"Unknown Error\")\n        result = utils.get_json(result)\n        if result['status']:\n            self.username = username\n            self.userapikey = result['userapikey']\n            self.save()\n        else:\n            raise RuntimeError(result['error'])\n\n        self.save()",
  "def browser_login(self):\n        \"\"\" Open a browser with a token that logs the user into a bokeh server.\n\n        .. note::\n            This is useful in multi-user mode.\n\n        Return:\n            None\n\n        \"\"\"\n        controller = browserlib.get_browser_controller()\n        url = urljoin(self.root_url, \"bokeh/loginfromapikey\")\n        url += \"?\" + urlencode({'username': self.username,\n                                'userapikey': self.userapikey})\n        controller.open(url)",
  "def data_source(self, name, data):\n        \"\"\" Makes and uploads a server data source to the server.\n\n        .. note::\n            The server must be configured with a data directory.\n\n        Args:\n            name (str) : name for the data source object\n            data (pd.DataFrame or np.array) : data to upload\n\n        Returns:\n            source : ServerDataSource\n\n        \"\"\"\n        raise NotImplementedError",
  "def list_data(self):\n        \"\"\" Return all the data soruces on the server.\n\n        Returns:\n            sources : JSON\n\n        \"\"\"\n        raise NotImplementedError",
  "def publish(self):\n        url = utils.urljoin(self.root_url, \"/bokeh/%s/publish\" % self.docid)\n        self.post_json(url)",
  "def execute(self, method, url, headers=None, **kwargs):\n        \"\"\" Execute an HTTP request using the current session.\n\n        Returns the response\n\n        Args:\n            method (string) : 'get' or 'post'\n            url (string) : url\n            headers (dict, optional) : any extra HTTP headers\n\n        Keyword Args:\n            Any extra arguments to pass into the requests library\n\n        Returns:\n            response\n\n        Returns the response\n        \"\"\"\n        import requests\n        import warnings\n        func = getattr(self.http_session, method)\n        try:\n            resp = func(url, headers=headers, **kwargs)\n        except requests.exceptions.ConnectionError as e:\n            warnings.warn(\"You need to start the bokeh-server to see this example.\")\n            raise e\n        if resp.status_code == 409:\n            raise DataIntegrityException\n\n        if resp.status_code == 401:\n            raise Exception('HTTP Unauthorized accessing')\n        return resp",
  "def execute_json(self, method, url, headers=None, **kwargs):\n        \"\"\" same as execute, except ensure that json content-type is\n        set in headers and interprets and returns the json response\n        \"\"\"\n        if headers is None:\n            headers = {}\n        headers['content-type'] = 'application/json'\n        resp = self.execute(method, url, headers=headers, **kwargs)\n        return utils.get_json(resp)",
  "def get_json(self, url, headers=None, **kwargs):\n        \"\"\" Return the result of an HTTP 'get'.\n\n        Args:\n            url (str) : the URL for the 'get' request\n            headers (dict, optional) : any extra HTTP headers\n\n        Keyword Args:\n            Any extra arguments to pass into the requests library\n\n        Returns:\n            response: JSON\n\n        \"\"\"\n        return self.execute_json('get', url, headers=headers, **kwargs)",
  "def post_json(self, url, headers=None, **kwargs):\n        \"\"\" Return the result of an HTTP 'post'\n\n        Args:\n            url (str) : the URL for the 'get' request\n            headers (dict, optional) : any extra HTTP headers\n\n        Keyword Args:\n            Any extra arguments to pass into the requests library\n\n        Returns:\n            response: JSON\n\n        \"\"\"\n        return self.execute_json('post', url, headers=headers, **kwargs)",
  "def userinfo(self):\n        if not hasattr(self, \"_userinfo\"):\n            url = urljoin(self.root_url, 'bokeh/userinfo/')\n            self._userinfo = self.get_json(url)\n        return self._userinfo",
  "def userinfo(self, val):\n        self._userinfo = val",
  "def base_url(self):\n        return urljoin(self.root_url, \"bokeh/bb/\")",
  "def get_api_key(self, docid):\n        \"\"\" Retrieve the document API key from the server.\n\n        Args:\n            docid (string) : docid of the document to retrive API key for\n\n        Returns:\n            apikey : string\n\n        \"\"\"\n        url = urljoin(self.root_url,\"bokeh/getdocapikey/%s\" % docid)\n        apikey = self.get_json(url)\n        if 'apikey' in apikey:\n            apikey = apikey['apikey']\n            logger.info('got read write apikey')\n        else:\n            apikey = apikey['readonlyapikey']\n            logger.info('got read only apikey')\n        return apikey",
  "def find_doc(self, name):\n        \"\"\" Return the docid of the document with a title matching ``name``.\n\n        .. note::\n            Creates a new document with the given title if one is not found.\n\n        Args:\n            name (string) : name for the document\n\n        Returns:\n            docid : str\n\n        \"\"\"\n\n        docs = self.userinfo.get('docs')\n\n        matching = [x for x in docs if x.get('title') == name]\n\n        if len(matching) == 0:\n            logger.info(\"No documents found, creating new document '%s'\" % name)\n            self.make_doc(name)\n            return self.find_doc(name)\n\n        elif len(matching) > 1:\n            logger.warning(\"Multiple documents with name '%s'\" % name)\n\n        return matching[0]['docid']",
  "def use_doc(self, name=None, docid=None):\n        \"\"\" Configure the session to use a given document.\n\n        Args:\n            name (str, optional) : name of the document to use\n            docid (str, optional) : id of the document to use\n\n        .. note::\n            only one of ``name`` or ``docid`` may be supplied.\n\n        Creates a document for with the given name if one is not present on\n        the server.\n\n        Returns:\n            None\n\n        \"\"\"\n        if docid is not None and name is not None:\n            raise ValueError(\"only one of 'name' or 'docid' can be supplied to use_doc(...)\")\n\n        if docid:\n            self.docid = docid\n        else:\n            self.docid = self.find_doc(name)\n        self.apikey = self.get_api_key(self.docid)",
  "def make_doc(self, title):\n        \"\"\" Makes a new document with the given title on the server\n\n        .. note:: user information is reloaded\n\n        Returns:\n            None\n\n        \"\"\"\n        url = urljoin(self.root_url,\"bokeh/doc/\")\n        data = protocol.serialize_json({'title' : title})\n        self.userinfo = self.post_json(url, data=data)",
  "def pull(self, typename=None, objid=None):\n        \"\"\" Pull JSON objects from the server.\n\n        Returns a specific object if both ``typename`` and ``objid`` are\n        supplied. Otherwise, returns all objects for the currently configured\n        document.\n\n        This is a low-level function.\n\n        Args:\n            typename (str, optional) : name of the type of object to pull\n            objid (str, optional) : ID of the object to pull\n\n        .. note::\n            you must supply either ``typename`` AND ``objid`` or omit both.\n\n        Returns:\n            attrs : JSON\n\n        \"\"\"\n        if typename is None and objid is None:\n            url = utils.urljoin(self.base_url, self.docid +\"/\")\n            attrs = self.get_json(url)\n\n        elif typename is None or objid is None:\n            raise ValueError(\"typename and objid must both be None, or neither.\")\n\n        else:\n            url = utils.urljoin(\n                self.base_url,\n                self.docid + \"/\" + typename + \"/\" + objid + \"/\"\n            )\n            attr = self.get_json(url)\n            attrs = [{\n                'type': typename,\n                'id': objid,\n                'attributes': attr\n            }]\n        return attrs",
  "def push(self, *jsonobjs):\n        \"\"\" Push JSON objects to the server.\n\n        This is a low-level function.\n\n        Args:\n            *jsonobjs (JSON) : objects to push to the server\n\n        Returns:\n            None\n\n        \"\"\"\n        data = protocol.serialize_json(jsonobjs)\n        url = utils.urljoin(self.base_url, self.docid + \"/\", \"bulkupsert\")\n        self.post_json(url, data=data)",
  "def gc(self):\n        url = utils.urljoin(self.base_url, self.docid + \"/\", \"gc\")\n        self.post_json(url)",
  "def load_document(self, doc):\n        \"\"\" Loads data for the session and merge with the given document.\n\n        Args:\n            doc (Document) : document to load data into\n\n        Returns:\n            None\n\n        \"\"\"\n        self.gc()\n        json_objs = self.pull()\n        doc.merge(json_objs)\n        doc.docid = self.docid",
  "def load_object(self, obj, doc):\n        \"\"\" Update an object in a document with data pulled from the server.\n\n        Args:\n            obj (PlotObject) : object to be updated\n            doc (Document) : the object's document\n\n        Returns:\n            None\n\n        \"\"\"\n        assert obj._id in doc._models\n        attrs = self.pull(typename=obj.__view_model__, objid=obj._id)\n        doc.load(*attrs)",
  "def store_document(self, doc, dirty_only=True):\n        \"\"\" Store a document on the server.\n\n        Returns the models that were actually pushed.\n\n        Args:\n            doc (Document) : the document to store\n            dirty_only (bool, optional) : whether to store only dirty objects. (default: True)\n\n        Returns:\n            models : list[PlotObject]\n\n        \"\"\"\n        doc._add_all()\n        models = doc._models.values()\n\n        if dirty_only:\n            models = [x for x in models if getattr(x, '_dirty', False)]\n\n        json_objs = doc.dump(*models)\n        self.push(*json_objs)\n\n        for model in models:\n            model._dirty = False\n\n        return models",
  "def store_objects(self, *objs, **kwargs):\n        \"\"\" Store objects on the server\n\n        Returns the objects that were actually stored.\n\n        Args:\n            *objs (PlotObject) : objects to store\n\n        Keywords Args:\n            dirty_only (bool, optional) : whether to store only dirty objects. (default: True)\n\n        Returns:\n            models : set[PlotObject]\n\n        \"\"\"\n\n        models = set()\n\n        for obj in objs:\n            models.update(obj.references())\n\n        if kwargs.pop('dirty_only', True):\n            models = list(models)\n\n        json_objs = utils.dump(models, self.docid)\n        self.push(*json_objs)\n\n        for model in models:\n            model._dirty = False\n\n        return models",
  "def object_link(self, obj):\n        \"\"\" Return a URL to a server page that will render the given object.\n\n        Args:\n            obj (PlotObject) : object to render\n\n        Returns:\n            url : str\n\n        \"\"\"\n        link = \"bokeh/doc/%s/%s\" % (self.docid, obj._id)\n        return utils.urljoin(self.root_url, link)",
  "def show(self, obj):\n        \"\"\" Display an object as HTML in IPython using its display protocol.\n\n        Args:\n            obj (PlotObject) : object to display\n\n        Returns:\n            None\n\n        \"\"\"\n        data = {'text/html': autoload_server(obj, self)}\n        utils.publish_display_data(data)",
  "def poll_document(self, document, interval=0.5):\n        \"\"\" Periodically ask the server for updates to the `document`. \"\"\"\n        try:\n            while True:\n                self.load_document(document)\n                time.sleep(interval)\n        except KeyboardInterrupt:\n            print()\n        except ConnectionError:\n            print(\"Connection to bokeh-server was terminated\")",
  "def _prep_data_source_df(self, name, dataframe):\n        name = tempfile.NamedTemporaryFile(prefix=\"bokeh_data\",\n                                           suffix=\".pandas\").name\n        store = pd.HDFStore(name)\n        store.append(\"__data__\", dataframe, format=\"table\", data_columns=True)\n        store.close()\n        return name",
  "def _prep_data_source_numpy(self, name, arr):\n        name = tempfile.NamedTemporaryFile(prefix=\"bokeh_data\",\n                                           suffix=\".table\").name\n        store = tables.File(name, 'w')\n        store.createArray(\"/\", \"__data__\", obj=arr)\n        store.close()\n        return name",
  "def __init__(self, *args, **kwargs):\n        if 'load_from_config' not in kwargs:\n            kwargs['load_from_config'] = False\n        self.client = kwargs.pop('client')\n        self.headers = {}\n        super(TestSession, self).__init__(*args, **kwargs)",
  "def username(self):\n        return self.headers.get('BOKEHUSER')",
  "def username(self, val):\n        self.headers.update({'BOKEHUSER': val})",
  "def userapikey(self):\n        return self.headers.get('BOKEHUSER-API-KEY')",
  "def userapikey(self, val):\n        self.headers.update({'BOKEHUSER-API-KEY': val})",
  "def execute(self, method, url, headers=None, **kwargs):\n        if headers is None:\n            headers = {}\n        func = getattr(self.client, method)\n        resp = func(url, headers=headers, **kwargs)\n        if resp.status_code == 409:\n            raise DataIntegrityException\n        if resp.status_code == 401:\n            raise Exception('HTTP Unauthorized accessing')\n        return resp",
  "def install_bokeh_magic():\n    \"An alternative way to install the bokeh_magic extension.\"\n    url = \"https://raw.github.com/bokeh/bokeh/master/extensions/bokeh_magic.py\"\n    ip.extension_manager.install_extension(url)\n    print(\"Bokeh_magic has been installed.\")",
  "class _NotSet(object):\n    pass",
  "class DeserializationError(Exception):\n    pass",
  "class Property(object):\n    \"\"\" Base class for all type properties. \"\"\"\n\n    def __init__(self, default=None, help=None):\n        \"\"\" This is how the descriptor is created in the class declaration \"\"\"\n        if isinstance(default, types.FunctionType): # aka. lazy value\n            self.validate(default())\n        else:\n            self.validate(default)\n\n        self._default = default\n        self.__doc__ = help\n        self.alternatives = []\n\n        # This gets set by the class decorator at class creation time\n        self.name = \"unnamed\"\n\n    def __str__(self):\n        return self.__class__.__name__\n\n    @property\n    def _name(self):\n        return \"_\" + self.name\n\n    @property\n    def default(self):\n        if not isinstance(self._default, types.FunctionType):\n            return copy(self._default)\n        else:\n            value = self._default()\n            self.validate(value)\n            return value\n\n    @classmethod\n    def autocreate(cls, name=None):\n        \"\"\" Called by the metaclass to create a\n        new instance of this descriptor\n        if the user just assigned it to a property without trailing\n        parentheses.\n        \"\"\"\n        return cls()\n\n    def matches(self, new, old):\n        # XXX: originally this code warned about not being able to compare values, but that\n        # doesn't make sense, because most comparisons involving numpy arrays will fail with\n        # ValueError exception, thus warning about inevitable.\n        try:\n            if new is None or old is None:\n                return new is old           # XXX: silence FutureWarning from NumPy\n            else:\n                return new == old\n        except (KeyboardInterrupt, SystemExit):\n            raise\n        except Exception as e:\n            logger.debug(\"could not compare %s and %s for property %s (Reason: %s)\", new, old, self.name, e)\n        return False\n\n    def from_json(self, json, models=None):\n        return json\n\n    def transform(self, value):\n        return value\n\n    def validate(self, value):\n        pass\n\n    def is_valid(self, value):\n        try:\n            self.validate(value)\n        except ValueError:\n            return False\n        else:\n            return True\n\n    def _get(self, obj):\n        if not hasattr(obj, self._name):\n            setattr(obj, self._name, self.default)\n        return getattr(obj, self._name)\n\n    def __get__(self, obj, owner=None):\n        if obj is not None:\n            return self._get(obj)\n        elif owner is not None:\n            return self\n        else:\n            raise ValueError(\"both 'obj' and 'owner' are None, don't know what to do\")\n\n    def __set__(self, obj, value):\n        try:\n            self.validate(value)\n        except ValueError as e:\n            for tp, converter in self.alternatives:\n                if tp.is_valid(value):\n                    value = converter(value)\n                    break\n            else:\n                raise e\n        else:\n            value = self.transform(value)\n\n        old = self.__get__(obj)\n        obj._changed_vars.add(self.name)\n        if self._name in obj.__dict__ and self.matches(value, old):\n            return\n        setattr(obj, self._name, value)\n        obj._dirty = True\n        if hasattr(obj, '_trigger'):\n            if hasattr(obj, '_block_callbacks') and obj._block_callbacks:\n                obj._callback_queue.append((self.name, old, value))\n            else:\n                obj._trigger(self.name, old, value)\n\n    def __delete__(self, obj):\n        if hasattr(obj, self._name):\n            delattr(obj, self._name)\n\n    @property\n    def has_ref(self):\n        return False\n\n    def accepts(self, tp, converter):\n        tp = ParameterizedProperty._validate_type_param(tp)\n        self.alternatives.append((tp, converter))\n        return self\n\n    def __or__(self, other):\n        return Either(self, other)",
  "class DataSpec(Property):\n    \"\"\" Because the BokehJS glyphs support a fixed value or a named\n    field for most data fields, we capture that in this descriptor.\n    Fields can have a fixed value, or be a name that is looked up\n    on the datasource (usually as a column or record array field).\n    Numerical data can also have units of screen or data space.\n\n    We mirror the JS convention in this Python descriptor.  For details,\n    see renderers/properties.coffee in BokehJS, and specifically the\n    select() function.\n\n    There are multiple ways to set a DataSpec, illustrated below with comments\n    and example code.\n\n    Setting DataSpecs\n\n\n    Simple example::\n\n        class Foo(HasProps):\n            x = DataSpec(\"x\", units=\"data\")\n\n        f = Foo()\n        f.x = \"fieldname\"  # Use the datasource field named \"fieldname\"\n        f.x = 12           # A fixed value of 12\n\n    Can provide a dict with the fields explicitly named::\n\n        f.width = {\"name\": \"foo\"}\n        f.size = {\"name\": \"foo\", \"units\": \"screen\"}\n\n    Reading DataSpecs\n\n\n    In the cases when the dataspec is set to just a field name or a\n    fixed value, then those are returned.  If the no values have\n    been set, then the value of to_dict() is returned.\n\n    In all cases, to determine the full dict that will be used to\n    represent this dataspec, use the to_dict() method.\n\n    Implementation\n\n\n    The DataSpec instance is stored in the class dict, and acts as a\n    descriptor.  Thus, it is shared between all instances of the class.\n    Instance-specific data is stored in the instance dict, in a private\n    variable named _[attrname].  This stores the actual value that the\n    user last set (and does not exist if the user has not yet set the\n    value).\n\n    \"\"\"\n\n    def __init__(self, field=None, units=\"data\", min_value=None, default=_NotSet, help=None):\n        \"\"\"\n        Parameters\n        ==========\n        **field** is the string name of a data column to look up.\n        **units** is either \"data\" or \"screen\"\n        \"\"\"\n        # Don't use .name because the HasProps metaclass uses that to\n        # store the attribute name on this descriptor.\n        if field is None or isinstance(field, string_types):\n            self.field = field\n        else:\n            raise ValueError(\"'field' must be a string or None, got %r\" % field)\n\n        self.units = units\n        self._default = default\n        self.min_value = min_value\n        self.__doc__ = help\n\n    @classmethod\n    def autocreate(cls, name=None):\n        # In this case, use the name the user assigned this DataSpec to\n        # as the default field name.\n        d = cls(field=name)\n        return d\n\n    def _get(self, obj):\n        \"\"\" Try to implement a \"natural\" interface: if the user just set\n        simple values or field names, the getter just returns those.\n        However, if the user has also overridden the \"units\" or \"default\"\n        settings, then a dictionary is returned.\n        \"\"\"\n        if hasattr(obj, self._name):\n            setval = getattr(obj, self._name)\n            if isinstance(setval, string_types):\n                # A string representing the field\n                return setval\n            elif not isinstance(setval, dict):\n                # Typically a number presenting the fixed value\n                return setval\n            else:\n                return self.to_dict(obj)\n        else:\n            # If the user hasn't set anything\n            if self.field is not None:\n                return self.field\n            if self.default != _NotSet:\n                return self.default\n            # XXX: implicit `return None` or unreachable?\n\n    def to_dict(self, obj):\n        # Build the complete dict\n        setval = getattr(obj, self._name, None)\n        if isinstance(setval, string_types):\n            d = {\"field\": setval, \"units\": self.units}\n        elif isinstance(setval, dict):\n            d = {\"units\": self.units}\n            d.update(setval)\n        elif setval is not None:\n            # a fixed value of some sort; no need to store the default value\n            d = {\"value\": setval, \"units\": self.units}\n        else:\n            # If the user never set a value\n            if self.field is not None:\n                d = {\"field\": self.field, \"units\": self.units}\n            elif self.default != _NotSet:\n                d = {\"value\": self.default, \"units\": self.units}\n            else:\n                d = {}\n\n        if \"value\" in d and self.min_value is not None:\n            if d[\"value\"] < self.min_value:\n                raise ValueError(\"value must be greater than %s\" % str(self.min_value))\n        return d\n\n    def __repr__(self):\n        return \"DataSpec(field=%r, units=%r)\" % (self.field, self.units)",
  "class ColorSpec(DataSpec):\n    \"\"\" Subclass of DataSpec for specifying colors.\n\n    Although this serves the same role as a DataSpec, its usage is somewhat\n    different because:\n\n    * Specifying a fixed value is much more common\n    * Strings can be both field identifiers or refer to one of the SVG\n      Named Colors (or be a hex value starting with \"#\")\n    * There are no units\n\n    For colors, because we support named colors and hex values prefaced\n    with a \"#\", when we are handed a string value, there is a little\n    interpretation: if the value is one of the 147 SVG named colors or\n    it starts with a \"#\", then it is interpreted as a value.  Otherwise,\n    it is treated as a field name.\n\n    If a 3-tuple is provided, then it is treated as an RGB (0..255).\n    If a 4-tuple is provided, then it is treated as an RGBa (0..255), with\n    alpha as a float between 0 and 1.  (This follows the HTML5 Canvas API.)\n\n    Unlike DataSpec, ColorSpecs do not have a \"units\" property.\n\n    When reading out a ColorSpec, it returns a tuple, hex value, or\n    field name\n\n    There are two common use cases for ColorSpec: setting a constant value,\n    and indicating a field name to look for on the datasource:\n\n    >>> class Bar(HasProps):\n    ...     col = ColorSpec(default=\"green\")\n    ...     col2 = ColorSpec(\"colorfield\")\n\n    >>> b = Bar()\n    >>> b.col = \"red\"  # sets a fixed value of red\n    >>> b.col\n    \"red\"\n    >>> b.col = \"myfield\"  # Use the datasource field named \"myfield\"\n    >>> b.col\n    \"myfield\"\n\n    For more examples, see tests/test_glyphs.py\n    \"\"\"\n\n    NAMEDCOLORS = set(colors.__colors__)\n\n    def __init__(self, field_or_value=None, field=None, value=None, default=_NotSet, help=None):\n        # The fancy footwork below is so we auto-interpret the first positional\n        # parameter as either a field or a fixed value.  If either \"field\" or\n        # \"value\" are then supplied as keyword arguments, then those will\n        # override the inferred value from the positional argument.\n\n        self.field = field\n        self._default = default\n        self.value = value\n        self.__doc__ = help\n\n        if field_or_value is not None:\n            if self.isconst(field_or_value):\n                self.value = field_or_value\n            else:\n                self.field = field_or_value\n\n        if not (self.field is None or isinstance(self.field, string_types)):\n            raise ValueError(\"'field' must be a string or None, got %r\" % self.field)\n\n        # We need to distinguish if the user ever explicitly sets the attribute; if\n        # they explicitly set it to None, we should pass on None in the dict.\n        self._isset = False\n\n    @classmethod\n    def isconst(cls, arg):\n        \"\"\" Returns True if the argument is a literal color.  Check for a\n        well-formed hexadecimal color value.\n        \"\"\"\n        return isinstance(arg, string_types) and \\\n               ((len(arg) == 7 and arg[0] == \"#\") or arg in cls.NAMEDCOLORS)\n\n    def _formattuple(self, colortuple):\n        if isinstance(colortuple, tuple):\n            if len(colortuple) == 3:\n                return \"rgb%r\" % (colortuple,)\n            else:\n                return \"rgba%r\" % (colortuple,)\n        else:\n            return colortuple\n\n    def _get(self, obj):\n        # One key difference in ColorSpec.__get__ from the base class is\n        # that we do not call self.to_dict() in any circumstance, because\n        # this could lead to formatting color tuples as \"rgb(R,G,B)\" instead\n        # of keeping them as tuples.\n        if hasattr(obj, self._name):\n            setval = getattr(obj, self._name)\n            if self.isconst(setval) or isinstance(setval, tuple):\n                # Fixed color value\n                return setval\n            elif isinstance(setval, string_types):\n                return setval\n            elif setval is None:\n                return None\n            else:\n                # setval should be a dict at this point\n                assert(isinstance(setval, dict))\n                return setval\n        else:\n            if self.value is not None:\n                return self.value\n            if self.default != _NotSet:\n                return self.default\n            else:\n                return self.field\n\n    def __set__(self, obj, arg):\n        self._isset = True\n        if isinstance(arg, tuple):\n            if len(arg) in (3, 4):\n                # RGB or RGBa\n                pass\n            else:\n                raise RuntimeError(\"Invalid tuple being assigned to ColorSpec; must be length 2, 3, or 4.\")\n        elif isinstance(arg, colors.Color):\n            arg = arg.to_css()\n        super(ColorSpec, self).__set__(obj, arg)\n\n    def to_dict(self, obj):\n        setval = getattr(obj, self._name, None)\n        if self.default != _NotSet and not self._isset:\n            setval = self.default\n        if setval is not None:\n            if self.isconst(setval):\n                # Hexadecimal or named color\n                return {\"value\": setval}\n            elif isinstance(setval, tuple):\n                # RGB or RGBa\n                # TODO: Should we validate that alpha is between 0..1?\n                return {\"value\": self._formattuple(setval)}\n            elif isinstance(setval, string_types):\n                return {\"field\": setval}\n            elif isinstance(setval, dict):\n                # this is considerably simpler than the DataSpec case because\n                # there are no units involved, and we've handled all of the\n                # value cases above.\n                return setval.copy()\n        else:\n            if self._isset:\n                if self.value is None:\n                    return {\"value\": None}\n                else:\n                    return {\"value\": getattr(obj, self._name, self.value)}\n            else:\n                if self.value:\n                    return {\"value\": self.value}\n                return {\"field\": self.field}\n\n    def __repr__(self):\n        return \"ColorSpec(field=%r)\" % self.field",
  "class Include(object):\n    \"\"\" Include other properties from mixin Models, with a given prefix. \"\"\"\n\n    def __init__(self, delegate, help=\"\", use_prefix=True):\n        if not (isinstance(delegate, type) and issubclass(delegate, HasProps)):\n            raise ValueError(\"expected a subclass of HasProps, got %r\" % delegate)\n\n        self.delegate = delegate\n        self.help = help\n        self.use_prefix = use_prefix",
  "class MetaHasProps(type):\n    def __new__(cls, class_name, bases, class_dict):\n        names = set()\n        names_with_refs = set()\n        container_names = set()\n\n        # First pre-process to handle all the Includes\n        includes = {}\n        removes = set()\n        for name, prop in class_dict.items():\n            if not isinstance(prop, Include):\n                continue\n\n            delegate = prop.delegate\n            if prop.use_prefix:\n                prefix = re.sub(\"_props$\", \"\", name) + \"_\"\n            else:\n                prefix = \"\"\n\n            for subpropname in delegate.class_properties(withbases=False):\n                fullpropname = prefix + subpropname\n                subprop = delegate.lookup(subpropname)\n                if isinstance(subprop, Property):\n                    # If it's an actual instance, then we need to make a copy\n                    # so two properties don't write to the same hidden variable\n                    # inside the instance.\n                    subprop = copy(subprop)\n                if \"%s\" in prop.help:\n                    doc = prop.help % subpropname.replace('_', ' ')\n                else:\n                    doc = prop.help\n                try:\n                    includes[fullpropname] = subprop(help=doc)\n                except TypeError:\n                    includes[fullpropname] = subprop\n                    subprop.__doc__ = doc\n            # Remove the name of the Include attribute itself\n            removes.add(name)\n\n        # Update the class dictionary, taking care not to overwrite values\n        # from the delegates that the subclass may have explicitly defined\n        for key, val in includes.items():\n            if key not in class_dict:\n                class_dict[key] = val\n        for tmp in removes:\n            del class_dict[tmp]\n\n        dataspecs = {}\n        for name, prop in class_dict.items():\n            if isinstance(prop, Property):\n                prop.name = name\n                if prop.has_ref:\n                    names_with_refs.add(name)\n                elif isinstance(prop, ContainerProperty):\n                    container_names.add(name)\n                names.add(name)\n                if isinstance(prop, DataSpec):\n                    dataspecs[name] = prop\n\n            elif isinstance(prop, type) and issubclass(prop, Property):\n                # Support the user adding a property without using parens,\n                # i.e. using just the Property subclass instead of an\n                # instance of the subclass\n                newprop = prop.autocreate(name=name)\n                class_dict[name] = newprop\n                newprop.name = name\n                names.add(name)\n\n                # Process dataspecs\n                if issubclass(prop, DataSpec):\n                    dataspecs[name] = newprop\n\n        class_dict[\"__properties__\"] = names\n        class_dict[\"__properties_with_refs__\"] = names_with_refs\n        class_dict[\"__container_props__\"] = container_names\n        if dataspecs:\n            class_dict[\"_dataspecs\"] = dataspecs\n        return type.__new__(cls, class_name, bases, class_dict)",
  "def accumulate_from_subclasses(cls, propname):\n    s = set()\n    for c in inspect.getmro(cls):\n        if issubclass(c, HasProps):\n            s.update(getattr(c, propname))\n    return s",
  "class HasProps(object):\n\n    def __init__(self, **properties):\n        super(HasProps, self).__init__()\n        self._changed_vars = set()\n\n        for name, value in properties.items():\n            setattr(self, name, value)\n\n    def __setattr__(self, name, value):\n        props = sorted(self.properties())\n\n        if name.startswith(\"_\") or name in props:\n            super(HasProps, self).__setattr__(name, value)\n        else:\n            matches, text = difflib.get_close_matches(name.lower(), props), \"similar\"\n\n            if not matches:\n                matches, text = props, \"possible\"\n\n            raise AttributeError(\"unexpected attribute '%s' to %s, %s attributes are %s\" %\n                (name, self.__class__.__name__, text, nice_join(matches)))\n\n    def clone(self):\n        \"\"\" Returns a duplicate of this object with all its properties\n        set appropriately.  Values which are containers are shallow-copied.\n        \"\"\"\n        return self.__class__(**self.changed_properties_with_values())\n\n    @classmethod\n    def lookup(cls, name):\n        return getattr(cls, name)\n\n    @classmethod\n    def properties_with_refs(cls):\n        \"\"\" Returns a set of the names of this object's properties that\n        have references. We traverse the class hierarchy and\n        pull together the full list of properties.\n        \"\"\"\n        if not hasattr(cls, \"__cached_allprops_with_refs\"):\n            s = accumulate_from_subclasses(cls, \"__properties_with_refs__\")\n            cls.__cached_allprops_with_refs = s\n        return cls.__cached_allprops_with_refs\n\n    @classmethod\n    def properties_containers(cls):\n        \"\"\" Returns a list of properties that are containers\n        \"\"\"\n        if not hasattr(cls, \"__cached_allprops_containers\"):\n            s = accumulate_from_subclasses(cls, \"__container_props__\")\n            cls.__cached_allprops_containers = s\n        return cls.__cached_allprops_containers\n\n    @classmethod\n    def properties(cls):\n        \"\"\" Returns a set of the names of this object's properties. We\n        traverse the class hierarchy and pull together the full\n        list of properties.\n        \"\"\"\n        if not hasattr(cls, \"__cached_allprops\"):\n            s = cls.class_properties()\n            cls.__cached_allprops = s\n        return cls.__cached_allprops\n\n    @classmethod\n    def dataspecs(cls):\n        \"\"\" Returns a set of the names of this object's dataspecs (and\n        dataspec subclasses).  Traverses the class hierarchy.\n        \"\"\"\n        if not hasattr(cls, \"__cached_dataspecs\"):\n            dataspecs = set()\n            for c in reversed(inspect.getmro(cls)):\n                if hasattr(c, \"_dataspecs\"):\n                    dataspecs.update(c._dataspecs.keys())\n            cls.__cached_dataspecs = dataspecs\n        return cls.__cached_dataspecs\n\n    @classmethod\n    def dataspecs_with_refs(cls):\n        dataspecs = {}\n        for c in reversed(inspect.getmro(cls)):\n            if hasattr(c, \"_dataspecs\"):\n                dataspecs.update(c._dataspecs)\n        return dataspecs\n\n    def changed_vars(self):\n        \"\"\" Returns which variables changed since the creation of the object,\n        or the last called to reset_changed_vars().\n        \"\"\"\n        return set.union(self._changed_vars, self.properties_with_refs(),\n                         self.properties_containers())\n\n    def reset_changed_vars(self):\n        self._changed_vars = set()\n\n    def properties_with_values(self):\n        return dict([ (attr, getattr(self, attr)) for attr in self.properties() ])\n\n    def changed_properties(self):\n        return self.changed_vars()\n\n    def changed_properties_with_values(self):\n        return dict([ (attr, getattr(self, attr)) for attr in self.changed_properties() ])\n\n    @classmethod\n    def class_properties(cls, withbases=True):\n        if withbases:\n            return accumulate_from_subclasses(cls, \"__properties__\")\n        else:\n            return set(cls.__properties__)\n\n    def set(self, **kwargs):\n        \"\"\" Sets a number of properties at once \"\"\"\n        for kw in kwargs:\n            setattr(self, kw, kwargs[kw])\n\n    def pprint_props(self, indent=0):\n        \"\"\" Prints the properties of this object, nicely formatted \"\"\"\n        for key, value in self.properties_with_values().items():\n            print(\"%s%s: %r\" % (\"  \"*indent, key, value))",
  "class PrimitiveProperty(Property):\n    \"\"\" A base class for simple property types. Subclasses should\n    define a class attribute ``_underlying_type`` that is a tuple\n    of acceptable type values for the property.\n\n    \"\"\"\n\n    _underlying_type = None\n\n    def validate(self, value):\n        super(PrimitiveProperty, self).validate(value)\n\n        if not (value is None or isinstance(value, self._underlying_type)):\n            raise ValueError(\"expected a value of type %s, got %s of type %s\" %\n                (nice_join([ cls.__name__ for cls in self._underlying_type ]), value, type(value).__name__))\n\n    def from_json(self, json, models=None):\n        if json is None or isinstance(json, self._underlying_type):\n            return json\n        else:\n            expected = nice_join([ cls.__name__ for cls in self._underlying_type ])\n            raise DeserializationError(\"%s expected %s, got %s\" % (self, expected, json))",
  "class Bool(PrimitiveProperty):\n    \"\"\" Boolean type property. \"\"\"\n    _underlying_type = (bool,)",
  "class Int(PrimitiveProperty):\n    \"\"\" Signed integer type property. \"\"\"\n    _underlying_type = bokeh_integer_types",
  "class Float(PrimitiveProperty):\n    \"\"\" Floating point type property. \"\"\"\n    _underlying_type = (float, ) + bokeh_integer_types",
  "class Complex(PrimitiveProperty):\n    \"\"\" Complex floating point type property. \"\"\"\n    _underlying_type = (complex, float) + bokeh_integer_types",
  "class String(PrimitiveProperty):\n    \"\"\" String type property. \"\"\"\n    _underlying_type = string_types",
  "class Regex(String):\n    \"\"\" Regex type property validates that text values match the\n    given regular expression.\n    \"\"\"\n    def __init__(self, regex, default=None, help=None):\n        self.regex = re.compile(regex)\n        super(Regex, self).__init__(default=default, help=help)\n\n    def validate(self, value):\n        super(Regex, self).validate(value)\n\n        if not (value is None or self.regex.match(value) is not None):\n            raise ValueError(\"expected a string matching %r pattern, got %r\" % (self.regex.pattern, value))\n\n    def __str__(self):\n        return \"%s(%r)\" % (self.__class__.__name__, self.regex.pattern)",
  "class ParameterizedProperty(Property):\n    \"\"\" Base class for Properties that have type parameters, e.g.\n    ``List(String)``.\n\n    \"\"\"\n\n    @staticmethod\n    def _validate_type_param(type_param):\n        if isinstance(type_param, type):\n            if issubclass(type_param, Property):\n                return type_param()\n            else:\n                type_param = type_param.__name__\n        elif isinstance(type_param, Property):\n            return type_param\n\n        raise ValueError(\"expected a property as type parameter, got %s\" % type_param)\n\n    @property\n    def type_params(self):\n        raise NotImplementedError(\"abstract method\")\n\n    @property\n    def has_ref(self):\n        return any(type_param.has_ref for type_param in self.type_params)",
  "class ContainerProperty(ParameterizedProperty):\n    \"\"\" Base class for Container-like type properties. \"\"\"\n    pass",
  "class Seq(ContainerProperty):\n    \"\"\" Sequence (list, tuple) type property.\n\n    \"\"\"\n\n    def _is_seq(self, value):\n        return isinstance(value, collections.Container) and not isinstance(value, collections.Mapping)\n\n    def _new_instance(self, value):\n        return value\n\n    def __init__(self, item_type, default=None, help=None):\n        self.item_type = self._validate_type_param(item_type)\n        super(Seq, self).__init__(default=default, help=help)\n\n    @property\n    def type_params(self):\n        return [self.item_type]\n\n    def validate(self, value):\n        super(Seq, self).validate(value)\n\n        if value is not None:\n            if not (self._is_seq(value) and all(self.item_type.is_valid(item) for item in value)):\n                raise ValueError(\"expected an element of %s, got %r\" % (self, value))\n\n    def __str__(self):\n        return \"%s(%s)\" % (self.__class__.__name__, self.item_type)\n\n    def from_json(self, json, models=None):\n        if json is None:\n            return None\n        elif isinstance(json, list):\n            return self._new_instance([ self.item_type.from_json(item, models) for item in json ])\n        else:\n            raise DeserializationError(\"%s expected a list or None, got %s\" % (self, json))",
  "class List(Seq):\n    \"\"\" Python list type property.\n\n    \"\"\"\n\n    def __init__(self, item_type, default=[], help=None):\n        super(List, self).__init__(item_type, default=default, help=help)\n\n    def _is_seq(self, value):\n        return isinstance(value, list)",
  "class Array(Seq):\n    \"\"\" NumPy array type property.\n\n    \"\"\"\n\n    def _is_seq(self, value):\n        import numpy as np\n        return isinstance(value, np.ndarray)\n\n    def _new_instance(self, value):\n        return np.array(value)",
  "class Dict(ContainerProperty):\n    \"\"\" Python dict type property.\n\n    If a default value is passed in, then a shallow copy of it will be\n    used for each new use of this property.\n\n    \"\"\"\n\n    def __init__(self, keys_type, values_type, default={}, help=None):\n        self.keys_type = self._validate_type_param(keys_type)\n        self.values_type = self._validate_type_param(values_type)\n        super(Dict, self).__init__(default=default, help=help)\n\n    @property\n    def type_params(self):\n        return [self.keys_type, self.values_type]\n\n    def validate(self, value):\n        super(Dict, self).validate(value)\n\n        if value is not None:\n            if not (isinstance(value, dict) and \\\n                    all(self.keys_type.is_valid(key) and self.values_type.is_valid(val) for key, val in iteritems(value))):\n                raise ValueError(\"expected an element of %s, got %r\" % (self, value))\n\n    def __str__(self):\n        return \"%s(%s, %s)\" % (self.__class__.__name__, self.keys_type, self.values_type)\n\n    def from_json(self, json, models=None):\n        if json is None:\n            return None\n        elif isinstance(json, dict):\n            return { self.keys_type.from_json(key, models): self.values_type.from_json(value, models) for key, value in iteritems(json) }\n        else:\n            raise DeserializationError(\"%s expected a dict or None, got %s\" % (self, json))",
  "class Tuple(ContainerProperty):\n    \"\"\" Tuple type property. \"\"\"\n    def __init__(self, tp1, tp2, *type_params, **kwargs):\n        self._type_params = list(map(self._validate_type_param, (tp1, tp2) + type_params))\n        super(Tuple, self).__init__(default=kwargs.get(\"default\"), help=kwargs.get(\"help\"))\n\n    @property\n    def type_params(self):\n        return self._type_params\n\n    def validate(self, value):\n        super(Tuple, self).validate(value)\n\n        if value is not None:\n            if not (isinstance(value, (tuple, list)) and len(self.type_params) == len(value) and \\\n                    all(type_param.is_valid(item) for type_param, item in zip(self.type_params, value))):\n                raise ValueError(\"expected an element of %s, got %r\" % (self, value))\n\n    def __str__(self):\n        return \"%s(%s)\" % (self.__class__.__name__, \", \".join(map(str, self.type_params)))\n\n    def from_json(self, json, models=None):\n        if json is None:\n            return None\n        elif isinstance(json, list):\n            return tuple(type_param.from_json(item, models) for type_param, item in zip(self.type_params, json))\n        else:\n            raise DeserializationError(\"%s expected a list or None, got %s\" % (self, json))",
  "class Instance(Property):\n    \"\"\" Instance type property, for references to other Models in the object\n    graph.\n\n    \"\"\"\n    def __init__(self, instance_type, default=None, help=None):\n        if not isinstance(instance_type, (type,) + string_types):\n            raise ValueError(\"expected a type or string, got %s\" % instance_type)\n\n        if isinstance(instance_type, type) and not issubclass(instance_type, HasProps):\n            raise ValueError(\"expected a subclass of HasProps, got %s\" % instance_type)\n\n        self._instance_type = instance_type\n\n        super(Instance, self).__init__(default=default, help=help)\n\n    @property\n    def instance_type(self):\n        if isinstance(self._instance_type, str):\n            module, name = self._instance_type.rsplit(\".\", 1)\n            self._instance_type = getattr(import_module(module, \"bokeh\"), name)\n\n        return self._instance_type\n\n    @property\n    def has_ref(self):\n        return True\n\n    def validate(self, value):\n        super(Instance, self).validate(value)\n\n        if value is not None:\n            if not isinstance(value, self.instance_type):\n                raise ValueError(\"expected an instance of type %s, got %s of type %s\" %\n                    (self.instance_type.__name__, value, type(value).__name__))\n\n    def __str__(self):\n        return \"%s(%s)\" % (self.__class__.__name__, self.instance_type.__name__)\n\n    def from_json(self, json, models=None):\n        if json is None:\n            return None\n        elif isinstance(json, dict):\n            from .plot_object import PlotObject\n            if issubclass(self.instance_type, PlotObject):\n                if models is None:\n                    raise DeserializationError(\"%s can't deserialize without models\" % self)\n                else:\n                    model = models.get(json[\"id\"])\n\n                    if model is not None:\n                        return model\n                    else:\n                        raise DeserializationError(\"%s failed to deserilize reference to %s\" % (self, json))\n            else:\n                attrs = {}\n\n                for name, value in iteritems(json):\n                    prop = self.instance_type.lookup(name)\n                    attrs[name] = prop.from_json(value, models)\n\n                # XXX: this doesn't work when Instance(Superclass) := Subclass()\n                # Serialization dict must carry type information to resolve this.\n                return self.instance_type(**attrs)\n        else:\n            raise DeserializationError(\"%s expected a dict or None, got %s\" % (self, json))",
  "class This(Property):\n    \"\"\" A reference to an instance of the class being defined. \"\"\"\n    pass",
  "class Any(Property):\n    \"\"\" Any type property accepts any values. \"\"\"\n    pass",
  "class Function(Property):\n    \"\"\" Function type property. \"\"\"\n    pass",
  "class Event(Property):\n    \"\"\" Event type property. \"\"\"\n    pass",
  "class Interval(ParameterizedProperty):\n    ''' Range type property ensures values are contained inside a given interval. '''\n    def __init__(self, interval_type, start, end, default=None, help=None):\n        self.interval_type = self._validate_type_param(interval_type)\n        self.interval_type.validate(start)\n        self.interval_type.validate(end)\n        self.start = start\n        self.end = end\n        super(Interval, self).__init__(default=default, help=help)\n\n    @property\n    def type_params(self):\n        return [self.interval_type]\n\n    def validate(self, value):\n        super(Interval, self).validate(value)\n\n        if not (value is None or self.interval_type.is_valid(value) and value >= self.start and value <= self.end):\n            raise ValueError(\"expected a value of type %s in range [%s, %s], got %r\" % (self.interval_type, self.start, self.end, value))\n\n    def __str__(self):\n        return \"%s(%s, %r, %r)\" % (self.__class__.__name__, self.interval_type, self.start, self.end)",
  "class Byte(Interval):\n    ''' Byte type property. '''\n    def __init__(self, default=0, help=None):\n        super(Byte, self).__init__(Int, 0, 255, default=default, help=help)",
  "class Either(ParameterizedProperty):\n    \"\"\" Takes a list of valid properties and validates against them in succession. \"\"\"\n\n    def __init__(self, tp1, tp2, *type_params, **kwargs):\n        self._type_params = list(map(self._validate_type_param, (tp1, tp2) + type_params))\n        default = kwargs.get(\"default\", self._type_params[0].default)\n        help = kwargs.get(\"help\")\n        super(Either, self).__init__(default=default, help=help)\n\n    @property\n    def type_params(self):\n        return self._type_params\n\n    def validate(self, value):\n        super(Either, self).validate(value)\n\n        if not (value is None or any(param.is_valid(value) for param in self.type_params)):\n            raise ValueError(\"expected an element of either %s, got %r\" % (nice_join(self.type_params), value))\n\n    def transform(self, value):\n        for param in self.type_params:\n            try:\n                return param.transform(value)\n            except ValueError:\n                pass\n\n        raise ValueError(\"Could not transform %r\" % value)\n\n    def from_json(self, json, models=None):\n        for tp in self.type_params:\n            try:\n                return tp.from_json(json, models)\n            except DeserializationError:\n                pass\n        else:\n            raise DeserializationError(\"%s couldn't deserialize %s\" % (self, json))\n\n    def __str__(self):\n        return \"%s(%s)\" % (self.__class__.__name__, \", \".join(map(str, self.type_params)))\n\n    def __or__(self, other):\n        return self.__class__(*(self.type_params + [other]), default=self._default, help=self.help)",
  "class Enum(Property):\n    \"\"\" An Enum with a list of allowed values. The first value in the list is\n    the default value, unless a default is provided with the \"default\" keyword\n    argument.\n    \"\"\"\n    def __init__(self, enum, *values, **kwargs):\n        if not (not values and isinstance(enum, enums.Enumeration)):\n            enum = enums.enumeration(enum, *values)\n\n        self.allowed_values = enum._values\n\n        default = kwargs.get(\"default\", enum._default)\n        help = kwargs.get(\"help\")\n        super(Enum, self).__init__(default=default, help=help)\n\n    def validate(self, value):\n        super(Enum, self).validate(value)\n\n        if not (value is None or value in self.allowed_values):\n            raise ValueError(\"invalid value for %s: %r; allowed values are %s\" % (self.name, value, nice_join(self.allowed_values)))\n\n    def __str__(self):\n        return \"%s(%s)\" % (self.__class__.__name__, \", \".join(map(repr, self.allowed_values)))",
  "class Auto(Enum):\n\n    def __init__(self):\n        super(Auto, self).__init__(\"auto\")\n\n    def __str__(self):\n        return self.__class__.__name__",
  "class Color(Either):\n    \"\"\" Accepts color definition in a variety of ways, and produces an\n    appropriate serialization of its value for whatever backend.\n\n    For colors, because we support named colors and hex values prefaced\n    with a \"#\", when we are handed a string value, there is a little\n    interpretation: if the value is one of the 147 SVG named colors or\n    it starts with a \"#\", then it is interpreted as a value.\n\n    If a 3-tuple is provided, then it is treated as an RGB (0..255).\n    If a 4-tuple is provided, then it is treated as an RGBa (0..255), with\n    alpha as a float between 0 and 1.  (This follows the HTML5 Canvas API.)\n    \"\"\"\n\n    def __init__(self, default=None, help=None):\n        types = (Enum(enums.NamedColor),\n                 Regex(\"^#[0-9a-fA-F]{6}$\"),\n                 Tuple(Byte, Byte, Byte),\n                 Tuple(Byte, Byte, Byte, Percent))\n        super(Color, self).__init__(*types, default=default, help=help)\n\n    def __str__(self):\n        return self.__class__.__name__",
  "class Align(Property):\n    pass",
  "class DashPattern(Either):\n    \"\"\" Dash type property.\n\n    Express patterns that describe line dashes.  ``DashPattern`` values\n    can be specified in a variety of ways:\n\n    * An enum: \"solid\", \"dashed\", \"dotted\", \"dotdash\", \"dashdot\"\n    * a tuple or list of integers in the `HTML5 Canvas dash specification style`_.\n      Note that if the list of integers has an odd number of elements, then\n      it is duplicated, and that duplicated list becomes the new dash list.\n\n    To indicate that dashing is turned off (solid lines), specify the empty\n    list [].\n\n    .. _HTML5 Canvas dash specification style: http://www.w3.org/html/wg/drafts/2dcontext/html5_canvas/#dash-list\n\n    \"\"\"\n\n    _dash_patterns = {\n        \"solid\": [],\n        \"dashed\": [6],\n        \"dotted\": [2,4],\n        \"dotdash\": [2,4,6,4],\n        \"dashdot\": [6,4,2,4],\n    }\n\n    def __init__(self, default=[], help=None):\n        types = Enum(enums.DashPattern), Regex(r\"^(\\d+(\\s+\\d+)*)?$\"), Seq(Int)\n        super(DashPattern, self).__init__(*types, default=default, help=help)\n\n    def transform(self, value):\n        value = super(DashPattern, self).transform(value)\n\n        if isinstance(value, string_types):\n            try:\n                return self._dash_patterns[value]\n            except KeyError:\n                return [int(x) for x in  value.split()]\n        else:\n            return value\n\n    def __str__(self):\n        return self.__class__.__name__",
  "class Size(Float):\n    \"\"\" Size type property.\n\n    .. note::\n        ``Size`` is equivalent to an unsigned int.\n\n    \"\"\"\n    def validate(self, value):\n        super(Size, self).validate(value)\n\n        if not (value is None or 0.0 <= value):\n            raise ValueError(\"expected a non-negative number, got %r\" % value)",
  "class Percent(Float):\n    \"\"\" Percentage type property.\n\n    Percents are useful for specifying alphas and coverage and extents; more\n    semantically meaningful than Float(0..1).\n\n    \"\"\"\n    def validate(self, value):\n        super(Percent, self).validate(value)\n\n        if not (value is None or 0.0 <= value <= 1.0):\n            raise ValueError(\"expected a value in range [0, 1], got %r\" % value)",
  "class Angle(Float):\n    \"\"\" Angle type property. \"\"\"\n    pass",
  "class Date(Property):\n    \"\"\" Date (not datetime) type property.\n\n    \"\"\"\n    def __init__(self, default=datetime.date.today(), help=None):\n        super(Date, self).__init__(default=default, help=help)\n\n    def validate(self, value):\n        super(Date, self).validate(value)\n\n        if not (value is None or isinstance(value, (datetime.date,) + string_types + (float,) + bokeh_integer_types)):\n            raise ValueError(\"expected a date, string or timestamp, got %r\" % value)\n\n    def transform(self, value):\n        value = super(Date, self).transform(value)\n\n        if isinstance(value, (float,) + bokeh_integer_types):\n            try:\n                value = datetime.date.fromtimestamp(value)\n            except ValueError:\n                value = datetime.date.fromtimestamp(value/1000)\n        elif isinstance(value, string_types):\n            value = dateutil.parser.parse(value).date()\n\n        return value",
  "class Datetime(Property):\n    \"\"\" Datetime type property.\n\n    \"\"\"\n\n    def __init__(self, default=datetime.date.today(), help=None):\n        super(Datetime, self).__init__(default=default, help=help)\n\n    def validate(self, value):\n        super(Datetime, self).validate(value)\n\n        if (isinstance(value, (datetime.datetime, datetime.date, np.datetime64))):\n            return\n        try:\n            import pandas\n            if isinstance(value, (pandas.Timestamp)):\n                return\n        except ImportError:\n            pass\n\n        raise ValueError(\"Expected a datetime instance, got %r\" % value)\n\n    def transform(self, value):\n        value = super(Datetime, self).transform(value)\n        return value",
  "class RelativeDelta(Dict):\n    \"\"\" RelativeDelta type property for time deltas.\n\n    \"\"\"\n\n    def __init__(self, default={}, help=None):\n        keys = Enum(\"years\", \"months\", \"days\", \"hours\", \"minutes\", \"seconds\", \"microseconds\")\n        values = Int\n        super(RelativeDelta, self).__init__(keys, values, default=default, help=help)\n\n    def __str__(self):\n        return self.__class__.__name__",
  "def __init__(self, default=None, help=None):\n        \"\"\" This is how the descriptor is created in the class declaration \"\"\"\n        if isinstance(default, types.FunctionType): # aka. lazy value\n            self.validate(default())\n        else:\n            self.validate(default)\n\n        self._default = default\n        self.__doc__ = help\n        self.alternatives = []\n\n        # This gets set by the class decorator at class creation time\n        self.name = \"unnamed\"",
  "def __str__(self):\n        return self.__class__.__name__",
  "def _name(self):\n        return \"_\" + self.name",
  "def default(self):\n        if not isinstance(self._default, types.FunctionType):\n            return copy(self._default)\n        else:\n            value = self._default()\n            self.validate(value)\n            return value",
  "def autocreate(cls, name=None):\n        \"\"\" Called by the metaclass to create a\n        new instance of this descriptor\n        if the user just assigned it to a property without trailing\n        parentheses.\n        \"\"\"\n        return cls()",
  "def matches(self, new, old):\n        # XXX: originally this code warned about not being able to compare values, but that\n        # doesn't make sense, because most comparisons involving numpy arrays will fail with\n        # ValueError exception, thus warning about inevitable.\n        try:\n            if new is None or old is None:\n                return new is old           # XXX: silence FutureWarning from NumPy\n            else:\n                return new == old\n        except (KeyboardInterrupt, SystemExit):\n            raise\n        except Exception as e:\n            logger.debug(\"could not compare %s and %s for property %s (Reason: %s)\", new, old, self.name, e)\n        return False",
  "def from_json(self, json, models=None):\n        return json",
  "def transform(self, value):\n        return value",
  "def validate(self, value):\n        pass",
  "def is_valid(self, value):\n        try:\n            self.validate(value)\n        except ValueError:\n            return False\n        else:\n            return True",
  "def _get(self, obj):\n        if not hasattr(obj, self._name):\n            setattr(obj, self._name, self.default)\n        return getattr(obj, self._name)",
  "def __get__(self, obj, owner=None):\n        if obj is not None:\n            return self._get(obj)\n        elif owner is not None:\n            return self\n        else:\n            raise ValueError(\"both 'obj' and 'owner' are None, don't know what to do\")",
  "def __set__(self, obj, value):\n        try:\n            self.validate(value)\n        except ValueError as e:\n            for tp, converter in self.alternatives:\n                if tp.is_valid(value):\n                    value = converter(value)\n                    break\n            else:\n                raise e\n        else:\n            value = self.transform(value)\n\n        old = self.__get__(obj)\n        obj._changed_vars.add(self.name)\n        if self._name in obj.__dict__ and self.matches(value, old):\n            return\n        setattr(obj, self._name, value)\n        obj._dirty = True\n        if hasattr(obj, '_trigger'):\n            if hasattr(obj, '_block_callbacks') and obj._block_callbacks:\n                obj._callback_queue.append((self.name, old, value))\n            else:\n                obj._trigger(self.name, old, value)",
  "def __delete__(self, obj):\n        if hasattr(obj, self._name):\n            delattr(obj, self._name)",
  "def has_ref(self):\n        return False",
  "def accepts(self, tp, converter):\n        tp = ParameterizedProperty._validate_type_param(tp)\n        self.alternatives.append((tp, converter))\n        return self",
  "def __or__(self, other):\n        return Either(self, other)",
  "def __init__(self, field=None, units=\"data\", min_value=None, default=_NotSet, help=None):\n        \"\"\"\n        Parameters\n        ==========\n        **field** is the string name of a data column to look up.\n        **units** is either \"data\" or \"screen\"\n        \"\"\"\n        # Don't use .name because the HasProps metaclass uses that to\n        # store the attribute name on this descriptor.\n        if field is None or isinstance(field, string_types):\n            self.field = field\n        else:\n            raise ValueError(\"'field' must be a string or None, got %r\" % field)\n\n        self.units = units\n        self._default = default\n        self.min_value = min_value\n        self.__doc__ = help",
  "def autocreate(cls, name=None):\n        # In this case, use the name the user assigned this DataSpec to\n        # as the default field name.\n        d = cls(field=name)\n        return d",
  "def _get(self, obj):\n        \"\"\" Try to implement a \"natural\" interface: if the user just set\n        simple values or field names, the getter just returns those.\n        However, if the user has also overridden the \"units\" or \"default\"\n        settings, then a dictionary is returned.\n        \"\"\"\n        if hasattr(obj, self._name):\n            setval = getattr(obj, self._name)\n            if isinstance(setval, string_types):\n                # A string representing the field\n                return setval\n            elif not isinstance(setval, dict):\n                # Typically a number presenting the fixed value\n                return setval\n            else:\n                return self.to_dict(obj)\n        else:\n            # If the user hasn't set anything\n            if self.field is not None:\n                return self.field\n            if self.default != _NotSet:\n                return self.default",
  "def to_dict(self, obj):\n        # Build the complete dict\n        setval = getattr(obj, self._name, None)\n        if isinstance(setval, string_types):\n            d = {\"field\": setval, \"units\": self.units}\n        elif isinstance(setval, dict):\n            d = {\"units\": self.units}\n            d.update(setval)\n        elif setval is not None:\n            # a fixed value of some sort; no need to store the default value\n            d = {\"value\": setval, \"units\": self.units}\n        else:\n            # If the user never set a value\n            if self.field is not None:\n                d = {\"field\": self.field, \"units\": self.units}\n            elif self.default != _NotSet:\n                d = {\"value\": self.default, \"units\": self.units}\n            else:\n                d = {}\n\n        if \"value\" in d and self.min_value is not None:\n            if d[\"value\"] < self.min_value:\n                raise ValueError(\"value must be greater than %s\" % str(self.min_value))\n        return d",
  "def __repr__(self):\n        return \"DataSpec(field=%r, units=%r)\" % (self.field, self.units)",
  "def __init__(self, field_or_value=None, field=None, value=None, default=_NotSet, help=None):\n        # The fancy footwork below is so we auto-interpret the first positional\n        # parameter as either a field or a fixed value.  If either \"field\" or\n        # \"value\" are then supplied as keyword arguments, then those will\n        # override the inferred value from the positional argument.\n\n        self.field = field\n        self._default = default\n        self.value = value\n        self.__doc__ = help\n\n        if field_or_value is not None:\n            if self.isconst(field_or_value):\n                self.value = field_or_value\n            else:\n                self.field = field_or_value\n\n        if not (self.field is None or isinstance(self.field, string_types)):\n            raise ValueError(\"'field' must be a string or None, got %r\" % self.field)\n\n        # We need to distinguish if the user ever explicitly sets the attribute; if\n        # they explicitly set it to None, we should pass on None in the dict.\n        self._isset = False",
  "def isconst(cls, arg):\n        \"\"\" Returns True if the argument is a literal color.  Check for a\n        well-formed hexadecimal color value.\n        \"\"\"\n        return isinstance(arg, string_types) and \\\n               ((len(arg) == 7 and arg[0] == \"#\") or arg in cls.NAMEDCOLORS)",
  "def _formattuple(self, colortuple):\n        if isinstance(colortuple, tuple):\n            if len(colortuple) == 3:\n                return \"rgb%r\" % (colortuple,)\n            else:\n                return \"rgba%r\" % (colortuple,)\n        else:\n            return colortuple",
  "def _get(self, obj):\n        # One key difference in ColorSpec.__get__ from the base class is\n        # that we do not call self.to_dict() in any circumstance, because\n        # this could lead to formatting color tuples as \"rgb(R,G,B)\" instead\n        # of keeping them as tuples.\n        if hasattr(obj, self._name):\n            setval = getattr(obj, self._name)\n            if self.isconst(setval) or isinstance(setval, tuple):\n                # Fixed color value\n                return setval\n            elif isinstance(setval, string_types):\n                return setval\n            elif setval is None:\n                return None\n            else:\n                # setval should be a dict at this point\n                assert(isinstance(setval, dict))\n                return setval\n        else:\n            if self.value is not None:\n                return self.value\n            if self.default != _NotSet:\n                return self.default\n            else:\n                return self.field",
  "def __set__(self, obj, arg):\n        self._isset = True\n        if isinstance(arg, tuple):\n            if len(arg) in (3, 4):\n                # RGB or RGBa\n                pass\n            else:\n                raise RuntimeError(\"Invalid tuple being assigned to ColorSpec; must be length 2, 3, or 4.\")\n        elif isinstance(arg, colors.Color):\n            arg = arg.to_css()\n        super(ColorSpec, self).__set__(obj, arg)",
  "def to_dict(self, obj):\n        setval = getattr(obj, self._name, None)\n        if self.default != _NotSet and not self._isset:\n            setval = self.default\n        if setval is not None:\n            if self.isconst(setval):\n                # Hexadecimal or named color\n                return {\"value\": setval}\n            elif isinstance(setval, tuple):\n                # RGB or RGBa\n                # TODO: Should we validate that alpha is between 0..1?\n                return {\"value\": self._formattuple(setval)}\n            elif isinstance(setval, string_types):\n                return {\"field\": setval}\n            elif isinstance(setval, dict):\n                # this is considerably simpler than the DataSpec case because\n                # there are no units involved, and we've handled all of the\n                # value cases above.\n                return setval.copy()\n        else:\n            if self._isset:\n                if self.value is None:\n                    return {\"value\": None}\n                else:\n                    return {\"value\": getattr(obj, self._name, self.value)}\n            else:\n                if self.value:\n                    return {\"value\": self.value}\n                return {\"field\": self.field}",
  "def __repr__(self):\n        return \"ColorSpec(field=%r)\" % self.field",
  "def __init__(self, delegate, help=\"\", use_prefix=True):\n        if not (isinstance(delegate, type) and issubclass(delegate, HasProps)):\n            raise ValueError(\"expected a subclass of HasProps, got %r\" % delegate)\n\n        self.delegate = delegate\n        self.help = help\n        self.use_prefix = use_prefix",
  "def __new__(cls, class_name, bases, class_dict):\n        names = set()\n        names_with_refs = set()\n        container_names = set()\n\n        # First pre-process to handle all the Includes\n        includes = {}\n        removes = set()\n        for name, prop in class_dict.items():\n            if not isinstance(prop, Include):\n                continue\n\n            delegate = prop.delegate\n            if prop.use_prefix:\n                prefix = re.sub(\"_props$\", \"\", name) + \"_\"\n            else:\n                prefix = \"\"\n\n            for subpropname in delegate.class_properties(withbases=False):\n                fullpropname = prefix + subpropname\n                subprop = delegate.lookup(subpropname)\n                if isinstance(subprop, Property):\n                    # If it's an actual instance, then we need to make a copy\n                    # so two properties don't write to the same hidden variable\n                    # inside the instance.\n                    subprop = copy(subprop)\n                if \"%s\" in prop.help:\n                    doc = prop.help % subpropname.replace('_', ' ')\n                else:\n                    doc = prop.help\n                try:\n                    includes[fullpropname] = subprop(help=doc)\n                except TypeError:\n                    includes[fullpropname] = subprop\n                    subprop.__doc__ = doc\n            # Remove the name of the Include attribute itself\n            removes.add(name)\n\n        # Update the class dictionary, taking care not to overwrite values\n        # from the delegates that the subclass may have explicitly defined\n        for key, val in includes.items():\n            if key not in class_dict:\n                class_dict[key] = val\n        for tmp in removes:\n            del class_dict[tmp]\n\n        dataspecs = {}\n        for name, prop in class_dict.items():\n            if isinstance(prop, Property):\n                prop.name = name\n                if prop.has_ref:\n                    names_with_refs.add(name)\n                elif isinstance(prop, ContainerProperty):\n                    container_names.add(name)\n                names.add(name)\n                if isinstance(prop, DataSpec):\n                    dataspecs[name] = prop\n\n            elif isinstance(prop, type) and issubclass(prop, Property):\n                # Support the user adding a property without using parens,\n                # i.e. using just the Property subclass instead of an\n                # instance of the subclass\n                newprop = prop.autocreate(name=name)\n                class_dict[name] = newprop\n                newprop.name = name\n                names.add(name)\n\n                # Process dataspecs\n                if issubclass(prop, DataSpec):\n                    dataspecs[name] = newprop\n\n        class_dict[\"__properties__\"] = names\n        class_dict[\"__properties_with_refs__\"] = names_with_refs\n        class_dict[\"__container_props__\"] = container_names\n        if dataspecs:\n            class_dict[\"_dataspecs\"] = dataspecs\n        return type.__new__(cls, class_name, bases, class_dict)",
  "def __init__(self, **properties):\n        super(HasProps, self).__init__()\n        self._changed_vars = set()\n\n        for name, value in properties.items():\n            setattr(self, name, value)",
  "def __setattr__(self, name, value):\n        props = sorted(self.properties())\n\n        if name.startswith(\"_\") or name in props:\n            super(HasProps, self).__setattr__(name, value)\n        else:\n            matches, text = difflib.get_close_matches(name.lower(), props), \"similar\"\n\n            if not matches:\n                matches, text = props, \"possible\"\n\n            raise AttributeError(\"unexpected attribute '%s' to %s, %s attributes are %s\" %\n                (name, self.__class__.__name__, text, nice_join(matches)))",
  "def clone(self):\n        \"\"\" Returns a duplicate of this object with all its properties\n        set appropriately.  Values which are containers are shallow-copied.\n        \"\"\"\n        return self.__class__(**self.changed_properties_with_values())",
  "def lookup(cls, name):\n        return getattr(cls, name)",
  "def properties_with_refs(cls):\n        \"\"\" Returns a set of the names of this object's properties that\n        have references. We traverse the class hierarchy and\n        pull together the full list of properties.\n        \"\"\"\n        if not hasattr(cls, \"__cached_allprops_with_refs\"):\n            s = accumulate_from_subclasses(cls, \"__properties_with_refs__\")\n            cls.__cached_allprops_with_refs = s\n        return cls.__cached_allprops_with_refs",
  "def properties_containers(cls):\n        \"\"\" Returns a list of properties that are containers\n        \"\"\"\n        if not hasattr(cls, \"__cached_allprops_containers\"):\n            s = accumulate_from_subclasses(cls, \"__container_props__\")\n            cls.__cached_allprops_containers = s\n        return cls.__cached_allprops_containers",
  "def properties(cls):\n        \"\"\" Returns a set of the names of this object's properties. We\n        traverse the class hierarchy and pull together the full\n        list of properties.\n        \"\"\"\n        if not hasattr(cls, \"__cached_allprops\"):\n            s = cls.class_properties()\n            cls.__cached_allprops = s\n        return cls.__cached_allprops",
  "def dataspecs(cls):\n        \"\"\" Returns a set of the names of this object's dataspecs (and\n        dataspec subclasses).  Traverses the class hierarchy.\n        \"\"\"\n        if not hasattr(cls, \"__cached_dataspecs\"):\n            dataspecs = set()\n            for c in reversed(inspect.getmro(cls)):\n                if hasattr(c, \"_dataspecs\"):\n                    dataspecs.update(c._dataspecs.keys())\n            cls.__cached_dataspecs = dataspecs\n        return cls.__cached_dataspecs",
  "def dataspecs_with_refs(cls):\n        dataspecs = {}\n        for c in reversed(inspect.getmro(cls)):\n            if hasattr(c, \"_dataspecs\"):\n                dataspecs.update(c._dataspecs)\n        return dataspecs",
  "def changed_vars(self):\n        \"\"\" Returns which variables changed since the creation of the object,\n        or the last called to reset_changed_vars().\n        \"\"\"\n        return set.union(self._changed_vars, self.properties_with_refs(),\n                         self.properties_containers())",
  "def reset_changed_vars(self):\n        self._changed_vars = set()",
  "def properties_with_values(self):\n        return dict([ (attr, getattr(self, attr)) for attr in self.properties() ])",
  "def changed_properties(self):\n        return self.changed_vars()",
  "def changed_properties_with_values(self):\n        return dict([ (attr, getattr(self, attr)) for attr in self.changed_properties() ])",
  "def class_properties(cls, withbases=True):\n        if withbases:\n            return accumulate_from_subclasses(cls, \"__properties__\")\n        else:\n            return set(cls.__properties__)",
  "def set(self, **kwargs):\n        \"\"\" Sets a number of properties at once \"\"\"\n        for kw in kwargs:\n            setattr(self, kw, kwargs[kw])",
  "def pprint_props(self, indent=0):\n        \"\"\" Prints the properties of this object, nicely formatted \"\"\"\n        for key, value in self.properties_with_values().items():\n            print(\"%s%s: %r\" % (\"  \"*indent, key, value))",
  "def validate(self, value):\n        super(PrimitiveProperty, self).validate(value)\n\n        if not (value is None or isinstance(value, self._underlying_type)):\n            raise ValueError(\"expected a value of type %s, got %s of type %s\" %\n                (nice_join([ cls.__name__ for cls in self._underlying_type ]), value, type(value).__name__))",
  "def from_json(self, json, models=None):\n        if json is None or isinstance(json, self._underlying_type):\n            return json\n        else:\n            expected = nice_join([ cls.__name__ for cls in self._underlying_type ])\n            raise DeserializationError(\"%s expected %s, got %s\" % (self, expected, json))",
  "def __init__(self, regex, default=None, help=None):\n        self.regex = re.compile(regex)\n        super(Regex, self).__init__(default=default, help=help)",
  "def validate(self, value):\n        super(Regex, self).validate(value)\n\n        if not (value is None or self.regex.match(value) is not None):\n            raise ValueError(\"expected a string matching %r pattern, got %r\" % (self.regex.pattern, value))",
  "def __str__(self):\n        return \"%s(%r)\" % (self.__class__.__name__, self.regex.pattern)",
  "def _validate_type_param(type_param):\n        if isinstance(type_param, type):\n            if issubclass(type_param, Property):\n                return type_param()\n            else:\n                type_param = type_param.__name__\n        elif isinstance(type_param, Property):\n            return type_param\n\n        raise ValueError(\"expected a property as type parameter, got %s\" % type_param)",
  "def type_params(self):\n        raise NotImplementedError(\"abstract method\")",
  "def has_ref(self):\n        return any(type_param.has_ref for type_param in self.type_params)",
  "def _is_seq(self, value):\n        return isinstance(value, collections.Container) and not isinstance(value, collections.Mapping)",
  "def _new_instance(self, value):\n        return value",
  "def __init__(self, item_type, default=None, help=None):\n        self.item_type = self._validate_type_param(item_type)\n        super(Seq, self).__init__(default=default, help=help)",
  "def type_params(self):\n        return [self.item_type]",
  "def validate(self, value):\n        super(Seq, self).validate(value)\n\n        if value is not None:\n            if not (self._is_seq(value) and all(self.item_type.is_valid(item) for item in value)):\n                raise ValueError(\"expected an element of %s, got %r\" % (self, value))",
  "def __str__(self):\n        return \"%s(%s)\" % (self.__class__.__name__, self.item_type)",
  "def from_json(self, json, models=None):\n        if json is None:\n            return None\n        elif isinstance(json, list):\n            return self._new_instance([ self.item_type.from_json(item, models) for item in json ])\n        else:\n            raise DeserializationError(\"%s expected a list or None, got %s\" % (self, json))",
  "def __init__(self, item_type, default=[], help=None):\n        super(List, self).__init__(item_type, default=default, help=help)",
  "def _is_seq(self, value):\n        return isinstance(value, list)",
  "def _is_seq(self, value):\n        import numpy as np\n        return isinstance(value, np.ndarray)",
  "def _new_instance(self, value):\n        return np.array(value)",
  "def __init__(self, keys_type, values_type, default={}, help=None):\n        self.keys_type = self._validate_type_param(keys_type)\n        self.values_type = self._validate_type_param(values_type)\n        super(Dict, self).__init__(default=default, help=help)",
  "def type_params(self):\n        return [self.keys_type, self.values_type]",
  "def validate(self, value):\n        super(Dict, self).validate(value)\n\n        if value is not None:\n            if not (isinstance(value, dict) and \\\n                    all(self.keys_type.is_valid(key) and self.values_type.is_valid(val) for key, val in iteritems(value))):\n                raise ValueError(\"expected an element of %s, got %r\" % (self, value))",
  "def __str__(self):\n        return \"%s(%s, %s)\" % (self.__class__.__name__, self.keys_type, self.values_type)",
  "def from_json(self, json, models=None):\n        if json is None:\n            return None\n        elif isinstance(json, dict):\n            return { self.keys_type.from_json(key, models): self.values_type.from_json(value, models) for key, value in iteritems(json) }\n        else:\n            raise DeserializationError(\"%s expected a dict or None, got %s\" % (self, json))",
  "def __init__(self, tp1, tp2, *type_params, **kwargs):\n        self._type_params = list(map(self._validate_type_param, (tp1, tp2) + type_params))\n        super(Tuple, self).__init__(default=kwargs.get(\"default\"), help=kwargs.get(\"help\"))",
  "def type_params(self):\n        return self._type_params",
  "def validate(self, value):\n        super(Tuple, self).validate(value)\n\n        if value is not None:\n            if not (isinstance(value, (tuple, list)) and len(self.type_params) == len(value) and \\\n                    all(type_param.is_valid(item) for type_param, item in zip(self.type_params, value))):\n                raise ValueError(\"expected an element of %s, got %r\" % (self, value))",
  "def __str__(self):\n        return \"%s(%s)\" % (self.__class__.__name__, \", \".join(map(str, self.type_params)))",
  "def from_json(self, json, models=None):\n        if json is None:\n            return None\n        elif isinstance(json, list):\n            return tuple(type_param.from_json(item, models) for type_param, item in zip(self.type_params, json))\n        else:\n            raise DeserializationError(\"%s expected a list or None, got %s\" % (self, json))",
  "def __init__(self, instance_type, default=None, help=None):\n        if not isinstance(instance_type, (type,) + string_types):\n            raise ValueError(\"expected a type or string, got %s\" % instance_type)\n\n        if isinstance(instance_type, type) and not issubclass(instance_type, HasProps):\n            raise ValueError(\"expected a subclass of HasProps, got %s\" % instance_type)\n\n        self._instance_type = instance_type\n\n        super(Instance, self).__init__(default=default, help=help)",
  "def instance_type(self):\n        if isinstance(self._instance_type, str):\n            module, name = self._instance_type.rsplit(\".\", 1)\n            self._instance_type = getattr(import_module(module, \"bokeh\"), name)\n\n        return self._instance_type",
  "def has_ref(self):\n        return True",
  "def validate(self, value):\n        super(Instance, self).validate(value)\n\n        if value is not None:\n            if not isinstance(value, self.instance_type):\n                raise ValueError(\"expected an instance of type %s, got %s of type %s\" %\n                    (self.instance_type.__name__, value, type(value).__name__))",
  "def __str__(self):\n        return \"%s(%s)\" % (self.__class__.__name__, self.instance_type.__name__)",
  "def from_json(self, json, models=None):\n        if json is None:\n            return None\n        elif isinstance(json, dict):\n            from .plot_object import PlotObject\n            if issubclass(self.instance_type, PlotObject):\n                if models is None:\n                    raise DeserializationError(\"%s can't deserialize without models\" % self)\n                else:\n                    model = models.get(json[\"id\"])\n\n                    if model is not None:\n                        return model\n                    else:\n                        raise DeserializationError(\"%s failed to deserilize reference to %s\" % (self, json))\n            else:\n                attrs = {}\n\n                for name, value in iteritems(json):\n                    prop = self.instance_type.lookup(name)\n                    attrs[name] = prop.from_json(value, models)\n\n                # XXX: this doesn't work when Instance(Superclass) := Subclass()\n                # Serialization dict must carry type information to resolve this.\n                return self.instance_type(**attrs)\n        else:\n            raise DeserializationError(\"%s expected a dict or None, got %s\" % (self, json))",
  "def __init__(self, interval_type, start, end, default=None, help=None):\n        self.interval_type = self._validate_type_param(interval_type)\n        self.interval_type.validate(start)\n        self.interval_type.validate(end)\n        self.start = start\n        self.end = end\n        super(Interval, self).__init__(default=default, help=help)",
  "def type_params(self):\n        return [self.interval_type]",
  "def validate(self, value):\n        super(Interval, self).validate(value)\n\n        if not (value is None or self.interval_type.is_valid(value) and value >= self.start and value <= self.end):\n            raise ValueError(\"expected a value of type %s in range [%s, %s], got %r\" % (self.interval_type, self.start, self.end, value))",
  "def __str__(self):\n        return \"%s(%s, %r, %r)\" % (self.__class__.__name__, self.interval_type, self.start, self.end)",
  "def __init__(self, default=0, help=None):\n        super(Byte, self).__init__(Int, 0, 255, default=default, help=help)",
  "def __init__(self, tp1, tp2, *type_params, **kwargs):\n        self._type_params = list(map(self._validate_type_param, (tp1, tp2) + type_params))\n        default = kwargs.get(\"default\", self._type_params[0].default)\n        help = kwargs.get(\"help\")\n        super(Either, self).__init__(default=default, help=help)",
  "def type_params(self):\n        return self._type_params",
  "def validate(self, value):\n        super(Either, self).validate(value)\n\n        if not (value is None or any(param.is_valid(value) for param in self.type_params)):\n            raise ValueError(\"expected an element of either %s, got %r\" % (nice_join(self.type_params), value))",
  "def transform(self, value):\n        for param in self.type_params:\n            try:\n                return param.transform(value)\n            except ValueError:\n                pass\n\n        raise ValueError(\"Could not transform %r\" % value)",
  "def from_json(self, json, models=None):\n        for tp in self.type_params:\n            try:\n                return tp.from_json(json, models)\n            except DeserializationError:\n                pass\n        else:\n            raise DeserializationError(\"%s couldn't deserialize %s\" % (self, json))",
  "def __str__(self):\n        return \"%s(%s)\" % (self.__class__.__name__, \", \".join(map(str, self.type_params)))",
  "def __or__(self, other):\n        return self.__class__(*(self.type_params + [other]), default=self._default, help=self.help)",
  "def __init__(self, enum, *values, **kwargs):\n        if not (not values and isinstance(enum, enums.Enumeration)):\n            enum = enums.enumeration(enum, *values)\n\n        self.allowed_values = enum._values\n\n        default = kwargs.get(\"default\", enum._default)\n        help = kwargs.get(\"help\")\n        super(Enum, self).__init__(default=default, help=help)",
  "def validate(self, value):\n        super(Enum, self).validate(value)\n\n        if not (value is None or value in self.allowed_values):\n            raise ValueError(\"invalid value for %s: %r; allowed values are %s\" % (self.name, value, nice_join(self.allowed_values)))",
  "def __str__(self):\n        return \"%s(%s)\" % (self.__class__.__name__, \", \".join(map(repr, self.allowed_values)))",
  "def __init__(self):\n        super(Auto, self).__init__(\"auto\")",
  "def __str__(self):\n        return self.__class__.__name__",
  "def __init__(self, default=None, help=None):\n        types = (Enum(enums.NamedColor),\n                 Regex(\"^#[0-9a-fA-F]{6}$\"),\n                 Tuple(Byte, Byte, Byte),\n                 Tuple(Byte, Byte, Byte, Percent))\n        super(Color, self).__init__(*types, default=default, help=help)",
  "def __str__(self):\n        return self.__class__.__name__",
  "def __init__(self, default=[], help=None):\n        types = Enum(enums.DashPattern), Regex(r\"^(\\d+(\\s+\\d+)*)?$\"), Seq(Int)\n        super(DashPattern, self).__init__(*types, default=default, help=help)",
  "def transform(self, value):\n        value = super(DashPattern, self).transform(value)\n\n        if isinstance(value, string_types):\n            try:\n                return self._dash_patterns[value]\n            except KeyError:\n                return [int(x) for x in  value.split()]\n        else:\n            return value",
  "def __str__(self):\n        return self.__class__.__name__",
  "def validate(self, value):\n        super(Size, self).validate(value)\n\n        if not (value is None or 0.0 <= value):\n            raise ValueError(\"expected a non-negative number, got %r\" % value)",
  "def validate(self, value):\n        super(Percent, self).validate(value)\n\n        if not (value is None or 0.0 <= value <= 1.0):\n            raise ValueError(\"expected a value in range [0, 1], got %r\" % value)",
  "def __init__(self, default=datetime.date.today(), help=None):\n        super(Date, self).__init__(default=default, help=help)",
  "def validate(self, value):\n        super(Date, self).validate(value)\n\n        if not (value is None or isinstance(value, (datetime.date,) + string_types + (float,) + bokeh_integer_types)):\n            raise ValueError(\"expected a date, string or timestamp, got %r\" % value)",
  "def transform(self, value):\n        value = super(Date, self).transform(value)\n\n        if isinstance(value, (float,) + bokeh_integer_types):\n            try:\n                value = datetime.date.fromtimestamp(value)\n            except ValueError:\n                value = datetime.date.fromtimestamp(value/1000)\n        elif isinstance(value, string_types):\n            value = dateutil.parser.parse(value).date()\n\n        return value",
  "def __init__(self, default=datetime.date.today(), help=None):\n        super(Datetime, self).__init__(default=default, help=help)",
  "def validate(self, value):\n        super(Datetime, self).validate(value)\n\n        if (isinstance(value, (datetime.datetime, datetime.date, np.datetime64))):\n            return\n        try:\n            import pandas\n            if isinstance(value, (pandas.Timestamp)):\n                return\n        except ImportError:\n            pass\n\n        raise ValueError(\"Expected a datetime instance, got %r\" % value)",
  "def transform(self, value):\n        value = super(Datetime, self).transform(value)\n        return value",
  "def __init__(self, default={}, help=None):\n        keys = Enum(\"years\", \"months\", \"days\", \"hours\", \"minutes\", \"seconds\", \"microseconds\")\n        values = Int\n        super(RelativeDelta, self).__init__(keys, values, default=default, help=help)",
  "def __str__(self):\n        return self.__class__.__name__",
  "def make_id():\n    global _simple_id\n    from . import settings\n    if settings.simple_ids(False):\n        _simple_id += 1\n        new_id = _simple_id\n    else:\n        new_id = uuid.uuid4()\n    return str(new_id)",
  "def urljoin(*args):\n    return reduce(sys_urljoin, args)",
  "def get_json(response):\n    \"\"\"unifying retrieving json from an http response,for requests <1.0, >1.0, and\n    flask test client\n    \"\"\"\n    if isinstance(response, flask.Response):\n        #flask testing\n        return json.loads(response.data.decode('utf-8'))\n    else:\n        #requests\n        if hasattr(response.json, '__call__'):\n            return response.json()\n        else:\n            return response.json",
  "def encode_utf8(u):\n    if sys.version_info[0] == 2:\n        u = u.encode('utf-8')\n    return u",
  "def decode_utf8(u):\n    if sys.version_info[0] == 2:\n        u = u.decode('utf-8')\n    return u",
  "def scale_delta(time):\n    if time > 0.0:\n        order = min(-int(math.floor(math.log10(time)) // 3), 3)\n    else:\n        order = 3\n\n    return time*_scales[order], _units[order]",
  "def is_py3():\n    return sys.version_info[0] == 3",
  "def is_pypy():\n    return platform.python_implementation() == \"PyPy\"",
  "def convert_references(json_obj):\n    from .plot_object import PlotObject\n    from .properties import HasProps\n    def convert(obj):\n        if isinstance(obj, PlotObject):\n            return obj.ref\n        elif isinstance(obj, HasProps):\n            return obj.to_dict()\n        else:\n            return obj\n    def helper(json_obj):\n        if isinstance(json_obj, list):\n            for idx, x in enumerate(json_obj):\n                json_obj[idx] = convert(x)\n        if isinstance(json_obj, dict):\n            for k, x in iteritems(json_obj):\n                json_obj[k] = convert(x)\n    json_apply(json_obj, helper)\n    return json_obj",
  "def dump(objs, docid, changed_only=True):\n    \"\"\" Dump a sequence of objects into JSON\n\n        Args:\n            changed_only (bool, optional) : whether to dump only attributes\n                that have had their values changed at some point (default: True)\n    \"\"\"\n    json_objs = []\n    for obj in objs:\n        ref = obj.ref\n        ref[\"attributes\"] = obj.vm_serialize(changed_only=changed_only)\n        ref[\"attributes\"].update({\"id\": ref[\"id\"], \"doc\" : docid})\n        json_objs.append(ref)\n    return json_objs",
  "def nice_join(seq, sep=\", \"):\n    seq = [str(x) for x in seq]\n\n    if len(seq) <= 1:\n        return sep.join(seq)\n    else:\n        return \"%s or %s\" % (sep.join(seq[:-1]), seq[-1])",
  "def publish_display_data(data, source='bokeh'):\n    \"\"\"Compatibility wrapper for IPython publish_display_data which removes the\n    `source` (first) argument in later versions.\n\n    Parameters\n    ----------\n    source : str\n    data : dict\n    \"\"\"\n    import IPython.core.displaypub as displaypub\n    try:\n        displaypub.publish_display_data(source, data)\n    except TypeError:\n        displaypub.publish_display_data(data)",
  "def is_ref(frag):\n    return isinstance(frag, dict) and \\\n           frag.get('type') and \\\n           frag.get('id')",
  "def json_apply(fragment, check_func, func):\n    \"\"\"recursively searches through a nested dict/lists\n    if check_func(fragment) is True, then we return\n    func(fragment)\n    \"\"\"\n    if check_func(fragment):\n        return func(fragment)\n    elif isinstance(fragment, list):\n        output = []\n        for val in fragment:\n            output.append(json_apply(val, check_func, func))\n        return output\n    elif isinstance(fragment, dict):\n        output = {}\n        for k, val in fragment.items():\n            output[k] = json_apply(val, check_func, func)\n        return output\n    else:\n        return fragment",
  "def resolve_json(fragment, models):\n    check_func = is_ref\n    def func(fragment):\n        if fragment['id'] in models:\n            return models[fragment['id']]\n        else:\n            logging.error(\"model not found for %s\", fragment)\n            return None\n    return json_apply(fragment, check_func, func)",
  "def convert(obj):\n        if isinstance(obj, PlotObject):\n            return obj.ref\n        elif isinstance(obj, HasProps):\n            return obj.to_dict()\n        else:\n            return obj",
  "def helper(json_obj):\n        if isinstance(json_obj, list):\n            for idx, x in enumerate(json_obj):\n                json_obj[idx] = convert(x)\n        if isinstance(json_obj, dict):\n            for k, x in iteritems(json_obj):\n                json_obj[k] = convert(x)",
  "def func(fragment):\n        if fragment['id'] in models:\n            return models[fragment['id']]\n        else:\n            logging.error(\"model not found for %s\", fragment)\n            return None",
  "def get_default_color(plot=None):\n    colors = [\n      \"#1f77b4\",\n      \"#ff7f0e\", \"#ffbb78\",\n      \"#2ca02c\", \"#98df8a\",\n      \"#d62728\", \"#ff9896\",\n      \"#9467bd\", \"#c5b0d5\",\n      \"#8c564b\", \"#c49c94\",\n      \"#e377c2\", \"#f7b6d2\",\n      \"#7f7f7f\",\n      \"#bcbd22\", \"#dbdb8d\",\n      \"#17becf\", \"#9edae5\"\n    ]\n    if plot:\n        renderers = plot.renderers\n        renderers = [x for x in renderers if x.__view_model__ == \"GlyphRenderer\"]\n        num_renderers = len(renderers)\n        return colors[num_renderers]\n    else:\n        return colors[0]",
  "def get_default_alpha(plot=None):\n    return 1.0",
  "def _glyph_doc(args, props, desc):\n    params_tuple =tuple(itertools.chain.from_iterable(sorted(list(args.items()))))\n    params = \"\\t%s : %s\\n\" * len(args) % params_tuple\n\n    return \"\"\"%s\n\n    Parameters\n    ----------\n    %s\n    Additionally, the following properties are accepted as keyword arguments: %s\n\n    Returns\n    -------\n    plot : :py:class:`Plot <bokeh.models.Plot>`\n    \"\"\" % (desc, params, props)",
  "def _match_data_params(argnames, glyphclass, datasource,\n                       args, kwargs):\n    \"\"\" Processes the arguments and kwargs passed in to __call__ to line\n    them up with the argnames of the underlying Glyph\n\n    Returns\n    ---------\n    glyph_params : dict of params that should be in the glyphspec\n    \"\"\"\n    # Go through the list of position and keyword arguments, matching up\n    # the full list of required glyph data attributes\n    attributes = dict(zip(argnames, args))\n    if len(args) < len(argnames):\n        for argname in argnames[len(args):]:\n            if argname in kwargs:\n                attributes[argname] = kwargs.pop(argname)\n            else:\n                raise RuntimeError(\"Missing required glyph parameter '%s'\" % argname)\n    # Go through keys in alpha order, so that *_units are handled after\n    # the dataspec dict is already created\n    dataspecs = glyphclass.dataspecs_with_refs()\n    for kw in kwargs:\n        if (kw.endswith(\"_units\") and kw[:-6] in dataspecs) or kw in dataspecs:\n            attributes[kw] = kwargs[kw]\n\n    glyph_params = {}\n    for var in sorted(attributes.keys()):\n        val = attributes[var]\n\n        if var.endswith(\"_units\") and var[:-6] in dataspecs:\n            dspec = var[:-6]\n            if dspec not in glyph_params:\n                raise RuntimeError(\"Cannot set units on undefined field '%s'\" % dspec)\n            curval = glyph_params[dspec]\n            if not isinstance(curval, dict):\n                # TODO: This assumes that string values are fields; this is invalid\n                # for ColorSpecs, but all this logic is to handle dataspec units, and\n                # ColorSpecs do not have units.  However, if there are other kinds of\n                # DataSpecs that do have string constants, then we will need to fix\n                # this up to have smarter detection of field names.\n                if isinstance(curval, string_types):\n                    glyph_params[dspec] = {\"field\": curval, \"units\": val}\n                else:\n                    glyph_params[dspec] = {\"value\": curval, \"units\": val}\n            else:\n                glyph_params[dspec][\"units\"] = val\n            continue\n\n        if isinstance(val, dict) or isinstance(val, Number):\n            glyph_val = val\n        elif isinstance(dataspecs.get(var, None), ColorSpec) and (ColorSpec.isconst(val) or val is None):\n            # This check for color constants needs to happen relatively early on because\n            # both strings and certain iterables are valid colors.\n            glyph_val = val\n        elif isinstance(val, string_types):\n            if glyphclass == glyphs.Text: # XXX: issubclass()\n                # TODO (bev) this is hacky, now that text is a DataSpec, it has to be a sequence\n                glyph_val = [val]\n            elif not isinstance(datasource, RemoteSource) and val not in datasource.column_names:\n                raise RuntimeError(\"Column name '%s' does not appear in data source %r\" % (val, datasource))\n            else:\n                if val not in datasource.column_names:\n                    datasource.column_names.append(val)\n                    datasource.data[val] = []\n                units = getattr(dataspecs[var], 'units', 'data')\n                glyph_val = {'field' : val, 'units' : units}\n        elif isinstance(val, np.ndarray):\n            if val.ndim != 1:\n                raise RuntimeError(\"Columns need to be 1D (%s is not)\" % var)\n            datasource.add(val, name=var)\n            units = getattr(dataspecs[var], 'units', 'data')\n            glyph_val = {'field' : var, 'units' : units}\n        elif isinstance(val, Iterable):\n            datasource.add(val, name=var)\n            units = getattr(dataspecs[var], 'units', 'data')\n            glyph_val = {'field' : var, 'units' : units}\n        else:\n            raise RuntimeError(\"Unexpected column type: %s\" % type(val))\n        glyph_params[var] = glyph_val\n    return glyph_params",
  "def _update_plot_data_ranges(plot, datasource, xcols, ycols):\n    \"\"\"\n    Parameters\n    ----------\n    plot : plot\n    datasource : datasource\n    xcols : names of columns that are in the X axis\n    ycols : names of columns that are in the Y axis\n    \"\"\"\n    if isinstance(plot.x_range, DataRange1d):\n        x_column_ref = [x for x in plot.x_range.sources if x.source == datasource]\n        if len(x_column_ref) > 0:\n            x_column_ref = x_column_ref[0]\n            for cname in xcols:\n                if cname not in x_column_ref.columns:\n                    x_column_ref.columns.append(cname)\n        else:\n            plot.x_range.sources.append(datasource.columns(*xcols))\n        plot.x_range._dirty = True\n\n    if isinstance(plot.y_range, DataRange1d):\n        y_column_ref = [y for y in plot.y_range.sources if y.source == datasource]\n        if len(y_column_ref) > 0:\n            y_column_ref = y_column_ref[0]\n            for cname in ycols:\n                if cname not in y_column_ref.columns:\n                    y_column_ref.columns.append(cname)\n        else:\n            plot.y_range.sources.append(datasource.columns(*ycols))\n        plot.y_range._dirty = True",
  "def _materialize_colors_and_alpha(kwargs, prefix=\"\", default_alpha=1.0):\n    \"\"\"\n    Given a kwargs dict, a prefix, and a default value, looks for different\n    color and alpha fields of the given prefix, and fills in the default value\n    if it doesn't exist.\n    \"\"\"\n    kwargs = kwargs.copy()\n\n    # TODO: The need to do this and the complexity of managing this kind of\n    # thing throughout the codebase really suggests that we need to have\n    # a real stylesheet class, where defaults and Types can declaratively\n    # substitute for this kind of imperative logic.\n    color = kwargs.pop(prefix+\"color\", get_default_color())\n    for argname in (\"fill_color\", \"line_color\"):\n        kwargs[argname] = kwargs.get(prefix + argname, color)\n\n    # NOTE: text fill color should really always default to black, hard coding\n    # this here now untils the stylesheet solution exists\n    kwargs[\"text_color\"] = kwargs.get(prefix + \"text_color\", \"black\")\n\n    alpha = kwargs.pop(prefix+\"alpha\", default_alpha)\n    for argname in (\"fill_alpha\", \"line_alpha\", \"text_alpha\"):\n        kwargs[argname] = kwargs.get(prefix + argname, alpha)\n\n    return kwargs",
  "def _get_legend(plot):\n    legend = [x for x in plot.renderers if x.__view_model__ == \"Legend\"]\n    if len(legend) > 0:\n        legend = legend[0]\n    else:\n        legend = None\n    return legend",
  "def _make_legend(plot):\n    legend = Legend(plot=plot)\n    plot.renderers.append(legend)\n    plot._dirty = True\n    return legend",
  "def _get_select_tool(plot):\n    \"\"\"returns select tool on a plot, if it's there\n    \"\"\"\n    select_tool = [x for x in plot.tools if x.__view_model__ == \"BoxSelectTool\"]\n    if len(select_tool) > 0:\n        select_tool = select_tool[0]\n    else:\n        select_tool = None\n    return select_tool",
  "def _get_range(range_input):\n    if range_input is None:\n        return DataRange1d()\n    if isinstance(range_input, Range):\n        return range_input\n    if isinstance(range_input, Sequence):\n        if all(isinstance(x, string_types) for x in range_input):\n            return FactorRange(factors=range_input)\n        if len(range_input) == 2:\n            try:\n                return Range1d(start=range_input[0], end=range_input[1])\n            except ValueError: # @mattpap suggests ValidationError instead\n                pass\n    raise ValueError(\"Unrecognized range input: '%s'\" % str(range_input))",
  "def _get_axis_class(axis_type, range_input):\n    if axis_type is None:\n        return None\n    elif axis_type == \"linear\":\n        return LinearAxis\n    elif axis_type == \"log\":\n        return LogAxis\n    elif axis_type == \"datetime\":\n        return DatetimeAxis\n    elif axis_type == \"auto\":\n        if isinstance(range_input, FactorRange):\n            return CategoricalAxis\n        elif isinstance(range_input, Range1d):\n            try:\n                # Easier way to validate type of Range1d parameters\n                Datetime.validate(Datetime(), range_input.start)\n                return DatetimeAxis\n            except ValueError:\n                pass\n        return LinearAxis\n    else:\n        raise ValueError(\"Unrecognized axis_type: '%r'\" % axis_type)",
  "def _get_num_minor_ticks(axis_class, num_minor_ticks):\n    if isinstance(num_minor_ticks, int):\n        if num_minor_ticks <= 1:\n            raise ValueError(\"num_minor_ticks must be > 1\")\n        return num_minor_ticks\n    if num_minor_ticks is None:\n        return 0\n    if num_minor_ticks == 'auto':\n        if axis_class is LogAxis:\n            return 10\n        return 5",
  "def _tool_from_string(name):\n    \"\"\" Takes a string and returns a corresponding `Tool` instance. \"\"\"\n    known_tools = sorted(_known_tools.keys())\n\n    if name in known_tools:\n        tool_fn = _known_tools[name]\n\n        if isinstance(tool_fn, string_types):\n            tool_fn = _known_tools[tool_fn]\n\n        return tool_fn()\n    else:\n        matches, text = difflib.get_close_matches(name.lower(), known_tools), \"similar\"\n\n        if not matches:\n            matches, text = known_tools, \"possible\"\n\n        raise ValueError(\"unexpected tool name '%s', %s tools are %s\" % (name, text, nice_join(matches)))",
  "def _process_tools_arg(plot, tools):\n    \"\"\" Adds tools to the plot object\n\n    Args:\n        plot (Plot): instance of a plot object\n        tools (seq[Tool or str]|str): list of tool types or string listing the\n            tool names. Those are converted using the _tool_from_string\n            function. I.e.: `wheel_zoom,box_zoom,reset`.\n\n    Returns:\n        list of Tools objects added to plot\n    \"\"\"\n    tool_objs = []\n    temp_tool_str = \"\"\n    repeated_tools = []\n\n    if isinstance(tools, (list, tuple)):\n        for tool in tools:\n            if isinstance(tool, Tool):\n                tool_objs.append(tool)\n            elif isinstance(tool, string_types):\n                temp_tool_str += tool + ','\n            else:\n                raise ValueError(\"tool should be a string or an instance of Tool class\")\n        tools = temp_tool_str\n\n    for tool in re.split(r\"\\s*,\\s*\", tools.strip()):\n        # re.split will return empty strings; ignore them.\n        if tool == \"\":\n            continue\n\n        tool_obj = _tool_from_string(tool)\n        tool_objs.append(tool_obj)\n\n    for typename, group in itertools.groupby(\n            sorted([tool.__class__.__name__ for tool in tool_objs])):\n        if len(list(group)) > 1:\n            repeated_tools.append(typename)\n\n    if repeated_tools:\n        warnings.warn(\"%s are being repeated\" % \",\".join(repeated_tools))\n\n    return tool_objs",
  "def _new_xy_plot(x_range=None, y_range=None, plot_width=None, plot_height=None,\n                 x_axis_type=\"auto\", y_axis_type=\"auto\",\n                 x_axis_location=\"below\", y_axis_location=\"left\",\n                 x_minor_ticks='auto', y_minor_ticks='auto',\n                 tools=\"pan,wheel_zoom,box_zoom,save,resize,reset\", **kw):\n    # Accept **kw to absorb other arguments which the actual factory functions\n    # might pass in, but that we don't care about\n\n    plot = Plot()\n    plot.title = kw.pop(\"title\", \"Plot\")\n\n    plot.toolbar_location = kw.pop(\"toolbar_location\", \"above\")\n\n    plot.x_range = _get_range(x_range)\n    plot.y_range = _get_range(y_range)\n\n    if plot_width: plot.plot_width = plot_width\n    if plot_height: plot.plot_height = plot_height\n\n    x_axiscls = _get_axis_class(x_axis_type, plot.x_range)\n    if x_axiscls:\n        if x_axiscls is LogAxis:\n            plot.x_mapper_type = 'log'\n        xaxis = x_axiscls(plot=plot)\n        xaxis.ticker.num_minor_ticks = _get_num_minor_ticks(x_axiscls, x_minor_ticks)\n        axis_label = kw.pop('x_axis_label', None)\n        if axis_label:\n            xaxis.axis_label = axis_label\n        xgrid = Grid(plot=plot, dimension=0, ticker=xaxis.ticker)\n        if x_axis_location == \"above\":\n            plot.above.append(xaxis)\n        elif x_axis_location == \"below\":\n            plot.below.append(xaxis)\n\n    y_axiscls = _get_axis_class(y_axis_type, plot.y_range)\n    if y_axiscls:\n        if y_axiscls is LogAxis:\n            plot.y_mapper_type = 'log'\n        yaxis = y_axiscls(plot=plot)\n        yaxis.ticker.num_minor_ticks = _get_num_minor_ticks(y_axiscls, y_minor_ticks)\n        axis_label = kw.pop('y_axis_label', None)\n        if axis_label:\n            yaxis.axis_label = axis_label\n        ygrid = Grid(plot=plot, dimension=1, ticker=yaxis.ticker)\n        if y_axis_location == \"left\":\n            plot.left.append(yaxis)\n        elif y_axis_location == \"right\":\n            plot.right.append(yaxis)\n\n    border_args = [\"min_border\", \"min_border_top\", \"min_border_bottom\", \"min_border_left\", \"min_border_right\"]\n    for arg in border_args:\n        if arg in kw:\n            setattr(plot, arg, kw.pop(arg))\n\n    fill_args = [\"background_fill\", \"border_fill\"]\n    for arg in fill_args:\n        if arg in kw:\n            setattr(plot, arg, kw.pop(arg))\n\n    style_arg_prefix = [\"title\", \"outline\"]\n    for prefix in style_arg_prefix:\n        for k in list(kw):\n            if k.startswith(prefix):\n                setattr(plot, k, kw.pop(k))\n\n    if 'toolbar_location' in list(kw):\n        plot.toolbar_location = kw.pop('toolbar_location')\n\n    tool_objs = []\n    temp_tool_str = str()\n\n    if isinstance(tools, list):\n        for tool in tools:\n            if isinstance(tool, Tool):\n                tool_objs.append(tool)\n            elif isinstance(tool, string_types):\n                temp_tool_str+=tool + ','\n            else:\n                raise ValueError(\"tool should be a string or an instance of Tool class\")\n        tools = temp_tool_str\n\n    repeated_tools = []\n\n    for tool in re.split(r\"\\s*,\\s*\", tools.strip()):\n        # re.split will return empty strings; ignore them.\n        if tool == \"\":\n            continue\n\n        tool_obj = _tool_from_string(tool)\n        tool_obj.plot = plot\n\n        tool_objs.append(tool_obj)\n\n    plot.tools.extend(tool_objs)\n\n    for typename, group in itertools.groupby(sorted([ tool.__class__.__name__ for tool in plot.tools ])):\n        if len(list(group)) > 1:\n            repeated_tools.append(typename)\n\n    if repeated_tools:\n        warnings.warn(\"%s are being repeated\" % \",\".join(repeated_tools))\n\n    return plot",
  "def _handle_1d_data_args(args, datasource=None, create_autoindex=True,\n        suggested_names=[]):\n    \"\"\" Returns a datasource and a list of names corresponding (roughly)\n    to the input data.  If only a single array was given, and an index\n    array was created, then the index's name is returned first.\n    \"\"\"\n    arrays = []\n    if datasource is None:\n        datasource = ColumnDataSource()\n    # First process all the arguments to homogenize shape.  After this\n    # process, \"arrays\" should contain a uniform list of string/ndarray/iterable\n    # corresponding to the inputs.\n    for arg in args:\n        if isinstance(arg, string_types):\n            # This has to be handled before our check for Iterable\n            arrays.append(arg)\n\n        elif isinstance(arg, np.ndarray):\n            if arg.ndim == 1:\n                arrays.append(arg)\n            else:\n                arrays.extend(arg)\n\n        elif isinstance(arg, Iterable):\n            arrays.append(arg)\n\n        elif isinstance(arg, Number):\n            arrays.append([arg])\n\n    # Now handle the case when they've only provided a single array of\n    # inputs (i.e. just the values, and no x positions).  Generate a new\n    # dummy array for this.\n    if create_autoindex and len(arrays) == 1:\n        arrays.insert(0, np.arange(len(arrays[0])))\n\n    # Now put all the data into a DataSource, or insert into existing one\n    names = []\n    for i, ary in enumerate(arrays):\n        if isinstance(ary, string_types):\n            name = ary\n        else:\n            if i < len(suggested_names):\n                name = suggested_names[i]\n            elif i == 0 and create_autoindex:\n                name = datasource.add(ary, name=\"_autoindex\")\n            else:\n                name = datasource.add(ary)\n        names.append(name)\n    return names, datasource",
  "class _list_attr_splat(list):\n    def __setattr__(self, attr, value):\n        for x in self:\n            setattr(x, attr, value)",
  "def __setattr__(self, attr, value):\n        for x in self:\n            setattr(x, attr, value)",
  "class OR(object): pass",
  "class IN(object): pass",
  "class GT(object): pass",
  "class LT(object): pass",
  "class EQ(object): pass",
  "class GEQ(object): pass",
  "class LEQ(object): pass",
  "class NEQ(object): pass",
  "def match(obj, selector, context={}):\n    ''' Test whether a particular object matches a given\n    selector.\n\n    Args:\n        obj (PlotObject) : object to Test\n        selector (JSON-like) : query selector\n            See module docs for details\n\n    Returns:\n        bool : True if the object matches, False otherwise\n\n    There are two selector keyss that are handled specially. The first\n    is 'type', which will do an isinstance check::\n\n        >>> from bokeh.plotting import line\n        >>> from bokeh.models import Axis\n        >>> p = line([1,2,3], [4,5,6])\n        >>> len(list(p.select({'type': Axis})))\n        2\n\n    There is also a a 'tags' attribute that `PlotObject` objects have,\n    that is a list of user-supplied values. The 'tags' selector key can\n    be used to query against this list of tags. An object matches if\n    any of the tags in the selector match any of the tags on the\n    object::\n\n        >>> from bokeh.plotting import line\n        >>> from bokeh.models import Axis\n        >>> p = line([1,2,3], [4,5,6])\n        >>> p.tags = [\"my plot\", 10]\n        >>> len(list(p.select({'tags': \"my plot\"})))\n        1\n        >>> len(list(p.select({'tags': [\"my plot\", 10]})))\n        1\n\n    '''\n    for key, val in selector.items():\n\n        # test attributes\n        if isinstance(key, string_types):\n\n            # special case 'type'\n            if key == \"type\":\n                # type supports IN, check for that first\n                if isinstance(val, dict) and list(val.keys()) == [IN]:\n                    if not any(isinstance(obj, x) for x in val[IN]): return False\n                # otherwise just check the type of the object against val\n                elif not isinstance(obj, val): return False\n\n            # special case 'tag'\n            elif key == 'tags':\n                if isinstance(val, string_types):\n                    if val not in obj.tags: return False\n                else:\n                    try:\n                        if not set(val) & set(obj.tags): return False\n                    except TypeError:\n                        if val not in obj.tags: return False\n\n            # if the object doesn't have the attr, it doesn't match\n            elif not hasattr(obj, key): return False\n\n            # if the value to check is a dict, recurse\n            else:\n                attr = getattr(obj, key)\n                if callable(attr):\n                    try:\n                        if not attr(val, **context): return False\n                    except:\n                        return False\n\n                elif isinstance(val, dict):\n                    if not match(attr, val): return False\n\n                else:\n                    if attr != val: return False\n\n        # test OR conditionals\n        elif key is OR:\n            if not _or(obj, val): return False\n\n        # test operands\n        elif key in _operators:\n            if not _operators[key](obj, val): return False\n\n        else:\n            raise ValueError(\"malformed query selector\")\n\n    return True",
  "def find(objs, selector, context={}):\n    ''' Query an object and all of its contained references\n    and yield objects that match the given selector.\n\n    Args:\n        obj (PlotObject) : object to query\n        selector (JSON-like) : query selector\n            See module docs for details\n\n    Yields:\n        PlotObject : objects that match the query\n\n    Examples:\n\n    '''\n    return (obj for obj in objs if match(obj, selector, context))",
  "def _or(obj, selectors):\n    return any(match(obj, selector) for selector in selectors)",
  "class DataIntegrityException(Exception):\n    pass",
  "class AuthenticationException(Exception):\n    pass",
  "class FillProps(HasProps):\n    \"\"\" Properties to use when performing fill operations while rendering.\n\n    Mirrors the BokehJS ``properties.Fill`` class.\n\n    \"\"\"\n\n    fill_color = ColorSpec(default=\"gray\", help=\"\"\"\n    A color to use to fill paths with.\n\n    Acceptable values are:\n\n    - any of the 147 named `CSS colors`_, e.g ``'green'``, ``'indigo'``\n    - an RGB(A) hex value, e.g., ``'#FF0000'``, ``'#44444444'``\n    - a 3-tuple of integers (r,g,b) between 0 and 255\n    - a 4-tuple of (r,g,b,a) where r,g,b are integers between 0..255 and a is between 0..1\n\n    .. _CSS colors: http://www.w3schools.com/cssref/css_colornames.asp\n\n    \"\"\")\n\n    fill_alpha = DataSpec(default=1.0, help=\"\"\"\n    An alpha value to use to fill paths with.\n\n    Acceptable values are floating point numbers between 0 (transparent)\n    and 1 (opaque).\n\n    \"\"\")",
  "class LineProps(HasProps):\n    \"\"\" Properties to use when performing stroke operations while rendering.\n\n    Mirrors the BokehJS ``properties.Line`` class.\n\n    \"\"\"\n\n    line_color = ColorSpec(default=\"black\", help=\"\"\"\n    A color to use to stroke paths with.\n\n    Acceptable values are:\n\n    - any of the 147 named `CSS colors`_, e.g ``'green'``, ``'indigo'``\n    - an RGB(A) hex value, e.g., ``'#FF0000'``, ``'#44444444'``\n    - a 3-tuple of integers (r,g,b) between 0 and 255\n    - a 4-tuple of (r,g,b,a) where r,g,b are integers between 0..255 and a is between 0..1\n\n    .. _CSS colors: http://www.w3schools.com/cssref/css_colornames.asp\n\n    \"\"\")\n\n    line_width = DataSpec(\"line_width\", default=1, help=\"\"\"\n    Stroke width in units of pixels.\n    \"\"\")\n\n    line_alpha = DataSpec(default=1.0, help=\"\"\"\n    An alpha value to use to stroke paths with.\n\n    Acceptable values are floating point numbers between 0 (transparent)\n    and 1 (opaque).\n\n    \"\"\")\n\n    line_join = Enum(LineJoin, help=\"\"\"\n    How path segments should be joined together.\n\n    Acceptable values are:\n\n    - ``'miter'`` |miter_join|\n    - ``'round'`` |round_join|\n    - ``'bevel'`` |bevel_join|\n\n    .. |miter_join| image:: /_images/miter_join.png\n       :height: 20\n    .. |round_join| image:: /_images/round_join.png\n       :height: 20\n    .. |bevel_join| image:: /_images/bevel_join.png\n       :height: 20\n\n    \"\"\")\n\n    line_cap = Enum(LineCap, help=\"\"\"\n    How path segments should be terminated.\n\n    Acceptable values are:\n\n    - ``'butt'`` |butt_cap|\n    - ``'round'`` |round_cap|\n    - ``'square'`` |square_cap|\n\n    .. |butt_cap| image:: /_images/butt_cap.png\n       :height: 20\n    .. |round_cap| image:: /_images/round_cap.png\n       :height: 20\n    .. |square_cap| image:: /_images/square_cap.png\n       :height: 20\n\n    \"\"\")\n\n    line_dash = DashPattern(help=\"\"\"\n    How should the line be dashed.\n    \"\"\")\n\n    line_dash_offset = Int(0, help=\"\"\"\n    The distance into the ``line_dash`` (in pixels) that the pattern should\n    start from.\n    \"\"\")",
  "class TextProps(HasProps):\n    \"\"\" Properties to use when performing text drawing operations while\n    rendering.\n\n    Mirrors the BokehJS ``properties.Text`` class.\n\n    .. note::\n        There is currently only support for filling text. An interface\n        to stroke the outlines of text has not yet been exposed.\n\n    \"\"\"\n\n    text_font = String(\"Helvetica\", help=\"\"\"\n    Name of a font to use for rendering text, e.g., ``'times'``,\n    ``'helvetica'``.\n\n    \"\"\")\n\n    text_font_size = String(\"12pt\")\n\n    text_font_style = Enum(FontStyle, help=\"\"\"\n    A style to use for rendering text.\n\n    Acceptable values are:\n\n    - ``'normal'`` normal text\n    - ``'italic'`` *italic text*\n    - ``'bold'`` **bold text**\n\n    \"\"\")\n\n    text_color = ColorSpec(default=\"#444444\", help=\"\"\"\n    A color to use to fill text with.\n\n    Acceptable values are:\n\n    - any of the 147 named `CSS colors`_, e.g ``'green'``, ``'indigo'``\n    - an RGB(A) hex value, e.g., ``'#FF0000'``, ``'#44444444'``\n    - a 3-tuple of integers (r,g,b) between 0 and 255\n    - a 4-tuple of (r,g,b,a) where r,g,b are integers between 0..255 and a is between 0..1\n\n    .. _CSS colors: http://www.w3schools.com/cssref/css_colornames.asp\n\n    \"\"\")\n\n    text_alpha = DataSpec(default=1.0, help=\"\"\"\n    An alpha value to use to fill text with.\n\n    Acceptable values are floating point numbers between 0 (transparent)\n    and 1 (opaque).\n\n    \"\"\")\n\n    text_align = Enum(TextAlign, help=\"\"\"\n    Horizontal anchor point to use when rendering text.\n\n    Acceptable values are:\n\n    - ``'left'``\n    - ``'right'``\n    - ``'center'``\n\n    \"\"\")\n\n    text_baseline = Enum(TextBaseline, default=\"bottom\", help=\"\"\"\n    Vertical anchor point to use when rendering text.\n\n    Acceptable values are:\n\n    - ``'top'``\n    - ``'middle'``\n    - ``'bottom'``\n    - ``'alphabetic'``\n    - ``'hanging'``\n\n    \"\"\")",
  "def get_browser_controller(browser=None):\n    browser = settings.browser(browser)\n\n    if browser is not None:\n        if browser == 'none':\n            class DummyWebBrowser(object):\n                def open(self, url, new=0, autoraise=True):\n                    pass\n\n            controller = DummyWebBrowser()\n        else:\n            controller = webbrowser.get(browser)\n    else:\n        controller = webbrowser\n\n    return controller",
  "def view(location, browser=None, new=\"same\", autoraise=True):\n        \"\"\" Opens a browser to view the specified location.\n\n        Args:\n            location (str) : location to open\n                If location does not begin with \"http:\" it is assumed\n                to be a file path on the local filesystem.\n            browser (str) : what browser to use\n            new (str) : how to open the location\n                Valid values are:\n                    * \"same\" - open in the current tab\n                    * \"tab\" - open a new tab in the current window\n                    * \"window\" - open in a new window\n            autoraise (bool) : whether to raise the new location\n\n        Returns:\n            None\n\n        \"\"\"\n        new_map = { \"same\": 0, \"window\": 1, \"tab\": 2 }\n        if location.startswith(\"http\"):\n            url = location\n        else:\n            url = \"file://\" + abspath(location)\n\n        try:\n            controller = get_browser_controller(browser)\n            controller.open(url, new=new_map[new], autoraise=autoraise)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except:\n            pass",
  "class DummyWebBrowser(object):\n                def open(self, url, new=0, autoraise=True):\n                    pass",
  "def open(self, url, new=0, autoraise=True):\n                    pass",
  "class Enumeration(object):\n    pass",
  "def enumeration(*values):\n    if not (values and all(isinstance(value, string_types) and value for value in values)):\n        raise ValueError(\"expected a non-empty sequence of strings, got %s\" % values)\n\n    if len(values) != len(set(values)):\n        raise ValueError(\"enumeration items must be unique, got %s\" % values)\n\n    attrs = dict([ (value, value) for value in values ])\n    attrs.update({\n        \"__slots__\": [],\n        \"_values\": list(values),\n        \"_default\": values[0],\n    })\n\n    return type(\"Enumeration\", (Enumeration,), attrs)()",
  "def convert_color(mplcolor):\n    \"Converts mpl color formats to Bokeh color formats.\"\n    charmap = dict(b=\"blue\", g=\"green\", r=\"red\", c=\"cyan\", m=\"magenta\",\n                   y=\"yellow\", k=\"black\", w=\"white\")\n    if mplcolor in charmap:\n        return charmap[mplcolor]\n\n    try:\n        colorfloat = float(mplcolor)\n        if 0 <= colorfloat <= 1.0:\n            # This is a grayscale value\n            return tuple([int(255 * colorfloat)] * 3)\n    except:\n        pass\n\n    if isinstance(mplcolor, tuple):\n        # These will be floats in the range 0..1\n        return int(255 * mplcolor[0]), int(255 * mplcolor[1]), int(255 * mplcolor[2])\n\n    return mplcolor",
  "def convert_dashes(dash):\n    \"\"\" Converts a Matplotlib dash specification\n\n    bokeh.properties.DashPattern supports the matplotlib named dash styles,\n    but not the little shorthand characters.  This function takes care of\n    mapping those.\n    \"\"\"\n    mpl_dash_map = {\n        \"-\": \"solid\",\n        \"--\": \"dashed\",\n        \":\": \"dotted\",\n        \"-.\": \"dashdot\",\n    }\n    # If the value doesn't exist in the map, then just return the value back.\n    return mpl_dash_map.get(dash, dash)",
  "def delete_last_col(x):\n    \"Just delete the last column of the array.\"\n    x = np.delete(x, (-1), axis=1)\n    return x",
  "def get_props_cycled(col, prop, fx=lambda x: x):\n    \"\"\" We need to cycle the `get.property` list (where property can be colors,\n    line_width, etc) as matplotlib does. We use itertools tools for do this\n    cycling ans slice manipulation.\n\n    Parameters:\n\n    col: matplotlib collection object\n    prop: property we want to get from matplotlib collection\n    fx: funtion (optional) to transform the elements from list obtained\n        after the property call. Deafults to identity function.\n    \"\"\"\n    n = len(col.get_paths())\n    t_prop = [fx(x) for x in prop]\n    sliced = islice(cycle(t_prop), None, n)\n    return list(sliced)",
  "def is_ax_end(r):\n    \"Check if the 'name' (if it exists) in the Glyph's datasource is 'ax_end'\"\n    if isinstance(r, GlyphRenderer):\n        try:\n            if r.data_source.data[\"name\"] == \"ax_end\":\n                return True\n        except KeyError as e:\n            return False\n    else:\n        return False",
  "def xkcd_line(x, y, xlim=None, ylim=None, mag=1.0, f1=30, f2=0.001, f3=5):\n    \"\"\"\n    Mimic a hand-drawn line from (x, y) data\n    Source: http://jakevdp.github.io/blog/2012/10/07/xkcd-style-plots-in-matplotlib/\n\n    Parameters\n    ----------\n    x, y : array_like\n        arrays to be modified\n    xlim, ylim : data range\n        the assumed plot range for the modification.  If not specified,\n        they will be guessed from the  data\n    mag : float\n        magnitude of distortions\n    f1, f2, f3 : int, float, int\n        filtering parameters.  f1 gives the size of the window, f2 gives\n        the high-frequency cutoff, f3 gives the size of the filter\n\n    Returns\n    -------\n    x, y : ndarrays\n        The modified lines\n    \"\"\"\n    x = np.asarray(x)\n    y = np.asarray(y)\n\n    # get limits for rescaling\n    if xlim is None:\n        xlim = (x.min(), x.max())\n    if ylim is None:\n        ylim = (y.min(), y.max())\n\n    if xlim[1] == xlim[0]:\n        xlim = ylim\n\n    if ylim[1] == ylim[0]:\n        ylim = xlim\n\n    # scale the data\n    x_scaled = (x - xlim[0]) * 1. / (xlim[1] - xlim[0])\n    y_scaled = (y - ylim[0]) * 1. / (ylim[1] - ylim[0])\n\n    # compute the total distance along the path\n    dx = x_scaled[1:] - x_scaled[:-1]\n    dy = y_scaled[1:] - y_scaled[:-1]\n    dist_tot = np.sum(np.sqrt(dx * dx + dy * dy))\n\n    # number of interpolated points is proportional to the distance\n    Nu = int(200 * dist_tot)\n    u = np.arange(-1, Nu + 1) * 1. / (Nu - 1)\n\n    # interpolate curve at sampled points\n    k = min(3, len(x) - 1)\n    res = interpolate.splprep([x_scaled, y_scaled], s=0, k=k)\n    x_int, y_int = interpolate.splev(u, res[0])\n\n    # we'll perturb perpendicular to the drawn line\n    dx = x_int[2:] - x_int[:-2]\n    dy = y_int[2:] - y_int[:-2]\n    dist = np.sqrt(dx * dx + dy * dy)\n\n    # create a filtered perturbation\n    coeffs = mag * np.random.normal(0, 0.01, len(x_int) - 2)\n    b = signal.firwin(f1, f2 * dist_tot, window=('kaiser', f3))\n    response = signal.lfilter(b, 1, coeffs)\n\n    x_int[1:-1] += response * dy / dist\n    y_int[1:-1] += response * dx / dist\n\n    # un-scale data\n    x_int = x_int[1:-1] * (xlim[1] - xlim[0]) + xlim[0]\n    y_int = y_int[1:-1] * (ylim[1] - ylim[0]) + ylim[0]\n\n    return x_int, y_int",
  "class BokehRenderer(Renderer):\n\n    def __init__(self, pd_obj, xkcd):\n        \"Initial setup.\"\n        self.fig = None\n        self.pd_obj = pd_obj\n        self.xkcd = xkcd\n        self.source = ColumnDataSource()\n        self.xdr = DataRange1d()\n        self.ydr = DataRange1d()\n        self.non_text = [] # to save the text we don't want to convert by draw_text\n\n    def open_figure(self, fig, props):\n        \"Get the main plot properties and create the plot.\"\n        self.width = int(props['figwidth'] * props['dpi'])\n        self.height = int(props['figheight'] * props['dpi'])\n        self.plot = Plot(x_range=self.xdr,\n                         y_range=self.ydr,\n                         plot_width=self.width,\n                         plot_height=self.height)\n\n    def close_figure(self, fig):\n        \"Complete the plot: add tools.\"\n        # Add tools\n        tool_objs = _process_tools_arg(self.plot, DEFAULT_TOOLS)\n        self.plot.add_tools(*tool_objs)\n\n        # Simple or Grid plot setup\n        if len(fig.axes) <= 1:\n            self.fig = self.plot\n        else:\n            # This list comprehension splits the plot.renderers list at the \"marker\"\n            # points returning small sublists corresponding with each subplot.\n            subrends = [list(x[1]) for x in itertools.groupby(\n                        self.plot.renderers, lambda x: is_ax_end(x)) if not x[0]]\n            plots = []\n            for i, axes in enumerate(fig.axes):\n                # create a new plot for each subplot\n                _plot = Plot(x_range=self.xdr,\n                             y_range=self.ydr,\n                             plot_width=self.width,\n                             plot_height=self.height)\n                _plot.title = \"\"\n                # and add new tools\n                _tool_objs = _process_tools_arg(_plot, DEFAULT_TOOLS)\n                _plot.add_tools(*_tool_objs)\n                # clean the plot ref from axis and grids\n                _plot_rends = subrends[i]\n                for r in _plot_rends:\n                    if not isinstance(r, GlyphRenderer):\n                        r.plot = None\n                # add all the renderers into the new subplot\n                _plot.add_layout(_plot_rends[0], 'below')  # xaxis\n                _plot.add_layout(_plot_rends[1], 'left')  # yaxis\n                _plot.add_layout(_plot_rends[2])  # xgrid\n                _plot.add_layout(_plot_rends[3])  # ygrid\n                for r in _plot_rends[4:]:  # all the glyphs\n                    _plot.renderers.append(r)\n                plots.append(_plot)\n            (a, b, c) = fig.axes[0].get_geometry()\n            p = np.array(plots)\n            n = np.resize(p, (a, b))\n            grid = GridPlot(children=n.tolist())\n            self.fig = grid\n\n    def open_axes(self, ax, props):\n        \"Get axes data and create the axes and grids\"\n        # Get axes, title and grid into class attributes.\n        self.ax = ax\n        self.plot.title = ax.get_title()\n        # to avoid title conversion by draw_text later\n        self.non_text.append(self.plot.title)\n        self.grid = ax.get_xgridlines()[0]\n\n        # Add axis\n        bxaxis = self.make_axis(ax.xaxis, \"below\", props['xscale'])\n        byaxis = self.make_axis(ax.yaxis, \"left\", props['yscale'])\n\n        # Add grids\n        self.make_grid(bxaxis, 0)\n        self.make_grid(byaxis, 1)\n\n        # Setup collections info\n        nones = (\"\", \" \", \"None\", \"none\", None)\n        cols = [col for col in self.ax.collections if col.get_paths() not in nones]\n\n        # Add collections renderers\n        [self.make_line_collection(col) for col in cols if isinstance(col, mpl.collections.LineCollection)]\n        [self.make_poly_collection(col) for col in cols if isinstance(col, mpl.collections.PolyCollection)]\n\n    def close_axes(self, ax):\n        \"Complete the axes adding axes-dependent plot props\"\n        background_fill = ax.get_axis_bgcolor()\n        if background_fill == 'w':\n            background_fill = 'white'\n        self.plot.background_fill = background_fill\n        if self.xkcd:\n            self.plot.title_text_font = \"Comic Sans MS, Textile, cursive\"\n            self.plot.title_text_font_style = \"bold\"\n            self.plot.title_text_color = \"black\"\n\n        # Add a \"marker\" Glyph to help the plot.renderers splitting in the GridPlot build\n        dummy_source = ColumnDataSource(data=dict(name=\"ax_end\"))\n        self.plot.renderers.append(GlyphRenderer(data_source=dummy_source, glyph=X()))\n\n    def open_legend(self, legend, props):\n        pass\n\n    def close_legend(self, legend):\n        pass\n\n    def draw_line(self, data, coordinates, style, label, mplobj=None):\n        \"Given a mpl line2d instance create a Bokeh Line glyph.\"\n        _x = data[:, 0]\n        if self.pd_obj is True:\n            try:\n                x = [pd.Period(ordinal=int(i), freq=self.ax.xaxis.freq).to_timestamp() for i in _x]\n            except AttributeError as e: #  we probably can make this one more intelligent later\n                x = _x\n        else:\n            x = _x\n\n        y = data[:, 1]\n        if self.xkcd:\n            x, y = xkcd_line(x, y)\n\n        line = Line()\n        line.x = self.source.add(x)\n        line.y = self.source.add(y)\n        self.xdr.sources.append(self.source.columns(line.x))\n        self.ydr.sources.append(self.source.columns(line.y))\n\n        line.line_color = style['color']\n        line.line_width = style['linewidth']\n        line.line_alpha = style['alpha']\n        line.line_dash = [int(i) for i in style['dasharray'].split(\",\")]  # str2list(int)\n        #style['zorder'] # not in Bokeh\n        #line.line_join = line2d.get_solid_joinstyle() # not in mplexporter\n        #line.line_cap = cap_style_map[line2d.get_solid_capstyle()] # not in mplexporter\n        if self.xkcd:\n            line.line_width = 3\n\n        self.plot.add_glyph(self.source, line)\n\n    def draw_markers(self, data, coordinates, style, label, mplobj=None):\n        \"Given a mpl line2d instance create a Bokeh Marker glyph.\"\n        x = data[:, 0]\n        y = data[:, 1]\n\n        marker_map = {\n            \"o\": Circle,\n            \"s\": Square,\n            \"+\": Cross,\n            \"^\": Triangle,\n            \"v\": InvertedTriangle,\n            \"x\": X,\n            \"D\": Diamond,\n            \"*\": Asterisk,\n        }\n\n        # Not all matplotlib markers are currently handled; fall back to Circle if we encounter an\n        # unhandled marker.  See http://matplotlib.org/api/markers_api.html for a list of markers.\n        try:\n            marker = marker_map[style['marker']]()\n        except KeyError:\n            warnings.warn(\"Unable to handle marker: %s; defaulting to Circle\" % style['marker'])\n            marker = Circle()\n        marker.x = self.source.add(x)\n        marker.y = self.source.add(y)\n        self.xdr.sources.append(self.source.columns(marker.x))\n        self.ydr.sources.append(self.source.columns(marker.y))\n\n        marker.line_color = style['edgecolor']\n        marker.fill_color = style['facecolor']\n        marker.line_width = style['edgewidth']\n        marker.size = style['markersize']\n        marker.fill_alpha = marker.line_alpha = style['alpha']\n        #style['zorder'] # not in Bokeh\n\n        self.plot.add_glyph(self.source, marker)\n\n    def draw_path_collection(self, paths, path_coordinates, path_transforms,\n                             offsets, offset_coordinates, offset_order,\n                             styles, mplobj=None):\n        \"\"\"Path not implemented in Bokeh, but we have our own line ans poly\n        collection implementations, so passing here to avoid the NonImplemented\n        error.\n        \"\"\"\n        pass\n\n    def draw_text(self, text, position, coordinates, style,\n                  text_type=None, mplobj=None):\n        \"Given a mpl text instance create a Bokeh Text glyph.\"\n        # mpl give you the title and axes names as a text object (with specific locations)\n        # inside the plot itself. That does not make sense inside Bokeh, so we\n        # just skip the title and axes names from the conversion and covert any other text.\n        if text not in self.non_text:\n          x, y = position\n          text = Text(x=x, y=y, text=[text])\n\n          alignment_map = {\"center\": \"middle\", \"top\": \"top\", \"bottom\": \"bottom\", \"baseline\": \"bottom\"}\n          # baseline not implemented in Bokeh, deafulting to bottom.\n          text.text_alpha = style['alpha']\n          text.text_font_size = \"%dpx\" % style['fontsize']\n          text.text_color = style['color']\n          text.text_align = style['halign']\n          text.text_baseline = alignment_map[style['valign']]\n          text.angle = style['rotation']\n          #style['zorder'] # not in Bokeh\n\n          ## Using get_fontname() works, but it's oftentimes not available in the browser,\n          ## so it's better to just use the font family here.\n          #text.text_font = mplText.get_fontname()) not in mplexporter\n          #text.text_font = mplText.get_fontfamily()[0] # not in mplexporter\n          #text.text_font_style = fontstyle_map[mplText.get_fontstyle()] # not in mplexporter\n          ## we don't really have the full range of font weights, but at least handle bold\n          #if mplText.get_weight() in (\"bold\", \"heavy\"):\n              #text.text_font_style = bold\n\n          self.plot.add_glyph(self.source, text)\n\n    def draw_image(self, imdata, extent, coordinates, style, mplobj=None):\n        pass\n\n    def make_axis(self, ax, location, scale):\n        \"Given a mpl axes instance, returns a Bokeh LinearAxis object.\"\n        # TODO:\n        #  * handle log scaling\n        #  * map `labelpad` to `major_label_standoff`\n        #  * deal with minor ticks once BokehJS supports them\n        #  * handle custom tick locations once that is added to bokehJS\n\n        # we need to keep the current axes names to avoid writing them in draw_text\n        self.non_text.append(ax.get_label_text())\n\n        if scale == \"linear\":\n            laxis = LinearAxis(axis_label=ax.get_label_text())\n        elif scale == \"date\":\n            laxis = DatetimeAxis(axis_label=ax.get_label_text())\n\n        self.plot.add_layout(laxis, location)\n\n        # First get the label properties by getting an mpl.Text object\n        #label = ax.get_label()\n        #self.text_props(label, laxis, prefix=\"axis_label_\")\n        #self.draw_text(label, position, coordinates, style, text_type=\"axis_label_\")\n\n        # To get the tick label format, we look at the first of the tick labels\n        # and assume the rest are formatted similarly.\n        #ticktext = ax.get_ticklabels()[0]\n        #self.text_props(ticktext, laxis, prefix=\"major_label_\")\n        #self.draw_text(ticktext, position, coordinates, style, text_type=\"major_label_\")\n\n        #newaxis.bounds = axis.get_data_interval()  # I think this is the right func...\n\n        if self.xkcd:\n            laxis.axis_line_width = 3\n            laxis.axis_label_text_font = \"Comic Sans MS, Textile, cursive\"\n            laxis.axis_label_text_font_style = \"bold\"\n            laxis.axis_label_text_color = \"black\"\n            laxis.major_label_text_font = \"Comic Sans MS, Textile, cursive\"\n            laxis.major_label_text_font_style = \"bold\"\n            laxis.major_label_text_color = \"black\"\n\n        return laxis\n\n    def make_grid(self, baxis, dimension):\n        \"Given a mpl axes instance, returns a Bokeh Grid object.\"\n        lgrid = Grid(dimension=dimension,\n                     ticker=baxis.ticker,\n                     grid_line_color=self.grid.get_color(),\n                     grid_line_width=self.grid.get_linewidth())\n\n        self.plot.add_layout(lgrid)\n\n    def make_line_collection(self, col):\n        \"Given a mpl collection instance create a Bokeh MultiLine glyph.\"\n        xydata = col.get_segments()\n        t_xydata = [np.transpose(seg) for seg in xydata]\n        xs = [t_xydata[x][0] for x in range(len(t_xydata))]\n        ys = [t_xydata[x][1] for x in range(len(t_xydata))]\n        if self.xkcd:\n            xkcd_xs = [xkcd_line(xs[i], ys[i])[0] for i in range(len(xs))]\n            xkcd_ys = [xkcd_line(xs[i], ys[i])[1] for i in range(len(ys))]\n            xs = xkcd_xs\n            ys = xkcd_ys\n\n        multiline = MultiLine()\n        multiline.xs = self.source.add(xs)\n        multiline.ys = self.source.add(ys)\n        self.xdr.sources.append(self.source.columns(multiline.xs))\n        self.ydr.sources.append(self.source.columns(multiline.ys))\n\n        self.multiline_props(multiline, col)\n\n        self.plot.add_glyph(self.source, multiline)\n\n    def make_poly_collection(self, col):\n        \"Given a mpl collection instance create a Bokeh Patches glyph.\"\n        paths = col.get_paths()\n        polygons = [paths[i].to_polygons() for i in range(len(paths))]\n        polygons = [np.transpose(delete_last_col(polygon)) for polygon in polygons]\n        xs = [polygons[i][0] for i in range(len(polygons))]\n        ys = [polygons[i][1] for i in range(len(polygons))]\n\n        patches = Patches()\n        patches.xs = self.source.add(xs)\n        patches.ys = self.source.add(ys)\n        self.xdr.sources.append(self.source.columns(patches.xs))\n        self.ydr.sources.append(self.source.columns(patches.ys))\n\n        self.patches_props(patches, col)\n\n        self.plot.add_glyph(self.source, patches)\n\n    def multiline_props(self, multiline, col):\n        \"Takes a mpl collection object to extract and set up some Bokeh multiline properties.\"\n        colors = get_props_cycled(col, col.get_colors(), fx=lambda x: mpl.colors.rgb2hex(x))\n        widths = get_props_cycled(col, col.get_linewidth())\n        multiline.line_color = self.source.add(colors)\n        multiline.line_width = self.source.add(widths)\n        multiline.line_alpha = col.get_alpha()\n        offset = col.get_linestyle()[0][0]\n        if not col.get_linestyle()[0][1]:\n            on_off = []\n        else:\n            on_off = map(int,col.get_linestyle()[0][1])\n        multiline.line_dash_offset = convert_dashes(offset)\n        multiline.line_dash = list(convert_dashes(tuple(on_off)))\n\n    def patches_props(self, patches, col):\n        \"Takes a mpl collection object to extract and set up some Bokeh patches properties.\"\n        face_colors = get_props_cycled(col, col.get_facecolors(), fx=lambda x: mpl.colors.rgb2hex(x))\n        patches.fill_color = self.source.add(face_colors)\n        edge_colors = get_props_cycled(col, col.get_edgecolors(), fx=lambda x: mpl.colors.rgb2hex(x))\n        patches.line_color = self.source.add(edge_colors)\n        widths = get_props_cycled(col, col.get_linewidth())\n        patches.line_width = self.source.add(widths)\n        patches.line_alpha = col.get_alpha()\n        offset = col.get_linestyle()[0][0]\n        if not col.get_linestyle()[0][1]:\n            on_off = []\n        else:\n            on_off = map(int,col.get_linestyle()[0][1])\n        patches.line_dash_offset = convert_dashes(offset)\n        patches.line_dash = list(convert_dashes(tuple(on_off)))",
  "def to_bokeh(fig=None, name=None, server=None, notebook=False, pd_obj=True, xkcd=False):\n    \"\"\" Uses bokeh to display a Matplotlib Figure.\n\n    You can store a bokeh plot in a standalone HTML file, as a document in\n    a Bokeh plot server, or embedded directly into an IPython Notebook\n    output cell.\n\n    Parameters\n    ----------\n\n    fig: matplotlib.figure.Figure\n        The figure to display. If None or not specified, then the current figure\n        will be used.\n\n    name: str (default=None)\n        If this option is provided, then the Bokeh figure will be saved into\n        this HTML file, and then a web browser will used to display it.\n\n    server: str (default=None)\n        Fully specified URL of bokeh plot server. Default bokeh plot server\n        URL is \"http://localhost:5006\" or simply \"deault\"\n\n    notebook: bool (default=False)\n        Return an output value from this function which represents an HTML\n        object that the IPython notebook can display. You can also use it with\n        a bokeh plot server just specifying the URL.\n\n    pd_obj: bool (default=True)\n        The implementation asumes you are plotting using the pandas.\n        You have the option to turn it off (False) to plot the datetime xaxis\n        with other non-pandas interfaces.\n\n    xkcd: bool (default=False)\n        If this option is True, then the Bokeh figure will be saved with a\n        xkcd style.\n    \"\"\"\n\n    if fig is None:\n        fig = plt.gcf()\n\n    if any([name, server, notebook]):\n        if name:\n            if not server:\n                filename = name + \".html\"\n                output_file(filename)\n            else:\n                output_server(name, url=server)\n        elif server:\n            if not notebook:\n                output_server(\"unnameuuuuuuuuuuuuuud\", url=server)\n            else:\n                output_notebook(url=server)\n        elif notebook:\n            output_notebook()\n    else:\n        output_file(\"Unnamed.html\")\n\n    doc = curdoc()\n\n    renderer = BokehRenderer(pd_obj, xkcd)\n    exporter = Exporter(renderer)\n\n    exporter.run(fig)\n\n    doc._current_plot = renderer.fig  # TODO (bev) do not rely on private attrs\n    doc.add(renderer.fig)\n\n    return renderer.fig",
  "def __init__(self, pd_obj, xkcd):\n        \"Initial setup.\"\n        self.fig = None\n        self.pd_obj = pd_obj\n        self.xkcd = xkcd\n        self.source = ColumnDataSource()\n        self.xdr = DataRange1d()\n        self.ydr = DataRange1d()\n        self.non_text = []",
  "def open_figure(self, fig, props):\n        \"Get the main plot properties and create the plot.\"\n        self.width = int(props['figwidth'] * props['dpi'])\n        self.height = int(props['figheight'] * props['dpi'])\n        self.plot = Plot(x_range=self.xdr,\n                         y_range=self.ydr,\n                         plot_width=self.width,\n                         plot_height=self.height)",
  "def close_figure(self, fig):\n        \"Complete the plot: add tools.\"\n        # Add tools\n        tool_objs = _process_tools_arg(self.plot, DEFAULT_TOOLS)\n        self.plot.add_tools(*tool_objs)\n\n        # Simple or Grid plot setup\n        if len(fig.axes) <= 1:\n            self.fig = self.plot\n        else:\n            # This list comprehension splits the plot.renderers list at the \"marker\"\n            # points returning small sublists corresponding with each subplot.\n            subrends = [list(x[1]) for x in itertools.groupby(\n                        self.plot.renderers, lambda x: is_ax_end(x)) if not x[0]]\n            plots = []\n            for i, axes in enumerate(fig.axes):\n                # create a new plot for each subplot\n                _plot = Plot(x_range=self.xdr,\n                             y_range=self.ydr,\n                             plot_width=self.width,\n                             plot_height=self.height)\n                _plot.title = \"\"\n                # and add new tools\n                _tool_objs = _process_tools_arg(_plot, DEFAULT_TOOLS)\n                _plot.add_tools(*_tool_objs)\n                # clean the plot ref from axis and grids\n                _plot_rends = subrends[i]\n                for r in _plot_rends:\n                    if not isinstance(r, GlyphRenderer):\n                        r.plot = None\n                # add all the renderers into the new subplot\n                _plot.add_layout(_plot_rends[0], 'below')  # xaxis\n                _plot.add_layout(_plot_rends[1], 'left')  # yaxis\n                _plot.add_layout(_plot_rends[2])  # xgrid\n                _plot.add_layout(_plot_rends[3])  # ygrid\n                for r in _plot_rends[4:]:  # all the glyphs\n                    _plot.renderers.append(r)\n                plots.append(_plot)\n            (a, b, c) = fig.axes[0].get_geometry()\n            p = np.array(plots)\n            n = np.resize(p, (a, b))\n            grid = GridPlot(children=n.tolist())\n            self.fig = grid",
  "def open_axes(self, ax, props):\n        \"Get axes data and create the axes and grids\"\n        # Get axes, title and grid into class attributes.\n        self.ax = ax\n        self.plot.title = ax.get_title()\n        # to avoid title conversion by draw_text later\n        self.non_text.append(self.plot.title)\n        self.grid = ax.get_xgridlines()[0]\n\n        # Add axis\n        bxaxis = self.make_axis(ax.xaxis, \"below\", props['xscale'])\n        byaxis = self.make_axis(ax.yaxis, \"left\", props['yscale'])\n\n        # Add grids\n        self.make_grid(bxaxis, 0)\n        self.make_grid(byaxis, 1)\n\n        # Setup collections info\n        nones = (\"\", \" \", \"None\", \"none\", None)\n        cols = [col for col in self.ax.collections if col.get_paths() not in nones]\n\n        # Add collections renderers\n        [self.make_line_collection(col) for col in cols if isinstance(col, mpl.collections.LineCollection)]\n        [self.make_poly_collection(col) for col in cols if isinstance(col, mpl.collections.PolyCollection)]",
  "def close_axes(self, ax):\n        \"Complete the axes adding axes-dependent plot props\"\n        background_fill = ax.get_axis_bgcolor()\n        if background_fill == 'w':\n            background_fill = 'white'\n        self.plot.background_fill = background_fill\n        if self.xkcd:\n            self.plot.title_text_font = \"Comic Sans MS, Textile, cursive\"\n            self.plot.title_text_font_style = \"bold\"\n            self.plot.title_text_color = \"black\"\n\n        # Add a \"marker\" Glyph to help the plot.renderers splitting in the GridPlot build\n        dummy_source = ColumnDataSource(data=dict(name=\"ax_end\"))\n        self.plot.renderers.append(GlyphRenderer(data_source=dummy_source, glyph=X()))",
  "def open_legend(self, legend, props):\n        pass",
  "def close_legend(self, legend):\n        pass",
  "def draw_line(self, data, coordinates, style, label, mplobj=None):\n        \"Given a mpl line2d instance create a Bokeh Line glyph.\"\n        _x = data[:, 0]\n        if self.pd_obj is True:\n            try:\n                x = [pd.Period(ordinal=int(i), freq=self.ax.xaxis.freq).to_timestamp() for i in _x]\n            except AttributeError as e: #  we probably can make this one more intelligent later\n                x = _x\n        else:\n            x = _x\n\n        y = data[:, 1]\n        if self.xkcd:\n            x, y = xkcd_line(x, y)\n\n        line = Line()\n        line.x = self.source.add(x)\n        line.y = self.source.add(y)\n        self.xdr.sources.append(self.source.columns(line.x))\n        self.ydr.sources.append(self.source.columns(line.y))\n\n        line.line_color = style['color']\n        line.line_width = style['linewidth']\n        line.line_alpha = style['alpha']\n        line.line_dash = [int(i) for i in style['dasharray'].split(\",\")]  # str2list(int)\n        #style['zorder'] # not in Bokeh\n        #line.line_join = line2d.get_solid_joinstyle() # not in mplexporter\n        #line.line_cap = cap_style_map[line2d.get_solid_capstyle()] # not in mplexporter\n        if self.xkcd:\n            line.line_width = 3\n\n        self.plot.add_glyph(self.source, line)",
  "def draw_markers(self, data, coordinates, style, label, mplobj=None):\n        \"Given a mpl line2d instance create a Bokeh Marker glyph.\"\n        x = data[:, 0]\n        y = data[:, 1]\n\n        marker_map = {\n            \"o\": Circle,\n            \"s\": Square,\n            \"+\": Cross,\n            \"^\": Triangle,\n            \"v\": InvertedTriangle,\n            \"x\": X,\n            \"D\": Diamond,\n            \"*\": Asterisk,\n        }\n\n        # Not all matplotlib markers are currently handled; fall back to Circle if we encounter an\n        # unhandled marker.  See http://matplotlib.org/api/markers_api.html for a list of markers.\n        try:\n            marker = marker_map[style['marker']]()\n        except KeyError:\n            warnings.warn(\"Unable to handle marker: %s; defaulting to Circle\" % style['marker'])\n            marker = Circle()\n        marker.x = self.source.add(x)\n        marker.y = self.source.add(y)\n        self.xdr.sources.append(self.source.columns(marker.x))\n        self.ydr.sources.append(self.source.columns(marker.y))\n\n        marker.line_color = style['edgecolor']\n        marker.fill_color = style['facecolor']\n        marker.line_width = style['edgewidth']\n        marker.size = style['markersize']\n        marker.fill_alpha = marker.line_alpha = style['alpha']\n        #style['zorder'] # not in Bokeh\n\n        self.plot.add_glyph(self.source, marker)",
  "def draw_path_collection(self, paths, path_coordinates, path_transforms,\n                             offsets, offset_coordinates, offset_order,\n                             styles, mplobj=None):\n        \"\"\"Path not implemented in Bokeh, but we have our own line ans poly\n        collection implementations, so passing here to avoid the NonImplemented\n        error.\n        \"\"\"\n        pass",
  "def draw_text(self, text, position, coordinates, style,\n                  text_type=None, mplobj=None):\n        \"Given a mpl text instance create a Bokeh Text glyph.\"\n        # mpl give you the title and axes names as a text object (with specific locations)\n        # inside the plot itself. That does not make sense inside Bokeh, so we\n        # just skip the title and axes names from the conversion and covert any other text.\n        if text not in self.non_text:\n          x, y = position\n          text = Text(x=x, y=y, text=[text])\n\n          alignment_map = {\"center\": \"middle\", \"top\": \"top\", \"bottom\": \"bottom\", \"baseline\": \"bottom\"}\n          # baseline not implemented in Bokeh, deafulting to bottom.\n          text.text_alpha = style['alpha']\n          text.text_font_size = \"%dpx\" % style['fontsize']\n          text.text_color = style['color']\n          text.text_align = style['halign']\n          text.text_baseline = alignment_map[style['valign']]\n          text.angle = style['rotation']\n          #style['zorder'] # not in Bokeh\n\n          ## Using get_fontname() works, but it's oftentimes not available in the browser,\n          ## so it's better to just use the font family here.\n          #text.text_font = mplText.get_fontname()) not in mplexporter\n          #text.text_font = mplText.get_fontfamily()[0] # not in mplexporter\n          #text.text_font_style = fontstyle_map[mplText.get_fontstyle()] # not in mplexporter\n          ## we don't really have the full range of font weights, but at least handle bold\n          #if mplText.get_weight() in (\"bold\", \"heavy\"):\n              #text.text_font_style = bold\n\n          self.plot.add_glyph(self.source, text)",
  "def draw_image(self, imdata, extent, coordinates, style, mplobj=None):\n        pass",
  "def make_axis(self, ax, location, scale):\n        \"Given a mpl axes instance, returns a Bokeh LinearAxis object.\"\n        # TODO:\n        #  * handle log scaling\n        #  * map `labelpad` to `major_label_standoff`\n        #  * deal with minor ticks once BokehJS supports them\n        #  * handle custom tick locations once that is added to bokehJS\n\n        # we need to keep the current axes names to avoid writing them in draw_text\n        self.non_text.append(ax.get_label_text())\n\n        if scale == \"linear\":\n            laxis = LinearAxis(axis_label=ax.get_label_text())\n        elif scale == \"date\":\n            laxis = DatetimeAxis(axis_label=ax.get_label_text())\n\n        self.plot.add_layout(laxis, location)\n\n        # First get the label properties by getting an mpl.Text object\n        #label = ax.get_label()\n        #self.text_props(label, laxis, prefix=\"axis_label_\")\n        #self.draw_text(label, position, coordinates, style, text_type=\"axis_label_\")\n\n        # To get the tick label format, we look at the first of the tick labels\n        # and assume the rest are formatted similarly.\n        #ticktext = ax.get_ticklabels()[0]\n        #self.text_props(ticktext, laxis, prefix=\"major_label_\")\n        #self.draw_text(ticktext, position, coordinates, style, text_type=\"major_label_\")\n\n        #newaxis.bounds = axis.get_data_interval()  # I think this is the right func...\n\n        if self.xkcd:\n            laxis.axis_line_width = 3\n            laxis.axis_label_text_font = \"Comic Sans MS, Textile, cursive\"\n            laxis.axis_label_text_font_style = \"bold\"\n            laxis.axis_label_text_color = \"black\"\n            laxis.major_label_text_font = \"Comic Sans MS, Textile, cursive\"\n            laxis.major_label_text_font_style = \"bold\"\n            laxis.major_label_text_color = \"black\"\n\n        return laxis",
  "def make_grid(self, baxis, dimension):\n        \"Given a mpl axes instance, returns a Bokeh Grid object.\"\n        lgrid = Grid(dimension=dimension,\n                     ticker=baxis.ticker,\n                     grid_line_color=self.grid.get_color(),\n                     grid_line_width=self.grid.get_linewidth())\n\n        self.plot.add_layout(lgrid)",
  "def make_line_collection(self, col):\n        \"Given a mpl collection instance create a Bokeh MultiLine glyph.\"\n        xydata = col.get_segments()\n        t_xydata = [np.transpose(seg) for seg in xydata]\n        xs = [t_xydata[x][0] for x in range(len(t_xydata))]\n        ys = [t_xydata[x][1] for x in range(len(t_xydata))]\n        if self.xkcd:\n            xkcd_xs = [xkcd_line(xs[i], ys[i])[0] for i in range(len(xs))]\n            xkcd_ys = [xkcd_line(xs[i], ys[i])[1] for i in range(len(ys))]\n            xs = xkcd_xs\n            ys = xkcd_ys\n\n        multiline = MultiLine()\n        multiline.xs = self.source.add(xs)\n        multiline.ys = self.source.add(ys)\n        self.xdr.sources.append(self.source.columns(multiline.xs))\n        self.ydr.sources.append(self.source.columns(multiline.ys))\n\n        self.multiline_props(multiline, col)\n\n        self.plot.add_glyph(self.source, multiline)",
  "def make_poly_collection(self, col):\n        \"Given a mpl collection instance create a Bokeh Patches glyph.\"\n        paths = col.get_paths()\n        polygons = [paths[i].to_polygons() for i in range(len(paths))]\n        polygons = [np.transpose(delete_last_col(polygon)) for polygon in polygons]\n        xs = [polygons[i][0] for i in range(len(polygons))]\n        ys = [polygons[i][1] for i in range(len(polygons))]\n\n        patches = Patches()\n        patches.xs = self.source.add(xs)\n        patches.ys = self.source.add(ys)\n        self.xdr.sources.append(self.source.columns(patches.xs))\n        self.ydr.sources.append(self.source.columns(patches.ys))\n\n        self.patches_props(patches, col)\n\n        self.plot.add_glyph(self.source, patches)",
  "def multiline_props(self, multiline, col):\n        \"Takes a mpl collection object to extract and set up some Bokeh multiline properties.\"\n        colors = get_props_cycled(col, col.get_colors(), fx=lambda x: mpl.colors.rgb2hex(x))\n        widths = get_props_cycled(col, col.get_linewidth())\n        multiline.line_color = self.source.add(colors)\n        multiline.line_width = self.source.add(widths)\n        multiline.line_alpha = col.get_alpha()\n        offset = col.get_linestyle()[0][0]\n        if not col.get_linestyle()[0][1]:\n            on_off = []\n        else:\n            on_off = map(int,col.get_linestyle()[0][1])\n        multiline.line_dash_offset = convert_dashes(offset)\n        multiline.line_dash = list(convert_dashes(tuple(on_off)))",
  "def patches_props(self, patches, col):\n        \"Takes a mpl collection object to extract and set up some Bokeh patches properties.\"\n        face_colors = get_props_cycled(col, col.get_facecolors(), fx=lambda x: mpl.colors.rgb2hex(x))\n        patches.fill_color = self.source.add(face_colors)\n        edge_colors = get_props_cycled(col, col.get_edgecolors(), fx=lambda x: mpl.colors.rgb2hex(x))\n        patches.line_color = self.source.add(edge_colors)\n        widths = get_props_cycled(col, col.get_linewidth())\n        patches.line_width = self.source.add(widths)\n        patches.line_alpha = col.get_alpha()\n        offset = col.get_linestyle()[0][0]\n        if not col.get_linestyle()[0][1]:\n            on_off = []\n        else:\n            on_off = map(int,col.get_linestyle()[0][1])\n        patches.line_dash_offset = convert_dashes(offset)\n        patches.line_dash = list(convert_dashes(tuple(on_off)))",
  "def _glyph_function(glyphclass, dsnames, argnames, docstring, xfields=[\"x\"], yfields=[\"y\"]):\n\n    def func(plot, *args, **kwargs):\n        # Note: We want to reuse the glyph functions by attaching them the Plot\n        # class. Imports are here to prevent circular imports.\n        from .plotting_helpers import (\n            _match_data_params, _update_plot_data_ranges,\n            _materialize_colors_and_alpha, _get_legend,\n            _make_legend, _get_select_tool\n        )\n        from .models import ColumnDataSource, GlyphRenderer, Plot, RemoteSource\n        source = kwargs.pop('source', None)\n        if source is None:\n            datasource = ColumnDataSource()\n        else:\n            datasource = source\n\n        legend_name = kwargs.pop(\"legend\", None)\n\n        if not isinstance(plot, Plot):\n            raise ValueError(\"expected plot object for first argument\")\n\n        # TODO (bev) this seems like it should be here but invalid kwargs\n        # currently get through (see also below)\n        # plot.update(**kwargs)\n\n        name = kwargs.pop('name', None)\n\n        select_tool = _get_select_tool(plot)\n\n        # Process the glyph dataspec parameters\n        glyph_params = _match_data_params(dsnames, glyphclass,\n                                          datasource,\n                                          args, _materialize_colors_and_alpha(kwargs))\n\n        x_data_fields = []\n        for xx in xfields:\n            if not isinstance(glyph_params[xx], dict): continue\n            if glyph_params[xx]['units'] == 'data': x_data_fields.append(glyph_params[xx]['field'])\n        y_data_fields = []\n        for yy in yfields:\n            if not isinstance(glyph_params[yy], dict): continue\n            if glyph_params[yy]['units'] == 'data': y_data_fields.append(glyph_params[yy]['field'])\n\n        _update_plot_data_ranges(plot, datasource, x_data_fields, y_data_fields)\n        kwargs.update(glyph_params)\n\n        glyph_props = glyphclass.properties() | set(argnames)\n        glyph_kwargs = dict((key, value) for (key, value) in iteritems(kwargs) if key in glyph_props)\n        glyph = glyphclass(**glyph_kwargs)\n\n        nonselection_glyph_params = _materialize_colors_and_alpha(kwargs, prefix='nonselection_', default_alpha=0.1)\n        nonselection_glyph = glyph.clone()\n\n        # TODO: (bev) This is a bit hacky.\n        if hasattr(nonselection_glyph, 'fill_color'):\n            nonselection_glyph.fill_color = nonselection_glyph_params['fill_color']\n            nonselection_glyph.fill_alpha = nonselection_glyph_params['fill_alpha']\n\n        if hasattr(nonselection_glyph, 'line_color'):\n            nonselection_glyph.line_color = nonselection_glyph_params['line_color']\n            nonselection_glyph.line_alpha = nonselection_glyph_params['line_alpha']\n\n        glyph_renderer = GlyphRenderer(\n            data_source=datasource,\n            glyph=glyph,\n            nonselection_glyph=nonselection_glyph,\n            name=name)\n\n        # TODO (bev) hacky, fix up when glyphspecs are simplified/removed\n        if 'x_range_name' in kwargs:\n            glyph_renderer.x_range_name = kwargs['x_range_name']\n        if 'y_range_name' in kwargs:\n            glyph_renderer.y_range_name = kwargs['y_range_name']\n\n        if legend_name:\n            legend = _get_legend(plot)\n            if not legend:\n                legend = _make_legend(plot)\n            legends = OrderedDict(legend.legends)\n            legends.setdefault(legend_name, []).append(glyph_renderer)\n            legend.legends = list(legends.items())\n\n        if select_tool :\n            select_tool.renderers.append(glyph_renderer)\n            select_tool._dirty = True\n\n        plot.renderers.append(glyph_renderer)\n        plot._dirty = True\n        return plot\n    func.__name__ = glyphclass.__view_model__\n    func.__doc__ = docstring\n    return func",
  "def func(plot, *args, **kwargs):\n        # Note: We want to reuse the glyph functions by attaching them the Plot\n        # class. Imports are here to prevent circular imports.\n        from .plotting_helpers import (\n            _match_data_params, _update_plot_data_ranges,\n            _materialize_colors_and_alpha, _get_legend,\n            _make_legend, _get_select_tool\n        )\n        from .models import ColumnDataSource, GlyphRenderer, Plot, RemoteSource\n        source = kwargs.pop('source', None)\n        if source is None:\n            datasource = ColumnDataSource()\n        else:\n            datasource = source\n\n        legend_name = kwargs.pop(\"legend\", None)\n\n        if not isinstance(plot, Plot):\n            raise ValueError(\"expected plot object for first argument\")\n\n        # TODO (bev) this seems like it should be here but invalid kwargs\n        # currently get through (see also below)\n        # plot.update(**kwargs)\n\n        name = kwargs.pop('name', None)\n\n        select_tool = _get_select_tool(plot)\n\n        # Process the glyph dataspec parameters\n        glyph_params = _match_data_params(dsnames, glyphclass,\n                                          datasource,\n                                          args, _materialize_colors_and_alpha(kwargs))\n\n        x_data_fields = []\n        for xx in xfields:\n            if not isinstance(glyph_params[xx], dict): continue\n            if glyph_params[xx]['units'] == 'data': x_data_fields.append(glyph_params[xx]['field'])\n        y_data_fields = []\n        for yy in yfields:\n            if not isinstance(glyph_params[yy], dict): continue\n            if glyph_params[yy]['units'] == 'data': y_data_fields.append(glyph_params[yy]['field'])\n\n        _update_plot_data_ranges(plot, datasource, x_data_fields, y_data_fields)\n        kwargs.update(glyph_params)\n\n        glyph_props = glyphclass.properties() | set(argnames)\n        glyph_kwargs = dict((key, value) for (key, value) in iteritems(kwargs) if key in glyph_props)\n        glyph = glyphclass(**glyph_kwargs)\n\n        nonselection_glyph_params = _materialize_colors_and_alpha(kwargs, prefix='nonselection_', default_alpha=0.1)\n        nonselection_glyph = glyph.clone()\n\n        # TODO: (bev) This is a bit hacky.\n        if hasattr(nonselection_glyph, 'fill_color'):\n            nonselection_glyph.fill_color = nonselection_glyph_params['fill_color']\n            nonselection_glyph.fill_alpha = nonselection_glyph_params['fill_alpha']\n\n        if hasattr(nonselection_glyph, 'line_color'):\n            nonselection_glyph.line_color = nonselection_glyph_params['line_color']\n            nonselection_glyph.line_alpha = nonselection_glyph_params['line_alpha']\n\n        glyph_renderer = GlyphRenderer(\n            data_source=datasource,\n            glyph=glyph,\n            nonselection_glyph=nonselection_glyph,\n            name=name)\n\n        # TODO (bev) hacky, fix up when glyphspecs are simplified/removed\n        if 'x_range_name' in kwargs:\n            glyph_renderer.x_range_name = kwargs['x_range_name']\n        if 'y_range_name' in kwargs:\n            glyph_renderer.y_range_name = kwargs['y_range_name']\n\n        if legend_name:\n            legend = _get_legend(plot)\n            if not legend:\n                legend = _make_legend(plot)\n            legends = OrderedDict(legend.legends)\n            legends.setdefault(legend_name, []).append(glyph_renderer)\n            legend.legends = list(legends.items())\n\n        if select_tool :\n            select_tool.renderers.append(glyph_renderer)\n            select_tool._dirty = True\n\n        plot.renderers.append(glyph_renderer)\n        plot._dirty = True\n        return plot",
  "def app_document(prefix, url=\"default\"):\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            docname = prefix + str(uuid.uuid4())\n            session = Session(name=url, root_url=url)\n            session.use_doc(docname)\n            session.load_document(curdoc())\n            curdoc().autoadd = False\n            curdoc().autostore = False\n\n            obj = func(*args, **kwargs)\n            tag = embed.autoload_server(obj, session)\n            obj._tag = tag\n\n            curdoc().add(obj)\n            changed = session.store_document(curdoc())\n\n            logger.debug(\"stored: %s\", str(changed))\n\n            return obj\n        wrapper.__name__ = func.__name__\n        return wrapper\n    return decorator",
  "def decorator(func):\n        def wrapper(*args, **kwargs):\n            docname = prefix + str(uuid.uuid4())\n            session = Session(name=url, root_url=url)\n            session.use_doc(docname)\n            session.load_document(curdoc())\n            curdoc().autoadd = False\n            curdoc().autostore = False\n\n            obj = func(*args, **kwargs)\n            tag = embed.autoload_server(obj, session)\n            obj._tag = tag\n\n            curdoc().add(obj)\n            changed = session.store_document(curdoc())\n\n            logger.debug(\"stored: %s\", str(changed))\n\n            return obj\n        wrapper.__name__ = func.__name__\n        return wrapper",
  "def wrapper(*args, **kwargs):\n            docname = prefix + str(uuid.uuid4())\n            session = Session(name=url, root_url=url)\n            session.use_doc(docname)\n            session.load_document(curdoc())\n            curdoc().autoadd = False\n            curdoc().autostore = False\n\n            obj = func(*args, **kwargs)\n            tag = embed.autoload_server(obj, session)\n            obj._tag = tag\n\n            curdoc().add(obj)\n            changed = session.store_document(curdoc())\n\n            logger.debug(\"stored: %s\", str(changed))\n\n            return obj",
  "class Document(object):\n    \"\"\" The Document class is a container to hold Bokeh objects that\n    requires reflecting to the client BokehJS library.\n\n    Attributes:\n        autoadd (bool) :\n        autostore (bool) :\n        context (PlotContext) : the plot context for this document\n        ref (str) : reference to the plot context for this document\n\n    \"\"\"\n\n    def __init__(self, json_objs=None):\n        self._current_plot = None\n        self._hold = False\n        self._models = {}\n\n        self.docid = str(uuid.uuid4())\n        self.autostore = True\n        self.autoadd = True\n\n        if json_objs:\n            self.load(*json_objs, dirty=False)\n\n        # must init context after loading JSON objs\n        self._init_context()\n\n    # properties\n\n    @property\n    def autoadd(self):\n        return self._autoadd\n\n    @autoadd.setter\n    def autoadd(self, value):\n        if not isinstance(value, bool):\n            raise TypeError(\"'autoadd' must be True or False\")\n        self._autoadd = value\n\n    @property\n    def autostore(self):\n        return self._autostore\n\n    @autostore.setter\n    def autostore(self, value):\n        if not isinstance(value, bool):\n            raise TypeError(\"'autostore' must be True or False\")\n        self._autostore = value\n\n    @property\n    def context(self):\n        return self._context\n\n    @context.setter\n    def context(self, value):\n        if not isinstance(value, PlotContext):\n            raise TypeError('Document.context may only be assigned to PlotContext objects')\n        try:\n            if self._context:\n                del self._models[self._context._id]\n        except AttributeError:\n            pass\n        other_pcs = [x for x in self._models.values() if x.__view_model__ == 'PlotContext']\n        other_pcs = [x for x in other_pcs if x._id != value._id]\n        if len(other_pcs) != 0:\n            raise DataIntegrityException(\"too many plot contexts found\")\n        self._add(value)\n        self._add(*value.references())\n        self._context = value\n\n    @property\n    def ref(self):\n        return self._context.ref\n\n    def clear(self):\n        \"\"\" Remove all plots from this `Document`\n\n        Returns:\n            None\n\n        \"\"\"\n        self.context.children = []\n        context = self.context\n        self._models = {}\n        self._add(context)\n\n    # functions for adding objects to documents\n\n    def add(self, *objects):\n        \"\"\" Add top-level objects (and any references they hold to sub-objects)\n        to this Document.\n\n        .. warning::\n            This function should only be called on top level objects such\n            as Plot, and Layout containers.\n\n        Args:\n            *objects (PlotObject) : objects to add to the Document\n\n        Returns:\n            None\n\n        \"\"\"\n        for obj in objects:\n            if obj not in self.context.children:\n                self.context.children.append(obj)\n                self.context._dirty = True\n            self._add(*obj.references())\n\n    def _add_all(self):\n        # fix for crossfilter - we should take this out soon, and just\n        # ensure that the entire graph is added before dump\n        for obj in self.context.references():\n            self._add(obj)\n        self.prune()\n\n    # functions for turning json objects into json models\n\n    def load(self, *objs, **kwargs):\n        \"\"\" Convert json objects to models and load them into this Document.\n\n        Args:\n            *objs (str) : json object strings to convert\n\n        Keyword Args:\n            Two optional keyword arguments are stripped from *kwargs:\n\n            existing (str) : what objects to trigger events on (default: 'existing')\n                valid values are:\n                * 'none' trigger no events\n                * 'all' trigger events on all models\n                * 'new' trigger events only on new models\n                * 'existing' trigger events on already existing models\n            dirty (bool) : whether to mark models as dirty (default: False)\n\n        Returns:\n            set[Plotobject] : models loaded from json\n\n        \"\"\"\n        events = kwargs.pop('events', 'existing')\n        if events not in ['all', 'none', 'new', 'existing']:\n            raise ValueError(\n                \"Invalid value for events: '%s', valid values are: 'all', 'none', 'new', 'existing'\" % events\n            )\n\n        dirty = kwargs.pop('dirty', False)\n\n        all_models = set()\n        new_models = set()\n\n        for obj in objs:\n            obj_id = obj['attributes']['id'] # XXX: obj['id']\n            obj_type = obj.get('subtype', obj['type'])\n            obj_attrs = obj['attributes']\n\n            if \"doc\" in obj_attrs:\n                del obj_attrs[\"doc\"]\n\n            if obj_id in self._models:\n                model = self._models[obj_id]\n                model._block_callbacks = True\n                model.load_json(obj_attrs, instance=model)\n            else:\n                cls = PlotObject.get_class(obj_type)\n                model = cls.load_json(obj_attrs)\n                if model is None:\n                    raise RuntimeError('Error loading model from JSON (type: %s, id: %s)' % (obj_type, obj_id))\n                self._add(model)\n                new_models.add(model)\n\n            all_models.add(model)\n\n        for m in all_models:\n            props = m.finalize(self._models)\n            m.update(**props)\n            m.setup_events()\n\n        if events == 'all':\n            self.execute_callback_queue(all_models)\n            self.clear_callback_queue(all_models)\n\n        if events == 'none':\n            self.clear_callback_queue(all_models)\n\n        if events == 'new':\n            self.execute_callback_queue(new_models)\n            self.clear_callback_queue(new_models)\n\n        elif events == 'existing':\n            self.execute_callback_queue(all_models-new_models)\n            self.clear_callback_queue(new_models)\n\n        self.enable_callbacks(all_models)\n\n        for m in all_models:\n            m._dirty = dirty\n\n        return all_models\n\n    def dump(self, *models):\n        \"\"\" Convert models to json objects.\n\n        Args:\n            *models (PlotObject) : models to convert to json objects\n                If models is empty, ``dump`` converts all models in this d\n                ocument.\n\n        Return:\n            dict : json objects\n\n        \"\"\"\n        self._add(*self.context.references())\n        if not models:\n            models = self._models.values()\n        json = dump(models, docid=self.docid)\n        return json\n\n    #------------------------------------------------------------------------\n    # Managing callbacks\n    #------------------------------------------------------------------------\n\n    def disable_callbacks(self, models=None):\n        \"\"\" Disable callbacks on given models.\n\n        Args:\n            models (seq[PlotObject], optional) : models to disable callbacks for (default: None)\n                If models is None, disables callbacks on all models in\n                this Document.\n\n        Returns:\n            None\n\n        \"\"\"\n        if models is None:\n            models = self._models.values()\n        for m in models:\n            m._block_callbacks = True\n\n    def enable_callbacks(self, models=None):\n        \"\"\" Enable callbacks on given models.\n\n        Args:\n            models (seq[PlotObject], optional) : models to enable callbacks for (default: None)\n                If models is None, enables callbacks on all models in\n                this Document.\n\n        Returns:\n            None\n\n        \"\"\"\n        if models is None:\n            models = self._models.values()\n\n        for m in models:\n            m._block_callbacks = False\n\n    def clear_callback_queue(self, models=None):\n        \"\"\" Clear the callback queue on given models.\n\n        Args:\n            models (seq[PlotObject], optional) : models to clear callbacks for (default: None)\n                If models is None, clears callback queue on all models\n                in this Document.\n\n        Returns:\n            None\n\n        \"\"\"\n        if models is None:\n            models = self._models.values()\n        for m in models:\n            del m._callback_queue[:]\n\n    def execute_callback_queue(self, models=None):\n        \"\"\" Execute all queued callbacks on the given models.\n\n        Args:\n            models (seq[PlotObject], optional) : models to execute callbacks for (default: None)\n                If models is None, executes the callback queue on all models\n                in this Document.\n\n        Returns:\n            None\n\n        \"\"\"\n        if models is None:\n            models = self._models.values()\n        for m in models:\n            for cb in m._callback_queue:\n                m._trigger(*cb)\n            del m._callback_queue[:]\n\n    #------------------------------------------------------------------------\n    # Helper functions\n    #------------------------------------------------------------------------\n\n    def _add(self, *objects):\n        \"\"\" Adds objects to this document.\n\n        \"\"\"\n        for obj in objects:\n            self._models[obj._id] = obj\n\n    def _init_context(self):\n        \"\"\" Initialize self.context appropriately.\n\n        If no plotcontext exists, creates one. If one exists in self._modes\n        (because we are on the server) re-use it.\n\n        \"\"\"\n        pcs = [x for x in self._models.values() if x.__view_model__ == 'PlotContext']\n        if len(pcs) == 0:\n            self.context = PlotContext()\n        elif len(pcs) == 1:\n            self._context = pcs[0]\n            self._add(self._context)\n        else:\n            raise DataIntegrityException(\"too many plot contexts found\")\n\n    def merge(self, json_objs):\n        \"\"\"Merge's json objects from another document into this one\n        using the plot context id from the json_objs which are passed in\n        children from this document are merged with the children from\n        the json that is passed in\n\n        Args:\n            json_objs : json objects from session.pull()\n\n        Returns:\n            None\n        \"\"\"\n        plot_contexts = [x for x in json_objs if x['type'] == 'PlotContext']\n        other_objects = [x for x in json_objs if x['type'] != 'PlotContext']\n        plot_context_json = plot_contexts[0]\n        children = set([x['id'] for x in plot_context_json['attributes']['children']])\n        for child in self.context.children:\n            ref = child.ref\n            if ref['id'] not in children:\n                plot_context_json['attributes']['children'].append(ref)\n        self.load(plot_context_json, *other_objects)\n        # set the new Plot Context\n        self.context = self._models[plot_context_json['id']]\n\n    def prune(self):\n        \"\"\"Remove all models that are not in the plot context\n        \"\"\"\n        all_models = self.context.references()\n        to_keep = set([x._id for x in all_models])\n        to_delete = set(self._models.keys()) - to_keep\n        to_delete_objs = []\n        for k in to_delete:\n            to_delete_objs.append(self._models.pop(k))\n        return to_delete_objs",
  "def __init__(self, json_objs=None):\n        self._current_plot = None\n        self._hold = False\n        self._models = {}\n\n        self.docid = str(uuid.uuid4())\n        self.autostore = True\n        self.autoadd = True\n\n        if json_objs:\n            self.load(*json_objs, dirty=False)\n\n        # must init context after loading JSON objs\n        self._init_context()",
  "def autoadd(self):\n        return self._autoadd",
  "def autoadd(self, value):\n        if not isinstance(value, bool):\n            raise TypeError(\"'autoadd' must be True or False\")\n        self._autoadd = value",
  "def autostore(self):\n        return self._autostore",
  "def autostore(self, value):\n        if not isinstance(value, bool):\n            raise TypeError(\"'autostore' must be True or False\")\n        self._autostore = value",
  "def context(self):\n        return self._context",
  "def context(self, value):\n        if not isinstance(value, PlotContext):\n            raise TypeError('Document.context may only be assigned to PlotContext objects')\n        try:\n            if self._context:\n                del self._models[self._context._id]\n        except AttributeError:\n            pass\n        other_pcs = [x for x in self._models.values() if x.__view_model__ == 'PlotContext']\n        other_pcs = [x for x in other_pcs if x._id != value._id]\n        if len(other_pcs) != 0:\n            raise DataIntegrityException(\"too many plot contexts found\")\n        self._add(value)\n        self._add(*value.references())\n        self._context = value",
  "def ref(self):\n        return self._context.ref",
  "def clear(self):\n        \"\"\" Remove all plots from this `Document`\n\n        Returns:\n            None\n\n        \"\"\"\n        self.context.children = []\n        context = self.context\n        self._models = {}\n        self._add(context)",
  "def add(self, *objects):\n        \"\"\" Add top-level objects (and any references they hold to sub-objects)\n        to this Document.\n\n        .. warning::\n            This function should only be called on top level objects such\n            as Plot, and Layout containers.\n\n        Args:\n            *objects (PlotObject) : objects to add to the Document\n\n        Returns:\n            None\n\n        \"\"\"\n        for obj in objects:\n            if obj not in self.context.children:\n                self.context.children.append(obj)\n                self.context._dirty = True\n            self._add(*obj.references())",
  "def _add_all(self):\n        # fix for crossfilter - we should take this out soon, and just\n        # ensure that the entire graph is added before dump\n        for obj in self.context.references():\n            self._add(obj)\n        self.prune()",
  "def load(self, *objs, **kwargs):\n        \"\"\" Convert json objects to models and load them into this Document.\n\n        Args:\n            *objs (str) : json object strings to convert\n\n        Keyword Args:\n            Two optional keyword arguments are stripped from *kwargs:\n\n            existing (str) : what objects to trigger events on (default: 'existing')\n                valid values are:\n                * 'none' trigger no events\n                * 'all' trigger events on all models\n                * 'new' trigger events only on new models\n                * 'existing' trigger events on already existing models\n            dirty (bool) : whether to mark models as dirty (default: False)\n\n        Returns:\n            set[Plotobject] : models loaded from json\n\n        \"\"\"\n        events = kwargs.pop('events', 'existing')\n        if events not in ['all', 'none', 'new', 'existing']:\n            raise ValueError(\n                \"Invalid value for events: '%s', valid values are: 'all', 'none', 'new', 'existing'\" % events\n            )\n\n        dirty = kwargs.pop('dirty', False)\n\n        all_models = set()\n        new_models = set()\n\n        for obj in objs:\n            obj_id = obj['attributes']['id'] # XXX: obj['id']\n            obj_type = obj.get('subtype', obj['type'])\n            obj_attrs = obj['attributes']\n\n            if \"doc\" in obj_attrs:\n                del obj_attrs[\"doc\"]\n\n            if obj_id in self._models:\n                model = self._models[obj_id]\n                model._block_callbacks = True\n                model.load_json(obj_attrs, instance=model)\n            else:\n                cls = PlotObject.get_class(obj_type)\n                model = cls.load_json(obj_attrs)\n                if model is None:\n                    raise RuntimeError('Error loading model from JSON (type: %s, id: %s)' % (obj_type, obj_id))\n                self._add(model)\n                new_models.add(model)\n\n            all_models.add(model)\n\n        for m in all_models:\n            props = m.finalize(self._models)\n            m.update(**props)\n            m.setup_events()\n\n        if events == 'all':\n            self.execute_callback_queue(all_models)\n            self.clear_callback_queue(all_models)\n\n        if events == 'none':\n            self.clear_callback_queue(all_models)\n\n        if events == 'new':\n            self.execute_callback_queue(new_models)\n            self.clear_callback_queue(new_models)\n\n        elif events == 'existing':\n            self.execute_callback_queue(all_models-new_models)\n            self.clear_callback_queue(new_models)\n\n        self.enable_callbacks(all_models)\n\n        for m in all_models:\n            m._dirty = dirty\n\n        return all_models",
  "def dump(self, *models):\n        \"\"\" Convert models to json objects.\n\n        Args:\n            *models (PlotObject) : models to convert to json objects\n                If models is empty, ``dump`` converts all models in this d\n                ocument.\n\n        Return:\n            dict : json objects\n\n        \"\"\"\n        self._add(*self.context.references())\n        if not models:\n            models = self._models.values()\n        json = dump(models, docid=self.docid)\n        return json",
  "def disable_callbacks(self, models=None):\n        \"\"\" Disable callbacks on given models.\n\n        Args:\n            models (seq[PlotObject], optional) : models to disable callbacks for (default: None)\n                If models is None, disables callbacks on all models in\n                this Document.\n\n        Returns:\n            None\n\n        \"\"\"\n        if models is None:\n            models = self._models.values()\n        for m in models:\n            m._block_callbacks = True",
  "def enable_callbacks(self, models=None):\n        \"\"\" Enable callbacks on given models.\n\n        Args:\n            models (seq[PlotObject], optional) : models to enable callbacks for (default: None)\n                If models is None, enables callbacks on all models in\n                this Document.\n\n        Returns:\n            None\n\n        \"\"\"\n        if models is None:\n            models = self._models.values()\n\n        for m in models:\n            m._block_callbacks = False",
  "def clear_callback_queue(self, models=None):\n        \"\"\" Clear the callback queue on given models.\n\n        Args:\n            models (seq[PlotObject], optional) : models to clear callbacks for (default: None)\n                If models is None, clears callback queue on all models\n                in this Document.\n\n        Returns:\n            None\n\n        \"\"\"\n        if models is None:\n            models = self._models.values()\n        for m in models:\n            del m._callback_queue[:]",
  "def execute_callback_queue(self, models=None):\n        \"\"\" Execute all queued callbacks on the given models.\n\n        Args:\n            models (seq[PlotObject], optional) : models to execute callbacks for (default: None)\n                If models is None, executes the callback queue on all models\n                in this Document.\n\n        Returns:\n            None\n\n        \"\"\"\n        if models is None:\n            models = self._models.values()\n        for m in models:\n            for cb in m._callback_queue:\n                m._trigger(*cb)\n            del m._callback_queue[:]",
  "def _add(self, *objects):\n        \"\"\" Adds objects to this document.\n\n        \"\"\"\n        for obj in objects:\n            self._models[obj._id] = obj",
  "def _init_context(self):\n        \"\"\" Initialize self.context appropriately.\n\n        If no plotcontext exists, creates one. If one exists in self._modes\n        (because we are on the server) re-use it.\n\n        \"\"\"\n        pcs = [x for x in self._models.values() if x.__view_model__ == 'PlotContext']\n        if len(pcs) == 0:\n            self.context = PlotContext()\n        elif len(pcs) == 1:\n            self._context = pcs[0]\n            self._add(self._context)\n        else:\n            raise DataIntegrityException(\"too many plot contexts found\")",
  "def merge(self, json_objs):\n        \"\"\"Merge's json objects from another document into this one\n        using the plot context id from the json_objs which are passed in\n        children from this document are merged with the children from\n        the json that is passed in\n\n        Args:\n            json_objs : json objects from session.pull()\n\n        Returns:\n            None\n        \"\"\"\n        plot_contexts = [x for x in json_objs if x['type'] == 'PlotContext']\n        other_objects = [x for x in json_objs if x['type'] != 'PlotContext']\n        plot_context_json = plot_contexts[0]\n        children = set([x['id'] for x in plot_context_json['attributes']['children']])\n        for child in self.context.children:\n            ref = child.ref\n            if ref['id'] not in children:\n                plot_context_json['attributes']['children'].append(ref)\n        self.load(plot_context_json, *other_objects)\n        # set the new Plot Context\n        self.context = self._models[plot_context_json['id']]",
  "def prune(self):\n        \"\"\"Remove all models that are not in the plot context\n        \"\"\"\n        all_models = self.context.references()\n        to_keep = set([x._id for x in all_models])\n        to_delete = set(self._models.keys()) - to_keep\n        to_delete_objs = []\n        for k in to_delete:\n            to_delete_objs.append(self._models.pop(k))\n        return to_delete_objs",
  "class Figure(Plot):\n    __subtype__ = \"Figure\"\n    __view_model__ = \"Plot\"\n\n    def __init__(self, *arg, **kw):\n\n        tools = kw.pop(\"tools\", DEFAULT_TOOLS)\n\n        x_range = kw.pop(\"x_range\", None)\n        y_range = kw.pop(\"y_range\", None)\n\n        x_axis_type = kw.pop(\"x_axis_type\", \"auto\")\n        y_axis_type = kw.pop(\"y_axis_type\", \"auto\")\n\n        x_minor_ticks = kw.pop('x_minor_ticks', 'auto')\n        y_minor_ticks = kw.pop('y_minor_ticks', 'auto')\n\n        x_axis_location = kw.pop(\"x_axis_location\", \"below\")\n        y_axis_location = kw.pop(\"y_axis_location\", \"left\")\n\n        x_axis_label = kw.pop(\"x_axis_label\", \"\")\n        y_axis_label = kw.pop(\"y_axis_label\", \"\")\n\n        super(Figure, self).__init__(*arg, **kw)\n\n        self.x_range = _get_range(x_range)\n        self.y_range = _get_range(y_range)\n\n        x_axiscls = _get_axis_class(x_axis_type, self.x_range)\n        if x_axiscls:\n            if x_axiscls is LogAxis:\n                self.x_mapper_type = 'log'\n            xaxis = x_axiscls(plot=self)\n            xaxis.ticker.num_minor_ticks = _get_num_minor_ticks(x_axiscls, x_minor_ticks)\n            axis_label = x_axis_label\n            if axis_label:\n                xaxis.axis_label = axis_label\n            xgrid = Grid(plot=self, dimension=0, ticker=xaxis.ticker)\n            if x_axis_location == \"above\":\n                self.above.append(xaxis)\n            elif x_axis_location == \"below\":\n                self.below.append(xaxis)\n\n        y_axiscls = _get_axis_class(y_axis_type, self.y_range)\n        if y_axiscls:\n            if y_axiscls is LogAxis:\n                self.y_mapper_type = 'log'\n            yaxis = y_axiscls(plot=self)\n            yaxis.ticker.num_minor_ticks = _get_num_minor_ticks(y_axiscls, y_minor_ticks)\n            axis_label = y_axis_label\n            if axis_label:\n                yaxis.axis_label = axis_label\n            ygrid = Grid(plot=self, dimension=1, ticker=yaxis.ticker)\n            if y_axis_location == \"left\":\n                self.left.append(yaxis)\n            elif y_axis_location == \"right\":\n                self.right.append(yaxis)\n\n        tool_objs = _process_tools_arg(self, tools)\n        self.add_tools(*tool_objs)\n\n    def _axis(self, *sides):\n        objs = []\n        for s in sides:\n            objs.extend(getattr(self, s, []))\n        axis = [obj for obj in objs if isinstance(obj, Axis)]\n        return _list_attr_splat(axis)\n\n    @property\n    def xaxis(self):\n        \"\"\" Get the current `x` axis object(s)\n\n        Returns:\n            splattable list of x-axis objects on this Plot\n        \"\"\"\n        return self._axis(\"above\", \"below\")\n\n    @property\n    def yaxis(self):\n        \"\"\" Get the current `y` axis object(s)\n\n        Returns:\n            splattable list of y-axis objects on this Plot\n        \"\"\"\n        return self._axis(\"left\", \"right\")\n\n    @property\n    def axis(self):\n        \"\"\" Get all the current axis objects\n\n        Returns:\n            splattable list of axis objects on this Plot\n        \"\"\"\n        return _list_attr_splat(self.xaxis + self.yaxis)\n\n    @property\n    def legend(self):\n        \"\"\" Get the current :class:`legend <bokeh.models.Legend>` object(s)\n\n        Returns:\n            splattable list of legend objects on this Plot\n        \"\"\"\n        legends = [obj for obj in self.renderers if isinstance(obj, Legend)]\n        return _list_attr_splat(legends)\n\n    def _grid(self, dimension):\n        grid = [obj for obj in self.renderers if isinstance(obj, Grid) and obj.dimension==dimension]\n        return _list_attr_splat(grid)\n\n    @property\n    def xgrid(self):\n        \"\"\" Get the current `x` :class:`grid <bokeh.models.Grid>` object(s)\n\n        Returns:\n            splattable list of legend objects on this Plot\n        \"\"\"\n        return self._grid(0)\n\n    @property\n    def ygrid(self):\n        \"\"\" Get the current `y` :class:`grid <bokeh.models.Grid>` object(s)\n\n        Returns:\n            splattable list of y-grid objects on this Plot\n        \"\"\"\n        return self._grid(1)\n\n    @property\n    def grid(self):\n        \"\"\" Get the current :class:`grid <bokeh.models.Grid>` object(s)\n\n        Returns:\n            splattable list of grid objects on this Plot\n        \"\"\"\n        return _list_attr_splat(self.xgrid + self.ygrid)\n\n    annular_wedge     = gf.annular_wedge\n    annulus           = gf.annulus\n    arc               = gf.arc\n    asterisk          = gf.asterisk\n    bezier            = gf.bezier\n    circle            = gf.circle\n    circle_cross      = gf.circle_cross\n    circle_x          = gf. circle_x\n    cross             = gf.cross\n    diamond           = gf.diamond\n    diamond_cross     = gf.diamond_cross\n    image             = gf.image\n    image_rgba        = gf.image_rgba\n    image_url         = gf.image_url\n    inverted_triangle = gf.inverted_triangle\n    line              = gf.line\n    multi_line        = gf.multi_line\n    oval              = gf.oval\n    patch             = gf.patch\n    patches           = gf.patches\n    quad              = gf.quad\n    quadratic         = gf.quadratic\n    ray               = gf.ray\n    rect              = gf.rect\n    segment           = gf.segment\n    square            = gf.square\n    square_cross      = gf.square_cross\n    square_x          = gf.square_x\n    text              = gf.text\n    triangle          = gf.triangle\n    wedge             = gf.wedge\n    x                 = gf.x\n\n    def scatter(self, *args, **kwargs):\n        \"\"\" Creates a scatter plot of the given x and y items.\n\n        Args:\n            *args : The data to plot.  Can be of several forms:\n\n                (X, Y)\n                    Two 1D arrays or iterables\n                (XNAME, YNAME)\n                    Two bokeh DataSource/ColumnsRef\n\n            marker (str, optional): a valid marker_type, defaults to \"circle\"\n            color (color value, optional): shorthand to set both fill and line color\n\n        All the :ref:`userguide_objects_line_properties` and :ref:`userguide_objects_fill_properties` are\n        also accepted as keyword parameters.\n\n        Examples:\n\n            >>> p.scatter([1,2,3],[4,5,6], fill_color=\"red\")\n            >>> p.scatter(\"data1\", \"data2\", source=data_source, ...)\n\n        \"\"\"\n        ds = kwargs.get(\"source\", None)\n        names, datasource = _handle_1d_data_args(args, datasource=ds)\n        kwargs[\"source\"] = datasource\n\n        markertype = kwargs.get(\"marker\", \"circle\")\n\n        # TODO: How to handle this? Just call curplot()?\n        if not len(_color_fields.intersection(set(kwargs.keys()))):\n            kwargs['color'] = get_default_color()\n        if not len(_alpha_fields.intersection(set(kwargs.keys()))):\n            kwargs['alpha'] = get_default_alpha()\n\n        if markertype not in _marker_types:\n            raise ValueError(\"Invalid marker type '%s'. Use markers() to see a list of valid marker types.\" % markertype)\n\n        # TODO (bev) make better when plotting.scatter is removed\n        conversions = {\n            \"*\": \"asterisk\",\n            \"+\": \"cross\",\n            \"o\": \"circle\",\n            \"ox\": \"circle_x\",\n            \"o+\": \"circle_cross\"\n        }\n        if markertype in conversions:\n            markertype = conversions[markertype]\n\n        return getattr(self, markertype)(*args, **kwargs)",
  "def curdoc():\n    ''' Return the current document.\n\n    Returns:\n        doc : the current default document object.\n    '''\n    try:\n        \"\"\"This is used when we need to call the plotting API from within\n        the server, within a request context.  (Applets do this for example)\n        in this case you still want the API to work but you don't want\n        to use the global module level document\n        \"\"\"\n        from flask import request\n        doc = request.bokeh_server_document\n        logger.debug(\"returning config from flask request\")\n        return doc\n    except (ImportError, RuntimeError, AttributeError):\n        return _default_document",
  "def cursession():\n    ''' Return the current session, if there is one.\n\n    Returns:\n        session : the current default session object (or None)\n    '''\n    return _default_session",
  "def reset_output():\n    ''' Deactivate all currently active output modes.\n\n    Subsequent calls to show() will not render until a new output mode is\n    activated.\n\n    Returns:\n        None\n\n    '''\n    global _default_document\n    global _default_session\n    global _default_file\n    global _default_notebook\n    _default_document = Document()\n    _default_session = None\n    _default_file = None\n    _default_notebook = None",
  "def figure(**kwargs):\n    ''' Activate a new figure for plotting.\n\n    All subsequent plotting operations will affect the new figure.\n\n    This function accepts all plot style keyword parameters.\n\n    Returns:\n       figure : a new :class:`Plot <bokeh.models.plots.Plot>`\n\n    '''\n    if 'plot_width' in kwargs and 'width' in kwargs:\n        raise ValueError(\"figure() called but both plot_width and width supplied, supply only one\")\n    if 'plot_height' in kwargs and 'height' in kwargs:\n        raise ValueError(\"figure() called but both plot_height and height supplied, supply only one\")\n    if 'height' in kwargs:\n        kwargs['plot_height'] = kwargs.pop('height')\n    if 'width' in kwargs:\n        kwargs['plot_width'] = kwargs.pop('width')\n\n    fig = Figure(**kwargs)\n    curdoc()._current_plot = fig\n    curdoc().add(fig)\n    return fig",
  "def output_server(docname, session=None, url=\"default\", name=None, clear=True):\n    \"\"\" Cause plotting commands to automatically persist plots to a Bokeh server.\n\n    Can use explicitly provided Session for persistence, or the default\n    session.\n\n    Args:\n        docname (str) : name of document to push on Bokeh server\n            An existing documents with the same name will be overwritten.\n        session (Session, optional) : An explicit session to use (default: None)\n            If session is None, use the default session\n        url (str, optianal) : URL of the Bokeh server  (default: \"default\")\n            if url is \"default\" use session.DEFAULT_SERVER_URL\n        name (str, optional) :\n            if name is None, use the server URL as the name\n        clear (bool, optional) :\n            should an existing server document be cleared of any existing\n            plots. (default: True)\n\n    Additional keyword arguments like **username**, **userapikey**,\n    and **base_url** can also be supplied.\n\n    Returns:\n        None\n\n    .. note:: Generally, this should be called at the beginning of an\n              interactive session or the top of a script.\n\n    .. note:: By default, calling this function will replaces any existing\n              default Server session.\n\n    \"\"\"\n    global _default_session\n    if url == \"default\":\n        url = DEFAULT_SERVER_URL\n    if name is None:\n        name = url\n    if not session:\n        if not _default_session:\n            _default_session = Session(name=name, root_url=url)\n        session = _default_session\n    else:\n        _default_session = session\n    session.use_doc(docname)\n    session.load_document(curdoc())\n    if clear:\n        curdoc().clear()",
  "def output_notebook(url=None, docname=None, session=None, name=None):\n    if session or url or name:\n        if docname is None:\n            docname = \"IPython Session at %s\" % time.ctime()\n        output_server(docname, url=url, session=session, name=name)\n    global _default_notebook\n    _default_notebook = True",
  "def output_file(filename, title=\"Bokeh Plot\", autosave=False, mode=\"inline\", root_dir=None):\n    \"\"\" Outputs to a static HTML file.\n\n    .. note:: This file will be overwritten each time show() or save() is invoked.\n\n    Args:\n        autosave (bool, optional) : whether to automatically save (default: False)\n            If **autosave** is True, then every time plot() or one of the other\n            visual functions is called, this causes the file to be saved. If it\n            is False, then the file is only saved upon calling show().\n\n        mode (str, optional) : how to inlude BokehJS (default: \"inline\")\n            **mode** can be 'inline', 'cdn', 'relative(-dev)' or 'absolute(-dev)'.\n            In the 'relative(-dev)' case, **root_dir** can be specified to indicate the\n            base directory from which the path to the various static files should be\n            computed.\n\n    .. note:: Generally, this should be called at the beginning of an\n              interactive session or the top of a script.\n\n    \"\"\"\n    global _default_file\n    _default_file = {\n        'filename'  : filename,\n        'resources' : Resources(mode=mode, root_dir=root_dir),\n        'autosave'  : autosave,\n        'title'     : title,\n    }\n\n    if os.path.isfile(filename):\n        print(\"Session output file '%s' already exists, will be overwritten.\" % filename)",
  "def show(obj, browser=None, new=\"tab\", url=None):\n    \"\"\" Immediately display a plot object.\n\n    In an IPython/Jupyter notebook, the output is displayed in an output cell.\n    Otherwise, a browser window or tab is autoraised to display the plot object.\n\n    Args:\n        obj (Widget/Plot object): a plot object to display\n\n        browser (str, optional) : browser to show with (default: None)\n            For systems that support it, the **browser** argument allows specifying\n            which browser to display in, e.g. \"safari\", \"firefox\", \"opera\",\n            \"windows-default\" (see the webbrowser module documentation in the\n            standard lib for more details).\n\n        new (str, optional) : new file output mode (default: \"tab\")\n            For file-based output, opens or raises the browser window\n            showing the current output file.  If **new** is 'tab', then\n            opens a new tab. If **new** is 'window', then opens a new window.\n    \"\"\"\n    filename = _default_file['filename'] if _default_file else None\n    session = cursession()\n    notebook = _default_notebook\n\n    # Map our string argument to the webbrowser.open argument\n    new_param = {'tab': 2, 'window': 1}[new]\n\n    controller = browserlib.get_browser_controller(browser=browser)\n\n    if notebook and session:\n        push(session=session)\n        snippet = autoload_server(obj, cursession())\n        publish_display_data({'text/html': snippet})\n\n    elif notebook:\n        publish_display_data({'text/html': notebook_div(obj)})\n\n    elif session:\n        push()\n        if url:\n            controller.open(url, new=new_param)\n        else:\n            controller.open(session.object_link(curdoc().context))\n\n    elif filename:\n        save(filename, obj=obj)\n        controller.open(\"file://\" + os.path.abspath(filename), new=new_param)",
  "def save(filename=None, resources=None, obj=None, title=None):\n    \"\"\" Updates the file with the data for the current document.\n\n    If a filename is supplied, or output_file(...) has been called, this will\n    save the plot to the given filename.\n\n    Args:\n        filename (str, optional) : filename to save document under (default: None)\n            if `filename` is None, the current output_file(...) filename is used if present\n        resources (Resources, optional) : BokehJS resource config to use\n            if `resources` is None, the current default resource config is used, failing that resources.INLINE is used\n\n        obj (Document or Widget/Plot object, optional)\n            if provided, then this is the object to save instead of curdoc()\n            and its curplot()\n        title (str, optional) : title of the bokeh plot (default: None)\n        \tif 'title' is None, the current default title config is used, failing that 'Bokeh Plot' is used\n\n    Returns:\n        None\n\n    \"\"\"\n    if filename is None and _default_file:\n        filename = _default_file['filename']\n\n    if resources is None and _default_file:\n        resources = _default_file['resources']\n\n    if title is None and _default_file:\n        title = _default_file['title']\n\n    if not filename:\n        warnings.warn(\"save() called but no filename was supplied and output_file(...) was never called, nothing saved\")\n        return\n\n    if not resources:\n        warnings.warn(\"save() called but no resources was supplied and output_file(...) was never called, defaulting to resources.INLINE\")\n        from .resources import INLINE\n        resources = INLINE\n\n\n    if not title:\n        warnings.warn(\"save() called but no title was supplied and output_file(...) was never called, using default title 'Bokeh Plot'\")\n        title = \"Bokeh Plot\"\n\n    if obj is None:\n        if not curplot():\n            warnings.warn(\"No current plot to save. Use renderer functions (circle, rect, etc.) to create a current plot (see http://bokeh.pydata.org/index.html)\")\n            return\n        doc = curdoc()\n    elif isinstance(obj, Widget):\n        doc = Document()\n        doc.add(obj)\n    elif isinstance(obj, Document):\n        doc = obj\n    else:\n        raise RuntimeError(\"Unable to save object of type '%s'\" % type(obj))\n\n    html = file_html(doc, resources, title)\n    with io.open(filename, \"w\", encoding=\"utf-8\") as f:\n        f.write(decode_utf8(html))",
  "def push(session=None, document=None):\n    \"\"\" Updates the server with the data for the current document.\n\n    Args:\n        session (Sesion, optional) : filename to save document under (default: None)\n            if `sessiokn` is None, the current output_server(...) session is used if present\n        document (Document, optional) : BokehJS document to push\n            if `document` is None, the current default document is pushed\n\n    Returns:\n        None\n\n    \"\"\"\n    if not session:\n        session = cursession()\n\n    if not document:\n        document = curdoc()\n\n    if session:\n        return session.store_document(curdoc())\n    else:\n        warnings.warn(\"push() called but no session was supplied and output_server(...) was never called, nothing pushed\")",
  "def markers():\n    \"\"\" Prints a list of valid marker types for scatter()\n\n    Returns:\n        None\n    \"\"\"\n    print(list(sorted(_marker_types.keys())))",
  "def _deduplicate_plots(plot, subplots):\n    doc = curdoc()\n    doc.context.children = list(set(doc.context.children) - set(subplots))\n    doc.add(plot)\n    doc._current_plot = plot",
  "def _push_or_save():\n    if cursession() and curdoc().autostore:\n        push()\n    if _default_file and _default_file['autosave']:\n        save()",
  "def gridplot(plot_arrangement, **kwargs):\n    \"\"\" Generate a plot that arranges several subplots into a grid.\n\n    Args:\n        plot_arrangement (nested list of Plots) : plots to arrange in a grid\n        **kwargs: additional attributes to pass in to GridPlot() constructor\n\n    .. note:: `plot_arrangement` can be nested, e.g [[p1, p2], [p3, p4]]\n\n    Returns:\n        grid_plot: a new :class:`GridPlot <bokeh.models.plots.GridPlot>`\n    \"\"\"\n    grid = GridPlot(children=plot_arrangement, **kwargs)\n    subplots = itertools.chain.from_iterable(plot_arrangement)\n    _deduplicate_plots(grid, subplots)\n    _push_or_save()\n    return grid",
  "def load_object(obj):\n    \"\"\"updates object from the server\n    \"\"\"\n    cursession().load_object(obj, curdoc())",
  "def __init__(self, *arg, **kw):\n\n        tools = kw.pop(\"tools\", DEFAULT_TOOLS)\n\n        x_range = kw.pop(\"x_range\", None)\n        y_range = kw.pop(\"y_range\", None)\n\n        x_axis_type = kw.pop(\"x_axis_type\", \"auto\")\n        y_axis_type = kw.pop(\"y_axis_type\", \"auto\")\n\n        x_minor_ticks = kw.pop('x_minor_ticks', 'auto')\n        y_minor_ticks = kw.pop('y_minor_ticks', 'auto')\n\n        x_axis_location = kw.pop(\"x_axis_location\", \"below\")\n        y_axis_location = kw.pop(\"y_axis_location\", \"left\")\n\n        x_axis_label = kw.pop(\"x_axis_label\", \"\")\n        y_axis_label = kw.pop(\"y_axis_label\", \"\")\n\n        super(Figure, self).__init__(*arg, **kw)\n\n        self.x_range = _get_range(x_range)\n        self.y_range = _get_range(y_range)\n\n        x_axiscls = _get_axis_class(x_axis_type, self.x_range)\n        if x_axiscls:\n            if x_axiscls is LogAxis:\n                self.x_mapper_type = 'log'\n            xaxis = x_axiscls(plot=self)\n            xaxis.ticker.num_minor_ticks = _get_num_minor_ticks(x_axiscls, x_minor_ticks)\n            axis_label = x_axis_label\n            if axis_label:\n                xaxis.axis_label = axis_label\n            xgrid = Grid(plot=self, dimension=0, ticker=xaxis.ticker)\n            if x_axis_location == \"above\":\n                self.above.append(xaxis)\n            elif x_axis_location == \"below\":\n                self.below.append(xaxis)\n\n        y_axiscls = _get_axis_class(y_axis_type, self.y_range)\n        if y_axiscls:\n            if y_axiscls is LogAxis:\n                self.y_mapper_type = 'log'\n            yaxis = y_axiscls(plot=self)\n            yaxis.ticker.num_minor_ticks = _get_num_minor_ticks(y_axiscls, y_minor_ticks)\n            axis_label = y_axis_label\n            if axis_label:\n                yaxis.axis_label = axis_label\n            ygrid = Grid(plot=self, dimension=1, ticker=yaxis.ticker)\n            if y_axis_location == \"left\":\n                self.left.append(yaxis)\n            elif y_axis_location == \"right\":\n                self.right.append(yaxis)\n\n        tool_objs = _process_tools_arg(self, tools)\n        self.add_tools(*tool_objs)",
  "def _axis(self, *sides):\n        objs = []\n        for s in sides:\n            objs.extend(getattr(self, s, []))\n        axis = [obj for obj in objs if isinstance(obj, Axis)]\n        return _list_attr_splat(axis)",
  "def xaxis(self):\n        \"\"\" Get the current `x` axis object(s)\n\n        Returns:\n            splattable list of x-axis objects on this Plot\n        \"\"\"\n        return self._axis(\"above\", \"below\")",
  "def yaxis(self):\n        \"\"\" Get the current `y` axis object(s)\n\n        Returns:\n            splattable list of y-axis objects on this Plot\n        \"\"\"\n        return self._axis(\"left\", \"right\")",
  "def axis(self):\n        \"\"\" Get all the current axis objects\n\n        Returns:\n            splattable list of axis objects on this Plot\n        \"\"\"\n        return _list_attr_splat(self.xaxis + self.yaxis)",
  "def legend(self):\n        \"\"\" Get the current :class:`legend <bokeh.models.Legend>` object(s)\n\n        Returns:\n            splattable list of legend objects on this Plot\n        \"\"\"\n        legends = [obj for obj in self.renderers if isinstance(obj, Legend)]\n        return _list_attr_splat(legends)",
  "def _grid(self, dimension):\n        grid = [obj for obj in self.renderers if isinstance(obj, Grid) and obj.dimension==dimension]\n        return _list_attr_splat(grid)",
  "def xgrid(self):\n        \"\"\" Get the current `x` :class:`grid <bokeh.models.Grid>` object(s)\n\n        Returns:\n            splattable list of legend objects on this Plot\n        \"\"\"\n        return self._grid(0)",
  "def ygrid(self):\n        \"\"\" Get the current `y` :class:`grid <bokeh.models.Grid>` object(s)\n\n        Returns:\n            splattable list of y-grid objects on this Plot\n        \"\"\"\n        return self._grid(1)",
  "def grid(self):\n        \"\"\" Get the current :class:`grid <bokeh.models.Grid>` object(s)\n\n        Returns:\n            splattable list of grid objects on this Plot\n        \"\"\"\n        return _list_attr_splat(self.xgrid + self.ygrid)",
  "def scatter(self, *args, **kwargs):\n        \"\"\" Creates a scatter plot of the given x and y items.\n\n        Args:\n            *args : The data to plot.  Can be of several forms:\n\n                (X, Y)\n                    Two 1D arrays or iterables\n                (XNAME, YNAME)\n                    Two bokeh DataSource/ColumnsRef\n\n            marker (str, optional): a valid marker_type, defaults to \"circle\"\n            color (color value, optional): shorthand to set both fill and line color\n\n        All the :ref:`userguide_objects_line_properties` and :ref:`userguide_objects_fill_properties` are\n        also accepted as keyword parameters.\n\n        Examples:\n\n            >>> p.scatter([1,2,3],[4,5,6], fill_color=\"red\")\n            >>> p.scatter(\"data1\", \"data2\", source=data_source, ...)\n\n        \"\"\"\n        ds = kwargs.get(\"source\", None)\n        names, datasource = _handle_1d_data_args(args, datasource=ds)\n        kwargs[\"source\"] = datasource\n\n        markertype = kwargs.get(\"marker\", \"circle\")\n\n        # TODO: How to handle this? Just call curplot()?\n        if not len(_color_fields.intersection(set(kwargs.keys()))):\n            kwargs['color'] = get_default_color()\n        if not len(_alpha_fields.intersection(set(kwargs.keys()))):\n            kwargs['alpha'] = get_default_alpha()\n\n        if markertype not in _marker_types:\n            raise ValueError(\"Invalid marker type '%s'. Use markers() to see a list of valid marker types.\" % markertype)\n\n        # TODO (bev) make better when plotting.scatter is removed\n        conversions = {\n            \"*\": \"asterisk\",\n            \"+\": \"cross\",\n            \"o\": \"circle\",\n            \"ox\": \"circle_x\",\n            \"o+\": \"circle_cross\"\n        }\n        if markertype in conversions:\n            markertype = conversions[markertype]\n\n        return getattr(self, markertype)(*args, **kwargs)",
  "def mergeFunctionMetadata(f, g):\n    \"\"\"\n    Overwrite C{g}'s name and docstring with values from C{f}.  Update\n    C{g}'s instance dictionary with C{f}'s.\n\n    To use this function safely you must use the return value. In Python 2.3,\n    L{mergeFunctionMetadata} will create a new function. In later versions of\n    Python, C{g} will be mutated and returned.\n\n    @return: A function that has C{g}'s behavior and metadata merged from\n        C{f}.\n    \"\"\"\n    try:\n        g.__name__ = f.__name__\n    except TypeError:\n        try:\n            merged = types.FunctionType(\n                g.func_code, g.func_globals,\n                f.__name__, inspect.getargspec(g)[-1],\n                g.func_closure)\n        except TypeError:\n            pass\n    else:\n        merged = g\n    try:\n        merged.__doc__ = f.__doc__\n    except (TypeError, AttributeError):\n        pass\n    try:\n        merged.__dict__.update(g.__dict__)\n        merged.__dict__.update(f.__dict__)\n    except (TypeError, AttributeError):\n        pass\n    merged.__module__ = f.__module__\n    return merged",
  "def _fullyQualifiedName(obj):\n    \"\"\"\n    Return the fully qualified name of a module, class, method or function.\n    Classes and functions need to be module level ones to be correctly\n    qualified.\n\n    @rtype: C{str}.\n    \"\"\"\n    name = obj.__name__\n    if inspect.isclass(obj) or inspect.isfunction(obj):\n        moduleName = obj.__module__\n        return \"%s.%s\" % (moduleName, name)\n    elif inspect.ismethod(obj):\n        className = _fullyQualifiedName(obj.im_class)\n        return \"%s.%s\" % (className, name)\n    return name",
  "def getWarningMethod():\n    \"\"\"\n    Return the warning method currently used to record deprecation warnings.\n    \"\"\"\n    return warn",
  "def setWarningMethod(newMethod):\n    \"\"\"\n    Set the warning method to use to record deprecation warnings.\n\n    The callable should take message, category and stacklevel. The return\n    value is ignored.\n    \"\"\"\n    global warn\n    warn = newMethod",
  "def _getDeprecationDocstring(version, replacement=None):\n    \"\"\"\n    Generate an addition to a deprecated object's docstring that explains its\n    deprecation.\n\n    @param version: the version it was deprecated.\n    @type version: L{Version}\n\n    @param replacement: The replacement, if specified.\n    @type replacement: C{str} or callable\n\n    @return: a string like \"Deprecated in Twisted 27.2.0; please use\n        twisted.timestream.tachyon.flux instead.\"\n    \"\"\"\n    doc = \"Deprecated in %s\" % (version,)\n    if replacement:\n        doc = \"%s; %s\" % (doc, _getReplacementString(replacement))\n    return doc + \".\"",
  "def _getReplacementString(replacement):\n    \"\"\"\n    Surround a replacement for a deprecated API with some polite text exhorting\n    the user to consider it as an alternative.\n\n    @type replacement: C{str} or callable\n\n    @return: a string like \"please use twisted.python.modules.getModule\n        instead\".\n    \"\"\"\n    if callable(replacement):\n        replacement = _fullyQualifiedName(replacement)\n    return \"please use %s instead\" % (replacement,)",
  "def _getDeprecationWarningString(fqpn, version, format=None, replacement=None):\n    \"\"\"\n    Return a string indicating that the Python name was deprecated in the given\n    version.\n\n    @param fqpn: Fully qualified Python name of the thing being deprecated\n    @type fqpn: C{str}\n\n    @param version: Version that C{fqpn} was deprecated in.\n    @type version: L{twisted.python.versions.Version}\n\n    @param format: A user-provided format to interpolate warning values into, or\n        L{DEPRECATION_WARNING_FORMAT\n        <twisted.python.deprecate.DEPRECATION_WARNING_FORMAT>} if C{None} is\n        given.\n    @type format: C{str}\n\n    @param replacement: what should be used in place of C{fqpn}. Either pass in\n        a string, which will be inserted into the warning message, or a\n        callable, which will be expanded to its full import path.\n    @type replacement: C{str} or callable\n\n    @return: A textual description of the deprecation\n    @rtype: C{str}\n    \"\"\"\n    if format is None:\n        format = DEPRECATION_WARNING_FORMAT\n    warningString = format % {\n        'fqpn': fqpn,\n        'version': version}\n    if replacement:\n        warningString = \"%s; %s\" % (\n            warningString, _getReplacementString(replacement))\n    return warningString",
  "def getDeprecationWarningString(callableThing, version, format=None,\n                                replacement=None):\n    \"\"\"\n    Return a string indicating that the callable was deprecated in the given\n    version.\n\n    @type callableThing: C{callable}\n    @param callableThing: Callable object to be deprecated\n\n    @type version: L{twisted.python.versions.Version}\n    @param version: Version that C{callableThing} was deprecated in\n\n    @type format: C{str}\n    @param format: A user-provided format to interpolate warning values into,\n        or L{DEPRECATION_WARNING_FORMAT\n        <twisted.python.deprecate.DEPRECATION_WARNING_FORMAT>} if C{None} is\n        given\n\n    @param callableThing: A callable to be deprecated.\n\n    @param version: The L{twisted.python.versions.Version} that the callable\n        was deprecated in.\n\n    @param replacement: what should be used in place of the callable. Either\n        pass in a string, which will be inserted into the warning message,\n        or a callable, which will be expanded to its full import path.\n    @type replacement: C{str} or callable\n\n    @return: A string describing the deprecation.\n    @rtype: C{str}\n    \"\"\"\n    return _getDeprecationWarningString(\n        _fullyQualifiedName(callableThing), version, format, replacement)",
  "def deprecated_module(name, version, replacement):\n    message = _getDeprecationWarningString(name, version, DEPRECATION_WARNING_FORMAT + ': ' + replacement)\n    warn(message, DeprecationWarning, stacklevel=2)",
  "def deprecated(version, replacement=None):\n    \"\"\"\n    Return a decorator that marks callables as deprecated.\n\n    @type version: L{twisted.python.versions.Version}\n    @param version: The version in which the callable will be marked as\n        having been deprecated.  The decorated function will be annotated\n        with this version, having it set as its C{deprecatedVersion}\n        attribute.\n\n    @param version: the version that the callable was deprecated in.\n    @type version: L{twisted.python.versions.Version}\n\n    @param replacement: what should be used in place of the callable. Either\n        pass in a string, which will be inserted into the warning message,\n        or a callable, which will be expanded to its full import path.\n    @type replacement: C{str} or callable\n    \"\"\"\n    def deprecationDecorator(function):\n        \"\"\"\n        Decorator that marks C{function} as deprecated.\n        \"\"\"\n        warningString = getDeprecationWarningString(\n            function, version, None, replacement)\n\n        def deprecatedFunction(*args, **kwargs):\n            warn(\n                warningString,\n                DeprecationWarning,\n                stacklevel=2)\n            return function(*args, **kwargs)\n\n        deprecatedFunction = mergeFunctionMetadata(\n            function, deprecatedFunction)\n        _appendToDocstring(deprecatedFunction,\n                           _getDeprecationDocstring(version, replacement))\n        deprecatedFunction.deprecatedVersion = version\n        return deprecatedFunction\n\n    return deprecationDecorator",
  "def _appendToDocstring(thingWithDoc, textToAppend):\n    \"\"\"\n    Append the given text to the docstring of C{thingWithDoc}.\n\n    If C{thingWithDoc} has no docstring, then the text just replaces the\n    docstring. If it has a single-line docstring then it appends a blank line\n    and the message text. If it has a multi-line docstring, then in appends a\n    blank line a the message text, and also does the indentation correctly.\n    \"\"\"\n    if thingWithDoc.__doc__:\n        docstringLines = thingWithDoc.__doc__.splitlines()\n    else:\n        docstringLines = []\n\n    if len(docstringLines) == 0:\n        docstringLines.append(textToAppend)\n    elif len(docstringLines) == 1:\n        docstringLines.extend(['', textToAppend, ''])\n    else:\n        spaces = docstringLines.pop()\n        docstringLines.extend(['',\n                               spaces + textToAppend,\n                               spaces])\n    thingWithDoc.__doc__ = '\\n'.join(docstringLines)",
  "class _ModuleProxy(object):\n    \"\"\"\n    Python module wrapper to hook module-level attribute access.\n\n    Access to deprecated attributes first checks\n    L{_ModuleProxy._deprecatedAttributes}, if the attribute does not appear\n    there then access falls through to L{_ModuleProxy._module}, the wrapped\n    module object.\n\n    @type _module: C{module}\n    @ivar _module: Module on which to hook attribute access.\n\n    @type _deprecatedAttributes: C{dict} mapping C{str} to\n        L{_DeprecatedAttribute}\n    @ivar _deprecatedAttributes: Mapping of attribute names to objects that\n        retrieve the module attribute's original value.\n    \"\"\"\n    def __init__(self, module):\n        object.__setattr__(self, '_module', module)\n        object.__setattr__(self, '_deprecatedAttributes', {})\n\n\n    def __repr__(self):\n        \"\"\"\n        Get a string containing the type of the module proxy and a\n        representation of the wrapped module object.\n        \"\"\"\n        _module = object.__getattribute__(self, '_module')\n        return '<%s module=%r>' % (\n            type(self).__name__,\n            _module)\n\n\n    def __setattr__(self, name, value):\n        \"\"\"\n        Set an attribute on the wrapped module object.\n        \"\"\"\n        _module = object.__getattribute__(self, '_module')\n        setattr(_module, name, value)\n\n\n    def __getattribute__(self, name):\n        \"\"\"\n        Get an attribute on the wrapped module object.\n\n        If the specified name has been deprecated then a warning is issued.\n        \"\"\"\n        _module = object.__getattribute__(self, '_module')\n        _deprecatedAttributes = object.__getattribute__(\n            self, '_deprecatedAttributes')\n\n        getter = _deprecatedAttributes.get(name)\n        if getter is not None:\n            value = getter.get()\n        else:\n            value = getattr(_module, name)\n        return value",
  "class _DeprecatedAttribute(object):\n    \"\"\"\n    Wrapper for deprecated attributes.\n\n    This is intended to be used by L{_ModuleProxy}. Calling\n    L{_DeprecatedAttribute.get} will issue a warning and retrieve the\n    underlying attribute's value.\n\n    @type module: C{module}\n    @ivar module: The original module instance containing this attribute\n\n    @type fqpn: C{str}\n    @ivar fqpn: Fully qualified Python name for the deprecated attribute\n\n    @type version: L{twisted.python.versions.Version}\n    @ivar version: Version that the attribute was deprecated in\n\n    @type message: C{str}\n    @ivar message: Deprecation message\n    \"\"\"\n    def __init__(self, module, name, version, message):\n        \"\"\"\n        Initialise a deprecated name wrapper.\n        \"\"\"\n        self.module = module\n        self.__name__ = name\n        self.fqpn = module.__name__ + '.' + name\n        self.version = version\n        self.message = message\n\n\n    def get(self):\n        \"\"\"\n        Get the underlying attribute value and issue a deprecation warning.\n        \"\"\"\n        # This might fail if the deprecated thing is a module inside a package.\n        # In that case, don't emit the warning this time.  The import system\n        # will come back again when it's not an AttributeError and we can emit\n        # the warning then.\n        result = getattr(self.module, self.__name__)\n        message = _getDeprecationWarningString(self.fqpn, self.version,\n            DEPRECATION_WARNING_FORMAT + ': ' + self.message)\n        warn(message, DeprecationWarning, stacklevel=3)\n        return result",
  "def _deprecateAttribute(proxy, name, version, message):\n    \"\"\"\n    Mark a module-level attribute as being deprecated.\n\n    @type proxy: L{_ModuleProxy}\n    @param proxy: The module proxy instance proxying the deprecated attributes\n\n    @type name: C{str}\n    @param name: Attribute name\n\n    @type version: L{twisted.python.versions.Version}\n    @param version: Version that the attribute was deprecated in\n\n    @type message: C{str}\n    @param message: Deprecation message\n    \"\"\"\n    _module = object.__getattribute__(proxy, '_module')\n    attr = _DeprecatedAttribute(_module, name, version, message)\n    # Add a deprecated attribute marker for this module's attribute. When this\n    # attribute is accessed via _ModuleProxy a warning is emitted.\n    _deprecatedAttributes = object.__getattribute__(\n        proxy, '_deprecatedAttributes')\n    _deprecatedAttributes[name] = attr",
  "def deprecatedModuleAttribute(version, message, moduleName, name):\n    \"\"\"\n    Declare a module-level attribute as being deprecated.\n\n    @type version: L{twisted.python.versions.Version}\n    @param version: Version that the attribute was deprecated in\n\n    @type message: C{str}\n    @param message: Deprecation message\n\n    @type moduleName: C{str}\n    @param moduleName: Fully-qualified Python name of the module containing\n        the deprecated attribute; if called from the same module as the\n        attributes are being deprecated in, using the C{__name__} global can\n        be helpful\n\n    @type name: C{str}\n    @param name: Attribute name to deprecate\n    \"\"\"\n    module = sys.modules[moduleName]\n    if not isinstance(module, _ModuleProxy):\n        module = _ModuleProxy(module)\n        sys.modules[moduleName] = module\n\n    _deprecateAttribute(module, name, version, message)",
  "def warnAboutFunction(offender, warningString):\n    \"\"\"\n    Issue a warning string, identifying C{offender} as the responsible code.\n\n    This function is used to deprecate some behavior of a function.  It differs\n    from L{warnings.warn} in that it is not limited to deprecating the behavior\n    of a function currently on the call stack.\n\n    @param function: The function that is being deprecated.\n\n    @param warningString: The string that should be emitted by this warning.\n    @type warningString: C{str}\n\n    @since: 11.0\n    \"\"\"\n    # inspect.getmodule() is attractive, but somewhat\n    # broken in Python < 2.6.  See Python bug 4845.\n    offenderModule = sys.modules[offender.__module__]\n    filename = inspect.getabsfile(offenderModule)\n    lineStarts = list(findlinestarts(offender.func_code))\n    lastLineNo = lineStarts[-1][1]\n    globals = offender.func_globals\n\n    kwargs = dict(\n        category=DeprecationWarning,\n        filename=filename,\n        lineno=lastLineNo,\n        module=offenderModule.__name__,\n        registry=globals.setdefault(\"__warningregistry__\", {}),\n        module_globals=None)\n\n    if sys.version_info[:2] < (2, 5):\n        kwargs.pop('module_globals')\n\n    warn_explicit(warningString, **kwargs)",
  "def deprecationDecorator(function):\n        \"\"\"\n        Decorator that marks C{function} as deprecated.\n        \"\"\"\n        warningString = getDeprecationWarningString(\n            function, version, None, replacement)\n\n        def deprecatedFunction(*args, **kwargs):\n            warn(\n                warningString,\n                DeprecationWarning,\n                stacklevel=2)\n            return function(*args, **kwargs)\n\n        deprecatedFunction = mergeFunctionMetadata(\n            function, deprecatedFunction)\n        _appendToDocstring(deprecatedFunction,\n                           _getDeprecationDocstring(version, replacement))\n        deprecatedFunction.deprecatedVersion = version\n        return deprecatedFunction",
  "def __init__(self, module):\n        object.__setattr__(self, '_module', module)\n        object.__setattr__(self, '_deprecatedAttributes', {})",
  "def __repr__(self):\n        \"\"\"\n        Get a string containing the type of the module proxy and a\n        representation of the wrapped module object.\n        \"\"\"\n        _module = object.__getattribute__(self, '_module')\n        return '<%s module=%r>' % (\n            type(self).__name__,\n            _module)",
  "def __setattr__(self, name, value):\n        \"\"\"\n        Set an attribute on the wrapped module object.\n        \"\"\"\n        _module = object.__getattribute__(self, '_module')\n        setattr(_module, name, value)",
  "def __getattribute__(self, name):\n        \"\"\"\n        Get an attribute on the wrapped module object.\n\n        If the specified name has been deprecated then a warning is issued.\n        \"\"\"\n        _module = object.__getattribute__(self, '_module')\n        _deprecatedAttributes = object.__getattribute__(\n            self, '_deprecatedAttributes')\n\n        getter = _deprecatedAttributes.get(name)\n        if getter is not None:\n            value = getter.get()\n        else:\n            value = getattr(_module, name)\n        return value",
  "def __init__(self, module, name, version, message):\n        \"\"\"\n        Initialise a deprecated name wrapper.\n        \"\"\"\n        self.module = module\n        self.__name__ = name\n        self.fqpn = module.__name__ + '.' + name\n        self.version = version\n        self.message = message",
  "def get(self):\n        \"\"\"\n        Get the underlying attribute value and issue a deprecation warning.\n        \"\"\"\n        # This might fail if the deprecated thing is a module inside a package.\n        # In that case, don't emit the warning this time.  The import system\n        # will come back again when it's not an AttributeError and we can emit\n        # the warning then.\n        result = getattr(self.module, self.__name__)\n        message = _getDeprecationWarningString(self.fqpn, self.version,\n            DEPRECATION_WARNING_FORMAT + ': ' + self.message)\n        warn(message, DeprecationWarning, stacklevel=3)\n        return result",
  "def deprecatedFunction(*args, **kwargs):\n            warn(\n                warningString,\n                DeprecationWarning,\n                stacklevel=2)\n            return function(*args, **kwargs)",
  "class Settings(object):\n    _prefix = \"BOKEH_\"\n    debugjs = False\n\n    @property\n    def _environ(self):\n        return os.environ\n\n    def _get(self, key, default=None):\n        return self._environ.get(self._prefix + key, default)\n\n    @property\n    def _is_dev(self):\n        return self._get_bool(\"DEV\", False)\n\n    def _dev_or_default(self, default, dev):\n        return dev if dev is not None and self._is_dev else default\n\n    def _get_str(self, key, default, dev=None):\n        return self._get(key, self._dev_or_default(default, dev))\n\n    def _get_bool(self, key, default, dev=None):\n        value = self._get(key)\n\n        if value is None:\n            value = self._dev_or_default(default, dev)\n        elif value.lower() in [\"true\", \"yes\", \"on\", \"1\"]:\n            value = True\n        elif value.lower() in [\"false\", \"no\", \"off\", \"0\"]:\n            value = False\n        else:\n            raise ValueError(\"invalid value %r for boolean property %s%s\" % (value, self._prefix, key))\n\n        return value\n\n    def browser(self, default=None):\n        return self._get_str(\"BROWSER\", default, \"none\")\n\n    def resources(self, default=None):\n        return self._get_str(\"RESOURCES\", default, \"absolute-dev\")\n\n    def rootdir(self, default=None):\n        return self._get_str(\"ROOTDIR\", default)\n\n    def version(self, default=None):\n        return self._get_str(\"VERSION\", default)\n\n    def local_docs_cdn(self, default=None):\n        return self._get_str(\"LOCAL_DOCS_CDN\", default)\n\n    def minified(self, default=None):\n        return self._get_bool(\"MINIFIED\", default, False)\n\n    def log_level(self, default=None):\n        return self._get_str(\"LOG_LEVEL\", default, \"debug\")\n\n    def py_log_level(self, default='info'):\n        level = self._get_str(\"PY_LOG_LEVEL\", default, \"debug\")\n        LEVELS = {'debug': logging.DEBUG,\n                  'info' : logging.INFO,\n                  'warn' : logging.WARNING,\n                  'error': logging.ERROR,\n                  'fatal': logging.CRITICAL}\n        return LEVELS[level]\n\n    def pretty(self, default=None):\n        return self._get_bool(\"PRETTY\", default, True)\n\n    def simple_ids(self, default=None):\n        return self._get_bool(\"SIMPLE_IDS\", default, True)\n\n    def notebook_resources(self, default=None):\n        return self._get_str(\"NOTEBOOK_RESOURCES\", default)\n\n    def notebook_verbose(self, default=None):\n        return self._get_bool(\"NOTEBOOK_VERBOSE\", default)\n\n    def notebook_hide_banner(self, default=None):\n        return self._get_bool(\"NOTEBOOK_HIDE_BANNER\", default)\n\n    def notebook_skip_load(self, default=None):\n        return self._get_bool(\"NOTEBOOK_SKIP_LOAD\", default)\n\n    \"\"\"\n    Server settings go here:\n    \"\"\"\n    def serverdir(self):\n        return join(dirname(abspath(__file__)), 'server')\n\n    def bokehjssrcdir(self):\n        if self.debugjs:\n            basedir = dirname(dirname(self.serverdir()))\n            bokehjsdir = join(basedir, 'bokehjs', 'src')\n\n            if exists(bokehjsdir):\n                return bokehjsdir\n\n        return None\n\n    def bokehjsdir(self):\n        if self.debugjs:\n            basedir = dirname(dirname(self.serverdir()))\n            bokehjsdir = join(basedir, 'bokehjs', 'build')\n\n            if exists(bokehjsdir):\n                return bokehjsdir\n\n        return join(self.serverdir(), 'static')\n\n    def js_files(self):\n        bokehjsdir = self.bokehjsdir()\n        js_files = []\n        for root, dirnames, files in os.walk(bokehjsdir):\n            for fname in files:\n                if fname.endswith(\".js\") and 'vendor' not in root:\n                    js_files.append(join(root, fname))\n        return js_files\n\n    def css_files(self):\n        bokehjsdir = self.bokehjsdir()\n        js_files = []\n        for root, dirnames, files in os.walk(bokehjsdir):\n            for fname in files:\n                if fname.endswith(\".css\") and 'vendor' not in root:\n                    js_files.append(join(root, fname))\n        return js_files",
  "def _environ(self):\n        return os.environ",
  "def _get(self, key, default=None):\n        return self._environ.get(self._prefix + key, default)",
  "def _is_dev(self):\n        return self._get_bool(\"DEV\", False)",
  "def _dev_or_default(self, default, dev):\n        return dev if dev is not None and self._is_dev else default",
  "def _get_str(self, key, default, dev=None):\n        return self._get(key, self._dev_or_default(default, dev))",
  "def _get_bool(self, key, default, dev=None):\n        value = self._get(key)\n\n        if value is None:\n            value = self._dev_or_default(default, dev)\n        elif value.lower() in [\"true\", \"yes\", \"on\", \"1\"]:\n            value = True\n        elif value.lower() in [\"false\", \"no\", \"off\", \"0\"]:\n            value = False\n        else:\n            raise ValueError(\"invalid value %r for boolean property %s%s\" % (value, self._prefix, key))\n\n        return value",
  "def browser(self, default=None):\n        return self._get_str(\"BROWSER\", default, \"none\")",
  "def resources(self, default=None):\n        return self._get_str(\"RESOURCES\", default, \"absolute-dev\")",
  "def rootdir(self, default=None):\n        return self._get_str(\"ROOTDIR\", default)",
  "def version(self, default=None):\n        return self._get_str(\"VERSION\", default)",
  "def local_docs_cdn(self, default=None):\n        return self._get_str(\"LOCAL_DOCS_CDN\", default)",
  "def minified(self, default=None):\n        return self._get_bool(\"MINIFIED\", default, False)",
  "def log_level(self, default=None):\n        return self._get_str(\"LOG_LEVEL\", default, \"debug\")",
  "def py_log_level(self, default='info'):\n        level = self._get_str(\"PY_LOG_LEVEL\", default, \"debug\")\n        LEVELS = {'debug': logging.DEBUG,\n                  'info' : logging.INFO,\n                  'warn' : logging.WARNING,\n                  'error': logging.ERROR,\n                  'fatal': logging.CRITICAL}\n        return LEVELS[level]",
  "def pretty(self, default=None):\n        return self._get_bool(\"PRETTY\", default, True)",
  "def simple_ids(self, default=None):\n        return self._get_bool(\"SIMPLE_IDS\", default, True)",
  "def notebook_resources(self, default=None):\n        return self._get_str(\"NOTEBOOK_RESOURCES\", default)",
  "def notebook_verbose(self, default=None):\n        return self._get_bool(\"NOTEBOOK_VERBOSE\", default)",
  "def notebook_hide_banner(self, default=None):\n        return self._get_bool(\"NOTEBOOK_HIDE_BANNER\", default)",
  "def notebook_skip_load(self, default=None):\n        return self._get_bool(\"NOTEBOOK_SKIP_LOAD\", default)",
  "def serverdir(self):\n        return join(dirname(abspath(__file__)), 'server')",
  "def bokehjssrcdir(self):\n        if self.debugjs:\n            basedir = dirname(dirname(self.serverdir()))\n            bokehjsdir = join(basedir, 'bokehjs', 'src')\n\n            if exists(bokehjsdir):\n                return bokehjsdir\n\n        return None",
  "def bokehjsdir(self):\n        if self.debugjs:\n            basedir = dirname(dirname(self.serverdir()))\n            bokehjsdir = join(basedir, 'bokehjs', 'build')\n\n            if exists(bokehjsdir):\n                return bokehjsdir\n\n        return join(self.serverdir(), 'static')",
  "def js_files(self):\n        bokehjsdir = self.bokehjsdir()\n        js_files = []\n        for root, dirnames, files in os.walk(bokehjsdir):\n            for fname in files:\n                if fname.endswith(\".js\") and 'vendor' not in root:\n                    js_files.append(join(root, fname))\n        return js_files",
  "def css_files(self):\n        bokehjsdir = self.bokehjsdir()\n        js_files = []\n        for root, dirnames, files in os.walk(bokehjsdir):\n            for fname in files:\n                if fname.endswith(\".css\") and 'vendor' not in root:\n                    js_files.append(join(root, fname))\n        return js_files",
  "def run_command(args, cwd=None, verbose=False):\n    try:\n        # remember shell=False, so use git.cmd on windows, not just git\n        p = subprocess.Popen(args, stdout=subprocess.PIPE, cwd=cwd)\n    except EnvironmentError:\n        e = sys.exc_info()[1]\n        if verbose:\n            print(\"unable to run %s\" % args[0])\n            print(e)\n        return None\n    stdout = p.communicate()[0].strip()\n    if sys.version >= '3':\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(\"unable to run %s (error)\" % args[0])\n        return None\n    return stdout",
  "def get_expanded_variables(versionfile_source):\n    # the code embedded in _version.py can just fetch the value of these\n    # variables. When used from setup.py, we don't want to import\n    # _version.py, so we do it with a regexp instead. This function is not\n    # used from _version.py.\n    variables = {}\n    try:\n        for line in open(versionfile_source,\"r\").readlines():\n            if line.strip().startswith(\"git_refnames =\"):\n                mo = re.search(r'=\\s*\"(.*)\"', line)\n                if mo:\n                    variables[\"refnames\"] = mo.group(1)\n            if line.strip().startswith(\"git_full =\"):\n                mo = re.search(r'=\\s*\"(.*)\"', line)\n                if mo:\n                    variables[\"full\"] = mo.group(1)\n    except EnvironmentError:\n        pass\n    return variables",
  "def versions_from_expanded_variables(variables, tag_prefix, verbose=False):\n    refnames = variables[\"refnames\"].strip()\n    if refnames.startswith(\"$Format\"):\n        if verbose:\n            print(\"variables are unexpanded, not using\")\n        return {} # unexpanded, so not in an unpacked git-archive tarball\n    refs = set([r.strip() for r in refnames.strip(\"()\").split(\",\")])\n    for ref in list(refs):\n        if not re.search(r'\\d', ref):\n            if verbose:\n                print(\"discarding '%s', no digits\" % ref)\n            refs.discard(ref)\n            # Assume all version tags have a digit. git's %d expansion\n            # behaves like git log --decorate=short and strips out the\n            # refs/heads/ and refs/tags/ prefixes that would let us\n            # distinguish between branches and tags. By ignoring refnames\n            # without digits, we filter out many common branch names like\n            # \"release\" and \"stabilization\", as well as \"HEAD\" and \"master\".\n    if verbose:\n        print(\"remaining refs: %s\" % \",\".join(sorted(refs)))\n    for ref in sorted(refs):\n        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(\"picking %s\" % r)\n            return { \"version\": r,\n                     \"full\": variables[\"full\"].strip() }\n    # no suitable tags, so we use the full revision id\n    if verbose:\n        print(\"no suitable tags, using full revision id\")\n    return { \"version\": variables[\"full\"].strip(),\n             \"full\": variables[\"full\"].strip() }",
  "def versions_from_vcs(tag_prefix, versionfile_source, verbose=False):\n    # this runs 'git' from the root of the source tree. That either means\n    # someone ran a setup.py command (and this code is in versioneer.py, so\n    # IN_LONG_VERSION_PY=False, thus the containing directory is the root of\n    # the source tree), or someone ran a project-specific entry point (and\n    # this code is in _version.py, so IN_LONG_VERSION_PY=True, thus the\n    # containing directory is somewhere deeper in the source tree). This only\n    # gets called if the git-archive 'subst' variables were *not* expanded,\n    # and _version.py hasn't already been rewritten with a short version\n    # string, meaning we're inside a checked out source tree.\n\n    try:\n        here = os.path.realpath(__file__)\n    except NameError:\n        # some py2exe/bbfreeze/non-CPython implementations don't do __file__\n        return {} # not always correct\n\n    # versionfile_source is the relative path from the top of the source tree\n    # (where the .git directory might live) to this file. Invert this to find\n    # the root from __file__.\n    root = here\n    if IN_LONG_VERSION_PY:\n        for i in range(len(versionfile_source.split(\"/\"))):\n            root = os.path.dirname(root)\n    else:\n        root = os.path.dirname(here)\n    if not os.path.exists(os.path.join(root, \".git\")):\n        if verbose:\n            print(\"no .git in %s\" % root)\n        return {}\n\n    stdout = run_command([GIT, \"describe\", \"--tags\", \"--dirty\", \"--always\"],\n                         cwd=root)\n    if stdout is None:\n        return {}\n    if not stdout.startswith(tag_prefix):\n        if verbose:\n            print(\"tag '%s' doesn't start with prefix '%s'\" % (stdout, tag_prefix))\n        return {}\n    tag = stdout[len(tag_prefix):]\n    stdout = run_command([GIT, \"rev-parse\", \"HEAD\"], cwd=root)\n    if stdout is None:\n        return {}\n    full = stdout.strip()\n    if tag.endswith(\"-dirty\"):\n        full += \"-dirty\"\n    return {\"version\": tag, \"full\": full}",
  "def versions_from_parentdir(parentdir_prefix, versionfile_source, verbose=False):\n    if IN_LONG_VERSION_PY:\n        # We're running from _version.py. If it's from a source tree\n        # (execute-in-place), we can work upwards to find the root of the\n        # tree, and then check the parent directory for a version string. If\n        # it's in an installed application, there's no hope.\n        try:\n            here = os.path.realpath(__file__)\n        except NameError:\n            # py2exe/bbfreeze/non-CPython don't have __file__\n            return {} # without __file__, we have no hope\n        # versionfile_source is the relative path from the top of the source\n        # tree to _version.py. Invert this to find the root from __file__.\n        root = here\n        for i in range(len(versionfile_source.split(\"/\"))):\n            root = os.path.dirname(root)\n    else:\n        # we're running from versioneer.py, which means we're running from\n        # the setup.py in a source tree. sys.argv[0] is setup.py in the root.\n        here = os.path.realpath(sys.argv[0])\n        root = os.path.dirname(here)\n\n    # Source tarballs conventionally unpack into a directory that includes\n    # both the project name and a version string.\n    dirname = os.path.basename(root)\n    if not dirname.startswith(parentdir_prefix):\n        if verbose:\n            print(\"guessing rootdir is '%s', but '%s' doesn't start with prefix '%s'\" %\n                  (root, dirname, parentdir_prefix))\n        return None\n    return {\"version\": dirname[len(parentdir_prefix):], \"full\": \"\"}",
  "def get_versions(default={\"version\": \"unknown\", \"full\": \"\"}, verbose=False):\n    variables = { \"refnames\": git_refnames, \"full\": git_full }\n    ver = versions_from_expanded_variables(variables, tag_prefix, verbose)\n    if not ver:\n        ver = versions_from_vcs(tag_prefix, versionfile_source, verbose)\n    if not ver:\n        ver = versions_from_parentdir(parentdir_prefix, versionfile_source,\n                                      verbose)\n    if not ver:\n        ver = default\n    return ver",
  "class Color(object):\n\n    def copy(self):\n        raise NotImplementedError\n\n    def __repr__(self):\n        return self.to_css()\n\n    def to_css(self):\n        raise NotImplementedError\n\n    def to_rgb(self):\n        raise NotImplementedError\n\n    def to_hsl(self):\n        raise NotImplementedError\n\n    def from_rgb(self, value):\n        raise NotImplementedError\n\n    def from_hsl(self, value):\n        raise NotImplementedError\n\n    def lighten(self, amount):\n        hsl = self.to_hsl()\n        hsl.l = self.clamp(hsl.l + amount)\n        return self.from_hsl(hsl)\n\n    def darken(self, amount):\n        hsl = self.to_hsl()\n        hsl.l = self.clamp(hsl.l - amount)\n        return self.from_hsl(hsl)\n\n    @staticmethod\n    def clamp(value, maximum=None):\n        value = max(value, 0)\n\n        if maximum is not None:\n            return min(value, maximum)\n        else:\n            return value",
  "class RGB(Color):\n\n    def __init__(self, r, g, b, a=1.0):\n        self.r = r\n        self.g = g\n        self.b = b\n        self.a = a\n\n    def copy(self):\n        return RGB(self.r, self.g, self.b, self.a)\n\n    def to_css(self):\n        if self.a == 1.0:\n            return \"rgb(%d, %d, %d)\" % (self.r, self.g, self.b)\n        else:\n            return \"rgba(%d, %d, %d, %s)\" % (self.r, self.g, self.b, self.a)\n\n    def to_hex(self):\n        return \"#%02X%02X%02X\" % (self.r, self.g, self.b)\n\n    def to_rgb(self):\n        return self.copy()\n\n    def to_hsl(self):\n        h, l, s = colorsys.rgb_to_hls(float(self.r)/255, float(self.g)/255, float(self.b)/255)\n        return HSL(round(h*360), s, l, self.a)\n\n    def from_rgb(self, value):\n        return value.copy()\n\n    def from_hsl(self, value):\n        return value.to_rgb()",
  "class HSL(Color):\n\n    def __init__(self, h, s, l, a=1.0):\n        self.h = h\n        self.s = s\n        self.l = l\n        self.a = a\n\n    def copy(self):\n        return HSL(self.h, self.s, self.l, self.a)\n\n    def to_css(self):\n        if self.a == 1.0:\n            return \"hsl(%d, %s%%, %s%%)\" % (self.h, self.s*100, self.l*100)\n        else:\n            return \"hsla(%d, %s%%, %s%%, %s)\" % (self.h, self.s*100, self.l*100, self.a)\n\n    def to_rgb(self):\n        r, g, b = colorsys.hls_to_rgb(float(self.h)/360, self.l, self.s)\n        return RGB(round(r*255), round(g*255), round(b*255), self.a)\n\n    def to_hsl(self):\n        return self.copy()\n\n    def from_rgb(self, value):\n        return value.to_hsl()\n\n    def from_hsl(self, value):\n        return value.copy()",
  "class NamedColor(RGB):\n\n    def __init__(self, name, r, g, b):\n        if name not in __colors__:\n            __colors__.append(name)\n\n        self.name = name\n        super(NamedColor, self).__init__(r, g, b)\n\n    def to_css(self):\n        return self.name",
  "def copy(self):\n        raise NotImplementedError",
  "def __repr__(self):\n        return self.to_css()",
  "def to_css(self):\n        raise NotImplementedError",
  "def to_rgb(self):\n        raise NotImplementedError",
  "def to_hsl(self):\n        raise NotImplementedError",
  "def from_rgb(self, value):\n        raise NotImplementedError",
  "def from_hsl(self, value):\n        raise NotImplementedError",
  "def lighten(self, amount):\n        hsl = self.to_hsl()\n        hsl.l = self.clamp(hsl.l + amount)\n        return self.from_hsl(hsl)",
  "def darken(self, amount):\n        hsl = self.to_hsl()\n        hsl.l = self.clamp(hsl.l - amount)\n        return self.from_hsl(hsl)",
  "def clamp(value, maximum=None):\n        value = max(value, 0)\n\n        if maximum is not None:\n            return min(value, maximum)\n        else:\n            return value",
  "def __init__(self, r, g, b, a=1.0):\n        self.r = r\n        self.g = g\n        self.b = b\n        self.a = a",
  "def copy(self):\n        return RGB(self.r, self.g, self.b, self.a)",
  "def to_css(self):\n        if self.a == 1.0:\n            return \"rgb(%d, %d, %d)\" % (self.r, self.g, self.b)\n        else:\n            return \"rgba(%d, %d, %d, %s)\" % (self.r, self.g, self.b, self.a)",
  "def to_hex(self):\n        return \"#%02X%02X%02X\" % (self.r, self.g, self.b)",
  "def to_rgb(self):\n        return self.copy()",
  "def to_hsl(self):\n        h, l, s = colorsys.rgb_to_hls(float(self.r)/255, float(self.g)/255, float(self.b)/255)\n        return HSL(round(h*360), s, l, self.a)",
  "def from_rgb(self, value):\n        return value.copy()",
  "def from_hsl(self, value):\n        return value.to_rgb()",
  "def __init__(self, h, s, l, a=1.0):\n        self.h = h\n        self.s = s\n        self.l = l\n        self.a = a",
  "def copy(self):\n        return HSL(self.h, self.s, self.l, self.a)",
  "def to_css(self):\n        if self.a == 1.0:\n            return \"hsl(%d, %s%%, %s%%)\" % (self.h, self.s*100, self.l*100)\n        else:\n            return \"hsla(%d, %s%%, %s%%, %s)\" % (self.h, self.s*100, self.l*100, self.a)",
  "def to_rgb(self):\n        r, g, b = colorsys.hls_to_rgb(float(self.h)/360, self.l, self.s)\n        return RGB(round(r*255), round(g*255), round(b*255), self.a)",
  "def to_hsl(self):\n        return self.copy()",
  "def from_rgb(self, value):\n        return value.to_hsl()",
  "def from_hsl(self, value):\n        return value.copy()",
  "def __init__(self, name, r, g, b):\n        if name not in __colors__:\n            __colors__.append(name)\n\n        self.name = name\n        super(NamedColor, self).__init__(r, g, b)",
  "def to_css(self):\n        return self.name",
  "def load_notebook(resources=None, verbose=False, hide_banner=False):\n    ''' Prepare the IPython notebook for displaying Bokeh plots.\n\n    Args:\n        resources (Resource, optional) :\n            how and where to load BokehJS from\n\n        verbose (bool, optional) :\n            whether to report detailed settings (default: False)\n\n        hide_banner (bool, optional):\n            whether to hide the Bokeh banner (default: False)\n\n    Returns:\n        None\n\n    '''\n    global _notebook_loaded\n\n    from .resources import INLINE\n    from .templates import NOTEBOOK_LOAD, RESOURCES\n\n    if resources is None:\n        resources = INLINE\n\n    plot_resources = RESOURCES.render(\n        js_raw = resources.js_raw,\n        css_raw = resources.css_raw,\n        js_files = resources.js_files,\n        css_files = resources.css_files,\n    )\n\n    if resources.mode == 'inline':\n        js_info = 'inline'\n        css_info = 'inline'\n    else:\n        js_info = resources.js_files[0] if len(resources.js_files) == 1 else resources.js_files\n        css_info = resources.css_files[0] if len(resources.css_files) == 1 else resources.css_files\n\n    warnings = [\"Warning: \" + msg['text'] for msg in resources.messages if msg['type'] == 'warn']\n\n    if _notebook_loaded:\n        warnings.append('Warning: BokehJS previously loaded')\n\n    _notebook_loaded = resources\n\n    html = NOTEBOOK_LOAD.render(\n        plot_resources = plot_resources,\n        logo_url = resources.logo_url,\n        verbose = verbose,\n        js_info = js_info,\n        css_info = css_info,\n        bokeh_version = __version__,\n        warnings = warnings,\n        hide_banner = hide_banner,\n    )\n    utils.publish_display_data({'text/html': html})",
  "def _print_versions():\n    import platform as pt\n    message = \"\"\"\n   Bokeh version: %s\n  Python version: %s-%s\n        Platform: %s\n    \"\"\" % (__version__, pt.python_version(),\n           pt.python_implementation(), pt.platform())\n    return(message)",
  "def print_versions():\n    \"\"\" Print the versions for Bokeh and the current Python and OS.\n\n    Returns:\n        None\n\n    \"\"\"\n    print(_print_versions())",
  "def report_issue(number=None , owner=\"bokeh\", repo=\"bokeh\",\n                 versions=True, browser=True):\n    \"\"\" Open or add to a Github issue programmatically.\n\n    This interactive function will ask you for some minimal content\n    and submit a new Github issue, adding information about your\n    current environment.\n\n    You can also call this function with a specific issue number to\n    add a comment to an already open issue.\n\n    Args:\n        number (int, optional) :\n            Omit to create a new issue, otherwise supply to comment on an\n            already created issue. (default: None)\n\n        owner (str, optional) : owner username (default: \"bokeh\")\n\n        repo (str, optional) : repository name (default: \"bokeh\")\n\n        versions (bool, optional) :\n            Whether to print system information. If True, add the current\n            system info to the end of the issue description. (default: True)\n\n        browser (bool, optional) :\n            Whether to open a browser automatically. If True, open a browser\n            to the GitHub issue page (default: True)\n\n    Notes:\n        Setting the environment variables GHUSER (Github username) and\n        GHPASS (Github password) will supply those values automatically\n        and streamline the dialog. Additionally, this function can report\n        on any GitHub project by changing the default parameters.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    import requests\n    import json\n    import os\n    import webbrowser\n\n    from six.moves import input\n    from six.moves.urllib.parse import urljoin\n\n    print(\"This is the Bokeh reporting engine.\\n\\n\"\n          \"You will be guided to build a GitHub issue.\\n\")\n\n    if number is None:\n        title = input('Issue title: ')\n        body = input('Description: ')\n    else:\n        body = input('Write your comment here: ')\n\n    ghuser, ghpass = (os.environ.get(x) for x in [\"GHUSER\", \"GHPASS\"])\n\n    if ghuser is None:\n        ghuser = input('GitHub username: ')\n    else:\n        print(\"Found GHUSER, using for GitHub username\")\n\n    if ghpass is None:\n        ghpass = input('GitHub password: ')\n    else:\n        print(\"Found GHPASS, using for GitHub password\")\n\n    base = \"https://api.github.com\"\n    if number is None:\n        url = \"/\".join([\"repos\", owner, repo, \"issues\"])\n        if versions:\n            data = {\"title\": title, \"body\": body + \"\\nSystem information:\" + _print_versions()}\n        else:\n            data = {\"title\": title, \"body\": body}\n    else:\n        url = \"/\".join([\"repos\", owner, repo, \"issues\", str(number), \"comments\"])\n        if versions:\n            data = {\"body\": body + \"\\nSystem information:\" + _print_versions()}\n        else:\n            data = {\"body\": body}\n    issues_url = urljoin(base, url)\n\n    print(\"\\nPreview:\\n\")\n    print(\"Title: \", data[\"title\"])\n    print(\"Description:\\n\\n\")\n    print(data[\"body\"])\n    value = input('Submit (y/n)? ')\n    if value.lower() in [\"true\", \"yes\", \"y\", \"1\"]:\n        r = requests.post(issues_url,\n                          auth=(ghuser, ghpass),\n                          headers={'Content-Type': 'application/json'},\n                          data=json.dumps(data))\n        if r.status_code == 201:\n            g = requests.get(issues_url)\n            if number is None:\n                print(\"Issue successfully submitted.\")\n                if browser:\n                    webbrowser.open_new(g.json()[0].get(\"html_url\"))\n            else:\n                print(\"Comment successfully submitted.\")\n                g = requests.get(issues_url)\n                if browser:\n                    webbrowser.open_new(g.json()[-1].get(\"html_url\"))\n        else:\n            print(\"Something failed, please check your username and password.\")\n    else:\n        print(\"Issue not submitted.\")",
  "def test(verbosity=1, xunitfile=None, exit=False):\n    \"\"\" Run the full Bokeh test suite, and output the results of the tests\n    to sys.stdout.\n\n    This function uses nosetests to discover which tests to run, and will\n    run tests in any 'tests' subdirectory within the Bokeh module.\n\n    Args:\n        verbosity (int, optional) :\n            Acceptatable values are 0 (less verbose) to 2 (most verbose)\n\n        xunitfile (str, optional) :\n            Write xunit-style XML test results to a given filename. This\n            is useful for running tests on a CI server. (default: None)\n\n        exit (bool, optional) :\n            Whether to return or exit. If True, call sys.exit with an\n            error code after the tests are finished. (default: False)\n\n    Returns:\n        int : nose return code\n\n    \"\"\"\n    import nose, os, sys\n\n    argv = ['nosetests', '--verbosity=%d' % verbosity]\n\n    # Output an xunit file if requested\n    if xunitfile:\n        argv.extend(['--with-xunit', '--xunit-file=%s' % xunitfile])\n\n    # Set the logging level to warn\n    argv.extend(['--logging-level=WARN'])\n\n    # Add all 'tests' subdirectories to the options\n    rootdir = os.path.dirname(__file__)\n    for root, dirs, files in os.walk(rootdir):\n        if 'tests' in dirs:\n            testsdir = os.path.join(root, 'tests')\n            argv.append(testsdir)\n            print('Test dir: %s' % testsdir[len(rootdir)+1:])\n\n    # print versions (handy when reporting problems)\n    print_versions()\n    sys.stdout.flush()\n\n    # Run the tests\n    return nose.main(argv=argv, exit=exit)",
  "def _server_static_dir():\n    return join(abspath(split(__file__)[0]), \"server\", \"static\")",
  "def _static_path(path):\n    path = normpath(join(_server_static_dir(), path))\n    if sys.platform == 'cygwin': path = realpath(path)\n    return path",
  "def _cdn_base_url():\n    return \"http://cdn.pydata.org\"",
  "def _get_cdn_urls(version=None, minified=True):\n    if version is None:\n        if settings.local_docs_cdn():\n            version = settings.local_docs_cdn()\n        else:\n            version = __version__.split('-')[0]\n\n    # check the 'dev' fingerprint\n    dev = version.split('.')[-2]\n    # check if we want minified js and css\n    _min = \".min\" if minified else \"\"\n\n    base_url = _cdn_base_url()\n    dev_container = 'bokeh/dev'\n    rel_container = 'bokeh/release'\n    container = dev_container if dev.startswith(('dev', 'rc')) else rel_container\n\n    result = {\n        'js_files'  : ['%s/%s/bokeh-%s%s.js' % (base_url, container, version, _min)],\n        'css_files' : ['%s/%s/bokeh-%s%s.css' % (base_url, container, version, _min)],\n        'messages'  : [],\n    }\n\n    if len(__version__.split('-')) > 1:\n        result['messages'].append({\n            \"type\" : \"warn\",\n            \"text\" : \"Requesting CDN BokehJS version '%s' from Bokeh development version '%s'. This configuration is unsupported and may not work!\" % (version, __version__)\n        })\n\n    return result",
  "def _get_server_urls(root_url, minified=True):\n    _min = \".min\" if minified else \"\"\n    result = {\n        'js_files'  : ['%sbokehjs/static/js/bokeh%s.js' % (root_url, _min)],\n        'css_files' : ['%sbokehjs/static/css/bokeh%s.css' % (root_url, _min)],\n        'messages'  : [],\n    }\n    return result",
  "def _inline(paths):\n    strings = []\n    for path in paths:\n        begin = \"/* BEGIN %s */\" % path\n        middle = open(path, 'rb').read().decode(\"utf-8\")\n        end = \"/* END %s */\" % path\n        strings.append(begin + '\\n' + middle + '\\n' + end)\n    return strings",
  "def _file_paths(files, minified):\n    if minified:\n        files = [ root + \".min\" + ext for (root, ext) in map(splitext, files) ]\n    return [ _static_path(file) for file in files ]",
  "class Resources(object):\n    ''' The Resources class encapsulates information relating to loading or\n    embedding BokehJS code and CSS.\n\n    Args:\n        mode (str) : how should BokehJS be included in output\n\n            See below for descriptions of available modes\n\n        version (str, optional) : what version of BokejJS to load\n\n            Only valid with the ``'cdn'`` mode\n\n        root_dir (str, optional) : root directory for loading BokehJS resources\n\n            Only valid with ``'relative'`` and ``'relative-dev'`` modes\n\n        minified (bool, optional) : whether JavaScript and CSS should be minified or not (default: True)\n\n        root_url (str, optional) : URL and port of Bokeh Server to load resources from\n\n            Only valid with ``'server'`` and ``'server-dev'`` modes\n\n    The following **mode** values are available for configuring a Resource object:\n\n    * ``'inline'`` configure to provide entire BokehJS code and CSS inline\n    * ``'cdn'`` configure to load BokehJS code and CS from ``http://cdn.pydata.org``\n    * ``'server'`` configure to load from a Bokeh Server\n    * ``'server-dev'`` same as ``server`` but supports non-concatenated JS using ``requirejs``\n    * ``'relative'`` configure to load relative to the given directory\n    * ``'relative-dev'`` same as ``relative`` but supports non-concatenated JS using ``requirejs``\n    * ``'absolute'`` configure to load from the installed Bokeh library static directory\n    * ``'absolute-dev'`` same as ``absolute`` but supports non-concatenated JS using ``requirejs``\n\n    Once configured, a Resource object exposes the following public attributes:\n\n    Attributes:\n        logo_url : location of the BokehJS logo image\n        js_raw : any raw JS that needs to be placed inside ``<script>`` tags\n        css_raw : any raw CSS that needs to be places inside ``<style>`` tags\n        js_files : URLs of any JS files that need to be loaded by ``<script>`` tags\n        css_files : URLS od any CSS files that need to be loaed by ``<link>`` tags\n        messages : any informational messages concering this configuration\n\n    These attributes are often useful as template parameters when embedding\n    Bokeh plots.\n\n    '''\n\n    _default_js_files = [\"js/bokeh.js\"]\n    _default_css_files = [\"css/bokeh.css\"]\n\n    _default_js_files_dev = ['js/vendor/requirejs/require.js', 'js/config.js']\n    _default_css_files_dev = ['css/bokeh.css']\n\n    _default_root_dir = \".\"\n    _default_root_url = \"http://127.0.0.1:5006/\"\n\n    logo_url = \"http://bokeh.pydata.org/_static/bokeh-transparent.png\"\n\n    def __init__(self, mode='inline', version=None, root_dir=None,\n                 minified=True, log_level=\"info\", root_url=None):\n        self.mode = settings.resources(mode)\n        self.root_dir = settings.rootdir(root_dir)\n        self.version = settings.version(version)\n        self.minified = settings.minified(minified)\n        self.log_level = settings.log_level(log_level)\n        if root_url and not root_url.endswith(\"/\"):\n            logger.warning(\"root_url should end with a /, adding one\")\n            root_url = root_url + \"/\"\n        self._root_url = root_url\n        if mode not in ['inline', 'cdn', 'server', 'server-dev', 'relative', 'relative-dev', 'absolute', 'absolute-dev']:\n            raise ValueError(\"wrong value for 'mode' parameter, expected 'inline', 'cdn', 'server(-dev)', 'relative(-dev)' or 'absolute(-dev)', got %r\" % self.mode)\n\n        if self.root_dir and not mode.startswith(\"relative\"):\n            raise ValueError(\"setting 'root_dir' makes sense only when 'mode' is set to 'relative'\")\n\n        if self.version and not mode.startswith('cdn'):\n            raise ValueError(\"setting 'version' makes sense only when 'mode' is set to 'cdn'\")\n\n        if root_url and not mode.startswith('server'):\n            raise ValueError(\"setting 'root_url' makes sense only when 'mode' is set to 'server'\")\n\n        self.dev = self.mode.endswith('-dev')\n        if self.dev:\n            self.mode = self.mode[:-4]\n\n        js_paths = self._js_paths(dev=self.dev, minified=self.minified)\n        css_paths = self._css_paths(dev=self.dev, minified=self.minified)\n        base_url = _static_path(\"js\")\n\n        self._js_raw = []\n        self._css_raw = []\n        self.js_files = []\n        self.css_files = []\n        self.messages = []\n\n        if self.mode == \"inline\":\n            self._js_raw = lambda: _inline(js_paths)\n            self._css_raw = lambda: _inline(css_paths)\n        elif self.mode == \"relative\":\n            root_dir = self.root_dir or self._default_root_dir\n            self.js_files = [ relpath(p, root_dir) for p in js_paths ]\n            self.css_files = [ relpath(p, root_dir) for p in css_paths ]\n            base_url = relpath(base_url, root_dir)\n        elif self.mode == \"absolute\":\n            self.js_files = list(js_paths)\n            self.css_files = list(css_paths)\n        elif self.mode == \"cdn\":\n            cdn = _get_cdn_urls(self.version, self.minified)\n            self.js_files = list(cdn['js_files'])\n            self.css_files = list(cdn['css_files'])\n            self.messages.extend(cdn['messages'])\n        elif self.mode == \"server\":\n            server = _get_server_urls(self.root_url, self.minified)\n            self.js_files = list(server['js_files'])\n            self.css_files = list(server['css_files'])\n            self.messages.extend(server['messages'])\n\n        if self.dev:\n            require = 'require.config({ baseUrl: \"%s\" });' % base_url\n            self._js_raw.append(require)\n\n    @property\n    def log_level(self):\n        return self._log_level\n\n    @log_level.setter\n    def log_level(self, level):\n        valid_levels = [\n            \"trace\", \"debug\", \"info\", \"warn\", \"error\", \"fatal\"\n        ]\n        if level not in valid_levels:\n            raise ValueError(\"Unknown log level '%s', valid levels are: %s\", str(valid_levels))\n        self._log_level = level\n\n\n    @property\n    def js_raw(self):\n        if six.callable(self._js_raw):\n            self._js_raw = self._js_raw()\n        if self.dev:\n            return self._js_raw\n        else:\n            return self._js_raw + ['Bokeh.set_log_level(\"%s\");' % self.log_level]\n\n    @property\n    def css_raw(self):\n        if six.callable(self._css_raw):\n            self._css_raw = self._css_raw()\n        return self._css_raw\n\n    @property\n    def root_url(self):\n        if self._root_url:\n            return self._root_url\n        else:\n            return self._default_root_url\n\n    def _js_paths(self, minified=True, dev=False):\n        files = self._default_js_files_dev if self.dev else self._default_js_files\n        return _file_paths(files, False if dev else minified)\n\n    def _css_paths(self, minified=True, dev=False):\n        files = self._default_css_files_dev if self.dev else self._default_css_files\n        return _file_paths(files, False if dev else minified)\n\n    @property\n    def js_wrapper(self):\n\n        def pad(text, n=4):\n            return \"\\n\".join([ \" \"*n + line for line in text.split(\"\\n\") ])\n\n        wrapper = lambda code: 'Bokeh.$(function() {\\n%s\\n});' % pad(code)\n\n        if self.dev:\n            js_wrapper = lambda code: 'require([\"jquery\", \"main\"], function($, Bokeh) {\\nBokeh.set_log_level(\"%s\");\\n%s\\n});' % (self.log_level, pad(wrapper(code)))\n        else:\n            js_wrapper = wrapper\n\n        return js_wrapper\n\n    def _autoload_path(self, elementid):\n        return self.root_url + \"bokeh/autoload.js/%s\" % elementid",
  "def __init__(self, mode='inline', version=None, root_dir=None,\n                 minified=True, log_level=\"info\", root_url=None):\n        self.mode = settings.resources(mode)\n        self.root_dir = settings.rootdir(root_dir)\n        self.version = settings.version(version)\n        self.minified = settings.minified(minified)\n        self.log_level = settings.log_level(log_level)\n        if root_url and not root_url.endswith(\"/\"):\n            logger.warning(\"root_url should end with a /, adding one\")\n            root_url = root_url + \"/\"\n        self._root_url = root_url\n        if mode not in ['inline', 'cdn', 'server', 'server-dev', 'relative', 'relative-dev', 'absolute', 'absolute-dev']:\n            raise ValueError(\"wrong value for 'mode' parameter, expected 'inline', 'cdn', 'server(-dev)', 'relative(-dev)' or 'absolute(-dev)', got %r\" % self.mode)\n\n        if self.root_dir and not mode.startswith(\"relative\"):\n            raise ValueError(\"setting 'root_dir' makes sense only when 'mode' is set to 'relative'\")\n\n        if self.version and not mode.startswith('cdn'):\n            raise ValueError(\"setting 'version' makes sense only when 'mode' is set to 'cdn'\")\n\n        if root_url and not mode.startswith('server'):\n            raise ValueError(\"setting 'root_url' makes sense only when 'mode' is set to 'server'\")\n\n        self.dev = self.mode.endswith('-dev')\n        if self.dev:\n            self.mode = self.mode[:-4]\n\n        js_paths = self._js_paths(dev=self.dev, minified=self.minified)\n        css_paths = self._css_paths(dev=self.dev, minified=self.minified)\n        base_url = _static_path(\"js\")\n\n        self._js_raw = []\n        self._css_raw = []\n        self.js_files = []\n        self.css_files = []\n        self.messages = []\n\n        if self.mode == \"inline\":\n            self._js_raw = lambda: _inline(js_paths)\n            self._css_raw = lambda: _inline(css_paths)\n        elif self.mode == \"relative\":\n            root_dir = self.root_dir or self._default_root_dir\n            self.js_files = [ relpath(p, root_dir) for p in js_paths ]\n            self.css_files = [ relpath(p, root_dir) for p in css_paths ]\n            base_url = relpath(base_url, root_dir)\n        elif self.mode == \"absolute\":\n            self.js_files = list(js_paths)\n            self.css_files = list(css_paths)\n        elif self.mode == \"cdn\":\n            cdn = _get_cdn_urls(self.version, self.minified)\n            self.js_files = list(cdn['js_files'])\n            self.css_files = list(cdn['css_files'])\n            self.messages.extend(cdn['messages'])\n        elif self.mode == \"server\":\n            server = _get_server_urls(self.root_url, self.minified)\n            self.js_files = list(server['js_files'])\n            self.css_files = list(server['css_files'])\n            self.messages.extend(server['messages'])\n\n        if self.dev:\n            require = 'require.config({ baseUrl: \"%s\" });' % base_url\n            self._js_raw.append(require)",
  "def log_level(self):\n        return self._log_level",
  "def log_level(self, level):\n        valid_levels = [\n            \"trace\", \"debug\", \"info\", \"warn\", \"error\", \"fatal\"\n        ]\n        if level not in valid_levels:\n            raise ValueError(\"Unknown log level '%s', valid levels are: %s\", str(valid_levels))\n        self._log_level = level",
  "def js_raw(self):\n        if six.callable(self._js_raw):\n            self._js_raw = self._js_raw()\n        if self.dev:\n            return self._js_raw\n        else:\n            return self._js_raw + ['Bokeh.set_log_level(\"%s\");' % self.log_level]",
  "def css_raw(self):\n        if six.callable(self._css_raw):\n            self._css_raw = self._css_raw()\n        return self._css_raw",
  "def root_url(self):\n        if self._root_url:\n            return self._root_url\n        else:\n            return self._default_root_url",
  "def _js_paths(self, minified=True, dev=False):\n        files = self._default_js_files_dev if self.dev else self._default_js_files\n        return _file_paths(files, False if dev else minified)",
  "def _css_paths(self, minified=True, dev=False):\n        files = self._default_css_files_dev if self.dev else self._default_css_files\n        return _file_paths(files, False if dev else minified)",
  "def js_wrapper(self):\n\n        def pad(text, n=4):\n            return \"\\n\".join([ \" \"*n + line for line in text.split(\"\\n\") ])\n\n        wrapper = lambda code: 'Bokeh.$(function() {\\n%s\\n});' % pad(code)\n\n        if self.dev:\n            js_wrapper = lambda code: 'require([\"jquery\", \"main\"], function($, Bokeh) {\\nBokeh.set_log_level(\"%s\");\\n%s\\n});' % (self.log_level, pad(wrapper(code)))\n        else:\n            js_wrapper = wrapper\n\n        return js_wrapper",
  "def _autoload_path(self, elementid):\n        return self.root_url + \"bokeh/autoload.js/%s\" % elementid",
  "def pad(text, n=4):\n            return \"\\n\".join([ \" \"*n + line for line in text.split(\"\\n\") ])",
  "class Viewable(MetaHasProps):\n    \"\"\" Any plot object (Data Model) which has its own View Model in the\n    persistence layer.\n\n    One thing to keep in mind is that a Viewable should have a single\n    unique representation in the persistence layer, but it might have\n    multiple concurrent client-side Views looking at it.  Those may\n    be from different machines altogether.\n    \"\"\"\n\n    # Stores a mapping from subclass __view_model__ names to classes\n    model_class_reverse_map = {}\n\n    # Mmmm.. metaclass inheritance.  On the one hand, it seems a little\n    # overkill. On the other hand, this is exactly the sort of thing\n    # it's meant for.\n    def __new__(cls, class_name, bases, class_dict):\n        if \"__view_model__\" not in class_dict:\n            class_dict[\"__view_model__\"] = class_name\n        class_dict[\"get_class\"] = Viewable.get_class\n\n        # Create the new class\n        newcls = super(Viewable,cls).__new__(cls, class_name, bases, class_dict)\n        entry = class_dict.get(\"__subtype__\", class_dict[\"__view_model__\"])\n        # Add it to the reverse map, but check for duplicates first\n        if entry in Viewable.model_class_reverse_map:\n            raise Warning(\"Duplicate __view_model__ or __subtype__ declaration of '%s' for \" \\\n                          \"class %s.  Previous definition: %s\" % \\\n                          (entry, class_name,\n                           Viewable.model_class_reverse_map[entry]))\n        Viewable.model_class_reverse_map[entry] = newcls\n        return newcls\n\n    @classmethod\n    def _preload_models(cls):\n        from . import models\n        from .crossfilter import models\n        from .charts import Chart\n\n    @classmethod\n    def get_class(cls, view_model_name):\n        \"\"\" Given a __view_model__ name, returns the corresponding class\n        object\n        \"\"\"\n        cls._preload_models()\n        d = Viewable.model_class_reverse_map\n        if view_model_name in d:\n            return d[view_model_name]\n        else:\n            raise KeyError(\"View model name '%s' not found\" % view_model_name)",
  "class PlotObject(HasProps):\n    \"\"\" Base class for all plot-related objects \"\"\"\n\n    session = Instance(\".session.Session\")\n    name = String()\n    tags = List(Any)\n\n    def __init__(self, **kwargs):\n        # Eventually should use our own memo instead of storing\n        # an attribute on the class\n        if \"id\" in kwargs:\n            self._id = kwargs.pop(\"id\")\n        else:\n            self._id = make_id()\n\n        self._dirty = True\n        self._callbacks_dirty = False\n        self._callbacks = {}\n        self._callback_queue = []\n        self._block_callbacks = False\n\n        block_events = kwargs.pop('_block_events', False)\n\n        if not block_events:\n            super(PlotObject, self).__init__(**kwargs)\n            self.setup_events()\n        else:\n            self._block_callbacks = True\n            super(PlotObject, self).__init__(**kwargs)\n\n    @property\n    def ref(self):\n\n        if \"__subtype__\" in self.__class__.__dict__:\n            return {\n                'type': self.__view_model__,\n                'subtype': self.__subtype__,\n                'id': self._id,\n            }\n        else:\n            return {\n                'type': self.__view_model__,\n                'id': self._id,\n            }\n\n    def setup_events(self):\n        pass\n\n    def select(self, selector):\n        ''' Query this object and all of its references for objects that\n        match the given selector.\n\n        Args:\n            selector (JSON-like) :\n\n        Returns:\n            seq[PlotObject]\n\n        '''\n        return find(self.references(), selector)\n\n    def set_select(self, selector, updates):\n        ''' Update objects that match a given selector with the specified\n        attribute/value updates.\n\n        Args:\n            selector (JSON-like) :\n            updates (dict) :\n\n        Returns:\n            None\n\n        '''\n        for obj in self.select(selector):\n            for key, val in updates.items():\n                setattr(obj, key, val)\n\n    @classmethod\n    def load_json(cls, attrs, instance=None):\n        \"\"\"Loads all json into a instance of cls, EXCEPT any references\n        which are handled in finalize\n        \"\"\"\n        if 'id' not in attrs:\n            raise RuntimeError(\"Unable to find 'id' attribute in JSON: %r\" % attrs)\n        _id = attrs.pop('id')\n\n        if not instance:\n            instance = cls(id=_id, _block_events=True)\n\n        ref_props = {}\n        for p in instance.properties_with_refs():\n            if p in attrs:\n                ref_props[p] = attrs.pop(p)\n        instance._ref_props = ref_props\n\n        instance.update(**attrs)\n        return instance\n\n    def layout(self, side, plot):\n        try:\n            return self in getattr(plot, side)\n        except:\n            return []\n\n    def finalize(self, models):\n        \"\"\"Convert any references into instances\n        models is a dict of id->model mappings\n        \"\"\"\n        attrs = {}\n\n        for name, json in iteritems(getattr(self, \"_ref_props\", {})):\n            prop = self.__class__.lookup(name)\n            attrs[name] = prop.from_json(json, models=models)\n\n        return attrs\n\n    @classmethod\n    def collect_plot_objects(cls, *input_objs):\n        \"\"\" Iterate over ``input_objs`` and descend through their structure\n        collecting all nested ``PlotObjects`` on the go. The resulting list\n        is duplicate-free based on objects' identifiers.\n        \"\"\"\n        ids = set([])\n        objs = []\n\n        def descend_props(obj):\n            for attr in obj.properties_with_refs():\n                descend(getattr(obj, attr))\n\n        def descend(obj):\n            if isinstance(obj, PlotObject):\n                if obj._id not in ids:\n                    ids.add(obj._id)\n                    descend_props(obj)\n                    objs.append(obj)\n            elif isinstance(obj, HasProps):\n                descend_props(obj)\n            elif isinstance(obj, (list, tuple)):\n                for item in obj:\n                    descend(item)\n            elif isinstance(obj, dict):\n                for key, value in iteritems(obj):\n                    descend(key); descend(value)\n\n        descend(input_objs)\n        return objs\n\n    def references(self):\n        \"\"\"Returns all ``PlotObjects`` that this object has references to. \"\"\"\n        return set(self.collect_plot_objects(self))\n\n    #---------------------------------------------------------------------\n    # View Model connection methods\n    #\n    # Whereas a rich client rendering framework can maintain view state\n    # alongside model state, we need an explicit send/receive protocol for\n    # communicating with a set of view models that reside on the front end.\n    # Many of the calls one would expect in a rich client map instead to\n    # batched updates on the M-VM-V approach.\n    #---------------------------------------------------------------------\n    def vm_props(self, changed_only=True):\n        \"\"\" Returns the ViewModel-related properties of this object.\n\n        Args:\n            changed_only (bool, optional) : whether to return only properties\n                that have had their values changed at some point (default: True)\n\n        \"\"\"\n        if changed_only:\n            props = self.changed_properties_with_values()\n        else:\n            props = self.properties_with_values()\n        props.pop(\"session\", None)\n\n        # XXX: For dataspecs, getattr() returns a meaningless value\n        # from serialization point of view. This should be handled in\n        # the properties module, but for now, fix serialized values here.\n        for attr, prop in iteritems(self.dataspecs_with_refs()):\n            if props.get(attr) is not None:\n                props[attr] = prop.to_dict(self)\n\n        return props\n\n    def vm_serialize(self, changed_only=True):\n        \"\"\" Returns a dictionary of the attributes of this object, in\n        a layout corresponding to what BokehJS expects at unmarshalling time.\n\n        Args:\n            changed_only (bool, optional) : whether to include only attributes\n                that have had their values changed at some point (default: True)\n\n        \"\"\"\n        attrs = self.vm_props(changed_only)\n        attrs['id'] = self._id\n        return attrs\n\n    def dump(self, docid=None, changed_only=True):\n        \"\"\"convert all references to json\n\n        Args:\n            changed_only (bool, optional) : whether to dump only attributes\n                that have had their values changed at some point (default: True)\n        \"\"\"\n        models = self.references()\n        return dump(models, docid=docid, changed_only=changed_only)\n\n    def update(self, **kwargs):\n        for k,v in kwargs.items():\n            setattr(self, k, v)\n\n    def __str__(self):\n        return \"%s, ViewModel:%s, ref _id: %s\" % (self.__class__.__name__,\n                self.__view_model__, getattr(self, \"_id\", None))\n\n    def on_change(self, attrname, obj, callbackname=None):\n        \"\"\"when attrname of self changes, call callbackname\n        on obj\n        \"\"\"\n        callbacks = self._callbacks.setdefault(attrname, [])\n        callback = dict(obj=obj, callbackname=callbackname)\n        if callback not in callbacks:\n            callbacks.append(callback)\n        self._callbacks_dirty = True\n\n    def _trigger(self, attrname, old, new):\n        \"\"\"attrname of self changed.  So call all callbacks\n        \"\"\"\n        callbacks = self._callbacks.get(attrname)\n        if callbacks:\n            for callback in callbacks:\n                obj = callback.get('obj')\n                callbackname = callback.get('callbackname')\n                fn = obj if callbackname is None else getattr(obj, callbackname)\n                fn(self, attrname, old, new)",
  "def __new__(cls, class_name, bases, class_dict):\n        if \"__view_model__\" not in class_dict:\n            class_dict[\"__view_model__\"] = class_name\n        class_dict[\"get_class\"] = Viewable.get_class\n\n        # Create the new class\n        newcls = super(Viewable,cls).__new__(cls, class_name, bases, class_dict)\n        entry = class_dict.get(\"__subtype__\", class_dict[\"__view_model__\"])\n        # Add it to the reverse map, but check for duplicates first\n        if entry in Viewable.model_class_reverse_map:\n            raise Warning(\"Duplicate __view_model__ or __subtype__ declaration of '%s' for \" \\\n                          \"class %s.  Previous definition: %s\" % \\\n                          (entry, class_name,\n                           Viewable.model_class_reverse_map[entry]))\n        Viewable.model_class_reverse_map[entry] = newcls\n        return newcls",
  "def _preload_models(cls):\n        from . import models\n        from .crossfilter import models\n        from .charts import Chart",
  "def get_class(cls, view_model_name):\n        \"\"\" Given a __view_model__ name, returns the corresponding class\n        object\n        \"\"\"\n        cls._preload_models()\n        d = Viewable.model_class_reverse_map\n        if view_model_name in d:\n            return d[view_model_name]\n        else:\n            raise KeyError(\"View model name '%s' not found\" % view_model_name)",
  "def __init__(self, **kwargs):\n        # Eventually should use our own memo instead of storing\n        # an attribute on the class\n        if \"id\" in kwargs:\n            self._id = kwargs.pop(\"id\")\n        else:\n            self._id = make_id()\n\n        self._dirty = True\n        self._callbacks_dirty = False\n        self._callbacks = {}\n        self._callback_queue = []\n        self._block_callbacks = False\n\n        block_events = kwargs.pop('_block_events', False)\n\n        if not block_events:\n            super(PlotObject, self).__init__(**kwargs)\n            self.setup_events()\n        else:\n            self._block_callbacks = True\n            super(PlotObject, self).__init__(**kwargs)",
  "def ref(self):\n\n        if \"__subtype__\" in self.__class__.__dict__:\n            return {\n                'type': self.__view_model__,\n                'subtype': self.__subtype__,\n                'id': self._id,\n            }\n        else:\n            return {\n                'type': self.__view_model__,\n                'id': self._id,\n            }",
  "def setup_events(self):\n        pass",
  "def select(self, selector):\n        ''' Query this object and all of its references for objects that\n        match the given selector.\n\n        Args:\n            selector (JSON-like) :\n\n        Returns:\n            seq[PlotObject]\n\n        '''\n        return find(self.references(), selector)",
  "def set_select(self, selector, updates):\n        ''' Update objects that match a given selector with the specified\n        attribute/value updates.\n\n        Args:\n            selector (JSON-like) :\n            updates (dict) :\n\n        Returns:\n            None\n\n        '''\n        for obj in self.select(selector):\n            for key, val in updates.items():\n                setattr(obj, key, val)",
  "def load_json(cls, attrs, instance=None):\n        \"\"\"Loads all json into a instance of cls, EXCEPT any references\n        which are handled in finalize\n        \"\"\"\n        if 'id' not in attrs:\n            raise RuntimeError(\"Unable to find 'id' attribute in JSON: %r\" % attrs)\n        _id = attrs.pop('id')\n\n        if not instance:\n            instance = cls(id=_id, _block_events=True)\n\n        ref_props = {}\n        for p in instance.properties_with_refs():\n            if p in attrs:\n                ref_props[p] = attrs.pop(p)\n        instance._ref_props = ref_props\n\n        instance.update(**attrs)\n        return instance",
  "def layout(self, side, plot):\n        try:\n            return self in getattr(plot, side)\n        except:\n            return []",
  "def finalize(self, models):\n        \"\"\"Convert any references into instances\n        models is a dict of id->model mappings\n        \"\"\"\n        attrs = {}\n\n        for name, json in iteritems(getattr(self, \"_ref_props\", {})):\n            prop = self.__class__.lookup(name)\n            attrs[name] = prop.from_json(json, models=models)\n\n        return attrs",
  "def collect_plot_objects(cls, *input_objs):\n        \"\"\" Iterate over ``input_objs`` and descend through their structure\n        collecting all nested ``PlotObjects`` on the go. The resulting list\n        is duplicate-free based on objects' identifiers.\n        \"\"\"\n        ids = set([])\n        objs = []\n\n        def descend_props(obj):\n            for attr in obj.properties_with_refs():\n                descend(getattr(obj, attr))\n\n        def descend(obj):\n            if isinstance(obj, PlotObject):\n                if obj._id not in ids:\n                    ids.add(obj._id)\n                    descend_props(obj)\n                    objs.append(obj)\n            elif isinstance(obj, HasProps):\n                descend_props(obj)\n            elif isinstance(obj, (list, tuple)):\n                for item in obj:\n                    descend(item)\n            elif isinstance(obj, dict):\n                for key, value in iteritems(obj):\n                    descend(key); descend(value)\n\n        descend(input_objs)\n        return objs",
  "def references(self):\n        \"\"\"Returns all ``PlotObjects`` that this object has references to. \"\"\"\n        return set(self.collect_plot_objects(self))",
  "def vm_props(self, changed_only=True):\n        \"\"\" Returns the ViewModel-related properties of this object.\n\n        Args:\n            changed_only (bool, optional) : whether to return only properties\n                that have had their values changed at some point (default: True)\n\n        \"\"\"\n        if changed_only:\n            props = self.changed_properties_with_values()\n        else:\n            props = self.properties_with_values()\n        props.pop(\"session\", None)\n\n        # XXX: For dataspecs, getattr() returns a meaningless value\n        # from serialization point of view. This should be handled in\n        # the properties module, but for now, fix serialized values here.\n        for attr, prop in iteritems(self.dataspecs_with_refs()):\n            if props.get(attr) is not None:\n                props[attr] = prop.to_dict(self)\n\n        return props",
  "def vm_serialize(self, changed_only=True):\n        \"\"\" Returns a dictionary of the attributes of this object, in\n        a layout corresponding to what BokehJS expects at unmarshalling time.\n\n        Args:\n            changed_only (bool, optional) : whether to include only attributes\n                that have had their values changed at some point (default: True)\n\n        \"\"\"\n        attrs = self.vm_props(changed_only)\n        attrs['id'] = self._id\n        return attrs",
  "def dump(self, docid=None, changed_only=True):\n        \"\"\"convert all references to json\n\n        Args:\n            changed_only (bool, optional) : whether to dump only attributes\n                that have had their values changed at some point (default: True)\n        \"\"\"\n        models = self.references()\n        return dump(models, docid=docid, changed_only=changed_only)",
  "def update(self, **kwargs):\n        for k,v in kwargs.items():\n            setattr(self, k, v)",
  "def __str__(self):\n        return \"%s, ViewModel:%s, ref _id: %s\" % (self.__class__.__name__,\n                self.__view_model__, getattr(self, \"_id\", None))",
  "def on_change(self, attrname, obj, callbackname=None):\n        \"\"\"when attrname of self changes, call callbackname\n        on obj\n        \"\"\"\n        callbacks = self._callbacks.setdefault(attrname, [])\n        callback = dict(obj=obj, callbackname=callbackname)\n        if callback not in callbacks:\n            callbacks.append(callback)\n        self._callbacks_dirty = True",
  "def _trigger(self, attrname, old, new):\n        \"\"\"attrname of self changed.  So call all callbacks\n        \"\"\"\n        callbacks = self._callbacks.get(attrname)\n        if callbacks:\n            for callback in callbacks:\n                obj = callback.get('obj')\n                callbackname = callback.get('callbackname')\n                fn = obj if callbackname is None else getattr(obj, callbackname)\n                fn(self, attrname, old, new)",
  "def descend_props(obj):\n            for attr in obj.properties_with_refs():\n                descend(getattr(obj, attr))",
  "def descend(obj):\n            if isinstance(obj, PlotObject):\n                if obj._id not in ids:\n                    ids.add(obj._id)\n                    descend_props(obj)\n                    objs.append(obj)\n            elif isinstance(obj, HasProps):\n                descend_props(obj)\n            elif isinstance(obj, (list, tuple)):\n                for item in obj:\n                    descend(item)\n            elif isinstance(obj, dict):\n                for key, value in iteritems(obj):\n                    descend(key); descend(value)",
  "class BokehJSONEncoder(json.JSONEncoder):\n    def transform_series(self, obj):\n        \"\"\"transform series\n        \"\"\"\n        vals = obj.values\n        return self.transform_array(vals)\n\n    # Check for astype failures (putative Numpy < 1.7)\n    dt2001 = np.datetime64('2001')\n    legacy_datetime64 = (dt2001.astype('int64') ==\n                         dt2001.astype('datetime64[ms]').astype('int64'))\n    def transform_array(self, obj):\n        \"\"\"Transform arrays into lists of json safe types\n        also handles pandas series, and replacing\n        nans and infs with strings\n        \"\"\"\n        ## not quite correct, truncates to ms..\n        if obj.dtype.kind == 'M':\n            if self.legacy_datetime64:\n                if obj.dtype == np.dtype('datetime64[ns]'):\n                    return (obj.astype('int64') / millifactor).tolist()\n                # else punt.\n            else:\n                return obj.astype('datetime64[ms]').astype('int64').tolist()\n        elif obj.dtype.kind in ('u', 'i', 'f'):\n            return self.transform_numerical_array(obj)\n        return obj.tolist()\n\n    def transform_numerical_array(self, obj):\n        \"\"\"handles nans/inf conversion\n        \"\"\"\n        if not np.isnan(obj).any() and not np.isinf(obj).any():\n            return obj.tolist()\n        else:\n            transformed = obj.astype('object')\n            transformed[np.isnan(obj)] = 'NaN'\n            transformed[np.isposinf(obj)] = 'Infinity'\n            transformed[np.isneginf(obj)] = '-Infinity'\n            return transformed.tolist()\n\n    def transform_python_types(self, obj):\n        \"\"\"handle special scalars, default to default json encoder\n        \"\"\"\n        # Pandas Timestamp\n        if is_pandas and isinstance(obj, pd.tslib.Timestamp):\n            return obj.value / millifactor  #nanosecond to millisecond\n        elif np.issubdtype(type(obj), np.float):\n            return float(obj)\n        elif np.issubdtype(type(obj), np.int):\n            return int(obj)\n        # Datetime, Date\n        elif isinstance(obj, (dt.datetime, dt.date)):\n            return calendar.timegm(obj.timetuple()) * 1000.\n        # Numpy datetime64\n        elif isinstance(obj, np.datetime64):\n            epoch_delta = obj - np.datetime64('1970-01-01T00:00:00Z')\n            return (epoch_delta / np.timedelta64(1, 'ms'))\n        # Time\n        elif isinstance(obj, dt.time):\n            return (obj.hour*3600 + obj.minute*60 + obj.second)*1000 + obj.microsecond / 1000.\n        elif is_dateutil and isinstance(obj, relativedelta):\n            return dict(years=obj.years, months=obj.months, days=obj.days, hours=obj.hours,\n                minutes=obj.minutes, seconds=obj.seconds, microseconds=obj.microseconds)\n        else:\n            return super(BokehJSONEncoder, self).default(obj)\n\n    def default(self, obj):\n        #argh! local import!\n        from .plot_object import PlotObject\n        from .properties import HasProps\n        from .colors import Color\n        ## array types\n        if is_pandas and isinstance(obj, (pd.Series, pd.Index)):\n            return self.transform_series(obj)\n        elif isinstance(obj, np.ndarray):\n            return self.transform_array(obj)\n        elif isinstance(obj, PlotObject):\n            return obj.ref\n        elif isinstance(obj, HasProps):\n            return obj.changed_properties_with_values()\n        elif isinstance(obj, Color):\n            return obj.to_css()\n        else:\n            return self.transform_python_types(obj)",
  "def serialize_json(obj, encoder=BokehJSONEncoder, **kwargs):\n    rslt =  json.dumps(obj, cls=encoder, **kwargs)\n    return rslt",
  "def status_obj(status):\n    return {'msgtype': 'status',\n            'status': status}",
  "def error_obj(error_msg):\n    return {\n        'msgtype': 'error',\n        'error_msg': error_msg}",
  "def transform_series(self, obj):\n        \"\"\"transform series\n        \"\"\"\n        vals = obj.values\n        return self.transform_array(vals)",
  "def transform_array(self, obj):\n        \"\"\"Transform arrays into lists of json safe types\n        also handles pandas series, and replacing\n        nans and infs with strings\n        \"\"\"\n        ## not quite correct, truncates to ms..\n        if obj.dtype.kind == 'M':\n            if self.legacy_datetime64:\n                if obj.dtype == np.dtype('datetime64[ns]'):\n                    return (obj.astype('int64') / millifactor).tolist()\n                # else punt.\n            else:\n                return obj.astype('datetime64[ms]').astype('int64').tolist()\n        elif obj.dtype.kind in ('u', 'i', 'f'):\n            return self.transform_numerical_array(obj)\n        return obj.tolist()",
  "def transform_numerical_array(self, obj):\n        \"\"\"handles nans/inf conversion\n        \"\"\"\n        if not np.isnan(obj).any() and not np.isinf(obj).any():\n            return obj.tolist()\n        else:\n            transformed = obj.astype('object')\n            transformed[np.isnan(obj)] = 'NaN'\n            transformed[np.isposinf(obj)] = 'Infinity'\n            transformed[np.isneginf(obj)] = '-Infinity'\n            return transformed.tolist()",
  "def transform_python_types(self, obj):\n        \"\"\"handle special scalars, default to default json encoder\n        \"\"\"\n        # Pandas Timestamp\n        if is_pandas and isinstance(obj, pd.tslib.Timestamp):\n            return obj.value / millifactor  #nanosecond to millisecond\n        elif np.issubdtype(type(obj), np.float):\n            return float(obj)\n        elif np.issubdtype(type(obj), np.int):\n            return int(obj)\n        # Datetime, Date\n        elif isinstance(obj, (dt.datetime, dt.date)):\n            return calendar.timegm(obj.timetuple()) * 1000.\n        # Numpy datetime64\n        elif isinstance(obj, np.datetime64):\n            epoch_delta = obj - np.datetime64('1970-01-01T00:00:00Z')\n            return (epoch_delta / np.timedelta64(1, 'ms'))\n        # Time\n        elif isinstance(obj, dt.time):\n            return (obj.hour*3600 + obj.minute*60 + obj.second)*1000 + obj.microsecond / 1000.\n        elif is_dateutil and isinstance(obj, relativedelta):\n            return dict(years=obj.years, months=obj.months, days=obj.days, hours=obj.hours,\n                minutes=obj.minutes, seconds=obj.seconds, microseconds=obj.microseconds)\n        else:\n            return super(BokehJSONEncoder, self).default(obj)",
  "def default(self, obj):\n        #argh! local import!\n        from .plot_object import PlotObject\n        from .properties import HasProps\n        from .colors import Color\n        ## array types\n        if is_pandas and isinstance(obj, (pd.Series, pd.Index)):\n            return self.transform_series(obj)\n        elif isinstance(obj, np.ndarray):\n            return self.transform_array(obj)\n        elif isinstance(obj, PlotObject):\n            return obj.ref\n        elif isinstance(obj, HasProps):\n            return obj.changed_properties_with_values()\n        elif isinstance(obj, Color):\n            return obj.to_css()\n        else:\n            return self.transform_python_types(obj)",
  "def components(plot_object, resources):\n    ''' Return HTML components to embed a Bokeh plot.\n\n    The data for the plot is stored directly in the returned HTML.\n\n    .. note:: The returned components assume that BokehJS resources\n              are **already loaded**.\n\n    Args:\n        plot_object (PlotObject) : Bokeh object to render\n            typically a Plot or PlotContext\n        resources (Resources, optional) : BokehJS resources config\n\n    Returns:\n        (script, div) : UTF-8 encoded\n\n    '''\n    ref = plot_object.ref\n    elementid = str(uuid.uuid4())\n\n    js = PLOT_JS.render(\n        elementid = elementid,\n        modelid = ref[\"id\"],\n        modeltype = ref[\"type\"],\n        all_models = serialize_json(plot_object.dump()),\n    )\n    script = PLOT_SCRIPT.render(\n        plot_js = resources.js_wrapper(js),\n    )\n    div = PLOT_DIV.render(elementid=elementid)\n\n    return encode_utf8(script), encode_utf8(div)",
  "def notebook_div(plot_object):\n    ''' Return HTML for a div that will display a Bokeh plot in an\n    IPython Notebook\n\n    The data for the plot is stored directly in the returned HTML.\n\n    Args:\n        plot_object (PlotObject) : Bokeh object to render\n            typically a Plot or PlotContext\n\n    Returns:\n        div : UTF-8 encoded HTML text\n\n    .. note:: Assumes ``bokeh.load_notebook()`` or the equivalent has\n              already been executed.\n\n    '''\n    ref = plot_object.ref\n    resources = Resources()\n    elementid = str(uuid.uuid4())\n\n    js = PLOT_JS.render(\n        elementid = elementid,\n        modelid = ref[\"id\"],\n        modeltype = ref[\"type\"],\n        all_models = serialize_json(plot_object.dump()),\n    )\n    script = PLOT_SCRIPT.render(\n        plot_js = resources.js_wrapper(js),\n    )\n    div = PLOT_DIV.render(elementid=elementid)\n    html = NOTEBOOK_DIV.render(\n        plot_script = script,\n        plot_div = div,\n    )\n    return encode_utf8(html)",
  "def file_html(plot_object, resources, title, template=FILE):\n    ''' Return an HTML document that embeds a Bokeh plot.\n\n    The data for the plot is stored directly in the returned HTML.\n\n    Args:\n        plot_object (PlotObject) : Bokeh object to render\n            typically a Plot or PlotContext\n        resources (Resources) : a resource configuration for BokehJS assets\n        title (str) : a title for the HTML document ``<title>`` tags\n        template (Template, optional) : HTML document template (default: FILE)\n            A Jinja2 Template, see bokeh.templates.FILE for the required\n            template parameters\n\n    Returns:\n        html : standalone HTML document with embedded plot\n\n    '''\n    plot_resources = RESOURCES.render(\n        js_raw = resources.js_raw,\n        css_raw = resources.css_raw,\n        js_files = resources.js_files,\n        css_files = resources.css_files,\n    )\n    script, div = components(plot_object, resources)\n    html = template.render(\n        title = title,\n        plot_resources = plot_resources,\n        plot_script = script,\n        plot_div = div,\n    )\n    return encode_utf8(html)",
  "def autoload_static(plot_object, resources, script_path):\n    ''' Return JavaScript code and a script tag that can be used to embed\n    Bokeh Plots.\n\n    The data for the plot is stored directly in the returned JavaScript code.\n\n    Args:\n        plot_object (PlotObject) :\n        resources (Resources) :\n        script_path (str) :\n\n    Returns:\n        (js, tag) :\n            JavaScript code to be saved at ``script_path`` and a ``<script>``\n            tag to load it\n\n    Raises:\n        ValueError\n\n    '''\n    if resources.mode == 'inline':\n        raise ValueError(\"autoload_static() requires non-inline resources\")\n\n    if resources.dev:\n        raise ValueError(\"autoload_static() only works with non-dev resources\")\n\n    elementid = str(uuid.uuid4())\n\n    js = AUTOLOAD.render(\n        all_models = serialize_json(plot_object.dump()),\n        js_url = resources.js_files[0],\n        css_files = resources.css_files,\n        elementid = elementid,\n    )\n\n    tag = AUTOLOAD_STATIC.render(\n        src_path = script_path,\n        elementid = elementid,\n        modelid = plot_object._id,\n        modeltype = plot_object.__view_model__,\n        loglevel = resources.log_level,\n    )\n\n    return encode_utf8(js), encode_utf8(tag)",
  "def autoload_server(plot_object, session):\n    ''' Return a script tag that can be used to embed Bokeh Plots from\n    a Bokeh Server.\n\n    The data for the plot is stored on the Bokeh Server.\n\n    Args:\n        plot_object (PlotObject) :\n        session (session) :\n\n    Returns:\n        tag :\n            a ``<script>`` tag that will execute an autoload script\n            loaded from the Bokeh Server\n\n    '''\n    elementid = str(uuid.uuid4())\n    resources = Resources(root_url=session.root_url, mode=\"server\")\n    tag = AUTOLOAD_SERVER.render(\n        src_path = resources._autoload_path(elementid),\n        elementid = elementid,\n        modelid = plot_object._id,\n        root_url = resources.root_url,\n        docid =  session.docid,\n        docapikey = session.apikey,\n        loglevel = resources.log_level,\n    )\n\n    return encode_utf8(tag)",
  "def autoload_server(plot_object, session, public=False):\n    ''' Return a script tag that can be used to embed Bokeh Plots from\n    a Bokeh Server.\n\n    The data for the plot is stored on the Bokeh Server.\n\n    Args:\n        plot_object (PlotObject) :\n        session (session) :\n\n    Returns:\n        tag :\n            a ``<script>`` tag that will execute an autoload script\n            loaded from the Bokeh Server\n\n    '''\n    elementid = str(uuid.uuid4())\n    resources = Resources(root_url=session.root_url, mode=\"server\")\n    tag = AUTOLOAD_SERVER.render(\n        src_path = resources._autoload_path(elementid),\n        elementid = elementid,\n        modelid = plot_object._id,\n        root_url = resources.root_url,\n        docid =  session.docid,\n        docapikey = session.apikey,\n        loglevel = resources.log_level,\n        public = public\n    )\n\n    return encode_utf8(tag)",
  "def fetch_daylight_hours(lat, lon, tz, dst, year):\n    \"\"\"Fetch daylight hours from sunrisesunset.com for a given location.\n\n       Parameters\n       ----------\n       lat  : float\n           Location's latitude.\n       lon  : float\n           Location's longitude.\n       tz   : int or float\n           Time zone offset from UTC. Use floats for half-hour time zones.\n       dst  : int\n           Daylight saving type, e.g. 0 -> none, 1 -> North America, 2 -> Europe.\n           See sunrisesunset.com/custom.asp for other possible values.\n       year : int\n           Year (1901..2099).\n    \"\"\"\n    daylight = []\n    summer = 0 if lat >= 0 else 1\n\n    for month in xrange(1, 12+1):\n        args = dict(url=url, lat=lat, lon=lon, tz=tz, dst=dst, year=year, month=month)\n        response = requests.get(\"%(url)s?comb_city_info=_;%(lon)s;%(lat)s;%(tz)s;%(dst)s&month=%(month)s&year=%(year)s&time_type=1&wadj=1\" % args)\n        entries = r1.findall(r0.sub(\"\", response.text))\n\n        for day, note, sunrise_hour, sunrise_minute, sunset_hour, sunset_minute in entries:\n            if note == \"DST Begins\":\n                summer = 1\n            elif note == \"DST Ends\":\n                summer = 0\n\n            date = datetime.date(year, month, int(day))\n            sunrise = datetime.time(int(sunrise_hour), int(sunrise_minute))\n            sunset = datetime.time(int(sunset_hour), int(sunset_minute))\n\n            daylight.append([date, sunrise, sunset, summer])\n\n    return pd.DataFrame(daylight, columns=[\"Date\", \"Sunrise\", \"Sunset\", \"Summer\"])",
  "def load_daylight_hours(file):\n    path = join(dirname(abspath(__file__)), file)\n    df = pd.read_csv(path, parse_dates=[\"Date\", \"Sunrise\", \"Sunset\"])\n\n    df[\"Date\"] = df.Date.map(lambda x: x.date())\n    df[\"Sunrise\"] = df.Sunrise.map(lambda x: x.time())\n    df[\"Sunset\"] = df.Sunset.map(lambda x: x.time())\n\n    return df",
  "def load_population():\n    csv_file = _data_dir(\"WPP2012_SA_DB03_POPULATION_QUINQUENNIAL.csv\")\n    df = pd.read_csv(csv_file, encoding=\"CP1250\")\n    df = df[df.Sex != \"Both\"]\n    df = df.drop([\"VarID\", \"Variant\", \"MidPeriod\", \"SexID\", \"AgeGrpSpan\"], axis=1)\n    df = df.rename(columns={\"Time\": \"Year\"})\n    df.Value *= 1000\n    return df",
  "def capitalize_words(string):\n    return \" \".join([ word.capitalize() for word in string.split(\" \") ])",
  "def _bokeh_dir(create=False):\n    bokeh_dir = expanduser(\"~/.bokeh\")\n    if not exists(bokeh_dir):\n        if not create: return bokeh_dir\n        print(\"Creating ~/.bokeh directory\")\n        try:\n            mkdir(bokeh_dir)\n        except OSError:\n            raise RuntimeError(\"could not create bokeh config directory at %s\" % bokeh_dir)\n    else:\n        if not isdir(bokeh_dir):\n            raise RuntimeError(\"%s exists but is not a directory\" % bokeh_dir)\n    return bokeh_dir",
  "def _data_dir(file_name=None, create=False):\n    try:\n        import yaml\n    except ImportError:\n        raise RuntimeError(\"'yaml' and 'pyyaml' are required to use bokeh.sampledata functions\")\n    bokeh_dir = _bokeh_dir(create=create)\n    data_dir = join(bokeh_dir, \"data\")\n    try:\n        config = yaml.load(open(join(bokeh_dir, 'config')))\n        data_dir = expanduser(config['sampledata_dir'])\n    except (IOError, TypeError):\n        pass\n    if not exists(data_dir):\n        if not create:\n            raise RuntimeError('bokeh sample data directory does not exist, please execute bokeh.sampledata.download()')\n        print(\"Creating %s directory\" % data_dir)\n        try:\n            mkdir(data_dir)\n        except OSError:\n            raise RuntimeError(\"could not create bokeh data directory at %s\" % data_dir)\n    else:\n        if not isdir(data_dir):\n            raise RuntimeError(\"%s exists but is not a directory\" % data_dir)\n    if file_name is not None:\n        return join(data_dir, file_name)\n    else:\n        return data_dir",
  "def download(progress=True):\n    '''\n    Download larger data sets for various Bokeh examples.\n    '''\n    data_dir = _data_dir(create=True)\n    print(\"Using data directory: %s\" % data_dir)\n\n    s3 = 'https://s3.amazonaws.com/bokeh_data/'\n    files = [\n        (s3, 'CGM.csv'),\n        (s3, 'US_Counties.zip'),\n        (s3, 'unemployment09.csv'),\n        (s3, 'AAPL.csv'),\n        (s3, 'FB.csv'),\n        (s3, 'GOOG.csv'),\n        (s3, 'IBM.csv'),\n        (s3, 'MSFT.csv'),\n        (s3, 'WPP2012_SA_DB03_POPULATION_QUINQUENNIAL.zip'),\n    ]\n\n    for base_url, file_name in files:\n        _getfile(base_url, file_name, data_dir, progress=progress)",
  "def _getfile(base_url, file_name, data_dir, progress=True):\n    file_url = join(base_url, file_name)\n    file_path = join(data_dir, file_name)\n\n    url = urlopen(file_url)\n\n    with open(file_path, 'wb') as file:\n        file_size = int(url.headers[\"Content-Length\"])\n        print(\"Downloading: %s (%d bytes)\" % (file_name, file_size))\n\n        fetch_size = 0\n        block_size = 16384\n\n        while True:\n            data = url.read(block_size)\n            if not data:\n                break\n\n            fetch_size += len(data)\n            file.write(data)\n\n            if progress:\n                status = \"\\r%10d [%6.2f%%]\" % (fetch_size, fetch_size*100.0/file_size)\n                stdout.write(status)\n                stdout.flush()\n\n    if progress:\n        print()\n\n    real_name, ext = splitext(file_name)\n\n    if ext == '.zip':\n        if not splitext(real_name)[1]:\n            real_name += \".csv\"\n\n        print(\"Unpacking: %s\" % real_name)\n\n        with ZipFile(file_path, 'r') as zip_file:\n            zip_file.extract(real_name, data_dir)\n\n        remove(file_path)",
  "def read_ical(name):\n    with open(join(dirname(__file__), name)) as ics:\n        ical = ICalendar.from_ical(ics.read())\n\n    return sorted([ (comp.get(\"dtstart\").dt, str(comp.get(\"summary\"))) for comp in ical.walk() if comp.name == \"VEVENT\" ])",
  "def _load_stock(filename):\n    data = {\n        'date' : [],\n        'open' : [],\n        'high' : [],\n        'low' : [],\n        'close' : [],\n        'volume' : [],\n        'adj_close': [],\n    }\n    with open(filename) as f:\n        next(f)\n        reader = csv.reader(f, delimiter=',')\n        for row in reader:\n            date, open_price, high, low, close, volume, adj_close = row\n            data['date'].append(date)\n            data['open'].append(float(open_price))\n            data['high'].append(float(high))\n            data['low'].append(float(low))\n            data['close'].append(float(close))\n            data['volume'].append(int(volume))\n            data['adj_close'].append(float(adj_close))\n    return data",
  "def crossdomain(origin=None, methods=None, headers=None,\n                max_age=21600, attach_to_all=True,\n                automatic_options=True):\n    if methods is not None:\n        methods = ', '.join(sorted(x.upper() for x in methods))\n\n    if headers is not None and not isinstance(headers, string_types):\n        headers = ', '.join(x.upper() for x in headers)\n\n    if not isinstance(origin, string_types):\n        origin = ', '.join(origin)\n\n    if isinstance(max_age, timedelta):\n        max_age = max_age.total_seconds()\n\n    def get_methods():\n        return methods\n        options_resp = current_app.make_default_options_response()\n        return options_resp.headers['allow']\n\n    def decorator(f):\n        @wraps(f)\n        def wrapped_function(*args, **kwargs):\n            if automatic_options and request.method == 'OPTIONS':\n                resp = current_app.make_default_options_response()\n            else:\n                resp = make_response(f(*args, **kwargs))\n            if not attach_to_all and request.method != 'OPTIONS':\n                return resp\n\n            h = resp.headers\n\n            h['Access-Control-Allow-Origin'] = origin\n            h['Access-Control-Allow-Methods'] = get_methods()\n            h['Access-Control-Max-Age'] = str(max_age)\n            requested_headers = request.headers.get(\n                'Access-Control-Request-Headers'\n            )\n            if headers is not None:\n                h['Access-Control-Allow-Headers'] = headers\n            elif requested_headers :\n                h['Access-Control-Allow-Headers'] = requested_headers\n            return resp\n        f.provide_automatic_options = False\n        return update_wrapper(wrapped_function, f)\n\n    return decorator",
  "def get_methods():\n        return methods\n        options_resp = current_app.make_default_options_response()\n        return options_resp.headers['allow']",
  "def decorator(f):\n        @wraps(f)\n        def wrapped_function(*args, **kwargs):\n            if automatic_options and request.method == 'OPTIONS':\n                resp = current_app.make_default_options_response()\n            else:\n                resp = make_response(f(*args, **kwargs))\n            if not attach_to_all and request.method != 'OPTIONS':\n                return resp\n\n            h = resp.headers\n\n            h['Access-Control-Allow-Origin'] = origin\n            h['Access-Control-Allow-Methods'] = get_methods()\n            h['Access-Control-Max-Age'] = str(max_age)\n            requested_headers = request.headers.get(\n                'Access-Control-Request-Headers'\n            )\n            if headers is not None:\n                h['Access-Control-Allow-Headers'] = headers\n            elif requested_headers :\n                h['Access-Control-Allow-Headers'] = requested_headers\n            return resp\n        f.provide_automatic_options = False\n        return update_wrapper(wrapped_function, f)",
  "def wrapped_function(*args, **kwargs):\n            if automatic_options and request.method == 'OPTIONS':\n                resp = current_app.make_default_options_response()\n            else:\n                resp = make_response(f(*args, **kwargs))\n            if not attach_to_all and request.method != 'OPTIONS':\n                return resp\n\n            h = resp.headers\n\n            h['Access-Control-Allow-Origin'] = origin\n            h['Access-Control-Allow-Methods'] = get_methods()\n            h['Access-Control-Max-Age'] = str(max_age)\n            requested_headers = request.headers.get(\n                'Access-Control-Request-Headers'\n            )\n            if headers is not None:\n                h['Access-Control-Allow-Headers'] = headers\n            elif requested_headers :\n                h['Access-Control-Allow-Headers'] = requested_headers\n            return resp",
  "class AbstractServerModelStorage(object):\n    \"\"\"Storage class for server side models (non backbone, that would be\n    document and user classes)\n    \"\"\"\n    def get(self, key):\n        \"\"\"given a key returns json objects\"\"\"\n        raise NotImplementedError\n\n    def set(self, key, val):\n        \"\"\"given a key and a json object, saves it\"\"\"\n        raise NotImplementedError\n\n    def create(self, key, val):\n        \"\"\"given a key and a json object, saves it\n        differs from set because this method should check\n        to make sure the object doesn't already exist\n        \"\"\"\n        raise NotImplementedError",
  "class RedisServerModelStorage(object):\n    def __init__(self, redisconn):\n        self.redisconn = redisconn\n\n    def get(self, key):\n        data = self.redisconn.get(key)\n        if data is None:\n            return None\n        attrs = json.loads(data.decode('utf-8'))\n        return attrs\n\n    def set(self, key, val):\n        self.redisconn.set(key, json.dumps(val))\n\n    def create(self, key, val):\n        with self.redisconn.pipeline() as pipe:\n            pipe.watch(key)\n            pipe.multi()\n            if self.redisconn.exists(key):\n                raise DataIntegrityException(\"%s already exists\" % key)\n            else:\n                pipe.set(key, json.dumps(val))\n            pipe.execute()",
  "class InMemoryServerModelStorage(object):\n    def __init__(self):\n        self._data = {}\n\n    def get(self, key):\n        data = self._data.get(key, None)\n        if data is None:\n            return None\n        attrs = json.loads(decode_utf8(data))\n        return attrs\n\n    def set(self, key, val):\n        self._data[key] = json.dumps(val)\n\n    def create(self, key, val):\n        if key in self._data:\n            raise DataIntegrityException(\"%s already exists\" % key)\n        self._data[key] = json.dumps(val)",
  "class ShelveServerModelStorage(object):\n\n    def get(self, key):\n        _data = shelve.open('bokeh.server')\n        key = encode_utf8(key)\n        data = _data.get(key, None)\n        if data is None:\n            return None\n        attrs = json.loads(decode_utf8(data))\n        _data.close()\n        return attrs\n\n    def set(self, key, val):\n        _data = shelve.open('bokeh.server')\n        key = encode_utf8(key)\n        _data[key] = json.dumps(val)\n        _data.close()\n\n    def create(self, key, val):\n        key = str(key)\n        _data = shelve.open('bokeh.server')\n        if key in _data:\n            raise DataIntegrityException(\"%s already exists\" % key)\n        _data[key] = json.dumps(val)\n        _data.close()",
  "class AbstractAuthentication(object):\n    def current_user_name(self):\n        \"\"\"obtain current user name from the current request\n        current request is obtained from flask request thread local\n        object\n        \"\"\"\n        raise NotImplementedError\n    def login(self, username):\n        \"\"\"login the user, sets whatever request information is necessary\n        (usually, session['username'] = username)\n        \"\"\"\n        raise NotImplementedError\n\n    def logout(self):\n        \"\"\"logs out the user, sets whatever request information is necessary\n        usually, session.pop('username')\n        \"\"\"\n        raise NotImplementedError\n\n    def current_user(self):\n        \"\"\"returns bokeh User object from self.current_user_name\n        \"\"\"\n        username = self.current_user_name()\n        if username is None:\n            return None\n        bokehuser = user.User.load(bokeh_app.servermodel_storage, username)\n        return bokehuser\n\n    def login_get(self):\n        \"\"\"custom login view\n        \"\"\"\n        raise NotImplementedError\n\n    def login_post(self):\n        \"\"\"custom login submission. Request form will have\n        username, password, and possibly an api field.\n        api indicates that we are\n        submitting via python, and we should try to return error\n        codes rather than flash messages\n        \"\"\"\n        raise NotImplementedError\n\n    def login_from_apikey(self):\n        \"\"\"login URL using apikey.  This is usually generated\n        by the python client\n        \"\"\"\n        raise NotImplementedError\n\n    def register_get(self):\n        \"\"\"custom register view\n        \"\"\"\n        raise NotImplementedError\n\n    def register_post(self):\n        \"\"\"custom register submission\n        request form will have username, password, password_confirm,\n        and possibly an api field. api indicates that we are\n        submitting via python, and we should try to return error\n        codes rather than flash messages\n        \"\"\"\n        raise NotImplementedError\n\n    def can_write_doc(self, docid):\n        \"\"\"whether or not a user can write to a doc\n        \"\"\"\n        raise NotImplementedError\n\n    def can_read_doc(self, docid):\n        \"\"\"whether or not a user can read a doc\n        \"\"\"\n        raise NotImplementedError",
  "class SingleUserAuthentication(AbstractAuthentication):\n    def can_write_doc(self, doc_or_docid, temporary_docid=None, userobj=None):\n        return True\n\n    def can_read_doc(self, doc_or_docid, temporary_docid=None, userobj=None):\n        return True\n\n    def current_user_name(self):\n        return \"defaultuser\"\n\n    def current_user(self):\n        \"\"\"returns bokeh User object matching defaultuser\n        if the user does not exist, one will be created\n        \"\"\"\n        username = self.current_user_name()\n        bokehuser = user.User.load(bokeh_app.servermodel_storage, username)\n        if bokehuser is not None:\n            return bokehuser\n        bokehuser = user.new_user(bokeh_app.servermodel_storage, \"defaultuser\",\n                                  str(uuid.uuid4()), apikey='nokey', docs=[])\n        return bokehuser",
  "class MultiUserAuthentication(AbstractAuthentication):\n    def can_write_doc(self, doc_or_docid, temporary_docid=None, userobj=None):\n        if not isinstance(doc_or_docid, docs.Doc):\n            doc = docs.Doc.load(bokeh_app.servermodel_storage, doc_or_docid)\n        else:\n            doc = doc_or_docid\n        if userobj is None:\n            userobj = self.current_user()\n        return convenience.can_write_from_request(doc, request, userobj,\n                                                  temporary_docid=temporary_docid)\n\n    def can_read_doc(self, doc_or_docid, temporary_docid=None, userobj=None):\n        if not isinstance(doc_or_docid, docs.Doc):\n            doc = docs.Doc.load(bokeh_app.servermodel_storage, doc_or_docid)\n        else:\n            doc = doc_or_docid\n        if userobj is None:\n            userobj = self.current_user()\n        return convenience.can_read_from_request(doc, request, userobj)\n\n\n    def login(self, username):\n        session['username'] = username\n\n    def print_connection_info(self, bokehuser):\n        logger.info(\"connect using the following\")\n        command = \"output_server(docname, username='%s', userapikey='%s')\"\n        command = command % (bokehuser.username, bokehuser.apikey)\n        logger.info(command)\n\n    def current_user_name(self):\n        # users can be authenticated by logging in (setting the session)\n        # or by setting fields in the http header (api keys, etc..)\n        username =  session.get('username', None)\n        if username:\n            return username\n        else:\n            # check for auth via apis and headers\n            bokehuser = user.apiuser_from_request(bokeh_app, request)\n            if bokehuser:\n                return bokehuser.username\n        return None\n\n    def register_get(self):\n        return render_template(\"register.html\", title=\"Register\")\n\n    def login_get(self):\n        return render_template(\"login.html\", title=\"Login\")\n\n    def register_post_api(self):\n        username = request.values['username']\n        password = request.values['password']\n        try:\n            bokehuser = user.new_user(\n                bokeh_app.servermodel_storage, username, password\n                )\n            self.login(username)\n            self.print_connection_info(bokehuser)\n        except UnauthorizedException:\n            return jsonify(status=False,\n                           error=\"user already exists\")\n        return jsonify(status=True,\n                       userapikey=bokehuser.apikey\n                       )\n\n    def register_post(self):\n        if request.values.get('api', None):\n            return self.register_post_api()\n        username = request.values['username']\n        password = request.values['password']\n        password_confirm = request.values['password_confirm']\n        if password != password_confirm:\n            flash(\"password and confirmation do not match\")\n            return redirect(url_for('.register_get'))\n        try:\n            bokehuser = user.new_user(\n                bokeh_app.servermodel_storage, username, password\n                )\n            self.login(username)\n            self.print_connection_info(bokehuser)\n        except UnauthorizedException:\n            flash(\"user already exists\")\n            return redirect(url_for('.register_get'))\n        return redirect(url_for(\".index\"))\n\n    def login_post_api(self):\n        username = request.values['username']\n        password = request.values['password']\n        try:\n            bokehuser = user.auth_user(bokeh_app.servermodel_storage,\n                                       username,\n                                       password)\n            self.login(username)\n            self.print_connection_info(bokehuser)\n        except UnauthorizedException:\n            return jsonify(status=False,\n                           error=\"incorrect login \")\n        return jsonify(status=True,\n                       userapikey=bokehuser.apikey\n                       )\n\n    def login_post(self):\n        if request.values.get('api', None):\n            return self.login_post_api()\n        username = request.values['username']\n        password = request.values['password']\n        try:\n            bokehuser = user.auth_user(bokeh_app.servermodel_storage,\n                                       username,\n                                       password=password)\n            self.login(username)\n            self.print_connection_info(bokehuser)\n        except UnauthorizedException:\n            flash(\"incorrect login exists\")\n            return redirect(url_for('.login_get'))\n        return redirect(url_for(\".index\"))\n\n    def login_from_apikey(self):\n        username = request.values.get('username')\n        apikey = request.values.get('userapikey')\n        try:\n            bokehuser = user.auth_user(bokeh_app.servermodel_storage,\n                                       username,\n                                       apikey=apikey)\n\n            self.login(username)\n            self.print_connection_info(bokehuser)\n        except UnauthorizedException:\n            flash(\"incorrect login\")\n            return redirect(url_for('.login_get'))\n        return redirect(url_for(\".index\"))\n\n    def logout(self):\n        session.pop('username', None)\n        return redirect(url_for(\".index\"))",
  "def get(self, key):\n        \"\"\"given a key returns json objects\"\"\"\n        raise NotImplementedError",
  "def set(self, key, val):\n        \"\"\"given a key and a json object, saves it\"\"\"\n        raise NotImplementedError",
  "def create(self, key, val):\n        \"\"\"given a key and a json object, saves it\n        differs from set because this method should check\n        to make sure the object doesn't already exist\n        \"\"\"\n        raise NotImplementedError",
  "def __init__(self, redisconn):\n        self.redisconn = redisconn",
  "def get(self, key):\n        data = self.redisconn.get(key)\n        if data is None:\n            return None\n        attrs = json.loads(data.decode('utf-8'))\n        return attrs",
  "def set(self, key, val):\n        self.redisconn.set(key, json.dumps(val))",
  "def create(self, key, val):\n        with self.redisconn.pipeline() as pipe:\n            pipe.watch(key)\n            pipe.multi()\n            if self.redisconn.exists(key):\n                raise DataIntegrityException(\"%s already exists\" % key)\n            else:\n                pipe.set(key, json.dumps(val))\n            pipe.execute()",
  "def __init__(self):\n        self._data = {}",
  "def get(self, key):\n        data = self._data.get(key, None)\n        if data is None:\n            return None\n        attrs = json.loads(decode_utf8(data))\n        return attrs",
  "def set(self, key, val):\n        self._data[key] = json.dumps(val)",
  "def create(self, key, val):\n        if key in self._data:\n            raise DataIntegrityException(\"%s already exists\" % key)\n        self._data[key] = json.dumps(val)",
  "def get(self, key):\n        _data = shelve.open('bokeh.server')\n        key = encode_utf8(key)\n        data = _data.get(key, None)\n        if data is None:\n            return None\n        attrs = json.loads(decode_utf8(data))\n        _data.close()\n        return attrs",
  "def set(self, key, val):\n        _data = shelve.open('bokeh.server')\n        key = encode_utf8(key)\n        _data[key] = json.dumps(val)\n        _data.close()",
  "def create(self, key, val):\n        key = str(key)\n        _data = shelve.open('bokeh.server')\n        if key in _data:\n            raise DataIntegrityException(\"%s already exists\" % key)\n        _data[key] = json.dumps(val)\n        _data.close()",
  "def current_user_name(self):\n        \"\"\"obtain current user name from the current request\n        current request is obtained from flask request thread local\n        object\n        \"\"\"\n        raise NotImplementedError",
  "def login(self, username):\n        \"\"\"login the user, sets whatever request information is necessary\n        (usually, session['username'] = username)\n        \"\"\"\n        raise NotImplementedError",
  "def logout(self):\n        \"\"\"logs out the user, sets whatever request information is necessary\n        usually, session.pop('username')\n        \"\"\"\n        raise NotImplementedError",
  "def current_user(self):\n        \"\"\"returns bokeh User object from self.current_user_name\n        \"\"\"\n        username = self.current_user_name()\n        if username is None:\n            return None\n        bokehuser = user.User.load(bokeh_app.servermodel_storage, username)\n        return bokehuser",
  "def login_get(self):\n        \"\"\"custom login view\n        \"\"\"\n        raise NotImplementedError",
  "def login_post(self):\n        \"\"\"custom login submission. Request form will have\n        username, password, and possibly an api field.\n        api indicates that we are\n        submitting via python, and we should try to return error\n        codes rather than flash messages\n        \"\"\"\n        raise NotImplementedError",
  "def login_from_apikey(self):\n        \"\"\"login URL using apikey.  This is usually generated\n        by the python client\n        \"\"\"\n        raise NotImplementedError",
  "def register_get(self):\n        \"\"\"custom register view\n        \"\"\"\n        raise NotImplementedError",
  "def register_post(self):\n        \"\"\"custom register submission\n        request form will have username, password, password_confirm,\n        and possibly an api field. api indicates that we are\n        submitting via python, and we should try to return error\n        codes rather than flash messages\n        \"\"\"\n        raise NotImplementedError",
  "def can_write_doc(self, docid):\n        \"\"\"whether or not a user can write to a doc\n        \"\"\"\n        raise NotImplementedError",
  "def can_read_doc(self, docid):\n        \"\"\"whether or not a user can read a doc\n        \"\"\"\n        raise NotImplementedError",
  "def can_write_doc(self, doc_or_docid, temporary_docid=None, userobj=None):\n        return True",
  "def can_read_doc(self, doc_or_docid, temporary_docid=None, userobj=None):\n        return True",
  "def current_user_name(self):\n        return \"defaultuser\"",
  "def current_user(self):\n        \"\"\"returns bokeh User object matching defaultuser\n        if the user does not exist, one will be created\n        \"\"\"\n        username = self.current_user_name()\n        bokehuser = user.User.load(bokeh_app.servermodel_storage, username)\n        if bokehuser is not None:\n            return bokehuser\n        bokehuser = user.new_user(bokeh_app.servermodel_storage, \"defaultuser\",\n                                  str(uuid.uuid4()), apikey='nokey', docs=[])\n        return bokehuser",
  "def can_write_doc(self, doc_or_docid, temporary_docid=None, userobj=None):\n        if not isinstance(doc_or_docid, docs.Doc):\n            doc = docs.Doc.load(bokeh_app.servermodel_storage, doc_or_docid)\n        else:\n            doc = doc_or_docid\n        if userobj is None:\n            userobj = self.current_user()\n        return convenience.can_write_from_request(doc, request, userobj,\n                                                  temporary_docid=temporary_docid)",
  "def can_read_doc(self, doc_or_docid, temporary_docid=None, userobj=None):\n        if not isinstance(doc_or_docid, docs.Doc):\n            doc = docs.Doc.load(bokeh_app.servermodel_storage, doc_or_docid)\n        else:\n            doc = doc_or_docid\n        if userobj is None:\n            userobj = self.current_user()\n        return convenience.can_read_from_request(doc, request, userobj)",
  "def login(self, username):\n        session['username'] = username",
  "def print_connection_info(self, bokehuser):\n        logger.info(\"connect using the following\")\n        command = \"output_server(docname, username='%s', userapikey='%s')\"\n        command = command % (bokehuser.username, bokehuser.apikey)\n        logger.info(command)",
  "def current_user_name(self):\n        # users can be authenticated by logging in (setting the session)\n        # or by setting fields in the http header (api keys, etc..)\n        username =  session.get('username', None)\n        if username:\n            return username\n        else:\n            # check for auth via apis and headers\n            bokehuser = user.apiuser_from_request(bokeh_app, request)\n            if bokehuser:\n                return bokehuser.username\n        return None",
  "def register_get(self):\n        return render_template(\"register.html\", title=\"Register\")",
  "def login_get(self):\n        return render_template(\"login.html\", title=\"Login\")",
  "def register_post_api(self):\n        username = request.values['username']\n        password = request.values['password']\n        try:\n            bokehuser = user.new_user(\n                bokeh_app.servermodel_storage, username, password\n                )\n            self.login(username)\n            self.print_connection_info(bokehuser)\n        except UnauthorizedException:\n            return jsonify(status=False,\n                           error=\"user already exists\")\n        return jsonify(status=True,\n                       userapikey=bokehuser.apikey\n                       )",
  "def register_post(self):\n        if request.values.get('api', None):\n            return self.register_post_api()\n        username = request.values['username']\n        password = request.values['password']\n        password_confirm = request.values['password_confirm']\n        if password != password_confirm:\n            flash(\"password and confirmation do not match\")\n            return redirect(url_for('.register_get'))\n        try:\n            bokehuser = user.new_user(\n                bokeh_app.servermodel_storage, username, password\n                )\n            self.login(username)\n            self.print_connection_info(bokehuser)\n        except UnauthorizedException:\n            flash(\"user already exists\")\n            return redirect(url_for('.register_get'))\n        return redirect(url_for(\".index\"))",
  "def login_post_api(self):\n        username = request.values['username']\n        password = request.values['password']\n        try:\n            bokehuser = user.auth_user(bokeh_app.servermodel_storage,\n                                       username,\n                                       password)\n            self.login(username)\n            self.print_connection_info(bokehuser)\n        except UnauthorizedException:\n            return jsonify(status=False,\n                           error=\"incorrect login \")\n        return jsonify(status=True,\n                       userapikey=bokehuser.apikey\n                       )",
  "def login_post(self):\n        if request.values.get('api', None):\n            return self.login_post_api()\n        username = request.values['username']\n        password = request.values['password']\n        try:\n            bokehuser = user.auth_user(bokeh_app.servermodel_storage,\n                                       username,\n                                       password=password)\n            self.login(username)\n            self.print_connection_info(bokehuser)\n        except UnauthorizedException:\n            flash(\"incorrect login exists\")\n            return redirect(url_for('.login_get'))\n        return redirect(url_for(\".index\"))",
  "def login_from_apikey(self):\n        username = request.values.get('username')\n        apikey = request.values.get('userapikey')\n        try:\n            bokehuser = user.auth_user(bokeh_app.servermodel_storage,\n                                       username,\n                                       apikey=apikey)\n\n            self.login(username)\n            self.print_connection_info(bokehuser)\n        except UnauthorizedException:\n            flash(\"incorrect login\")\n            return redirect(url_for('.login_get'))\n        return redirect(url_for(\".index\"))",
  "def logout(self):\n        session.pop('username', None)\n        return redirect(url_for(\".index\"))",
  "class MultiDictionary(object):\n    def __init__(self):\n        self.dict = {}\n\n    def add(self, k, v):\n        self.dict.setdefault(k, set()).add(v)\n\n    def remove_val(self, k, v):\n        self.dict.setdefault(k, set()).remove(v)\n        if len(self.dict[k]) == 0:\n            self.remove(k)\n\n    def remove(self, k):\n        del self.dict[k]\n\n    def get(self, *args):\n        return self.dict.get(*args)",
  "class WebSocketManager(object):\n    def __init__(self):\n        self.sockets = {}\n        self.topic_clientid_map = MultiDictionary()\n        self.clientid_topic_map = MultiDictionary()\n        self.auth_functions = {}\n        atexit.register(self._atexit)\n\n    def _atexit(self):\n        if len(self.sockets) != 0:\n            log.warning(\"Not all websocket connections were closed properly\")\n\n    def remove_clientid(self, clientid):\n        topics = self.clientid_topic_map.get(clientid, [])\n        for topic in topics:\n            self.topic_clientid_map.remove_val(topic, clientid)\n\n    def remove_topic(self, topic):\n        clientids = self.topic_clientid_map.get(topic)\n        for clientid in clientids:\n            self.clientid_topic_map.remove_val(clientid, topic)\n\n    def subscribe_socket(self, socket, topic, clientid=None):\n        if clientid is None :\n            clientid = str(uuid.uuid4())\n        self.subscribe(clientid, topic)\n        self.add_socket(socket, clientid)\n\n    def can_subscribe(self, clientid, topic):\n        #auth goes here\n        return True\n\n    def register_auth(self, authtype, func):\n        self.auth_functions[authtype] = func\n\n    def auth(self, authtoken, topic):\n        #authtoken - some string, whatever you want it to be\n        #topic - string topic, of syntax type:value.\n        #topic type maps to auth function\n        authtype, topic = topic.split(\":\", 1)\n        if self.auth_functions.get(authtype):\n            return self.auth_functions[authtype](authtoken, topic)\n        else:\n            return True\n\n    def subscribe(self, clientid, topic):\n        if self.can_subscribe(clientid, topic):\n            log.debug(\"subscribe %s, %s\", topic, clientid)\n            self.topic_clientid_map.add(topic, clientid)\n            self.clientid_topic_map.add(clientid, topic)\n\n    def add_socket(self, socket, clientid):\n        log.debug(\"add socket %s\", clientid)\n        self.sockets[clientid] = socket\n\n    def remove_socket(self, clientid):\n        log.debug(\"remove socket %s\", clientid)\n        self.sockets.pop(clientid, None)\n\n    def send(self, topic, msg, exclude=None):\n        if exclude is None:\n            exclude = set()\n        log.debug(\"sending to %s\", self.topic_clientid_map.get(topic, []))\n        for clientid in tuple(self.topic_clientid_map.get(topic, [])):\n            socket = self.sockets.get(clientid, None)\n            if not socket:\n                continue\n            if clientid in exclude:\n                continue\n            try:\n                socket.write_message(topic + \":\" + msg)\n            except Exception as e: #what exception is this?if a client disconnects\n                log.exception(e)\n                self.remove_socket(clientid)\n                self.remove_clientid(clientid)",
  "def __init__(self):\n        self.dict = {}",
  "def add(self, k, v):\n        self.dict.setdefault(k, set()).add(v)",
  "def remove_val(self, k, v):\n        self.dict.setdefault(k, set()).remove(v)\n        if len(self.dict[k]) == 0:\n            self.remove(k)",
  "def remove(self, k):\n        del self.dict[k]",
  "def get(self, *args):\n        return self.dict.get(*args)",
  "def __init__(self):\n        self.sockets = {}\n        self.topic_clientid_map = MultiDictionary()\n        self.clientid_topic_map = MultiDictionary()\n        self.auth_functions = {}\n        atexit.register(self._atexit)",
  "def _atexit(self):\n        if len(self.sockets) != 0:\n            log.warning(\"Not all websocket connections were closed properly\")",
  "def remove_clientid(self, clientid):\n        topics = self.clientid_topic_map.get(clientid, [])\n        for topic in topics:\n            self.topic_clientid_map.remove_val(topic, clientid)",
  "def remove_topic(self, topic):\n        clientids = self.topic_clientid_map.get(topic)\n        for clientid in clientids:\n            self.clientid_topic_map.remove_val(clientid, topic)",
  "def subscribe_socket(self, socket, topic, clientid=None):\n        if clientid is None :\n            clientid = str(uuid.uuid4())\n        self.subscribe(clientid, topic)\n        self.add_socket(socket, clientid)",
  "def can_subscribe(self, clientid, topic):\n        #auth goes here\n        return True",
  "def register_auth(self, authtype, func):\n        self.auth_functions[authtype] = func",
  "def auth(self, authtoken, topic):\n        #authtoken - some string, whatever you want it to be\n        #topic - string topic, of syntax type:value.\n        #topic type maps to auth function\n        authtype, topic = topic.split(\":\", 1)\n        if self.auth_functions.get(authtype):\n            return self.auth_functions[authtype](authtoken, topic)\n        else:\n            return True",
  "def subscribe(self, clientid, topic):\n        if self.can_subscribe(clientid, topic):\n            log.debug(\"subscribe %s, %s\", topic, clientid)\n            self.topic_clientid_map.add(topic, clientid)\n            self.clientid_topic_map.add(clientid, topic)",
  "def add_socket(self, socket, clientid):\n        log.debug(\"add socket %s\", clientid)\n        self.sockets[clientid] = socket",
  "def remove_socket(self, clientid):\n        log.debug(\"remove socket %s\", clientid)\n        self.sockets.pop(clientid, None)",
  "def send(self, topic, msg, exclude=None):\n        if exclude is None:\n            exclude = set()\n        log.debug(\"sending to %s\", self.topic_clientid_map.get(topic, []))\n        for clientid in tuple(self.topic_clientid_map.get(topic, [])):\n            socket = self.sockets.get(clientid, None)\n            if not socket:\n                continue\n            if clientid in exclude:\n                continue\n            try:\n                socket.write_message(topic + \":\" + msg)\n            except Exception as e: #what exception is this?if a client disconnects\n                log.exception(e)\n                self.remove_socket(clientid)\n                self.remove_clientid(clientid)",
  "class Gzip(object):\n    def __init__(self, app, compress_level=6, minimum_size=500):\n        self.app = app\n        self.compress_level = compress_level\n        self.minimum_size = minimum_size\n        self.app.after_request(self.after_request)\n\n    def after_request(self, response):\n        accept_encoding = request.headers.get('Accept-Encoding', '')\n        if response.status_code < 200 or \\\n           response.status_code >= 300 or \\\n           not response.content_length or \\\n           response.content_length < self.minimum_size or \\\n           'gzip' not in accept_encoding.lower() or \\\n           'Content-Encoding' in response.headers:\n            return response\n\n        response.direct_passthrough = False\n        gzip_buffer = six.BytesIO()\n        gzip_file = gzip.GzipFile(mode='wb', compresslevel=self.compress_level, fileobj=gzip_buffer)\n        gzip_file.write(response.data)\n        gzip_file.close()\n        response.data = gzip_buffer.getvalue()\n        response.headers['Content-Encoding'] = 'gzip'\n        response.headers['Content-Length'] = len(response.data)\n\n        return response",
  "def __init__(self, app, compress_level=6, minimum_size=500):\n        self.app = app\n        self.compress_level = compress_level\n        self.minimum_size = minimum_size\n        self.app.after_request(self.after_request)",
  "def after_request(self, response):\n        accept_encoding = request.headers.get('Accept-Encoding', '')\n        if response.status_code < 200 or \\\n           response.status_code >= 300 or \\\n           not response.content_length or \\\n           response.content_length < self.minimum_size or \\\n           'gzip' not in accept_encoding.lower() or \\\n           'Content-Encoding' in response.headers:\n            return response\n\n        response.direct_passthrough = False\n        gzip_buffer = six.BytesIO()\n        gzip_file = gzip.GzipFile(mode='wb', compresslevel=self.compress_level, fileobj=gzip_buffer)\n        gzip_file.write(response.data)\n        gzip_file.close()\n        response.data = gzip_buffer.getvalue()\n        response.headers['Content-Encoding'] = 'gzip'\n        response.headers['Content-Length'] = len(response.data)\n\n        return response",
  "class Subscriber(object):\n    def __init__(self, ctx, addrs, wsmanager):\n        self.ctx = ctx\n        self.addrs = addrs\n        self.wsmanager = wsmanager\n        self.kill = False\n\n    def run(self):\n        sockets = []\n        poller = zmq.Poller()\n        for addr in self.addrs:\n            socket = self.ctx.socket(zmq.SUB)\n            log.debug('SUB CONNECT: %s' % addr)\n            socket.connect(addr)\n            socket.setsockopt_string(zmq.SUBSCRIBE, u\"\")\n            sockets.append(socket)\n            poller.register(socket, zmq.POLLIN)\n        try:\n            while not self.kill:\n                socks = dict(poller.poll(timeout * 1000))\n                for socket, v in socks.items():\n                    msg = socket.recv_json()\n                    topic, msg, exclude = msg['topic'], msg['msg'], msg['exclude']\n                    self.wsmanager.send(topic, msg, exclude=exclude)\n        except zmq.ContextTerminated:\n            pass\n        finally:\n            for s in sockets:\n                s.close()\n\n    def start(self):\n        self.thread = Thread(target=self.run)\n        self.thread.start()\n\n    def stop(self):\n        self.kill = True",
  "def __init__(self, ctx, addrs, wsmanager):\n        self.ctx = ctx\n        self.addrs = addrs\n        self.wsmanager = wsmanager\n        self.kill = False",
  "def run(self):\n        sockets = []\n        poller = zmq.Poller()\n        for addr in self.addrs:\n            socket = self.ctx.socket(zmq.SUB)\n            log.debug('SUB CONNECT: %s' % addr)\n            socket.connect(addr)\n            socket.setsockopt_string(zmq.SUBSCRIBE, u\"\")\n            sockets.append(socket)\n            poller.register(socket, zmq.POLLIN)\n        try:\n            while not self.kill:\n                socks = dict(poller.poll(timeout * 1000))\n                for socket, v in socks.items():\n                    msg = socket.recv_json()\n                    topic, msg, exclude = msg['topic'], msg['msg'], msg['exclude']\n                    self.wsmanager.send(topic, msg, exclude=exclude)\n        except zmq.ContextTerminated:\n            pass\n        finally:\n            for s in sockets:\n                s.close()",
  "def start(self):\n        self.thread = Thread(target=self.run)\n        self.thread.start()",
  "def stop(self):\n        self.kill = True",
  "class BokehBlueprint(flask.Blueprint):\n\n    def __init__(self, *args, **kwargs):\n        super(BokehBlueprint, self).__init__(*args, **kwargs)\n        self.debugjs = None\n\n    def setup(self, backend, backbone_storage, servermodel_storage,\n              authentication):\n        self.backend = backend\n        self.backbone_storage = backbone_storage\n        self.servermodel_storage = servermodel_storage\n        self.authentication = authentication\n        self.bokehjsdir = settings.bokehjsdir()\n        self.bokehjssrcdir = settings.bokehjssrcdir()\n\n    def current_user(self):\n        return self.authentication.current_user()\n\n    def js_files(self):\n        bokehjsdir = self.bokehjsdir\n        js_files = []\n        for root, dirnames, files in walk(bokehjsdir):\n            for fname in files:\n                if fname.endswith(\".js\") and 'vendor' not in root:\n                    js_files.append(join(root, fname))\n        return js_files",
  "def __init__(self, *args, **kwargs):\n        super(BokehBlueprint, self).__init__(*args, **kwargs)\n        self.debugjs = None",
  "def setup(self, backend, backbone_storage, servermodel_storage,\n              authentication):\n        self.backend = backend\n        self.backbone_storage = backbone_storage\n        self.servermodel_storage = servermodel_storage\n        self.authentication = authentication\n        self.bokehjsdir = settings.bokehjsdir()\n        self.bokehjssrcdir = settings.bokehjssrcdir()",
  "def current_user(self):\n        return self.authentication.current_user()",
  "def js_files(self):\n        bokehjsdir = self.bokehjsdir\n        js_files = []\n        for root, dirnames, files in walk(bokehjsdir):\n            for fname in files:\n                if fname.endswith(\".js\") and 'vendor' not in root:\n                    js_files.append(join(root, fname))\n        return js_files",
  "class StoreAdapter(object):\n    \"\"\"API modeled after Redis that other stores have to adapt to. \"\"\"\n\n    def mget(self, doc_keys):\n        raise NotImplementedError(\"abstract method\")\n\n    def mset(self, data):\n        raise NotImplementedError(\"abstract method\")\n\n    def sadd(self, doc_key, *keys):\n        raise NotImplementedError(\"abstract method\")\n\n    def srem(self, doc_key, member_key):\n        raise NotImplementedError(\"abstract method\")\n\n    def smembers(self, doc_key):\n        raise NotImplementedError(\"abstract method\")\n\n    def set(self, key, data):\n        raise NotImplementedError(\"abstract method\")\n\n    def delete(self, key):\n        raise NotImplementedError(\"abstract method\")",
  "def dockey(docid):\n    docid = encode_utf8('doc:' + docid)\n    return docid",
  "def modelkey(typename, docid, modelid):\n    docid = encode_utf8(docid)\n    modelid = encode_utf8(modelid)\n    return 'bbmodel:%s:%s:%s' % (typename, docid, modelid)",
  "def callbackskey(typename, docid, modelid):\n    return 'bbcallback:%s:%s:%s' % (typename, docid, modelid)",
  "def parse_modelkey(key):\n    _, typename, docid, modelid = decode_utf8(key).split(\":\")\n    return typename, docid, modelid",
  "def prune(document, temporary_docid=None, delete=False):\n    if temporary_docid is not None:\n        storage_id = temporary_docid\n    else:\n        storage_id = document.docid\n    to_delete = document.prune()\n    if delete:\n        for objid in to_delete:\n            bokeh_app.backbone_storage.del_obj(storage_id, objid)",
  "class PersistentBackboneStorage(object):\n    \"\"\"Base class for `RedisBackboneStorage`, `InMemoryBackboneStorage`, etc. \"\"\"\n\n\n    def pull(self, docid, typename=None, objid=None):\n        \"\"\"you need to call this with either typename AND objid\n        or leave out both.  leaving them out means retrieve all\n        otherwise, retrieves a specific object\n        \"\"\"\n        doc_keys = self.smembers(dockey(docid))\n        attrs = self.mget(doc_keys)\n        data = []\n        for k, attr in zip(doc_keys, attrs):\n            typename, _, modelid = parse_modelkey(k)\n            attr = protocol.deserialize_json(decode_utf8(attr))\n            data.append({'type': typename, 'attributes': attr})\n        return data\n\n    def get_document(self, docid):\n        json_objs = self.pull(docid)\n        doc = Document(json_objs)\n        doc.docid = docid\n        return doc\n\n    def store_objects(self, docid, *objs, **kwargs):\n        dirty_only = kwargs.pop('dirty_only', True)\n        models = set()\n        for obj in objs:\n            models.add(obj.references())\n        if dirty_only:\n            models = list(models)\n        json_objs = dump(models, docid)\n        self.push(docid, *json_objs)\n        for mod in models:\n            mod._dirty = False\n        return models\n\n    def store_document(self, doc, temporary_docid=None, dirty_only=True):\n        \"\"\"store all dirty models\n        \"\"\"\n        # This is not so nice - we need to use doc with the original docid\n        # when we create json objs, however use the temporary_docid\n        # when we actually store the values\n        # TODO: refactor this API in the future for better separation\n        if temporary_docid is not None:\n            storage_id = temporary_docid\n        else:\n            storage_id = doc.docid\n        logger.debug(\"storing objects to %s\", storage_id)\n        models = doc._models.values()\n        if dirty_only:\n            models = [x for x in models if hasattr(x, '_dirty') and x._dirty]\n        json_objs = doc.dump(*models)\n        self.push(storage_id, *json_objs)\n        for mod in models:\n            mod._dirty = False\n        return models\n\n    def push(self, docid, *jsonobjs):\n        keys = [modelkey(attr['type'],\n                              docid,\n                              attr['attributes']['id']) for attr in jsonobjs]\n        for attr in jsonobjs:\n            attr['attributes']['doc'] = docid\n        attrs = [protocol.serialize_json(attr['attributes']) for attr in jsonobjs]\n        dkey = dockey(docid)\n        data = dict(zip(keys, attrs))\n        self.mset(data)\n        self.sadd(dkey, *keys)\n\n    def del_obj(self, docid, m):\n        mkey = modelkey(m.__view_model__, docid, m._id)\n        self.srem(dockey(docid), mkey)\n        self.delete(mkey)\n\n    # UNUSED FOR NOW\n    # def load_all_callbacks(self, get_json=False):\n    #     \"\"\"get_json = return json of callbacks, rather than\n    #     loading them into models\n    #     \"\"\"\n    #     doc_keys = self.smembers(dockey(docid))\n    #     callback_keys = [x.replace(\"bbmodel\", \"bbcallback\") for x in doc_keys]\n    #     callbacks = self.mget(callback_keys)\n    #     callbacks = [x for x in callbacks if x]\n    #     callbacks = [protocol.deserialize_json(x) for x in callbacks]\n    #     if get_json:\n    #         return callbacks\n    #     self.load_callbacks_json(callbacks)\n\n    def store_callbacks(self, to_store):\n        for callbacks in to_store:\n            typename = callbacks['type']\n            _id = callbacks['id']\n            key = self.callbackskey(typename, self.docid, _id)\n            data = self.serialize(callbacks)\n            self.set(key, data)",
  "class RedisBackboneStorage(PersistentBackboneStorage):\n    \"\"\"storage used by the webserver to work with\n    a user's documents.  uses redis directly.\n    \"\"\"\n    def __init__(self, redis):\n        self.redis = redis\n        super(RedisBackboneStorage, self).__init__()\n\n    def mget(self, doc_keys):\n        if not doc_keys:\n            return []\n        vals = self.redis.mget(doc_keys)\n        return [None if val is None else val.decode('utf-8') for val in vals]\n\n    def mset(self, data):\n        self.redis.mset(data)\n\n    def sadd(self, doc_key, *keys):\n        self.redis.sadd(doc_key, *keys)\n\n    def srem(self, doc_key, mkey):\n        self.redis.srem(doc_key, mkey)\n\n    def smembers(self, doc_key):\n        vals = self.redis.smembers(doc_key)\n        return [None if val is None else val.decode('utf-8') for val in vals]\n\n    def set(self, key, data):\n        self.redis.set(key, data)\n\n    def delete(self, mkey):\n        self.redis.delete(mkey)",
  "class InMemoryBackboneStorage(PersistentBackboneStorage):\n    \"\"\"storage used by the webserver to work with\n    a user's documents.  uses in memory data store directly.\n    \"\"\"\n    def __init__(self):\n        self._inmem_data = {}\n        self._inmem_sets = defaultdict(set)\n\n    def mget(self, doc_keys):\n        return [self._inmem_data.get(key, None) for key in doc_keys]\n\n    def mset(self, data):\n        self._inmem_data.update(data)\n\n    def sadd(self, doc_key, *keys):\n        self._inmem_sets[doc_key].update(keys)\n\n    def srem(self, doc_key, member_key):\n        inmem_set = self._inmem_sets[doc_key]\n        try: inmem_set.remove(member_key)\n        except KeyError: pass\n\n    def smembers(self, doc_key):\n        return list(self._inmem_sets[doc_key])\n\n    def set(self, key, data):\n        self._inmem_data[key] = data\n\n    def delete(self, key):\n        del self._inmem_data[key]",
  "class ShelveBackboneStorage(PersistentBackboneStorage):\n    \"\"\"storage used by the webserver to work with\n    a user's documents.  uses shelve data store directly.\n    \"\"\"\n\n    @contextlib.contextmanager\n    def shelve_data(self):\n        data = shelve.open('bokeh.data')\n        try:\n            yield data\n        finally:\n            data.close()\n\n    @contextlib.contextmanager\n    def shelve_sets(self):\n        sets = shelve.open('bokeh.sets')\n        try:\n            yield sets\n        finally:\n            sets.close()\n\n    def mget(self, doc_keys):\n        with self.shelve_data() as _shelve_data:\n            return [_shelve_data.get(key, None) for key in doc_keys]\n\n    def mset(self, data):\n        with self.shelve_data() as _shelve_data:\n            for k, v in data.items():\n                _shelve_data[k] = v\n\n    def sadd(self, doc_key, *keys):\n        with self.shelve_sets() as _shelve_sets:\n            shelve_set = _shelve_sets.get(doc_key, set())\n            shelve_set.update(keys)\n            _shelve_sets[doc_key] = shelve_set\n\n    def srem(self, doc_key, member_key):\n        with self.shelve_sets() as _shelve_sets:\n            shelve_set = _shelve_sets[doc_key]\n            try: shelve_set.remove(member_key)\n            except KeyError: pass\n            _shelve_sets[doc_key] = shelve_set\n\n    def smembers(self, doc_key):\n        with self.shelve_sets() as _shelve_sets:\n            return list(_shelve_sets.get(doc_key, []))\n\n    def set(self, key, data):\n        with self.shelve_data() as _shelve_data:\n            _shelve_data[key] = data\n\n    def delete(self, key):\n        with self.shelve_data() as _shelve_data:\n            del _shelve_data[key]",
  "def get_temporary_docid(request, docid):\n    key = 'temporary-%s' % docid\n    return request.headers.get(key, None)",
  "class BokehServerTransaction(object):\n    #hugo - is this the right name?\n    \"\"\"Context Manager for a req/rep response cycle of the bokeh server\n    responsible for\n    -  determining whether the current user can read from a document\n    -  determining whether the current user can write to a bokeh document\n    (or temporary document for copy on write)\n    -  stitching together documents to form a copy on write view\n    -  saving changes to the document\n\n    at the start of the context manager, self.clientdoc is populated\n    with an instance of bokeh.document.Document with all the data\n    loaded in (including from the copy on write context if specified)\n    at the end of the context manager, changed models are written\n    to the appropriate storage location.  and changed models are\n    written to self.changed\n\n    currently deletes aren't really working properly with cow - but we\n    don't really make use of model deletions yet\n    \"\"\"\n    def __init__(self, server_userobj, server_docobj, mode,\n                 temporary_docid=None):\n        \"\"\"\n        bokeh_app : bokeh_app blueprint\n        server_userobj : instance of bokeh.server.models.user.User - current user\n          for a request\n        server_docobj : instance of bokeh.server.models.docs.Doc\n        mode : 'r', or 'rw', or 'auto' - auto means rw if possible, else r\n        temporary_docid : temporary docid for copy on write\n        \"\"\"\n        logger.debug(\n            \"created transaction with %s, %s\",\n            server_docobj.docid, temporary_docid\n        )\n        self.server_userobj = server_userobj\n        self.server_docobj = server_docobj\n        self.temporary_docid = temporary_docid\n        can_write = bokeh_app.authentication.can_write_doc(\n            self.server_docobj,\n            userobj=self.server_userobj,\n            temporary_docid=self.temporary_docid)\n        if can_write:\n            can_read = True\n        else:\n            can_read = bokeh_app.authentication.can_read_doc(\n                self.server_docobj,\n                userobj=self.server_userobj)\n        docid = self.server_docobj.docid\n        if mode not in {'auto', 'rw', 'r'}:\n            raise Authentication('Unknown Mode')\n        if mode == 'auto':\n            if not can_write and not can_read:\n                raise AuthenticationException(\"could not read from %s\" % docid)\n            if can_write:\n                mode = 'rw'\n            else:\n                mode = 'r'\n        else:\n            if mode == 'rw':\n                if not can_write:\n                    raise AuthenticationException(\"could not write to %s\" % docid)\n            elif mode == 'r':\n                if not can_read:\n                    raise AuthenticationException(\"could not read from %s\" % docid)\n        self.mode = mode\n        if self.mode == 'rw':\n            self.apikey = self.server_docobj.apikey\n        else:\n            self.apikey = self.server_docobj.readonlyapikey\n    @property\n    def write_docid(self):\n        if self.temporary_docid:\n            return self.temporary_docid\n        else:\n            return self.server_docobj.docid\n\n    def load(self, gc=False):\n        from .views.backbone import init_bokeh\n        clientdoc = bokeh_app.backbone_storage.get_document(self.server_docobj.docid)\n        if self.temporary_docid:\n            temporary_json = bokeh_app.backbone_storage.pull(self.temporary_docid)\n            #no events - because we're loading from datastore, so nothing is new\n            clientdoc.load(*temporary_json, events='none', dirty=False)\n        if gc and self.mode != 'rw':\n            raise AuthenticationException(\"cannot run garbage collection in read only mode\")\n        elif gc and self.mode == 'rw':\n            prune(clientdoc, delete=True)\n        else:\n            prune(clientdoc)\n        init_bokeh(clientdoc)\n        self.clientdoc = clientdoc\n\n    def save(self):\n        if self.mode != 'rw':\n            raise AuthenticationException(\"cannot save in read only mode\")\n        self.changed = bokeh_app.backbone_storage.store_document(\n            self.clientdoc,\n            temporary_docid=self.temporary_docid\n        )",
  "def mget(self, doc_keys):\n        raise NotImplementedError(\"abstract method\")",
  "def mset(self, data):\n        raise NotImplementedError(\"abstract method\")",
  "def sadd(self, doc_key, *keys):\n        raise NotImplementedError(\"abstract method\")",
  "def srem(self, doc_key, member_key):\n        raise NotImplementedError(\"abstract method\")",
  "def smembers(self, doc_key):\n        raise NotImplementedError(\"abstract method\")",
  "def set(self, key, data):\n        raise NotImplementedError(\"abstract method\")",
  "def delete(self, key):\n        raise NotImplementedError(\"abstract method\")",
  "def pull(self, docid, typename=None, objid=None):\n        \"\"\"you need to call this with either typename AND objid\n        or leave out both.  leaving them out means retrieve all\n        otherwise, retrieves a specific object\n        \"\"\"\n        doc_keys = self.smembers(dockey(docid))\n        attrs = self.mget(doc_keys)\n        data = []\n        for k, attr in zip(doc_keys, attrs):\n            typename, _, modelid = parse_modelkey(k)\n            attr = protocol.deserialize_json(decode_utf8(attr))\n            data.append({'type': typename, 'attributes': attr})\n        return data",
  "def get_document(self, docid):\n        json_objs = self.pull(docid)\n        doc = Document(json_objs)\n        doc.docid = docid\n        return doc",
  "def store_objects(self, docid, *objs, **kwargs):\n        dirty_only = kwargs.pop('dirty_only', True)\n        models = set()\n        for obj in objs:\n            models.add(obj.references())\n        if dirty_only:\n            models = list(models)\n        json_objs = dump(models, docid)\n        self.push(docid, *json_objs)\n        for mod in models:\n            mod._dirty = False\n        return models",
  "def store_document(self, doc, temporary_docid=None, dirty_only=True):\n        \"\"\"store all dirty models\n        \"\"\"\n        # This is not so nice - we need to use doc with the original docid\n        # when we create json objs, however use the temporary_docid\n        # when we actually store the values\n        # TODO: refactor this API in the future for better separation\n        if temporary_docid is not None:\n            storage_id = temporary_docid\n        else:\n            storage_id = doc.docid\n        logger.debug(\"storing objects to %s\", storage_id)\n        models = doc._models.values()\n        if dirty_only:\n            models = [x for x in models if hasattr(x, '_dirty') and x._dirty]\n        json_objs = doc.dump(*models)\n        self.push(storage_id, *json_objs)\n        for mod in models:\n            mod._dirty = False\n        return models",
  "def push(self, docid, *jsonobjs):\n        keys = [modelkey(attr['type'],\n                              docid,\n                              attr['attributes']['id']) for attr in jsonobjs]\n        for attr in jsonobjs:\n            attr['attributes']['doc'] = docid\n        attrs = [protocol.serialize_json(attr['attributes']) for attr in jsonobjs]\n        dkey = dockey(docid)\n        data = dict(zip(keys, attrs))\n        self.mset(data)\n        self.sadd(dkey, *keys)",
  "def del_obj(self, docid, m):\n        mkey = modelkey(m.__view_model__, docid, m._id)\n        self.srem(dockey(docid), mkey)\n        self.delete(mkey)",
  "def store_callbacks(self, to_store):\n        for callbacks in to_store:\n            typename = callbacks['type']\n            _id = callbacks['id']\n            key = self.callbackskey(typename, self.docid, _id)\n            data = self.serialize(callbacks)\n            self.set(key, data)",
  "def __init__(self, redis):\n        self.redis = redis\n        super(RedisBackboneStorage, self).__init__()",
  "def mget(self, doc_keys):\n        if not doc_keys:\n            return []\n        vals = self.redis.mget(doc_keys)\n        return [None if val is None else val.decode('utf-8') for val in vals]",
  "def mset(self, data):\n        self.redis.mset(data)",
  "def sadd(self, doc_key, *keys):\n        self.redis.sadd(doc_key, *keys)",
  "def srem(self, doc_key, mkey):\n        self.redis.srem(doc_key, mkey)",
  "def smembers(self, doc_key):\n        vals = self.redis.smembers(doc_key)\n        return [None if val is None else val.decode('utf-8') for val in vals]",
  "def set(self, key, data):\n        self.redis.set(key, data)",
  "def delete(self, mkey):\n        self.redis.delete(mkey)",
  "def __init__(self):\n        self._inmem_data = {}\n        self._inmem_sets = defaultdict(set)",
  "def mget(self, doc_keys):\n        return [self._inmem_data.get(key, None) for key in doc_keys]",
  "def mset(self, data):\n        self._inmem_data.update(data)",
  "def sadd(self, doc_key, *keys):\n        self._inmem_sets[doc_key].update(keys)",
  "def srem(self, doc_key, member_key):\n        inmem_set = self._inmem_sets[doc_key]\n        try: inmem_set.remove(member_key)\n        except KeyError: pass",
  "def smembers(self, doc_key):\n        return list(self._inmem_sets[doc_key])",
  "def set(self, key, data):\n        self._inmem_data[key] = data",
  "def delete(self, key):\n        del self._inmem_data[key]",
  "def shelve_data(self):\n        data = shelve.open('bokeh.data')\n        try:\n            yield data\n        finally:\n            data.close()",
  "def shelve_sets(self):\n        sets = shelve.open('bokeh.sets')\n        try:\n            yield sets\n        finally:\n            sets.close()",
  "def mget(self, doc_keys):\n        with self.shelve_data() as _shelve_data:\n            return [_shelve_data.get(key, None) for key in doc_keys]",
  "def mset(self, data):\n        with self.shelve_data() as _shelve_data:\n            for k, v in data.items():\n                _shelve_data[k] = v",
  "def sadd(self, doc_key, *keys):\n        with self.shelve_sets() as _shelve_sets:\n            shelve_set = _shelve_sets.get(doc_key, set())\n            shelve_set.update(keys)\n            _shelve_sets[doc_key] = shelve_set",
  "def srem(self, doc_key, member_key):\n        with self.shelve_sets() as _shelve_sets:\n            shelve_set = _shelve_sets[doc_key]\n            try: shelve_set.remove(member_key)\n            except KeyError: pass\n            _shelve_sets[doc_key] = shelve_set",
  "def smembers(self, doc_key):\n        with self.shelve_sets() as _shelve_sets:\n            return list(_shelve_sets.get(doc_key, []))",
  "def set(self, key, data):\n        with self.shelve_data() as _shelve_data:\n            _shelve_data[key] = data",
  "def delete(self, key):\n        with self.shelve_data() as _shelve_data:\n            del _shelve_data[key]",
  "def __init__(self, server_userobj, server_docobj, mode,\n                 temporary_docid=None):\n        \"\"\"\n        bokeh_app : bokeh_app blueprint\n        server_userobj : instance of bokeh.server.models.user.User - current user\n          for a request\n        server_docobj : instance of bokeh.server.models.docs.Doc\n        mode : 'r', or 'rw', or 'auto' - auto means rw if possible, else r\n        temporary_docid : temporary docid for copy on write\n        \"\"\"\n        logger.debug(\n            \"created transaction with %s, %s\",\n            server_docobj.docid, temporary_docid\n        )\n        self.server_userobj = server_userobj\n        self.server_docobj = server_docobj\n        self.temporary_docid = temporary_docid\n        can_write = bokeh_app.authentication.can_write_doc(\n            self.server_docobj,\n            userobj=self.server_userobj,\n            temporary_docid=self.temporary_docid)\n        if can_write:\n            can_read = True\n        else:\n            can_read = bokeh_app.authentication.can_read_doc(\n                self.server_docobj,\n                userobj=self.server_userobj)\n        docid = self.server_docobj.docid\n        if mode not in {'auto', 'rw', 'r'}:\n            raise Authentication('Unknown Mode')\n        if mode == 'auto':\n            if not can_write and not can_read:\n                raise AuthenticationException(\"could not read from %s\" % docid)\n            if can_write:\n                mode = 'rw'\n            else:\n                mode = 'r'\n        else:\n            if mode == 'rw':\n                if not can_write:\n                    raise AuthenticationException(\"could not write to %s\" % docid)\n            elif mode == 'r':\n                if not can_read:\n                    raise AuthenticationException(\"could not read from %s\" % docid)\n        self.mode = mode\n        if self.mode == 'rw':\n            self.apikey = self.server_docobj.apikey\n        else:\n            self.apikey = self.server_docobj.readonlyapikey",
  "def write_docid(self):\n        if self.temporary_docid:\n            return self.temporary_docid\n        else:\n            return self.server_docobj.docid",
  "def load(self, gc=False):\n        from .views.backbone import init_bokeh\n        clientdoc = bokeh_app.backbone_storage.get_document(self.server_docobj.docid)\n        if self.temporary_docid:\n            temporary_json = bokeh_app.backbone_storage.pull(self.temporary_docid)\n            #no events - because we're loading from datastore, so nothing is new\n            clientdoc.load(*temporary_json, events='none', dirty=False)\n        if gc and self.mode != 'rw':\n            raise AuthenticationException(\"cannot run garbage collection in read only mode\")\n        elif gc and self.mode == 'rw':\n            prune(clientdoc, delete=True)\n        else:\n            prune(clientdoc)\n        init_bokeh(clientdoc)\n        self.clientdoc = clientdoc",
  "def save(self):\n        if self.mode != 'rw':\n            raise AuthenticationException(\"cannot save in read only mode\")\n        self.changed = bokeh_app.backbone_storage.store_document(\n            self.clientdoc,\n            temporary_docid=self.temporary_docid\n        )",
  "class ThreadDevice(ZMQThreadDevice):\n\n    def __init__(self, ctx, device_type=zmq.QUEUE, in_type=None, out_type=None):\n        self.ctx = ctx\n        # XXX: super(ThreadDevice, self).__init__(device_type, in_type, out_type)\n        #      but ZMQThreadDevice is an old-style class (yay!).\n        ZMQThreadDevice.__init__(self, device_type, in_type, out_type)\n\n    def context_factory(self):\n        return self.ctx",
  "class Forwarder(object):\n    def __init__(self, ctx, input_addr, output_addr):\n        self.device = ThreadDevice(ctx, zmq.FORWARDER, in_type=zmq.SUB, out_type=zmq.PUB)\n        self.device.bind_in(input_addr)\n        self.device.bind_out(output_addr)\n        self.device.setsockopt_in(zmq.SUBSCRIBE, b\"\")\n\n    def start(self):\n        self.device.start()\n\n    def stop(self):\n        self.device.ctx.term()\n        self.device.join()",
  "def __init__(self, ctx, device_type=zmq.QUEUE, in_type=None, out_type=None):\n        self.ctx = ctx\n        # XXX: super(ThreadDevice, self).__init__(device_type, in_type, out_type)\n        #      but ZMQThreadDevice is an old-style class (yay!).\n        ZMQThreadDevice.__init__(self, device_type, in_type, out_type)",
  "def context_factory(self):\n        return self.ctx",
  "def __init__(self, ctx, input_addr, output_addr):\n        self.device = ThreadDevice(ctx, zmq.FORWARDER, in_type=zmq.SUB, out_type=zmq.PUB)\n        self.device.bind_in(input_addr)\n        self.device.bind_out(output_addr)\n        self.device.setsockopt_in(zmq.SUBSCRIBE, b\"\")",
  "def start(self):\n        self.device.start()",
  "def stop(self):\n        self.device.ctx.term()\n        self.device.join()",
  "class Settings(object):\n    _debugjs = False\n    _ctx = None\n    fields = _defaults.keys()\n\n    def reset(self):\n        for k,v in _defaults.items():\n            setattr(self, k, v)\n\n    @property\n    def ctx(self):\n        if self._ctx is None or self._ctx.closed:\n            self._ctx = zmq.Context()\n        return self._ctx\n\n    @property\n    def debugjs(self):\n        return bokeh_settings.debugjs\n\n    @debugjs.setter\n    def debugjs(self, val):\n        bokeh_settings.debugjs = val\n\n    def from_file(self, filename=None):\n        name = \"_bokeh_server_configuration\"\n        mod = imp.load_source(name, filename)\n        for k in self.fields:\n            v = getattr(mod, k, None)\n            if v is not None:\n                setattr(self, k, v)\n        self.process_settings()\n\n    def from_dict(self, input_dict):\n        for k,v in input_dict.items():\n            setattr(self, k, v)\n\n    def from_args(self, args):\n        self.ip = args.ip\n        self.port = args.port\n        self.multi_user = args.multi_user\n        self.model_backend = {'type' : args.backend}\n        if self.model_backend['type'] == 'redis':\n            self.model_backend.update({\n                'redis_port' : args.redis_port,\n                'start-redis' : args.start_redis\n            })\n        self.ws_conn_string = args.ws_conn_string\n        self.debug = args.debug\n        self.debugjs = args.debugjs\n        self.splitjs = args.splitjs\n        self.robust_reload = args.robust_reload\n        self.verbose = args.verbose\n        self.run_forwarder = True\n        if args.blaze_config is not None:\n            self.blaze_config = args.blaze_config\n        if args.script:\n            self.scripts = [args.script]\n\n    def process_settings(self):\n        if self.url_prefix:\n            if not self.url_prefix.startswith(\"/\"):\n                self.url_prefix = \"/\" + self.url_prefix\n            if self.url_prefix.endswith(\"/\"):\n                self.url_prefix = self.url_prefix[:-1]",
  "def reset(self):\n        for k,v in _defaults.items():\n            setattr(self, k, v)",
  "def ctx(self):\n        if self._ctx is None or self._ctx.closed:\n            self._ctx = zmq.Context()\n        return self._ctx",
  "def debugjs(self):\n        return bokeh_settings.debugjs",
  "def debugjs(self, val):\n        bokeh_settings.debugjs = val",
  "def from_file(self, filename=None):\n        name = \"_bokeh_server_configuration\"\n        mod = imp.load_source(name, filename)\n        for k in self.fields:\n            v = getattr(mod, k, None)\n            if v is not None:\n                setattr(self, k, v)\n        self.process_settings()",
  "def from_dict(self, input_dict):\n        for k,v in input_dict.items():\n            setattr(self, k, v)",
  "def from_args(self, args):\n        self.ip = args.ip\n        self.port = args.port\n        self.multi_user = args.multi_user\n        self.model_backend = {'type' : args.backend}\n        if self.model_backend['type'] == 'redis':\n            self.model_backend.update({\n                'redis_port' : args.redis_port,\n                'start-redis' : args.start_redis\n            })\n        self.ws_conn_string = args.ws_conn_string\n        self.debug = args.debug\n        self.debugjs = args.debugjs\n        self.splitjs = args.splitjs\n        self.robust_reload = args.robust_reload\n        self.verbose = args.verbose\n        self.run_forwarder = True\n        if args.blaze_config is not None:\n            self.blaze_config = args.blaze_config\n        if args.script:\n            self.scripts = [args.script]",
  "def process_settings(self):\n        if self.url_prefix:\n            if not self.url_prefix.startswith(\"/\"):\n                self.url_prefix = \"/\" + self.url_prefix\n            if self.url_prefix.endswith(\"/\"):\n                self.url_prefix = self.url_prefix[:-1]",
  "class WebSocketHandler(websocket.WebSocketHandler):\n    @property\n    def manager(self):\n        return self.application.wsmanager\n\n    #accept all domains for now.. maybe rethink this later?\n    def check_origin(self, origin):\n        return True\n\n    def open(self):\n        ## TODO - set client id to continuum client id\n        self.clientid = str(uuid.uuid4())\n        self.manager.add_socket(self, self.clientid)\n\n    def on_close(self):\n        self.manager.remove_socket(self.clientid)\n\n    def on_message(self, message):\n        msgobj = protocol.deserialize_json(message)\n        msgtype = msgobj.get('msgtype')\n        if msgtype == 'subscribe':\n            auth = msgobj['auth']\n            topic = msgobj['topic']\n            if self.manager.auth(auth, topic):\n                self.manager.subscribe(self.clientid, topic)\n                msg = protocol.serialize_json(\n                    protocol.status_obj(['subscribesuccess', topic, self.clientid])\n                )\n                self.write_message(topic + \":\" + msg)\n            else:\n                msg = protocol.serialize_web(protocol.error_obj('unauthorized'))\n                self.write_message(topic + \":\" + msg)",
  "class TornadoWebSocketApplication(Application):\n    def __init__(self, handlers, **settings):\n        super(TornadoWebSocketApplication, self).__init__(handlers, **settings)\n        self.wsmanager = WebSocketManager()\n        zmqaddrs = settings.pop('zmqaddrs')\n        self.subscriber = Subscriber(zmqaddrs, self.wsmanager)\n\n    def stop(self):\n        ## Hugo:  not sure how this is supposed to work\n        ## but apparently you need to stop and then\n        ## start the tornado loop to get it to finish....\n        ioloop.IOLoop.instance().stop()\n        self.server.stop()\n        self.subscriber.kill = True\n        self.subscriber.thread.join()\n        if hasattr(self, 'thread'):\n            self.thread.join()\n\n    def start(self, thread=False):\n        def helper():\n            self.subscriber.start()\n            ioloop.IOLoop.instance().start()\n        if thread:\n            self.thread = threading.Thread(target=helper)\n            self.thread.start()\n        else:\n            helper()\n\n    def set_server(self, server):\n        self.server = server\n\n    def listen(self, port, address=\"\", **kwargs):\n        self.server = HTTPServer(self, **kwargs)\n        self.server.listen(port, address)",
  "def make_app(url_prefix, zmqaddrs, port):\n    if url_prefix is None or url_prefix == \"/\":\n        url = \"/bokeh/sub/\"\n    else:\n        if not url_prefix.startswith(\"/\"):\n            url_prefix = \"/\" + url_prefix\n        if not url_prefix.endswith(\"/\"):\n            url_prefix = url_prefix + \"/\"\n        url = url_prefix + \"bokeh/sub/\"\n    application = TornadoWebSocketApplication([(url, WebSocketHandler)],\n                                              zmqaddrs=zmqaddrs\n    )\n    application.listen(port)\n    return application",
  "def manager(self):\n        return self.application.wsmanager",
  "def check_origin(self, origin):\n        return True",
  "def open(self):\n        ## TODO - set client id to continuum client id\n        self.clientid = str(uuid.uuid4())\n        self.manager.add_socket(self, self.clientid)",
  "def on_close(self):\n        self.manager.remove_socket(self.clientid)",
  "def on_message(self, message):\n        msgobj = protocol.deserialize_json(message)\n        msgtype = msgobj.get('msgtype')\n        if msgtype == 'subscribe':\n            auth = msgobj['auth']\n            topic = msgobj['topic']\n            if self.manager.auth(auth, topic):\n                self.manager.subscribe(self.clientid, topic)\n                msg = protocol.serialize_json(\n                    protocol.status_obj(['subscribesuccess', topic, self.clientid])\n                )\n                self.write_message(topic + \":\" + msg)\n            else:\n                msg = protocol.serialize_web(protocol.error_obj('unauthorized'))\n                self.write_message(topic + \":\" + msg)",
  "def __init__(self, handlers, **settings):\n        super(TornadoWebSocketApplication, self).__init__(handlers, **settings)\n        self.wsmanager = WebSocketManager()\n        zmqaddrs = settings.pop('zmqaddrs')\n        self.subscriber = Subscriber(zmqaddrs, self.wsmanager)",
  "def stop(self):\n        ## Hugo:  not sure how this is supposed to work\n        ## but apparently you need to stop and then\n        ## start the tornado loop to get it to finish....\n        ioloop.IOLoop.instance().stop()\n        self.server.stop()\n        self.subscriber.kill = True\n        self.subscriber.thread.join()\n        if hasattr(self, 'thread'):\n            self.thread.join()",
  "def start(self, thread=False):\n        def helper():\n            self.subscriber.start()\n            ioloop.IOLoop.instance().start()\n        if thread:\n            self.thread = threading.Thread(target=helper)\n            self.thread.start()\n        else:\n            helper()",
  "def set_server(self, server):\n        self.server = server",
  "def listen(self, port, address=\"\", **kwargs):\n        self.server = HTTPServer(self, **kwargs)\n        self.server.listen(port, address)",
  "def helper():\n            self.subscriber.start()\n            ioloop.IOLoop.instance().start()",
  "class Publisher(object):\n    def __init__(self, ctx, zmqaddr, queue):\n        self.ctx = ctx\n        self.zmqaddr = zmqaddr\n        self.queue = queue\n        self.kill = False\n\n    def run(self):\n        log.debug('zmqpub starting: %s' % self.zmqaddr)\n        socket = self.ctx.socket(zmq.PUB)\n        socket.connect(self.zmqaddr)\n        try:\n            while not self.kill:\n                try:\n                    message = self.queue.get(timeout=timeout)\n                    socket.send_string(str(message))\n                except Empty:\n                    pass\n        finally:\n            socket.close()\n        log.debug('zmqpub exiting')\n    def send(self, topic, msg, exclude=[]):\n        msg = json.dumps({'topic' : topic,\n                          'msg' : msg,\n                          'exclude' : list(exclude)})\n        self.queue.put(msg)\n\n    def start(self):\n        self.thread = Thread(target=self.run)\n        self.thread.start()\n\n    def stop(self):\n        self.kill = True\n        if hasattr(self, 'thread'):\n            self.thread.join()",
  "def __init__(self, ctx, zmqaddr, queue):\n        self.ctx = ctx\n        self.zmqaddr = zmqaddr\n        self.queue = queue\n        self.kill = False",
  "def run(self):\n        log.debug('zmqpub starting: %s' % self.zmqaddr)\n        socket = self.ctx.socket(zmq.PUB)\n        socket.connect(self.zmqaddr)\n        try:\n            while not self.kill:\n                try:\n                    message = self.queue.get(timeout=timeout)\n                    socket.send_string(str(message))\n                except Empty:\n                    pass\n        finally:\n            socket.close()\n        log.debug('zmqpub exiting')",
  "def send(self, topic, msg, exclude=[]):\n        msg = json.dumps({'topic' : topic,\n                          'msg' : msg,\n                          'exclude' : list(exclude)})\n        self.queue.put(msg)",
  "def start(self):\n        self.thread = Thread(target=self.run)\n        self.thread.start()",
  "def stop(self):\n        self.kill = True\n        if hasattr(self, 'thread'):\n            self.thread.join()",
  "class ManagedProcess(object):\n    def __init__(self, args, name, pidfilename,\n                 stdout=None, stdin=None, stderr=None,\n                 kill_old=True):\n        self.name = name\n        self.pidfilename = pidfilename\n        data = self.read_pidfile()\n        pid = data.get(name)\n\n        if pid and kill_old:\n            try:\n                os.kill(pid, signal.SIGINT)\n            except OSError:\n                #this is ok, just means process is not running\n                pass\n        elif pid and not kill_old:\n            raise Exception(\"process %s is running on PID %s\" % (name, pid))\n\n        try:\n            self.proc = subprocess.Popen(args, stdout=stdout, stderr=stderr, stdin=stdin)\n        except OSError as error:\n            raise OSError(error.errno, \"unable to execute: %s\" % \" \".join(args))\n\n        self.add_to_pidfile()\n        self.closed = False\n\n    def read_pidfile(self):\n        if os.path.exists(self.pidfilename):\n            with open(self.pidfilename, \"r\") as f:\n                data = json.load(f)\n        else:\n            data = {}\n        return data\n\n    def add_to_pidfile(self):\n        data = self.read_pidfile()\n        data[self.name] = self.proc.pid\n        with open(self.pidfilename, \"w+\") as f:\n            json.dump(data, f)\n\n    def remove_from_pidfile(self):\n        data = self.read_pidfile()\n        if self.name in data:\n            del data[self.name]\n        with open(self.pidfilename, \"w+\") as f:\n            json.dump(data, f)\n\n\n    def close(self):\n        if not self.closed:\n            self.proc.kill()\n            self.proc.communicate()\n            self.remove_from_pidfile()\n            self.closed = True",
  "def start_redis(pidfilename, port, data_dir, loglevel=\"warning\",\n                data_file='redis.db', save=True,\n                stdout=sys.stdout, stderr=sys.stderr):\n    base_config = os.path.join(os.path.dirname(__file__), 'redis.conf')\n    with open(base_config) as f:\n        redisconf = f.read()\n    savestr = ''\n    if save: savestr = 'save 10 1'\n    redisconf = redisconf % {'port' : port,\n                             'dbdir' : data_dir,\n                             'dbfile' : data_file,\n                             'loglevel' : loglevel,\n                             'save' : savestr}\n    mproc = ManagedProcess(['redis-server', '-'], 'redis', pidfilename,\n                           stdout=stdout,\n                           stderr=stderr,\n                           stdin=subprocess.PIPE\n                           )\n    mproc.proc.stdin.write(redisconf.encode())\n    mproc.proc.stdin.close()\n    return mproc",
  "def __init__(self, args, name, pidfilename,\n                 stdout=None, stdin=None, stderr=None,\n                 kill_old=True):\n        self.name = name\n        self.pidfilename = pidfilename\n        data = self.read_pidfile()\n        pid = data.get(name)\n\n        if pid and kill_old:\n            try:\n                os.kill(pid, signal.SIGINT)\n            except OSError:\n                #this is ok, just means process is not running\n                pass\n        elif pid and not kill_old:\n            raise Exception(\"process %s is running on PID %s\" % (name, pid))\n\n        try:\n            self.proc = subprocess.Popen(args, stdout=stdout, stderr=stderr, stdin=stdin)\n        except OSError as error:\n            raise OSError(error.errno, \"unable to execute: %s\" % \" \".join(args))\n\n        self.add_to_pidfile()\n        self.closed = False",
  "def read_pidfile(self):\n        if os.path.exists(self.pidfilename):\n            with open(self.pidfilename, \"r\") as f:\n                data = json.load(f)\n        else:\n            data = {}\n        return data",
  "def add_to_pidfile(self):\n        data = self.read_pidfile()\n        data[self.name] = self.proc.pid\n        with open(self.pidfilename, \"w+\") as f:\n            json.dump(data, f)",
  "def remove_from_pidfile(self):\n        data = self.read_pidfile()\n        if self.name in data:\n            del data[self.name]\n        with open(self.pidfilename, \"w+\") as f:\n            json.dump(data, f)",
  "def close(self):\n        if not self.closed:\n            self.proc.kill()\n            self.proc.communicate()\n            self.remove_from_pidfile()\n            self.closed = True",
  "def build_parser():\n    parser = argparse.ArgumentParser(description=\"Start the Bokeh plot server\")\n\n    # general configuration\n    general = parser.add_argument_group('General Options')\n    general.add_argument(\"--ip\",\n                         help=\"IP address that the bokeh server will listen on (default: 127.0.0.1)\",\n                         type=str,\n                         default=\"127.0.0.1\"\n                         )\n    general.add_argument(\"--port\", \"--bokeh-port\",\n                         help=\"Port that the bokeh server will listen on (default: 5006)\",\n                         type=int,\n                         default=5006\n                         )\n    general.add_argument(\"--url-prefix\",\n                         help=\"URL prefix for server. e.g. 'host:port/<prefix>/bokeh' (default: None)\",\n                         type=str\n                         )\n\n    # advanced configuration\n    advanced = parser.add_argument_group('Advanced Options')\n    advanced.add_argument(\"-D\", \"--blaze-config\",\n                          help=\"blaze_config_File\",\n                          type=str,\n                          default=None\n                          )\n    advanced.add_argument(\"-m\", \"--multi-user\",\n                          help=\"start in multi-user configuration (default: False)\",\n                          action=\"store_true\",\n                          default=False\n                          )\n    advanced.add_argument(\"--script\",\n                          help=\"script to load (for applets)\",\n                          default=None,\n                          type=str\n                          )\n\n    # storage config\n    storage = parser.add_argument_group('Storage Options')\n    storage.add_argument(\"--backend\",\n                         help=\"storage backend: [ redis | memory | shelve ], (default: %s)\" % DEFAULT_BACKEND,\n                         type=str,\n                         default=DEFAULT_BACKEND\n                         )\n    storage.add_argument(\"--redis-port\",\n                         help=\"port for redis server to listen on (default: 7001)\",\n                         type=int,\n                         default=7001\n                         )\n    storage.add_argument(\"--start-redis\",\n                         help=\"start redis\",\n                         action=\"store_true\",\n                         dest=\"start_redis\",\n                         )\n    storage.add_argument(\"--no-start-redis\",\n                         help=\"do not start redis\",\n                         action=\"store_false\",\n                         dest=\"start_redis\",\n                         )\n    parser.set_defaults(start_redis=True)\n\n    # websockets config\n    websockets = parser.add_argument_group('Websocket Options')\n    websockets.add_argument(\"--ws-conn-string\",\n                            help=\"connection string for websocket (unnecessary if auto-starting)\",\n                            default=None\n                            )\n    # dev, debugging, etc.\n    class DevAction(argparse.Action):\n        def __call__(self, parser, namespace, values, option_string=None):\n            namespace.splitjs = True\n            namespace.debugjs = True\n            namespace.backend = 'memory'\n\n    dev = parser.add_argument_group('Development Options')\n    dev.add_argument(\"-d\", \"--debug\",\n                     action=\"store_true\",\n                     default=False,\n                     help=\"use debug mode for Flask\"\n                     )\n    dev.add_argument(\"--dev\",\n                     action=DevAction,\n                     nargs=0,\n                     help=\"run server in development mode\"\n                     )\n    dev.add_argument(\"--filter-logs\",\n                     action=\"store_true\",\n                     default=False,\n                     help=\"don't show 'GET /static/... 200 OK', useful with --splitjs\")\n    dev.add_argument(\"-j\", \"--debugjs\",\n                     action=\"store_true\",\n                     default=False,\n                     help=\"serve BokehJS files from the bokehjs build directory in the source tree\"\n                     )\n    dev.add_argument(\"-s\", \"--splitjs\",\n                     action=\"store_true\",\n                     default=False,\n                     help=\"serve individual JS files instead of compiled bokeh.js, requires --debugjs\"\n                     )\n    dev.add_argument(\"--robust-reload\",\n                     help=\"protect debug server reloading from syntax errors\",\n                     default=False,\n                     action=\"store_true\",\n                     )\n    dev.add_argument(\"-v\", \"--verbose\",\n                     action=\"store_true\",\n                     default=False\n                     )\n\n    return parser",
  "def run():\n    parser = build_parser()\n    args = parser.parse_args(sys.argv[1:])\n\n    level = logging.DEBUG if args.debug else logging.INFO\n    # TODO: this does nothing - because bokeh/__init__.py is already imported\n    # and basicConfig was already called\n    logging.basicConfig(level=level, format=\"%(asctime)s:%(levelname)s:%(name)s:%(message)s\")\n\n\n    backend_options = args.backend\n    if backend_options == 'redis':\n        if args.start_redis:\n            backend_options += \" (start=%s, port=%d)\" % (args.start_redis, args.redis_port)\n        else:\n            backend_options += \" (start=False)\"\n\n    onoff = {True:\"ON\", False:\"OFF\"}\n\n    py_options = \", \".join(\n        name.replace('_', '-') + \":\" + onoff[vars(args).get(name)] for name in ['debug', 'verbose', 'filter_logs', 'multi_user']\n    )\n    js_options = \", \".join(\n        name + \":\" + onoff[vars(args).get(name)]for name in ['splitjs', 'debugjs']\n    )\n\n    if not args.debug or os.environ.get('WERKZEUG_RUN_MAIN') == 'true':\n        print(\"\"\"\n    Bokeh Server Configuration\n    ==========================\n    python version : %s\n    bokeh version  : %s\n    listening      : %s:%d\n    backend        : %s\n    python options : %s\n    js options     : %s\n    \"\"\" % (\n        sys.version.split()[0], __version__,\n        args.ip, args.port,\n        backend_options,\n        py_options,\n        js_options,\n    ))\n\n    settings.debugjs = args.debugjs\n    start_server(args)",
  "def start_server(args):\n    from . import start\n    start.start_simple_server(args)",
  "def start_with_reloader(args, js_files, robust):\n    def helper():\n        start_server(args)\n    if robust:\n        helper = robust_reloader(helper)\n    werkzeug.serving.run_with_reloader(\n        helper, extra_files=js_files)",
  "class DevAction(argparse.Action):\n        def __call__(self, parser, namespace, values, option_string=None):\n            namespace.splitjs = True\n            namespace.debugjs = True\n            namespace.backend = 'memory'",
  "def helper():\n        start_server(args)",
  "def __call__(self, parser, namespace, values, option_string=None):\n            namespace.splitjs = True\n            namespace.debugjs = True\n            namespace.backend = 'memory'",
  "def doc_prepare():\n    server_settings.model_backend = {'type' : 'memory'}\n    configure_flask()\n    register_blueprint()\n    return app",
  "def start_redis():\n    work_dir = getattr(bokeh_app, 'work_dir', os.getcwd())\n    data_file = getattr(bokeh_app, 'data_file', 'redis.db')\n    stdout = getattr(bokeh_app, 'stdout', sys.stdout)\n    stderr = getattr(bokeh_app, 'stdout', sys.stderr)\n    redis_save = getattr(bokeh_app, 'redis_save', True)\n    mproc = services.start_redis(pidfilename=os.path.join(work_dir, \"bokehpids.json\"),\n                                 port=bokeh_app.backend.get('redis_port', 6379),\n                                 data_dir=work_dir,\n                                 data_file=data_file,\n                                 stdout=stdout,\n                                 stderr=stderr,\n                                save=redis_save)\n    bokeh_app.redis_proc = mproc",
  "def make_tornado(config_file=None):\n    configure_flask(config_file=config_file)\n    register_blueprint()\n    tornado_app = make_tornado_app(flask_app=app)\n    return tornado_app",
  "def start_simple_server(args=None):\n    global server\n    configure_flask(config_argparse=args)\n    if server_settings.model_backend.get('start-redis', False):\n        start_redis()\n    register_blueprint()\n    tornado_app = make_tornado_app(flask_app=app)\n    server = HTTPServer(tornado_app)\n    server.listen(server_settings.port, server_settings.ip)\n    ioloop.IOLoop.instance().start()",
  "def stop():\n    if hasattr(bokeh_app, 'redis_proc'):\n        bokeh_app.redis_proc.close()\n    server.stop()\n    bokehapp = server.request_callback\n    bokehapp.stop_threads()\n    ioloop.IOLoop.instance().stop()\n    ioloop.IOLoop.instance().clear_instance()",
  "def configure_flask(config_argparse=None, config_file=None, config_dict=None):\n    if config_argparse:\n        server_settings.from_args(config_argparse)\n    if config_dict:\n        server_settings.from_dict(config_dict)\n    if config_file:\n        server_settings.from_file(config_file)\n    for handler in logging.getLogger().handlers:\n        handler.addFilter(StaticFilter())\n    # must import views before running apps\n    from .views import deps\n    # this just shuts up pyflakes\n    deps\n    backend = server_settings.model_backend\n    if backend['type'] == 'redis':\n        import redis\n        rhost = backend.get('redis_host', '127.0.0.1')\n        rport = backend.get('redis_port', REDIS_PORT)\n        bbstorage = RedisBackboneStorage(redis.Redis(host=rhost, port=rport, db=2))\n        servermodel_storage = RedisServerModelStorage(redis.Redis(host=rhost,\n                                                                  port=rport, db=3))\n    elif backend['type'] == 'memory':\n        bbstorage = InMemoryBackboneStorage()\n        servermodel_storage = InMemoryServerModelStorage()\n\n    elif backend['type'] == 'shelve':\n        bbstorage = ShelveBackboneStorage()\n        servermodel_storage = ShelveServerModelStorage()\n\n    if not server_settings.multi_user:\n        authentication = SingleUserAuthentication()\n    else:\n        authentication = MultiUserAuthentication()\n    bokeh_app.url_prefix = server_settings.url_prefix\n    bokeh_app.publisher = Publisher(server_settings.ctx, server_settings.pub_zmqaddr, Queue())\n\n    for script in server_settings.scripts:\n        script_dir = dirname(script)\n        if script_dir not in sys.path:\n            print (\"adding %s to python path\" % script_dir)\n            sys.path.append(script_dir)\n        print (\"importing %s\" % script)\n        imp.load_source(\"_bokeh_app\", script)\n\n    #todo - push some of this into bokeh_app.setup?\n    bokeh_app.setup(\n        backend,\n        bbstorage,\n        servermodel_storage,\n        authentication,\n    )",
  "def register_blueprint():\n    global registered\n    if registered:\n        warnings.warn(\n            \"register_blueprint has already been called, why is it being called again\"\n        )\n        return\n    blaze_blueprint = get_mbs_blueprint(config_file=server_settings.blaze_config)\n    app.register_blueprint(bokeh_app, url_prefix=server_settings.url_prefix)\n    if blaze_blueprint:\n        app.register_blueprint(blaze_blueprint, url_prefix=server_settings.url_prefix)\n    registered = True",
  "class SimpleBokehTornadoApp(Application):\n    def __init__(self, flask_app, **settings):\n        self.flask_app = flask_app\n        tornado_flask = WSGIContainer(flask_app)\n        url_prefix = server_settings.url_prefix\n        handlers = [\n            (url_prefix + \"/bokeh/sub\", websocket.WebSocketHandler),\n            (r\".*\", FallbackHandler, dict(fallback=tornado_flask))\n        ]\n        super(SimpleBokehTornadoApp, self).__init__(handlers, **settings)\n        self.wsmanager = websocket.WebSocketManager()\n        def auth(auth, docid):\n            #HACKY\n            if docid.startswith(\"temporary-\"):\n                return True\n            doc = docs.Doc.load(bokeh_app.servermodel_storage, docid)\n            status = mconv.can_read_doc_api(doc, auth)\n            return status\n        self.wsmanager.register_auth('bokehplot', auth)\n\n        self.subscriber = Subscriber(server_settings.ctx,\n                                     [server_settings.sub_zmqaddr],\n                                     self.wsmanager)\n        if server_settings.run_forwarder:\n            self.forwarder = Forwarder(server_settings.ctx, server_settings.pub_zmqaddr, server_settings.sub_zmqaddr)\n        else:\n            self.forwarder = None\n\n    def start_threads(self):\n        bokeh_app.publisher.start()\n        self.subscriber.start()\n        if self.forwarder:\n            self.forwarder.start()\n\n    def stop_threads(self):\n        bokeh_app.publisher.stop()\n        self.subscriber.stop()\n        if self.forwarder:\n            self.forwarder.stop()",
  "class StaticFilter(logging.Filter):\n    def filter(self, record):\n        msg = record.getMessage()\n        return not (msg.startswith((\"200 GET /static\", \"200 GET /bokehjs/static\")))",
  "def make_tornado_app(flask_app=None):\n    if flask_app is None:\n        flask_app = app\n    if server_settings.debug:\n        flask_app.debug = True\n    flask_app.secret_key = server_settings.secret_key\n    tornado_app = SimpleBokehTornadoApp(flask_app, debug=server_settings.debug)\n    tornado_app.start_threads()\n    return tornado_app",
  "def __init__(self, flask_app, **settings):\n        self.flask_app = flask_app\n        tornado_flask = WSGIContainer(flask_app)\n        url_prefix = server_settings.url_prefix\n        handlers = [\n            (url_prefix + \"/bokeh/sub\", websocket.WebSocketHandler),\n            (r\".*\", FallbackHandler, dict(fallback=tornado_flask))\n        ]\n        super(SimpleBokehTornadoApp, self).__init__(handlers, **settings)\n        self.wsmanager = websocket.WebSocketManager()\n        def auth(auth, docid):\n            #HACKY\n            if docid.startswith(\"temporary-\"):\n                return True\n            doc = docs.Doc.load(bokeh_app.servermodel_storage, docid)\n            status = mconv.can_read_doc_api(doc, auth)\n            return status\n        self.wsmanager.register_auth('bokehplot', auth)\n\n        self.subscriber = Subscriber(server_settings.ctx,\n                                     [server_settings.sub_zmqaddr],\n                                     self.wsmanager)\n        if server_settings.run_forwarder:\n            self.forwarder = Forwarder(server_settings.ctx, server_settings.pub_zmqaddr, server_settings.sub_zmqaddr)\n        else:\n            self.forwarder = None",
  "def start_threads(self):\n        bokeh_app.publisher.start()\n        self.subscriber.start()\n        if self.forwarder:\n            self.forwarder.start()",
  "def stop_threads(self):\n        bokeh_app.publisher.stop()\n        self.subscriber.stop()\n        if self.forwarder:\n            self.forwarder.stop()",
  "def filter(self, record):\n        msg = record.getMessage()\n        return not (msg.startswith((\"200 GET /static\", \"200 GET /bokehjs/static\")))",
  "def auth(auth, docid):\n            #HACKY\n            if docid.startswith(\"temporary-\"):\n                return True\n            doc = docs.Doc.load(bokeh_app.servermodel_storage, docid)\n            status = mconv.can_read_doc_api(doc, auth)\n            return status",
  "def _make_range(r):\n    \"\"\"Create a range from the start/end values passed.\n       This function is required because some BokehJS Range objects\n       have ids but some don't and some have docs but some don't...\n       so this is sort of a #Hack....\n\n       This may be removed when a better plot_state mechanism is created.\n    \"\"\"\n    return Range1d(start=r['start'], end=r['end'])",
  "def render(docid, datasourceid, glyphid):\n    #load bokeh document\n    bokehuser = bokeh_app.authentication.current_user()\n    request_username = bokehuser.username\n    clientdoc = bokeh_app.backbone_storage.get_document(docid)\n    prune(clientdoc)\n    #\n\n    #init plotting.py\n    init_bokeh(clientdoc)\n    serverdatasource = clientdoc._models[datasourceid]\n    glyph = clientdoc._models[glyphid]\n    parameters = serverdatasource.transform\n    json_data = request.json\n    json_data['expr'] = serverdatasource.expr\n    json_data['namespace'] = serverdatasource.namespace\n    plot_state = json_data['plot_state']\n    render_state = json_data.get('render_state', None)\n    auto_bounds = json_data.get('auto_bounds', False)\n\n    #convert json objects into actual range objects (hacky!)\n    plot_state=dict([(k, _make_range(r)) for k,r in iteritems(plot_state)])\n\n    #compute blaze data using the blaze server blueprint\n    expr, result = _compserver(json_data)\n\n    #convert blaze server output into other dataframe or numpy\n    data_type = parameters.get('type', 'DataFrame')\n    if  data_type == 'DataFrame':\n        data = into(pd.DataFrame, result)\n    elif data_type == 'ndarray':\n        data = into(np.ndarray, result)\n\n    #call downsampling\n    resample_op = serverdatasource.transform['resample']\n    if resample_op == 'abstract rendering':\n        result = ar_downsample.downsample(\n            data,\n            serverdatasource,\n            glyph,\n            plot_state,\n            render_state,\n            auto_bounds,\n        )\n    elif resample_op == 'line1d':\n        result = line1d_downsample(\n            data,\n            serverdatasource,\n            glyph,\n            plot_state,\n            render_state,\n            auto_bounds,\n        )\n    elif resample_op == 'heatmap':\n        result = heatmap_downsample(\n            data,\n            serverdatasource,\n            glyph,\n            plot_state,\n            render_state,\n            auto_bounds,\n        )\n\n\n    #return results\n    result = make_json(protocol.serialize_json(result))\n    return result",
  "def convert_range_to_time(range_obj):\n    #assume millis from javascript\n    if isinstance(range_obj.start, int):\n        range_obj.start = dt.datetime.fromtimestamp(range_obj.start / 1000.0)\n    if isinstance(range_obj.end, int):\n        range_obj.end = dt.datetime.fromtimestamp(range_obj.end / 1000.0)",
  "def line1d_downsample(raw_data, data_source, glyph, plot_state,\n                      render_state, auto_bounds):\n    domain_name = glyph.x['field']\n    range_name = glyph.y['field']\n    domain_col = raw_data[domain_name]\n    range_col = raw_data[range_name]\n\n    if auto_bounds:\n        plot_state['data_x'].start = domain_col.min()\n        plot_state['data_x'].end = domain_col.max()\n        plot_state['data_y'].start = range_col.min()\n        plot_state['data_y'].end = range_col.max()\n    if domain_col.dtype.kind == \"M\":\n        convert_range_to_time(plot_state['data_x'])\n    if range_col.dtype.kind == \"M\":\n        convert_range_to_time(plot_state['data_y'])\n    if data_source.transform.get('direction', 'x') == 'x':\n        domain_r = plot_state['data_x']\n        range_r = plot_state['data_y']\n        domain_screen_r = plot_state['screen_x']\n    else:\n        raise NotImplementedError\n    screen_d_span = _span(domain_screen_r)\n    data_d_span = _span(domain_r)\n    data_r_span = _span(range_r)\n    domain_limit = [domain_r.start, domain_r.end]\n    if domain_col.dtype.kind == \"M\":\n        domain_limit = np.array(domain_limit).astype('datetime64[ms]')\n    raw_data = raw_data[(domain_col > domain_limit[0]) & (domain_col < domain_limit[1])]\n    result = line_downsample.downsample(raw_data.to_records(),\n                                        domain_name,\n                                        range_name,\n                                        domain_limit,\n                                        data_r_span,\n                                        screen_d_span,\n                                        'minmax')\n    result['x_range'] = {'start': plot_state['data_x'].start,\n                         'end': plot_state['data_x'].end}\n    result['y_range'] = {'start': plot_state['data_y'].start,\n                         'end': plot_state['data_y'].end}\n    return result",
  "def heatmap_downsample(raw_data, data_source, glyph, plot_state,\n                       render_state, auto_bounds):\n    x_r = plot_state['data_x']\n    y_r = plot_state['data_y']\n\n    screen_x_r = plot_state['screen_x']\n    screen_y_r = plot_state['screen_x']\n    x_resolution = float(_span(screen_x_r))\n    y_resolution = float(_span(screen_y_r))\n\n    global_x_range = data_source.transform['global_x_range']\n    global_y_range = data_source.transform['global_y_range']\n    global_offset_x = data_source.transform.get('global_offset_x', 0)\n    global_offset_y = data_source.transform.get('global_offset_y', 0)\n\n    image_x_axis = np.linspace(global_x_range[0],\n                               global_x_range[1],\n                               raw_data.shape[1])\n    image_y_axis = np.linspace(global_y_range[0],\n                               global_y_range[1],\n                               raw_data.shape[0])\n    result = image_downsample.downsample(\n        raw_data, image_x_axis, image_y_axis,\n        plot_state['data_x'], plot_state['data_y'], x_resolution,\n        y_resolution)\n    output = result\n    return output",
  "def get_blueprint(config_file=None):\n    retval = None\n    try:\n        from . import views\n        import mbs.app\n        retval = mbs.app.mbsbp\n        mbs.app.setup_app(config_file=config_file)\n    except ImportError as e:\n        msg = \"could not import multiuser blaze server %s.  This is fine if you do not intend to use blaze capabilities in the bokeh server\"\n        msg = msg % str(e)\n        warnings.warn(msg)\n    else:\n        return retval",
  "def bokehjs_file(filename):\n    \"\"\" Return a specific BokehJS deployment file\n\n    :param filename: name of the file to retrieve\n\n    :status 200: file is found\n    :status 404: file is not found\n\n    \"\"\"\n    return flask.send_from_directory(bokeh_app.bokehjsdir, filename)",
  "def bokehjssrc_file(filename):\n    \"\"\" Return a specific BokehJS source code file\n\n    :param filename: name of the file to retrieve\n\n    :status 200: file is found\n    :status 404: file is not found\n\n    \"\"\"\n    return flask.send_from_directory(bokeh_app.bokehjssrcdir, filename)",
  "def init_bokeh(clientdoc):\n    request.bokeh_server_document = clientdoc\n    clientdoc.autostore = False\n    clientdoc.autoadd = False",
  "def gc(docid):\n    client = request.headers.get('client', 'python')\n    doc = docs.Doc.load(bokeh_app.servermodel_storage, docid)\n    bokehuser = bokeh_app.current_user()\n    temporary_docid = get_temporary_docid(request, docid)\n    t = BokehServerTransaction(\n        bokehuser, doc, 'rw', temporary_docid=temporary_docid\n    )\n    t.load(gc=True)\n    t.save()\n    return jsonify(status='success')",
  "def bulk_upsert(docid):\n    ''' Update or insert new objects for a given :class:`Document <bokeh.document.Document>`.\n\n    :param docid: id of the :class:`Document <bokeh.document.Document>`\n        to update or insert into\n\n    :status 200: when user is authorized\n    :status 401: when user is not authorized\n\n    '''\n    # endpoint is only used by python, therefore we don't process\n    # callbacks here\n    client = request.headers.get('client', 'python')\n    doc = docs.Doc.load(bokeh_app.servermodel_storage, docid)\n    bokehuser = bokeh_app.current_user()\n    temporary_docid = get_temporary_docid(request, docid)\n    t = BokehServerTransaction(\n        bokehuser, doc, 'rw', temporary_docid=temporary_docid\n    )\n    t.load()\n    clientdoc = t.clientdoc\n    data = protocol.deserialize_json(request.data.decode('utf-8'))\n    if client == 'python':\n        clientdoc.load(*data, events='none', dirty=True)\n    else:\n        clientdoc.load(*data, events='existing', dirty=True)\n    t.save()\n    msg = ws_update(clientdoc, t.write_docid, t.changed)\n    return make_json(msg)",
  "def ws_update(clientdoc, docid, models):\n    log.debug(\"sending wsupdate to %s\", docid)\n    attrs = clientdoc.dump(*models)\n    msg = protocol.serialize_json({'msgtype' : 'modelpush',\n                                   'modelspecs' : attrs\n                               })\n    bokeh_app.publisher.send(\"bokehplot:\" + docid, msg)\n    return msg",
  "def ws_delete(clientdoc, docid, models):\n    attrs = clientdoc.dump(*models)\n    msg = {\n        'msgtype'    : 'modeldel',\n        'modelspecs' : attrs,\n    }\n    msg = protocol.serialize_json(msg)\n    bokeh_app.wsmanager.send(\"bokehplot:\" + docid, msg)\n    return msg",
  "def create(docid, typename):\n    ''' Update or insert new objects for a given :class:`Document <bokeh.document.Document>`.\n\n    :param docid: id of the :class:`Document <bokeh.document.Document>`\n        to update or insert into\n\n    :status 200: when user is authorized\n    :status 401: when user is not authorized\n\n    '''\n    doc = docs.Doc.load(bokeh_app.servermodel_storage, docid)\n    bokehuser = bokeh_app.current_user()\n    temporary_docid = get_temporary_docid(request, docid)\n    t = BokehServerTransaction(\n        bokehuser, doc, 'rw', temporary_docid=temporary_docid\n    )\n    t.load()\n    modeldata = protocol.deserialize_json(request.data.decode('utf-8'))\n    modeldata = [{'type' : typename,\n                  'attributes' : modeldata}]\n    clientdoc.load(*modeldata, dirty=True)\n    t.save()\n    ws_update(clientdoc, t.write_docid, modeldata)\n    return protocol.serialize_json(modeldata[0]['attributes'])",
  "def _bulkget(docid, typename=None):\n    doc = docs.Doc.load(bokeh_app.servermodel_storage, docid)\n    bokehuser = bokeh_app.current_user()\n    temporary_docid = get_temporary_docid(request, docid)\n    t = BokehServerTransaction(\n        bokehuser, doc, 'r', temporary_docid=temporary_docid\n    )\n    t.load()\n    clientdoc = t.clientdoc\n    all_models = clientdoc._models.values()\n    if typename is not None:\n        attrs = clientdoc.dump(*[x for x in all_models \\\n                                 if x.__view_model__==typename])\n        attrs = [x['attributes'] for x in attrs]\n        return make_json(protocol.serialize_json(attrs))\n    else:\n        attrs = clientdoc.dump(*all_models)\n        return make_json(protocol.serialize_json(attrs))",
  "def bulkget_without_typename(docid):\n    ''' Retrieve all objects for a given :class:`Document <bokeh.document.Document>`.\n\n    :param docid: id of the :class:`Document <bokeh.document.Document>`\n        to update or insert into\n\n    :status 200: when user is authorized\n    :status 401: when user is not authorized\n\n    '''\n    return _bulkget(docid)",
  "def bulkget_with_typename(docid):\n    ''' Retrieve all objects of a specified typename for a\n    given :class:`Document <bokeh.document.Document>`.\n\n    :param docid: id of the :class:`Document <bokeh.document.Document>`\n        to update or insert into\n    :param typename: the type of objects to find and return\n\n    :status 200: when user is authorized\n    :status 401: when user is not authorized\n\n    '''\n    return _bulkget(docid, typename)",
  "def _handle_specific_model(docid, typename, id, method):\n    if method == 'PUT':\n        return update(docid, typename, id)\n    elif method == 'PATCH':\n        return update(docid, typename, id)\n    elif method == 'GET':\n        return getbyid(docid, typename, id)\n    elif method == 'DELETE':\n        return delete(docid, typename, id)",
  "def _handle_specific_model_get(docid, typename, id):\n    ''' Retrieve a specific model with a given id and typename for a\n    given :class:`Document <bokeh.document.Document>`.\n\n    :param docid: id of the :class:`Document <bokeh.document.Document>`\n        to update or insert into\n    :param typename: the type of objects to find and return\n    :param id: unique id of the object to retrieve\n\n    :status 200: when user is authorized\n    :status 401: when user is not authorized\n\n    '''\n    return _handle_specific_model(docid, typename, id, request.method)",
  "def _handle_specific_model_put(docid, typename, id):\n    ''' Update a specific model with a given id and typename for a\n    given :class:`Document <bokeh.document.Document>`.\n\n    :param docid: id of the :class:`Document <bokeh.document.Document>`\n        to update or insert into\n    :param typename: the type of objects to find and return\n    :param id: unique id of the object to retrieve\n\n    :status 200: when user is authorized\n    :status 401: when user is not authorized\n\n    '''\n    return _handle_specific_model(docid, typename, id, request.method)",
  "def _handle_specific_model_patch(docid, typename, id):\n    ''' Update a specific model with a given id and typename for a\n    given :class:`Document <bokeh.document.Document>`.\n\n    :param docid: id of the :class:`Document <bokeh.document.Document>`\n        to update or insert into\n    :param typename: the type of objects to find and return\n    :param id: unique id of the object to retrieve\n\n    :status 200: when user is authorized\n    :status 401: when user is not authorized\n\n    '''\n    return _handle_specific_model(docid, typename, id, request.method)",
  "def _handle_specific_model_delete(docid, typename, id):\n    ''' Delete a specific model with a given id and typename for a\n    given :class:`Document <bokeh.document.Document>`.\n\n    :param docid: id of the :class:`Document <bokeh.document.Document>`\n        to update or insert into\n    :param typename: the type of objects to find and return\n    :param id: unique id of the object to retrieve\n\n    :status 200: when user is authorized\n    :status 401: when user is not authorized\n\n    '''\n    return _handle_specific_model(docid, typename, id, request.method)",
  "def getbyid(docid, typename, id):\n    doc = docs.Doc.load(bokeh_app.servermodel_storage, docid)\n    bokehuser = bokeh_app.current_user()\n    temporary_docid = get_temporary_docid(request, docid)\n    t = BokehServerTransaction(\n        bokehuser, doc, 'r', temporary_docid=temporary_docid\n    )\n    t.load()\n    clientdoc = t.clientdoc\n    attr = clientdoc.dump(clientdoc._models[id])[0]['attributes']\n    return make_json(protocol.serialize_json(attr))",
  "def update(docid, typename, id):\n    \"\"\"we need to distinguish between writing and patching models\n    namely in writing, we shouldn't remove unspecified attrs\n    (we currently don't handle this correctly)\n    \"\"\"\n    doc = docs.Doc.load(bokeh_app.servermodel_storage, docid)\n    bokehuser = bokeh_app.current_user()\n    temporary_docid = get_temporary_docid(request, docid)\n    t = BokehServerTransaction(\n        bokehuser, doc, 'rw', temporary_docid=temporary_docid\n    )\n    t.load()\n    modeldata = protocol.deserialize_json(request.data.decode('utf-8'))\n    ### horrible hack, we need to pop off the noop object if it exists\n    modeldata.pop('noop', None)\n    clientdoc = t.clientdoc\n    log.info(\"loading done %s\", len(clientdoc._models.values()))\n    # patch id is not passed...\n    modeldata['id'] = id\n    modeldata = {'type' : typename,\n                 'attributes' : modeldata}\n    clientdoc.load(modeldata, events='existing', dirty=True)\n    t.save()\n    ws_update(clientdoc, t.write_docid, t.changed)\n    # backbone expects us to send back attrs of this model, but it doesn't\n    # make sense to do so because we modify other models, and we want this to\n    # all go out over the websocket channel\n    return make_json(protocol.serialize_json({'noop' : True}))",
  "def delete(docid, typename, id):\n    #I don't think this works right now\n    doc = docs.Doc.load(bokeh_app.servermodel_storage, docid)\n    bokehuser = bokeh_app.current_user()\n    temporary_docid = get_temporary_docid(request, docid)\n    t = BokehServerTransaction(\n        bokehuser, doc, 'rw', temporary_docid=temporary_docid\n    )\n    model = t.clientdoc._models[id]\n    bokeh_app.backbone_storage.del_obj(t.write_docid, obj)\n    t.save()\n    ws_delete(clientdoc, t.write_docid, [model])\n    return make_json(protocol.serialize_json(clientdoc.dump(model)[0]['attributes']))",
  "def login_required(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        if not bokeh_app.current_user():\n            return abort(401, \"You must be logged in\")\n        return func(*args, **kwargs)\n    return wrapper",
  "def wrapper(*args, **kwargs):\n        if not bokeh_app.current_user():\n            return abort(401, \"You must be logged in\")\n        return func(*args, **kwargs)",
  "def request_resources():\n    \"\"\"Creates resources instance based on url info from\n    current app/request context\n    \"\"\"\n    if bokeh_app.url_prefix:\n        # strip of leading slash\n        root_url  = request.url_root + bokeh_app.url_prefix[1:]\n    else:\n        root_url  = request.url_root\n    resources = Resources(root_url=root_url, mode='server')\n    return resources",
  "def render(fname, **kwargs):\n    resources = request_resources()\n    bokeh_prefix = resources.root_url\n    return render_template(fname, bokeh_prefix=bokeh_prefix,\n                           **kwargs)",
  "def ping():\n    ''' Test whether Bokeh server is up.\n\n    :status 200:\n\n    '''\n    # test route, to know if the server is up\n    return \"pong\"",
  "def index(*unused_all, **kwargs):\n    ''' Render main page.\n\n    :status 200: if current user logged in\n    :status 302: otherwise redirect to login\n\n    '''\n    bokehuser = bokeh_app.current_user()\n    if not bokehuser:\n        return redirect(url_for('.login_get'))\n    return render('bokeh.html',\n                  splitjs=server_settings.splitjs,\n                  username=bokehuser.username,\n                  title=\"Bokeh Documents for %s\" % bokehuser.username\n    )",
  "def favicon():\n    ''' Return favicon.\n\n    :status 200: return favicon\n\n    '''\n    return send_from_directory(os.path.join(bokeh_app.root_path, 'static'),\n                               'favicon.ico', mimetype='image/x-icon')",
  "def _makedoc(redisconn, u, title):\n    docid = str(uuid.uuid4())\n    if isinstance(u, string_types):\n        u = user.User.load(redisconn, u)\n    clientdoc = bokeh_app.backbone_storage.get_document(docid)\n    prune(clientdoc)\n    u.add_doc(docid, title)\n    doc = docs.new_doc(bokeh_app, docid,\n                       title, clientdoc,\n                       rw_users=[u.username])\n    u.save(redisconn)\n    bokeh_app.backbone_storage.store_document(clientdoc)\n    return doc",
  "def makedoc():\n    if request.json:\n        title = request.json['title']\n    else:\n        title = request.values['title']\n    bokehuser = bokeh_app.current_user()\n    try:\n        _makedoc(bokeh_app.servermodel_storage, bokehuser, title)\n    except DataIntegrityException as e:\n        return abort(409, e.message)\n    jsonstring = protocol.serialize_web(bokehuser.to_public_json())\n    msg = protocol.serialize_web({'msgtype' : 'docchange'})\n    bokeh_app.publisher.send(\"bokehuser:\" + bokehuser.username, msg)\n    return make_json(jsonstring)",
  "def deletedoc(docid):\n    bokehuser = bokeh_app.current_user()\n    try:\n        bokehuser.remove_doc(docid)\n        bokehuser.save(bokeh_app.servermodel_storage)\n    except DataIntegrityException as e:\n        return abort(409, e.message)\n    jsonstring = protocol.serialize_web(bokehuser.to_public_json())\n    msg = protocol.serialize_web({'msgtype' : 'docchange'})\n    bokeh_app.publisher.send(\"bokehuser:\" + bokehuser.username, msg)\n    return make_json(jsonstring)",
  "def get_doc_api_key(docid):\n    doc = docs.Doc.load(bokeh_app.servermodel_storage, docid)\n    bokehuser = bokeh_app.current_user()\n    t = BokehServerTransaction(bokehuser, doc, 'auto')\n    if t.mode == 'rw':\n        return jsonify({'apikey' : t.server_docobj.apikey})\n    else:\n        return jsonify({'readonlyapikey' : t.server_docobj.readonlyapikey})",
  "def get_user():\n    bokehuser = bokeh_app.current_user()\n    content = protocol.serialize_web(bokehuser.to_public_json())\n    return make_json(content)",
  "def get_bokeh_info(docid):\n    doc = docs.Doc.load(bokeh_app.servermodel_storage, docid)\n    bokehuser = bokeh_app.current_user()\n    temporary_docid = get_temporary_docid(request, docid)\n    t = BokehServerTransaction(bokehuser, doc, 'r',\n                               temporary_docid=temporary_docid\n    )\n    t.load()\n    clientdoc = t.clientdoc\n    all_models = clientdoc._models.values()\n    log.info(\"num models: %s\", len(all_models))\n    all_models = clientdoc.dump(*all_models)\n    returnval = {'plot_context_ref' : doc.plot_context_ref,\n                 'docid' : docid,\n                 'all_models' : all_models,\n                 'apikey' : t.apikey}\n    returnval = protocol.serialize_json(returnval)\n    #i don't think we need to set the header here...\n    result = make_json(returnval,\n                       headers={\"Access-Control-Allow-Origin\": \"*\"})\n    return result",
  "def show_doc_by_title(title):\n    bokehuser = bokeh_app.current_user()\n    docs = [ doc for doc in bokehuser.docs if doc['title'] == title ]\n    doc = docs[0] if len(docs) != 0 else abort(404)\n    docid = doc['docid']\n    return render('show.html', title=title, docid=docid, splitjs=server_settings.splitjs)",
  "def doc_by_title():\n    if request.json:\n        title = request.json['title']\n    else:\n        title = request.values['title']\n    bokehuser = bokeh_app.current_user()\n    docs = [doc for doc in bokehuser.docs if doc['title'] == title]\n    if len(docs) == 0:\n        try:\n            doc = _makedoc(bokeh_app.servermodel_storage, bokehuser, title)\n            docid = doc.docid\n        except DataIntegrityException as e:\n            return abort(409, e.message)\n        msg = protocol.serialize_web({'msgtype' : 'docchange'})\n        bokeh_app.publisher.send(\"bokehuser:\" + bokehuser.username, msg)\n    else:\n        doc = docs[0]\n        docid = doc['docid']\n    return get_bokeh_info(docid)",
  "def sampleerror():\n    return 1 + \"sdf\"",
  "def autoload_js(elementid):\n    ''' Return autoload script for given elementid\n\n    :param elementid: DOM element ID to target\n\n    :status 200: return script\n\n    '''\n    resources = request_resources()\n    rendered = AUTOLOAD.render(\n        js_url = resources.js_files[0],\n        css_files = resources.css_files,\n        elementid = elementid,\n    )\n    return Response(rendered, 200,\n                    {'Content-Type':'application/javascript'})",
  "def get_bokeh_info_one_object(docid, objid):\n    doc = docs.Doc.load(bokeh_app.servermodel_storage, docid)\n    bokehuser = bokeh_app.current_user()\n    temporary_docid = get_temporary_docid(request, docid)\n    t = BokehServerTransaction(\n        bokehuser, doc, 'r', temporary_docid=temporary_docid\n    )\n    t.load()\n    clientdoc = t.clientdoc\n    obj = clientdoc._models[objid]\n    objs = obj.references()\n    all_models = clientdoc.dump(*objs)\n    returnval = {'plot_context_ref' : doc.plot_context_ref,\n                 'docid' : docid,\n                 'all_models' : all_models,\n                 'apikey' : t.apikey,\n                 'type' : obj.__view_model__\n    }\n    returnval = protocol.serialize_json(returnval)\n    result = make_json(returnval,\n                       headers={\"Access-Control-Allow-Origin\": \"*\"})\n    return result",
  "def show_obj(docid, objid):\n    bokehuser = bokeh_app.current_user()\n    doc = docs.Doc.load(bokeh_app.servermodel_storage, docid)\n    if not bokehuser and not doc.public:\n        return redirect(url_for(\".login_get\", next=request.url))\n    resources = request_resources()\n    public = request.values.get('public', 'false').lower() == 'true'\n    if public:\n        public = 'true'\n    else:\n        public = 'false'\n    return render(\"oneobj.html\",\n                  elementid=str(uuid.uuid4()),\n                  docid=docid,\n                  objid=objid,\n                  public=public,\n                  hide_navbar=True,\n                  splitjs=server_settings.splitjs,\n                  username=bokehuser.username,\n                  loglevel=resources.log_level)",
  "def wsurl():\n    if server_settings.ws_conn_string:\n        return server_settings.ws_conn_string\n    if request.scheme == \"http\":\n        scheme = 'ws'\n    else:\n        scheme = 'wss'\n    url = \"%s://%s%s\" % (scheme,\n                         request.host,\n                         server_settings.url_prefix + \"/bokeh/sub\")\n    return url",
  "def generatejs(parentname, modulename, classname):\n    return render_template(\n        \"app.js\",\n        modulename=modulename,\n        classname=classname,\n        parentname=parentname\n    )",
  "def make_json(jsonstring, status_code=200, headers={}):\n    \"\"\" Like jsonify, except accepts string, so we can do our own custom\n    json serialization.  should move this to continuumweb later\n    \"\"\"\n    return current_app.response_class(\n        response=jsonstring,\n        status=status_code,\n        headers=headers,\n        mimetype='application/json'\n    )",
  "def handle_auth_error(func):\n    \"\"\"Decorator wraps a function and watches for AuthenticationException\n    If one is thrown, log and abort 401 instead\n    \"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except AuthenticationException as e:\n            logger.exception(e)\n            return abort(401)\n    return wrapper",
  "def check_read_authentication(func):\n    @wraps(func)\n    def wrapper(docid, *args, **kwargs):\n        if bokeh_app.authentication.can_read_doc(docid):\n            return func(docid, *args, **kwargs)\n        else:\n            abort(401)\n    return wrapper",
  "def check_write_authentication(func):\n    @wraps(func)\n    def wrapper(docid, *args, **kwargs):\n        if bokeh_app.authentication.can_write_doc(docid):\n            return func(docid, *args, **kwargs)\n        else:\n            abort(401)\n    return wrapper",
  "def login_get():\n    ''' Log in a user from a form.\n\n    :status 200: render login view\n\n    '''\n    return bokeh_app.authentication.login_get()",
  "def login_post():\n    ''' Log in user from a submission.\n\n    :status 200: if API flag set, log in status\n    :status 302: if API flag not set, redirect to index on\n        success, to login on failue\n\n    '''\n    return bokeh_app.authentication.login_post()",
  "def login_from_apikey():\n    ''' Log in a user from an API key.\n\n    :status 302: redirect to index on success, to login on failure\n\n    '''\n    return bokeh_app.authentication.login_from_apikey()",
  "def register_get():\n    ''' Register a new user via a view.\n\n    :status 200: render registration form\n\n    '''\n    return bokeh_app.authentication.register_get()",
  "def register_post():\n    ''' Register a new user via a submission.\n\n    :status 200: registration result\n\n    '''\n    return bokeh_app.authentication.register_post()",
  "def logout():\n    ''' Log out the current user.\n\n    :status 302: redirect to index\n\n    '''\n    return bokeh_app.authentication.logout()",
  "def publish(docid):\n    bokehuser = bokeh_app.current_user()\n    doc = docs.Doc.load(bokeh_app.servermodel_storage, docid)\n    if not bokeh_app.authentication.can_write_doc(docid):\n        return abort(401)\n    doc.published = True\n    doc.save(bokeh_app.servermodel_storage)\n    return jsonify(status='success')",
  "def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except AuthenticationException as e:\n            logger.exception(e)\n            return abort(401)",
  "def wrapper(docid, *args, **kwargs):\n        if bokeh_app.authentication.can_read_doc(docid):\n            return func(docid, *args, **kwargs)\n        else:\n            abort(401)",
  "def wrapper(docid, *args, **kwargs):\n        if bokeh_app.authentication.can_write_doc(docid):\n            return func(docid, *args, **kwargs)\n        else:\n            abort(401)",
  "def can_read_doc_api(doc, apikey):\n    if can_write_doc_api(doc, apikey):\n        return True\n    return apikey == doc.readonlyapikey",
  "def can_write_doc_api(doc, apikey):\n    return apikey == doc.apikey",
  "def can_read_doc(doc, bokehuser):\n    return bokehuser.username in doc.r_users",
  "def can_write_doc(doc, bokehuser):\n    return bokehuser.username in doc.rw_users",
  "def can_write_from_request(doc, request, user, temporary_docid=None):\n    # temporary_docid is a uuid - we're not too concerned about auth around it\n    # since it's a UUID and disposable\n    if temporary_docid:\n        return can_read_from_request(doc, request, user)\n    if request.headers.get('BOKEH-API-KEY'):\n        return doc.apikey == request.headers['BOKEH-API-KEY']\n    else:\n        if not user: return False\n        return can_write_doc(doc, user)",
  "def can_read_from_request(doc, request, user):\n    # No temporary docid here - temporary docid is about read-write permissions,\n    # and has no impact on read permissions\n    if doc.published:\n        return True\n    if can_write_from_request(doc, request, user):\n        return True\n    else:\n        if request.headers.get('BOKEH-API-KEY'):\n            return doc.readonlyapikey == request.headers['BOKEH-API-KEY']\n        else:\n            if not user: return False\n            return can_read_doc(doc, user)",
  "def apiuser_from_request(app, request):\n    apikey = request.headers.get('BOKEHUSER-API-KEY')\n    if not apikey:\n        return None\n    username = request.headers['BOKEHUSER']\n    bokehuser = User.load(app.servermodel_storage, username)\n    if bokehuser is None:\n        return None\n    if bokehuser.apikey == apikey:\n        return bokehuser\n    else:\n        return None",
  "def new_user(client, username, password, apikey=None, docs=None):\n    \"\"\"there is probably a race condition here if the same user\n    is registered at the same time since redis doesn't have transactions\n    \"\"\"\n    if apikey is None:\n        apikey = str(uuid.uuid4())\n    passhash = generate_password_hash(password, method='sha1')\n    user = User(username, passhash, apikey, docs=docs)\n    user.create(client)\n    return user",
  "def auth_user(client, username, password=None, apikey=None):\n    user = User.load(client, username)\n    if user is None:\n        raise models.UnauthorizedException\n    if password and check_password_hash(user.passhash, password):\n        return user\n    elif apikey and user.apikey == apikey:\n        return user\n    else:\n        raise models.UnauthorizedException",
  "class User(models.ServerModel):\n    idfield = 'username'\n    typename = 'user'\n    #we're using username as the id for now...\n    def __init__(self, username, passhash, apikey, docs=None):\n        self.apikey = apikey\n        self.username = username\n        self.passhash = passhash\n        if docs is None:\n            docs = []\n        self.docs = docs\n\n    @classmethod\n    def load(cls, client, objid):\n        attrs = cls.load_json(client, objid)\n        if attrs is None:\n            return None\n        changed = False\n        if not attrs.get('apikey'):\n            attrs['apikey'] = str(uuid.uuid4())\n            changed = True\n        docs = attrs.get('docs')\n        newdocs = []\n        for doc in docs:\n            if isinstance(doc, string_types):\n                doc = Doc.load(client, doc)\n                newdocs.append({'title' : doc.title,\n                                'docid' : doc.docid})\n                changed = True\n            else:\n                newdocs.append(doc)\n        attrs['docs'] = newdocs\n        obj = cls.from_json(attrs)\n        if changed:\n            obj.save(client)\n        return obj\n\n    def add_doc(self, docid, title):\n        matching = [x for x in self.docs if x.get('title') == title]\n        if len(matching) > 0:\n            raise DataIntegrityException('title already exists')\n        self.docs.append({'docid' : docid, 'title' : title})\n\n    def remove_doc(self, docid):\n        matching = [x for x in self.docs if x.get('docid') == docid]\n        if len(matching) == 0:\n            raise DataIntegrityException('no document found')\n        self.docs = [x for x in self.docs if x.get('docid') != docid]\n\n\n    def to_public_json(self):\n        return {'username' : self.username,\n                'docs' : self.docs}\n\n    def to_json(self):\n        return {'username' : self.username,\n                'passhash' : self.passhash,\n                'apikey' : self.apikey,\n                'docs' : self.docs,\n                }\n\n    @staticmethod\n    def from_json(obj):\n        return User(obj['username'],\n                    obj['passhash'],\n                    obj['apikey'],\n                    obj['docs'],\n                    )",
  "def __init__(self, username, passhash, apikey, docs=None):\n        self.apikey = apikey\n        self.username = username\n        self.passhash = passhash\n        if docs is None:\n            docs = []\n        self.docs = docs",
  "def load(cls, client, objid):\n        attrs = cls.load_json(client, objid)\n        if attrs is None:\n            return None\n        changed = False\n        if not attrs.get('apikey'):\n            attrs['apikey'] = str(uuid.uuid4())\n            changed = True\n        docs = attrs.get('docs')\n        newdocs = []\n        for doc in docs:\n            if isinstance(doc, string_types):\n                doc = Doc.load(client, doc)\n                newdocs.append({'title' : doc.title,\n                                'docid' : doc.docid})\n                changed = True\n            else:\n                newdocs.append(doc)\n        attrs['docs'] = newdocs\n        obj = cls.from_json(attrs)\n        if changed:\n            obj.save(client)\n        return obj",
  "def add_doc(self, docid, title):\n        matching = [x for x in self.docs if x.get('title') == title]\n        if len(matching) > 0:\n            raise DataIntegrityException('title already exists')\n        self.docs.append({'docid' : docid, 'title' : title})",
  "def remove_doc(self, docid):\n        matching = [x for x in self.docs if x.get('docid') == docid]\n        if len(matching) == 0:\n            raise DataIntegrityException('no document found')\n        self.docs = [x for x in self.docs if x.get('docid') != docid]",
  "def to_public_json(self):\n        return {'username' : self.username,\n                'docs' : self.docs}",
  "def to_json(self):\n        return {'username' : self.username,\n                'passhash' : self.passhash,\n                'apikey' : self.apikey,\n                'docs' : self.docs,\n                }",
  "def from_json(obj):\n        return User(obj['username'],\n                    obj['passhash'],\n                    obj['apikey'],\n                    obj['docs'],\n                    )",
  "def new_doc(flaskapp, docid, title, clientdoc, rw_users=None, r_users=None,\n            apikey=None, readonlyapikey=None):\n    if not apikey: apikey = str(uuid.uuid4())\n    if not readonlyapikey: readonlyapikey = str(uuid.uuid4())\n    plot_context = PlotContext()\n    clientdoc.context = plot_context\n    if rw_users is None: rw_users = []\n    if r_users is None: r_users = []\n    doc = Doc(docid, title, rw_users, r_users,\n              plot_context.ref, apikey, readonlyapikey, False)\n    doc.save(flaskapp.servermodel_storage)\n    return doc",
  "class Doc(models.ServerModel):\n    typename = 'doc'\n    idfield = 'docid'\n\n    def __init__(self, docid, title, rw_users, r_users,\n                 plot_context_ref, apikey, readonlyapikey, published):\n\n        self.published = published\n        self.docid = docid\n        self.title = title\n        self.rw_users = rw_users\n        self.r_users = r_users\n        self.plot_context_ref = plot_context_ref\n        self.apikey = apikey\n        self.readonlyapikey = readonlyapikey\n\n    def to_json(self):\n        return {'docid' : self.docid,\n                'title' : self.title,\n                'rw_users' : self.rw_users,\n                'r_users' : self.r_users,\n                'plot_context_ref' : self.plot_context_ref,\n                'apikey' : self.apikey,\n                'readonlyapikey' : self.readonlyapikey,\n                'published' : self.published\n                }\n\n    @classmethod\n    def load(cls, client, objid):\n        attrs = cls.load_json(client, objid)\n        #adding readonly api key if it's not there\n        if 'readonlyapikey' not in attrs:\n            attrs['readonlyapikey'] = str(uuid.uuid4())\n        if 'published' not in attrs:\n            attrs['published'] = False\n        obj = cls.from_json(attrs)\n        obj.save(client)\n        return obj\n\n    @staticmethod\n    def from_json(obj):\n        return Doc(obj['docid'], obj['title'],\n                   obj['rw_users'], obj['r_users'],\n                   obj['plot_context_ref'], obj['apikey'],\n                   obj['readonlyapikey'],\n                   obj['published']\n        )",
  "def __init__(self, docid, title, rw_users, r_users,\n                 plot_context_ref, apikey, readonlyapikey, published):\n\n        self.published = published\n        self.docid = docid\n        self.title = title\n        self.rw_users = rw_users\n        self.r_users = r_users\n        self.plot_context_ref = plot_context_ref\n        self.apikey = apikey\n        self.readonlyapikey = readonlyapikey",
  "def to_json(self):\n        return {'docid' : self.docid,\n                'title' : self.title,\n                'rw_users' : self.rw_users,\n                'r_users' : self.r_users,\n                'plot_context_ref' : self.plot_context_ref,\n                'apikey' : self.apikey,\n                'readonlyapikey' : self.readonlyapikey,\n                'published' : self.published\n                }",
  "def load(cls, client, objid):\n        attrs = cls.load_json(client, objid)\n        #adding readonly api key if it's not there\n        if 'readonlyapikey' not in attrs:\n            attrs['readonlyapikey'] = str(uuid.uuid4())\n        if 'published' not in attrs:\n            attrs['published'] = False\n        obj = cls.from_json(attrs)\n        obj.save(client)\n        return obj",
  "def from_json(obj):\n        return Doc(obj['docid'], obj['title'],\n                   obj['rw_users'], obj['r_users'],\n                   obj['plot_context_ref'], obj['apikey'],\n                   obj['readonlyapikey'],\n                   obj['published']\n        )",
  "class UnauthorizedException(Exception):\n    pass",
  "class ServerModel(object):\n    idfield = None\n    typename = None\n\n    @classmethod\n    def modelkey(cls, objid):\n        return \"model:%s:%s\"% (cls.typename, objid)\n\n    def mykey(self):\n        return self.modelkey(getattr(self, self.idfield))\n\n    def to_json(self):\n        raise NotImplementedError\n\n    @staticmethod\n    def from_json(obj):\n        raise NotImplementedError\n\n    def save(self, client):\n        client.set(self.mykey(), self.to_json())\n\n    def create(self, client):\n        try:\n            client.create(self.mykey(), self.to_json())\n        except DataIntegrityException:\n            raise UnauthorizedException(self.mykey())\n\n    @classmethod\n    def load_json(cls, client, objid):\n        data = client.get(cls.modelkey(objid))\n        if data is None:\n            return None\n        return data\n\n    @classmethod\n    def load(cls, client, objid):\n        attrs = cls.load_json(client, objid)\n        if attrs is None:\n            return None\n        return cls.from_json(attrs)",
  "def modelkey(cls, objid):\n        return \"model:%s:%s\"% (cls.typename, objid)",
  "def mykey(self):\n        return self.modelkey(getattr(self, self.idfield))",
  "def to_json(self):\n        raise NotImplementedError",
  "def from_json(obj):\n        raise NotImplementedError",
  "def save(self, client):\n        client.set(self.mykey(), self.to_json())",
  "def create(self, client):\n        try:\n            client.create(self.mykey(), self.to_json())\n        except DataIntegrityException:\n            raise UnauthorizedException(self.mykey())",
  "def load_json(cls, client, objid):\n        data = client.get(cls.modelkey(objid))\n        if data is None:\n            return None\n        return data",
  "def load(cls, client, objid):\n        attrs = cls.load_json(client, objid)\n        if attrs is None:\n            return None\n        return cls.from_json(attrs)",
  "def broadcast_reload():\n    from bokeh.server.app import bokeh_app\n    if hasattr(bokeh_app, 'wsmanager'):\n        bokeh_app.wsmanager.send('debug:debug', 'reload')",
  "def _wait_for_edit(extra_files=[], interval=1):\n    \"\"\"Waits until one of the files we're using have changed\n    \"\"\"\n    from itertools import chain\n    mtimes = {}\n    while 1:\n        for filename in chain(_iter_module_files(), extra_files or ()):\n            try:\n                mtime = os.stat(filename).st_mtime\n            except OSError:\n                continue\n            old_time = mtimes.get(filename)\n            if old_time is None:\n                mtimes[filename] = mtime\n                continue\n            elif mtime > old_time:\n                return\n        time.sleep(interval)",
  "def robust_reloader(func):\n    def wrapper(*args, **kwargs):\n        atexit.register(broadcast_reload)\n        try:\n            print ('running robust reloader')\n            func(*args, **kwargs)\n        except KeyboardInterrupt:\n            raise\n        except Exception:\n            \"\"\"If in robust reload mode, gather all dependent files\n            and wait until something has changed - and if so,\n            exit(3) (that's what werkzeug looks for to determine\n            whether or not to reload)\n            \"\"\"\n            extra_files = []\n            traceback.print_exc()\n            exc_type, exc_value, exc_tb = sys.exc_info()\n            tb = exc_tb\n            while tb:\n                filename = tb.tb_frame.f_code.co_filename\n                extra_files.append(filename)\n                tb = tb.tb_next\n            if isinstance(exc_value, SyntaxError):\n                extra_files.append(exc_value.filename)\n            print ('wait for edit')\n            _wait_for_edit(extra_files=extra_files)\n            sys.exit(3)\n    return wrapper",
  "def wrapper(*args, **kwargs):\n        atexit.register(broadcast_reload)\n        try:\n            print ('running robust reloader')\n            func(*args, **kwargs)\n        except KeyboardInterrupt:\n            raise\n        except Exception:\n            \"\"\"If in robust reload mode, gather all dependent files\n            and wait until something has changed - and if so,\n            exit(3) (that's what werkzeug looks for to determine\n            whether or not to reload)\n            \"\"\"\n            extra_files = []\n            traceback.print_exc()\n            exc_type, exc_value, exc_tb = sys.exc_info()\n            tb = exc_tb\n            while tb:\n                filename = tb.tb_frame.f_code.co_filename\n                extra_files.append(filename)\n                tb = tb.tb_next\n            if isinstance(exc_value, SyntaxError):\n                extra_files.append(exc_value.filename)\n            print ('wait for edit')\n            _wait_for_edit(extra_files=extra_files)\n            sys.exit(3)",
  "def object_page(prefix):\n    \"\"\" Decorator for a function which turns an object into a web page\n\n    from bokeh.server.app import bokeh_app\n    @bokeh_app.route(\"/myapp\")\n    @object_page(\"mypage\")\n    def make_object():\n        #make some bokeh object here\n        return obj\n\n    This decorator will\n      - create a randomized title for a bokeh document using the prefix\n      - initialize bokeh plotting libraries to use that document\n      - call the function you pass in, add that object to the plot context\n      - render that object in a web page\n\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            ## setup the randomly titled document\n            docname = prefix + str(uuid.uuid4())\n            bokehuser = bokeh_app.current_user()\n            try:\n                doc = _makedoc(bokeh_app.servermodel_storage, bokehuser, docname)\n            except DataIntegrityException as e:\n                return abort(409, e.message)\n            docid = doc.docid\n            clientdoc = bokeh_app.backbone_storage.get_document(docid)\n\n            ## initialize our plotting APIs to use that document\n\n            init_bokeh(clientdoc)\n            obj = func(*args, **kwargs)\n            clientdoc.add(obj)\n            bokeh_app.backbone_storage.store_document(clientdoc)\n            if hasattr(obj, 'extra_generated_classes'):\n                extra_generated_classes = obj.extra_generated_classes\n            else:\n                extra_generated_classes = []\n\n            resources = Resources()\n            return render_template(\"oneobj.html\",\n                                   elementid=str(uuid.uuid4()),\n                                   docid=docid,\n                                   objid=obj._id,\n                                   hide_navbar=True,\n                                   extra_generated_classes=extra_generated_classes,\n                                   splitjs=server_settings.splitjs,\n                                   username=bokehuser.username,\n                                   loglevel=resources.log_level)\n        wrapper.__name__ = func.__name__\n        return wrapper\n\n    return decorator",
  "def decorator(func):\n        def wrapper(*args, **kwargs):\n            ## setup the randomly titled document\n            docname = prefix + str(uuid.uuid4())\n            bokehuser = bokeh_app.current_user()\n            try:\n                doc = _makedoc(bokeh_app.servermodel_storage, bokehuser, docname)\n            except DataIntegrityException as e:\n                return abort(409, e.message)\n            docid = doc.docid\n            clientdoc = bokeh_app.backbone_storage.get_document(docid)\n\n            ## initialize our plotting APIs to use that document\n\n            init_bokeh(clientdoc)\n            obj = func(*args, **kwargs)\n            clientdoc.add(obj)\n            bokeh_app.backbone_storage.store_document(clientdoc)\n            if hasattr(obj, 'extra_generated_classes'):\n                extra_generated_classes = obj.extra_generated_classes\n            else:\n                extra_generated_classes = []\n\n            resources = Resources()\n            return render_template(\"oneobj.html\",\n                                   elementid=str(uuid.uuid4()),\n                                   docid=docid,\n                                   objid=obj._id,\n                                   hide_navbar=True,\n                                   extra_generated_classes=extra_generated_classes,\n                                   splitjs=server_settings.splitjs,\n                                   username=bokehuser.username,\n                                   loglevel=resources.log_level)\n        wrapper.__name__ = func.__name__\n        return wrapper",
  "def wrapper(*args, **kwargs):\n            ## setup the randomly titled document\n            docname = prefix + str(uuid.uuid4())\n            bokehuser = bokeh_app.current_user()\n            try:\n                doc = _makedoc(bokeh_app.servermodel_storage, bokehuser, docname)\n            except DataIntegrityException as e:\n                return abort(409, e.message)\n            docid = doc.docid\n            clientdoc = bokeh_app.backbone_storage.get_document(docid)\n\n            ## initialize our plotting APIs to use that document\n\n            init_bokeh(clientdoc)\n            obj = func(*args, **kwargs)\n            clientdoc.add(obj)\n            bokeh_app.backbone_storage.store_document(clientdoc)\n            if hasattr(obj, 'extra_generated_classes'):\n                extra_generated_classes = obj.extra_generated_classes\n            else:\n                extra_generated_classes = []\n\n            resources = Resources()\n            return render_template(\"oneobj.html\",\n                                   elementid=str(uuid.uuid4()),\n                                   docid=docid,\n                                   objid=obj._id,\n                                   hide_navbar=True,\n                                   extra_generated_classes=extra_generated_classes,\n                                   splitjs=server_settings.splitjs,\n                                   username=bokehuser.username,\n                                   loglevel=resources.log_level)",
  "def cross(start, facets):\n    \"\"\"Creates a unique combination of provided facets.\n    A cross product of an initial set of starting facets with a new set of\n    facets.\n\n    Args:\n      start (list): List of lists of facets\n      facets (list): List of facets\n\n    Returns:\n      list: a list of lists of unique combinations of facets\n\n    \"\"\"\n    new = [[facet] for facet in facets]\n    result = []\n    for x in start:\n        for n in new:\n            result.append(x + n)\n    return result",
  "def hide_axes(plot, axes=('x', 'y')):\n    \"\"\"Hides the axes of the plot by setting component alphas.\n\n    Args:\n      plot (Figure): a valid figure with x and y axes\n      axes (tuple or list or str, optional): the axes to hide the axis on.\n\n    \"\"\"\n    if isinstance(axes, str):\n        axes = tuple(axes)\n\n    for label in axes:\n        axis = getattr(plot, label + 'axis')\n        axis = axis[0]\n        axis.major_label_text_alpha = 0.0\n        axis.major_label_text_font_size = '0pt'\n        axis.axis_line_alpha = 0.0\n        axis.major_tick_line_alpha = 0.0\n        axis.minor_tick_line_alpha = 0.0\n\n    plot.min_border = 0",
  "def make_histogram_source(series):\n    \"\"\"Creates a ColumnDataSource containing the bins of the input series.\n\n    Args:\n      series (:py:class:`~pandas.Series`): description\n\n    Returns:\n      ColumnDataSource: includes bin centers with count of items in the bins\n\n    \"\"\"\n    counts, bins = np.histogram(series, bins=50)\n    centers = pd.rolling_mean(bins, 2)[1:]\n\n    return ColumnDataSource(data={'counts': counts, 'centers': centers})",
  "def make_continuous_bar_source(df, x_field, y_field, agg):\n    \"\"\"Makes discrete, then creates representation of the bars to be plotted.\n\n    Args:\n      df (DataFrame): contains the data to be converted to a discrete form\n      x_field (str): the column in df that maps to the x dim of the plot\n      y_field (str):  the column in df that maps to the y dim of the plot\n      agg (str): the type of aggregation to be used\n\n    Returns:\n      ColumnDataSource: aggregated, discrete form of x,y values\n\n    \"\"\"\n\n    # Generate dataframe required to use the categorical bar source function\n    labels, edges = pd.cut(x=df[x_field], bins=20, retbins=True, labels=False)\n    centers = pd.rolling_mean(edges, 2)[1:]\n\n    # store new value of x as the bin it fell into\n    df[x_field] = centers[labels]\n\n    # After making it discrete, create the categorical bar source\n    return make_categorical_bar_source(df, x_field, y_field, agg)",
  "def make_categorical_bar_source(df, x_field, y_field, agg):\n    \"\"\"Creates representation of the bars to be plotted.\n\n    Args:\n      df (DataFrame): contains the data to be converted to a discrete form\n      x_field (str): the column in df that maps to the x dim of the plot\n      y_field (str):  the column in df that maps to the y dim of the plot\n      agg (str): the type of aggregation to be used\n\n    Returns:\n      ColumnDataSource: aggregated, discrete form of x,y values\n\n    \"\"\"\n\n    # Get the y values after grouping by the x values\n    group = df.groupby(x_field)[y_field]\n    aggregate = getattr(group, agg)\n\n    # Convert back to a DataFrame on the aggregated data\n    result = aggregate().reset_index()\n\n    return ColumnDataSource(data=result)",
  "def make_factor_source(series):\n    \"\"\"Generate data source that is based on the unique values in the series.\n\n    Args:\n      series (:py:class:`~pandas.Series`): contains categorical-like data\n\n    Returns:\n      ColumnDataSource: contains the unique values from the series\n\n    \"\"\"\n    return ColumnDataSource(data={'factors': series.unique()})",
  "def make_bar_plot(datasource, counts_name=\"counts\",\n                  centers_name=\"centers\",\n                  bar_width=0.7,\n                  x_range=None,\n                  y_range=None,\n                  plot_width=500, plot_height=500,\n                  tools=\"pan,wheel_zoom,box_zoom,save,resize,box_select,reset\",\n                  title_text_font_size=\"12pt\"):\n    \"\"\"Utility function to set/calculate default parameters of a bar plot.\n\n    Args:\n      datasource (ColumnDataSource): represents bars to plot\n      counts_name (str): column corresponding to height of the bars\n      centers_name (str): column corresponding to the location of the bars\n      bar_width (float): the width of the bars in the bar plot\n      x_range (list): list of two values, the min and max of the x axis range\n      plot_width (float): width of the plot in pixels\n      plot_height (float): height of the plot in pixels\n      tools (str): comma separated tool names to add to the plot\n      title_text_font_size (str): size of the plot title, e.g., '12pt'\n\n    Returns:\n      figure: plot generated from the provided parameters\n\n    \"\"\"\n    top = np.max(datasource.data[counts_name])\n\n    # Create the figure container\n    plot = figure(\n        title=\"\", title_text_font_size=title_text_font_size,\n        plot_width=plot_width, plot_height=plot_height,\n        x_range=x_range, y_range=[0, top], tools=tools)\n\n    # Get the bar values\n    y = [val/2.0 for val in datasource.data[counts_name]]\n\n    # Generate the bars in the figure\n    plot.rect(centers_name, y, bar_width, counts_name, source=datasource)\n\n    plot.min_border = 0\n    plot.h_symmetry = False\n    plot.v_symmetry = False\n\n    select_tool = _get_select_tool(plot)\n    if select_tool:\n        select_tool.dimensions = ['width']\n\n    return plot",
  "def make_histogram(datasource,\n                   counts_name=\"counts\",\n                   centers_name=\"centers\",\n                   x_range=None,\n                   bar_width=0.7,\n                   plot_width=500,\n                   plot_height=500,\n                   min_border=40,\n                   tools=None,\n                   title_text_font_size=\"12pt\"):\n    \"\"\"Utility function to create a histogram figure.\n\n    This is used to create the filter widgets for continuous data in\n    CrossFilter.\n\n    Args:\n      datasource (ColumnDataSource): represents bars to plot\n      counts_name (str): column corresponding to height of the bars\n      centers_name (str): column corresponding to the location of the bars\n      x_range (list): list of two values, the min and max of the x axis range\n      bar_width (float): the width of the bars in the bar plot\n      plot_width (float): width of the plot in pixels\n      plot_height (float): height of the plot in pixels\n      min_border (float): minimum border width of figure in pixels\n      tools (str): comma separated tool names to add to the plot\n      title_text_font_size (str): size of the plot title, e.g., '12pt'\n\n    Returns:\n      figure: histogram plot generated from the provided parameters\n\n    \"\"\"\n    start = np.min(datasource.data[centers_name]) - bar_width\n    end = np.max(datasource.data[centers_name]) - bar_width\n    plot = make_bar_plot(\n        datasource, counts_name=counts_name, centers_name=centers_name,\n        x_range=[start, end], plot_width=plot_width, plot_height=plot_height,\n        tools=tools, title_text_font_size=title_text_font_size)\n    return plot",
  "class DiscreteFacet(object):\n    \"\"\"Pairing of a field and a unique value, representing a subset of the\n    total data.\"\"\"\n\n    def __init__(self, field, value, label=None):\n        \"\"\"Sets object properties and creates label if not provided.\n\n        Args:\n          field (str): name of the column\n          value: unique value defined for the column\n          label (str, optional): string representation of the value\n\n        \"\"\"\n        if label is None:\n            label = str(value)\n        self.field = field\n        self.label = label\n        self._value = value\n\n    def __repr__(self):\n        return \"%s:%s\"%(self.field, self.label)\n\n    def filter(self, df):\n        \"\"\"Filters the provided DataFrame to the subset corresponding to value.\n\n        Args:\n          df (DataFrame): contains a column of ``field``\n\n        Returns:\n          DataFrame: filtered to rows, where column ``field`` has values\n            equal to ``_value``.\n\n        \"\"\"\n        return df[df[self.field] == self._value]",
  "class ContinuousFacet(DiscreteFacet):\n    \"\"\"Represents a range of values for a field in a DataFrame.\"\"\"\n\n    def __init__(self, field, value, bins, label=None):\n        \"\"\"Calls parent ``DiscreteFacet`` and stores bins for later filtering.\n\n        Args:\n          field (str): name of the column\n          value (str): center of range of values in the column\n          bins (list[float]): start and inclusive stop value for the bin\n          label (str, optional): string representation\n\n        \"\"\"\n        super(ContinuousFacet, self).__init__(field, value, label=label)\n        self.bins = bins\n\n    def filter(self, df):\n        \"\"\"Filters the provided DataFrame to the subset corresponding to bins.\n\n        Args:\n          df (DataFrame): contains a column of ``field``\n\n        Returns:\n          DataFrame: filtered to rows, where column ``field`` has values\n            within the bounds of ``bins``.\n\n        \"\"\"\n        if self.bins[0] is not None:\n            df = df[df[self.field] > self.bins[0]]\n        if self.bins[1] is not None:\n            df = df[df[self.field] <= self.bins[1]]\n        return df",
  "class CrossFilter(PlotObject):\n    \"\"\"Interactive filtering and faceting application with multiple plot\n    types\"\"\"\n\n    # identify properties for the data\n    columns = List(Dict(String, Any))\n    data = Instance(ColumnDataSource)\n    filtered_data = Instance(ColumnDataSource)\n\n    # list of datasources to use for filtering widgets\n    filter_sources = Dict(String, Instance(ColumnDataSource))\n\n    # list of columns we are filtering\n    filtering_columns = List(String)\n\n    # dict of column name to filtering widgets\n    filter_widgets = Dict(String, Instance(PlotObject))\n\n    # dict which aggregates all the selections from the different filtering\n    # widgets\n    filtered_selections = Dict(String, Dict(String, Any))\n\n    # list of facet vars\n    facet_x = List(String, default=[])\n    facet_y = List(String, default=[])\n    facet_tab = List(String, default=[])\n\n    # the displayed plot object\n    plot = Instance(PlotObject)\n    x_range = Instance(Range)\n    y_range = Instance(Range)\n\n    # configuration properties for the plot\n    plot_type = Enum(\"line\", \"scatter\", \"bar\")\n    plot_map = {'line': CrossLinePlugin,\n                'scatter': CrossScatterPlugin,\n                'bar': CrossBarPlugin}\n    x = String\n    y = String\n    agg = String\n    color = String\n    title = String\n    height = Int()\n    width = Int()\n\n    # identify the selector/drop-down properties\n    plot_selector = Instance(Select)\n    x_selector = Instance(Select)\n    y_selector = Instance(Select)\n    agg_selector = Instance(Select)\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Creates original and filtered ColumnDataSource and handles defaults.\n\n        The df and starting configuration are only provided the first time\n        init is called, within the create method.\n\n        Kwargs:\n          df (DataFrame): the data to use in the crossfilter app\n          plot_type (str, optional): starting plot type\n          agg (str, optional): starting aggregation type\n\n        \"\"\"\n        if 'df' in kwargs:\n            self._df = kwargs.pop('df')\n\n            # initialize a \"pure\" and filtered data source based on df\n            kwargs['data'] = ColumnDataSource(data=self.df)\n            kwargs['filtered_data'] = ColumnDataSource(data=self.df)\n\n        # default plot type\n        if 'plot_type' not in kwargs:\n            kwargs['plot_type'] = \"scatter\"\n\n        # default aggregation type\n        if 'agg' not in kwargs:\n            kwargs['agg'] = 'sum'\n\n        if 'plot_map' in kwargs:\n            self.plot_map = kwargs.pop('plot_map')\n\n        super(CrossFilter, self).__init__(**kwargs)\n\n    @classmethod\n    def create(cls, **kwargs):\n        \"\"\"Performs all one-time construction of bokeh objects.\n\n        This classmethod is required due to the way that bokeh handles the\n        python and javascript components. The initialize method will be\n        called each additional time the app is updated (including once in\n        the create method), but the PlotObject infrastructure will find that\n        the object already exists in any future calls, and will not create a\n        new object.\n\n        Kwargs:\n          df (DataFrame): the data to use in the crossfilter app\n          plot_type (str, optional): starting plot type\n          agg (str, optional): starting aggregation type\n\n        :return:\n        \"\"\"\n        obj = cls(**kwargs)\n        obj.set_metadata()\n        choices = obj.make_plot_choices()\n        obj.update_plot_choices(choices)\n        obj.set_plot()\n        obj.set_input_selector()\n        return obj\n\n    def set_input_selector(self):\n        \"\"\"Creates and configures each selector (drop-down menu).\"\"\"\n\n        col_names = [x['name'] for x in self.columns]\n\n        self.plot_selector = Select.create(\n            title=\"PlotType\",\n            name=\"plot_type\",\n            value=self.plot_type,\n            options=[\"line\", \"scatter\", \"bar\"],\n        )\n\n        self.x_selector = Select.create(\n            name=\"x\",\n            value=self.x,\n            options=col_names,\n        )\n\n        self.y_selector = Select.create(\n            name=\"y\",\n            value=self.y,\n            options=col_names,\n        )\n\n        self.agg_selector = Select.create(\n            name='agg',\n            value=self.agg,\n            options=['sum', 'mean', 'last'],\n        )\n\n    def update_plot_choices(self, input_dict):\n        \"\"\"Sets object attributes corresponding to input_dict's values.\n\n        Args:\n          input_dict (dict): dict with x, y, and plot_type keys\n\n        \"\"\"\n        for k, v in input_dict.items():\n            if getattr(self, k) is None:\n                setattr(self, k, v)\n\n    def get_plot_class(self):\n        \"\"\"Return the class for the current plot selection.\"\"\"\n        return self.plot_map[self.plot_type]\n\n    def column_descriptor_dict(self):\n        \"\"\"Creates column stats dict with keys of column names.\n\n        Returns:\n          dict: dict with key per column in data, where values are column stats\n\n        \"\"\"\n        column_descriptors = {}\n        for x in self.columns:\n            column_descriptors[x['name']] = x\n        return column_descriptors\n\n    def continuous_columns(self):\n        \"\"\"Returns list of column descriptors for the non-Discrete columns.\n\n        Returns:\n          list(dict): list of dicts, containing metadata about columns\n\n        \"\"\"\n        return [x for x in self.columns if x['type'] != 'DiscreteColumn']\n\n    def discrete_columns(self):\n        \"\"\"Returns list of column descriptors for the Discrete columns.\n\n        Returns:\n          list(dict): list of dicts, containing metadata about columns\n\n        \"\"\"\n        return [x for x in self.columns if x['type'] == 'DiscreteColumn']\n\n    def make_plot_choices(self):\n        \"\"\"Selects first two continuous columns for x,y during initial setup\n\n        Returns:\n          dict: x, y, and plot_type keys and values for initial setup\n\n        \"\"\"\n        x, y = [x['name'] for x in self.continuous_columns()[:2]]\n        return {'x': x, 'y': y, 'plot_type': 'scatter'}\n\n    def set_plot(self):\n        \"\"\"Makes and sets the plot based on the current configuration of app.\"\"\"\n\n        self.update_xy_ranges(source=self.df)\n        plot = self.make_plot()\n        self.plot = plot\n        curdoc()._add_all()\n\n    def make_plot(self):\n        \"\"\"Makes the correct plot layout type, based on app's current config.\n\n        Returns:\n          PlotObject: one plot, grid of plots, or tabs of plots/grids of plots\n\n        \"\"\"\n\n        if self.facet_tab:\n            facets = self.make_facets(dimension='tab')\n\n            # generate a list of panels, containing plot/plots for each facet\n            tabs = [self.make_tab(content=self.create_plot_page(\n                tab_facet=facet), tab_label=self.facet_title(facet)) for facet\n                    in facets]\n            return Tabs(tabs=tabs)\n        else:\n            return self.create_plot_page()\n\n\n    def create_plot_page(self, tab_facet=None):\n        \"\"\"Generates a single visible page of a plot or plots.\n\n        Args:\n          tab_facet (DiscreteFacet or ContinuousFacet): a facet to filter on\n\n        Returns:\n          PlotObject: a single or grid of plots\n\n        \"\"\"\n        # no faceting\n        if all([len(self.facet_x) == 0,\n                len(self.facet_y) == 0]):\n            plot_page = self.make_single_plot(facet=tab_facet)\n\n        # x xor y faceting\n        if all([(len(self.facet_x) != 0) ^ (len(self.facet_y) != 0)]):\n            plot_page = self.make_1d_facet_plot(facet=tab_facet)\n\n        # x and y faceting\n        if all([len(self.facet_x) != 0,\n                len(self.facet_y) != 0]):\n            plot_page = self.make_2d_facet_plot(facet=tab_facet)\n\n        if isinstance(plot_page, GridPlot):\n            self.apply_grid_style(plot_page)\n\n        return plot_page\n\n    @staticmethod\n    def make_tab(content, tab_label):\n        \"\"\"Creates a container for the contents of a tab.\n\n        Args:\n          content (PlotObject): the primary content of the tab\n          tab_label (str): the text to place in the tab\n\n        Returns:\n          Panel: represents a single tab in a group of tabs\n\n        \"\"\"\n        return Panel(child=content, title=tab_label)\n\n    def make_facets(self, dimension):\n        \"\"\"Creates combination of all facets for the provided dimension\n\n        Args:\n          dimension (str): name of the dimension to create facets for\n\n        Returns:\n          list(list(DiscreteFacet or ContinuousFacet)): list of list of\n            unique facet combinations\n\n        \"\"\"\n        if dimension == 'x':\n            facets = self.facet_x\n        elif dimension == 'y':\n            facets = self.facet_y\n        else:\n            facets = self.facet_tab\n\n        # create facets for each column\n        column_descriptor_dict = self.column_descriptor_dict()\n        all_facets = [[]]\n        for field in facets:\n\n            # create facets from discrete columns\n            if column_descriptor_dict[field]['type'] == 'DiscreteColumn':\n                field_facets = [DiscreteFacet(field, val) for val in\n                                np.unique(self.df[field].values)]\n\n                # combine any facets as required\n                all_facets = cross(all_facets, field_facets)\n            else:\n                # create quantile based discrete data and pairs of bins\n                categorical, bins = pd.qcut(self.df[field], 4, retbins=True)\n                cats = categorical.cat.categories\n                bins = [[bins[idx], bins[idx + 1]] for idx in\n                        range(len(bins) - 1)]\n                bins[0][0] = None\n\n                # create list of facets\n                field_facets = [ContinuousFacet(field, value, bin) for\n                                bin, value in zip(bins, cats)]\n\n                # combine any facets as required\n                all_facets = cross(all_facets, field_facets)\n\n        return all_facets\n\n    @staticmethod\n    def facet_title(facets):\n        \"\"\"Joins list of facets by commas.\n\n        Args:\n          facets (list(DiscreteFacet or ContinuousFacet)): list of facets,\n          which are a combination of column and unique value within it\n\n        Returns:\n          str: string representation of the combination of facets\n\n        \"\"\"\n        title = \",\".join([str(x) for x in facets])\n        return title\n\n    def facet_data(self, facets, df=None):\n        \"\"\"Filters data to the rows associated with the given facet.\n\n        Args:\n          facets (list(DiscreteFacet or ContinuousFacet)): list of facets,\n          which are a combination of column and unique value within it\n          df (DataFrame, optional): data to be filtered on\n\n        Returns:\n          DataFrame: filtered DataFrame based on provided facets\n\n        \"\"\"\n        if df is None:\n            df = self.filtered_df\n        for f in facets:\n            df = f.filter(df)\n        return df\n\n    def make_1d_facet_plot(self, facet=None):\n        \"\"\"Creates the faceted plots when a facet is added to the x axis.\n\n        Returns:\n          GridPlot: a grid of plots, where each plot has subset of data\n\n        \"\"\"\n        if self.facet_x:\n            all_facets = self.make_facets('x')\n        else:\n            all_facets = self.make_facets('y')\n\n        plots = []\n\n        # loop over facets and create single plots for data subset\n        for facets in all_facets:\n            title = self.facet_title(facets)\n\n            if facet:\n                facets += facet\n\n            df = self.facet_data(facets, self.filtered_df)\n            plot = self.make_single_plot(\n                df=df, title=title, plot_height=200, plot_width=200,\n                tools=\"pan,wheel_zoom,reset\", facet=facets\n            )\n\n            # append single plot to list of plots\n            plots.append(plot)\n\n        # create squarish grid based on number of plots\n        chunk_size = int(np.ceil(np.sqrt(len(plots))))\n\n        # create list of lists of plots, where each list of plots is a row\n        grid_plots = []\n        for i in range(0, len(plots), chunk_size):\n            chunk = plots[i:i + chunk_size]\n            grid_plots.append(chunk)\n\n        self.hide_internal_axes(grid_plots)\n\n        # return the grid as the plot\n        return GridPlot(children=grid_plots, plot_width=200*chunk_size)\n\n    def make_2d_facet_plot(self, facet=None):\n        \"\"\"Creates the grid of plots when there are both x and y facets.\n\n        Returns:\n          GridPlot: grid of x and y facet combinations\n\n        \"\"\"\n\n        # ToDo: gracefully handle large combinations of facets\n        all_facets_x = self.make_facets('x')\n        all_facets_y = self.make_facets('y')\n\n        grid_plots = []\n\n        # y faceting down column\n        for facets_y in all_facets_y:\n\n            # x faceting across row\n            row = []\n            for facets_x in all_facets_x:\n\n                # build the facets and title\n                facets = facets_x + facets_y\n                title = self.facet_title(facets)\n\n                # must filter by any extra facets provided for facet tab\n                if facet:\n                    filter_facets = facets + facet\n                else:\n                    filter_facets = facets\n\n                df = self.facet_data(filter_facets, self.filtered_df)\n                plot = self.make_single_plot(\n                    df=df, title=title, plot_height=200, plot_width=200,\n                    tools=\"pan,wheel_zoom,reset\", facet=facets\n                )\n                row.append(plot)\n\n            # append the row to the list of rows\n            grid_plots.append(row)\n\n            self.hide_internal_axes(grid_plots)\n\n        # return the grid of plots as the plot\n        return GridPlot(children=grid_plots, plot_width=200*len(all_facets_x))\n\n    @staticmethod\n    def apply_facet_style(plot):\n        \"\"\"Applies facet-specific style for a given plot.\n\n        Override this method to modify the look of a customized CrossFilter\n        for all plugins. Or, apply custom styles in the plugin, since the\n        plugin will be told if it is currently being faceted.\n\n        \"\"\"\n        plot.title_text_font_size = \"9pt\"\n        plot.min_border = 0\n\n    def apply_single_plot_style(self, plot):\n        \"\"\"Applies styles when we have only one plot.\n\n        Override this method to modify the look of a customized CrossFilter\n        for all plugins.\n\n        \"\"\"\n        plot.min_border_left = 60\n\n    def apply_grid_style(self, grid_plot):\n        \"\"\"Applies facet-specific style for the grid of faceted plots.\n\n        Override this method to modify the look of a customized CrossFilter\n        for all plugins. Or, apply custom styles in the plugin, since the\n        plugin will be told if it is currently being faceted.\n\n        \"\"\"\n        grid_plot.title_text_font_size = \"12pt\"\n        grid_plot.title_text_font_style = \"bold\"\n        grid_plot.title = self.title\n\n    @staticmethod\n    def hide_internal_axes(grid_plots):\n        \"\"\"Hides the internal axes for a grid of plots.\n\n        Args:\n          grid_plots (list(list(Figure))): list of rows (list), containing plots\n\n        \"\"\"\n        for i, row in enumerate(grid_plots):\n            is_bottom = i + 1 == len(grid_plots)\n\n            for j, plot in enumerate(row):\n                if j != 0:\n                    if is_bottom:\n                        hide_axes(plot, axes='y')\n                    else:\n                        hide_axes(plot)\n\n                elif j == 0 and not is_bottom:\n                    hide_axes(plot, axes='x')\n\n    def make_single_plot(self, df=None, title=None,\n                         plot_width=700,\n                         plot_height=680,\n                         tools=\"pan,wheel_zoom,box_zoom,save,resize,\"\n                               \"box_select,reset\",\n                         facet=None):\n        \"\"\"Creates a plot based on the current app configuration.\n\n        Args:\n          df (DataFrame, optional): data to use for the plot\n          title (str, optional): plot title\n          plot_width (float, optional): width of plot in pixels\n          plot_height (float, optional): height of plot in pixels\n          tools (str, optional): comma separated string of tool names\n\n        Returns:\n          PlotObject: the generated plot\n\n        \"\"\"\n        faceting = False\n\n        # df is not provided when we are not faceting\n        if df is None:\n            source = self.filtered_data\n        else:\n            df = self.facet_data(facets=facet, df=df)\n\n            # create column data source with filtered df\n            source = ColumnDataSource(data=df)\n            faceting = True\n\n        # check for tab faceting and filter if provided\n        if facet:\n            df = self.facet_data(facets=facet, df=df)\n            source = ColumnDataSource(data=df)\n\n        # get the helper class for the plot type selected\n        plot_class = self.get_plot_class()\n\n        # initialize the plugin class\n        plugin = plot_class(source=source,\n                            title_text_font_size=\"12pt\",\n                            title_text_font_style = \"bold\",\n                            plot_height=plot_height,\n                            plot_width=plot_width,\n                            tools=tools,\n                            title=title,\n                            x_range=self.x_range,\n                            y_range=self.y_range,\n                            facet=faceting,\n                            crossfilter=self)\n\n        # generate plot\n        plot = plugin.get_plot()\n\n        # apply faceting-specific styling if required\n        if facet:\n            self.apply_facet_style(plot)\n            self.title = plugin.title\n        else:\n            self.apply_single_plot_style(plot)\n            self.title = plot.title\n\n        return plot\n\n    def update_xy_ranges(self, source):\n        \"\"\"Updates common x_range, y_range to use for creating figures.\n\n        Args:\n          source (ColumnDataSource): the source to return correct range for\n\n        \"\"\"\n        plt_cls = self.get_plot_class()\n        x_range, y_range = plt_cls.make_xy_ranges(cf=self)\n\n        # store x and y range from the plot class\n        self.x_range = x_range\n        self.y_range = y_range\n\n    def plot_attribute_change(self, obj, attrname, old, new):\n        \"\"\"Updates app's attribute and plot when view configuration changes.\n\n        Args:\n          obj (Widget): the object that has an attribute change\n          attrname (str): name of the attribute\n          old (type): the previous value of unknown type\n          new (type): the new value of unknown type\n\n        \"\"\"\n        setattr(self, obj.name, new)\n        self.set_plot()\n\n    def facet_change(self, obj, attrname, old, new):\n        \"\"\"Updates plot when any facet configuration changes.\n\n        Args:\n          obj (Widget): the object that has an attribute change\n          attrname (str): name of the attribute\n          old (type): the previous value of unknown type\n          new (type): the new value of unknown type\n\n        \"\"\"\n        self.set_plot()\n\n    @property\n    def df(self):\n        \"\"\"The core data that is used by the app for plotting.\n\n        Returns:\n          DataFrame: the original data structure\n\n        \"\"\"\n        if hasattr(self, '_df'):\n            return self._df\n        else:\n            if self.data:\n                return self.data.to_df()\n\n    @property\n    def filtered_df(self):\n        \"\"\"The subset of the data to use for plotting.\n\n        Returns:\n          DataFrame: the original data structure\n\n        \"\"\"\n        if hasattr(self, '_df'):\n            return self._df\n        else:\n            if self.filtered_data:\n                return self.filtered_data.to_df()\n\n    def update(self, **kwargs):\n        \"\"\"Updates CrossFilter attributes each time the model changes.\n\n        The events are setup each time so that we can add event handlers to\n        the selection/filtering widgets as they are added.\n\n        \"\"\"\n        super(CrossFilter, self).update(**kwargs)\n        self.setup_events()\n\n    def setup_events(self):\n        \"\"\"Registers events each time the app changes state.\"\"\"\n\n        # watch the app's filtering_columns attribute to setup filters\n        self.on_change('filtering_columns', self, 'setup_filter_widgets')\n\n        # register any available filter widget\n        for obj in self.filter_widgets.values():\n            if isinstance(obj, InputWidget):\n                obj.on_change('value', self, 'handle_filter_selection')\n\n        # watch app column data source attribute for changes\n        for obj in self.filter_sources.values():\n            obj.on_change('selected', self, 'handle_filter_selection')\n\n        # selector event registration\n        if self.plot_selector:\n            self.plot_selector.on_change('value', self, 'plot_attribute_change')\n        if self.x_selector:\n            self.x_selector.on_change('value', self, 'plot_attribute_change')\n        if self.y_selector:\n            self.y_selector.on_change('value', self, 'plot_attribute_change')\n        if self.agg_selector:\n            self.agg_selector.on_change('value', self, 'plot_attribute_change')\n\n        # register to watch the app's facet attributes\n        self.on_change('facet_x', self, 'facet_change')\n        self.on_change('facet_y', self, 'facet_change')\n        self.on_change('facet_tab', self, 'facet_change')\n\n    def handle_filter_selection(self, obj, attrname, old, new):\n        \"\"\"Filters the data source whenever a filter widget changes.\n\n        Args:\n          obj (Widget): the object that has an attribute change\n          attrname (str): name of the attribute\n          old (type): the previous value of unknown type\n          new (type): the new value of unknown type\n\n        \"\"\"\n        df = self.df\n\n        # loop over the column metadata\n        for descriptor in self.columns:\n            colname = descriptor['name']\n\n            # handle discrete selections\n            if descriptor['type'] == 'DiscreteColumn' and \\\n                            colname in self.filter_widgets:\n                selected = self.filter_widgets[colname].value\n                if not selected:\n                    continue\n                if isinstance(selected, six.string_types):\n                    df = df[colname == selected]\n                else:\n                    df = df[np.in1d(df[colname], selected)]\n\n            # handle time or continuous selections\n            elif descriptor['type'] in ('TimeColumn', 'ContinuousColumn') and \\\n                            colname in self.filter_widgets:\n                obj = self.filter_sources[colname]\n\n                # hack because we don't have true range selection\n                if not obj.selected:\n                    continue\n                min_idx = np.min(obj.selected)\n                max_idx = np.max(obj.selected)\n\n                min_val = obj.data['centers'][min_idx]\n                max_val = obj.data['centers'][max_idx]\n                df = df[(df[colname] >= min_val) & (df[colname] <= max_val)]\n\n        # update filtered data and force plot update\n        for colname in self.data.column_names:\n            self.filtered_data.data[colname] = df[colname]\n            self.filtered_data._dirty = True\n        self.set_plot()\n\n    def clear_selections(self, obj, attrname, old, new):\n        \"\"\"Updates filter widgets and sources as they are removed.\n\n        Args:\n          obj (Widget): the object that has an attribute change\n          attrname (str): name of the attribute\n          old (type): the previous value of unknown type\n          new (type): the new value of unknown type\n\n        \"\"\"\n        diff = set(old) - set(new)\n        column_descriptor_dict = self.column_descriptor_dict()\n\n        # delete any removed filter widgets\n        if len(diff) > 0:\n            for col in diff:\n                metadata = column_descriptor_dict[col]\n                if metadata['type'] != 'DiscreteColumn':\n                    del self.filter_sources[col]\n                del self.filter_widgets[col]\n\n        # update the data based on latest changes\n        if diff:\n            self.handle_filter_selection(obj, attrname, old, new)\n\n\n    def setup_filter_widgets(self, obj, attrname, old, new):\n        \"\"\"Creates new filter widget each time a new column is added to filters.\n\n        Args:\n          obj (Widget): the object that has an attribute change\n          attrname (str): name of the attribute\n          old (type): the previous value of unknown type\n          new (type): the new value of unknown type\n\n        \"\"\"\n        self.clear_selections(obj, attrname, old, new)\n\n        # add new widget as required for each column set to filter on\n        column_descriptor_dict = self.column_descriptor_dict()\n        for col in self.filtering_columns:\n\n            metadata = column_descriptor_dict[col]\n            if not col in self.filter_widgets:\n\n                # discrete\n                if metadata['type'] == 'DiscreteColumn':\n                    select = MultiSelect.create(\n                        name=col,\n                        options=self.df[col].unique().tolist())\n                    self.filter_widgets[col] = select\n\n                # continuous\n                else:\n                    source = make_histogram_source(self.df[col])\n                    self.filter_sources[col] = source\n                    hist_plot = make_histogram(self.filter_sources[col],\n                                               plot_width=200, plot_height=100,\n                                               title_text_font_size='8pt',\n                                               tools='box_select'\n                    )\n                    hist_plot.title = col\n                    self.filter_widgets[col] = hist_plot\n\n        curdoc()._add_all()\n\n    def set_metadata(self):\n        \"\"\"Creates a list of dicts, containing summary info for each column.\n\n        The descriptions are stored in the ``columns`` property.\n\n        \"\"\"\n        descriptors = []\n\n        columns = self.df.columns\n        for c in columns:\n\n            # get description for column from pandas DataFrame\n            desc = self.df[c].describe()\n\n            # DiscreteColumn\n            if self.df[c].dtype == object:\n                descriptors.append({\n                    'type': \"DiscreteColumn\",\n                    'name': c,\n                    'count': desc['count'],\n                    'unique': desc['unique'],\n                    'top': desc['top'],\n                    'freq': desc['freq'],\n                })\n\n            # TimeColumn\n            elif self.df[c].dtype == np.datetime64:\n                descriptors.append({\n                    'type': \"TimeColumn\",\n                    'name': c,\n                    'count': desc['count'],\n                    'unique': desc['unique'],\n                    'first': desc['first'],\n                    'last': desc['last'],\n                })\n\n            # ContinuousColumn\n            else:\n                descriptors.append({\n                    'type': \"ContinuousColumn\",\n                    'name': c,\n                    'count': desc['count'],\n                    'mean': \"%.2f\"%desc['mean'],\n                    'std': \"%.2f\"%desc['std'],\n                    'min': \"%.2f\"%desc['min'],\n                    'max': \"%.2f\"%desc['max'],\n                })\n\n        self.columns = descriptors",
  "def __init__(self, field, value, label=None):\n        \"\"\"Sets object properties and creates label if not provided.\n\n        Args:\n          field (str): name of the column\n          value: unique value defined for the column\n          label (str, optional): string representation of the value\n\n        \"\"\"\n        if label is None:\n            label = str(value)\n        self.field = field\n        self.label = label\n        self._value = value",
  "def __repr__(self):\n        return \"%s:%s\"%(self.field, self.label)",
  "def filter(self, df):\n        \"\"\"Filters the provided DataFrame to the subset corresponding to value.\n\n        Args:\n          df (DataFrame): contains a column of ``field``\n\n        Returns:\n          DataFrame: filtered to rows, where column ``field`` has values\n            equal to ``_value``.\n\n        \"\"\"\n        return df[df[self.field] == self._value]",
  "def __init__(self, field, value, bins, label=None):\n        \"\"\"Calls parent ``DiscreteFacet`` and stores bins for later filtering.\n\n        Args:\n          field (str): name of the column\n          value (str): center of range of values in the column\n          bins (list[float]): start and inclusive stop value for the bin\n          label (str, optional): string representation\n\n        \"\"\"\n        super(ContinuousFacet, self).__init__(field, value, label=label)\n        self.bins = bins",
  "def filter(self, df):\n        \"\"\"Filters the provided DataFrame to the subset corresponding to bins.\n\n        Args:\n          df (DataFrame): contains a column of ``field``\n\n        Returns:\n          DataFrame: filtered to rows, where column ``field`` has values\n            within the bounds of ``bins``.\n\n        \"\"\"\n        if self.bins[0] is not None:\n            df = df[df[self.field] > self.bins[0]]\n        if self.bins[1] is not None:\n            df = df[df[self.field] <= self.bins[1]]\n        return df",
  "def __init__(self, *args, **kwargs):\n        \"\"\"Creates original and filtered ColumnDataSource and handles defaults.\n\n        The df and starting configuration are only provided the first time\n        init is called, within the create method.\n\n        Kwargs:\n          df (DataFrame): the data to use in the crossfilter app\n          plot_type (str, optional): starting plot type\n          agg (str, optional): starting aggregation type\n\n        \"\"\"\n        if 'df' in kwargs:\n            self._df = kwargs.pop('df')\n\n            # initialize a \"pure\" and filtered data source based on df\n            kwargs['data'] = ColumnDataSource(data=self.df)\n            kwargs['filtered_data'] = ColumnDataSource(data=self.df)\n\n        # default plot type\n        if 'plot_type' not in kwargs:\n            kwargs['plot_type'] = \"scatter\"\n\n        # default aggregation type\n        if 'agg' not in kwargs:\n            kwargs['agg'] = 'sum'\n\n        if 'plot_map' in kwargs:\n            self.plot_map = kwargs.pop('plot_map')\n\n        super(CrossFilter, self).__init__(**kwargs)",
  "def create(cls, **kwargs):\n        \"\"\"Performs all one-time construction of bokeh objects.\n\n        This classmethod is required due to the way that bokeh handles the\n        python and javascript components. The initialize method will be\n        called each additional time the app is updated (including once in\n        the create method), but the PlotObject infrastructure will find that\n        the object already exists in any future calls, and will not create a\n        new object.\n\n        Kwargs:\n          df (DataFrame): the data to use in the crossfilter app\n          plot_type (str, optional): starting plot type\n          agg (str, optional): starting aggregation type\n\n        :return:\n        \"\"\"\n        obj = cls(**kwargs)\n        obj.set_metadata()\n        choices = obj.make_plot_choices()\n        obj.update_plot_choices(choices)\n        obj.set_plot()\n        obj.set_input_selector()\n        return obj",
  "def set_input_selector(self):\n        \"\"\"Creates and configures each selector (drop-down menu).\"\"\"\n\n        col_names = [x['name'] for x in self.columns]\n\n        self.plot_selector = Select.create(\n            title=\"PlotType\",\n            name=\"plot_type\",\n            value=self.plot_type,\n            options=[\"line\", \"scatter\", \"bar\"],\n        )\n\n        self.x_selector = Select.create(\n            name=\"x\",\n            value=self.x,\n            options=col_names,\n        )\n\n        self.y_selector = Select.create(\n            name=\"y\",\n            value=self.y,\n            options=col_names,\n        )\n\n        self.agg_selector = Select.create(\n            name='agg',\n            value=self.agg,\n            options=['sum', 'mean', 'last'],\n        )",
  "def update_plot_choices(self, input_dict):\n        \"\"\"Sets object attributes corresponding to input_dict's values.\n\n        Args:\n          input_dict (dict): dict with x, y, and plot_type keys\n\n        \"\"\"\n        for k, v in input_dict.items():\n            if getattr(self, k) is None:\n                setattr(self, k, v)",
  "def get_plot_class(self):\n        \"\"\"Return the class for the current plot selection.\"\"\"\n        return self.plot_map[self.plot_type]",
  "def column_descriptor_dict(self):\n        \"\"\"Creates column stats dict with keys of column names.\n\n        Returns:\n          dict: dict with key per column in data, where values are column stats\n\n        \"\"\"\n        column_descriptors = {}\n        for x in self.columns:\n            column_descriptors[x['name']] = x\n        return column_descriptors",
  "def continuous_columns(self):\n        \"\"\"Returns list of column descriptors for the non-Discrete columns.\n\n        Returns:\n          list(dict): list of dicts, containing metadata about columns\n\n        \"\"\"\n        return [x for x in self.columns if x['type'] != 'DiscreteColumn']",
  "def discrete_columns(self):\n        \"\"\"Returns list of column descriptors for the Discrete columns.\n\n        Returns:\n          list(dict): list of dicts, containing metadata about columns\n\n        \"\"\"\n        return [x for x in self.columns if x['type'] == 'DiscreteColumn']",
  "def make_plot_choices(self):\n        \"\"\"Selects first two continuous columns for x,y during initial setup\n\n        Returns:\n          dict: x, y, and plot_type keys and values for initial setup\n\n        \"\"\"\n        x, y = [x['name'] for x in self.continuous_columns()[:2]]\n        return {'x': x, 'y': y, 'plot_type': 'scatter'}",
  "def set_plot(self):\n        \"\"\"Makes and sets the plot based on the current configuration of app.\"\"\"\n\n        self.update_xy_ranges(source=self.df)\n        plot = self.make_plot()\n        self.plot = plot\n        curdoc()._add_all()",
  "def make_plot(self):\n        \"\"\"Makes the correct plot layout type, based on app's current config.\n\n        Returns:\n          PlotObject: one plot, grid of plots, or tabs of plots/grids of plots\n\n        \"\"\"\n\n        if self.facet_tab:\n            facets = self.make_facets(dimension='tab')\n\n            # generate a list of panels, containing plot/plots for each facet\n            tabs = [self.make_tab(content=self.create_plot_page(\n                tab_facet=facet), tab_label=self.facet_title(facet)) for facet\n                    in facets]\n            return Tabs(tabs=tabs)\n        else:\n            return self.create_plot_page()",
  "def create_plot_page(self, tab_facet=None):\n        \"\"\"Generates a single visible page of a plot or plots.\n\n        Args:\n          tab_facet (DiscreteFacet or ContinuousFacet): a facet to filter on\n\n        Returns:\n          PlotObject: a single or grid of plots\n\n        \"\"\"\n        # no faceting\n        if all([len(self.facet_x) == 0,\n                len(self.facet_y) == 0]):\n            plot_page = self.make_single_plot(facet=tab_facet)\n\n        # x xor y faceting\n        if all([(len(self.facet_x) != 0) ^ (len(self.facet_y) != 0)]):\n            plot_page = self.make_1d_facet_plot(facet=tab_facet)\n\n        # x and y faceting\n        if all([len(self.facet_x) != 0,\n                len(self.facet_y) != 0]):\n            plot_page = self.make_2d_facet_plot(facet=tab_facet)\n\n        if isinstance(plot_page, GridPlot):\n            self.apply_grid_style(plot_page)\n\n        return plot_page",
  "def make_tab(content, tab_label):\n        \"\"\"Creates a container for the contents of a tab.\n\n        Args:\n          content (PlotObject): the primary content of the tab\n          tab_label (str): the text to place in the tab\n\n        Returns:\n          Panel: represents a single tab in a group of tabs\n\n        \"\"\"\n        return Panel(child=content, title=tab_label)",
  "def make_facets(self, dimension):\n        \"\"\"Creates combination of all facets for the provided dimension\n\n        Args:\n          dimension (str): name of the dimension to create facets for\n\n        Returns:\n          list(list(DiscreteFacet or ContinuousFacet)): list of list of\n            unique facet combinations\n\n        \"\"\"\n        if dimension == 'x':\n            facets = self.facet_x\n        elif dimension == 'y':\n            facets = self.facet_y\n        else:\n            facets = self.facet_tab\n\n        # create facets for each column\n        column_descriptor_dict = self.column_descriptor_dict()\n        all_facets = [[]]\n        for field in facets:\n\n            # create facets from discrete columns\n            if column_descriptor_dict[field]['type'] == 'DiscreteColumn':\n                field_facets = [DiscreteFacet(field, val) for val in\n                                np.unique(self.df[field].values)]\n\n                # combine any facets as required\n                all_facets = cross(all_facets, field_facets)\n            else:\n                # create quantile based discrete data and pairs of bins\n                categorical, bins = pd.qcut(self.df[field], 4, retbins=True)\n                cats = categorical.cat.categories\n                bins = [[bins[idx], bins[idx + 1]] for idx in\n                        range(len(bins) - 1)]\n                bins[0][0] = None\n\n                # create list of facets\n                field_facets = [ContinuousFacet(field, value, bin) for\n                                bin, value in zip(bins, cats)]\n\n                # combine any facets as required\n                all_facets = cross(all_facets, field_facets)\n\n        return all_facets",
  "def facet_title(facets):\n        \"\"\"Joins list of facets by commas.\n\n        Args:\n          facets (list(DiscreteFacet or ContinuousFacet)): list of facets,\n          which are a combination of column and unique value within it\n\n        Returns:\n          str: string representation of the combination of facets\n\n        \"\"\"\n        title = \",\".join([str(x) for x in facets])\n        return title",
  "def facet_data(self, facets, df=None):\n        \"\"\"Filters data to the rows associated with the given facet.\n\n        Args:\n          facets (list(DiscreteFacet or ContinuousFacet)): list of facets,\n          which are a combination of column and unique value within it\n          df (DataFrame, optional): data to be filtered on\n\n        Returns:\n          DataFrame: filtered DataFrame based on provided facets\n\n        \"\"\"\n        if df is None:\n            df = self.filtered_df\n        for f in facets:\n            df = f.filter(df)\n        return df",
  "def make_1d_facet_plot(self, facet=None):\n        \"\"\"Creates the faceted plots when a facet is added to the x axis.\n\n        Returns:\n          GridPlot: a grid of plots, where each plot has subset of data\n\n        \"\"\"\n        if self.facet_x:\n            all_facets = self.make_facets('x')\n        else:\n            all_facets = self.make_facets('y')\n\n        plots = []\n\n        # loop over facets and create single plots for data subset\n        for facets in all_facets:\n            title = self.facet_title(facets)\n\n            if facet:\n                facets += facet\n\n            df = self.facet_data(facets, self.filtered_df)\n            plot = self.make_single_plot(\n                df=df, title=title, plot_height=200, plot_width=200,\n                tools=\"pan,wheel_zoom,reset\", facet=facets\n            )\n\n            # append single plot to list of plots\n            plots.append(plot)\n\n        # create squarish grid based on number of plots\n        chunk_size = int(np.ceil(np.sqrt(len(plots))))\n\n        # create list of lists of plots, where each list of plots is a row\n        grid_plots = []\n        for i in range(0, len(plots), chunk_size):\n            chunk = plots[i:i + chunk_size]\n            grid_plots.append(chunk)\n\n        self.hide_internal_axes(grid_plots)\n\n        # return the grid as the plot\n        return GridPlot(children=grid_plots, plot_width=200*chunk_size)",
  "def make_2d_facet_plot(self, facet=None):\n        \"\"\"Creates the grid of plots when there are both x and y facets.\n\n        Returns:\n          GridPlot: grid of x and y facet combinations\n\n        \"\"\"\n\n        # ToDo: gracefully handle large combinations of facets\n        all_facets_x = self.make_facets('x')\n        all_facets_y = self.make_facets('y')\n\n        grid_plots = []\n\n        # y faceting down column\n        for facets_y in all_facets_y:\n\n            # x faceting across row\n            row = []\n            for facets_x in all_facets_x:\n\n                # build the facets and title\n                facets = facets_x + facets_y\n                title = self.facet_title(facets)\n\n                # must filter by any extra facets provided for facet tab\n                if facet:\n                    filter_facets = facets + facet\n                else:\n                    filter_facets = facets\n\n                df = self.facet_data(filter_facets, self.filtered_df)\n                plot = self.make_single_plot(\n                    df=df, title=title, plot_height=200, plot_width=200,\n                    tools=\"pan,wheel_zoom,reset\", facet=facets\n                )\n                row.append(plot)\n\n            # append the row to the list of rows\n            grid_plots.append(row)\n\n            self.hide_internal_axes(grid_plots)\n\n        # return the grid of plots as the plot\n        return GridPlot(children=grid_plots, plot_width=200*len(all_facets_x))",
  "def apply_facet_style(plot):\n        \"\"\"Applies facet-specific style for a given plot.\n\n        Override this method to modify the look of a customized CrossFilter\n        for all plugins. Or, apply custom styles in the plugin, since the\n        plugin will be told if it is currently being faceted.\n\n        \"\"\"\n        plot.title_text_font_size = \"9pt\"\n        plot.min_border = 0",
  "def apply_single_plot_style(self, plot):\n        \"\"\"Applies styles when we have only one plot.\n\n        Override this method to modify the look of a customized CrossFilter\n        for all plugins.\n\n        \"\"\"\n        plot.min_border_left = 60",
  "def apply_grid_style(self, grid_plot):\n        \"\"\"Applies facet-specific style for the grid of faceted plots.\n\n        Override this method to modify the look of a customized CrossFilter\n        for all plugins. Or, apply custom styles in the plugin, since the\n        plugin will be told if it is currently being faceted.\n\n        \"\"\"\n        grid_plot.title_text_font_size = \"12pt\"\n        grid_plot.title_text_font_style = \"bold\"\n        grid_plot.title = self.title",
  "def hide_internal_axes(grid_plots):\n        \"\"\"Hides the internal axes for a grid of plots.\n\n        Args:\n          grid_plots (list(list(Figure))): list of rows (list), containing plots\n\n        \"\"\"\n        for i, row in enumerate(grid_plots):\n            is_bottom = i + 1 == len(grid_plots)\n\n            for j, plot in enumerate(row):\n                if j != 0:\n                    if is_bottom:\n                        hide_axes(plot, axes='y')\n                    else:\n                        hide_axes(plot)\n\n                elif j == 0 and not is_bottom:\n                    hide_axes(plot, axes='x')",
  "def make_single_plot(self, df=None, title=None,\n                         plot_width=700,\n                         plot_height=680,\n                         tools=\"pan,wheel_zoom,box_zoom,save,resize,\"\n                               \"box_select,reset\",\n                         facet=None):\n        \"\"\"Creates a plot based on the current app configuration.\n\n        Args:\n          df (DataFrame, optional): data to use for the plot\n          title (str, optional): plot title\n          plot_width (float, optional): width of plot in pixels\n          plot_height (float, optional): height of plot in pixels\n          tools (str, optional): comma separated string of tool names\n\n        Returns:\n          PlotObject: the generated plot\n\n        \"\"\"\n        faceting = False\n\n        # df is not provided when we are not faceting\n        if df is None:\n            source = self.filtered_data\n        else:\n            df = self.facet_data(facets=facet, df=df)\n\n            # create column data source with filtered df\n            source = ColumnDataSource(data=df)\n            faceting = True\n\n        # check for tab faceting and filter if provided\n        if facet:\n            df = self.facet_data(facets=facet, df=df)\n            source = ColumnDataSource(data=df)\n\n        # get the helper class for the plot type selected\n        plot_class = self.get_plot_class()\n\n        # initialize the plugin class\n        plugin = plot_class(source=source,\n                            title_text_font_size=\"12pt\",\n                            title_text_font_style = \"bold\",\n                            plot_height=plot_height,\n                            plot_width=plot_width,\n                            tools=tools,\n                            title=title,\n                            x_range=self.x_range,\n                            y_range=self.y_range,\n                            facet=faceting,\n                            crossfilter=self)\n\n        # generate plot\n        plot = plugin.get_plot()\n\n        # apply faceting-specific styling if required\n        if facet:\n            self.apply_facet_style(plot)\n            self.title = plugin.title\n        else:\n            self.apply_single_plot_style(plot)\n            self.title = plot.title\n\n        return plot",
  "def update_xy_ranges(self, source):\n        \"\"\"Updates common x_range, y_range to use for creating figures.\n\n        Args:\n          source (ColumnDataSource): the source to return correct range for\n\n        \"\"\"\n        plt_cls = self.get_plot_class()\n        x_range, y_range = plt_cls.make_xy_ranges(cf=self)\n\n        # store x and y range from the plot class\n        self.x_range = x_range\n        self.y_range = y_range",
  "def plot_attribute_change(self, obj, attrname, old, new):\n        \"\"\"Updates app's attribute and plot when view configuration changes.\n\n        Args:\n          obj (Widget): the object that has an attribute change\n          attrname (str): name of the attribute\n          old (type): the previous value of unknown type\n          new (type): the new value of unknown type\n\n        \"\"\"\n        setattr(self, obj.name, new)\n        self.set_plot()",
  "def facet_change(self, obj, attrname, old, new):\n        \"\"\"Updates plot when any facet configuration changes.\n\n        Args:\n          obj (Widget): the object that has an attribute change\n          attrname (str): name of the attribute\n          old (type): the previous value of unknown type\n          new (type): the new value of unknown type\n\n        \"\"\"\n        self.set_plot()",
  "def df(self):\n        \"\"\"The core data that is used by the app for plotting.\n\n        Returns:\n          DataFrame: the original data structure\n\n        \"\"\"\n        if hasattr(self, '_df'):\n            return self._df\n        else:\n            if self.data:\n                return self.data.to_df()",
  "def filtered_df(self):\n        \"\"\"The subset of the data to use for plotting.\n\n        Returns:\n          DataFrame: the original data structure\n\n        \"\"\"\n        if hasattr(self, '_df'):\n            return self._df\n        else:\n            if self.filtered_data:\n                return self.filtered_data.to_df()",
  "def update(self, **kwargs):\n        \"\"\"Updates CrossFilter attributes each time the model changes.\n\n        The events are setup each time so that we can add event handlers to\n        the selection/filtering widgets as they are added.\n\n        \"\"\"\n        super(CrossFilter, self).update(**kwargs)\n        self.setup_events()",
  "def setup_events(self):\n        \"\"\"Registers events each time the app changes state.\"\"\"\n\n        # watch the app's filtering_columns attribute to setup filters\n        self.on_change('filtering_columns', self, 'setup_filter_widgets')\n\n        # register any available filter widget\n        for obj in self.filter_widgets.values():\n            if isinstance(obj, InputWidget):\n                obj.on_change('value', self, 'handle_filter_selection')\n\n        # watch app column data source attribute for changes\n        for obj in self.filter_sources.values():\n            obj.on_change('selected', self, 'handle_filter_selection')\n\n        # selector event registration\n        if self.plot_selector:\n            self.plot_selector.on_change('value', self, 'plot_attribute_change')\n        if self.x_selector:\n            self.x_selector.on_change('value', self, 'plot_attribute_change')\n        if self.y_selector:\n            self.y_selector.on_change('value', self, 'plot_attribute_change')\n        if self.agg_selector:\n            self.agg_selector.on_change('value', self, 'plot_attribute_change')\n\n        # register to watch the app's facet attributes\n        self.on_change('facet_x', self, 'facet_change')\n        self.on_change('facet_y', self, 'facet_change')\n        self.on_change('facet_tab', self, 'facet_change')",
  "def handle_filter_selection(self, obj, attrname, old, new):\n        \"\"\"Filters the data source whenever a filter widget changes.\n\n        Args:\n          obj (Widget): the object that has an attribute change\n          attrname (str): name of the attribute\n          old (type): the previous value of unknown type\n          new (type): the new value of unknown type\n\n        \"\"\"\n        df = self.df\n\n        # loop over the column metadata\n        for descriptor in self.columns:\n            colname = descriptor['name']\n\n            # handle discrete selections\n            if descriptor['type'] == 'DiscreteColumn' and \\\n                            colname in self.filter_widgets:\n                selected = self.filter_widgets[colname].value\n                if not selected:\n                    continue\n                if isinstance(selected, six.string_types):\n                    df = df[colname == selected]\n                else:\n                    df = df[np.in1d(df[colname], selected)]\n\n            # handle time or continuous selections\n            elif descriptor['type'] in ('TimeColumn', 'ContinuousColumn') and \\\n                            colname in self.filter_widgets:\n                obj = self.filter_sources[colname]\n\n                # hack because we don't have true range selection\n                if not obj.selected:\n                    continue\n                min_idx = np.min(obj.selected)\n                max_idx = np.max(obj.selected)\n\n                min_val = obj.data['centers'][min_idx]\n                max_val = obj.data['centers'][max_idx]\n                df = df[(df[colname] >= min_val) & (df[colname] <= max_val)]\n\n        # update filtered data and force plot update\n        for colname in self.data.column_names:\n            self.filtered_data.data[colname] = df[colname]\n            self.filtered_data._dirty = True\n        self.set_plot()",
  "def clear_selections(self, obj, attrname, old, new):\n        \"\"\"Updates filter widgets and sources as they are removed.\n\n        Args:\n          obj (Widget): the object that has an attribute change\n          attrname (str): name of the attribute\n          old (type): the previous value of unknown type\n          new (type): the new value of unknown type\n\n        \"\"\"\n        diff = set(old) - set(new)\n        column_descriptor_dict = self.column_descriptor_dict()\n\n        # delete any removed filter widgets\n        if len(diff) > 0:\n            for col in diff:\n                metadata = column_descriptor_dict[col]\n                if metadata['type'] != 'DiscreteColumn':\n                    del self.filter_sources[col]\n                del self.filter_widgets[col]\n\n        # update the data based on latest changes\n        if diff:\n            self.handle_filter_selection(obj, attrname, old, new)",
  "def setup_filter_widgets(self, obj, attrname, old, new):\n        \"\"\"Creates new filter widget each time a new column is added to filters.\n\n        Args:\n          obj (Widget): the object that has an attribute change\n          attrname (str): name of the attribute\n          old (type): the previous value of unknown type\n          new (type): the new value of unknown type\n\n        \"\"\"\n        self.clear_selections(obj, attrname, old, new)\n\n        # add new widget as required for each column set to filter on\n        column_descriptor_dict = self.column_descriptor_dict()\n        for col in self.filtering_columns:\n\n            metadata = column_descriptor_dict[col]\n            if not col in self.filter_widgets:\n\n                # discrete\n                if metadata['type'] == 'DiscreteColumn':\n                    select = MultiSelect.create(\n                        name=col,\n                        options=self.df[col].unique().tolist())\n                    self.filter_widgets[col] = select\n\n                # continuous\n                else:\n                    source = make_histogram_source(self.df[col])\n                    self.filter_sources[col] = source\n                    hist_plot = make_histogram(self.filter_sources[col],\n                                               plot_width=200, plot_height=100,\n                                               title_text_font_size='8pt',\n                                               tools='box_select'\n                    )\n                    hist_plot.title = col\n                    self.filter_widgets[col] = hist_plot\n\n        curdoc()._add_all()",
  "def set_metadata(self):\n        \"\"\"Creates a list of dicts, containing summary info for each column.\n\n        The descriptions are stored in the ``columns`` property.\n\n        \"\"\"\n        descriptors = []\n\n        columns = self.df.columns\n        for c in columns:\n\n            # get description for column from pandas DataFrame\n            desc = self.df[c].describe()\n\n            # DiscreteColumn\n            if self.df[c].dtype == object:\n                descriptors.append({\n                    'type': \"DiscreteColumn\",\n                    'name': c,\n                    'count': desc['count'],\n                    'unique': desc['unique'],\n                    'top': desc['top'],\n                    'freq': desc['freq'],\n                })\n\n            # TimeColumn\n            elif self.df[c].dtype == np.datetime64:\n                descriptors.append({\n                    'type': \"TimeColumn\",\n                    'name': c,\n                    'count': desc['count'],\n                    'unique': desc['unique'],\n                    'first': desc['first'],\n                    'last': desc['last'],\n                })\n\n            # ContinuousColumn\n            else:\n                descriptors.append({\n                    'type': \"ContinuousColumn\",\n                    'name': c,\n                    'count': desc['count'],\n                    'mean': \"%.2f\"%desc['mean'],\n                    'std': \"%.2f\"%desc['std'],\n                    'min': \"%.2f\"%desc['min'],\n                    'max': \"%.2f\"%desc['max'],\n                })\n\n        self.columns = descriptors",
  "class CrossFilterPlugin(object):\n    \"\"\"An adapter class between CrossFilter and custom plotting plugins.\n\n    This adapter is used to provide a consistent interface between single\n    plot generation and CrossFilter through some core behaviors. A simple\n    plugin can simply override only make_plot to provide a custom plot type.\n\n    See CrossLinePlugin for a simple example.\n\n    Kwargs:\n      crossfilter (CrossFilter): reference to the CrossFilter app\n      source (ColumnDataSource): the pre-filtered source for the plot\n      x_range (Range): the common x range to use for plotting\n      y_range (Range): the common y range to use for plotting\n      title_text_font_size (str): string of font size, e.g., \"12pt\"\n      title_text_font_style (str): string of font style, e.g., \"bold\"\n      plot_height (float): height of the plot in pixels\n      plot_width (float): width of the plot in pixels\n      tools (str): the string of tools to add to the plot\n      facet (bool): whether or not we are faceting\n      title (str, optional): overrides the derived title property\n\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        self.cf = kwargs.pop('crossfilter', None)\n        self.x = self.cf.x\n        self.y = self.cf.y\n        self.source = kwargs.pop('source', None)\n        self.facet = kwargs.pop('facet', False)\n        self.col_meta = self.cf.column_descriptor_dict()\n        self.valid_plot = True\n\n        # get any provided title, else generate our own\n        self._title = kwargs.pop('title', None)\n        if not self._title:\n            self._title = self.title\n        self.args = args\n        self.kwargs = kwargs\n\n    def get_plot(self):\n        \"\"\"Validates, makes blank figure, then makes the plot as necessary.\n\n        It is meant for this method to not be overridden. The methods called\n        by this method should be overridden first, and this one only as a last\n        resort, since it provides a common interface for plugins.\n\n        \"\"\"\n        self.validate_plot()\n        plot = self.make_figure(**self.kwargs)\n        if self.valid_plot:\n            plot = self.make_plot(plot)\n        return plot\n\n    def make_figure(self, **kwargs):\n        \"\"\"Generates the blank figure for the provided options.\n\n        The purpose of this is to avoid for child plugins to have to repeat\n        this functionality. It is likely to not need to override this behavior.\n\n        \"\"\"\n        kwargs['title'] = self._title\n        return figure(**kwargs)\n\n    def make_plot(self, plot):\n        \"\"\"Plots the data into the provided plot.\n\n        The primary method to be overridden to create a custom plugin.\n\n        \"\"\"\n        plot.scatter(self.x, self.y, source=self.source)\n        return plot\n\n    def validate_plot(self):\n        \"\"\"Called before plotting data to check to see if we should plot.\n\n        If valid_plot is set to False in this method, then the figure will\n        still be created, but no data will be plotted.\n\n        \"\"\"\n        if not self.facet:\n            if len(self.source.data[self.x]) == 0 or len(\n                    self.source.data[self.y]) == 0:\n                self._title = 'All data is filtered out.'\n                self.valid_plot = False\n\n    @property\n    def title(self):\n        \"\"\"Generates a title for the plot, and can be overridden per plugin.\n\n        Returns:\n          str: a string to place into the title that provides additional plot info\n\n        \"\"\"\n        return \"%s vs. %s\" % (self.y.title(), self.x.title())\n\n    @property\n    def x_type(self):\n        return self.col_meta[self.x]['type']\n\n    @property\n    def y_type(self):\n        return self.col_meta[self.y]['type']\n\n    @property\n    def df(self):\n        return self.source.to_df()\n\n    @staticmethod\n    def make_xy_ranges(cf):\n        \"\"\"Generates x/y ranges specific to the plot type for the plugin.\n\n        This method is static so that CrossFilter can generate common ranges\n        to construct multiple plots from the plugin. A custom plugin would\n        override this method if it has unique range types.\n\n        Args:\n          cf (CrossFilter): a reference to the CrossFilter object, used to\n            get data to generate the ranges.\n\n        Returns:\n          (Range, Range): returns xrange, yrange\n\n        \"\"\"\n        col_meta = cf.column_descriptor_dict()\n        df = cf.df\n\n        if col_meta[cf.x]['type'] == 'DiscreteColumn':\n            x_range = FactorRange(factors=sorted(set(df[cf.x])))\n        else:\n            x_vals = df[cf.x]\n            x_range = DataRange1d(start=x_vals.min(), end=x_vals.max())\n\n        if col_meta[cf.y]['type'] == 'DiscreteColumn':\n            y_range = FactorRange(factors=sorted(set(df[cf.y])))\n        else:\n            y_vals = df[cf.y]\n            y_range = DataRange1d(start=y_vals.min(), end=y_vals.max())\n\n        return x_range, y_range",
  "class CrossBarPlugin(CrossFilterPlugin):\n    \"\"\"Bar plot plugin for CrossFilter.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n\n        cf = kwargs['crossfilter']\n        self.agg = cf.agg\n        super(CrossBarPlugin, self).__init__(*args, **kwargs)\n\n    def make_plot(self, plot):\n        self.transform_data()\n        y = [val/2.0 for val in self.source.data[self.y]]\n        plot.rect(self.x, y, self.bar_width, self.y, source=self.source)\n        plot.h_symmetry = False\n        plot.v_symmetry = False\n\n        select_tool = _get_select_tool(plot)\n        if select_tool:\n            select_tool.dimensions = ['width']\n        return plot\n\n    def transform_data(self):\n        \"\"\"Generates custom source that describes the bars to be plotted.\"\"\"\n        width_factor = 0.8\n\n        if self.x_type != 'DiscreteColumn':\n            self.source = make_continuous_bar_source(self.df, self.x, self.y,\n                                                     self.agg)\n            x_vals = self.source.data[self.x]\n            if len(x_vals) >= 2:\n                self.bar_width = np.min(np.diff(x_vals) * width_factor)\n            else:\n                self.bar_width = width_factor\n        else:\n            self.source = make_categorical_bar_source(self.df, self.x, self.y,\n                                                      self.agg)\n            self.bar_width = width_factor\n\n    def validate_plot(self):\n        super(CrossBarPlugin, self).validate_plot()\n\n        if self.y_type == 'DiscreteColumn':\n            self._title = 'Bar does not support discrete y column'\n            self.valid_plot = False\n\n        if self.x == self.y:\n            self._title = 'Bar does not support x and y of same column'\n            self.valid_plot = False\n\n        if self.df.empty:\n            if not self.facet:\n                self._title = 'All data is filtered out'\n            self.valid_plot = False\n\n    @property\n    def title(self):\n        return \"%s(%s) by %s\" % (self.agg.title(), self.y.title(),\n                                 self.x.title())\n\n    @staticmethod\n    def make_xy_ranges(cf, bar_width=0.7):\n        \"\"\"Returns ranges for a given bar width.\n\n        Args:\n          cf (CrossFilter): the CrossFilter app\n          bar_width (float, optional): width of bar that affects ranges\n\n        Returns:\n          (xrange, yrange): the x/y ranges to use for the bar plot\n\n        \"\"\"\n        df = cf.filtered_df\n        col_meta = cf.column_descriptor_dict()\n\n        # only return new ranges if x and y aren't identical\n        if cf.x != cf.y:\n\n            # create x range\n            if col_meta[cf.x]['type'] != 'DiscreteColumn':\n                source = make_continuous_bar_source(df, cf.x, cf.y, cf.agg)\n                x_range = Range1d(start=df[cf.x].min() - bar_width,\n                                  end=df[cf.x].max() + bar_width)\n            else:\n                source = make_categorical_bar_source(df, cf.x, cf.y, cf.agg)\n                x_range = FactorRange(factors=source.data[cf.x])\n\n            # create y range\n            top = np.max(source.data[cf.y]) * 1.05\n            y_range = Range1d(start=0, end=top)\n            return x_range, y_range\n        else:\n            return cf.x_range, cf.y_range",
  "class CrossScatterPlugin(CrossFilterPlugin):\n    \"\"\"Scatter plot plugin for CrossFilter.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(CrossScatterPlugin, self).__init__(*args, **kwargs)",
  "class CrossLinePlugin(CrossFilterPlugin):\n    \"\"\"Line plot plugin for CrossFilter.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(CrossLinePlugin, self).__init__(*args, **kwargs)\n\n    def make_plot(self, plot):\n        plot.line(self.x, self.y, source=self.source)\n        return plot",
  "def __init__(self, *args, **kwargs):\n        self.cf = kwargs.pop('crossfilter', None)\n        self.x = self.cf.x\n        self.y = self.cf.y\n        self.source = kwargs.pop('source', None)\n        self.facet = kwargs.pop('facet', False)\n        self.col_meta = self.cf.column_descriptor_dict()\n        self.valid_plot = True\n\n        # get any provided title, else generate our own\n        self._title = kwargs.pop('title', None)\n        if not self._title:\n            self._title = self.title\n        self.args = args\n        self.kwargs = kwargs",
  "def get_plot(self):\n        \"\"\"Validates, makes blank figure, then makes the plot as necessary.\n\n        It is meant for this method to not be overridden. The methods called\n        by this method should be overridden first, and this one only as a last\n        resort, since it provides a common interface for plugins.\n\n        \"\"\"\n        self.validate_plot()\n        plot = self.make_figure(**self.kwargs)\n        if self.valid_plot:\n            plot = self.make_plot(plot)\n        return plot",
  "def make_figure(self, **kwargs):\n        \"\"\"Generates the blank figure for the provided options.\n\n        The purpose of this is to avoid for child plugins to have to repeat\n        this functionality. It is likely to not need to override this behavior.\n\n        \"\"\"\n        kwargs['title'] = self._title\n        return figure(**kwargs)",
  "def make_plot(self, plot):\n        \"\"\"Plots the data into the provided plot.\n\n        The primary method to be overridden to create a custom plugin.\n\n        \"\"\"\n        plot.scatter(self.x, self.y, source=self.source)\n        return plot",
  "def validate_plot(self):\n        \"\"\"Called before plotting data to check to see if we should plot.\n\n        If valid_plot is set to False in this method, then the figure will\n        still be created, but no data will be plotted.\n\n        \"\"\"\n        if not self.facet:\n            if len(self.source.data[self.x]) == 0 or len(\n                    self.source.data[self.y]) == 0:\n                self._title = 'All data is filtered out.'\n                self.valid_plot = False",
  "def title(self):\n        \"\"\"Generates a title for the plot, and can be overridden per plugin.\n\n        Returns:\n          str: a string to place into the title that provides additional plot info\n\n        \"\"\"\n        return \"%s vs. %s\" % (self.y.title(), self.x.title())",
  "def x_type(self):\n        return self.col_meta[self.x]['type']",
  "def y_type(self):\n        return self.col_meta[self.y]['type']",
  "def df(self):\n        return self.source.to_df()",
  "def make_xy_ranges(cf):\n        \"\"\"Generates x/y ranges specific to the plot type for the plugin.\n\n        This method is static so that CrossFilter can generate common ranges\n        to construct multiple plots from the plugin. A custom plugin would\n        override this method if it has unique range types.\n\n        Args:\n          cf (CrossFilter): a reference to the CrossFilter object, used to\n            get data to generate the ranges.\n\n        Returns:\n          (Range, Range): returns xrange, yrange\n\n        \"\"\"\n        col_meta = cf.column_descriptor_dict()\n        df = cf.df\n\n        if col_meta[cf.x]['type'] == 'DiscreteColumn':\n            x_range = FactorRange(factors=sorted(set(df[cf.x])))\n        else:\n            x_vals = df[cf.x]\n            x_range = DataRange1d(start=x_vals.min(), end=x_vals.max())\n\n        if col_meta[cf.y]['type'] == 'DiscreteColumn':\n            y_range = FactorRange(factors=sorted(set(df[cf.y])))\n        else:\n            y_vals = df[cf.y]\n            y_range = DataRange1d(start=y_vals.min(), end=y_vals.max())\n\n        return x_range, y_range",
  "def __init__(self, *args, **kwargs):\n\n        cf = kwargs['crossfilter']\n        self.agg = cf.agg\n        super(CrossBarPlugin, self).__init__(*args, **kwargs)",
  "def make_plot(self, plot):\n        self.transform_data()\n        y = [val/2.0 for val in self.source.data[self.y]]\n        plot.rect(self.x, y, self.bar_width, self.y, source=self.source)\n        plot.h_symmetry = False\n        plot.v_symmetry = False\n\n        select_tool = _get_select_tool(plot)\n        if select_tool:\n            select_tool.dimensions = ['width']\n        return plot",
  "def transform_data(self):\n        \"\"\"Generates custom source that describes the bars to be plotted.\"\"\"\n        width_factor = 0.8\n\n        if self.x_type != 'DiscreteColumn':\n            self.source = make_continuous_bar_source(self.df, self.x, self.y,\n                                                     self.agg)\n            x_vals = self.source.data[self.x]\n            if len(x_vals) >= 2:\n                self.bar_width = np.min(np.diff(x_vals) * width_factor)\n            else:\n                self.bar_width = width_factor\n        else:\n            self.source = make_categorical_bar_source(self.df, self.x, self.y,\n                                                      self.agg)\n            self.bar_width = width_factor",
  "def validate_plot(self):\n        super(CrossBarPlugin, self).validate_plot()\n\n        if self.y_type == 'DiscreteColumn':\n            self._title = 'Bar does not support discrete y column'\n            self.valid_plot = False\n\n        if self.x == self.y:\n            self._title = 'Bar does not support x and y of same column'\n            self.valid_plot = False\n\n        if self.df.empty:\n            if not self.facet:\n                self._title = 'All data is filtered out'\n            self.valid_plot = False",
  "def title(self):\n        return \"%s(%s) by %s\" % (self.agg.title(), self.y.title(),\n                                 self.x.title())",
  "def make_xy_ranges(cf, bar_width=0.7):\n        \"\"\"Returns ranges for a given bar width.\n\n        Args:\n          cf (CrossFilter): the CrossFilter app\n          bar_width (float, optional): width of bar that affects ranges\n\n        Returns:\n          (xrange, yrange): the x/y ranges to use for the bar plot\n\n        \"\"\"\n        df = cf.filtered_df\n        col_meta = cf.column_descriptor_dict()\n\n        # only return new ranges if x and y aren't identical\n        if cf.x != cf.y:\n\n            # create x range\n            if col_meta[cf.x]['type'] != 'DiscreteColumn':\n                source = make_continuous_bar_source(df, cf.x, cf.y, cf.agg)\n                x_range = Range1d(start=df[cf.x].min() - bar_width,\n                                  end=df[cf.x].max() + bar_width)\n            else:\n                source = make_categorical_bar_source(df, cf.x, cf.y, cf.agg)\n                x_range = FactorRange(factors=source.data[cf.x])\n\n            # create y range\n            top = np.max(source.data[cf.y]) * 1.05\n            y_range = Range1d(start=0, end=top)\n            return x_range, y_range\n        else:\n            return cf.x_range, cf.y_range",
  "def __init__(self, *args, **kwargs):\n        super(CrossScatterPlugin, self).__init__(*args, **kwargs)",
  "def __init__(self, *args, **kwargs):\n        super(CrossLinePlugin, self).__init__(*args, **kwargs)",
  "def make_plot(self, plot):\n        plot.line(self.x, self.y, source=self.source)\n        return plot",
  "def _make_method(prop_name):\n    def method(self, value):\n        setattr(self._options, prop_name, value)\n        return self\n    method.__doc__ = \"\"\" Chained method for %s option.\n    \"\"\" % prop_name\n    return method",
  "def _chained_options(opts_type):\n    def wrapper(cls):\n        orig_init = cls.__init__\n\n        cls_props = set(cls.properties())\n\n        def __init__(self, *args, **kwargs):\n            self._options = opts_type(**kwargs)\n            orig_init(self)\n\n        cls.__init__ = __init__\n\n        for prop_name in opts_type.properties():\n            if prop_name not in cls_props:\n                setattr(cls, prop_name, _make_method(prop_name))\n\n        return cls\n    return wrapper",
  "class Chart(Plot):\n    \"\"\" The main Chart class, the core of the ``Bokeh.charts`` interface.\n\n    \"\"\"\n\n    __view_model__ = \"Plot\"\n    __subtype__ = \"Chart\"\n\n    def __init__(self):\n        \"\"\"\n\n        \"\"\"\n        super(Chart, self).__init__(\n            title=self._options.title,\n            plot_height=self._options.height,\n            plot_width=self._options.width,\n            id=self._options.id or make_id()\n        )\n\n        self._glyphs = []\n        self._built = False\n\n        self._builders = []\n        self._renderer_map = []\n\n        # Add to document and session if server output is asked\n        _doc = None\n        if _doc:\n            self._doc = _doc\n        else:\n            self._doc = Document()\n\n        if self._options.server:\n            _session = None\n            if _session:\n                self._session = _session\n            else:\n                self._session = Session()\n\n        # create chart axis, grids and tools\n        self.start_plot()\n\n    def add_renderers(self, builder, renderers):\n        self.renderers += renderers\n        self._renderer_map.extend({ r._id : builder for r in renderers })\n\n    def add_builder(self, builder):\n        self._builders.append(builder)\n        builder.create(self)\n\n        # Add tools if supposed to\n        if self._options.tools:\n            # reset tools so a categorical builder can add only the\n            # supported tools\n            self.tools = []\n            self.create_tools(self._options.tools)\n\n    def create_axes(self):\n        self._xaxis = self.make_axis(\"below\", self._options.xscale, self._options.xlabel)\n        self._yaxis = self.make_axis(\"left\", self._options.yscale, self._options.ylabel)\n\n    def create_grids(self, xgrid=True, ygrid=True):\n        if xgrid:\n            self.make_grid(0, self._xaxis.ticker)\n        if ygrid:\n            self.make_grid(1, self._yaxis.ticker)\n\n    def create_tools(self, tools):\n        # if no tools customization let's create the default tools\n        if isinstance(tools, bool) and tools:\n            tools = DEFAULT_TOOLS\n        elif isinstance(tools, bool):\n            # in case tools == False just exit\n            return\n\n        tool_objs = _process_tools_arg(self, tools)\n        self.add_tools(*tool_objs)\n\n    def start_plot(self):\n        \"\"\"Add the axis, grids and tools\n        \"\"\"\n        self.create_axes()\n        self.create_grids(self._options.xgrid, self._options.ygrid)\n\n        # Add tools if supposed to\n        if self._options.tools:\n            self.create_tools(self._options.tools)\n\n    def add_legend(self, legends):\n        \"\"\"Add the legend to your plot, and the plot to a new Document.\n\n        It also add the Document to a new Session in the case of server output.\n\n        Args:\n            legends(List(Tuple(String, List(GlyphRenderer)): A list of\n                tuples that maps text labels to the legend to corresponding\n                renderers that should draw sample representations for those\n                labels.\n        \"\"\"\n        orientation = None\n        if self._options.legend is True:\n            orientation = \"top_left\"\n        else:\n            orientation = self._options.legend\n\n        if orientation:\n            legend = Legend(orientation=orientation, legends=legends)\n            self.add_layout(legend)\n\n    def make_axis(self, location, scale, label):\n        \"\"\"Create linear, date or categorical axis depending on the location,\n        scale and with the proper labels.\n\n        Args:\n            location(str): the space localization of the axis. It can be\n                ``left``, ``right``, ``above`` or ``below``.\n            scale (str): the scale on the axis. It can be ``linear``, ``datetime``\n                or ``categorical``.\n            label (str): the label on the axis.\n\n        Return:\n            axis: Axis instance\n        \"\"\"\n\n        if scale == \"linear\" or scale == \"auto\":\n            axis = LinearAxis(axis_label=label)\n        elif scale == \"datetime\":\n            axis = DatetimeAxis(axis_label=label)\n        elif scale == \"categorical\":\n            axis = CategoricalAxis(\n                major_label_orientation=np.pi / 4, axis_label=label\n            )\n\n        self.add_layout(axis, location)\n        return axis\n\n    def make_grid(self, dimension, ticker):\n        \"\"\"Create the grid just passing the axis and dimension.\n\n        Args:\n            dimension(int): the dimension of the axis, ie. xaxis=0, yaxis=1.\n            ticker (obj): the axis.ticker object\n\n        Return:\n            grid: Grid instance\n        \"\"\"\n\n        grid = Grid(dimension=dimension, ticker=ticker)\n        self.add_layout(grid)\n\n        return grid\n\n    def show(self):\n        \"\"\"Main show function.\n\n        It shows the plot in file, server and notebook outputs.\n        \"\"\"\n        # Add to document and session\n        if self._options.server:\n            if self._options.server is True:\n                self._servername = \"untitled_chart\"\n            else:\n                self._servername = self._options.server\n\n            self._session.use_doc(self._servername)\n            self._session.load_document(self._doc)\n\n        if not self._doc._current_plot == self:\n            self._doc._current_plot = self\n            self._doc.add(self)\n\n        if self._options.filename:\n            if self._options.filename is True:\n                filename = \"untitled\"\n            else:\n                filename = self._options.filename\n\n            with open(filename, \"w\") as f:\n                f.write(file_html(self._doc, INLINE, self.title))\n            print(\"Wrote %s\" % filename)\n            view(filename)\n        elif self._options.filename is False and \\\n                        self._options.server is False and \\\n                        self._options.notebook is False:\n            print(\"You have a provide a filename (filename='foo.html' or\"\n                  \" .filename('foo.html')) to save your plot.\")\n\n        if self._options.server:\n            self.session.store_document(self._doc)\n            link = self._session.object_link(self._doc.context)\n            view(link)\n\n        if self._options.notebook:\n            from bokeh.embed import notebook_div\n            publish_display_data({'text/html': notebook_div(self)})",
  "def method(self, value):\n        setattr(self._options, prop_name, value)\n        return self",
  "def wrapper(cls):\n        orig_init = cls.__init__\n\n        cls_props = set(cls.properties())\n\n        def __init__(self, *args, **kwargs):\n            self._options = opts_type(**kwargs)\n            orig_init(self)\n\n        cls.__init__ = __init__\n\n        for prop_name in opts_type.properties():\n            if prop_name not in cls_props:\n                setattr(cls, prop_name, _make_method(prop_name))\n\n        return cls",
  "def __init__(self):\n        \"\"\"\n\n        \"\"\"\n        super(Chart, self).__init__(\n            title=self._options.title,\n            plot_height=self._options.height,\n            plot_width=self._options.width,\n            id=self._options.id or make_id()\n        )\n\n        self._glyphs = []\n        self._built = False\n\n        self._builders = []\n        self._renderer_map = []\n\n        # Add to document and session if server output is asked\n        _doc = None\n        if _doc:\n            self._doc = _doc\n        else:\n            self._doc = Document()\n\n        if self._options.server:\n            _session = None\n            if _session:\n                self._session = _session\n            else:\n                self._session = Session()\n\n        # create chart axis, grids and tools\n        self.start_plot()",
  "def add_renderers(self, builder, renderers):\n        self.renderers += renderers\n        self._renderer_map.extend({ r._id : builder for r in renderers })",
  "def add_builder(self, builder):\n        self._builders.append(builder)\n        builder.create(self)\n\n        # Add tools if supposed to\n        if self._options.tools:\n            # reset tools so a categorical builder can add only the\n            # supported tools\n            self.tools = []\n            self.create_tools(self._options.tools)",
  "def create_axes(self):\n        self._xaxis = self.make_axis(\"below\", self._options.xscale, self._options.xlabel)\n        self._yaxis = self.make_axis(\"left\", self._options.yscale, self._options.ylabel)",
  "def create_grids(self, xgrid=True, ygrid=True):\n        if xgrid:\n            self.make_grid(0, self._xaxis.ticker)\n        if ygrid:\n            self.make_grid(1, self._yaxis.ticker)",
  "def create_tools(self, tools):\n        # if no tools customization let's create the default tools\n        if isinstance(tools, bool) and tools:\n            tools = DEFAULT_TOOLS\n        elif isinstance(tools, bool):\n            # in case tools == False just exit\n            return\n\n        tool_objs = _process_tools_arg(self, tools)\n        self.add_tools(*tool_objs)",
  "def start_plot(self):\n        \"\"\"Add the axis, grids and tools\n        \"\"\"\n        self.create_axes()\n        self.create_grids(self._options.xgrid, self._options.ygrid)\n\n        # Add tools if supposed to\n        if self._options.tools:\n            self.create_tools(self._options.tools)",
  "def add_legend(self, legends):\n        \"\"\"Add the legend to your plot, and the plot to a new Document.\n\n        It also add the Document to a new Session in the case of server output.\n\n        Args:\n            legends(List(Tuple(String, List(GlyphRenderer)): A list of\n                tuples that maps text labels to the legend to corresponding\n                renderers that should draw sample representations for those\n                labels.\n        \"\"\"\n        orientation = None\n        if self._options.legend is True:\n            orientation = \"top_left\"\n        else:\n            orientation = self._options.legend\n\n        if orientation:\n            legend = Legend(orientation=orientation, legends=legends)\n            self.add_layout(legend)",
  "def make_axis(self, location, scale, label):\n        \"\"\"Create linear, date or categorical axis depending on the location,\n        scale and with the proper labels.\n\n        Args:\n            location(str): the space localization of the axis. It can be\n                ``left``, ``right``, ``above`` or ``below``.\n            scale (str): the scale on the axis. It can be ``linear``, ``datetime``\n                or ``categorical``.\n            label (str): the label on the axis.\n\n        Return:\n            axis: Axis instance\n        \"\"\"\n\n        if scale == \"linear\" or scale == \"auto\":\n            axis = LinearAxis(axis_label=label)\n        elif scale == \"datetime\":\n            axis = DatetimeAxis(axis_label=label)\n        elif scale == \"categorical\":\n            axis = CategoricalAxis(\n                major_label_orientation=np.pi / 4, axis_label=label\n            )\n\n        self.add_layout(axis, location)\n        return axis",
  "def make_grid(self, dimension, ticker):\n        \"\"\"Create the grid just passing the axis and dimension.\n\n        Args:\n            dimension(int): the dimension of the axis, ie. xaxis=0, yaxis=1.\n            ticker (obj): the axis.ticker object\n\n        Return:\n            grid: Grid instance\n        \"\"\"\n\n        grid = Grid(dimension=dimension, ticker=ticker)\n        self.add_layout(grid)\n\n        return grid",
  "def show(self):\n        \"\"\"Main show function.\n\n        It shows the plot in file, server and notebook outputs.\n        \"\"\"\n        # Add to document and session\n        if self._options.server:\n            if self._options.server is True:\n                self._servername = \"untitled_chart\"\n            else:\n                self._servername = self._options.server\n\n            self._session.use_doc(self._servername)\n            self._session.load_document(self._doc)\n\n        if not self._doc._current_plot == self:\n            self._doc._current_plot = self\n            self._doc.add(self)\n\n        if self._options.filename:\n            if self._options.filename is True:\n                filename = \"untitled\"\n            else:\n                filename = self._options.filename\n\n            with open(filename, \"w\") as f:\n                f.write(file_html(self._doc, INLINE, self.title))\n            print(\"Wrote %s\" % filename)\n            view(filename)\n        elif self._options.filename is False and \\\n                        self._options.server is False and \\\n                        self._options.notebook is False:\n            print(\"You have a provide a filename (filename='foo.html' or\"\n                  \" .filename('foo.html')) to save your plot.\")\n\n        if self._options.server:\n            self.session.store_document(self._doc)\n            link = self._session.object_link(self._doc.context)\n            view(link)\n\n        if self._options.notebook:\n            from bokeh.embed import notebook_div\n            publish_display_data({'text/html': notebook_div(self)})",
  "def __init__(self, *args, **kwargs):\n            self._options = opts_type(**kwargs)\n            orig_init(self)",
  "def cycle_colors(chunk, palette=_default_cycle_palette):\n    \"\"\" Build a color list just cycling through a given palette.\n\n    Args:\n        chuck (seq): the chunk of elements to generate the color list\n        palette (seq[color]) : a palette of colors to cycle through\n\n    Returns:\n        colors\n\n    \"\"\"\n    colors = []\n\n    g = itertools.cycle(palette)\n    for i in range(len(chunk)):\n        colors.append(next(g))\n\n    return colors",
  "def make_scatter(source, x, y, markertype, color, line_color=None,\n                 size=10, fill_alpha=0.2, line_alpha=1.0):\n    \"\"\"Create a marker glyph and appends it to the renderers list.\n\n    Args:\n        source (obj): datasource object containing markers references.\n        x (str or list[float]) : values or field names of line ``x`` coordinates\n        y (str or list[float]) : values or field names of line ``y`` coordinates\n        markertype (int or str): Marker type to use (e.g., 2, 'circle', etc.)\n        color (str): color of the points\n        size (int) : size of the scatter marker\n        fill_alpha(float) : alpha value of the fill color\n        line_alpha(float) : alpha value of the line color\n\n    Return:\n        scatter: Marker Glyph instance\n    \"\"\"\n    if line_color is None:\n        line_color = color\n\n    _marker_types = OrderedDict(\n        [\n            (\"circle\", Circle),\n            (\"square\", Square),\n            (\"triangle\", Triangle),\n            (\"diamond\", Diamond),\n            (\"inverted_triangle\", InvertedTriangle),\n            (\"asterisk\", Asterisk),\n            (\"cross\", Cross),\n            (\"x\", X),\n            (\"circle_cross\", CircleCross),\n            (\"circle_x\", CircleX),\n            (\"square_x\", SquareX),\n            (\"square_cross\", SquareCross),\n            (\"diamond_cross\", DiamondCross),\n        ]\n    )\n\n    g = itertools.cycle(_marker_types.keys())\n    if isinstance(markertype, int):\n        for i in range(markertype):\n            shape = next(g)\n    else:\n        shape = markertype\n    glyph = _marker_types[shape](\n        x=x, y=y, size=size, fill_color=color, fill_alpha=fill_alpha,\n        line_color=line_color, line_alpha=line_alpha\n    )\n\n    return GlyphRenderer(data_source=source, glyph=glyph)",
  "def chunk(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\n\n    Args:\n        l (list: the incomming list to be chunked\n        n (int): lenght of you chucks\n    \"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]",
  "def polar_to_cartesian(r, start_angles, end_angles):\n    \"\"\"Translate polar coordinates to cartesian.\n\n    Args:\n    r (float): radial coordinate\n    start_angles (list(float)): list of start angles\n    end_angles (list(float)): list of end_angles angles\n\n    Returns:\n        x, y points\n    \"\"\"\n    cartesian = lambda r, alpha: (r*cos(alpha), r*sin(alpha))\n    points = []\n\n    for start, end in zip(start_angles, end_angles):\n        points.append(cartesian(r, (end + start)/2))\n\n    return zip(*points)",
  "class Figure(object):\n    def __init__(self, *charts, **kwargs):\n        self.filename = kwargs.pop('filename', None)\n        self.server = kwargs.pop('server', None)\n        self.notebook = kwargs.pop('notebook', None)\n        self.title = kwargs.pop('title', '')\n        self.children = kwargs.pop('children', None)\n        self.charts = charts\n        self.doc = Document()\n        self.doc.hold(True)\n        self._plots = []\n\n        if self.server:\n            self.session = Session()\n            self.session.use_doc(self.server)\n            self.session.load_document(self.doc)\n\n        if self.children:\n            from bokeh.models import VBox\n            self.doc.add(VBox(children=self.children))\n\n        self.plot = None\n        for i, chart in enumerate(self.charts):\n            chart.doc = self.doc\n            if self.server:\n                chart.session = self.session\n\n            # Force the chart to create the underlying plot\n            chart._setup_show()\n            chart._prepare_show()\n            chart._show_teardown()\n\n            if not self.title:\n                self.title = chart.chart.title\n\n            self._plots += chart.chart._plots\n\n        # reset the pot title with the one set for the Figure\n        self.doc._current_plot.title = self.title\n\n    def show(self):\n        \"\"\"Main show function.\n\n        It shows the Figure in file, server and notebook outputs.\n        \"\"\"\n        show(self, self.title, self.filename, self.server, self.notebook)",
  "def show(obj, title='test', filename=False, server=False, notebook=False, **kws):\n    \"\"\" 'shows' a plot object, by auto-raising the window or tab\n    displaying the current plot (for file/server output modes) or displaying\n    it in an output cell (IPython notebook).\n\n    Args:\n        obj (Widget/Plot object, optional): it accepts a plot object and just shows it.\n\n    \"\"\"\n    if filename:\n        if filename is True:\n            filename = \"untitled\"\n        else:\n            filename = filename\n\n        with open(filename, \"w\") as f:\n            f.write(file_html(obj.doc, INLINE, title))\n        print(\"Wrote %s\" % filename)\n        view(filename)\n\n    elif filename is False and server is False and notebook is False:\n        print(\"You have to provide a filename (filename='foo.html' or\"\n              \" .filename('foo.html')) to save your plot.\")\n\n    if server:\n        obj.session.store_document(obj.doc)\n        link = obj.session.object_link(obj.doc.context)\n        view(link)\n\n    if notebook:\n        from bokeh.embed import notebook_div\n        for plot in obj._plots:\n            publish_display_data({'text/html': notebook_div(plot)})",
  "def __init__(self, *charts, **kwargs):\n        self.filename = kwargs.pop('filename', None)\n        self.server = kwargs.pop('server', None)\n        self.notebook = kwargs.pop('notebook', None)\n        self.title = kwargs.pop('title', '')\n        self.children = kwargs.pop('children', None)\n        self.charts = charts\n        self.doc = Document()\n        self.doc.hold(True)\n        self._plots = []\n\n        if self.server:\n            self.session = Session()\n            self.session.use_doc(self.server)\n            self.session.load_document(self.doc)\n\n        if self.children:\n            from bokeh.models import VBox\n            self.doc.add(VBox(children=self.children))\n\n        self.plot = None\n        for i, chart in enumerate(self.charts):\n            chart.doc = self.doc\n            if self.server:\n                chart.session = self.session\n\n            # Force the chart to create the underlying plot\n            chart._setup_show()\n            chart._prepare_show()\n            chart._show_teardown()\n\n            if not self.title:\n                self.title = chart.chart.title\n\n            self._plots += chart.chart._plots\n\n        # reset the pot title with the one set for the Figure\n        self.doc._current_plot.title = self.title",
  "def show(self):\n        \"\"\"Main show function.\n\n        It shows the Figure in file, server and notebook outputs.\n        \"\"\"\n        show(self, self.title, self.filename, self.server, self.notebook)",
  "def create_and_build(builder_class, values, **kws):\n    builder_props = set(builder_class.properties())\n\n    # create the new builder\n    builder_kws = { k:v for k,v in kws.items() if k in builder_props}\n    builder = builder_class(values, **builder_kws)\n\n    # create a chart to return, since there isn't one already\n    chart_kws = { k:v for k,v in kws.items() if k not in builder_props}\n    chart = Chart(**chart_kws)\n    chart.add_builder(builder)\n\n    return chart",
  "class Builder(HasProps):\n    \"\"\" A prototype class to inherit each new chart Builder type.\n\n    It provides useful methods to be used by the inherited builder classes,\n    in order to automate most of the charts creation tasks and leave the\n    core customization to specialized builder classes. In that pattern\n    inherited builders just need to provide:\n\n     - the following methods:\n        * _yield_renderers: yields the glyphs to be rendered into the plot (and\n            eventually create the self._legends attribute to be used to\n            create the proper legends when builder is called to build\n            the glyphs on a Chart object\n        * _process_data(optional): Get the input data and calculates the 'data'\n            attribute to be used to calculate the source data\n        * _set_sources(optional): Push data into the self.source attribute\n            (of type ColumnDataSource) and build the proper ranges\n            (self.x_range and self.y_range).\n\n    - the following attributes:\n        x_range:\n        y_range:\n        _legends:\n\n\n    so Builder can use it all to _yield_renderers on a chart when called with the\n    create method.\n\n    \"\"\"\n\n    x_range = Instance(Range)\n    y_range = Instance(Range)\n\n    palette = Seq(Color, default=DEFAULT_PALETTE)\n\n    def __init__(self, values=None, **kws):\n        \"\"\"Common arguments to be used by all the inherited classes.\n\n        Args:\n            values (iterable): iterable 2d representing the data series\n                values matrix.\n            legend (str, bool): the legend of your plot. The legend content is\n                inferred from incoming input.It can be ``top_left``,\n                ``top_right``, ``bottom_left``, ``bottom_right``.\n                It is ``top_right`` is you set it as True.\n            palette(list, optional): a list containing the colormap as hex values.\n\n\n        Attributes:\n            source (obj): datasource object for your plot,\n                initialized as a dummy None.\n            x_range (obj): x-associated datarange object for you plot,\n                initialized as a dummy None.\n            y_range (obj): y-associated datarange object for you plot,\n                initialized as a dummy None.\n            groups (list): to be filled with the incoming groups of data.\n                Useful for legend construction.\n            data (dict): to be filled with the incoming data and be passed\n                to the ColumnDataSource in each chart inherited class.\n                Needed for _set_And_get method.\n            attr (list): to be filled with the new attributes created after\n                loading the data dict.\n                Needed for _set_And_get method.\n        \"\"\"\n        super(Builder, self).__init__(**kws)\n        if values is None:\n            values = []\n\n        self._values = values\n        # TODO: No real reason why legends should be *private*, should be\n        # legends\n        self._legends = []\n        self._data = {}\n        self._groups = []\n        self._attr = []\n\n    def _adapt_values(self):\n        \"\"\"Prepare the input data.\n\n        Converts data input (self._values) to a DataAdapter and creates\n        instance index if needed\n        \"\"\"\n        if hasattr(self, 'index'):\n            self._values_index, self._values = DataAdapter.get_index_and_data(\n                self._values, self.index\n            )\n        else:\n            if not isinstance(self._values, DataAdapter):\n                self._values = DataAdapter(self._values, force_alias=False)\n\n    def _process_data(self):\n        \"\"\"Get the input data.\n\n        It has to be implemented by any of the inherited class\n        representing each different chart type. It is the place\n        where we make specific calculations for each chart.\n        \"\"\"\n        pass\n\n    def _set_sources(self):\n        \"\"\"Push data into the ColumnDataSource and build the\n        proper ranges.\n\n        It has to be implemented by any of the inherited class\n        representing each different chart type.\n        \"\"\"\n        pass\n\n    def _yield_renderers(self):\n        \"\"\" Generator that yields the glyphs to be draw on the plot\n\n        It has to be implemented by any of the inherited class\n        representing each different chart type.\n        \"\"\"\n        pass\n\n    def create(self, chart=None):\n        self._adapt_values()\n        self._process_data()\n        self._set_sources()\n        renderers = self._yield_renderers()\n\n        chart.add_renderers(self, renderers)\n\n        # create chart ranges..\n        if not chart.x_range:\n            chart.x_range = self.x_range\n        if not chart.y_range:\n            chart.y_range = self.y_range\n\n        # always contribute legends, let Chart sort it out\n        legends = self._legends\n        chart.add_legend(legends)\n\n        return chart\n\n    #***************************\n    # Some helper methods\n    #***************************\n\n    def _set_and_get(self, data, prefix, attr, val, content):\n        \"\"\"Set a new attr and then get it to fill the self._data dict.\n\n        Keep track of the attributes created.\n\n        Args:\n            data (dict): where to store the new attribute content\n            attr (list): where to store the new attribute names\n            val (string): name of the new attribute\n            content (obj): content of the new attribute\n        \"\"\"\n        data[prefix + val] = content\n        attr.append(prefix + val)\n\n    def set_and_get(self, prefix, val, content):\n        \"\"\"Set a new attr and then get it to fill the self._data dict.\n\n        Keep track of the attributes created.\n\n        Args:\n            prefix (str): prefix of the new attribute\n            val (string): name of the new attribute\n            content (obj): content of the new attribute\n        \"\"\"\n        self._set_and_get(self._data, prefix, self._attr, val, content)",
  "def __init__(self, values=None, **kws):\n        \"\"\"Common arguments to be used by all the inherited classes.\n\n        Args:\n            values (iterable): iterable 2d representing the data series\n                values matrix.\n            legend (str, bool): the legend of your plot. The legend content is\n                inferred from incoming input.It can be ``top_left``,\n                ``top_right``, ``bottom_left``, ``bottom_right``.\n                It is ``top_right`` is you set it as True.\n            palette(list, optional): a list containing the colormap as hex values.\n\n\n        Attributes:\n            source (obj): datasource object for your plot,\n                initialized as a dummy None.\n            x_range (obj): x-associated datarange object for you plot,\n                initialized as a dummy None.\n            y_range (obj): y-associated datarange object for you plot,\n                initialized as a dummy None.\n            groups (list): to be filled with the incoming groups of data.\n                Useful for legend construction.\n            data (dict): to be filled with the incoming data and be passed\n                to the ColumnDataSource in each chart inherited class.\n                Needed for _set_And_get method.\n            attr (list): to be filled with the new attributes created after\n                loading the data dict.\n                Needed for _set_And_get method.\n        \"\"\"\n        super(Builder, self).__init__(**kws)\n        if values is None:\n            values = []\n\n        self._values = values\n        # TODO: No real reason why legends should be *private*, should be\n        # legends\n        self._legends = []\n        self._data = {}\n        self._groups = []\n        self._attr = []",
  "def _adapt_values(self):\n        \"\"\"Prepare the input data.\n\n        Converts data input (self._values) to a DataAdapter and creates\n        instance index if needed\n        \"\"\"\n        if hasattr(self, 'index'):\n            self._values_index, self._values = DataAdapter.get_index_and_data(\n                self._values, self.index\n            )\n        else:\n            if not isinstance(self._values, DataAdapter):\n                self._values = DataAdapter(self._values, force_alias=False)",
  "def _process_data(self):\n        \"\"\"Get the input data.\n\n        It has to be implemented by any of the inherited class\n        representing each different chart type. It is the place\n        where we make specific calculations for each chart.\n        \"\"\"\n        pass",
  "def _set_sources(self):\n        \"\"\"Push data into the ColumnDataSource and build the\n        proper ranges.\n\n        It has to be implemented by any of the inherited class\n        representing each different chart type.\n        \"\"\"\n        pass",
  "def _yield_renderers(self):\n        \"\"\" Generator that yields the glyphs to be draw on the plot\n\n        It has to be implemented by any of the inherited class\n        representing each different chart type.\n        \"\"\"\n        pass",
  "def create(self, chart=None):\n        self._adapt_values()\n        self._process_data()\n        self._set_sources()\n        renderers = self._yield_renderers()\n\n        chart.add_renderers(self, renderers)\n\n        # create chart ranges..\n        if not chart.x_range:\n            chart.x_range = self.x_range\n        if not chart.y_range:\n            chart.y_range = self.y_range\n\n        # always contribute legends, let Chart sort it out\n        legends = self._legends\n        chart.add_legend(legends)\n\n        return chart",
  "def _set_and_get(self, data, prefix, attr, val, content):\n        \"\"\"Set a new attr and then get it to fill the self._data dict.\n\n        Keep track of the attributes created.\n\n        Args:\n            data (dict): where to store the new attribute content\n            attr (list): where to store the new attribute names\n            val (string): name of the new attribute\n            content (obj): content of the new attribute\n        \"\"\"\n        data[prefix + val] = content\n        attr.append(prefix + val)",
  "def set_and_get(self, prefix, val, content):\n        \"\"\"Set a new attr and then get it to fill the self._data dict.\n\n        Keep track of the attributes created.\n\n        Args:\n            prefix (str): prefix of the new attribute\n            val (string): name of the new attribute\n            content (obj): content of the new attribute\n        \"\"\"\n        self._set_and_get(self._data, prefix, self._attr, val, content)",
  "class ChartOptions(HasProps):\n\n    id = String(None, help=\"\"\"\n    Id of the chart.\n    \"\"\")\n\n    title = String(None, help=\"\"\"\n    A title for the chart.\n    \"\"\")\n\n    legend = Either(Bool, Enum(Orientation), help=\"\"\"\n    A location where the legend should draw itself.\n    \"\"\")\n\n    xgrid = Bool(True, help=\"\"\"\n    Whether to draw an x-grid.\n    \"\"\")\n\n    ygrid = Bool(True, help=\"\"\"\n    Whether to draw an y-grid.\n    \"\"\")\n\n    xlabel = String(\"x-axis\", help=\"\"\"\n    A label for the (default) x-axis.\n    \"\"\")\n\n    ylabel = String(\"y-axis\", help=\"\"\"\n    A label for the (default) y-axis.\n    \"\"\")\n\n    xscale = Either(Auto, Enum(Scale), help=\"\"\"\n    What kind of scale to use for the x-axis.\n    \"\"\")\n\n    yscale = Either(Auto, Enum(Scale), help=\"\"\"\n    What kind of scale to use for the y-axis.\n    \"\"\")\n\n    width = Int(600, help=\"\"\"\n    Width of the rendered chart, in pixels.\n    \"\"\")\n\n    height = Int(400, help=\"\"\"\n    Height of the rendered chart, in pixels.\n    \"\"\")\n\n    filename = Either(Bool(False), String, help=\"\"\"\n    A name for the file to save this chart to.\n    \"\"\")\n\n    server = Either(Bool(False), String, help=\"\"\"\n    A name to use to save this chart to on server.\n    \"\"\")\n\n    notebook = Either(Bool(False), String, help=\"\"\"\n    Whether to display the plot inline in an IPython/Jupyter\n    notebook.\n    \"\"\")\n\n    tools = Either(Bool(True), String, help=\"\"\"\n    Whether to add default tools the the chart.\n    \"\"\")",
  "class DataAdapter(object):\n    \"\"\"\n    Adapter object used to normalize Charts inputs to a common interface.\n    Supported inputs are dict, list, tuple, np.ndarray and pd.DataFrame.\n    \"\"\"\n    def __init__(self, data, index=None, columns=None, force_alias=True):\n        self._values = self.validate_values(data)\n\n        self.convert_index_to_int = False\n        self._columns_map = {}\n        self.convert_items_to_dict = False\n\n        if columns is None and force_alias:\n            # no column 'labels' defined for data... in this case we use\n            # default names\n            keys = getattr(self._values, 'keys', None)\n            if callable(keys):\n                columns = list(keys())\n\n            elif keys is None:\n                columns = list(map(str, range(len(data))))\n\n            else:\n                columns = list(keys)\n\n        if columns:\n            self._columns = columns\n\n            # define a mapping between the real keys to access data and the aliases\n            # we have defined using 'columns'\n            self._columns_map = dict(zip(columns, self.keys()))\n\n        if index is not None:\n            self._index = index\n            self.convert_items_to_dict = True\n\n        elif force_alias:\n            _index = getattr(self._values, 'index', None)\n\n            # check because if it is a callable self._values is not a\n            # dataframe (probably a list)\n            if _index is None:\n                indexes = self.index\n\n                if isinstance(indexes[0], int):\n                    self._index = DEFAULT_INDEX_ALIASES[:][:len(self.values()[0])]\n                    self.convert_items_to_dict = True\n\n            elif not callable(_index):\n                self._index = list(_index)\n                self.convert_items_to_dict = True\n\n            else:\n                self._index = DEFAULT_INDEX_ALIASES[:][:len(self.values()[0])]\n                self.convert_items_to_dict = True\n\n    @staticmethod\n    def is_number(value):\n        numbers = (float, ) + bokeh_integer_types\n        return isinstance(value, numbers)\n\n    @staticmethod\n    def is_datetime(value):\n        try:\n            dt = Datetime(value)\n            dt # shut up pyflakes\n            return True\n\n        except ValueError:\n            return False\n\n    @staticmethod\n    def validate_values(values):\n        if np and isinstance(values, np.ndarray):\n            if len(values.shape) == 1:\n                return np.array([values])\n\n            else:\n                return values\n\n        elif pd and isinstance(values, pd.DataFrame):\n            return values\n\n        elif isinstance(values, (dict, OrderedDict)):\n            if all(DataAdapter.is_number(x) for x in values.values()):\n                return values\n\n            return values\n\n        elif isinstance(values, (list, tuple)):\n            if all(DataAdapter.is_number(x) for x in values):\n                return [values]\n\n            return values\n\n        # TODO: Improve this error message..\n        raise TypeError(\"Input type not supported! %s\" % values)\n\n\n    def index_converter(self, x):\n        key = self._columns_map.get(x, x)\n        if self.convert_index_to_int:\n            key = int(key)\n        return key\n\n    def keys(self):\n        # assuming it's a dict or dataframe\n        keys = getattr(self._values, \"keys\", None)\n\n        if callable(keys):\n            return list(keys())\n\n        elif keys is None:\n            # assuming that only non-dict like objects can raise this error\n            # it's probably because we have an iterable instead of a mapper\n            # in this case let's use indices as groups keys\n            self.convert_index_to_int = True\n            indexes = range(len(self._values))\n            return list(map(str, indexes))\n\n        else:\n            return list(keys)\n\n    def __len__(self):\n        return len(self.values())\n\n    def __iter__(self):\n        for k in self.keys():\n            yield k\n\n    def __getitem__(self, key):\n        val = self._values[self.index_converter(key)]\n\n        # if we have \"index aliases\" we need to remap the values...\n        if self.convert_items_to_dict:\n            val = dict(zip(self._index, val))\n\n        return val\n\n    def values(self):\n        values = getattr(self._values, \"values\", None)\n\n        if callable(values):\n            return list(values())\n\n        elif values is None:\n            return self._values\n\n        else:\n            # assuming it's a dataframe, in that case it returns transposed\n            # values compared to it's dict equivalent..\n            return list(values.T)\n\n    def items(self):\n        return [(key, self[key]) for key in self]\n\n    def iterkeys(self):\n        return iter(self)\n\n    def itervalues(self):\n        for k in self:\n            yield self[k]\n\n    def iteritems(self):\n        for k in self:\n            yield (k, self[k])\n\n    @property\n    def columns(self):\n        try:\n            return self._columns\n\n        except AttributeError:\n            return list(self.keys())\n\n    @property\n    def index(self):\n        try:\n            return self._index\n\n        except AttributeError:\n            index = getattr(self._values, \"index\", None)\n\n            if not callable(index) and index is not None:\n                # guess it's a pandas dataframe..\n                return index\n\n        # no, it's not. So it's probably a list so let's get the\n        # values and check\n        values = self.values()\n\n        if isinstance(values, dict):\n            return list(values.keys())\n\n        else:\n            first_el = self.values()[0]\n\n            if isinstance(first_el, dict):\n                indexes = list(first_el.keys())\n\n            else:\n                indexes = range(0, len(self.values()[0]))\n                self._index = indexes\n            return indexes\n\n    #-----------------------------------------------------------------------------\n    # Convenience methods\n    #-----------------------------------------------------------------------------\n    @staticmethod\n    def get_index_and_data(values, index=None):\n        \"\"\"Parse values (that must be one of the DataAdapter supported\n        input types) and create an separate/create index and data\n        depending on values type and index.\n\n        Args:\n            values (iterable): container that holds data to be plotted using\n                on the Chart classes\n\n        Returns:\n            xs: iterable that represents the data index\n            values: iterable containing the values to be plotted\n        \"\"\"\n        if hasattr(values, 'keys'):\n            if index is not None:\n                if isinstance(index, string_types):\n                    xs = values[index]\n\n                else:\n                    xs = index\n\n            else:\n                try:\n                    xs = values.index\n\n                except AttributeError:\n                    values = DataAdapter(values, force_alias=False)\n                    xs = values.index\n\n        else:\n            if index is None:\n                values = DataAdapter(values, force_alias=False)\n                xs = values.index\n\n            elif isinstance(index, string_types):\n                msg = \"String indexes are only supported for DataFrame and dict inputs\"\n                raise TypeError(msg)\n\n            else:\n                xs = index\n                values = DataAdapter(values, force_alias=False)\n\n        return xs, values",
  "def __init__(self, data, index=None, columns=None, force_alias=True):\n        self._values = self.validate_values(data)\n\n        self.convert_index_to_int = False\n        self._columns_map = {}\n        self.convert_items_to_dict = False\n\n        if columns is None and force_alias:\n            # no column 'labels' defined for data... in this case we use\n            # default names\n            keys = getattr(self._values, 'keys', None)\n            if callable(keys):\n                columns = list(keys())\n\n            elif keys is None:\n                columns = list(map(str, range(len(data))))\n\n            else:\n                columns = list(keys)\n\n        if columns:\n            self._columns = columns\n\n            # define a mapping between the real keys to access data and the aliases\n            # we have defined using 'columns'\n            self._columns_map = dict(zip(columns, self.keys()))\n\n        if index is not None:\n            self._index = index\n            self.convert_items_to_dict = True\n\n        elif force_alias:\n            _index = getattr(self._values, 'index', None)\n\n            # check because if it is a callable self._values is not a\n            # dataframe (probably a list)\n            if _index is None:\n                indexes = self.index\n\n                if isinstance(indexes[0], int):\n                    self._index = DEFAULT_INDEX_ALIASES[:][:len(self.values()[0])]\n                    self.convert_items_to_dict = True\n\n            elif not callable(_index):\n                self._index = list(_index)\n                self.convert_items_to_dict = True\n\n            else:\n                self._index = DEFAULT_INDEX_ALIASES[:][:len(self.values()[0])]\n                self.convert_items_to_dict = True",
  "def is_number(value):\n        numbers = (float, ) + bokeh_integer_types\n        return isinstance(value, numbers)",
  "def is_datetime(value):\n        try:\n            dt = Datetime(value)\n            dt # shut up pyflakes\n            return True\n\n        except ValueError:\n            return False",
  "def validate_values(values):\n        if np and isinstance(values, np.ndarray):\n            if len(values.shape) == 1:\n                return np.array([values])\n\n            else:\n                return values\n\n        elif pd and isinstance(values, pd.DataFrame):\n            return values\n\n        elif isinstance(values, (dict, OrderedDict)):\n            if all(DataAdapter.is_number(x) for x in values.values()):\n                return values\n\n            return values\n\n        elif isinstance(values, (list, tuple)):\n            if all(DataAdapter.is_number(x) for x in values):\n                return [values]\n\n            return values\n\n        # TODO: Improve this error message..\n        raise TypeError(\"Input type not supported! %s\" % values)",
  "def index_converter(self, x):\n        key = self._columns_map.get(x, x)\n        if self.convert_index_to_int:\n            key = int(key)\n        return key",
  "def keys(self):\n        # assuming it's a dict or dataframe\n        keys = getattr(self._values, \"keys\", None)\n\n        if callable(keys):\n            return list(keys())\n\n        elif keys is None:\n            # assuming that only non-dict like objects can raise this error\n            # it's probably because we have an iterable instead of a mapper\n            # in this case let's use indices as groups keys\n            self.convert_index_to_int = True\n            indexes = range(len(self._values))\n            return list(map(str, indexes))\n\n        else:\n            return list(keys)",
  "def __len__(self):\n        return len(self.values())",
  "def __iter__(self):\n        for k in self.keys():\n            yield k",
  "def __getitem__(self, key):\n        val = self._values[self.index_converter(key)]\n\n        # if we have \"index aliases\" we need to remap the values...\n        if self.convert_items_to_dict:\n            val = dict(zip(self._index, val))\n\n        return val",
  "def values(self):\n        values = getattr(self._values, \"values\", None)\n\n        if callable(values):\n            return list(values())\n\n        elif values is None:\n            return self._values\n\n        else:\n            # assuming it's a dataframe, in that case it returns transposed\n            # values compared to it's dict equivalent..\n            return list(values.T)",
  "def items(self):\n        return [(key, self[key]) for key in self]",
  "def iterkeys(self):\n        return iter(self)",
  "def itervalues(self):\n        for k in self:\n            yield self[k]",
  "def iteritems(self):\n        for k in self:\n            yield (k, self[k])",
  "def columns(self):\n        try:\n            return self._columns\n\n        except AttributeError:\n            return list(self.keys())",
  "def index(self):\n        try:\n            return self._index\n\n        except AttributeError:\n            index = getattr(self._values, \"index\", None)\n\n            if not callable(index) and index is not None:\n                # guess it's a pandas dataframe..\n                return index\n\n        # no, it's not. So it's probably a list so let's get the\n        # values and check\n        values = self.values()\n\n        if isinstance(values, dict):\n            return list(values.keys())\n\n        else:\n            first_el = self.values()[0]\n\n            if isinstance(first_el, dict):\n                indexes = list(first_el.keys())\n\n            else:\n                indexes = range(0, len(self.values()[0]))\n                self._index = indexes\n            return indexes",
  "def get_index_and_data(values, index=None):\n        \"\"\"Parse values (that must be one of the DataAdapter supported\n        input types) and create an separate/create index and data\n        depending on values type and index.\n\n        Args:\n            values (iterable): container that holds data to be plotted using\n                on the Chart classes\n\n        Returns:\n            xs: iterable that represents the data index\n            values: iterable containing the values to be plotted\n        \"\"\"\n        if hasattr(values, 'keys'):\n            if index is not None:\n                if isinstance(index, string_types):\n                    xs = values[index]\n\n                else:\n                    xs = index\n\n            else:\n                try:\n                    xs = values.index\n\n                except AttributeError:\n                    values = DataAdapter(values, force_alias=False)\n                    xs = values.index\n\n        else:\n            if index is None:\n                values = DataAdapter(values, force_alias=False)\n                xs = values.index\n\n            elif isinstance(index, string_types):\n                msg = \"String indexes are only supported for DataFrame and dict inputs\"\n                raise TypeError(msg)\n\n            else:\n                xs = index\n                values = DataAdapter(values, force_alias=False)\n\n        return xs, values",
  "def TimeSeries(values, index=None, xscale='datetime', **kws):\n    return create_and_build(\n        TimeSeriesBuilder, values, index=index, xscale=xscale, **kws\n    )",
  "class TimeSeriesBuilder(Builder):\n    \"\"\"This is the TimeSeries class and it is in charge of plotting\n    TimeSeries charts in an easy and intuitive way.\n\n    Essentially, we provide a way to ingest the data, make the proper\n    calculations and push the references into a source object.\n    We additionally make calculations for the ranges.\n    And finally add the needed lines taking the references from the source.\n\n    Examples:\n        import datetime\n        from collections import OrderedDict\n        from bokeh.charts import TimeSeries\n\n        now = datetime.datetime.now()\n        delta = datetime.timedelta(minutes=1)\n        dts = [now + delta*i for i in range(5)]\n        dtss = ['%s'%dt for dt in dts]\n        xyvalues = OrderedDict({'Date': dts})\n        y_python = xyvalues['python'] = [2, 3, 7, 5, 26]\n        y_pypy = xyvalues['pypy'] = [12, 33, 47, 15, 126]\n        y_jython = xyvalues['jython'] = [22, 43, 10, 25, 26]\n\n        ts = TimeSeries(xyvalues, index='Date', title=\"timeseries\",\n                        ylabel='Stock Prices', filename=\"stocks_ts.html\")\n        ts.show()\n\n    \"\"\"\n\n    index = Any(help=\"\"\"\n    An index to be used for all data series as follows:\n\n    - A 1d iterable of any sort that will be used as\n        series common index\n\n    - As a string that corresponds to the key of the\n        mapping to be used as index (and not as data\n        series) if area.values is a mapping (like a dict,\n        an OrderedDict or a pandas DataFrame)\n\n    \"\"\")\n\n    def _process_data(self):\n        \"\"\"Take the x/y data from the timeseries values.\n\n        It calculates the chart properties accordingly. Then build a dict\n        containing references to all the points to be used by\n        the line glyph inside the ``_yield_renderers`` method.\n\n        \"\"\"\n        self._data = dict()\n\n        # list to save all the attributes we are going to create\n        self._attr = []\n        xs = self._values_index\n        for col in self._values.keys():\n            if isinstance(self.index, string_types) \\\n                and col == self.index:\n                continue\n\n            # save every the groups available in the incomming input\n            self._groups.append(col)\n            self.set_and_get(\"x_\", col, xs)\n            self.set_and_get(\"y_\", col, self._values[col])\n\n    def _set_sources(self):\n        \"\"\"Push the TimeSeries data into the ColumnDataSource and\n        calculate the proper ranges.\n        \"\"\"\n        self._source = ColumnDataSource(self._data)\n        self.x_range = DataRange1d(sources=[self._source.columns(self._attr[0])])\n        y_names = self._attr[1::2]\n        endy = max(max(self._data[i]) for i in y_names)\n        starty = min(min(self._data[i]) for i in y_names)\n        self.y_range = Range1d(\n            start=starty - 0.1 * (endy - starty),\n            end=endy + 0.1 * (endy - starty)\n        )\n\n    def _yield_renderers(self):\n        \"\"\"Use the line glyphs to connect the xy points in the time series.\n\n        Takes reference points from the data loaded at the ColumnDataSource.\n        \"\"\"\n        self._duplet = list(chunk(self._attr, 2))\n        colors = cycle_colors(self._duplet, self.palette)\n\n        for i, (x, y) in enumerate(self._duplet, start=1):\n            glyph = Line(x=x, y=y, line_color=colors[i - 1])\n            renderer = GlyphRenderer(data_source=self._source, glyph=glyph)\n            self._legends.append((self._groups[i-1], [renderer]))\n            yield renderer",
  "def _process_data(self):\n        \"\"\"Take the x/y data from the timeseries values.\n\n        It calculates the chart properties accordingly. Then build a dict\n        containing references to all the points to be used by\n        the line glyph inside the ``_yield_renderers`` method.\n\n        \"\"\"\n        self._data = dict()\n\n        # list to save all the attributes we are going to create\n        self._attr = []\n        xs = self._values_index\n        for col in self._values.keys():\n            if isinstance(self.index, string_types) \\\n                and col == self.index:\n                continue\n\n            # save every the groups available in the incomming input\n            self._groups.append(col)\n            self.set_and_get(\"x_\", col, xs)\n            self.set_and_get(\"y_\", col, self._values[col])",
  "def _set_sources(self):\n        \"\"\"Push the TimeSeries data into the ColumnDataSource and\n        calculate the proper ranges.\n        \"\"\"\n        self._source = ColumnDataSource(self._data)\n        self.x_range = DataRange1d(sources=[self._source.columns(self._attr[0])])\n        y_names = self._attr[1::2]\n        endy = max(max(self._data[i]) for i in y_names)\n        starty = min(min(self._data[i]) for i in y_names)\n        self.y_range = Range1d(\n            start=starty - 0.1 * (endy - starty),\n            end=endy + 0.1 * (endy - starty)\n        )",
  "def _yield_renderers(self):\n        \"\"\"Use the line glyphs to connect the xy points in the time series.\n\n        Takes reference points from the data loaded at the ColumnDataSource.\n        \"\"\"\n        self._duplet = list(chunk(self._attr, 2))\n        colors = cycle_colors(self._duplet, self.palette)\n\n        for i, (x, y) in enumerate(self._duplet, start=1):\n            glyph = Line(x=x, y=y, line_color=colors[i - 1])\n            renderer = GlyphRenderer(data_source=self._source, glyph=glyph)\n            self._legends.append((self._groups[i-1], [renderer]))\n            yield renderer",
  "def HeatMap(values, xscale=\"categorical\", yscale=\"categorical\",\n            xgrid=False, ygrid=False, **kw):\n    chart = create_and_build(\n        HeatMapBuilder, values, xscale=xscale, yscale=yscale,\n        xgrid=xgrid, ygrid=ygrid, **kw\n    )\n    chart.add_tools(HoverTool(tooltips=[(\"value\", \"@rate\")]))\n    return chart",
  "class HeatMapBuilder(Builder):\n    \"\"\"This is the HeatMap class and it is in charge of plotting\n    HeatMap chart in an easy and intuitive way.\n\n    Essentially, it provides a way to ingest the data, make the proper\n    calculations and push the references into a source object.\n    We additionally make calculations for the ranges.\n    And finally add the needed glyphs (rects) taking the references\n    from the source.\n\n    Examples:\n    from collections import OrderedDict\n    from bokeh.charts import HeatMap\n\n    xyvalues = OrderedDict()\n    xyvalues['apples'] = [4,5,8]\n    xyvalues['bananas'] = [1,2,4]\n    xyvalues['pears'] = [6,5,4]\n    hm = HeatMap(xyvalues, title=\"categorical heatmap\", filename=\"cat_heatmap.html\")\n    hm.show()\n    \"\"\"\n\n    def _process_data(self):\n        \"\"\"Take the CategoricalHeatMap data from the input **value.\n\n        It calculates the chart properties accordingly. Then build a dict\n        containing references to all the calculated points to be used by\n        the rect glyph inside the ``_yield_renderers`` method.\n\n        \"\"\"\n        self._catsx = list(self._values.columns)\n        self._catsy = list(self._values.index)\n\n        # Set up the data for plotting. We will need to have values for every\n        # pair of year/month names. Map the rate to a color.\n        catx = []\n        caty = []\n        color = []\n        rate = []\n        for y in self._catsy:\n            for m in self._catsx:\n                catx.append(m)\n                caty.append(y)\n                rate.append(self._values[m][y])\n\n        # Now that we have the min and max rates\n        factor = len(self.palette) - 1\n        den = max(rate) - min(rate)\n        for y in self._catsy:\n            for m in self._catsx:\n                c = int(round(factor*(self._values[m][y] - min(rate)) / den))\n                color.append(self.palette[c])\n\n        width = [0.95] * len(catx)\n        height = [0.95] * len(catx)\n\n        self._data = dict(catx=catx, caty=caty, color=color, rate=rate,\n                         width=width, height=height)\n\n    def _set_sources(self):\n        \"\"\"Push the CategoricalHeatMap data into the ColumnDataSource\n        and calculate the proper ranges.\n        \"\"\"\n        self._source = ColumnDataSource(self._data)\n        self.x_range = FactorRange(factors=self._catsx)\n        self.y_range = FactorRange(factors=self._catsy)\n\n    def _yield_renderers(self):\n        \"\"\"Use the rect glyphs to display the categorical heatmap.\n\n        Takes reference points from data loaded at the ColumnDataSurce.\n        \"\"\"\n        glyph = Rect(\n            x=\"catx\", y=\"caty\",\n            width=\"width\", height=\"height\",\n            fill_color=\"color\", fill_alpha=0.7,\n            line_color=\"white\"\n        )\n        renderer = GlyphRenderer(data_source=self._source, glyph=glyph)\n        # TODO: Legend??\n        yield renderer\n\n    def _adapt_values(self):\n        \"\"\"Prepare the input data.\n\n        Converts data input (self._values) to a DataAdapter\n        \"\"\"\n        self._values = DataAdapter(self._values, force_alias=True)",
  "def _process_data(self):\n        \"\"\"Take the CategoricalHeatMap data from the input **value.\n\n        It calculates the chart properties accordingly. Then build a dict\n        containing references to all the calculated points to be used by\n        the rect glyph inside the ``_yield_renderers`` method.\n\n        \"\"\"\n        self._catsx = list(self._values.columns)\n        self._catsy = list(self._values.index)\n\n        # Set up the data for plotting. We will need to have values for every\n        # pair of year/month names. Map the rate to a color.\n        catx = []\n        caty = []\n        color = []\n        rate = []\n        for y in self._catsy:\n            for m in self._catsx:\n                catx.append(m)\n                caty.append(y)\n                rate.append(self._values[m][y])\n\n        # Now that we have the min and max rates\n        factor = len(self.palette) - 1\n        den = max(rate) - min(rate)\n        for y in self._catsy:\n            for m in self._catsx:\n                c = int(round(factor*(self._values[m][y] - min(rate)) / den))\n                color.append(self.palette[c])\n\n        width = [0.95] * len(catx)\n        height = [0.95] * len(catx)\n\n        self._data = dict(catx=catx, caty=caty, color=color, rate=rate,\n                         width=width, height=height)",
  "def _set_sources(self):\n        \"\"\"Push the CategoricalHeatMap data into the ColumnDataSource\n        and calculate the proper ranges.\n        \"\"\"\n        self._source = ColumnDataSource(self._data)\n        self.x_range = FactorRange(factors=self._catsx)\n        self.y_range = FactorRange(factors=self._catsy)",
  "def _yield_renderers(self):\n        \"\"\"Use the rect glyphs to display the categorical heatmap.\n\n        Takes reference points from data loaded at the ColumnDataSurce.\n        \"\"\"\n        glyph = Rect(\n            x=\"catx\", y=\"caty\",\n            width=\"width\", height=\"height\",\n            fill_color=\"color\", fill_alpha=0.7,\n            line_color=\"white\"\n        )\n        renderer = GlyphRenderer(data_source=self._source, glyph=glyph)\n        # TODO: Legend??\n        yield renderer",
  "def _adapt_values(self):\n        \"\"\"Prepare the input data.\n\n        Converts data input (self._values) to a DataAdapter\n        \"\"\"\n        self._values = DataAdapter(self._values, force_alias=True)",
  "def Histogram(values, bins, mu=None, sigma=None, density=True, **kws):\n    return create_and_build(\n        HistogramBuilder, values, bins=bins, mu=mu, sigma=sigma, density=density,\n        **kws\n    )",
  "class HistogramBuilder(Builder):\n    \"\"\"This is the Histogram class and it is in charge of plotting\n    histograms in an easy and intuitive way.\n\n    Essentially, we provide a way to ingest the data, make the proper\n    calculations and push the references into a source object.\n    We additionally make calculations for the ranges.\n    And finally add the needed glyphs (quads and lines) taking the\n    references from the source.\n\n    Examples:\n        from collections import OrderedDict\n        from bokeh.charts import Histogram\n\n        mu, sigma = 0, 0.5\n        normal = [1, 2, 3, 1]\n        lognormal = [5, 4, 4, 1]\n        distributions = OrderedDict(normal=normal, lognormal=lognormal)\n        hist = Histogram(distributions, bins=5, notebook=True,\n            title='Histogram', ylabel=\"frequency\", legend=True)\n        hist.show()\n    \"\"\"\n\n    bins = Int(10, help=\"\"\"\n    Number of bins to use for the histogram. (default: 10)\n    \"\"\")\n\n    mu = Float(help=\"\"\"\n    Theoretical mean value for the normal distribution. (default: None)\n    \"\"\")\n\n    sigma = Float(help=\"\"\"\n    Theoretical standard deviation value for the normal distribution.\n    (default: None)\n    \"\"\")\n\n    density = Bool(True, help=\"\"\"\n    Whether to normalize the histogram. (default: True)\n\n    If True, the result is the value of the probability *density* function\n    at the bin, normalized such that the *integral* over the range is 1. If\n    False, the result will contain the number of samples in each bin.\n\n    For more info check ``numpy.histogram`` function documentation.\n\n    \"\"\")\n\n    def _process_data(self):\n        \"\"\"Take the Histogram data from the input **value.\n\n        It calculates the chart properties accordingly. Then build a dict\n        containing references to all the calculated points to be used by\n        the quad and line glyphs inside the ``_yield_renderers`` method.\n        \"\"\"\n        # list to save all the groups available in the incomming input\n        self._groups.extend(self._values.keys())\n\n        # fill the data dictionary with the proper values\n        for i, val in enumerate(self._values.keys()):\n            self.set_and_get(\"\", val, self._values[val])\n            #build the histogram using the set bins number\n            hist, edges = np.histogram(\n                np.array(self._data[val]), density=self.density, bins=self.bins\n            )\n            self.set_and_get(\"hist\", val, hist)\n            self.set_and_get(\"edges\", val, edges)\n            self.set_and_get(\"left\", val, edges[:-1])\n            self.set_and_get(\"right\", val, edges[1:])\n            self.set_and_get(\"bottom\", val, np.zeros(len(hist)))\n\n            self._mu_and_sigma = False\n            if self.mu is not None and self.sigma is not None:\n                if _is_scipy:\n                    self._mu_and_sigma = True\n                    self.set_and_get(\"x\", val, np.linspace(-2, 2, len(self._data[val])))\n                    den = 2 * self.sigma ** 2\n                    x_val = self._data[\"x\" + val]\n                    x_val_mu = x_val - self.mu\n                    sigsqr2pi = self.sigma * np.sqrt(2 * np.pi)\n                    pdf = 1 / (sigsqr2pi) * np.exp(-x_val_mu ** 2 / den)\n                    self.set_and_get(\"pdf\", val, pdf)\n                    self._groups.append(\"pdf\")\n                    cdf = (1 + scipy.special.erf(x_val_mu / np.sqrt(den))) / 2\n                    self.set_and_get(\"cdf\", val, cdf)\n                    self._groups.append(\"cdf\")\n                else:\n                    print(\"You need scipy to get the theoretical probability distributions.\")\n\n    def _set_sources(self):\n        \"\"\"Push the Histogram data into the ColumnDataSource and calculate\n        the proper ranges.\"\"\"\n        self._source = ColumnDataSource(data=self._data)\n\n        if not self._mu_and_sigma:\n            x_names, y_names = self._attr[2::6], self._attr[1::6]\n        else:\n            x_names, y_names = self._attr[2::9], self._attr[1::9]\n\n        endx = max(max(self._data[i]) for i in x_names)\n        startx = min(min(self._data[i]) for i in x_names)\n        self.x_range = Range1d(start=startx - 0.1 * (endx - startx),\n                           end=endx + 0.1 * (endx - startx))\n\n        endy = max(max(self._data[i]) for i in y_names)\n        self.y_range = Range1d(start=0, end=1.1 * endy)\n\n    def _yield_renderers(self):\n        \"\"\"Use the several glyphs to display the Histogram and pdf/cdf.\n\n        It uses the quad (and line) glyphs to display the Histogram\n        bars, taking as reference points the data loaded at the\n        ColumnDataSurce.\n        \"\"\"\n        if not self._mu_and_sigma:\n            sextets = list(chunk(self._attr, 6))\n            colors = cycle_colors(sextets, self.palette)\n\n            # TODO (bev) this is a perfect use for a namedtuple\n            # sextet: values, his, edges, left, right, bottom\n            for i, sextet in enumerate(sextets):\n\n                glyph = Quad(\n                    top=sextet[1], bottom=sextet[5], left=sextet[3], right=sextet[4],\n                    fill_color=colors[i], fill_alpha=0.7,\n                    line_color=\"white\", line_alpha=1.0\n                )\n                renderer = GlyphRenderer(data_source=self._source, glyph=glyph)\n                self._legends.append((self._groups[i], [renderer]))\n                yield renderer\n\n        else:\n            nonets = list(chunk(self._attr, 9))\n            colors = cycle_colors(nonets, self.palette)\n\n            # TODO (bev) this is a perfect use for a namedtuple\n            # nonet: values, his, edges, left, right, bottom, x, pdf, cdf\n            for i, nonet in enumerate(nonets):\n\n                glyph = Quad(\n                    top=nonet[1], bottom=nonet[5], left=nonet[3], right=nonet[4],\n                    fill_color=colors[i], fill_alpha=0.7,\n                    line_color=\"white\", line_alpha=1.0\n                )\n                renderer = GlyphRenderer(data_source=self._source, glyph=glyph)\n                self._legends.append((self._groups[i], [renderer]))\n                yield renderer\n\n                glyph = Line(x=nonet[6], y=nonet[7], line_color=\"black\")\n                yield GlyphRenderer(data_source=self._source, glyph=glyph)\n\n                glyph = Line(x=nonet[6], y=nonet[8], line_color=\"blue\")\n                yield GlyphRenderer(data_source=self._source, glyph=glyph)",
  "def _process_data(self):\n        \"\"\"Take the Histogram data from the input **value.\n\n        It calculates the chart properties accordingly. Then build a dict\n        containing references to all the calculated points to be used by\n        the quad and line glyphs inside the ``_yield_renderers`` method.\n        \"\"\"\n        # list to save all the groups available in the incomming input\n        self._groups.extend(self._values.keys())\n\n        # fill the data dictionary with the proper values\n        for i, val in enumerate(self._values.keys()):\n            self.set_and_get(\"\", val, self._values[val])\n            #build the histogram using the set bins number\n            hist, edges = np.histogram(\n                np.array(self._data[val]), density=self.density, bins=self.bins\n            )\n            self.set_and_get(\"hist\", val, hist)\n            self.set_and_get(\"edges\", val, edges)\n            self.set_and_get(\"left\", val, edges[:-1])\n            self.set_and_get(\"right\", val, edges[1:])\n            self.set_and_get(\"bottom\", val, np.zeros(len(hist)))\n\n            self._mu_and_sigma = False\n            if self.mu is not None and self.sigma is not None:\n                if _is_scipy:\n                    self._mu_and_sigma = True\n                    self.set_and_get(\"x\", val, np.linspace(-2, 2, len(self._data[val])))\n                    den = 2 * self.sigma ** 2\n                    x_val = self._data[\"x\" + val]\n                    x_val_mu = x_val - self.mu\n                    sigsqr2pi = self.sigma * np.sqrt(2 * np.pi)\n                    pdf = 1 / (sigsqr2pi) * np.exp(-x_val_mu ** 2 / den)\n                    self.set_and_get(\"pdf\", val, pdf)\n                    self._groups.append(\"pdf\")\n                    cdf = (1 + scipy.special.erf(x_val_mu / np.sqrt(den))) / 2\n                    self.set_and_get(\"cdf\", val, cdf)\n                    self._groups.append(\"cdf\")\n                else:\n                    print(\"You need scipy to get the theoretical probability distributions.\")",
  "def _set_sources(self):\n        \"\"\"Push the Histogram data into the ColumnDataSource and calculate\n        the proper ranges.\"\"\"\n        self._source = ColumnDataSource(data=self._data)\n\n        if not self._mu_and_sigma:\n            x_names, y_names = self._attr[2::6], self._attr[1::6]\n        else:\n            x_names, y_names = self._attr[2::9], self._attr[1::9]\n\n        endx = max(max(self._data[i]) for i in x_names)\n        startx = min(min(self._data[i]) for i in x_names)\n        self.x_range = Range1d(start=startx - 0.1 * (endx - startx),\n                           end=endx + 0.1 * (endx - startx))\n\n        endy = max(max(self._data[i]) for i in y_names)\n        self.y_range = Range1d(start=0, end=1.1 * endy)",
  "def _yield_renderers(self):\n        \"\"\"Use the several glyphs to display the Histogram and pdf/cdf.\n\n        It uses the quad (and line) glyphs to display the Histogram\n        bars, taking as reference points the data loaded at the\n        ColumnDataSurce.\n        \"\"\"\n        if not self._mu_and_sigma:\n            sextets = list(chunk(self._attr, 6))\n            colors = cycle_colors(sextets, self.palette)\n\n            # TODO (bev) this is a perfect use for a namedtuple\n            # sextet: values, his, edges, left, right, bottom\n            for i, sextet in enumerate(sextets):\n\n                glyph = Quad(\n                    top=sextet[1], bottom=sextet[5], left=sextet[3], right=sextet[4],\n                    fill_color=colors[i], fill_alpha=0.7,\n                    line_color=\"white\", line_alpha=1.0\n                )\n                renderer = GlyphRenderer(data_source=self._source, glyph=glyph)\n                self._legends.append((self._groups[i], [renderer]))\n                yield renderer\n\n        else:\n            nonets = list(chunk(self._attr, 9))\n            colors = cycle_colors(nonets, self.palette)\n\n            # TODO (bev) this is a perfect use for a namedtuple\n            # nonet: values, his, edges, left, right, bottom, x, pdf, cdf\n            for i, nonet in enumerate(nonets):\n\n                glyph = Quad(\n                    top=nonet[1], bottom=nonet[5], left=nonet[3], right=nonet[4],\n                    fill_color=colors[i], fill_alpha=0.7,\n                    line_color=\"white\", line_alpha=1.0\n                )\n                renderer = GlyphRenderer(data_source=self._source, glyph=glyph)\n                self._legends.append((self._groups[i], [renderer]))\n                yield renderer\n\n                glyph = Line(x=nonet[6], y=nonet[7], line_color=\"black\")\n                yield GlyphRenderer(data_source=self._source, glyph=glyph)\n\n                glyph = Line(x=nonet[6], y=nonet[8], line_color=\"blue\")\n                yield GlyphRenderer(data_source=self._source, glyph=glyph)",
  "def Area(values, index=None, **kws):\n    return create_and_build(AreaBuilder, values, index=index, **kws)",
  "class AreaBuilder(Builder):\n    \"\"\"This is the Area class and it is in charge of plotting\n    Area chart in an easy and intuitive way.\n\n    Essentially, it provides a way to ingest the data, make the proper\n    calculations and push the references into a source object.\n    We additionally make calculations for the ranges.\n    And finally add the needed glyphs (patch) taking the references\n    from the source.\n\n    Examples:\n        from collections import OrderedDict\n        from bokeh.charts import Area\n\n        # create some example data\n        xyvalues = OrderedDict(\n            python=[2, 3, 7, 5, 26, 221, 44, 233, 254, 265, 266, 267, 120],\n            pypy=[12, 33, 47, 15, 126, 121, 144, 233, 254, 225, 226, 267, 110],\n            jython=[22, 43, 10, 25, 26, 101, 114, 203, 194, 215, 201, 227, 139],\n        )\n\n        # create an area chart\n        area = Area(\n            xyvalues, title=\"Area Chart\", xlabel='time',\n            ylabel='memory', filename=\"area.html\", stacked=True,\n        )\n        area.show()\n    \"\"\"\n\n    stacked = Bool(False, help=\"\"\"\n    Whether to stack the areas. (Defaults to False)\n\n    If True, areas are draw as a stack, to show the relationship of\n    parts to a whole. Otherwise, areas are layered above one another.\n\n    \"\"\")\n\n    index = Any(help=\"\"\"\n    An index to be used for all data series as follows:\n\n    - A 1d iterable of any sort that will be used as\n        series common index\n\n    - As a string that corresponds to the key of the\n        mapping to be used as index (and not as data\n        series) if area.values is a mapping (like a dict,\n        an OrderedDict or a pandas DataFrame)\n\n    \"\"\")\n\n    def _process_data(self):\n        \"\"\"Calculate the chart properties accordingly from area.values.\n        Then build a dict containing references to all the points to be used by\n        the patch glyph inside the ``_yield_renderers`` method.\n\n        \"\"\"\n        xs = self._values_index\n        last = np.zeros(len(xs))\n        x2 = np.hstack((xs[::-1], xs))\n        self.set_and_get(\"x\", \"\", x2)\n\n        for grp in self._values.keys():\n            # TODO: This condition may be removed or changed depending on\n            # the validation of self.index\n            if isinstance(self.index, string_types) and grp == self.index:\n                continue\n\n            # get single series values\n            col_values = self._values[grp]\n            _values = [col_values[x] for indx, x in enumerate(xs)]\n\n            # to draw area we need 2 coordinates. The lower values will always\n            # be:\n            # - 0 in case of non stacked area\n            # - the previous series top value in case of stacked charts\n            next = last + _values\n            values = np.hstack((last[::-1], next))\n\n            # only update when stacked, otherwise we always want to start from 0\n            if self.stacked:\n                last = next\n\n            # save values and new group\n            self.set_and_get(\"y_\", grp, values)\n            self._groups.append(grp)\n\n    def _set_sources(self):\n        \"\"\"\n        Push the Line data into the ColumnDataSource and calculate the proper ranges.\n        \"\"\"\n        self._source = ColumnDataSource(self._data)\n        self.x_range = DataRange1d(sources=[self._source.columns(\"x\")])\n        y_names = self._attr[1:]\n        endy = max(max(self._data[i]) for i in y_names)\n        starty = min(min(self._data[i]) for i in y_names)\n        self.y_range =  Range1d(\n            start=starty - 0.1 * (endy - starty),\n            end=endy + 0.1 * (endy - starty)\n        )\n\n    def _yield_renderers(self):\n        \"\"\"Use the patch glyphs to fill the area connecting the xy points\n         in the series taken from the data added with area._process_data.\n\n        Takes reference points from the data loaded at the ColumnDataSource.\n        \"\"\"\n        colors = cycle_colors(self._attr, self.palette)\n        # parse all series. We exclude the first attr as it's the x values\n        # added for the index\n        for i, series_name in enumerate(self._attr[1:]):\n\n            glyph = Patch(\n                x='x', y=series_name, fill_color=colors[i], fill_alpha=0.9)\n            renderer = GlyphRenderer(data_source=self._source, glyph=glyph)\n            self._legends.append((self._groups[i], [renderer]))\n            yield renderer",
  "def _process_data(self):\n        \"\"\"Calculate the chart properties accordingly from area.values.\n        Then build a dict containing references to all the points to be used by\n        the patch glyph inside the ``_yield_renderers`` method.\n\n        \"\"\"\n        xs = self._values_index\n        last = np.zeros(len(xs))\n        x2 = np.hstack((xs[::-1], xs))\n        self.set_and_get(\"x\", \"\", x2)\n\n        for grp in self._values.keys():\n            # TODO: This condition may be removed or changed depending on\n            # the validation of self.index\n            if isinstance(self.index, string_types) and grp == self.index:\n                continue\n\n            # get single series values\n            col_values = self._values[grp]\n            _values = [col_values[x] for indx, x in enumerate(xs)]\n\n            # to draw area we need 2 coordinates. The lower values will always\n            # be:\n            # - 0 in case of non stacked area\n            # - the previous series top value in case of stacked charts\n            next = last + _values\n            values = np.hstack((last[::-1], next))\n\n            # only update when stacked, otherwise we always want to start from 0\n            if self.stacked:\n                last = next\n\n            # save values and new group\n            self.set_and_get(\"y_\", grp, values)\n            self._groups.append(grp)",
  "def _set_sources(self):\n        \"\"\"\n        Push the Line data into the ColumnDataSource and calculate the proper ranges.\n        \"\"\"\n        self._source = ColumnDataSource(self._data)\n        self.x_range = DataRange1d(sources=[self._source.columns(\"x\")])\n        y_names = self._attr[1:]\n        endy = max(max(self._data[i]) for i in y_names)\n        starty = min(min(self._data[i]) for i in y_names)\n        self.y_range =  Range1d(\n            start=starty - 0.1 * (endy - starty),\n            end=endy + 0.1 * (endy - starty)\n        )",
  "def _yield_renderers(self):\n        \"\"\"Use the patch glyphs to fill the area connecting the xy points\n         in the series taken from the data added with area._process_data.\n\n        Takes reference points from the data loaded at the ColumnDataSource.\n        \"\"\"\n        colors = cycle_colors(self._attr, self.palette)\n        # parse all series. We exclude the first attr as it's the x values\n        # added for the index\n        for i, series_name in enumerate(self._attr[1:]):\n\n            glyph = Patch(\n                x='x', y=series_name, fill_color=colors[i], fill_alpha=0.9)\n            renderer = GlyphRenderer(data_source=self._source, glyph=glyph)\n            self._legends.append((self._groups[i], [renderer]))\n            yield renderer",
  "def Horizon(values, index=None, num_folds=3, pos_color='#006400',\n            neg_color='#6495ed', xscale='datetime', xgrid=False, ygrid=False,\n            **kws):\n    tools = kws.get('tools', True)\n\n    if tools == True:\n        tools = \"save,resize,reset\"\n    elif isinstance(tools, string_types):\n        tools = tools.replace('pan', '')\n        tools = tools.replace('wheel_zoom', '')\n        tools = tools.replace('box_zoom', '')\n        tools = tools.replace(',,', ',')\n    kws['tools'] = tools\n\n    chart = create_and_build(\n        HorizonBuilder, values, index=index, num_folds=num_folds, pos_color=pos_color,\n        neg_color=neg_color, xscale=xscale, xgrid=xgrid, ygrid=ygrid, **kws\n    )\n\n    # Hide numerical axis\n    chart.left[0].hide = True\n\n    # Add the series names to the y axis\n    chart.extra_y_ranges = {\"series\": FactorRange(factors=chart._builders[0]._series)}\n    chart.add_layout(CategoricalAxis(y_range_name=\"series\"), 'left')\n\n    return chart",
  "class HorizonBuilder(Builder):\n\n    \"\"\"This is the Horizon class and it is in charge of plotting\n    Horizon charts in an easy and intuitive way.\n\n    Essentially, we provide a way to ingest the data, separate the data into\n    a number of folds which stack on top of each others.\n    We additionally make calculations for the ranges.\n    And finally add the needed lines taking the references from the source.\n\n    Examples:\n        import datetime\n        from collections import OrderedDict\n        from bokeh.charts import Horizon\n\n        now = datetime.datetime.now()\n        delta = datetime.timedelta(minutes=1)\n        xyvalues = OrderedDict({'Date': dts})\n        y_python = xyvalues['python'] = [2, 3, 7, 5, 26]\n        y_pypy = xyvalues['pypy'] = [12, 33, 47, 15, 126]\n        y_jython = xyvalues['jython'] = [22, 43, 10, 25, 26]\n\n        hz = Horizon(xyvalues, index='Date', title=\"horizon\", legend=\"top_left\",\n                        ylabel='Stock Prices', filename=\"stocks_ts.html\")\n        hz.show()\n\n    \"\"\"\n\n    index = Any(help=\"\"\"\n    An index to be used for all data series as follows:\n\n    - A 1d iterable of any sort that will be used as\n        series common index\n\n    - As a string that corresponds to the key of the\n        mapping to be used as index (and not as data\n        series) if area.values is a mapping (like a dict,\n        an OrderedDict or a pandas DataFrame)\n\n    \"\"\")\n\n    neg_color = Color(\"#6495ed\", help=\"\"\"\n    The color of the positive folds. (default: \"#6495ed\")\n    \"\"\")\n\n    num_folds = Int(3, help=\"\"\"\n    The number of folds stacked on top of each other. (default: 3)\n    \"\"\")\n\n    pos_color = Color(\"#006400\", help=\"\"\"\n    The color of the positive folds. (default: \"#006400\")\n    \"\"\")\n\n\n    def __init__(self, values, **kws):\n        \"\"\"\n        Args:\n            values (iterable): iterable 2d representing the data series\n                values matrix.\n            index (str|1d iterable, optional): can be used to specify a\n                common custom index for all data series as follows:\n                    - As a 1d iterable of any sort (of datetime values)\n                        that will be used as series common index\n                    - As a string that corresponds to the key of the\n                        mapping to be used as index (and not as data\n                        series) if area.values is a mapping (like a dict,\n                        an OrderedDict or a pandas DataFrame). The values\n                        must be datetime values.\n            legend (str, optional): the legend of your chart. The legend\n                content is inferred from incoming input.It can be\n                ``top_left``, ``top_right``, ``bottom_left``,\n                ``bottom_right``. ``top_right`` is set if you set it\n                 as True. Defaults to None.\n            palette(list, optional): a list containing the colormap as\n                hex values.\n            num_folds (int, optional):\n            pos_color (hex color string, optional): t\n            neg_color (hex color string, optional): the color of\n                the negative folds\n                (default: #6495ed)\n\n        Attributes:\n            source (obj): datasource object for your plot,\n                initialized as a dummy None.\n            x_range (obj): x-associated datarange object for you plot,\n                initialized as a dummy None.\n            y_range (obj): y-associated datarange object for you plot,\n                initialized as a dummy None.\n            groups (list): to be filled with the incoming groups of data.\n                Useful for legend construction.\n            data (dict): to be filled with the incoming data and be passed\n                to the ColumnDataSource in each chart inherited class.\n                Needed for _set_And_get method.\n            attr (list): to be filled with the new attributes created after\n                loading the data dict.\n                Needed for _set_And_get method.\n        \"\"\"\n        super(HorizonBuilder, self).__init__(values, **kws)\n\n        self._fold_names = []\n        self._source = None\n        self._series = []\n        self._fold_height = {}\n        self._max_y = 0\n\n\n    def fold_coordinates(self, y, fold_no, fold_height, y_origin=0, graph_ratio=1):\n        \"\"\" Function that calculate the coordinates for a value given a fold\n        \"\"\"\n        height = fold_no * fold_height\n        quotient, remainder = divmod(abs(y), float(height))\n        v = fold_height\n\n        # quotient would be 0 if the coordinate is represented in this fold\n        # layer\n        if math.floor(quotient) == 0:\n            v = 0\n            if remainder >= height - fold_height:\n                v = remainder - height + fold_height\n\n        v = v * graph_ratio\n        # Return tuple of the positive and negative relevant position of\n        # the coordinate against the provided fold layer\n        if y > 0:\n            return (v + y_origin, fold_height * graph_ratio + y_origin)\n        else:\n            return (y_origin, fold_height * graph_ratio - v + y_origin)\n\n    def pad_list(self, l, padded_value=None):\n        \"\"\" Function that insert padded values at the start and end of\n        the list (l). If padded_value not provided, then duplicate the\n        values next to each end of the list\n        \"\"\"\n        if len(l) > 0:\n            l.insert(0, l[0] if padded_value is None else padded_value)\n            l.append(l[-1] if padded_value is None else padded_value)\n        return l\n\n    def _process_data(self):\n        \"\"\"Use x/y data from the horizon values.\n\n        It calculates the chart properties accordingly. Then build a dict\n        containing references to all the points to be used by\n        the multiple area glyphes inside the ``_yield_renderers`` method.\n\n        \"\"\"\n        for col in self._values.keys():\n            if isinstance(self.index, string_types) and col == self.index:\n                continue\n\n            self._series.append(col)\n            self._max_y = max(max(self._values[col]), self._max_y)\n\n            v_index = [x for x in self._values_index]\n            self.set_and_get(\"x_\", col, self.pad_list(v_index))\n\n        self._fold_height = self._max_y / self.num_folds\n        self._graph_ratio = self.num_folds / len(self._series)\n\n        fill_alpha = []\n        fill_color = []\n\n        for serie_no, serie in enumerate(self._series):\n\n            self.set_and_get('y_', serie, self._values[serie])\n            y_origin = serie_no * self._max_y / len(self._series)\n\n            for fold_itr in range(1, self.num_folds + 1):\n\n                layers_datapoints = [self.fold_coordinates(\n                    x, fold_itr, self._fold_height, y_origin, self._graph_ratio) for x in self._values[serie]]\n                pos_points, neg_points = map(list, zip(*(layers_datapoints)))\n\n                alpha = 1.0 * (abs(fold_itr)) / self.num_folds\n\n                # Y coordinates above 0\n                pos_points = self.pad_list(pos_points, y_origin)\n                self.set_and_get(\"y_fold%s_\" % fold_itr, serie, pos_points)\n                self._fold_names.append(\"y_fold%s_%s\" % (fold_itr, serie))\n                fill_color.append(self.pos_color)\n                fill_alpha.append(alpha)\n\n                # Y coordinates below 0\n                neg_points = self.pad_list(\n                    neg_points, self._fold_height * self._graph_ratio + y_origin)\n                self.set_and_get(\"y_fold-%s_\" % fold_itr, serie, neg_points)\n                self._fold_names.append(\"y_fold-%s_%s\" % (fold_itr, serie))\n                fill_color.append(self.neg_color)\n                fill_alpha.append(alpha)\n\n                # Groups shown in the legend will only appear once\n                if serie_no == 0:\n                    self._groups.append(str(self._fold_height * fold_itr))\n                    self._groups.append(str(self._fold_height * -fold_itr))\n\n        self.set_and_get('fill_', 'alpha', fill_alpha)\n        self.set_and_get('fill_', 'color', fill_color)\n        self.set_and_get('x_', 'all', [self._data[\n                         'x_%s' % serie] for serie in self._series for y in range(self.num_folds * 2)])\n        self.set_and_get(\n            'y_', 'all', [self._data[f_name] for f_name in self._fold_names])\n\n    def _set_sources(self):\n        \"\"\"Push the Horizon data into the ColumnDataSource and\n        calculate the proper ranges.\n        \"\"\"\n        self._source = ColumnDataSource(self._data)\n        self.x_range = DataRange1d(rangepadding=0, sources=[self._source.columns(self._attr[0])])\n        self.y_range = Range1d(start=0, end=self._max_y)\n\n    def _yield_renderers(self):\n        \"\"\"Use the patch glyphs to connect the xy points in the time series.\n        It requires the positive and negative layers\n        Takes reference points from the data loaded at the ColumnDataSource.\n        \"\"\"\n        patches = Patches(\n            fill_color='fill_color', fill_alpha='fill_alpha', xs='x_all', ys='y_all')\n        renderer = GlyphRenderer(data_source=self._source, glyph=patches)\n        # self._legends.append((self._groups[i-1], [renderer]))\n        yield renderer",
  "def __init__(self, values, **kws):\n        \"\"\"\n        Args:\n            values (iterable): iterable 2d representing the data series\n                values matrix.\n            index (str|1d iterable, optional): can be used to specify a\n                common custom index for all data series as follows:\n                    - As a 1d iterable of any sort (of datetime values)\n                        that will be used as series common index\n                    - As a string that corresponds to the key of the\n                        mapping to be used as index (and not as data\n                        series) if area.values is a mapping (like a dict,\n                        an OrderedDict or a pandas DataFrame). The values\n                        must be datetime values.\n            legend (str, optional): the legend of your chart. The legend\n                content is inferred from incoming input.It can be\n                ``top_left``, ``top_right``, ``bottom_left``,\n                ``bottom_right``. ``top_right`` is set if you set it\n                 as True. Defaults to None.\n            palette(list, optional): a list containing the colormap as\n                hex values.\n            num_folds (int, optional):\n            pos_color (hex color string, optional): t\n            neg_color (hex color string, optional): the color of\n                the negative folds\n                (default: #6495ed)\n\n        Attributes:\n            source (obj): datasource object for your plot,\n                initialized as a dummy None.\n            x_range (obj): x-associated datarange object for you plot,\n                initialized as a dummy None.\n            y_range (obj): y-associated datarange object for you plot,\n                initialized as a dummy None.\n            groups (list): to be filled with the incoming groups of data.\n                Useful for legend construction.\n            data (dict): to be filled with the incoming data and be passed\n                to the ColumnDataSource in each chart inherited class.\n                Needed for _set_And_get method.\n            attr (list): to be filled with the new attributes created after\n                loading the data dict.\n                Needed for _set_And_get method.\n        \"\"\"\n        super(HorizonBuilder, self).__init__(values, **kws)\n\n        self._fold_names = []\n        self._source = None\n        self._series = []\n        self._fold_height = {}\n        self._max_y = 0",
  "def fold_coordinates(self, y, fold_no, fold_height, y_origin=0, graph_ratio=1):\n        \"\"\" Function that calculate the coordinates for a value given a fold\n        \"\"\"\n        height = fold_no * fold_height\n        quotient, remainder = divmod(abs(y), float(height))\n        v = fold_height\n\n        # quotient would be 0 if the coordinate is represented in this fold\n        # layer\n        if math.floor(quotient) == 0:\n            v = 0\n            if remainder >= height - fold_height:\n                v = remainder - height + fold_height\n\n        v = v * graph_ratio\n        # Return tuple of the positive and negative relevant position of\n        # the coordinate against the provided fold layer\n        if y > 0:\n            return (v + y_origin, fold_height * graph_ratio + y_origin)\n        else:\n            return (y_origin, fold_height * graph_ratio - v + y_origin)",
  "def pad_list(self, l, padded_value=None):\n        \"\"\" Function that insert padded values at the start and end of\n        the list (l). If padded_value not provided, then duplicate the\n        values next to each end of the list\n        \"\"\"\n        if len(l) > 0:\n            l.insert(0, l[0] if padded_value is None else padded_value)\n            l.append(l[-1] if padded_value is None else padded_value)\n        return l",
  "def _process_data(self):\n        \"\"\"Use x/y data from the horizon values.\n\n        It calculates the chart properties accordingly. Then build a dict\n        containing references to all the points to be used by\n        the multiple area glyphes inside the ``_yield_renderers`` method.\n\n        \"\"\"\n        for col in self._values.keys():\n            if isinstance(self.index, string_types) and col == self.index:\n                continue\n\n            self._series.append(col)\n            self._max_y = max(max(self._values[col]), self._max_y)\n\n            v_index = [x for x in self._values_index]\n            self.set_and_get(\"x_\", col, self.pad_list(v_index))\n\n        self._fold_height = self._max_y / self.num_folds\n        self._graph_ratio = self.num_folds / len(self._series)\n\n        fill_alpha = []\n        fill_color = []\n\n        for serie_no, serie in enumerate(self._series):\n\n            self.set_and_get('y_', serie, self._values[serie])\n            y_origin = serie_no * self._max_y / len(self._series)\n\n            for fold_itr in range(1, self.num_folds + 1):\n\n                layers_datapoints = [self.fold_coordinates(\n                    x, fold_itr, self._fold_height, y_origin, self._graph_ratio) for x in self._values[serie]]\n                pos_points, neg_points = map(list, zip(*(layers_datapoints)))\n\n                alpha = 1.0 * (abs(fold_itr)) / self.num_folds\n\n                # Y coordinates above 0\n                pos_points = self.pad_list(pos_points, y_origin)\n                self.set_and_get(\"y_fold%s_\" % fold_itr, serie, pos_points)\n                self._fold_names.append(\"y_fold%s_%s\" % (fold_itr, serie))\n                fill_color.append(self.pos_color)\n                fill_alpha.append(alpha)\n\n                # Y coordinates below 0\n                neg_points = self.pad_list(\n                    neg_points, self._fold_height * self._graph_ratio + y_origin)\n                self.set_and_get(\"y_fold-%s_\" % fold_itr, serie, neg_points)\n                self._fold_names.append(\"y_fold-%s_%s\" % (fold_itr, serie))\n                fill_color.append(self.neg_color)\n                fill_alpha.append(alpha)\n\n                # Groups shown in the legend will only appear once\n                if serie_no == 0:\n                    self._groups.append(str(self._fold_height * fold_itr))\n                    self._groups.append(str(self._fold_height * -fold_itr))\n\n        self.set_and_get('fill_', 'alpha', fill_alpha)\n        self.set_and_get('fill_', 'color', fill_color)\n        self.set_and_get('x_', 'all', [self._data[\n                         'x_%s' % serie] for serie in self._series for y in range(self.num_folds * 2)])\n        self.set_and_get(\n            'y_', 'all', [self._data[f_name] for f_name in self._fold_names])",
  "def _set_sources(self):\n        \"\"\"Push the Horizon data into the ColumnDataSource and\n        calculate the proper ranges.\n        \"\"\"\n        self._source = ColumnDataSource(self._data)\n        self.x_range = DataRange1d(rangepadding=0, sources=[self._source.columns(self._attr[0])])\n        self.y_range = Range1d(start=0, end=self._max_y)",
  "def _yield_renderers(self):\n        \"\"\"Use the patch glyphs to connect the xy points in the time series.\n        It requires the positive and negative layers\n        Takes reference points from the data loaded at the ColumnDataSource.\n        \"\"\"\n        patches = Patches(\n            fill_color='fill_color', fill_alpha='fill_alpha', xs='x_all', ys='y_all')\n        renderer = GlyphRenderer(data_source=self._source, glyph=patches)\n        # self._legends.append((self._groups[i-1], [renderer]))\n        yield renderer",
  "def Bar(values, cat=None, stacked=False, xscale=\"categorical\", yscale=\"linear\",\n        xgrid=False, ygrid=True, continuous_range=None, **kw):\n\n    if continuous_range and not isinstance(continuous_range, Range1d):\n        raise ValueError(\n            \"continuous_range must be an instance of bokeh.models.ranges.Range1d\"\n        )\n\n    # The continuous_range is the y_range (until we implement HBar charts)\n    y_range = continuous_range\n\n    return create_and_build(\n        BarBuilder, values, cat=cat, stacked=stacked,\n        xscale=xscale, yscale=yscale,\n        xgrid=xgrid, ygrid=ygrid, y_range=y_range, **kw\n    )",
  "class BarBuilder(Builder):\n    \"\"\"This is the Bar class and it is in charge of plotting\n    Bar chart (grouped and stacked) in an easy and intuitive way.\n\n    Essentially, it provides a way to ingest the data, make the proper\n    calculations and push the references into a source object.\n    We additionally make calculations for the ranges.\n    And finally add the needed glyphs (rects) taking the references\n    from the source.\n\n    The x_range is categorical, and is made either from the cat argument\n    or from the indexes of the passed values if no cat is supplied.  The\n    y_range can be supplied as the parameter continuous_range,\n    or will be calculated as a linear range (Range1d) based on the supplied\n    values using the following rules:\n\n     * with all positive data: start = 0, end = 1.1 * max\n     * with all negative data: start = 1.1 * min, end = 0\n     * with mixed sign data:   start = 1.1 * min, end = 1.1 * max\n\n    Examples:\n\n        from collections import OrderedDict\n\n        xyvalues = OrderedDict()\n        xyvalues['python']=[2, 5]\n        xyvalues['pypy']=[12, 40]\n        xyvalues['jython']=[22, 30]\n\n        # For a stacked bar chart\n        stacked_bar = Bar(\n            xyvalues, ['1st', '2nd'], stacked=True, filename=\"stacked_bar.html\"\n        )\n        stacked_bar.show()\n\n        # For a grouped bar chart with a custom y_range\n\n        from bokeh.models import Range1d\n\n        custom_range = Range1d(start=2, end=40)\n        grouped_bar = Bar(\n            xyvalues, ['1st', '2nd'],\n            continuous_range=custom_range, filename=\"grouped.html\"\n        )\n        grouped_bar.show()\n    \"\"\"\n\n    cat = Either(Bool, List(Any), help=\"\"\"\n    List of string representing the categories. (Defaults to None.)\n    \"\"\")\n\n    stacked = Bool(False, help=\"\"\"\n    Whether to stack the bars. (Defaults to False)\n\n    If True, bars are draw as a stack, to show the relationship of\n    parts to a whole. Otherwise, bars are grouped on the same chart.\n\n    \"\"\")\n\n    def _process_data(self):\n        \"\"\"Take the Bar data from the input **value.\n\n        It calculates the chart properties accordingly. Then build a dict\n        containing references to all the calculated points to be used by\n        the rect glyph inside the ``_yield_renderers`` method.\n        \"\"\"\n        if not self.cat:\n            self.cat = [str(x) for x in self._values.index]\n\n        width = [0.8] * len(self.cat)\n        # width should decrease proportionally to the value length.\n        # 1./len(value) doesn't work well as the width needs to decrease a\n        # little bit faster\n        width_cat = [min(0.2, (1. / len(self._values)) ** 1.1)] * len(self.cat)\n        zero = np.zeros(len(self.cat))\n        self._data = dict(\n            cat=self.cat, width=width, width_cat=width_cat, zero=zero\n        )\n        # list to save all the groups available in the incomming input grouping\n        step = np.linspace(0, 1.0, len(self._values.keys()) + 1, endpoint=False)\n        self._groups.extend(self._values.keys())\n\n        for i, val in enumerate(self._values.keys()):\n            self.set_and_get(\"\", val, self._values[val])\n            mid = np.array(self._values[val]) / 2\n            self.set_and_get(\"mid\", val, mid)\n            self.set_and_get(\"stacked\", val, zero + mid)\n            # Grouped\n            grouped = [c + \":\" + str(step[i + 1]) for c in self.cat]\n            self.set_and_get(\"cat\", val, grouped)\n            # Stacked\n            zero += self._values[val]\n\n    def _set_sources(self):\n        \"\"\"Push the Bar data into the ColumnDataSource and calculate\n        the proper ranges.\n        \"\"\"\n        self._source = ColumnDataSource(self._data)\n        self.x_range = FactorRange(factors=self._source.data[\"cat\"])\n\n        if not self.y_range:\n            if self.stacked:\n                data = np.array(self._data['zero'])\n            else:\n                cats = [i for i in self._attr if not i.startswith((\"mid\", \"stacked\", \"cat\"))]\n                data = np.array([self._data[cat] for cat in cats])\n\n            all_positive = True if np.all(data > 0) else False\n            all_negative = True if np.all(data < 0) else False\n            # Set the start value\n            if all_positive:\n                start = 0\n            else:\n                start = 1.1 * data.min()  # Will always be negative\n\n            # Set the end value\n            if all_negative:\n                end = 0\n            else:\n                end = 1.1 * data.max()\n\n            self.y_range = Range1d(start=start, end=end)\n\n    def _yield_renderers(self):\n        \"\"\"Use the rect glyphs to display the bars.\n\n        Takes reference points from data loaded at the ColumnDataSource.\n        \"\"\"\n        quartets = list(chunk(self._attr, 4))\n        colors = cycle_colors(quartets, self.palette)\n\n        # quartet elements are: [data, mid, stacked, cat]\n        for i, quartet in enumerate(quartets):\n            if self.stacked:\n                glyph = Rect(\n                    x=\"cat\", y=quartet[2],\n                    width=\"width\", height=quartet[0],\n                    fill_color=colors[i], fill_alpha=0.7,\n                    line_color=\"white\"\n                )\n            else:  # Grouped\n                glyph = Rect(\n                    x=quartet[3], y=quartet[1],\n                    width=\"width_cat\", height=quartet[0],\n                    fill_color=colors[i], fill_alpha=0.7,\n                    line_color=\"white\"\n                )\n            renderer = GlyphRenderer(data_source=self._source, glyph=glyph)\n            self._legends.append((self._groups[i], [renderer]))\n            yield renderer",
  "def _process_data(self):\n        \"\"\"Take the Bar data from the input **value.\n\n        It calculates the chart properties accordingly. Then build a dict\n        containing references to all the calculated points to be used by\n        the rect glyph inside the ``_yield_renderers`` method.\n        \"\"\"\n        if not self.cat:\n            self.cat = [str(x) for x in self._values.index]\n\n        width = [0.8] * len(self.cat)\n        # width should decrease proportionally to the value length.\n        # 1./len(value) doesn't work well as the width needs to decrease a\n        # little bit faster\n        width_cat = [min(0.2, (1. / len(self._values)) ** 1.1)] * len(self.cat)\n        zero = np.zeros(len(self.cat))\n        self._data = dict(\n            cat=self.cat, width=width, width_cat=width_cat, zero=zero\n        )\n        # list to save all the groups available in the incomming input grouping\n        step = np.linspace(0, 1.0, len(self._values.keys()) + 1, endpoint=False)\n        self._groups.extend(self._values.keys())\n\n        for i, val in enumerate(self._values.keys()):\n            self.set_and_get(\"\", val, self._values[val])\n            mid = np.array(self._values[val]) / 2\n            self.set_and_get(\"mid\", val, mid)\n            self.set_and_get(\"stacked\", val, zero + mid)\n            # Grouped\n            grouped = [c + \":\" + str(step[i + 1]) for c in self.cat]\n            self.set_and_get(\"cat\", val, grouped)\n            # Stacked\n            zero += self._values[val]",
  "def _set_sources(self):\n        \"\"\"Push the Bar data into the ColumnDataSource and calculate\n        the proper ranges.\n        \"\"\"\n        self._source = ColumnDataSource(self._data)\n        self.x_range = FactorRange(factors=self._source.data[\"cat\"])\n\n        if not self.y_range:\n            if self.stacked:\n                data = np.array(self._data['zero'])\n            else:\n                cats = [i for i in self._attr if not i.startswith((\"mid\", \"stacked\", \"cat\"))]\n                data = np.array([self._data[cat] for cat in cats])\n\n            all_positive = True if np.all(data > 0) else False\n            all_negative = True if np.all(data < 0) else False\n            # Set the start value\n            if all_positive:\n                start = 0\n            else:\n                start = 1.1 * data.min()  # Will always be negative\n\n            # Set the end value\n            if all_negative:\n                end = 0\n            else:\n                end = 1.1 * data.max()\n\n            self.y_range = Range1d(start=start, end=end)",
  "def _yield_renderers(self):\n        \"\"\"Use the rect glyphs to display the bars.\n\n        Takes reference points from data loaded at the ColumnDataSource.\n        \"\"\"\n        quartets = list(chunk(self._attr, 4))\n        colors = cycle_colors(quartets, self.palette)\n\n        # quartet elements are: [data, mid, stacked, cat]\n        for i, quartet in enumerate(quartets):\n            if self.stacked:\n                glyph = Rect(\n                    x=\"cat\", y=quartet[2],\n                    width=\"width\", height=quartet[0],\n                    fill_color=colors[i], fill_alpha=0.7,\n                    line_color=\"white\"\n                )\n            else:  # Grouped\n                glyph = Rect(\n                    x=quartet[3], y=quartet[1],\n                    width=\"width_cat\", height=quartet[0],\n                    fill_color=colors[i], fill_alpha=0.7,\n                    line_color=\"white\"\n                )\n            renderer = GlyphRenderer(data_source=self._source, glyph=glyph)\n            self._legends.append((self._groups[i], [renderer]))\n            yield renderer",
  "def BoxPlot(values, marker=\"circle\", outliers=True, xscale=\"categorical\", yscale=\"linear\",\n        xgrid=False, ygrid=True, **kw):\n    return create_and_build(\n        BoxPlotBuilder, values, marker=marker, outliers=outliers,\n        xscale=xscale, yscale=yscale, xgrid=xgrid, ygrid=ygrid, **kw\n    )",
  "class BoxPlotBuilder(Builder):\n    \"\"\"This is the BoxPlot class and it is in charge of plotting\n    scatter plots in an easy and intuitive way.\n\n    Essentially, we provide a way to ingest the data, make the proper\n    calculations and push the references into a source object.\n    We additionally make calculations for the ranges.\n    And finally add the needed glyphs (rects, lines and markers)\n    taking the references from the source.\n\n    Examples:\n\n        from collections import OrderedDict\n        import numpy as np\n        from bokeh.charts import BoxPlot\n        from bokeh.sampledata.olympics2014 import data\n\n        data = {d['abbr']: d['medals'] for d in data['data'] if d['medals']['total'] > 0}\n        countries = sorted(data.keys(), key=lambda x: data[x]['total'], reverse=True)\n\n        gold = np.array([data[abbr]['gold'] for abbr in countries], dtype=np.float)\n        silver = np.array([data[abbr]['silver'] for abbr in countries], dtype=np.float)\n        bronze = np.array([data[abbr]['bronze'] for abbr in countries], dtype=np.float)\n\n        medals = OrderedDict(bronze=bronze, silver=silver, gold=gold)\n\n        boxplot = BoxPlot(medals, marker=\"circle\", outliers=True,\n                          title=\"boxplot, dict_input\", xlabel=\"medal type\", ylabel=\"medal count\",\n                          width=800, height=600, notebook=True)\n        boxplot.show()\n    \"\"\"\n\n    # TODO: (bev) should be an enumeration\n    marker = String(help=\"\"\"\n    The marker type to use (e.g., ``circle``) if outliers=True.\n    \"\"\")\n\n    outliers = Bool(help=\"\"\"\n    Whether to display markers for any outliers.\n    \"\"\")\n\n    def _process_data(self):\n        \"\"\"Take the BoxPlot data from the input **value.\n\n        It calculates the chart properties accordingly. Then build a dict\n        containing references to all the calculated points to be used by\n        the quad, segments and markers glyphs inside the ``_yield_renderers`` method.\n\n        Args:\n            cat (list): categories as a list of strings.\n            marker (int or string, optional): if outliers=True, the marker type to use\n                e.g., ``circle``.\n            outliers (bool, optional): Whether to plot outliers.\n            values (dict or pd obj): the values to be plotted as bars.\n        \"\"\"\n        self._data_segment = dict()\n        self._attr_segment = []\n        self._data_rect = dict()\n        self._attr_rect = []\n        self._data_scatter = dict()\n        self._attr_scatter = []\n        self._data_legend = dict()\n\n        if isinstance(self._values, pd.DataFrame):\n            self._groups = self._values.columns\n        else:\n            self._groups = list(self._values.keys())\n\n        # add group to the self._data_segment dict\n        self._data_segment[\"groups\"] = self._groups\n\n        # add group and witdh to the self._data_rect dict\n        self._data_rect[\"groups\"] = self._groups\n        self._data_rect[\"width\"] = [0.8] * len(self._groups)\n\n        # self._data_scatter does not need references to groups now,\n        # they will be added later.\n        # add group to the self._data_legend dict\n        self._data_legend[\"groups\"] = self._groups\n\n        # all the list we are going to use to save calculated values\n        q0_points = []\n        q2_points = []\n        iqr_centers = []\n        iqr_lengths = []\n        lower_points = []\n        upper_points = []\n        upper_center_boxes = []\n        upper_height_boxes = []\n        lower_center_boxes = []\n        lower_height_boxes = []\n        out_x, out_y, out_color = ([], [], [])\n\n        for i, level in enumerate(self._groups):\n            # Compute quantiles, center points, heights, IQR, etc.\n            # quantiles\n            q = np.percentile(self._values[level], [25, 50, 75])\n            q0_points.append(q[0])\n            q2_points.append(q[2])\n\n            # IQR related stuff...\n            iqr_centers.append((q[2] + q[0]) / 2)\n            iqr = q[2] - q[0]\n            iqr_lengths.append(iqr)\n            lower = q[1] - 1.5 * iqr\n            upper = q[1] + 1.5 * iqr\n            lower_points.append(lower)\n            upper_points.append(upper)\n\n            # rect center points and heights\n            upper_center_boxes.append((q[2] + q[1]) / 2)\n            upper_height_boxes.append(q[2] - q[1])\n            lower_center_boxes.append((q[1] + q[0]) / 2)\n            lower_height_boxes.append(q[1] - q[0])\n\n            # Store indices of outliers as list\n            outliers = np.where(\n                (self._values[level] > upper) | (self._values[level] < lower)\n            )[0]\n            out = self._values[level][outliers]\n            for o in out:\n                out_x.append(level)\n                out_y.append(o)\n                out_color.append(self.palette[i])\n\n        # Store\n        self.set_and_get(self._data_scatter, self._attr_scatter, \"out_x\", out_x)\n        self.set_and_get(self._data_scatter, self._attr_scatter, \"out_y\", out_y)\n        self.set_and_get(self._data_scatter, self._attr_scatter, \"colors\", out_color)\n\n        self.set_and_get(self._data_segment, self._attr_segment, \"q0\", q0_points)\n        self.set_and_get(self._data_segment, self._attr_segment, \"lower\", lower_points)\n        self.set_and_get(self._data_segment, self._attr_segment, \"q2\", q2_points)\n        self.set_and_get(self._data_segment, self._attr_segment, \"upper\", upper_points)\n\n        self.set_and_get(self._data_rect, self._attr_rect, \"iqr_centers\", iqr_centers)\n        self.set_and_get(self._data_rect, self._attr_rect, \"iqr_lengths\", iqr_lengths)\n        self.set_and_get(self._data_rect, self._attr_rect, \"upper_center_boxes\", upper_center_boxes)\n        self.set_and_get(self._data_rect, self._attr_rect, \"upper_height_boxes\", upper_height_boxes)\n        self.set_and_get(self._data_rect, self._attr_rect, \"lower_center_boxes\", lower_center_boxes)\n        self.set_and_get(self._data_rect, self._attr_rect, \"lower_height_boxes\", lower_height_boxes)\n        self.set_and_get(self._data_rect, self._attr_rect, \"colors\", self.palette)\n\n    def _set_sources(self):\n        \"Push the BoxPlot data into the ColumnDataSource and calculate the proper ranges.\"\n        self._source_segment = ColumnDataSource(self._data_segment)\n        self._source_scatter = ColumnDataSource(self._data_scatter)\n        self._source_rect = ColumnDataSource(self._data_rect)\n        self._source_legend = ColumnDataSource(self._data_legend)\n        self.x_range = FactorRange(factors=self._source_segment.data[\"groups\"])\n\n        start_y = min(self._data_segment[self._attr_segment[1]])\n        end_y = max(self._data_segment[self._attr_segment[3]])\n\n        ## Expand min/max to encompass outliers\n        if self.outliers:\n            start_out_y = min(self._data_scatter[self._attr_scatter[1]])\n            end_out_y = max(self._data_scatter[self._attr_scatter[1]])\n            # it could be no outliers in some sides...\n            start_y = min(start_y, start_out_y)\n            end_y = max(end_y, end_out_y)\n        self.y_range = Range1d(start=start_y - 0.1 * (end_y - start_y),\n                           end=end_y + 0.1 * (end_y - start_y))\n\n    def _yield_renderers(self):\n        \"\"\"Use the several glyphs to display the Boxplot.\n\n        It uses the selected marker glyph to display the points, segments to\n        display the iqr and rects to display the boxes, taking as reference\n        points the data loaded at the ColumnDataSurce.\n        \"\"\"\n        ats = self._attr_segment\n\n        glyph = Segment(\n            x0=\"groups\", y0=ats[1], x1=\"groups\", y1=ats[0],\n            line_color=\"black\", line_width=2\n        )\n        yield GlyphRenderer(data_source=self._source_segment, glyph=glyph)\n\n        glyph = Segment(\n            x0=\"groups\", y0=ats[2], x1=\"groups\", y1=ats[3],\n            line_color=\"black\", line_width=2\n        )\n        yield GlyphRenderer(data_source=self._source_segment, glyph=glyph)\n\n        atr = self._attr_rect\n\n        glyph = Rect(\n            x=\"groups\", y=atr[0], width=\"width\", height=atr[1],\n            line_color=\"black\", line_width=2, fill_color=None,\n        )\n        yield GlyphRenderer(data_source=self._source_rect, glyph=glyph)\n\n        glyph = Rect(\n            x=\"groups\", y=atr[2], width=\"width\", height=atr[3],\n            line_color=\"black\", fill_color=atr[6],\n        )\n        yield GlyphRenderer(data_source=self._source_rect, glyph=glyph)\n\n        glyph = Rect(\n            x=\"groups\", y=atr[4], width=\"width\", height=atr[5],\n            line_color=\"black\", fill_color=atr[6],\n        )\n        yield GlyphRenderer(data_source=self._source_rect, glyph=glyph)\n\n        if self.outliers:\n            yield make_scatter(self._source_scatter, self._attr_scatter[0],\n                              self._attr_scatter[1], self.marker,\n                              self._attr_scatter[2])\n\n        # We need to build the legend here using dummy glyphs\n        for i, level in enumerate(self._groups):\n            # TODO: (bev) what is this None business?\n            glyph = Rect(\n                x=\"groups\", y=None,\n                width=None, height=None,\n                line_color=\"black\", fill_color=self.palette[i])\n            renderer = GlyphRenderer(data_source=self._source_legend, glyph=glyph)\n\n            # need to manually select the proper glyphs to be rendered as legends\n            self._legends.append((self._groups[i], [renderer]))\n\n    # Some helper methods\n    def set_and_get(self, data, attr, val, content):\n        \"\"\"Set a new attr and then get it to fill the self._data dict.\n\n        Keep track of the attributes created.\n\n        Args:\n            data (dict): where to store the new attribute content\n            attr (list): where to store the new attribute names\n            val (string): name of the new attribute\n            content (obj): content of the new attribute\n        \"\"\"\n        self._set_and_get(data, \"\", attr, val, content)",
  "def _process_data(self):\n        \"\"\"Take the BoxPlot data from the input **value.\n\n        It calculates the chart properties accordingly. Then build a dict\n        containing references to all the calculated points to be used by\n        the quad, segments and markers glyphs inside the ``_yield_renderers`` method.\n\n        Args:\n            cat (list): categories as a list of strings.\n            marker (int or string, optional): if outliers=True, the marker type to use\n                e.g., ``circle``.\n            outliers (bool, optional): Whether to plot outliers.\n            values (dict or pd obj): the values to be plotted as bars.\n        \"\"\"\n        self._data_segment = dict()\n        self._attr_segment = []\n        self._data_rect = dict()\n        self._attr_rect = []\n        self._data_scatter = dict()\n        self._attr_scatter = []\n        self._data_legend = dict()\n\n        if isinstance(self._values, pd.DataFrame):\n            self._groups = self._values.columns\n        else:\n            self._groups = list(self._values.keys())\n\n        # add group to the self._data_segment dict\n        self._data_segment[\"groups\"] = self._groups\n\n        # add group and witdh to the self._data_rect dict\n        self._data_rect[\"groups\"] = self._groups\n        self._data_rect[\"width\"] = [0.8] * len(self._groups)\n\n        # self._data_scatter does not need references to groups now,\n        # they will be added later.\n        # add group to the self._data_legend dict\n        self._data_legend[\"groups\"] = self._groups\n\n        # all the list we are going to use to save calculated values\n        q0_points = []\n        q2_points = []\n        iqr_centers = []\n        iqr_lengths = []\n        lower_points = []\n        upper_points = []\n        upper_center_boxes = []\n        upper_height_boxes = []\n        lower_center_boxes = []\n        lower_height_boxes = []\n        out_x, out_y, out_color = ([], [], [])\n\n        for i, level in enumerate(self._groups):\n            # Compute quantiles, center points, heights, IQR, etc.\n            # quantiles\n            q = np.percentile(self._values[level], [25, 50, 75])\n            q0_points.append(q[0])\n            q2_points.append(q[2])\n\n            # IQR related stuff...\n            iqr_centers.append((q[2] + q[0]) / 2)\n            iqr = q[2] - q[0]\n            iqr_lengths.append(iqr)\n            lower = q[1] - 1.5 * iqr\n            upper = q[1] + 1.5 * iqr\n            lower_points.append(lower)\n            upper_points.append(upper)\n\n            # rect center points and heights\n            upper_center_boxes.append((q[2] + q[1]) / 2)\n            upper_height_boxes.append(q[2] - q[1])\n            lower_center_boxes.append((q[1] + q[0]) / 2)\n            lower_height_boxes.append(q[1] - q[0])\n\n            # Store indices of outliers as list\n            outliers = np.where(\n                (self._values[level] > upper) | (self._values[level] < lower)\n            )[0]\n            out = self._values[level][outliers]\n            for o in out:\n                out_x.append(level)\n                out_y.append(o)\n                out_color.append(self.palette[i])\n\n        # Store\n        self.set_and_get(self._data_scatter, self._attr_scatter, \"out_x\", out_x)\n        self.set_and_get(self._data_scatter, self._attr_scatter, \"out_y\", out_y)\n        self.set_and_get(self._data_scatter, self._attr_scatter, \"colors\", out_color)\n\n        self.set_and_get(self._data_segment, self._attr_segment, \"q0\", q0_points)\n        self.set_and_get(self._data_segment, self._attr_segment, \"lower\", lower_points)\n        self.set_and_get(self._data_segment, self._attr_segment, \"q2\", q2_points)\n        self.set_and_get(self._data_segment, self._attr_segment, \"upper\", upper_points)\n\n        self.set_and_get(self._data_rect, self._attr_rect, \"iqr_centers\", iqr_centers)\n        self.set_and_get(self._data_rect, self._attr_rect, \"iqr_lengths\", iqr_lengths)\n        self.set_and_get(self._data_rect, self._attr_rect, \"upper_center_boxes\", upper_center_boxes)\n        self.set_and_get(self._data_rect, self._attr_rect, \"upper_height_boxes\", upper_height_boxes)\n        self.set_and_get(self._data_rect, self._attr_rect, \"lower_center_boxes\", lower_center_boxes)\n        self.set_and_get(self._data_rect, self._attr_rect, \"lower_height_boxes\", lower_height_boxes)\n        self.set_and_get(self._data_rect, self._attr_rect, \"colors\", self.palette)",
  "def _set_sources(self):\n        \"Push the BoxPlot data into the ColumnDataSource and calculate the proper ranges.\"\n        self._source_segment = ColumnDataSource(self._data_segment)\n        self._source_scatter = ColumnDataSource(self._data_scatter)\n        self._source_rect = ColumnDataSource(self._data_rect)\n        self._source_legend = ColumnDataSource(self._data_legend)\n        self.x_range = FactorRange(factors=self._source_segment.data[\"groups\"])\n\n        start_y = min(self._data_segment[self._attr_segment[1]])\n        end_y = max(self._data_segment[self._attr_segment[3]])\n\n        ## Expand min/max to encompass outliers\n        if self.outliers:\n            start_out_y = min(self._data_scatter[self._attr_scatter[1]])\n            end_out_y = max(self._data_scatter[self._attr_scatter[1]])\n            # it could be no outliers in some sides...\n            start_y = min(start_y, start_out_y)\n            end_y = max(end_y, end_out_y)\n        self.y_range = Range1d(start=start_y - 0.1 * (end_y - start_y),\n                           end=end_y + 0.1 * (end_y - start_y))",
  "def _yield_renderers(self):\n        \"\"\"Use the several glyphs to display the Boxplot.\n\n        It uses the selected marker glyph to display the points, segments to\n        display the iqr and rects to display the boxes, taking as reference\n        points the data loaded at the ColumnDataSurce.\n        \"\"\"\n        ats = self._attr_segment\n\n        glyph = Segment(\n            x0=\"groups\", y0=ats[1], x1=\"groups\", y1=ats[0],\n            line_color=\"black\", line_width=2\n        )\n        yield GlyphRenderer(data_source=self._source_segment, glyph=glyph)\n\n        glyph = Segment(\n            x0=\"groups\", y0=ats[2], x1=\"groups\", y1=ats[3],\n            line_color=\"black\", line_width=2\n        )\n        yield GlyphRenderer(data_source=self._source_segment, glyph=glyph)\n\n        atr = self._attr_rect\n\n        glyph = Rect(\n            x=\"groups\", y=atr[0], width=\"width\", height=atr[1],\n            line_color=\"black\", line_width=2, fill_color=None,\n        )\n        yield GlyphRenderer(data_source=self._source_rect, glyph=glyph)\n\n        glyph = Rect(\n            x=\"groups\", y=atr[2], width=\"width\", height=atr[3],\n            line_color=\"black\", fill_color=atr[6],\n        )\n        yield GlyphRenderer(data_source=self._source_rect, glyph=glyph)\n\n        glyph = Rect(\n            x=\"groups\", y=atr[4], width=\"width\", height=atr[5],\n            line_color=\"black\", fill_color=atr[6],\n        )\n        yield GlyphRenderer(data_source=self._source_rect, glyph=glyph)\n\n        if self.outliers:\n            yield make_scatter(self._source_scatter, self._attr_scatter[0],\n                              self._attr_scatter[1], self.marker,\n                              self._attr_scatter[2])\n\n        # We need to build the legend here using dummy glyphs\n        for i, level in enumerate(self._groups):\n            # TODO: (bev) what is this None business?\n            glyph = Rect(\n                x=\"groups\", y=None,\n                width=None, height=None,\n                line_color=\"black\", fill_color=self.palette[i])\n            renderer = GlyphRenderer(data_source=self._source_legend, glyph=glyph)\n\n            # need to manually select the proper glyphs to be rendered as legends\n            self._legends.append((self._groups[i], [renderer]))",
  "def set_and_get(self, data, attr, val, content):\n        \"\"\"Set a new attr and then get it to fill the self._data dict.\n\n        Keep track of the attributes created.\n\n        Args:\n            data (dict): where to store the new attribute content\n            attr (list): where to store the new attribute names\n            val (string): name of the new attribute\n            content (obj): content of the new attribute\n        \"\"\"\n        self._set_and_get(data, \"\", attr, val, content)",
  "def Step(values, index=None, **kws):\n    return create_and_build(StepBuilder, values, index=index, **kws)",
  "class StepBuilder(Builder):\n    \"\"\"This is the Step class and it is in charge of plotting\n    Step charts in an easy and intuitive way.\n\n    Essentially, we provide a way to ingest the data, make the proper\n    calculations and push the references into a source object.\n    We additionally make calculations for the ranges.\n    And finally add the needed lines taking the references from the\n    source.\n\n    \"\"\"\n\n    index = Any(help=\"\"\"\n    An index to be used for all data series as follows:\n\n    - A 1d iterable of any sort that will be used as\n        series common index\n\n    - As a string that corresponds to the key of the\n        mapping to be used as index (and not as data\n        series) if area.values is a mapping (like a dict,\n        an OrderedDict or a pandas DataFrame)\n\n    \"\"\")\n\n    def _process_data(self):\n        \"\"\"It calculates the chart properties accordingly from Step.values.\n        Then build a dict containing references to all the points to be\n        used by the segment glyph inside the ``_yield_renderers`` method.\n        \"\"\"\n        self._data = dict()\n        self._groups = []\n\n        orig_xs = self._values_index\n        xs = np.empty(2*len(orig_xs)-1, dtype=np.int)\n        xs[::2] = orig_xs[:]\n        xs[1::2] = orig_xs[1:]\n        self._data['x'] = xs\n\n        for i, col in enumerate(self._values.keys()):\n            if isinstance(self.index, string_types) and col == self.index:\n                continue\n\n            # save every new group we find\n            self._groups.append(col)\n\n            orig_ys = np.array([self._values[col][x] for x in orig_xs])\n            ys = np.empty(2*len(orig_ys)-1)\n            ys[::2] = orig_ys[:]\n            ys[1::2] = orig_ys[:-1]\n            self._data['y_%s' % col] = ys\n\n    def _set_sources(self):\n        \"\"\" Push the Step data into the ColumnDataSource and calculate\n        the proper ranges.\n        \"\"\"\n        sc = self._source = ColumnDataSource(self._data)\n        self.x_range = DataRange1d(sources=[sc.columns(\"x\")])\n\n        y_sources = [sc.columns(\"y_%s\" % col) for col in self._groups]\n        self.y_range = DataRange1d(sources=y_sources)\n\n    def _yield_renderers(self):\n        \"\"\"Use the line glyphs to connect the xy points in the Step.\n\n        Takes reference points from the data loaded at the ColumnDataSource.\n        \"\"\"\n        colors = cycle_colors(self._groups, self.palette)\n\n        for i, name in enumerate(self._groups):\n            # draw the step horizontal segment\n            glyph = Line(x=\"x\", y=\"y_%s\" % name, line_color=colors[i], line_width=2)\n            renderer = GlyphRenderer(data_source=self._source, glyph=glyph)\n            self._legends.append((self._groups[i], [renderer]))\n            yield renderer",
  "def _process_data(self):\n        \"\"\"It calculates the chart properties accordingly from Step.values.\n        Then build a dict containing references to all the points to be\n        used by the segment glyph inside the ``_yield_renderers`` method.\n        \"\"\"\n        self._data = dict()\n        self._groups = []\n\n        orig_xs = self._values_index\n        xs = np.empty(2*len(orig_xs)-1, dtype=np.int)\n        xs[::2] = orig_xs[:]\n        xs[1::2] = orig_xs[1:]\n        self._data['x'] = xs\n\n        for i, col in enumerate(self._values.keys()):\n            if isinstance(self.index, string_types) and col == self.index:\n                continue\n\n            # save every new group we find\n            self._groups.append(col)\n\n            orig_ys = np.array([self._values[col][x] for x in orig_xs])\n            ys = np.empty(2*len(orig_ys)-1)\n            ys[::2] = orig_ys[:]\n            ys[1::2] = orig_ys[:-1]\n            self._data['y_%s' % col] = ys",
  "def _set_sources(self):\n        \"\"\" Push the Step data into the ColumnDataSource and calculate\n        the proper ranges.\n        \"\"\"\n        sc = self._source = ColumnDataSource(self._data)\n        self.x_range = DataRange1d(sources=[sc.columns(\"x\")])\n\n        y_sources = [sc.columns(\"y_%s\" % col) for col in self._groups]\n        self.y_range = DataRange1d(sources=y_sources)",
  "def _yield_renderers(self):\n        \"\"\"Use the line glyphs to connect the xy points in the Step.\n\n        Takes reference points from the data loaded at the ColumnDataSource.\n        \"\"\"\n        colors = cycle_colors(self._groups, self.palette)\n\n        for i, name in enumerate(self._groups):\n            # draw the step horizontal segment\n            glyph = Line(x=\"x\", y=\"y_%s\" % name, line_color=colors[i], line_width=2)\n            renderer = GlyphRenderer(data_source=self._source, glyph=glyph)\n            self._legends.append((self._groups[i], [renderer]))\n            yield renderer",
  "def Line(values, index=None, **kws):\n    return create_and_build(LineBuilder, values, index=index, **kws)",
  "class LineBuilder(Builder):\n    \"\"\"This is the Line class and it is in charge of plotting\n    Line charts in an easy and intuitive way.\n    Essentially, we provide a way to ingest the data, make the proper\n    calculations and push the references into a source object.\n    We additionally make calculations for the ranges.\n    And finally add the needed lines taking the references from the source.\n    \"\"\"\n\n    index = Any(help=\"\"\"\n    An index to be used for all data series as follows:\n\n    - A 1d iterable of any sort that will be used as\n        series common index\n\n    - As a string that corresponds to the key of the\n        mapping to be used as index (and not as data\n        series) if area.values is a mapping (like a dict,\n        an OrderedDict or a pandas DataFrame)\n\n    \"\"\")\n\n    def _process_data(self):\n        \"\"\"Calculate the chart properties accordingly from line.values.\n        Then build a dict containing references to all the points to be\n        used by the line glyph inside the ``_yield_renderers`` method.\n        \"\"\"\n        self._data = dict()\n        # list to save all the attributes we are going to create\n        self._attr = []\n        xs = self._values_index\n        self.set_and_get(\"x\", \"\", np.array(xs))\n        for col in self._values.keys():\n            if isinstance(self.index, string_types) and col == self.index:\n                continue\n\n            # save every new group we find\n            self._groups.append(col)\n            values = [self._values[col][x] for x in xs]\n            self.set_and_get(\"y_\", col, values)\n\n    def _set_sources(self):\n        \"\"\"\n        Push the Line data into the ColumnDataSource and calculate the\n        proper ranges.\n        \"\"\"\n        self._source = ColumnDataSource(self._data)\n        self.x_range = DataRange1d(sources=[self._source.columns(\"x\")])\n\n        y_names = self._attr[1:]\n        endy = max(max(self._data[i]) for i in y_names)\n        starty = min(min(self._data[i]) for i in y_names)\n        self.y_range = Range1d(\n            start=starty - 0.1 * (endy - starty),\n            end=endy + 0.1 * (endy - starty)\n        )\n\n    def _yield_renderers(self):\n        \"\"\"Use the line glyphs to connect the xy points in the Line.\n        Takes reference points from the data loaded at the ColumnDataSource.\n        \"\"\"\n        colors = cycle_colors(self._attr, self.palette)\n        for i, duplet in enumerate(self._attr[1:], start=1):\n            glyph = LineGlyph(x='x', y=duplet, line_color=colors[i - 1])\n            renderer = GlyphRenderer(data_source=self._source, glyph=glyph)\n            self._legends.append((self._groups[i-1], [renderer]))\n            yield renderer",
  "def _process_data(self):\n        \"\"\"Calculate the chart properties accordingly from line.values.\n        Then build a dict containing references to all the points to be\n        used by the line glyph inside the ``_yield_renderers`` method.\n        \"\"\"\n        self._data = dict()\n        # list to save all the attributes we are going to create\n        self._attr = []\n        xs = self._values_index\n        self.set_and_get(\"x\", \"\", np.array(xs))\n        for col in self._values.keys():\n            if isinstance(self.index, string_types) and col == self.index:\n                continue\n\n            # save every new group we find\n            self._groups.append(col)\n            values = [self._values[col][x] for x in xs]\n            self.set_and_get(\"y_\", col, values)",
  "def _set_sources(self):\n        \"\"\"\n        Push the Line data into the ColumnDataSource and calculate the\n        proper ranges.\n        \"\"\"\n        self._source = ColumnDataSource(self._data)\n        self.x_range = DataRange1d(sources=[self._source.columns(\"x\")])\n\n        y_names = self._attr[1:]\n        endy = max(max(self._data[i]) for i in y_names)\n        starty = min(min(self._data[i]) for i in y_names)\n        self.y_range = Range1d(\n            start=starty - 0.1 * (endy - starty),\n            end=endy + 0.1 * (endy - starty)\n        )",
  "def _yield_renderers(self):\n        \"\"\"Use the line glyphs to connect the xy points in the Line.\n        Takes reference points from the data loaded at the ColumnDataSource.\n        \"\"\"\n        colors = cycle_colors(self._attr, self.palette)\n        for i, duplet in enumerate(self._attr[1:], start=1):\n            glyph = LineGlyph(x='x', y=duplet, line_color=colors[i - 1])\n            renderer = GlyphRenderer(data_source=self._source, glyph=glyph)\n            self._legends.append((self._groups[i-1], [renderer]))\n            yield renderer",
  "def Dot(values, cat=None, stem=True, xscale=\"categorical\", yscale=\"linear\",\n        xgrid=False, ygrid=True, **kws):\n    return create_and_build(\n        DotBuilder, values, cat=cat, stem=stem, xscale=xscale, yscale=yscale,\n        xgrid=xgrid, ygrid=ygrid, **kws\n    )",
  "class DotBuilder(Builder):\n    \"\"\"This is the Dot class and it is in charge of plotting Dot chart\n     in an easy and intuitive way.\n\n    Essentially, it provides a way to ingest the data, make the proper\n    calculations and push the references into a source object.\n    We additionally make calculations for the ranges.\n    And finally add the needed glyphs (segments and circles) taking\n    the references from the source.\n\n    \"\"\"\n\n    cat = Either(Bool, List(Any), help=\"\"\"\n    List of string representing the categories. (Defaults to None.)\n    \"\"\")\n\n    stem = Bool(True, help=\"\"\"\n    Whether to draw a stem from each do to the axis.\n    \"\"\")\n\n    def _process_data(self):\n        \"\"\"Take the Dot data from the input **value.\n\n        It calculates the chart properties accordingly. Then build a dict\n        containing references to all the calculated points to be used by\n        the rect glyph inside the ``_yield_renderers`` method.\n\n        \"\"\"\n        if not self.cat:\n            self.cat = [str(x) for x in self._values.index]\n\n        self._data = dict(cat=self.cat, zero=np.zeros(len(self.cat)))\n        # list to save all the attributes we are going to create\n        # list to save all the groups available in the incoming input\n        # Grouping\n        self._groups.extend(self._values.keys())\n        step = np.linspace(0, 1.0, len(self._values.keys()) + 1, endpoint=False)\n\n        for i, val in enumerate(self._values.keys()):\n            values = self._values[val]\n            # original y value\n            self.set_and_get(\"\", val, values)\n            # x value\n            cats = [c + \":\" + str(step[i + 1]) for c in self.cat]\n            self.set_and_get(\"cat\", val, cats)\n            # zeros\n            self.set_and_get(\"z_\", val, np.zeros(len(values)))\n            # segment top y value\n            self.set_and_get(\"seg_top_\", val, values)\n\n    def _set_sources(self):\n        \"\"\"Push the Dot data into the ColumnDataSource and calculate\n        the proper ranges.\n        \"\"\"\n        self._source = ColumnDataSource(self._data)\n        self.x_range = FactorRange(factors=self._source.data[\"cat\"])\n        cat = [i for i in self._attr if not i.startswith((\"cat\",))]\n        end = 1.1 * max(max(self._data[i]) for i in cat)\n        self.y_range = Range1d(start=0, end=end)\n\n    def _yield_renderers(self):\n        \"\"\"Use the rect glyphs to display the bars.\n\n        Takes reference points from data loaded at the source and\n        renders circle glyphs (and segments) on the related\n        coordinates.\n        \"\"\"\n        self._tuples = list(chunk(self._attr, 4))\n        colors = cycle_colors(self._tuples, self.palette)\n\n        # quartet elements are: [data, cat, zeros, segment_top]\n        for i, quartet in enumerate(self._tuples):\n            # draw segment first so when scatter will be place on top of it\n            # and it won't show segment chunk on top of the circle\n            if self.stem:\n                glyph = Segment(\n                    x0=quartet[1], y0=quartet[2], x1=quartet[1], y1=quartet[3],\n                    line_color=\"black\", line_width=2\n                )\n                yield GlyphRenderer(data_source=self._source, glyph=glyph)\n\n            renderer = make_scatter(\n                self._source, quartet[1], quartet[0], 'circle',\n                colors[i - 1], line_color='black', size=15, fill_alpha=1.,\n            )\n            self._legends.append((self._groups[i], [renderer]))\n            yield renderer",
  "def _process_data(self):\n        \"\"\"Take the Dot data from the input **value.\n\n        It calculates the chart properties accordingly. Then build a dict\n        containing references to all the calculated points to be used by\n        the rect glyph inside the ``_yield_renderers`` method.\n\n        \"\"\"\n        if not self.cat:\n            self.cat = [str(x) for x in self._values.index]\n\n        self._data = dict(cat=self.cat, zero=np.zeros(len(self.cat)))\n        # list to save all the attributes we are going to create\n        # list to save all the groups available in the incoming input\n        # Grouping\n        self._groups.extend(self._values.keys())\n        step = np.linspace(0, 1.0, len(self._values.keys()) + 1, endpoint=False)\n\n        for i, val in enumerate(self._values.keys()):\n            values = self._values[val]\n            # original y value\n            self.set_and_get(\"\", val, values)\n            # x value\n            cats = [c + \":\" + str(step[i + 1]) for c in self.cat]\n            self.set_and_get(\"cat\", val, cats)\n            # zeros\n            self.set_and_get(\"z_\", val, np.zeros(len(values)))\n            # segment top y value\n            self.set_and_get(\"seg_top_\", val, values)",
  "def _set_sources(self):\n        \"\"\"Push the Dot data into the ColumnDataSource and calculate\n        the proper ranges.\n        \"\"\"\n        self._source = ColumnDataSource(self._data)\n        self.x_range = FactorRange(factors=self._source.data[\"cat\"])\n        cat = [i for i in self._attr if not i.startswith((\"cat\",))]\n        end = 1.1 * max(max(self._data[i]) for i in cat)\n        self.y_range = Range1d(start=0, end=end)",
  "def _yield_renderers(self):\n        \"\"\"Use the rect glyphs to display the bars.\n\n        Takes reference points from data loaded at the source and\n        renders circle glyphs (and segments) on the related\n        coordinates.\n        \"\"\"\n        self._tuples = list(chunk(self._attr, 4))\n        colors = cycle_colors(self._tuples, self.palette)\n\n        # quartet elements are: [data, cat, zeros, segment_top]\n        for i, quartet in enumerate(self._tuples):\n            # draw segment first so when scatter will be place on top of it\n            # and it won't show segment chunk on top of the circle\n            if self.stem:\n                glyph = Segment(\n                    x0=quartet[1], y0=quartet[2], x1=quartet[1], y1=quartet[3],\n                    line_color=\"black\", line_width=2\n                )\n                yield GlyphRenderer(data_source=self._source, glyph=glyph)\n\n            renderer = make_scatter(\n                self._source, quartet[1], quartet[0], 'circle',\n                colors[i - 1], line_color='black', size=15, fill_alpha=1.,\n            )\n            self._legends.append((self._groups[i], [renderer]))\n            yield renderer",
  "def Donut(values,  cat=None, width=800, height=800, xgrid=False, ygrid=False, **kws):\n    return create_and_build(\n        DonutBuilder, values, cat=cat, width=width, height=height,\n        xgrid=xgrid, ygrid=ygrid, **kws\n    )",
  "class DonutBuilder(Builder):\n    \"\"\"This is the Donut class and it is in charge of plotting\n    Donut chart in an easy and intuitive way.\n\n    Essentially, it provides a way to ingest the data, make the proper\n    calculations and push the references into a source object.\n    We additionally make calculations for the donut slices and angles.\n    And finally add the needed glyphs (Wedges and AnnularWedges) taking\n    the references from the source.\n\n    Examples:\n\n        xyvalues = OrderedDict()\n        # TODO: Fix bug for donut breaking when inputs that are not float\n        xyvalues['python'] = [2., 5., 3.]\n        xyvalues['pypy'] = [4., 1., 4.]\n        xyvalues['jython'] = [6., 4., 3.]\n        cat = ['Devs', 'Dev Ops', 'Scientists']\n        donut = Donut(xyvalues, cat, title=\"Medals Donut\",\n            xlabel='Cat', ylabel='Lang', filename=\"donut.html\")\n        donut.show()\n    \"\"\"\n\n    cat = Either(Bool, List(Any), help=\"\"\"\n    List of string representing the categories. (Defaults to None.)\n    \"\"\")\n\n    def _process_data(self):\n        \"\"\"Take the chart data from self._values.\n\n        It calculates the chart properties accordingly (start/end angles).\n        Then build a dict containing references to all the calculated\n        points to be used by the Wedge glyph inside the ``_yield_renderers`` method.\n\n        \"\"\"\n        dd = dict(zip(self._values.keys(), self._values.values()))\n        self._df = df = pd.DataFrame(dd)\n        self._groups = df.index = self.cat\n        df.columns = self._values.keys()\n\n        # Get the sum per category\n        aggregated = df.T.sum()\n        # Get the total (sum of all categories)\n        self._total_units = total = aggregated.sum()\n        radians = lambda x: 2*pi*(x/total)\n        angles = aggregated.map(radians).cumsum()\n        end_angles = angles.tolist()\n        start_angles = [0] + end_angles[:-1]\n        colors = cycle_colors(self.cat, self.palette)\n        self.set_and_get(\"\", \"colors\", colors)\n        self.set_and_get(\"\", \"end\", end_angles)\n        self.set_and_get(\"\", \"start\", start_angles)\n\n    def _set_sources(self):\n        \"\"\"Push the Donut data into the ColumnDataSource and calculate\n         the proper ranges.\n\n        \"\"\"\n        self._source = ColumnDataSource(self._data)\n        self.x_range = Range1d(start=-2, end=2)\n        self.y_range = Range1d(start=-2, end=2)\n\n    def draw_central_wedge(self):\n        \"\"\"Draw the central part of the donut wedge from donut.source and\n         its calculated start and end angles.\n\n        \"\"\"\n        glyph = Wedge(\n            x=0, y=0, radius=1, start_angle=\"start\", end_angle=\"end\",\n            line_color=\"white\", line_width=2, fill_color=\"colors\"\n        )\n        yield GlyphRenderer(data_source=self._source, glyph=glyph)\n\n    def draw_central_descriptions(self):\n        \"\"\"Draw the descriptions to be placed on the central part of the\n        donut wedge\n        \"\"\"\n        text = [\"%s\" % cat for cat in self.cat]\n        x, y = polar_to_cartesian(0.7, self._data[\"start\"], self._data[\"end\"])\n        text_source = ColumnDataSource(dict(text=text, x=x, y=y))\n        glyph = Text(\n                x=\"x\", y=\"y\", text=\"text\",\n                text_align=\"center\", text_baseline=\"middle\"\n            )\n        yield GlyphRenderer(data_source=text_source, glyph=glyph)\n\n    def draw_external_ring(self, colors=None):\n        \"\"\"Draw the external part of the donut wedge from donut.source\n         and its related descriptions\n        \"\"\"\n        if colors is None:\n            colors = cycle_colors(self.cat, self.palette)\n\n        first = True\n        for i, (cat, start_angle, end_angle) in enumerate(zip(\n                self.cat, self._data['start'], self._data['end'])):\n            details = self._df.ix[i]\n            radians = lambda x: 2*pi*(x/self._total_units)\n\n            angles = details.map(radians).cumsum() + start_angle\n            end = angles.tolist() + [end_angle]\n            start = [start_angle] + end[:-1]\n            base_color = colors[i]\n            #fill = [ base_color.lighten(i*0.05) for i in range(len(details) + 1) ]\n            fill = [base_color for i in range(len(details) + 1)]\n            text = [rowlabel for rowlabel in details.index]\n            x, y = polar_to_cartesian(1.25, start, end)\n\n            source = ColumnDataSource(dict(start=start, end=end, fill=fill))\n\n            glyph = AnnularWedge(\n                x=0, y=0, inner_radius=1, outer_radius=1.5,\n                start_angle=\"start\", end_angle=\"end\",\n                line_color=\"white\", line_width=2,\n                fill_color=\"fill\"\n            )\n            yield GlyphRenderer(data_source=source, glyph=glyph)\n\n            text_angle = [(start[i]+end[i])/2 for i in range(len(start))]\n            text_angle = [angle + pi if pi/2 < angle < 3*pi/2 else angle\n                          for angle in text_angle]\n\n            if first and text:\n                text.insert(0, '')\n                offset = pi / 48\n                text_angle.insert(0, text_angle[0] - offset)\n                start.insert(0, start[0] - offset)\n                end.insert(0, end[0] - offset)\n                x, y = polar_to_cartesian(1.25, start, end)\n                first = False\n            data = dict(text=text, x=x, y=y, angle=text_angle)\n            text_source = ColumnDataSource(data)\n            glyph = Text(\n                x=\"x\", y=\"y\", text=\"text\", angle=\"angle\",\n                text_align=\"center\", text_baseline=\"middle\"\n            )\n            yield GlyphRenderer(data_source=text_source, glyph=glyph)\n\n    def _yield_renderers(self):\n        \"\"\"Use the AnnularWedge and Wedge glyphs to display the wedges.\n\n        Takes reference points from data loaded at the ColumnDataSurce.\n        \"\"\"\n        # build the central round area of the donut\n        renderers = []\n        renderers += self.draw_central_wedge()\n        # write central descriptions\n        renderers += self.draw_central_descriptions()\n        # build external donut ring\n        renderers += self.draw_external_ring()\n        return renderers",
  "def _process_data(self):\n        \"\"\"Take the chart data from self._values.\n\n        It calculates the chart properties accordingly (start/end angles).\n        Then build a dict containing references to all the calculated\n        points to be used by the Wedge glyph inside the ``_yield_renderers`` method.\n\n        \"\"\"\n        dd = dict(zip(self._values.keys(), self._values.values()))\n        self._df = df = pd.DataFrame(dd)\n        self._groups = df.index = self.cat\n        df.columns = self._values.keys()\n\n        # Get the sum per category\n        aggregated = df.T.sum()\n        # Get the total (sum of all categories)\n        self._total_units = total = aggregated.sum()\n        radians = lambda x: 2*pi*(x/total)\n        angles = aggregated.map(radians).cumsum()\n        end_angles = angles.tolist()\n        start_angles = [0] + end_angles[:-1]\n        colors = cycle_colors(self.cat, self.palette)\n        self.set_and_get(\"\", \"colors\", colors)\n        self.set_and_get(\"\", \"end\", end_angles)\n        self.set_and_get(\"\", \"start\", start_angles)",
  "def _set_sources(self):\n        \"\"\"Push the Donut data into the ColumnDataSource and calculate\n         the proper ranges.\n\n        \"\"\"\n        self._source = ColumnDataSource(self._data)\n        self.x_range = Range1d(start=-2, end=2)\n        self.y_range = Range1d(start=-2, end=2)",
  "def draw_central_wedge(self):\n        \"\"\"Draw the central part of the donut wedge from donut.source and\n         its calculated start and end angles.\n\n        \"\"\"\n        glyph = Wedge(\n            x=0, y=0, radius=1, start_angle=\"start\", end_angle=\"end\",\n            line_color=\"white\", line_width=2, fill_color=\"colors\"\n        )\n        yield GlyphRenderer(data_source=self._source, glyph=glyph)",
  "def draw_central_descriptions(self):\n        \"\"\"Draw the descriptions to be placed on the central part of the\n        donut wedge\n        \"\"\"\n        text = [\"%s\" % cat for cat in self.cat]\n        x, y = polar_to_cartesian(0.7, self._data[\"start\"], self._data[\"end\"])\n        text_source = ColumnDataSource(dict(text=text, x=x, y=y))\n        glyph = Text(\n                x=\"x\", y=\"y\", text=\"text\",\n                text_align=\"center\", text_baseline=\"middle\"\n            )\n        yield GlyphRenderer(data_source=text_source, glyph=glyph)",
  "def draw_external_ring(self, colors=None):\n        \"\"\"Draw the external part of the donut wedge from donut.source\n         and its related descriptions\n        \"\"\"\n        if colors is None:\n            colors = cycle_colors(self.cat, self.palette)\n\n        first = True\n        for i, (cat, start_angle, end_angle) in enumerate(zip(\n                self.cat, self._data['start'], self._data['end'])):\n            details = self._df.ix[i]\n            radians = lambda x: 2*pi*(x/self._total_units)\n\n            angles = details.map(radians).cumsum() + start_angle\n            end = angles.tolist() + [end_angle]\n            start = [start_angle] + end[:-1]\n            base_color = colors[i]\n            #fill = [ base_color.lighten(i*0.05) for i in range(len(details) + 1) ]\n            fill = [base_color for i in range(len(details) + 1)]\n            text = [rowlabel for rowlabel in details.index]\n            x, y = polar_to_cartesian(1.25, start, end)\n\n            source = ColumnDataSource(dict(start=start, end=end, fill=fill))\n\n            glyph = AnnularWedge(\n                x=0, y=0, inner_radius=1, outer_radius=1.5,\n                start_angle=\"start\", end_angle=\"end\",\n                line_color=\"white\", line_width=2,\n                fill_color=\"fill\"\n            )\n            yield GlyphRenderer(data_source=source, glyph=glyph)\n\n            text_angle = [(start[i]+end[i])/2 for i in range(len(start))]\n            text_angle = [angle + pi if pi/2 < angle < 3*pi/2 else angle\n                          for angle in text_angle]\n\n            if first and text:\n                text.insert(0, '')\n                offset = pi / 48\n                text_angle.insert(0, text_angle[0] - offset)\n                start.insert(0, start[0] - offset)\n                end.insert(0, end[0] - offset)\n                x, y = polar_to_cartesian(1.25, start, end)\n                first = False\n            data = dict(text=text, x=x, y=y, angle=text_angle)\n            text_source = ColumnDataSource(data)\n            glyph = Text(\n                x=\"x\", y=\"y\", text=\"text\", angle=\"angle\",\n                text_align=\"center\", text_baseline=\"middle\"\n            )\n            yield GlyphRenderer(data_source=text_source, glyph=glyph)",
  "def _yield_renderers(self):\n        \"\"\"Use the AnnularWedge and Wedge glyphs to display the wedges.\n\n        Takes reference points from data loaded at the ColumnDataSurce.\n        \"\"\"\n        # build the central round area of the donut\n        renderers = []\n        renderers += self.draw_central_wedge()\n        # write central descriptions\n        renderers += self.draw_central_descriptions()\n        # build external donut ring\n        renderers += self.draw_external_ring()\n        return renderers",
  "def Scatter(values, **kws):\n    return create_and_build(ScatterBuilder, values, **kws)",
  "class ScatterBuilder(Builder):\n    \"\"\"This is the Scatter class and it is in charge of plotting\n    Scatter charts in an easy and intuitive way.\n\n    Essentially, we provide a way to ingest the data, make the proper\n    calculations and push the references into a source object.\n    We additionally make calculations for the ranges. And finally add\n    the needed glyphs (markers) taking the references from the source.\n\n    Example:\n        from collections import OrderedDict\n\n        from bokeh.charts import Scatter\n        xyvalues = OrderedDict()\n        xyvalues['python'] = [(1, 2), (3, 3), (4, 7), (5, 5), (8, 26)]\n        xyvalues['pypy'] = [(1, 12), (2, 23), (4, 47), (5, 15), (8, 46)]\n        xyvalues['jython'] = [(1, 22), (2, 43), (4, 10), (6, 25), (8, 26)]\n\n        scatter = Scatter(xyvalues, title=\"Languages Scatter\",\n            filename=\"scatter.html\")\n        scatter.show()\n    \"\"\"\n\n    # TODO: (bev) should be an enumeration\n    marker = String(\"circle\", help=\"\"\"\n    The marker type to use (default: ``circle``).\n    \"\"\")\n\n    def _process_data(self):\n        \"\"\"Take the scatter.values data to calculate the chart properties\n        accordingly. Then build a dict containing references to all the\n        calculated points to be used by the marker glyph inside the\n        ``_yield_renderers`` method.\n        \"\"\"\n        self._data = dict()\n        # list to save all the attributes we are going to create\n        self._attr = []\n        # list to save all the groups available in the incoming input\n        self._groups.extend(self._values.keys())\n        # Grouping\n        self.parse_data()\n\n    @property\n    def parse_data(self):\n        \"\"\"Parse data received from self._values and create correct x, y\n        series values checking if input is a pandas DataFrameGroupBy\n        object or one of the stardard supported types (that can be\n        converted to a DataAdapter)\n        \"\"\"\n        if pd is not None and \\\n                isinstance(self._values, pd.core.groupby.DataFrameGroupBy):\n            return self._parse_groupped_data\n        else:\n            return self._parse_data\n\n    def _parse_groupped_data(self):\n        \"\"\"Parse data in self._values in case it's a pandas\n        DataFrameGroupBy and create the data 'x_...' and 'y_...' values\n        for all data series\n        \"\"\"\n        for i, val in enumerate(self._values.keys()):\n            xy = self._values[val]\n            self._set_and_get(\"x_\", val, xy[:, 0])\n            self._set_and_get(\"y_\", val, xy[:, 1])\n\n    def _parse_data(self):\n        \"\"\"Parse data in self._values in case it's an iterable (not a pandas\n        DataFrameGroupBy) and create the data 'x_...' and 'y_...' values\n        for all data series\n        \"\"\"\n        for i, val in enumerate(self._values.keys()):\n            x_, y_ = [], []\n            xy = self._values[val]\n            for value in self._values.index:\n                x_.append(xy[value][0])\n                y_.append(xy[value][1])\n\n            self.set_and_get(\"x_\", val, x_)\n            self.set_and_get(\"y_\", val, y_)\n\n    def _set_sources(self):\n        \"\"\"Push the Scatter data into the ColumnDataSource and\n        calculate the proper ranges.\"\"\"\n        self._source = ColumnDataSource(self._data)\n\n        x_names, y_names = self._attr[::2], self._attr[1::2]\n        endx = max(max(self._data[i]) for i in x_names)\n        startx = min(min(self._data[i]) for i in x_names)\n        self.x_range = Range1d(\n            start=startx - 0.1 * (endx - startx),\n            end=endx + 0.1 * (endx - startx)\n        )\n        endy = max(max(self._data[i]) for i in y_names)\n        starty = min(min(self._data[i]) for i in y_names)\n        self.y_range = Range1d(\n            start=starty - 0.1 * (endy - starty),\n            end=endy + 0.1 * (endy - starty)\n        )\n\n    def _yield_renderers(self):\n        \"\"\"Use the marker glyphs to display the points.\n\n        Takes reference points from data loaded at the ColumnDataSource.\n        \"\"\"\n        duplets = list(chunk(self._attr, 2))\n        colors = cycle_colors(duplets, self.palette)\n\n        for i, duplet in enumerate(duplets, start=1):\n            renderer = make_scatter(\n                self._source, duplet[0], duplet[1], self.marker, colors[i - 1]\n            )\n            self._legends.append((self._groups[i-1], [renderer]))\n            yield renderer\n\n    def _adapt_values(self):\n        \"\"\"Prepare context before main show method is invoked.\n\n        Customize show preliminary actions by handling DataFrameGroupBy\n        values in order to create the series values and labels.\"\"\"\n        # check if pandas is installed\n        if pd:\n            # if it is we try to take advantage of it's data structures\n            # asumming we get an groupby object\n            if isinstance(self._values, pd.core.groupby.DataFrameGroupBy):\n                pdict = OrderedDict()\n\n                for i in self._values.groups.keys():\n                    self._labels = self._values.get_group(i).columns\n                    xname = self._values.get_group(i).columns[0]\n                    yname = self._values.get_group(i).columns[1]\n                    x = getattr(self._values.get_group(i), xname)\n                    y = getattr(self._values.get_group(i), yname)\n                    pdict[i] = np.array([x.values, y.values]).T\n\n                self._values = DataAdapter(pdict)\n                self._labels = self._values.keys()\n            else:\n                self._values = DataAdapter(self._values)\n                self._labels = self._values.keys()\n\n        else:\n            self._values = DataAdapter(self._values)\n            self._labels = self._values.keys()",
  "def _process_data(self):\n        \"\"\"Take the scatter.values data to calculate the chart properties\n        accordingly. Then build a dict containing references to all the\n        calculated points to be used by the marker glyph inside the\n        ``_yield_renderers`` method.\n        \"\"\"\n        self._data = dict()\n        # list to save all the attributes we are going to create\n        self._attr = []\n        # list to save all the groups available in the incoming input\n        self._groups.extend(self._values.keys())\n        # Grouping\n        self.parse_data()",
  "def parse_data(self):\n        \"\"\"Parse data received from self._values and create correct x, y\n        series values checking if input is a pandas DataFrameGroupBy\n        object or one of the stardard supported types (that can be\n        converted to a DataAdapter)\n        \"\"\"\n        if pd is not None and \\\n                isinstance(self._values, pd.core.groupby.DataFrameGroupBy):\n            return self._parse_groupped_data\n        else:\n            return self._parse_data",
  "def _parse_groupped_data(self):\n        \"\"\"Parse data in self._values in case it's a pandas\n        DataFrameGroupBy and create the data 'x_...' and 'y_...' values\n        for all data series\n        \"\"\"\n        for i, val in enumerate(self._values.keys()):\n            xy = self._values[val]\n            self._set_and_get(\"x_\", val, xy[:, 0])\n            self._set_and_get(\"y_\", val, xy[:, 1])",
  "def _parse_data(self):\n        \"\"\"Parse data in self._values in case it's an iterable (not a pandas\n        DataFrameGroupBy) and create the data 'x_...' and 'y_...' values\n        for all data series\n        \"\"\"\n        for i, val in enumerate(self._values.keys()):\n            x_, y_ = [], []\n            xy = self._values[val]\n            for value in self._values.index:\n                x_.append(xy[value][0])\n                y_.append(xy[value][1])\n\n            self.set_and_get(\"x_\", val, x_)\n            self.set_and_get(\"y_\", val, y_)",
  "def _set_sources(self):\n        \"\"\"Push the Scatter data into the ColumnDataSource and\n        calculate the proper ranges.\"\"\"\n        self._source = ColumnDataSource(self._data)\n\n        x_names, y_names = self._attr[::2], self._attr[1::2]\n        endx = max(max(self._data[i]) for i in x_names)\n        startx = min(min(self._data[i]) for i in x_names)\n        self.x_range = Range1d(\n            start=startx - 0.1 * (endx - startx),\n            end=endx + 0.1 * (endx - startx)\n        )\n        endy = max(max(self._data[i]) for i in y_names)\n        starty = min(min(self._data[i]) for i in y_names)\n        self.y_range = Range1d(\n            start=starty - 0.1 * (endy - starty),\n            end=endy + 0.1 * (endy - starty)\n        )",
  "def _yield_renderers(self):\n        \"\"\"Use the marker glyphs to display the points.\n\n        Takes reference points from data loaded at the ColumnDataSource.\n        \"\"\"\n        duplets = list(chunk(self._attr, 2))\n        colors = cycle_colors(duplets, self.palette)\n\n        for i, duplet in enumerate(duplets, start=1):\n            renderer = make_scatter(\n                self._source, duplet[0], duplet[1], self.marker, colors[i - 1]\n            )\n            self._legends.append((self._groups[i-1], [renderer]))\n            yield renderer",
  "def _adapt_values(self):\n        \"\"\"Prepare context before main show method is invoked.\n\n        Customize show preliminary actions by handling DataFrameGroupBy\n        values in order to create the series values and labels.\"\"\"\n        # check if pandas is installed\n        if pd:\n            # if it is we try to take advantage of it's data structures\n            # asumming we get an groupby object\n            if isinstance(self._values, pd.core.groupby.DataFrameGroupBy):\n                pdict = OrderedDict()\n\n                for i in self._values.groups.keys():\n                    self._labels = self._values.get_group(i).columns\n                    xname = self._values.get_group(i).columns[0]\n                    yname = self._values.get_group(i).columns[1]\n                    x = getattr(self._values.get_group(i), xname)\n                    y = getattr(self._values.get_group(i), yname)\n                    pdict[i] = np.array([x.values, y.values]).T\n\n                self._values = DataAdapter(pdict)\n                self._labels = self._values.keys()\n            else:\n                self._values = DataAdapter(self._values)\n                self._labels = self._values.keys()\n\n        else:\n            self._values = DataAdapter(self._values)\n            self._labels = self._values.keys()",
  "def _loadAR():\n    \"\"\"\n    Utility to load abstract rendering (AR) and checks for version match.\n    Keeps the import from occurring\n    unless you actually try to use AR.  MUST be called before actually\n    calling any AR package items (typically only invoked on the server\n    in response to an AR-reliant plot request).\n\n    This is more complex than just an import because\n    AR is exposed as several backbone modules.  If the AR modules\n    were directly imported, then errors would occur whenever ar_downsample\n    is imported.  This causes error messages on python client side (where\n    AR isn't actually needed) and on the server side even when AR isn't used.\n    Since the AR modules are used throughout this module, just importing at\n    use point inside this module is cumbersome.  Using 'globals()' and\n    importlib allows this method to be called before any AR proper items\n    are used but still have the imports appear at the module level.\n    \"\"\"\n    try:\n        from importlib import import_module\n        globals()[\"ari\"] = import_module(\"abstract_rendering\")\n    except:\n        print(_AR_MESSAGE)\n        raise\n\n    expected = dict(zip([\"major\", \"minor\", \"micro\"], map(int, _AR_VERSION.split(\".\"))))\n    if not hasattr(ari, \"__version_info__\"):\n        raise ImportError(\"Abstract rendering version not found; import aborted.\")\n\n    if (ari.__version_info__[\"major\"] != expected[\"major\"]\n       or ari.__version_info__[\"minor\"] != expected[\"minor\"]\n       or ari.__version_info__[\"micro\"] < expected[\"micro\"]):\n           raise ImportError(\"Abstract rendering version mismatched.\" +\n                             \"Expecting at least {0}, found {1}\".format(_AR_VERSION, ari.__version__))\n\n    try:\n        globals()[\"ar\"] = import_module(\"abstract_rendering.core\")\n        globals()[\"categories\"] = import_module(\"abstract_rendering.categories\")\n        globals()[\"contour\"] = import_module(\"abstract_rendering.contour\")\n        globals()[\"infos\"] = import_module(\"abstract_rendering.infos\")\n        globals()[\"general\"] = import_module(\"abstract_rendering.general\")\n        globals()[\"glyphset\"] = import_module(\"abstract_rendering.glyphset\")\n        globals()[\"npg\"] = import_module(\"abstract_rendering.numpyglyphs\")\n        globals()[\"numeric\"] = import_module(\"abstract_rendering.numeric\")\n        globals()[\"util\"] = import_module(\"abstract_rendering.util\")\n    except:\n        print(_AR_MESSAGE)\n        raise",
  "class Proxy(PlotObject):\n    \"\"\"\n    Proxy objects stand in for the abstract rendering (AR) configuration\n    classes. Basically, the AR implementation doesn't rely on Bokeh, so\n    it doesn't know about the properties BUT the Bokeh needs be able to\n    construct/modify/inspect AR configurations.  Proxy classes hold the\n    relevant parameters for constructing AR classes in a way that Bokeh\n    can inspect. Furthermore, 'reify' produces an AR class from a\n    proxy instance.\n    \"\"\"\n    def reify(self, **kwargs):\n        raise NotImplementedError(\"Unimplemented\")",
  "class Sum(Proxy):\n    \"Add up all incoming values\"\n    def reify(self, **kwargs):\n        return numeric.Sum()",
  "class Count(Proxy):\n    \"Count how many items there are (ignores values)\"\n    def reify(self, **kwargs):\n        glyphs = kwargs.get('glyphset', None)\n        if isinstance(glyphs, npg.Glyphset):\n            return npg.PointCount()\n        else:\n            return numeric.Count()",
  "class CountCategories(Proxy):\n    \"Count how many items there are in provided categories\"\n    def reify(self, **kwargs):\n        glyphs = kwargs.get('glyphset', None)\n        if isinstance(glyphs, npg.Glyphset):\n            return npg.PointCountCategories()\n        else:\n            return categories.CountCategories()",
  "class Const(Proxy):\n    \"Return a single value\"\n    val = Any()\n\n    def reify(self, **kwargs):\n        return infos.const(self.val)",
  "class Encode(Proxy):\n    \"Convert a set of values to numeric codes.\"\n    cats = List(Any)\n    defcat = Int(-1)\n\n    def reify(self, **kwargs):\n        return infos.encode(self.cats, defcat=self.defcat)",
  "class AutoEncode(Proxy):\n    \"Convert a set of values to numeric codes.\"\n\n    def reify(self, **kwargs):\n        return infos.AutoEncode()",
  "class Shader(Proxy):\n    def __add__(self, other):\n        \"Build a sequence of shaders\"\n        return Seq(first=self, second=other)\n\n    def reformat(self, result, *args):\n        \"\"\"\n        Modify the standard AR shader output to one that Bokeh can use.\n        Response to 'None' should produce an empty basic dataset.\n\n        The default behavior for None is [[0]] (most shaders produce images).\n        The default behavior for all other values is identity.\n        \"\"\"\n        return np.array([[0]]) if result is None else result",
  "class Seq(Shader):\n    \"Sequence of shaders\"\n    first = Instance(Shader)\n    second = Instance(Shader)\n\n    def reify(self, **kwargs):\n        return self.first.reify(**kwargs) + self.second.reify(**kwargs)\n\n    @property\n    def out(self):\n        return self.second.out\n\n    def reformat(self, result, *args):\n        return self.second.reformat(result, *args)",
  "class Id(Shader):\n    \"Identity shader.  A safe default.\"\n    out = \"image\"\n\n    def reify(self, **kwargs):\n        return general.Id()",
  "class BinarySegment(Shader):\n    \"\"\"\n    Divide the input space into two regions.\n\n    divider - Value that divides the two regions\n    high - Value for regions equal to or above the divider\n    low - Value for cells below divider\n    \"\"\"\n    out = \"image\"\n    high = Any\n    low = Any\n    divider = Any   # TODO: Restrict to numbers...\n\n    def reify(self, **kwargs):\n        return numeric.BinarySegment(self.low, self.high, self.divider)",
  "class Interpolate(Shader):\n    \"Interpolate between high and low number\"\n    out = \"image\"\n    high = Any     # TODO: Restrict to numbers...\n    low = Any\n\n    def reify(self, **kwargs):\n        return numeric.Interpolate(self.low, self.high)",
  "class ImageShader(Shader):\n    out = \"image_rgb\"\n\n    def _reformatColor(self, color):\n        if isinstance(color, tuple) or isinstance(color, list):\n            parts = len(color)\n            if parts == 3:\n                return tuple(color)+(255,)\n            if parts == 4:\n                return tuple(color[0:3]) + (min(abs(color[3])*255, 255),)\n            raise ValueError(\"Improperly formatted tuple for color %s\" % color)\n\n        if isinstance(color, str) or isinstance(color, unicode):\n            if color[0] == \"#\":\n                color = color.lstrip('#')\n                lv = len(color)\n                rgb = tuple(int(color[i:i + lv // 3], 16) for i in range(0, lv, lv // 3))\n                return [rgb[0], rgb[1], rgb[2], 255]\n            else:\n                try:\n                    rgb = getattr(colors, color).to_rgb()\n                    return [rgb.r, rgb.g, rgb.b, 255]\n                except:\n                    raise ValueError(\"Unknown color string %s\" % color)\n\n    def reformat(self, image, *args):\n        if image is None:\n            return np.array([[0]])\n        else:\n            if not image.flags.contiguous:\n                # TODO: Numpyglyphs *sometimes* return a non-continugous array?\n                image = np.ascontiguousarray(image)\n            return image.view(dtype=np.int32).reshape(image.shape[0:2])",
  "class ToCounts(Shader):\n    \"Convert count-of-categories to just counts\"\n    out = \"image\"\n\n    def reify(self, **kwargs):\n        return categories.ToCounts()",
  "class NonZeros(Shader):\n    \"How many non-zero categories are there?\"\n    out = \"image\"\n\n    def reify(self, **kwargs):\n        return categories.NonZeros()",
  "class Ratio(Shader):\n    \"Ratio of some category to total of all categories.\"\n    out = \"image\"\n    focus = Int(-1)\n\n    def reify(self, **kwargs):\n        return categories.Ratio(focus=self.focus)",
  "class HDAlpha(ImageShader):\n    colors = List(Color)\n    background = Color((255, 255, 255, 0))\n    alphamin = Float(.3)\n    log = Bool(False)\n    base = Int(10)\n\n    def reify(self, **kwargs):\n        colors = map(self._reformatColor, self.colors)\n        return categories.HDAlpha(\n                colors,\n                background=self._reformatColor(self.background),\n                alphamin=self.alphamin,\n                log=self.log,\n                logbase=self.base)",
  "class InterpolateColor(ImageShader):\n    \"\"\"\n    Interpolate between a high and low color\n\n    * high - High color (default: red)\n    * low - Low color (default: white)\n    * reserve - Color for empty values (default: transparent)\n    * empty - Empty value (default: 0)\n\n    \"\"\"\n    # TODO: Make color format conversion fluid\n    #       ...possibly by some change to properties.Color\n    out = \"image_rgb\"\n    high = Color((255, 0, 0, 1))\n    low = Color((255, 255, 255, 1))\n    reserve = Color((255, 255, 255, 0))\n    empty = Any(0)\n\n    def reify(self, **kwargs):\n        return numeric.InterpolateColors(\n            self._reformatColor(self.low),\n            self._reformatColor(self.high),\n            reserve=self._reformatColor(self.reserve),\n            empty=self.empty)",
  "class Sqrt(Shader):\n    \"Square root of all values\"\n    out = \"image\"\n\n    def reify(self, **kwargs):\n        return numeric.Sqrt()",
  "class Cuberoot(Shader):\n    \"Cube root of all values\"\n    out = \"image\"\n\n    def reify(self, **kwargs):\n        return numeric.Cuberoot()",
  "class Log(Shader):\n    \"Log (base 10) of values\"\n    out = \"image\"\n\n    def reify(self, **kwargs):\n        return npg.Log10()",
  "class Spread(Shader):\n    \"\"\"Spread values out in a regular pattern from their origin.\"\"\"\n    # TODO: Add shape parameter\n\n    out = \"image\"\n    factor = Int\n    shape = String(\"circle\")\n    anti_alias = Bool(False)\n\n    def reify(self, **kwargs):\n        return npg.Spread(factor=self.factor,\n                          shape=self.shape,\n                          anti_alias=self.anti_alias)",
  "class Contour(Shader):\n    \"\"\"\n    ISO Contours\n\n    levels -- Either how many ISO contours to make (int)\n              or the exact contour levels (list).\n              Both cases indicate how many contour lines to create.\n    \"\"\"\n    out = \"multi_line\"\n    levels = Either(Int, List(Float), default=5)\n    palette = List(Color)\n\n    def reify(self, **kwargs):\n        return contour.Contour(levels=self.levels, points=False)\n\n    def reformat(self, contours, *args):\n        if contours is None:\n            return {'xxs': [], 'yys': [], 'levels': []}\n\n        xxs = []\n        yys = []\n        levels = sorted(contours.keys())\n        colors = []\n        (xmin, ymin) = args\n\n        # Re-arrange results and project xs/ys back to the data space\n        for (level, color) in zip(levels, self.palette):\n            for trace in contours[level]:\n                (xs, ys) = trace\n                xs = xs+xmin\n                ys = ys+ymin\n                xxs.append(xs)\n                yys.append(ys)\n                colors.append(color)\n\n        return {'levels': levels,\n                'colors': colors,\n                'xs': xxs,\n                'ys': yys}",
  "def replot(plot,\n           agg=Count(), info=Const(val=1), shader=Id(),\n           remove_original=True,\n           plot_opts={}, **kwargs):\n    \"\"\"\n    Treat the passed plot as an base plot for abstract rendering, generate the\n    proper Bokeh plot based on the passed parameters.\n\n    This is a convenience for:\n    > src=source(plot, ...)\n    > <plot-type>(source=src, <params>, **ar.mapping(src))\n\n    Transfers plot title, width, height from the original plot\n\n    plot -- Plot to based new plot off of\n    remove_original -- Remove the source plot from the document (default: true)\n    plot_opts -- Options passed directly to soure.\n                   This parameter only needs to be used if there is a name\n                   conflict bewteen  target plot type and\n                   source options (see kwargs note)\n    **kwargs -- Arugments for the plot or source as needed.\n                If in conflict, a kwarg will be applied to the source function.\n                If this causes incorrecte behavior, the plot arguments may\n                be put in the plot_opts parameter instead.\n    returns -- A new plot\n    \"\"\"\n\n    # Remove the base plot (if requested)\n    if remove_original and plot in curdoc().context.children:\n        curdoc().context.children.remove(plot)\n\n    # Sift kwargs\n    source_opts = {}\n    for key in ServerDataSource().vm_props():\n        if key in kwargs:\n            source_opts[key] = kwargs.pop(key)\n\n    for name in get_function_code(source).co_varnames:\n        if name in kwargs:\n            source_opts[name] = kwargs.pop(name)\n\n    # Transfer specific items from the source plot, then updates from kwargs\n    fig_opts = {}\n    fig_opts['plot_width'] = kwargs.pop('plot_width', plot.plot_width)\n    fig_opts['plot_height'] = kwargs.pop('plot_height', plot.plot_height)\n    fig_opts['title'] = kwargs.pop('title', plot.title)\n\n    src = source(plot, agg, info, shader, **source_opts)\n    plot_opts.update(mapping(src))\n\n    new_plot = figure(**fig_opts)\n    if shader.out == \"image\":\n        new_plot.image(source=src, **plot_opts)\n    elif shader.out == \"image_rgb\":\n        new_plot.image_rgba(source=src, **plot_opts)\n    elif shader.out == \"multi_line\":\n        new_plot.multi_line(source=src, **plot_opts)\n    else:\n        raise ValueError(\"Unhandled output type %s\" % shader.out)\n\n    return new_plot",
  "def _renderer(plot):\n    \"\"\"\n    Acquire the renderer of interest.\n    More specifically, the first child of the plot\n    that is both a renderer and related to a server data source.\n\n    TODO: How to be more specific about what to do AR on?\n          Currently just takes the first renderer with a server data source\n    \"\"\"\n    return [r for r in plot.renderers\n            if (isinstance(r, GlyphRenderer)\n                and isinstance(r.data_source, ServerDataSource))][0]",
  "def get_glyphspec(glyph):\n    spec = glyph.vm_serialize()\n    spec['type'] = glyph.__view_model__\n    return spec",
  "def source(plot,\n           agg=Count(), info=Const(val=1), shader=Id(),\n           points=False, balancedZoom=False, **kwargs):\n    # Acquire information from renderer...\n    rend = _renderer(plot)\n    datasource = rend.data_source\n    kwargs['data_url'] = datasource.data_url\n    kwargs['expr'] = datasource.expr\n    kwargs['namespace'] = datasource.namespace\n    # TODO: Use reformat here?\n    if shader.out == \"image\" or shader.out == \"image_rgb\":\n        kwargs['data'] = {'image': [],\n                          'x': [0],\n                          'y': [0],\n                          'global_x_range': [0, 50],\n                          'global_y_range': [0, 50],\n                          'global_offset_x': [0],\n                          'global_offset_y': [0],\n                          'dw': [1],\n                          'dh': [1],\n                          'render_state': {}}\n    elif shader.out == \"multi_line\":\n        kwargs['data'] = {'xs': [[]],\n                          'ys': [[]],\n                          'colors': [],\n                          'render_state': {}}\n    else:\n        raise ValueError(\"Unrecognized shader output type %s\" % shader.out)\n\n    kwargs['transform'] = {\n        'resample': \"abstract rendering\",\n        'auto_bounds' : True,\n        'agg': agg,\n        'info': info,\n        'shader': shader,\n        'glyphspec': rend.glyph,\n        'balancedZoom': balancedZoom,\n        'points': points}\n\n    return ServerDataSource(**kwargs)",
  "def mapping(source):\n    \"Setup property mapping dictionary from source to output glyph type.\"\n\n    trans = source.transform\n    out = trans['shader'].out\n\n    if out == 'image' or out == 'image_rgb':\n        keys = source.data.keys()\n        m = dict(zip(keys, keys))\n        m['x_range'] = Range1d(start=0, end=0)\n        m['y_range'] = Range1d(start=0, end=0)\n        return m\n    elif out == 'multi_line':\n        keys = source.data.keys()\n        m = dict(zip(keys, keys))\n        m['line_color'] = 'colors'\n        return m\n\n    else:\n        raise ValueError(\"Unrecognized shader output type %s\" % out)",
  "def make_glyphset(xcol, ycol, datacol, glyphspec, transform):\n    # TODO: Do more detection to find if it is an area implantation.\n    #       If so, make a selector with the right shape pattern and use a point shaper\n    shaper = _shaper(glyphspec, transform['points'])\n    if isinstance(shaper, glyphset.ToPoint):\n        points = np.zeros((len(xcol), 4), order=\"F\")\n        points[:, 0] = xcol\n        points[:, 1] = ycol\n        glyphs = npg.Glyphset(points, datacol)\n    else:\n        glyphs = glyphset.Glyphset([xcol, ycol], datacol,\n                                   shaper, colMajor=True)\n    return glyphs",
  "def _generate_render_state(plot_state):\n    data_x_span = float(_span(plot_state['data_x']))\n    data_y_span = float(_span(plot_state['data_y']))\n\n    data_x_span = math.ceil(data_x_span * 100) / 100.0\n    data_y_span = math.ceil(data_y_span * 100) / 100.0\n\n    return {'x_span': data_x_span,\n            'y_span': data_y_span}",
  "def downsample(raw_data, data_source, glyph, plot_state, render_state, auto_bounds):\n    _loadAR()  # Must be called before any attempts to use AR proper\n\n    # XXX: transform['glyphspec'] is really a glyph\n    data = raw_data\n    transform = data_source.transform\n    # the glyph which is passed in is the glyph asking for this computation\n    # the glyph that is stored on the data source is the original glyph that\n    # generated the data source (so if original is square, then new is image_rgba)\n    glyphspec = get_glyphspec(transform['glyphspec'])\n    xcol = glyphspec['x']['field']\n    ycol = glyphspec['y']['field']\n    datacol = _datacolumn(glyphspec)\n\n    if render_state == _generate_render_state(plot_state):\n        logger.info(\"Skipping update; render state unchanged\")\n        return {'render_state': \"NO UPDATE\"}\n\n    # Translate the resample parameters to server-side rendering....\n    # TODO: Do more to preserve the 'natural' data form and have make_glyphset build the 'right thing' (tm)\n    if not isinstance(data, dict):\n        columns = [xcol, ycol] + ([datacol] if datacol else [])\n        data = data[columns]\n\n    xcol = data[xcol]\n    ycol = data[ycol]\n    datacol = data[datacol] if datacol else np.zeros_like(xcol)\n    glyphs = make_glyphset(xcol, ycol, datacol, glyphspec, transform)\n    shader = transform['shader']\n\n    if shader.out == \"image\" or shader.out == \"image_rgb\":\n        rslt = downsample_image(xcol, ycol, glyphs, transform, plot_state, auto_bounds)\n    elif shader.out == \"multi_line\":\n        rslt = downsample_line(xcol, ycol, glyphs, transform, plot_state, auto_bounds)\n    else:\n        raise ValueError(\"Unhandled shader output '{0}.\".format(shader.out))\n\n    rslt['render_state'] = _generate_render_state(plot_state)\n\n    return rslt",
  "def downsample_line(xcol, ycol, glyphs, transform, plot_state, auto_bounds):\n    # todo handle flipped axes (start < end)\n    if auto_bounds:\n        plot_state['data_x'].start = xcol.min()\n        plot_state['data_x'].end = xcol.max()\n        plot_state['data_y'].start = ycol.min()\n        plot_state['data_y'].end = ycol.max()\n    screen_x_span = float(_span(plot_state['screen_x']))\n    screen_y_span = float(_span(plot_state['screen_y']))\n    data_x_span = float(_span(plot_state['data_x']))\n    data_y_span = float(_span(plot_state['data_y']))\n    shader = transform['shader']\n\n    bounds = glyphs.bounds()\n    plot_size = [bounds[2], bounds[3]]\n    balancedZoom = transform.get('balancedZoom', False)\n\n    vt = util.zoom_fit(plot_size, bounds, balanced=balancedZoom)\n\n    lines = ar.render(glyphs,\n                      transform['info'].reify(),\n                      transform['agg'].reify(glyphset=glyphs),\n                      shader.reify(),\n                      plot_size, vt)\n\n    result = {'data' : shader.reformat(lines, xcol.min(), ycol.min())}\n    result['x_range'] = {'start': plot_state['data_x'].start,\n                         'end': plot_state['data_x'].end}\n    result['y_range'] = {'start': plot_state['data_y'].start,\n                         'end': plot_state['data_y'].end}\n    logger.info(\"Finished line-producing downsample\")\n    return result",
  "def downsample_image(xcol, ycol, glyphs, transform, plot_state, auto_bounds):\n    logger.info(\"Starting image-producing downsample\")\n    # todo handle flipped axes (start < end)\n    if auto_bounds:\n        plot_state['data_x'].start = xcol.min()\n        plot_state['data_x'].end = xcol.max()\n        plot_state['data_y'].start = ycol.min()\n        plot_state['data_y'].end = ycol.max()\n    screen_x_span = float(_span(plot_state['screen_x']))\n    screen_y_span = float(_span(plot_state['screen_y']))\n    data_x_span = float(_span(plot_state['data_x']))\n    data_y_span = float(_span(plot_state['data_y']))\n    shader = transform['shader']\n    balanced_zoom = transform.get('balancedZoom', False)\n\n    bounds = glyphs.bounds()\n    scale_x = data_x_span/screen_x_span\n    scale_y = data_y_span/screen_y_span\n    plot_size = (bounds[2]/scale_x, bounds[3]/scale_y)\n\n    vt = util.zoom_fit(plot_size, bounds, balanced=False)\n    (tx, ty, sx, sy) = vt\n\n    image = ar.render(glyphs,\n                      transform['info'].reify(),\n                      transform['agg'].reify(glyphset=glyphs),\n                      shader.reify(),\n                      plot_size, vt)\n\n    image = shader.reformat(image)\n    result = {'data' : {'image': [image],\n                        'x': [plot_state['data_x'].start],\n                        'y': [plot_state['data_y'].start],\n                        'dw': [data_x_span],\n                        'dh': [data_y_span]}}\n    result['x_range'] = {'start': plot_state['data_x'].start,\n                         'end': plot_state['data_x'].end}\n    result['y_range'] = {'start': plot_state['data_y'].start,\n                         'end': plot_state['data_y'].end}\n    logger.info(\"Finished image-producing downsample\")\n    return result",
  "def _datacolumn(glyphspec):\n    \"\"\"Search the glyphspec to determine what the data column is.\n    Returns the column name or False if none was found\n\n    Precedence:\n    Type > Fill Color > Fill Alpha > Line Color > Line Alpha\n\n    TODO: Support a tuple-like datacolumn\n    TODO: Support direct AR override parameter to directly specify data column\n    TODO: Line width?\n    \"\"\"\n\n    def maybe_get(key):\n        return (key in glyphspec\n                and isinstance(glyphspec[key], dict)\n                and glyphspec[key].get('field', False))\n    return (maybe_get('type')\n            or maybe_get('fill_color')\n            or maybe_get('fill_alpha')\n            or maybe_get('line_color')\n            or maybe_get('line_alpha'))",
  "def _span(r):\n    \"\"\"Distance in a Range1D\"\"\"\n    end = r.end if r.end is not None else 0\n    start = r.start if r.start is not None else 0\n    return abs(end-start)",
  "def _shaper(glyphspec, points):\n    \"\"\"Construct the AR shaper to match the given shape code.\"\"\"\n    glyphtype = glyphspec['type']\n\n    tox = glyphset.idx(0)\n    toy = glyphset.idx(1)\n\n    if points:\n        sizer = glyphset.const(1)\n        return glyphset.ToPoint(tox, toy, sizer, sizer)\n\n    # XXX: glyphspec['dataspec'] won't work anymore, because previously all dataspecs\n    # were serialized no matter if dirty or not. Now only changed dataspecs are being\n    # serialized. Use glyphs directly to fix this issue.\n\n    if glyphtype == 'Square':\n        size = glyphspec.get('size', {}).get('value', 1)\n        sizer = glyphset.const(size)\n        return glyphset.ToRect(tox, toy, sizer, sizer)\n    if glyphtype == 'Circle':\n        # XXX: what about 'size' and different units?\n        size = glyphspec.get('radius', {}).get('value', 1)\n        sizer = glyphset.const(size)\n        return glyphset.ToRect(tox, toy, sizer, sizer)\n    else:\n        raise ValueError(\"Unrecogized shape, received '{0}'\".format(glyphtype.lower()))",
  "def hdalpha(plot, cats=None, palette=None, log=True, spread=0, **kwargs):\n    \"\"\"\n    Produce a plot using high-definition alpha composition (HDAlpha).\n    HDAlpha essentially makes a heatmap for each of a number of categories,\n    with different colors for each category.  Then those colors are composed\n    in a way that ensures that oversaturate on does not occur.\n    This is a convenience function that encodes a common configuration,\n    and parameters to support the most common variations.\n\n    plot -- Plot to convert into an HDAlpha plot\n    cats -- What are the expected categories?\n                 Default is None, indicating that categories\n                 should be determined automatically.\n    palette -- What colors should be used. Colors are matched to categories in\n              order, so the first color is associated with the first category.\n              Default is a rainbow palette with 8 steps.\n    spread -- How far (if any) should values be spread. Default is 0.\n    long -- Log-transform each category prior to composing? Default is True.\n    kwargs -- Further arguments passed on to replot for greater control.\n\n    Note: Category to color association follows the rules of the HDAlpha\n         operator.  Refer to that documentation for corner case resolution.\n    \"\"\"\n\n    if cats is None:\n        info = AutoEncode()\n    else:\n        info = Encode(cats=cats)\n\n    if palette is None:\n        palette = [\"#e41a1c\", \"#377eb8\", \"#4daf4a\", \"#984ea3\", \"#ff7f00\", \"#ffff33\", \"#a65628\", \"#f781bf\"]\n\n    kwargs['points'] = kwargs.get('points', True)\n\n    return replot(plot,\n                  info=info,\n                  agg=CountCategories(),\n                  shader=Spread(factor=spread) + HDAlpha(colors=palette, log=log),\n                  **kwargs)",
  "def heatmap(plot,\n            client_color=False,\n            low=(255, 200, 200), high=(255, 0, 0),\n            spread=0, transform=\"cbrt\", **kwargs):\n    \"\"\"\n    Produce a heatmap from a set of shapes.\n    A heatmap is a scale of how often a single thing occurs.\n    This is a convenience function that encodes a common configuration,\n    and parameters to support the most common variations.\n\n\n    plot -- Plot to convert into a heatmap\n    low -- Low color of the heatmap.  Default is a light red\n    high -- High color of the heatmap. Default is full saturation red.\n    spread -- How far (if any) should values be spread. Default is 0.\n    transform -- Apply a transformation before building a color ramp?\n                 Understood values are 'cbrt', 'log', 'none' and None.\n                 The default is 'cbrt', for cuberoot, an approximation of\n                 perceptual correction on monochromatic scales.\n    kwargs -- Further arguments passed on to replot for greater control.\n    \"\"\"\n\n    transform = transform.lower() if transform is not None else None\n\n    if client_color:\n        shader = Id()\n        kwargs['reserve_val'] = kwargs.get('reserve_val', 0)\n    else:\n        shader = InterpolateColor(low=low, high=high)\n\n    if transform == \"cbrt\":\n        shader = Cuberoot() + shader\n    elif transform == \"log\":\n        shader = Log() + shader\n    elif transform == \"none\" or transform is None:\n        pass\n    else:\n        raise ValueError(\"Unrecognized transform '{0}'\".format(transform))\n\n    if spread > 0:\n        shader = Spread(factor=spread, shape=\"circle\") + shader\n\n    kwargs['points'] = kwargs.get('points', True)\n\n    return replot(plot,\n                  agg=Count(),\n                  info=Const(val=1),\n                  shader=shader,\n                  **kwargs)",
  "def contours(plot, palette=None, transform='cbrt', spread=0, **kwargs):\n    \"\"\"\n    Create ISO contours from a given plot\n\n    plot -- Plot to convert into iso contours\n    spread -- How far (if any) should values be spread. Default is 0.\n    palette -- What should the line colors be?\n               The number of lines is determined by the number of colors.\n    transform -- Apply a transformation before building the iso contours?\n                 Understood values are 'cbrt', 'log', 'none' and None.\n                 The default is 'cbrt', for cuberoot, an approximation of\n                 perceptual correction for monochromatic scales.\n    kwargs -- Further arguments passed on to replot for greater control.\n    \"\"\"\n    if palette is None:\n        palette = [\"#C6DBEF\", \"#9ECAE1\", \"#6BAED6\", \"#4292C6\", \"#2171B5\", \"#08519C\", \"#08306B\"]\n\n    shader = Contour(levels=len(palette), palette=palette)\n\n    if transform == \"cbrt\":\n        shader = Cuberoot() + shader\n    elif transform == \"log\":\n        shader = Log() + shader\n    elif transform == \"none\" or transform is None:\n        pass\n    else:\n        raise ValueError(\"Unrecognized transform '{0}'\".format(transform))\n\n    if spread > 0:\n        shader = Spread(factor=spread, shape=\"circle\") + shader\n\n    return replot(plot,\n                  agg=Count(),\n                  info=Const(val=1),\n                  shader=shader,\n                  **kwargs)",
  "def reify(self, **kwargs):\n        raise NotImplementedError(\"Unimplemented\")",
  "def reify(self, **kwargs):\n        return numeric.Sum()",
  "def reify(self, **kwargs):\n        glyphs = kwargs.get('glyphset', None)\n        if isinstance(glyphs, npg.Glyphset):\n            return npg.PointCount()\n        else:\n            return numeric.Count()",
  "def reify(self, **kwargs):\n        glyphs = kwargs.get('glyphset', None)\n        if isinstance(glyphs, npg.Glyphset):\n            return npg.PointCountCategories()\n        else:\n            return categories.CountCategories()",
  "def reify(self, **kwargs):\n        return infos.const(self.val)",
  "def reify(self, **kwargs):\n        return infos.encode(self.cats, defcat=self.defcat)",
  "def reify(self, **kwargs):\n        return infos.AutoEncode()",
  "def __add__(self, other):\n        \"Build a sequence of shaders\"\n        return Seq(first=self, second=other)",
  "def reformat(self, result, *args):\n        \"\"\"\n        Modify the standard AR shader output to one that Bokeh can use.\n        Response to 'None' should produce an empty basic dataset.\n\n        The default behavior for None is [[0]] (most shaders produce images).\n        The default behavior for all other values is identity.\n        \"\"\"\n        return np.array([[0]]) if result is None else result",
  "def reify(self, **kwargs):\n        return self.first.reify(**kwargs) + self.second.reify(**kwargs)",
  "def out(self):\n        return self.second.out",
  "def reformat(self, result, *args):\n        return self.second.reformat(result, *args)",
  "def reify(self, **kwargs):\n        return general.Id()",
  "def reify(self, **kwargs):\n        return numeric.BinarySegment(self.low, self.high, self.divider)",
  "def reify(self, **kwargs):\n        return numeric.Interpolate(self.low, self.high)",
  "def _reformatColor(self, color):\n        if isinstance(color, tuple) or isinstance(color, list):\n            parts = len(color)\n            if parts == 3:\n                return tuple(color)+(255,)\n            if parts == 4:\n                return tuple(color[0:3]) + (min(abs(color[3])*255, 255),)\n            raise ValueError(\"Improperly formatted tuple for color %s\" % color)\n\n        if isinstance(color, str) or isinstance(color, unicode):\n            if color[0] == \"#\":\n                color = color.lstrip('#')\n                lv = len(color)\n                rgb = tuple(int(color[i:i + lv // 3], 16) for i in range(0, lv, lv // 3))\n                return [rgb[0], rgb[1], rgb[2], 255]\n            else:\n                try:\n                    rgb = getattr(colors, color).to_rgb()\n                    return [rgb.r, rgb.g, rgb.b, 255]\n                except:\n                    raise ValueError(\"Unknown color string %s\" % color)",
  "def reformat(self, image, *args):\n        if image is None:\n            return np.array([[0]])\n        else:\n            if not image.flags.contiguous:\n                # TODO: Numpyglyphs *sometimes* return a non-continugous array?\n                image = np.ascontiguousarray(image)\n            return image.view(dtype=np.int32).reshape(image.shape[0:2])",
  "def reify(self, **kwargs):\n        return categories.ToCounts()",
  "def reify(self, **kwargs):\n        return categories.NonZeros()",
  "def reify(self, **kwargs):\n        return categories.Ratio(focus=self.focus)",
  "def reify(self, **kwargs):\n        colors = map(self._reformatColor, self.colors)\n        return categories.HDAlpha(\n                colors,\n                background=self._reformatColor(self.background),\n                alphamin=self.alphamin,\n                log=self.log,\n                logbase=self.base)",
  "def reify(self, **kwargs):\n        return numeric.InterpolateColors(\n            self._reformatColor(self.low),\n            self._reformatColor(self.high),\n            reserve=self._reformatColor(self.reserve),\n            empty=self.empty)",
  "def reify(self, **kwargs):\n        return numeric.Sqrt()",
  "def reify(self, **kwargs):\n        return numeric.Cuberoot()",
  "def reify(self, **kwargs):\n        return npg.Log10()",
  "def reify(self, **kwargs):\n        return npg.Spread(factor=self.factor,\n                          shape=self.shape,\n                          anti_alias=self.anti_alias)",
  "def reify(self, **kwargs):\n        return contour.Contour(levels=self.levels, points=False)",
  "def reformat(self, contours, *args):\n        if contours is None:\n            return {'xxs': [], 'yys': [], 'levels': []}\n\n        xxs = []\n        yys = []\n        levels = sorted(contours.keys())\n        colors = []\n        (xmin, ymin) = args\n\n        # Re-arrange results and project xs/ys back to the data space\n        for (level, color) in zip(levels, self.palette):\n            for trace in contours[level]:\n                (xs, ys) = trace\n                xs = xs+xmin\n                ys = ys+ymin\n                xxs.append(xs)\n                yys.append(ys)\n                colors.append(color)\n\n        return {'levels': levels,\n                'colors': colors,\n                'xs': xxs,\n                'ys': yys}",
  "def maybe_get(key):\n        return (key in glyphspec\n                and isinstance(glyphspec[key], dict)\n                and glyphspec[key].get('field', False))",
  "def source(**kwargs):\n  kwargs['transform'] = {'resample':'heatmap',\n                         'global_x_range' : [0, 10],\n                         'global_y_range' : [0, 10],\n                         'global_offset_x' : [0],\n                         'global_offset_y' : [0],\n                         'type' : 'ndarray'\n\n  }\n  kwargs['data'] = {'x': [0],\n                    'y': [0],\n                    'dw' : [10],\n                    'dh' : [10],\n                   }\n  return ServerDataSource(**kwargs)",
  "def downsample(image, image_x_axis, image_y_axis,\n               x_bounds, y_bounds, x_resolution, y_resolution):\n    x_resolution, y_resolution = int(round(x_resolution)), int(round(y_resolution))\n    x_bounds = [x_bounds.start, x_bounds.end]\n    y_bounds = [y_bounds.start, y_bounds.end]\n    x_bounds = np.searchsorted(image_x_axis, x_bounds)\n    y_bounds = np.searchsorted(image_y_axis, y_bounds)\n    #y_bounds = image.shape[0] + 1 - y_bounds[::-1]\n\n    if x_resolution == 0 or y_resolution == 0:\n        subset = np.zeros((1,1), dtype=image.dtype)\n    else:\n        subset = image[y_bounds[0]:y_bounds[1],\n                       x_bounds[0]:x_bounds[1]]\n        x_downsample_factor = max(round(subset.shape[1] / x_resolution / 3.), 1)\n        y_downsample_factor = max(round(subset.shape[0] / y_resolution / 3.), 1)\n        subset = subset[::x_downsample_factor, ::y_downsample_factor]\n        subset = scipy.misc.imresize(subset, (x_resolution, y_resolution),\n                                     interp='nearest')\n\n    bounds = image_x_axis[x_bounds[0]:x_bounds[1]]\n    dw = np.max(bounds) - np.min(bounds)\n    bounds = image_y_axis[y_bounds[0]:y_bounds[1]]\n    dh = np.max(bounds) - np.min(bounds)\n    return {'data' : {'image': [subset],\n                      'x': [image_x_axis[x_bounds[0]]],\n                      'y': [image_y_axis[y_bounds[0]]],\n                      'dw': [dw],\n                      'dh': [dh],\n                  }\n        }",
  "def source(direction='x', method='minmax', auto_bounds=True, **kwargs):\n    if direction != 'x':\n        warnings.warn(\"other directions besides x not implemented yet\")\n        raise NotImplementedError\n    kwargs['transform'] = {'resample': 'line1d',\n                           'direction': direction,\n                           'auto_bounds' : auto_bounds,\n                           'method': method}\n    return ServerDataSource(**kwargs)",
  "def downsample(data,\n               domain_column,\n               primary_data_column,\n               domain_limit,\n               range_limit,\n               domain_resolution,\n               method):\n    \"\"\"\n    data : record numpy array of values, shape (N,)\n    domain_column : column index representing the domain of the plot\n    primary_data_column : column index of data that is used for\n    min/max decimation\n    domain_limit : bounds of domain (tuple of length 2)\n    domain_resolution : # of samples\n    method : 'maxmin' encodes the max/min point.\n             'mid' encode the midpoint of each bin-range\n    output:\n    list of (domain,data) pairs, where they are each 1d vectors\n    \"\"\"\n    # sort data\n    indexes = np.argsort(data[domain_column])\n    data = data[indexes]\n\n    # truncate data based on domain_limits\n    domain = data[domain_column]\n    left_idx = np.searchsorted(domain, domain_limit[0], side='left')\n    right_idx = np.searchsorted(domain, domain_limit[1], side='right')\n    data = data[left_idx:right_idx + 1]\n\n    domain = data[domain_column]\n    bucket_size = (domain_limit[1] - domain_limit[0]) / domain_resolution\n    buckets = (domain - domain.min()) / bucket_size\n    buckets = np.floor(buckets)\n    starting_boundaries = np.searchsorted(buckets,\n                                          np.unique(buckets),\n                                          side='left').tolist()\n    ending_boundaries = starting_boundaries[1:]\n    ending_boundaries.append(None)\n    if len(starting_boundaries) * 2 > len(data):\n        # So little data, don't bother downsampling it\n        downsampled_data = data\n    else:\n        downsampled_data = []\n        for st, ed in zip(starting_boundaries, ending_boundaries):\n            subdata = data[st:ed]\n            if subdata.shape[0] == 0:\n                continue\n\n            primary_column = subdata[primary_data_column]\n            idx = np.argsort(primary_column)\n            # downsample\n            if method == 'minmax':\n                min_idx = idx[0]\n                max_idx = idx[-1]\n                subdata = subdata[[min_idx, max_idx]]\n            elif method == 'mid':\n                mid_idx = idx[len(idx)/2]\n                subdata = subdata[[mid_idx]]\n            else:\n                raise ValueError(\"Line downsample method not known: \" + method)\n            downsampled_data.append(subdata)\n\n        downsampled_data = np.concatenate(downsampled_data)\n\n    # resort data\n    indexes = np.argsort(downsampled_data[domain_column])\n    downsampled_data = downsampled_data[indexes]\n\n    columns = dict([(k, downsampled_data[k]) \\\n                    for k in downsampled_data.dtype.names])\n    result = {\n        'data': columns,\n    }\n    return result",
  "class bokeh_plot(nodes.General, nodes.Element):\n    pass",
  "def _source_position(argument):\n    return choice(argument, ('below', 'above', 'none'))",
  "class BokehPlotDirective(Directive):\n\n    has_content = True\n    optional_arguments = 2\n\n    option_spec = {\n        'source-position' : _source_position,\n        'linenos'         : flag,\n        'emphasize-lines' : unchanged,\n    }\n\n    def run(self):\n        # filename *or* python code content, but not both\n        if self.arguments and self.content:\n            raise RuntimeError(\"bokeh-plot:: directive can't have both args and content\")\n\n        env = self.state.document.settings.env\n        app = env.app\n\n        if not hasattr(env, 'bokeh_plot_tmpdir'):\n            env.bokeh_plot_tmpdir = mkdtemp()\n            app.verbose(\"creating new temp dir for bokeh-plot cache: %s\" % env.bokeh_plot_tmpdir)\n        else:\n            tmpdir = env.bokeh_plot_tmpdir\n            if not exists(tmpdir) or not isdir(tmpdir):\n                app.verbose(\"creating new temp dir for bokeh-plot cache: %s\" % env.bokeh_plot_tmpdir)\n                env.bokeh_plot_tmpdir = mkdtemp()\n            else:\n                app.verbose(\"using existing temp dir for bokeh-plot cache: %s\" % env.bokeh_plot_tmpdir)\n\n        # TODO (bev) verify that this is always the correct thing\n        rst_source = self.state_machine.node.document['source']\n        rst_dir = dirname(rst_source)\n        rst_filename = basename(rst_source)\n\n        target_id = \"%s.bokeh-plot-%d\" % (rst_filename, env.new_serialno('bokeh-plot'))\n        target_node = nodes.target('', '', ids=[target_id])\n        result = [target_node]\n\n        try:\n            source = self._get_source()\n        except Exception:\n            node = nodes.error(None,\n                               nodes.paragraph(text=\"Unable to generate Bokeh plot at %s:%d:\" % (basename(rst_source), self.lineno)),\n                               nodes.paragraph(text=str(sys.exc_info()[1])))\n            return [node]\n\n        source_position = self.options.get('source-position', 'below')\n\n        if source_position == 'above':\n            result += self._get_source_nodes(source)\n\n        node = bokeh_plot()\n        node['target_id'] = target_id\n        node['source'] = source\n        node['relpath'] = relpath(rst_dir, env.srcdir)\n        node['rst_source'] = rst_source\n        node['rst_lineno'] = self.lineno\n        if 'alt' in self.options:\n            node['alt'] = self.options['alt']\n        if self.arguments:\n            node['path'] = self.arguments[0]\n            env.note_dependency(node['path'])\n        if len(self.arguments) == 2:\n            node['symbol'] = self.arguments[1]\n        result += [node]\n\n        if source_position == 'below':\n            result += self._get_source_nodes(source)\n\n        return result\n\n    def _get_source(self):\n        if self.arguments:\n            source = open(self.arguments[0], \"r\").read()\n            source = decode_utf8(source)\n        else:\n            source = u\"\"\n            for line in self.content:\n                source += \"%s\\n\" % line\n        return source\n\n    def _get_source_nodes(self, source):\n        linenos = 'linenos' in self.options\n        emphasize_lines = self.options.get('emphasize-lines', False)\n        if emphasize_lines: linenos = True\n        result = ViewList()\n        text = SOURCE_TEMPLATE.render(source=source, linenos=linenos, emphasize_lines=emphasize_lines)\n        for line in text.split(\"\\n\"):\n            result.append(line, \"<bokeh-plot>\")\n        node = nodes.paragraph()\n        node.document = self.state.document\n        self.state.nested_parse(result, 0, node)\n        return node.children",
  "def _noop(*args, **kwargs):\n    pass",
  "def _show(obj=None):\n    if obj:\n        plotting._obj = obj",
  "def _render_plot(source, symbol):\n    plotting._default_document = Document()\n    namespace = {}\n    # need to remove any encoding comment before compiling unicode\n    pat = re.compile(r\"^# -\\*- coding: (.*) -\\*-$\", re.M)\n    source = pat.sub(\"\", source)\n    code = compile(source, \"<string>\", mode=\"exec\")\n    eval(code, namespace)\n    # TODO (bev) remove this crap\n    if symbol is not None:\n        if 'bokeh.charts' in source:\n            obj = namespace[symbol].chart.plot\n        else:\n            obj = namespace[symbol]\n    else:\n        obj = plotting._obj\n    return obj",
  "def html_visit_bokeh_plot(self, node):\n    env = self.builder.env\n    dest_dir = join(self.builder.outdir, node[\"relpath\"])\n\n    try:\n        if node.has_key('path'):\n            path = node['path']\n            filename = \"bokeh-plot-%s.js\" %  hashlib.md5(path.encode('utf-8')).hexdigest()\n            dest_path = join(dest_dir, filename)\n            tmpdir = join(env.bokeh_plot_tmpdir, node[\"relpath\"])\n            if not exists(tmpdir): makedirs(tmpdir)\n            cached_path = join(tmpdir, filename)\n\n            if out_of_date(path, cached_path) or not exists(cached_path+\".script\"):\n                self.builder.app.verbose(\"generating new plot for '%s'\" % path)\n                plot = _render_plot(node['source'], node.get('symbol'))\n                js, script = autoload_static(plot, CDN, filename)\n                with open(cached_path, \"w\") as f:\n                    f.write(js)\n                with open(cached_path+\".script\", \"w\") as f:\n                    f.write(script)\n            else:\n                self.builder.app.verbose(\"using cached plot for '%s'\" % path)\n                script = open(cached_path+\".script\", \"r\").read()\n\n            if not exists(dest_dir): makedirs(dest_dir)\n            copy(cached_path, dest_path)\n        else:\n            filename = node['target_id'] + \".js\"\n            dest_path = join(dest_dir, filename)\n            plot = _render_plot(node['source'], None)\n            js, script = autoload_static(plot, CDN, filename)\n            self.builder.app.verbose(\"saving inline plot at: %s\" % dest_path)\n            with open(dest_path, \"w\") as f:\n                f.write(js)\n\n        html = SCRIPT_TEMPLATE.render(script=script)\n        self.body.append(html)\n    except Exception:\n        err_node = nodes.error(None,\n                               nodes.paragraph(text=\"Unable to generate Bokeh plot at %s:%d:\" % (node['rst_source'], node['rst_lineno'])),\n                               nodes.paragraph(text=str(sys.exc_info()[1])))\n        node.children.append(err_node)\n        raise nodes.SkipDeparture\n    else:\n        raise nodes.SkipNode",
  "def latex_visit_bokeh_plot(self, node):\n    if 'alt' in node.attributes:\n        self.body.append(_('[graph: %s]') % node['alt'])\n    else:\n        self.body.append(_('[graph]'))\n    raise nodes.SkipNode",
  "def texinfo_visit_bokeh_plot(self, node):\n    if 'alt' in node.attributes:\n        self.body.append(_('[graph: %s]') % node['alt'])\n    else:\n        self.body.append(_('[graph]'))\n    raise nodes.SkipNode",
  "def text_visit_bokeh_plot(self, node):\n    if 'alt' in node.attributes:\n        self.add_text(_('[graph: %s]') % node['alt'])\n    else:\n        self.add_text(_('[graph]'))\n    raise nodes.SkipNode",
  "def man_visit_bokeh_plot(self, node):\n    if 'alt' in node.attributes:\n        self.body.append(_('[graph: %s]') % node['alt'])\n    else:\n        self.body.append(_('[graph]'))\n    raise nodes.SkipNode",
  "def setup(app):\n    app.add_node(bokeh_plot,\n                 html=(html_visit_bokeh_plot, None),\n                 latex=(latex_visit_bokeh_plot, None),\n                 texinfo=(texinfo_visit_bokeh_plot, None),\n                 text=(text_visit_bokeh_plot, None),\n                 man=(man_visit_bokeh_plot, None))\n    app.add_directive('bokeh-plot', BokehPlotDirective)",
  "def run(self):\n        # filename *or* python code content, but not both\n        if self.arguments and self.content:\n            raise RuntimeError(\"bokeh-plot:: directive can't have both args and content\")\n\n        env = self.state.document.settings.env\n        app = env.app\n\n        if not hasattr(env, 'bokeh_plot_tmpdir'):\n            env.bokeh_plot_tmpdir = mkdtemp()\n            app.verbose(\"creating new temp dir for bokeh-plot cache: %s\" % env.bokeh_plot_tmpdir)\n        else:\n            tmpdir = env.bokeh_plot_tmpdir\n            if not exists(tmpdir) or not isdir(tmpdir):\n                app.verbose(\"creating new temp dir for bokeh-plot cache: %s\" % env.bokeh_plot_tmpdir)\n                env.bokeh_plot_tmpdir = mkdtemp()\n            else:\n                app.verbose(\"using existing temp dir for bokeh-plot cache: %s\" % env.bokeh_plot_tmpdir)\n\n        # TODO (bev) verify that this is always the correct thing\n        rst_source = self.state_machine.node.document['source']\n        rst_dir = dirname(rst_source)\n        rst_filename = basename(rst_source)\n\n        target_id = \"%s.bokeh-plot-%d\" % (rst_filename, env.new_serialno('bokeh-plot'))\n        target_node = nodes.target('', '', ids=[target_id])\n        result = [target_node]\n\n        try:\n            source = self._get_source()\n        except Exception:\n            node = nodes.error(None,\n                               nodes.paragraph(text=\"Unable to generate Bokeh plot at %s:%d:\" % (basename(rst_source), self.lineno)),\n                               nodes.paragraph(text=str(sys.exc_info()[1])))\n            return [node]\n\n        source_position = self.options.get('source-position', 'below')\n\n        if source_position == 'above':\n            result += self._get_source_nodes(source)\n\n        node = bokeh_plot()\n        node['target_id'] = target_id\n        node['source'] = source\n        node['relpath'] = relpath(rst_dir, env.srcdir)\n        node['rst_source'] = rst_source\n        node['rst_lineno'] = self.lineno\n        if 'alt' in self.options:\n            node['alt'] = self.options['alt']\n        if self.arguments:\n            node['path'] = self.arguments[0]\n            env.note_dependency(node['path'])\n        if len(self.arguments) == 2:\n            node['symbol'] = self.arguments[1]\n        result += [node]\n\n        if source_position == 'below':\n            result += self._get_source_nodes(source)\n\n        return result",
  "def _get_source(self):\n        if self.arguments:\n            source = open(self.arguments[0], \"r\").read()\n            source = decode_utf8(source)\n        else:\n            source = u\"\"\n            for line in self.content:\n                source += \"%s\\n\" % line\n        return source",
  "def _get_source_nodes(self, source):\n        linenos = 'linenos' in self.options\n        emphasize_lines = self.options.get('emphasize-lines', False)\n        if emphasize_lines: linenos = True\n        result = ViewList()\n        text = SOURCE_TEMPLATE.render(source=source, linenos=linenos, emphasize_lines=emphasize_lines)\n        for line in text.split(\"\\n\"):\n            result.append(line, \"<bokeh-plot>\")\n        node = nodes.paragraph()\n        node.document = self.state.document\n        self.state.nested_parse(result, 0, node)\n        return node.children",
  "def out_of_date(original, derived):\n    \"\"\" Test whether a derived file is newer than its original.\n\n    Args:\n        original (str) : full path to original file\n        derived (str) : full path to derived file\n\n    Returns:\n        bool :\n            True if original is newer or derived does not\n            exist, False otherwise\n\n    Raises:\n        RuntimeError : if original does not exists\n\n    \"\"\"\n    if not exists(original):\n        raise RuntimeError()\n\n    if not exists(derived):\n        return True\n\n    return os.stat(derived).st_mtime < os.stat(original).st_mtime",
  "class BokehModelDirective(Directive):\n\n    has_content = True\n    required_arguments = 1\n\n    def run(self):\n\n        model_path = self.arguments[0]\n        module_name, model_name = model_path.rsplit(\".\", 1)\n\n        try:\n            module = importlib.import_module(module_name)\n        except ImportError:\n            pass\n\n        model = getattr(module, model_name, None)\n        if model is None:\n            pass\n\n        if type(model) != Viewable:\n            pass\n\n        model_obj = model()\n\n        model_json = json.dumps(\n            json.loads(serialize_json(model_obj.dump(changed_only=False))),\n            sort_keys=True,\n            indent=2,\n            separators=(',', ': ')\n        )\n\n        rst_text = MODEL_TEMPLATE.render(\n            model_path=model_path,\n            model_json=model_json,\n        )\n\n        result = ViewList()\n        for line in rst_text.split(\"\\n\"):\n            result.append(line, \"<bokeh-model>\")\n        node = nodes.paragraph()\n        node.document = self.state.document\n        nested_parse_with_titles(self.state, result, node)\n        return node.children",
  "def setup(app):\n    app.add_directive_to_domain('py', 'bokeh-model', BokehModelDirective)",
  "def run(self):\n\n        model_path = self.arguments[0]\n        module_name, model_name = model_path.rsplit(\".\", 1)\n\n        try:\n            module = importlib.import_module(module_name)\n        except ImportError:\n            pass\n\n        model = getattr(module, model_name, None)\n        if model is None:\n            pass\n\n        if type(model) != Viewable:\n            pass\n\n        model_obj = model()\n\n        model_json = json.dumps(\n            json.loads(serialize_json(model_obj.dump(changed_only=False))),\n            sort_keys=True,\n            indent=2,\n            separators=(',', ': ')\n        )\n\n        rst_text = MODEL_TEMPLATE.render(\n            model_path=model_path,\n            model_json=model_json,\n        )\n\n        result = ViewList()\n        for line in rst_text.split(\"\\n\"):\n            result.append(line, \"<bokeh-model>\")\n        node = nodes.paragraph()\n        node.document = self.state.document\n        nested_parse_with_titles(self.state, result, node)\n        return node.children",
  "class bokeh_palette(nodes.General, nodes.Element):\n    pass",
  "class BokehPaletteDirective(Directive):\n\n    has_content = False\n    required_arguments = 1\n\n    def run(self):\n        node = bokeh_palette()\n        node['module'] = self.arguments[0]\n        return [node]",
  "def html_visit_bokeh_palette(self, node):\n    # NOTE: currently only handles the existing Brewer palettes\n    names = sorted(brewer)\n    for name in names:\n        palette = brewer[name]\n        numbers = sorted(palette)\n\n        html = BREWER_TEMPLATE.render(name=name, numbers=numbers, palette=palette)\n        self.body.append(html)\n    raise nodes.SkipNode",
  "def latex_visit_bokeh_palette(self, node):\n    self.body.append(_('[palette: %s]' % node['module']))\n    raise nodes.SkipNode",
  "def texinfo_visit_bokeh_palette(self, node):\n    self.body.append(_('[palette: %s]' % node['module']))\n    raise nodes.SkipNode",
  "def text_visit_bokeh_palette(self, node):\n    self.body.append(_('[palette: %s]' % node['module']))\n    raise nodes.SkipNode",
  "def man_visit_bokeh_palette(self, node):\n    self.body.append(_('[palette: %s]' % node['module']))\n    raise nodes.SkipNode",
  "def setup(app):\n    app.add_node(bokeh_palette,\n                 html=(html_visit_bokeh_palette, None),\n                 latex=(latex_visit_bokeh_palette, None),\n                 texinfo=(texinfo_visit_bokeh_palette, None),\n                 text=(text_visit_bokeh_palette, None),\n                 man=(man_visit_bokeh_palette, None))\n    app.add_directive('bokeh-palette', BokehPaletteDirective)",
  "def run(self):\n        node = bokeh_palette()\n        node['module'] = self.arguments[0]\n        return [node]",
  "class collapsible_code_block(nodes.General, nodes.Element):\n    pass",
  "class CollapsibleCodeBlock(CodeBlock):\n\n    option_spec = CodeBlock.option_spec\n    option_spec.update(heading=unchanged)\n\n    def run(self):\n        env = self.state.document.settings.env\n\n        rst_source = self.state_machine.node.document['source']\n        rst_dir = dirname(rst_source)\n        rst_filename = basename(rst_source)\n\n        target_id = \"%s.ccb-%d\" % (rst_filename, env.new_serialno('bokeh-plot'))\n        target_id = target_id.replace(\".\", \"-\")\n        target_node = nodes.target('', '', ids=[target_id])\n\n        node = collapsible_code_block()\n        node['target_id'] = target_id\n        node['heading'] = self.options.get('heading', \"Code\")\n\n        cb = CodeBlock.run(self)\n        node.setup_child(cb[0])\n        node.children.append(cb[0])\n\n        return [target_node, node]",
  "def html_visit_collapsible_code_block(self, node):\n    self.body.append(\n        PROLOGUE_TEMPLATE.render(\n            id=node['target_id'],\n            heading=node['heading']\n        )\n    )",
  "def html_depart_collapsible_code_block(self, node):\n    self.body.append(EPILOGUE_TEMPLATE.render())",
  "def setup(app):\n    app.add_node(\n        collapsible_code_block,\n        html=(\n            html_visit_collapsible_code_block,\n            html_depart_collapsible_code_block\n        )\n    )\n    app.add_directive('collapsible-code-block', CollapsibleCodeBlock)",
  "def run(self):\n        env = self.state.document.settings.env\n\n        rst_source = self.state_machine.node.document['source']\n        rst_dir = dirname(rst_source)\n        rst_filename = basename(rst_source)\n\n        target_id = \"%s.ccb-%d\" % (rst_filename, env.new_serialno('bokeh-plot'))\n        target_id = target_id.replace(\".\", \"-\")\n        target_node = nodes.target('', '', ids=[target_id])\n\n        node = collapsible_code_block()\n        node['target_id'] = target_id\n        node['heading'] = self.options.get('heading', \"Code\")\n\n        cb = CodeBlock.run(self)\n        node.setup_child(cb[0])\n        node.children.append(cb[0])\n\n        return [target_node, node]",
  "class BokehGalleryDirective(Directive):\n\n    has_content = True\n    required_arguments = 1\n\n    option_spec = {\n        'source-position' : unchanged\n    }\n\n    def run(self):\n\n        env = self.state.document.settings.env\n        app = env.app\n\n        env.note_reread()\n\n        dest_dir = join(dirname(self.state_machine.node.source), \"gallery\")\n        if not exists (dest_dir): makedirs(dest_dir)\n\n        target_id = \"bokeh-plot-%d\" % env.new_serialno('bokeh-plot')\n        target_node = nodes.target('', '', ids=[target_id])\n        result = [target_node]\n\n        source_position = self.options.get('source-position', 'below')\n\n        spec = json.load(open(self.arguments[0]))\n\n        details = spec['details']\n\n        for i, detail in enumerate(details):\n            path = detail['path']\n            name = detail['name']\n            prev_ref, next_ref = None, None\n            if i > 0:\n                prev_ref = \"gallery_\" + details[i-1]['name']\n            if i < len(details)-1:\n                next_ref = \"gallery_\" + details[i+1]['name']\n            rst = DETAIL_TEMPLATE.render(\n                name=name,\n                underline=\"#\"*len(name),\n                path=abspath(\"../\" + path),\n                symbol=detail.get('symbol'),\n                prev_ref=prev_ref,\n                up_ref=\"gallery\",\n                next_ref=next_ref,\n                source_position=source_position,\n            )\n            with open(join(dest_dir, \"%s.rst\" % name), \"w\") as f:\n                f.write(rst)\n            env.read_doc(join(\"docs\", \"gallery\", name), app=app)\n\n        result = ViewList()\n        names = [detail['name'] for detail in details]\n        text = GALLERY_TEMPLATE.render(names=names)\n        for line in text.split(\"\\n\"):\n            result.append(line, \"<bokeh-gallery>\")\n        node = nodes.paragraph()\n        node.document = self.state.document\n        self.state.nested_parse(result, 0, node)\n        return node.children",
  "def setup(app):\n    app.add_directive('bokeh-gallery', BokehGalleryDirective)",
  "def run(self):\n\n        env = self.state.document.settings.env\n        app = env.app\n\n        env.note_reread()\n\n        dest_dir = join(dirname(self.state_machine.node.source), \"gallery\")\n        if not exists (dest_dir): makedirs(dest_dir)\n\n        target_id = \"bokeh-plot-%d\" % env.new_serialno('bokeh-plot')\n        target_node = nodes.target('', '', ids=[target_id])\n        result = [target_node]\n\n        source_position = self.options.get('source-position', 'below')\n\n        spec = json.load(open(self.arguments[0]))\n\n        details = spec['details']\n\n        for i, detail in enumerate(details):\n            path = detail['path']\n            name = detail['name']\n            prev_ref, next_ref = None, None\n            if i > 0:\n                prev_ref = \"gallery_\" + details[i-1]['name']\n            if i < len(details)-1:\n                next_ref = \"gallery_\" + details[i+1]['name']\n            rst = DETAIL_TEMPLATE.render(\n                name=name,\n                underline=\"#\"*len(name),\n                path=abspath(\"../\" + path),\n                symbol=detail.get('symbol'),\n                prev_ref=prev_ref,\n                up_ref=\"gallery\",\n                next_ref=next_ref,\n                source_position=source_position,\n            )\n            with open(join(dest_dir, \"%s.rst\" % name), \"w\") as f:\n                f.write(rst)\n            env.read_doc(join(\"docs\", \"gallery\", name), app=app)\n\n        result = ViewList()\n        names = [detail['name'] for detail in details]\n        text = GALLERY_TEMPLATE.render(names=names)\n        for line in text.split(\"\\n\"):\n            result.append(line, \"<bokeh-gallery>\")\n        node = nodes.paragraph()\n        node.document = self.state.document\n        self.state.nested_parse(result, 0, node)\n        return node.children",
  "def bokeh_commit(name, rawtext, text, lineno, inliner, options={}, content=[]):\n    \"\"\"Link to a Bokeh Github issue.\n\n    Returns 2 part tuple containing list of nodes to insert into the\n    document and a list of system messages.  Both are allowed to be\n    empty.\n\n    \"\"\"\n    app = inliner.document.settings.env.app\n    node = make_gh_link_node(app, rawtext, 'commit', 'commit', 'commit', text, options)\n    return [node], []",
  "def bokeh_issue(name, rawtext, text, lineno, inliner, options={}, content=[]):\n    \"\"\"Link to a Bokeh Github issue.\n\n    Returns 2 part tuple containing list of nodes to insert into the\n    document and a list of system messages.  Both are allowed to be\n    empty.\n\n    \"\"\"\n    app = inliner.document.settings.env.app\n    try:\n        issue_num = int(text)\n        if issue_num <= 0:\n            raise ValueError\n    except ValueError:\n        msg = inliner.reporter.error(\n            'Github issue number must be a number greater than or equal to 1; '\n            '\"%s\" is invalid.' % text, line=lineno)\n        prb = inliner.problematic(rawtext, rawtext, msg)\n        return [prb], [msg]\n    node = make_gh_link_node(app, rawtext, 'issue', 'issue', 'issues', str(issue_num), options)\n    return [node], []",
  "def bokeh_milestone(name, rawtext, text, lineno, inliner, options={}, content=[]):\n    \"\"\"Link to a Bokeh Github issue.\n\n    Returns 2 part tuple containing list of nodes to insert into the\n    document and a list of system messages.  Both are allowed to be\n    empty.\n\n    \"\"\"\n    app = inliner.document.settings.env.app\n    node = make_gh_link_node(app, rawtext, 'milestone', 'milestone', 'milestones', text, options)\n    return [node], []",
  "def bokeh_pull(name, rawtext, text, lineno, inliner, options={}, content=[]):\n    \"\"\"Link to a Bokeh Github issue.\n\n    Returns 2 part tuple containing list of nodes to insert into the\n    document and a list of system messages.  Both are allowed to be\n    empty.\n\n    \"\"\"\n    app = inliner.document.settings.env.app\n    try:\n        issue_num = int(text)\n        if issue_num <= 0:\n            raise ValueError\n    except ValueError:\n        msg = inliner.reporter.error(\n            'Github pull request number must be a number greater than or equal to 1; '\n            '\"%s\" is invalid.' % text, line=lineno)\n        prb = inliner.problematic(rawtext, rawtext, msg)\n        return [prb], [msg]\n    node = make_gh_link_node(app, rawtext, 'pull', 'pull request', 'pull', str(issue_num), options)\n    return [node], []",
  "def make_gh_link_node(app, rawtext, role, kind, api_type, id, options={}):\n    \"\"\" Return a link to a Bokeh Github resource.\n\n    Args:\n        app (Sphinx app) : current app\n        rawtext (str) : text being replaced with link node.\n        role (str) : role name\n        kind (str) : resource type (issue, pull, etc.)\n        api_type (str) : type for api link\n        id : (str) : id of the resource to link to\n        options (dict) : options dictionary passed to role function\n\n    \"\"\"\n    url = \"%s/%s/%s\" % (BOKEH_GH, api_type, id)\n    request = urllib.request.Request(url)\n    request.get_method = lambda : 'HEAD'\n    try:\n        response = urllib.request.urlopen(request, timeout=5)\n    except urllib.error.HTTPError:\n        app.warn(\"URL '%s' for :bokeh-%s: role could not be loaded\" % (url, role))\n    else:\n        if response.getcode() >= 400:\n            app.warn(\"URL '%s' for :bokeh-%s: role could not be loaded\" % (url, role))\n    set_classes(options)\n    node = nodes.reference(\n        rawtext, kind + ' ' + utils.unescape(id), refuri=url, **options)\n    return node",
  "def setup(app):\n    app.add_role('bokeh-commit', bokeh_commit)\n    app.add_role('bokeh-issue', bokeh_issue)\n    app.add_role('bokeh-milestone', bokeh_milestone)\n    app.add_role('bokeh-pull', bokeh_pull)",
  "class PropDocumenter(AttributeDocumenter):\n    directivetype = 'bokeh-prop'\n    objtype = 'prop'\n    priority = 20 # prefer over default autoattribute directive\n\n    @classmethod\n    def can_document_member(cls, member, membername, isattr, parent):\n        # MetaHasProps creates instances even if user just\n        # supplies property class name\n        return isinstance(member, Property)\n\n    def add_directive_header(self, sig):\n        # Note: we are supplying our own version of this function because\n        # bokeh-model should not be passed `sig` as an argument, and there is\n        # no way to suppress this behaviour in the default version\n        domain = getattr(self, 'domain', 'py')\n        directive = getattr(self, 'directivetype', self.objtype)\n        name = self.format_name()\n        self.add_line(u'.. %s:%s:: %s.%s' % (domain, directive, self.modname, name),\n                      '<autodoc>')\n        if self.options.noindex:\n            self.add_line(u'   :noindex:', '<autodoc>')",
  "class ModelDocumenter(ClassDocumenter):\n    directivetype = 'bokeh-model'\n    objtype = 'model'\n    priority = 20 # prefer over default autoclass directive\n\n    @classmethod\n    def can_document_member(cls, member, membername, isattr, parent):\n        return isinstance(member, class_types) and issubclass(member, PlotObject)\n\n    def add_directive_header(self, sig):\n        # Note: we are supplying our own version of this function because\n        # bokeh-model should not be passed `sig` as an argument, and there is\n        # no way to suppress this behaviour in the default version\n        domain = getattr(self, 'domain', 'py')\n        directive = getattr(self, 'directivetype', self.objtype)\n        name = self.format_name()\n        self.add_line(u'.. %s:%s:: %s.%s' % (domain, directive, self.modname, name),\n                      '<autodoc>')\n        if self.options.noindex:\n            self.add_line(u'   :noindex:', '<autodoc>')",
  "def setup(app):\n    app.add_autodocumenter(PropDocumenter)\n    app.add_autodocumenter(ModelDocumenter)",
  "def can_document_member(cls, member, membername, isattr, parent):\n        # MetaHasProps creates instances even if user just\n        # supplies property class name\n        return isinstance(member, Property)",
  "def add_directive_header(self, sig):\n        # Note: we are supplying our own version of this function because\n        # bokeh-model should not be passed `sig` as an argument, and there is\n        # no way to suppress this behaviour in the default version\n        domain = getattr(self, 'domain', 'py')\n        directive = getattr(self, 'directivetype', self.objtype)\n        name = self.format_name()\n        self.add_line(u'.. %s:%s:: %s.%s' % (domain, directive, self.modname, name),\n                      '<autodoc>')\n        if self.options.noindex:\n            self.add_line(u'   :noindex:', '<autodoc>')",
  "def can_document_member(cls, member, membername, isattr, parent):\n        return isinstance(member, class_types) and issubclass(member, PlotObject)",
  "def add_directive_header(self, sig):\n        # Note: we are supplying our own version of this function because\n        # bokeh-model should not be passed `sig` as an argument, and there is\n        # no way to suppress this behaviour in the default version\n        domain = getattr(self, 'domain', 'py')\n        directive = getattr(self, 'directivetype', self.objtype)\n        name = self.format_name()\n        self.add_line(u'.. %s:%s:: %s.%s' % (domain, directive, self.modname, name),\n                      '<autodoc>')\n        if self.options.noindex:\n            self.add_line(u'   :noindex:', '<autodoc>')",
  "class BokehPropDirective(Directive):\n\n    has_content = True\n    required_arguments = 1\n\n    def run(self):\n\n        prop_path = self.arguments[0]\n        module_path, model_name, prop_name = prop_path.rsplit('.', 2)\n\n        try:\n            module = importlib.import_module(module_path)\n        except ImportError:\n            pass\n\n        model = getattr(module, model_name, None)\n        if model is None:\n            pass\n\n        if type(model) != Viewable:\n            pass\n\n        model_obj = model()\n\n        prop = getattr(model_obj.__class__, prop_name)\n\n        type_info = self._get_type_info(prop)\n\n        rst_text = PROP_TEMPLATE.render(\n            name=prop_name,\n            module=module_path,\n            type_info=type_info,\n            doc=\"\" if prop.__doc__ is None else textwrap.dedent(prop.__doc__),\n        )\n\n        result = ViewList()\n        for line in rst_text.split(\"\\n\"):\n            result.append(line, \"<bokeh-prop>\")\n        node = nodes.paragraph()\n        node.document = self.state.document\n        nested_parse_with_titles(self.state, result, node)\n        return node.children\n\n    def _get_type_info(self, prop):\n        desc = str(prop)\n        template = \":class:`~bokeh.properties.%s`\\ \"\n        # some of the property names are substrings of other property names\n        # so first go through greedily replacing the longest possible match\n        # with a unique id (PROP_NAMES is reverse sorted by length)\n        for i, name in enumerate(PROP_NAMES):\n            desc = desc.replace(name, \"__ID%d\" % i)\n        # now replace the unique id with the corresponding prop name. Go in\n        # reverse to make sure replacements are greedy\n        for i in range(len(PROP_NAMES)-1, 0, -1):\n            name = PROP_NAMES[i]\n            desc = desc.replace(\"__ID%d\" % i, template % name)\n        return desc",
  "def setup(app):\n    app.add_directive_to_domain('py', 'bokeh-prop', BokehPropDirective)",
  "def run(self):\n\n        prop_path = self.arguments[0]\n        module_path, model_name, prop_name = prop_path.rsplit('.', 2)\n\n        try:\n            module = importlib.import_module(module_path)\n        except ImportError:\n            pass\n\n        model = getattr(module, model_name, None)\n        if model is None:\n            pass\n\n        if type(model) != Viewable:\n            pass\n\n        model_obj = model()\n\n        prop = getattr(model_obj.__class__, prop_name)\n\n        type_info = self._get_type_info(prop)\n\n        rst_text = PROP_TEMPLATE.render(\n            name=prop_name,\n            module=module_path,\n            type_info=type_info,\n            doc=\"\" if prop.__doc__ is None else textwrap.dedent(prop.__doc__),\n        )\n\n        result = ViewList()\n        for line in rst_text.split(\"\\n\"):\n            result.append(line, \"<bokeh-prop>\")\n        node = nodes.paragraph()\n        node.document = self.state.document\n        nested_parse_with_titles(self.state, result, node)\n        return node.children",
  "def _get_type_info(self, prop):\n        desc = str(prop)\n        template = \":class:`~bokeh.properties.%s`\\ \"\n        # some of the property names are substrings of other property names\n        # so first go through greedily replacing the longest possible match\n        # with a unique id (PROP_NAMES is reverse sorted by length)\n        for i, name in enumerate(PROP_NAMES):\n            desc = desc.replace(name, \"__ID%d\" % i)\n        # now replace the unique id with the corresponding prop name. Go in\n        # reverse to make sure replacements are greedy\n        for i in range(len(PROP_NAMES)-1, 0, -1):\n            name = PROP_NAMES[i]\n            desc = desc.replace(\"__ID%d\" % i, template % name)\n        return desc",
  "class Foo(PlotObject):\n    \"\"\" This is a Foo model. \"\"\"\n    index = Either(Auto, Enum('abc', 'def', 'xzy'), help=\"doc for index\")\n    value = Tuple(Float, Float, help=\"doc for value\")",
  "class Bar(PlotObject):\n    \"\"\" This is a Bar model. \"\"\"\n    thing = List(Int, help=\"doc for thing\")",
  "class TickFormatter(PlotObject):\n    \"\"\" A base class for all tick formatter types. ``TickFormatter`` is\n    not generally useful to instantiate on its own.\n\n    \"\"\"\n    pass",
  "class BasicTickFormatter(TickFormatter):\n    \"\"\" Display tick values from continuous ranges as \"basic numbers\",\n    using scientific notation when appropriate by default.\n\n    \"\"\"\n    precision = Either(Auto, Int, help=\"\"\"\n    How many digits of precision to display in tick labels.\n    \"\"\")\n\n    use_scientific = Bool(True, help=\"\"\"\n    Whether to ever display scientific notation. If ``True``, then\n    when to use scientific notation is controlled by ``power_limit_low``\n    and ``power_limit_high``.\n    \"\"\")\n\n    power_limit_high = Int(5, help=\"\"\"\n    Limit the use of scientific notation to when::\n\n        log(x) >= power_limit_high\n\n    \"\"\")\n\n    power_limit_low = Int(-3, help=\"\"\"\n    Limit the use of scientific notation to when::\n\n        log(x) <= power_limit_low\n\n    \"\"\")",
  "class LogTickFormatter(TickFormatter):\n    \"\"\" Display tick values from continuous ranges as powers\n    of some base.\n\n    Most often useful in conjunction with a ``LogTicker``.\n\n    \"\"\"\n    ticker = Instance(Ticker, help=\"\"\"\n    The corresponding ``LogTicker``, used to determine the correct\n    base to use. If unset, the formatter will use base 10 as a default.\n    \"\"\")",
  "class CategoricalTickFormatter(TickFormatter):\n    \"\"\" Display tick values from categorical ranges as string\n    values.\n\n    \"\"\"\n    pass",
  "class DatetimeTickFormatter(TickFormatter):\n    \"\"\" Display tick values from a continuous range as formatted\n    datetimes.\n\n    \"\"\"\n\n    formats = Dict(Enum(DatetimeUnits), List(String), help=\"\"\"\n    User defined formats for displaying datetime values.\n\n    The enum values correspond roughly to different \"time scales\". The\n    corresponding value is a list of `strftime`_ formats to use for\n    formatting datetime values that fall in in that \"time scale\".\n\n    This list of supported `strftime`_ formats is reproduced below.\n\n    .. warning::\n        The client library BokehJS uses the `timezone`_ library to\n        format datetimes. The inclusion of the list below is based on the\n        claim that `timezone`_ makes to support \"the full compliment\n        of GNU date format specifiers.\" However, this claim has not\n        been tested exhaustively against this list. If you find formats\n        that do not function as expected, please submit a `github issue`,\n        so that the documentation can be updated appropriately.\n\n    %a\n        The abbreviated name of the day of the week according to the\n        current locale.\n\n    %A\n        The full name of the day of the week according to the current\n        locale.\n\n    %b\n        The abbreviated month name according to the current locale.\n\n    %B\n        The full month name according to the current locale.\n\n    %c\n        The preferred date and time representation for the current\n        locale.\n\n    %C\n        The century number (year/100) as a 2-digit integer.\n\n    %d\n        The day of the month as a decimal number (range 01 to 31).\n\n    %D\n        Equivalent to %m/%d/%y.  (Americans should note that in many\n        other countries %d/%m/%y is rather common. This means that in\n        international context this format is ambiguous and should not\n        be used.)\n\n    %e\n        Like %d, the day of the month as a decimal number, but a\n        leading zero is replaced by a space.\n\n    %F\n        Equivalent to %Y-%m-%d (the ISO 8601 date format).\n\n    %G\n        The ISO 8601 week-based year with century as a decimal number.\n        The 4-digit year corresponding to the ISO week number (see %V).\n        This has the same format and value as %Y, except that if the\n        ISO week number belongs to the previous or next year, that year\n        is used instead.\n\n    %g\n        Like %G, but without century, that is, with a 2-digit year (00-99).\n\n    %h\n        Equivalent to %b.\n\n    %H\n        The hour as a decimal number using a 24-hour clock (range 00\n        to 23).\n\n    %I\n        The hour as a decimal number using a 12-hour clock (range 01\n        to 12).\n\n    %j\n        The day of the year as a decimal number (range 001 to 366).\n\n    %k\n        The hour (24-hour clock) as a decimal number (range 0 to 23).\n        Single digits are preceded by a blank.  (See also %H.)\n\n    %l\n        The hour (12-hour clock) as a decimal number (range 1 to 12).\n        Single digits are preceded by a blank.  (See also %I.)  (TZ)\n\n    %m\n        The month as a decimal number (range 01 to 12).\n\n    %M\n        The minute as a decimal number (range 00 to 59).\n\n    %n\n        A newline character.\n\n    %p\n        Either \"AM\" or \"PM\" according to the given time value, or the\n        corresponding strings for the current locale.  Noon is treated\n        as \"PM\" and midnight as \"AM\".\n\n    %P\n        Like %p but in lowercase: \"am\" or \"pm\" or a corresponding\n        string for the current locale.\n\n    %r\n        The time in a.m. or p.m. notation.  In the POSIX locale this\n        is equivalent to %I:%M:%S %p.\n\n    %R\n        The time in 24-hour notation (%H:%M). For a version including\n        the seconds, see %T below.\n\n    %s\n        The number of seconds since the Epoch, 1970-01-01 00:00:00\n        +0000 (UTC).\n\n    %S\n        The second as a decimal number (range 00 to 60).  (The range\n        is up to 60 to allow for occasional leap seconds.)\n\n    %t\n        A tab character.\n\n    %T\n        The time in 24-hour notation (%H:%M:%S).\n\n    %u\n        The day of the week as a decimal, range 1 to 7, Monday being 1.\n        See also %w.\n\n    %U\n        The week number of the current year as a decimal number, range\n        00 to 53, starting with the first Sunday as the first day of\n        week 01.  See also %V and %W.\n\n    %V\n        The ISO 8601 week number (see NOTES) of the current year as a\n        decimal number, range 01 to 53, where week 1 is the first week\n        that has at least 4 days in the new year.  See also %U and %W.\n\n    %w\n        The day of the week as a decimal, range 0 to 6, Sunday being 0.\n        See also %u.\n\n    %W\n        The week number of the current year as a decimal number, range\n        00 to 53, starting with the first Monday as the first day of\n        week 01.\n\n    %x\n        The preferred date representation for the current locale\n        without the time.\n\n    %X\n        The preferred time representation for the current locale\n        without the date.\n\n    %y\n        The year as a decimal number without a century (range 00 to 99).\n\n    %Y\n        The year as a decimal number including the century.\n\n    %z\n        The +hhmm or -hhmm numeric timezone (that is, the hour and\n        minute offset from UTC).\n\n    %Z\n        The timezone name or abbreviation.\n\n    %%\n        A literal '%' character.\n\n    .. _strftime: http://man7.org/linux/man-pages/man3/strftime.3.html\n    .. _timezone: http://bigeasy.github.io/timezone/\n    .. _github issue: https://github.com/bokeh/bokeh/issues\n\n    \"\"\")",
  "class Ticker(PlotObject):\n    \"\"\" A base class for all ticker types. ``Ticker`` is\n    not generally useful to instantiate on its own.\n\n    \"\"\"\n\n    num_minor_ticks = Int(5, help=\"\"\"\n    The number of minor tick positions to generate between\n    adjacent major tick values.\n    \"\"\")",
  "class AdaptiveTicker(Ticker):\n    \"\"\" Generate \"nice\" round ticks at any magnitude.\n\n    Creates ticks that are \"base\" multiples of a set of given\n    mantissas. For example, with ``base=10`` and\n    ``mantissas=[1, 2, 5]``, the ticker will generate the sequence::\n\n        ..., 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100, ...\n\n    \"\"\"\n\n    base = Float(10.0, help=\"\"\"\n    The multiplier to use for scaling mantissas.\n    \"\"\")\n\n    mantissas = List(Float, [2, 5, 10], help=\"\"\"\n    The acceptable list numbers to generate multiples of.\n    \"\"\")\n\n    min_interval = Float(0.0, help=\"\"\"\n    The smallest allowable interval between two adjacent ticks.\n    \"\"\")\n\n    max_interval = Float(100.0, help=\"\"\"\n    The largest allowable interval between two adjacent ticks.\n    \"\"\")",
  "class CompositeTicker(Ticker):\n    \"\"\" Combine different tickers at different scales.\n\n    Uses the ``min_interval`` and ``max_interval`` interval attributes\n    of the tickers to select the appropriate ticker at different\n    scales.\n\n    \"\"\"\n\n    tickers = List(Instance(Ticker), help=\"\"\"\n    A list of Ticker objects to combine at different scales in order\n    to generate tick values. The supplied tickers should be in order.\n    Specifically, if S comes before T, then it should be the case that::\n\n        S.get_max_interval() < T.get_min_interval()\n\n    \"\"\")",
  "class SingleIntervalTicker(Ticker):\n    \"\"\" Generate evenly spaced ticks at a fixed interval regardless of\n    scale.\n\n    \"\"\"\n\n    interval = Float(help=\"\"\"\n    The interval between adjacent ticks.\n    \"\"\")",
  "class DaysTicker(SingleIntervalTicker):\n    \"\"\" Generate ticks spaced apart by specific, even multiples of days.\n\n    \"\"\"\n    days = List(Int, help=\"\"\"\n    The intervals of days to use.\n    \"\"\")",
  "class MonthsTicker(SingleIntervalTicker):\n    \"\"\" Generate ticks spaced apart by specific, even multiples of months.\n\n    \"\"\"\n    months = List(Int, help=\"\"\"\n    The intervals of months to use.\n    \"\"\")",
  "class YearsTicker(SingleIntervalTicker):\n    \"\"\" Generate ticks spaced apart even numbers of years.\n\n    \"\"\"",
  "class BasicTicker(AdaptiveTicker):\n    \"\"\" Generate ticks on a linear scale.\n\n    .. note::\n        This class may be renamed to ``LinearTicker`` in the future.\n\n    \"\"\"",
  "class LogTicker(AdaptiveTicker):\n    \"\"\" Generate ticks on a log scale.\n\n    \"\"\"",
  "class CategoricalTicker(Ticker):\n    \"\"\" Generate ticks for categorical ranges.\n\n    \"\"\"",
  "class DatetimeTicker(Ticker):\n    \"\"\" Generate nice ticks across different date and time scales.\n\n    \"\"\"",
  "class Axis(GuideRenderer):\n    \"\"\" A base class that defines common properties for all axis types.\n    ``Axis`` is not generally useful to instantiate on its own.\n\n    \"\"\"\n\n    hide = Bool(False, help=\"\"\"\n    Ability to hide the entire axis from the plot.\n    \"\"\")\n\n    location = Either(Auto, Enum(Location), help=\"\"\"\n    Where should labels and ticks be located in relation to the axis rule.\n    \"\"\")\n\n    bounds = Either(Auto, Tuple(Float, Float), Tuple(Datetime, Datetime), help=\"\"\"\n    Bounds for the rendered axis. If unset, the axis will span the\n    entire plot in the given dimension.\n    \"\"\")\n\n    x_range_name = String('default', help=\"\"\"\n    A particular (named) x-range to use for computing screen\n    locations when rendering an axis on the plot. If unset, use the\n    default x-range.\n    \"\"\")\n\n    y_range_name = String('default', help=\"\"\"\n    A particular (named) y-range to use for computing screen\n    locations when rendering an axis on the plot. If unset, use the\n    default y-range.\n    \"\"\")\n\n    ticker = Instance(Ticker, help=\"\"\"\n    A Ticker to use for computing locations of axis components.\n    \"\"\")\n\n    formatter = Instance(TickFormatter, help=\"\"\"\n    A TickFormatter to use for formatting the visual appearance\n    of ticks.\n    \"\"\")\n\n    axis_label = String(help=\"\"\"\n    A text label for the axis, displayed parallel to the axis rule.\n\n    .. note::\n        LaTeX notation is not currently supported; please see\n        :bokeh-issue:`647` to track progress or contribute.\n\n    \"\"\")\n\n    axis_label_standoff = Int(help=\"\"\"\n    The distance in pixels that the axis labels should be offset\n    from the tick labels.\n    \"\"\")\n\n    axis_label_props = Include(TextProps, help=\"\"\"\n    The %s of the axis label.\n    \"\"\")\n\n    major_label_standoff = Int(help=\"\"\"\n    The distance in pixels that the major tick labels should be\n    offset from the associated ticks.\n    \"\"\")\n\n    major_label_orientation = Either(Enum(\"horizontal\", \"vertical\"), Float, help=\"\"\"\n    What direction the major label text should be oriented. If a i\n    number is supplied, the angle of the text is measured from horizontal.\n    \"\"\")\n\n    major_label_props = Include(TextProps, help=\"\"\"\n    The %s of the major tick labels.\n    \"\"\")\n\n    axis_props = Include(LineProps, help=\"\"\"\n    The %s of the axis line.\n    \"\"\")\n\n    major_tick_props = Include(LineProps, help=\"\"\"\n    The %s of the major ticks.\n    \"\"\")\n\n    major_tick_in = Int(help=\"\"\"\n    The distance in pixels that major ticks should extend into the\n    main plot area.\n    \"\"\")\n\n    major_tick_out = Int(help=\"\"\"\n    The distance in pixels that major ticks should extend out of the\n    main plot area.\n    \"\"\")\n\n    minor_tick_props = Include(LineProps, help=\"\"\"\n    The %s of the minor ticks.\n    \"\"\")\n\n    minor_tick_in = Int(help=\"\"\"\n    The distance in pixels that minor ticks should extend into the\n    main plot area.\n    \"\"\")\n\n    minor_tick_out = Int(help=\"\"\"\n    The distance in pixels that major ticks should extend out of the\n    main plot area.\n    \"\"\")",
  "class ContinuousAxis(Axis):\n    \"\"\" A base class for all numeric, non-categorica axes types.\n    ``ContinuousAxis`` is not generally useful to instantiate on its own.\n\n    \"\"\"\n    pass",
  "class LinearAxis(ContinuousAxis):\n    \"\"\" An axis that picks nice numbers for tick locations on a\n    linear scale. Configured with a ``BasicTickFormatter`` by default.\n\n    \"\"\"\n    def __init__(self, ticker=None, formatter=None, **kwargs):\n        if ticker is None:\n            ticker = BasicTicker()\n        if formatter is None:\n            formatter = BasicTickFormatter()\n        super(LinearAxis, self).__init__(ticker=ticker, formatter=formatter, **kwargs)",
  "class LogAxis(ContinuousAxis):\n    \"\"\" An axis that picks nice numbers for tick locations on a\n    log scale. Configured with a ``LogTickFormatter`` by default.\n\n    \"\"\"\n\n    def __init__(self, ticker=None, formatter=None, **kwargs):\n        if ticker is None:\n            ticker = LogTicker(num_minor_ticks=10)\n        if formatter is None:\n            formatter = LogTickFormatter(ticker=ticker)\n        super(LogAxis, self).__init__(ticker=ticker, formatter=formatter, **kwargs)",
  "class CategoricalAxis(Axis):\n    \"\"\" An axis that picks evenly spaced tick locations for a\n    collection of categories/factors.\n\n    \"\"\"\n    def __init__(self, ticker=None, formatter=None, **kwargs):\n        if ticker is None:\n            ticker = CategoricalTicker()\n        if formatter is None:\n            formatter = CategoricalTickFormatter()\n        super(CategoricalAxis, self).__init__(ticker=ticker, formatter=formatter, **kwargs)",
  "class DatetimeAxis(LinearAxis):\n    \"\"\" An LinearAxis that picks nice numbers for tick locations on\n    a datetime scale. Configured with a ``DatetimeTickFormatter`` by\n    default.\n\n    \"\"\"\n    axis_label = String(\"date\", help=\"\"\"\n    DateTime ``axis_label`` defaults to \"date\".\n    \"\"\")\n\n    # TODO: (bev) this should be an Enum, if it is exposed at all\n    scale = String(\"time\")\n\n    num_labels = Int(8)\n\n    char_width = Int(10)\n\n    fill_ratio = Float(0.3)\n\n    def __init__(self, ticker=None, formatter=None, **kwargs):\n        if ticker is None:\n            ticker = DatetimeTicker()\n        if formatter is None:\n            formatter = DatetimeTickFormatter()\n        super(DatetimeAxis, self).__init__(ticker=ticker, formatter=formatter, **kwargs)",
  "def __init__(self, ticker=None, formatter=None, **kwargs):\n        if ticker is None:\n            ticker = BasicTicker()\n        if formatter is None:\n            formatter = BasicTickFormatter()\n        super(LinearAxis, self).__init__(ticker=ticker, formatter=formatter, **kwargs)",
  "def __init__(self, ticker=None, formatter=None, **kwargs):\n        if ticker is None:\n            ticker = LogTicker(num_minor_ticks=10)\n        if formatter is None:\n            formatter = LogTickFormatter(ticker=ticker)\n        super(LogAxis, self).__init__(ticker=ticker, formatter=formatter, **kwargs)",
  "def __init__(self, ticker=None, formatter=None, **kwargs):\n        if ticker is None:\n            ticker = CategoricalTicker()\n        if formatter is None:\n            formatter = CategoricalTickFormatter()\n        super(CategoricalAxis, self).__init__(ticker=ticker, formatter=formatter, **kwargs)",
  "def __init__(self, ticker=None, formatter=None, **kwargs):\n        if ticker is None:\n            ticker = DatetimeTicker()\n        if formatter is None:\n            formatter = DatetimeTickFormatter()\n        super(DatetimeAxis, self).__init__(ticker=ticker, formatter=formatter, **kwargs)",
  "class Widget(PlotObject):\n    \"\"\" A base class for all interact widget types. ``Widget``\n    is not generally useful to instantiate on its own.\n\n    \"\"\"\n\n    disabled = Bool(False, help=\"\"\"\n    Whether the widget will be disabled when rendered. If ``True``,\n    the widget will be greyed-out, and not respond to UI events.\n    \"\"\")\n\n    # TODO: (mp) Not yet, because it breaks plotting/notebook examples.\n    # Rename to _repr_html_ if we decide to enable this by default.\n    def __repr_html__(self):\n        return notebook_div(self)\n\n    @property\n    def html(self):\n        from IPython.core.display import HTML\n        return HTML(self.__repr_html__())",
  "def __repr_html__(self):\n        return notebook_div(self)",
  "def html(self):\n        from IPython.core.display import HTML\n        return HTML(self.__repr_html__())",
  "class Renderer(PlotObject):\n    \"\"\" A base class for renderer types. ``Renderer`` is not\n    generally useful to instantiate on its own.\n\n    \"\"\"",
  "class GlyphRenderer(Renderer):\n    \"\"\"\n\n    \"\"\"\n\n    data_source = Instance(DataSource, help=\"\"\"\n    Local data source to use when rendering glyphs on the plot.\n    \"\"\")\n\n    x_range_name = String('default', help=\"\"\"\n    A particular (named) x-range to use for computing screen\n    locations when rendering glyphs on the plot. If unset, use the\n    default x-range.\n    \"\"\")\n\n    y_range_name = String('default', help=\"\"\"\n    A particular (named) y-range to use for computing screen\n    locations when rendering glyphs on the plot. If unset, use the\n    default -range.\n    \"\"\")\n\n    # TODO: (bev) is this actually used?\n    units = Enum(Units)\n\n    glyph = Instance(Glyph, help=\"\"\"\n    The glyph to render, in conjunction with the supplied data source\n    and ranges.\n    \"\"\")\n\n    selection_glyph = Instance(Glyph, help=\"\"\"\n    An optional glyph used for selected points.\n    \"\"\")\n\n    nonselection_glyph = Instance(Glyph, help=\"\"\"\n    An optional glyph used for explicitly non-selected points\n    (i.e., non-selected when there are other points that are selected,\n    but not when no points at all are selected.)\n    \"\"\")",
  "class Legend(Renderer):\n    \"\"\" Render informational legends for a plot.\n\n    \"\"\"\n\n    plot = Instance(\".models.plots.Plot\", help=\"\"\"\n    The Plot to which this Legend is attached.\n    \"\"\")\n\n    orientation = Enum(Orientation, help=\"\"\"\n    The location where the legend should draw itself.\n    \"\"\")\n\n    border_props = Include(LineProps, help=\"\"\"\n    The %s for the legend border outline.\n    \"\"\")\n\n    label_props = Include(TextProps, help=\"\"\"\n    The %s for the legend labels.\n    \"\"\")\n\n    label_standoff = Int(15, help=\"\"\"\n    The distance in pixels\n    \"\"\")\n\n    label_height = Int(20, help=\"\"\"\n    The height in pixels that the area that legend labels should occupy.\n    \"\"\")\n\n    label_width = Int(50, help=\"\"\"\n    The width in pixels that the area that legend labels should occupy.\n    \"\"\")\n\n    glyph_height = Int(20, help=\"\"\"\n    The height in pixels that the rendered legend glyph should occupy.\n    \"\"\")\n\n    glyph_width = Int(20, help=\"\"\"\n    The width in pixels that the rendered legend glyph should occupy.\n    \"\"\")\n\n    legend_padding = Int(10, help=\"\"\"\n    Amount of padding around the legend.\n    \"\"\")\n\n    legend_spacing = Int(3, help=\"\"\"\n    Amount of spacing between legend entried\n    \"\"\")\n\n    legends = List(Tuple(String, List(Instance(GlyphRenderer))), help=\"\"\"\n    A list of tuples that maps text labels to the legend to corresponding\n    renderers that should draw sample representations for those labels.\n\n    .. note::\n        The ``legends`` attribute may also be set from a dict or OrderedDict,\n        but note that if a dict is used, the order of the legend entries is\n        unspecified.\n\n    \"\"\").accepts(\n        Dict(String, List(Instance(GlyphRenderer))), lambda d: list(d.items())\n    )",
  "class GuideRenderer(Renderer):\n    \"\"\" A base class for all guide renderer types. ``GuideRenderer`` is\n    not generally useful to instantiate on its own.\n\n    \"\"\"\n\n    plot = Instance(\".models.plots.Plot\", help=\"\"\"\n    The plot to which this guide renderer is attached.\n    \"\"\")\n\n    def __init__(self, **kwargs):\n        super(GuideRenderer, self).__init__(**kwargs)\n\n        if self.plot is not None:\n            if self not in self.plot.renderers:\n                self.plot.renderers.append(self)",
  "def __init__(self, **kwargs):\n        super(GuideRenderer, self).__init__(**kwargs)\n\n        if self.plot is not None:\n            if self not in self.plot.renderers:\n                self.plot.renderers.append(self)",
  "class DataSource(PlotObject):\n    \"\"\" A base class for data source types. ``DataSource`` is\n    not generally useful to instantiate on its own.\n\n    \"\"\"\n\n    column_names = List(String, help=\"\"\"\n    An list of names for all the columns in this DataSource.\n    \"\"\")\n\n    selected = List(Int, help=\"\"\"\n    A list of selected indices on this DataSource.\n    \"\"\")\n\n    def columns(self, *columns):\n        \"\"\" Returns a ColumnsRef object for a column or set of columns\n        on this data source.\n\n        Args:\n            *columns\n\n        Returns:\n            ColumnsRef\n\n        \"\"\"\n        return ColumnsRef(source=self, columns=list(columns))",
  "class ColumnsRef(HasProps):\n    \"\"\" A utility object to allow referring to a collection of columns\n    from a specified data source, all together.\n\n    \"\"\"\n\n    source = Instance(DataSource, help=\"\"\"\n    A data source to reference.\n    \"\"\")\n\n    columns = List(String, help=\"\"\"\n    A list of column names to reference from ``source``.\n    \"\"\")",
  "class ColumnDataSource(DataSource):\n    \"\"\" Maps names of columns to sequences or arrays.\n\n    If the ColumnDataSource initializer is called with a single\n    argument that is a dict, that argument is used as the value for\n    the \"data\" attribute. For example::\n\n        ColumnDataSource(mydict) # same as ColumnDataSource(data=mydict)\n\n    .. note::\n        There is an implicit assumption that all the columns in a\n        a given ColumnDataSource have the same length.\n\n    \"\"\"\n\n    data = Dict(String, Any, help=\"\"\"\n    Mapping of column names to sequences of data. The data can be, e.g,\n    Python lists or tuples, NumPy arrays, etc.\n    \"\"\")\n\n    def __init__(self, *args, **kw):\n        \"\"\" If called with a single argument that is a dict, treat\n        that implicitly as the \"data\" attribute.\n        \"\"\"\n        if len(args) == 1 and \"data\" not in kw:\n            kw[\"data\"] = args[0]\n        # TODO (bev) invalid to pass args and \"data\", check and raise exception\n        raw_data = kw.pop(\"data\", {})\n        if not isinstance(raw_data, dict):\n            import pandas as pd\n            if isinstance(raw_data, pd.DataFrame):\n                raw_data = self.from_df(raw_data)\n            else:\n                raise ValueError(\"expected a dict or pandas.DataFrame, got %s\" % raw_data)\n        for name, data in raw_data.items():\n            self.add(data, name)\n        super(ColumnDataSource, self).__init__(**kw)\n\n    # TODO: (bev) why not just return a ColumnDataSource?\n    @classmethod\n    def from_df(cls, data):\n        \"\"\" Create a ``dict`` of columns from a Pandas DataFrame,\n        suitable for creating a ColumnDataSource.\n\n        Args:\n            data (DataFrame) : data to convert\n\n        Returns:\n            dict(str, list)\n\n        \"\"\"\n        index = data.index\n        new_data = {}\n        for colname in data:\n            new_data[colname] = data[colname].tolist()\n        if index.name:\n            new_data[index.name] = index.tolist()\n        elif index.names and not all([x is None for x in index.names]):\n            new_data[\"_\".join(index.names)] = index.tolist()\n        else:\n            new_data[\"index\"] = index.tolist()\n        return new_data\n\n    def to_df(self):\n        \"\"\" Convert this data source to pandas dataframe.\n\n        If ``column_names`` is set, use those. Otherwise let Pandas\n        infer the column names. The ``column_names`` property can be\n        used both to order and filter the columns.\n\n        Returns:\n            DataFrame\n\n        \"\"\"\n        import pandas as pd\n        if self.column_names:\n            return pd.DataFrame(self.data, columns=self.column_names)\n        else:\n            return pd.DataFrame(self.data)\n\n    def add(self, data, name=None):\n        \"\"\" Appends a new column of data to the data source.\n\n        Args:\n            data (seq) : new data to add\n            name (str, optional) : column name to use.\n                If not supplied, generate a name go the form \"Series ####\"\n\n        Returns:\n            str:  the column name used\n\n        \"\"\"\n        if name is None:\n            n = len(self.data)\n            while \"Series %d\"%n in self.data:\n                n += 1\n            name = \"Series %d\"%n\n        self.column_names.append(name)\n        self.data[name] = data\n        return name\n\n    def remove(self, name):\n        \"\"\" Remove a column of data.\n\n        Args:\n            name (str) : name of the column to remove\n\n        Returns:\n            None\n\n        .. note::\n            If the column name does not exist, a warning is issued.\n\n        \"\"\"\n        try:\n            self.column_names.remove(name)\n            del self.data[name]\n        except (ValueError, KeyError):\n            import warnings\n            warnings.warn(\"Unable to find column '%s' in data source\" % name)\n\n    def push_notebook(self):\n        \"\"\" Update date for a plot in the IPthon notebook in place.\n\n        This function can be be used to update data in plot data sources\n        in the IPython notebook, without having to use the Bokeh server.\n\n        Returns:\n            None\n\n        .. warning::\n            The current implementation leaks memory in the IPython notebook,\n            due to accumulating JS code. This function typically works well\n            with light UI interactions, but should not be used for continuously\n            updating data. See :bokeh-issue:`1732` for more details and to\n            track progress on potential fixes.\n\n        \"\"\"\n        from IPython.core import display\n        from bokeh.protocol import serialize_json\n        id = self.ref['id']\n        model = self.ref['type']\n        json = serialize_json(self.vm_serialize())\n        js = \"\"\"\n            var ds = Bokeh.Collections('{model}').get('{id}');\n            var data = {json};\n            ds.set(data);\n        \"\"\".format(model=model, id=id, json=json)\n        display.display_javascript(js, raw=True)",
  "class RemoteSource(DataSource):\n    data_url = String(help=\"\"\"\n    The URL to the endpoint for the data.\n    \"\"\")\n    data = Dict(String, Any, help=\"\"\"\n    Additional data to include directly in this data source object. The\n    columns provided here are merged with those from the Bokeh server.\n    \"\"\")\n    polling_interval = Int(help=\"\"\"\n    polling interval for updating data source in milliseconds\n    \"\"\")",
  "class AjaxDataSource(RemoteSource):\n    method = String('POST', help=\"http method - GET or POST\")",
  "class BlazeDataSource(RemoteSource):\n    #blaze parts\n    expr = Dict(String, Any(), help=\"\"\"\n    blaze expression graph in json form\n    \"\"\")\n    namespace = Dict(String, Any(), help=\"\"\"\n    namespace in json form for evaluating blaze expression graph\n    \"\"\")",
  "class ServerDataSource(BlazeDataSource):\n    \"\"\" A data source that referes to data located on a Bokeh server.\n\n    The data from the server is loaded on-demand by the client.\n    \"\"\"\n    # Paramters of data transformation operations\n    # The 'Any' is used to pass primtives around.\n    # TODO: (jc) Find/create a property type for 'any primitive/atomic value'\n    transform = Dict(String,Either(Instance(PlotObject), Any), help=\"\"\"\n    Paramters of the data transformation operations.\n\n    The associated valuse is minimally a tag that says which downsample routine\n    to use.  For some downsamplers, parameters are passed this way too.\n    \"\"\")",
  "def columns(self, *columns):\n        \"\"\" Returns a ColumnsRef object for a column or set of columns\n        on this data source.\n\n        Args:\n            *columns\n\n        Returns:\n            ColumnsRef\n\n        \"\"\"\n        return ColumnsRef(source=self, columns=list(columns))",
  "def __init__(self, *args, **kw):\n        \"\"\" If called with a single argument that is a dict, treat\n        that implicitly as the \"data\" attribute.\n        \"\"\"\n        if len(args) == 1 and \"data\" not in kw:\n            kw[\"data\"] = args[0]\n        # TODO (bev) invalid to pass args and \"data\", check and raise exception\n        raw_data = kw.pop(\"data\", {})\n        if not isinstance(raw_data, dict):\n            import pandas as pd\n            if isinstance(raw_data, pd.DataFrame):\n                raw_data = self.from_df(raw_data)\n            else:\n                raise ValueError(\"expected a dict or pandas.DataFrame, got %s\" % raw_data)\n        for name, data in raw_data.items():\n            self.add(data, name)\n        super(ColumnDataSource, self).__init__(**kw)",
  "def from_df(cls, data):\n        \"\"\" Create a ``dict`` of columns from a Pandas DataFrame,\n        suitable for creating a ColumnDataSource.\n\n        Args:\n            data (DataFrame) : data to convert\n\n        Returns:\n            dict(str, list)\n\n        \"\"\"\n        index = data.index\n        new_data = {}\n        for colname in data:\n            new_data[colname] = data[colname].tolist()\n        if index.name:\n            new_data[index.name] = index.tolist()\n        elif index.names and not all([x is None for x in index.names]):\n            new_data[\"_\".join(index.names)] = index.tolist()\n        else:\n            new_data[\"index\"] = index.tolist()\n        return new_data",
  "def to_df(self):\n        \"\"\" Convert this data source to pandas dataframe.\n\n        If ``column_names`` is set, use those. Otherwise let Pandas\n        infer the column names. The ``column_names`` property can be\n        used both to order and filter the columns.\n\n        Returns:\n            DataFrame\n\n        \"\"\"\n        import pandas as pd\n        if self.column_names:\n            return pd.DataFrame(self.data, columns=self.column_names)\n        else:\n            return pd.DataFrame(self.data)",
  "def add(self, data, name=None):\n        \"\"\" Appends a new column of data to the data source.\n\n        Args:\n            data (seq) : new data to add\n            name (str, optional) : column name to use.\n                If not supplied, generate a name go the form \"Series ####\"\n\n        Returns:\n            str:  the column name used\n\n        \"\"\"\n        if name is None:\n            n = len(self.data)\n            while \"Series %d\"%n in self.data:\n                n += 1\n            name = \"Series %d\"%n\n        self.column_names.append(name)\n        self.data[name] = data\n        return name",
  "def remove(self, name):\n        \"\"\" Remove a column of data.\n\n        Args:\n            name (str) : name of the column to remove\n\n        Returns:\n            None\n\n        .. note::\n            If the column name does not exist, a warning is issued.\n\n        \"\"\"\n        try:\n            self.column_names.remove(name)\n            del self.data[name]\n        except (ValueError, KeyError):\n            import warnings\n            warnings.warn(\"Unable to find column '%s' in data source\" % name)",
  "def push_notebook(self):\n        \"\"\" Update date for a plot in the IPthon notebook in place.\n\n        This function can be be used to update data in plot data sources\n        in the IPython notebook, without having to use the Bokeh server.\n\n        Returns:\n            None\n\n        .. warning::\n            The current implementation leaks memory in the IPython notebook,\n            due to accumulating JS code. This function typically works well\n            with light UI interactions, but should not be used for continuously\n            updating data. See :bokeh-issue:`1732` for more details and to\n            track progress on potential fixes.\n\n        \"\"\"\n        from IPython.core import display\n        from bokeh.protocol import serialize_json\n        id = self.ref['id']\n        model = self.ref['type']\n        json = serialize_json(self.vm_serialize())\n        js = \"\"\"\n            var ds = Bokeh.Collections('{model}').get('{id}');\n            var data = {json};\n            ds.set(data);\n        \"\"\".format(model=model, id=id, json=json)\n        display.display_javascript(js, raw=True)",
  "class GMapOptions(HasProps):\n    \"\"\" Options for GMapPlot objects.\n\n    \"\"\"\n\n    lat = Float(help=\"\"\"\n    The latitude where the map should be centered.\n    \"\"\")\n\n    lng = Float(help=\"\"\"\n    The longitude where the map should be centered.\n    \"\"\")\n\n    zoom = Int(12, help=\"\"\"\n    The initial `zoom level`_ to use when displaying the GMapPlot.\n\n    .. _zoom level: https://developers.google.com/maps/documentation/staticmaps/#Zoomlevels\n\n    \"\"\")\n\n    map_type = Enum(MapType, help=\"\"\"\n    The `map type`_ to use for the GMapPlot.\n\n    .. _map type: https://developers.google.com/maps/documentation/staticmaps/#MapTypes\n\n    \"\"\")",
  "class GMapPlot(Plot):\n    \"\"\" A Bokeh Plot with a `Google Map`_ displayed underneath.\n\n    .. _Google Map: https://www.google.com/maps/\n\n    \"\"\"\n\n    map_options = Instance(GMapOptions, help=\"\"\"\n    Options for displaying the plot.\n    \"\"\")",
  "class GeoJSOptions(HasProps):\n    \"\"\" Options for GeoJSPlot objects.\n\n    \"\"\"\n\n    lat = Float(help=\"\"\"\n    The latitude where the map should be centered.\n    \"\"\")\n\n    lng = Float(help=\"\"\"\n    The longitude where the map should be centered.\n    \"\"\")\n\n    zoom = Int(12, help=\"\"\"\n    The initial zoom level to use when displaying the GeoJSPlot.\n    \"\"\")",
  "class GeoJSPlot(Plot):\n    \"\"\" A Bokeh Plot with a `GeoJS Map`_ displayed underneath.\n\n    .. warning::\n        GeoJSPlot support should be considered experimental, a subject\n        to revision or removal.\n\n    .. _GeoJS Map: https://github.com/OpenGeoscience/geojs\n\n    \"\"\"\n\n    map_options = Instance(GeoJSOptions, help=\"\"\"\n    Options for displaying the plot.\n    \"\"\")",
  "class _list_attr_splat(list):\n    def __setattr__(self, attr, value):\n        for x in self:\n            setattr(x, attr, value)",
  "class PlotContext(PlotObject):\n    \"\"\" A container for multiple plot objects.\n\n    ``PlotContext`` objects are a source of confusion. Their purpose\n    is to collect together different top-level objects (e.g., ``Plot``\n    or layout widgets). The reason for this is that different plots may\n    need to share ranges or data sources between them. A ``PlotContext``\n    is a container in which such sharing can occur between the contained\n    objects.\n    \"\"\"\n\n    children = List(Instance(PlotObject), help=\"\"\"\n    A list of top level objects in this ``PlotContext`` container.\n    \"\"\")",
  "class PlotList(PlotContext):\n    # just like plot context, except plot context has special meaning\n    # everywhere, so plotlist is the generic one\n    pass",
  "class Plot(Widget):\n    \"\"\" Model representing a plot, containing glyphs, guides, annotations.\n\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        if \"tool_events\" not in kwargs:\n            kwargs[\"tool_events\"] = ToolEvents()\n        super(Plot, self).__init__(**kwargs)\n\n    def select(self, selector):\n        ''' Query this object and all of its references for objects that\n        match the given selector.\n\n        Args:\n            selector (JSON-like) :\n\n        Returns:\n            seq[PlotObject]\n\n        '''\n        return _list_attr_splat(find(self.references(), selector, {'plot': self}))\n\n    def row(self, row, gridplot):\n        ''' Return whether this plot is in a given row of a GridPlot.\n\n        Args:\n            row (int) : index of the row to test\n            gridplot (GridPlot) : the GridPlot to check\n\n        Returns:\n            bool\n\n        '''\n        return self in gridplot.row(row)\n\n    def column(self, col, gridplot):\n        ''' Return whether this plot is in a given column of a GridPlot.\n\n        Args:\n            col (int) : index of the column to test\n            gridplot (GridPlot) : the GridPlot to check\n\n        Returns:\n            bool\n\n        '''\n        return self in gridplot.column(col)\n\n    def add_layout(self, obj, place='center'):\n        ''' Adds an object to the plot in a specified place.\n\n        Args:\n            obj (Renderer) : the object to add to the Plot\n            place (str, optional) : where to add the object (default: 'center')\n                Valid places are: 'left', 'right', 'above', 'below', 'center'.\n\n        Returns:\n            None\n\n        '''\n        valid_places = ['left', 'right', 'above', 'below', 'center']\n        if place not in valid_places:\n            raise ValueError(\n                \"Invalid place '%s' specified. Valid place values are: %s\" % (place, nice_join(valid_places))\n            )\n\n        if hasattr(obj, 'plot'):\n            if obj.plot is not None:\n                 raise ValueError(\"object to be added already has 'plot' attribute set\")\n            obj.plot = self\n\n        self.renderers.append(obj)\n\n        if place is not 'center':\n            getattr(self, place).append(obj)\n\n    def add_tools(self, *tools):\n        ''' Adds an tools to the plot.\n\n        Args:\n            *tools (Tool) : the tools to add to the Plot\n\n        Returns:\n            None\n\n        '''\n        if not all(isinstance(tool, Tool) for tool in tools):\n            raise ValueError(\"All arguments to add_tool must be Tool subclasses.\")\n\n        for tool in tools:\n            if tool.plot is not None:\n                 raise ValueError(\"tool %s to be added already has 'plot' attribute set\" % tool)\n            tool.plot = self\n            self.tools.append(tool)\n\n    def add_glyph(self, source_or_glyph, glyph=None, **kw):\n        ''' Adds a glyph to the plot with associated data sources and ranges.\n\n        This function will take care of creating and configurinf a Glyph object,\n        and then add it to the plot's list of renderers.\n\n        Args:\n            source (DataSource) : a data source for the glyphs to all use\n            glyph (Glyph) : the glyph to add to the Plot\n\n        Keyword Arguments:\n            Any additional keyword arguments are passed on as-is to the\n            Glyph initializer.\n\n        Returns:\n            glyph : Glyph\n\n        '''\n        if glyph is not None:\n            source = source_or_glyph\n        else:\n            source, glyph = ColumnDataSource(), source_or_glyph\n\n        if not isinstance(source, DataSource):\n            raise ValueError(\"'source' argument to add_glyph() must be DataSource subclass\")\n\n        if not isinstance(glyph, Glyph):\n            raise ValueError(\"'glyph' argument to add_glyph() must be Glyph subclass\")\n\n        g = GlyphRenderer(data_source=source, glyph=glyph, **kw)\n        self.renderers.append(g)\n        return g\n\n    x_range = Instance(Range, help=\"\"\"\n    The (default) data range of the horizontal dimension of the plot.\n    \"\"\")\n\n    y_range = Instance(Range, help=\"\"\"\n    The (default) data range of the vertical dimension of the plot.\n    \"\"\")\n\n    x_mapper_type = Either(Auto, String, help=\"\"\"\n    What kind of mapper to use to convert x-coordinates in data space\n    into x-coordinates in screen space.\n\n    Typically this can be determined automatically, but this property\n    can be useful to, e.g., show datetime values as floating point\n    \"seconds since epoch\" instead of formatted dates.\n    \"\"\")\n\n    y_mapper_type = Either(Auto, String, help=\"\"\"\n    What kind of mapper to use to convert y-coordinates in data space\n    into y-coordinates in screen space.\n\n    Typically this can be determined automatically, but this property\n    can be useful to, e.g., show datetime values as floating point\n    \"seconds since epoch\" instead of formatted dates\n    \"\"\")\n\n    extra_x_ranges = Dict(String, Instance(Range1d), help=\"\"\"\n    Additional named ranges to make available for mapping x-coordinates.\n\n    This is useful for adding additional axes.\n    \"\"\")\n\n    extra_y_ranges = Dict(String, Instance(Range), help=\"\"\"\n    Additional named ranges to make available for mapping y-coordinates.\n\n    This is useful for adding additional axes.\n    \"\"\")\n\n    title = String('', help=\"\"\"\n    A title for the plot.\n    \"\"\")\n\n    title_props = Include(TextProps, help=\"\"\"\n    The %s for the plot title.\n    \"\"\")\n\n    outline_props = Include(LineProps, help=\"\"\"\n    The %s for the plot border outline.\n    \"\"\")\n\n    renderers = List(Instance(Renderer), help=\"\"\"\n    A list of all renderers for this plot, including guides and annotations\n    in addition to glyphs and markers.\n\n    This property can be manipulated by hand, but the ``add_glyph`` and\n    ``add_layout`` methods are recommended to help make sure all necessary\n    setup is performed.\n    \"\"\")\n\n    tools = List(Instance(Tool), help=\"\"\"\n    A list of tools to add to the plot.\n    \"\"\")\n\n    tool_events = Instance(ToolEvents, help=\"\"\"\n    A ToolEvents object to share and report tool events.\n    \"\"\")\n\n    left  = List(Instance(Renderer), help=\"\"\"\n    A list of renderers to occupy the area to the left of the plot.\n    \"\"\")\n\n    right = List(Instance(Renderer), help=\"\"\"\n    A list of renderers to occupy the area to the right of the plot.\n    \"\"\")\n\n    above = List(Instance(Renderer), help=\"\"\"\n    A list of renderers to occupy the area above of the plot.\n    \"\"\")\n\n    below = List(Instance(Renderer), help=\"\"\"\n    A list of renderers to occupy the area below of the plot.\n    \"\"\")\n\n    toolbar_location = Enum(Location, help=\"\"\"\n    Where the toolbar will be located. If set to None, no toolbar\n    will be attached to the plot.\n    \"\"\")\n\n    logo = Enum(\"normal\", \"grey\", help=\"\"\"\n    What version of the Bokeh logo to display on the toolbar. If\n    set to None, no logo will be displayed.\n    \"\"\")\n\n    plot_height = Int(600, help=\"\"\"\n    Total height of the entire plot (including any axes, titles,\n    border padding, etc.)\n\n    .. note::\n        This corresponds directly to the height of the HTML\n        canvas that will be used.\n\n    \"\"\")\n\n    plot_width = Int(600, help=\"\"\"\n    Total width of the entire plot (including any axes, titles,\n    border padding, etc.)\n\n    .. note::\n        This corresponds directly to the width of the HTML\n        canvas that will be used.\n\n    \"\"\")\n\n    background_fill = Color(\"white\", help=\"\"\"\n\n    \"\"\")\n\n    border_fill = Color(\"white\", help=\"\"\"\n\n    \"\"\")\n\n    min_border_top = Int(50, help=\"\"\"\n    Minimum size in pixels of the padding region above the top of the\n    central plot region.\n\n    .. note::\n        This is a *minimum*. The padding region may expand as needed to\n        accommodate titles or axes, etc.\n\n    \"\"\")\n\n    min_border_bottom = Int(50, help=\"\"\"\n    Minimum size in pixels of the padding region below the bottom of\n    the central plot region.\n\n    .. note::\n        This is a *minimum*. The padding region may expand as needed to\n        accommodate titles or axes, etc.\n\n    \"\"\")\n\n    min_border_left = Int(50, help=\"\"\"\n    Minimum size in pixels of the padding region to the left of\n    the central plot region.\n\n    .. note::\n        This is a *minimum*. The padding region may expand as needed to\n        accommodate titles or axes, etc.\n\n    \"\"\")\n\n    min_border_right = Int(50, help=\"\"\"\n    Minimum size in pixels of the padding region to the right of\n    the central plot region.\n\n    .. note::\n        This is a *minimum*. The padding region may expand as needed to\n        accommodate titles or axes, etc.\n\n    \"\"\")\n\n    min_border = Int(50, help=\"\"\"\n    A convenience property to set all all the ``min_X_border`` properties\n    to the same value. If an individual border property is explicitly set,\n    it will override ``min_border``.\n    \"\"\")\n\n    h_symmetry = Bool(True, help=\"\"\"\n    Whether the total horizontal padding on both sides of the plot will\n    be made equal (the left or right padding amount, whichever is larger).\n    \"\"\")\n\n    v_symmetry = Bool(False, help=\"\"\"\n    Whether the total vertical padding on both sides of the plot will\n    be made equal (the top or bottom padding amount, whichever is larger).\n    \"\"\")",
  "class GridPlot(Plot):\n    \"\"\" A 2D grid of plots rendered on separate canvases in an HTML table.\n\n    \"\"\"\n\n    children = List(List(Instance(Plot)), help=\"\"\"\n    An array of plots to display in a grid, given as a list of lists of\n    Plot objects. To leave a position in the grid empty, pass None for\n    that position in the ``children`` list.\n    \"\"\")\n\n    border_space = Int(0, help=\"\"\"\n    Distance (in pixels) between adjacent plots.\n    \"\"\")\n\n    def select(self, selector):\n        ''' Query this object and all of its references for objects that\n        match the given selector.\n\n        Args:\n            selector (JSON-like) :\n\n        Returns:\n            seq[PlotObject]\n\n        '''\n        return _list_attr_splat(find(self.references(), selector, {'gridplot': self}))\n\n    def column(self, col):\n        ''' Return a given column of plots from this GridPlot.\n\n        Args:\n            col (int) : index of the column to return\n\n        Returns:\n            seq[Plot] : column of plots\n\n        '''\n        try:\n            return [row[col] for row in self.children]\n        except:\n            return []\n\n    def row(self, row):\n        ''' Return a given row of plots from this GridPlot.\n\n        Args:\n            rwo (int) : index of the row to return\n\n        Returns:\n            seq[Plot] : row of plots\n\n        '''\n        try:\n            return self.children[row]\n        except:\n            return []",
  "def __setattr__(self, attr, value):\n        for x in self:\n            setattr(x, attr, value)",
  "def __init__(self, **kwargs):\n        if \"tool_events\" not in kwargs:\n            kwargs[\"tool_events\"] = ToolEvents()\n        super(Plot, self).__init__(**kwargs)",
  "def select(self, selector):\n        ''' Query this object and all of its references for objects that\n        match the given selector.\n\n        Args:\n            selector (JSON-like) :\n\n        Returns:\n            seq[PlotObject]\n\n        '''\n        return _list_attr_splat(find(self.references(), selector, {'plot': self}))",
  "def row(self, row, gridplot):\n        ''' Return whether this plot is in a given row of a GridPlot.\n\n        Args:\n            row (int) : index of the row to test\n            gridplot (GridPlot) : the GridPlot to check\n\n        Returns:\n            bool\n\n        '''\n        return self in gridplot.row(row)",
  "def column(self, col, gridplot):\n        ''' Return whether this plot is in a given column of a GridPlot.\n\n        Args:\n            col (int) : index of the column to test\n            gridplot (GridPlot) : the GridPlot to check\n\n        Returns:\n            bool\n\n        '''\n        return self in gridplot.column(col)",
  "def add_layout(self, obj, place='center'):\n        ''' Adds an object to the plot in a specified place.\n\n        Args:\n            obj (Renderer) : the object to add to the Plot\n            place (str, optional) : where to add the object (default: 'center')\n                Valid places are: 'left', 'right', 'above', 'below', 'center'.\n\n        Returns:\n            None\n\n        '''\n        valid_places = ['left', 'right', 'above', 'below', 'center']\n        if place not in valid_places:\n            raise ValueError(\n                \"Invalid place '%s' specified. Valid place values are: %s\" % (place, nice_join(valid_places))\n            )\n\n        if hasattr(obj, 'plot'):\n            if obj.plot is not None:\n                 raise ValueError(\"object to be added already has 'plot' attribute set\")\n            obj.plot = self\n\n        self.renderers.append(obj)\n\n        if place is not 'center':\n            getattr(self, place).append(obj)",
  "def add_tools(self, *tools):\n        ''' Adds an tools to the plot.\n\n        Args:\n            *tools (Tool) : the tools to add to the Plot\n\n        Returns:\n            None\n\n        '''\n        if not all(isinstance(tool, Tool) for tool in tools):\n            raise ValueError(\"All arguments to add_tool must be Tool subclasses.\")\n\n        for tool in tools:\n            if tool.plot is not None:\n                 raise ValueError(\"tool %s to be added already has 'plot' attribute set\" % tool)\n            tool.plot = self\n            self.tools.append(tool)",
  "def add_glyph(self, source_or_glyph, glyph=None, **kw):\n        ''' Adds a glyph to the plot with associated data sources and ranges.\n\n        This function will take care of creating and configurinf a Glyph object,\n        and then add it to the plot's list of renderers.\n\n        Args:\n            source (DataSource) : a data source for the glyphs to all use\n            glyph (Glyph) : the glyph to add to the Plot\n\n        Keyword Arguments:\n            Any additional keyword arguments are passed on as-is to the\n            Glyph initializer.\n\n        Returns:\n            glyph : Glyph\n\n        '''\n        if glyph is not None:\n            source = source_or_glyph\n        else:\n            source, glyph = ColumnDataSource(), source_or_glyph\n\n        if not isinstance(source, DataSource):\n            raise ValueError(\"'source' argument to add_glyph() must be DataSource subclass\")\n\n        if not isinstance(glyph, Glyph):\n            raise ValueError(\"'glyph' argument to add_glyph() must be Glyph subclass\")\n\n        g = GlyphRenderer(data_source=source, glyph=glyph, **kw)\n        self.renderers.append(g)\n        return g",
  "def select(self, selector):\n        ''' Query this object and all of its references for objects that\n        match the given selector.\n\n        Args:\n            selector (JSON-like) :\n\n        Returns:\n            seq[PlotObject]\n\n        '''\n        return _list_attr_splat(find(self.references(), selector, {'gridplot': self}))",
  "def column(self, col):\n        ''' Return a given column of plots from this GridPlot.\n\n        Args:\n            col (int) : index of the column to return\n\n        Returns:\n            seq[Plot] : column of plots\n\n        '''\n        try:\n            return [row[col] for row in self.children]\n        except:\n            return []",
  "def row(self, row):\n        ''' Return a given row of plots from this GridPlot.\n\n        Args:\n            rwo (int) : index of the row to return\n\n        Returns:\n            seq[Plot] : row of plots\n\n        '''\n        try:\n            return self.children[row]\n        except:\n            return []",
  "class ColorMapper(PlotObject):\n    \"\"\" Base class for color mapper types. `ColorMapper`` is not\n    generally useful to instantiate on its own.\n\n    \"\"\"",
  "class LinearColorMapper(ColorMapper):\n    \"\"\" Map numbers in a range [*low*, *high*] linearly into a\n    sequence of colors (a palette).\n\n    For example, if the range is [0, 99] and the palette is\n    ``['red', 'green', 'blue']``, the values would be mapped as\n    follows::\n\n             x < 0  : 'red'     # values < low are clamped\n        0 >= x < 33 : 'red'\n       33 >= x < 66 : 'green'\n       66 >= x < 99 : 'blue'\n       99 >= x      : 'blue'    # values > high are clamped\n\n    \"\"\"\n\n    palette = Seq(Color, help=\"\"\"\n    A sequence of colors to use as the target palette for mapping.\n\n    This property can also be set as a ``String``, to the name of\n    any of the palettes shown in :ref:`bokeh_dot_palettes`.\n    \"\"\").accepts(Enum(Palette), lambda pal: getattr(palettes, pal))\n\n    low = Float(help=\"\"\"\n    The minimum value of the range to map into the palette. Values below\n    this are clamped to ``low``.\n    \"\"\")\n\n    high = Float(help=\"\"\"\n    The maximum value of the range to map into the palette. Values above\n    this are clamped to ``high``.\n    \"\"\")\n\n    # TODO: (jc) what is the color code for transparent?\n    # TODO: (bev) better docstring\n    reserve_color = Color(\"#ffffff\", help=\"\"\"\n    Used by Abstract Rendering.\n    \"\"\")\n\n    # TODO: (bev) better docstring\n    reserve_val = Float(default=None, help=\"\"\"\n    Used by Abstract Rendering.\n    \"\"\")\n\n    def __init__(self, palette=None, **kwargs):\n        if palette is not None: kwargs['palette'] = palette\n        super(LinearColorMapper, self).__init__(**kwargs)",
  "def __init__(self, palette=None, **kwargs):\n        if palette is not None: kwargs['palette'] = palette\n        super(LinearColorMapper, self).__init__(**kwargs)",
  "class Grid(GuideRenderer):\n    \"\"\" Display horizontal or vertical grid lines at locations\n    given by a supplied ``Ticker``.\n\n    \"\"\"\n    dimension = Int(0, help=\"\"\"\n    Which dimension the Axis Grid lines will intersect. The\n    x-axis is dimension 0 (vertical Grid lines) and the y-axis\n    is dimension 1 (horizontal Grid lines).\n    \"\"\")\n\n    bounds = Either(Auto, Tuple(Float, Float), help=\"\"\"\n    Bounds for the rendered grid lines. If unset, the grid\n    lines will span the entire plot in the given dimension.\n    \"\"\")\n\n    # Note: we must allow the possibility of setting both\n    # range names be cause if a grid line is \"traced\" along\n    # a path, ranges in both dimensions will matter.\n\n    x_range_name = String('default', help=\"\"\"\n    A particular (named) x-range to use for computing screen\n    locations when rendering a grid on the plot. If unset, use the\n    default x-range.\n    \"\"\")\n\n    y_range_name = String('default', help=\"\"\"\n    A particular (named) y-range to use for computing screen\n    locations when rendering a grid on the plot. If unset, use the\n    default y-range.\n    \"\"\")\n\n    ticker = Instance(Ticker, help=\"\"\"\n    The Ticker to use for computing locations for the Grid lines.\n    \"\"\")\n\n    grid_props = Include(LineProps, help=\"\"\"\n    The %s of the Grid lines.\n    \"\"\")\n\n    band_props = Include(FillProps, help=\"\"\"\n    The %s of alternating bands between Grid lines.\n    \"\"\")",
  "class Glyph(PlotObject):\n    \"\"\" Base class for all glyphs/marks/geoms/whatever-you-call-'em in Bokeh.\n\n    \"\"\"\n\n    visible = Bool(help=\"\"\"\n    Whether the glyph should render or not.\n    \"\"\")",
  "class AnnularWedge(Glyph):\n    \"\"\" Render annular wedges.\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/AnnularWedge.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/AnnularWedge.py``\n\n    \"\"\"\n\n    x = DataSpec(\"x\", help=\"\"\"\n    The x-coordinates of the center of the annular wedges.\n    \"\"\")\n\n    y = DataSpec(\"y\", help=\"\"\"\n    The y-coordinates of the center of the annular wedges.\n    \"\"\")\n\n    # TODO: (bev) should default to \"inner_radius\" field?\n    inner_radius = DataSpec(min_value=0, help=\"\"\"\n    The inner radii of the annular wedges.\n    \"\"\")\n\n    # TODO: (bev) should default to \"outer_radius\" field?\n    outer_radius = DataSpec(min_value=0, help=\"\"\"\n    The outer radii of the annular wedges.\n    \"\"\")\n\n    start_angle = DataSpec(\"start_angle\", help=\"\"\"\n    The angles to start the annular wedges, in radians, as measured from\n    the horizontal.\n    \"\"\")\n\n    end_angle = DataSpec(\"end_angle\", help=\"\"\"\n    The angles to end the annular wedges, in radians, as measured from\n    the horizontal.\n    \"\"\")\n\n    direction = Enum(Direction, help=\"\"\"\n    Which direction to stroke between the start and end angles.\n    \"\"\")\n\n    line_props = Include(LineProps, use_prefix=False, help=\"\"\"\n    The %s values for the annular wedges.\n    \"\"\")\n\n    fill_props = Include(FillProps, use_prefix=False, help=\"\"\"\n    The %s values for the annular wedges.\n    \"\"\")",
  "class Annulus(Glyph):\n    \"\"\" Render annuli.\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/Annulus.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/Annulus.py``\n\n    \"\"\"\n\n    x = DataSpec(\"x\", help=\"\"\"\n    The x-coordinates of the center of the annuli.\n    \"\"\")\n\n    y = DataSpec(\"y\", help=\"\"\"\n    The y-coordinates of the center of the annuli.\n    \"\"\")\n\n    # TODO: (bev) should default to \"inner_radius\" field?\n    inner_radius = DataSpec(min_value=0, help=\"\"\"\n    The inner radii of the annuli.\n    \"\"\")\n\n    # TODO: (bev) should default to \"outer_radius\" field?\n    outer_radius = DataSpec(min_value=0, help=\"\"\"\n    The outer radii of the annuli.\n    \"\"\")\n\n    line_props = Include(LineProps, use_prefix=False, help=\"\"\"\n    The %s values for the annuli.\n    \"\"\")\n\n    fill_props = Include(FillProps, use_prefix=False, help=\"\"\"\n    The %s values for the annuli.\n    \"\"\")",
  "class Arc(Glyph):\n    \"\"\" Render arcs.\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/Arc.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/Arc.py``\n\n    \"\"\"\n\n    x = DataSpec(\"x\", help=\"\"\"\n    The x-coordinates of the center of the arcs.\n    \"\"\")\n\n    y = DataSpec(\"y\", help=\"\"\"\n    The y-coordinates of the center of the arcs.\n    \"\"\")\n\n    # TODO: (bev) should default to \"radius\" field?\n    radius = DataSpec(min_value=0, help=\"\"\"\n    Radius of the arc.\n    \"\"\")\n\n    start_angle = DataSpec(\"start_angle\", help=\"\"\"\n    The angles to start the arcs, in radians, as measured from the horizontal.\n    \"\"\")\n\n    end_angle = DataSpec(\"end_angle\", help=\"\"\"\n    The angles to end the arcs, in radians, as measured from the horizontal.\n    \"\"\")\n\n    direction = Enum(Direction, help=\"\"\"\n    Which direction to stroke between the start and end angles.\n    \"\"\")\n    line_props = Include(LineProps, use_prefix=False, help=\"\"\"\n    The %s values for the arcs.\n    \"\"\")",
  "class Bezier(Glyph):\n    u\"\"\" Render B\u00e9zier curves.\n\n    For more information consult the `Wikipedia article for B\u00e9zier curve`_.\n\n    .. _Wikipedia article for B\u00e9zier curve: http://en.wikipedia.org/wiki/B\u00e9zier_curve\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/Bezier.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/Bezier.py``\n\n    \"\"\"\n\n    x0 = DataSpec(\"x0\", help=\"\"\"\n    The x-coordinates of the starting points.\n    \"\"\")\n\n    y0 = DataSpec(\"y0\", help=\"\"\"\n    The y-coordinates of the starting points.\n    \"\"\")\n\n    x1 = DataSpec(\"x1\", help=\"\"\"\n    The x-coordinates of the ending points.\n    \"\"\")\n\n    y1 = DataSpec(\"y1\", help=\"\"\"\n    The y-coordinates of the ending points.\n    \"\"\")\n\n    cx0 = DataSpec(\"cx0\", help=\"\"\"\n    The x-coordinates of first control points.\n    \"\"\")\n\n    cy0 = DataSpec(\"cy0\", help=\"\"\"\n    The y-coordinates of first control points.\n    \"\"\")\n\n    cx1 = DataSpec(\"cx1\", help=\"\"\"\n    The x-coordinates of second control points.\n    \"\"\")\n\n    cy1 = DataSpec(\"cy1\", help=\"\"\"\n    The y-coordinates of second control points.\n    \"\"\")\n\n    line_props = Include(LineProps, use_prefix=False, help=u\"\"\"\n    The %s values for the B\u00e9zier curves.\n    \"\"\")",
  "class Gear(Glyph):\n    \"\"\" Render gears.\n\n    The details and nomenclature concerning gear construction can\n    be quite involved. For more information, consult the `Wikipedia\n    article for Gear`_.\n\n    .. _Wikipedia article for Gear: http://en.wikipedia.org/wiki/Gear\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/Gear.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/Gear.py``\n\n    \"\"\"\n\n    x = DataSpec(\"x\", help=\"\"\"\n    The x-coordinates of the center of the gears.\n    \"\"\")\n\n    y = DataSpec(\"y\", help=\"\"\"\n    The y-coordinates of the center of the gears.\n    \"\"\")\n\n    angle = DataSpec(default=0, help=\"\"\"\n    The angle the gears are rotated from horizontal. [rad]\n    \"\"\")\n\n    module = DataSpec(\"module\", help=\"\"\"\n    A scaling factor, given by::\n\n        m = p / pi\n\n    where *p* is the circular pitch, defined as the distance from one\n    face of a tooth to the corresponding face of an adjacent tooth on\n    the same gear, measured along the pitch circle. [float]\n    \"\"\")\n\n    teeth = DataSpec(\"teeth\", help=\"\"\"\n    How many teeth the gears have. [int]\n    \"\"\")\n\n    pressure_angle = DataSpec(default=20, help= \"\"\"\n    The complement of the angle between the direction that the teeth\n    exert force on each other, and the line joining the centers of the\n    two gears. [deg]\n    \"\"\")\n\n    # TODO: (bev) evidently missing a test for default value\n    shaft_size = DataSpec(default=0.3, help=\"\"\"\n    The central gear shaft size as a percentage of the overall gear\n    size. [float]\n    \"\"\")\n\n    # TODO: (bev) evidently missing a test for default value\n    internal = DataSpec(default=False, help=\"\"\"\n    Whether the gear teeth are internal. [bool]\n    \"\"\")\n\n    line_props = Include(LineProps, use_prefix=False, help=\"\"\"\n    The %s values for the gears.\n    \"\"\")\n\n    fill_props = Include(FillProps, use_prefix=False, help=\"\"\"\n    The %s values for the gears.\n    \"\"\")",
  "class Image(Glyph):\n    \"\"\" Render images given as scalar data together with a color\n    mapper.\n\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        if 'palette' in kwargs and 'color_mapper' in kwargs:\n            raise ValueError(\"only one of 'palette' and 'color_mapper' may be specified\")\n\n        palette = kwargs.pop('palette', None)\n        if palette is not None:\n            mapper = LinearColorMapper(palette)\n\n            reserve_val = kwargs.pop('reserve_val', None)\n            if reserve_val is not None:\n                mapper.reserve_val = reserve_val\n\n            reserve_color = kwargs.pop('reserve_color', None)\n            if reserve_color is not None:\n                mapper.reserve_color = reserve_color\n\n            kwargs['color_mapper'] = mapper\n\n        super(Image, self).__init__(**kwargs)\n\n    image = DataSpec(\"image\", help=\"\"\"\n    The arrays of scalar data for the images to be colormapped.\n    \"\"\")\n\n    x = DataSpec(\"x\", help=\"\"\"\n    The x-coordinates to locate the image anchors.\n    \"\"\")\n\n    y = DataSpec(\"y\", help=\"\"\"\n    The y-coordinates to locate the image anchors.\n    \"\"\")\n\n    dw = DataSpec(\"dw\", help=\"\"\"\n    The widths of the plot regions that the images will occupy.\n\n    .. note::\n        This is not the number of pixels that an image is wide.\n        That number is fixed by the image itself.\n\n    \"\"\")\n\n    dh = DataSpec(\"dh\", help=\"\"\"\n    The height of the plot region that the image will occupy.\n\n    .. note::\n        This is not the number of pixels that an image is tall.\n        That number is fixed by the image itself.\n\n    \"\"\")\n\n    dilate = Bool(False, help=\"\"\"\n    Whether to always round fractional pixel locations in such a way\n    as to make the images bigger.\n\n    This setting may be useful if pixel rounding errors are causing\n    images to have a gap between them, when they should appear flush.\n\n    \"\"\")\n\n    color_mapper = Instance(LinearColorMapper, help=\"\"\"\n    A ``ColorMapper`` to use to map the scalar data from ``image``\n    into RGBA values for display.\n\n    .. note::\n        The color mapping step happens on the client.\n\n    \"\"\")",
  "class ImageRGBA(Glyph):\n    \"\"\" Render images given as RGBA data.\n\n    \"\"\"\n\n    image = DataSpec(\"image\", help=\"\"\"\n    The arrays of RGBA data for the images.\n    \"\"\")\n\n    x = DataSpec(\"x\", help=\"\"\"\n    The x-coordinates to locate the image anchors.\n    \"\"\")\n\n    y = DataSpec(\"y\", help=\"\"\"\n    The y-coordinates to locate the image anchors.\n    \"\"\")\n\n    rows = DataSpec(\"rows\", help=\"\"\"\n    The numbers of rows in the images\n    \"\"\")\n\n    cols = DataSpec(\"cols\", help=\"\"\"\n    The numbers of columns in the images\n    \"\"\")\n\n    dw = DataSpec(\"dw\", help=\"\"\"\n    The widths of the plot regions that the images will occupy.\n\n    .. note::\n        This is not the number of pixels that an image is wide.\n        That number is fixed by the image itself.\n\n    \"\"\")\n\n    dh = DataSpec(\"dh\", help=\"\"\"\n    The height of the plot region that the image will occupy.\n\n    .. note::\n        This is not the number of pixels that an image is tall.\n        That number is fixed by the image itself.\n\n    \"\"\")\n\n    dilate = Bool(False, help=\"\"\"\n    Whether to always round fractional pixel locations in such a way\n    as to make the images bigger.\n\n    This setting may be useful if pixel rounding errors are causing\n    images to have a gap between them, when they should appear flush.\n    \"\"\")",
  "class ImageURL(Glyph):\n    \"\"\" Render images loaded from given URLs.\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/ImageURL.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/ImageURL.py``\n\n    \"\"\"\n\n    url = DataSpec(\"url\", help=\"\"\"\n    The URLs to retrieve images from.\n\n    .. note::\n        The actual retrieving and loading of the images happens on\n        the client.\n\n    \"\"\")\n\n    x = DataSpec(\"x\", help=\"\"\"\n    The x-coordinates to locate the image anchors.\n    \"\"\")\n\n    y = DataSpec(\"y\", help=\"\"\"\n    The y-coordinates to locate the image anchors.\n    \"\"\")\n\n    # TODO: (bev) rename to \"dw\" for consistency\n    w = DataSpec(\"w\", help=\"\"\"\n    The widths of the plot regions that the images will occupy.\n\n    .. note::\n        This is not the number of pixels that an image is wide.\n        That number is fixed by the image itself.\n\n    .. note::\n        This may be renamed to \"dw\" in the future.\n\n    \"\"\")\n\n    # TODO: (bev) rename to \"dh\" for consistency\n    h = DataSpec(\"h\", help=\"\"\"\n    The height of the plot region that the image will occupy.\n\n    .. note::\n        This is not the number of pixels that an image is tall.\n        That number is fixed by the image itself.\n\n    .. note::\n        This may be renamed to \"dh\" in the future.\n\n    \"\"\")\n\n    angle = DataSpec(default=0, help=\"\"\"\n    The angles to rotate the images, in radians as measured from the\n    horizontal.\n    \"\"\")\n\n    dilate = Bool(False, help=\"\"\"\n    Whether to always round fractional pixel locations in such a way\n    as to make the images bigger.\n\n    This setting may be useful if pixel rounding errors are causing\n    images to have a gap between them, when they should appear flush.\n    \"\"\")\n\n    anchor = Enum(Anchor, help=\"\"\"\n    What position of the image should be anchored at the `x`, `y`\n    coordinates.\n    \"\"\")",
  "class Line(Glyph):\n    \"\"\" Render a single line.\n\n    .. note::\n        The ``Line`` glyph is different from most other glyphs in that\n        the vector of values only produces one glyph on the Plot.\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/Line.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/Line.py``\n\n    \"\"\"\n\n    x = DataSpec(\"x\", help=\"\"\"\n    The x-coordinates for the points of the line.\n    \"\"\")\n\n    y = DataSpec(\"y\", help=\"\"\"\n    The y-coordinates for the points of the line.\n    \"\"\")\n\n    line_props = Include(LineProps, use_prefix=False, help=\"\"\"\n    The %s values for the line.\n    \"\"\")",
  "class MultiLine(Glyph):\n    \"\"\" Render several lines.\n\n    .. note::\n        The data for the ``MultiLine`` glyph is different in that the\n        vector of values is not a vector of scalars. Rather, it is a\n        \"list of lists\".\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/MultiLine.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/MultiLine.py``\n\n    \"\"\"\n    xs = DataSpec(\"xs\", help=\"\"\"\n    The x-coordinates for all the lines, given as a \"list of lists\".\n    \"\"\")\n\n    ys = DataSpec(\"ys\", help=\"\"\"\n    The x-coordinates for all the lines, given as a \"list of lists\".\n    \"\"\")\n\n    line_props = Include(LineProps, use_prefix=False, help=\"\"\"\n    The %s values for the lines.\n    \"\"\")",
  "class Oval(Glyph):\n    u\"\"\" Render ovals.\n\n    .. note::\n        This glyph renders ovals using B\u00e9zier curves, which are similar,\n        but not identical to ellipses.\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/Oval.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/Oval.py``\n\n    \"\"\"\n\n    x = DataSpec(\"x\", help=\"\"\"\n    The x-coordinates of the centers of the ovals.\n    \"\"\")\n\n    y = DataSpec(\"y\", help=\"\"\"\n    The y-coordinates of the centers of the ovals.\n    \"\"\")\n\n    width = DataSpec(\"width\", help=\"\"\"\n    The overall widths of each oval.\n    \"\"\")\n\n    height = DataSpec(\"height\", help=\"\"\"\n    The overall height of each oval.\n    \"\"\")\n\n    angle = DataSpec(\"angle\", help=\"\"\"\n    The angle the ovals are rotated from horizontal. [rad]\n    \"\"\")\n\n    line_props = Include(LineProps, use_prefix=False, help=\"\"\"\n    The %s values for the ovals.\n    \"\"\")\n\n    fill_props = Include(FillProps, use_prefix=False, help=\"\"\"\n    The %s values for the ovals.\n    \"\"\")",
  "class Patch(Glyph):\n    \"\"\" Render a single patch.\n\n    .. note::\n        The ``Patch`` glyph is different from most other glyphs in that\n        the vector of values only produces one glyph on the Plot.\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/Patch.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/Patch.py``\n\n    \"\"\"\n\n    x = DataSpec(\"x\", help=\"\"\"\n    The x-coordinates for the points of the patch.\n\n    .. note::\n        A patch may comprise multiple polygons. In this case the\n        x-coordinates for each polygon should be separated by NaN\n        values in the sequence.\n\n    \"\"\")\n\n    y = DataSpec(\"y\", help=\"\"\"\n    The y-coordinates for the points of the patch.\n\n    .. note::\n        A patch may comprise multiple polygons. In this case the\n        y-coordinates for each polygon should be separated by NaN\n        values in the sequence.\n\n    \"\"\")\n\n    line_props = Include(LineProps, use_prefix=False, help=\"\"\"\n    The %s values for the patch.\n    \"\"\")\n\n    fill_props = Include(FillProps, use_prefix=False, help=\"\"\"\n    The %s values for the patch.\n    \"\"\")",
  "class Patches(Glyph):\n    \"\"\" Render several patches.\n\n    .. note::\n        The data for the ``Patches`` glyph is different in that the\n        vector of values is not a vector of scalars. Rather, it is a\n        \"list of lists\".\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/Patches.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/Patches.py``\n\n    \"\"\"\n\n    xs = DataSpec(\"xs\", help=\"\"\"\n    The x-coordinates for all the patches, given as a \"list of lists\".\n\n    .. note::\n        Individual patches may comprise multiple polygons. In this case\n        the x-coordinates for each polygon should be separated by NaN\n        values in the sublists.\n\n    \"\"\")\n\n    ys = DataSpec(\"ys\", help=\"\"\"\n    The y-coordinates for all the patches, given as a \"list of lists\".\n\n    .. note::\n        Individual patches may comprise multiple polygons. In this case\n        the y-coordinates for each polygon should be separated by NaN\n        values in the sublists.\n\n    \"\"\")\n\n    line_props = Include(LineProps, use_prefix=False, help=\"\"\"\n    The %s values for the patches.\n    \"\"\")\n\n    fill_props = Include(FillProps, use_prefix=False, help=\"\"\"\n    The %s values for the patches.\n    \"\"\")",
  "class Quad(Glyph):\n    \"\"\" Render axis-aligned quads.\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/Quad.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/Quad.py``\n\n    \"\"\"\n\n    left = DataSpec(\"left\", help=\"\"\"\n    The x-coordinates of the left edges.\n    \"\"\")\n\n    right = DataSpec(\"right\", help=\"\"\"\n    The x-coordinates of the right edges.\n    \"\"\")\n\n    bottom = DataSpec(\"bottom\", help=\"\"\"\n    The y-coordinates of the bottom edges.\n    \"\"\")\n\n    top = DataSpec(\"top\", help=\"\"\"\n    The y-coordinates of the top edges.\n    \"\"\")\n\n    line_props = Include(LineProps, use_prefix=False, help=\"\"\"\n    The %s values for the quads.\n    \"\"\")\n\n    fill_props = Include(FillProps, use_prefix=False, help=\"\"\"\n    The %s values for the quads.\n    \"\"\")",
  "class Quadratic(Glyph):\n    \"\"\" Render parabolas.\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/Quadratic.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/Quadratic.py``\n\n    \"\"\"\n\n    x0 = DataSpec(\"x0\", help=\"\"\"\n    The x-coordinates of the starting points.\n    \"\"\")\n\n    y0 = DataSpec(\"y0\", help=\"\"\"\n    The y-coordinates of the starting points.\n    \"\"\")\n\n    x1 = DataSpec(\"x1\", help=\"\"\"\n    The x-coordinates of the ending points.\n    \"\"\")\n\n    y1 = DataSpec(\"y1\", help=\"\"\"\n    The y-coordinates of the ending points.\n    \"\"\")\n\n    cx = DataSpec(\"cx\", help=\"\"\"\n    The x-coordinates of the control points.\n    \"\"\")\n\n    cy = DataSpec(\"cy\", help=\"\"\"\n    The y-coordinates of the control points.\n    \"\"\")\n\n    line_props = Include(LineProps, use_prefix=False, help=\"\"\"\n    The %s values for the parabolas.\n    \"\"\")",
  "class Ray(Glyph):\n    \"\"\" Render rays.\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/Ray.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/Ray.py``\n\n    \"\"\"\n\n    x = DataSpec(\"x\", help=\"\"\"\n    The x-coordinates to start the rays.\n    \"\"\")\n\n    y = DataSpec(\"y\", help=\"\"\"\n    The y-coordinates to start the rays.\n    \"\"\")\n\n    angle = DataSpec(\"angle\", help=\"\"\"\n    The angles in randian to extend the rays, as measured from the\n    horizontal.\n    \"\"\")\n\n    # TODO: (bev) should default to \"length\" field?\n    length = DataSpec(units=\"screen\", help=\"\"\"\n    The length to extend the ray. Note that this ``length`` defaults\n    to screen units.\n    \"\"\")\n\n    line_props = Include(LineProps, use_prefix=False, help=\"\"\"\n    The %s values for the rays.\n    \"\"\")",
  "class Rect(Glyph):\n    \"\"\" Render rectangles.\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/Rect.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/Rect.py``\n\n    \"\"\"\n\n    x = DataSpec(\"x\", help=\"\"\"\n    The x-coordinates of the centers of the rectangles.\n    \"\"\")\n\n    y = DataSpec(\"y\", help=\"\"\"\n    The y-coordinates of the centers of the rectangles.\n    \"\"\")\n\n    width = DataSpec(\"width\", help=\"\"\"\n    The overall widths of the rectangles.\n    \"\"\")\n\n    height = DataSpec(\"height\", help=\"\"\"\n    The overall heights of the rectangles.\n    \"\"\")\n\n    angle = DataSpec(\"angle\", help=\"\"\"\n    The angles to rotate the rectangles, in radians, as measured from\n    the horizontal.\n    \"\"\")\n\n    dilate = Bool(False, help=\"\"\"\n    Whether to always round fractional pixel locations in such a way\n    as to make the rectangles bigger.\n\n    This setting may be useful if pixel rounding errors are causing\n    rectangles to have a gap between them, when they should appear\n    flush.\n    \"\"\")\n\n    line_props = Include(LineProps, use_prefix=False, help=\"\"\"\n    The %s values for the rectangles.\n    \"\"\")\n\n    fill_props = Include(FillProps, use_prefix=False, help=\"\"\"\n    The %s values for the rectangles.\n    \"\"\")",
  "class Segment(Glyph):\n    \"\"\" Render segments.\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/Segment.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/Segment.py``\n\n    \"\"\"\n\n    x0 = DataSpec(\"x0\", help=\"\"\"\n    The x-coordinates of the starting points.\n    \"\"\")\n\n    y0 = DataSpec(\"y0\", help=\"\"\"\n    The y-coordinates of the starting points.\n    \"\"\")\n\n    x1 = DataSpec(\"x1\", help=\"\"\"\n    The x-coordinates of the ending points.\n    \"\"\")\n\n    y1 = DataSpec(\"y1\", help=\"\"\"\n    The y-coordinates of the ending points.\n    \"\"\")\n\n    line_props = Include(LineProps, use_prefix=False, help=\"\"\"\n    The %s values for the segments.\n    \"\"\")",
  "class Text(Glyph):\n    \"\"\" Render text.\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/Text.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/Text.py``\n\n    \"\"\"\n\n    x = DataSpec(\"x\", help=\"\"\"\n    The x-coordinates to locate the text anchors.\n    \"\"\")\n\n    y = DataSpec(\"y\", help=\"\"\"\n    The y-coordinates to locate the text anchors.\n    \"\"\")\n\n    text = DataSpec(\"text\", help=\"\"\"\n    The text values to render.\n    \"\"\")\n\n    angle = DataSpec(default=0, help=\"\"\"\n    The angles to rotate the text, in radians,, as measured from the horizontal.\n    \"\"\")\n\n    x_offset = DataSpec(\"x_offset\", units=\"screen\", default=0, help=\"\"\"\n    Offset values to apply to the x-coordinates.\n\n    This is useful, for instance, if it is desired to \"float\" text a fixed\n    distance in screen units from a given data position.\n    \"\"\")\n\n    y_offset = DataSpec(\"y_offset\", units=\"screen\", default=0, help=\"\"\"\n    Offset values to apply to the y-coordinates.\n\n    This is useful, for instance, if it is desired to \"float\" text a fixed\n    distance in screen units from a given data position.\n    \"\"\")\n\n    text_props = Include(TextProps, use_prefix=False, help=\"\"\"\n    The %s values for the text.\n    \"\"\")",
  "class Wedge(Glyph):\n    \"\"\" Render wedges.\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/Wedge.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/Wedge.py``\n\n    \"\"\"\n\n    x = DataSpec(\"x\", help=\"\"\"\n    The x-coordinates of the points of the wedges.\n    \"\"\")\n\n    y = DataSpec(\"y\", help=\"\"\"\n    The y-coordinates of the points of the wedges.\n    \"\"\")\n\n    # TODO: (bev) should default to \"radius\" field?\n    radius = DataSpec(min_value=0, help=\"\"\"\n    Radii of the wedges.\n    \"\"\")\n\n    start_angle = DataSpec(\"start_angle\", help=\"\"\"\n    The angles to start the wedges, in radians, as measured from the horizontal.\n    \"\"\")\n\n    end_angle = DataSpec(\"end_angle\", help=\"\"\"\n    The angles to end the wedges, in radians as measured from the horizontal.\n    \"\"\")\n\n    direction = Enum(Direction, help=\"\"\"\n    Which direction to stroke between the start and end angles.\n    \"\"\")\n\n    line_props = Include(LineProps, use_prefix=False, help=\"\"\"\n    The %s values for the wedges.\n    \"\"\")\n\n    fill_props = Include(FillProps, use_prefix=False, help=\"\"\"\n    The %s values for the wedges.\n    \"\"\")",
  "def __init__(self, **kwargs):\n        if 'palette' in kwargs and 'color_mapper' in kwargs:\n            raise ValueError(\"only one of 'palette' and 'color_mapper' may be specified\")\n\n        palette = kwargs.pop('palette', None)\n        if palette is not None:\n            mapper = LinearColorMapper(palette)\n\n            reserve_val = kwargs.pop('reserve_val', None)\n            if reserve_val is not None:\n                mapper.reserve_val = reserve_val\n\n            reserve_color = kwargs.pop('reserve_color', None)\n            if reserve_color is not None:\n                mapper.reserve_color = reserve_color\n\n            kwargs['color_mapper'] = mapper\n\n        super(Image, self).__init__(**kwargs)",
  "class Range(PlotObject):\n    \"\"\" A base class for all range types. ``Range`` is not generally\n    useful to instantiate on its own.\n\n    \"\"\"",
  "class Range1d(Range):\n    \"\"\" A fixed, closed range [start, end] in a continuous scalar\n    dimension.\n\n    In addition to supplying ``start`` and ``end`` keyword arguments\n    to the ``Range1d`` initializer, you can also instantiate with\n    the convenience syntax::\n\n        Range(0, 10) # equivalent to Range(start=0, end=10)\n\n    \"\"\"\n\n    start = Either(Float, Datetime, Int, help=\"\"\"\n    The start of the range.\n    \"\"\")\n\n    end = Either(Float, Datetime, Int, help=\"\"\"\n    The end of the range.\n    \"\"\")\n\n    def __init__(self, *args, **kwargs):\n        if args and ('start' in kwargs or 'end' in kwargs):\n            raise ValueError(\"'start' and 'end' keywords cannot be used with positional arguments\")\n        elif args and len(args) != 2:\n            raise ValueError('Only Range1d(start, end) acceptable when using positional arguments')\n        elif args:\n            kwargs['start'] = args[0]\n            kwargs['end'] = args[1]\n        super(Range1d, self).__init__(**kwargs)",
  "class DataRange(Range):\n    \"\"\" A base class for all data range types. ``DataRange`` is not\n    generally useful to instantiate on its own.\n\n    \"\"\"\n\n    sources = List(Instance(ColumnsRef), help=\"\"\"\n    A list of data source columns to compute the envelope of.\n    \"\"\")",
  "class DataRange1d(DataRange):\n    \"\"\" An auto-fitting range in a continuous scalar dimension.\n\n    \"\"\"\n\n    rangepadding = Float(0.1, help=\"\"\"\n    A percentage of the total range size to add as padding to\n    the range start and end.\n    \"\"\")\n\n    start = Float(help=\"\"\"\n    An explicitly supplied range start. If provided, will override\n    automatically computed start value.\n    \"\"\")\n\n    end = Float(help=\"\"\"\n    An explicitly supplied range end. If provided, will override\n    automatically computed end value.\n    \"\"\")",
  "class FactorRange(Range):\n    \"\"\" A range in a categorical dimension.\n\n    In addition to supplying ``factors`` keyword argument to the\n    ``FactorRange`` initializer, you can also instantiate with\n    the convenience syntax::\n\n        FactorRange(\"foo\", \"bar\") # equivalent to FactorRange(factors=[\"foo\", \"bar\"])\n\n    .. note::\n        ``FactorRange`` may be renamed to ``CategoricalRange`` in\n        the future.\n\n    \"\"\"\n\n    factors = Either(List(String), List(Int), help=\"\"\"\n    A list of string or integer factors (categories) to comprise\n    this categorical range.\n    \"\"\")\n\n    def __init__(self, *args, **kwargs):\n        if args and \"factors\" in kwargs:\n            raise ValueError(\"'factors' keyword cannot be used with positional arguments\")\n        elif args:\n            kwargs['factors'] = list(args)\n        super(FactorRange, self).__init__(**kwargs)",
  "def __init__(self, *args, **kwargs):\n        if args and ('start' in kwargs or 'end' in kwargs):\n            raise ValueError(\"'start' and 'end' keywords cannot be used with positional arguments\")\n        elif args and len(args) != 2:\n            raise ValueError('Only Range1d(start, end) acceptable when using positional arguments')\n        elif args:\n            kwargs['start'] = args[0]\n            kwargs['end'] = args[1]\n        super(Range1d, self).__init__(**kwargs)",
  "def __init__(self, *args, **kwargs):\n        if args and \"factors\" in kwargs:\n            raise ValueError(\"'factors' keyword cannot be used with positional arguments\")\n        elif args:\n            kwargs['factors'] = list(args)\n        super(FactorRange, self).__init__(**kwargs)",
  "class ToolEvents(PlotObject):\n    \"\"\"\n\n    \"\"\"\n\n    geometries = List(Dict(String, Any))",
  "class Tool(PlotObject):\n    \"\"\" A base class for all interactive tool types. ``Tool`` is\n    not generally useful to instantiate on its own.\n\n    \"\"\"\n\n    plot = Instance(\".models.plots.Plot\", help=\"\"\"\n    The Plot that this tool will act on.\n    \"\"\")",
  "class PanTool(Tool):\n    \"\"\" *toolbar icon*: |pan_icon|\n\n    The pan tool allows the user to pan a Plot by left-dragging\n    a mouse, or on touch devices by dragging a finger or stylus, across\n    the plot region.\n\n    The pan tool also activates the border regions of a Plot for \"single\n    axis\" panning. For instance, dragging in the vertical border or axis\n    will effect a pan in the vertical direction only, with the horizontal\n    dimension kept fixed.\n\n    .. |pan_icon| image:: /_images/icons/Pan.png\n        :height: 18pt\n\n    \"\"\"\n\n    dimensions = List(Enum(Dimension), default=[\"width\", \"height\"], help=\"\"\"\n    Which dimensions the pan tool is constrained to act in. By default\n    the pan tool will pan in any dimension, but can be configured to only\n    pan horizontally across the width of the plot, or vertically across the\n    height of the plot.\n    \"\"\")",
  "class WheelZoomTool(Tool):\n    \"\"\" *toolbar icon*: |wheel_zoom_icon|\n\n    The wheel zoom tool will zoom the plot in and out, centered on the\n    current mouse location.\n\n    The wheel zoom tool also activates the border regions of a Plot for\n    \"single axis\" zooming. For instance, zooming in the vertical border or\n    axis will effect a zoom in the vertical direction only, with the\n    horizontal dimension kept fixed.\n\n    .. |wheel_zoom_icon| image:: /_images/icons/WheelZoom.png\n        :height: 18pt\n\n    \"\"\"\n\n    dimensions = List(Enum(Dimension), default=[\"width\", \"height\"], help=\"\"\"\n    Which dimensions the wheel zoom tool is constrained to act in. By\n    default the wheel zoom tool will zoom in any dimension, but can be\n    configured to only zoom horizontally across the width of the plot, or\n    vertically across the height of the plot.\n    \"\"\")",
  "class PreviewSaveTool(Tool):\n    \"\"\" *toolbar icon*: |save_icon|\n\n    The preview/save tool is an action. When activated in the toolbar, the\n    tool presents a modal dialog with an image reproduction of the Plot, which\n    may be saved as a png image by right clicking on the image.\n\n    .. note::\n        Work is ongoing to support headless (svg, png) image creation without\n        requireing user interaction. See  :bokeh-issue:`538` to track progress\n        or contribute.\n\n    .. |save_icon| image:: /_images/icons/Save.png\n        :height: 18pt\n\n    \"\"\"",
  "class ResetTool(Tool):\n    \"\"\" *toolbar icon*: |reset_icon|\n\n    The reset tool is an action. When activated in teh toolbar, the tool\n    resets the data bounds of the plot to their values when the plot was\n    initially created.\n\n    .. note::\n        This tool does not also reset the plot canvas size, if the plot\n        has been resized using the ``ResizeTool``. That feature may be\n        added in a future release.\n\n    .. |reset_icon| image:: /_images/icons/Reset.png\n        :height: 18pt\n    \"\"\"",
  "class ResizeTool(Tool):\n    \"\"\" *toolbar icon*: |resize_icon|\n\n    The resize tool allows the user to left-drag a mouse or drag a finger\n    to resize the entire plot area on the screen.\n\n    .. |resize_icon| image:: /_images/icons/Resize.png\n        :height: 18pt\n\n    \"\"\"",
  "class TapTool(Tool):\n    \"\"\" *toolbar icon*: |tap_select_icon|\n\n    The tap selection tool allows the user to select at single points by\n    left-clicking a mouse, or tapping with a finger.\n\n    .. |tap_select_icon| image:: /_images/icons/TapSelect.png\n        :height: 18pt\n\n   .. note::\n        Selections can be comprised of multiple regions, even those\n        made by different selection tools. Hold down the <<shift>> key\n        while making a selection to append the new selection to any\n        previous seletion that might exist.\n\n    \"\"\"\n\n    names = List(String, help=\"\"\"\n    A list of names to query for. If set, only renderers that\n    have a matching value for their ``name`` attribute will be used.\n    \"\"\")\n\n    renderers = List(Instance(Renderer), help=\"\"\"\n    An explicit list of renderers to hit test again. If unset,\n    defaults to all renderers on a plot.\n    \"\"\")\n\n    always_active = Bool(True, help=\"\"\"\n    Whether the hover tool must be explicitly activated.\n    \"\"\")",
  "class CrosshairTool(Tool):\n    \"\"\" *toolbar icon*: |inspector_icon|\n\n    The crosshair tool is a passive inspector tool. It is generally on\n    at all times, but can be configured in the inspector's menu\n    associated with the *toolbar icon* shown above.\n\n    The crosshair tool draws a crosshair annotation over the plot,\n    centered on the current mouse position. The crosshair tool may be\n    configured to draw across only one dimension by setting the\n    ``dimension`` property to only ``width`` or ``height``.\n\n    .. |inspector_icon| image:: /_images/icons/Inspector.png\n        :height: 18pt\n\n    \"\"\"",
  "class BoxZoomTool(Tool):\n    \"\"\" *toolbar icon*: |box_zoom_icon|\n\n    The box zoom tool allows users to define a rectangular\n    region of a Plot to zoom to by dragging he mouse or a\n    finger over the plot region. The end of the drag\n    event indicates the selection region is ready.\n\n    .. |box_zoom_icon| image:: /_images/icons/BoxZoom.png\n        :height: 18pt\n\n    \"\"\"\n\n    dimensions = List(Enum(Dimension), default=[\"width\", \"height\"], help=\"\"\"\n    Which dimensions the zoom box is to be free in. By default,\n    users may freely draw zoom boxes with any dimensions. If only\n    \"width\" is supplied, the box will be constrained to span the entire\n    vertical space of the plot, only the horizontal dimension can be\n    controlled. If only \"height\" is supplied, the box will be constrained\n    to span the entire horizontal space of the plot, and the vertical\n    dimension can be controlled.\n    \"\"\")",
  "class BoxSelectTool(Tool):\n    \"\"\" *toolbar icon*: |box_select_icon|\n\n    The box selection tool allows users to make selections on a\n    Plot by indicating a rectangular region by dragging the\n    mouse or a finger over the plot region. The end of the drag\n    event indicates the selection region is ready.\n\n    .. |box_select_icon| image:: /_images/icons/BoxSelect.png\n        :height: 18pt\n\n    \"\"\"\n\n    names = List(String, help=\"\"\"\n    A list of names to query for. If set, only renderers that\n    have a matching value for their ``name`` attribute will be used.\n    \"\"\")\n\n    renderers = List(Instance(Renderer), help=\"\"\"\n    An explicit list of renderers to hit test again. If unset,\n    defaults to all renderers on a plot.\n    \"\"\")\n\n    select_every_mousemove = Bool(True, help=\"\"\"\n    An explicit list of renderers to hit test again. If unset,\n    defaults to all renderers on a plot.\n    \"\"\")\n\n    dimensions = List(Enum(Dimension), default=[\"width\", \"height\"], help=\"\"\"\n    Which dimensions the box selection is to be free in. By default,\n    users may freely draw selections boxes with any dimensions. If only\n    \"width\" is supplied, the box will be constrained to span the entire\n    vertical space of the plot, only the horizontal dimension can be\n    controlled. If only \"height\" is supplied, the box will be constrained\n    to span the entire horizontal space of the plot, and the vertical\n    dimension can be controlled.\n    \"\"\")",
  "class BoxSelectionOverlay(Renderer):\n    \"\"\" An overlay renderer that Tool objects can use to render a\n    'rubber band' selection box on a Plot.\n\n    \"\"\"\n\n    __view_model__ = 'BoxSelection'\n\n    tool = Instance(Tool, help=\"\"\"\n    The tool that this overlay should respond to.\n    \"\"\")",
  "class LassoSelectTool(Tool):\n    \"\"\" *toolbar icon*: |lasso_select_icon|\n\n    The lasso selection tool allows users to make selections on a\n    Plot by indicating a free-drawn \"lasso\" region by dragging the\n    mouse or a finger over the plot region. The end of the drag\n    event indicates the selection region is ready.\n\n    .. note::\n        Selections can be comprised of multiple regions, even those\n        made by different selection tools. Hold down the <<shift>> key\n        while making a selection to append the new selection to any\n        previous seletion that might exist.\n\n    .. |lasso_select_icon| image:: /_images/icons/LassoSelect.png\n        :height: 18pt\n    \"\"\"\n\n    names = List(String, help=\"\"\"\n    A list of names to query for. If set, only renderers that\n    have a matching value for their ``name`` attribute will be used.\n    \"\"\")\n\n    renderers = List(Instance(Renderer), help=\"\"\"\n    An explicit list of renderers to hit test again. If unset,\n    defaults to all renderers on a plot.\n    \"\"\")\n\n    select_every_mousemove = Bool(True, help=\"\"\"\n    Whether a selection computation should happen on every mouse\n    event, or only once, when the selection region is completed.\n    \"\"\")",
  "class PolySelectTool(Tool):\n    \"\"\" *toolbar icon*: |poly_select_icon|\n\n    The polygon selection tool allows users to make selections on a\n    Plot by indicating a polygonal region with mouse clicks. single\n    clicks (or taps) add successive points to the definition of the\n    polygon, and a double click (or tap) indicates the selection\n    region is ready.\n\n    .. note::\n        Selections can be comprised of multiple regions, even those\n        made by different selection tools. Hold down the <<shift>> key\n        while making a selection to append the new selection to any\n        previous seletion that might exist.\n\n    .. |poly_select_icon| image:: /_images/icons/PolygonSelect.png\n        :height: 18pt\n    \"\"\"\n\n    names = List(String, help=\"\"\"\n    A list of names to query for. If set, only renderers that\n    have a matching value for their ``name`` attribute will be used.\n    \"\"\")\n\n    renderers = List(Instance(Renderer), help=\"\"\"\n    An explicit list of renderers to hit test again. If unset,\n    defaults to all renderers on a plot.\n    \"\"\")",
  "class HoverTool(Tool):\n    \"\"\" *toolbar icon*: |inspector_icon|\n\n    The hover tool is a passive inspector tool. It is generally on at\n    all times, but can be configured in the inspector's menu associated\n    with the *toolbar icon* shown above.\n\n    The hover tool displays informational tooltips whenever the cursor\n    is directly is over a glyph. The data to show comes from the glyph's\n    data source, and what is to be displayed is configurable through a\n    ``tooltips`` attribute that maps display names to columns in the\n    data source, or to special known variables.\n\n    Here is an example of how to configure and use the hover tool::\n\n        # Add tooltip (name, field) pairs to the tool. See below for a\n        # description of possible field values.\n        hover.tooltips = [\n            (\"index\", \"$index\"),\n            (\"(x,y)\", \"($x, $y)\"),\n            (\"radius\", \"@radius\"),\n            (\"fill color\", \"$color[hex, swatch]:fill_color\"),\n            (\"foo\", \"@foo\"),\n            (\"bar\", \"@bar\"),\n        ]\n\n    .. note::\n        Point hit testing is not currently available on all glyphs. Hover tool\n        currently does not work with line or image type glyphs.\n\n    .. |hover_icon| image:: /_images/icons/Inspector.png\n        :height: 18pt\n    \"\"\"\n\n    names = List(String, help=\"\"\"\n    A list of names to query for. If set, only renderers that\n    have a matching value for their ``name`` attribute will be used.\n    \"\"\")\n\n    renderers = List(Instance(Renderer), help=\"\"\"\n    An explicit list of renderers to hit test again. If unset,\n    defaults to all renderers on a plot.\n    \"\"\")\n\n    tooltips = List(Tuple(String, String), help=\"\"\"\n    The (name, field) pairs describing what the hover tool should\n    display when there is a hit.\n\n    Field names starting with \"@\" are interpreted as columns on the\n    data source. For instance, \"@temp\" would look up values to display\n    from the \"temp\" column of the data source.\n\n    Field names starting with \"$\" are special, known fields:\n\n    :$index: index of selected point in the data source\n    :$x: x-coordindate under the cursor in data space\n    :$y: y-coordindate under the cursor in data space\n    :$sx: x-coordindate under the cursor in screen (canvas) space\n    :$sy: y-coordindate under the cursor in screen (canvas) space\n    :$color: color data from data source, with the syntax:\n        ``$color[options]:field_name``. The available options\n        are: 'hex' (to display the color as a hex value), and\n        'swatch' to also display a small color swatch.\n\n    .. note::\n        The tooltips attribute can also be configured with a mapping type,\n        e.g. ``dict`` or ``OrderedDict``. However, if a ``dict`` is used,\n        the visual presentation order is unpecified.\n\n    \"\"\").accepts(Dict(String, String), lambda d: list(d.items()))\n\n    always_active = Bool(True, help=\"\"\"\n    Whether the hover tool must be explicitly activated.\n    \"\"\")\n\n    snap_to_data = Bool(True, help=\"\"\"\n    Whether the tooltip position should snap to the \"center\" position\n    of the associated glyph. For instance, if set to True, the tooltip\n    will point to the center of any marker (e.g., ``Circle``, `` Square``)\n    regardless of the cursor position, as long as the cursor hits the\n    glyph.\n\n    .. note::\n        Not all glyphs support this feature. Currenly all marker glyphs,\n        the ``Rect`` and ``Quad`` glyphs are supported.\n\n    \"\"\")",
  "class Marker(Glyph):\n    \"\"\" Base class for glyphs that are simple markers with line and\n    fill properties, located at an (x, y) location with a specified\n    size.\n\n    .. note::\n        For simplicity, all markers have both line and fill properties\n        declared, however some markers (`Asterisk`, `Cross`, `X`) only\n        draw lines. For these markers, the fill values are simply\n        ignored.\n\n    \"\"\"\n\n    x = DataSpec(\"x\", help=\"\"\"\n    The x-axis coordinates for the center of the markers.\n    \"\"\")\n\n    y = DataSpec(\"y\", help=\"\"\"\n    The y-axis coordinates for the center of the markers.\n    \"\"\")\n\n    size = DataSpec(units=\"screen\", min_value=0, default=4, help=\"\"\"\n    The size (diameter) values for the markers. Interpreted as\n    \"screen space\" units by default.\n    \"\"\")\n\n    line_props = Include(LineProps, use_prefix=False, help=\"\"\"\n    The %s values for the markers.\n    \"\"\")\n\n    fill_props = Include(FillProps, use_prefix=False, help=\"\"\"\n    The %s values for the markers.\n    \"\"\")",
  "class Asterisk(Marker):\n    \"\"\" Render asterisk '*' markers.\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/Asterisk.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/Asterisk.py``\n\n    \"\"\"",
  "class Circle(Marker):\n    \"\"\" Render circle markers.\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/Circle.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/Circle.py``\n\n    \"\"\"\n\n    radius = DataSpec(units=\"data\", min_value=0, default=None, help=\"\"\"\n    The radius values for circle markers. Interpreted in\n    \"data space\" units by default.\n\n    .. note::\n        Circle markers are slightly unusual in that they support specifying\n        a radius in addition to a size. Only one of ``radius`` or ``size``\n        should be given.\n\n    .. warning::\n        Note that ``Circle`` glyphs are always drawn as circles on the screen,\n        even in cases where the data space aspect ratio is not 1-1. In all\n        cases where radius or size units are specified as \"data\", the\n        \"distance\" for the radius is measured along the horizontal axis.\n        If the aspect ratio is very large or small, the drawn circles may\n        appear much larger or smaller than expected. See :bokeh-issue:`626`\n        for more information.\n\n    \"\"\")\n\n    radius_dimension = Enum(enumeration('x', 'y'), help=\"\"\"\n    What dimension to measure circle radii along.\n\n    When the data space aspect ratio is not 1-1, then the size of the drawn\n    circles depends on what direction is used to measure the \"distance\" of\n    the radius. This property allows that direction to be controlled.\n    \"\"\")",
  "class CircleCross(Marker):\n    \"\"\" Render circle markers with a '+' cross through the center.\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/CircleCross.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/CircleCross.py``\n\n    \"\"\"",
  "class CircleX(Marker):\n    \"\"\" Render circle markers with an 'X' cross through the center.\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/CircleX.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/CircleX.py``\n\n    \"\"\"",
  "class Cross(Marker):\n    \"\"\" Render '+' cross markers.\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/Cross.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/Cross.py``\n\n    \"\"\"",
  "class Diamond(Marker):\n    \"\"\" Render diamond markers.\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/Diamond.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/Diamond.py``\n\n    \"\"\"",
  "class DiamondCross(Marker):\n    \"\"\" Render diamond markers with a '+' cross through the center.\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/DiamondCross.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/DiamondCross.py``\n\n    \"\"\"",
  "class InvertedTriangle(Marker):\n    \"\"\" Render upside-down triangle markers.\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/InvertedTriangle.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/InvertedTriangle.py``\n\n    \"\"\"",
  "class Square(Marker):\n    \"\"\" Render a square marker, optionally rotated.\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/Square.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/Square.py``\n\n    \"\"\"\n\n    angle = DataSpec(\"angle\", help=\"\"\"\n    The angles (in radians) to rotate square markers.\n    \"\"\")",
  "class SquareCross(Marker):\n    \"\"\" Render square markers with a '+' cross through the center.\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/SquareCross.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/SquareCross.py``\n\n    \"\"\"",
  "class SquareX(Marker):\n    \"\"\" Render square markers with an 'X' cross through the center.\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/SquareX.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/SquareX.py``\n\n    \"\"\"",
  "class Triangle(Marker):\n    \"\"\" Render triangle markers.\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/Triangle.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/Triangle.py``\n\n    \"\"\"",
  "class X(Marker):\n    \"\"\" Render a 'X' cross markers.\n\n    Example\n    -------\n\n    .. bokeh-plot:: ../tests/glyphs/X.py\n        :source-position: none\n\n    *source:* ``tests/glyphs/X.py``\n\n    \"\"\"",
  "class Dialog(Widget):\n    \"\"\" Simple dialog box with string message.\n\n    \"\"\"\n\n    visible = Bool(False, help=\"\"\"\n    Whether this dialog is visible or not.\n    \"\"\")\n\n    # TODO (bev) : \"closeable\" would be more common spelling\n    closable = Bool(True, help=\"\"\"\n    Whether this dialog is closable or not.\n    \"\"\")\n\n    title = String(help=\"\"\"\n    The title of the dialog widget.\n    \"\"\")\n\n    content = String(help=\"\"\"\n    The message displayed by this dialog.\n    \"\"\")\n\n    buttons = List(String, help=\"\"\"\n    A list of button labels.\n    \"\"\")",
  "class AbstractButton(Widget):\n    \"\"\" A base class that defines common properties for all\n    button types. ``AbstractButton`` is not generally useful to\n    instantiate on its own.\n\n    \"\"\"\n\n    label = String(\"Button\", help=\"\"\"\n    The text label for the button to display.\n    \"\"\")\n\n    icon = Instance(AbstractIcon, help=\"\"\"\n    An optional image appearing to the left of button's text.\n    \"\"\")\n\n    type = Enum(ButtonType, help=\"\"\"\n    A style for the button, signifying it's role.\n    \"\"\")",
  "class Button(AbstractButton):\n    \"\"\" A click button.\n\n    \"\"\"\n\n    clicks = Int(0, help=\"\"\"\n    A private property used to trigger ``on_click`` event handler.\n    \"\"\")\n\n    def on_click(self, handler):\n        \"\"\" Set up a handler for button clicks.\n\n        Args:\n            handler (func) : handler function to call when button is clicked.\n\n        Returns:\n            None\n\n        \"\"\"\n        self.on_change('clicks', lambda obj, attr, old, new: handler())",
  "class Toggle(AbstractButton):\n    \"\"\" A two-state toggle button.\n\n    \"\"\"\n\n    active = Bool(False, help=\"\"\"\n    The initial state of a button. Also used to trigger ``on_click`` event\n    handler.\n    \"\"\")\n\n    def on_click(self, handler):\n        \"\"\" Set up a handler for button state changes (clicks).\n\n        Args:\n            handler (func) : handler function to call when button is toggled.\n\n        Returns:\n            None\n\n        \"\"\"\n        self.on_change('active', lambda obj, attr, old, new: handler(new))",
  "class Dropdown(AbstractButton):\n    \"\"\" A dropdown button.\n\n    \"\"\"\n\n    action = String(help=\"\"\"\n    A private property used to trigger ``on_click`` event handler.\n    \"\"\")\n\n    default_action = String(help=\"\"\"\n    The default action, otherwise the first item in ``menu`` will be used.\n    \"\"\")\n\n    menu = List(Tuple(String, String), help=\"\"\"\n    Button's dropdown menu consisting of entries containing item's text and\n    action name. Use ``None`` as a menu separator.\n    \"\"\")\n\n    def on_click(self, handler):\n        \"\"\" Set up a handler for button or menu item clicks.\n\n        Args:\n            handler (func) : handler function to call when button is activated.\n\n        Returns:\n            None\n\n        \"\"\"\n        self.on_change('action', lambda obj, attr, old, new: handler(new))",
  "def on_click(self, handler):\n        \"\"\" Set up a handler for button clicks.\n\n        Args:\n            handler (func) : handler function to call when button is clicked.\n\n        Returns:\n            None\n\n        \"\"\"\n        self.on_change('clicks', lambda obj, attr, old, new: handler())",
  "def on_click(self, handler):\n        \"\"\" Set up a handler for button state changes (clicks).\n\n        Args:\n            handler (func) : handler function to call when button is toggled.\n\n        Returns:\n            None\n\n        \"\"\"\n        self.on_change('active', lambda obj, attr, old, new: handler(new))",
  "def on_click(self, handler):\n        \"\"\" Set up a handler for button or menu item clicks.\n\n        Args:\n            handler (func) : handler function to call when button is activated.\n\n        Returns:\n            None\n\n        \"\"\"\n        self.on_change('action', lambda obj, attr, old, new: handler(new))",
  "class Paragraph(Widget):\n    \"\"\" A block (paragraph) of text.\n\n    \"\"\"\n\n    text = String(help=\"\"\"\n    The contents of the widget.\n    \"\"\")\n\n    width = Int(500, help=\"\"\"\n    The width of the block in pixels.\n    \"\"\")\n\n    height = Int(400, help=\"\"\"\n    The height of the block in pixels.\n    \"\"\")",
  "class PreText(Paragraph):\n    \"\"\" A block (paragraph) of pre-formatted text.\n\n    \"\"\"",
  "class Panel(Widget):\n    \"\"\" A single-widget container with title bar and controls.\n\n    \"\"\"\n\n    title = String(help=\"\"\"\n    An optional text title of the panel.\n    \"\"\")\n\n    child = Instance(Widget, help=\"\"\"\n    The child widget. If you need more children, use a layout widget,\n    e.g. ``HBox`` or ``VBox``.\n    \"\"\")\n\n    closable = Bool(False, help=\"\"\"\n    Whether this panel is closeable or not. If True, an \"x\" button will\n    appear.\n    \"\"\")",
  "class Tabs(Widget):\n    \"\"\" A panel widget with navigation tabs.\n\n    \"\"\"\n\n    tabs = List(Instance(Panel), help=\"\"\"\n    The list of child panel widgets.\n    \"\"\")\n\n    active = Int(0, help=\"\"\"\n    The index of the active tab.\n    \"\"\")",
  "class Layout(Widget):\n    \"\"\" An abstract base class for layout widgets. ``Layout`` is not\n    generally useful to instantiate on its own.\n\n    \"\"\"\n\n    width = Int(help=\"\"\"\n    An optional width for the widget (in pixels).\n    \"\"\")\n\n    height = Int(help=\"\"\"\n    An optional height for the widget (in pixels).\n    \"\"\")",
  "class BaseBox(Layout):\n    \"\"\" Abstract base class for HBox and VBox. Do not use directly.\n    \"\"\"\n    \n    def __init__(self, *args, **kwargs):\n        if len(args) > 0 and \"children\" in kwargs:\n            raise ValueError(\"'children' keyword cannot be used with positional arguments\")\n        if (len(args) == 1 and hasattr(args[0], '__iter__') and \n            not isinstance(args[0], Widget)):\n            # Note: check that not Widget, in case Widget/Layout ever gets __iter__\n            kwargs[\"children\"] = list(args[0])\n        elif len(args) > 0:\n            kwargs[\"children\"] = list(args)\n        super(BaseBox, self).__init__(**kwargs)\n\n    children = List(Instance(Widget), help=\"\"\"\n    The list of children, which can be other widgets (including layouts)\n    and plots.\n    \"\"\")",
  "class HBox(BaseBox):\n    \"\"\" Lay out child widgets in a single horizontal row.\n    \n    Children can be specified as positional arguments, as a single argument\n    that is a sequence, or using the ``children`` keyword argument.\n    \"\"\"",
  "class VBox(BaseBox):\n    \"\"\" Lay out child widgets in a single vertical row.\n    \n    Children can be specified as positional arguments, as a single argument\n    that is a sequence, or using the ``children`` keyword argument.\n    \"\"\"",
  "class VBoxForm(VBox):\n    \"\"\"\n\n    \"\"\"",
  "def __init__(self, *args, **kwargs):\n        if len(args) > 0 and \"children\" in kwargs:\n            raise ValueError(\"'children' keyword cannot be used with positional arguments\")\n        if (len(args) == 1 and hasattr(args[0], '__iter__') and \n            not isinstance(args[0], Widget)):\n            # Note: check that not Widget, in case Widget/Layout ever gets __iter__\n            kwargs[\"children\"] = list(args[0])\n        elif len(args) > 0:\n            kwargs[\"children\"] = list(args)\n        super(BaseBox, self).__init__(**kwargs)",
  "class InputWidget(Widget):\n    \"\"\" Abstract base class for input widgets. `InputWidget`` is not\n    generally useful to instantiate on its own.\n\n    \"\"\"\n\n    title = String(help=\"\"\"\n    Widget's label.\n    \"\"\")\n\n    name = String(help=\"\"\"\n    Widget's name.\n    \"\"\")\n\n    value = String(help=\"\"\"\n    Initial or input value.\n    \"\"\")\n\n    @classmethod\n    def coerce_value(cls, val):\n        prop_obj = cls.lookup('value')\n        if isinstance(prop_obj, Float):\n            return float(val)\n        elif isinstance(prop_obj, Int):\n            return int(val)\n        elif isinstance(prop_obj, String):\n            return str(val)\n        else:\n            return val\n\n    @classmethod\n    def create(cls, *args, **kwargs):\n        \"\"\" Only called the first time we make an object,\n        whereas __init__ is called every time it's loaded\n\n        \"\"\"\n        if kwargs.get('title') is None:\n            kwargs['title'] = kwargs['name']\n        if kwargs.get('value') is not None:\n            kwargs['value'] = cls.coerce_value(kwargs.get('value'))\n        return cls(**kwargs)",
  "class TextInput(InputWidget):\n    \"\"\" Single-line input widget. \"\"\"\n\n    value = String(help=\"\"\"\n    Initial or entered value.\n    \"\"\")",
  "class Select(InputWidget):\n    \"\"\" Single-select widget.\n\n    \"\"\"\n\n    options = List(Either(String, Dict(String, String)), help=\"\"\"\n    Available selection options.\n    \"\"\")\n\n    value = String(help=\"\"\"\n    Initial or selected value.\n    \"\"\")\n\n    @classmethod\n    def create(self, *args, **kwargs):\n        options = kwargs.pop('options', [])\n        new_options = []\n        for opt in options:\n            if isinstance(opt, six.string_types):\n                opt = {'name' : opt, 'value' : opt}\n            new_options.append(opt)\n        kwargs['options'] = new_options\n        return super(Select, self).create(*args, **kwargs)",
  "class MultiSelect(Select):\n    \"\"\" Multi-select widget.\n\n    \"\"\"\n\n    value = List(String, help=\"\"\"\n    Initial or selected values.\n    \"\"\")\n\n    @classmethod\n    def create(self, *args, **kwargs):\n        options = kwargs.pop('options', [])\n        new_options = []\n        for opt in options:\n            if isinstance(opt, six.string_types):\n                opt = {'name' : opt, 'value' : opt}\n            new_options.append(opt)\n        kwargs['options'] = new_options\n        return super(Select, self).create(*args, **kwargs)",
  "class Slider(InputWidget):\n    \"\"\" Slider-based number selection widget.\n\n    \"\"\"\n\n    value = Float(help=\"\"\"\n    Initial or selected value.\n    \"\"\")\n\n    start = Float(help=\"\"\"\n    The minimum allowable value.\n    \"\"\")\n\n    end = Float(help=\"\"\"\n    The maximum allowable value.\n    \"\"\")\n\n    step = Float(help=\"\"\"\n    The step between consecutive values.\n    \"\"\")\n\n    orientation = Enum(\"horizontal\", \"vertical\", help=\"\"\"\n    Orient the slider either horizontally (default) or vertically.\n    \"\"\")",
  "class DateRangeSlider(InputWidget):\n    \"\"\" Slider-based date range selection widget.\n\n    \"\"\"\n\n    value = Tuple(Date, Date, help=\"\"\"\n    The initial or selected date range.\n    \"\"\")\n\n    bounds = Tuple(Date, Date, help=\"\"\"\n    The earliest and latest allowable dates.\n    \"\"\")\n\n    range = Tuple(RelativeDelta, RelativeDelta, help=\"\"\"\n    [TDB]\n    \"\"\")\n\n    step = RelativeDelta(help=\"\"\"\n    The step between consecutive dates.\n    \"\"\")\n\n    # formatter = Either(String, Function(Date))\n    # scales = DateRangeSliderScales ... # first, next, stop, label, format\n\n    enabled = Bool(True, help=\"\"\"\n    Enable or disable this widget.\n    \"\"\")\n\n    arrows = Bool(True, help=\"\"\"\n    Whether to show clickable arrows on both ends of the slider.\n    \"\"\")\n\n    value_labels = Enum(\"show\", \"hide\", \"change\", help=\"\"\"\n    Show or hide value labels on both sides of the slider.\n    \"\"\")\n\n    wheel_mode = Enum(\"scroll\", \"zoom\", default=None, help=\"\"\"\n    Whether mouse zoom should scroll or zoom selected range (or\n    do nothing).\n    \"\"\")",
  "class DatePicker(InputWidget):\n    \"\"\" Calendar-based date picker widget.\n\n    \"\"\"\n\n    value = Date(help=\"\"\"\n    The initial or picked date.\n    \"\"\")\n\n    min_date = Date(default=None, help=\"\"\"\n    Optional earliest allowable date.\n    \"\"\")\n\n    max_date = Date(default=None, help=\"\"\"\n    Optional latest allowable date.\n    \"\"\")",
  "def coerce_value(cls, val):\n        prop_obj = cls.lookup('value')\n        if isinstance(prop_obj, Float):\n            return float(val)\n        elif isinstance(prop_obj, Int):\n            return int(val)\n        elif isinstance(prop_obj, String):\n            return str(val)\n        else:\n            return val",
  "def create(cls, *args, **kwargs):\n        \"\"\" Only called the first time we make an object,\n        whereas __init__ is called every time it's loaded\n\n        \"\"\"\n        if kwargs.get('title') is None:\n            kwargs['title'] = kwargs['name']\n        if kwargs.get('value') is not None:\n            kwargs['value'] = cls.coerce_value(kwargs.get('value'))\n        return cls(**kwargs)",
  "def create(self, *args, **kwargs):\n        options = kwargs.pop('options', [])\n        new_options = []\n        for opt in options:\n            if isinstance(opt, six.string_types):\n                opt = {'name' : opt, 'value' : opt}\n            new_options.append(opt)\n        kwargs['options'] = new_options\n        return super(Select, self).create(*args, **kwargs)",
  "def create(self, *args, **kwargs):\n        options = kwargs.pop('options', [])\n        new_options = []\n        for opt in options:\n            if isinstance(opt, six.string_types):\n                opt = {'name' : opt, 'value' : opt}\n            new_options.append(opt)\n        kwargs['options'] = new_options\n        return super(Select, self).create(*args, **kwargs)",
  "class AbstractGroup(Widget):\n    \"\"\" Abstract base class for all kinds of groups. ``AbstractGroup``\n    is not generally useful to instantiate on its own.\n\n    \"\"\"\n\n    labels = List(String, help=\"\"\"\n    List of text labels contained in this group.\n    \"\"\")\n\n    def on_click(self, handler):\n        \"\"\" Set up a handler for button check/radio box clicks including\n        the selected indices.\n\n        Args:\n            handler (func) : handler function to call when button is clicked.\n\n        Returns:\n            None\n\n        \"\"\"\n        self.on_change('active', lambda obj, attr, old, new: handler(new))",
  "class ButtonGroup(AbstractGroup):\n    \"\"\" Abstract base class for groups with items rendered as buttons.\n    ``ButtonGroup`` is not generally useful to instantiate on its own.\n\n     \"\"\"\n\n    type = Enum(ButtonType, help=\"\"\"\n    A style for the button, signifying it's role.\n    \"\"\")",
  "class Group(AbstractGroup):\n    \"\"\" Abstract base class for groups with items rendered as check/radio\n    boxes.\n\n    \"\"\"\n\n    inline = Bool(False, help=\"\"\"\n    Should items be arrange vertically (``False``) or horizontally\n    in-line (``True``).\n    \"\"\")",
  "class CheckboxGroup(Group):\n    \"\"\" A group of check boxes.\n\n    \"\"\"\n\n    active = List(Int, help=\"\"\"\n    The list of indices of selected check boxes.\n    \"\"\")",
  "class RadioGroup(Group):\n    \"\"\" A group of radio boxes.\n\n    \"\"\"\n\n    active = Int(None, help=\"\"\"\n    The index of the selected radio box, or ``None`` if nothing is\n    selected.\n    \"\"\")",
  "class CheckboxButtonGroup(ButtonGroup):\n    \"\"\" A group of check boxes rendered as toggle buttons.\n\n    \"\"\"\n\n    active = List(Int, help=\"\"\"\n    The list of indices of selected check boxes.\n    \"\"\")",
  "class RadioButtonGroup(ButtonGroup):\n    \"\"\" A group of radio boxes rendered as toggle buttons.\n\n    \"\"\"\n\n    active = Int(None, help=\"\"\"\n    The index of the selected radio box, or ``None`` if nothing is\n    selected.\n    \"\"\")",
  "def on_click(self, handler):\n        \"\"\" Set up a handler for button check/radio box clicks including\n        the selected indices.\n\n        Args:\n            handler (func) : handler function to call when button is clicked.\n\n        Returns:\n            None\n\n        \"\"\"\n        self.on_change('active', lambda obj, attr, old, new: handler(new))",
  "class AbstractIcon(Widget):\n    \"\"\" An abstract base class for icon widgets. ``AbstractIcon``\n    is not generally useful to instantiate on its own.\n\n    \"\"\"",
  "class Icon(AbstractIcon):\n    \"\"\" A \"stock\" icon based on FontAwesome.\n\n    \"\"\"\n\n    name = Enum(NamedIcon, help=\"\"\"\n    What icon to use. See http://fortawesome.github.io/Font-Awesome/icons/\n    for the list of available icons.\n    \"\"\")\n\n    size = Float(None, help=\"\"\"\n    The size multiplier (1x, 2x, ..., 5x).\n    \"\"\")\n\n    flip = Enum(\"horizontal\", \"vertical\", default=None, help=\"\"\"\n    Optionally flip the icon horizontally or vertically.\n    \"\"\")\n\n    spin = Bool(False, help=\"\"\"\n    Indicates a spinning (animated) icon. This value is ignored for\n    icons that do not support spinning.\n    \"\"\")",
  "class CellFormatter(PlotObject):\n    \"\"\" Abstract base class for data table's cell formatters.\n\n    \"\"\"",
  "class CellEditor(PlotObject):\n    \"\"\" Abstract base class for data table's cell editors.\n\n    \"\"\"",
  "class StringFormatter(CellFormatter):\n    \"\"\" Basic string cell formatter.\n\n    \"\"\"\n\n    font_style = Enum(FontStyle, help=\"\"\"\n    An optional text font style, e.g. bold, italic.\n    \"\"\")\n\n    text_align = Enum(TextAlign, help=\"\"\"\n    An optional text align, i.e. left, center or right.\n    \"\"\")\n\n    text_color = Color(help=\"\"\"\n    An optional text color. See :class:`bokeh.properties.Color` for\n    details.\n    \"\"\")",
  "class NumberFormatter(StringFormatter):\n    \"\"\" Number cell formatter.\n\n    \"\"\"\n\n    format = String(\"0,0\", help=\"\"\"\n    The numer format, as defined in the following tables.\n\n    * numbers:\n\n    +------------+--------------+---------------+\n    | Number     | Format       | String        |\n    +============+==============+===============+\n    | 10000      | '0,0.0000'   | 10,000.0000   |\n    | 10000.23   | '0,0'        | 10,000        |\n    | 10000.23   | '+0,0'       | +10,000       |\n    | -10000     | '0,0.0'      | -10,000.0     |\n    | 10000.1234 | '0.000'      | 10000.123     |\n    | 10000.1234 | '0[.]00000'  | 10000.12340   |\n    | -10000     | '(0,0.0000)' | (10,000.0000) |\n    | -0.23      | '.00'        | -.23          |\n    | -0.23      | '(.00)'      | (.23)         |\n    | 0.23       | '0.00000'    | 0.23000       |\n    | 0.23       | '0.0[0000]'  | 0.23          |\n    | 1230974    | '0.0a'       | 1.2m          |\n    | 1460       | '0 a'        | 1 k           |\n    | -104000    | '0a'         | -104k         |\n    | 1          | '0o'         | 1st           |\n    | 52         | '0o'         | 52nd          |\n    | 23         | '0o'         | 23rd          |\n    | 100        | '0o'         | 100th         |\n    +------------+--------------+---------------+\n\n    * currency:\n\n    +------------+--------------+---------------+\n    | Number     | Format       | String        |\n    +============+==============+===============+\n    | 1000.234   | '$0,0.00'    | $1,000.23     |\n    | 1000.2     | '0,0[.]00 $' | 1,000.20 $    |\n    | 1001       | '$ 0,0[.]00' | $ 1,001       |\n    | -1000.234  | '($0,0)'     | ($1,000)      |\n    | -1000.234  | '$0.00'      | -$1000.23     |\n    | 1230974    | '($ 0.00 a)' | $ 1.23 m      |\n    +------------+--------------+---------------+\n\n    * bytes:\n\n    +---------------+-----------+---------------+\n    | Number        | Format    | String        |\n    +===============+===========+===============+\n    | 100           | '0b'      | 100B          |\n    | 2048          | '0 b'     | 2 KB          |\n    | 7884486213    | '0.0b'    | 7.3GB         |\n    | 3467479682787 | '0.000 b' | 3.154 TB      |\n    +---------------+-----------+---------------+\n\n    * percentages:\n\n    +-------------+-------------+---------------+\n    | Number      | Format      | String        |\n    +=============+=============+===============+\n    | 1           | '0%'        | 100%          |\n    | 0.974878234 | '0.000%'    | 97.488%       |\n    | -0.43       | '0 %'       | -43 %         |\n    | 0.43        | '(0.000 %)' | 43.000 %      |\n    +-------------+-------------+---------------+\n\n    * time:\n\n    +------------+--------------+---------------+\n    | Number     | Format       | String        |\n    +============+==============+===============+\n    | 25         | '00:00:00'   | 0:00:25       |\n    | 238        | '00:00:00'   | 0:03:58       |\n    | 63846      | '00:00:00'   | 17:44:06      |\n    +------------+--------------+---------------+\n    \"\"\")\n\n    language = Enum(\"be-nl\", \"chs\", \"cs\", \"da-dk\", \"de-ch\", \"de\", \"en\", \"en-gb\",\n        \"es-ES\", \"es\", \"et\", \"fi\", \"fr-CA\", \"fr-ch\", \"fr\", \"hu\", \"it\", \"ja\",\n        \"nl-nl\", \"pl\", \"pt-br\", \"pt-pt\", \"ru\", \"ru-UA\", \"sk\", \"th\", \"tr\", \"uk-UA\", help=\"\"\"\n    The language to use for formatting language-specific features (e.g. thousands separator).\n    \"\"\")",
  "class BooleanFormatter(CellFormatter):\n    \"\"\" Boolean (check mark) cell formatter.\n\n    \"\"\"\n\n    icon = Enum('check', 'check-circle', 'check-circle-o', 'check-square', 'check-square-o', help=\"\"\"\n    The icon visualizing the check mark.\n    \"\"\")",
  "class DateFormatter(CellFormatter):\n    \"\"\" Date cell formatter.\n\n    \"\"\"\n\n    format = Either(Enum(DateFormat), String, default='yy M d', help=\"\"\"\n    The date format can be combinations of the following:\n\n    d\n        day of month (no leading zero)\n\n    dd\n        day of month (two digit)\n\n    o\n        day of year (no leading zeros)\n\n    oo\n        day of year (three digit)\n\n    D\n        day name short\n\n    DD\n        day name long\n\n    m\n        month of year (no leading zero)\n\n    mm\n        month of year (two digit)\n\n    M\n        month name short\n\n    MM\n        month name long\n\n    y\n        year (two digit)\n\n    yy\n        year (four digit)\n\n    @\n        Unix timestamp (ms since 01/01/1970)\n\n    !\n        Windows ticks (100ns since 01/01/0001)\n\n    \"...\"\n        literal text\n\n    ''\n        single quote\n    \"\"\")",
  "class StringEditor(CellEditor):\n    \"\"\" Basic string cell editor with auto-completion.\n\n    \"\"\"\n\n    completions = List(String, help=\"\"\"\n    An optional list of completion strings.\n    \"\"\")",
  "class TextEditor(CellEditor):\n    \"\"\" Multi-line string cell editor.\n\n    \"\"\"",
  "class SelectEditor(CellEditor):\n    \"\"\" Select cell editor.\n\n    \"\"\"\n\n    options = List(String, help=\"\"\"\n    The list of options to select from.\n    \"\"\")",
  "class PercentEditor(CellEditor):\n    \"\"\" ``IntEditor`` optimized for editing percentages.\n\n    \"\"\"",
  "class CheckboxEditor(CellEditor):\n    \"\"\" Boolean value cell editor.\n\n    \"\"\"",
  "class IntEditor(CellEditor):\n    \"\"\" Spinner-based integer cell editor.\n\n    \"\"\"\n\n    step = Int(1, help=\"\"\"\n    The major step value.\n    \"\"\")",
  "class NumberEditor(CellEditor):\n    \"\"\" Spinner-based number cell editor.\n\n    \"\"\"\n\n    step = Float(0.01, help=\"\"\"\n    The major step value.\n    \"\"\")",
  "class TimeEditor(CellEditor):\n    \"\"\" Spinner-based time cell editor.\n\n    \"\"\"",
  "class DateEditor(CellEditor):\n    \"\"\" Calendar-based date cell editor.\n\n    \"\"\"",
  "class TableColumn(PlotObject):\n    \"\"\" Table column widget.\n\n    \"\"\"\n\n    field = String(help=\"\"\"\n    The name of the field mapping to a column in the data source.\n    \"\"\")\n\n    title = String(help=\"\"\"\n    The title of this column. If not set, column's data field is\n    used instead.\n    \"\"\")\n\n    width = Int(300, help=\"\"\"\n    The width or maximum width (depending on data table's configuration)\n    in pixels of this column.\n    \"\"\")\n\n    formatter = Instance(CellFormatter, lambda: StringFormatter(), help=\"\"\"\n    The cell formatter for this column. By default, a simple string\n    formatter is used.\n    \"\"\")\n\n    editor = Instance(CellEditor, lambda: StringEditor(), help=\"\"\"\n    The cell editor for this column. By default, a simple string editor\n    is used.\n    \"\"\")\n\n    sortable = Bool(True, help=\"\"\"\n    Whether this column is sortable or not. Note that data table has\n    to have sorting enabled to allow sorting in general.\n    \"\"\")\n\n    default_sort = Enum(\"ascending\", \"descending\", help=\"\"\"\n    The default sorting order. By default ``ascending`` order is used.\n    \"\"\")",
  "class TableWidget(Widget):\n    \"\"\" Abstract base class for data table (data grid) widgets.\n\n    \"\"\"\n\n    source = Instance(DataSource, help=\"\"\"\n    The source of data for the widget.\n    \"\"\")",
  "class DataTable(TableWidget):\n    \"\"\" Two dimensional grid for visualisation and editing large amounts\n    of data.\n\n    \"\"\"\n\n    columns = List(Instance(TableColumn), help=\"\"\"\n    The list of child column widgets.\n    \"\"\")\n\n    width = Int(None, help=\"\"\"\n    Optional width in pixels of the table widget. By default, uses all\n    horizontal space available.\n    \"\"\")\n\n    height = Either(Int(400), Auto, help=\"\"\"\n    Height in pixels of the table widget. Use ``Auto`` to make the widget\n    adjust its height automatically. Note that ``Auto`` is inefficient for\n    large amounts of data, so should be used with care.\n    \"\"\")\n\n    fit_columns = Bool(True, help=\"\"\"\n    Whether columns should be fit to the available width. This results in no\n    horizontal scrollbar showing up, but data can get unreadable if there is\n    no enough space available. If set to ``True``, columns' width is\n    understood as maximum width.\n    \"\"\")\n\n    sortable = Bool(True, help=\"\"\"\n    Allows to sort table's contents. By default natural order is preserved.\n    To sort a column, click on it's header. Clicking one more time changes\n    sort direction. Use Ctrl + click to return to natural order. Use\n    Shift + click to sort multiple columns simultaneously.\n    \"\"\")\n\n    editable = Bool(False, help=\"\"\"\n    Allows to edit table's contents. Needs cell editors to be configured on\n    columns that are required to be editable.\n    \"\"\")\n\n    selectable = Either(Bool(True), Enum(\"checkbox\"), help=\"\"\"\n    Whether a table's rows can be selected or not. Using ``checkbox`` is\n    equivalent  to ``True``, but makes selection visible through a checkbox\n    for each row,  instead of highlighting rows. Multiple selection is\n    allowed and can be achieved by either clicking multiple checkboxes (if\n    enabled) or using Shift + click on rows.\n    \"\"\")\n\n    row_headers = Bool(True, help=\"\"\"\n    Enable or disable row headers, i.e. the index column.\n    \"\"\")",
  "def color_to_hex(color):\n    \"\"\"Convert matplotlib color code to hex color code\"\"\"\n    if color is None or colorConverter.to_rgba(color)[3] == 0:\n        return 'none'\n    else:\n        rgb = colorConverter.to_rgb(color)\n        return '#{0:02X}{1:02X}{2:02X}'.format(*(int(255 * c) for c in rgb))",
  "def many_to_one(input_dict):\n    \"\"\"Convert a many-to-one mapping to a one-to-one mapping\"\"\"\n    return dict((key, val)\n                for keys, val in input_dict.items()\n                for key in keys)",
  "def get_dasharray(obj, i=None):\n    \"\"\"Get an SVG dash array for the given matplotlib linestyle\n\n    Parameters\n    ----------\n    obj : matplotlib object\n        The matplotlib line or path object, which must have a get_linestyle()\n        method which returns a valid matplotlib line code\n    i : integer (optional)\n\n    Returns\n    -------\n    dasharray : string\n        The HTML/SVG dasharray code associated with the object.\n    \"\"\"\n    if obj.__dict__.get('_dashSeq', None) is not None:\n        return ','.join(map(str, obj._dashSeq))\n    else:\n        ls = obj.get_linestyle()\n        if i is not None:\n            ls = ls[i]\n\n        dasharray = LINESTYLES.get(ls, None)\n        if dasharray is None:\n            warnings.warn(\"dash style '{0}' not understood: \"\n                          \"defaulting to solid.\".format(ls))\n            dasharray = LINESTYLES['-']\n        return dasharray",
  "def SVG_path(path, transform=None, simplify=False):\n    \"\"\"Construct the vertices and SVG codes for the path\n\n    Parameters\n    ----------\n    path : matplotlib.Path object\n\n    transform : matplotlib transform (optional)\n        if specified, the path will be transformed before computing the output.\n\n    Returns\n    -------\n    vertices : array\n        The shape (M, 2) array of vertices of the Path. Note that some Path\n        codes require multiple vertices, so the length of these vertices may\n        be longer than the list of path codes.\n    path_codes : list\n        A length N list of single-character path codes, N <= M. Each code is\n        a single character, in ['L','M','S','C','Z']. See the standard SVG\n        path specification for a description of these.\n    \"\"\"\n    if transform is not None:\n        path = path.transformed(transform)\n\n    vc_tuples = [(vertices if path_code != Path.CLOSEPOLY else [],\n                  PATH_DICT[path_code])\n                 for (vertices, path_code)\n                 in path.iter_segments(simplify=simplify)]\n\n    if not vc_tuples:\n        # empty path is a special case\n        return np.zeros((0, 2)), []\n    else:\n        vertices, codes = zip(*vc_tuples)\n        vertices = np.array(list(itertools.chain(*vertices))).reshape(-1, 2)\n        return vertices, list(codes)",
  "def get_path_style(path, fill=True):\n    \"\"\"Get the style dictionary for matplotlib path objects\"\"\"\n    style = {}\n    style['alpha'] = path.get_alpha()\n    if style['alpha'] is None:\n        style['alpha'] = 1\n    style['edgecolor'] = color_to_hex(path.get_edgecolor())\n    if fill:\n        style['facecolor'] = color_to_hex(path.get_facecolor())\n    else:\n        style['facecolor'] = 'none'\n    style['edgewidth'] = path.get_linewidth()\n    style['dasharray'] = get_dasharray(path)\n    style['zorder'] = path.get_zorder()\n    return style",
  "def get_line_style(line):\n    \"\"\"Get the style dictionary for matplotlib line objects\"\"\"\n    style = {}\n    style['alpha'] = line.get_alpha()\n    if style['alpha'] is None:\n        style['alpha'] = 1\n    style['color'] = color_to_hex(line.get_color())\n    style['linewidth'] = line.get_linewidth()\n    style['dasharray'] = get_dasharray(line)\n    style['zorder'] = line.get_zorder()\n    return style",
  "def get_marker_style(line):\n    \"\"\"Get the style dictionary for matplotlib marker objects\"\"\"\n    style = {}\n    style['alpha'] = line.get_alpha()\n    if style['alpha'] is None:\n        style['alpha'] = 1\n\n    style['facecolor'] = color_to_hex(line.get_markerfacecolor())\n    style['edgecolor'] = color_to_hex(line.get_markeredgecolor())\n    style['edgewidth'] = line.get_markeredgewidth()\n\n    style['marker'] = line.get_marker()\n    markerstyle = MarkerStyle(line.get_marker())\n    markersize = line.get_markersize()\n    markertransform = (markerstyle.get_transform()\n                       + Affine2D().scale(markersize, -markersize))\n    style['markerpath'] = SVG_path(markerstyle.get_path(),\n                                   markertransform)\n    style['markersize'] = markersize\n    style['zorder'] = line.get_zorder()\n    return style",
  "def get_text_style(text):\n    \"\"\"Return the text style dict for a text instance\"\"\"\n    style = {}\n    style['alpha'] = text.get_alpha()\n    if style['alpha'] is None:\n        style['alpha'] = 1\n    style['fontsize'] = text.get_size()\n    style['color'] = color_to_hex(text.get_color())\n    style['halign'] = text.get_horizontalalignment()  # left, center, right\n    style['valign'] = text.get_verticalalignment()  # baseline, center, top\n    style['rotation'] = text.get_rotation()\n    style['zorder'] = text.get_zorder()\n    return style",
  "def get_axis_properties(axis):\n    \"\"\"Return the property dictionary for a matplotlib.Axis instance\"\"\"\n    props = {}\n    label1On = axis._major_tick_kw.get('label1On', True)\n\n    if isinstance(axis, matplotlib.axis.XAxis):\n        if label1On:\n            props['position'] = \"bottom\"\n        else:\n            props['position'] = \"top\"\n    elif isinstance(axis, matplotlib.axis.YAxis):\n        if label1On:\n            props['position'] = \"left\"\n        else:\n            props['position'] = \"right\"\n    else:\n        raise ValueError(\"{0} should be an Axis instance\".format(axis))\n\n    # Use tick values if appropriate\n    locator = axis.get_major_locator()\n    props['nticks'] = len(locator())\n    if isinstance(locator, ticker.FixedLocator):\n        props['tickvalues'] = list(locator())\n    else:\n        props['tickvalues'] = None\n\n    # Find tick formats\n    formatter = axis.get_major_formatter()\n    if isinstance(formatter, ticker.NullFormatter):\n        props['tickformat'] = \"\"\n    elif not any(label.get_visible() for label in axis.get_ticklabels()):\n        props['tickformat'] = \"\"\n    else:\n        props['tickformat'] = None\n\n    # Get axis scale\n    props['scale'] = axis.get_scale()\n\n    # Get major tick label size (assumes that's all we really care about!)\n    labels = axis.get_ticklabels()\n    if labels:\n        props['fontsize'] = labels[0].get_fontsize()\n    else:\n        props['fontsize'] = None\n\n    # Get associated grid\n    props['grid'] = get_grid_style(axis)\n\n    return props",
  "def get_grid_style(axis):\n    gridlines = axis.get_gridlines()\n    if axis._gridOnMajor and len(gridlines) > 0:\n        color = color_to_hex(gridlines[0].get_color())\n        alpha = gridlines[0].get_alpha()\n        dasharray = get_dasharray(gridlines[0])\n        return dict(gridOn=True,\n                    color=color,\n                    dasharray=dasharray,\n                    alpha=alpha)\n    else:\n        return {\"gridOn\":False}",
  "def get_figure_properties(fig):\n    return {'figwidth': fig.get_figwidth(),\n            'figheight': fig.get_figheight(),\n            'dpi': fig.dpi}",
  "def get_axes_properties(ax):\n    props = {'axesbg': color_to_hex(ax.patch.get_facecolor()),\n             'axesbgalpha': ax.patch.get_alpha(),\n             'bounds': ax.get_position().bounds,\n             'dynamic': ax.get_navigate(),\n             'axes': [get_axis_properties(ax.xaxis),\n                      get_axis_properties(ax.yaxis)]}\n\n    for axname in ['x', 'y']:\n        axis = getattr(ax, axname + 'axis')\n        domain = getattr(ax, 'get_{0}lim'.format(axname))()\n        lim = domain\n        if isinstance(axis.converter, matplotlib.dates.DateConverter):\n            scale = 'date'\n            try:\n                import pandas as pd\n                from pandas.tseries.converter import PeriodConverter\n            except ImportError:\n                pd = None\n\n            if (pd is not None and isinstance(axis.converter,\n                                              PeriodConverter)):\n                _dates = [pd.Period(ordinal=int(d), freq=axis.freq)\n                          for d in domain]\n                domain = [(d.year, d.month - 1, d.day,\n                           d.hour, d.minute, d.second, 0)\n                          for d in _dates]\n            else:\n                domain = [(d.year, d.month - 1, d.day,\n                           d.hour, d.minute, d.second,\n                           d.microsecond * 1E-3)\n                          for d in matplotlib.dates.num2date(domain)]\n        else:\n            scale = axis.get_scale()\n\n        if scale not in ['date', 'linear', 'log']:\n            raise ValueError(\"Unknown axis scale: \"\n                             \"{0}\".format(axis[axname].get_scale()))\n\n        props[axname + 'scale'] = scale\n        props[axname + 'lim'] = lim\n        props[axname + 'domain'] = domain\n\n    return props",
  "def iter_all_children(obj, skipContainers=False):\n    \"\"\"\n    Returns an iterator over all childen and nested children using\n    obj's get_children() method\n\n    if skipContainers is true, only childless objects are returned.\n    \"\"\"\n    if hasattr(obj, 'get_children') and len(obj.get_children()) > 0:\n        for child in obj.get_children():\n            if not skipContainers:\n                yield child\n            # could use `yield from` in python 3...\n            for grandchild in iter_all_children(child, skipContainers):\n                yield grandchild\n    else:\n        yield obj",
  "def get_legend_properties(ax, legend):\n    handles, labels = ax.get_legend_handles_labels()\n    visible = legend.get_visible()\n    return {'handles': handles, 'labels': labels, 'visible': visible}",
  "def image_to_base64(image):\n    \"\"\"\n    Convert a matplotlib image to a base64 png representation\n\n    Parameters\n    ----------\n    image : matplotlib image object\n        The image to be converted.\n\n    Returns\n    -------\n    image_base64 : string\n        The UTF8-encoded base64 string representation of the png image.\n    \"\"\"\n    ax = image.axes\n    binary_buffer = io.BytesIO()\n\n    # image is saved in axes coordinates: we need to temporarily\n    # set the correct limits to get the correct image\n    lim = ax.axis()\n    ax.axis(image.get_extent())\n    image.write_png(binary_buffer)\n    ax.axis(lim)\n\n    binary_buffer.seek(0)\n    return base64.b64encode(binary_buffer.read()).decode('utf-8')",
  "def ipynb_vega_init():\n    \"\"\"Initialize the IPython notebook display elements\n\n    This function borrows heavily from the excellent vincent package:\n    http://github.com/wrobstory/vincent\n    \"\"\"\n    try:\n        from IPython.core.display import display, HTML\n    except ImportError:\n        print('IPython Notebook could not be loaded.')\n\n    require_js = '''\n    if (window['d3'] === undefined) {{\n        require.config({{ paths: {{d3: \"http://d3js.org/d3.v3.min\"}} }});\n        require([\"d3\"], function(d3) {{\n          window.d3 = d3;\n          {0}\n        }});\n    }};\n    if (window['topojson'] === undefined) {{\n        require.config(\n            {{ paths: {{topojson: \"http://d3js.org/topojson.v1.min\"}} }}\n            );\n        require([\"topojson\"], function(topojson) {{\n          window.topojson = topojson;\n        }});\n    }};\n    '''\n    d3_geo_projection_js_url = \"http://d3js.org/d3.geo.projection.v0.min.js\"\n    d3_layout_cloud_js_url = (\"http://wrobstory.github.io/d3-cloud/\"\n                              \"d3.layout.cloud.js\")\n    topojson_js_url = \"http://d3js.org/topojson.v1.min.js\"\n    vega_js_url = 'http://trifacta.github.com/vega/vega.js'\n\n    dep_libs = '''$.getScript(\"%s\", function() {\n        $.getScript(\"%s\", function() {\n            $.getScript(\"%s\", function() {\n                $.getScript(\"%s\", function() {\n                        $([IPython.events]).trigger(\"vega_loaded.vincent\");\n                })\n            })\n        })\n    });''' % (d3_geo_projection_js_url, d3_layout_cloud_js_url,\n              topojson_js_url, vega_js_url)\n    load_js = require_js.format(dep_libs)\n    html = '<script>'+load_js+'</script>'\n    display(HTML(html))",
  "class Exporter(object):\n    \"\"\"Matplotlib Exporter\n\n    Parameters\n    ----------\n    renderer : Renderer object\n        The renderer object called by the exporter to create a figure\n        visualization.  See mplexporter.Renderer for information on the\n        methods which should be defined within the renderer.\n    close_mpl : bool\n        If True (default), close the matplotlib figure as it is rendered. This\n        is useful for when the exporter is used within the notebook, or with\n        an interactive matplotlib backend.\n    \"\"\"\n\n    def __init__(self, renderer, close_mpl=True):\n        self.close_mpl = close_mpl\n        self.renderer = renderer\n\n    def run(self, fig):\n        \"\"\"\n        Run the exporter on the given figure\n\n        Parmeters\n        ---------\n        fig : matplotlib.Figure instance\n            The figure to export\n        \"\"\"\n        # Calling savefig executes the draw() command, putting elements\n        # in the correct place.\n        fig.savefig(io.BytesIO(), format='png', dpi=fig.dpi)\n        if self.close_mpl:\n            import matplotlib.pyplot as plt\n            plt.close(fig)\n        self.crawl_fig(fig)\n\n    @staticmethod\n    def process_transform(transform, ax=None, data=None, return_trans=False,\n                          force_trans=None):\n        \"\"\"Process the transform and convert data to figure or data coordinates\n\n        Parameters\n        ----------\n        transform : matplotlib Transform object\n            The transform applied to the data\n        ax : matplotlib Axes object (optional)\n            The axes the data is associated with\n        data : ndarray (optional)\n            The array of data to be transformed.\n        return_trans : bool (optional)\n            If true, return the final transform of the data\n        force_trans : matplotlib.transform instance (optional)\n            If supplied, first force the data to this transform\n\n        Returns\n        -------\n        code : string\n            Code is either \"data\", \"axes\", \"figure\", or \"display\", indicating\n            the type of coordinates output.\n        transform : matplotlib transform\n            the transform used to map input data to output data.\n            Returned only if return_trans is True\n        new_data : ndarray\n            Data transformed to match the given coordinate code.\n            Returned only if data is specified\n        \"\"\"\n        if isinstance(transform, transforms.BlendedGenericTransform):\n            warnings.warn(\"Blended transforms not yet supported. \"\n                          \"Zoom behavior may not work as expected.\")\n\n        if force_trans is not None:\n            if data is not None:\n                data = (transform - force_trans).transform(data)\n            transform = force_trans\n\n        code = \"display\"\n        if ax is not None:\n            for (c, trans) in [(\"data\", ax.transData),\n                               (\"axes\", ax.transAxes),\n                               (\"figure\", ax.figure.transFigure),\n                               (\"display\", transforms.IdentityTransform())]:\n                if transform.contains_branch(trans):\n                    code, transform = (c, transform - trans)\n                    break\n\n        if data is not None:\n            if return_trans:\n                return code, transform.transform(data), transform\n            else:\n                return code, transform.transform(data)\n        else:\n            if return_trans:\n                return code, transform\n            else:\n                return code\n\n    def crawl_fig(self, fig):\n        \"\"\"Crawl the figure and process all axes\"\"\"\n        with self.renderer.draw_figure(fig=fig,\n                                       props=utils.get_figure_properties(fig)):\n            for ax in fig.axes:\n                self.crawl_ax(ax)\n\n    def crawl_ax(self, ax):\n        \"\"\"Crawl the axes and process all elements within\"\"\"\n        with self.renderer.draw_axes(ax=ax,\n                                     props=utils.get_axes_properties(ax)):\n            for line in ax.lines:\n                self.draw_line(ax, line)\n            for text in ax.texts:\n                self.draw_text(ax, text)\n            for (text, ttp) in zip([ax.xaxis.label, ax.yaxis.label, ax.title],\n                                   [\"xlabel\", \"ylabel\", \"title\"]):\n                if(hasattr(text, 'get_text') and text.get_text()):\n                    self.draw_text(ax, text, force_trans=ax.transAxes,\n                                   text_type=ttp)\n            for artist in ax.artists:\n                # TODO: process other artists\n                if isinstance(artist, matplotlib.text.Text):\n                    self.draw_text(ax, artist)\n            for patch in ax.patches:\n                self.draw_patch(ax, patch)\n            for collection in ax.collections:\n                self.draw_collection(ax, collection)\n            for image in ax.images:\n                self.draw_image(ax, image)\n\n            legend = ax.get_legend()\n            if legend is not None:\n                props = utils.get_legend_properties(ax, legend)\n                with self.renderer.draw_legend(legend=legend, props=props):\n                    if props['visible']:\n                        self.crawl_legend(ax, legend)\n\n    def crawl_legend(self, ax, legend):\n        \"\"\"\n        Recursively look through objects in legend children\n        \"\"\"\n        legendElements = list(utils.iter_all_children(legend._legend_box,\n                                                      skipContainers=True))\n        legendElements.append(legend.legendPatch)\n        for child in legendElements:\n            # force a large zorder so it appears on top\n            child.set_zorder(1E6 + child.get_zorder())\n\n            try:\n                # What kind of object...\n                if isinstance(child, matplotlib.patches.Patch):\n                    self.draw_patch(ax, child, force_trans=ax.transAxes)\n                elif isinstance(child, matplotlib.text.Text):\n                    if not (child is legend.get_children()[-1]\n                            and child.get_text() == 'None'):\n                        self.draw_text(ax, child, force_trans=ax.transAxes)\n                elif isinstance(child, matplotlib.lines.Line2D):\n                    self.draw_line(ax, child, force_trans=ax.transAxes)\n                else:\n                    warnings.warn(\"Legend element %s not impemented\" % child)\n            except NotImplementedError:\n                warnings.warn(\"Legend element %s not impemented\" % child)\n\n    def draw_line(self, ax, line, force_trans=None):\n        \"\"\"Process a matplotlib line and call renderer.draw_line\"\"\"\n        coordinates, data = self.process_transform(line.get_transform(),\n                                                   ax, line.get_xydata(),\n                                                   force_trans=force_trans)\n        linestyle = utils.get_line_style(line)\n        if linestyle['dasharray'] in ['None', 'none', None]:\n            linestyle = None\n        markerstyle = utils.get_marker_style(line)\n        if (markerstyle['marker'] in ['None', 'none', None]\n                or markerstyle['markerpath'][0].size == 0):\n            markerstyle = None\n        label = line.get_label()\n        if markerstyle or linestyle:\n            self.renderer.draw_marked_line(data=data, coordinates=coordinates,\n                                           linestyle=linestyle,\n                                           markerstyle=markerstyle,\n                                           label=label,\n                                           mplobj=line)\n\n    def draw_text(self, ax, text, force_trans=None, text_type=None):\n        \"\"\"Process a matplotlib text object and call renderer.draw_text\"\"\"\n        content = text.get_text()\n        if content:\n            transform = text.get_transform()\n            position = text.get_position()\n            coords, position = self.process_transform(transform, ax,\n                                                      position,\n                                                      force_trans=force_trans)\n            style = utils.get_text_style(text)\n            self.renderer.draw_text(text=content, position=position,\n                                    coordinates=coords,\n                                    text_type=text_type,\n                                    style=style, mplobj=text)\n\n    def draw_patch(self, ax, patch, force_trans=None):\n        \"\"\"Process a matplotlib patch object and call renderer.draw_path\"\"\"\n        vertices, pathcodes = utils.SVG_path(patch.get_path())\n        transform = patch.get_transform()\n        coordinates, vertices = self.process_transform(transform,\n                                                       ax, vertices,\n                                                       force_trans=force_trans)\n        linestyle = utils.get_path_style(patch, fill=patch.get_fill())\n        self.renderer.draw_path(data=vertices,\n                                coordinates=coordinates,\n                                pathcodes=pathcodes,\n                                style=linestyle,\n                                mplobj=patch)\n\n    def draw_collection(self, ax, collection,\n                        force_pathtrans=None,\n                        force_offsettrans=None):\n        \"\"\"Process a matplotlib collection and call renderer.draw_collection\"\"\"\n        (transform, transOffset,\n         offsets, paths) = collection._prepare_points()\n\n        offset_coords, offsets = self.process_transform(\n            transOffset, ax, offsets, force_trans=force_offsettrans)\n\n        processed_paths = [utils.SVG_path(path) for path in paths]\n        path_coords, tr = self.process_transform(\n            transform, ax, return_trans=True, force_trans=force_pathtrans)\n\n        processed_paths = [(tr.transform(path[0]), path[1])\n                           for path in processed_paths]\n\n        path_transforms = collection.get_transforms()\n        try:\n            # matplotlib 1.3: path_transforms are transform objects.\n            # Convert them to numpy arrays.\n            path_transforms = [t.get_matrix() for t in path_transforms]\n        except AttributeError:\n            # matplotlib 1.4: path transforms are already numpy arrays.\n            pass\n\n        styles = {'linewidth': collection.get_linewidths(),\n                  'facecolor': collection.get_facecolors(),\n                  'edgecolor': collection.get_edgecolors(),\n                  'alpha': collection._alpha,\n                  'zorder': collection.get_zorder()}\n\n        offset_dict = {\"data\": \"before\",\n                       \"screen\": \"after\"}\n        offset_order = offset_dict[collection.get_offset_position()]\n\n        self.renderer.draw_path_collection(paths=processed_paths,\n                                           path_coordinates=path_coords,\n                                           path_transforms=path_transforms,\n                                           offsets=offsets,\n                                           offset_coordinates=offset_coords,\n                                           offset_order=offset_order,\n                                           styles=styles,\n                                           mplobj=collection)\n\n    def draw_image(self, ax, image):\n        \"\"\"Process a matplotlib image object and call renderer.draw_image\"\"\"\n        self.renderer.draw_image(imdata=utils.image_to_base64(image),\n                                 extent=image.get_extent(),\n                                 coordinates=\"data\",\n                                 style={\"alpha\": image.get_alpha(),\n                                        \"zorder\": image.get_zorder()},\n                                 mplobj=image)",
  "def __init__(self, renderer, close_mpl=True):\n        self.close_mpl = close_mpl\n        self.renderer = renderer",
  "def run(self, fig):\n        \"\"\"\n        Run the exporter on the given figure\n\n        Parmeters\n        ---------\n        fig : matplotlib.Figure instance\n            The figure to export\n        \"\"\"\n        # Calling savefig executes the draw() command, putting elements\n        # in the correct place.\n        fig.savefig(io.BytesIO(), format='png', dpi=fig.dpi)\n        if self.close_mpl:\n            import matplotlib.pyplot as plt\n            plt.close(fig)\n        self.crawl_fig(fig)",
  "def process_transform(transform, ax=None, data=None, return_trans=False,\n                          force_trans=None):\n        \"\"\"Process the transform and convert data to figure or data coordinates\n\n        Parameters\n        ----------\n        transform : matplotlib Transform object\n            The transform applied to the data\n        ax : matplotlib Axes object (optional)\n            The axes the data is associated with\n        data : ndarray (optional)\n            The array of data to be transformed.\n        return_trans : bool (optional)\n            If true, return the final transform of the data\n        force_trans : matplotlib.transform instance (optional)\n            If supplied, first force the data to this transform\n\n        Returns\n        -------\n        code : string\n            Code is either \"data\", \"axes\", \"figure\", or \"display\", indicating\n            the type of coordinates output.\n        transform : matplotlib transform\n            the transform used to map input data to output data.\n            Returned only if return_trans is True\n        new_data : ndarray\n            Data transformed to match the given coordinate code.\n            Returned only if data is specified\n        \"\"\"\n        if isinstance(transform, transforms.BlendedGenericTransform):\n            warnings.warn(\"Blended transforms not yet supported. \"\n                          \"Zoom behavior may not work as expected.\")\n\n        if force_trans is not None:\n            if data is not None:\n                data = (transform - force_trans).transform(data)\n            transform = force_trans\n\n        code = \"display\"\n        if ax is not None:\n            for (c, trans) in [(\"data\", ax.transData),\n                               (\"axes\", ax.transAxes),\n                               (\"figure\", ax.figure.transFigure),\n                               (\"display\", transforms.IdentityTransform())]:\n                if transform.contains_branch(trans):\n                    code, transform = (c, transform - trans)\n                    break\n\n        if data is not None:\n            if return_trans:\n                return code, transform.transform(data), transform\n            else:\n                return code, transform.transform(data)\n        else:\n            if return_trans:\n                return code, transform\n            else:\n                return code",
  "def crawl_fig(self, fig):\n        \"\"\"Crawl the figure and process all axes\"\"\"\n        with self.renderer.draw_figure(fig=fig,\n                                       props=utils.get_figure_properties(fig)):\n            for ax in fig.axes:\n                self.crawl_ax(ax)",
  "def crawl_ax(self, ax):\n        \"\"\"Crawl the axes and process all elements within\"\"\"\n        with self.renderer.draw_axes(ax=ax,\n                                     props=utils.get_axes_properties(ax)):\n            for line in ax.lines:\n                self.draw_line(ax, line)\n            for text in ax.texts:\n                self.draw_text(ax, text)\n            for (text, ttp) in zip([ax.xaxis.label, ax.yaxis.label, ax.title],\n                                   [\"xlabel\", \"ylabel\", \"title\"]):\n                if(hasattr(text, 'get_text') and text.get_text()):\n                    self.draw_text(ax, text, force_trans=ax.transAxes,\n                                   text_type=ttp)\n            for artist in ax.artists:\n                # TODO: process other artists\n                if isinstance(artist, matplotlib.text.Text):\n                    self.draw_text(ax, artist)\n            for patch in ax.patches:\n                self.draw_patch(ax, patch)\n            for collection in ax.collections:\n                self.draw_collection(ax, collection)\n            for image in ax.images:\n                self.draw_image(ax, image)\n\n            legend = ax.get_legend()\n            if legend is not None:\n                props = utils.get_legend_properties(ax, legend)\n                with self.renderer.draw_legend(legend=legend, props=props):\n                    if props['visible']:\n                        self.crawl_legend(ax, legend)",
  "def crawl_legend(self, ax, legend):\n        \"\"\"\n        Recursively look through objects in legend children\n        \"\"\"\n        legendElements = list(utils.iter_all_children(legend._legend_box,\n                                                      skipContainers=True))\n        legendElements.append(legend.legendPatch)\n        for child in legendElements:\n            # force a large zorder so it appears on top\n            child.set_zorder(1E6 + child.get_zorder())\n\n            try:\n                # What kind of object...\n                if isinstance(child, matplotlib.patches.Patch):\n                    self.draw_patch(ax, child, force_trans=ax.transAxes)\n                elif isinstance(child, matplotlib.text.Text):\n                    if not (child is legend.get_children()[-1]\n                            and child.get_text() == 'None'):\n                        self.draw_text(ax, child, force_trans=ax.transAxes)\n                elif isinstance(child, matplotlib.lines.Line2D):\n                    self.draw_line(ax, child, force_trans=ax.transAxes)\n                else:\n                    warnings.warn(\"Legend element %s not impemented\" % child)\n            except NotImplementedError:\n                warnings.warn(\"Legend element %s not impemented\" % child)",
  "def draw_line(self, ax, line, force_trans=None):\n        \"\"\"Process a matplotlib line and call renderer.draw_line\"\"\"\n        coordinates, data = self.process_transform(line.get_transform(),\n                                                   ax, line.get_xydata(),\n                                                   force_trans=force_trans)\n        linestyle = utils.get_line_style(line)\n        if linestyle['dasharray'] in ['None', 'none', None]:\n            linestyle = None\n        markerstyle = utils.get_marker_style(line)\n        if (markerstyle['marker'] in ['None', 'none', None]\n                or markerstyle['markerpath'][0].size == 0):\n            markerstyle = None\n        label = line.get_label()\n        if markerstyle or linestyle:\n            self.renderer.draw_marked_line(data=data, coordinates=coordinates,\n                                           linestyle=linestyle,\n                                           markerstyle=markerstyle,\n                                           label=label,\n                                           mplobj=line)",
  "def draw_text(self, ax, text, force_trans=None, text_type=None):\n        \"\"\"Process a matplotlib text object and call renderer.draw_text\"\"\"\n        content = text.get_text()\n        if content:\n            transform = text.get_transform()\n            position = text.get_position()\n            coords, position = self.process_transform(transform, ax,\n                                                      position,\n                                                      force_trans=force_trans)\n            style = utils.get_text_style(text)\n            self.renderer.draw_text(text=content, position=position,\n                                    coordinates=coords,\n                                    text_type=text_type,\n                                    style=style, mplobj=text)",
  "def draw_patch(self, ax, patch, force_trans=None):\n        \"\"\"Process a matplotlib patch object and call renderer.draw_path\"\"\"\n        vertices, pathcodes = utils.SVG_path(patch.get_path())\n        transform = patch.get_transform()\n        coordinates, vertices = self.process_transform(transform,\n                                                       ax, vertices,\n                                                       force_trans=force_trans)\n        linestyle = utils.get_path_style(patch, fill=patch.get_fill())\n        self.renderer.draw_path(data=vertices,\n                                coordinates=coordinates,\n                                pathcodes=pathcodes,\n                                style=linestyle,\n                                mplobj=patch)",
  "def draw_collection(self, ax, collection,\n                        force_pathtrans=None,\n                        force_offsettrans=None):\n        \"\"\"Process a matplotlib collection and call renderer.draw_collection\"\"\"\n        (transform, transOffset,\n         offsets, paths) = collection._prepare_points()\n\n        offset_coords, offsets = self.process_transform(\n            transOffset, ax, offsets, force_trans=force_offsettrans)\n\n        processed_paths = [utils.SVG_path(path) for path in paths]\n        path_coords, tr = self.process_transform(\n            transform, ax, return_trans=True, force_trans=force_pathtrans)\n\n        processed_paths = [(tr.transform(path[0]), path[1])\n                           for path in processed_paths]\n\n        path_transforms = collection.get_transforms()\n        try:\n            # matplotlib 1.3: path_transforms are transform objects.\n            # Convert them to numpy arrays.\n            path_transforms = [t.get_matrix() for t in path_transforms]\n        except AttributeError:\n            # matplotlib 1.4: path transforms are already numpy arrays.\n            pass\n\n        styles = {'linewidth': collection.get_linewidths(),\n                  'facecolor': collection.get_facecolors(),\n                  'edgecolor': collection.get_edgecolors(),\n                  'alpha': collection._alpha,\n                  'zorder': collection.get_zorder()}\n\n        offset_dict = {\"data\": \"before\",\n                       \"screen\": \"after\"}\n        offset_order = offset_dict[collection.get_offset_position()]\n\n        self.renderer.draw_path_collection(paths=processed_paths,\n                                           path_coordinates=path_coords,\n                                           path_transforms=path_transforms,\n                                           offsets=offsets,\n                                           offset_coordinates=offset_coords,\n                                           offset_order=offset_order,\n                                           styles=styles,\n                                           mplobj=collection)",
  "def draw_image(self, ax, image):\n        \"\"\"Process a matplotlib image object and call renderer.draw_image\"\"\"\n        self.renderer.draw_image(imdata=utils.image_to_base64(image),\n                                 extent=image.get_extent(),\n                                 coordinates=\"data\",\n                                 style={\"alpha\": image.get_alpha(),\n                                        \"zorder\": image.get_zorder()},\n                                 mplobj=image)",
  "class VincentRenderer(Renderer):\n    def open_figure(self, fig, props):\n        self.chart = None\n        self.figwidth = int(props['figwidth'] * props['dpi'])\n        self.figheight = int(props['figheight'] * props['dpi'])\n\n    def draw_line(self, data, coordinates, style, label, mplobj=None):\n        import vincent  # only import if VincentRenderer is used\n        if coordinates != 'data':\n            warnings.warn(\"Only data coordinates supported. Skipping this\")\n        linedata = {'x': data[:, 0],\n                    'y': data[:, 1]}\n        line = vincent.Line(linedata, iter_idx='x',\n                            width=self.figwidth, height=self.figheight)\n\n        # TODO: respect the other style settings\n        line.scales['color'].range = [style['color']]\n\n        if self.chart is None:\n            self.chart = line\n        else:\n            warnings.warn(\"Multiple plot elements not yet supported\")\n\n    def draw_markers(self, data, coordinates, style, label, mplobj=None):\n        import vincent  # only import if VincentRenderer is used\n        if coordinates != 'data':\n            warnings.warn(\"Only data coordinates supported. Skipping this\")\n        markerdata = {'x': data[:, 0],\n                      'y': data[:, 1]}\n        markers = vincent.Scatter(markerdata, iter_idx='x',\n                                  width=self.figwidth, height=self.figheight)\n\n        # TODO: respect the other style settings\n        markers.scales['color'].range = [style['facecolor']]\n\n        if self.chart is None:\n            self.chart = markers\n        else:\n            warnings.warn(\"Multiple plot elements not yet supported\")",
  "def fig_to_vincent(fig):\n    \"\"\"Convert a matplotlib figure to a vincent object\"\"\"\n    renderer = VincentRenderer()\n    exporter = Exporter(renderer)\n    exporter.run(fig)\n    return renderer.chart",
  "def open_figure(self, fig, props):\n        self.chart = None\n        self.figwidth = int(props['figwidth'] * props['dpi'])\n        self.figheight = int(props['figheight'] * props['dpi'])",
  "def draw_line(self, data, coordinates, style, label, mplobj=None):\n        import vincent  # only import if VincentRenderer is used\n        if coordinates != 'data':\n            warnings.warn(\"Only data coordinates supported. Skipping this\")\n        linedata = {'x': data[:, 0],\n                    'y': data[:, 1]}\n        line = vincent.Line(linedata, iter_idx='x',\n                            width=self.figwidth, height=self.figheight)\n\n        # TODO: respect the other style settings\n        line.scales['color'].range = [style['color']]\n\n        if self.chart is None:\n            self.chart = line\n        else:\n            warnings.warn(\"Multiple plot elements not yet supported\")",
  "def draw_markers(self, data, coordinates, style, label, mplobj=None):\n        import vincent  # only import if VincentRenderer is used\n        if coordinates != 'data':\n            warnings.warn(\"Only data coordinates supported. Skipping this\")\n        markerdata = {'x': data[:, 0],\n                      'y': data[:, 1]}\n        markers = vincent.Scatter(markerdata, iter_idx='x',\n                                  width=self.figwidth, height=self.figheight)\n\n        # TODO: respect the other style settings\n        markers.scales['color'].range = [style['facecolor']]\n\n        if self.chart is None:\n            self.chart = markers\n        else:\n            warnings.warn(\"Multiple plot elements not yet supported\")",
  "class Renderer(object):\n    @staticmethod\n    def ax_zoomable(ax):\n        return bool(ax and ax.get_navigate())\n\n    @staticmethod\n    def ax_has_xgrid(ax):\n        return bool(ax and ax.xaxis._gridOnMajor and ax.yaxis.get_gridlines())\n\n    @staticmethod\n    def ax_has_ygrid(ax):\n        return bool(ax and ax.yaxis._gridOnMajor and ax.yaxis.get_gridlines())\n\n    @property\n    def current_ax_zoomable(self):\n        return self.ax_zoomable(self._current_ax)\n\n    @property\n    def current_ax_has_xgrid(self):\n        return self.ax_has_xgrid(self._current_ax)\n\n    @property\n    def current_ax_has_ygrid(self):\n        return self.ax_has_ygrid(self._current_ax)\n\n    @contextmanager\n    def draw_figure(self, fig, props):\n        if hasattr(self, \"_current_fig\") and self._current_fig is not None:\n            warnings.warn(\"figure embedded in figure: something is wrong\")\n        self._current_fig = fig\n        self._fig_props = props\n        self.open_figure(fig=fig, props=props)\n        yield\n        self.close_figure(fig=fig)\n        self._current_fig = None\n        self._fig_props = {}\n\n    @contextmanager\n    def draw_axes(self, ax, props):\n        if hasattr(self, \"_current_ax\") and self._current_ax is not None:\n            warnings.warn(\"axes embedded in axes: something is wrong\")\n        self._current_ax = ax\n        self._ax_props = props\n        self.open_axes(ax=ax, props=props)\n        yield\n        self.close_axes(ax=ax)\n        self._current_ax = None\n        self._ax_props = {}\n\n    @contextmanager\n    def draw_legend(self, legend, props):\n        self._current_legend = legend\n        self._legend_props = props\n        self.open_legend(legend=legend, props=props)\n        yield\n        self.close_legend(legend=legend)\n        self._current_legend = None\n        self._legend_props = {}\n\n    # Following are the functions which should be overloaded in subclasses\n\n    def open_figure(self, fig, props):\n        \"\"\"\n        Begin commands for a particular figure.\n\n        Parameters\n        ----------\n        fig : matplotlib.Figure\n            The Figure which will contain the ensuing axes and elements\n        props : dictionary\n            The dictionary of figure properties\n        \"\"\"\n        pass\n\n    def close_figure(self, fig):\n        \"\"\"\n        Finish commands for a particular figure.\n\n        Parameters\n        ----------\n        fig : matplotlib.Figure\n            The figure which is finished being drawn.\n        \"\"\"\n        pass\n\n    def open_axes(self, ax, props):\n        \"\"\"\n        Begin commands for a particular axes.\n\n        Parameters\n        ----------\n        ax : matplotlib.Axes\n            The Axes which will contain the ensuing axes and elements\n        props : dictionary\n            The dictionary of axes properties\n        \"\"\"\n        pass\n\n    def close_axes(self, ax):\n        \"\"\"\n        Finish commands for a particular axes.\n\n        Parameters\n        ----------\n        ax : matplotlib.Axes\n            The Axes which is finished being drawn.\n        \"\"\"\n        pass\n\n    def open_legend(self, legend, props):\n        \"\"\"\n        Beging commands for a particular legend.\n\n        Parameters\n        ----------\n        legend : matplotlib.legend.Legend\n                The Legend that will contain the ensuing elements\n        props : dictionary\n                The dictionary of legend properties\n        \"\"\"\n        pass\n\n    def close_legend(self, legend):\n        \"\"\"\n        Finish commands for a particular legend.\n\n        Parameters\n        ----------\n        legend : matplotlib.legend.Legend\n                The Legend which is finished being drawn\n        \"\"\"\n        pass\n\n    def draw_marked_line(self, data, coordinates, linestyle, markerstyle,\n                         label, mplobj=None):\n        \"\"\"Draw a line that also has markers.\n\n        If this isn't reimplemented by a renderer object, by default, it will\n        make a call to BOTH draw_line and draw_markers when both markerstyle\n        and linestyle are not None in the same Line2D object.\n\n        \"\"\"\n        if linestyle is not None:\n            self.draw_line(data, coordinates, linestyle, label, mplobj)\n        if markerstyle is not None:\n            self.draw_markers(data, coordinates, markerstyle, label, mplobj)\n\n    def draw_line(self, data, coordinates, style, label, mplobj=None):\n        \"\"\"\n        Draw a line. By default, draw the line via the draw_path() command.\n        Some renderers might wish to override this and provide more\n        fine-grained behavior.\n\n        In matplotlib, lines are generally created via the plt.plot() command,\n        though this command also can create marker collections.\n\n        Parameters\n        ----------\n        data : array_like\n            A shape (N, 2) array of datapoints.\n        coordinates : string\n            A string code, which should be either 'data' for data coordinates,\n            or 'figure' for figure (pixel) coordinates.\n        style : dictionary\n            a dictionary specifying the appearance of the line.\n        mplobj : matplotlib object\n            the matplotlib plot element which generated this line\n        \"\"\"\n        pathcodes = ['M'] + (data.shape[0] - 1) * ['L']\n        pathstyle = dict(facecolor='none', **style)\n        pathstyle['edgecolor'] = pathstyle.pop('color')\n        pathstyle['edgewidth'] = pathstyle.pop('linewidth')\n        self.draw_path(data=data, coordinates=coordinates,\n                       pathcodes=pathcodes, style=pathstyle, mplobj=mplobj)\n\n    @staticmethod\n    def _iter_path_collection(paths, path_transforms, offsets, styles):\n        \"\"\"Build an iterator over the elements of the path collection\"\"\"\n        N = max(len(paths), len(offsets))\n\n        if not path_transforms:\n            path_transforms = [np.eye(3)]\n\n        edgecolor = styles['edgecolor']\n        if np.size(edgecolor) == 0:\n            edgecolor = ['none']\n        facecolor = styles['facecolor']\n        if np.size(facecolor) == 0:\n            facecolor = ['none']\n\n        elements = [paths, path_transforms, offsets,\n                    edgecolor, styles['linewidth'], facecolor]\n\n        it = itertools\n        return it.islice(py3k.zip(*py3k.map(it.cycle, elements)), N)\n\n    def draw_path_collection(self, paths, path_coordinates, path_transforms,\n                             offsets, offset_coordinates, offset_order,\n                             styles, mplobj=None):\n        \"\"\"\n        Draw a collection of paths. The paths, offsets, and styles are all\n        iterables, and the number of paths is max(len(paths), len(offsets)).\n\n        By default, this is implemented via multiple calls to the draw_path()\n        function. For efficiency, Renderers may choose to customize this\n        implementation.\n\n        Examples of path collections created by matplotlib are scatter plots,\n        histograms, contour plots, and many others.\n\n        Parameters\n        ----------\n        paths : list\n            list of tuples, where each tuple has two elements:\n            (data, pathcodes).  See draw_path() for a description of these.\n        path_coordinates: string\n            the coordinates code for the paths, which should be either\n            'data' for data coordinates, or 'figure' for figure (pixel)\n            coordinates.\n        path_transforms: array_like\n            an array of shape (*, 3, 3), giving a series of 2D Affine\n            transforms for the paths. These encode translations, rotations,\n            and scalings in the standard way.\n        offsets: array_like\n            An array of offsets of shape (N, 2)\n        offset_coordinates : string\n            the coordinates code for the offsets, which should be either\n            'data' for data coordinates, or 'figure' for figure (pixel)\n            coordinates.\n        offset_order : string\n            either \"before\" or \"after\". This specifies whether the offset\n            is applied before the path transform, or after.  The matplotlib\n            backend equivalent is \"before\"->\"data\", \"after\"->\"screen\".\n        styles: dictionary\n            A dictionary in which each value is a list of length N, containing\n            the style(s) for the paths.\n        mplobj : matplotlib object\n            the matplotlib plot element which generated this collection\n        \"\"\"\n        if offset_order == \"before\":\n            raise NotImplementedError(\"offset before transform\")\n\n        for tup in self._iter_path_collection(paths, path_transforms,\n                                              offsets, styles):\n            (path, path_transform, offset, ec, lw, fc) = tup\n            vertices, pathcodes = path\n            path_transform = transforms.Affine2D(path_transform)\n            vertices = path_transform.transform(vertices)\n            # This is a hack:\n            if path_coordinates == \"figure\":\n                path_coordinates = \"points\"\n            style = {\"edgecolor\": utils.color_to_hex(ec),\n                     \"facecolor\": utils.color_to_hex(fc),\n                     \"edgewidth\": lw,\n                     \"dasharray\": \"10,0\",\n                     \"alpha\": styles['alpha'],\n                     \"zorder\": styles['zorder']}\n            self.draw_path(data=vertices, coordinates=path_coordinates,\n                           pathcodes=pathcodes, style=style, offset=offset,\n                           offset_coordinates=offset_coordinates,\n                           mplobj=mplobj)\n\n    def draw_markers(self, data, coordinates, style, label, mplobj=None):\n        \"\"\"\n        Draw a set of markers. By default, this is done by repeatedly\n        calling draw_path(), but renderers should generally overload\n        this method to provide a more efficient implementation.\n\n        In matplotlib, markers are created using the plt.plot() command.\n\n        Parameters\n        ----------\n        data : array_like\n            A shape (N, 2) array of datapoints.\n        coordinates : string\n            A string code, which should be either 'data' for data coordinates,\n            or 'figure' for figure (pixel) coordinates.\n        style : dictionary\n            a dictionary specifying the appearance of the markers.\n        mplobj : matplotlib object\n            the matplotlib plot element which generated this marker collection\n        \"\"\"\n        vertices, pathcodes = style['markerpath']\n        pathstyle = dict((key, style[key]) for key in ['alpha', 'edgecolor',\n                                                       'facecolor', 'zorder',\n                                                       'edgewidth'])\n        pathstyle['dasharray'] = \"10,0\"\n        for vertex in data:\n            self.draw_path(data=vertices, coordinates=\"points\",\n                           pathcodes=pathcodes, style=pathstyle,\n                           offset=vertex, offset_coordinates=coordinates,\n                           mplobj=mplobj)\n\n    def draw_text(self, text, position, coordinates, style,\n                  text_type=None, mplobj=None):\n        \"\"\"\n        Draw text on the image.\n\n        Parameters\n        ----------\n        text : string\n            The text to draw\n        position : tuple\n            The (x, y) position of the text\n        coordinates : string\n            A string code, which should be either 'data' for data coordinates,\n            or 'figure' for figure (pixel) coordinates.\n        style : dictionary\n            a dictionary specifying the appearance of the text.\n        text_type : string or None\n            if specified, a type of text such as \"xlabel\", \"ylabel\", \"title\"\n        mplobj : matplotlib object\n            the matplotlib plot element which generated this text\n        \"\"\"\n        raise NotImplementedError()\n\n    def draw_path(self, data, coordinates, pathcodes, style,\n                  offset=None, offset_coordinates=\"data\", mplobj=None):\n        \"\"\"\n        Draw a path.\n\n        In matplotlib, paths are created by filled regions, histograms,\n        contour plots, patches, etc.\n\n        Parameters\n        ----------\n        data : array_like\n            A shape (N, 2) array of datapoints.\n        coordinates : string\n            A string code, which should be either 'data' for data coordinates,\n            'figure' for figure (pixel) coordinates, or \"points\" for raw\n            point coordinates (useful in conjunction with offsets, below).\n        pathcodes : list\n            A list of single-character SVG pathcodes associated with the data.\n            Path codes are one of ['M', 'm', 'L', 'l', 'Q', 'q', 'T', 't',\n                                   'S', 's', 'C', 'c', 'Z', 'z']\n            See the SVG specification for details.  Note that some path codes\n            consume more than one datapoint (while 'Z' consumes none), so\n            in general, the length of the pathcodes list will not be the same\n            as that of the data array.\n        style : dictionary\n            a dictionary specifying the appearance of the line.\n        offset : list (optional)\n            the (x, y) offset of the path. If not given, no offset will\n            be used.\n        offset_coordinates : string (optional)\n            A string code, which should be either 'data' for data coordinates,\n            or 'figure' for figure (pixel) coordinates.\n        mplobj : matplotlib object\n            the matplotlib plot element which generated this path\n        \"\"\"\n        raise NotImplementedError()\n\n    def draw_image(self, imdata, extent, coordinates, style, mplobj=None):\n        \"\"\"\n        Draw an image.\n\n        Parameters\n        ----------\n        imdata : string\n            base64 encoded png representation of the image\n        extent : list\n            the axes extent of the image: [xmin, xmax, ymin, ymax]\n        coordinates: string\n            A string code, which should be either 'data' for data coordinates,\n            or 'figure' for figure (pixel) coordinates.\n        style : dictionary\n            a dictionary specifying the appearance of the image\n        mplobj : matplotlib object\n            the matplotlib plot object which generated this image\n        \"\"\"\n        raise NotImplementedError()",
  "def ax_zoomable(ax):\n        return bool(ax and ax.get_navigate())",
  "def ax_has_xgrid(ax):\n        return bool(ax and ax.xaxis._gridOnMajor and ax.yaxis.get_gridlines())",
  "def ax_has_ygrid(ax):\n        return bool(ax and ax.yaxis._gridOnMajor and ax.yaxis.get_gridlines())",
  "def current_ax_zoomable(self):\n        return self.ax_zoomable(self._current_ax)",
  "def current_ax_has_xgrid(self):\n        return self.ax_has_xgrid(self._current_ax)",
  "def current_ax_has_ygrid(self):\n        return self.ax_has_ygrid(self._current_ax)",
  "def draw_figure(self, fig, props):\n        if hasattr(self, \"_current_fig\") and self._current_fig is not None:\n            warnings.warn(\"figure embedded in figure: something is wrong\")\n        self._current_fig = fig\n        self._fig_props = props\n        self.open_figure(fig=fig, props=props)\n        yield\n        self.close_figure(fig=fig)\n        self._current_fig = None\n        self._fig_props = {}",
  "def draw_axes(self, ax, props):\n        if hasattr(self, \"_current_ax\") and self._current_ax is not None:\n            warnings.warn(\"axes embedded in axes: something is wrong\")\n        self._current_ax = ax\n        self._ax_props = props\n        self.open_axes(ax=ax, props=props)\n        yield\n        self.close_axes(ax=ax)\n        self._current_ax = None\n        self._ax_props = {}",
  "def draw_legend(self, legend, props):\n        self._current_legend = legend\n        self._legend_props = props\n        self.open_legend(legend=legend, props=props)\n        yield\n        self.close_legend(legend=legend)\n        self._current_legend = None\n        self._legend_props = {}",
  "def open_figure(self, fig, props):\n        \"\"\"\n        Begin commands for a particular figure.\n\n        Parameters\n        ----------\n        fig : matplotlib.Figure\n            The Figure which will contain the ensuing axes and elements\n        props : dictionary\n            The dictionary of figure properties\n        \"\"\"\n        pass",
  "def close_figure(self, fig):\n        \"\"\"\n        Finish commands for a particular figure.\n\n        Parameters\n        ----------\n        fig : matplotlib.Figure\n            The figure which is finished being drawn.\n        \"\"\"\n        pass",
  "def open_axes(self, ax, props):\n        \"\"\"\n        Begin commands for a particular axes.\n\n        Parameters\n        ----------\n        ax : matplotlib.Axes\n            The Axes which will contain the ensuing axes and elements\n        props : dictionary\n            The dictionary of axes properties\n        \"\"\"\n        pass",
  "def close_axes(self, ax):\n        \"\"\"\n        Finish commands for a particular axes.\n\n        Parameters\n        ----------\n        ax : matplotlib.Axes\n            The Axes which is finished being drawn.\n        \"\"\"\n        pass",
  "def open_legend(self, legend, props):\n        \"\"\"\n        Beging commands for a particular legend.\n\n        Parameters\n        ----------\n        legend : matplotlib.legend.Legend\n                The Legend that will contain the ensuing elements\n        props : dictionary\n                The dictionary of legend properties\n        \"\"\"\n        pass",
  "def close_legend(self, legend):\n        \"\"\"\n        Finish commands for a particular legend.\n\n        Parameters\n        ----------\n        legend : matplotlib.legend.Legend\n                The Legend which is finished being drawn\n        \"\"\"\n        pass",
  "def draw_marked_line(self, data, coordinates, linestyle, markerstyle,\n                         label, mplobj=None):\n        \"\"\"Draw a line that also has markers.\n\n        If this isn't reimplemented by a renderer object, by default, it will\n        make a call to BOTH draw_line and draw_markers when both markerstyle\n        and linestyle are not None in the same Line2D object.\n\n        \"\"\"\n        if linestyle is not None:\n            self.draw_line(data, coordinates, linestyle, label, mplobj)\n        if markerstyle is not None:\n            self.draw_markers(data, coordinates, markerstyle, label, mplobj)",
  "def draw_line(self, data, coordinates, style, label, mplobj=None):\n        \"\"\"\n        Draw a line. By default, draw the line via the draw_path() command.\n        Some renderers might wish to override this and provide more\n        fine-grained behavior.\n\n        In matplotlib, lines are generally created via the plt.plot() command,\n        though this command also can create marker collections.\n\n        Parameters\n        ----------\n        data : array_like\n            A shape (N, 2) array of datapoints.\n        coordinates : string\n            A string code, which should be either 'data' for data coordinates,\n            or 'figure' for figure (pixel) coordinates.\n        style : dictionary\n            a dictionary specifying the appearance of the line.\n        mplobj : matplotlib object\n            the matplotlib plot element which generated this line\n        \"\"\"\n        pathcodes = ['M'] + (data.shape[0] - 1) * ['L']\n        pathstyle = dict(facecolor='none', **style)\n        pathstyle['edgecolor'] = pathstyle.pop('color')\n        pathstyle['edgewidth'] = pathstyle.pop('linewidth')\n        self.draw_path(data=data, coordinates=coordinates,\n                       pathcodes=pathcodes, style=pathstyle, mplobj=mplobj)",
  "def _iter_path_collection(paths, path_transforms, offsets, styles):\n        \"\"\"Build an iterator over the elements of the path collection\"\"\"\n        N = max(len(paths), len(offsets))\n\n        if not path_transforms:\n            path_transforms = [np.eye(3)]\n\n        edgecolor = styles['edgecolor']\n        if np.size(edgecolor) == 0:\n            edgecolor = ['none']\n        facecolor = styles['facecolor']\n        if np.size(facecolor) == 0:\n            facecolor = ['none']\n\n        elements = [paths, path_transforms, offsets,\n                    edgecolor, styles['linewidth'], facecolor]\n\n        it = itertools\n        return it.islice(py3k.zip(*py3k.map(it.cycle, elements)), N)",
  "def draw_path_collection(self, paths, path_coordinates, path_transforms,\n                             offsets, offset_coordinates, offset_order,\n                             styles, mplobj=None):\n        \"\"\"\n        Draw a collection of paths. The paths, offsets, and styles are all\n        iterables, and the number of paths is max(len(paths), len(offsets)).\n\n        By default, this is implemented via multiple calls to the draw_path()\n        function. For efficiency, Renderers may choose to customize this\n        implementation.\n\n        Examples of path collections created by matplotlib are scatter plots,\n        histograms, contour plots, and many others.\n\n        Parameters\n        ----------\n        paths : list\n            list of tuples, where each tuple has two elements:\n            (data, pathcodes).  See draw_path() for a description of these.\n        path_coordinates: string\n            the coordinates code for the paths, which should be either\n            'data' for data coordinates, or 'figure' for figure (pixel)\n            coordinates.\n        path_transforms: array_like\n            an array of shape (*, 3, 3), giving a series of 2D Affine\n            transforms for the paths. These encode translations, rotations,\n            and scalings in the standard way.\n        offsets: array_like\n            An array of offsets of shape (N, 2)\n        offset_coordinates : string\n            the coordinates code for the offsets, which should be either\n            'data' for data coordinates, or 'figure' for figure (pixel)\n            coordinates.\n        offset_order : string\n            either \"before\" or \"after\". This specifies whether the offset\n            is applied before the path transform, or after.  The matplotlib\n            backend equivalent is \"before\"->\"data\", \"after\"->\"screen\".\n        styles: dictionary\n            A dictionary in which each value is a list of length N, containing\n            the style(s) for the paths.\n        mplobj : matplotlib object\n            the matplotlib plot element which generated this collection\n        \"\"\"\n        if offset_order == \"before\":\n            raise NotImplementedError(\"offset before transform\")\n\n        for tup in self._iter_path_collection(paths, path_transforms,\n                                              offsets, styles):\n            (path, path_transform, offset, ec, lw, fc) = tup\n            vertices, pathcodes = path\n            path_transform = transforms.Affine2D(path_transform)\n            vertices = path_transform.transform(vertices)\n            # This is a hack:\n            if path_coordinates == \"figure\":\n                path_coordinates = \"points\"\n            style = {\"edgecolor\": utils.color_to_hex(ec),\n                     \"facecolor\": utils.color_to_hex(fc),\n                     \"edgewidth\": lw,\n                     \"dasharray\": \"10,0\",\n                     \"alpha\": styles['alpha'],\n                     \"zorder\": styles['zorder']}\n            self.draw_path(data=vertices, coordinates=path_coordinates,\n                           pathcodes=pathcodes, style=style, offset=offset,\n                           offset_coordinates=offset_coordinates,\n                           mplobj=mplobj)",
  "def draw_markers(self, data, coordinates, style, label, mplobj=None):\n        \"\"\"\n        Draw a set of markers. By default, this is done by repeatedly\n        calling draw_path(), but renderers should generally overload\n        this method to provide a more efficient implementation.\n\n        In matplotlib, markers are created using the plt.plot() command.\n\n        Parameters\n        ----------\n        data : array_like\n            A shape (N, 2) array of datapoints.\n        coordinates : string\n            A string code, which should be either 'data' for data coordinates,\n            or 'figure' for figure (pixel) coordinates.\n        style : dictionary\n            a dictionary specifying the appearance of the markers.\n        mplobj : matplotlib object\n            the matplotlib plot element which generated this marker collection\n        \"\"\"\n        vertices, pathcodes = style['markerpath']\n        pathstyle = dict((key, style[key]) for key in ['alpha', 'edgecolor',\n                                                       'facecolor', 'zorder',\n                                                       'edgewidth'])\n        pathstyle['dasharray'] = \"10,0\"\n        for vertex in data:\n            self.draw_path(data=vertices, coordinates=\"points\",\n                           pathcodes=pathcodes, style=pathstyle,\n                           offset=vertex, offset_coordinates=coordinates,\n                           mplobj=mplobj)",
  "def draw_text(self, text, position, coordinates, style,\n                  text_type=None, mplobj=None):\n        \"\"\"\n        Draw text on the image.\n\n        Parameters\n        ----------\n        text : string\n            The text to draw\n        position : tuple\n            The (x, y) position of the text\n        coordinates : string\n            A string code, which should be either 'data' for data coordinates,\n            or 'figure' for figure (pixel) coordinates.\n        style : dictionary\n            a dictionary specifying the appearance of the text.\n        text_type : string or None\n            if specified, a type of text such as \"xlabel\", \"ylabel\", \"title\"\n        mplobj : matplotlib object\n            the matplotlib plot element which generated this text\n        \"\"\"\n        raise NotImplementedError()",
  "def draw_path(self, data, coordinates, pathcodes, style,\n                  offset=None, offset_coordinates=\"data\", mplobj=None):\n        \"\"\"\n        Draw a path.\n\n        In matplotlib, paths are created by filled regions, histograms,\n        contour plots, patches, etc.\n\n        Parameters\n        ----------\n        data : array_like\n            A shape (N, 2) array of datapoints.\n        coordinates : string\n            A string code, which should be either 'data' for data coordinates,\n            'figure' for figure (pixel) coordinates, or \"points\" for raw\n            point coordinates (useful in conjunction with offsets, below).\n        pathcodes : list\n            A list of single-character SVG pathcodes associated with the data.\n            Path codes are one of ['M', 'm', 'L', 'l', 'Q', 'q', 'T', 't',\n                                   'S', 's', 'C', 'c', 'Z', 'z']\n            See the SVG specification for details.  Note that some path codes\n            consume more than one datapoint (while 'Z' consumes none), so\n            in general, the length of the pathcodes list will not be the same\n            as that of the data array.\n        style : dictionary\n            a dictionary specifying the appearance of the line.\n        offset : list (optional)\n            the (x, y) offset of the path. If not given, no offset will\n            be used.\n        offset_coordinates : string (optional)\n            A string code, which should be either 'data' for data coordinates,\n            or 'figure' for figure (pixel) coordinates.\n        mplobj : matplotlib object\n            the matplotlib plot element which generated this path\n        \"\"\"\n        raise NotImplementedError()",
  "def draw_image(self, imdata, extent, coordinates, style, mplobj=None):\n        \"\"\"\n        Draw an image.\n\n        Parameters\n        ----------\n        imdata : string\n            base64 encoded png representation of the image\n        extent : list\n            the axes extent of the image: [xmin, xmax, ymin, ymax]\n        coordinates: string\n            A string code, which should be either 'data' for data coordinates,\n            or 'figure' for figure (pixel) coordinates.\n        style : dictionary\n            a dictionary specifying the appearance of the image\n        mplobj : matplotlib object\n            the matplotlib plot object which generated this image\n        \"\"\"\n        raise NotImplementedError()",
  "class FakeRenderer(Renderer):\n    \"\"\"\n    Fake Renderer\n\n    This is a fake renderer which simply outputs a text tree representing the\n    elements found in the plot(s).  This is used in the unit tests for the\n    package.\n\n    Below are the methods your renderer must implement. You are free to do\n    anything you wish within the renderer (i.e. build an XML or JSON\n    representation, call an external API, etc.)  Here the renderer just\n    builds a simple string representation for testing purposes.\n    \"\"\"\n    def __init__(self):\n        self.output = \"\"\n\n    def open_figure(self, fig, props):\n        self.output += \"opening figure\\n\"\n\n    def close_figure(self, fig):\n        self.output += \"closing figure\\n\"\n\n    def open_axes(self, ax, props):\n        self.output += \"  opening axes\\n\"\n\n    def close_axes(self, ax):\n        self.output += \"  closing axes\\n\"\n\n    def open_legend(self, legend, props):\n        self.output += \"    opening legend\\n\"\n\n    def close_legend(self, legend):\n        self.output += \"    closing legend\\n\"\n\n    def draw_text(self, text, position, coordinates, style,\n                  text_type=None, mplobj=None):\n        self.output += \"    draw text '{0}' {1}\\n\".format(text, text_type)\n\n    def draw_path(self, data, coordinates, pathcodes, style,\n                  offset=None, offset_coordinates=\"data\", mplobj=None):\n        self.output += \"    draw path with {0} vertices\\n\".format(data.shape[0])\n\n    def draw_image(self, imdata, extent, coordinates, style, mplobj=None):\n        self.output += \"    draw image of size {0}\\n\".format(len(imdata))",
  "class FullFakeRenderer(FakeRenderer):\n    \"\"\"\n    Renderer with the full complement of methods.\n\n    When the following are left undefined, they will be implemented via\n    other methods in the class.  They can be defined explicitly for\n    more efficient or specialized use within the renderer implementation.\n    \"\"\"\n    def draw_line(self, data, coordinates, style, label, mplobj=None):\n        self.output += \"    draw line with {0} points\\n\".format(data.shape[0])\n\n    def draw_markers(self, data, coordinates, style, label, mplobj=None):\n        self.output += \"    draw {0} markers\\n\".format(data.shape[0])\n\n    def draw_path_collection(self, paths, path_coordinates, path_transforms,\n                             offsets, offset_coordinates, offset_order,\n                             styles, mplobj=None):\n        self.output += (\"    draw path collection \"\n                        \"with {0} offsets\\n\".format(offsets.shape[0]))",
  "def __init__(self):\n        self.output = \"\"",
  "def open_figure(self, fig, props):\n        self.output += \"opening figure\\n\"",
  "def close_figure(self, fig):\n        self.output += \"closing figure\\n\"",
  "def open_axes(self, ax, props):\n        self.output += \"  opening axes\\n\"",
  "def close_axes(self, ax):\n        self.output += \"  closing axes\\n\"",
  "def open_legend(self, legend, props):\n        self.output += \"    opening legend\\n\"",
  "def close_legend(self, legend):\n        self.output += \"    closing legend\\n\"",
  "def draw_text(self, text, position, coordinates, style,\n                  text_type=None, mplobj=None):\n        self.output += \"    draw text '{0}' {1}\\n\".format(text, text_type)",
  "def draw_path(self, data, coordinates, pathcodes, style,\n                  offset=None, offset_coordinates=\"data\", mplobj=None):\n        self.output += \"    draw path with {0} vertices\\n\".format(data.shape[0])",
  "def draw_image(self, imdata, extent, coordinates, style, mplobj=None):\n        self.output += \"    draw image of size {0}\\n\".format(len(imdata))",
  "def draw_line(self, data, coordinates, style, label, mplobj=None):\n        self.output += \"    draw line with {0} points\\n\".format(data.shape[0])",
  "def draw_markers(self, data, coordinates, style, label, mplobj=None):\n        self.output += \"    draw {0} markers\\n\".format(data.shape[0])",
  "def draw_path_collection(self, paths, path_coordinates, path_transforms,\n                             offsets, offset_coordinates, offset_order,\n                             styles, mplobj=None):\n        self.output += (\"    draw path collection \"\n                        \"with {0} offsets\\n\".format(offsets.shape[0]))",
  "class VegaRenderer(Renderer):\n    def open_figure(self, fig, props):\n        self.props = props\n        self.figwidth = int(props['figwidth'] * props['dpi'])\n        self.figheight = int(props['figheight'] * props['dpi'])\n        self.data = []\n        self.scales = []\n        self.axes = []\n        self.marks = []\n            \n    def open_axes(self, ax, props):\n        if len(self.axes) > 0:\n            warnings.warn(\"multiple axes not yet supported\")\n        self.axes = [dict(type=\"x\", scale=\"x\", ticks=10),\n                     dict(type=\"y\", scale=\"y\", ticks=10)]\n        self.scales = [dict(name=\"x\",\n                            domain=props['xlim'],\n                            type=\"linear\",\n                            range=\"width\",\n                        ),\n                       dict(name=\"y\",\n                            domain=props['ylim'],\n                            type=\"linear\",\n                            range=\"height\",\n                        ),]\n\n    def draw_line(self, data, coordinates, style, label, mplobj=None):\n        if coordinates != 'data':\n            warnings.warn(\"Only data coordinates supported. Skipping this\")\n        dataname = \"table{0:03d}\".format(len(self.data) + 1)\n\n        # TODO: respect the other style settings\n        self.data.append({'name': dataname,\n                          'values': [dict(x=d[0], y=d[1]) for d in data]})\n        self.marks.append({'type': 'line',\n                           'from': {'data': dataname},\n                           'properties': {\n                               \"enter\": {\n                                   \"interpolate\": {\"value\": \"monotone\"},\n                                   \"x\": {\"scale\": \"x\", \"field\": \"data.x\"},\n                                   \"y\": {\"scale\": \"y\", \"field\": \"data.y\"},\n                                   \"stroke\": {\"value\": style['color']},\n                                   \"strokeOpacity\": {\"value\": style['alpha']},\n                                   \"strokeWidth\": {\"value\": style['linewidth']},\n                               }\n                           }\n                       })\n\n    def draw_markers(self, data, coordinates, style, label, mplobj=None):\n        if coordinates != 'data':\n            warnings.warn(\"Only data coordinates supported. Skipping this\")\n        dataname = \"table{0:03d}\".format(len(self.data) + 1)\n\n        # TODO: respect the other style settings\n        self.data.append({'name': dataname,\n                          'values': [dict(x=d[0], y=d[1]) for d in data]})\n        self.marks.append({'type': 'symbol',\n                           'from': {'data': dataname},\n                           'properties': {\n                               \"enter\": {\n                                   \"interpolate\": {\"value\": \"monotone\"},\n                                   \"x\": {\"scale\": \"x\", \"field\": \"data.x\"},\n                                   \"y\": {\"scale\": \"y\", \"field\": \"data.y\"},\n                                   \"fill\": {\"value\": style['facecolor']},\n                                   \"fillOpacity\": {\"value\": style['alpha']},\n                                   \"stroke\": {\"value\": style['edgecolor']},\n                                   \"strokeOpacity\": {\"value\": style['alpha']},\n                                   \"strokeWidth\": {\"value\": style['edgewidth']},\n                               }\n                           }\n                       })\n\n    def draw_text(self, text, position, coordinates, style,\n                  text_type=None, mplobj=None):\n        if text_type == 'xlabel':\n            self.axes[0]['title'] = text\n        elif text_type == 'ylabel':\n            self.axes[1]['title'] = text",
  "class VegaHTML(object):\n    def __init__(self, renderer):\n        self.specification = dict(width=renderer.figwidth,\n                                  height=renderer.figheight,\n                                  data=renderer.data,\n                                  scales=renderer.scales,\n                                  axes=renderer.axes,\n                                  marks=renderer.marks)\n\n    def html(self):\n        \"\"\"Build the HTML representation for IPython.\"\"\"\n        id = random.randint(0, 2 ** 16)\n        html = '<div id=\"vis%d\"></div>' % id\n        html += '<script>\\n'\n        html += VEGA_TEMPLATE % (json.dumps(self.specification), id)\n        html += '</script>\\n'\n        return html\n\n    def _repr_html_(self):\n        return self.html()",
  "def fig_to_vega(fig, notebook=False):\n    \"\"\"Convert a matplotlib figure to vega dictionary\n\n    if notebook=True, then return an object which will display in a notebook\n    otherwise, return an HTML string.\n    \"\"\"\n    renderer = VegaRenderer()\n    Exporter(renderer).run(fig)\n    vega_html = VegaHTML(renderer)\n    if notebook:\n        return vega_html\n    else:\n        return vega_html.html()",
  "def open_figure(self, fig, props):\n        self.props = props\n        self.figwidth = int(props['figwidth'] * props['dpi'])\n        self.figheight = int(props['figheight'] * props['dpi'])\n        self.data = []\n        self.scales = []\n        self.axes = []\n        self.marks = []",
  "def open_axes(self, ax, props):\n        if len(self.axes) > 0:\n            warnings.warn(\"multiple axes not yet supported\")\n        self.axes = [dict(type=\"x\", scale=\"x\", ticks=10),\n                     dict(type=\"y\", scale=\"y\", ticks=10)]\n        self.scales = [dict(name=\"x\",\n                            domain=props['xlim'],\n                            type=\"linear\",\n                            range=\"width\",\n                        ),\n                       dict(name=\"y\",\n                            domain=props['ylim'],\n                            type=\"linear\",\n                            range=\"height\",\n                        ),]",
  "def draw_line(self, data, coordinates, style, label, mplobj=None):\n        if coordinates != 'data':\n            warnings.warn(\"Only data coordinates supported. Skipping this\")\n        dataname = \"table{0:03d}\".format(len(self.data) + 1)\n\n        # TODO: respect the other style settings\n        self.data.append({'name': dataname,\n                          'values': [dict(x=d[0], y=d[1]) for d in data]})\n        self.marks.append({'type': 'line',\n                           'from': {'data': dataname},\n                           'properties': {\n                               \"enter\": {\n                                   \"interpolate\": {\"value\": \"monotone\"},\n                                   \"x\": {\"scale\": \"x\", \"field\": \"data.x\"},\n                                   \"y\": {\"scale\": \"y\", \"field\": \"data.y\"},\n                                   \"stroke\": {\"value\": style['color']},\n                                   \"strokeOpacity\": {\"value\": style['alpha']},\n                                   \"strokeWidth\": {\"value\": style['linewidth']},\n                               }\n                           }\n                       })",
  "def draw_markers(self, data, coordinates, style, label, mplobj=None):\n        if coordinates != 'data':\n            warnings.warn(\"Only data coordinates supported. Skipping this\")\n        dataname = \"table{0:03d}\".format(len(self.data) + 1)\n\n        # TODO: respect the other style settings\n        self.data.append({'name': dataname,\n                          'values': [dict(x=d[0], y=d[1]) for d in data]})\n        self.marks.append({'type': 'symbol',\n                           'from': {'data': dataname},\n                           'properties': {\n                               \"enter\": {\n                                   \"interpolate\": {\"value\": \"monotone\"},\n                                   \"x\": {\"scale\": \"x\", \"field\": \"data.x\"},\n                                   \"y\": {\"scale\": \"y\", \"field\": \"data.y\"},\n                                   \"fill\": {\"value\": style['facecolor']},\n                                   \"fillOpacity\": {\"value\": style['alpha']},\n                                   \"stroke\": {\"value\": style['edgecolor']},\n                                   \"strokeOpacity\": {\"value\": style['alpha']},\n                                   \"strokeWidth\": {\"value\": style['edgewidth']},\n                               }\n                           }\n                       })",
  "def draw_text(self, text, position, coordinates, style,\n                  text_type=None, mplobj=None):\n        if text_type == 'xlabel':\n            self.axes[0]['title'] = text\n        elif text_type == 'ylabel':\n            self.axes[1]['title'] = text",
  "def __init__(self, renderer):\n        self.specification = dict(width=renderer.figwidth,\n                                  height=renderer.figheight,\n                                  data=renderer.data,\n                                  scales=renderer.scales,\n                                  axes=renderer.axes,\n                                  marks=renderer.marks)",
  "def html(self):\n        \"\"\"Build the HTML representation for IPython.\"\"\"\n        id = random.randint(0, 2 ** 16)\n        html = '<div id=\"vis%d\"></div>' % id\n        html += '<script>\\n'\n        html += VEGA_TEMPLATE % (json.dumps(self.specification), id)\n        html += '</script>\\n'\n        return html",
  "def _repr_html_(self):\n        return self.html()",
  "def keep_source_input_sync(filepath, callback, start=0):\n    \"\"\" Monitor file at filepath checking for new lines (similar to\n    tail -f) and calls callback on every new line found.\n\n    Args:\n        filepath (str): path to the series data file (\n            i.e.: /source/to/my/data.csv)\n        callback (callable): function to be called with the a DataFrame\n            created from the new lines found from file at filepath\n            starting byte start\n        start (int): specifies where to start reading from the file at\n            filepath.\n            Default: 0\n\n    Returns:\n        DataFrame created from data read from filepath\n    \"\"\"\n    if filepath is None:\n        msg = \"No Input! Please specify --source_filename or --buffer t\"\n        raise IOError(msg)\n\n    if filepath.lower().startswith('http'):\n        # Create a request for the given URL.\n\n        while True:\n            request = urllib2.Request(filepath)\n            data = get_data_from_url(request, start)\n\n            f = io.BytesIO(data)\n            f.seek(start)\n            line = f.readline()     # See note below\n\n            if not line:\n                continue   # No data, try again\n\n            callback(line)\n            start = len(data)\n    else:\n        f = open(filepath, 'r')\n        f.seek(start)\n        while True:\n            line = f.readline()     # See note below\n            if not line:\n                continue   # No data, try again\n            callback(line)\n\n    source = pd.read_csv(filepath)\n    return source",
  "def get_data_from_url(request, start=0, length=0):\n    \"\"\" Read from request after adding headers to retrieve data from byte\n    specified in start.\n\n    request (urllib2.Request): request object related to the data to read\n    start (int, optional): byte to start reading from.\n        Default: 0\n    length: length of the data range to read from start. If 0 it reads\n        until the end of the stream.\n        Default: 0\n\n    Returns:\n        String read from request\n    \"\"\"\n    ranged = False\n    # Add the header to specify the range to download.\n    if start and length:\n        request.add_header(\"Range\", \"bytes=%d-%d\" % (start, start + length - 1))\n    elif start:\n        request.add_header(\"Range\", \"bytes=%s-\" % start)\n\n    response = urllib2.urlopen(request)\n    # If a content-range header is present, partial retrieval worked.\n    if \"content-range\" in response.headers:\n        print(\"Partial retrieval successful.\")\n\n        # The header contains the string 'bytes', followed by a space, then the\n        # range in the format 'start-end', followed by a slash and then the total\n        # size of the page (or an asterix if the total size is unknown). Lets get\n        # the range and total size from this.\n        _range, total = response.headers['content-range'].split(' ')[-1].split('/')\n        # Print a message giving the range information.\n        if total == '*':\n            print(\"Bytes %s of an unknown total were retrieved.\" % _range)\n        else:\n            print(\"Bytes %s of a total of %s were retrieved.\" % (_range, total))\n\n    # # No header, so partial retrieval was unsuccessful.\n    # else:\n    #     print \"Unable to use partial retrieval.\"\n    data = response.read()\n\n    return data",
  "def parse_output_config(output):\n    \"\"\"Parse the output specification string and return the related chart\n    output attribute.\n\n    Attr:\n        output (str): String with the syntax convention specified for the\n            cli output option is as follows: <output_type>://<type_arg>\n            Valid values:\n                output_type: file or server\n                type_arg:\n                    file_path if output_type is file\n                    serve path if output_type is server\n\n    Returns:\n        dictionary containing the output arguments to pass to a chart object\n    \"\"\"\n    output_type, output_options = output.split('://')\n\n    if output_type == 'file':\n        return {'filename': output_options}\n\n    elif output_type == 'server':\n        # TODO: check if server configuration is as flexible as with plotting\n        #       interface and add support for url/name if so.\n        out_opt = output_options.split(\"@\")\n        attrnames = ['server', 'url', 'name']\n\n        # unpack server output parametrs in order to pass them to the plot\n        # creation function\n        kws = dict((attrn, val) for attrn, val in zip( attrnames, out_opt))\n        return {'server': kws['server']}\n\n    else:\n        msg = \"Unknown output type %s found. Please use: file|server\"\n        print (msg % output_type)\n        return {}",
  "def get_chart_params(title, output, show_legend=False):\n    \"\"\"Parse output type and output options and return related chart\n    parameters. For example: returns filename if output_type is file\n    or server it output_type is server\n\n    Args:\n        title (str): the title of your plot.\n        output (str): selected output. Follows the following convention:\n            <output_type>://<type_arg> where output_type can be\n            `file` (in that case type_arg specifies the file path) or\n            `server` (in that case type_arg specify the server name).\n\n\n    Returns:\n        dictionary containing the arguments to pass to a chart object\n        related to title and output options\n    \"\"\"\n    params = {'title': title, 'legend': show_legend}\n    output_params = parse_output_config(output)\n    if output_params:\n        params.update(output_params)\n\n    return params",
  "def get_data_series(series, source, indexes):\n    \"\"\"Generate an OrderedDict from the source series excluding index\n    and all series not specified in series.\n\n    Args:\n        series (list(str)): list of strings specifying the names of the\n            series to keep from source\n        source (DataFrame): pandas DataFrame with the data series to be\n            plotted\n        indexes (lst(str)): name of the series of source to be used as index.\n\n    Returns:\n        OrderedDict with the data series from source\n    \"\"\"\n    series = define_series(series, source, indexes)\n    # generate charts data\n    data_series = OrderedDict()\n    for i, colname in enumerate(series+indexes):\n        try:\n            data_series[colname] = source[colname]\n        except KeyError:\n            raise KeyError(hm.ERR_MSG_SERIES_NOT_FOUND % (colname, source.keys()))\n\n    return data_series",
  "def define_series(series, source, indexes):\n    \"\"\"If series is empty returns source_columns excluding the column\n    where column == index. Otherwise returns the series.split(',')\n\n    Args:\n        series (str): string that contains the names of the\n            series to keep from source, separated by `,`\n        source (DataFrame): pandas DataFrame with the data series to be\n            plotted\n        indexes (lst(str)): name of the series of source to be used as index.\n\n    Returns:\n        list of the names (as str) of the series except index\n    \"\"\"\n    if not series:\n        return [c for c in source.columns if c not in indexes]\n    else:\n        return series.split(',')",
  "def get_charts_mapping():\n    \"\"\"Return a dict with chart classes names (lower case) as keys and\n    their related class as values.\n\n    Returns:\n        dict mapping chart classes names to chart classes\n    \"\"\"\n    mapping = {}\n    for (clsname, cls) in charts.__dict__.items():\n        try:\n            # TODO: We may need to restore the objects filtering\n            # when charts creators (or builders registration) is added\n            # to the charts API\n            mapping[clsname.lower()] = cls\n        except TypeError:\n            pass\n    return mapping",
  "def cli(input_source, output, title, chart_type, series, palette, index,\n        buffer, sync_with_source, update_ranges, show_legend, window_size,\n        map_, smart_filters, map_zoom, map_layer):\n    \"\"\"Bokeh Command Line Tool is a minimal client to access high level plotting\n    functionality provided by bokeh.charts API.\n\n    Examples:\n\n    >> python bokeh-cli.py --title \"My Nice Plot\" --series \"High,Low,Close\"\n    --chart_type \"Line\" --palette Reds --input sample_data/stocks_data.csv\n\n    >> cat sample_data/stocks_data.csv | python bokeh-cli.py --buffer t\n\n    >> python bokeh-cli.py --help\n    \"\"\"\n    cli = CLI(\n        input_source, output, title, chart_type, series, palette, index, buffer,\n        sync_with_source, update_ranges, show_legend, window_size, map_,\n        smart_filters, map_zoom, map_layer\n    )\n    cli.run()",
  "class CLI(object):\n    \"\"\"This is the Bokeh Command Line Interface class and it is in\n    charge of providing a very high level access to bokeh charts and\n    extends it with functionality.\n\n    \"\"\"\n    def __init__(self, input_source, output, title, chart_type, series, palette,\n                 index, buffer, sync_with_source, update_ranges, show_legend,\n                 window_size, map_, smart_filters, map_zoom, map_layer):\n        \"\"\"Args:\n        input_source (str): path to the series data file (i.e.:\n            /source/to/my/data.csv)\n            NOTE: this can be either a path to a local file or an url\n        output (str, optional): Selects the plotting output, which\n            could either be sent to an html file or a bokeh server\n            instance. Syntax convention for this option is as follows:\n            <output_type>://<type_arg>\n\n            where:\n              - output_type: 'file' or 'server'\n              - 'file' type options: path_to_output_file\n              - 'server' type options syntax: docname[@url][@name]\n\n            Defaults to: --output file://cli_output.html\n\n            Examples:\n                --output file://cli_output.html\n                --output file:///home/someuser/bokeh_rocks/cli_output.html\n                --output server://clidemo\n\n            Default: file://cli_output.html.\n        title (str, optional): the title of your chart.\n            Default: None.\n        chart_type (str, optional): charts classes to use to consume and\n            render the input data.\n            Default: Line.\n        series (str, optional): Name of the series from the input source\n            to include in the plot. If not specified all source series\n            will be included.\n            Defaults to None.\n        palette (str, optional): name of the colors palette to use.\n            Default: None.\n        index (str, optional): Name of the data series to be used as the\n            index when plotting. By default the first series found on the\n            input file is taken\n            Default: None\n        buffer (str, optional): if is `t` reads data source as string from\n            input buffer using StringIO(sys.stdin.read()) instead of\n            reading from a file or an url.\n            Default: \"f\"\n        sync_with_source (bool, optional): if True keep the charts source\n            created on bokeh-server sync'ed with the source acting like\n            `tail -f`.\n            Default: False\n        window_size (int, optional): show up to N values then start dropping\n            off older ones\n            Default: '0'\n\n\n        Attributes:\n            source (obj): datasource object for the created chart.\n            chart (obj): created chart object.\n        \"\"\"\n        self.input = input_source\n        self.series = series\n        self.index = index\n        self.last_byte = -1\n        self.sync_with_source = sync_with_source\n        self.update_ranges = update_ranges\n        self.show_legend = show_legend\n        self.window_size = int(window_size)\n        self.smart_filters = smart_filters\n        self.map_options = {}\n        self.current_selection = []\n\n        self.source = self.get_input(input_source, buffer)\n        # get the charts specified by the user\n        self.factories = create_chart_factories(chart_type)\n\n        if palette:\n            print (\"Sorry, custom palettes not supported yet, coming soon!\")\n\n        # define charts init parameters specified from cmd line and create chart\n        self.chart_args = get_chart_params(\n            title, output, show_legend=self.show_legend\n        )\n        if self.smart_filters:\n            self.chart_args['tools'] = \"pan,wheel_zoom,box_zoom,reset,save,\" \\\n                                       \"box_select,lasso_select\"\n\n        if map_:\n            self.map_options['lat'], self.map_options['lng'] = \\\n                [float(x) for x in map_.strip().split(',')]\n\n            self.map_options['zoom'] = int(map_zoom)\n            # Yeah, unfortunate namings.. :-)\n            self.map_options['map_type'] = map_layer\n\n    def on_selection_changed(self, obj, attrname, old, new):\n        self.current_selection = new\n\n    def limit_source(self, source):\n        \"\"\" Limit source to cli.window_size, if set.\n\n        Args:\n            source (mapping): dict-like object\n        \"\"\"\n        if self.window_size:\n            for key in source.keys():\n                source[key] = source[key][-self.window_size:]\n\n    def run(self):\n        \"\"\" Start the CLI logic creating the input source, data conversions,\n        chart instances to show and all other niceties provided by CLI\n        \"\"\"\n        try:\n            self.limit_source(self.source)\n\n            children = []\n            if self.smart_filters:\n                copy_selection = Button(label=\"copy current selection\")\n                copy_selection.on_click(self.on_copy)\n                children.append(copy_selection)\n\n            self.chart = create_chart(\n                self.series, self.source, self.index, self.factories,\n                self.map_options, children=children, **self.chart_args\n            )\n            self.chart.show()\n\n            self.has_ranged_x_axis = 'ranged_x_axis' in self.source.columns\n            self.columns = [c for c in self.source.columns if c != 'ranged_x_axis']\n\n            if self.smart_filters:\n                for chart in self.chart.charts:\n                    chart.source.on_change('selected', self, 'on_selection_changed')\n                self.chart.session.poll_document(self.chart.doc)\n\n        except TypeError:\n            if not self.series:\n                series_list = ', '.join(self.chart.values.keys())\n                print(hm.ERR_MSG_TEMPL % series_list)\n                raise\n\n        if self.sync_with_source:\n            keep_source_input_sync(self.input, self.update_source, self.last_byte)\n\n    def on_copy(self, *args, **kws):\n        print(\"COPYING CONTENT!\")\n        # TODO: EXPERIMENTAL!!! THIS EXPOSE MANY SECURITY ISSUES AND SHOULD\n        #       BE REMOVED ASAP!\n        txt = ''\n        for rowind in self.current_selection:\n            row = self.source.iloc[rowind]\n            txt += u\"%s\\n\" % (u\",\".join(str(row[c]) for c in self.columns))\n\n        os.system(\"echo '%s' | pbcopy\" % txt)\n\n    def update_source(self, new_source):\n        \"\"\" Update self.chart source with the new data retrieved from\n         new_source. It is done by parsing the new source line,\n         trasforming it to data to be appended to self.chart source\n         updating it on chart.session and actually updating chart.session\n         objects.\n\n        Args:\n            new_source (str): string that contains the new source row to\n                read to the current chart source.\n        \"\"\"\n        ns = pd.read_csv(StringIO(new_source), names=self.columns)\n        len_source = len(self.source)\n\n        if self.has_ranged_x_axis:\n            ns['ranged_x_axis'] = [len_source]\n            self.index = 'ranged_x_axis'\n\n        ns.index = [len_source]\n        self.source = pd.concat([self.source, ns])\n\n        # TODO: This should be replaced with something that just computes\n        #       the new data and source\n        fig = create_chart(self.series, ns, self.index, self.factories,\n                          self.map_options, **self.chart_args)\n\n        for i, _c in enumerate(fig.charts):\n            if not isinstance(_c, bc.GMap):\n                # TODO: nested charts are getting ridiculous. Need a better\n                #       better interface for charts :-)\n                scc = self.chart.charts[i]\n                for k, v in _c.source.data.items():\n                    scc.source.data[k] = list(scc.source.data[k]) + list(v)\n\n                self.limit_source(scc.source.data)\n                chart = scc.chart\n                chart.session.store_objects(scc.source)\n\n                if self.update_ranges:\n                    plot = chart.plot\n                    plot.y_range.start = min(\n                        plot.y_range.start, _c.chart.plot.y_range.start\n                    )\n                    plot.y_range.end = max(\n                        plot.y_range.end, _c.chart.plot.y_range.end\n                    )\n                    plot.x_range.start = min(\n                        plot.x_range.start, _c.chart.plot.x_range.start\n                    )\n                    plot.x_range.end = max(\n                        plot.x_range.end, _c.chart.plot.x_range.end\n                    )\n                    chart.session.store_objects(plot)\n\n    def get_input(self, filepath, buffer):\n        \"\"\"Parse received input options. If buffer is not false (=='f') if\n        gets input data from input buffer othewise opens file specified in\n        sourcefilename,\n\n        Args:\n            filepath (str): path to the file to read from to retrieve data\n            buffer (str): if == 't' reads data from input buffer\n\n        Returns:\n            string read from filepath/buffer\n        \"\"\"\n\n        if buffer != 'f':\n            filepath = StringIO(sys.stdin.read())\n        elif filepath is None:\n            msg = \"No Input! Please specify --source_filename or --buffer t\"\n            raise IOError(msg)\n        else:\n            if filepath.lower().startswith('http'):\n                # Create a request for the given URL.\n                request = urllib2.Request(filepath)\n                data = get_data_from_url(request)\n                self.last_byte = len(data)\n\n            else:\n                filepath = open(filepath, 'r').read()\n                self.last_byte = len(filepath)\n                filepath = StringIO(filepath)\n\n        source = pd.read_csv(filepath)\n        return source",
  "def create_chart(series, source, index, factories, map_options=None, children=None, **args):\n    \"\"\"Create charts instances from types specified in factories using\n    data series names, source, index and args\n\n    Args:\n        series (list(str)): list of strings specifying the names of the\n            series to keep from source\n        source (DataFrame): pandas DataFrame with the data series to be\n            plotted\n        index (str): name of the series of source to be used as index.\n        factories (list(ChartObject)): list of chart classes to be used\n            to create the charts to be plotted\n        **args: arguments to pass to the charts when creating them.\n    \"\"\"\n    if not index:\n        # if no index was specified as for x axis\n        # we take a default \"range\"\n        index = 'ranged_x_axis'\n        # add the new x range data to the source dataframe\n        source[index] = range(len(source[source.columns[0]]))\n\n    indexes = [x for x in index.split(',') if x]\n    data_series = get_data_series(series, source, indexes)\n    # parse queries to create the charts..\n\n    charts = []\n    for chart_type in factories:\n        if chart_type == bc.GMap:\n            if not map_options or \\\n                    not all([x in map_options for x in ['lat', 'lng']]):\n                raise ValueError(\"GMap Charts need lat and lon coordinates!\")\n\n            all_args = dict(map_options)\n            all_args.update(args)\n            chart = chart_type(**all_args)\n\n        else:\n            if chart_type == bc.TimeSeries:\n                # in case the x axis type is datetime that column must be converted to\n                # datetime\n                data_series[index] = pd.to_datetime(source[index])\n\n            elif chart_type == bc.Scatter:\n                if len(indexes) == 1:\n                    scatter_ind = [x for x in data_series.pop(indexes[0]).values]\n                    scatter_ind = [scatter_ind] * len(data_series)\n\n                else:\n                    scatter_ind = []\n                    for key in indexes:\n                        scatter_ind.append([x for x in data_series.pop(key).values])\n\n                    if len(scatter_ind) != len(data_series):\n                        err_msg = \"Number of multiple indexes must be equals\" \\\n                                  \" to the number of series\"\n                        raise ValueError(err_msg)\n\n                for ind, key in enumerate(data_series):\n                    values = data_series[key].values\n                    data_series[key] = zip(scatter_ind[ind], values)\n\n            chart = chart_type(data_series, **args)\n            if hasattr(chart, 'index'):\n                chart.index = index\n\n        charts.append(chart)\n\n    fig = bc_utils.Figure(*charts, children=children, **args)\n    return fig",
  "def create_chart_factories(chart_types):\n    \"\"\"Receive the chart type(s) specified by the user and build a\n    list of the their related functions.\n\n    Args:\n        series (str): string that contains the name of the\n            chart classes to use when creating the chart, separated by `,`\n\n    example:\n\n    >> create_chart_factories('Line,step')\n      [Line, Step]\n    \"\"\"\n    return [get_chart(name) for name in chart_types.split(',') if name]",
  "def get_chart(class_name):\n    \"\"\"Return the bokeh class specified in class_name.\n\n    Args:\n        class_name (str): name of the chart class to return (i.e.: Line|step)\n    \"\"\"\n    return CHARTS_MAP[class_name.strip().lower()]",
  "def __init__(self, input_source, output, title, chart_type, series, palette,\n                 index, buffer, sync_with_source, update_ranges, show_legend,\n                 window_size, map_, smart_filters, map_zoom, map_layer):\n        \"\"\"Args:\n        input_source (str): path to the series data file (i.e.:\n            /source/to/my/data.csv)\n            NOTE: this can be either a path to a local file or an url\n        output (str, optional): Selects the plotting output, which\n            could either be sent to an html file or a bokeh server\n            instance. Syntax convention for this option is as follows:\n            <output_type>://<type_arg>\n\n            where:\n              - output_type: 'file' or 'server'\n              - 'file' type options: path_to_output_file\n              - 'server' type options syntax: docname[@url][@name]\n\n            Defaults to: --output file://cli_output.html\n\n            Examples:\n                --output file://cli_output.html\n                --output file:///home/someuser/bokeh_rocks/cli_output.html\n                --output server://clidemo\n\n            Default: file://cli_output.html.\n        title (str, optional): the title of your chart.\n            Default: None.\n        chart_type (str, optional): charts classes to use to consume and\n            render the input data.\n            Default: Line.\n        series (str, optional): Name of the series from the input source\n            to include in the plot. If not specified all source series\n            will be included.\n            Defaults to None.\n        palette (str, optional): name of the colors palette to use.\n            Default: None.\n        index (str, optional): Name of the data series to be used as the\n            index when plotting. By default the first series found on the\n            input file is taken\n            Default: None\n        buffer (str, optional): if is `t` reads data source as string from\n            input buffer using StringIO(sys.stdin.read()) instead of\n            reading from a file or an url.\n            Default: \"f\"\n        sync_with_source (bool, optional): if True keep the charts source\n            created on bokeh-server sync'ed with the source acting like\n            `tail -f`.\n            Default: False\n        window_size (int, optional): show up to N values then start dropping\n            off older ones\n            Default: '0'\n\n\n        Attributes:\n            source (obj): datasource object for the created chart.\n            chart (obj): created chart object.\n        \"\"\"\n        self.input = input_source\n        self.series = series\n        self.index = index\n        self.last_byte = -1\n        self.sync_with_source = sync_with_source\n        self.update_ranges = update_ranges\n        self.show_legend = show_legend\n        self.window_size = int(window_size)\n        self.smart_filters = smart_filters\n        self.map_options = {}\n        self.current_selection = []\n\n        self.source = self.get_input(input_source, buffer)\n        # get the charts specified by the user\n        self.factories = create_chart_factories(chart_type)\n\n        if palette:\n            print (\"Sorry, custom palettes not supported yet, coming soon!\")\n\n        # define charts init parameters specified from cmd line and create chart\n        self.chart_args = get_chart_params(\n            title, output, show_legend=self.show_legend\n        )\n        if self.smart_filters:\n            self.chart_args['tools'] = \"pan,wheel_zoom,box_zoom,reset,save,\" \\\n                                       \"box_select,lasso_select\"\n\n        if map_:\n            self.map_options['lat'], self.map_options['lng'] = \\\n                [float(x) for x in map_.strip().split(',')]\n\n            self.map_options['zoom'] = int(map_zoom)\n            # Yeah, unfortunate namings.. :-)\n            self.map_options['map_type'] = map_layer",
  "def on_selection_changed(self, obj, attrname, old, new):\n        self.current_selection = new",
  "def limit_source(self, source):\n        \"\"\" Limit source to cli.window_size, if set.\n\n        Args:\n            source (mapping): dict-like object\n        \"\"\"\n        if self.window_size:\n            for key in source.keys():\n                source[key] = source[key][-self.window_size:]",
  "def run(self):\n        \"\"\" Start the CLI logic creating the input source, data conversions,\n        chart instances to show and all other niceties provided by CLI\n        \"\"\"\n        try:\n            self.limit_source(self.source)\n\n            children = []\n            if self.smart_filters:\n                copy_selection = Button(label=\"copy current selection\")\n                copy_selection.on_click(self.on_copy)\n                children.append(copy_selection)\n\n            self.chart = create_chart(\n                self.series, self.source, self.index, self.factories,\n                self.map_options, children=children, **self.chart_args\n            )\n            self.chart.show()\n\n            self.has_ranged_x_axis = 'ranged_x_axis' in self.source.columns\n            self.columns = [c for c in self.source.columns if c != 'ranged_x_axis']\n\n            if self.smart_filters:\n                for chart in self.chart.charts:\n                    chart.source.on_change('selected', self, 'on_selection_changed')\n                self.chart.session.poll_document(self.chart.doc)\n\n        except TypeError:\n            if not self.series:\n                series_list = ', '.join(self.chart.values.keys())\n                print(hm.ERR_MSG_TEMPL % series_list)\n                raise\n\n        if self.sync_with_source:\n            keep_source_input_sync(self.input, self.update_source, self.last_byte)",
  "def on_copy(self, *args, **kws):\n        print(\"COPYING CONTENT!\")\n        # TODO: EXPERIMENTAL!!! THIS EXPOSE MANY SECURITY ISSUES AND SHOULD\n        #       BE REMOVED ASAP!\n        txt = ''\n        for rowind in self.current_selection:\n            row = self.source.iloc[rowind]\n            txt += u\"%s\\n\" % (u\",\".join(str(row[c]) for c in self.columns))\n\n        os.system(\"echo '%s' | pbcopy\" % txt)",
  "def update_source(self, new_source):\n        \"\"\" Update self.chart source with the new data retrieved from\n         new_source. It is done by parsing the new source line,\n         trasforming it to data to be appended to self.chart source\n         updating it on chart.session and actually updating chart.session\n         objects.\n\n        Args:\n            new_source (str): string that contains the new source row to\n                read to the current chart source.\n        \"\"\"\n        ns = pd.read_csv(StringIO(new_source), names=self.columns)\n        len_source = len(self.source)\n\n        if self.has_ranged_x_axis:\n            ns['ranged_x_axis'] = [len_source]\n            self.index = 'ranged_x_axis'\n\n        ns.index = [len_source]\n        self.source = pd.concat([self.source, ns])\n\n        # TODO: This should be replaced with something that just computes\n        #       the new data and source\n        fig = create_chart(self.series, ns, self.index, self.factories,\n                          self.map_options, **self.chart_args)\n\n        for i, _c in enumerate(fig.charts):\n            if not isinstance(_c, bc.GMap):\n                # TODO: nested charts are getting ridiculous. Need a better\n                #       better interface for charts :-)\n                scc = self.chart.charts[i]\n                for k, v in _c.source.data.items():\n                    scc.source.data[k] = list(scc.source.data[k]) + list(v)\n\n                self.limit_source(scc.source.data)\n                chart = scc.chart\n                chart.session.store_objects(scc.source)\n\n                if self.update_ranges:\n                    plot = chart.plot\n                    plot.y_range.start = min(\n                        plot.y_range.start, _c.chart.plot.y_range.start\n                    )\n                    plot.y_range.end = max(\n                        plot.y_range.end, _c.chart.plot.y_range.end\n                    )\n                    plot.x_range.start = min(\n                        plot.x_range.start, _c.chart.plot.x_range.start\n                    )\n                    plot.x_range.end = max(\n                        plot.x_range.end, _c.chart.plot.x_range.end\n                    )\n                    chart.session.store_objects(plot)",
  "def get_input(self, filepath, buffer):\n        \"\"\"Parse received input options. If buffer is not false (=='f') if\n        gets input data from input buffer othewise opens file specified in\n        sourcefilename,\n\n        Args:\n            filepath (str): path to the file to read from to retrieve data\n            buffer (str): if == 't' reads data from input buffer\n\n        Returns:\n            string read from filepath/buffer\n        \"\"\"\n\n        if buffer != 'f':\n            filepath = StringIO(sys.stdin.read())\n        elif filepath is None:\n            msg = \"No Input! Please specify --source_filename or --buffer t\"\n            raise IOError(msg)\n        else:\n            if filepath.lower().startswith('http'):\n                # Create a request for the given URL.\n                request = urllib2.Request(filepath)\n                data = get_data_from_url(request)\n                self.last_byte = len(data)\n\n            else:\n                filepath = open(filepath, 'r').read()\n                self.last_byte = len(filepath)\n                filepath = StringIO(filepath)\n\n        source = pd.read_csv(filepath)\n        return source"
]