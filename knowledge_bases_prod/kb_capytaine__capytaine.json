[
  "class FreeSurface():\n    \"\"\"A cartesian mesh on which the free surface elevation will be computed.\n\n    Has a :code:`mesh` attribute to behave kind of like FloatingBody when\n    building of the influence matrix.\n\n    Parameters\n    ----------\n    x_range: Tuple[float, float], optional\n        extreme values of the mesh in the x direction\n    nx: int, optional\n        number of cells in the x direction\n    y_range: Tuple[float, float], optional\n        extreme values of the mesh in the y direction\n    ny: int, optional\n        number of cells in the y direction\n    name: string, optional\n        a name for the free surface object\n\n\n    .. todo:: Generalize to non-cartesian meshes.\n              In particular, it could be of interest to build meshes having the\n              same symmetry as a given floating body to speed up the\n              construction of the influence matrix.\n\n    .. seealso::\n\n        :meth:`~capytaine.bem.nemoh.Nemoh.get_free_surface_elevation`\n            The main function requiring a FreeSurface object.\n    \"\"\"\n    def __init__(self, x_range=(-50.0, 50.0), nx=10, y_range=(-50.0, 50.0), ny=10, name=None):\n        self.x_range = x_range\n        self.nx = nx\n        self.y_range = y_range\n        self.ny = ny\n\n        if name is None:\n            self.name = f\"free_surface_{next(Mesh._ids)}\"\n        else:\n            self.name = name\n\n        self.mesh = self._generate_mesh()\n\n    def _generate_mesh(self):\n        \"\"\"Generate a 2D cartesian mesh.\"\"\"\n        nodes = np.zeros(((self.nx+1)*(self.ny+1), 3), dtype=float)\n        panels = np.zeros((self.nx*self.ny, 4), dtype=int)\n\n        X = np.linspace(*self.x_range, self.nx+1)\n        Y = np.linspace(*self.y_range, self.ny+1)\n        for i, (x, y, z) in enumerate(product(X, Y, [0.0])):\n            nodes[i, :] = x, y, z\n\n        for k, (i, j) in enumerate(product(range(0, self.nx), range(0, self.ny))):\n            panels[k, :] = (j+i*(self.ny+1),\n                            (j+1)+i*(self.ny+1),\n                            (j+1)+(i+1)*(self.ny+1),\n                            j+(i+1)*(self.ny+1))\n\n        return Mesh(nodes, panels, name=f\"{self.name}_mesh\")\n\n    @property\n    def area(self):\n        \"\"\"The total area covered by the mesh.\"\"\"\n        return (np.abs(self.x_range[1] - self.x_range[0])\n                * np.abs(self.y_range[1] - self.y_range[0]))\n\n    def incoming_waves(self, problem: \"DiffractionProblem\") -> np.ndarray:\n        \"\"\"Free surface elevation of the undisturbed incoming waves\n        for a given diffraction problem.\n        Kept for legacy, but not recommended for use.\n        \"\"\"\n        from capytaine.bem.airy_waves import airy_waves_free_surface_elevation\n        return airy_waves_free_surface_elevation(self, problem)",
  "def __init__(self, x_range=(-50.0, 50.0), nx=10, y_range=(-50.0, 50.0), ny=10, name=None):\n        self.x_range = x_range\n        self.nx = nx\n        self.y_range = y_range\n        self.ny = ny\n\n        if name is None:\n            self.name = f\"free_surface_{next(Mesh._ids)}\"\n        else:\n            self.name = name\n\n        self.mesh = self._generate_mesh()",
  "def _generate_mesh(self):\n        \"\"\"Generate a 2D cartesian mesh.\"\"\"\n        nodes = np.zeros(((self.nx+1)*(self.ny+1), 3), dtype=float)\n        panels = np.zeros((self.nx*self.ny, 4), dtype=int)\n\n        X = np.linspace(*self.x_range, self.nx+1)\n        Y = np.linspace(*self.y_range, self.ny+1)\n        for i, (x, y, z) in enumerate(product(X, Y, [0.0])):\n            nodes[i, :] = x, y, z\n\n        for k, (i, j) in enumerate(product(range(0, self.nx), range(0, self.ny))):\n            panels[k, :] = (j+i*(self.ny+1),\n                            (j+1)+i*(self.ny+1),\n                            (j+1)+(i+1)*(self.ny+1),\n                            j+(i+1)*(self.ny+1))\n\n        return Mesh(nodes, panels, name=f\"{self.name}_mesh\")",
  "def area(self):\n        \"\"\"The total area covered by the mesh.\"\"\"\n        return (np.abs(self.x_range[1] - self.x_range[0])\n                * np.abs(self.y_range[1] - self.y_range[0]))",
  "def incoming_waves(self, problem: \"DiffractionProblem\") -> np.ndarray:\n        \"\"\"Free surface elevation of the undisturbed incoming waves\n        for a given diffraction problem.\n        Kept for legacy, but not recommended for use.\n        \"\"\"\n        from capytaine.bem.airy_waves import airy_waves_free_surface_elevation\n        return airy_waves_free_surface_elevation(self, problem)",
  "def compute_kochin(result, theta, ref_point=(0.0, 0.0)):\n    \"\"\"Compute the far field coefficient\n\n    Parameters\n    ----------\n    result: LinearPotentialFlowResult\n        solved potential flow problem\n    theta: float or 1-dim array of floats\n        angles at which the coefficient is computed\n    ref_point: couple of float, optional\n        point of reference around which the far field coefficient is computed\n\n    Returns\n    -------\n    H: same type as theta\n        values of the Kochin function\n    \"\"\"\n\n    if result.sources is None:\n        raise Exception(f\"\"\"The values of the sources of {result} cannot been found.\n        They probably have not been stored by the solver because the option keep_details=True have not been set.\n        Please re-run the resolution with this option.\"\"\")\n\n    k = result.wavenumber\n    h = result.water_depth\n\n    # omega_bar.shape = (nb_faces, 2) @ (2, nb_theta)\n    omega_bar = (result.body.mesh.faces_centers[:, 0:2] - ref_point) @ (np.cos(theta), np.sin(theta))\n\n    if 0 <= k*h < 20:\n        cih = np.cosh(k*(result.body.mesh.faces_centers[:, 2]+h))/np.cosh(k*h)\n    else:\n        cih = np.exp(k*result.body.mesh.faces_centers[:, 2])\n\n    # cih.shape = (nb_faces,)\n    # omega_bar.T.shape = (nb_theta, nb_faces)\n    # result.body.mesh.faces_areas.shape = (nb_faces,)\n    zs = cih * np.exp(-1j * k * omega_bar.T) * result.body.mesh.faces_areas\n\n    # zs.shape = (nb_theta, nb_faces)\n    # result.sources.shape = (nb_faces,)\n    return zs @ result.sources/(4*np.pi)",
  "def rao(dataset, wave_direction=0.0, dissipation=None, stiffness=None):\n    \"\"\"Response Amplitude Operator.\n\n    Parameters\n    ----------\n    dataset: xarray Dataset\n        The hydrodynamical dataset.\n        This function supposes that variables named 'inertia_matrix' and 'hydrostatic_stiffness' are in the dataset.\n        Other variables can be computed by Capytaine, by those two should be manually added to the dataset.\n    wave_direction: float, optional\n        The direction of the incoming waves.\n        Default: 0 radian (x direction)\n    dissipation: array, optional\n        An optional dissipation matrix (e.g. Power Take Off) to be included in the RAO.\n        Default: none.\n    stiffness: array, optional\n        An optional stiffness matrix (e.g. mooring stiffness) to be included in the RAO.\n        Default: none.\n\n    Returns\n    -------\n    xarray DataArray\n        The RAO as an array depending of omega and the degree of freedom.\n    \"\"\"\n\n    # ASSEMBLE MATRICES\n    H = rao_transfer_function(dataset, dissipation, stiffness)\n\n    LOG.info(\"Compute RAO.\")\n\n    freq_dims = set(dataset.dims) & {'omega', 'period', 'wavelength', 'wavenumber'}\n    if len(freq_dims) != 1:\n        raise ValueError(\"The dataset provided to compute the RAO should one (and only one) dimension\" +\n                         \" among the following: 'omega', 'period', 'wavelength' or 'wavenumber'\\n\" +\n                         f\"The received dataset has the following dimensions: {dataset.dims}\")\n    main_freq_type = freq_dims.pop()\n\n    if 'excitation_force' not in dataset:\n        dataset['excitation_force'] = dataset['Froude_Krylov_force'] + dataset['diffraction_force']\n    excitation = dataset['excitation_force'].sel(wave_direction=wave_direction)\n\n    # SOLVE LINEAR SYSTEMS\n    # Reorder dimensions of the arrays to be sure to solve the right system.\n    H = H.transpose(main_freq_type, 'radiating_dof', 'influenced_dof')\n    excitation = excitation.transpose(main_freq_type,  'influenced_dof')\n\n    # Solve the linear systems (one for each value of omega)\n    X = np.linalg.solve(H, excitation)\n\n    return xr.DataArray(X, coords=[dataset.coords[main_freq_type], dataset.coords['radiating_dof']], dims=[main_freq_type, 'radiating_dof'])",
  "def rao_transfer_function(dataset, dissipation=None, stiffness=None):\n    \"\"\"Complex-valued matrix used for the computation of the RAO.\n\n    Parameters\n    ----------\n    dataset: xarray Dataset\n        The hydrodynamical dataset.\n        This function supposes that variables named 'inertia_matrix' and 'hydrostatic_stiffness' are in the dataset.\n        Other variables can be computed by Capytaine, by those two should be manually added to the dataset.\n    dissipation: array, optional\n        An optional dissipation matrix (e.g. Power Take Off) to be included in the transfer function.\n        Default: none.\n    stiffness: array, optional\n        An optional stiffness matrix (e.g. mooring stiffness) to be included in the transfer function.\n        Default: none.\n\n    Returns\n    -------\n    xarray DataArray\n        The matrix as an array depending of omega and the degrees of freedom.\n    \"\"\"\n\n    if not hasattr(dataset, 'inertia_matrix'):\n        raise AttributeError('Computing the impedance matrix requires a :code:`inertia_matrix` matrix to be defined in the hydrodynamical dataset')\n\n    if not hasattr(dataset, 'hydrostatic_stiffness'):\n        raise AttributeError('Computing the impedance matrix requires a :code:`hydrostatic_stiffness` matrix to be defined in the hydrodynamical dataset')\n\n    # ASSEMBLE MATRICES\n    omega = dataset.coords['omega']  # Range of frequencies in the dataset\n\n    H = (-omega**2*(dataset['inertia_matrix'] + dataset['added_mass'])\n         - 1j*omega*dataset['radiation_damping']\n         + dataset['hydrostatic_stiffness'])\n\n    if dissipation is not None:\n        H = H - 1j*omega*dissipation\n\n    if stiffness is not None:\n        H = H + stiffness\n\n    return H",
  "def impedance(dataset, dissipation=None, stiffness=None):\n    \"\"\"Complex-valued mechanical impedance matrix.\n    See Falnes for more theoretical details::\n\n        @book{falnes2002ocean,\n              title={Ocean Waves and Oscillating Systems: Linear Interactions Including Wave-Energy Extraction},\n              author={Falnes, J.},\n              isbn={9781139431934},\n              url={https://books.google.com/books?id=bl1FyQjCklgC},\n              year={2002},\n              publisher={Cambridge University Press}\n        }\n\n    Parameters\n    ----------\n    dataset: xarray Dataset\n        The hydrodynamical dataset.\n        This function supposes that variables named 'inertia_matrix' and 'hydrostatic_stiffness' are in the dataset.\n        Other variables can be computed by Capytaine, by those two should be manually added to the dataset.\n    dissipation: array, optional\n        An optional dissipation matrix (e.g. Power Take Off) to be included in the impedance.\n        Default: none.\n    stiffness: array, optional\n        An optional stiffness matrix (e.g. mooring stiffness) to be included in the impedance.\n        Default: none.\n\n    Returns\n    -------\n    xarray DataArray\n        The impedance as an array depending of omega and the degrees of freedom.\n    \"\"\"\n    return 1/(-1j * dataset.coords[\"omega\"]) * rao_transfer_function(dataset, dissipation, stiffness)",
  "def main():\n    args = parser.parse_args()\n    for paramfile in args.paramfiles:\n        problems = import_cal_file(paramfile)\n        solver = BEMSolver()\n        results = [solver.solve(pb) for pb in problems]\n        data = assemble_dataset(results)\n        print(data)\n\n        results_directory = os.path.join(os.path.dirname(paramfile), 'results')\n        try:\n            os.mkdir(results_directory)\n        except FileExistsError:\n            LOG.warning(\"The 'results' directory already exists. You might be overwriting existing data.\")\n\n        LOG.info(\"Write results in legacy tecplot format.\")\n        write_dataset_as_tecplot_files(results_directory, data)",
  "class Animation:\n    \"\"\"Class to generate an animation of a result of Capytaine,\n    including the elevation of the free surface.\n\n    The animation is made of a short loop of a single period of the solution in frequency domain.\n\n    Parameters\n    ----------\n    loop_duration: float\n        Duration in the loop. For real time animation, the period of the motion.\n    fps: int, optional\n        Number of frames per second in the animation (default: 24).\n\n    Attributes\n    ----------\n    frames_per_loop: int\n        Number of frames in one loop\n    actors: list of vtk actor objects\n        The objects in the scene.\n    \"\"\"\n\n    def __init__(self, loop_duration, fps=24):\n        self.fps = fps\n        self.frames_per_loop = int(fps * loop_duration)\n        self.actors = []\n\n        self._precomputed_polydatas = {}\n        self._current_frame = 0\n\n    # @classmethod\n    # def from_result(self, result):\n    #     from capytaine.bodies import TRANSLATION_DOFS_DIRECTIONS, ROTATION_DOFS_AXIS\n    #\n    #         if display_dof.lower() in TRANSLATION_DOFS_DIRECTIONS:\n    #             direction = np.asarray(TRANSLATION_DOFS_DIRECTIONS[display_dof.lower()])\n    #             def translation_motion(self, frame):\n    #                 nonlocal direction\n    #                 pos = np.asarray(self.body_actor.GetPosition())\n    #                 pos = (1 - direction) * pos + \\\n    #                       direction * np.cos(2*np.pi*(frame % self.frames_per_loop)/self.frames_per_loop)\n    #                 self.body_actor.SetPosition(*pos)\n    #             self.update_body_position = translation_motion\n    #\n    #         elif display_dof.lower() in ROTATION_DOFS_AXIS:\n    #             direction = np.asarray(ROTATION_DOFS_AXIS[display_dof.lower()])\n    #             def rotation_motion(self, frame):\n    #                 nonlocal direction\n    #                 pos = np.asarray(self.body_actor.GetOrientation())\n    #                 pos = (1 - direction) * pos + \\\n    #                       direction * np.cos(2*np.pi*(frame % self.frames_per_loop)/self.frames_per_loop)\n    #                 self.body_actor.SetOrientation(*pos)\n    #             self.update_body_position = rotation_motion\n\n\n    def _add_actor(self, mesh, faces_motion=None, faces_colors=None, edges=False):\n        \"\"\"Add an animated object to the scene.\"\"\"\n        if faces_motion is not None:\n            nodes_motion = compute_node_data(mesh.merged(), faces_motion)\n        else:\n            nodes_motion = None\n\n        base_polydata = compute_vtk_polydata(mesh.merged())\n        mapper = vtk.vtkPolyDataMapper()\n        mapper.SetInputData(base_polydata)\n\n        actor = vtk.vtkActor()\n        if edges:\n            actor.GetProperty().EdgeVisibilityOn()\n        actor.GetProperty().SetInterpolationToGouraud()\n        actor.SetMapper(mapper)\n\n        if nodes_motion is not None or faces_colors is not None:\n            LOG.info(f\"Precompute motions of {mesh.name} before animation.\")\n            self._precomputed_polydatas[actor] = []\n\n            for i_frame in range(self.frames_per_loop):\n                new_polydata = vtk.vtkPolyData()\n                new_polydata.DeepCopy(base_polydata)\n\n                if nodes_motion is not None:\n                    # Change points positions at frame i\n                    current_deformation = (\n                            np.abs(nodes_motion)*np.cos(np.angle(nodes_motion)-2*pi*i_frame/self.frames_per_loop)\n                    )\n\n                    points = new_polydata.GetPoints()\n                    for j in range(mesh.nb_vertices):\n                        point = points.GetPoint(j)\n                        point = np.asarray(point) + current_deformation[j]\n                        points.SetPoint(j, tuple(point))\n\n                if faces_colors is not None:\n                    # Evaluate scalar field at frame i\n                    current_colors = (\n                        np.abs(faces_colors)*np.cos(np.angle(faces_colors)-2*pi*i_frame/self.frames_per_loop)\n                    )\n                    max_val = max(abs(faces_colors))\n                    vtk_faces_colors = vtk.vtkFloatArray()\n                    for i, color in enumerate(current_colors):\n                        vtk_faces_colors.InsertValue(i, (color+max_val)/(2*max_val))\n                    new_polydata.GetCellData().SetScalars(vtk_faces_colors)\n\n                self._precomputed_polydatas[actor].append(new_polydata)\n        else:\n            self._precomputed_polydatas[actor] = None\n\n        self.actors.append(actor)\n\n        return actor\n\n    def add_body(self, body, faces_motion=None, faces_colors=None, edges=False):\n        \"\"\"Add an floating body to the scene.\n\n        Parameters\n        ----------\n        body: FloatingBody\n            The object to include in the scene.\n        faces_motion: dof, optional\n            The motion of the body defined at the center of the faces.\n        faces_colors: iterable of complex numbers, optional\n            Scalar field over the surface of the body that should be displayed with colors.\n        edges: bool, optional\n            Draw the edges of the mesh in the scene.\n\n        Returns\n        -------\n        vtk actor object\n        \"\"\"\n        actor = self._add_actor(body.mesh.merged(), faces_motion=faces_motion,\n                                faces_colors=faces_colors, edges=edges)\n        if faces_colors is None:\n            actor.GetProperty().SetColor((1, 1, 0))\n        else:\n            lut = vtk.vtkLookupTable()\n            lut.SetNumberOfColors(50)\n            lut.SetHueRange(0, 0.6)\n            lut.SetSaturationRange(0.5, 0.5)\n            lut.SetValueRange(0.8, 0.8)\n            lut.Build()\n            actor.GetMapper().SetLookupTable(lut)\n\n        return actor\n\n    def add_free_surface(self, free_surface, faces_elevation):\n        \"\"\"Add the free surface to the scene.\n\n        Parameters\n        ----------\n        free_surface: FreeSurface\n            The free surface object\n        faces_elevation: array of complex numbers\n            The elevation of each face of the meshed free surface given as a complex number.\n\n        Returns\n        -------\n        vtk actor object\n        \"\"\"\n        faces_motion = np.array([(0, 0, elevation) for elevation in faces_elevation])\n        actor = self._add_actor(free_surface.mesh, faces_motion=faces_motion, faces_colors=faces_motion[:, 2])\n\n        lut = vtk.vtkLookupTable()\n        lut.SetNumberOfColors(50)\n        lut.SetHueRange(0.58, 0.58)\n        lut.SetSaturationRange(0.5, 0.5)\n        lut.SetValueRange(0.4, 0.6)\n        lut.Build()\n        actor.GetMapper().SetLookupTable(lut)\n\n        return actor\n\n    def _callback(self, renderer, event):\n        for actor in self.actors:\n            if self._precomputed_polydatas[actor] is not None:\n                actor.GetMapper().SetInputData(self._precomputed_polydatas[actor][self._current_frame % self.frames_per_loop])\n        renderer.GetRenderWindow().Render()\n        self._current_frame += 1\n\n    def run(self, camera_position=(-10.0, -10.0, 10.0), resolution=(1280, 720), top_light_intensity=0.5):\n        \"\"\"Run the animation.\n\n        Parameters\n        ----------\n        camera_position: 3-ple of floats, optional\n            The starting position of the camera in the scene.\n        resolution: 2-ple of ints, optional\n            Resolution of the video in pixels.\n        top_light_intensity: float between 0 and 1\n            Intensity of the light source at the top of the scene (default: 0.5)\n        \"\"\"\n        # Setup a renderer, render window, and interactor\n        renderer = vtk.vtkRenderer()\n        renderer.SetBackground(1, 1, 1)  # Background color white\n        for actor in self.actors:\n            renderer.AddActor(actor)\n        renderer.Modified()\n\n        camera = vtk.vtkCamera()\n        camera.SetPosition(*camera_position)\n        camera.SetFocalPoint(0, 0, 0)\n        camera.SetViewUp(0, 0, 1)\n        renderer.SetActiveCamera(camera)\n\n        light = vtk.vtkLight()\n        light.SetLightTypeToHeadlight()\n        renderer.AddLight(light)\n\n        if top_light_intensity > 0.0:\n            light = vtk.vtkLight()\n            light.SetDirectionAngle(0, 0)\n            light.SetLightTypeToSceneLight()\n            light.SetIntensity(top_light_intensity)\n            renderer.AddLight(light)\n\n        render_window = vtk.vtkRenderWindow()\n        render_window.SetSize(*resolution)\n        render_window.SetWindowName(\"Capytaine animation\")\n        render_window.AddRenderer(renderer)\n\n        render_window_interactor = vtk.vtkRenderWindowInteractor()\n        render_window_interactor.SetRenderWindow(render_window)\n        render_window_interactor.GetInteractorStyle().SetCurrentStyleToTrackballCamera()\n\n        render_window.Render()\n\n        render_window_interactor.Initialize()  # Initialize must be called prior to creating timer events.\n\n        render_window_interactor.AddObserver('TimerEvent', self._callback)\n        render_window_interactor.CreateRepeatingTimer(int(1000 / self.fps))\n\n        render_window_interactor.Start()\n\n        # Run until stopped by user.\n\n        del render_window_interactor\n        del render_window\n\n    def save(self, filepath, nb_loops=1, camera_position=(-10.0, -10.0, 10.0), resolution=(1280, 720), top_light_intensity=0.5):\n        \"\"\"Save the animation in a video file.\n\n        Parameters\n        ----------\n        filepath: string\n            Path of the output file.\n        nb_loop: int, optional\n            Number of periods to save in the file.\n        camera_position: 3-ple of floats, optional\n            The starting position of the camera in the scene.\n        resolution: 2-ple of ints, optional\n            Resolution of the video in pixels.\n        top_light_intensity: float between 0 and 1\n            Intensity of the light source at the top of the scene (default: 0.5)\n        \"\"\"\n        renderer = vtk.vtkRenderer()\n        renderer.SetBackground(1, 1, 1)  # Background color white\n        for actor in self.actors:\n            renderer.AddActor(actor)\n        renderer.Modified()\n\n        camera = vtk.vtkCamera()\n        camera.SetPosition(*camera_position)\n        camera.SetFocalPoint(0, 0, 0)\n        camera.SetViewUp(0, 0, 1)\n        renderer.SetActiveCamera(camera)\n\n        light = vtk.vtkLight()\n        light.SetLightTypeToHeadlight()\n        renderer.AddLight(light)\n\n        if top_light_intensity > 0.0:\n            light = vtk.vtkLight()\n            light.SetDirectionAngle(0, 0)\n            light.SetLightTypeToSceneLight()\n            light.SetIntensity(top_light_intensity)\n            renderer.AddLight(light)\n\n        render_window = vtk.vtkRenderWindow()\n        render_window.SetSize(*resolution)\n        render_window.OffScreenRenderingOn()\n        render_window.AddRenderer(renderer)\n\n        image_filter = vtk.vtkWindowToImageFilter()\n        image_filter.SetInput(render_window)\n        image_filter.SetInputBufferTypeToRGB()\n        image_filter.ReadFrontBufferOff()\n\n        writer = vtk.vtkOggTheoraWriter()\n        writer.SetInputConnection(image_filter.GetOutputPort())\n        writer.SetFileName(filepath)\n        writer.SetRate(self.fps)\n\n        writer.Start()\n\n        for i_frame in range(nb_loops*self.frames_per_loop):\n            self._callback(renderer, None)\n            image_filter.Modified()\n            writer.Write()\n\n        writer.End()\n        render_window.Finalize()\n\n        del image_filter\n        del writer\n        del render_window\n\n    def embed_in_notebook(self, resolution=(640, 360), **kwargs):\n        from tempfile import mkstemp\n        from IPython.core.display import Video\n        # Requires Ipython 7.14 or higher, for this patch: https://github.com/ipython/ipython/pull/12212/\n\n        filepath = mkstemp(suffix=\".ogv\")[1]\n        self.save(filepath, nb_loops=1, resolution=resolution, **kwargs)\n        return Video(filepath, embed=True, width=resolution[0], html_attributes=\"controls loop autoplay\")",
  "def __init__(self, loop_duration, fps=24):\n        self.fps = fps\n        self.frames_per_loop = int(fps * loop_duration)\n        self.actors = []\n\n        self._precomputed_polydatas = {}\n        self._current_frame = 0",
  "def _add_actor(self, mesh, faces_motion=None, faces_colors=None, edges=False):\n        \"\"\"Add an animated object to the scene.\"\"\"\n        if faces_motion is not None:\n            nodes_motion = compute_node_data(mesh.merged(), faces_motion)\n        else:\n            nodes_motion = None\n\n        base_polydata = compute_vtk_polydata(mesh.merged())\n        mapper = vtk.vtkPolyDataMapper()\n        mapper.SetInputData(base_polydata)\n\n        actor = vtk.vtkActor()\n        if edges:\n            actor.GetProperty().EdgeVisibilityOn()\n        actor.GetProperty().SetInterpolationToGouraud()\n        actor.SetMapper(mapper)\n\n        if nodes_motion is not None or faces_colors is not None:\n            LOG.info(f\"Precompute motions of {mesh.name} before animation.\")\n            self._precomputed_polydatas[actor] = []\n\n            for i_frame in range(self.frames_per_loop):\n                new_polydata = vtk.vtkPolyData()\n                new_polydata.DeepCopy(base_polydata)\n\n                if nodes_motion is not None:\n                    # Change points positions at frame i\n                    current_deformation = (\n                            np.abs(nodes_motion)*np.cos(np.angle(nodes_motion)-2*pi*i_frame/self.frames_per_loop)\n                    )\n\n                    points = new_polydata.GetPoints()\n                    for j in range(mesh.nb_vertices):\n                        point = points.GetPoint(j)\n                        point = np.asarray(point) + current_deformation[j]\n                        points.SetPoint(j, tuple(point))\n\n                if faces_colors is not None:\n                    # Evaluate scalar field at frame i\n                    current_colors = (\n                        np.abs(faces_colors)*np.cos(np.angle(faces_colors)-2*pi*i_frame/self.frames_per_loop)\n                    )\n                    max_val = max(abs(faces_colors))\n                    vtk_faces_colors = vtk.vtkFloatArray()\n                    for i, color in enumerate(current_colors):\n                        vtk_faces_colors.InsertValue(i, (color+max_val)/(2*max_val))\n                    new_polydata.GetCellData().SetScalars(vtk_faces_colors)\n\n                self._precomputed_polydatas[actor].append(new_polydata)\n        else:\n            self._precomputed_polydatas[actor] = None\n\n        self.actors.append(actor)\n\n        return actor",
  "def add_body(self, body, faces_motion=None, faces_colors=None, edges=False):\n        \"\"\"Add an floating body to the scene.\n\n        Parameters\n        ----------\n        body: FloatingBody\n            The object to include in the scene.\n        faces_motion: dof, optional\n            The motion of the body defined at the center of the faces.\n        faces_colors: iterable of complex numbers, optional\n            Scalar field over the surface of the body that should be displayed with colors.\n        edges: bool, optional\n            Draw the edges of the mesh in the scene.\n\n        Returns\n        -------\n        vtk actor object\n        \"\"\"\n        actor = self._add_actor(body.mesh.merged(), faces_motion=faces_motion,\n                                faces_colors=faces_colors, edges=edges)\n        if faces_colors is None:\n            actor.GetProperty().SetColor((1, 1, 0))\n        else:\n            lut = vtk.vtkLookupTable()\n            lut.SetNumberOfColors(50)\n            lut.SetHueRange(0, 0.6)\n            lut.SetSaturationRange(0.5, 0.5)\n            lut.SetValueRange(0.8, 0.8)\n            lut.Build()\n            actor.GetMapper().SetLookupTable(lut)\n\n        return actor",
  "def add_free_surface(self, free_surface, faces_elevation):\n        \"\"\"Add the free surface to the scene.\n\n        Parameters\n        ----------\n        free_surface: FreeSurface\n            The free surface object\n        faces_elevation: array of complex numbers\n            The elevation of each face of the meshed free surface given as a complex number.\n\n        Returns\n        -------\n        vtk actor object\n        \"\"\"\n        faces_motion = np.array([(0, 0, elevation) for elevation in faces_elevation])\n        actor = self._add_actor(free_surface.mesh, faces_motion=faces_motion, faces_colors=faces_motion[:, 2])\n\n        lut = vtk.vtkLookupTable()\n        lut.SetNumberOfColors(50)\n        lut.SetHueRange(0.58, 0.58)\n        lut.SetSaturationRange(0.5, 0.5)\n        lut.SetValueRange(0.4, 0.6)\n        lut.Build()\n        actor.GetMapper().SetLookupTable(lut)\n\n        return actor",
  "def _callback(self, renderer, event):\n        for actor in self.actors:\n            if self._precomputed_polydatas[actor] is not None:\n                actor.GetMapper().SetInputData(self._precomputed_polydatas[actor][self._current_frame % self.frames_per_loop])\n        renderer.GetRenderWindow().Render()\n        self._current_frame += 1",
  "def run(self, camera_position=(-10.0, -10.0, 10.0), resolution=(1280, 720), top_light_intensity=0.5):\n        \"\"\"Run the animation.\n\n        Parameters\n        ----------\n        camera_position: 3-ple of floats, optional\n            The starting position of the camera in the scene.\n        resolution: 2-ple of ints, optional\n            Resolution of the video in pixels.\n        top_light_intensity: float between 0 and 1\n            Intensity of the light source at the top of the scene (default: 0.5)\n        \"\"\"\n        # Setup a renderer, render window, and interactor\n        renderer = vtk.vtkRenderer()\n        renderer.SetBackground(1, 1, 1)  # Background color white\n        for actor in self.actors:\n            renderer.AddActor(actor)\n        renderer.Modified()\n\n        camera = vtk.vtkCamera()\n        camera.SetPosition(*camera_position)\n        camera.SetFocalPoint(0, 0, 0)\n        camera.SetViewUp(0, 0, 1)\n        renderer.SetActiveCamera(camera)\n\n        light = vtk.vtkLight()\n        light.SetLightTypeToHeadlight()\n        renderer.AddLight(light)\n\n        if top_light_intensity > 0.0:\n            light = vtk.vtkLight()\n            light.SetDirectionAngle(0, 0)\n            light.SetLightTypeToSceneLight()\n            light.SetIntensity(top_light_intensity)\n            renderer.AddLight(light)\n\n        render_window = vtk.vtkRenderWindow()\n        render_window.SetSize(*resolution)\n        render_window.SetWindowName(\"Capytaine animation\")\n        render_window.AddRenderer(renderer)\n\n        render_window_interactor = vtk.vtkRenderWindowInteractor()\n        render_window_interactor.SetRenderWindow(render_window)\n        render_window_interactor.GetInteractorStyle().SetCurrentStyleToTrackballCamera()\n\n        render_window.Render()\n\n        render_window_interactor.Initialize()  # Initialize must be called prior to creating timer events.\n\n        render_window_interactor.AddObserver('TimerEvent', self._callback)\n        render_window_interactor.CreateRepeatingTimer(int(1000 / self.fps))\n\n        render_window_interactor.Start()\n\n        # Run until stopped by user.\n\n        del render_window_interactor\n        del render_window",
  "def save(self, filepath, nb_loops=1, camera_position=(-10.0, -10.0, 10.0), resolution=(1280, 720), top_light_intensity=0.5):\n        \"\"\"Save the animation in a video file.\n\n        Parameters\n        ----------\n        filepath: string\n            Path of the output file.\n        nb_loop: int, optional\n            Number of periods to save in the file.\n        camera_position: 3-ple of floats, optional\n            The starting position of the camera in the scene.\n        resolution: 2-ple of ints, optional\n            Resolution of the video in pixels.\n        top_light_intensity: float between 0 and 1\n            Intensity of the light source at the top of the scene (default: 0.5)\n        \"\"\"\n        renderer = vtk.vtkRenderer()\n        renderer.SetBackground(1, 1, 1)  # Background color white\n        for actor in self.actors:\n            renderer.AddActor(actor)\n        renderer.Modified()\n\n        camera = vtk.vtkCamera()\n        camera.SetPosition(*camera_position)\n        camera.SetFocalPoint(0, 0, 0)\n        camera.SetViewUp(0, 0, 1)\n        renderer.SetActiveCamera(camera)\n\n        light = vtk.vtkLight()\n        light.SetLightTypeToHeadlight()\n        renderer.AddLight(light)\n\n        if top_light_intensity > 0.0:\n            light = vtk.vtkLight()\n            light.SetDirectionAngle(0, 0)\n            light.SetLightTypeToSceneLight()\n            light.SetIntensity(top_light_intensity)\n            renderer.AddLight(light)\n\n        render_window = vtk.vtkRenderWindow()\n        render_window.SetSize(*resolution)\n        render_window.OffScreenRenderingOn()\n        render_window.AddRenderer(renderer)\n\n        image_filter = vtk.vtkWindowToImageFilter()\n        image_filter.SetInput(render_window)\n        image_filter.SetInputBufferTypeToRGB()\n        image_filter.ReadFrontBufferOff()\n\n        writer = vtk.vtkOggTheoraWriter()\n        writer.SetInputConnection(image_filter.GetOutputPort())\n        writer.SetFileName(filepath)\n        writer.SetRate(self.fps)\n\n        writer.Start()\n\n        for i_frame in range(nb_loops*self.frames_per_loop):\n            self._callback(renderer, None)\n            image_filter.Modified()\n            writer.Write()\n\n        writer.End()\n        render_window.Finalize()\n\n        del image_filter\n        del writer\n        del render_window",
  "def embed_in_notebook(self, resolution=(640, 360), **kwargs):\n        from tempfile import mkstemp\n        from IPython.core.display import Video\n        # Requires Ipython 7.14 or higher, for this patch: https://github.com/ipython/ipython/pull/12212/\n\n        filepath = mkstemp(suffix=\".ogv\")[1]\n        self.save(filepath, nb_loops=1, resolution=resolution, **kwargs)\n        return Video(filepath, embed=True, width=resolution[0], html_attributes=\"controls loop autoplay\")",
  "class FloatingBodyViewer(MeshViewer):\n\n    def __init__(self):\n        super().__init__()\n        self.dofs_data = {}\n\n    def add_body(self, body, **kwargs):\n        self.add_mesh(body.mesh, **kwargs)\n\n        for dof in body.dofs:\n            vtk_data_array = vtk.vtkFloatArray()\n            vtk_data_array.SetNumberOfComponents(3)\n            vtk_data_array.SetNumberOfTuples(body.mesh.nb_faces)\n            for i, vector in enumerate(body.dofs[dof]):\n                vtk_data_array.SetTuple3(i, *vector)\n            self.dofs_data[dof] = vtk_data_array",
  "def __init__(self):\n        super().__init__()\n        self.dofs_data = {}",
  "def add_body(self, body, **kwargs):\n        self.add_mesh(body.mesh, **kwargs)\n\n        for dof in body.dofs:\n            vtk_data_array = vtk.vtkFloatArray()\n            vtk_data_array.SetNumberOfComponents(3)\n            vtk_data_array.SetNumberOfTuples(body.mesh.nb_faces)\n            for i, vector in enumerate(body.dofs[dof]):\n                vtk_data_array.SetTuple3(i, *vector)\n            self.dofs_data[dof] = vtk_data_array",
  "class MeshViewer:\n    \"\"\"This class implements a viewer based on VTK\"\"\"\n    def __init__(self):\n\n        # Building renderer\n        self.renderer = vtk.vtkRenderer()\n        self.renderer.SetBackground(0.7706, 0.8165, 1.0)\n\n        # Building render window\n        self.render_window = vtk.vtkRenderWindow()\n        self.render_window.SetSize(1024, 768)\n        self.render_window.SetWindowName(\"Mesh viewer\")\n        self.render_window.AddRenderer(self.renderer)\n\n        # Building interactor\n        self.render_window_interactor = vtk.vtkRenderWindowInteractor()\n        self.render_window_interactor.SetRenderWindow(self.render_window)\n        self.render_window_interactor.GetInteractorStyle().SetCurrentStyleToTrackballCamera()\n        self.render_window_interactor.AddObserver('KeyPressEvent', self.on_key_press, 0.0)\n\n        # Building axes view\n        axes = vtk.vtkAxesActor()\n        widget = vtk.vtkOrientationMarkerWidget()\n        widget.SetOrientationMarker(axes)\n        self.widget = widget\n\n        self.widget.SetInteractor(self.render_window_interactor)\n        self.widget.SetEnabled(1)\n        self.widget.InteractiveOn()\n\n        # Building command annotations\n        command_text = (\"left mouse : rotate\\n\"\n                        \"right mouse : zoom\\n\"\n                        \"middle mouse : pan\\n\"\n                        \"ctrl+left mouse : spin\\n\"\n                        \"n : (un)show normals\\n\"\n                        \"b : (un)show axes box\\n\"\n                        \"f : focus on the mouse cursor\\n\"\n                        \"r : reset view\\n\"\n                        \"s : surface representation\\n\"\n                        \"w : wire representation\\n\"\n                        \"h : (un)show Oxy plane\\n\"\n                        \"x : save\\n\"\n                        \"c : screenshot\\n\"\n                        \"q : quit\")\n\n        corner_annotation = vtk.vtkCornerAnnotation()\n        corner_annotation.SetLinearFontScaleFactor(2)\n        corner_annotation.SetNonlinearFontScaleFactor(1)\n        corner_annotation.SetMaximumFontSize(20)\n        corner_annotation.SetText(3, command_text)\n        corner_annotation.GetTextProperty().SetColor(0., 0., 0.)\n        self.renderer.AddViewProp(corner_annotation)\n\n        copyright_text = (f\"Capytaine \u2014 Copyright 2017-{__year__} University College Dublin\\n\"\n                          f\"based on Meshmagick Viewer \u2014 Copyright 2014-{__year__}, \u00c9cole Centrale de Nantes\")\n\n        copyright_annotation = vtk.vtkCornerAnnotation()\n        copyright_annotation.SetLinearFontScaleFactor(0.5)\n        copyright_annotation.SetNonlinearFontScaleFactor(1)\n        copyright_annotation.SetMaximumFontSize(12)\n        copyright_annotation.SetText(1, copyright_text)\n        copyright_annotation.GetTextProperty().SetColor(0., 0., 0.)\n        self.renderer.AddViewProp(copyright_annotation)\n\n        self.polydatas = []\n        self.normals = []\n        self.axes = []\n        self.oxy_plane = None\n\n    # DISPLAY OF A MESH\n\n    def add_polydata(self, polydata, color=(1, 1, 0), representation='surface'):\n        \"\"\"Add a polydata object to the viewer\n\n        Parameters\n        ----------\n        polydata : vtkPolyData\n            the object to be added\n        color : array_like, optional\n            the color of the object. Default is yellow (1, 1, 0)\n        representation : str\n            the representation mode of the object ('surface' or 'wireframe'). Default is 'surface'.\n        \"\"\"\n\n        assert isinstance(polydata, vtk.vtkPolyData)\n        assert representation in ('surface', 'wireframe')\n\n        self.polydatas.append(polydata)\n\n        # Building mapper\n        mapper = vtk.vtkPolyDataMapper()\n        mapper.SetInputData(polydata)\n\n        # Building actor\n        actor = vtk.vtkActor()\n        actor.SetMapper(mapper)\n\n        # Properties setting\n        actor.GetProperty().SetColor(color)\n        actor.GetProperty().EdgeVisibilityOn()\n        actor.GetProperty().SetEdgeColor(0, 0, 0)\n        actor.GetProperty().SetLineWidth(1)\n        actor.GetProperty().SetPointSize(10)\n        if representation == 'wireframe':\n            actor.GetProperty().SetRepresentationToWireframe()\n\n        self.renderer.AddActor(actor)\n        self.renderer.Modified()\n\n    def add_mesh(self, mesh, color=None, representation='surface'):\n        vtk_polydata = compute_vtk_polydata(mesh)\n        if color is None:\n            color = next(COLORS)\n        self.add_polydata(vtk_polydata, color=color, representation=representation)\n\n    # ADD MORE DETAILS\n\n    def add_oxy_plane(self):\n        \"\"\"Displays the Oxy plane\"\"\"\n\n        one_mesh_in_the_viewer = self.polydatas[0]\n\n        plane = vtk.vtkPlaneSource()\n        (xmin, xmax, ymin, ymax, _, _) = one_mesh_in_the_viewer.GetBounds()\n\n        dx = max(0.1 * (xmax - xmin), 0.1)\n        dy = max(0.1 * (ymax - ymin), 0.1)\n\n        plane.SetOrigin(xmin - dx, ymax + dy, 0)\n        plane.SetPoint1(xmin - dx, ymin - dy, 0)\n        plane.SetPoint2(xmax + dx, ymax + dy, 0)\n        plane.Update()\n        polydata = plane.GetOutput()\n\n        mapper = vtk.vtkPolyDataMapper()\n        mapper.SetInputData(polydata)\n\n        actor = vtk.vtkActor()\n        actor.SetMapper(mapper)\n\n        color = [0., 102. / 255, 204. / 255]\n        actor.GetProperty().SetColor(color)\n        actor.GetProperty().SetEdgeColor(0, 0, 0)\n        actor.GetProperty().SetLineWidth(1)\n\n        self.renderer.AddActor(actor)\n        self.renderer.Modified()\n        self.oxy_plane = actor\n\n    def show_normals(self):\n        \"\"\"Shows the normals of the current objects\"\"\"\n\n        for polydata in self.polydatas:\n            normals = vtk.vtkPolyDataNormals()\n            normals.ConsistencyOff()\n            normals.ComputeCellNormalsOn()\n            normals.SetInputData(polydata)\n            normals.Update()\n\n            normals_at_centers = vtk.vtkCellCenters()\n            normals_at_centers.SetInputConnection(normals.GetOutputPort())\n\n            arrows = vtk.vtkArrowSource()\n            arrows.SetTipResolution(16)\n            arrows.SetTipLength(0.5)\n            arrows.SetTipRadius(0.1)\n\n            glyph = vtk.vtkGlyph3D()\n            glyph.SetSourceConnection(arrows.GetOutputPort())\n            glyph.SetInputConnection(normals_at_centers.GetOutputPort())\n            glyph.SetVectorModeToUseNormal()\n            glyph.SetScaleModeToScaleByVector()\n            glyph.SetScaleFactor(1)  # FIXME: may be too big ...\n            # glyph.SetVectorModeToUseNormal()\n            # glyph.SetVectorModeToUseVector()\n            # glyph.SetScaleModeToDataScalingOff()\n            glyph.Update()\n\n            glyph_mapper = vtk.vtkPolyDataMapper()\n            glyph_mapper.SetInputConnection(glyph.GetOutputPort())\n\n            glyph_actor = vtk.vtkActor()\n            glyph_actor.SetMapper(glyph_mapper)\n\n            self.renderer.AddActor(glyph_actor)\n            self.normals.append(glyph_actor)\n\n    def show_axes(self):\n        \"\"\"Shows the axes around the main object\"\"\"\n\n        tprop = vtk.vtkTextProperty()\n        tprop.SetColor(0., 0., 0.)\n        tprop.ShadowOn()\n\n        axes = vtk.vtkCubeAxesActor2D()\n        axes.SetInputData(self.polydatas[0])\n\n        axes.SetCamera(self.renderer.GetActiveCamera())\n        axes.SetLabelFormat(\"%6.4g\")\n        axes.SetFlyModeToOuterEdges()\n        axes.SetFontFactor(0.8)\n        axes.SetAxisTitleTextProperty(tprop)\n        axes.SetAxisLabelTextProperty(tprop)\n        # axes.DrawGridLinesOn()\n\n        self.renderer.AddViewProp(axes)\n        self.axes.append(axes)\n\n    def save(self):\n        \"\"\"Saves the main object in a 'mmviewer_save.vtp' vtp file is the current folder\"\"\"\n\n        writer = vtk.vtkXMLPolyDataWriter()\n        writer.SetDataModeToAscii()\n        writer.SetFileName('mmviewer_save.vtp')\n\n        for polydata in self.polydatas:\n            writer.SetInputData(polydata)\n        writer.Write()\n\n        print(\"File 'mmviewer_save.vtp' written in %s\" % getcwd())\n        return\n\n    def screenshot(self):\n        \"\"\"Saves a screenshot of the current window in a file screenshot.png\"\"\"\n        w2if = vtk.vtkWindowToImageFilter()\n        w2if.SetInput(self.render_window)\n        w2if.Update()\n\n        writer = vtk.vtkPNGWriter()\n        writer.SetFileName(\"screenshot.png\")\n        writer.SetInputData(w2if.GetOutput())\n        writer.Write()\n\n        print(\"File 'screenshot.png' written in %s\" % getcwd())\n        return\n\n    # INTERACTION\n\n    def show(self):\n        \"\"\"Show the viewer\"\"\"\n        self.renderer.ResetCamera()\n        self.render_window.Render()\n        self.render_window_interactor.Start()\n\n    def on_key_press(self, obj, event):\n        \"\"\"Event trig at keystroke\"\"\"\n        key = obj.GetKeySym()\n\n        if key == 'n':\n            if self.normals:\n                # self.normals = False\n                for actor in self.normals:\n                    self.renderer.RemoveActor(actor)\n                self.renderer.Render()\n                self.normals = []\n            else:\n                self.show_normals()\n                self.renderer.Render()\n\n        elif key == 'b':\n            if self.axes:\n                for axis in self.axes:\n                    self.renderer.RemoveActor(axis)\n                self.axes = []\n            else:\n                self.show_axes()\n\n        elif key == 'x':\n            self.save()\n\n        elif key == 'c':\n            self.screenshot()\n\n        elif key == 'h':\n            if self.oxy_plane:\n                self.renderer.RemoveActor(self.oxy_plane)\n                self.oxy_plane = None\n            else:\n                self.add_oxy_plane()\n\n        elif key == 'e' or key == 'q':\n            self.render_window_interactor.GetRenderWindow().Finalize()\n            self.render_window_interactor.TerminateApp()\n\n    def finalize(self):\n        \"\"\"Cleanly close the viewer\"\"\"\n        del self.render_window\n        del self.render_window_interactor\n\n    # =======================================\n    # =======================================\n    # =======================================\n\n    # OTHER METHODS THAT ARE CURRENTLY UNUSED\n\n    def add_point(self, pos, color=(0, 0, 0)):\n        \"\"\"Add a point to the viewer\n\n        Parameters\n        ----------\n        pos : array_like\n            The point's position\n        color : array_like, optional\n            The RGB color required for the point. Default is (0, 0, 0) corresponding to black.\n\n        Returns\n        -------\n        vtkPolyData\n        \"\"\"\n\n        assert len(pos) == 3\n\n        p = vtk.vtkPoints()\n        v = vtk.vtkCellArray()\n\n        i = p.InsertNextPoint(pos)\n        v.InsertNextCell(1)\n        v.InsertCellPoint(i)\n\n        pd = vtk.vtkPolyData()\n        pd.SetPoints(p)\n        pd.SetVerts(v)\n\n        self.add_polydata(pd, color=color)\n\n        return pd\n\n    def add_line(self, p0, p1, color=(0, 0, 0)):\n        \"\"\"Add a line to the viewer\n\n        Parameters\n        ----------\n        p0 : array_like\n            position of one end point of the line\n        p1 : array_like\n            position of a second end point of the line\n        color : array_like, optional\n            RGB color of the line. Default is black (0, 0, 0)\n\n        Returns\n        -------\n        vtkPolyData\n        \"\"\"\n\n        assert len(p0) == 3 and len(p1) == 3\n\n        points = vtk.vtkPoints()\n        points.InsertNextPoint(p0)\n        points.InsertNextPoint(p1)\n\n        line = vtk.vtkLine()\n        line.GetPointIds().SetId(0, 0)\n        line.GetPointIds().SetId(1, 1)\n\n        lines = vtk.vtkCellArray()\n        lines.InsertNextCell(line)\n\n        lines_pd = vtk.vtkPolyData()\n        lines_pd.SetPoints(points)\n        lines_pd.SetLines(lines)\n\n        self.add_polydata(lines_pd, color=color)\n\n        return lines_pd\n\n    def add_vector(self, point, value, scale=1, color=(0, 0, 0)):\n        \"\"\"Add a vector to the viewer\n\n        Parameters\n        ----------\n        point : array_like\n            starting point position of the vector\n        value : float\n            the magnitude of the vector\n        scale : float, optional\n            the scaling to apply to the vector for better visualization. Default is 1.\n        color : array_like\n            The color of the vector. Default is black (0, 0, 0)\n        \"\"\"\n\n        points = vtk.vtkPoints()\n        idx = points.InsertNextPoint(point)\n\n        vert = vtk.vtkCellArray()\n        vert.InsertNextCell(1)\n        vert.InsertCellPoint(idx)\n        pd_point = vtk.vtkPolyData()\n        pd_point.SetPoints(points)\n        pd_point.SetVerts(vert)\n\n        arrow = vtk.vtkArrowSource()\n        arrow.SetTipResolution(16)\n        arrow.SetTipLength(0.1)\n        arrow.SetTipRadius(0.02)\n        arrow.SetShaftRadius(0.005)\n\n        vec = vtk.vtkFloatArray()\n        vec.SetNumberOfComponents(3)\n        v0, v1, v2 = value / scale\n        vec.InsertTuple3(idx, v0, v1, v2)\n        pd_point.GetPointData().SetVectors(vec)\n\n        g_glyph = vtk.vtkGlyph3D()\n        # g_glyph.SetScaleModeToDataScalingOff()\n        g_glyph.SetVectorModeToUseVector()\n        g_glyph.SetInputData(pd_point)\n        g_glyph.SetSourceConnection(arrow.GetOutputPort())\n        g_glyph.SetScaleModeToScaleByVector()\n        # g_glyph.SetScaleFactor(10)\n        g_glyph.ScalingOn()\n        g_glyph.Update()\n\n        g_glyph_mapper = vtk.vtkPolyDataMapper()\n        g_glyph_mapper.SetInputConnection(g_glyph.GetOutputPort())\n\n        g_glyph_actor = vtk.vtkActor()\n        g_glyph_actor.SetMapper(g_glyph_mapper)\n        g_glyph_actor.GetProperty().SetColor(color)\n\n        self.renderer.AddActor(g_glyph_actor)\n\n    def add_plane(self, center, normal):\n        \"\"\"Add a plane to the viewer\n\n        Parameters\n        ----------\n        center : array_like\n            The origin of the plane\n        normal : array_like\n            The normal of the plane\n        \"\"\"\n\n        plane = vtk.vtkPlaneSource()\n        plane.SetCenter(center)\n        plane.SetNormal(normal)\n\n        mapper = vtk.vtkPolyDataMapper()\n        mapper.SetInputData(plane.GetOutput())",
  "def __init__(self):\n\n        # Building renderer\n        self.renderer = vtk.vtkRenderer()\n        self.renderer.SetBackground(0.7706, 0.8165, 1.0)\n\n        # Building render window\n        self.render_window = vtk.vtkRenderWindow()\n        self.render_window.SetSize(1024, 768)\n        self.render_window.SetWindowName(\"Mesh viewer\")\n        self.render_window.AddRenderer(self.renderer)\n\n        # Building interactor\n        self.render_window_interactor = vtk.vtkRenderWindowInteractor()\n        self.render_window_interactor.SetRenderWindow(self.render_window)\n        self.render_window_interactor.GetInteractorStyle().SetCurrentStyleToTrackballCamera()\n        self.render_window_interactor.AddObserver('KeyPressEvent', self.on_key_press, 0.0)\n\n        # Building axes view\n        axes = vtk.vtkAxesActor()\n        widget = vtk.vtkOrientationMarkerWidget()\n        widget.SetOrientationMarker(axes)\n        self.widget = widget\n\n        self.widget.SetInteractor(self.render_window_interactor)\n        self.widget.SetEnabled(1)\n        self.widget.InteractiveOn()\n\n        # Building command annotations\n        command_text = (\"left mouse : rotate\\n\"\n                        \"right mouse : zoom\\n\"\n                        \"middle mouse : pan\\n\"\n                        \"ctrl+left mouse : spin\\n\"\n                        \"n : (un)show normals\\n\"\n                        \"b : (un)show axes box\\n\"\n                        \"f : focus on the mouse cursor\\n\"\n                        \"r : reset view\\n\"\n                        \"s : surface representation\\n\"\n                        \"w : wire representation\\n\"\n                        \"h : (un)show Oxy plane\\n\"\n                        \"x : save\\n\"\n                        \"c : screenshot\\n\"\n                        \"q : quit\")\n\n        corner_annotation = vtk.vtkCornerAnnotation()\n        corner_annotation.SetLinearFontScaleFactor(2)\n        corner_annotation.SetNonlinearFontScaleFactor(1)\n        corner_annotation.SetMaximumFontSize(20)\n        corner_annotation.SetText(3, command_text)\n        corner_annotation.GetTextProperty().SetColor(0., 0., 0.)\n        self.renderer.AddViewProp(corner_annotation)\n\n        copyright_text = (f\"Capytaine \u2014 Copyright 2017-{__year__} University College Dublin\\n\"\n                          f\"based on Meshmagick Viewer \u2014 Copyright 2014-{__year__}, \u00c9cole Centrale de Nantes\")\n\n        copyright_annotation = vtk.vtkCornerAnnotation()\n        copyright_annotation.SetLinearFontScaleFactor(0.5)\n        copyright_annotation.SetNonlinearFontScaleFactor(1)\n        copyright_annotation.SetMaximumFontSize(12)\n        copyright_annotation.SetText(1, copyright_text)\n        copyright_annotation.GetTextProperty().SetColor(0., 0., 0.)\n        self.renderer.AddViewProp(copyright_annotation)\n\n        self.polydatas = []\n        self.normals = []\n        self.axes = []\n        self.oxy_plane = None",
  "def add_polydata(self, polydata, color=(1, 1, 0), representation='surface'):\n        \"\"\"Add a polydata object to the viewer\n\n        Parameters\n        ----------\n        polydata : vtkPolyData\n            the object to be added\n        color : array_like, optional\n            the color of the object. Default is yellow (1, 1, 0)\n        representation : str\n            the representation mode of the object ('surface' or 'wireframe'). Default is 'surface'.\n        \"\"\"\n\n        assert isinstance(polydata, vtk.vtkPolyData)\n        assert representation in ('surface', 'wireframe')\n\n        self.polydatas.append(polydata)\n\n        # Building mapper\n        mapper = vtk.vtkPolyDataMapper()\n        mapper.SetInputData(polydata)\n\n        # Building actor\n        actor = vtk.vtkActor()\n        actor.SetMapper(mapper)\n\n        # Properties setting\n        actor.GetProperty().SetColor(color)\n        actor.GetProperty().EdgeVisibilityOn()\n        actor.GetProperty().SetEdgeColor(0, 0, 0)\n        actor.GetProperty().SetLineWidth(1)\n        actor.GetProperty().SetPointSize(10)\n        if representation == 'wireframe':\n            actor.GetProperty().SetRepresentationToWireframe()\n\n        self.renderer.AddActor(actor)\n        self.renderer.Modified()",
  "def add_mesh(self, mesh, color=None, representation='surface'):\n        vtk_polydata = compute_vtk_polydata(mesh)\n        if color is None:\n            color = next(COLORS)\n        self.add_polydata(vtk_polydata, color=color, representation=representation)",
  "def add_oxy_plane(self):\n        \"\"\"Displays the Oxy plane\"\"\"\n\n        one_mesh_in_the_viewer = self.polydatas[0]\n\n        plane = vtk.vtkPlaneSource()\n        (xmin, xmax, ymin, ymax, _, _) = one_mesh_in_the_viewer.GetBounds()\n\n        dx = max(0.1 * (xmax - xmin), 0.1)\n        dy = max(0.1 * (ymax - ymin), 0.1)\n\n        plane.SetOrigin(xmin - dx, ymax + dy, 0)\n        plane.SetPoint1(xmin - dx, ymin - dy, 0)\n        plane.SetPoint2(xmax + dx, ymax + dy, 0)\n        plane.Update()\n        polydata = plane.GetOutput()\n\n        mapper = vtk.vtkPolyDataMapper()\n        mapper.SetInputData(polydata)\n\n        actor = vtk.vtkActor()\n        actor.SetMapper(mapper)\n\n        color = [0., 102. / 255, 204. / 255]\n        actor.GetProperty().SetColor(color)\n        actor.GetProperty().SetEdgeColor(0, 0, 0)\n        actor.GetProperty().SetLineWidth(1)\n\n        self.renderer.AddActor(actor)\n        self.renderer.Modified()\n        self.oxy_plane = actor",
  "def show_normals(self):\n        \"\"\"Shows the normals of the current objects\"\"\"\n\n        for polydata in self.polydatas:\n            normals = vtk.vtkPolyDataNormals()\n            normals.ConsistencyOff()\n            normals.ComputeCellNormalsOn()\n            normals.SetInputData(polydata)\n            normals.Update()\n\n            normals_at_centers = vtk.vtkCellCenters()\n            normals_at_centers.SetInputConnection(normals.GetOutputPort())\n\n            arrows = vtk.vtkArrowSource()\n            arrows.SetTipResolution(16)\n            arrows.SetTipLength(0.5)\n            arrows.SetTipRadius(0.1)\n\n            glyph = vtk.vtkGlyph3D()\n            glyph.SetSourceConnection(arrows.GetOutputPort())\n            glyph.SetInputConnection(normals_at_centers.GetOutputPort())\n            glyph.SetVectorModeToUseNormal()\n            glyph.SetScaleModeToScaleByVector()\n            glyph.SetScaleFactor(1)  # FIXME: may be too big ...\n            # glyph.SetVectorModeToUseNormal()\n            # glyph.SetVectorModeToUseVector()\n            # glyph.SetScaleModeToDataScalingOff()\n            glyph.Update()\n\n            glyph_mapper = vtk.vtkPolyDataMapper()\n            glyph_mapper.SetInputConnection(glyph.GetOutputPort())\n\n            glyph_actor = vtk.vtkActor()\n            glyph_actor.SetMapper(glyph_mapper)\n\n            self.renderer.AddActor(glyph_actor)\n            self.normals.append(glyph_actor)",
  "def show_axes(self):\n        \"\"\"Shows the axes around the main object\"\"\"\n\n        tprop = vtk.vtkTextProperty()\n        tprop.SetColor(0., 0., 0.)\n        tprop.ShadowOn()\n\n        axes = vtk.vtkCubeAxesActor2D()\n        axes.SetInputData(self.polydatas[0])\n\n        axes.SetCamera(self.renderer.GetActiveCamera())\n        axes.SetLabelFormat(\"%6.4g\")\n        axes.SetFlyModeToOuterEdges()\n        axes.SetFontFactor(0.8)\n        axes.SetAxisTitleTextProperty(tprop)\n        axes.SetAxisLabelTextProperty(tprop)\n        # axes.DrawGridLinesOn()\n\n        self.renderer.AddViewProp(axes)\n        self.axes.append(axes)",
  "def save(self):\n        \"\"\"Saves the main object in a 'mmviewer_save.vtp' vtp file is the current folder\"\"\"\n\n        writer = vtk.vtkXMLPolyDataWriter()\n        writer.SetDataModeToAscii()\n        writer.SetFileName('mmviewer_save.vtp')\n\n        for polydata in self.polydatas:\n            writer.SetInputData(polydata)\n        writer.Write()\n\n        print(\"File 'mmviewer_save.vtp' written in %s\" % getcwd())\n        return",
  "def screenshot(self):\n        \"\"\"Saves a screenshot of the current window in a file screenshot.png\"\"\"\n        w2if = vtk.vtkWindowToImageFilter()\n        w2if.SetInput(self.render_window)\n        w2if.Update()\n\n        writer = vtk.vtkPNGWriter()\n        writer.SetFileName(\"screenshot.png\")\n        writer.SetInputData(w2if.GetOutput())\n        writer.Write()\n\n        print(\"File 'screenshot.png' written in %s\" % getcwd())\n        return",
  "def show(self):\n        \"\"\"Show the viewer\"\"\"\n        self.renderer.ResetCamera()\n        self.render_window.Render()\n        self.render_window_interactor.Start()",
  "def on_key_press(self, obj, event):\n        \"\"\"Event trig at keystroke\"\"\"\n        key = obj.GetKeySym()\n\n        if key == 'n':\n            if self.normals:\n                # self.normals = False\n                for actor in self.normals:\n                    self.renderer.RemoveActor(actor)\n                self.renderer.Render()\n                self.normals = []\n            else:\n                self.show_normals()\n                self.renderer.Render()\n\n        elif key == 'b':\n            if self.axes:\n                for axis in self.axes:\n                    self.renderer.RemoveActor(axis)\n                self.axes = []\n            else:\n                self.show_axes()\n\n        elif key == 'x':\n            self.save()\n\n        elif key == 'c':\n            self.screenshot()\n\n        elif key == 'h':\n            if self.oxy_plane:\n                self.renderer.RemoveActor(self.oxy_plane)\n                self.oxy_plane = None\n            else:\n                self.add_oxy_plane()\n\n        elif key == 'e' or key == 'q':\n            self.render_window_interactor.GetRenderWindow().Finalize()\n            self.render_window_interactor.TerminateApp()",
  "def finalize(self):\n        \"\"\"Cleanly close the viewer\"\"\"\n        del self.render_window\n        del self.render_window_interactor",
  "def add_point(self, pos, color=(0, 0, 0)):\n        \"\"\"Add a point to the viewer\n\n        Parameters\n        ----------\n        pos : array_like\n            The point's position\n        color : array_like, optional\n            The RGB color required for the point. Default is (0, 0, 0) corresponding to black.\n\n        Returns\n        -------\n        vtkPolyData\n        \"\"\"\n\n        assert len(pos) == 3\n\n        p = vtk.vtkPoints()\n        v = vtk.vtkCellArray()\n\n        i = p.InsertNextPoint(pos)\n        v.InsertNextCell(1)\n        v.InsertCellPoint(i)\n\n        pd = vtk.vtkPolyData()\n        pd.SetPoints(p)\n        pd.SetVerts(v)\n\n        self.add_polydata(pd, color=color)\n\n        return pd",
  "def add_line(self, p0, p1, color=(0, 0, 0)):\n        \"\"\"Add a line to the viewer\n\n        Parameters\n        ----------\n        p0 : array_like\n            position of one end point of the line\n        p1 : array_like\n            position of a second end point of the line\n        color : array_like, optional\n            RGB color of the line. Default is black (0, 0, 0)\n\n        Returns\n        -------\n        vtkPolyData\n        \"\"\"\n\n        assert len(p0) == 3 and len(p1) == 3\n\n        points = vtk.vtkPoints()\n        points.InsertNextPoint(p0)\n        points.InsertNextPoint(p1)\n\n        line = vtk.vtkLine()\n        line.GetPointIds().SetId(0, 0)\n        line.GetPointIds().SetId(1, 1)\n\n        lines = vtk.vtkCellArray()\n        lines.InsertNextCell(line)\n\n        lines_pd = vtk.vtkPolyData()\n        lines_pd.SetPoints(points)\n        lines_pd.SetLines(lines)\n\n        self.add_polydata(lines_pd, color=color)\n\n        return lines_pd",
  "def add_vector(self, point, value, scale=1, color=(0, 0, 0)):\n        \"\"\"Add a vector to the viewer\n\n        Parameters\n        ----------\n        point : array_like\n            starting point position of the vector\n        value : float\n            the magnitude of the vector\n        scale : float, optional\n            the scaling to apply to the vector for better visualization. Default is 1.\n        color : array_like\n            The color of the vector. Default is black (0, 0, 0)\n        \"\"\"\n\n        points = vtk.vtkPoints()\n        idx = points.InsertNextPoint(point)\n\n        vert = vtk.vtkCellArray()\n        vert.InsertNextCell(1)\n        vert.InsertCellPoint(idx)\n        pd_point = vtk.vtkPolyData()\n        pd_point.SetPoints(points)\n        pd_point.SetVerts(vert)\n\n        arrow = vtk.vtkArrowSource()\n        arrow.SetTipResolution(16)\n        arrow.SetTipLength(0.1)\n        arrow.SetTipRadius(0.02)\n        arrow.SetShaftRadius(0.005)\n\n        vec = vtk.vtkFloatArray()\n        vec.SetNumberOfComponents(3)\n        v0, v1, v2 = value / scale\n        vec.InsertTuple3(idx, v0, v1, v2)\n        pd_point.GetPointData().SetVectors(vec)\n\n        g_glyph = vtk.vtkGlyph3D()\n        # g_glyph.SetScaleModeToDataScalingOff()\n        g_glyph.SetVectorModeToUseVector()\n        g_glyph.SetInputData(pd_point)\n        g_glyph.SetSourceConnection(arrow.GetOutputPort())\n        g_glyph.SetScaleModeToScaleByVector()\n        # g_glyph.SetScaleFactor(10)\n        g_glyph.ScalingOn()\n        g_glyph.Update()\n\n        g_glyph_mapper = vtk.vtkPolyDataMapper()\n        g_glyph_mapper.SetInputConnection(g_glyph.GetOutputPort())\n\n        g_glyph_actor = vtk.vtkActor()\n        g_glyph_actor.SetMapper(g_glyph_mapper)\n        g_glyph_actor.GetProperty().SetColor(color)\n\n        self.renderer.AddActor(g_glyph_actor)",
  "def add_plane(self, center, normal):\n        \"\"\"Add a plane to the viewer\n\n        Parameters\n        ----------\n        center : array_like\n            The origin of the plane\n        normal : array_like\n            The normal of the plane\n        \"\"\"\n\n        plane = vtk.vtkPlaneSource()\n        plane.SetCenter(center)\n        plane.SetNormal(normal)\n\n        mapper = vtk.vtkPolyDataMapper()\n        mapper.SetInputData(plane.GetOutput())",
  "def compute_vtk_polydata(mesh: Union[Mesh, CollectionOfMeshes]):\n    \"\"\"Transform a mesh into vtkPolydata.\"\"\"\n\n    # Create a vtkPoints object and store the points in it\n    points = vtk.vtkPoints()\n    for point in mesh.vertices:\n        points.InsertNextPoint(point)\n\n    # Create a vtkCellArray to store faces\n    faces = vtk.vtkCellArray()\n    for face_ids in mesh.faces:\n        if face_ids[0] == face_ids[-1]:\n            # Triangle\n            curface = face_ids[:3]\n            vtk_face = vtk.vtkTriangle()\n        else:\n            # Quadrangle\n            curface = face_ids[:4]\n            vtk_face = vtk.vtkQuad()\n\n        for idx, id in enumerate(curface):\n            vtk_face.GetPointIds().SetId(idx, id)\n\n        faces.InsertNextCell(vtk_face)\n\n    vtk_polydata = vtk.vtkPolyData()\n    vtk_polydata.SetPoints(points)\n    vtk_polydata.SetPolys(faces)\n\n    return vtk_polydata",
  "def compute_node_data(mesh: Union[Mesh, CollectionOfMeshes],\n                      face_data):\n    \"\"\"Transform data defined at the center of the faces to data defined at the nodes of the mesh\n    by a simple averaging of the values of the neighboring faces.\n\n    Parameters\n    ----------\n    mesh: Mesh or CollectionOfMeshes\n        the mesh on which the face face_data are defined\n    face_data: numpy array of shape (mesh.nb_faces, ...)\n        the data defined on the center of the faces of the mesh\n\n    Returns\n    -------\n    node_data: numpy array of shape (mesh.nb_vertices, ...)\n        the same data averaged on the nodes\n    \"\"\"\n\n    import numpy as np\n\n    mesh = mesh.merged()\n    assert face_data.shape[0] == mesh.nb_faces\n\n    # Initialize output array\n    node_data_shape = (mesh.vertices.shape[0], ) + face_data.shape[1:]\n    node_data = np.zeros(node_data_shape, dtype=complex)\n\n    # Keep track of the number of faces near each vertex\n    faces_near_nodes_shape = (mesh.vertices.shape[0], ) + (1, ) * len(face_data.shape[1:])\n    nb_faces_near_nodes = np.zeros(faces_near_nodes_shape, dtype=np.int8)\n\n    for i, vertices in enumerate(mesh.faces):\n        for vertex in vertices:\n            nb_faces_near_nodes[vertex] += 1\n            node_data[vertex, ...] += face_data[i, ...]\n\n    node_data /= nb_faces_near_nodes\n    return node_data",
  "class FloatingBody(ClippableMixin, Abstract3DObject):\n    \"\"\"A floating body described as a mesh and some degrees of freedom.\n\n    The mesh structure is stored as a Mesh from capytaine.mesh.mesh or a\n    CollectionOfMeshes from capytaine.mesh.meshes_collection.\n\n    The degrees of freedom (dofs) are stored as a dict associating a name to\n    a complex-valued array of shape (nb_faces, 3). To each face of the body\n    (as indexed in the mesh) corresponds a complex-valued 3d vector, which\n    defines the displacement of the center of the face in frequency domain.\n\n    Parameters\n    ----------\n    mesh : Mesh or CollectionOfMeshes, optional\n        the mesh describing the geometry of the floating body.\n        If none is given, a empty one is created.\n    dofs : dict, optional\n        the degrees of freedom of the body.\n        If none is given, a empty dictionary is initialized.\n    mass : float or None, optional\n        the mass of the body in kilograms.\n        Required only for some hydrostatics computation.\n        If None, the mass is implicitly assumed to be the mass of displaced water.\n    center_of_mass: 3-element array, optional\n        the position of the center of mass.\n        Required only for some hydrostatics computation.\n    name : str, optional\n        a name for the body.\n        If none is given, the one of the mesh is used.\n    \"\"\"\n\n    def __init__(self, mesh=None, dofs=None, mass=None, center_of_mass=None, name=None):\n        if mesh is None:\n            self.mesh = Mesh(name=\"dummy_mesh\")\n\n        elif meshio is not None and isinstance(mesh, meshio._mesh.Mesh):\n            from capytaine.io.meshio import load_from_meshio\n            self.mesh = load_from_meshio(mesh)\n\n        elif isinstance(mesh, Mesh) or isinstance(mesh, CollectionOfMeshes):\n            self.mesh = mesh\n\n        else:\n            raise TypeError(\"Unrecognized `mesh` object passed to the FloatingBody constructor.\")\n\n        if name is None and mesh is None:\n            self.name = \"dummy_body\"\n        elif name is None:\n            self.name = self.mesh.name\n        else:\n            self.name = name\n\n        self.mass = mass\n        if center_of_mass is not None:\n            self.center_of_mass = np.asarray(center_of_mass, dtype=float)\n        else:\n            self.center_of_mass = None\n\n        if self.mesh.nb_vertices == 0 or self.mesh.nb_faces == 0:\n            LOG.warning(f\"New floating body (with empty mesh!): {self.name}.\")\n        else:\n            self.mesh.heal_mesh()\n            LOG.info(f\"New floating body: {self.name}.\")\n\n        if dofs is None:\n            self.dofs = {}\n        elif isinstance(dofs, RigidBodyDofsPlaceholder):\n            if dofs.rotation_center is not None:\n                self.rotation_center = np.asarray(dofs.rotation_center, dtype=float)\n            self.dofs = {}\n            self.add_all_rigid_body_dofs()\n        else:\n            self.dofs = dofs\n\n    @staticmethod\n    def from_meshio(mesh, name=None) -> 'FloatingBody':\n        \"\"\"Create a FloatingBody from a meshio mesh object.\"\"\"\n        from capytaine.io.meshio import load_from_meshio\n        return FloatingBody(mesh=load_from_meshio(mesh, name), name=name)\n\n    @staticmethod\n    def from_file(filename: str, file_format=None, name=None) -> 'FloatingBody':\n        \"\"\"Create a FloatingBody from a mesh file using meshmagick.\"\"\"\n        from capytaine.io.mesh_loaders import load_mesh\n        if name is None: name = filename\n        mesh = load_mesh(filename, file_format, name=f\"{name}_mesh\")\n        return FloatingBody(mesh, name=name)\n\n    def __lt__(self, other: 'FloatingBody') -> bool:\n        \"\"\"Arbitrary order. The point is to sort together the problems involving the same body.\"\"\"\n        return self.name < other.name\n\n    ##########\n    #  Dofs  #\n    ##########\n\n    @property\n    def nb_dofs(self) -> int:\n        \"\"\"Number of degrees of freedom.\"\"\"\n        return len(self.dofs)\n\n    def add_translation_dof(self, direction=None, name=None, amplitude=1.0) -> None:\n        \"\"\"Add a new translation dof (in place).\n        If no direction is given, the code tries to infer it from the name.\n\n        Parameters\n        ----------\n        direction : array of shape (3,), optional\n            the direction of the translation\n        name : str, optional\n            a name for the degree of freedom\n        amplitude : float, optional\n            amplitude of the dof (default: 1.0 m/s)\n        \"\"\"\n        if direction is None:\n            if name is not None and name.lower() in TRANSLATION_DOFS_DIRECTIONS:\n                direction = TRANSLATION_DOFS_DIRECTIONS[name.lower()]\n            else:\n                raise ValueError(\"A direction needs to be specified for the dof.\")\n\n        if name is None:\n            name = f\"dof_{self.nb_dofs}_translation\"\n\n        direction = np.asarray(direction)\n        assert direction.shape == (3,)\n\n        motion = np.empty((self.mesh.nb_faces, 3))\n        motion[:, :] = direction\n        self.dofs[name] = amplitude * motion\n\n    def add_rotation_dof(self, axis=None, name=None, amplitude=1.0) -> None:\n        \"\"\"Add a new rotation dof (in place).\n        If no axis is given, the code tries to infer it from the name.\n\n        Parameters\n        ----------\n        axis: Axis, optional\n            the axis of the rotation\n        name : str, optional\n            a name for the degree of freedom\n        amplitude : float, optional\n            amplitude of the dof (default: 1.0)\n        \"\"\"\n        if axis is None:\n            if name is not None and name.lower() in ROTATION_DOFS_AXIS:\n                axis_direction = ROTATION_DOFS_AXIS[name.lower()]\n                for point_attr in ('rotation_center', 'center_of_mass', 'geometric_center'):\n                    if hasattr(self, point_attr) and getattr(self, point_attr) is not None:\n                        axis_point = getattr(self, point_attr)\n                        LOG.info(f\"The rotation dof {name} has been initialized around the point: \"\n                                 f\"{self.name}.{point_attr} = {getattr(self, point_attr)}\")\n                        break\n                else:\n                    axis_point = np.array([0, 0, 0])\n                    LOG.warning(f\"The rotation dof {name} has been initialized \"\n                                f\"around the origin of the domain (0, 0, 0).\")\n            else:\n                raise ValueError(\"A direction needs to be specified for the dof.\")\n        else:\n            axis_point = axis.point\n            axis_direction = axis.vector\n\n        if name is None:\n            name = f\"dof_{self.nb_dofs}_rotation\"\n\n        if self.mesh.nb_faces == 0:\n            self.dofs[name] = np.empty((self.mesh.nb_faces, 3))\n        else:\n            motion = np.cross(axis_point - self.mesh.faces_centers, axis_direction)\n            self.dofs[name] = amplitude * motion\n\n    def add_all_rigid_body_dofs(self) -> None:\n        \"\"\"Add the six degrees of freedom of rigid bodies (in place).\"\"\"\n        self.add_translation_dof(name=\"Surge\")\n        self.add_translation_dof(name=\"Sway\")\n        self.add_translation_dof(name=\"Heave\")\n        self.add_rotation_dof(name=\"Roll\")\n        self.add_rotation_dof(name=\"Pitch\")\n        self.add_rotation_dof(name=\"Yaw\")\n\n    def integrate_pressure(self, pressure):\n        forces = {}\n        for dof_name in self.dofs:\n            # Scalar product on each face:\n            normal_dof_amplitude_on_face = - np.sum(self.dofs[dof_name] * self.mesh.faces_normals, axis=1)\n            # The minus sign in the above line is because we want the force of the fluid on the body and not the force of the body on the fluid.\n            # Sum over all faces:\n            forces[dof_name] = np.sum(pressure * normal_dof_amplitude_on_face * self.mesh.faces_areas)\n        return forces\n\n    @inplace_transformation\n    def keep_only_dofs(self, dofs):\n        for dof in list(self.dofs.keys()):\n            if dof not in dofs:\n                del self.dofs[dof]\n\n        if hasattr(self, 'inertia_matrix'):\n            self.inertia_matrix = self.inertia_matrix.sel(radiating_dof=dofs, influenced_dof=dofs)\n        if hasattr(self, 'hydrostatic_stiffness'):\n            self.hydrostatic_stiffness = self.hydrostatic_stiffness.sel(radiating_dof=dofs, influenced_dof=dofs)\n\n        return self\n\n    def add_dofs_labels_to_vector(self, vector):\n        \"\"\"Helper function turning a bare vector into a vector labelled by the name of the dofs of the body,\n        to be used for instance for the computation of RAO.\"\"\"\n        return xr.DataArray(data=np.asarray(vector), dims=['influenced_dof'],\n                            coords={'influenced_dof': list(self.dofs)},\n                            )\n\n    def add_dofs_labels_to_matrix(self, matrix):\n        \"\"\"Helper function turning a bare matrix into a matrix labelled by the name of the dofs of the body,\n        to be used for instance for the computation of RAO.\"\"\"\n        return xr.DataArray(data=np.asarray(matrix), dims=['influenced_dof', 'radiating_dof'],\n                            coords={'influenced_dof': list(self.dofs), 'radiating_dof': list(self.dofs)},\n                            )\n\n    ###################\n    # Hydrostatics #\n    ###################\n\n    def surface_integral(self, data, **kwargs):\n        \"\"\"Returns integral of given data along wet surface area.\"\"\"\n        return self.mesh.surface_integral(data, **kwargs)\n\n    def waterplane_integral(self, data, **kwargs):\n        \"\"\"Returns integral of given data along water plane area.\"\"\"\n        return self.mesh.waterplane_integral(data, **kwargs)\n\n    @property\n    def wet_surface_area(self):\n        \"\"\"Returns wet surface area.\"\"\"\n        return self.mesh.wet_surface_area\n\n    @property\n    def volumes(self):\n        \"\"\"Returns volumes using x, y, z components of the FloatingBody.\"\"\"\n        return self.mesh.volumes\n\n    @property\n    def volume(self):\n        \"\"\"Returns volume of the FloatingBody.\"\"\"\n        return self.mesh.volume\n\n    def disp_mass(self, *, rho=1000):\n        return self.mesh.disp_mass(rho=rho)\n\n    @property\n    def center_of_buoyancy(self):\n        \"\"\"Returns center of buoyancy of the FloatingBody.\"\"\"\n        return self.mesh.center_of_buoyancy\n\n    @property\n    def waterplane_area(self):\n        \"\"\"Returns water plane area of the FloatingBody.\"\"\"\n        return self.mesh.waterplane_area\n\n    @property\n    def waterplane_center(self):\n        \"\"\"Returns water plane center of the FloatingBody.\n\n        Note: Returns None if the FloatingBody is full submerged.\n        \"\"\"\n        return self.mesh.waterplane_center\n\n    @property\n    def transversal_metacentric_radius(self):\n        \"\"\"Returns transversal metacentric radius of the mesh.\"\"\"\n        inertia_moment = -self.waterplane_integral(self.mesh.faces_centers[:,1]**2)\n        return inertia_moment / self.volume\n\n    @property\n    def longitudinal_metacentric_radius(self):\n        \"\"\"Returns longitudinal metacentric radius of the mesh.\"\"\"\n        inertia_moment = -self.waterplane_integral(self.mesh.faces_centers[:,0]**2)\n        return inertia_moment / self.volume\n\n    @property\n    def transversal_metacentric_height(self):\n        \"\"\"Returns transversal metacentric height of the mesh.\"\"\"\n        gb = self.center_of_mass - self.center_of_buoyancy\n        return self.transversal_metacentric_radius - gb[2]\n\n    @property\n    def longitudinal_metacentric_height(self):\n        \"\"\"Returns longitudinal metacentric height of the mesh.\"\"\"\n        gb = self.center_of_mass - self.center_of_buoyancy\n        return self.longitudinal_metacentric_radius - gb[2]\n\n    def dof_normals(self, dof):\n        \"\"\"Returns dot product of the surface face normals and DOF\"\"\"\n        return np.sum(self.mesh.faces_normals * dof, axis=1)\n\n    def _infer_rotation_center(self):\n        \"\"\"Hacky way to infer the point around which the rotation dofs are defined.\n        (Assuming all three rotation dofs are defined around the same point).\n        In the future, should be replaced by something more robust.\n        \"\"\"\n        if hasattr(self, \"rotation_center\"):\n            return np.asarray(self.rotation_center)\n\n        else:\n            try:\n                xc1 = self.dofs[\"Pitch\"][:, 2] + self.mesh.faces_centers[:, 0]\n                xc2 = -self.dofs[\"Yaw\"][:, 1] + self.mesh.faces_centers[:, 0]\n                yc1 = self.dofs[\"Yaw\"][:, 0] + self.mesh.faces_centers[:, 1]\n                yc2 = -self.dofs[\"Roll\"][:, 2] + self.mesh.faces_centers[:, 1]\n                zc1 = -self.dofs[\"Pitch\"][:, 0] + self.mesh.faces_centers[:, 2]\n                zc2 = self.dofs[\"Roll\"][:, 1] + self.mesh.faces_centers[:, 2]\n\n                # All items should be identical in a given vector\n                assert np.isclose(xc1, xc1[0]).all()\n                assert np.isclose(yc1, yc1[0]).all()\n                assert np.isclose(zc1, zc1[0]).all()\n\n                # Both vector should be identical\n                assert np.allclose(xc1, xc2)\n                assert np.allclose(yc1, yc2)\n                assert np.allclose(zc1, zc2)\n\n                return np.array([xc1[0], yc1[0], zc1[0]])\n\n            except Exception as e:\n                raise ValueError(\n                        f\"Failed to infer the rotation center of {self.name} to compute rigid body hydrostatics.\\n\"\n                        f\"Possible fix: add a `rotation_center` attibute to {self.name}.\\n\"\n                        \"Note that rigid body hydrostatic methods currently assume that the three rotation dofs have the same rotation center.\"\n                        ) from e\n\n    def each_hydrostatic_stiffness(self, influenced_dof_name, radiating_dof_name, *,\n                                         influenced_dof_div=0.0, rho=1000.0, g=9.81):\n        r\"\"\"\n        Return the hydrostatic stiffness for a pair of DOFs.\n\n        :math:`C_{ij} = \\rho g\\iint_S (\\hat{n} \\cdot V_j) (w_i + z D_i) dS`\n\n        where :math:`\\hat{n}` is surface normal,\n\n        :math:`V_i = u_i \\hat{n}_x + v_i \\hat{n}_y + w_i \\hat{n}_z` is DOF vector and\n\n        :math:`D_i = \\nabla \\cdot V_i` is the divergence of the DOF.\n\n        Parameters\n        ----------\n        influenced_dof_name : str\n            Name of influenced DOF vector of the FloatingBody\n        radiating_dof_name: str\n            Name of radiating DOF vector of the FloatingBody\n        influenced_dof_div: np.ndarray (Face_count), optional\n            Influenced DOF divergence of the FloatingBody, by default 0.0.\n        rho: float, optional\n            water density, by default 1000.0\n        g: float, optional\n            Gravity acceleration, by default 9.81\n\n        Returns\n        -------\n        hs_ij: xarray.variable\n            hydrostatic_stiffness of ith DOF and jth DOF.\n\n        Note\n        ----\n            This function computes the hydrostatic stiffness assuming :math:`D_{i} = 0`.\n            If :math:`D_i \\neq 0`, input the divergence interpolated to face centers.\n\n            General integral equations are used for the rigid body modes and\n            Neumann (1994) method is used for flexible modes.\n\n        References\n        ----------\n            Newman, John Nicholas. \"Wave effects on deformable bodies.\"Applied ocean\n            research\" 16.1 (1994): 47-59.\n            http://resolver.tudelft.nl/uuid:0adff84c-43c7-43aa-8cd8-d4c44240bed8\n\n        \"\"\"\n        # Newman (1994) formula is not 'complete' as recovering the rigid body\n        # terms is not possible. https://doi.org/10.1115/1.3058702.\n\n        # Alternative is to use the general equation of hydrostatic and\n        # restoring coefficient for rigid modes and use Newman equation for elastic\n        # modes.\n\n        rigid_dof_names = (\"Surge\", \"Sway\", \"Heave\", \"Roll\", \"Pitch\", \"Yaw\")\n        dof_pair = (influenced_dof_name, radiating_dof_name)\n\n        if set(dof_pair).issubset(set(rigid_dof_names)):\n            if self.center_of_mass is None:\n                raise ValueError(f\"Trying to compute rigid-body hydrostatic stiffness for {self.name}, but no center of mass has been defined.\\n\"\n                                 f\"Suggested solution: define a `center_of_mass` attribute for the FloatingBody {self.name}.\")\n            mass = self.disp_mass(rho=rho) if self.mass is None else self.mass\n            xc, yc, zc = self._infer_rotation_center()\n\n            if dof_pair == (\"Heave\", \"Heave\"):\n                norm_hs_stiff = self.waterplane_area\n            elif dof_pair in [(\"Heave\", \"Roll\"), (\"Roll\", \"Heave\")]:\n                norm_hs_stiff = -self.waterplane_integral(self.mesh.faces_centers[:,1] - yc)\n            elif dof_pair in [(\"Heave\", \"Pitch\"), (\"Pitch\", \"Heave\")]:\n                norm_hs_stiff = self.waterplane_integral(self.mesh.faces_centers[:,0] - xc)\n            elif dof_pair == (\"Roll\", \"Roll\"):\n                norm_hs_stiff = (\n                        -self.waterplane_integral((self.mesh.faces_centers[:,1] - yc)**2)\n                        + self.volume*(self.center_of_buoyancy[2] - zc) - mass/rho*(self.center_of_mass[2] - zc)\n                )\n            elif dof_pair in [(\"Roll\", \"Pitch\"), (\"Pitch\", \"Roll\")]:\n                norm_hs_stiff = self.waterplane_integral((self.mesh.faces_centers[:,0] - xc)\n                                                          * (self.mesh.faces_centers[:,1] - yc))\n            elif dof_pair == (\"Roll\", \"Yaw\"):\n                norm_hs_stiff = - self.volume*(self.center_of_buoyancy[0] - xc) + mass/rho*(self.center_of_mass[0] - xc)\n            elif dof_pair == (\"Pitch\", \"Pitch\"):\n                norm_hs_stiff = (\n                        -self.waterplane_integral((self.mesh.faces_centers[:,0] - xc)**2)\n                        + self.volume*(self.center_of_buoyancy[2] - zc) - mass/rho*(self.center_of_mass[2] - zc)\n                        )\n            elif dof_pair == (\"Pitch\", \"Yaw\"):\n                norm_hs_stiff = - self.volume*(self.center_of_buoyancy[1] - yc) + mass/rho*(self.center_of_mass[1] - yc)\n            else:\n                norm_hs_stiff = 0.0\n        else:\n            if self.mass is not None and np.isclose(self.mass, self.disp_mass(rho), rtol=1e-4):\n                raise NotImplementedError(\n                        f\"Trying to compute the hydrostatic stiffness for dofs {radiating_dof_name} and {influenced_dof_name}\"\n                        f\"of body {self.name}, which is not neutrally buoyant (mass={body.mass}, disp_mass={body.disp_mass(rho)}.\\n\"\n                        f\"This case has not been implemented in Capytaine. You need either a single rigid body or a neutrally buoyant body.\"\n                        )\n\n            # Newman (1994) formula for flexible DOFs\n            influenced_dof = np.array(self.dofs[influenced_dof_name])\n            radiating_dof = np.array(self.dofs[radiating_dof_name])\n            influenced_dof_div_array = np.array(influenced_dof_div)\n\n            radiating_dof_normal = self.dof_normals(radiating_dof)\n            z_influenced_dof_div = influenced_dof[:,2] + self.mesh.faces_centers[:,2] * influenced_dof_div_array\n            norm_hs_stiff = self.surface_integral( -radiating_dof_normal * z_influenced_dof_div)\n\n        hs_stiff = rho * g * norm_hs_stiff\n\n        return xr.DataArray([[hs_stiff]],\n                            dims=['influenced_dof', 'radiating_dof'],\n                            coords={'influenced_dof': [influenced_dof_name],\n                            'radiating_dof': [radiating_dof_name]},\n                            name=\"hydrostatic_stiffness\"\n                            )\n\n    def compute_hydrostatic_stiffness(self, *, divergence=None, rho=1000.0, g=9.81):\n        r\"\"\"\n        Compute hydrostatic stiffness matrix for all DOFs of the body.\n\n        :math:`C_{ij} = \\rho g\\iint_S (\\hat{n} \\cdot V_j) (w_i + z D_i) dS`\n\n        where :math:`\\hat{n}` is surface normal,\n\n        :math:`V_i = u_i \\hat{n}_x + v_i \\hat{n}_y + w_i \\hat{n}_z` is DOF vector and\n\n        :math:`D_i = \\nabla \\cdot V_i` is the divergence of the DOF.\n\n        Parameters\n        ----------\n        divergence : dict mapping a dof name to an array of shape (nb_faces) or\n                        xarray.DataArray of shape (nb_dofs \u00d7 nb_faces), optional\n            Divergence of the DOFs, by default None\n        rho : float, optional\n            Water density, by default 1000.0\n        g: float, optional\n            Gravity acceleration, by default 9.81\n\n        Returns\n        -------\n        xr.DataArray\n            Matrix of hydrostatic stiffness\n\n        Note\n        ----\n            This function computes the hydrostatic stiffness assuming :math:`D_{i} = 0`.\n            If :math:`D_i \\neq 0`, input the divergence interpolated to face centers.\n\n            General integral equations are used for the rigid body modes and\n            Neumann (1994) method is used for flexible modes.\n\n        References\n        ----------\n            Newman, John Nicholas. \"Wave effects on deformable bodies.\"Applied ocean\n            research\" 16.1 (1994): 47-59.\n            http://resolver.tudelft.nl/uuid:0adff84c-43c7-43aa-8cd8-d4c44240bed8\n\n        \"\"\"\n        if len(self.dofs) == 0:\n            raise AttributeError(\"Cannot compute hydrostatics stiffness on {} since no dof has been defined.\".format(self.name))\n\n        def divergence_dof(influenced_dof):\n            if divergence is None:\n                return 0.0\n            elif isinstance(divergence, dict) and influenced_dof in divergence.keys():\n                return divergence[influenced_dof]\n            elif isinstance(divergence, xr.DataArray) and influenced_dof in divergence.coords[\"influenced_dof\"]:\n                return divergence.sel(influenced_dof=influenced_dof).values\n            else:\n                LOG.warning(\"Computing hydrostatic stiffness without the divergence of {}\".format(influenced_dof))\n                return 0.0\n\n        hs_set =  xr.merge([\n            self.each_hydrostatic_stiffness(\n                influenced_dof_name, radiating_dof_name,\n                influenced_dof_div = divergence_dof(influenced_dof_name),\n                rho=rho, g=g\n                )\n            for radiating_dof_name in self.dofs\n            for influenced_dof_name in self.dofs\n            ])\n\n        # Reorder dofs\n        K = hs_set.hydrostatic_stiffness.sel(influenced_dof=list(self.dofs.keys()), radiating_dof=list(self.dofs.keys()))\n        return K\n\n    def compute_rigid_body_inertia(self, *, rho=1000, output_type=\"body_dofs\"):\n        \"\"\"\n        Inertia Mass matrix of the body for 6 rigid DOFs.\n\n        Parameters\n        ----------\n        rho : float, optional\n            Density of water, by default 1000.0\n        output_type : {\"body_dofs\", \"rigid_dofs\", \"all_dofs\"}\n            Type of DOFs for mass mat output, by default \"body_dofs\".\n\n        Returns\n        -------\n        xarray.DataArray\n            Inertia matrix\n\n        Raises\n        ------\n        ValueError\n            If output_type is not in {\"body_dofs\", \"rigid_dofs\", \"all_dofs\"}.\n        \"\"\"\n        if self.center_of_mass is None:\n            raise ValueError(f\"Trying to compute rigid-body inertia matrix for {self.name}, but no center of mass has been defined.\\n\"\n                             f\"Suggested solution: define a `center_of_mass` attribute for the FloatingBody {self.name}.\")\n\n        rc = self._infer_rotation_center()\n        fcs = (self.mesh.faces_centers - rc).T\n        combinations = np.array([fcs[0]**2, fcs[1]**2, fcs[2]**2, fcs[0]*fcs[1],\n                                 fcs[1]*fcs[2], fcs[2]*fcs[0]])\n        integrals = np.array([\n            [np.sum(normal_i * fcs[axis] * combination * self.mesh.faces_areas)\n            for combination in combinations]\n            for axis, normal_i in enumerate(self.mesh.faces_normals.T)])\n\n\n        inertias = np.array([\n            (integrals[0,1]   + integrals[0,2]   + integrals[1,1]/3\n             + integrals[1,2]   + integrals[2,1] + integrals[2,2]/3)/3,\n            (integrals[0,0]/3 + integrals[0,2]   + integrals[1,0]\n             + integrals[1,2]   + integrals[2,0] + integrals[2,2]/3)/3,\n            (integrals[0,0]/3 + integrals[0,1]   + integrals[1,0]\n             + integrals[1,1]/3 + integrals[2,0] + integrals[2,1]  )/3,\n            integrals[2,3],\n            integrals[0,4],\n            integrals[1,5]\n        ])\n\n        cog = self.center_of_mass - rc\n        volume = self.volume\n        volumic_inertia_matrix = np.array([\n            [ volume        , 0              , 0               ,\n              0             , volume*cog[2]  , -volume*cog[1]  ],\n            [ 0             , volume         , 0               ,\n             -volume*cog[2] , 0              , volume*cog[0]   ],\n            [ 0             , 0              , volume          ,\n              volume*cog[1] , -volume*cog[0] , 0 ]             ,\n            [ 0             , -volume*cog[2] , volume*cog[1]   ,\n              inertias[0]   , -inertias[3]   , -inertias[5]    ],\n            [ volume*cog[2] , 0              , -volume*cog[0]  ,\n             -inertias[3]   , inertias[1]    , -inertias[4]    ],\n            [-volume*cog[1] , volume*cog[0]  , 0               ,\n             -inertias[5]   , -inertias[4]   , inertias[2]     ],\n        ])\n\n        density = rho if self.mass is None else self.mass/volume\n        inertia_matrix = density * volumic_inertia_matrix\n\n        # Rigid DOFs\n        rigid_dof_names = [\"Surge\", \"Sway\", \"Heave\", \"Roll\", \"Pitch\", \"Yaw\"]\n        rigid_inertia_matrix_xr = xr.DataArray(data=np.asarray(inertia_matrix),\n                            dims=['influenced_dof', 'radiating_dof'],\n                            coords={'influenced_dof': rigid_dof_names,\n                                    'radiating_dof': rigid_dof_names},\n                            name=\"inertia_matrix\")\n\n        # Body DOFs (Default as np.nan)\n        body_dof_names = list(self.dofs)\n        body_dof_count = len(body_dof_names)\n        other_dofs_inertia_matrix_xr = xr.DataArray(np.nan * np.zeros([body_dof_count, body_dof_count]),\n                                    dims=['influenced_dof', 'radiating_dof'],\n                                    coords={'influenced_dof': body_dof_names,\n                                            'radiating_dof': body_dof_names},\n                                    name=\"inertia_matrix\")\n\n        total_mass_xr = xr.merge([rigid_inertia_matrix_xr, other_dofs_inertia_matrix_xr], compat=\"override\").inertia_matrix\n\n        non_rigid_dofs = set(body_dof_names) - set(rigid_dof_names)\n\n        if output_type == \"body_dofs\":\n            if len(non_rigid_dofs) > 0:\n                LOG.warning(f\"Non-rigid dofs: {non_rigid_dofs} are detected and \\\nrespective inertia coefficients are assigned as NaN.\")\n\n            inertia_matrix_xr = total_mass_xr.sel(influenced_dof=body_dof_names,\n                                                  radiating_dof=body_dof_names)\n        elif output_type == \"rigid_dofs\":\n            inertia_matrix_xr = total_mass_xr.sel(influenced_dof=rigid_dof_names,\n                                                  radiating_dof=rigid_dof_names)\n        elif output_type == \"all_dofs\":\n            if len(non_rigid_dofs) > 0:\n                LOG.warning(\"Non-rigid dofs: {non_rigid_dofs} are detected and \\\nrespective inertia coefficients are assigned as NaN.\")\n\n            inertia_matrix_xr = total_mass_xr\n        else:\n            raise ValueError(f\"output_type should be either 'body_dofs', \\\n'all_dofs' or 'rigid_dofs'. Given output_type = '{output_type}'.\")\n\n        return inertia_matrix_xr\n\n\n    def compute_hydrostatics(self, *, rho=1000.0, g=9.81, divergence=None):\n        \"\"\"Compute hydrostatics of the FloatingBody.\n\n        Parameters\n        ----------\n        rho : float, optional\n            Density of Water. The default is 1000.\n        g: float, optional\n            Gravity acceleration. The default is 9.81.\n        divergence : np.ndarray, optional\n            Divergence of the DOFs.\n\n        Returns\n        -------\n        hydrostatics : dict\n            All hydrostatics values of the FloatingBody.\n        \"\"\"\n        if self.center_of_mass is None:\n            raise ValueError(f\"Trying to compute hydrostatics for {self.name}, but no center of mass has been defined.\\n\"\n                             f\"Suggested solution: define a `center_of_mass` attribute for the FloatingBody {self.name}.\")\n\n        immersed_self = self.immersed_part()\n\n        full_mesh_vertices = self.mesh.vertices\n        coord_max = full_mesh_vertices.max(axis=0)\n        coord_min = full_mesh_vertices.min(axis=0)\n        full_length, full_breadth, depth = full_mesh_vertices.max(axis=0) - full_mesh_vertices.min(axis=0)\n\n        vertices = immersed_self.mesh.vertices\n        sub_length, sub_breadth, _ = vertices.max(axis=0) - vertices.min(axis=0)\n\n        if abs(immersed_self.waterplane_area) > 1e-10:\n            water_plane_idx = np.isclose(vertices[:,2], 0.0)\n            water_plane = vertices[water_plane_idx][:,:-1]\n            wl_length, wl_breadth = water_plane.max(axis=0) - water_plane.min(axis=0)\n        else:\n            wl_length, wl_breadth = 0.0, 0.0\n\n        hydrostatics = {}\n        hydrostatics[\"g\"] = g\n        hydrostatics[\"rho\"] = rho\n        hydrostatics[\"center_of_mass\"] = self.center_of_mass\n\n        hydrostatics[\"wet_surface_area\"] = immersed_self.wet_surface_area\n        hydrostatics[\"disp_volumes\"] = immersed_self.volumes\n        hydrostatics[\"disp_volume\"] = immersed_self.volume\n        hydrostatics[\"disp_mass\"] = immersed_self.disp_mass(rho=rho)\n        hydrostatics[\"center_of_buoyancy\"] = immersed_self.center_of_buoyancy\n        hydrostatics[\"waterplane_center\"] = np.append(immersed_self.waterplane_center, 0.0)\n        hydrostatics[\"waterplane_area\"] = immersed_self.waterplane_area\n        hydrostatics[\"transversal_metacentric_radius\"] = immersed_self.transversal_metacentric_radius\n        hydrostatics[\"longitudinal_metacentric_radius\"] = immersed_self.longitudinal_metacentric_radius\n        hydrostatics[\"transversal_metacentric_height\"] = immersed_self.transversal_metacentric_height\n        hydrostatics[\"longitudinal_metacentric_height\"] = immersed_self.longitudinal_metacentric_height\n        self.hydrostatic_stiffness = hydrostatics[\"hydrostatic_stiffness\"] = immersed_self.compute_hydrostatic_stiffness(\n            divergence=divergence, rho=rho, g=g)\n\n        hydrostatics[\"length_overall\"] = full_length\n        hydrostatics[\"breadth_overall\"] = full_breadth\n        hydrostatics[\"depth\"] = depth\n        hydrostatics[\"draught\"] = np.abs(coord_min[2])\n        hydrostatics[\"length_at_waterline\"] = wl_length\n        hydrostatics[\"breadth_at_waterline\"] = wl_breadth\n        hydrostatics[\"length_overall_submerged\"] = sub_length\n        hydrostatics[\"breadth_overall_submerged\"] = sub_breadth\n        if any(dof.lower() in {\"surge\", \"sway\", \"heave\", \"roll\", \"pitch\", \"yaw\"}\n               for dof in self.dofs) > 0: # If there is at least one rigid body dof:\n            self.inertia_matrix = hydrostatics[\"inertia_matrix\"] = self.compute_rigid_body_inertia(rho=rho)\n\n        return hydrostatics\n\n\n    ###################\n    # Transformations #\n    ###################\n\n    def __add__(self, body_to_add: 'FloatingBody') -> 'FloatingBody':\n        return self.join_bodies(body_to_add)\n\n    def join_bodies(*bodies, name=None) -> 'FloatingBody':\n        if name is None:\n            name = \"+\".join(body.name for body in bodies)\n        meshes = CollectionOfMeshes([body.mesh for body in bodies], name=f\"{name}_mesh\")\n        dofs = FloatingBody.combine_dofs(bodies)\n\n        if all(body.mass is not None for body in bodies):\n            new_mass = sum(body.mass for body in bodies)\n        else:\n            new_mass = None\n\n        if (all(body.mass is not None for body in bodies)\n                and all(body.center_of_mass is not None for body in bodies)):\n            new_cog = sum(body.mass*np.asarray(body.center_of_mass) for body in bodies)/new_mass\n        else:\n            new_cog = None\n\n        joined_bodies = FloatingBody(\n            mesh=meshes, dofs=dofs, mass=new_mass, center_of_mass=new_cog, name=name\n            )\n\n        for matrix_name in [\"inertia_matrix\", \"hydrostatic_stiffness\"]:\n            if all(hasattr(body, matrix_name) for body in bodies):\n                from scipy.linalg import block_diag\n                setattr(joined_bodies, matrix_name, joined_bodies.add_dofs_labels_to_matrix(\n                        block_diag(*[getattr(body, matrix_name) for body in bodies])\n                        ))\n\n        return joined_bodies\n\n    @staticmethod\n    def combine_dofs(bodies) -> dict:\n        \"\"\"Combine the degrees of freedom of several bodies.\"\"\"\n        dofs = {}\n        cum_nb_faces = accumulate(chain([0], (body.mesh.nb_faces for body in bodies)))\n        total_nb_faces = sum(body.mesh.nb_faces for body in bodies)\n        for body, nbf in zip(bodies, cum_nb_faces):\n            # nbf is the cumulative number of faces of the previous subbodies,\n            # that is the offset of the indices of the faces of the current body.\n            for name, dof in body.dofs.items():\n                new_dof = np.zeros((total_nb_faces, 3))\n                new_dof[nbf:nbf+len(dof), :] = dof\n                if '__' not in name:\n                    new_dof_name = '__'.join([body.name, name])\n                else:\n                    # The body is probably a combination of bodies already.\n                    # So for the associativity of the + operation,\n                    # it is better to keep the same name.\n                    new_dof_name = name\n                dofs[new_dof_name] = new_dof\n        return dofs\n\n    def copy(self, name=None) -> 'FloatingBody':\n        \"\"\"Return a deep copy of the body.\n\n        Parameters\n        ----------\n        name : str, optional\n            a name for the new copy\n        \"\"\"\n        new_body = copy.deepcopy(self)\n        if name is None:\n            new_body.name = f\"copy_of_{self.name}\"\n            LOG.debug(f\"Copy {self.name}.\")\n        else:\n            new_body.name = name\n            LOG.debug(f\"Copy {self.name} under the name {name}.\")\n        return new_body\n\n    def assemble_regular_array(self, distance, nb_bodies):\n        \"\"\"Create an regular array of identical bodies.\n\n        Parameters\n        ----------\n        distance : float\n            Center-to-center distance between objects in the array\n        nb_bodies : couple of ints\n            Number of objects in the x and y directions.\n\n        Returns\n        -------\n        FloatingBody\n        \"\"\"\n        bodies = (self.translated((i*distance, j*distance, 0), name=f\"{i}_{j}\") for j in range(nb_bodies[1]) for i in range(nb_bodies[0]))\n        array = FloatingBody.join_bodies(*bodies)\n        array.mesh = build_regular_array_of_meshes(self.mesh, distance, nb_bodies)\n        array.name = f\"array_of_{self.name}\"\n        return array\n\n    def assemble_arbitrary_array(self, locations:np.ndarray):\n\n        if not isinstance(locations, np.ndarray):\n            raise TypeError('locations must be of type np.ndarray')\n        assert locations.shape[1] == 2, 'locations must be of shape nx2, received {:}'.format(locations.shape)\n        n = locations.shape[0]\n\n        fb_list = []\n        for idx, li in enumerate(locations):\n            fb1 = self.copy()\n            fb1.translate(np.append(li,0))\n            fb1.name = 'arbitrary_array_body{:02d}'.format(idx)\n            fb_list.append(fb1)\n\n        arbitrary_array = fb_list[0].join_bodies(*fb_list[1:])\n\n        return arbitrary_array\n\n    def extract_faces(self, id_faces_to_extract, return_index=False):\n        \"\"\"Create a new FloatingBody by extracting some faces from the mesh.\n        The dofs evolve accordingly.\n        \"\"\"\n        if isinstance(self.mesh, CollectionOfMeshes):\n            raise NotImplementedError  # TODO\n\n        if return_index:\n            new_mesh, id_v = Mesh.extract_faces(self.mesh, id_faces_to_extract, return_index)\n        else:\n            new_mesh = Mesh.extract_faces(self.mesh, id_faces_to_extract, return_index)\n        new_body = FloatingBody(new_mesh)\n        LOG.info(f\"Extract floating body from {self.name}.\")\n\n        new_body.dofs = {}\n        for name, dof in self.dofs.items():\n            new_body.dofs[name] = dof[id_faces_to_extract, :]\n\n        if return_index:\n            return new_body, id_v\n        else:\n            return new_body\n\n    def sliced_by_plane(self, plane):\n        return FloatingBody(mesh=self.mesh.sliced_by_plane(plane), dofs=self.dofs, name=self.name)\n\n    def minced(self, nb_slices=(8, 8, 4)):\n        \"\"\"Experimental method decomposing the mesh as a hierarchical structure.\n\n        Parameters\n        ----------\n        nb_slices: Tuple[int, int, int]\n            The number of slices in each of the x, y and z directions.\n            Only powers of 2 are supported at the moment.\n\n        Returns\n        -------\n        FloatingBody\n        \"\"\"\n        minced_body = self.copy()\n\n        # Extreme points of the mesh in each directions.\n        x_min, x_max, y_min, y_max, z_min, z_max = self.mesh.axis_aligned_bbox\n        sizes = [(x_min, x_max), (y_min, y_max), (z_min, z_max)]\n\n        directions = [np.array(d) for d in [(1, 0, 0), (0, 1, 0), (0, 0, 1)]]\n\n        def _slice_positions_at_depth(i):\n            \"\"\"Helper function.\n\n            Returns a list of floats as follows:\n            i=1 -> [1/2]\n            i=2 -> [1/4, 3/4]\n            i=3 -> [1/8, 3/8, 5/8, 7/8]\n                   ...\n            \"\"\"\n            denominator = 2**i\n            return [numerator/denominator for numerator in range(1, denominator, 2)]\n\n        # GENERATE ALL THE PLANES THAT WILL BE USED TO MINCE THE MESH\n        planes = []\n        for direction, nb_slices_in_dir, (min_coord, max_coord) in zip(directions, nb_slices, sizes):\n            planes_in_dir = []\n\n            depth_of_treelike_structure = int(np.log2(nb_slices_in_dir))\n            for i_depth in range(1, depth_of_treelike_structure+1):\n                planes_in_dir_at_depth = []\n                for relative_position in _slice_positions_at_depth(i_depth):\n                    slice_position = (min_coord + relative_position*(max_coord-min_coord))*direction\n                    plane = Plane(normal=direction, point=slice_position)\n                    planes_in_dir_at_depth.append(plane)\n                planes_in_dir.append(planes_in_dir_at_depth)\n            planes.append(planes_in_dir)\n\n        # SLICE THE MESH\n        intermingled_x_y_z = chain.from_iterable(zip_longest(*planes))\n        for planes in intermingled_x_y_z:\n            if planes is not None:\n                for plane in planes:\n                    minced_body = minced_body.sliced_by_plane(plane)\n        return minced_body\n\n    @inplace_transformation\n    def mirror(self, plane):\n        self.mesh.mirror(plane)\n        for dof in self.dofs:\n            self.dofs[dof] -= 2 * np.outer(np.dot(self.dofs[dof], plane.normal), plane.normal)\n        for point_attr in ('geometric_center', 'rotation_center', 'center_of_mass'):\n            if point_attr in self.__dict__ and self.__dict__[point_attr] is not None:\n                self.__dict__[point_attr] -= 2 * (np.dot(self.__dict__[point_attr], plane.normal) - plane.c) * plane.normal\n        return self\n\n    @inplace_transformation\n    def translate(self, vector, *args, **kwargs):\n        self.mesh.translate(vector, *args, **kwargs)\n        for point_attr in ('geometric_center', 'rotation_center', 'center_of_mass'):\n            if point_attr in self.__dict__ and self.__dict__[point_attr] is not None:\n                self.__dict__[point_attr] += vector\n        return self\n\n    @inplace_transformation\n    def rotate(self, axis, angle):\n        self.mesh.rotate(axis, angle)\n        for point_attr in ('geometric_center', 'rotation_center', 'center_of_mass'):\n            if point_attr in self.__dict__ and self.__dict__[point_attr] is not None:\n                self.__dict__[point_attr] = axis.rotate_points([self.__dict__[point_attr]], angle)\n        for dof in self.dofs:\n            self.dofs[dof] = axis.rotate_vectors(self.dofs[dof], angle)\n        return self\n\n    @inplace_transformation\n    def clip(self, plane):\n        # Clip mesh\n        LOG.info(f\"Clipping {self.name} with respect to {plane}\")\n        self.mesh.clip(plane)\n\n        # Clip dofs\n        ids = self.mesh._clipping_data['faces_ids']\n        for dof in self.dofs:\n            if len(ids) > 0:\n                self.dofs[dof] = np.array(self.dofs[dof])[ids]\n            else:\n                self.dofs[dof] = np.empty((0, 3))\n        return self\n\n\n    #############\n    #  Display  #\n    #############\n\n    def __short_str__(self):\n        return (f\"{self.__class__.__name__}(..., name=\\\"{self.name}\\\")\")\n\n    def _optional_params_str(self):\n        items = []\n        if self.mass is not None: items.append(f\"mass={self.mass}, \")\n        if self.center_of_mass is not None: items.append(f\"center_of_mass={self.center_of_mass}, \")\n        return ''.join(items)\n\n    def __str__(self):\n        short_dofs = '{' + ', '.join('\"{}\": ...'.format(d) for d in self.dofs) + '}'\n        return (f\"{self.__class__.__name__}(mesh={self.mesh.__short_str__()}, dofs={short_dofs}, {self._optional_params_str()}name=\\\"{self.name}\\\")\")\n\n    def __repr__(self):\n        short_dofs = '{' + ', '.join('\"{}\": ...'.format(d) for d in self.dofs) + '}'\n        return (f\"{self.__class__.__name__}(mesh={str(self.mesh)}, dofs={short_dofs}, {self._optional_params_str()}name=\\\"{self.name}\\\")\")\n\n    def _repr_pretty_(self, p, cycle):\n        p.text(self.__str__())\n\n    def __rich_repr__(self):\n        class DofWithShortRepr:\n            def __repr__(self):\n                return '...'\n        yield \"mesh\", self.mesh\n        yield \"dofs\", {d: DofWithShortRepr() for d in self.dofs}\n        if self.mass is not None:\n            yield \"mass\", self.mass, None\n        if self.center_of_mass is not None:\n            yield \"center_of_mass\", tuple(self.center_of_mass)\n        yield \"name\", self.name\n\n    def show(self, **kwargs):\n        from capytaine.ui.vtk.body_viewer import FloatingBodyViewer\n        viewer = FloatingBodyViewer()\n        viewer.add_body(self, **kwargs)\n        viewer.show()\n        viewer.finalize()\n\n    def show_matplotlib(self, *args, **kwargs):\n        return self.mesh.show_matplotlib(*args, **kwargs)\n\n    def animate(self, motion, *args, **kwargs):\n        \"\"\"Display a motion as a 3D animation.\n\n        Parameters\n        ==========\n        motion: dict or pd.Series or str\n            A dict or series mapping the name of the dofs to its amplitude.\n            If a single string is passed, it is assumed to be the name of a dof\n            and this dof with a unit amplitude will be displayed.\n        \"\"\"\n        from capytaine.ui.vtk.animation import Animation\n        if isinstance(motion, str):\n            motion = {motion: 1.0}\n        elif isinstance(motion, xr.DataArray):\n            motion = {k: motion.sel(radiating_dof=k).data for k in motion.coords[\"radiating_dof\"].data}\n\n        if any(dof not in self.dofs for dof in motion):\n            missing_dofs = set(motion.keys()) - set(self.dofs.keys())\n            raise ValueError(f\"Trying to animate the body {self.name} using dof(s) {missing_dofs}, but no dof of this name is defined for {self.name}.\")\n\n        animation = Animation(*args, **kwargs)\n        animation._add_actor(self.mesh.merged(), faces_motion=sum(motion[dof_name] * dof for dof_name, dof in self.dofs.items() if dof_name in motion))\n        return animation\n\n    @property\n    def minimal_computable_wavelength(self):\n        \"\"\"For accuracy of the resolution, wavelength should not be smaller than this value.\"\"\"\n        return 8*self.mesh.faces_radiuses.max()",
  "def __init__(self, mesh=None, dofs=None, mass=None, center_of_mass=None, name=None):\n        if mesh is None:\n            self.mesh = Mesh(name=\"dummy_mesh\")\n\n        elif meshio is not None and isinstance(mesh, meshio._mesh.Mesh):\n            from capytaine.io.meshio import load_from_meshio\n            self.mesh = load_from_meshio(mesh)\n\n        elif isinstance(mesh, Mesh) or isinstance(mesh, CollectionOfMeshes):\n            self.mesh = mesh\n\n        else:\n            raise TypeError(\"Unrecognized `mesh` object passed to the FloatingBody constructor.\")\n\n        if name is None and mesh is None:\n            self.name = \"dummy_body\"\n        elif name is None:\n            self.name = self.mesh.name\n        else:\n            self.name = name\n\n        self.mass = mass\n        if center_of_mass is not None:\n            self.center_of_mass = np.asarray(center_of_mass, dtype=float)\n        else:\n            self.center_of_mass = None\n\n        if self.mesh.nb_vertices == 0 or self.mesh.nb_faces == 0:\n            LOG.warning(f\"New floating body (with empty mesh!): {self.name}.\")\n        else:\n            self.mesh.heal_mesh()\n            LOG.info(f\"New floating body: {self.name}.\")\n\n        if dofs is None:\n            self.dofs = {}\n        elif isinstance(dofs, RigidBodyDofsPlaceholder):\n            if dofs.rotation_center is not None:\n                self.rotation_center = np.asarray(dofs.rotation_center, dtype=float)\n            self.dofs = {}\n            self.add_all_rigid_body_dofs()\n        else:\n            self.dofs = dofs",
  "def from_meshio(mesh, name=None) -> 'FloatingBody':\n        \"\"\"Create a FloatingBody from a meshio mesh object.\"\"\"\n        from capytaine.io.meshio import load_from_meshio\n        return FloatingBody(mesh=load_from_meshio(mesh, name), name=name)",
  "def from_file(filename: str, file_format=None, name=None) -> 'FloatingBody':\n        \"\"\"Create a FloatingBody from a mesh file using meshmagick.\"\"\"\n        from capytaine.io.mesh_loaders import load_mesh\n        if name is None: name = filename\n        mesh = load_mesh(filename, file_format, name=f\"{name}_mesh\")\n        return FloatingBody(mesh, name=name)",
  "def __lt__(self, other: 'FloatingBody') -> bool:\n        \"\"\"Arbitrary order. The point is to sort together the problems involving the same body.\"\"\"\n        return self.name < other.name",
  "def nb_dofs(self) -> int:\n        \"\"\"Number of degrees of freedom.\"\"\"\n        return len(self.dofs)",
  "def add_translation_dof(self, direction=None, name=None, amplitude=1.0) -> None:\n        \"\"\"Add a new translation dof (in place).\n        If no direction is given, the code tries to infer it from the name.\n\n        Parameters\n        ----------\n        direction : array of shape (3,), optional\n            the direction of the translation\n        name : str, optional\n            a name for the degree of freedom\n        amplitude : float, optional\n            amplitude of the dof (default: 1.0 m/s)\n        \"\"\"\n        if direction is None:\n            if name is not None and name.lower() in TRANSLATION_DOFS_DIRECTIONS:\n                direction = TRANSLATION_DOFS_DIRECTIONS[name.lower()]\n            else:\n                raise ValueError(\"A direction needs to be specified for the dof.\")\n\n        if name is None:\n            name = f\"dof_{self.nb_dofs}_translation\"\n\n        direction = np.asarray(direction)\n        assert direction.shape == (3,)\n\n        motion = np.empty((self.mesh.nb_faces, 3))\n        motion[:, :] = direction\n        self.dofs[name] = amplitude * motion",
  "def add_rotation_dof(self, axis=None, name=None, amplitude=1.0) -> None:\n        \"\"\"Add a new rotation dof (in place).\n        If no axis is given, the code tries to infer it from the name.\n\n        Parameters\n        ----------\n        axis: Axis, optional\n            the axis of the rotation\n        name : str, optional\n            a name for the degree of freedom\n        amplitude : float, optional\n            amplitude of the dof (default: 1.0)\n        \"\"\"\n        if axis is None:\n            if name is not None and name.lower() in ROTATION_DOFS_AXIS:\n                axis_direction = ROTATION_DOFS_AXIS[name.lower()]\n                for point_attr in ('rotation_center', 'center_of_mass', 'geometric_center'):\n                    if hasattr(self, point_attr) and getattr(self, point_attr) is not None:\n                        axis_point = getattr(self, point_attr)\n                        LOG.info(f\"The rotation dof {name} has been initialized around the point: \"\n                                 f\"{self.name}.{point_attr} = {getattr(self, point_attr)}\")\n                        break\n                else:\n                    axis_point = np.array([0, 0, 0])\n                    LOG.warning(f\"The rotation dof {name} has been initialized \"\n                                f\"around the origin of the domain (0, 0, 0).\")\n            else:\n                raise ValueError(\"A direction needs to be specified for the dof.\")\n        else:\n            axis_point = axis.point\n            axis_direction = axis.vector\n\n        if name is None:\n            name = f\"dof_{self.nb_dofs}_rotation\"\n\n        if self.mesh.nb_faces == 0:\n            self.dofs[name] = np.empty((self.mesh.nb_faces, 3))\n        else:\n            motion = np.cross(axis_point - self.mesh.faces_centers, axis_direction)\n            self.dofs[name] = amplitude * motion",
  "def add_all_rigid_body_dofs(self) -> None:\n        \"\"\"Add the six degrees of freedom of rigid bodies (in place).\"\"\"\n        self.add_translation_dof(name=\"Surge\")\n        self.add_translation_dof(name=\"Sway\")\n        self.add_translation_dof(name=\"Heave\")\n        self.add_rotation_dof(name=\"Roll\")\n        self.add_rotation_dof(name=\"Pitch\")\n        self.add_rotation_dof(name=\"Yaw\")",
  "def integrate_pressure(self, pressure):\n        forces = {}\n        for dof_name in self.dofs:\n            # Scalar product on each face:\n            normal_dof_amplitude_on_face = - np.sum(self.dofs[dof_name] * self.mesh.faces_normals, axis=1)\n            # The minus sign in the above line is because we want the force of the fluid on the body and not the force of the body on the fluid.\n            # Sum over all faces:\n            forces[dof_name] = np.sum(pressure * normal_dof_amplitude_on_face * self.mesh.faces_areas)\n        return forces",
  "def keep_only_dofs(self, dofs):\n        for dof in list(self.dofs.keys()):\n            if dof not in dofs:\n                del self.dofs[dof]\n\n        if hasattr(self, 'inertia_matrix'):\n            self.inertia_matrix = self.inertia_matrix.sel(radiating_dof=dofs, influenced_dof=dofs)\n        if hasattr(self, 'hydrostatic_stiffness'):\n            self.hydrostatic_stiffness = self.hydrostatic_stiffness.sel(radiating_dof=dofs, influenced_dof=dofs)\n\n        return self",
  "def add_dofs_labels_to_vector(self, vector):\n        \"\"\"Helper function turning a bare vector into a vector labelled by the name of the dofs of the body,\n        to be used for instance for the computation of RAO.\"\"\"\n        return xr.DataArray(data=np.asarray(vector), dims=['influenced_dof'],\n                            coords={'influenced_dof': list(self.dofs)},\n                            )",
  "def add_dofs_labels_to_matrix(self, matrix):\n        \"\"\"Helper function turning a bare matrix into a matrix labelled by the name of the dofs of the body,\n        to be used for instance for the computation of RAO.\"\"\"\n        return xr.DataArray(data=np.asarray(matrix), dims=['influenced_dof', 'radiating_dof'],\n                            coords={'influenced_dof': list(self.dofs), 'radiating_dof': list(self.dofs)},\n                            )",
  "def surface_integral(self, data, **kwargs):\n        \"\"\"Returns integral of given data along wet surface area.\"\"\"\n        return self.mesh.surface_integral(data, **kwargs)",
  "def waterplane_integral(self, data, **kwargs):\n        \"\"\"Returns integral of given data along water plane area.\"\"\"\n        return self.mesh.waterplane_integral(data, **kwargs)",
  "def wet_surface_area(self):\n        \"\"\"Returns wet surface area.\"\"\"\n        return self.mesh.wet_surface_area",
  "def volumes(self):\n        \"\"\"Returns volumes using x, y, z components of the FloatingBody.\"\"\"\n        return self.mesh.volumes",
  "def volume(self):\n        \"\"\"Returns volume of the FloatingBody.\"\"\"\n        return self.mesh.volume",
  "def disp_mass(self, *, rho=1000):\n        return self.mesh.disp_mass(rho=rho)",
  "def center_of_buoyancy(self):\n        \"\"\"Returns center of buoyancy of the FloatingBody.\"\"\"\n        return self.mesh.center_of_buoyancy",
  "def waterplane_area(self):\n        \"\"\"Returns water plane area of the FloatingBody.\"\"\"\n        return self.mesh.waterplane_area",
  "def waterplane_center(self):\n        \"\"\"Returns water plane center of the FloatingBody.\n\n        Note: Returns None if the FloatingBody is full submerged.\n        \"\"\"\n        return self.mesh.waterplane_center",
  "def transversal_metacentric_radius(self):\n        \"\"\"Returns transversal metacentric radius of the mesh.\"\"\"\n        inertia_moment = -self.waterplane_integral(self.mesh.faces_centers[:,1]**2)\n        return inertia_moment / self.volume",
  "def longitudinal_metacentric_radius(self):\n        \"\"\"Returns longitudinal metacentric radius of the mesh.\"\"\"\n        inertia_moment = -self.waterplane_integral(self.mesh.faces_centers[:,0]**2)\n        return inertia_moment / self.volume",
  "def transversal_metacentric_height(self):\n        \"\"\"Returns transversal metacentric height of the mesh.\"\"\"\n        gb = self.center_of_mass - self.center_of_buoyancy\n        return self.transversal_metacentric_radius - gb[2]",
  "def longitudinal_metacentric_height(self):\n        \"\"\"Returns longitudinal metacentric height of the mesh.\"\"\"\n        gb = self.center_of_mass - self.center_of_buoyancy\n        return self.longitudinal_metacentric_radius - gb[2]",
  "def dof_normals(self, dof):\n        \"\"\"Returns dot product of the surface face normals and DOF\"\"\"\n        return np.sum(self.mesh.faces_normals * dof, axis=1)",
  "def _infer_rotation_center(self):\n        \"\"\"Hacky way to infer the point around which the rotation dofs are defined.\n        (Assuming all three rotation dofs are defined around the same point).\n        In the future, should be replaced by something more robust.\n        \"\"\"\n        if hasattr(self, \"rotation_center\"):\n            return np.asarray(self.rotation_center)\n\n        else:\n            try:\n                xc1 = self.dofs[\"Pitch\"][:, 2] + self.mesh.faces_centers[:, 0]\n                xc2 = -self.dofs[\"Yaw\"][:, 1] + self.mesh.faces_centers[:, 0]\n                yc1 = self.dofs[\"Yaw\"][:, 0] + self.mesh.faces_centers[:, 1]\n                yc2 = -self.dofs[\"Roll\"][:, 2] + self.mesh.faces_centers[:, 1]\n                zc1 = -self.dofs[\"Pitch\"][:, 0] + self.mesh.faces_centers[:, 2]\n                zc2 = self.dofs[\"Roll\"][:, 1] + self.mesh.faces_centers[:, 2]\n\n                # All items should be identical in a given vector\n                assert np.isclose(xc1, xc1[0]).all()\n                assert np.isclose(yc1, yc1[0]).all()\n                assert np.isclose(zc1, zc1[0]).all()\n\n                # Both vector should be identical\n                assert np.allclose(xc1, xc2)\n                assert np.allclose(yc1, yc2)\n                assert np.allclose(zc1, zc2)\n\n                return np.array([xc1[0], yc1[0], zc1[0]])\n\n            except Exception as e:\n                raise ValueError(\n                        f\"Failed to infer the rotation center of {self.name} to compute rigid body hydrostatics.\\n\"\n                        f\"Possible fix: add a `rotation_center` attibute to {self.name}.\\n\"\n                        \"Note that rigid body hydrostatic methods currently assume that the three rotation dofs have the same rotation center.\"\n                        ) from e",
  "def each_hydrostatic_stiffness(self, influenced_dof_name, radiating_dof_name, *,\n                                         influenced_dof_div=0.0, rho=1000.0, g=9.81):\n        r\"\"\"\n        Return the hydrostatic stiffness for a pair of DOFs.\n\n        :math:`C_{ij} = \\rho g\\iint_S (\\hat{n} \\cdot V_j) (w_i + z D_i) dS`\n\n        where :math:`\\hat{n}` is surface normal,\n\n        :math:`V_i = u_i \\hat{n}_x + v_i \\hat{n}_y + w_i \\hat{n}_z` is DOF vector and\n\n        :math:`D_i = \\nabla \\cdot V_i` is the divergence of the DOF.\n\n        Parameters\n        ----------\n        influenced_dof_name : str\n            Name of influenced DOF vector of the FloatingBody\n        radiating_dof_name: str\n            Name of radiating DOF vector of the FloatingBody\n        influenced_dof_div: np.ndarray (Face_count), optional\n            Influenced DOF divergence of the FloatingBody, by default 0.0.\n        rho: float, optional\n            water density, by default 1000.0\n        g: float, optional\n            Gravity acceleration, by default 9.81\n\n        Returns\n        -------\n        hs_ij: xarray.variable\n            hydrostatic_stiffness of ith DOF and jth DOF.\n\n        Note\n        ----\n            This function computes the hydrostatic stiffness assuming :math:`D_{i} = 0`.\n            If :math:`D_i \\neq 0`, input the divergence interpolated to face centers.\n\n            General integral equations are used for the rigid body modes and\n            Neumann (1994) method is used for flexible modes.\n\n        References\n        ----------\n            Newman, John Nicholas. \"Wave effects on deformable bodies.\"Applied ocean\n            research\" 16.1 (1994): 47-59.\n            http://resolver.tudelft.nl/uuid:0adff84c-43c7-43aa-8cd8-d4c44240bed8\n\n        \"\"\"\n        # Newman (1994) formula is not 'complete' as recovering the rigid body\n        # terms is not possible. https://doi.org/10.1115/1.3058702.\n\n        # Alternative is to use the general equation of hydrostatic and\n        # restoring coefficient for rigid modes and use Newman equation for elastic\n        # modes.\n\n        rigid_dof_names = (\"Surge\", \"Sway\", \"Heave\", \"Roll\", \"Pitch\", \"Yaw\")\n        dof_pair = (influenced_dof_name, radiating_dof_name)\n\n        if set(dof_pair).issubset(set(rigid_dof_names)):\n            if self.center_of_mass is None:\n                raise ValueError(f\"Trying to compute rigid-body hydrostatic stiffness for {self.name}, but no center of mass has been defined.\\n\"\n                                 f\"Suggested solution: define a `center_of_mass` attribute for the FloatingBody {self.name}.\")\n            mass = self.disp_mass(rho=rho) if self.mass is None else self.mass\n            xc, yc, zc = self._infer_rotation_center()\n\n            if dof_pair == (\"Heave\", \"Heave\"):\n                norm_hs_stiff = self.waterplane_area\n            elif dof_pair in [(\"Heave\", \"Roll\"), (\"Roll\", \"Heave\")]:\n                norm_hs_stiff = -self.waterplane_integral(self.mesh.faces_centers[:,1] - yc)\n            elif dof_pair in [(\"Heave\", \"Pitch\"), (\"Pitch\", \"Heave\")]:\n                norm_hs_stiff = self.waterplane_integral(self.mesh.faces_centers[:,0] - xc)\n            elif dof_pair == (\"Roll\", \"Roll\"):\n                norm_hs_stiff = (\n                        -self.waterplane_integral((self.mesh.faces_centers[:,1] - yc)**2)\n                        + self.volume*(self.center_of_buoyancy[2] - zc) - mass/rho*(self.center_of_mass[2] - zc)\n                )\n            elif dof_pair in [(\"Roll\", \"Pitch\"), (\"Pitch\", \"Roll\")]:\n                norm_hs_stiff = self.waterplane_integral((self.mesh.faces_centers[:,0] - xc)\n                                                          * (self.mesh.faces_centers[:,1] - yc))\n            elif dof_pair == (\"Roll\", \"Yaw\"):\n                norm_hs_stiff = - self.volume*(self.center_of_buoyancy[0] - xc) + mass/rho*(self.center_of_mass[0] - xc)\n            elif dof_pair == (\"Pitch\", \"Pitch\"):\n                norm_hs_stiff = (\n                        -self.waterplane_integral((self.mesh.faces_centers[:,0] - xc)**2)\n                        + self.volume*(self.center_of_buoyancy[2] - zc) - mass/rho*(self.center_of_mass[2] - zc)\n                        )\n            elif dof_pair == (\"Pitch\", \"Yaw\"):\n                norm_hs_stiff = - self.volume*(self.center_of_buoyancy[1] - yc) + mass/rho*(self.center_of_mass[1] - yc)\n            else:\n                norm_hs_stiff = 0.0\n        else:\n            if self.mass is not None and np.isclose(self.mass, self.disp_mass(rho), rtol=1e-4):\n                raise NotImplementedError(\n                        f\"Trying to compute the hydrostatic stiffness for dofs {radiating_dof_name} and {influenced_dof_name}\"\n                        f\"of body {self.name}, which is not neutrally buoyant (mass={body.mass}, disp_mass={body.disp_mass(rho)}.\\n\"\n                        f\"This case has not been implemented in Capytaine. You need either a single rigid body or a neutrally buoyant body.\"\n                        )\n\n            # Newman (1994) formula for flexible DOFs\n            influenced_dof = np.array(self.dofs[influenced_dof_name])\n            radiating_dof = np.array(self.dofs[radiating_dof_name])\n            influenced_dof_div_array = np.array(influenced_dof_div)\n\n            radiating_dof_normal = self.dof_normals(radiating_dof)\n            z_influenced_dof_div = influenced_dof[:,2] + self.mesh.faces_centers[:,2] * influenced_dof_div_array\n            norm_hs_stiff = self.surface_integral( -radiating_dof_normal * z_influenced_dof_div)\n\n        hs_stiff = rho * g * norm_hs_stiff\n\n        return xr.DataArray([[hs_stiff]],\n                            dims=['influenced_dof', 'radiating_dof'],\n                            coords={'influenced_dof': [influenced_dof_name],\n                            'radiating_dof': [radiating_dof_name]},\n                            name=\"hydrostatic_stiffness\"\n                            )",
  "def compute_hydrostatic_stiffness(self, *, divergence=None, rho=1000.0, g=9.81):\n        r\"\"\"\n        Compute hydrostatic stiffness matrix for all DOFs of the body.\n\n        :math:`C_{ij} = \\rho g\\iint_S (\\hat{n} \\cdot V_j) (w_i + z D_i) dS`\n\n        where :math:`\\hat{n}` is surface normal,\n\n        :math:`V_i = u_i \\hat{n}_x + v_i \\hat{n}_y + w_i \\hat{n}_z` is DOF vector and\n\n        :math:`D_i = \\nabla \\cdot V_i` is the divergence of the DOF.\n\n        Parameters\n        ----------\n        divergence : dict mapping a dof name to an array of shape (nb_faces) or\n                        xarray.DataArray of shape (nb_dofs \u00d7 nb_faces), optional\n            Divergence of the DOFs, by default None\n        rho : float, optional\n            Water density, by default 1000.0\n        g: float, optional\n            Gravity acceleration, by default 9.81\n\n        Returns\n        -------\n        xr.DataArray\n            Matrix of hydrostatic stiffness\n\n        Note\n        ----\n            This function computes the hydrostatic stiffness assuming :math:`D_{i} = 0`.\n            If :math:`D_i \\neq 0`, input the divergence interpolated to face centers.\n\n            General integral equations are used for the rigid body modes and\n            Neumann (1994) method is used for flexible modes.\n\n        References\n        ----------\n            Newman, John Nicholas. \"Wave effects on deformable bodies.\"Applied ocean\n            research\" 16.1 (1994): 47-59.\n            http://resolver.tudelft.nl/uuid:0adff84c-43c7-43aa-8cd8-d4c44240bed8\n\n        \"\"\"\n        if len(self.dofs) == 0:\n            raise AttributeError(\"Cannot compute hydrostatics stiffness on {} since no dof has been defined.\".format(self.name))\n\n        def divergence_dof(influenced_dof):\n            if divergence is None:\n                return 0.0\n            elif isinstance(divergence, dict) and influenced_dof in divergence.keys():\n                return divergence[influenced_dof]\n            elif isinstance(divergence, xr.DataArray) and influenced_dof in divergence.coords[\"influenced_dof\"]:\n                return divergence.sel(influenced_dof=influenced_dof).values\n            else:\n                LOG.warning(\"Computing hydrostatic stiffness without the divergence of {}\".format(influenced_dof))\n                return 0.0\n\n        hs_set =  xr.merge([\n            self.each_hydrostatic_stiffness(\n                influenced_dof_name, radiating_dof_name,\n                influenced_dof_div = divergence_dof(influenced_dof_name),\n                rho=rho, g=g\n                )\n            for radiating_dof_name in self.dofs\n            for influenced_dof_name in self.dofs\n            ])\n\n        # Reorder dofs\n        K = hs_set.hydrostatic_stiffness.sel(influenced_dof=list(self.dofs.keys()), radiating_dof=list(self.dofs.keys()))\n        return K",
  "def compute_rigid_body_inertia(self, *, rho=1000, output_type=\"body_dofs\"):\n        \"\"\"\n        Inertia Mass matrix of the body for 6 rigid DOFs.\n\n        Parameters\n        ----------\n        rho : float, optional\n            Density of water, by default 1000.0\n        output_type : {\"body_dofs\", \"rigid_dofs\", \"all_dofs\"}\n            Type of DOFs for mass mat output, by default \"body_dofs\".\n\n        Returns\n        -------\n        xarray.DataArray\n            Inertia matrix\n\n        Raises\n        ------\n        ValueError\n            If output_type is not in {\"body_dofs\", \"rigid_dofs\", \"all_dofs\"}.\n        \"\"\"\n        if self.center_of_mass is None:\n            raise ValueError(f\"Trying to compute rigid-body inertia matrix for {self.name}, but no center of mass has been defined.\\n\"\n                             f\"Suggested solution: define a `center_of_mass` attribute for the FloatingBody {self.name}.\")\n\n        rc = self._infer_rotation_center()\n        fcs = (self.mesh.faces_centers - rc).T\n        combinations = np.array([fcs[0]**2, fcs[1]**2, fcs[2]**2, fcs[0]*fcs[1],\n                                 fcs[1]*fcs[2], fcs[2]*fcs[0]])\n        integrals = np.array([\n            [np.sum(normal_i * fcs[axis] * combination * self.mesh.faces_areas)\n            for combination in combinations]\n            for axis, normal_i in enumerate(self.mesh.faces_normals.T)])\n\n\n        inertias = np.array([\n            (integrals[0,1]   + integrals[0,2]   + integrals[1,1]/3\n             + integrals[1,2]   + integrals[2,1] + integrals[2,2]/3)/3,\n            (integrals[0,0]/3 + integrals[0,2]   + integrals[1,0]\n             + integrals[1,2]   + integrals[2,0] + integrals[2,2]/3)/3,\n            (integrals[0,0]/3 + integrals[0,1]   + integrals[1,0]\n             + integrals[1,1]/3 + integrals[2,0] + integrals[2,1]  )/3,\n            integrals[2,3],\n            integrals[0,4],\n            integrals[1,5]\n        ])\n\n        cog = self.center_of_mass - rc\n        volume = self.volume\n        volumic_inertia_matrix = np.array([\n            [ volume        , 0              , 0               ,\n              0             , volume*cog[2]  , -volume*cog[1]  ],\n            [ 0             , volume         , 0               ,\n             -volume*cog[2] , 0              , volume*cog[0]   ],\n            [ 0             , 0              , volume          ,\n              volume*cog[1] , -volume*cog[0] , 0 ]             ,\n            [ 0             , -volume*cog[2] , volume*cog[1]   ,\n              inertias[0]   , -inertias[3]   , -inertias[5]    ],\n            [ volume*cog[2] , 0              , -volume*cog[0]  ,\n             -inertias[3]   , inertias[1]    , -inertias[4]    ],\n            [-volume*cog[1] , volume*cog[0]  , 0               ,\n             -inertias[5]   , -inertias[4]   , inertias[2]     ],\n        ])\n\n        density = rho if self.mass is None else self.mass/volume\n        inertia_matrix = density * volumic_inertia_matrix\n\n        # Rigid DOFs\n        rigid_dof_names = [\"Surge\", \"Sway\", \"Heave\", \"Roll\", \"Pitch\", \"Yaw\"]\n        rigid_inertia_matrix_xr = xr.DataArray(data=np.asarray(inertia_matrix),\n                            dims=['influenced_dof', 'radiating_dof'],\n                            coords={'influenced_dof': rigid_dof_names,\n                                    'radiating_dof': rigid_dof_names},\n                            name=\"inertia_matrix\")\n\n        # Body DOFs (Default as np.nan)\n        body_dof_names = list(self.dofs)\n        body_dof_count = len(body_dof_names)\n        other_dofs_inertia_matrix_xr = xr.DataArray(np.nan * np.zeros([body_dof_count, body_dof_count]),\n                                    dims=['influenced_dof', 'radiating_dof'],\n                                    coords={'influenced_dof': body_dof_names,\n                                            'radiating_dof': body_dof_names},\n                                    name=\"inertia_matrix\")\n\n        total_mass_xr = xr.merge([rigid_inertia_matrix_xr, other_dofs_inertia_matrix_xr], compat=\"override\").inertia_matrix\n\n        non_rigid_dofs = set(body_dof_names) - set(rigid_dof_names)\n\n        if output_type == \"body_dofs\":\n            if len(non_rigid_dofs) > 0:\n                LOG.warning(f\"Non-rigid dofs: {non_rigid_dofs} are detected and \\\nrespective inertia coefficients are assigned as NaN.\")\n\n            inertia_matrix_xr = total_mass_xr.sel(influenced_dof=body_dof_names,\n                                                  radiating_dof=body_dof_names)\n        elif output_type == \"rigid_dofs\":\n            inertia_matrix_xr = total_mass_xr.sel(influenced_dof=rigid_dof_names,\n                                                  radiating_dof=rigid_dof_names)\n        elif output_type == \"all_dofs\":\n            if len(non_rigid_dofs) > 0:\n                LOG.warning(\"Non-rigid dofs: {non_rigid_dofs} are detected and \\\nrespective inertia coefficients are assigned as NaN.\")\n\n            inertia_matrix_xr = total_mass_xr\n        else:\n            raise ValueError(f\"output_type should be either 'body_dofs', \\\n'all_dofs' or 'rigid_dofs'. Given output_type = '{output_type}'.\")\n\n        return inertia_matrix_xr",
  "def compute_hydrostatics(self, *, rho=1000.0, g=9.81, divergence=None):\n        \"\"\"Compute hydrostatics of the FloatingBody.\n\n        Parameters\n        ----------\n        rho : float, optional\n            Density of Water. The default is 1000.\n        g: float, optional\n            Gravity acceleration. The default is 9.81.\n        divergence : np.ndarray, optional\n            Divergence of the DOFs.\n\n        Returns\n        -------\n        hydrostatics : dict\n            All hydrostatics values of the FloatingBody.\n        \"\"\"\n        if self.center_of_mass is None:\n            raise ValueError(f\"Trying to compute hydrostatics for {self.name}, but no center of mass has been defined.\\n\"\n                             f\"Suggested solution: define a `center_of_mass` attribute for the FloatingBody {self.name}.\")\n\n        immersed_self = self.immersed_part()\n\n        full_mesh_vertices = self.mesh.vertices\n        coord_max = full_mesh_vertices.max(axis=0)\n        coord_min = full_mesh_vertices.min(axis=0)\n        full_length, full_breadth, depth = full_mesh_vertices.max(axis=0) - full_mesh_vertices.min(axis=0)\n\n        vertices = immersed_self.mesh.vertices\n        sub_length, sub_breadth, _ = vertices.max(axis=0) - vertices.min(axis=0)\n\n        if abs(immersed_self.waterplane_area) > 1e-10:\n            water_plane_idx = np.isclose(vertices[:,2], 0.0)\n            water_plane = vertices[water_plane_idx][:,:-1]\n            wl_length, wl_breadth = water_plane.max(axis=0) - water_plane.min(axis=0)\n        else:\n            wl_length, wl_breadth = 0.0, 0.0\n\n        hydrostatics = {}\n        hydrostatics[\"g\"] = g\n        hydrostatics[\"rho\"] = rho\n        hydrostatics[\"center_of_mass\"] = self.center_of_mass\n\n        hydrostatics[\"wet_surface_area\"] = immersed_self.wet_surface_area\n        hydrostatics[\"disp_volumes\"] = immersed_self.volumes\n        hydrostatics[\"disp_volume\"] = immersed_self.volume\n        hydrostatics[\"disp_mass\"] = immersed_self.disp_mass(rho=rho)\n        hydrostatics[\"center_of_buoyancy\"] = immersed_self.center_of_buoyancy\n        hydrostatics[\"waterplane_center\"] = np.append(immersed_self.waterplane_center, 0.0)\n        hydrostatics[\"waterplane_area\"] = immersed_self.waterplane_area\n        hydrostatics[\"transversal_metacentric_radius\"] = immersed_self.transversal_metacentric_radius\n        hydrostatics[\"longitudinal_metacentric_radius\"] = immersed_self.longitudinal_metacentric_radius\n        hydrostatics[\"transversal_metacentric_height\"] = immersed_self.transversal_metacentric_height\n        hydrostatics[\"longitudinal_metacentric_height\"] = immersed_self.longitudinal_metacentric_height\n        self.hydrostatic_stiffness = hydrostatics[\"hydrostatic_stiffness\"] = immersed_self.compute_hydrostatic_stiffness(\n            divergence=divergence, rho=rho, g=g)\n\n        hydrostatics[\"length_overall\"] = full_length\n        hydrostatics[\"breadth_overall\"] = full_breadth\n        hydrostatics[\"depth\"] = depth\n        hydrostatics[\"draught\"] = np.abs(coord_min[2])\n        hydrostatics[\"length_at_waterline\"] = wl_length\n        hydrostatics[\"breadth_at_waterline\"] = wl_breadth\n        hydrostatics[\"length_overall_submerged\"] = sub_length\n        hydrostatics[\"breadth_overall_submerged\"] = sub_breadth\n        if any(dof.lower() in {\"surge\", \"sway\", \"heave\", \"roll\", \"pitch\", \"yaw\"}\n               for dof in self.dofs) > 0: # If there is at least one rigid body dof:\n            self.inertia_matrix = hydrostatics[\"inertia_matrix\"] = self.compute_rigid_body_inertia(rho=rho)\n\n        return hydrostatics",
  "def __add__(self, body_to_add: 'FloatingBody') -> 'FloatingBody':\n        return self.join_bodies(body_to_add)",
  "def join_bodies(*bodies, name=None) -> 'FloatingBody':\n        if name is None:\n            name = \"+\".join(body.name for body in bodies)\n        meshes = CollectionOfMeshes([body.mesh for body in bodies], name=f\"{name}_mesh\")\n        dofs = FloatingBody.combine_dofs(bodies)\n\n        if all(body.mass is not None for body in bodies):\n            new_mass = sum(body.mass for body in bodies)\n        else:\n            new_mass = None\n\n        if (all(body.mass is not None for body in bodies)\n                and all(body.center_of_mass is not None for body in bodies)):\n            new_cog = sum(body.mass*np.asarray(body.center_of_mass) for body in bodies)/new_mass\n        else:\n            new_cog = None\n\n        joined_bodies = FloatingBody(\n            mesh=meshes, dofs=dofs, mass=new_mass, center_of_mass=new_cog, name=name\n            )\n\n        for matrix_name in [\"inertia_matrix\", \"hydrostatic_stiffness\"]:\n            if all(hasattr(body, matrix_name) for body in bodies):\n                from scipy.linalg import block_diag\n                setattr(joined_bodies, matrix_name, joined_bodies.add_dofs_labels_to_matrix(\n                        block_diag(*[getattr(body, matrix_name) for body in bodies])\n                        ))\n\n        return joined_bodies",
  "def combine_dofs(bodies) -> dict:\n        \"\"\"Combine the degrees of freedom of several bodies.\"\"\"\n        dofs = {}\n        cum_nb_faces = accumulate(chain([0], (body.mesh.nb_faces for body in bodies)))\n        total_nb_faces = sum(body.mesh.nb_faces for body in bodies)\n        for body, nbf in zip(bodies, cum_nb_faces):\n            # nbf is the cumulative number of faces of the previous subbodies,\n            # that is the offset of the indices of the faces of the current body.\n            for name, dof in body.dofs.items():\n                new_dof = np.zeros((total_nb_faces, 3))\n                new_dof[nbf:nbf+len(dof), :] = dof\n                if '__' not in name:\n                    new_dof_name = '__'.join([body.name, name])\n                else:\n                    # The body is probably a combination of bodies already.\n                    # So for the associativity of the + operation,\n                    # it is better to keep the same name.\n                    new_dof_name = name\n                dofs[new_dof_name] = new_dof\n        return dofs",
  "def copy(self, name=None) -> 'FloatingBody':\n        \"\"\"Return a deep copy of the body.\n\n        Parameters\n        ----------\n        name : str, optional\n            a name for the new copy\n        \"\"\"\n        new_body = copy.deepcopy(self)\n        if name is None:\n            new_body.name = f\"copy_of_{self.name}\"\n            LOG.debug(f\"Copy {self.name}.\")\n        else:\n            new_body.name = name\n            LOG.debug(f\"Copy {self.name} under the name {name}.\")\n        return new_body",
  "def assemble_regular_array(self, distance, nb_bodies):\n        \"\"\"Create an regular array of identical bodies.\n\n        Parameters\n        ----------\n        distance : float\n            Center-to-center distance between objects in the array\n        nb_bodies : couple of ints\n            Number of objects in the x and y directions.\n\n        Returns\n        -------\n        FloatingBody\n        \"\"\"\n        bodies = (self.translated((i*distance, j*distance, 0), name=f\"{i}_{j}\") for j in range(nb_bodies[1]) for i in range(nb_bodies[0]))\n        array = FloatingBody.join_bodies(*bodies)\n        array.mesh = build_regular_array_of_meshes(self.mesh, distance, nb_bodies)\n        array.name = f\"array_of_{self.name}\"\n        return array",
  "def assemble_arbitrary_array(self, locations:np.ndarray):\n\n        if not isinstance(locations, np.ndarray):\n            raise TypeError('locations must be of type np.ndarray')\n        assert locations.shape[1] == 2, 'locations must be of shape nx2, received {:}'.format(locations.shape)\n        n = locations.shape[0]\n\n        fb_list = []\n        for idx, li in enumerate(locations):\n            fb1 = self.copy()\n            fb1.translate(np.append(li,0))\n            fb1.name = 'arbitrary_array_body{:02d}'.format(idx)\n            fb_list.append(fb1)\n\n        arbitrary_array = fb_list[0].join_bodies(*fb_list[1:])\n\n        return arbitrary_array",
  "def extract_faces(self, id_faces_to_extract, return_index=False):\n        \"\"\"Create a new FloatingBody by extracting some faces from the mesh.\n        The dofs evolve accordingly.\n        \"\"\"\n        if isinstance(self.mesh, CollectionOfMeshes):\n            raise NotImplementedError  # TODO\n\n        if return_index:\n            new_mesh, id_v = Mesh.extract_faces(self.mesh, id_faces_to_extract, return_index)\n        else:\n            new_mesh = Mesh.extract_faces(self.mesh, id_faces_to_extract, return_index)\n        new_body = FloatingBody(new_mesh)\n        LOG.info(f\"Extract floating body from {self.name}.\")\n\n        new_body.dofs = {}\n        for name, dof in self.dofs.items():\n            new_body.dofs[name] = dof[id_faces_to_extract, :]\n\n        if return_index:\n            return new_body, id_v\n        else:\n            return new_body",
  "def sliced_by_plane(self, plane):\n        return FloatingBody(mesh=self.mesh.sliced_by_plane(plane), dofs=self.dofs, name=self.name)",
  "def minced(self, nb_slices=(8, 8, 4)):\n        \"\"\"Experimental method decomposing the mesh as a hierarchical structure.\n\n        Parameters\n        ----------\n        nb_slices: Tuple[int, int, int]\n            The number of slices in each of the x, y and z directions.\n            Only powers of 2 are supported at the moment.\n\n        Returns\n        -------\n        FloatingBody\n        \"\"\"\n        minced_body = self.copy()\n\n        # Extreme points of the mesh in each directions.\n        x_min, x_max, y_min, y_max, z_min, z_max = self.mesh.axis_aligned_bbox\n        sizes = [(x_min, x_max), (y_min, y_max), (z_min, z_max)]\n\n        directions = [np.array(d) for d in [(1, 0, 0), (0, 1, 0), (0, 0, 1)]]\n\n        def _slice_positions_at_depth(i):\n            \"\"\"Helper function.\n\n            Returns a list of floats as follows:\n            i=1 -> [1/2]\n            i=2 -> [1/4, 3/4]\n            i=3 -> [1/8, 3/8, 5/8, 7/8]\n                   ...\n            \"\"\"\n            denominator = 2**i\n            return [numerator/denominator for numerator in range(1, denominator, 2)]\n\n        # GENERATE ALL THE PLANES THAT WILL BE USED TO MINCE THE MESH\n        planes = []\n        for direction, nb_slices_in_dir, (min_coord, max_coord) in zip(directions, nb_slices, sizes):\n            planes_in_dir = []\n\n            depth_of_treelike_structure = int(np.log2(nb_slices_in_dir))\n            for i_depth in range(1, depth_of_treelike_structure+1):\n                planes_in_dir_at_depth = []\n                for relative_position in _slice_positions_at_depth(i_depth):\n                    slice_position = (min_coord + relative_position*(max_coord-min_coord))*direction\n                    plane = Plane(normal=direction, point=slice_position)\n                    planes_in_dir_at_depth.append(plane)\n                planes_in_dir.append(planes_in_dir_at_depth)\n            planes.append(planes_in_dir)\n\n        # SLICE THE MESH\n        intermingled_x_y_z = chain.from_iterable(zip_longest(*planes))\n        for planes in intermingled_x_y_z:\n            if planes is not None:\n                for plane in planes:\n                    minced_body = minced_body.sliced_by_plane(plane)\n        return minced_body",
  "def mirror(self, plane):\n        self.mesh.mirror(plane)\n        for dof in self.dofs:\n            self.dofs[dof] -= 2 * np.outer(np.dot(self.dofs[dof], plane.normal), plane.normal)\n        for point_attr in ('geometric_center', 'rotation_center', 'center_of_mass'):\n            if point_attr in self.__dict__ and self.__dict__[point_attr] is not None:\n                self.__dict__[point_attr] -= 2 * (np.dot(self.__dict__[point_attr], plane.normal) - plane.c) * plane.normal\n        return self",
  "def translate(self, vector, *args, **kwargs):\n        self.mesh.translate(vector, *args, **kwargs)\n        for point_attr in ('geometric_center', 'rotation_center', 'center_of_mass'):\n            if point_attr in self.__dict__ and self.__dict__[point_attr] is not None:\n                self.__dict__[point_attr] += vector\n        return self",
  "def rotate(self, axis, angle):\n        self.mesh.rotate(axis, angle)\n        for point_attr in ('geometric_center', 'rotation_center', 'center_of_mass'):\n            if point_attr in self.__dict__ and self.__dict__[point_attr] is not None:\n                self.__dict__[point_attr] = axis.rotate_points([self.__dict__[point_attr]], angle)\n        for dof in self.dofs:\n            self.dofs[dof] = axis.rotate_vectors(self.dofs[dof], angle)\n        return self",
  "def clip(self, plane):\n        # Clip mesh\n        LOG.info(f\"Clipping {self.name} with respect to {plane}\")\n        self.mesh.clip(plane)\n\n        # Clip dofs\n        ids = self.mesh._clipping_data['faces_ids']\n        for dof in self.dofs:\n            if len(ids) > 0:\n                self.dofs[dof] = np.array(self.dofs[dof])[ids]\n            else:\n                self.dofs[dof] = np.empty((0, 3))\n        return self",
  "def __short_str__(self):\n        return (f\"{self.__class__.__name__}(..., name=\\\"{self.name}\\\")\")",
  "def _optional_params_str(self):\n        items = []\n        if self.mass is not None: items.append(f\"mass={self.mass}, \")\n        if self.center_of_mass is not None: items.append(f\"center_of_mass={self.center_of_mass}, \")\n        return ''.join(items)",
  "def __str__(self):\n        short_dofs = '{' + ', '.join('\"{}\": ...'.format(d) for d in self.dofs) + '}'\n        return (f\"{self.__class__.__name__}(mesh={self.mesh.__short_str__()}, dofs={short_dofs}, {self._optional_params_str()}name=\\\"{self.name}\\\")\")",
  "def __repr__(self):\n        short_dofs = '{' + ', '.join('\"{}\": ...'.format(d) for d in self.dofs) + '}'\n        return (f\"{self.__class__.__name__}(mesh={str(self.mesh)}, dofs={short_dofs}, {self._optional_params_str()}name=\\\"{self.name}\\\")\")",
  "def _repr_pretty_(self, p, cycle):\n        p.text(self.__str__())",
  "def __rich_repr__(self):\n        class DofWithShortRepr:\n            def __repr__(self):\n                return '...'\n        yield \"mesh\", self.mesh\n        yield \"dofs\", {d: DofWithShortRepr() for d in self.dofs}\n        if self.mass is not None:\n            yield \"mass\", self.mass, None\n        if self.center_of_mass is not None:\n            yield \"center_of_mass\", tuple(self.center_of_mass)\n        yield \"name\", self.name",
  "def show(self, **kwargs):\n        from capytaine.ui.vtk.body_viewer import FloatingBodyViewer\n        viewer = FloatingBodyViewer()\n        viewer.add_body(self, **kwargs)\n        viewer.show()\n        viewer.finalize()",
  "def show_matplotlib(self, *args, **kwargs):\n        return self.mesh.show_matplotlib(*args, **kwargs)",
  "def animate(self, motion, *args, **kwargs):\n        \"\"\"Display a motion as a 3D animation.\n\n        Parameters\n        ==========\n        motion: dict or pd.Series or str\n            A dict or series mapping the name of the dofs to its amplitude.\n            If a single string is passed, it is assumed to be the name of a dof\n            and this dof with a unit amplitude will be displayed.\n        \"\"\"\n        from capytaine.ui.vtk.animation import Animation\n        if isinstance(motion, str):\n            motion = {motion: 1.0}\n        elif isinstance(motion, xr.DataArray):\n            motion = {k: motion.sel(radiating_dof=k).data for k in motion.coords[\"radiating_dof\"].data}\n\n        if any(dof not in self.dofs for dof in motion):\n            missing_dofs = set(motion.keys()) - set(self.dofs.keys())\n            raise ValueError(f\"Trying to animate the body {self.name} using dof(s) {missing_dofs}, but no dof of this name is defined for {self.name}.\")\n\n        animation = Animation(*args, **kwargs)\n        animation._add_actor(self.mesh.merged(), faces_motion=sum(motion[dof_name] * dof for dof_name, dof in self.dofs.items() if dof_name in motion))\n        return animation",
  "def minimal_computable_wavelength(self):\n        \"\"\"For accuracy of the resolution, wavelength should not be smaller than this value.\"\"\"\n        return 8*self.mesh.faces_radiuses.max()",
  "def divergence_dof(influenced_dof):\n            if divergence is None:\n                return 0.0\n            elif isinstance(divergence, dict) and influenced_dof in divergence.keys():\n                return divergence[influenced_dof]\n            elif isinstance(divergence, xr.DataArray) and influenced_dof in divergence.coords[\"influenced_dof\"]:\n                return divergence.sel(influenced_dof=influenced_dof).values\n            else:\n                LOG.warning(\"Computing hydrostatic stiffness without the divergence of {}\".format(influenced_dof))\n                return 0.0",
  "def _slice_positions_at_depth(i):\n            \"\"\"Helper function.\n\n            Returns a list of floats as follows:\n            i=1 -> [1/2]\n            i=2 -> [1/4, 3/4]\n            i=3 -> [1/8, 3/8, 5/8, 7/8]\n                   ...\n            \"\"\"\n            denominator = 2**i\n            return [numerator/denominator for numerator in range(1, denominator, 2)]",
  "class DofWithShortRepr:\n            def __repr__(self):\n                return '...'",
  "def __repr__(self):\n                return '...'",
  "class RigidBodyDofsPlaceholder:\n    \"\"\"Pass an instance of this class to the FloatingBody initializer to initialize the 6 ridig body dofs.\"\"\"\n\n    def __init__(self, rotation_center=None):\n        self.rotation_center = rotation_center\n\n    def __str__(self):\n        return \"RigidBodyDofsPlaceholder()\"\n\n    def _repr_pretty_(self, p, cycle):\n        p.text(self.__str__())",
  "def rigid_body_dofs(rotation_center=None):\n    return RigidBodyDofsPlaceholder(rotation_center=rotation_center)",
  "def __init__(self, rotation_center=None):\n        self.rotation_center = rotation_center",
  "def __str__(self):\n        return \"RigidBodyDofsPlaceholder()\"",
  "def _repr_pretty_(self, p, cycle):\n        p.text(self.__str__())",
  "class Sphere(FloatingBody):\n    \"\"\"Sphere\n    Deprecated: please prefer capytaine.meshes.predefined.mesh_sphere()\n\n    Parameters\n    ----------\n    radius : float\n        radius of the sphere\n    center : 3-ple or array of shape (3,)\n        position of the geometric center of the sphere\n    ntheta : int\n        number of panels along a meridian (or number of parallels-1)\n    nphi : int\n        number of panels along a parallel (or number of meridians-1)\n    axial_symmetry : bool\n        if True, use the axial symmetry to build the mesh (default: True)\n    clip_free_surface : bool\n        if True, only mesh the part of the sphere where z < 0 (default: False),\n        can be used with center to obtain any clipped sphere,\n        if True, then ntheta is the number of parallel below the free surface.\n    name : string\n        a name identifying the sphere (default: \"sphere_id\" where id is an unique integer).\n    \"\"\"\n\n    def __init__(self, *, radius=1.0, center=(0, 0, 0),\n                 ntheta=10, nphi=10, clip_free_surface=False,\n                 axial_symmetry=True, clever=None,\n                 name=None):\n\n        LOG.warning(\"Deprecation warning: The class Sphere() is deprecated. \"\n                \"Please prefer the function capytaine.meshes.predefined.mesh_sphere()\")\n\n        if clever is not None:\n            LOG.warning(\"Deprecation warning: `clever` argument for Sphere is deprecated. \"\n                        \"Use `axial_symmetry` instead.\")\n\n        if name is None:\n            name = f\"sphere_{next(Mesh._ids)}\"\n\n        if clip_free_surface:\n            if center[2] < -radius:  # fully immersed\n                pass\n            elif center[2] < radius:\n                ntheta = int(ntheta*np.pi/np.arccos(center[2]/radius))\n            else:\n                raise ValueError(\"Impossible to mesh the immersed hull of a sphere completely out of the water\")\n\n        mesh = mesh_sphere(radius=radius, center=center, resolution=(ntheta, nphi), axial_symmetry=axial_symmetry, name=f\"{name}_mesh\")\n\n        if clip_free_surface:\n            mesh.keep_immersed_part()\n\n        self.radius = radius\n        self.geometric_center = np.array(center, dtype=float)\n        FloatingBody.__init__(self, mesh=mesh, name=name)",
  "def __init__(self, *, radius=1.0, center=(0, 0, 0),\n                 ntheta=10, nphi=10, clip_free_surface=False,\n                 axial_symmetry=True, clever=None,\n                 name=None):\n\n        LOG.warning(\"Deprecation warning: The class Sphere() is deprecated. \"\n                \"Please prefer the function capytaine.meshes.predefined.mesh_sphere()\")\n\n        if clever is not None:\n            LOG.warning(\"Deprecation warning: `clever` argument for Sphere is deprecated. \"\n                        \"Use `axial_symmetry` instead.\")\n\n        if name is None:\n            name = f\"sphere_{next(Mesh._ids)}\"\n\n        if clip_free_surface:\n            if center[2] < -radius:  # fully immersed\n                pass\n            elif center[2] < radius:\n                ntheta = int(ntheta*np.pi/np.arccos(center[2]/radius))\n            else:\n                raise ValueError(\"Impossible to mesh the immersed hull of a sphere completely out of the water\")\n\n        mesh = mesh_sphere(radius=radius, center=center, resolution=(ntheta, nphi), axial_symmetry=axial_symmetry, name=f\"{name}_mesh\")\n\n        if clip_free_surface:\n            mesh.keep_immersed_part()\n\n        self.radius = radius\n        self.geometric_center = np.array(center, dtype=float)\n        FloatingBody.__init__(self, mesh=mesh, name=name)",
  "class Rectangle(FloatingBody):\n    \"\"\"One-sided vertical rectangle (along y and z).\n\n    By default, the normals are oriented in the positive y direction.\n\n    Parameters\n    ----------\n    size : couple of floats, optional\n        dimensions of the rectangle (width and height)\n    resolution : couple of ints, optional\n        number of faces along each of the two directions\n    center : 3-ple of floats, optional\n        position of the geometric center of the rectangle, default: (0, 0, 0)\n    normal: 3-ple of floats, optional\n        normal vector, default: along x axis\n    translational_symmetry : bool, optional\n        if True, use the translation symmetry to speed up the computations\n    reflection_symmetry : bool, optional\n        if True, use the reflection symmetry to speed up the computations\n    name : string, optional\n        a name for the body\n    \"\"\"\n\n    def __init__(self, size=(5.0, 5.0), resolution=(5, 5),\n                 center=(0, 0, 0), normal=(1, 0, 0),\n                 translational_symmetry=False, reflection_symmetry=False, name=None):\n\n        LOG.warning(\"Deprecation warning: The class Rectangle() is deprecated. \"\n                    \"Please prefer the function capytaine.meshes.predefined.mesh_rectangle()\")\n\n        self.size = np.asarray(size, dtype=float)\n        self.geometric_center = np.asarray(center, dtype=float)\n\n        if name is None:\n            name = f\"rectangle_{next(Mesh._ids)}\"\n\n        mesh = mesh_rectangle(size=size, resolution=resolution, center=center, normal=normal,\n                translation_symmetry=translational_symmetry, reflection_symmetry=reflection_symmetry,\n                name=f\"{name}_mesh\")\n        FloatingBody.__init__(self, mesh=mesh, name=name)",
  "class RectangularParallelepiped(FloatingBody):\n    \"\"\"Six rectangles forming a parallelepiped.\n\n    Parameters\n    ----------\n    size : 3-ple of floats, optional\n        dimensions of the parallelepiped (width, thickness, height) for coordinates (x, y, z).\n    resolution : 3-ple of ints, optional\n        number of faces along the three directions\n    center : 3-ple of floats, optional\n        coordinates of the geometric center of the parallelepiped\n    top: bool, optional\n        whether or not to close the parallelepiped on the top\n    bottom: bool, optional\n        whether or not to close the parallelepiped on the bottom\n    reflection_symmetry : bool, optional\n        use xOz and yOz symmetry plane to generate the mesh\n    translational_symmetry : bool, optional\n        if True, use the translation symmetry in the x direction to speed up the computations.\n        To use the translation symmetry in the y direction, create a x-symmetric body and then rotate it by pi/2.\n    name : string, optional\n        a name for the body\n    \"\"\"\n\n    def __init__(self,\n                 size=(1.0, 1.0, 1.0), resolution=(4, 4, 4),\n                 center=(0, 0, 0),\n                 top=True, bottom=True,\n                 reflection_symmetry=False,\n                 translational_symmetry=False,\n                 name=None):\n\n        LOG.warning(\"Deprecation warning: The class RectangularParallelepiped() is deprecated. \"\n                    \"Please prefer the function capytaine.meshes.predefined.mesh_parallelepiped()\")\n\n        if name is None:\n            name = f\"rectangular_parallelepiped_{next(Mesh._ids)}\"\n\n        missing_sides = set()\n        if not top: missing_sides.add(\"top\")\n        if not bottom: missing_sides.add(\"bottom\")\n\n        mesh = mesh_parallelepiped(size=size, resolution=resolution, center=center,\n                missing_sides=missing_sides,\n                translation_symmetry=translational_symmetry, reflection_symmetry=reflection_symmetry,\n                name=f\"{name}_mesh\")\n\n        FloatingBody.__init__(self, mesh=mesh, name=name)",
  "class OpenRectangularParallelepiped(RectangularParallelepiped):\n    def __init__(self, *args, **kwargs):\n        RectangularParallelepiped.__init__(self, top=False, bottom=False, *args, **kwargs)",
  "def __init__(self, size=(5.0, 5.0), resolution=(5, 5),\n                 center=(0, 0, 0), normal=(1, 0, 0),\n                 translational_symmetry=False, reflection_symmetry=False, name=None):\n\n        LOG.warning(\"Deprecation warning: The class Rectangle() is deprecated. \"\n                    \"Please prefer the function capytaine.meshes.predefined.mesh_rectangle()\")\n\n        self.size = np.asarray(size, dtype=float)\n        self.geometric_center = np.asarray(center, dtype=float)\n\n        if name is None:\n            name = f\"rectangle_{next(Mesh._ids)}\"\n\n        mesh = mesh_rectangle(size=size, resolution=resolution, center=center, normal=normal,\n                translation_symmetry=translational_symmetry, reflection_symmetry=reflection_symmetry,\n                name=f\"{name}_mesh\")\n        FloatingBody.__init__(self, mesh=mesh, name=name)",
  "def __init__(self,\n                 size=(1.0, 1.0, 1.0), resolution=(4, 4, 4),\n                 center=(0, 0, 0),\n                 top=True, bottom=True,\n                 reflection_symmetry=False,\n                 translational_symmetry=False,\n                 name=None):\n\n        LOG.warning(\"Deprecation warning: The class RectangularParallelepiped() is deprecated. \"\n                    \"Please prefer the function capytaine.meshes.predefined.mesh_parallelepiped()\")\n\n        if name is None:\n            name = f\"rectangular_parallelepiped_{next(Mesh._ids)}\"\n\n        missing_sides = set()\n        if not top: missing_sides.add(\"top\")\n        if not bottom: missing_sides.add(\"bottom\")\n\n        mesh = mesh_parallelepiped(size=size, resolution=resolution, center=center,\n                missing_sides=missing_sides,\n                translation_symmetry=translational_symmetry, reflection_symmetry=reflection_symmetry,\n                name=f\"{name}_mesh\")\n\n        FloatingBody.__init__(self, mesh=mesh, name=name)",
  "def __init__(self, *args, **kwargs):\n        RectangularParallelepiped.__init__(self, top=False, bottom=False, *args, **kwargs)",
  "class Disk(FloatingBody):\n    \"\"\"(One-sided) disk.\n    Deprecated: please prefer capytaine.meshes.predefined.mesh_disk()\n\n    Parameters\n    ----------\n    radius : float, optional\n        radius of the disk\n    resolution : 2-ple of int, optional\n        number of panels along a radius and around the disk\n    center : 3-ple or array of shape (3,), optional\n        position of the geometric center of the disk\n    normal: 3-ple of floats, optional\n        normal vector, default: along x axis\n    axial_symmetry : bool, optional\n        if True, use the axial symmetry to speed up the computations\n    reflection_symmetry : bool, optional\n        if True, use the reflection symmetry to speed up the computations\n    name : str, optional\n        a string naming the floating body\n    \"\"\"\n\n    def __init__(self, radius=1.0, resolution=(3, 5),\n                 center=(0, 0, 0), normal=(1, 0, 0),\n                 reflection_symmetry=False, axial_symmetry=False,\n                 name=None):\n        LOG.warning(\"Deprecation warning: The class Disk() is deprecated. \"\n                \"Please prefer the function capytaine.meshes.predefined.mesh_disk()\")\n\n        if name is None:\n            name = f\"disk_{next(Mesh._ids)}\"\n\n        self.radius = float(radius)\n        self.geometric_center = np.asarray(center, dtype=float)\n        mesh = mesh_disk(radius=radius, center=center, normal=normal, resolution=resolution,\n                reflection_symmetry=reflection_symmetry, axial_symmetry=axial_symmetry, name=f\"{name}_mesh\")\n        FloatingBody.__init__(self, mesh=mesh, name=name)",
  "class HorizontalCylinder(FloatingBody):\n    \"\"\"Horizontal cylinder\n    Deprecated: please prefer capytaine.meshes.predefined.mesh_horizontal_cylinder()\n\n    Parameters\n    ----------\n    length : float, optional\n        length of the cylinder\n    radius : float, optional\n        radius of the cylinder\n    center : 3-ple or array of shape (3,), optional\n        position of the geometric center of the cylinder\n    nx : int, optional\n        number of circular slices\n    ntheta : int, optional\n        number of panels along a circular slice of the cylinder\n    nr : int, optional\n        number of panels along a radius on the extremities of the cylinder\n    reflection_symmetry : bool, optional\n        if True, returns a ReflectionSymmetricMesh\n    translation_symmetry : bool, optional\n        if True, uses a TranslationalSymmetricMesh internally for the main part of the cylinder\n    name : str, optional\n        a string naming the floating body\n    \"\"\"\n\n    def __init__(self, length=10.0, radius=1.0, center=(0, 0, 0),\n                 nx=10, ntheta=10, nr=2,\n                 reflection_symmetry=True, translation_symmetry=False,\n                 clever=None,\n                 name=None):\n\n        LOG.warning(\"Deprecation warning: The class HorizontalCylinder() is deprecated. \"\n                \"Please prefer the function capytaine.meshes.predefined.mesh_horizontal_cylinder()\")\n\n        self.length = length\n        self.radius = radius\n        self.geometric_center = np.asarray(center, dtype=float)\n\n        if name is None:\n            name = f\"cylinder_{next(Mesh._ids)}\"\n\n        mesh = mesh_horizontal_cylinder(length=length, radius=radius, center=center,\n                resolution=(nr, ntheta, nx), reflection_symmetry=reflection_symmetry,\n                translation_symmetry=translation_symmetry, name=f\"{name}_mesh\")\n        FloatingBody.__init__(self, mesh=mesh, name=name)",
  "class VerticalCylinder(FloatingBody):\n    \"\"\"Vertical cylinder.\n    Deprecated: please prefer capytaine.meshes.predefined.mesh_vertical_cylinder()\n\n    Parameters\n    ----------\n    length : float, optional\n        length of the cylinder\n    radius : float, optional\n        radius of the cylinder\n    center : 3-ple or array of shape (3,), optional\n        position of the geometric center of the cylinder\n    nx : int, optional\n        number of circular slices\n    ntheta : int, optional\n        number of panels along a circular slice of the cylinder\n    nr : int, optional\n        number of panels along a radius on the extremities of the cylinder\n    clever : bool, optional\n        if True, uses the mesh symmetries\n    name : str, optional\n        a string naming the floating body\n    \"\"\"\n\n    def __init__(self, length=10.0, radius=1.0, center=(0, 0, 0),\n                 nx=10, ntheta=10, nr=2,\n                 clever=True, name=None):\n        LOG.warning(\"Deprecation warning: The class VerticalCylinder() is deprecated. \"\n                \"Please prefer the function capytaine.meshes.predefined.mesh_vertical_cylinder()\")\n\n        self.length = length\n        self.radius = radius\n        self.geometric_center = np.asarray(center, dtype=float)\n\n        if name is None:\n            name = f\"cylinder_{next(Mesh._ids)}\"\n\n        mesh = mesh_vertical_cylinder(length=length, radius=radius, center=center,\n                resolution=(nr, ntheta, nx), reflection_symmetry=False,\n                axial_symmetry=clever, name=f\"{name}_mesh\")\n\n        FloatingBody.__init__(self, mesh=mesh, name=name)",
  "def __init__(self, radius=1.0, resolution=(3, 5),\n                 center=(0, 0, 0), normal=(1, 0, 0),\n                 reflection_symmetry=False, axial_symmetry=False,\n                 name=None):\n        LOG.warning(\"Deprecation warning: The class Disk() is deprecated. \"\n                \"Please prefer the function capytaine.meshes.predefined.mesh_disk()\")\n\n        if name is None:\n            name = f\"disk_{next(Mesh._ids)}\"\n\n        self.radius = float(radius)\n        self.geometric_center = np.asarray(center, dtype=float)\n        mesh = mesh_disk(radius=radius, center=center, normal=normal, resolution=resolution,\n                reflection_symmetry=reflection_symmetry, axial_symmetry=axial_symmetry, name=f\"{name}_mesh\")\n        FloatingBody.__init__(self, mesh=mesh, name=name)",
  "def __init__(self, length=10.0, radius=1.0, center=(0, 0, 0),\n                 nx=10, ntheta=10, nr=2,\n                 reflection_symmetry=True, translation_symmetry=False,\n                 clever=None,\n                 name=None):\n\n        LOG.warning(\"Deprecation warning: The class HorizontalCylinder() is deprecated. \"\n                \"Please prefer the function capytaine.meshes.predefined.mesh_horizontal_cylinder()\")\n\n        self.length = length\n        self.radius = radius\n        self.geometric_center = np.asarray(center, dtype=float)\n\n        if name is None:\n            name = f\"cylinder_{next(Mesh._ids)}\"\n\n        mesh = mesh_horizontal_cylinder(length=length, radius=radius, center=center,\n                resolution=(nr, ntheta, nx), reflection_symmetry=reflection_symmetry,\n                translation_symmetry=translation_symmetry, name=f\"{name}_mesh\")\n        FloatingBody.__init__(self, mesh=mesh, name=name)",
  "def __init__(self, length=10.0, radius=1.0, center=(0, 0, 0),\n                 nx=10, ntheta=10, nr=2,\n                 clever=True, name=None):\n        LOG.warning(\"Deprecation warning: The class VerticalCylinder() is deprecated. \"\n                \"Please prefer the function capytaine.meshes.predefined.mesh_vertical_cylinder()\")\n\n        self.length = length\n        self.radius = radius\n        self.geometric_center = np.asarray(center, dtype=float)\n\n        if name is None:\n            name = f\"cylinder_{next(Mesh._ids)}\"\n\n        mesh = mesh_vertical_cylinder(length=length, radius=radius, center=center,\n                resolution=(nr, ntheta, nx), reflection_symmetry=False,\n                axial_symmetry=clever, name=f\"{name}_mesh\")\n\n        FloatingBody.__init__(self, mesh=mesh, name=name)",
  "class AbstractGreenFunction(ABC):\n    \"\"\"Abstract method to evaluate the Green function.\"\"\"\n\n    @abstractmethod\n    def evaluate(self, mesh1, mesh2, free_surface, sea_bottom, wavenumber):\n        pass",
  "def evaluate(self, mesh1, mesh2, free_surface, sea_bottom, wavenumber):\n        pass",
  "class Delhommeau(AbstractGreenFunction):\n    \"\"\"The Green function as implemented in Aquadyn and Nemoh.\n\n    Parameters\n    ----------\n    tabulation_nr: int, optional\n        Number of tabulation points for horizontal distance.\n        If 0 is given, the no tabulation is used.\n        Default: 328, as in Nemoh.\n    tabulation_nz: int, optional\n        Number of tabulation points for vertical distance.\n        If 0 is given, the no tabulation is used.\n        Default: 46, as in Nemoh.\n    tabulation_nb_integration_points: int, optional\n        Number of points for the numerical integration w.r.t. :math:`theta` of Delhommeau's integrals\n        Default: 251, as in Nemoh.\n    finite_depth_prony_decomposition_method: string, optional\n        The implementation of the Prony decomposition used to compute the finite water_depth Green function.\n        Accepted values: :code:`'fortran'` for Nemoh's implementation (by default), :code:`'python'` for an experimental Python implementation.\n        See :func:`find_best_exponential_decomposition`.\n    floating_point_precision: string, optional\n        Either :code:`'float32'` for single precision computations or :code:`'float64'` for double precision computations.\n        Default: :code:`'float64'`.\n\n    Attributes\n    ----------\n    tabulated_r_range: numpy.array of shape (tabulation_nr,) and type floating_point_precision\n    tabulated_z_range: numpy.array of shape (tabulation_nz,) and type floating_point_precision\n        Coordinates of the tabulation points.\n    tabulated_integrals: numpy.array of shape (tabulation_nr, tabulation_nz, 2, 2) and type floating_point_precision\n        Tabulated Delhommeau integrals.\n    \"\"\"\n\n    fortran_core_basename = \"Delhommeau\"\n\n    def __init__(self, *,\n                 tabulation_nr=400,\n                 tabulation_nz=80,\n                 tabulation_nb_integration_points=251,\n                 finite_depth_prony_decomposition_method='fortran',\n                 floating_point_precision='float64',\n                 ):\n\n        self.fortran_core = import_module(f\"capytaine.green_functions.libs.{self.fortran_core_basename}_{floating_point_precision}\")\n\n        self.tabulated_r_range = self.fortran_core.delhommeau_integrals.default_r_spacing(tabulation_nr)\n        self.tabulated_z_range = self.fortran_core.delhommeau_integrals.default_z_spacing(tabulation_nz)\n        self.tabulated_integrals = self.fortran_core.delhommeau_integrals.construct_tabulation(\n                self.tabulated_r_range, self.tabulated_z_range, tabulation_nb_integration_points\n                )\n        self.floating_point_precision = floating_point_precision\n\n        self.finite_depth_prony_decomposition_method = finite_depth_prony_decomposition_method\n\n        self.exportable_settings = {\n            'green_function': self.__class__.__name__,\n            'tabulation_nr': tabulation_nr,\n            'tabulation_nz': tabulation_nz,\n            'tabulation_nb_integration_points': tabulation_nb_integration_points,\n            'finite_depth_prony_decomposition_method': finite_depth_prony_decomposition_method,\n            'floating_point_precision': floating_point_precision,\n        }\n\n        self._hash = hash(self.exportable_settings.values())\n\n    def __hash__(self):\n        return self._hash\n\n    def __str__(self):\n        params = f\"tabulation_nz={self.exportable_settings['tabulation_nz']}\"\n        params += f\", tabulation_nr={self.exportable_settings['tabulation_nr']}\"\n        params += f\", tabulation_nb_integration_points={self.exportable_settings['tabulation_nb_integration_points']}\"\n        params += f\", floating_point_precision=\\'{self.exportable_settings['floating_point_precision']}\\'\"\n        return f\"{self.__class__.__name__}({params})\"\n\n    def __repr__(self):\n        return self.__str__()\n\n    def _repr_pretty_(self, p, cycle):\n        p.text(self.__str__())\n\n    @lru_cache(maxsize=128)\n    def find_best_exponential_decomposition(self, dimensionless_omega, dimensionless_wavenumber):\n        \"\"\"Compute the decomposition of a part of the finite water_depth Green function as a sum of exponential functions.\n\n        Two implementations are available: the legacy Fortran implementation from Nemoh and a newer one written in Python.\n        For some still unexplained reasons, the two implementations do not always give the exact same result.\n        Until the problem is better understood, the Fortran implementation is the default one, to ensure consistency with Nemoh.\n        The Fortran version is also significantly faster...\n\n        Results are cached.\n\n        Parameters\n        ----------\n        dimensionless_omega: float\n            dimensionless angular frequency: :math:`kh \\\\tanh (kh) = \\\\omega^2 h/g`\n        dimensionless_wavenumber: float\n            dimensionless wavenumber: :math:`kh`\n        method: string, optional\n            the implementation that should be used to compute the Prony decomposition\n\n        Returns\n        -------\n        Tuple[np.ndarray, np.ndarray]\n            the amplitude and growth rates of the exponentials\n        \"\"\"\n\n        LOG.debug(f\"\\tCompute Prony decomposition in finite water_depth Green function \"\n                  f\"for dimless_omega=%.2e and dimless_wavenumber=%.2e\",\n                  dimensionless_omega, dimensionless_wavenumber)\n\n        if self.finite_depth_prony_decomposition_method.lower() == 'python':\n            # The function that will be approximated.\n            @np.vectorize\n            def f(x):\n                return self.fortran_core.initialize_green_wave.ff(x, dimensionless_omega, dimensionless_wavenumber)\n\n            # Try different increasing number of exponentials\n            for n_exp in range(4, 31, 2):\n\n                # The coefficients are computed on a resolution of 4*n_exp+1 ...\n                X = np.linspace(-0.1, 20.0, 4*n_exp+1)\n                a, lamda = exponential_decomposition(X, f(X), n_exp)\n\n                # ... and they are evaluated on a finer discretization.\n                X = np.linspace(-0.1, 20.0, 8*n_exp+1)\n                if error_exponential_decomposition(X, f(X), a, lamda) < 1e-4:\n                    break\n\n            else:\n                LOG.warning(\"No suitable exponential decomposition has been found\"\n                            \"for dimless_omega=%.2e and dimless_wavenumber=%.2e\",\n                            dimensionless_omega, dimensionless_wavenumber)\n\n        elif self.finite_depth_prony_decomposition_method.lower() == 'fortran':\n            lamda, a, nexp = self.fortran_core.old_prony_decomposition.lisc(dimensionless_omega, dimensionless_wavenumber)\n            lamda = lamda[:nexp]\n            a = a[:nexp]\n\n        else:\n            raise ValueError(\"Unrecognized method name for the Prony decomposition.\")\n\n        # Add one more exponential function (actually a constant).\n        # It is not clear where it comes from exactly in the theory...\n        a = np.concatenate([a, np.array([2])])\n        lamda = np.concatenate([lamda, np.array([0.0])])\n\n        return a, lamda\n\n    def evaluate(self, mesh1, mesh2, free_surface=0.0, water_depth=np.infty, wavenumber=1.0, early_dot_product=True):\n        r\"\"\"The main method of the class, called by the engine to assemble the influence matrices.\n\n        Parameters\n        ----------\n        mesh1: Mesh or CollectionOfMeshes or list of points\n            mesh of the receiving body (where the potential is measured)\n            if only S is wanted or early_dot_product is False, then only a list of points as an array of shape (n, 3) can be passed.\n        mesh2: Mesh or CollectionOfMeshes\n            mesh of the source body (over which the source distribution is integrated)\n        free_surface: float, optional\n            position of the free surface (default: :math:`z = 0`)\n        water_depth: float, optional\n            constant depth of water (default: :math:`+\\infty`)\n        wavenumber: float, optional\n            wavenumber (default: 1.0)\n        early_dot_product: boolean, optional\n            if False, return K as a (n, m, 3) array storing \u222b\u2207G\n            if True, return K as a (n, m) array storing \u222b\u2207G\u00b7n\n\n        Returns\n        -------\n        tuple of numpy arrays\n            the matrices :math:`S` and :math:`K`\n        \"\"\"\n\n        if free_surface == np.infty: # No free surface, only a single Rankine source term\n\n            a_exp, lamda_exp = np.empty(1), np.empty(1)  # Dummy arrays that won't actually be used by the fortran code.\n\n            coeffs = np.array((1.0, 0.0, 0.0))\n\n        elif water_depth == np.infty:\n\n            a_exp, lamda_exp = np.empty(1), np.empty(1)  # Idem\n\n            if wavenumber == 0.0:\n                coeffs = np.array((1.0, 1.0, 0.0))\n            elif wavenumber == np.infty:\n                coeffs = np.array((1.0, -1.0, 0.0))\n            else:\n                coeffs = np.array((1.0, 1.0, 1.0))\n\n        else:  # Finite water_depth\n            a_exp, lamda_exp = self.find_best_exponential_decomposition(\n                wavenumber*water_depth*np.tanh(wavenumber*water_depth),\n                wavenumber*water_depth,\n            )\n            if wavenumber == 0.0:\n                raise NotImplementedError\n            elif wavenumber == np.infty:\n                raise NotImplementedError\n            else:\n                coeffs = np.array((1.0, 1.0, 1.0))\n\n        if isinstance(mesh1, Mesh) or isinstance(mesh1, CollectionOfMeshes):\n            collocation_points = mesh1.faces_centers\n            nb_collocation_points = mesh1.nb_faces\n            early_dot_product_normals = mesh1.faces_normals\n        elif isinstance(mesh1, np.ndarray) and mesh1.ndim ==2 and mesh1.shape[1] == 3:\n            collocation_points = mesh1\n            nb_collocation_points = mesh1.shape[0]\n            early_dot_product_normals = np.zeros((nb_collocation_points, 3))  # Hopefully unused\n        else:\n            raise ValueError(f\"Unrecognized input for {self.__class__.__name__}.evaluate\")\n\n        if self.floating_point_precision == \"float32\":\n            dtype = \"complex64\"\n        elif self.floating_point_precision == \"float64\":\n            dtype = \"complex128\"\n        else:\n            raise NotImplementedError\n\n        S = np.empty((nb_collocation_points, mesh2.nb_faces), order=\"F\", dtype=dtype)\n        K = np.empty((nb_collocation_points, mesh2.nb_faces, 1 if early_dot_product else 3), order=\"F\", dtype=dtype)\n\n        # Main call to Fortran code\n        self.fortran_core.matrices.build_matrices(\n            collocation_points,  early_dot_product_normals,\n            mesh2.vertices,      mesh2.faces + 1,\n            mesh2.faces_centers, mesh2.faces_normals,\n            mesh2.faces_areas,   mesh2.faces_radiuses,\n            *mesh2.quadrature_points,\n            wavenumber, water_depth,\n            coeffs,\n            self.tabulated_r_range, self.tabulated_z_range, self.tabulated_integrals,\n            lamda_exp, a_exp,\n            mesh1 is mesh2,\n            S, K\n        )\n\n        if np.any(np.isnan(S)) or np.any(np.isnan(K)):\n            raise RuntimeError(\"Green function returned a NaN in the interaction matrix.\\n\"\n                    \"It could be due to overlapping panels.\")\n\n        if early_dot_product: K = K.reshape((nb_collocation_points, mesh2.nb_faces))\n\n        return S, K",
  "class XieDelhommeau(Delhommeau):\n    \"\"\"Variant of Nemoh's Green function, more accurate near the free surface.\n\n    Same arguments and methods as :class:`Delhommeau`.\n    \"\"\"\n\n    fortran_core_basename = \"XieDelhommeau\"",
  "def __init__(self, *,\n                 tabulation_nr=400,\n                 tabulation_nz=80,\n                 tabulation_nb_integration_points=251,\n                 finite_depth_prony_decomposition_method='fortran',\n                 floating_point_precision='float64',\n                 ):\n\n        self.fortran_core = import_module(f\"capytaine.green_functions.libs.{self.fortran_core_basename}_{floating_point_precision}\")\n\n        self.tabulated_r_range = self.fortran_core.delhommeau_integrals.default_r_spacing(tabulation_nr)\n        self.tabulated_z_range = self.fortran_core.delhommeau_integrals.default_z_spacing(tabulation_nz)\n        self.tabulated_integrals = self.fortran_core.delhommeau_integrals.construct_tabulation(\n                self.tabulated_r_range, self.tabulated_z_range, tabulation_nb_integration_points\n                )\n        self.floating_point_precision = floating_point_precision\n\n        self.finite_depth_prony_decomposition_method = finite_depth_prony_decomposition_method\n\n        self.exportable_settings = {\n            'green_function': self.__class__.__name__,\n            'tabulation_nr': tabulation_nr,\n            'tabulation_nz': tabulation_nz,\n            'tabulation_nb_integration_points': tabulation_nb_integration_points,\n            'finite_depth_prony_decomposition_method': finite_depth_prony_decomposition_method,\n            'floating_point_precision': floating_point_precision,\n        }\n\n        self._hash = hash(self.exportable_settings.values())",
  "def __hash__(self):\n        return self._hash",
  "def __str__(self):\n        params = f\"tabulation_nz={self.exportable_settings['tabulation_nz']}\"\n        params += f\", tabulation_nr={self.exportable_settings['tabulation_nr']}\"\n        params += f\", tabulation_nb_integration_points={self.exportable_settings['tabulation_nb_integration_points']}\"\n        params += f\", floating_point_precision=\\'{self.exportable_settings['floating_point_precision']}\\'\"\n        return f\"{self.__class__.__name__}({params})\"",
  "def __repr__(self):\n        return self.__str__()",
  "def _repr_pretty_(self, p, cycle):\n        p.text(self.__str__())",
  "def find_best_exponential_decomposition(self, dimensionless_omega, dimensionless_wavenumber):\n        \"\"\"Compute the decomposition of a part of the finite water_depth Green function as a sum of exponential functions.\n\n        Two implementations are available: the legacy Fortran implementation from Nemoh and a newer one written in Python.\n        For some still unexplained reasons, the two implementations do not always give the exact same result.\n        Until the problem is better understood, the Fortran implementation is the default one, to ensure consistency with Nemoh.\n        The Fortran version is also significantly faster...\n\n        Results are cached.\n\n        Parameters\n        ----------\n        dimensionless_omega: float\n            dimensionless angular frequency: :math:`kh \\\\tanh (kh) = \\\\omega^2 h/g`\n        dimensionless_wavenumber: float\n            dimensionless wavenumber: :math:`kh`\n        method: string, optional\n            the implementation that should be used to compute the Prony decomposition\n\n        Returns\n        -------\n        Tuple[np.ndarray, np.ndarray]\n            the amplitude and growth rates of the exponentials\n        \"\"\"\n\n        LOG.debug(f\"\\tCompute Prony decomposition in finite water_depth Green function \"\n                  f\"for dimless_omega=%.2e and dimless_wavenumber=%.2e\",\n                  dimensionless_omega, dimensionless_wavenumber)\n\n        if self.finite_depth_prony_decomposition_method.lower() == 'python':\n            # The function that will be approximated.\n            @np.vectorize\n            def f(x):\n                return self.fortran_core.initialize_green_wave.ff(x, dimensionless_omega, dimensionless_wavenumber)\n\n            # Try different increasing number of exponentials\n            for n_exp in range(4, 31, 2):\n\n                # The coefficients are computed on a resolution of 4*n_exp+1 ...\n                X = np.linspace(-0.1, 20.0, 4*n_exp+1)\n                a, lamda = exponential_decomposition(X, f(X), n_exp)\n\n                # ... and they are evaluated on a finer discretization.\n                X = np.linspace(-0.1, 20.0, 8*n_exp+1)\n                if error_exponential_decomposition(X, f(X), a, lamda) < 1e-4:\n                    break\n\n            else:\n                LOG.warning(\"No suitable exponential decomposition has been found\"\n                            \"for dimless_omega=%.2e and dimless_wavenumber=%.2e\",\n                            dimensionless_omega, dimensionless_wavenumber)\n\n        elif self.finite_depth_prony_decomposition_method.lower() == 'fortran':\n            lamda, a, nexp = self.fortran_core.old_prony_decomposition.lisc(dimensionless_omega, dimensionless_wavenumber)\n            lamda = lamda[:nexp]\n            a = a[:nexp]\n\n        else:\n            raise ValueError(\"Unrecognized method name for the Prony decomposition.\")\n\n        # Add one more exponential function (actually a constant).\n        # It is not clear where it comes from exactly in the theory...\n        a = np.concatenate([a, np.array([2])])\n        lamda = np.concatenate([lamda, np.array([0.0])])\n\n        return a, lamda",
  "def evaluate(self, mesh1, mesh2, free_surface=0.0, water_depth=np.infty, wavenumber=1.0, early_dot_product=True):\n        r\"\"\"The main method of the class, called by the engine to assemble the influence matrices.\n\n        Parameters\n        ----------\n        mesh1: Mesh or CollectionOfMeshes or list of points\n            mesh of the receiving body (where the potential is measured)\n            if only S is wanted or early_dot_product is False, then only a list of points as an array of shape (n, 3) can be passed.\n        mesh2: Mesh or CollectionOfMeshes\n            mesh of the source body (over which the source distribution is integrated)\n        free_surface: float, optional\n            position of the free surface (default: :math:`z = 0`)\n        water_depth: float, optional\n            constant depth of water (default: :math:`+\\infty`)\n        wavenumber: float, optional\n            wavenumber (default: 1.0)\n        early_dot_product: boolean, optional\n            if False, return K as a (n, m, 3) array storing \u222b\u2207G\n            if True, return K as a (n, m) array storing \u222b\u2207G\u00b7n\n\n        Returns\n        -------\n        tuple of numpy arrays\n            the matrices :math:`S` and :math:`K`\n        \"\"\"\n\n        if free_surface == np.infty: # No free surface, only a single Rankine source term\n\n            a_exp, lamda_exp = np.empty(1), np.empty(1)  # Dummy arrays that won't actually be used by the fortran code.\n\n            coeffs = np.array((1.0, 0.0, 0.0))\n\n        elif water_depth == np.infty:\n\n            a_exp, lamda_exp = np.empty(1), np.empty(1)  # Idem\n\n            if wavenumber == 0.0:\n                coeffs = np.array((1.0, 1.0, 0.0))\n            elif wavenumber == np.infty:\n                coeffs = np.array((1.0, -1.0, 0.0))\n            else:\n                coeffs = np.array((1.0, 1.0, 1.0))\n\n        else:  # Finite water_depth\n            a_exp, lamda_exp = self.find_best_exponential_decomposition(\n                wavenumber*water_depth*np.tanh(wavenumber*water_depth),\n                wavenumber*water_depth,\n            )\n            if wavenumber == 0.0:\n                raise NotImplementedError\n            elif wavenumber == np.infty:\n                raise NotImplementedError\n            else:\n                coeffs = np.array((1.0, 1.0, 1.0))\n\n        if isinstance(mesh1, Mesh) or isinstance(mesh1, CollectionOfMeshes):\n            collocation_points = mesh1.faces_centers\n            nb_collocation_points = mesh1.nb_faces\n            early_dot_product_normals = mesh1.faces_normals\n        elif isinstance(mesh1, np.ndarray) and mesh1.ndim ==2 and mesh1.shape[1] == 3:\n            collocation_points = mesh1\n            nb_collocation_points = mesh1.shape[0]\n            early_dot_product_normals = np.zeros((nb_collocation_points, 3))  # Hopefully unused\n        else:\n            raise ValueError(f\"Unrecognized input for {self.__class__.__name__}.evaluate\")\n\n        if self.floating_point_precision == \"float32\":\n            dtype = \"complex64\"\n        elif self.floating_point_precision == \"float64\":\n            dtype = \"complex128\"\n        else:\n            raise NotImplementedError\n\n        S = np.empty((nb_collocation_points, mesh2.nb_faces), order=\"F\", dtype=dtype)\n        K = np.empty((nb_collocation_points, mesh2.nb_faces, 1 if early_dot_product else 3), order=\"F\", dtype=dtype)\n\n        # Main call to Fortran code\n        self.fortran_core.matrices.build_matrices(\n            collocation_points,  early_dot_product_normals,\n            mesh2.vertices,      mesh2.faces + 1,\n            mesh2.faces_centers, mesh2.faces_normals,\n            mesh2.faces_areas,   mesh2.faces_radiuses,\n            *mesh2.quadrature_points,\n            wavenumber, water_depth,\n            coeffs,\n            self.tabulated_r_range, self.tabulated_z_range, self.tabulated_integrals,\n            lamda_exp, a_exp,\n            mesh1 is mesh2,\n            S, K\n        )\n\n        if np.any(np.isnan(S)) or np.any(np.isnan(K)):\n            raise RuntimeError(\"Green function returned a NaN in the interaction matrix.\\n\"\n                    \"It could be due to overlapping panels.\")\n\n        if early_dot_product: K = K.reshape((nb_collocation_points, mesh2.nb_faces))\n\n        return S, K",
  "def f(x):\n                return self.fortran_core.initialize_green_wave.ff(x, dimensionless_omega, dimensionless_wavenumber)",
  "def clip(source_mesh: Mesh, plane: Plane, vicinity_tol=1e-12, name=None):\n    \"\"\"Return a new mesh containing the source mesh clipped by the plane.\n\n    Parameters\n    ----------\n    source_mesh : Mesh\n        The mesh to be clipped.\n    plane : Plane, optional\n        The clipping plane.\n    vicinity_tol : float, optional\n        The absolute tolerance to consider en vertex is on the plane. Default is 1e-12.\n    name: string, optional\n        A name for the new clipped mesh.\n    \"\"\"\n    vertices_data = _vertices_positions_wrt_plane(source_mesh, plane, vicinity_tol)\n\n    nb_vertices_above_or_on_plane = np.count_nonzero(\n        vertices_data['vertices_above_mask'] | vertices_data['vertices_on_mask']\n    )\n    nb_vertices_below_or_on_plane = np.count_nonzero(\n        vertices_data['vertices_below_mask'] | vertices_data['vertices_on_mask']\n    )\n\n    if nb_vertices_above_or_on_plane == source_mesh.nb_vertices:\n        LOG.warning(f\"Clipping {source_mesh.name} by {plane}: all vertices are removed.\")\n        clipped_mesh = Mesh(None, None)\n        clipped_mesh._clipping_data = dict(faces_ids=[])\n\n    elif nb_vertices_below_or_on_plane == source_mesh.nb_vertices:\n        LOG.info(f\"Clipping {source_mesh.name} by {plane}: no action.\")\n        clipped_mesh = source_mesh.copy()\n        clipped_mesh._clipping_data = dict(faces_ids=list(range(source_mesh.nb_faces)))\n\n    else:\n        upper_mesh, crown_mesh, lower_mesh = _partition_mesh(vertices_data, source_mesh)\n\n        if crown_mesh.nb_faces > 0:\n            clipped_crown_mesh = _clip_crown(crown_mesh, plane)\n            clipped_mesh = lower_mesh + clipped_crown_mesh\n            clipped_mesh._clipping_data = {\n                'faces_ids': np.concatenate((lower_mesh._clipping_data['faces_ids'],\n                                             clipped_crown_mesh._clipping_data['faces_ids']))\n            }\n        else:\n            clipped_mesh = lower_mesh\n\n    if name is None:\n        clipped_mesh.name = f'{source_mesh.name}_clipped'\n    clipped_mesh.remove_unused_vertices()\n\n    return clipped_mesh",
  "def _vertices_positions_wrt_plane(source_mesh, plane, vicinity_tol):\n    \"\"\"Classifies vertices with respect to the clipping plane.\"\"\"\n    vertices_distances = plane.distance_to_point(source_mesh.vertices)\n    vertices_data = {'vertices_distances': vertices_distances,\n                     'vertices_above_mask': vertices_distances > vicinity_tol,\n                     'vertices_on_mask': np.abs(vertices_distances) < vicinity_tol,\n                     'vertices_below_mask': vertices_distances < -vicinity_tol\n                     }\n    return vertices_data",
  "def _partition_mesh(vertices_data, source_mesh):\n    \"\"\"Partitions the mesh in 3 with respect to the plane:\n    * upper_mesh: part entirely above the clipping plane\n    * crown_mesh: part intersecting the clipping plane\n    * lower_mesh: part entirely under the clipping plane\n    \"\"\"\n    nb_vertices_above_per_face = vertices_data['vertices_above_mask'][source_mesh.faces].sum(axis=1)\n    nb_vertices_below_per_face = vertices_data['vertices_below_mask'][source_mesh.faces].sum(axis=1)\n\n    # Simple criteria ensuring that _faces are totally above or below the plane (4 _vertices at the same side)\n    # Works for both triangles and quadrangles\n    above_faces_mask = nb_vertices_above_per_face == 4\n    below_faces_mask = nb_vertices_below_per_face == 4\n    crown_faces_mask = np.logical_not(np.logical_or(above_faces_mask, below_faces_mask))\n\n    faces_ids = {\n        'upper_mesh': np.where(above_faces_mask)[0],\n        'crown_mesh': np.where(crown_faces_mask)[0],\n        'lower_mesh': np.where(below_faces_mask)[0]\n    }\n\n    partition = []\n    for name in ['upper_mesh', 'crown_mesh', 'lower_mesh']:\n        new_mesh, vertices_ids = source_mesh.extract_faces(\n            id_faces_to_extract=faces_ids[name], return_index=True, name=name)\n        new_mesh._clipping_data = {\n            'faces_ids': faces_ids[name],\n            'vertices_ids': vertices_ids,\n            'vertices_distances': vertices_data['vertices_distances'][vertices_ids],\n            'above_vertices_mask': vertices_data['vertices_above_mask'][vertices_ids],\n            'on_vertices_mask': vertices_data['vertices_on_mask'][vertices_ids],\n            'below_vertices_mask': vertices_data['vertices_below_mask'][vertices_ids],\n        }\n        partition.append(new_mesh)\n\n    return partition",
  "def _clip_crown(crown_mesh, plane):\n    \"\"\"Performs the clipping operation of the crown_mesh and determines the obtained boundaries.\n    This is the heart method of the class.\n    \"\"\"\n    vertices = crown_mesh.vertices\n\n    above_vertices_mask = crown_mesh._clipping_data['above_vertices_mask']\n    below_vertices_mask = crown_mesh._clipping_data['below_vertices_mask']\n\n    on_vertices_mask = crown_mesh._clipping_data['on_vertices_mask']\n    # TODO: Vertices pre-projection to be done here\n\n    vertices_distances = crown_mesh._clipping_data['vertices_distances']\n\n    # Init\n    clipped_crown_mesh_faces = list()\n    clipped_crown_mesh_relative_faces_ids = list()\n\n    direct_boundary_edges = dict()\n    inv_boundary_edges = dict()\n    intersections_vertices = list()\n\n    index_new_vertices = crown_mesh.nb_vertices\n\n    for face_id in range(crown_mesh.nb_faces):\n\n        face = crown_mesh.get_face(face_id)\n\n        # # Determining the type of face clipping\n        v_above_face = np.where(above_vertices_mask[face])[0]\n        v_on_face = np.where(on_vertices_mask[face])[0]\n        v_below_face = np.where(below_vertices_mask[face])[0]\n\n        nb_above = len(v_above_face)\n        nb_on = len(v_on_face)\n        nb_below = len(v_below_face)\n\n        face_type = str(nb_above) + str(nb_on) + str(nb_below)\n\n        if face_type == '202':\n            #    0*-----*3\n            #     |     |\n            # ----o-----o----\n            #     |     |\n            #    1*-----*2\n            if v_above_face[1] == v_above_face[0] + 1:\n                face = np.roll(face, -v_above_face[1])\n            p0, p1, p2, p3 = vertices[face]\n            ileft = plane.get_edge_intersection(p0, p1)\n            iright = plane.get_edge_intersection(p2, p3)\n            intersections_vertices += [ileft, iright]\n            boundary_edge = [index_new_vertices, index_new_vertices + 1]\n            clipped_crown_mesh_faces.append([index_new_vertices, face[1], face[2], index_new_vertices + 1])\n            clipped_crown_mesh_relative_faces_ids.append(face_id)\n            index_new_vertices += 2\n\n        elif face_type == '301':\n            #      *2\n            #     / \\\n            #    /   \\\n            #   /     \\\n            # 3*       *1\n            #   \\     /\n            # ---o---o---\n            #     \\ /\n            #      *0\n            face = np.roll(face, -v_below_face[0])\n            p0, p1, p3 = vertices[face[[0, 1, 3]]]\n            ileft = plane.get_edge_intersection(p0, p3)\n            iright = plane.get_edge_intersection(p0, p1)\n            intersections_vertices += [ileft, iright]\n            boundary_edge = [index_new_vertices, index_new_vertices + 1]\n            clipped_crown_mesh_faces.append([index_new_vertices, face[0], index_new_vertices + 1, index_new_vertices])\n            clipped_crown_mesh_relative_faces_ids.append(face_id)\n            index_new_vertices += 2\n\n        elif face_type == '103':\n            #      *0\n            #     / \\\n            # ---o---o---\n            #   /     \\\n            # 1* - - - *3\n            #   \\     /\n            #    \\   /\n            #     \\ /\n            #      *2\n            face = np.roll(face, -v_above_face[0])\n            p0, p1, p3 = vertices[face[[0, 1, 3]]]\n            ileft = plane.get_edge_intersection(p0, p1)\n            iright = plane.get_edge_intersection(p0, p3)\n            intersections_vertices += [ileft, iright]\n            boundary_edge = [index_new_vertices, index_new_vertices + 1]\n            clipped_crown_mesh_faces.append([index_new_vertices, face[1], face[3], index_new_vertices + 1])\n            clipped_crown_mesh_relative_faces_ids.append(face_id)\n            clipped_crown_mesh_faces.append([face[1], face[2], face[3], face[1]])\n            clipped_crown_mesh_relative_faces_ids.append(face_id)\n            index_new_vertices += 2\n\n        elif face_type == '102':\n            #      *O\n            #     / \\\n            # ---o---o---\n            #   /     \\\n            # 1*-------*2\n            face = np.roll(face, -v_above_face[0])\n            p0, p1, p2 = vertices[face]\n            ileft = plane.get_edge_intersection(p0, p1)\n            iright = plane.get_edge_intersection(p0, p2)\n            intersections_vertices += [ileft, iright]\n            boundary_edge = [index_new_vertices, index_new_vertices + 1]\n            clipped_crown_mesh_faces.append([index_new_vertices, face[1], face[2], index_new_vertices + 1])\n            clipped_crown_mesh_relative_faces_ids.append(face_id)\n            index_new_vertices += 2\n\n        elif face_type == '201':  # done\n            #  2*-------*1\n            #    \\     /\n            #  ---o---o---\n            #      \\ /\n            #       *0\n            face = np.roll(face, -v_below_face[0])\n            p0, p1, p2 = vertices[face]\n            ileft = plane.get_edge_intersection(p0, p2)\n            iright = plane.get_edge_intersection(p0, p1)\n            intersections_vertices += [ileft, iright]\n            boundary_edge = [index_new_vertices, index_new_vertices + 1]\n            clipped_crown_mesh_faces.append([index_new_vertices, face[0], index_new_vertices + 1, index_new_vertices])\n            clipped_crown_mesh_relative_faces_ids.append(face_id)\n            index_new_vertices += 2\n\n        elif face_type == '211':\n            #        *3                   *1\n            #       / \\                  / \\\n            #      /   *2       or     2*   \\\n            #    0/   /                  \\   \\0\n            # ---*---o---              ---o---*---\n            #     \\ /                      \\ /\n            #      *1                       *3\n            #\n\n            face = np.roll(face, -v_on_face[0])\n            if vertices_distances[face[1]] < 0.:\n                p1, p2 = vertices[face[[1, 2]]]\n                iright = plane.get_edge_intersection(p1, p2)\n                intersections_vertices.append(iright)\n                boundary_edge = [face[0], index_new_vertices]\n                clipped_crown_mesh_faces.append([face[0], face[1], index_new_vertices, face[0]])\n            else:\n                p2, p3 = vertices[face[[2, 3]]]\n                ileft = plane.get_edge_intersection(p2, p3)\n                intersections_vertices.append(ileft)\n                boundary_edge = [index_new_vertices, face[0]]\n                clipped_crown_mesh_faces.append([index_new_vertices, face[3], face[0], index_new_vertices])\n            clipped_crown_mesh_relative_faces_ids.append(face_id)\n            index_new_vertices += 1\n\n        elif face_type == '112':\n            #       *3                     *1\n            #      / \\                    / \\\n            #  ---*---o---      or    ---o---*---\n            #     0\\   \\                /   /0\n            #       \\   *2            2*   /\n            #        \\ /                \\ /\n            #         *1                 *3\n            face = np.roll(face, -v_on_face[0])\n            if vertices_distances[face[1]] < 0.:\n                p2, p3 = vertices[face[[2, 3]]]\n                iright = plane.get_edge_intersection(p2, p3)\n                intersections_vertices.append(iright)\n                boundary_edge = [face[0], index_new_vertices]\n                clipped_crown_mesh_faces.append([face[0], face[1], face[2], index_new_vertices])\n            else:\n                p1, p2 = vertices[face[[1, 2]]]\n                ileft = plane.get_edge_intersection(p1, p2)\n                intersections_vertices.append(ileft)\n                boundary_edge = [index_new_vertices, face[0]]\n                clipped_crown_mesh_faces.append([index_new_vertices, face[2], face[3], face[0]])\n            clipped_crown_mesh_relative_faces_ids.append(face_id)\n            index_new_vertices += 1\n\n        elif face_type == '013':\n            # -----*-----\n            #     / \\\n            #    /   \\\n            #   *     *\n            #    \\   /\n            #     \\ /\n            #      *\n            boundary_edge = None\n            clipped_crown_mesh_faces.append(list(face))\n            clipped_crown_mesh_relative_faces_ids.append(face_id)\n\n        elif face_type == '210' or face_type == '310':\n            #   *-------*               *\n            #    \\ 210 /               / \\ 310\n            #     \\   /               *   *\n            #      \\ /                 \\ /\n            #   ----*----           ----*----\n            boundary_edge = None\n\n        elif face_type == '111':\n            #        *2              *1\n            #       /|               |\\\n            #      / |               | \\\n            #  ---*--o---    or   ---o--*---\n            #     0\\ |               | /0\n            #       \\|               |/\n            #        *1              *2\n            face = np.roll(face, -v_on_face[0])\n            p1, p2 = vertices[face[[1, 2]]]\n            if vertices_distances[face[1]] < 0.:\n                iright = plane.get_edge_intersection(p1, p2)\n                intersections_vertices.append(iright)\n                boundary_edge = [face[0], index_new_vertices]\n                clipped_crown_mesh_faces.append([face[0], face[1], index_new_vertices, face[0]])\n            else:\n                ileft = plane.get_edge_intersection(p1, p2)\n                intersections_vertices.append(ileft)\n                boundary_edge = [index_new_vertices, face[0]]\n                clipped_crown_mesh_faces.append([index_new_vertices, face[2], face[0], index_new_vertices])\n            clipped_crown_mesh_relative_faces_ids.append(face_id)\n            index_new_vertices += 1\n\n        elif face_type == '120':\n            #         *O\n            #        / \\\n            #       /   \\\n            #     1/     \\2\n            # ----*-------*----\n            # face = np.roll(face, -v_above_face[0])\n            # boundary_edge = [face[1], face[2]]\n            # FIXME: quick fix here : robust ?\n            boundary_edge = None\n\n        elif face_type == '021':\n            #  ----*-------*----\n            #      2\\     /1\n            #        \\   /\n            #         \\ /\n            #          *0\n            face = np.roll(face, -v_below_face[0])\n            boundary_edge = [face[2], face[1]]\n            face = list(face)\n            face.append(face[0])\n            clipped_crown_mesh_faces.append(face)\n            clipped_crown_mesh_relative_faces_ids.append(face_id)\n\n        elif face_type == '022':\n            # ----*-----*----\n            #    0|     |3\n            #     |     |\n            #    1*-----*2\n            if v_on_face[1] == v_on_face[0] + 1:\n                face = np.roll(face, -v_on_face[1])\n            boundary_edge = [face[0], face[3]]\n            clipped_crown_mesh_faces.append(list(face))\n            clipped_crown_mesh_relative_faces_ids.append(face_id)\n\n        elif face_type == '012':\n            #   ------*------\n            #        / \\\n            #       /   \\\n            #      /     \\\n            #     *-------*\n            boundary_edge = None\n            face = list(face)\n            face.append(face[0])\n            clipped_crown_mesh_faces.append(face)\n            clipped_crown_mesh_relative_faces_ids.append(face_id)\n\n        elif face_type == '220':\n            #    0*-----*3\n            #     |     |\n            #    1|     |2\n            # ----*-----*----\n\n            # if v_above_face[1] == v_above_face[0] + 1:\n            #     face = np.roll(face, -v_above_face[1])\n            # boundary_edge = [face[1], face[2]]\n            # FIXME: quick fix here : robust ?\n            boundary_edge = None\n\n        elif face_type == '121':\n            #       *0\n            #      / \\\n            #     /   \\\n            # ---*-----*---\n            #    1\\   /3\n            #      \\ /\n            #       *2\n            face = np.roll(face, -v_above_face[0])\n            boundary_edge = [face[1], face[3]]\n            clipped_crown_mesh_faces.append([face[1], face[2], face[3], face[1]])\n            clipped_crown_mesh_relative_faces_ids.append(face_id)\n\n        elif face_type == '300' or face_type == '400':\n            #       *               *-----*\n            #      / \\              |     |\n            #     /300\\      or     | 400 |\n            #    *-----*            *-----*\n            # ____________       ______________\n            boundary_edge = None\n\n        elif face_type == '003':\n            #  -----------\n            #       *\n            #      / \\\n            #     /   \\\n            #    *-----*\n            boundary_edge = None\n            face = list(face)\n            face.append(face[0])\n            clipped_crown_mesh_faces.append(face)\n            clipped_crown_mesh_relative_faces_ids.append(face_id)\n\n        elif face_type == '004':\n            #  ---------------\n            #      *-----*\n            #      |     |\n            #      |     |\n            #      *-----*\n            boundary_edge = None\n            clipped_crown_mesh_faces.append(list(face))\n            clipped_crown_mesh_relative_faces_ids.append(face_id)\n\n        elif face_type == '030' or face_type == '040':\n            # Face is totally on the plane --> rare case...\n            boundary_edge = None\n\n        else:\n            raise Exception(\"Face %u clipping case %s not known.\" % (face_id, face_type))\n\n        # Building boundary connectivity\n        if boundary_edge is not None:\n            direct_boundary_edges[boundary_edge[0]] = boundary_edge[1]\n            inv_boundary_edges[boundary_edge[1]] = boundary_edge[0]\n\n    if len(intersections_vertices) > 0:\n        vertices = np.concatenate((vertices, intersections_vertices))\n\n    clipped_crown_mesh = Mesh(vertices, clipped_crown_mesh_faces)\n    clipped_crown_mesh._clipping_data = {\n        'faces_ids': crown_mesh._clipping_data['faces_ids'][clipped_crown_mesh_relative_faces_ids]\n    }\n\n    return clipped_crown_mesh",
  "def compute_faces_properties(mesh):\n    \"\"\"Compute the faces properties of the mesh\"\"\"\n\n    # faces_areas, faces_normals, faces_centers = mm.get_all_faces_properties(mesh._vertices, mesh._faces)\n    nf = mesh.nb_faces\n\n    # triangle_mask = _faces[:, 0] == _faces[:, -1]\n    # nb_triangles = np.sum(triangle_mask)\n    # quads_mask = np.invert(triangle_mask)\n    # nb_quads = nf - nb_triangles\n\n    faces_areas = np.zeros(nf, dtype=float)\n    faces_normals = np.zeros((nf, 3), dtype=float)\n    faces_centers = np.zeros((nf, 3), dtype=float)\n\n    # Collectively dealing with triangles\n    # triangles = _faces[triangle_mask]\n    triangles_id = mesh.triangles_ids\n    triangles = mesh._faces[triangles_id]\n\n    triangles_normals = np.cross(mesh._vertices[triangles[:, 1]] - mesh._vertices[triangles[:, 0]],\n                                 mesh._vertices[triangles[:, 2]] - mesh._vertices[triangles[:, 0]])\n    triangles_normals_norm = np.linalg.norm(triangles_normals, axis=1)\n\n    degenerate_triangle = np.abs(triangles_normals_norm) < 1e-12\n    triangles_id = triangles_id[~degenerate_triangle]\n    triangles_normals = triangles_normals[~degenerate_triangle, :]\n    triangles_normals_norm = triangles_normals_norm[~degenerate_triangle]\n    triangles = triangles[~degenerate_triangle, :]\n    # Now, continue the computations without the degenerate triangles\n\n    faces_normals[triangles_id] = triangles_normals / triangles_normals_norm[:, np.newaxis]\n    faces_areas[triangles_id] = triangles_normals_norm / 2.\n    faces_centers[triangles_id] = np.sum(mesh._vertices[triangles[:, :3]], axis=1) / 3.\n\n    # Collectively dealing with quads\n    quads_id = mesh.quadrangles_ids\n    quads = mesh._faces[quads_id]\n    # quads = _faces[quads_mask]\n\n    quads_normals = np.cross(mesh._vertices[quads[:, 2]] - mesh._vertices[quads[:, 0]],\n                             mesh._vertices[quads[:, 3]] - mesh._vertices[quads[:, 1]])\n\n    quads_normals_norm = np.linalg.norm(quads_normals, axis=1)\n\n    degenerate_quad = np.abs(quads_normals_norm) < 1e-12\n    quads_id = quads_id[~degenerate_quad]\n    quads_normals = quads_normals[~degenerate_quad]\n    quads_normals_norm = quads_normals_norm[~degenerate_quad]\n    quads = quads[~degenerate_quad, :]\n    # Now, continue the computations without the degenerate quads\n\n    faces_normals[quads_id] = quads_normals / quads_normals_norm[:, np.newaxis]\n\n    a1 = np.linalg.norm(np.cross(mesh._vertices[quads[:, 1]] - mesh._vertices[quads[:, 0]],\n                                 mesh._vertices[quads[:, 2]] - mesh._vertices[quads[:, 0]]), axis=1) * 0.5\n    a2 = np.linalg.norm(np.cross(mesh._vertices[quads[:, 3]] - mesh._vertices[quads[:, 0]],\n                                 mesh._vertices[quads[:, 2]] - mesh._vertices[quads[:, 0]]), axis=1) * 0.5\n    faces_areas[quads_id] = a1 + a2\n\n    c1 = np.sum(mesh._vertices[quads[:, :3]], axis=1) / 3.\n    c2 = (np.sum(mesh._vertices[quads[:, 2:4]], axis=1) + mesh._vertices[quads[:, 0]]) / 3.\n\n    faces_centers[quads_id] = (np.array(([a1, ] * 3)).T * c1 + np.array(([a2, ] * 3)).T * c2)\n    faces_centers[quads_id] /= np.array(([faces_areas[quads_id], ] * 3)).T\n\n    faces_radiuses = compute_radiuses(mesh, faces_centers)\n\n    return {'faces_areas': faces_areas,\n            'faces_normals': faces_normals,\n            'faces_centers': faces_centers,\n            'faces_radiuses': faces_radiuses,\n            }",
  "def compute_radiuses(mesh, faces_centers):\n    \"\"\"Compute the radiuses of the faces of the mesh.\n\n    The radius is defined here as the maximal distance between the center\n    of mass of a cell and one of its points.\"\"\"\n\n    # Coordinates of all the vertices grouped by face\n    faces_vertices = mesh.vertices[mesh.faces, :]\n    # faces_vertices.shape == (nb_faces, 4, 3)\n\n    # Reorder the axes for array broadcasting below\n    faces_vertices = np.moveaxis(faces_vertices, 0, 1)\n    # faces_vertices.shape == (4, nb_faces, 3)\n\n    # Get all the vectors between the center of faces and their vertices.\n    radial_vector = faces_centers - faces_vertices\n    # radial_vector.shape == (4, nb_faces, 3)\n\n    # Keep the maximum length\n    faces_radiuses = np.max(np.linalg.norm(radial_vector, axis=2), axis=0)\n    # faces_radiuses.shape = (nb_faces)\n\n    return faces_radiuses",
  "def compute_connectivity(mesh):\n    \"\"\"Compute the connectivities of the mesh.\n\n    It concerns further connectivity than simple faces/vertices connectivities. It computes the vertices / vertices, vertices / faces and faces / faces connectivities.\n\n    Note\n    ----\n    * Note that if the mesh is not conformal, the algorithm may not perform correctly\n\n    TODO: The computation of boundaries should be in the future computed with help of VTK\n    \"\"\"\n\n    nv = mesh.nb_vertices\n    nf = mesh.nb_faces\n\n    mesh_closed = True\n\n    # Building connectivities\n\n    # Establishing v_v and v_f connectivities\n    v_v = dict([(i, set()) for i in range(nv)])\n    v_f = dict([(i, set()) for i in range(nv)])\n    for (iface, face) in enumerate(mesh._faces):\n        if face[0] == face[-1]:\n            face_w = face[:3]\n        else:\n            face_w = face\n        for (index, iV) in enumerate(face_w):\n            v_f[iV].add(iface)\n            v_v[face_w[index - 1]].add(iV)\n            v_v[iV].add(face_w[index - 1])\n\n    # Connectivity f_f\n    boundary_edges = dict()\n\n    f_f = dict([(i, set()) for i in range(nf)])\n    for ivertex in range(nv):\n        set1 = v_f[ivertex]\n        for iadj_v in v_v[ivertex]:\n            set2 = v_f[iadj_v]\n            intersection = list(set1 & set2)\n            if len(intersection) == 2:\n                f_f[intersection[0]].add(intersection[1])\n                f_f[intersection[1]].add(intersection[0])\n\n            elif len(intersection) == 1:\n                boundary_face = mesh._faces[intersection[0]]\n\n                if boundary_face[0] == boundary_face[-1]:\n                    boundary_face = boundary_face[:3]\n                ids = np.where((boundary_face == ivertex) + (boundary_face == iadj_v))[0]\n\n                if ids[1] != ids[0]+1:\n                    i_v_orig, i_v_target = boundary_face[ids]\n                else:\n                    i_v_target, i_v_orig = boundary_face[ids]\n\n                boundary_edges[i_v_orig] = i_v_target\n            else:\n                raise RuntimeError('Unexpected error while computing mesh connectivities')\n\n    # Computing boundaries\n    boundaries = list()\n    # TODO: calculer des boundaries fermees et ouvertes (closed_boundaries et open_boundaries) et mettre dans dict\n    while True:\n        try:\n            boundary = list()\n            i_v0_init, i_v1 = boundary_edges.popitem()\n            boundary.append(i_v0_init)\n            boundary.append(i_v1)\n            i_v0 = i_v1\n\n            while True:\n                try:\n                    i_v1 = boundary_edges.pop(i_v0)\n                    boundary.append(i_v1)\n                    i_v0 = i_v1\n                except KeyError:\n                    if boundary[0] != boundary[-1]:\n                        print('Boundary is not closed !!!')\n                    else:\n                        boundaries.append(boundary)\n                    break\n        except KeyError:\n            break\n\n    return {'v_v': v_v,\n            'v_f': v_f,\n            'f_f': f_f,\n            'boundaries': boundaries}",
  "class CollectionOfMeshes(ClippableMixin, SurfaceIntegralsMixin, Abstract3DObject):\n    \"\"\"A tuple of meshes.\n    It gives access to all the vertices of all the sub-meshes as if it were a mesh itself.\n    Collections can be nested to store meshes in a tree structure.\n\n    Parameters\n    ----------\n    meshes: Iterable of Mesh or CollectionOfMeshes\n        meshes in the collection\n    name : str, optional\n        a name for the collection\n    \"\"\"\n\n    def __init__(self, meshes: Iterable[Union[Mesh, 'CollectionOfMeshes']], name=None):\n\n        self._meshes = tuple(meshes)\n\n        for mesh in self._meshes:\n            assert isinstance(mesh, Mesh) or isinstance(mesh, CollectionOfMeshes)\n\n        if name is None:\n            self.name = f'collection_of_meshes_{next(Mesh._ids)}'\n        else:\n            self.name = str(name)\n\n        LOG.debug(f\"New collection of meshes: {repr(self)}\")\n\n    def __short_str__(self):\n        return (f\"{self.__class__.__name__}(..., name=\\\"{self.name}\\\")\")\n\n    def __str__(self):\n        if len(self._meshes) < 3:\n            meshes_str = ', '.join(m.__short_str__() for m in self._meshes)\n        else:\n            meshes_str = self._meshes[0].__short_str__() + \", ..., \" + self._meshes[-1].__short_str__()\n        return f\"{self.__class__.__name__}([{meshes_str}], name=\\\"{self.name}\\\")\"\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}({', '.join(str(m) for m in self._meshes)}, name=\\\"{self.name}\\\")\"\n\n    def _repr_pretty_(self, p, cycle):\n        p.text(self.__str__())\n\n    def __rich_repr__(self):\n        class WrappedString:\n            def __init__(self, s):\n                self.s = s\n            def __repr__(self):\n                return self.s\n        for m in self._meshes:\n            yield m\n        yield \"name\", self.name\n\n    def __iter__(self):\n        return iter(self._meshes)\n\n    def __len__(self):\n        return len(self._meshes)\n\n    def __getitem__(self, item):\n        return self._meshes.__getitem__(item)\n\n    def __eq__(self, other):\n        if isinstance(other, CollectionOfMeshes):\n            return self._meshes == other._meshes\n        else:\n            return NotImplemented\n\n    def __hash__(self):\n        return hash(self._meshes)\n\n    def tree_view(self, **kwargs):\n        body_tree_views = []\n        for i, mesh in enumerate(self):\n            tree_view = mesh.tree_view(**kwargs)\n            if i == len(self)-1:\n                prefix = ' \u2514\u2500'\n                shift  = '   '\n            else:\n                prefix = ' \u251c\u2500'\n                shift  = ' \u2502 '\n            body_tree_views.append(prefix + tree_view.replace('\\n', '\\n' + shift))\n\n        return self.__short_str__() + '\\n' + '\\n'.join(body_tree_views)\n\n    def copy(self, name=None):\n        from copy import deepcopy\n        new_mesh = deepcopy(self)\n        if name is not None:\n            new_mesh.name = name\n        return new_mesh\n\n    @inplace_transformation\n    def heal_mesh(self, closed_mesh=False):\n        for mesh in self:\n            mesh.heal_mesh(closed_mesh=closed_mesh)\n\n    ##############\n    # Properties #\n    ##############\n\n    @property\n    def nb_submeshes(self):\n        return len(self)\n\n    @property\n    def nb_vertices(self):\n        return sum(mesh.nb_vertices for mesh in self)\n\n    @property\n    def nb_faces(self):\n        return sum(mesh.nb_faces for mesh in self)\n\n    @property\n    def volume(self):\n        return sum(mesh.volume for mesh in self)\n\n    @property\n    def vertices(self):\n        return np.concatenate([mesh.vertices for mesh in self])\n\n    @property\n    def faces(self):\n        \"\"\"Return the indices of the vertices forming each of the faces. For the\n        later submeshes, the indices of the vertices has to be shifted to\n        correspond to their index in the concatenated array self.vertices.\n        \"\"\"\n        nb_vertices = accumulate(chain([0], (mesh.nb_vertices for mesh in self[:-1])))\n        return np.concatenate([mesh.faces + nbv for mesh, nbv in zip(self, nb_vertices)])\n\n    @property\n    def faces_normals(self):\n        return np.concatenate([mesh.faces_normals for mesh in self])\n\n    @property\n    def faces_areas(self):\n        return np.concatenate([mesh.faces_areas for mesh in self])\n\n    @property\n    def faces_centers(self):\n        return np.concatenate([mesh.faces_centers for mesh in self])\n\n    @property\n    def faces_radiuses(self):\n        return np.concatenate([mesh.faces_radiuses for mesh in self])\n\n    @property\n    def quadrature_points(self):\n        quad_submeshes = [mesh.quadrature_points for mesh in self]\n        return (\n            np.concatenate([quad[0] for quad in quad_submeshes]),  # Points\n            np.concatenate([quad[1] for quad in quad_submeshes])   # Weights\n        )\n\n    @property\n    def quadrature_method(self):\n        methods_submeshes = [mesh.quadrature_method for mesh in self]\n        if len(set(methods_submeshes)) == 1:\n            return methods_submeshes[0]  # All the same methods\n        else:\n            return \"Mixed quadrature method\"\n\n    def compute_quadrature(self, method):\n        for mesh in self:\n            mesh.compute_quadrature(method)\n\n    @property\n    def center_of_mass_of_nodes(self):\n        return sum([mesh.nb_vertices*mesh.center_of_mass_of_nodes for mesh in self])/self.nb_vertices\n\n    @property\n    @lru_cache(maxsize=1024)\n    def diameter_of_nodes(self):\n        return self.merged().diameter_of_nodes  # TODO: improve implementation\n\n    def indices_of_mesh(self, mesh_index: int) -> slice:\n        \"\"\"Return the indices of the faces for the sub-mesh given as argument.\"\"\"\n        start = sum((mesh.nb_faces for mesh in self[:mesh_index]))  # Number of faces in previous meshes\n        return slice(start, start + self[mesh_index].nb_faces)\n\n    def submesh_containing_face(self, id_face):\n        total_faces = 0\n        for id_mesh in range(self.nb_submeshes):\n            total_faces += self[id_mesh].nb_faces\n            if id_face < total_faces:\n                return id_mesh, id_face - (total_faces - self[id_mesh].nb_faces)\n\n    ##################\n    # Transformation #\n    ##################\n\n    def merged(self, name=None) -> Mesh:\n        \"\"\"Merge the sub-meshes and return a full mesh.\n        If the collection contains other collections, they are merged recursively.\n        Optionally, a new name can be given to the resulting mesh.\"\"\"\n        if name is None:\n            name = self.name\n        merged = Mesh(self.vertices, self.faces, name=name)\n        merged.merge_duplicates()\n        merged.heal_triangles()\n        return merged\n\n    def extract_one_face(self, id_face):\n        id_mesh, relative_id_face = self.submesh_containing_face(id_face)\n        mesh = self[id_mesh]\n\n        extracted_mesh = mesh.extract_one_face(relative_id_face)\n\n        if hasattr(mesh, '__internals__'):\n            for prop in mesh.__internals__:\n                if prop[:4] == \"face\":\n                    extracted_mesh.__internals__[prop] = mesh.__internals__[prop][[relative_id_face]]\n\n        return extracted_mesh\n\n    def extract_faces(self, *args, **kwargs):\n        return self.merged().extract_faces(*args, **kwargs)\n\n    def sliced_by_plane(self, plane):\n        return CollectionOfMeshes([mesh.sliced_by_plane(plane) for mesh in self], name=self.name)\n\n    @inplace_transformation\n    def translate(self, vector):\n        for mesh in self:\n            mesh.translate(vector)\n\n    @inplace_transformation\n    def rotate(self, axis, angle):\n        for mesh in self:\n            mesh.rotate(axis, angle)\n\n    @inplace_transformation\n    def mirror(self, plane):\n        for mesh in self:\n            mesh.mirror(plane)\n\n    @inplace_transformation\n    def clip(self, plane):\n        self._clipping_data = {'faces_ids': []}\n        faces_shifts = list(accumulate(chain([0], (mesh.nb_faces for mesh in self[:-1]))))\n        for mesh, faces_shift in zip(self, faces_shifts):\n            mesh.clip(plane)\n            self._clipping_data['faces_ids'].extend([i + faces_shift for i in mesh._clipping_data['faces_ids']])\n        self._clipping_data['faces_ids'] = np.asarray(self._clipping_data['faces_ids'])\n        self.prune_empty_meshes()\n\n    def symmetrized(self, plane):\n        from capytaine.meshes.symmetric import ReflectionSymmetricMesh\n        half = self.clipped(plane, name=f\"{self.name}_half\")\n        return ReflectionSymmetricMesh(half, plane=plane, name=f\"symmetrized_of_{self.name}\")\n\n    @inplace_transformation\n    def prune_empty_meshes(self):\n        \"\"\"Remove empty meshes from the collection.\"\"\"\n        self._meshes = tuple(mesh for mesh in self if mesh.nb_faces > 0 and mesh.nb_vertices > 0)\n\n    @property\n    def axis_aligned_bbox(self):\n        \"\"\"Get the axis aligned bounding box of the mesh.\n\n        Returns\n        -------\n        tuple\n            (xmin, xmax, ymin, ymax, zmin, zmax)\n        \"\"\"\n        if self.nb_vertices > 0:\n            x, y, z = self.vertices.T\n            return (x.min(), x.max(),\n                    y.min(), y.max(),\n                    z.min(), z.max())\n        else:\n            return tuple(np.zeros(6))\n\n    def show(self, **kwargs):\n        from capytaine.ui.vtk.mesh_viewer import MeshViewer\n\n        viewer = MeshViewer()\n        for mesh in self:\n            viewer.add_mesh(mesh.merged(), **kwargs)\n        viewer.show()\n        viewer.finalize()\n\n    def show_matplotlib(self, *args, **kwargs):\n        self.merged().show_matplotlib(*args, **kwargs)",
  "def __init__(self, meshes: Iterable[Union[Mesh, 'CollectionOfMeshes']], name=None):\n\n        self._meshes = tuple(meshes)\n\n        for mesh in self._meshes:\n            assert isinstance(mesh, Mesh) or isinstance(mesh, CollectionOfMeshes)\n\n        if name is None:\n            self.name = f'collection_of_meshes_{next(Mesh._ids)}'\n        else:\n            self.name = str(name)\n\n        LOG.debug(f\"New collection of meshes: {repr(self)}\")",
  "def __short_str__(self):\n        return (f\"{self.__class__.__name__}(..., name=\\\"{self.name}\\\")\")",
  "def __str__(self):\n        if len(self._meshes) < 3:\n            meshes_str = ', '.join(m.__short_str__() for m in self._meshes)\n        else:\n            meshes_str = self._meshes[0].__short_str__() + \", ..., \" + self._meshes[-1].__short_str__()\n        return f\"{self.__class__.__name__}([{meshes_str}], name=\\\"{self.name}\\\")\"",
  "def __repr__(self):\n        return f\"{self.__class__.__name__}({', '.join(str(m) for m in self._meshes)}, name=\\\"{self.name}\\\")\"",
  "def _repr_pretty_(self, p, cycle):\n        p.text(self.__str__())",
  "def __rich_repr__(self):\n        class WrappedString:\n            def __init__(self, s):\n                self.s = s\n            def __repr__(self):\n                return self.s\n        for m in self._meshes:\n            yield m\n        yield \"name\", self.name",
  "def __iter__(self):\n        return iter(self._meshes)",
  "def __len__(self):\n        return len(self._meshes)",
  "def __getitem__(self, item):\n        return self._meshes.__getitem__(item)",
  "def __eq__(self, other):\n        if isinstance(other, CollectionOfMeshes):\n            return self._meshes == other._meshes\n        else:\n            return NotImplemented",
  "def __hash__(self):\n        return hash(self._meshes)",
  "def tree_view(self, **kwargs):\n        body_tree_views = []\n        for i, mesh in enumerate(self):\n            tree_view = mesh.tree_view(**kwargs)\n            if i == len(self)-1:\n                prefix = ' \u2514\u2500'\n                shift  = '   '\n            else:\n                prefix = ' \u251c\u2500'\n                shift  = ' \u2502 '\n            body_tree_views.append(prefix + tree_view.replace('\\n', '\\n' + shift))\n\n        return self.__short_str__() + '\\n' + '\\n'.join(body_tree_views)",
  "def copy(self, name=None):\n        from copy import deepcopy\n        new_mesh = deepcopy(self)\n        if name is not None:\n            new_mesh.name = name\n        return new_mesh",
  "def heal_mesh(self, closed_mesh=False):\n        for mesh in self:\n            mesh.heal_mesh(closed_mesh=closed_mesh)",
  "def nb_submeshes(self):\n        return len(self)",
  "def nb_vertices(self):\n        return sum(mesh.nb_vertices for mesh in self)",
  "def nb_faces(self):\n        return sum(mesh.nb_faces for mesh in self)",
  "def volume(self):\n        return sum(mesh.volume for mesh in self)",
  "def vertices(self):\n        return np.concatenate([mesh.vertices for mesh in self])",
  "def faces(self):\n        \"\"\"Return the indices of the vertices forming each of the faces. For the\n        later submeshes, the indices of the vertices has to be shifted to\n        correspond to their index in the concatenated array self.vertices.\n        \"\"\"\n        nb_vertices = accumulate(chain([0], (mesh.nb_vertices for mesh in self[:-1])))\n        return np.concatenate([mesh.faces + nbv for mesh, nbv in zip(self, nb_vertices)])",
  "def faces_normals(self):\n        return np.concatenate([mesh.faces_normals for mesh in self])",
  "def faces_areas(self):\n        return np.concatenate([mesh.faces_areas for mesh in self])",
  "def faces_centers(self):\n        return np.concatenate([mesh.faces_centers for mesh in self])",
  "def faces_radiuses(self):\n        return np.concatenate([mesh.faces_radiuses for mesh in self])",
  "def quadrature_points(self):\n        quad_submeshes = [mesh.quadrature_points for mesh in self]\n        return (\n            np.concatenate([quad[0] for quad in quad_submeshes]),  # Points\n            np.concatenate([quad[1] for quad in quad_submeshes])   # Weights\n        )",
  "def quadrature_method(self):\n        methods_submeshes = [mesh.quadrature_method for mesh in self]\n        if len(set(methods_submeshes)) == 1:\n            return methods_submeshes[0]  # All the same methods\n        else:\n            return \"Mixed quadrature method\"",
  "def compute_quadrature(self, method):\n        for mesh in self:\n            mesh.compute_quadrature(method)",
  "def center_of_mass_of_nodes(self):\n        return sum([mesh.nb_vertices*mesh.center_of_mass_of_nodes for mesh in self])/self.nb_vertices",
  "def diameter_of_nodes(self):\n        return self.merged().diameter_of_nodes",
  "def indices_of_mesh(self, mesh_index: int) -> slice:\n        \"\"\"Return the indices of the faces for the sub-mesh given as argument.\"\"\"\n        start = sum((mesh.nb_faces for mesh in self[:mesh_index]))  # Number of faces in previous meshes\n        return slice(start, start + self[mesh_index].nb_faces)",
  "def submesh_containing_face(self, id_face):\n        total_faces = 0\n        for id_mesh in range(self.nb_submeshes):\n            total_faces += self[id_mesh].nb_faces\n            if id_face < total_faces:\n                return id_mesh, id_face - (total_faces - self[id_mesh].nb_faces)",
  "def merged(self, name=None) -> Mesh:\n        \"\"\"Merge the sub-meshes and return a full mesh.\n        If the collection contains other collections, they are merged recursively.\n        Optionally, a new name can be given to the resulting mesh.\"\"\"\n        if name is None:\n            name = self.name\n        merged = Mesh(self.vertices, self.faces, name=name)\n        merged.merge_duplicates()\n        merged.heal_triangles()\n        return merged",
  "def extract_one_face(self, id_face):\n        id_mesh, relative_id_face = self.submesh_containing_face(id_face)\n        mesh = self[id_mesh]\n\n        extracted_mesh = mesh.extract_one_face(relative_id_face)\n\n        if hasattr(mesh, '__internals__'):\n            for prop in mesh.__internals__:\n                if prop[:4] == \"face\":\n                    extracted_mesh.__internals__[prop] = mesh.__internals__[prop][[relative_id_face]]\n\n        return extracted_mesh",
  "def extract_faces(self, *args, **kwargs):\n        return self.merged().extract_faces(*args, **kwargs)",
  "def sliced_by_plane(self, plane):\n        return CollectionOfMeshes([mesh.sliced_by_plane(plane) for mesh in self], name=self.name)",
  "def translate(self, vector):\n        for mesh in self:\n            mesh.translate(vector)",
  "def rotate(self, axis, angle):\n        for mesh in self:\n            mesh.rotate(axis, angle)",
  "def mirror(self, plane):\n        for mesh in self:\n            mesh.mirror(plane)",
  "def clip(self, plane):\n        self._clipping_data = {'faces_ids': []}\n        faces_shifts = list(accumulate(chain([0], (mesh.nb_faces for mesh in self[:-1]))))\n        for mesh, faces_shift in zip(self, faces_shifts):\n            mesh.clip(plane)\n            self._clipping_data['faces_ids'].extend([i + faces_shift for i in mesh._clipping_data['faces_ids']])\n        self._clipping_data['faces_ids'] = np.asarray(self._clipping_data['faces_ids'])\n        self.prune_empty_meshes()",
  "def symmetrized(self, plane):\n        from capytaine.meshes.symmetric import ReflectionSymmetricMesh\n        half = self.clipped(plane, name=f\"{self.name}_half\")\n        return ReflectionSymmetricMesh(half, plane=plane, name=f\"symmetrized_of_{self.name}\")",
  "def prune_empty_meshes(self):\n        \"\"\"Remove empty meshes from the collection.\"\"\"\n        self._meshes = tuple(mesh for mesh in self if mesh.nb_faces > 0 and mesh.nb_vertices > 0)",
  "def axis_aligned_bbox(self):\n        \"\"\"Get the axis aligned bounding box of the mesh.\n\n        Returns\n        -------\n        tuple\n            (xmin, xmax, ymin, ymax, zmin, zmax)\n        \"\"\"\n        if self.nb_vertices > 0:\n            x, y, z = self.vertices.T\n            return (x.min(), x.max(),\n                    y.min(), y.max(),\n                    z.min(), z.max())\n        else:\n            return tuple(np.zeros(6))",
  "def show(self, **kwargs):\n        from capytaine.ui.vtk.mesh_viewer import MeshViewer\n\n        viewer = MeshViewer()\n        for mesh in self:\n            viewer.add_mesh(mesh.merged(), **kwargs)\n        viewer.show()\n        viewer.finalize()",
  "def show_matplotlib(self, *args, **kwargs):\n        self.merged().show_matplotlib(*args, **kwargs)",
  "class WrappedString:\n            def __init__(self, s):\n                self.s = s\n            def __repr__(self):\n                return self.s",
  "def __init__(self, s):\n                self.s = s",
  "def __repr__(self):\n                return self.s",
  "class Mesh(ClippableMixin, SurfaceIntegralsMixin, Abstract3DObject):\n    \"\"\"A class to handle unstructured 2D meshes in a 3D space.\n\n    Parameters\n    ----------\n    vertices : array_like of shape (nv, 3)\n        Array of mesh vertices coordinates.Each line of the array represents one vertex\n        coordinates\n    faces : array_like of shape (nf, 4)\n        Arrays of mesh connectivities for faces. Each line of the array represents indices of\n        vertices that form the face, expressed in counterclockwise order to ensure outward normals\n        description.\n    name : str, optional\n        The name of the mesh. If None, the mesh is given an automatic name based on its internal ID.\n    \"\"\"\n\n    _ids = count(0)  # A counter for automatic naming of new meshes.\n\n    def __init__(self, vertices=None, faces=None, name=None):\n\n        if vertices is None or len(vertices) == 0:\n            vertices = np.zeros((0, 3))\n\n        if faces is None or len(faces) == 0:\n            faces = np.zeros((0, 4))\n\n        if name is None:\n            self.name = f'mesh_{next(Mesh._ids)}'\n        else:\n            self.name = str(name)\n\n        self.__internals__ = dict()\n        self.vertices = vertices  # Not a direct assignment, goes through the setter method below.\n        self.faces = faces  # Not a direct assignment, goes through the setter method below.\n\n        LOG.debug(f\"New mesh: {repr(self)}\")\n\n    def __short_str__(self):\n        return (f\"{self.__class__.__name__}(..., name=\\\"{self.name}\\\")\")\n\n    def __str__(self):\n        return (f\"{self.__class__.__name__}(vertices=[[... {self.nb_vertices} vertices ...]], \"\n                f\"faces=[[... {self.nb_faces} faces ...]], name=\\\"{self.name}\\\")\")\n\n    def __repr__(self):\n        # shift = len(self.__class__.__name__) + 1\n        # vert_str = np.array_repr(self.vertices).replace('\\n', '\\n' + (shift + 9)*' ')\n        # faces_str = np.array_repr(self.faces).replace('\\n', '\\n' + (shift + 6)*' ')\n        # return f\"{self.__class__.__name__}(\\n{' '*shift}vertices={vert_str},\\n{' '*shift}faces={faces_str}\\n{' '*shift}name=\\\"{self.name}\\\"\\n)\"\n        return (f\"{self.__class__.__name__}(vertices=[[... {self.nb_vertices} vertices ...]], \"\n                f\"faces=[[... {self.nb_faces} faces ...]], name=\\\"{self.name}\\\")\")\n\n    def _repr_pretty_(self, p, cycle):\n        p.text(self.__str__())\n\n    def __rich_repr__(self):\n        class CustomRepr:\n            def __init__(self, n, kind):\n                self.n = n\n                self.kind = kind\n            def __repr__(self):\n                return \"[[... {} {} ...]]\".format(self.n, self.kind)\n        yield \"vertices\", CustomRepr(self.nb_vertices, \"vertices\")\n        yield \"faces\", CustomRepr(self.nb_faces, \"faces\")\n        yield \"name\", self.name\n\n    @property\n    def nb_vertices(self) -> int:\n        \"\"\"Get the number of vertices in the mesh.\"\"\"\n        return self._vertices.shape[0]\n\n    @property\n    def vertices(self) -> np.ndarray:\n        \"\"\"Get the vertices array coordinate of the mesh.\"\"\"\n        return self._vertices\n\n    @vertices.setter\n    def vertices(self, value) -> None:\n        self._vertices = np.array(value, dtype=float)\n        assert self._vertices.shape[1] == 3, \\\n            \"Vertices of a mesh should be provided as a sequence of 3-ple.\"\n        self.__internals__.clear()\n\n    @property\n    def nb_faces(self) -> int:\n        \"\"\"Get the number of faces in the mesh.\"\"\"\n        return self._faces.shape[0]\n\n    @property\n    def faces(self) -> np.ndarray:\n        \"\"\"Get the faces connectivity array of the mesh.\"\"\"\n        return self._faces\n\n    @faces.setter\n    def faces(self, faces):\n        faces = np.array(faces, dtype=int)\n        assert np.all(faces >= 0), \\\n            \"Faces of a mesh should be provided as positive integers (ids of vertices)\"\n        assert faces.shape[1] == 4, \\\n            \"Faces of a mesh should be provided as a sequence of 4-ple.\"\n        assert len(faces) == 0 or faces.max()+1 <= self.nb_vertices, \\\n            \"The array of faces should only reference vertices that are in the mesh.\"\n        self._faces = faces\n        self.__internals__.clear()\n\n    def copy(self, name=None) -> 'Mesh':\n        \"\"\"Get a copy of the current mesh instance.\n\n        Parameters\n        ----------\n        name : string, optional\n            a name for the new mesh\n\n        Returns\n        -------\n        Mesh\n            mesh instance copy\n        \"\"\"\n        from copy import deepcopy\n        new_mesh = deepcopy(self)\n        if name is not None:\n            new_mesh.name = name\n        return new_mesh\n\n    def merged(self):\n        \"\"\"Dummy method to be generalized for collections of meshes.\"\"\"\n        return self\n\n    def tree_view(self, **kwargs):\n        \"\"\"Dummy method to be generalized for collections of meshes.\"\"\"\n        return self.__short_str__()\n\n    def to_meshmagick(self):\n        \"\"\"Convert the Mesh object as a Mesh object from meshmagick.\n        Mostly for debugging.\"\"\"\n        from meshmagick.mesh import Mesh\n        meshmagick_mesh = Mesh(self.vertices, self.faces, name=self.name)\n        meshmagick_mesh.heal_mesh()\n        return meshmagick_mesh\n\n    ##################\n    #  Extract face  #\n    ##################\n\n    def get_face(self, face_id):\n        \"\"\"Get the face described by its vertices connectivity.\n\n        Parameters\n        ----------\n        face_id : int\n            Face id\n\n        Returns\n        -------\n        ndarray\n            If the face is a triangle, the array has 3 components, else it has 4 (quadrangle)\n        \"\"\"\n        if self.is_triangle(face_id):\n            return self._faces[face_id, :3]\n        else:\n            return self._faces[face_id]\n\n    def extract_one_face(self, id_face):\n        vertices = self.vertices[self.faces[id_face, :], :]\n        mesh = Mesh(vertices=vertices, faces=np.array([[0, 1, 2, 3]]), name=f\"single_face_from_{self.name}\")\n\n        for prop in self.__internals__:\n            if prop[:4] == \"face\":\n                mesh.__internals__[prop] = self.__internals__[prop][[id_face]]\n\n        return mesh\n\n    def extract_faces(self, id_faces_to_extract, return_index=False, name=None):\n        \"\"\"\n        Extracts a new mesh from a selection of faces ids\n\n        Parameters\n        ----------\n        id_faces_to_extract : ndarray\n            Indices of faces that have to be extracted\n        return_index: bool, optional\n            Flag to output old indices\n        name: string, optional\n            Name for the new mesh\n\n        Returns\n        -------\n        Mesh\n            A new Mesh instance composed of the extracted faces\n        \"\"\"\n        nv = self.nb_vertices\n\n        # Determination of the vertices to keep\n        vertices_mask = np.zeros(nv, dtype=bool)\n        vertices_mask[self._faces[id_faces_to_extract].flatten()] = True\n        id_v = np.arange(nv)[vertices_mask]\n\n        # Building up the vertex array\n        v_extracted = self._vertices[id_v]\n        new_id__v = np.arange(nv)\n        new_id__v[id_v] = np.arange(len(id_v))\n\n        faces_extracted = self._faces[id_faces_to_extract]\n        faces_extracted = new_id__v[faces_extracted.flatten()].reshape((len(id_faces_to_extract), 4))\n\n        extracted_mesh = Mesh(v_extracted, faces_extracted)\n\n        for prop in self.__internals__:\n            if prop[:4] == \"face\":\n                extracted_mesh.__internals__[prop] = self.__internals__[prop][id_faces_to_extract]\n\n        if name is None:\n            if self.name is not None and self.name.startswith(\"mesh_extracted_from_\"):\n                extracted_mesh.name = self.name\n            else:\n                extracted_mesh.name = f\"mesh_extracted_from_{self.name}\"\n        else:\n            extracted_mesh.name = name\n\n        if return_index:\n            return extracted_mesh, id_v\n        else:\n            return extracted_mesh\n\n    def sliced_by_plane(self, plane: Plane):\n        from capytaine.meshes.collections import CollectionOfMeshes\n        faces_ids_on_one_side = np.where(plane.distance_to_point(self.faces_centers) < 0)[0]\n        if len(faces_ids_on_one_side) == 0 or len(faces_ids_on_one_side) == self.nb_faces:\n            return self.copy()\n        else:\n            mesh_part_1 = self.extract_faces(faces_ids_on_one_side)\n            mesh_part_2 = self.extract_faces(list(set(range(self.nb_faces)) - set(faces_ids_on_one_side)))\n            return CollectionOfMeshes([mesh_part_1, mesh_part_2],\n                                      name=f\"{self.name}_splitted_by_{plane}\")\n\n\n    #####################\n    #  Mean and radius  #\n    #####################\n\n    @property\n    def center_of_mass_of_nodes(self):\n        \"\"\"(Non-weighted) center of mass of the nodes of the mesh.\"\"\"\n        if 'center_of_mass_of_nodes' not in self.__internals__:\n            center_of_mass_of_nodes = np.mean(self.vertices, axis=0)\n            self.__internals__['center_of_mass_of_nodes'] = center_of_mass_of_nodes\n            return center_of_mass_of_nodes\n        return self.__internals__['center_of_mass_of_nodes']\n\n    @property\n    def diameter_of_nodes(self):\n        \"\"\"Maximum distance between two nodes of the mesh.\"\"\"\n        if 'diameter_of_nodes' not in self.__internals__:\n            diameter_of_nodes = 2*np.max(\n                np.linalg.norm(self.vertices - self.center_of_mass_of_nodes, axis=-1)\n            )\n            self.__internals__['diameter_of_nodes'] = diameter_of_nodes\n            return diameter_of_nodes\n        return self.__internals__['diameter_of_nodes']\n\n    ######################\n    #  Faces properties  #\n    ######################\n\n    @property\n    def faces_areas(self) -> np.ndarray:\n        \"\"\"Get the array of faces areas of the mesh.\"\"\"\n        if 'faces_areas' not in self.__internals__:\n            self.__internals__.update(compute_faces_properties(self))\n        return self.__internals__['faces_areas']\n\n    @property\n    def faces_centers(self) -> np.ndarray:\n        \"\"\"Get the array of faces centers of the mesh.\"\"\"\n        if 'faces_centers' not in self.__internals__:\n            self.__internals__.update(compute_faces_properties(self))\n        return self.__internals__['faces_centers']\n\n    @property\n    def faces_normals(self) -> np.ndarray:\n        \"\"\"Get the array of faces normals of the mesh.\"\"\"\n        if 'faces_normals' not in self.__internals__:\n            self.__internals__.update(compute_faces_properties(self))\n        return self.__internals__['faces_normals']\n\n    @property\n    def faces_radiuses(self) -> np.ndarray:\n        \"\"\"Get the array of faces radiuses of the mesh.\"\"\"\n        if 'faces_radiuses' not in self.__internals__:\n            self.__internals__.update(compute_faces_properties(self))\n        return self.__internals__['faces_radiuses']\n\n    @property\n    def quadrature_points(self):\n        if 'quadrature' in self.__internals__:\n            return self.__internals__['quadrature']\n        else:\n            # Default: first order quadrature\n            return (\n                self.faces_centers.reshape((self.nb_faces, 1, 3)),  # Points\n                self.faces_areas.reshape((self.nb_faces, 1))        # Weights\n            )\n\n    @property\n    def quadrature_method(self):\n        if 'quadrature_method' in self.__internals__:\n            return self.__internals__['quadrature_method']\n        else:\n            return None\n\n    def compute_quadrature(self, method):\n        quadpy = import_optional_dependency(\"quadpy\")\n        transform = quadpy.c2.transform\n        get_detJ = quadpy.cn._helpers.get_detJ\n\n        if method is None:\n            # No quadrature (i.e. default first order quadrature)\n            if 'quadrature' in self.__internals__:\n                del self.__internals__['quadrature']\n                del self.__internals__['quadrature_method']\n            else:\n                pass\n\n        elif isinstance(method, quadpy.c2._helpers.C2Scheme):\n            assert method.points.shape[0] == method.dim == 2\n            nb_points = method.points.shape[1]\n            points = np.empty((self.nb_faces, nb_points, 3))\n            weights = np.empty((self.nb_faces, nb_points))\n\n            self.heal_triangles()\n\n            for i_face in range(self.nb_faces):\n                # Define a local frame (Oxyz) such that\n                # * the corner A of the quadrilateral panel is the origin of the local frame\n                # * the edge AB of the quadrilateral panel is along the local x-axis,\n                # * the quadrilateral panel is within the local xy-plane (that is, its normal is along the local z-axis).\n                # Hence, the corners of the panels all have 0 as z-coordinate in the local frame.\n\n                # Coordinates in global frame\n                global_A, global_B, global_C, global_D = self.vertices[self.faces[i_face, :], :]\n                n = self.faces_normals[i_face, :]\n\n                ex = (global_B-global_A)/norm(global_B-global_A)  # unit vector of the local x-axis\n                ez = n/norm(n)                                    # unit vector of the local z-axis\n                ey = np.cross(ex, ez)                             # unit vector of the local y-axis, such that the basis is orthonormal\n\n                R = np.array([ex, ey, ez])\n                local_A = np.zeros((3,))             # coordinates of A in local frame, should be zero by construction\n                local_B = R @ (global_B - global_A)  # coordinates of B in local frame\n                local_C = R @ (global_C - global_A)  # coordinates of C in local frame\n                local_D = R @ (global_D - global_A)  # coordinates of D in local frame\n\n                local_quadrilateral = np.array([[local_A, local_D], [local_B, local_C]])[:, :, :-1]\n                # Removing last index in last dimension because not interested in z-coordinate which is 0.\n\n                local_quadpoints = transform(method.points, local_quadrilateral)\n\n                local_quadpoints_in_3d = np.concatenate([local_quadpoints, np.zeros((nb_points, 1))], axis=1)\n                global_quadpoints = np.array([R.T @ p for p in local_quadpoints_in_3d]) + global_A\n                points[i_face, :, :] = global_quadpoints\n\n                weights[i_face, :] = method.weights * 4 * np.abs(get_detJ(method.points, local_quadrilateral))\n\n            self.__internals__['quadrature'] = (points, weights)\n            self.__internals__['quadrature_method'] = method\n\n        else:\n            raise NotImplementedError\n\n    ###############################\n    #  Triangles and quadrangles  #\n    ###############################\n\n    def is_triangle(self, face_id) -> bool:\n        \"\"\"Returns if a face is a triangle\n\n        Parameters\n        ----------\n        face_id : int\n            Face id\n        \"\"\"\n        assert 0 <= face_id < self.nb_faces\n        return self._faces[face_id, 0] == self._faces[face_id, -1]\n\n    def _compute_triangles_quadrangles(self):\n        triangle_mask = (self._faces[:, 0] == self._faces[:, -1])\n        quadrangles_mask = np.invert(triangle_mask)\n        triangles_quadrangles = {'triangles_ids': np.where(triangle_mask)[0],\n                                 'quadrangles_ids': np.where(quadrangles_mask)[0]}\n        self.__internals__.update(triangles_quadrangles)\n\n    @property\n    def triangles_ids(self) -> np.ndarray:\n        \"\"\"Get the array of ids of triangle shaped faces.\"\"\"\n        if 'triangles_ids' not in self.__internals__:\n            self._compute_triangles_quadrangles()\n        return self.__internals__['triangles_ids']\n\n    @property\n    def nb_triangles(self) -> int:\n        \"\"\"Get the number of triangles in the mesh.\"\"\"\n        if 'triangles_ids'not in self.__internals__:\n            self._compute_triangles_quadrangles()\n        return len(self.__internals__['triangles_ids'])\n\n    @property\n    def quadrangles_ids(self) -> np.ndarray:\n        \"\"\"Get the array of ids of quadrangle shaped faces.\"\"\"\n        if 'triangles_ids' not in self.__internals__:\n            self._compute_triangles_quadrangles()\n        return self.__internals__['quadrangles_ids']\n\n    @property\n    def nb_quadrangles(self) -> int:\n        \"\"\"Get the number of quadrangles in the mesh.\"\"\"\n        if 'triangles_ids' not in self.__internals__:\n            self._compute_triangles_quadrangles()\n        return len(self.__internals__['quadrangles_ids'])\n\n    #############\n    #  Display  #\n    #############\n\n    @property\n    def axis_aligned_bbox(self):\n        \"\"\"Get the axis aligned bounding box of the mesh.\n\n        Returns\n        -------\n        tuple\n            (xmin, xmax, ymin, ymax, zmin, zmax)\n        \"\"\"\n        if self.nb_vertices > 0:\n            x, y, z = self._vertices.T\n            return (x.min(), x.max(),\n                    y.min(), y.max(),\n                    z.min(), z.max())\n        else:\n            return tuple(np.zeros(6))\n\n    @property\n    def squared_axis_aligned_bbox(self):\n        \"\"\"Get a squared axis aligned bounding box of the mesh.\n\n        Returns\n        -------\n        tuple\n            (xmin, xmax, ymin, ymax, zmin, zmax)\n\n        Note\n        ----\n        This method differs from `axis_aligned_bbox()` by the fact that\n        the bounding box that is returned is squared but have the same center as the `axis_aligned_bbox()`.\n        \"\"\"\n        xmin, xmax, ymin, ymax, zmin, zmax = self.axis_aligned_bbox\n        (x0, y0, z0) = np.array([xmin+xmax, ymin+ymax, zmin+zmax]) * 0.5\n        d = (np.array([xmax-xmin, ymax-ymin, zmax-zmin]) * 0.5).max()\n\n        return x0-d, x0+d, y0-d, y0+d, z0-d, z0+d\n\n    def show(self, **kwargs):\n        self.show_vtk(**kwargs)\n\n    def show_vtk(self, **kwargs):\n        \"\"\"Shows the mesh in the vtk viewer\"\"\"\n        from capytaine.ui.vtk.mesh_viewer import MeshViewer\n\n        viewer = MeshViewer()\n        viewer.add_mesh(self, **kwargs)\n        viewer.show()\n        viewer.finalize()\n\n    def show_matplotlib(self, ax=None,\n                        normal_vectors=False, scale_normal_vector=None,\n                        saveas=None, color_field=None, cmap=None,\n                        cbar_label=None,\n                        **kwargs):\n        \"\"\"Poor man's viewer with matplotlib.\n\n        Parameters\n        ----------\n        ax: matplotlib axis\n            The 3d axis in which to plot the mesh. If not provided, create a new one.\n        normal_vectors: bool\n            If True, print normal vector.\n        scale_normal_vector: array of shape (nb_faces, )\n            Scale separately each of the normal vectors.\n        saveas: str\n            File path where to save the image.\n        color_field: array of shape (nb_faces, )\n            Scalar field to be plot on the mesh (optional).\n        cmap: matplotlib colormap\n            Colormap to use for field plotting.\n        cbar_label: string\n            Label for colormap\n\n        Other parameters are passed to Poly3DCollection.\n        \"\"\"\n        matplotlib = import_optional_dependency(\"matplotlib\")\n        plt = matplotlib.pyplot\n        cm = matplotlib.cm\n\n        mpl_toolkits = import_optional_dependency(\"mpl_toolkits\", package_name=\"matplotlib\")\n        Poly3DCollection = mpl_toolkits.mplot3d.art3d.Poly3DCollection\n\n        default_axis = ax is None\n        if default_axis:\n            fig = plt.figure()\n            ax = fig.add_subplot(111, projection=\"3d\")\n\n        faces = []\n        for face in self.faces:\n            vertices = []\n            for index_vertex in face:\n                vertices.append(self.vertices[int(index_vertex), :])\n            faces.append(vertices)\n\n        if color_field is None:\n            if 'facecolors' not in kwargs:\n                kwargs['facecolors'] = \"yellow\"\n        else:\n            if cmap is None:\n                cmap = matplotlib.colormaps['coolwarm']\n            m = cm.ScalarMappable(cmap=cmap)\n            m.set_array([min(color_field), max(color_field)])\n            m.set_clim(vmin=min(color_field), vmax=max(color_field))\n            colors = m.to_rgba(color_field)\n            kwargs['facecolors'] = colors\n        if 'edgecolor' not in kwargs:\n            kwargs['edgecolor'] = 'k'\n\n        ax.add_collection3d(Poly3DCollection(faces, **kwargs))\n\n        if color_field is not None:\n            cbar = plt.colorbar(m, ax=ax)\n            if cbar_label is not None:\n                cbar.set_label(cbar_label)\n\n\n\n        # Plot normal vectors.\n        if normal_vectors:\n            if scale_normal_vector is not None:\n                vectors = (scale_normal_vector * self.faces_normals.T).T\n            else:\n                vectors = self.faces_normals\n            ax.quiver(*zip(*self.faces_centers), *zip(*vectors), length=0.2)\n\n\n        ax.set_xlabel(\"x\")\n        ax.set_ylabel(\"y\")\n        ax.set_zlabel(\"z\")\n\n        xmin, xmax, ymin, ymax, zmin, zmax = self.squared_axis_aligned_bbox\n        ax.set_xlim(xmin, xmax)\n        ax.set_ylim(ymin, ymax)\n        ax.set_zlim(zmin, zmax)\n\n        if default_axis:\n            if saveas is not None:\n                plt.tight_layout()\n                plt.savefig(saveas)\n            else:\n                plt.show()\n\n    ################################\n    #  Transformation of the mesh  #\n    ################################\n\n    @inplace_transformation\n    def translate(self, vector) -> 'Mesh':\n        \"\"\"Translates the mesh in 3D giving the 3 distances along coordinate axes.\n\n        Parameters\n        ----------\n        vector : array_like\n            translation vector\n        \"\"\"\n        vector = np.asarray(vector, dtype=float)\n        assert vector.shape == (3,), \"The translation vector should be given as a 3-ple of values.\"\n\n        self.vertices += vector\n\n        return self\n\n    @inplace_transformation\n    def rotate(self, axis, angle) -> 'Mesh':\n        \"\"\"Rotate the mesh of a given angle around an axis.\n\n        Parameters\n        ----------\n        axis : Axis\n        angle : float\n        \"\"\"\n\n        self._vertices = axis.rotate_points(self._vertices, angle)\n\n        return self\n\n    # OTHER\n    @inplace_transformation\n    def flip_normals(self) -> 'Mesh':\n        \"\"\"Flips every normals of the mesh.\"\"\"\n\n        self._faces = np.fliplr(self._faces)\n\n        return self\n\n    @inplace_transformation\n    def mirror(self, plane) -> 'Mesh':\n        \"\"\"Flip the mesh with respect to a plane.\n\n        Parameters\n        ----------\n        plane : Plane\n            The mirroring plane\n        \"\"\"\n        self.vertices -= 2 * np.outer(np.dot(self.vertices, plane.normal) - plane.c, plane.normal)\n        self.flip_normals()\n        return self\n\n    def symmetrized(self, plane):\n        from capytaine.meshes.symmetric import ReflectionSymmetricMesh\n        half = self.clipped(plane, name=f\"{self.name}_half\")\n        return ReflectionSymmetricMesh(half, plane=plane, name=f\"symmetrized_of_{self.name}\")\n\n    @inplace_transformation\n    def clip(self, plane) -> 'Mesh':\n        from capytaine.meshes.clipper import clip\n        clipped_self = clip(self, plane=plane)\n        self.vertices = clipped_self.vertices\n        self.faces = clipped_self.faces\n        self._clipping_data = clipped_self._clipping_data\n        return self\n\n    @inplace_transformation\n    def triangulate_quadrangles(self) -> 'Mesh':\n        \"\"\"Triangulates every quadrangles of the mesh by simple splitting.\n        Each quadrangle gives two triangles.\n\n        Note\n        ----\n        No checking on the triangle quality is done.\n        \"\"\"\n        # Defining both triangles id lists to be generated from quadrangles\n        t1 = (0, 1, 2)\n        t2 = (0, 2, 3)\n\n        faces = self._faces\n\n        # Triangulation\n        new_faces = faces[self.quadrangles_ids].copy()\n        new_faces[:, :3] = new_faces[:, t1]\n        new_faces[:, -1] = new_faces[:, 0]\n\n        faces[self.quadrangles_ids, :3] = faces[:, t2][self.quadrangles_ids]\n        faces[self.quadrangles_ids, -1] = faces[self.quadrangles_ids, 0]\n\n        faces = np.concatenate((faces, new_faces))\n\n        LOG.info('\\nTriangulating quadrangles')\n        if self.nb_quadrangles != 0:\n            LOG.info('\\t-->{:d} quadrangles have been split in triangles'.format(self.nb_quadrangles))\n\n        self._faces = faces\n\n        return self\n\n    ####################\n    #  Combine meshes  #\n    ####################\n\n    def join_meshes(*meshes, name=None):\n        from capytaine.meshes.collections import CollectionOfMeshes\n        return CollectionOfMeshes(meshes, name=name).merged()\n\n    def __add__(self, mesh_to_add) -> 'Mesh':\n        return self.join_meshes(mesh_to_add)\n\n    ####################\n    #  Compare meshes  #\n    ####################\n    # The objective is to write a mesh as a set of faces in order to check for equality or to\n    # compute differences of meshes. Each face can be represented as a 4x3 array (4 triplets of\n    # coordinates).\n    # However, it is tricky on several aspects:\n    #   * The builtin set class compares the hashes of its objects. Since numpy ndarray are not\n    #   hashable, the 4x3 array can be transformed into a tuple of tuples (which is hashable).\n    #   * Two faces with different numbering of the faces (but the same ordering) are recorded as\n    #   different.\n    #   * Two vertices equal up to machine precision can be recorded as different, due to rounding\n    #   errors.\n    #\n    # A possible solution is to define a Face class and a Vertex class with the appropriate __eq__\n    # and __hash__.\n    #\n    # The current implementation below is a rough draft.\n    # However, the equality shall still be use for testing.\n\n    def as_set_of_faces(self):\n        return frozenset(frozenset(tuple(vertex) for vertex in face) for face in self.vertices[self.faces])\n\n    @staticmethod\n    def from_set_of_faces(set_of_faces):\n        faces = []\n        vertices = []\n        for face in set_of_faces:\n            ids_of_vertices_in_face = []\n\n            for vertex in face:\n                if vertex not in vertices:\n                    i = len(vertices)\n                    vertices.append(vertex)\n                else:\n                    i = vertices.index(vertex)\n                ids_of_vertices_in_face.append(i)\n\n            if len(ids_of_vertices_in_face) == 3:\n                # Add a fourth node identical to the first one\n                ids_of_vertices_in_face.append(ids_of_vertices_in_face[0])\n\n            faces.append(ids_of_vertices_in_face)\n        return Mesh(vertices=vertices, faces=faces)\n\n    def __eq__(self, other):\n        if not isinstance(other, Mesh):\n            return NotImplemented\n        else:\n            return self.as_set_of_faces() == other.as_set_of_faces()\n\n    def __hash__(self):\n        if 'hash' not in self.__internals__:\n            self.__internals__['hash'] = hash(self.as_set_of_faces())\n        return self.__internals__['hash']\n\n    ##################\n    #  Mesh quality  #\n    ##################\n\n    def merge_duplicates(self, **kwargs):\n        return merge_duplicates(self, **kwargs)\n\n    def heal_normals(self, **kwargs):\n        return heal_normals(self, **kwargs)\n\n    def remove_unused_vertices(self, **kwargs):\n        return remove_unused_vertices(self, **kwargs)\n\n    def heal_triangles(self, **kwargs):\n        return heal_triangles(self, **kwargs)\n\n    def remove_degenerated_faces(self, **kwargs):\n        return remove_degenerated_faces(self, **kwargs)\n\n    @inplace_transformation\n    def heal_mesh(self, closed_mesh=True):\n        \"\"\"Heals the mesh for different tests available.\n\n        It applies:\n\n        * Unused vertices removal\n        * Degenerate faces removal\n        * Duplicate vertices merging\n        * Triangles healing\n        * Normal healing\n        \"\"\"\n        self.remove_unused_vertices()\n        self.remove_degenerated_faces()\n        self.merge_duplicates()\n        self.heal_triangles()\n        if closed_mesh:\n            self.heal_normals()\n        return self",
  "def __init__(self, vertices=None, faces=None, name=None):\n\n        if vertices is None or len(vertices) == 0:\n            vertices = np.zeros((0, 3))\n\n        if faces is None or len(faces) == 0:\n            faces = np.zeros((0, 4))\n\n        if name is None:\n            self.name = f'mesh_{next(Mesh._ids)}'\n        else:\n            self.name = str(name)\n\n        self.__internals__ = dict()\n        self.vertices = vertices  # Not a direct assignment, goes through the setter method below.\n        self.faces = faces  # Not a direct assignment, goes through the setter method below.\n\n        LOG.debug(f\"New mesh: {repr(self)}\")",
  "def __short_str__(self):\n        return (f\"{self.__class__.__name__}(..., name=\\\"{self.name}\\\")\")",
  "def __str__(self):\n        return (f\"{self.__class__.__name__}(vertices=[[... {self.nb_vertices} vertices ...]], \"\n                f\"faces=[[... {self.nb_faces} faces ...]], name=\\\"{self.name}\\\")\")",
  "def __repr__(self):\n        # shift = len(self.__class__.__name__) + 1\n        # vert_str = np.array_repr(self.vertices).replace('\\n', '\\n' + (shift + 9)*' ')\n        # faces_str = np.array_repr(self.faces).replace('\\n', '\\n' + (shift + 6)*' ')\n        # return f\"{self.__class__.__name__}(\\n{' '*shift}vertices={vert_str},\\n{' '*shift}faces={faces_str}\\n{' '*shift}name=\\\"{self.name}\\\"\\n)\"\n        return (f\"{self.__class__.__name__}(vertices=[[... {self.nb_vertices} vertices ...]], \"\n                f\"faces=[[... {self.nb_faces} faces ...]], name=\\\"{self.name}\\\")\")",
  "def _repr_pretty_(self, p, cycle):\n        p.text(self.__str__())",
  "def __rich_repr__(self):\n        class CustomRepr:\n            def __init__(self, n, kind):\n                self.n = n\n                self.kind = kind\n            def __repr__(self):\n                return \"[[... {} {} ...]]\".format(self.n, self.kind)\n        yield \"vertices\", CustomRepr(self.nb_vertices, \"vertices\")\n        yield \"faces\", CustomRepr(self.nb_faces, \"faces\")\n        yield \"name\", self.name",
  "def nb_vertices(self) -> int:\n        \"\"\"Get the number of vertices in the mesh.\"\"\"\n        return self._vertices.shape[0]",
  "def vertices(self) -> np.ndarray:\n        \"\"\"Get the vertices array coordinate of the mesh.\"\"\"\n        return self._vertices",
  "def vertices(self, value) -> None:\n        self._vertices = np.array(value, dtype=float)\n        assert self._vertices.shape[1] == 3, \\\n            \"Vertices of a mesh should be provided as a sequence of 3-ple.\"\n        self.__internals__.clear()",
  "def nb_faces(self) -> int:\n        \"\"\"Get the number of faces in the mesh.\"\"\"\n        return self._faces.shape[0]",
  "def faces(self) -> np.ndarray:\n        \"\"\"Get the faces connectivity array of the mesh.\"\"\"\n        return self._faces",
  "def faces(self, faces):\n        faces = np.array(faces, dtype=int)\n        assert np.all(faces >= 0), \\\n            \"Faces of a mesh should be provided as positive integers (ids of vertices)\"\n        assert faces.shape[1] == 4, \\\n            \"Faces of a mesh should be provided as a sequence of 4-ple.\"\n        assert len(faces) == 0 or faces.max()+1 <= self.nb_vertices, \\\n            \"The array of faces should only reference vertices that are in the mesh.\"\n        self._faces = faces\n        self.__internals__.clear()",
  "def copy(self, name=None) -> 'Mesh':\n        \"\"\"Get a copy of the current mesh instance.\n\n        Parameters\n        ----------\n        name : string, optional\n            a name for the new mesh\n\n        Returns\n        -------\n        Mesh\n            mesh instance copy\n        \"\"\"\n        from copy import deepcopy\n        new_mesh = deepcopy(self)\n        if name is not None:\n            new_mesh.name = name\n        return new_mesh",
  "def merged(self):\n        \"\"\"Dummy method to be generalized for collections of meshes.\"\"\"\n        return self",
  "def tree_view(self, **kwargs):\n        \"\"\"Dummy method to be generalized for collections of meshes.\"\"\"\n        return self.__short_str__()",
  "def to_meshmagick(self):\n        \"\"\"Convert the Mesh object as a Mesh object from meshmagick.\n        Mostly for debugging.\"\"\"\n        from meshmagick.mesh import Mesh\n        meshmagick_mesh = Mesh(self.vertices, self.faces, name=self.name)\n        meshmagick_mesh.heal_mesh()\n        return meshmagick_mesh",
  "def get_face(self, face_id):\n        \"\"\"Get the face described by its vertices connectivity.\n\n        Parameters\n        ----------\n        face_id : int\n            Face id\n\n        Returns\n        -------\n        ndarray\n            If the face is a triangle, the array has 3 components, else it has 4 (quadrangle)\n        \"\"\"\n        if self.is_triangle(face_id):\n            return self._faces[face_id, :3]\n        else:\n            return self._faces[face_id]",
  "def extract_one_face(self, id_face):\n        vertices = self.vertices[self.faces[id_face, :], :]\n        mesh = Mesh(vertices=vertices, faces=np.array([[0, 1, 2, 3]]), name=f\"single_face_from_{self.name}\")\n\n        for prop in self.__internals__:\n            if prop[:4] == \"face\":\n                mesh.__internals__[prop] = self.__internals__[prop][[id_face]]\n\n        return mesh",
  "def extract_faces(self, id_faces_to_extract, return_index=False, name=None):\n        \"\"\"\n        Extracts a new mesh from a selection of faces ids\n\n        Parameters\n        ----------\n        id_faces_to_extract : ndarray\n            Indices of faces that have to be extracted\n        return_index: bool, optional\n            Flag to output old indices\n        name: string, optional\n            Name for the new mesh\n\n        Returns\n        -------\n        Mesh\n            A new Mesh instance composed of the extracted faces\n        \"\"\"\n        nv = self.nb_vertices\n\n        # Determination of the vertices to keep\n        vertices_mask = np.zeros(nv, dtype=bool)\n        vertices_mask[self._faces[id_faces_to_extract].flatten()] = True\n        id_v = np.arange(nv)[vertices_mask]\n\n        # Building up the vertex array\n        v_extracted = self._vertices[id_v]\n        new_id__v = np.arange(nv)\n        new_id__v[id_v] = np.arange(len(id_v))\n\n        faces_extracted = self._faces[id_faces_to_extract]\n        faces_extracted = new_id__v[faces_extracted.flatten()].reshape((len(id_faces_to_extract), 4))\n\n        extracted_mesh = Mesh(v_extracted, faces_extracted)\n\n        for prop in self.__internals__:\n            if prop[:4] == \"face\":\n                extracted_mesh.__internals__[prop] = self.__internals__[prop][id_faces_to_extract]\n\n        if name is None:\n            if self.name is not None and self.name.startswith(\"mesh_extracted_from_\"):\n                extracted_mesh.name = self.name\n            else:\n                extracted_mesh.name = f\"mesh_extracted_from_{self.name}\"\n        else:\n            extracted_mesh.name = name\n\n        if return_index:\n            return extracted_mesh, id_v\n        else:\n            return extracted_mesh",
  "def sliced_by_plane(self, plane: Plane):\n        from capytaine.meshes.collections import CollectionOfMeshes\n        faces_ids_on_one_side = np.where(plane.distance_to_point(self.faces_centers) < 0)[0]\n        if len(faces_ids_on_one_side) == 0 or len(faces_ids_on_one_side) == self.nb_faces:\n            return self.copy()\n        else:\n            mesh_part_1 = self.extract_faces(faces_ids_on_one_side)\n            mesh_part_2 = self.extract_faces(list(set(range(self.nb_faces)) - set(faces_ids_on_one_side)))\n            return CollectionOfMeshes([mesh_part_1, mesh_part_2],\n                                      name=f\"{self.name}_splitted_by_{plane}\")",
  "def center_of_mass_of_nodes(self):\n        \"\"\"(Non-weighted) center of mass of the nodes of the mesh.\"\"\"\n        if 'center_of_mass_of_nodes' not in self.__internals__:\n            center_of_mass_of_nodes = np.mean(self.vertices, axis=0)\n            self.__internals__['center_of_mass_of_nodes'] = center_of_mass_of_nodes\n            return center_of_mass_of_nodes\n        return self.__internals__['center_of_mass_of_nodes']",
  "def diameter_of_nodes(self):\n        \"\"\"Maximum distance between two nodes of the mesh.\"\"\"\n        if 'diameter_of_nodes' not in self.__internals__:\n            diameter_of_nodes = 2*np.max(\n                np.linalg.norm(self.vertices - self.center_of_mass_of_nodes, axis=-1)\n            )\n            self.__internals__['diameter_of_nodes'] = diameter_of_nodes\n            return diameter_of_nodes\n        return self.__internals__['diameter_of_nodes']",
  "def faces_areas(self) -> np.ndarray:\n        \"\"\"Get the array of faces areas of the mesh.\"\"\"\n        if 'faces_areas' not in self.__internals__:\n            self.__internals__.update(compute_faces_properties(self))\n        return self.__internals__['faces_areas']",
  "def faces_centers(self) -> np.ndarray:\n        \"\"\"Get the array of faces centers of the mesh.\"\"\"\n        if 'faces_centers' not in self.__internals__:\n            self.__internals__.update(compute_faces_properties(self))\n        return self.__internals__['faces_centers']",
  "def faces_normals(self) -> np.ndarray:\n        \"\"\"Get the array of faces normals of the mesh.\"\"\"\n        if 'faces_normals' not in self.__internals__:\n            self.__internals__.update(compute_faces_properties(self))\n        return self.__internals__['faces_normals']",
  "def faces_radiuses(self) -> np.ndarray:\n        \"\"\"Get the array of faces radiuses of the mesh.\"\"\"\n        if 'faces_radiuses' not in self.__internals__:\n            self.__internals__.update(compute_faces_properties(self))\n        return self.__internals__['faces_radiuses']",
  "def quadrature_points(self):\n        if 'quadrature' in self.__internals__:\n            return self.__internals__['quadrature']\n        else:\n            # Default: first order quadrature\n            return (\n                self.faces_centers.reshape((self.nb_faces, 1, 3)),  # Points\n                self.faces_areas.reshape((self.nb_faces, 1))        # Weights\n            )",
  "def quadrature_method(self):\n        if 'quadrature_method' in self.__internals__:\n            return self.__internals__['quadrature_method']\n        else:\n            return None",
  "def compute_quadrature(self, method):\n        quadpy = import_optional_dependency(\"quadpy\")\n        transform = quadpy.c2.transform\n        get_detJ = quadpy.cn._helpers.get_detJ\n\n        if method is None:\n            # No quadrature (i.e. default first order quadrature)\n            if 'quadrature' in self.__internals__:\n                del self.__internals__['quadrature']\n                del self.__internals__['quadrature_method']\n            else:\n                pass\n\n        elif isinstance(method, quadpy.c2._helpers.C2Scheme):\n            assert method.points.shape[0] == method.dim == 2\n            nb_points = method.points.shape[1]\n            points = np.empty((self.nb_faces, nb_points, 3))\n            weights = np.empty((self.nb_faces, nb_points))\n\n            self.heal_triangles()\n\n            for i_face in range(self.nb_faces):\n                # Define a local frame (Oxyz) such that\n                # * the corner A of the quadrilateral panel is the origin of the local frame\n                # * the edge AB of the quadrilateral panel is along the local x-axis,\n                # * the quadrilateral panel is within the local xy-plane (that is, its normal is along the local z-axis).\n                # Hence, the corners of the panels all have 0 as z-coordinate in the local frame.\n\n                # Coordinates in global frame\n                global_A, global_B, global_C, global_D = self.vertices[self.faces[i_face, :], :]\n                n = self.faces_normals[i_face, :]\n\n                ex = (global_B-global_A)/norm(global_B-global_A)  # unit vector of the local x-axis\n                ez = n/norm(n)                                    # unit vector of the local z-axis\n                ey = np.cross(ex, ez)                             # unit vector of the local y-axis, such that the basis is orthonormal\n\n                R = np.array([ex, ey, ez])\n                local_A = np.zeros((3,))             # coordinates of A in local frame, should be zero by construction\n                local_B = R @ (global_B - global_A)  # coordinates of B in local frame\n                local_C = R @ (global_C - global_A)  # coordinates of C in local frame\n                local_D = R @ (global_D - global_A)  # coordinates of D in local frame\n\n                local_quadrilateral = np.array([[local_A, local_D], [local_B, local_C]])[:, :, :-1]\n                # Removing last index in last dimension because not interested in z-coordinate which is 0.\n\n                local_quadpoints = transform(method.points, local_quadrilateral)\n\n                local_quadpoints_in_3d = np.concatenate([local_quadpoints, np.zeros((nb_points, 1))], axis=1)\n                global_quadpoints = np.array([R.T @ p for p in local_quadpoints_in_3d]) + global_A\n                points[i_face, :, :] = global_quadpoints\n\n                weights[i_face, :] = method.weights * 4 * np.abs(get_detJ(method.points, local_quadrilateral))\n\n            self.__internals__['quadrature'] = (points, weights)\n            self.__internals__['quadrature_method'] = method\n\n        else:\n            raise NotImplementedError",
  "def is_triangle(self, face_id) -> bool:\n        \"\"\"Returns if a face is a triangle\n\n        Parameters\n        ----------\n        face_id : int\n            Face id\n        \"\"\"\n        assert 0 <= face_id < self.nb_faces\n        return self._faces[face_id, 0] == self._faces[face_id, -1]",
  "def _compute_triangles_quadrangles(self):\n        triangle_mask = (self._faces[:, 0] == self._faces[:, -1])\n        quadrangles_mask = np.invert(triangle_mask)\n        triangles_quadrangles = {'triangles_ids': np.where(triangle_mask)[0],\n                                 'quadrangles_ids': np.where(quadrangles_mask)[0]}\n        self.__internals__.update(triangles_quadrangles)",
  "def triangles_ids(self) -> np.ndarray:\n        \"\"\"Get the array of ids of triangle shaped faces.\"\"\"\n        if 'triangles_ids' not in self.__internals__:\n            self._compute_triangles_quadrangles()\n        return self.__internals__['triangles_ids']",
  "def nb_triangles(self) -> int:\n        \"\"\"Get the number of triangles in the mesh.\"\"\"\n        if 'triangles_ids'not in self.__internals__:\n            self._compute_triangles_quadrangles()\n        return len(self.__internals__['triangles_ids'])",
  "def quadrangles_ids(self) -> np.ndarray:\n        \"\"\"Get the array of ids of quadrangle shaped faces.\"\"\"\n        if 'triangles_ids' not in self.__internals__:\n            self._compute_triangles_quadrangles()\n        return self.__internals__['quadrangles_ids']",
  "def nb_quadrangles(self) -> int:\n        \"\"\"Get the number of quadrangles in the mesh.\"\"\"\n        if 'triangles_ids' not in self.__internals__:\n            self._compute_triangles_quadrangles()\n        return len(self.__internals__['quadrangles_ids'])",
  "def axis_aligned_bbox(self):\n        \"\"\"Get the axis aligned bounding box of the mesh.\n\n        Returns\n        -------\n        tuple\n            (xmin, xmax, ymin, ymax, zmin, zmax)\n        \"\"\"\n        if self.nb_vertices > 0:\n            x, y, z = self._vertices.T\n            return (x.min(), x.max(),\n                    y.min(), y.max(),\n                    z.min(), z.max())\n        else:\n            return tuple(np.zeros(6))",
  "def squared_axis_aligned_bbox(self):\n        \"\"\"Get a squared axis aligned bounding box of the mesh.\n\n        Returns\n        -------\n        tuple\n            (xmin, xmax, ymin, ymax, zmin, zmax)\n\n        Note\n        ----\n        This method differs from `axis_aligned_bbox()` by the fact that\n        the bounding box that is returned is squared but have the same center as the `axis_aligned_bbox()`.\n        \"\"\"\n        xmin, xmax, ymin, ymax, zmin, zmax = self.axis_aligned_bbox\n        (x0, y0, z0) = np.array([xmin+xmax, ymin+ymax, zmin+zmax]) * 0.5\n        d = (np.array([xmax-xmin, ymax-ymin, zmax-zmin]) * 0.5).max()\n\n        return x0-d, x0+d, y0-d, y0+d, z0-d, z0+d",
  "def show(self, **kwargs):\n        self.show_vtk(**kwargs)",
  "def show_vtk(self, **kwargs):\n        \"\"\"Shows the mesh in the vtk viewer\"\"\"\n        from capytaine.ui.vtk.mesh_viewer import MeshViewer\n\n        viewer = MeshViewer()\n        viewer.add_mesh(self, **kwargs)\n        viewer.show()\n        viewer.finalize()",
  "def show_matplotlib(self, ax=None,\n                        normal_vectors=False, scale_normal_vector=None,\n                        saveas=None, color_field=None, cmap=None,\n                        cbar_label=None,\n                        **kwargs):\n        \"\"\"Poor man's viewer with matplotlib.\n\n        Parameters\n        ----------\n        ax: matplotlib axis\n            The 3d axis in which to plot the mesh. If not provided, create a new one.\n        normal_vectors: bool\n            If True, print normal vector.\n        scale_normal_vector: array of shape (nb_faces, )\n            Scale separately each of the normal vectors.\n        saveas: str\n            File path where to save the image.\n        color_field: array of shape (nb_faces, )\n            Scalar field to be plot on the mesh (optional).\n        cmap: matplotlib colormap\n            Colormap to use for field plotting.\n        cbar_label: string\n            Label for colormap\n\n        Other parameters are passed to Poly3DCollection.\n        \"\"\"\n        matplotlib = import_optional_dependency(\"matplotlib\")\n        plt = matplotlib.pyplot\n        cm = matplotlib.cm\n\n        mpl_toolkits = import_optional_dependency(\"mpl_toolkits\", package_name=\"matplotlib\")\n        Poly3DCollection = mpl_toolkits.mplot3d.art3d.Poly3DCollection\n\n        default_axis = ax is None\n        if default_axis:\n            fig = plt.figure()\n            ax = fig.add_subplot(111, projection=\"3d\")\n\n        faces = []\n        for face in self.faces:\n            vertices = []\n            for index_vertex in face:\n                vertices.append(self.vertices[int(index_vertex), :])\n            faces.append(vertices)\n\n        if color_field is None:\n            if 'facecolors' not in kwargs:\n                kwargs['facecolors'] = \"yellow\"\n        else:\n            if cmap is None:\n                cmap = matplotlib.colormaps['coolwarm']\n            m = cm.ScalarMappable(cmap=cmap)\n            m.set_array([min(color_field), max(color_field)])\n            m.set_clim(vmin=min(color_field), vmax=max(color_field))\n            colors = m.to_rgba(color_field)\n            kwargs['facecolors'] = colors\n        if 'edgecolor' not in kwargs:\n            kwargs['edgecolor'] = 'k'\n\n        ax.add_collection3d(Poly3DCollection(faces, **kwargs))\n\n        if color_field is not None:\n            cbar = plt.colorbar(m, ax=ax)\n            if cbar_label is not None:\n                cbar.set_label(cbar_label)\n\n\n\n        # Plot normal vectors.\n        if normal_vectors:\n            if scale_normal_vector is not None:\n                vectors = (scale_normal_vector * self.faces_normals.T).T\n            else:\n                vectors = self.faces_normals\n            ax.quiver(*zip(*self.faces_centers), *zip(*vectors), length=0.2)\n\n\n        ax.set_xlabel(\"x\")\n        ax.set_ylabel(\"y\")\n        ax.set_zlabel(\"z\")\n\n        xmin, xmax, ymin, ymax, zmin, zmax = self.squared_axis_aligned_bbox\n        ax.set_xlim(xmin, xmax)\n        ax.set_ylim(ymin, ymax)\n        ax.set_zlim(zmin, zmax)\n\n        if default_axis:\n            if saveas is not None:\n                plt.tight_layout()\n                plt.savefig(saveas)\n            else:\n                plt.show()",
  "def translate(self, vector) -> 'Mesh':\n        \"\"\"Translates the mesh in 3D giving the 3 distances along coordinate axes.\n\n        Parameters\n        ----------\n        vector : array_like\n            translation vector\n        \"\"\"\n        vector = np.asarray(vector, dtype=float)\n        assert vector.shape == (3,), \"The translation vector should be given as a 3-ple of values.\"\n\n        self.vertices += vector\n\n        return self",
  "def rotate(self, axis, angle) -> 'Mesh':\n        \"\"\"Rotate the mesh of a given angle around an axis.\n\n        Parameters\n        ----------\n        axis : Axis\n        angle : float\n        \"\"\"\n\n        self._vertices = axis.rotate_points(self._vertices, angle)\n\n        return self",
  "def flip_normals(self) -> 'Mesh':\n        \"\"\"Flips every normals of the mesh.\"\"\"\n\n        self._faces = np.fliplr(self._faces)\n\n        return self",
  "def mirror(self, plane) -> 'Mesh':\n        \"\"\"Flip the mesh with respect to a plane.\n\n        Parameters\n        ----------\n        plane : Plane\n            The mirroring plane\n        \"\"\"\n        self.vertices -= 2 * np.outer(np.dot(self.vertices, plane.normal) - plane.c, plane.normal)\n        self.flip_normals()\n        return self",
  "def symmetrized(self, plane):\n        from capytaine.meshes.symmetric import ReflectionSymmetricMesh\n        half = self.clipped(plane, name=f\"{self.name}_half\")\n        return ReflectionSymmetricMesh(half, plane=plane, name=f\"symmetrized_of_{self.name}\")",
  "def clip(self, plane) -> 'Mesh':\n        from capytaine.meshes.clipper import clip\n        clipped_self = clip(self, plane=plane)\n        self.vertices = clipped_self.vertices\n        self.faces = clipped_self.faces\n        self._clipping_data = clipped_self._clipping_data\n        return self",
  "def triangulate_quadrangles(self) -> 'Mesh':\n        \"\"\"Triangulates every quadrangles of the mesh by simple splitting.\n        Each quadrangle gives two triangles.\n\n        Note\n        ----\n        No checking on the triangle quality is done.\n        \"\"\"\n        # Defining both triangles id lists to be generated from quadrangles\n        t1 = (0, 1, 2)\n        t2 = (0, 2, 3)\n\n        faces = self._faces\n\n        # Triangulation\n        new_faces = faces[self.quadrangles_ids].copy()\n        new_faces[:, :3] = new_faces[:, t1]\n        new_faces[:, -1] = new_faces[:, 0]\n\n        faces[self.quadrangles_ids, :3] = faces[:, t2][self.quadrangles_ids]\n        faces[self.quadrangles_ids, -1] = faces[self.quadrangles_ids, 0]\n\n        faces = np.concatenate((faces, new_faces))\n\n        LOG.info('\\nTriangulating quadrangles')\n        if self.nb_quadrangles != 0:\n            LOG.info('\\t-->{:d} quadrangles have been split in triangles'.format(self.nb_quadrangles))\n\n        self._faces = faces\n\n        return self",
  "def join_meshes(*meshes, name=None):\n        from capytaine.meshes.collections import CollectionOfMeshes\n        return CollectionOfMeshes(meshes, name=name).merged()",
  "def __add__(self, mesh_to_add) -> 'Mesh':\n        return self.join_meshes(mesh_to_add)",
  "def as_set_of_faces(self):\n        return frozenset(frozenset(tuple(vertex) for vertex in face) for face in self.vertices[self.faces])",
  "def from_set_of_faces(set_of_faces):\n        faces = []\n        vertices = []\n        for face in set_of_faces:\n            ids_of_vertices_in_face = []\n\n            for vertex in face:\n                if vertex not in vertices:\n                    i = len(vertices)\n                    vertices.append(vertex)\n                else:\n                    i = vertices.index(vertex)\n                ids_of_vertices_in_face.append(i)\n\n            if len(ids_of_vertices_in_face) == 3:\n                # Add a fourth node identical to the first one\n                ids_of_vertices_in_face.append(ids_of_vertices_in_face[0])\n\n            faces.append(ids_of_vertices_in_face)\n        return Mesh(vertices=vertices, faces=faces)",
  "def __eq__(self, other):\n        if not isinstance(other, Mesh):\n            return NotImplemented\n        else:\n            return self.as_set_of_faces() == other.as_set_of_faces()",
  "def __hash__(self):\n        if 'hash' not in self.__internals__:\n            self.__internals__['hash'] = hash(self.as_set_of_faces())\n        return self.__internals__['hash']",
  "def merge_duplicates(self, **kwargs):\n        return merge_duplicates(self, **kwargs)",
  "def heal_normals(self, **kwargs):\n        return heal_normals(self, **kwargs)",
  "def remove_unused_vertices(self, **kwargs):\n        return remove_unused_vertices(self, **kwargs)",
  "def heal_triangles(self, **kwargs):\n        return heal_triangles(self, **kwargs)",
  "def remove_degenerated_faces(self, **kwargs):\n        return remove_degenerated_faces(self, **kwargs)",
  "def heal_mesh(self, closed_mesh=True):\n        \"\"\"Heals the mesh for different tests available.\n\n        It applies:\n\n        * Unused vertices removal\n        * Degenerate faces removal\n        * Duplicate vertices merging\n        * Triangles healing\n        * Normal healing\n        \"\"\"\n        self.remove_unused_vertices()\n        self.remove_degenerated_faces()\n        self.merge_duplicates()\n        self.heal_triangles()\n        if closed_mesh:\n            self.heal_normals()\n        return self",
  "class CustomRepr:\n            def __init__(self, n, kind):\n                self.n = n\n                self.kind = kind\n            def __repr__(self):\n                return \"[[... {} {} ...]]\".format(self.n, self.kind)",
  "def __init__(self, n, kind):\n                self.n = n\n                self.kind = kind",
  "def __repr__(self):\n                return \"[[... {} {} ...]]\".format(self.n, self.kind)",
  "class SurfaceIntegralsMixin(ABC):\n    # These methods need to be defined for both Mesh and CollectionOfMeshes with the exact same definitions.\n    # To avoid redunduncy, they are defined here in a mixin inherited by both Mesh and CollectionOfMeshes.\n\n    def surface_integral(self, data, **kwargs):\n        \"\"\"Returns integral of given data along wet surface area.\"\"\"\n        return np.sum(data * self.faces_areas, **kwargs)\n\n    def waterplane_integral(self, data, **kwargs):\n        \"\"\"Returns integral of given data along water plane area.\"\"\"\n        return self.surface_integral(self.faces_normals[:,2] * data, **kwargs)\n\n    @property\n    def wet_surface_area(self):\n        \"\"\"Returns wet surface area.\"\"\"\n        return self.surface_integral(1)\n\n    @property\n    def volumes(self):\n        \"\"\"Returns volumes using x, y, z components of the mesh.\"\"\"\n        norm_coord = self.faces_normals * self.faces_centers\n        return self.surface_integral(norm_coord.T, axis=1)\n\n    @property\n    def volume(self):\n        \"\"\"Returns volume of the mesh.\"\"\"\n        return np.mean(self.volumes)\n\n    def disp_mass(self, *, rho=1000):\n        return rho * self.volume\n\n    @property\n    def center_of_buoyancy(self):\n        \"\"\"Returns center of buoyancy of the mesh.\"\"\"\n        coords_sq_norm = self.faces_normals * self.faces_centers**2\n        return self.surface_integral(coords_sq_norm.T, axis=1) / (2*self.volume)\n\n    @property\n    def waterplane_area(self):\n        \"\"\"Returns water plane area of the mesh.\"\"\"\n        waterplane_area = -self.waterplane_integral(1)\n        return waterplane_area\n\n    @property\n    def waterplane_center(self):\n        \"\"\"Returns water plane center of the mesh.\n\n        Note: Returns None if the mesh is full submerged.\n        \"\"\"\n        waterplane_area = self.waterplane_area\n        if abs(waterplane_area) < 1e-10:\n            return None\n        else:\n            waterplane_center = -self.waterplane_integral(\n                self.faces_centers.T, axis=1) / waterplane_area\n            return waterplane_center[:-1]",
  "def surface_integral(self, data, **kwargs):\n        \"\"\"Returns integral of given data along wet surface area.\"\"\"\n        return np.sum(data * self.faces_areas, **kwargs)",
  "def waterplane_integral(self, data, **kwargs):\n        \"\"\"Returns integral of given data along water plane area.\"\"\"\n        return self.surface_integral(self.faces_normals[:,2] * data, **kwargs)",
  "def wet_surface_area(self):\n        \"\"\"Returns wet surface area.\"\"\"\n        return self.surface_integral(1)",
  "def volumes(self):\n        \"\"\"Returns volumes using x, y, z components of the mesh.\"\"\"\n        norm_coord = self.faces_normals * self.faces_centers\n        return self.surface_integral(norm_coord.T, axis=1)",
  "def volume(self):\n        \"\"\"Returns volume of the mesh.\"\"\"\n        return np.mean(self.volumes)",
  "def disp_mass(self, *, rho=1000):\n        return rho * self.volume",
  "def center_of_buoyancy(self):\n        \"\"\"Returns center of buoyancy of the mesh.\"\"\"\n        coords_sq_norm = self.faces_normals * self.faces_centers**2\n        return self.surface_integral(coords_sq_norm.T, axis=1) / (2*self.volume)",
  "def waterplane_area(self):\n        \"\"\"Returns water plane area of the mesh.\"\"\"\n        waterplane_area = -self.waterplane_integral(1)\n        return waterplane_area",
  "def waterplane_center(self):\n        \"\"\"Returns water plane center of the mesh.\n\n        Note: Returns None if the mesh is full submerged.\n        \"\"\"\n        waterplane_area = self.waterplane_area\n        if abs(waterplane_area) < 1e-10:\n            return None\n        else:\n            waterplane_center = -self.waterplane_integral(\n                self.faces_centers.T, axis=1) / waterplane_area\n            return waterplane_center[:-1]",
  "class SymmetricMesh(CollectionOfMeshes):\n    def __repr__(self):\n        reprer = reprlib.Repr()\n        reprer.maxstring = 90\n        reprer.maxother = 90\n        slice_name = reprer.repr(self._meshes[0])\n        if self.name is not None:\n            return f\"{self.__class__.__name__}({slice_name}, name={self.name})\"\n        else:\n            return f\"{self.__class__.__name__}({slice_name})\"",
  "class ReflectionSymmetricMesh(SymmetricMesh):\n    \"\"\"A mesh with one vertical symmetry plane.\n\n    Parameters\n    ----------\n    half : Mesh or CollectionOfMeshes\n        a mesh describing half of the body\n    plane : Plane\n        the symmetry plane across which the half body is mirrored\n    name :str, optional\n        a name for the mesh\n    \"\"\"\n\n    def __init__(self, half: Union[Mesh, CollectionOfMeshes], plane: Plane, name=None):\n        assert isinstance(half, Mesh) or isinstance(half, CollectionOfMeshes)\n        assert isinstance(plane, Plane)\n        assert plane.normal[2] == 0, \"Only vertical reflection planes are supported in ReflectionSymmetry classes.\"\n\n        other_half = half.mirrored(plane, name=f\"mirrored_of_{half.name}\")\n\n        if name is None:\n            name = f\"reflection_of_{half.name}\"\n\n        self.plane = plane.copy()\n\n        super().__init__((half, other_half), name=name)\n\n        if self.name is not None:\n            LOG.debug(f\"New mirror symmetric mesh: {self.name}.\")\n        else:\n            LOG.debug(f\"New mirror symmetric mesh.\")\n\n    def __str__(self):\n        return f\"{self.__class__.__name__}({self.half.__short_str__()}, plane={self.plane}, name=\\\"{self.name}\\\")\"\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self.half}, plane={self.plane}, name=\\\"{self.name}\\\")\"\n\n    def __rich_repr__(self):\n        yield self.half\n        yield \"plane\", self.plane\n        yield \"name\", self.name\n\n    @property\n    def half(self):\n        return self[0]\n\n    def tree_view(self, fold_symmetry=True, **kwargs):\n        if fold_symmetry:\n            return (self.__short_str__() + '\\n' + ' \u251c\u2500' + self.half.tree_view().replace('\\n', '\\n \u2502 ') + '\\n'\n                    + f\" \u2514\u2500mirrored copy of the above {self.half.__short_str__()}\")\n        else:\n            return CollectionOfMeshes.tree_view(self, **kwargs)\n\n    def __deepcopy__(self, *args):\n        return ReflectionSymmetricMesh(self.half.copy(), self.plane, name=self.name)\n\n    def join_meshes(*meshes, name=None):\n        assert all(isinstance(mesh, ReflectionSymmetricMesh) for mesh in meshes), \\\n            \"Only meshes with the same symmetry can be joined together.\"\n        assert all(meshes[0].plane == mesh.plane for mesh in meshes), \\\n            \"Only reflection symmetric meshes with the same reflection plane can be joined together.\"\n        half_mesh = CollectionOfMeshes([mesh.half for mesh in meshes], name=f\"half_of_{name}\" if name is not None else None)\n        return ReflectionSymmetricMesh(half_mesh, plane=meshes[0].plane, name=name)\n\n    @inplace_transformation\n    def translate(self, vector):\n        self.plane.translate(vector)\n        CollectionOfMeshes.translate(self, vector)\n        return self\n\n    @inplace_transformation\n    def rotate(self, axis: Axis, angle: float):\n        self.plane.rotate(axis, angle)\n        CollectionOfMeshes.rotate(self, axis, angle)\n        return self\n\n    @inplace_transformation\n    def mirror(self, plane: Plane):\n        self.plane.mirror(plane)\n        CollectionOfMeshes.mirror(self, plane)\n        return self",
  "class TranslationalSymmetricMesh(SymmetricMesh):\n    \"\"\"A mesh with a repeating pattern by translation.\n\n    Parameters\n    ----------\n    mesh_slice : Mesh or CollectionOfMeshes\n        the pattern that will be repeated to form the whole body\n    translation : array(3)\n        the vector of the translation\n    nb_repetitions : int, optional\n        the number of repetitions of the pattern (excluding the original one, default: 1)\n    name : str, optional\n        a name for the mesh\n    \"\"\"\n\n    def __init__(self, mesh_slice: Union[Mesh, CollectionOfMeshes], translation, nb_repetitions=1, name=None):\n        assert isinstance(mesh_slice, Mesh) or isinstance(mesh_slice, CollectionOfMeshes)\n        assert isinstance(nb_repetitions, int)\n        assert nb_repetitions >= 1\n\n        translation = np.asarray(translation).copy()\n        assert translation.shape == (3,)\n        assert translation[2] == 0  # Only horizontal translation are supported.\n\n        slices = [mesh_slice]\n        for i in range(1, nb_repetitions+1):\n            slices.append(mesh_slice.translated(vector=i*translation, name=f\"repetition_{i}_of_{mesh_slice.name}\"))\n\n        if name is None:\n            name = f\"translation_of_{mesh_slice.name}\"\n\n        self.translation = translation\n\n        super().__init__(slices, name=name)\n\n        if self.name is not None:\n            LOG.debug(f\"New translation symmetric mesh: {self.name}.\")\n        else:\n            LOG.debug(f\"New translation symmetric mesh.\")\n\n    @property\n    def first_slice(self):\n        return self[0]\n\n    def __str__(self):\n        return f\"{self.__class__.__name__}({self.first_slice.__short_str__()}, translation={self.translation}, nb_repetitions={len(self)-1}, name=\\\"{self.name}\\\")\"\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self.first_slice}, translation={self.translation}, nb_repetitions={len(self)-1}, name=\\\"{self.name}\\\")\"\n\n    def __rich_repr__(self):\n        yield self.first_slice\n        yield \"translation\", self.translation\n        yield \"nb_repetitions\", len(self)-1\n        yield \"name\", self.name\n\n    def tree_view(self, fold_symmetry=True, **kwargs):\n        if fold_symmetry:\n            return (self.__short_str__() + '\\n' + ' \u251c\u2500' + self.first_slice.tree_view().replace('\\n', '\\n \u2502 ') + '\\n'\n                    + f\" \u2514\u2500{len(self)-1} translated copies of the above {self.first_slice.__short_str__()}\")\n        else:\n            return CollectionOfMeshes.tree_view(self, **kwargs)\n\n    def __deepcopy__(self, *args):\n        return TranslationalSymmetricMesh(self.first_slice.copy(), self.translation, nb_repetitions=len(self) - 1, name=self.name)\n\n    @inplace_transformation\n    def translate(self, vector):\n        CollectionOfMeshes.translate(self, vector)\n        return self\n\n    @inplace_transformation\n    def rotate(self, axis: Axis, angle: float):\n        self.translation = axis.rotate_vectors([self.translation], angle)[0, :]\n        CollectionOfMeshes.rotate(self, axis, angle)\n        return self\n\n    @inplace_transformation\n    def mirror(self, plane: Plane):\n        self.translation -= 2 * (self.translation @ plane.normal) * plane.normal\n        CollectionOfMeshes.mirror(self, plane)\n        return self\n\n    def join_meshes(*meshes, name=None):\n        assert all(isinstance(mesh, TranslationalSymmetricMesh) for mesh in meshes), \\\n            \"Only meshes with the same symmetry can be joined together.\"\n        assert all(np.allclose(meshes[0].translation, mesh.translation) for mesh in meshes), \\\n            \"Only translation symmetric meshes with the same translation vector can be joined together.\"\n        assert all(len(meshes[0]) == len(mesh) for mesh in meshes), \\\n            \"Only symmetric meshes with the same number of elements can be joined together.\"\n        mesh_strip = CollectionOfMeshes([mesh.first_slice for mesh in meshes], name=f\"strip_of_{name}\" if name is not None else None)\n        return TranslationalSymmetricMesh(mesh_strip, translation=meshes[0].translation, nb_repetitions=len(meshes[0]) - 1, name=name)",
  "def build_regular_array_of_meshes(base_mesh, distance, nb_bodies):\n    \"\"\"Create an array of objects using TranslationalSymmetries.\n\n    Parameters\n    ----------\n    base_mesh : Mesh or CollectionOfMeshes or SymmetricMesh\n        The mesh to duplicate to create the array\n    distance : float\n        Center-to-center distance between objects in the array\n    nb_bodies : couple of ints\n        Number of objects in the x and y directions.\n\n    Returns\n    -------\n    TranslationalSymmetricMesh\n    \"\"\"\n    if nb_bodies[0] == 1:\n        line = base_mesh\n    else:\n        line = TranslationalSymmetricMesh(base_mesh, translation=(distance, 0.0, 0.0), nb_repetitions=nb_bodies[0] - 1,\n                                          name=f'line_of_{base_mesh.name}')\n    if nb_bodies[1] == 1:\n        array = line\n    else:\n        array = TranslationalSymmetricMesh(line, translation=(0.0, distance, 0.0), nb_repetitions=nb_bodies[1] - 1,\n                                           name=f'array_of_{base_mesh.name}')\n    return array",
  "class AxialSymmetricMesh(SymmetricMesh):\n    \"\"\"A mesh with a repeating pattern by rotation.\n\n    Parameters\n    ----------\n    mesh_slice : Mesh or CollectionOfMeshes\n        the pattern that will be repeated to form the whole body\n    axis : Axis, optional\n        symmetry axis\n    nb_repetitions : int, optional\n        the number of repetitions of the pattern (excluding the original one, default: 1)\n    name : str, optional\n        a name for the mesh\n    \"\"\"\n    def __init__(self, mesh_slice: Union[Mesh, CollectionOfMeshes], axis: Axis=Oz_axis, nb_repetitions: int=1, name=None):\n        assert isinstance(mesh_slice, Mesh) or isinstance(mesh_slice, CollectionOfMeshes)\n        assert isinstance(nb_repetitions, int)\n        assert nb_repetitions >= 1\n        assert isinstance(axis, Axis)\n\n        slices = [mesh_slice]\n        for i in range(1, nb_repetitions+1):\n            slices.append(mesh_slice.rotated(axis, angle=2*i*np.pi/(nb_repetitions+1),\n                                             name=f\"rotation_{i}_of_{mesh_slice.name}\"))\n\n        if name is None:\n            name = f\"rotation_of_{mesh_slice.name}\"\n\n        self.axis = axis.copy()\n\n        super().__init__(slices, name=name)\n\n        if not axis.is_parallel_to(Oz_axis):\n            LOG.warning(f\"{self.name} is an axi-symmetric mesh along a non vertical axis.\")\n\n        if self.name is not None:\n            LOG.debug(f\"New rotation symmetric mesh: {self.name}.\")\n        else:\n            LOG.debug(f\"New rotation symmetric mesh.\")\n\n    @staticmethod\n    def from_profile(profile: Union[Callable, Iterable[float]],\n                     z_range: Iterable[float]=np.linspace(-5, 0, 20),\n                     axis: Axis=Oz_axis,\n                     nphi: int=20,\n                     name=None):\n        \"\"\"Return a floating body using the axial symmetry.\n        The shape of the body can be defined either with a function defining the profile as [f(z), 0, z] for z in z_range.\n        Alternatively, the profile can be defined as a list of points.\n        The number of vertices along the vertical direction is len(z_range) in the first case and profile.shape[0] in the second case.\n\n        Parameters\n        ----------\n        profile : function(float \u2192 float)  or  array(N, 3)\n            define the shape of the body either as a function or a list of points.\n        z_range: array(N), optional\n            used only if the profile is defined as a function.\n        axis : Axis\n            symmetry axis\n        nphi : int, optional\n            number of vertical slices forming the body\n        name : str, optional\n            name of the generated body (optional)\n\n        Returns\n        -------\n        AxialSymmetricMesh\n            the generated mesh\n        \"\"\"\n\n        if name is None:\n            name = \"axisymmetric_mesh\"\n\n        if callable(profile):\n            z_range = np.asarray(z_range)\n            x_values = [profile(z) for z in z_range]\n            profile_array = np.stack([x_values, np.zeros(len(z_range)), z_range]).T\n        else:\n            profile_array = np.asarray(profile)\n\n        assert len(profile_array.shape) == 2\n        assert profile_array.shape[1] == 3\n\n        n = profile_array.shape[0]\n        angle = 2 * np.pi / nphi\n\n        nodes_slice = np.concatenate([profile_array, axis.rotate_points(profile_array, angle)])\n        faces_slice = np.array([[i, i+n, i+n+1, i+1] for i in range(n-1)])\n        body_slice = Mesh(nodes_slice, faces_slice, name=f\"slice_of_{name}\")\n        body_slice.merge_duplicates()\n        body_slice.heal_triangles()\n\n        return AxialSymmetricMesh(body_slice, axis=axis, nb_repetitions=nphi - 1, name=name)\n\n    @property\n    def first_slice(self):\n        return self[0]\n\n    def __str__(self):\n        return f\"{self.__class__.__name__}({self.first_slice.__short_str__()}, axis={self.axis}, nb_repetitions={len(self)-1}, name=\\\"{self.name}\\\")\"\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self.first_slice}, axis={self.axis}, nb_repetitions={len(self)-1}, name=\\\"{self.name}\\\")\"\n\n    def __rich_repr__(self):\n        yield self.first_slice\n        yield \"axis\", self.axis\n        yield \"nb_repetitions\", len(self)-1\n        yield \"name\", self.name\n\n    def tree_view(self, fold_symmetry=True, **kwargs):\n        if fold_symmetry:\n            return (self.__short_str__() + '\\n' + ' \u251c\u2500' + self.first_slice.tree_view().replace('\\n', '\\n \u2502 ') + '\\n'\n                    + f\" \u2514\u2500{len(self)-1} rotated copies of the above {self.first_slice.__short_str__()}\")\n        else:\n            return CollectionOfMeshes.tree_view(self, **kwargs)\n\n    def __deepcopy__(self, *args):\n        return AxialSymmetricMesh(self.first_slice.copy(), axis=self.axis.copy(), nb_repetitions=len(self) - 1, name=self.name)\n\n    def join_meshes(*meshes, name=None):\n        assert all(isinstance(mesh, AxialSymmetricMesh) for mesh in meshes), \\\n            \"Only meshes with the same symmetry can be joined together.\"\n        assert all(meshes[0].axis == mesh.axis for mesh in meshes), \\\n            \"Only axisymmetric meshes with the same symmetry axis can be joined together.\"\n        assert all(len(meshes[0]) == len(mesh) for mesh in meshes), \\\n            \"Only axisymmetric meshes with the same number of elements can be joined together.\"\n        mesh_slice = CollectionOfMeshes([mesh.first_slice for mesh in meshes], name=f\"slice_of_{name}\" if name is not None else None)\n        return AxialSymmetricMesh(mesh_slice, axis=meshes[0].axis, nb_repetitions=len(meshes[0]) - 1, name=name)\n\n    @inplace_transformation\n    def translate(self, vector):\n        self.axis.translate(vector)\n        CollectionOfMeshes.translate(self, vector)\n        return self\n\n    @inplace_transformation\n    def rotate(self, other_axis: Axis, angle: float):\n        self.axis.rotate(other_axis, angle)\n        CollectionOfMeshes.rotate(self, other_axis, angle)\n        return self\n\n    @inplace_transformation\n    def mirror(self, plane: Plane):\n        self.axis.mirror(plane)\n        CollectionOfMeshes.mirror(self, plane)\n        return self",
  "def __repr__(self):\n        reprer = reprlib.Repr()\n        reprer.maxstring = 90\n        reprer.maxother = 90\n        slice_name = reprer.repr(self._meshes[0])\n        if self.name is not None:\n            return f\"{self.__class__.__name__}({slice_name}, name={self.name})\"\n        else:\n            return f\"{self.__class__.__name__}({slice_name})\"",
  "def __init__(self, half: Union[Mesh, CollectionOfMeshes], plane: Plane, name=None):\n        assert isinstance(half, Mesh) or isinstance(half, CollectionOfMeshes)\n        assert isinstance(plane, Plane)\n        assert plane.normal[2] == 0, \"Only vertical reflection planes are supported in ReflectionSymmetry classes.\"\n\n        other_half = half.mirrored(plane, name=f\"mirrored_of_{half.name}\")\n\n        if name is None:\n            name = f\"reflection_of_{half.name}\"\n\n        self.plane = plane.copy()\n\n        super().__init__((half, other_half), name=name)\n\n        if self.name is not None:\n            LOG.debug(f\"New mirror symmetric mesh: {self.name}.\")\n        else:\n            LOG.debug(f\"New mirror symmetric mesh.\")",
  "def __str__(self):\n        return f\"{self.__class__.__name__}({self.half.__short_str__()}, plane={self.plane}, name=\\\"{self.name}\\\")\"",
  "def __repr__(self):\n        return f\"{self.__class__.__name__}({self.half}, plane={self.plane}, name=\\\"{self.name}\\\")\"",
  "def __rich_repr__(self):\n        yield self.half\n        yield \"plane\", self.plane\n        yield \"name\", self.name",
  "def half(self):\n        return self[0]",
  "def tree_view(self, fold_symmetry=True, **kwargs):\n        if fold_symmetry:\n            return (self.__short_str__() + '\\n' + ' \u251c\u2500' + self.half.tree_view().replace('\\n', '\\n \u2502 ') + '\\n'\n                    + f\" \u2514\u2500mirrored copy of the above {self.half.__short_str__()}\")\n        else:\n            return CollectionOfMeshes.tree_view(self, **kwargs)",
  "def __deepcopy__(self, *args):\n        return ReflectionSymmetricMesh(self.half.copy(), self.plane, name=self.name)",
  "def join_meshes(*meshes, name=None):\n        assert all(isinstance(mesh, ReflectionSymmetricMesh) for mesh in meshes), \\\n            \"Only meshes with the same symmetry can be joined together.\"\n        assert all(meshes[0].plane == mesh.plane for mesh in meshes), \\\n            \"Only reflection symmetric meshes with the same reflection plane can be joined together.\"\n        half_mesh = CollectionOfMeshes([mesh.half for mesh in meshes], name=f\"half_of_{name}\" if name is not None else None)\n        return ReflectionSymmetricMesh(half_mesh, plane=meshes[0].plane, name=name)",
  "def translate(self, vector):\n        self.plane.translate(vector)\n        CollectionOfMeshes.translate(self, vector)\n        return self",
  "def rotate(self, axis: Axis, angle: float):\n        self.plane.rotate(axis, angle)\n        CollectionOfMeshes.rotate(self, axis, angle)\n        return self",
  "def mirror(self, plane: Plane):\n        self.plane.mirror(plane)\n        CollectionOfMeshes.mirror(self, plane)\n        return self",
  "def __init__(self, mesh_slice: Union[Mesh, CollectionOfMeshes], translation, nb_repetitions=1, name=None):\n        assert isinstance(mesh_slice, Mesh) or isinstance(mesh_slice, CollectionOfMeshes)\n        assert isinstance(nb_repetitions, int)\n        assert nb_repetitions >= 1\n\n        translation = np.asarray(translation).copy()\n        assert translation.shape == (3,)\n        assert translation[2] == 0  # Only horizontal translation are supported.\n\n        slices = [mesh_slice]\n        for i in range(1, nb_repetitions+1):\n            slices.append(mesh_slice.translated(vector=i*translation, name=f\"repetition_{i}_of_{mesh_slice.name}\"))\n\n        if name is None:\n            name = f\"translation_of_{mesh_slice.name}\"\n\n        self.translation = translation\n\n        super().__init__(slices, name=name)\n\n        if self.name is not None:\n            LOG.debug(f\"New translation symmetric mesh: {self.name}.\")\n        else:\n            LOG.debug(f\"New translation symmetric mesh.\")",
  "def first_slice(self):\n        return self[0]",
  "def __str__(self):\n        return f\"{self.__class__.__name__}({self.first_slice.__short_str__()}, translation={self.translation}, nb_repetitions={len(self)-1}, name=\\\"{self.name}\\\")\"",
  "def __repr__(self):\n        return f\"{self.__class__.__name__}({self.first_slice}, translation={self.translation}, nb_repetitions={len(self)-1}, name=\\\"{self.name}\\\")\"",
  "def __rich_repr__(self):\n        yield self.first_slice\n        yield \"translation\", self.translation\n        yield \"nb_repetitions\", len(self)-1\n        yield \"name\", self.name",
  "def tree_view(self, fold_symmetry=True, **kwargs):\n        if fold_symmetry:\n            return (self.__short_str__() + '\\n' + ' \u251c\u2500' + self.first_slice.tree_view().replace('\\n', '\\n \u2502 ') + '\\n'\n                    + f\" \u2514\u2500{len(self)-1} translated copies of the above {self.first_slice.__short_str__()}\")\n        else:\n            return CollectionOfMeshes.tree_view(self, **kwargs)",
  "def __deepcopy__(self, *args):\n        return TranslationalSymmetricMesh(self.first_slice.copy(), self.translation, nb_repetitions=len(self) - 1, name=self.name)",
  "def translate(self, vector):\n        CollectionOfMeshes.translate(self, vector)\n        return self",
  "def rotate(self, axis: Axis, angle: float):\n        self.translation = axis.rotate_vectors([self.translation], angle)[0, :]\n        CollectionOfMeshes.rotate(self, axis, angle)\n        return self",
  "def mirror(self, plane: Plane):\n        self.translation -= 2 * (self.translation @ plane.normal) * plane.normal\n        CollectionOfMeshes.mirror(self, plane)\n        return self",
  "def join_meshes(*meshes, name=None):\n        assert all(isinstance(mesh, TranslationalSymmetricMesh) for mesh in meshes), \\\n            \"Only meshes with the same symmetry can be joined together.\"\n        assert all(np.allclose(meshes[0].translation, mesh.translation) for mesh in meshes), \\\n            \"Only translation symmetric meshes with the same translation vector can be joined together.\"\n        assert all(len(meshes[0]) == len(mesh) for mesh in meshes), \\\n            \"Only symmetric meshes with the same number of elements can be joined together.\"\n        mesh_strip = CollectionOfMeshes([mesh.first_slice for mesh in meshes], name=f\"strip_of_{name}\" if name is not None else None)\n        return TranslationalSymmetricMesh(mesh_strip, translation=meshes[0].translation, nb_repetitions=len(meshes[0]) - 1, name=name)",
  "def __init__(self, mesh_slice: Union[Mesh, CollectionOfMeshes], axis: Axis=Oz_axis, nb_repetitions: int=1, name=None):\n        assert isinstance(mesh_slice, Mesh) or isinstance(mesh_slice, CollectionOfMeshes)\n        assert isinstance(nb_repetitions, int)\n        assert nb_repetitions >= 1\n        assert isinstance(axis, Axis)\n\n        slices = [mesh_slice]\n        for i in range(1, nb_repetitions+1):\n            slices.append(mesh_slice.rotated(axis, angle=2*i*np.pi/(nb_repetitions+1),\n                                             name=f\"rotation_{i}_of_{mesh_slice.name}\"))\n\n        if name is None:\n            name = f\"rotation_of_{mesh_slice.name}\"\n\n        self.axis = axis.copy()\n\n        super().__init__(slices, name=name)\n\n        if not axis.is_parallel_to(Oz_axis):\n            LOG.warning(f\"{self.name} is an axi-symmetric mesh along a non vertical axis.\")\n\n        if self.name is not None:\n            LOG.debug(f\"New rotation symmetric mesh: {self.name}.\")\n        else:\n            LOG.debug(f\"New rotation symmetric mesh.\")",
  "def from_profile(profile: Union[Callable, Iterable[float]],\n                     z_range: Iterable[float]=np.linspace(-5, 0, 20),\n                     axis: Axis=Oz_axis,\n                     nphi: int=20,\n                     name=None):\n        \"\"\"Return a floating body using the axial symmetry.\n        The shape of the body can be defined either with a function defining the profile as [f(z), 0, z] for z in z_range.\n        Alternatively, the profile can be defined as a list of points.\n        The number of vertices along the vertical direction is len(z_range) in the first case and profile.shape[0] in the second case.\n\n        Parameters\n        ----------\n        profile : function(float \u2192 float)  or  array(N, 3)\n            define the shape of the body either as a function or a list of points.\n        z_range: array(N), optional\n            used only if the profile is defined as a function.\n        axis : Axis\n            symmetry axis\n        nphi : int, optional\n            number of vertical slices forming the body\n        name : str, optional\n            name of the generated body (optional)\n\n        Returns\n        -------\n        AxialSymmetricMesh\n            the generated mesh\n        \"\"\"\n\n        if name is None:\n            name = \"axisymmetric_mesh\"\n\n        if callable(profile):\n            z_range = np.asarray(z_range)\n            x_values = [profile(z) for z in z_range]\n            profile_array = np.stack([x_values, np.zeros(len(z_range)), z_range]).T\n        else:\n            profile_array = np.asarray(profile)\n\n        assert len(profile_array.shape) == 2\n        assert profile_array.shape[1] == 3\n\n        n = profile_array.shape[0]\n        angle = 2 * np.pi / nphi\n\n        nodes_slice = np.concatenate([profile_array, axis.rotate_points(profile_array, angle)])\n        faces_slice = np.array([[i, i+n, i+n+1, i+1] for i in range(n-1)])\n        body_slice = Mesh(nodes_slice, faces_slice, name=f\"slice_of_{name}\")\n        body_slice.merge_duplicates()\n        body_slice.heal_triangles()\n\n        return AxialSymmetricMesh(body_slice, axis=axis, nb_repetitions=nphi - 1, name=name)",
  "def first_slice(self):\n        return self[0]",
  "def __str__(self):\n        return f\"{self.__class__.__name__}({self.first_slice.__short_str__()}, axis={self.axis}, nb_repetitions={len(self)-1}, name=\\\"{self.name}\\\")\"",
  "def __repr__(self):\n        return f\"{self.__class__.__name__}({self.first_slice}, axis={self.axis}, nb_repetitions={len(self)-1}, name=\\\"{self.name}\\\")\"",
  "def __rich_repr__(self):\n        yield self.first_slice\n        yield \"axis\", self.axis\n        yield \"nb_repetitions\", len(self)-1\n        yield \"name\", self.name",
  "def tree_view(self, fold_symmetry=True, **kwargs):\n        if fold_symmetry:\n            return (self.__short_str__() + '\\n' + ' \u251c\u2500' + self.first_slice.tree_view().replace('\\n', '\\n \u2502 ') + '\\n'\n                    + f\" \u2514\u2500{len(self)-1} rotated copies of the above {self.first_slice.__short_str__()}\")\n        else:\n            return CollectionOfMeshes.tree_view(self, **kwargs)",
  "def __deepcopy__(self, *args):\n        return AxialSymmetricMesh(self.first_slice.copy(), axis=self.axis.copy(), nb_repetitions=len(self) - 1, name=self.name)",
  "def join_meshes(*meshes, name=None):\n        assert all(isinstance(mesh, AxialSymmetricMesh) for mesh in meshes), \\\n            \"Only meshes with the same symmetry can be joined together.\"\n        assert all(meshes[0].axis == mesh.axis for mesh in meshes), \\\n            \"Only axisymmetric meshes with the same symmetry axis can be joined together.\"\n        assert all(len(meshes[0]) == len(mesh) for mesh in meshes), \\\n            \"Only axisymmetric meshes with the same number of elements can be joined together.\"\n        mesh_slice = CollectionOfMeshes([mesh.first_slice for mesh in meshes], name=f\"slice_of_{name}\" if name is not None else None)\n        return AxialSymmetricMesh(mesh_slice, axis=meshes[0].axis, nb_repetitions=len(meshes[0]) - 1, name=name)",
  "def translate(self, vector):\n        self.axis.translate(vector)\n        CollectionOfMeshes.translate(self, vector)\n        return self",
  "def rotate(self, other_axis: Axis, angle: float):\n        self.axis.rotate(other_axis, angle)\n        CollectionOfMeshes.rotate(self, other_axis, angle)\n        return self",
  "def mirror(self, plane: Plane):\n        self.axis.mirror(plane)\n        CollectionOfMeshes.mirror(self, plane)\n        return self",
  "def merge_duplicates(mesh, atol=1e-8):\n    \"\"\"Merges the duplicate vertices of the mesh in place.\n\n    Parameters\n    ----------\n    atol : float, optional\n        Absolute tolerance. default is 1e-8\n\n    Returns\n    -------\n    new_id : ndarray\n        Array of indices that merges the vertices.\n    \"\"\"\n    uniq, new_id = merge_duplicate_rows(mesh.vertices, atol=atol)\n\n    nv_init = mesh.nb_vertices\n\n    # Updating mesh data\n    mesh.vertices = uniq\n    mesh.faces = new_id[mesh.faces] # Faces vertices ids are updated here\n\n    nv_final = mesh.nb_vertices\n\n    LOG.debug(\"* Merging duplicate vertices that lie in an absolute proximity of %.1E...\", atol)\n    delta_n = nv_init - nv_final\n    if delta_n == 0:\n        LOG.debug(\"\\t--> No duplicate vertices have been found\")\n    else:\n        LOG.debug(\"\\t--> Initial number of vertices : %u\", nv_init)\n        LOG.debug(\"\\t--> Final number of vertices   : %u\", nv_final)\n        LOG.debug(\"\\t--> %u vertices have been merged\", delta_n)\n\n    # if mesh._has_connectivity():\n    #     mesh._remove_connectivity()\n\n    return new_id",
  "def merge_duplicate_rows(arr, atol=1e-8):\n    \"\"\"Returns a new node array where close nodes have been merged into one node (following atol).\n\n    Parameters\n    ----------\n    arr : array_like\n        array of the coordinates of the mesh's nodes\n    atol : float, optional\n        the tolerance used to define nodes that are coincident and\n        that have to be merged\n\n    Returns\n    -------\n    arr : ndarray\n        array of the coordinates of the mesh's nodes where\n        every node is different\n    newID : ndarray\n        array of the new new vertices IDs\n    \"\"\"\n    # This function is a bottleneck in the clipping routines\n    # TODO: use np.unique to cluster groups --> acceleration !!\n\n    # atol = pow(10, -decimals)\n\n    arr = np.asarray(arr)\n\n    nv, nbdim = arr.shape\n\n    levels = [0, nv]\n    iperm = np.arange(nv)\n\n    for dim in range(nbdim):\n        # Sorting the first dimension\n        values = arr[:, dim].copy()\n        if dim > 0:\n            values = values[iperm]\n        levels_tmp = []\n        for (ilevel, istart) in enumerate(levels[:-1]):\n            istop = levels[ilevel+1]\n\n            if istop-istart > 1:\n                level_values = values[istart:istop]\n                iperm_view = iperm[istart:istop]\n\n                iperm_tmp = level_values.argsort()\n\n                level_values[:] = level_values[iperm_tmp]\n                iperm_view[:] = iperm_view[iperm_tmp]\n\n                levels_tmp.append(istart)\n                vref = values[istart]\n\n                for idx in range(istart, istop):\n                    cur_val = values[idx]\n                    if np.abs(cur_val - vref) > atol:\n                        levels_tmp.append(idx)\n                        vref = cur_val\n\n            else:\n                levels_tmp.append(levels[ilevel])\n        if len(levels_tmp) == nv:\n            # No duplicate rows\n            # if verbose:\n            # LOG.debug \"\\t -> No duplicate _vertices detected :)\"\n            newID = np.arange(nv)\n\n        levels_tmp.append(nv)\n        levels = levels_tmp\n\n    else:\n        # Building the new merged node list\n        arr_tmp = []\n        newID = np.arange(nv)\n        for (ilevel, istart) in enumerate(levels[:-1]):\n            istop = levels[ilevel+1]\n\n            arr_tmp.append(arr[iperm[istart]])\n            newID[iperm[list(range(istart, istop))]] = ilevel\n        arr = np.array(arr_tmp, dtype=float)\n        # Applying renumbering to cells\n        # if F is not None:\n        #     for cell in F:\n        #         cell[:] = newID[cell]\n\n        # if verbose:\n        # nv_new = arr.shape[0]\n        # LOG.debug \"\\t -> Initial number of nodes : {:d}\".format(nv)\n        # LOG.debug \"\\t -> New number of nodes     : {:d}\".format(nv_new)\n        # LOG.debug \"\\t -> {:d} nodes have been merged\".format(nv-nv_new)\n\n    # if F is not None:\n    #     if return_index:\n    #         return arr, F, newID\n    #     else:\n    #         return arr, F\n    # else:\n    return arr, newID",
  "def heal_normals(mesh):\n    \"\"\"Heals the mesh's normals orientations so that they have a consistent orientation and try to make them outward.\n    \"\"\"\n    # TODO: return the different groups of a mesh in case it is made of several unrelated groups\n\n    nv = mesh.nb_vertices\n    nf = mesh.nb_faces\n    faces = mesh._faces\n\n    # Building connectivities\n    connectivities = compute_connectivity(mesh)\n    v_v = connectivities[\"v_v\"]\n    v_f = connectivities[\"v_f\"]\n    f_f = connectivities[\"f_f\"]\n    boundaries = connectivities[\"boundaries\"]\n\n    if len(boundaries) > 0:\n        mesh_closed = False\n    else:\n        mesh_closed = True\n\n    # Flooding the mesh to find inconsistent normals\n    type_cell = np.zeros(nf, dtype=np.int32)\n    type_cell[:] = 4\n    type_cell[mesh.triangles_ids] = 3\n\n    f_vis = np.zeros(nf, dtype=bool)\n    f_vis[0] = True\n    stack = [0]\n    nb_reversed = 0\n    while 1:\n        if len(stack) == 0:\n            if np.any(np.logical_not(f_vis)):\n                iface = np.where(np.logical_not(f_vis))[0][0]\n                stack.append(iface)\n                f_vis[iface] = True\n            else:\n                break\n\n        iface = stack.pop()\n        face = faces[iface]\n        s1 = set(face)\n\n        for iadj_f in f_f[iface]:\n            if f_vis[iadj_f]:\n                continue\n            f_vis[iadj_f] = True\n            # Removing the other pointer\n            f_f[iadj_f].remove(iface)  # So as it won't go from iadj_f to iface in the future\n\n            # Shared vertices\n            adjface = faces[iadj_f]\n            s2 = set(adjface)\n            # try:\n            common_vertices = list(s1 & s2)\n            if len(common_vertices) == 2:\n                i_v1, i_v2 = common_vertices\n            else:\n                LOG.warning('faces %u and %u have more than 2 vertices in common !', iface, iadj_f)\n                continue\n\n            # Checking normal consistency\n            face_ref = np.roll(face[:type_cell[iface]], -np.where(face == i_v1)[0][0])\n            adj_face_ref = np.roll(adjface[:type_cell[iadj_f]], -np.where(adjface == i_v1)[0][0])\n\n            if face_ref[1] == i_v2:\n                i = 1\n            else:\n                i = -1\n\n            if adj_face_ref[i] == i_v2:\n                # Reversing normal\n                nb_reversed += 1\n                faces[iadj_f] = np.flipud(faces[iadj_f])\n\n            # Appending to the stack\n            stack.append(iadj_f)\n\n    LOG.debug(\"* Healing normals to make them consistent and if possible outward\")\n    if nb_reversed > 0:\n        LOG.debug('\\t--> %u faces have been reversed to make normals consistent across the mesh' % (nb_reversed))\n    else:\n        LOG.debug(\"\\t--> Normals orientations are consistent\")\n\n    mesh._faces = faces\n\n    # Checking if the normals are outward\n    if mesh_closed:\n        zmax = np.max(mesh._vertices[:, 2])\n\n        areas = mesh.faces_areas\n        normals = mesh.faces_normals\n        centers = mesh.faces_centers\n        # areas, normals, centers = get_all_faces_properties(vertices, faces)\n\n        hs = (np.array([(centers[:, 2] - zmax) * areas, ] * 3).T * normals).sum(axis=0)\n\n        tol = 1e-9\n        if np.fabs(hs[0]) > tol or np.fabs(hs[1]) > tol:\n            LOG.warning(\"\\t--> the mesh does not seem watertight although marked as closed...\")\n\n        if hs[2] < 0:\n            flipped = True\n            mesh.flip_normals()\n        else:\n            flipped = False\n\n        if flipped:\n            LOG.debug('\\t--> Every normals have been reversed to be outward')\n\n    else:\n        LOG.info(\"\\t--> Mesh is not closed, meshmagick cannot test if the normals are outward\")\n\n    return mesh",
  "def remove_unused_vertices(mesh):\n    \"\"\"Removes unused vertices in the mesh in place.\n\n    Those are vertices that are not used by any face connectivity.\n    \"\"\"\n    # TODO: implementer return_index !!\n    nv = mesh.nb_vertices\n    vertices, faces = mesh._vertices, mesh._faces\n\n    used_v = np.zeros(nv, dtype=bool)\n    used_v[sum(list(map(list, faces)), [])] = True\n    nb_used_v = sum(used_v)\n\n    if nb_used_v < nv:\n        new_id__v = np.arange(nv)\n        new_id__v[used_v] = np.arange(nb_used_v)\n        faces = new_id__v[faces]\n        vertices = vertices[used_v]\n\n    mesh._vertices, mesh._faces = vertices, faces\n\n    LOG.debug(\"* Removing unused vertices in the mesh:\")\n    if nb_used_v < nv:\n        unused_v = np.where(np.logical_not(used_v))[0]\n        vlist_str = '[' + ', '.join(str(iV) for iV in unused_v) + ']'\n        LOG.debug(\"\\t--> %u unused vertices have been removed\" % (nv - nb_used_v))\n    else:\n        LOG.debug(\"\\t--> No unused vertices\")\n\n    return mesh",
  "def heal_triangles(mesh):\n    \"\"\"Makes the triangle connectivity consistent (in place).\n\n    A general face is stored internally as a 4 integer array. It allows to describe indices of a quadrangle's vertices. For triangles, the first index should be equal to the last. This method ensures that this rule is applied everywhere and correct bad triangles description.\n    \"\"\"\n    faces = mesh._faces\n\n    quads = faces[:, 0] != faces[:, -1]\n    nquads_init = sum(quads)\n\n    faces[quads] = np.roll(faces[quads], 1, axis=1)\n    quads = faces[:, 0] != faces[:, -1]\n\n    faces[quads] = np.roll(faces[quads], 1, axis=1)\n    quads = faces[:, 0] != faces[:, -1]\n\n    faces[quads] = np.roll(faces[quads], 1, axis=1)\n    quads = faces[:, 0] != faces[:, -1]\n    nquads_final = sum(quads)\n\n    mesh._faces = faces\n\n    LOG.debug(\"* Ensuring consistent definition of triangles:\")\n    if nquads_final < nquads_init:\n        LOG.debug(\"\\t--> %u triangles were described the wrong way and have been corrected\" % (\n        nquads_init - nquads_final))\n    else:\n        LOG.debug(\"\\t--> Triangle description is consistent\")\n\n    return mesh",
  "def remove_degenerated_faces(mesh, rtol=1e-5):\n    \"\"\"Removes tiny triangles from the mesh (in place).\n\n    Tiny triangles are those whose area is lower than the mean triangle area in the mesh times the relative\n    tolerance given.\n\n    Parameters\n    ----------\n    rtol : float, optional\n        Positive relative tolerance\n    \"\"\"\n\n    assert 0 < rtol\n\n    # TODO: implementer un retour d'index des faces extraites\n    areas = mesh.faces_areas\n    area_threshold = areas.mean() * float(rtol)\n\n    # Detecting faces that have null area\n    faces = mesh._faces[np.logical_not(areas < area_threshold)]\n    nb_removed = mesh.nb_faces - faces.shape[0]\n    LOG.debug('* Removing degenerated faces')\n    if nb_removed > 0:\n        LOG.debug('\\t-->%u degenerated faces have been removed' % nb_removed)\n    else:\n        LOG.debug('\\t--> No degenerated faces')\n\n    mesh._faces = faces\n\n    return mesh",
  "def print_quality(mesh):\n    \"\"\"Returns data on the mesh quality.\n    Needs to be tested...\n\n    It uses VTK and is reproduced from\n    http://vtk.org/gitweb?p=VTK.git;a=blob;f=Filters/Verdict/Testing/Python/MeshQuality.py\n    \"\"\"\n    # This function is reproduced from\n    # http://vtk.org/gitweb?p=VTK.git;a=blob;f=Filters/Verdict/Testing/Python/MeshQuality.py\n    polydata = mesh._vtk_polydata()\n    quality = vtk.vtkMeshQuality()\n    quality.SetInputData(polydata)\n\n    def DumpQualityStats(iq, arrayname):\n        an = iq.GetOutput().GetFieldData().GetArray(arrayname)\n        cardinality = an.GetComponent(0, 4)\n        range = list()\n        range.append(an.GetComponent(0, 0))\n        range.append(an.GetComponent(0, 2))\n        average = an.GetComponent(0, 1)\n        stdDev = math.sqrt(math.fabs(an.GetComponent(0, 3)))\n        outStr = '%s%g%s%g\\n%s%g%s%g' % (\n            '    range: ', range[0], '  -  ', range[1],\n            '    average: ', average, '  , standard deviation: ', stdDev)\n        return outStr\n\n    # Here we define the various mesh types and labels for output.\n    meshTypes = [\n        ['Triangle', 'Triangle',\n         [['QualityMeasureToArea', ' Area Ratio:'],\n          ['QualityMeasureToEdgeRatio', ' Edge Ratio:'],\n          ['QualityMeasureToAspectRatio', ' Aspect Ratio:'],\n          ['QualityMeasureToRadiusRatio', ' Radius Ratio:'],\n          ['QualityMeasureToAspectFrobenius', ' Frobenius Norm:'],\n          ['QualityMeasureToMinAngle', ' Minimal Angle:']\n          ]\n         ],\n\n        ['Quad', 'Quadrilateral',\n         [['QualityMeasureToArea', ' Area Ratio:'],\n          ['QualityMeasureToEdgeRatio', ' Edge Ratio:'],\n          ['QualityMeasureToAspectRatio', ' Aspect Ratio:'],\n          ['QualityMeasureToRadiusRatio', ' Radius Ratio:'],\n          ['QualityMeasureToMedAspectFrobenius',\n           ' Average Frobenius Norm:'],\n          ['QualityMeasureToMaxAspectFrobenius',\n           ' Maximal Frobenius Norm:'],\n          ['QualityMeasureToMinAngle', ' Minimal Angle:']\n          ]\n         ]\n    ]\n    res = ''\n    if polydata.GetNumberOfCells() > 0:\n        for meshType in meshTypes:\n            res += '\\n%s%s' % (meshType[1], ' quality of the mesh ')\n            quality.Update()\n            an = quality.GetOutput().GetFieldData().GetArray('Mesh ' + meshType[1] + ' Quality')\n            cardinality = an.GetComponent(0, 4)\n\n            res = ''.join((res, '(%u elements):\\n' % cardinality))\n\n            # res += '('+str(cardinality) +meshType[1]+'):\\n'\n\n            for measure in meshType[2]:\n                eval('quality.Set' + meshType[0] + measure[0] + '()')\n                quality.Update()\n                res += '\\n%s\\n%s' % (\n                    measure[1],\n                    DumpQualityStats(quality, 'Mesh ' + meshType[1] + ' Quality')\n                )\n            res += '\\n'\n\n    info = \"\"\"\\n\\nDefinition of the different quality measures is given\n    in the verdict library manual :\n    http://www.vtk.org/Wiki/images/6/6b/VerdictManual-revA.pdf\\n\"\"\"\n\n    res += info\n    print(res)\n    return",
  "def DumpQualityStats(iq, arrayname):\n        an = iq.GetOutput().GetFieldData().GetArray(arrayname)\n        cardinality = an.GetComponent(0, 4)\n        range = list()\n        range.append(an.GetComponent(0, 0))\n        range.append(an.GetComponent(0, 2))\n        average = an.GetComponent(0, 1)\n        stdDev = math.sqrt(math.fabs(an.GetComponent(0, 3)))\n        outStr = '%s%g%s%g\\n%s%g%s%g' % (\n            '    range: ', range[0], '  -  ', range[1],\n            '    average: ', average, '  , standard deviation: ', stdDev)\n        return outStr",
  "def inplace_transformation(inplace_function):\n    \"\"\"Decorator for methods transforming 3D objects:\n    * Add the optional argument `inplace` to return a new object instead of doing the transformation in place.\n    * If the object has properties cached in an \"__internals__\" dict, they are deleted.\n    \"\"\"\n    def enhanced_inplace_function(self, *args, inplace=True, name=None, **kwargs):\n        if not inplace:\n            object3d = self.copy(name=name)\n        else:\n            object3d = self\n        inplace_function(object3d, *args, **kwargs)\n        if hasattr(object3d, '__internals__'):\n            object3d.__internals__.clear()\n        return object3d\n    return enhanced_inplace_function",
  "class Abstract3DObject(ABC):\n    \"\"\"Abstract class for 3d objects that can be transformed in 3d.\n    The child classes have to define `mirror`, `rotate` and `translate`,\n    then more routines such as `translate_x` and `translated` are automatically available.\"\"\"\n\n    @abstractmethod\n    def translate(self, vector):\n        pass\n\n    @abstractmethod\n    def rotate(self, axis, angle):\n        pass\n\n    @abstractmethod\n    def mirror(self, plane):\n        pass\n\n    @inplace_transformation\n    def translate_x(self, tx):\n        return self.translate((tx, 0., 0.))\n\n    @inplace_transformation\n    def translate_y(self, ty):\n        return self.translate((0., ty, 0.))\n\n    @inplace_transformation\n    def translate_z(self, tz):\n        return self.translate((0., 0., tz))\n\n    @inplace_transformation\n    def translate_point_to_point(self, point_a, point_b):\n        return self.translate(np.asarray(point_b) - np.asarray(point_a))\n\n    @inplace_transformation\n    def rotate_x(self, thetax):\n        return self.rotate(Ox_axis, thetax)\n\n    @inplace_transformation\n    def rotate_y(self, thetay):\n        return self.rotate(Oy_axis, thetay)\n\n    @inplace_transformation\n    def rotate_z(self, thetaz):\n        return self.rotate(Oz_axis, thetaz)\n\n    @inplace_transformation\n    def rotate_around_center_to_align_vectors(self, center, vec1, vec2):\n        \"\"\"Rotate self such that if vec1 is in self, then it will point in the same direction as vec2.\"\"\"\n        vec1 = np.asarray(vec1)\n        vec2 = np.asarray(vec2)\n        if parallel_vectors_with_same_direction(vec1, vec2):\n            return self\n        else:\n            if parallel_vectors(vec1, vec2):\n                if parallel_vectors(vec1, e_x):\n                    axis = Axis(vector=np.cross(vec1, e_y), point=center)\n                else:\n                    axis = Axis(vector=np.cross(vec1, e_x), point=center)\n                return self.rotate(axis, np.pi)\n            else:\n                axis = Axis(vector=np.cross(vec1, vec2), point=center)\n                return self.rotate(axis, np.arccos(np.dot(vec1, vec2)))\n\n    def translated(self, *args, **kwargs):\n        return self.translate(*args, inplace=False, **kwargs)\n\n    def rotated(self, *args, **kwargs):\n        return self.rotate(*args, inplace=False, **kwargs)\n\n    def mirrored(self, *args, **kwargs):\n        return self.mirror(*args, inplace=False, **kwargs)\n\n    def translated_x(self, *args, **kwargs):\n        return self.translate_x(*args, inplace=False, **kwargs)\n\n    def translated_y(self, *args, **kwargs):\n        return self.translate_y(*args, inplace=False, **kwargs)\n\n    def translated_z(self, *args, **kwargs):\n        return self.translate_z(*args, inplace=False, **kwargs)\n\n    def translated_point_to_point(self, *args, **kwargs):\n        return self.translate_point_to_point(*args, inplace=False, **kwargs)\n\n    def rotated_x(self, *args, **kwargs):\n        return self.rotate_x(*args, inplace=False, **kwargs)\n\n    def rotated_y(self, *args, **kwargs):\n        return self.rotate_y(*args, inplace=False, **kwargs)\n\n    def rotated_z(self, *args, **kwargs):\n        return self.rotate_z(*args, inplace=False, **kwargs)\n\n    def rotated_around_center_to_align_vectors(self, *args, **kwargs):\n        return self.rotate_around_center_to_align_vectors(*args, inplace=False, **kwargs)",
  "class ClippableMixin(ABC):\n    \"\"\"Abstract base class for object that can be clipped.\n    The child classes should inplement a `clip` method, then this abstract\n    class will append the new methods `clipped`, `keep_immersed_part` and\n    `immersed_part`, all based on `clip`.\n    \"\"\"\n\n    @abstractmethod\n    def clip(self, plane):\n        pass\n\n    def clipped(self, plane, **kwargs):\n        # Same API as for the other transformations\n        return self.clip(plane, inplace=False, **kwargs)\n\n    @inplace_transformation\n    def keep_immersed_part(self, free_surface=0.0, *, sea_bottom=None, water_depth=None):\n        self.clip(Plane(normal=(0, 0, 1), point=(0, 0, free_surface)))\n        water_depth = _get_water_depth(free_surface, water_depth, sea_bottom,\n                                       default_water_depth=np.infty)\n        if water_depth < np.infty:\n            self.clip(Plane(normal=(0, 0, -1), point=(0, 0, free_surface-water_depth)))\n        return self\n\n    def immersed_part(self, free_surface=0.0, *, sea_bottom=None, water_depth=None):\n        return self.keep_immersed_part(free_surface, inplace=False, name=self.name,\n                                       sea_bottom=sea_bottom, water_depth=water_depth)",
  "def orthogonal_vectors(vec1, vec2) -> bool:\n    return np.linalg.norm(vec1 @ vec2) < 1e-6",
  "def parallel_vectors(vec1, vec2) -> bool:\n    return np.linalg.norm(np.cross(vec1, vec2)) < 1e-6",
  "def parallel_vectors_with_same_direction(vec1, vec2) -> bool:\n    return parallel_vectors(vec1, vec2) and np.dot(vec1, vec2) > 0",
  "class Axis(Abstract3DObject):\n    def __init__(self, vector=(1, 0, 0), point=(0, 0, 0)):\n        assert len(vector) == 3, \"Vector of an axis should be given as a 3-ple of values.\"\n        assert len(point) == 3, \"Point of an axis should be given as a 3-ple of values.\"\n        vector = np.array(vector, float)\n        self.vector = vector / np.linalg.norm(vector)\n        self.point = np.array(point, float)\n\n    def __repr__(self):\n        return f\"Axis(vector={self.vector}, point={self.point})\"\n\n    def __contains__(self, other_point):\n        if len(other_point) == 3:\n            other_point = np.asarray(other_point, dtype=float)\n            return parallel_vectors(other_point - self.point, self.vector)\n        else:\n            raise NotImplementedError\n\n    def __eq__(self, other):\n        if isinstance(self, Axis):\n            return (self is other) or (self.point in other and parallel_vectors(self.vector, other.vector))\n        else:\n            return NotImplemented\n\n    def is_orthogonal_to(self, other):\n        if isinstance(other, Plane):\n            return parallel_vectors(self.vector, other.normal)\n        elif len(other) == 3:  # The other is supposed to be a vector given as a 3-ple\n            return orthogonal_vectors(self.vector, other)\n        else:\n            raise NotImplementedError\n\n    def is_parallel_to(self, other):\n        if isinstance(other, Plane):\n            return orthogonal_vectors(self.vector, other.normal)\n        elif isinstance(other, Axis):\n            return parallel_vectors(self.vector, other.vector)\n        elif len(other) == 3:  # The other is supposed to be a vector given as a 3-ple\n            return parallel_vectors(self.vector, other)\n        else:\n            raise NotImplementedError\n\n    def angle_with_respect_to(self, other_axis: 'Axis') -> float:\n        \"\"\"Angle between two axes.\"\"\"\n        return np.arccos(np.dot(self.vector, other_axis.vector))\n\n    ################################\n    #  Transformation of the axis  #\n    ################################\n\n    def copy(self, name=None):\n        return Axis(vector=self.vector.copy(), point=self.point.copy())\n\n    @inplace_transformation\n    def translate(self, vector):\n        self.point += vector\n        return self\n\n    @inplace_transformation\n    def rotate(self, axis, angle):\n        rot_matrix = axis.rotation_matrix(angle)\n        self.point = rot_matrix @ (self.point - axis.point) + axis.point\n        self.vector = rot_matrix @ self.vector\n        return self\n\n    @inplace_transformation\n    def mirror(self, plane):\n        self.point -= 2 * (self.point @ plane.normal - plane.c) * plane.normal\n        self.vector -= 2 * (self.vector @ plane.normal) * plane.normal\n        return self\n\n    ###########\n    #  Other  #\n    ###########\n\n    def rotation_matrix(self, theta):\n        \"\"\"Rotation matrix around the vector according to Rodrigues' formula.\"\"\"\n        ux, uy, uz = self.vector\n        W = np.array([[0, -uz, uy],\n                      [uz, 0, -ux],\n                      [-uy, ux, 0]])\n        return np.identity(3) + np.sin(theta)*W + 2*np.sin(theta/2)**2 * (W @ W)\n\n    def rotate_vectors(self, vectors, angle):\n        vectors = np.asarray(vectors)\n        return (self.rotation_matrix(angle) @ vectors.T).T\n\n    def rotate_points(self, points, angle):\n        points = np.asarray(points)\n        return self.rotate_vectors(points - self.point, angle) + self.point",
  "class Plane(Abstract3DObject):\n    \"\"\"3D plane, oriented by the direction of their normal.\"\"\"\n    def __init__(self, normal=(0.0, 0.0, 1.0), point=(0.0, 0.0, 0.0)):\n        normal = np.asarray(normal, dtype=float)\n        self.normal = normal / np.linalg.norm(normal)\n        self.point = np.asarray(point, dtype=float)\n\n    def __repr__(self):\n        return f\"Plane(normal={self.normal}, point={self.point})\"\n\n    def __contains__(self, other):\n        if isinstance(other, Axis):\n            return other.point in self and orthogonal_vectors(self.normal, other.vector)\n        elif len(other) == 3:\n            return orthogonal_vectors(other - self.point, self.normal)\n        else:\n            raise NotImplementedError\n\n    def __eq__(self, other):\n        \"\"\"Plane are considered equal only when their normal are pointing in the same direction.\"\"\"\n        if isinstance(other, Plane):\n            return ((self is other) or\n                    (other.point in self and parallel_vectors_with_same_direction(self.normal, other.normal)))\n        else:\n            return NotImplemented\n\n    def is_orthogonal_to(self, other):\n        if isinstance(other, Axis):\n            return parallel_vectors(self.normal, other.vector)\n        elif isinstance(other, Plane):\n            return orthogonal_vectors(self.normal, other.normal)\n        elif len(other) == 3:  # The other is supposed to be a vector given as a 3-ple\n            return parallel_vectors(self.normal, other)\n        else:\n            raise NotImplementedError\n\n    @property\n    def c(self):\n        \"\"\"Distance from plane to origin.\"\"\"\n        return np.linalg.norm(self.normal @ self.point)\n\n    @property\n    def s(self):\n        \"\"\"Distance from origin to plane along the normal\"\"\"\n        return np.dot(self.normal, self.point)\n\n    #################################\n    #  Transformation of the plane  #\n    #################################\n\n    def copy(self, name=None):\n        return Plane(normal=self.normal.copy(), point=self.point.copy())\n\n    @inplace_transformation\n    def translate(self, vector):\n        self.point = self.point + np.asarray(vector)\n        return self\n\n    @inplace_transformation\n    def rotate(self, axis, angle):\n        rot_matrix = axis.rotation_matrix(angle)\n        self.point = rot_matrix @ self.point\n        self.normal = rot_matrix @ self.normal\n        return self\n\n    @inplace_transformation\n    def mirror(self, plane):\n        self.point -= 2 * (self.point @ plane.normal - plane.c) * plane.normal\n        self.normal -= 2 * (self.normal @ plane.normal) * plane.normal\n        return self\n\n    ###########\n    #  Other  #\n    ###########\n\n    def distance_to_point(self, points):\n        \"\"\"\n        Return the orthogonal distance of points with respect to the plane.\n        The distance is counted positively on one side of the plane and negatively on the other.\n\n        Parameters\n        ----------\n        points : ndarray\n            Array of points coordinates\n\n        Returns\n        -------\n        dist : ndarray\n            Array of distances of points with respect to the plane\n        \"\"\"\n        return np.dot(points, self.normal) - np.dot(self.point, self.normal)\n\n    def get_edge_intersection(self, p0, p1):\n        \"\"\"\n        Returns the coordinates of the intersection point between the plane and the edge P0P1.\n\n        Parameters\n        ----------\n        p0 : ndarray\n            Coordinates of point p0\n        p1 : ndarray\n            Coordinates of point P1\n\n        Returns\n        -------\n        I : ndarray\n            Coordinates of intersection point\n        \"\"\"\n        assert len(p0) == 3 and len(p1) == 3\n\n        p0n = np.dot(p0, self.normal)\n        p1n = np.dot(p1, self.normal)\n        t = (p0n - self.s) / (p0n - p1n)\n        if t < 0. or t > 1.:\n            raise RuntimeError('Intersection is outside the edge')\n        return (1-t) * p0 + t * p1",
  "def enhanced_inplace_function(self, *args, inplace=True, name=None, **kwargs):\n        if not inplace:\n            object3d = self.copy(name=name)\n        else:\n            object3d = self\n        inplace_function(object3d, *args, **kwargs)\n        if hasattr(object3d, '__internals__'):\n            object3d.__internals__.clear()\n        return object3d",
  "def translate(self, vector):\n        pass",
  "def rotate(self, axis, angle):\n        pass",
  "def mirror(self, plane):\n        pass",
  "def translate_x(self, tx):\n        return self.translate((tx, 0., 0.))",
  "def translate_y(self, ty):\n        return self.translate((0., ty, 0.))",
  "def translate_z(self, tz):\n        return self.translate((0., 0., tz))",
  "def translate_point_to_point(self, point_a, point_b):\n        return self.translate(np.asarray(point_b) - np.asarray(point_a))",
  "def rotate_x(self, thetax):\n        return self.rotate(Ox_axis, thetax)",
  "def rotate_y(self, thetay):\n        return self.rotate(Oy_axis, thetay)",
  "def rotate_z(self, thetaz):\n        return self.rotate(Oz_axis, thetaz)",
  "def rotate_around_center_to_align_vectors(self, center, vec1, vec2):\n        \"\"\"Rotate self such that if vec1 is in self, then it will point in the same direction as vec2.\"\"\"\n        vec1 = np.asarray(vec1)\n        vec2 = np.asarray(vec2)\n        if parallel_vectors_with_same_direction(vec1, vec2):\n            return self\n        else:\n            if parallel_vectors(vec1, vec2):\n                if parallel_vectors(vec1, e_x):\n                    axis = Axis(vector=np.cross(vec1, e_y), point=center)\n                else:\n                    axis = Axis(vector=np.cross(vec1, e_x), point=center)\n                return self.rotate(axis, np.pi)\n            else:\n                axis = Axis(vector=np.cross(vec1, vec2), point=center)\n                return self.rotate(axis, np.arccos(np.dot(vec1, vec2)))",
  "def translated(self, *args, **kwargs):\n        return self.translate(*args, inplace=False, **kwargs)",
  "def rotated(self, *args, **kwargs):\n        return self.rotate(*args, inplace=False, **kwargs)",
  "def mirrored(self, *args, **kwargs):\n        return self.mirror(*args, inplace=False, **kwargs)",
  "def translated_x(self, *args, **kwargs):\n        return self.translate_x(*args, inplace=False, **kwargs)",
  "def translated_y(self, *args, **kwargs):\n        return self.translate_y(*args, inplace=False, **kwargs)",
  "def translated_z(self, *args, **kwargs):\n        return self.translate_z(*args, inplace=False, **kwargs)",
  "def translated_point_to_point(self, *args, **kwargs):\n        return self.translate_point_to_point(*args, inplace=False, **kwargs)",
  "def rotated_x(self, *args, **kwargs):\n        return self.rotate_x(*args, inplace=False, **kwargs)",
  "def rotated_y(self, *args, **kwargs):\n        return self.rotate_y(*args, inplace=False, **kwargs)",
  "def rotated_z(self, *args, **kwargs):\n        return self.rotate_z(*args, inplace=False, **kwargs)",
  "def rotated_around_center_to_align_vectors(self, *args, **kwargs):\n        return self.rotate_around_center_to_align_vectors(*args, inplace=False, **kwargs)",
  "def clip(self, plane):\n        pass",
  "def clipped(self, plane, **kwargs):\n        # Same API as for the other transformations\n        return self.clip(plane, inplace=False, **kwargs)",
  "def keep_immersed_part(self, free_surface=0.0, *, sea_bottom=None, water_depth=None):\n        self.clip(Plane(normal=(0, 0, 1), point=(0, 0, free_surface)))\n        water_depth = _get_water_depth(free_surface, water_depth, sea_bottom,\n                                       default_water_depth=np.infty)\n        if water_depth < np.infty:\n            self.clip(Plane(normal=(0, 0, -1), point=(0, 0, free_surface-water_depth)))\n        return self",
  "def immersed_part(self, free_surface=0.0, *, sea_bottom=None, water_depth=None):\n        return self.keep_immersed_part(free_surface, inplace=False, name=self.name,\n                                       sea_bottom=sea_bottom, water_depth=water_depth)",
  "def __init__(self, vector=(1, 0, 0), point=(0, 0, 0)):\n        assert len(vector) == 3, \"Vector of an axis should be given as a 3-ple of values.\"\n        assert len(point) == 3, \"Point of an axis should be given as a 3-ple of values.\"\n        vector = np.array(vector, float)\n        self.vector = vector / np.linalg.norm(vector)\n        self.point = np.array(point, float)",
  "def __repr__(self):\n        return f\"Axis(vector={self.vector}, point={self.point})\"",
  "def __contains__(self, other_point):\n        if len(other_point) == 3:\n            other_point = np.asarray(other_point, dtype=float)\n            return parallel_vectors(other_point - self.point, self.vector)\n        else:\n            raise NotImplementedError",
  "def __eq__(self, other):\n        if isinstance(self, Axis):\n            return (self is other) or (self.point in other and parallel_vectors(self.vector, other.vector))\n        else:\n            return NotImplemented",
  "def is_orthogonal_to(self, other):\n        if isinstance(other, Plane):\n            return parallel_vectors(self.vector, other.normal)\n        elif len(other) == 3:  # The other is supposed to be a vector given as a 3-ple\n            return orthogonal_vectors(self.vector, other)\n        else:\n            raise NotImplementedError",
  "def is_parallel_to(self, other):\n        if isinstance(other, Plane):\n            return orthogonal_vectors(self.vector, other.normal)\n        elif isinstance(other, Axis):\n            return parallel_vectors(self.vector, other.vector)\n        elif len(other) == 3:  # The other is supposed to be a vector given as a 3-ple\n            return parallel_vectors(self.vector, other)\n        else:\n            raise NotImplementedError",
  "def angle_with_respect_to(self, other_axis: 'Axis') -> float:\n        \"\"\"Angle between two axes.\"\"\"\n        return np.arccos(np.dot(self.vector, other_axis.vector))",
  "def copy(self, name=None):\n        return Axis(vector=self.vector.copy(), point=self.point.copy())",
  "def translate(self, vector):\n        self.point += vector\n        return self",
  "def rotate(self, axis, angle):\n        rot_matrix = axis.rotation_matrix(angle)\n        self.point = rot_matrix @ (self.point - axis.point) + axis.point\n        self.vector = rot_matrix @ self.vector\n        return self",
  "def mirror(self, plane):\n        self.point -= 2 * (self.point @ plane.normal - plane.c) * plane.normal\n        self.vector -= 2 * (self.vector @ plane.normal) * plane.normal\n        return self",
  "def rotation_matrix(self, theta):\n        \"\"\"Rotation matrix around the vector according to Rodrigues' formula.\"\"\"\n        ux, uy, uz = self.vector\n        W = np.array([[0, -uz, uy],\n                      [uz, 0, -ux],\n                      [-uy, ux, 0]])\n        return np.identity(3) + np.sin(theta)*W + 2*np.sin(theta/2)**2 * (W @ W)",
  "def rotate_vectors(self, vectors, angle):\n        vectors = np.asarray(vectors)\n        return (self.rotation_matrix(angle) @ vectors.T).T",
  "def rotate_points(self, points, angle):\n        points = np.asarray(points)\n        return self.rotate_vectors(points - self.point, angle) + self.point",
  "def __init__(self, normal=(0.0, 0.0, 1.0), point=(0.0, 0.0, 0.0)):\n        normal = np.asarray(normal, dtype=float)\n        self.normal = normal / np.linalg.norm(normal)\n        self.point = np.asarray(point, dtype=float)",
  "def __repr__(self):\n        return f\"Plane(normal={self.normal}, point={self.point})\"",
  "def __contains__(self, other):\n        if isinstance(other, Axis):\n            return other.point in self and orthogonal_vectors(self.normal, other.vector)\n        elif len(other) == 3:\n            return orthogonal_vectors(other - self.point, self.normal)\n        else:\n            raise NotImplementedError",
  "def __eq__(self, other):\n        \"\"\"Plane are considered equal only when their normal are pointing in the same direction.\"\"\"\n        if isinstance(other, Plane):\n            return ((self is other) or\n                    (other.point in self and parallel_vectors_with_same_direction(self.normal, other.normal)))\n        else:\n            return NotImplemented",
  "def is_orthogonal_to(self, other):\n        if isinstance(other, Axis):\n            return parallel_vectors(self.normal, other.vector)\n        elif isinstance(other, Plane):\n            return orthogonal_vectors(self.normal, other.normal)\n        elif len(other) == 3:  # The other is supposed to be a vector given as a 3-ple\n            return parallel_vectors(self.normal, other)\n        else:\n            raise NotImplementedError",
  "def c(self):\n        \"\"\"Distance from plane to origin.\"\"\"\n        return np.linalg.norm(self.normal @ self.point)",
  "def s(self):\n        \"\"\"Distance from origin to plane along the normal\"\"\"\n        return np.dot(self.normal, self.point)",
  "def copy(self, name=None):\n        return Plane(normal=self.normal.copy(), point=self.point.copy())",
  "def translate(self, vector):\n        self.point = self.point + np.asarray(vector)\n        return self",
  "def rotate(self, axis, angle):\n        rot_matrix = axis.rotation_matrix(angle)\n        self.point = rot_matrix @ self.point\n        self.normal = rot_matrix @ self.normal\n        return self",
  "def mirror(self, plane):\n        self.point -= 2 * (self.point @ plane.normal - plane.c) * plane.normal\n        self.normal -= 2 * (self.normal @ plane.normal) * plane.normal\n        return self",
  "def distance_to_point(self, points):\n        \"\"\"\n        Return the orthogonal distance of points with respect to the plane.\n        The distance is counted positively on one side of the plane and negatively on the other.\n\n        Parameters\n        ----------\n        points : ndarray\n            Array of points coordinates\n\n        Returns\n        -------\n        dist : ndarray\n            Array of distances of points with respect to the plane\n        \"\"\"\n        return np.dot(points, self.normal) - np.dot(self.point, self.normal)",
  "def get_edge_intersection(self, p0, p1):\n        \"\"\"\n        Returns the coordinates of the intersection point between the plane and the edge P0P1.\n\n        Parameters\n        ----------\n        p0 : ndarray\n            Coordinates of point p0\n        p1 : ndarray\n            Coordinates of point P1\n\n        Returns\n        -------\n        I : ndarray\n            Coordinates of intersection point\n        \"\"\"\n        assert len(p0) == 3 and len(p1) == 3\n\n        p0n = np.dot(p0, self.normal)\n        p1n = np.dot(p1, self.normal)\n        t = (p0n - self.s) / (p0n - p1n)\n        if t < 0. or t > 1.:\n            raise RuntimeError('Intersection is outside the edge')\n        return (1-t) * p0 + t * p1",
  "def mesh_sphere(*, radius=1.0, center=(0.0, 0.0, 0.0), resolution=(10, 10), axial_symmetry=False, name=None):\n    \"\"\"Sphere\n\n    Parameters\n    ----------\n    radius : float\n        radius of the sphere\n    center : 3-ple or array of shape (3,)\n        position of the geometric center of the sphere\n    resolution : couple of ints\n        number of panels along a meridian (or number of parallels-1) and along a parallel (or number of meridians-1)\n    axial_symmetry : bool\n        if True, use the axial symmetry to build the mesh (default: False)\n    name : string\n        a name identifying the sphere (default: \"sphere_id\" where id is an unique integer).\n    \"\"\"\n\n    if name is None:\n        name = f\"sphere_{next(Mesh._ids)}\"\n\n    ntheta, nphi = resolution\n\n    theta = np.linspace(0.0, pi, ntheta+1)\n    points_on_a_meridian = radius * np.stack([np.sin(theta), np.zeros_like(theta), -np.cos(theta)], axis=1)\n\n    symmetry_axis = Axis(vector=[0, 0, 1], point=[0, 0, 0])\n    mesh = AxialSymmetricMesh.from_profile(points_on_a_meridian, axis=symmetry_axis, nphi=nphi, name=name)\n\n    if not axial_symmetry:\n        mesh = mesh.merged()\n\n    mesh.heal_mesh()\n    mesh.translate(center)\n    mesh.geometric_center = np.asarray(center, dtype=float)\n    return mesh",
  "def mesh_rectangle(*, size=(5.0, 5.0), resolution=(5, 5), center=(0.0, 0.0, 0.0), normal=(0.0, 0.0, 1.0), translation_symmetry=False, reflection_symmetry=False, name=None):\n    \"\"\"One-sided rectangle.\n\n    By default, the rectangle is horizontal, the normals are oriented upwards.\n\n    Parameters\n    ----------\n    size : couple of floats, optional\n        dimensions of the rectangle (width and height)\n    resolution : couple of ints, optional\n        number of faces along each of the two directions\n    center : 3-ple of floats, optional\n        position of the geometric center of the rectangle, default: (0, 0, 0)\n    normal: 3-ple of floats, optional\n        normal vector, default: (0, 0, 1)\n    translation_symmetry : bool, optional\n        if True, use the translation symmetry to speed up the computations\n    reflection_symmetry : bool, optional\n        if True, use the reflection symmetry to speed up the computations\n    name : string, optional\n        a name for the body\n    \"\"\"\n\n    assert len(size) == 2, \"Size of a rectangle should be given as a couple of values.\"\n    assert all([h > 0 for h in size]), \"Size of the rectangle mesh should be given as positive values.\"\n\n    assert len(resolution) == 2, \"Resolution of a rectangle should be given as a couple a values.\"\n    assert all([h > 0 for h in resolution]), \"Resolution of the rectangle mesh should be given as positive values.\"\n    assert all([i == int(i) for i in resolution]), \"Resolution of a rectangle should be given as integer values.\"\n\n    assert len(center) == 3, \"Position of the center of a rectangle should be given a 3-ple of values.\"\n\n    width, height = size\n    nw, nh = resolution\n\n    if name is None:\n        name = f\"rectangle_{next(Mesh._ids)}\"\n\n    if translation_symmetry and reflection_symmetry:\n        raise NotImplementedError(\"Rectangle generation with both reflection and translation symmetries \"\n                                  \"has not been implemented.\")\n\n    if reflection_symmetry:\n        if nw % 2 == 1:\n            raise ValueError(\"To use the reflection symmetry of the mesh, \"\n                             \"it should have an even number of panels in this direction.\")\n\n        half_mesh = mesh_rectangle(size=(width/2, height), resolution=(nw//2, nh),\n                                   center=(0, -width/4, 0), normal=(0.0, 0.0, 1.0),\n                                   translation_symmetry=False, reflection_symmetry=False,\n                                   name=f\"half_of_{name}\")\n        mesh = ReflectionSymmetricMesh(half_mesh, plane=xOz_Plane, name=name)\n\n    elif translation_symmetry:\n            strip = mesh_rectangle(size=(width/nw, height), resolution=(1, nh),\n                                   center=(0, -width/2 + width/(2*nw), 0), normal=(0.0, 0.0, 1.0),\n                                   translation_symmetry=False, reflection_symmetry=False,\n                                   name=f\"strip_of_{name}\")\n            mesh = TranslationalSymmetricMesh(strip,\n                                              translation=np.asarray([0, width/nw, 0]),\n                                              nb_repetitions=int(nw)-1,\n                                              name=name)\n\n    else:\n        y_range = np.linspace(-width/2, width/2, nw+1)\n        z_range = np.linspace(-height/2, height/2, nh+1)\n        nodes = np.array(list(product([0.0], y_range, z_range)), dtype=float)\n        panels = np.array([(j+i*(nh+1), j+1+i*(nh+1), j+1+(i+1)*(nh+1), j+(i+1)*(nh+1))\n                             for (i, j) in product(range(nw), range(nh))])\n\n        mesh = Mesh(nodes, panels, name=name)\n\n    mesh.heal_mesh()\n    mesh.translate(center)\n    mesh.rotate_around_center_to_align_vectors(center, mesh.faces_normals[0], normal)\n    mesh.geometric_center = np.asarray(center, dtype=float)\n    return mesh",
  "def mesh_parallelepiped(size=(1.0, 1.0, 1.0), resolution=(4, 4, 4), center=(0, 0, 0),\n             missing_sides=set(), reflection_symmetry=False, translation_symmetry=False,\n             name=None):\n    \"\"\"Six rectangles forming a parallelepiped.\n\n    Parameters\n    ----------\n    size : 3-ple of floats, optional\n        dimensions of the parallelepiped (width, thickness, height) for coordinates (x, y, z).\n    resolution : 3-ple of ints, optional\n        number of faces along the three directions\n    center : 3-ple of floats, optional\n        coordinates of the geometric center of the parallelepiped\n    missing_sides : set of string, optional\n        if one of the keyword \"top\", \"bottom\", \"front\", \"back\", \"left\", \"right\" is in the set,\n        then the corresponding side is not included in the parallelepiped.\n        May be ignored when building a mesh with a symmetry.\n    reflection_symmetry : bool, optional\n        use xOz and yOz symmetry plane to generate the mesh\n    translation_symmetry : bool, optional\n        if True, use the translation symmetry in the x direction to speed up the computations.\n        To use the translation symmetry in the y direction, create a x-symmetric body and then rotate it by pi/2.\n    name : string, optional\n        a name for the body\n    \"\"\"\n\n    assert len(size) == 3, \"Size of a rectangular parallelepiped should be given as a 3-ple of values.\"\n    assert all([h > 0 for h in size]), \"Size of the rectangular mesh should be given as positive values.\"\n\n    assert len(resolution) == 3, \"Resolution of a rectangular parallelepiped should be given as a 3-ple a values.\"\n    assert all([h > 0 for h in resolution]), \"Resolution of the rectangular parallelepiped mesh \" \\\n                                             \"should be given as positive values.\"\n    assert all([i == int(i) for i in resolution]), \"Resolution of a rectangular parallelepiped \" \\\n                                                   \"should be given as integer values.\"\n\n    assert len(center) == 3, \"Position of the center of a parallelepiped should be given a 3-ple of values.\"\n\n    width, thickness, height = size\n    nw, nth, nh = resolution\n    if name is None:\n        name = f\"rectangular_parallelepiped_{next(Mesh._ids)}\"\n\n    if translation_symmetry and reflection_symmetry:\n        raise NotImplementedError(\"Parallelepiped generation with both reflection and translation symmetries \"\n                                  \"has not been implemented.\")\n\n    if reflection_symmetry:\n        if (nw % 2 == 1 or nth % 2 == 1):\n            raise ValueError(\"To use the reflection symmetry of the mesh, \"\n                             \"it should have an even number of panels in this direction.\")\n\n        missing_sides_in_quarter = missing_sides | {\"right\", \"back\"}\n        quarter_mesh = mesh_parallelepiped(\n                size=(width/2, thickness/2, height), resolution=(nw//2, nth//2, nh),\n                center=(-width/4, -thickness/4, 0), missing_sides=missing_sides_in_quarter,\n                reflection_symmetry=False, translation_symmetry=False,\n                name=f\"quarter_of_{name}\"\n                )\n\n        half_mesh = ReflectionSymmetricMesh(quarter_mesh, plane=yOz_Plane, name=f\"half_of_{name}\")\n        mesh = ReflectionSymmetricMesh(half_mesh, plane=xOz_Plane, name=f\"{name}\")\n\n    elif translation_symmetry:\n\n        missing_sides_in_strip = missing_sides | {\"left\", \"right\"}\n        strip = mesh_parallelepiped(\n                size=(width/nw, thickness, height), resolution=(1, nth, nh),\n                center=(-width/2 + width/(2*nw), 0, 0), missing_sides=missing_sides_in_strip,\n                reflection_symmetry=False, translation_symmetry=False,\n                name=f\"strip_of_{name}\"\n                )\n\n        open_parallelepiped = TranslationalSymmetricMesh(\n            strip,\n            translation=(width/nw, 0, 0), nb_repetitions=int(nw)-1,\n            name=f\"body_of_{name}\"\n        )\n\n        components_of_mesh = [open_parallelepiped]\n        if \"right\" not in missing_sides:\n            components_of_mesh.append(\n                    mesh_rectangle(\n                        size=(thickness, height), resolution=(nth, nh),\n                        center=(width/2, 0, 0), normal=(1, 0, 0),\n                        name=f\"right_side_of_{name}\"\n                        ))\n        if \"left\" not in missing_sides:\n            components_of_mesh.append(\n                    mesh_rectangle(\n                        size=(thickness, height), resolution=(nth, nh),\n                        center=(-width/2, 0, 0), normal=(-1, 0, 0),\n                        name=f\"left_side_of_{name}\"\n                        ))\n\n        mesh = CollectionOfMeshes(components_of_mesh, name=name)\n\n    else:\n\n        sides = []\n        if \"left\" not in missing_sides:\n            sides.append(\n                    mesh_rectangle(size=(thickness, height), resolution=(nth, nh), center=(-width/2, 0, 0),\n                        normal=(-1, 0, 0), name=f\"left_of_{name}\")\n                    )\n        if \"right\" not in missing_sides:\n            sides.append(\n                    mesh_rectangle(size=(thickness, height), resolution=(nth, nh), center=(width/2, 0, 0),\n                        normal=(1, 0, 0), name=f\"right_of_{name}\")\n                    )\n        if \"front\" not in missing_sides:\n            sides.append(\n                    mesh_rectangle(size=(width, height), resolution=(nw, nh), center=(0, -thickness/2, 0),\n                        normal=(0, -1, 0), name=f\"front_of_{name}\")\n                    )\n        if \"back\" not in missing_sides:\n            sides.append(\n                    mesh_rectangle(size=(width, height), resolution=(nw, nh), center=(0, thickness/2, 0),\n                        normal=(0, 1, 0), name=f\"back_of_{name}\")\n                    )\n        if \"top\" not in missing_sides:\n            sides.append(\n                    mesh_rectangle(size=(thickness, width), resolution=(nth, nw), center=(0, 0, height/2),\n                        normal=(0, 0, 1), name=f\"top_of_{name}\")\n                    )\n        if \"bottom\" not in missing_sides:\n            sides.append(\n                    mesh_rectangle(size=(thickness, width), resolution=(nth, nw), center=(0, 0, -height/2),\n                        normal=(0, 0, -1), name=f\"bottom_of_{name}\")\n                    )\n        mesh = CollectionOfMeshes(sides, name=name).merged()\n\n    mesh.heal_mesh()\n    mesh.translate(center)\n    mesh.geometric_center = np.asarray(center, dtype=float)\n    return mesh",
  "def mesh_disk(*, radius=1.0, center=(0, 0, 0), normal=(0, 0, 1), resolution=(3, 6),\n        reflection_symmetry=False, axial_symmetry=False, name=None, _theta_max=2*pi):\n    \"\"\"(One-sided) disk.\n\n    Parameters\n    ----------\n    radius : float, optional\n        radius of the disk\n    center : 3-ple or array of shape (3,), optional\n        position of the geometric center of the disk\n    normal: 3-ple of floats, optional\n        normal vector, default: along x axis\n    resolution : 2-ple of int, optional\n        number of panels along a radius and around the disk\n    axial_symmetry : bool, optional\n        if True, returns an AxialSymmetricMesh\n    reflection_symmetry : bool, optional\n        if True, returns a ReflectionSymmetricMesh\n    name : str, optional\n        a string naming the mesh\n    _theta_max: float, optional\n        internal parameter, to return an arc circle instead of a full circle\n    \"\"\"\n    assert radius > 0, \"Radius of the disk mesh should be given as a positive value.\"\n\n    assert len(resolution) == 2, \"Resolution of a disk should be given as a couple of values.\"\n    assert all([h > 0 for h in resolution]), \"Resolution of the disk mesh should be given as positive values.\"\n    assert all([i == int(i) for i in resolution]), \"Resolution of a disk should be given as integer values.\"\n\n    assert len(center) == 3, \"Position of the center of a disk should be given a 3-ple of values.\"\n\n    nr, ntheta = resolution\n\n    if name is None:\n        name = f\"disk_{next(Mesh._ids)}\"\n\n    if reflection_symmetry and axial_symmetry:\n        raise NotImplementedError(\"Disks with both symmetries have not been implemented.\")\n\n    LOG.debug(f\"New disk of radius {radius} and resolution {resolution}, named {name}.\")\n\n    if reflection_symmetry:\n        if ntheta % 2 == 1:\n            raise ValueError(\"To use the reflection symmetry of the mesh, \"\n                             \"it should have an even number of panels in this direction.\")\n\n        half_mesh = mesh_disk(radius=radius, _theta_max=_theta_max/2, resolution=(nr, ntheta//2),\n                center=(0, 0, 0), normal=(0, 0, 1),\n                reflection_symmetry=False, axial_symmetry=False, name=f\"half_of_{name}\")\n        mesh = ReflectionSymmetricMesh(half_mesh, plane=xOz_Plane, name=name)\n\n    elif axial_symmetry:\n        mesh_slice = mesh_disk(radius=radius, _theta_max=_theta_max/ntheta, resolution=(nr, 1),\n                center=(0, 0, 0), normal=(0, 0, 1),\n                reflection_symmetry=False, axial_symmetry=False, name=f\"slice_of_{name}\")\n        mesh = AxialSymmetricMesh(mesh_slice, axis=Oz_axis, nb_repetitions=ntheta - 1, name=name)\n\n    else:\n        theta_range = np.linspace(0, _theta_max, ntheta+1)\n        r_range = np.linspace(0.0, radius, nr+1)\n        nodes = np.array([(0, r*sin(t), r*cos(t)) for (r, t) in product(r_range, theta_range)])\n        panels = np.array([(j+i*(ntheta+1), j+1+i*(ntheta+1), j+1+(i+1)*(ntheta+1), j+(i+1)*(ntheta+1))\n                                for (i, j) in product(range(0, nr), range(0, ntheta))])\n\n        mesh = Mesh(nodes, panels, name=name)\n\n    mesh.heal_mesh()\n    mesh.translate(center)\n    mesh.rotate_around_center_to_align_vectors(center, mesh.faces_normals[0], normal)\n    mesh.geometric_center = np.asarray(center, dtype=float)\n    return mesh",
  "def mesh_vertical_cylinder(*, length=10.0, radius=1.0, center=(0, 0, 0),\n        resolution=(2, 8, 10), axial_symmetry=False, reflection_symmetry=False, name=None, _theta_max=2*pi):\n    \"\"\"Vertical cylinder.\n\n    Total number of panels = (2*resolution[0] + resolution[2])*resolution[1]\n\n    Parameters\n    ----------\n    length : float, optional\n        length of the cylinder\n    radius : float, optional\n        radius of the cylinder\n    center : 3-ple or array of shape (3,), optional\n        position of the geometric center of the cylinder\n    resolution : 3-ple of int, optional\n        (number of panel along a radius at the end, number of panels around a slice, number of slices)\n        Mnemonic: same ordering as the cylindrical coordinates (nr, ntheta, nz)\n    axial_symmetry : bool, optional\n        if True, returns an AxialSymmetricMesh\n    reflection_symmetry : bool, optional\n        if True, returns a ReflectionSymmetricMesh\n    name : str, optional\n        a string naming the mesh\n    _theta_max: float, optional\n        internal parameter, to return an arc circle instead of a full circle\n    \"\"\"\n    assert length > 0, \"Length of a cylinder should be given as a positive value.\"\n    assert radius > 0, \"Radius of a cylinder should be given as a positive value.\"\n\n    assert len(resolution) == 3, \"Resolution of a cylinder should be given as a 3-ple of values.\"\n    assert all([h >= 0 for h in resolution]), \"Resolution of a cylinder should be given as positive values.\"\n    assert all([i == int(i) for i in resolution]), \"Resolution of a cylinder should be given as integer values.\"\n\n    assert len(center) == 3, \"Position of the center of a cylinder should be given a 3-ple of values.\"\n\n    nr, ntheta, nz = resolution\n\n    if name is None:\n        name = f\"cylinder_{next(Mesh._ids)}\"\n\n    LOG.debug(f\"New vertical cylinder of length {length}, radius {radius} and resolution {resolution}, named {name}.\")\n\n    if reflection_symmetry and axial_symmetry:\n        raise NotImplementedError(\"Vertical cylinders with both symmetries have not been implemented.\")\n\n    if reflection_symmetry:\n        if ntheta % 2 == 1:\n            raise ValueError(\"To use the reflection symmetry of the mesh, \"\n                             \"it should have an even number of panels in this direction.\")\n\n        half_cylinder = mesh_vertical_cylinder(length=length, radius=radius, center=(0, 0, 0),\n                resolution=(nr, ntheta//2, nz), reflection_symmetry=False, axial_symmetry=False,\n                name=f\"half_{name}\", _theta_max=_theta_max/2)\n\n        mesh = ReflectionSymmetricMesh(half_cylinder, plane=xOz_Plane, name=name)\n\n    elif axial_symmetry:\n\n        mesh_slice = mesh_vertical_cylinder(length=length, radius=radius, resolution=(nr, 1, nz), center=(0, 0, 0),\n                reflection_symmetry=False, axial_symmetry=False, name=f\"slice_of_{name}\", _theta_max=_theta_max/ntheta)\n        mesh = AxialSymmetricMesh(mesh_slice, axis=Oz_axis, nb_repetitions=ntheta - 1, name=name)\n\n    else:\n        theta_range = np.linspace(0, _theta_max, ntheta+1)\n        z_range = np.linspace(-length/2, length/2, nz+1)\n        if nr > 0:\n            r_range = np.linspace(0.0, radius, nr+1)\n            nodes = np.concatenate([\n                np.array([(r*sin(t), r*cos(t), -length/2) for (r, t) in product(r_range, theta_range)]),\n                np.array([(radius*sin(t), radius*cos(t), z) for (z, t) in product(z_range, theta_range)]),\n                np.array([(r*sin(t), r*cos(t), length/2) for (r, t) in product(r_range[::-1], theta_range)]),\n                ])\n        else:\n            r_range = np.array([])\n            nodes = np.array([(radius*sin(t), radius*cos(t), z) for (z, t) in product(z_range, theta_range)])\n        panels = np.array([(j+i*(ntheta+1), j+(i+1)*(ntheta+1), j+1+(i+1)*(ntheta+1), j+1+i*(ntheta+1), )\n                                for (i, j) in product(range(nz+2*(len(r_range))), range(ntheta))])\n\n        mesh = Mesh(nodes, panels, name=name)\n\n    mesh.heal_mesh()\n    mesh.translate(center)\n    mesh.geometric_center = np.asarray(center, dtype=float)\n    return mesh",
  "def mesh_horizontal_cylinder(*, length=10.0, radius=1.0, center=(0, 0, 0),\n        resolution=(2, 8, 10), reflection_symmetry=False, translation_symmetry=False, name=None, _theta_max=2*pi):\n    \"\"\"Cylinder aligned along Ox axis.\n\n    Total number of panels = (2*resolution[0] + resolution[2])*resolution[1]\n\n    Parameters\n    ----------\n    length : float, optional\n        length of the cylinder\n    radius : float, optional\n        radius of the cylinder\n    center : 3-ple or array of shape (3,), optional\n        position of the geometric center of the cylinder\n    resolution : 3-ple of int, optional\n        (number of panel along a radius at the end, number of panels around a slice, number of slices)\n        Mnemonic: same ordering as the cylindrical coordinates (nr, ntheta, nz)\n    reflection_symmetry : bool, optional\n        if True, returns a ReflectionSymmetricMesh\n    translation_symmetry : bool, optional\n        if True, uses a TranslationalSymmetricMesh internally for the main part of the cylinder\n    name : str, optional\n        a string naming the mesh\n    _theta_max: float, optional\n        internal parameter, to return an arc circle instead of a full circle\n    \"\"\"\n\n    assert length > 0, \"Length of a cylinder should be given as a positive value.\"\n    assert radius > 0, \"Radius of a cylinder should be given as a positive value.\"\n\n    assert len(resolution) == 3, \"Resolution of a cylinder should be given as a 3-ple of values.\"\n    assert all([h >= 0 for h in resolution]), \"Resolution of a cylinder should be given as positive values.\"\n    assert all([i == int(i) for i in resolution]), \"Resolution of a cylinder should be given as integer values.\"\n\n    assert len(center) == 3, \"Position of the center of a cylinder should be given a 3-ple of values.\"\n\n    if name is None:\n        name = f\"cylinder_{next(Mesh._ids)}\"\n\n    LOG.debug(f\"New horizontal cylinder of length {length}, radius {radius} and resolution {resolution}, named {name}.\")\n\n    nr, ntheta, nx = resolution\n\n    if reflection_symmetry:\n        if ntheta % 2 == 1:\n            raise ValueError(\"To use the reflection symmetry of the mesh, \"\n                             \"it should have an even number of panels in this direction.\")\n\n        half_cylinder = mesh_horizontal_cylinder(\n                length=length, radius=radius, center=(0, 0, 0),\n                resolution=(nr, ntheta//2, nx),\n                reflection_symmetry=False, translation_symmetry=translation_symmetry,\n                name=f\"half_{name}\", _theta_max=_theta_max/2,\n                )\n\n        mesh = ReflectionSymmetricMesh(half_cylinder, plane=xOz_Plane, name=name)\n\n    else:\n        if translation_symmetry:\n            slice = mesh_horizontal_cylinder(\n                    length=length/nx, radius=radius, center=(-length/2 + length/(2*nx), 0, 0),\n                    resolution=(0, ntheta, 1),\n                    reflection_symmetry=False, translation_symmetry=False,\n                    name=f\"slice_of_{name}\", _theta_max=_theta_max,\n                    )\n\n            open_cylinder = TranslationalSymmetricMesh(\n                    slice, translation=np.asarray([length / nx, 0.0, 0.0]),\n                    nb_repetitions=nx-1, name=f\"open_{name}\")\n\n        else: # General case\n            theta_range = np.linspace(0, _theta_max, ntheta+1)\n            x_range = np.linspace(-length/2, length/2, nx+1)\n            nodes = np.array([(x, radius*sin(t), -radius*cos(t)) for (x, t) in product(x_range, theta_range)])\n\n            panels = np.array([(i+j*(ntheta+1), i+1+j*(ntheta+1), i+1+(j+1)*(ntheta+1), i+(j+1)*(ntheta+1))\n                                for (i, j) in product(range(ntheta), range(nx))])\n\n            open_cylinder = Mesh(nodes, panels, name=f\"open_{name}\")\n\n        if nr > 0:\n            side = mesh_disk(radius=radius, center=(-length/2, 0, 0), normal=(-1, 0, 0),\n                             reflection_symmetry=False, resolution=(nr, ntheta), name=f\"side_of_{name}\",\n                             _theta_max=_theta_max)\n            other_side = side.mirrored(yOz_Plane, name=f\"other_side_of_{name}\")\n            mesh = CollectionOfMeshes([open_cylinder, side, other_side], name=name)\n            if not translation_symmetry:\n                mesh = mesh.merged()\n        else:\n            mesh = open_cylinder.copy(name=name)\n\n    mesh.heal_mesh()\n    mesh.translate(center)\n    mesh.geometric_center = np.asarray(center, dtype=float)\n    return mesh",
  "def import_cal_file(filepath):\n    \"\"\"Read a Nemoh.cal file and return a list of problems.\"\"\"\n\n    with open(filepath, 'r') as cal_file:\n\n        cal_file.readline()  # Unused line.\n        rho = float(cal_file.readline().split()[0])\n        g = float(cal_file.readline().split()[0])\n        water_depth = float(cal_file.readline().split()[0])\n        if water_depth == 0.0:\n            water_depth = np.infty\n        xeff, yeff = (float(x) for x in cal_file.readline().split()[0:2])\n\n        bodies = []\n\n        cal_file.readline()  # Unused line.\n        nb_bodies = int(cal_file.readline().split()[0])\n        for _ in range(nb_bodies):\n            cal_file.readline()  # Unused line.\n            mesh_file = cal_file.readline().split()[0].strip()\n            mesh_file = os.path.join(os.path.dirname(filepath), mesh_file)  # mesh path are relative to Nemoh.cal\n            cal_file.readline()  # Number of points, number of panels (unused)\n\n            if os.path.splitext(mesh_file)[1] == '.py':\n                from importlib.util import spec_from_file_location, module_from_spec\n                spec = spec_from_file_location(\"body_initialization_module\", mesh_file)\n                body_initialization = module_from_spec(spec)\n                spec.loader.exec_module(body_initialization)\n                body = body_initialization.body\n            else:\n                body = FloatingBody.from_file(mesh_file)\n\n            nb_dofs = int(cal_file.readline().split()[0])\n            for i_dof in range(nb_dofs):\n                dof_data = cal_file.readline().split()\n                if int(dof_data[0]) == 1:\n                    direction = np.array([float(x) for x in dof_data[1:4]])\n                    body.add_translation_dof(direction=direction)\n                elif int(dof_data[0]) == 2:\n                    direction = np.array([float(x) for x in dof_data[1:4]])\n                    center_of_mass = np.array([float(x) for x in dof_data[4:7]])\n                    body.add_rotation_dof(Axis(vector=direction, point=center_of_mass))\n\n            nb_forces = int(cal_file.readline().split()[0])\n            for i_force in range(nb_forces):\n                force_data = cal_file.readline().split()\n                if int(force_data[0]) == 1:\n                    direction = np.array([float(x) for x in force_data[1:4]])\n                elif int(force_data[0]) == 2:\n                    direction = np.array([float(x) for x in force_data[1:4]])\n                    center_of_mass = np.array([float(x) for x in force_data[4:7]])\n            # TODO: use the generalized forces.\n\n            nb_additional_lines = int(cal_file.readline().split()[0])\n            for _ in range(nb_additional_lines):\n                cal_file.readline()  # The additional lines are just ignored.\n\n            bodies.append(body)\n\n        if nb_bodies > 1:\n            bodies = FloatingBody.join_bodies(*bodies)\n        else:\n            bodies = bodies[0]\n\n        cal_file.readline()  # Unused line.\n        frequency_data_string_without_comment = cal_file.readline().split('!')[0]\n        frequency_data = frequency_data_string_without_comment.split()\n        if len(frequency_data) == 3:  # Nemoh v2 format\n            omega_range = np.linspace(float(frequency_data[1]), float(frequency_data[2]), int(frequency_data[0]))\n        else:\n            type_of_frequency_data = int(frequency_data[0])\n            if type_of_frequency_data == 1:  # angular frequency\n                omega_range = np.linspace(float(frequency_data[2]), float(frequency_data[3]), int(frequency_data[1]))\n            elif type_of_frequency_data == 2:  # frequency\n                omega_range = 2*np.pi*np.linspace(float(frequency_data[2]), float(frequency_data[3]), int(frequency_data[1]))\n            elif type_of_frequency_data == 3:  # period\n                omega_range = 2*np.pi/np.linspace(float(frequency_data[2]), float(frequency_data[3]), int(frequency_data[1]))\n            else:\n                raise ValueError(f\"Cannot parse the frequency data \\\"{frequency_data_string_without_comment}\\\" in {filepath}.\")\n\n\n        direction_data = cal_file.readline().split()\n        direction_range = np.linspace(float(direction_data[1]), float(direction_data[2]), int(direction_data[0]))\n        direction_range = np.pi/180*direction_range  # conversion from degrees to radians.\n\n        # The options below are not implemented yet.\n\n        cal_file.readline()  # Unused line.\n        irf_data = cal_file.readline()\n        show_pressure = cal_file.readline().split()[0] == \"1\"\n        kochin_data = cal_file.readline().split()\n        kochin_range = np.linspace(float(kochin_data[1]), float(kochin_data[2]), int(kochin_data[0]))\n        free_surface_data = cal_file.readline().split()\n\n    # Generate Capytaine's problem objects\n    env_args = dict(body=bodies, rho=rho, water_depth=water_depth, g=g)\n    problems = []\n    for omega in omega_range:\n        for direction in direction_range:\n            problems.append(DiffractionProblem(wave_direction=direction, omega=omega, **env_args))\n        for dof in bodies.dofs:\n            problems.append(RadiationProblem(radiating_dof=dof, omega=omega, **env_args))\n\n    return problems",
  "def export_as_Nemoh_directory(problem, directory_name, omega_range=None):\n    \"\"\"Export radiation problems as Nemoh 2.0 directory (experimental).\n\n    TODO: Diffraction problem.\n\n    Parameters\n    ----------\n    problem : RadiationProblem\n        the problem that should be exported\n    directory_name : string\n        path to the directory\n    omega_range : list of float or array of float, optional\n        the exported problem will be set up with the following linear range:\n        linspace(min(omega_range), max(omega_range), len(omega_range))\n    \"\"\"\n\n    if os.path.isdir(directory_name):\n        LOG.warning(f\"\"\"Exporting problem in already existing directory: {directory_name}\n             You might be overwriting existing files!\"\"\")\n    else:\n        os.makedirs(directory_name)\n\n    # Export the mesh\n    write_MAR(\n        os.path.join(directory_name, f'{problem.body.name}.dat'),\n        problem.body.mesh.vertices,\n        problem.body.mesh.faces,\n        # xOz_symmetry=isinstance(problem.body, ReflectionSymmetry)\n    )\n\n    # Set range of frequencies\n    if omega_range is None:\n        omega_nb_steps = 1\n        omega_start = problem.omega\n        omega_stop = problem.omega\n    else:\n        omega_nb_steps = len(omega_range)\n        omega_start = min(omega_range)\n        omega_stop = max(omega_range)\n\n    # Write Nemoh.cal\n    with open(os.path.join(directory_name, \"Nemoh.cal\"), \"w\") as nemoh_cal:\n        nemoh_cal.write(\n                DEFAULT_NEMOH_CAL.format(\n                    rho=problem.rho,\n                    g=problem.g,\n                    depth=problem.water_depth if problem.water_depth < np.infty else 0,\n                    mesh_filename=f'{problem.body.name}.dat',\n                    mesh_vertices=problem.body.mesh.nb_vertices,\n                    mesh_faces=problem.body.mesh.nb_faces,\n                    omega_nb_steps=omega_nb_steps,\n                    omega_start=omega_start,\n                    omega_stop=omega_stop,\n                    )\n                )\n\n    # Write input.txt\n    with open(os.path.join(directory_name, \"input.txt\"), \"w\") as input_txt:\n        input_txt.write(DEFAULT_INPUT_TXT)\n\n    # Write ID.dat\n    with open(os.path.join(directory_name, \"ID.dat\"), \"w\") as id_dat:\n        id_dat.write(f\"1\\n.\")",
  "def write_dataset_as_tecplot_files(results_directory, data):\n    \"\"\"Write some of the data from a xarray dataset into legacy tecplot file outputs.\"\"\"\n\n    if 'added_mass' in data:\n        with open(os.path.join(results_directory, 'RadiationCoefficients.tec'), 'w') as fi:\n            for i in range(len(data['radiating_dof'])+1):\n                fi.write(f'...\\n')\n            for dof in data.radiating_dof:\n                fi.write(f'{dof.values}\\n')\n                for o in data.omega:\n                    fi.write(f'  {o.values:e}  ')\n                    for dof2 in data.influenced_dof:\n                        fi.write(f\"{data['added_mass'].sel(omega=o, radiating_dof=dof, influenced_dof=dof2).values:e}\")\n                        fi.write('  ')\n                        fi.write(f\"{data['radiation_damping'].sel(omega=o, radiating_dof=dof, influenced_dof=dof2).values:e}\")\n                        fi.write('  ')\n                    fi.write('\\n')\n\n    if 'diffraction_force' in data:\n        data['excitation_force'] = data['Froude_Krylov_force'] + data['diffraction_force']\n        with open(os.path.join(results_directory, 'ExcitationForce.tec'), 'w') as fi:\n            for i in range(len(data.influenced_dof)+1):\n                fi.write(f'...\\n')\n            for wave_direction in data.wave_direction.values:\n                fi.write(f'angle={wave_direction}\\n')\n                for o in data.omega.values:\n                    fi.write(f'  {o:e}  ')\n                    for dof in data.influenced_dof:\n                        val = data['excitation_force'].sel(omega=o, wave_direction=wave_direction, influenced_dof=dof).values\n                        fi.write(f'{np.abs(val):e}')\n                        fi.write('  ')\n                        fi.write(f'{np.angle(val):e}')\n                        fi.write('  ')\n                    fi.write('\\n')",
  "def export_hydrostatics(hydrostatics_directory, bodies):\n    \"\"\"Determine filenames (following Nemoh convention) and call the .dat file writer\"\"\"\n\n    if os.path.isdir(hydrostatics_directory):\n        LOG.warning(f\"\"\"Exporting problem in already existing directory: {hydrostatics_directory}\n             You might be overwriting existing files!\"\"\")\n    else:\n        os.makedirs(hydrostatics_directory)\n\n    def hydrostatics_writer(hydrostatics_file_path, kh_file_path, body):\n        \"\"\"Write the Hydrostatics.dat and KH.dat files\"\"\"\n        with open(hydrostatics_file_path, 'w') as hf:\n            for j in range(3):\n                line =  f'XF = {body.center_of_buoyancy[j]:7.4f} - XG = {body.center_of_mass[j]:7.4f} \\n'\n                hf.write(line)\n            line = f'Displacement = {body.volume:1.6E}'\n            hf.write(line)\n            hf.close()\n        np.savetxt(kh_file_path, body.hydrostatic_stiffness.values, fmt='%1.6E')\n\n    if isinstance(bodies, FloatingBody):\n        bodies = [bodies]\n    \n    hydrostatics_file_name = \"Hydrostatics.dat\"\n    kh_file_name = \"KH.dat\"\n    \n    body_count = len(bodies)\n    if body_count == 1:\n        body = bodies[0]\n        hydrostatics_file_path = os.path.join(hydrostatics_directory, hydrostatics_file_name)\n        kh_file_path = os.path.join(hydrostatics_directory, kh_file_name)\n        hydrostatics_writer(hydrostatics_file_path, kh_file_path, body)\n    else:\n        for (i, body) in enumerate(bodies):\n            hydrostatics_file_path = os.path.join(hydrostatics_directory, f\"Hydrostatics_{i}.dat\")\n            kh_file_path = os.path.join(hydrostatics_directory, f\"KH_{i}.dat\")\n            hydrostatics_writer(hydrostatics_file_path, kh_file_path, body)",
  "def hydrostatics_writer(hydrostatics_file_path, kh_file_path, body):\n        \"\"\"Write the Hydrostatics.dat and KH.dat files\"\"\"\n        with open(hydrostatics_file_path, 'w') as hf:\n            for j in range(3):\n                line =  f'XF = {body.center_of_buoyancy[j]:7.4f} - XG = {body.center_of_mass[j]:7.4f} \\n'\n                hf.write(line)\n            line = f'Displacement = {body.volume:1.6E}'\n            hf.write(line)\n            hf.close()\n        np.savetxt(kh_file_path, body.hydrostatic_stiffness.values, fmt='%1.6E')",
  "def load_from_meshio(mesh, name=None):\n    \"\"\"Create a Mesh from a meshio mesh object.\"\"\"\n    meshio = import_optional_dependency(\"meshio\")\n    if not isinstance(mesh, meshio._mesh.Mesh):\n        raise TypeError('mesh must be of type meshio._mesh.Mesh, received {:}'.format(type(mesh)))\n\n    def all_faces_as_quads(cells):\n        all_faces = []\n        if 'quad' in cells:\n            all_faces.append(cells['quad'])\n        if 'triangle' in cells:\n            num_triangles = len(mesh.cells_dict['triangle'])\n            LOG.info(\"Stored {:} triangle faces as quadrilaterals\".format(num_triangles))\n            triangles_as_quads = np.empty((cells['triangle'].shape[0], 4), dtype=int)\n            triangles_as_quads[:, :3] = cells['triangle'][:, :]\n            triangles_as_quads[:, 3] = cells['triangle'][:, 2]  # Repeat one node to make a quad\n            all_faces.append(triangles_as_quads)\n        return np.concatenate(all_faces)\n\n    if name is None:\n        name = f'mesh_from_meshio_{next(Mesh._ids)}'\n\n    return Mesh(vertices=mesh.points, faces=all_faces_as_quads(mesh.cells_dict), name=name)",
  "def all_faces_as_quads(cells):\n        all_faces = []\n        if 'quad' in cells:\n            all_faces.append(cells['quad'])\n        if 'triangle' in cells:\n            num_triangles = len(mesh.cells_dict['triangle'])\n            LOG.info(\"Stored {:} triangle faces as quadrilaterals\".format(num_triangles))\n            triangles_as_quads = np.empty((cells['triangle'].shape[0], 4), dtype=int)\n            triangles_as_quads[:, :3] = cells['triangle'][:, :]\n            triangles_as_quads[:, 3] = cells['triangle'][:, 2]  # Repeat one node to make a quad\n            all_faces.append(triangles_as_quads)\n        return np.concatenate(all_faces)",
  "def _check_file(filename, name=None):\n    if not os.path.isfile(filename):\n        raise IOError(\"file %s not found\" % filename)\n    return",
  "def load_mesh(filename, file_format=None, name=None):\n    \"\"\"Driver function that loads every mesh file format known by meshmagick.\n    Dispatch to one of the other function depending on file_format.\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file on disk\n    file_format: str, optional\n        format of the mesh defined in the extension_dict dictionary\n    name: str, optional\n        name for the created mesh object\n\n    Returns\n    -------\n    Mesh or SymmetricMesh\n        the loaded mesh\n    \"\"\"\n    _check_file(filename)\n\n    if file_format is None:\n        _, file_format = os.path.splitext(filename)\n        file_format = file_format.strip('.')\n\n    if file_format not in extension_dict:\n        raise IOError('Extension \".%s\" is not known' % file_format)\n\n    loader = extension_dict[file_format]\n\n    if name is None: name = filename\n\n    return loader(filename, name)",
  "def load_RAD(filename, name=None):\n    \"\"\"Loads RADIOSS mesh files. This export file format may be chosen in ICEM meshing program.\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file on disk\n\n    Returns\n    -------\n    Mesh\n        the loaded mesh\n\n    Note\n    ----\n    RAD files have a 1-indexing\n    \"\"\"\n\n    import re\n    _check_file(filename)\n    ifile = open(filename, 'r')\n    data = ifile.read()\n    ifile.close()\n\n    # node_line = r'\\s*\\d+(?:\\s*' + real_str + '){3}'\n    node_line = r'\\s*\\d+\\s*(' + real_str + r')\\s*(' + real_str + r')\\s*(' + real_str + ')'\n    node_section = r'((?:' + node_line + ')+)'\n\n    elem_line = r'^\\s*(?:\\d+\\s+){6}\\d+\\s*[\\r\\n]+'\n    elem_section = r'((?:' + elem_line + '){3,})'\n\n    pattern_node_line = re.compile(node_line, re.MULTILINE)\n    # pattern_node_line_group = re.compile(node_line, re.MULTILINE)\n    pattern_elem_line = re.compile(elem_line, re.MULTILINE)\n    pattern_node_section = re.compile(node_section, re.MULTILINE)\n    pattern_elem_section = re.compile(elem_section, re.MULTILINE)\n\n    vertices = []\n    node_section = pattern_node_section.search(data).group(1)\n    for node in pattern_node_line.finditer(node_section):\n        vertices.append(list(map(float, list(node.groups()))))\n    vertices = np.asarray(vertices, dtype=float)\n\n    faces = []\n    elem_section = pattern_elem_section.search(data).group(1)\n    for elem in pattern_elem_line.findall(elem_section):\n        faces.append(list(map(int, elem.strip().split()[3:])))\n    faces = np.asarray(faces, dtype=int) - 1\n\n    return Mesh(vertices, faces, name)",
  "def load_HST(filename, name=None):\n    \"\"\"Loads HYDROSTAR (Bureau Veritas (c)) mesh files.\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file on disk\n\n    Returns\n    -------\n    Mesh\n        the loaded mesh\n\n    Note\n    ----\n    HST files have a 1-indexing\n    \"\"\"\n\n    _check_file(filename)\n\n    with open(filename, 'r') as f:\n        lines = f.readlines()\n\n    optional_keywords = ['PROJECT', 'SYMMETRY']\n    not_implemented_optional_keywords = ['USER', 'REFLENGTH', 'GRAVITY', 'RHO', 'NBBODY']\n\n    vertices = []\n    faces = []\n    optional_data = {kw: None for kw in optional_keywords}\n    current_context = None\n    ignored_lines = []\n\n    for i_line, line in enumerate(lines):\n        line = line.lstrip()\n\n        if line == '':\n            continue\n\n        elif line.startswith(\"COORDINATES\"):\n            current_context = 'vertices'\n\n        elif current_context == 'vertices' and line.startswith(\"ENDCOORDINATES\"):\n            current_context = None\n\n        elif line.startswith(\"PANEL\"):\n            panels_type = int(line[10:])\n            current_context = ('panels', panels_type)\n\n        elif (current_context == ('panels', 0) or current_context == ('panels', 1)) and line.startswith(\"ENDPANEL\"):\n            current_context = None\n\n        elif current_context == 'vertices':  # parse vertex coordinates\n            numbers = line.split()\n            if len(numbers) == 4:\n                i_vertex, x, y, z = numbers\n                if int(i_vertex) != len(vertices) + 1:\n                    raise ValueError(\n                        f\"HST mesh reader expected the next vertex to be indexed as {len(vertices)+1}, \"\n                        f\"but it was actually indexed as {i_vertex} (line {i_line+1} of {filename}).\")\n            elif len(numbers) == 3:\n                x, y, z = numbers\n            vertices.append([x, y, z])\n\n        elif current_context == ('panels', 0):  # parse face definition (no index given)\n            numbers = line.split()\n            if len(numbers) == 3:\n                v1, v2, v3 = numbers\n                v4 = v3\n            elif len(numbers) == 4:\n                v1, v2, v3, v4 = numbers\n            faces.append([v1, v2, v3, v4])\n\n        elif current_context == ('panels', 1):  # parse face definition\n            numbers = line.split()\n            if len(numbers) == 4:\n                i_face, v1, v2, v3 = numbers\n                v4 = v3\n            elif len(numbers) == 5:\n                i_face, v1, v2, v3, v4 = numbers\n\n            if int(i_face) != len(faces) + 1:\n                ii = len(faces) + 1\n                raise ValueError(f\"HST mesh reader expected the next face to be indexed {ii},\\n\"\n                                 f\"but it was actually indexed with {i_face} (line {i_line+1} of file {filename}).\")\n            faces.append([v1, v2, v3, v4])\n\n        elif line.startswith(\"ENDFILE\"):\n            break\n\n        else:\n            for keyword in optional_data:\n                if line.startswith(keyword):\n                    optional_data[keyword] = line[len(keyword)+1:].lstrip(':').strip()\n                    break\n            else:\n                ignored_lines.append((i_line+1, line))\n\n    if len(ignored_lines) > 0:\n        formatted_ignored_lines = [\"{: 4} | {}\".format(i, line.strip('\\n')) for (i, line) in ignored_lines]\n        LOG.warning(f\"HST mesh reader ignored the following lines from file {filename}:\\n\" + \"\\n\".join(formatted_ignored_lines))\n\n    vertices = np.array(vertices, dtype=float)\n    faces = np.array(faces, dtype=int) - 1\n\n    if name is None: name = optional_data['PROJECT']\n\n    if optional_data['SYMMETRY'] == '1':\n        return ReflectionSymmetricMesh(Mesh(vertices, faces, f\"half_of_{name}\"), xOz_Plane, name)\n    elif optional_data['SYMMETRY'] == '2':\n        return ReflectionSymmetricMesh(ReflectionSymmetricMesh(Mesh(vertices, faces, f\"quarter_of_{name}\"), yOz_Plane, f\"half_of_{name}\"), xOz_Plane, name)\n    else:\n        return Mesh(vertices, faces, name)",
  "def load_DAT(filename, name=None):\n    \"\"\"Not implemented.\n    Intended to load .DAT files used in DIODORE (PRINCIPIA (c))\n    \"\"\"\n    _check_file(filename)\n    raise NotImplementedError",
  "def load_INP(filename, name=None):\n    \"\"\"Loads DIODORE (PRINCIPIA (c)) configuration file format.\n\n    It parses the .INP file and extracts meshes defined in subsequent .DAT files using the different information\n    contained in the .INP file.\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file on disk\n\n    Returns\n    -------\n    Mesh\n        the loaded mesh\n\n    Note\n    ----\n    INP/DAT files use a 1-indexing\n    \"\"\"\n    _check_file(filename)\n    import re\n\n    with open(filename, 'r') as f:\n        text = f.read()\n\n    # Retrieving frames into a dictionary frames\n    pattern_frame_str = r'^\\s*\\*FRAME,NAME=(.+)[\\r\\n]+(.*)'\n    pattern_frame = re.compile(pattern_frame_str, re.MULTILINE)\n\n    frames = {}\n    for match in pattern_frame.finditer(text):\n        frame_name = match.group(1).strip()\n        frame_vector = re.split(r'[, ]', match.group(2).strip())\n        frames[frame_name] = np.asarray(list(map(float, frame_vector)))\n\n    # Storing the inp layout into a list of dictionary\n    pattern_node_elements = re.compile(r'^\\s*\\*(NODE|ELEMENT),(.*)', re.MULTILINE)\n    layout = []\n    mesh_files = {}\n    for match in pattern_node_elements.finditer(text):\n        field_dict = dict()\n        field_dict['type'] = match.group(1)\n        if field_dict['type'] == 'NODE':\n            field_dict['INCREMENT'] = 'NO'\n        opts = match.group(2).split(',')\n        for opt in opts:\n            key, pair = opt.split('=')\n            field_dict[key] = pair.strip()\n\n        # Retrieving information on mesh files and their usage\n        file = field_dict['INPUT']\n        if file in mesh_files:\n            mesh_files[file][field_dict['type'] + '_CALL_INP'] += 1\n        else:\n            mesh_files[file] = {}\n            mesh_files[file]['NODE_CALL_INP'] = 0\n            mesh_files[file]['ELEMENT_CALL_INP'] = 0\n            mesh_files[file][field_dict['type'] + '_CALL_INP'] += 1\n\n        layout.append(field_dict)\n\n        # RETRIEVING DATA SECTIONS FROM MESHFILES\n        # patterns for recognition of sections\n    node_line = r'\\s*\\d+(?:\\s+' + real_str + '){3}'\n    node_section = r'((?:' + node_line + ')+)'\n    elem_line = r'^ +\\d+(?: +\\d+){3,4}[\\r\\n]+'  # 3 -> triangle, 4 -> quadrangle\n    elem_section = r'((?:' + elem_line + ')+)'\n    pattern_node_line = re.compile(node_line, re.MULTILINE)\n    pattern_elem_line = re.compile(elem_line, re.MULTILINE)\n    pattern_node_section = re.compile(node_section, re.MULTILINE)\n    pattern_elem_section = re.compile(elem_section, re.MULTILINE)\n\n    for file in mesh_files:\n        try:\n            meshfile = open(os.path.join(os.path.dirname(filename), file + '.DAT'), 'r')\n        except:\n            raise IOError('File {0:s} not found'.format(file + '.DAT'))\n        data = meshfile.read()\n        meshfile.close()\n\n        node_section = pattern_node_section.findall(data)\n        if len(node_section) > 1:\n            raise IOError(\"\"\"Several NODE sections into a .DAT file is not supported by meshmagick\n                              as it is considered as bad practice\"\"\")\n        node_array = []\n        idx_array = []\n        for node in pattern_node_line.findall(node_section[0]):\n            node = node.split()\n\n            node[0] = int(node[0])\n            idx_array.append(node[0])\n            node[1:] = list(map(float, node[1:]))\n            node_array.append(node[1:])\n\n        mesh_files[file]['NODE_SECTION'] = node_array\n\n        # Detecting renumberings to do\n        real_idx = 0\n        # renumberings = []\n        id_new = - np.ones(max(idx_array) + 1, dtype=int)\n        # FIXME: cette partie est tres buggee !!!\n        for i, idx in enumerate(idx_array):\n            id_new[idx] = i+1\n\n        mesh_files[file]['ELEM_SECTIONS'] = []\n        for elem_section in pattern_elem_section.findall(data):\n\n            elem_array = []\n            for elem in pattern_elem_line.findall(elem_section):\n                elem = list(map(int, elem.split()))\n                # for node in elem[1:]:\n                elem = id_new[elem[1:]].tolist()\n                if len(elem) == 3:  # Case of a triangle, we repeat the first node at the last position\n                    elem.append(elem[0])\n\n                elem_array.append(list(map(int, elem)))\n            mesh_files[file]['ELEM_SECTIONS'].append(elem_array)\n        mesh_files[file]['nb_elem_sections'] = len(mesh_files[file]['ELEM_SECTIONS'])\n\n        mesh_files[file]['nb_elem_sections_used'] = 0\n\n    nb_nodes = 0\n    nb_elems = 0\n    for field in layout:\n        file = field['INPUT']\n        if field['type'] == 'NODE':\n            nodes = np.asarray(mesh_files[file]['NODE_SECTION'], dtype=float)\n            # Translation of nodes according to frame option id any\n            nodes += frames[field['FRAME']]  # TODO: s'assurer que frame est une options obligatoire...\n\n            if nb_nodes == 0:\n                vertices = nodes.copy()\n                nb_nodes = vertices.shape[0]\n                increment = False\n                continue\n\n            if field['INCREMENT'] == 'NO':\n                vertices[idx, :] = nodes.copy()\n                increment = False\n            else:\n                vertices = np.concatenate((vertices, nodes))\n                nb_nodes = vertices.shape[0]\n                increment = True\n        else:  # this is an ELEMENT section\n            elem_section = np.asarray(mesh_files[file]['ELEM_SECTIONS'][mesh_files[file]['nb_elem_sections_used']],\n                                      dtype=int)\n\n            mesh_files[file]['nb_elem_sections_used'] += 1\n            if mesh_files[file]['nb_elem_sections_used'] == mesh_files[file]['nb_elem_sections']:\n                mesh_files[file]['nb_elem_sections_used'] = 0\n\n            # Updating to new id of nodes\n            elems = elem_section\n            if increment:\n                elems += nb_nodes\n\n            if nb_elems == 0:\n                faces = elems.copy()\n                nb_elems = faces.shape[0]\n                continue\n            else:\n                faces = np.concatenate((faces, elems))\n                nb_elems = faces.shape[0]\n\n    return Mesh(vertices, faces-1, name)",
  "def load_TEC(filename, name=None):\n    \"\"\"Loads TECPLOT (Tecplot (c)) mesh files.\n\n    It relies on the tecplot file reader from the VTK library.\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file on disk\n\n    Returns\n    -------\n    Mesh\n        the loaded mesh\n\n    Note\n    ----\n    TEC files have a 1-indexing\n    \"\"\"\n\n    import re\n\n    _check_file(filename)\n\n    data_pattern = re.compile(\n                    r'ZONE.*\\s*N\\s*=\\s*(\\d+)\\s*,\\s*E=\\s*(\\d+)\\s*,\\s*F\\s*=\\s*FEPOINT\\s*,\\s*ET\\s*=\\s*QUADRILATERAL\\s+'\n                    + r'(^(?:\\s*' + real_str + r'){3,})\\s+'\n                    + r'(^(?:\\s*\\d+)*)', re.MULTILINE)\n\n    with open(filename, 'r') as f:\n        data = f.read()\n\n    nv, nf, vertices, faces = data_pattern.search(data).groups()\n    nv = int(nv)\n    nf = int(nf)\n\n    vertices = np.asarray(list(map(float, vertices.split())), dtype=float).reshape((nv, -1))[:, :3]\n    faces = np.asarray(list(map(int, faces.split())), dtype=int).reshape((nf, 4))-1\n\n    return Mesh(vertices, faces, name)",
  "def load_VTU(filename, name=None):\n    \"\"\"Loads VTK file format in the new XML format (vtu file extension for unstructured meshes).\n\n    It relies on the reader from the VTK library.\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file on disk\n\n    Returns\n    -------\n    Mesh\n        the loaded mesh\n\n    Note\n    ----\n    VTU files have a 0-indexing\n    \"\"\"\n\n    _check_file(filename)\n\n    vtk = import_optional_dependency(\"vtk\")\n\n    reader = vtk.vtkXMLUnstructuredGridReader()\n    reader.SetFileName(str(filename))\n    reader.Update()\n    vtk_mesh = reader.GetOutput()\n\n    vertices, faces = _dump_vtk(vtk_mesh)\n    return Mesh(vertices, faces, name)",
  "def load_VTP(filename, name=None):\n    \"\"\"Loads VTK file format in the new XML format (vtp file extension for polydata meshes).\n\n    It relies on the reader from the VTK library.\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file on disk\n\n    Returns\n    -------\n    Mesh\n        the loaded mesh\n\n    Note\n    ----\n    VTP files have a 0-indexing\n    \"\"\"\n    _check_file(filename)\n\n    vtk = import_optional_dependency(\"vtk\")\n\n    reader = vtk.vtkXMLPolyDataReader()\n    reader.SetFileName(str(filename))\n    reader.Update()\n    vtk_mesh = reader.GetOutput()\n\n    vertices, faces = _dump_vtk(vtk_mesh)\n    return Mesh(vertices, faces, name)",
  "def load_VTK(filename, name=None):\n    \"\"\"Loads VTK file format in the legacy format (vtk file extension).\n\n    It relies on the reader from the VTK library.\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file on disk\n\n    Returns\n    -------\n    Mesh\n        the loaded mesh\n\n    Note\n    ----\n    VTU files have a 0-indexing\n    \"\"\"\n    _check_file(filename)\n\n    vtk = import_optional_dependency(\"vtk\")\n\n    reader = vtk.vtkPolyDataReader()\n    reader.SetFileName(str(filename))\n    reader.Update()\n    vtk_mesh = reader.GetOutput()\n\n    vertices, faces = _dump_vtk(vtk_mesh)\n    return Mesh(vertices, faces, name)",
  "def _dump_vtk(vtk_mesh):\n    \"\"\"Internal driver function that uses the VTK library to read VTK polydata or vtk unstructured grid data structures\n\n    Returns\n    -------\n    vertices: ndarray\n        numpy array of the coordinates of the mesh's nodes\n    faces: ndarray\n        numpy array of the faces' nodes connectivities\n    \"\"\"\n\n    nv = vtk_mesh.GetNumberOfPoints()\n    vertices = np.zeros((nv, 3), dtype=float)\n    for k in range(nv):\n        vertices[k] = np.array(vtk_mesh.GetPoint(k))\n\n    nf = vtk_mesh.GetNumberOfCells()\n    faces = np.zeros((nf, 4), dtype=int)\n    for k in range(nf):\n        cell = vtk_mesh.GetCell(k)\n        nv_facet = cell.GetNumberOfPoints()\n        for l in range(nv_facet):\n            faces[k][l] = cell.GetPointId(l)\n        if nv_facet == 3:\n            faces[k][3] = faces[k][0]\n\n    return vertices, faces",
  "def load_STL(filename, name=None):\n    \"\"\"Loads STL file format.\n\n    It relies on the reader from the VTK library. As STL file format maintains a redundant set of vertices for each\n    faces of the mesh, it returns a merged list of nodes and connectivity array by using the merge_duplicates function.\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file on disk\n\n    Returns\n    -------\n    Mesh\n        the loaded mesh\n\n    Note\n    ----\n    STL files have a 0-indexing\n    \"\"\"\n    vtk = import_optional_dependency(\"vtk\")\n\n    from capytaine.meshes.quality import merge_duplicate_rows\n\n    _check_file(filename)\n\n    reader = vtk.vtkSTLReader()\n    reader.SetFileName(str(filename))\n    reader.Update()\n\n    data = reader.GetOutputDataObject(0)\n\n    nv = data.GetNumberOfPoints()\n    vertices = np.zeros((nv, 3), dtype=float)\n    for k in range(nv):\n        vertices[k] = np.array(data.GetPoint(k))\n    nf = data.GetNumberOfCells()\n    faces = np.zeros((nf, 4), dtype=int)\n    for k in range(nf):\n        cell = data.GetCell(k)\n        if cell is not None:\n            for l in range(3):\n                faces[k][l] = cell.GetPointId(l)\n                faces[k][3] = faces[k][0]  # always repeating the first node as stl is triangle only\n\n    # Merging duplicates nodes\n    vertices, new_id = merge_duplicate_rows(vertices)\n    faces = new_id[faces]\n\n    return Mesh(vertices, faces, name)",
  "def load_NAT(filename, name=None):\n    \"\"\"This function loads natural file format for meshes.\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file on disk\n\n    Returns\n    -------\n    Mesh\n        the loaded mesh\n\n    Notes\n    -----\n    The file format is as follow::\n\n        xsym    ysym\n        n    m\n        x1    y1    z1\n        .\n        .\n        .\n        xn    yn    zn\n        i1    j1    k1    l1\n        .\n        .\n        .\n        im    jm    km    lm\n\n    where :\n    n : number of nodes\n    m : number of cells\n    x1 y1 z1 : cartesian coordinates of node 1\n    i1 j1 k1 l1 : counterclock wise Ids of nodes for cell 1\n    if cell 1 is a triangle, i1==l1\n\n    Note\n    ----\n    NAT files have a 1-indexing\n    \"\"\"\n\n    _check_file(filename)\n\n    ifile = open(filename, 'r')\n    ifile.readline()\n    nv, nf = list(map(int, ifile.readline().split()))\n\n    vertices = []\n    for i in range(nv):\n        vertices.append(list(map(float, ifile.readline().split())))\n    vertices = np.array(vertices, dtype=float)\n\n    faces = []\n    for i in range(nf):\n        faces.append(list(map(int, ifile.readline().split())))\n    faces = np.array(faces, dtype=int)\n\n    ifile.close()\n    return Mesh(vertices, faces-1, name)",
  "def load_GDF(filename, name=None):\n    \"\"\"Loads WAMIT (Wamit INC. (c)) GDF mesh files.\n\n    As GDF file format maintains a redundant set of vertices for each faces of the mesh, it returns a merged list of\n    nodes and connectivity array by using the merge_duplicates function.\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file on disk\n\n    Returns\n    -------\n    Mesh or ReflectionSymmetricMesh\n        the loaded mesh\n\n    Note\n    ----\n    GDF files have a 1-indexing\n    \"\"\"\n\n    _check_file(filename)\n\n    with open(str(filename)) as gdf_file:\n        title = gdf_file.readline()\n        ulen, grav = map(float, gdf_file.readline().split()[:2])\n        isx, isy = map(int, gdf_file.readline().split()[:2])\n        npan = int(gdf_file.readline().split()[0])\n        faces_vertices = np.genfromtxt(gdf_file)\n\n    vertices, indices = np.unique(faces_vertices, axis=0, return_inverse=True)\n    faces = indices.reshape(-1, 4)\n\n    if faces.shape[0] != npan:\n        raise ValueError(\n            f\"In {filename} npan value: {npan} is not equal to face count: \\\n                {faces.shape[0]}.\"\n        )\n\n    if isx == 1 and isy == 1:\n        return ReflectionSymmetricMesh(ReflectionSymmetricMesh(Mesh(vertices, faces, f\"quarter_of_{name}\"), yOz_Plane, f\"half_of_{name}\"), xOz_Plane, name)\n    elif isx == 1:\n        return ReflectionSymmetricMesh(Mesh(vertices, faces, f\"half_of_{name}\"), yOz_Plane, name)\n    elif isy == 1:\n        return ReflectionSymmetricMesh(Mesh(vertices, faces, f\"half_of_{name}\"), xOz_Plane, name)\n    else:\n        return Mesh(vertices, faces, name)",
  "def load_MAR(filename, name=None):\n    \"\"\"Loads Nemoh (Ecole Centrale de Nantes) mesh files.\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file on disk\n\n    Returns\n    -------\n    Mesh or ReflectionSymmetry\n        the loaded mesh\n\n    Note\n    ----\n    MAR files have a 1-indexing\n    \"\"\"\n\n    _check_file(filename)\n\n    ifile = open(filename, 'r')\n\n    header = ifile.readline()\n    _, symmetric_mesh = header.split()\n\n    vertices = []\n    while 1:\n        line = ifile.readline()\n        line = line.split()\n        if line[0] == '0':\n            break\n        vertices.append(list(map(float, line[1:])))\n\n    vertices = np.array(vertices, dtype=float)\n    faces = []\n    while 1:\n        line = ifile.readline()\n        line = line.split()\n        if line[0] == '0':\n            break\n        faces.append(list(map(int, line)))\n\n    faces = np.array(faces, dtype=int)\n\n    ifile.close()\n\n    if int(symmetric_mesh) == 1:\n        if name is None:\n            half_mesh = Mesh(vertices, faces-1)\n            return ReflectionSymmetricMesh(half_mesh, plane=xOz_Plane)\n        else:\n            half_mesh = Mesh(vertices, faces-1, name=f\"half_of_{name}\")\n            return ReflectionSymmetricMesh(half_mesh, plane=xOz_Plane, name=name)\n    else:\n        return Mesh(vertices, faces-1, name)",
  "def load_MSH(filename, name=None):\n    \"\"\"Loads .MSH mesh files generated by GMSH by C. Geuzaine and J.F. Remacle.\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file on disk\n\n    Returns\n    -------\n    Mesh\n        the loaded mesh\n\n    Note\n    ----\n    MSH files have a 1-indexing\n    \"\"\"\n\n    import re\n\n    _check_file(filename)\n\n    with open(filename, 'r') as file:\n        data = file.read()\n\n    nb_nodes, nodes_data = re.search(r'\\$Nodes\\n(\\d+)\\n(.+)\\$EndNodes', data, re.DOTALL).groups()\n    nb_elts, elts_data = re.search(r'\\$Elements\\n(\\d+)\\n(.+)\\$EndElements', data, re.DOTALL).groups()\n\n    vertices = np.asarray(list(map(float, nodes_data.split())), dtype=float).reshape((-1, 4))[:, 1:]\n    vertices = np.ascontiguousarray(vertices)\n    faces = []\n\n    # Triangles\n    for tri_elt in re.findall(r'(^\\d+\\s2(?:\\s\\d+)+?$)', elts_data, re.MULTILINE):\n        tri_elt = list(map(int, tri_elt.split()))\n        triangle = tri_elt[-3:]\n        triangle.append(triangle[0])\n        faces.append(triangle)\n\n    for quad_elt in re.findall(r'(^\\d+\\s3(?:\\s\\d+)+?$)', elts_data, re.MULTILINE):\n        quad_elt = list(map(int, quad_elt.split()))\n        quadrangle = quad_elt[-4:]\n        faces.append(quadrangle)\n\n    faces = np.asarray(faces, dtype=int) - 1\n\n    return Mesh(vertices, faces, name)",
  "def load_MED(filename, name=None):\n    \"\"\"Loads MED mesh files generated by SALOME MECA.\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file on disk\n\n    Returns\n    -------\n    Mesh\n        the loaded mesh\n\n    Note\n    ----\n    MED files have a 1-indexing\n    \"\"\"\n\n    try:\n        import h5py\n    except ImportError:\n        raise ImportError('MED file format reader needs h5py module to be installed')\n\n    _check_file(filename)\n\n    file = h5py.File(filename)\n\n    list_of_names = []\n    file.visit(list_of_names.append)\n\n    nb_quadrangles = nb_triangles = 0\n\n    for item in list_of_names:\n        if '/NOE/COO' in item:\n            vertices = file[item][:].reshape((3, -1)).T\n            nv = vertices.shape[0]\n        if '/MAI/TR3/NOD' in item:\n            triangles = file[item][:].reshape((3, -1)).T - 1\n            nb_triangles = triangles.shape[0]\n        if '/MAI/QU4/NOD' in item:\n            quadrangles = file[item][:].reshape((4, -1)).T - 1\n            nb_quadrangles = quadrangles.shape[0]\n\n    file.close()\n\n    if nb_triangles == 0:\n        triangles = np.zeros((0, 4), dtype=int)\n    else:\n        triangles = np.column_stack((triangles, triangles[:, 0]))\n    if nb_quadrangles == 0:\n        quadrangles = np.zeros((0, 4), dtype=int)\n\n    faces = np.row_stack([triangles, quadrangles])\n\n    return Mesh(vertices, faces, name=name)",
  "def load_WRL(filename, name=None):\n    \"\"\"Loads VRML 2.0 mesh files.\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file on disk\n\n    Returns\n    -------\n    Mesh\n        the loaded mesh\n    \"\"\"\n    import re\n\n    vtk = import_optional_dependency(\"vtk\")\n\n    _check_file(filename)\n\n    # Checking version\n    with open(filename, 'r') as f:\n        line = f.readline()\n        ver = re.search(r'#VRML\\s+V(\\d.\\d)', line).group(1)\n        if not ver == '2.0':\n            raise NotImplementedError('VRML loader only supports VRML 2.0 format (version %s given)' % ver)\n\n    importer = vtk.vtkVRMLImporter()\n    importer.SetFileName(str(filename))\n    importer.Update()\n\n    actors = importer.GetRenderer().GetActors()\n    actors.InitTraversal()\n    dataset = actors.GetNextActor().GetMapper().GetInput()\n\n    return _dump_vtk(dataset)",
  "def load_NEM(filename, name=None):\n    \"\"\"Loads mesh files that are used by the ``Mesh`` tool included in Nemoh.\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file on disk\n\n    Returns\n    -------\n    Mesh\n        the loaded mesh\n\n    Note\n    ----\n    This format is different from that is used directly by Nemoh software. It is only dedicated to the Mesh tool.\n    \"\"\"\n\n    _check_file(filename)\n\n    ifile = open(filename, 'r')\n\n    nv = int(ifile.readline())\n    nf = int(ifile.readline())\n\n    vertices = []\n    for ivertex in range(nv):\n        vertices.append(list(map(float, ifile.readline().split())))\n    vertices = np.asarray(vertices, dtype=float)\n\n    faces = []\n    for iface in range(nf):\n        faces.append(list(map(int, ifile.readline().split())))\n    faces = np.asarray(faces, dtype=int)\n    faces -= 1\n\n    return Mesh(vertices, faces, name)",
  "def load_PNL(filename, name=None):\n    \"\"\"Load mesh using HAMS file format.\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file on disk\n\n    Returns\n    -------\n    Mesh or ReflectionSymmetricMesh\n        the loaded mesh\n    \"\"\"\n\n    with open(filename, 'r') as f:\n        # Skip 3 title lines\n        f.readline()\n        f.readline()\n        f.readline()\n        # Read header data\n        nb_faces, nb_vertices, x_sym, y_sym = map(int, f.readline().split())\n        # Skip 2 more lines\n        f.readline()\n        f.readline()\n        vertices = np.genfromtxt((f.readline() for _ in range(nb_vertices)), usecols=(1, 2, 3))\n        # Skip 3 more lines\n        f.readline()\n        f.readline()\n        f.readline()\n        faces = np.zeros((nb_faces, 4), dtype=int)\n        for i in range(nb_faces):\n            index, nb_corners, *data = map(int, f.readline().split())\n            assert i+1 == index\n            if nb_corners == 3:  # Triangle\n                assert len(data) == 3\n                faces[i, 0:3] = data\n                faces[i, 3] = faces[i, 2]  # Convention for triangles in Capytaine: repeat last vertex\n            elif int(nb_corners) == 4:  # Quadrangle\n                assert len(data) == 4\n                faces[i, :] = data\n    faces = faces - 1  # Going from Fortran 1-based indices to Numpy 0-based indices\n\n    if x_sym == 1 and y_sym == 0:\n        half_mesh = Mesh(vertices, faces, name=(f\"half_of_{name}\" if name is not None else None))\n        return ReflectionSymmetricMesh(half_mesh, plane=yOz_Plane, name=name)\n    elif x_sym == 0 and y_sym == 1:\n        half_mesh = Mesh(vertices, faces, name=(f\"half_of_{name}\" if name is not None else None))\n        return ReflectionSymmetricMesh(half_mesh, plane=xOz_Plane, name=name)\n    elif x_sym == 1 and y_sym == 1:\n        quarter_mesh = Mesh(vertices, faces, name=(f\"quarter_of_{name}\" if name is not None else None))\n        half_mesh = ReflectionSymmetricMesh(quarter_mesh, plane=xOz_Plane, name=(f\"half_of_{name}\" if name is not None else None))\n        return ReflectionSymmetricMesh(half_mesh, plane=yOz_Plane, name=name)\n    else:\n        return Mesh(vertices, faces, name)",
  "def write_mesh(filename, vertices, faces, file_format):\n    \"\"\"Driver function that writes every mesh file file_format known by meshmagick\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file to be written on disk\n    vertices: ndarray\n        numpy array of the coordinates of the mesh's nodes\n    faces: ndarray\n        numpy array of the faces' nodes connectivities\n    file_format: str\n        file_format of the mesh defined in the extension_dict dictionary\n    \"\"\"\n\n    if file_format not in extension_dict:\n        raise IOError('Extension \"%s\" is not known' % file_format)\n\n    writer = extension_dict[file_format]\n\n    writer(filename, vertices, faces)",
  "def write_DAT(filename, vertices, faces):\n    \"\"\"Writes .DAT file format for the DIODORE (PRINCIPA (c)) software.\n\n    It also displays suggestions for inclusion into the .INP configuration\n    file.\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file to be written on disk\n    vertices: ndarray\n        numpy array of the coordinates of the mesh's nodes\n    faces: ndarray\n        numpy array of the faces' nodes connectivities\n    \"\"\"\n\n    root_filename, ext = os.path.splitext(filename)\n    filename = root_filename + ext.upper()\n    ofile = open(filename, 'w')\n\n    ofile.write('$\\n$ Data for DIODORE input file : {0}\\n'.format(root_filename.upper()))\n    ofile.write('$ GENERATED BY MESHMAGICK ON {0}\\n$\\n'.format(time.strftime('%c')))\n\n    ofile.write('$ NODE\\n')\n    vertex_block = \\\n        ''.join(\n            (\n                '\\n'.join(\n                    ''.join(\n                        (\n                            '{:8d}'.format(idx+1),\n                            ''.join('{:13.5E}'.format(elt) for elt in node)\n                        )\n                    ) for (idx, node) in enumerate(vertices)\n                ),\n\n                '\\n*RETURN\\n'\n            )\n        )\n    ofile.write(vertex_block)\n\n    quad_block = '$\\n$ ELEMENT,TYPE=Q4C000,ELSTRUCTURE={0}'.format(root_filename.upper())\n    tri_block = '$\\n$ ELEMENT,TYPE=T3C000,ELSTRUCTURE={0}'.format(root_filename.upper())\n    nq = 0\n    nt = 0\n    for (idx, cell) in enumerate(faces+1):\n        if cell[0] != cell[-1]:\n            # quadrangle\n            nq += 1\n            quad_block = ''.join(\n                (\n                    quad_block,\n                    '\\n',\n                    '{:8d}'.format(idx+1),\n                    ''.join('{:8d}'.format(node_id) for node_id in cell)\n                )\n            )\n\n        else:\n            # Triangle\n            nt += 1\n            tri_block = ''.join(\n                (\n                    tri_block,\n                    '\\n',\n                    '{:8d}'.format(idx+1),\n                    ''.join('{:8d}'.format(node_id) for node_id in cell[:3])\n                )\n            )\n\n    print('-------------------------------------------------')\n    print('Suggestion for .inp DIODORE input file :')\n    print('')\n    print('*NODE,INPUT={0},FRAME=???'.format(root_filename))\n\n    if nq > 0:\n        quad_block = ''.join((quad_block, '\\n*RETURN\\n'))\n        ofile.write(quad_block)\n        print('*ELEMENT,TYPE=Q4C000,ELSTRUCTURE={0},INPUT={0}'.format(root_filename))\n    if nt > 0:\n        tri_block = ''.join((tri_block, '\\n*RETURN\\n'))\n        ofile.write(tri_block)\n        print('*ELEMENT,TYPE=T3C000,ELSTRUCTURE={0},INPUT={0}'.format(root_filename))\n\n    print('')\n    print('-------------------------------------------------')\n    ofile.close()",
  "def write_HST(filename, vertices, faces):\n    \"\"\"Writes .HST file format for the HYDROSTAR (Bureau Veritas (c)) software.\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file to be written on disk\n    vertices: ndarray\n        numpy array of the coordinates of the mesh's nodes\n    faces: ndarray\n        numpy array of the faces' nodes connectivities\n    \"\"\"\n    # TODO: allow many bodies\n\n    ofile = open(filename, 'w')\n\n    ofile.write(''.join((\n        'PROJECT:\\n',\n        'USERS:   meshmagick\\n\\n'\n        'NBODY   1\\n'\n        'RHO   1025.0\\n'\n        'GRAVITY   9.81\\n\\n'\n    )))\n\n    coordinates_block = ''.join((  # block\n            'COORDINATES\\n',\n            '\\n'.join(  # line\n                ''.join(\n                    (\n                        '{:10d}'.format(idx+1),  # index\n                        ''.join('{:16.6E}'.format(elt) for elt in node)  # node coordinates\n                    )\n                ) for (idx, node) in enumerate(vertices)\n            ),\n            '\\nENDCOORDINATES\\n\\n'\n    ))\n\n    ofile.write(coordinates_block)\n\n    cells_coordinates = ''.join((  # block\n        'PANEL TYPE 0\\n',\n        '\\n'.join(  # line\n            ''.join(\n                '{:10d}'.format(node_idx) for node_idx in cell\n            ) for cell in faces + 1\n        ),\n        '\\nENDPANEL\\n\\n'\n    ))\n\n    ofile.write(cells_coordinates)\n\n    ofile.write('ENDFILE\\n')\n\n    ofile.close()",
  "def write_TEC(filename, vertices, faces):\n    \"\"\"Writes .TEC file format for the TECPLOT (Tecplot (c)) visualisation software.\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file to be written on disk\n    vertices: ndarray\n        numpy array of the coordinates of the mesh's nodes\n    faces: ndarray\n        numpy array of the faces' nodes connectivities\n    \"\"\"\n\n    ofile = open(filename, 'w')\n\n    nv = vertices.shape[0]\n    nf = faces.shape[0]\n\n    ofile.write('TITLE = \\\" THIS FILE WAS GENERATED BY MESHMAGICK - FICHIER : {} \\\" \\n'.format(filename))\n\n    ofile.write('VARIABLES = \\\"X\\\",\\\"Y\\\",\\\"Z\\\" \\n')\n    ofile.write('ZONE T=\\\"MESH\\\" \\n')\n    ofile.write('N={nv:10d} ,E={nf:10d} , F=FEPOINT, ET=QUADRILATERAL\\n'.format(nv=nv, nf=nf))\n\n    node_block = '\\n'.join( # block\n        ''.join(\n            ''.join('{:16.6E}'.format(elt) for elt in node)\n        ) for node in vertices\n    ) + '\\n'\n    ofile.write(node_block)\n\n    cells_block = '\\n'.join(  # block\n        ''.join(\n            ''.join('{:10d}'.format(node_id) for node_id in cell)\n        ) for cell in faces + 1\n    ) + '\\n'\n    ofile.write(cells_block)\n\n    ofile.close()\n\n    return 1",
  "def write_VTU(filename, vertices, faces):\n    \"\"\"Writes .vtu file format for the paraview (Kitware (c)) visualisation software.\n\n    It relies on the VTK library for its writer. VTU files use the last XML file format of the VTK library.\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file to be written on disk\n    vertices: ndarray\n        numpy array of the coordinates of the mesh's nodes\n    faces: ndarray\n        numpy array of the faces' nodes connectivities\n    \"\"\"\n    vtk = import_optional_dependency(\"vtk\")\n\n    writer = vtk.vtkXMLUnstructuredGridWriter()\n    writer.SetDataModeToAscii()\n    writer.SetFileName(str(filename))\n\n    unstructured_grid = _build_vtkUnstructuredGrid(vertices, faces)\n    writer.SetInputData(unstructured_grid)\n    writer.Write()",
  "def write_VTP(filename, vertices, faces):\n    \"\"\"Writes .vtp file format for the Paraview (Kitware (c)) visualisation software.\n\n    It relies on the VTK library for its writer. VTP files use the last XML file format of the VTK library and\n    correspond to polydata.\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file to be written on disk\n    vertices: ndarray\n        numpy array of the coordinates of the mesh's nodes\n    faces: ndarray\n        numpy array of the faces' nodes connectivities\n    \"\"\"\n\n    vtk = import_optional_dependency(\"vtk\")\n\n    writer = vtk.vtkXMLPolyDataWriter()\n    writer.SetDataModeToAscii()\n    writer.SetFileName(str(filename))\n\n    polydata = _build_vtkPolyData(vertices, faces)\n    writer.SetInputData(polydata)\n    writer.Write()",
  "def write_VTK(filename, vertices, faces):\n    \"\"\"Writes .vtk file format for the Paraview (Kitware (c)) visualisation software.\n\n    It relies on the VTK library for its writer. VTK files use the legagy ASCII file format of the VTK library.\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file to be written on disk\n    vertices: ndarray\n        numpy array of the coordinates of the mesh's nodes\n    faces: ndarray\n        numpy array of the faces' nodes connectivities\n    \"\"\"\n\n    nv = vertices.shape[0]\n    nf = faces.shape[0]\n\n    triangle_mask = (faces[:, 0] == faces[:, -1])\n    quadrangles_mask = np.invert(triangle_mask)\n    nb_triangles = len(np.where(triangle_mask)[0])\n    nb_quandrangles = len(np.where(quadrangles_mask)[0])\n\n    with open(filename, 'w') as f:\n\n        f.write('# vtk DataFile Version 4.0\\n')\n        f.write('vtk file generated by meshmagick on %s\\n' % time.strftime('%c'))\n        f.write('ASCII\\n')\n        f.write('DATASET POLYDATA\\n')\n        f.write('POINTS %u float\\n' % nv)\n\n        for vertex in vertices:\n            f.write('%f %f %f\\n' % (vertex[0], vertex[1], vertex[2]))\n\n        f.write('POLYGONS %u %u\\n' % (nf, 4*nb_triangles+5*nb_quandrangles))\n\n        for face in faces:\n            if face[0] == face[-1]:  # Triangle\n                f.write('3 %u %u %u\\n' % (face[0], face[1], face[2]))\n            else:  # Quadrangle\n                f.write('4 %u %u %u %u\\n' % (face[0], face[1], face[2], face[3]))",
  "def _build_vtkUnstructuredGrid(vertices, faces):\n    \"\"\"Internal function that builds a VTK object for manipulation by the VTK library.\n\n    Parameters\n    ----------\n    vertices: ndarray\n        numpy array of the coordinates of the mesh's nodes\n    faces: ndarray\n        numpy array of the faces' nodes connectivities\n\n    Returns\n    -------\n    vtkObject\n    \"\"\"\n\n    vtk = import_optional_dependency(\"vtk\")\n\n    nv = max(np.shape(vertices))\n    nf = max(np.shape(faces))\n\n    vtk_mesh = vtk.vtkUnstructuredGrid()\n    vtk_mesh.Allocate(nf, nf)\n\n    # Building the vtkPoints data structure\n    vtk_points = vtk.vtkPoints()\n    vtk_points.SetNumberOfPoints(nv)\n    for idx, vertex in enumerate(vertices):\n        vtk_points.SetPoint(idx, vertex)\n\n    vtk_mesh.SetPoints(vtk_points)  # Storing the points into vtk_mesh\n\n    # Building the vtkCell data structure\n    for cell in faces:\n        if cell[-1] in cell[:-1]:\n            vtk_cell = vtk.vtkTriangle()\n            nc = 3\n        else:\n            # #print 'quadrangle'\n            vtk_cell = vtk.vtkQuad()\n            nc = 4\n\n        for k in range(nc):\n            vtk_cell.GetPointIds().SetId(k, cell[k])\n\n        vtk_mesh.InsertNextCell(vtk_cell.GetCellType(), vtk_cell.GetPointIds())\n    return vtk_mesh",
  "def _build_vtkPolyData(vertices, faces):\n    \"\"\"Builds a vtkPolyData object from vertices and faces\"\"\"\n\n    vtk = import_optional_dependency(\"vtk\")\n\n    # Create a vtkPoints object and store the points in it\n    points = vtk.vtkPoints()\n    for point in vertices:\n        points.InsertNextPoint(point)\n\n    # Create a vtkCellArray to store faces\n    cell_array = vtk.vtkCellArray()\n    for face_ids in faces:\n        if face_ids[0] == face_ids[-1]:\n            # Triangle\n            curface = face_ids[:3]\n            vtk_face = vtk.vtkTriangle()\n        else:\n            # Quadrangle\n            curface = face_ids[:4]\n            vtk_face = vtk.vtkQuad()\n\n        for idx, id in enumerate(curface):\n            vtk_face.GetPointIds().SetId(idx, id)\n\n        cell_array.InsertNextCell(vtk_face)\n\n    polydata_mesh = vtk.vtkPolyData()\n    polydata_mesh.SetPoints(points)\n    polydata_mesh.SetPolys(cell_array)\n\n    return polydata_mesh",
  "def write_NAT(filename, vertices, faces):\n    \"\"\"Writes .nat file format as defined into the load_NAT function.\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file to be written on disk\n    vertices: ndarray\n        numpy array of the coordinates of the mesh's nodes\n    faces: ndarray\n        numpy array of the faces' nodes connectivities\n\n    See Also\n    --------\n    load_NAT\n    \"\"\"\n\n    ofile = open(filename, 'w')\n\n    nv = max(np.shape(vertices))\n    nf = max(np.shape(faces))\n\n    ofile.write('%6u%6u\\n' % (0, 0))  # lire les symmetries dans args...\n    ofile.write('%6u%6u\\n' % (nv, nf))\n    for vertex in vertices:\n        ofile.write('%15.6E%15.6E%15.6E\\n' % (vertex[0], vertex[1], vertex[2]))\n    for cell in faces+1:\n        ofile.write('%10u%10u%10u%10u\\n' % (cell[0], cell[1], cell[2], cell[3]))\n\n    ofile.close()",
  "def write_NEM(filename, vertices, faces):\n    \"\"\"Writes mesh files used by the Mesh tool included in Nemoh\n\n    Parameters\n    ----------\n    filename : str\n        name of the mesh file to be written on disk\n    vertices: ndarray\n        numpy array of the coordinates of the mesh's nodes\n    faces: ndarray\n        numpy array of the faces' nodes connectivities\n\n    Note\n    ----\n    This file format is different from that used by Nemoh itself. It is only used by the Mesh tool.\n    \"\"\"\n    ofile = open(filename, 'w')\n\n    ofile.write('%u\\n' % vertices.shape[0])\n    ofile.write('%u\\n' % faces.shape[0])\n\n    for vertex in vertices:\n        ofile.write('%15.6f\\t%15.6f\\t%15.6f\\n' % (vertex[0], vertex[1], vertex[2]))\n\n    for face in faces+1:\n        ofile.write('%10u\\t%10u\\t%10u\\t%10u\\n' % (face[0], face[1], face[2], face[3]))\n\n    ofile.close()",
  "def write_GDF(filename, vertices, faces, ulen=100.0, gravity=9.81, isx=0, isy=0):\n    \"\"\"Writes .gdf file format for the WAMIT (Wamit INC. (c)) BEM software.\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file to be written on disk\n    vertices: ndarray\n        numpy array of the coordinates of the mesh's nodes\n    faces: ndarray\n        numpy array of the faces' nodes connectivities\n    ulen: float, optional\n        length scale. The default is 100.0\n    gravity: float, optional\n        acceleration of gravity. The default is 9.81\n    isx: {0, 1}, optional\n        symmetry in x-axis. The default is 0\n    isy: {0, 1}, optional \n        symmetry in y-axis. The default is 0\n    \"\"\"\n\n    nf = max(np.shape(faces))\n\n    ofile = open(filename, 'w')\n\n    ofile.write('GDF file generated by meshmagick on %s\\n' % time.strftime('%c'))\n\n    ofile.write('%16.6f%16.6f\\n' % (ulen, gravity))\n    ofile.write('%12u%12u\\n' % (isx, isy))  # TODO : mettre les symetries en argument\n    ofile.write('%12u\\n' % nf)\n\n    for cell in faces:\n        for k in range(4):\n            cur_vertices = vertices[cell[k], :]\n            ofile.write('%16.6E%16.6E%16.6E\\n' % (cur_vertices[0], cur_vertices[1], cur_vertices[2]))\n\n    ofile.close()",
  "def write_MAR(filename, vertices, faces):\n    \"\"\"Writes mesh files to be used with Nemoh BEM software (Ecole Centrale de Nantes)\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file to be written on disk\n    vertices: ndarray\n        numpy array of the coordinates of the mesh's nodes\n    faces: ndarray\n        numpy array of the faces' nodes connectivities\n    \"\"\"\n\n    # TODO: detect symmetry in Oxz plane\n\n    ofile = open(filename, 'w')\n\n    ofile.write('{0:6d}{1:6d}\\n'.format(2, 0))  # TODO : mettre les symetries en argument\n\n    for (idx, vertex) in enumerate(vertices):\n        ofile.write('{0:6d}{1:16.6f}{2:16.6f}{3:16.6f}\\n'.format(idx+1, vertex[0], vertex[1], vertex[2]))\n\n    ofile.write('{0:6d}{1:6d}{2:6d}{3:6d}{4:6d}\\n'.format(0, 0, 0, 0, 0))\n\n    cell_block = '\\n'.join(\n        ''.join('{0:10d}'.format(elt) for elt in cell)\n        for cell in faces + 1\n    ) + '\\n'\n    ofile.write(cell_block)\n    ofile.write('%6u%6u%6u%6u\\n' % (0, 0, 0, 0))\n\n    ofile.close()\n\n    print('WARNING: if you described only one part of the mesh using symmetry for Nemoh, you may manually modify the ' \\\n          'file header accordingly')",
  "def write_RAD(filename, vertices, faces):\n    raise NotImplementedError",
  "def write_STL(filename, vertices, faces):\n    \"\"\"Writes .stl file format. It relies on the VTK library for its writer.\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file to be written on disk\n    vertices: ndarray\n        numpy array of the coordinates of the mesh's nodes\n    faces: ndarray\n        numpy array of the faces' nodes connectivities\n    \"\"\"\n\n    # TODO : replace this implementation by using the vtk functionalities\n\n    # Triangulating quads\n    t1 = (0, 1, 2)\n    t2 = (0, 2, 3)\n\n    quads_ids = np.where(faces[:, 0] != faces[:, -1])[0]\n\n    new_faces = faces[quads_ids].copy()\n    new_faces[:, :3] = new_faces[:, t1]\n    new_faces[:, -1] = new_faces[:, 0]\n\n    faces[quads_ids, :3] = faces[:, t2][quads_ids]\n    faces[quads_ids, -1] = faces[quads_ids, 0]\n\n    faces = np.concatenate((faces, new_faces))\n\n    # Writing file\n    ofile = open(filename, 'w')\n\n    ofile.write('solid meshmagick\\n')\n\n    for face in faces:\n        if face[0] != face[3]:\n            raise RuntimeError(\"\"\"Only full triangle meshes are accepted in STL files.\n              Please consider using the --triangulate-quadrangles option (-tq) to\n              perform a prior triangulation of the mesh\"\"\")\n\n        # Computing normal\n        v0 = vertices[face[0], :]\n        v1 = vertices[face[1], :]\n        v2 = vertices[face[2], :]\n\n        n = np.cross(v1 - v0, v2 - v0)\n        n /= np.linalg.norm(n)\n\n        block_facet = ''.join(['  facet normal ', ''.join('%15.6e' % ni for ni in n) + '\\n',\n                               '    outer loop\\n',\n                               '      vertex', ''.join('%15.6e' % Vi for Vi in v0) + '\\n',\n                               '      vertex', ''.join('%15.6e' % Vi for Vi in v1) + '\\n',\n                               '      vertex', ''.join('%15.6e' % Vi for Vi in v2) + '\\n',\n                               '    endloop\\n',\n                               '  endfacet\\n'])\n        ofile.write(block_facet)\n    ofile.write('endsolid meshmagick\\n')\n    ofile.close()",
  "def write_INP(filename, vertices, faces):\n    raise NotImplementedError('INP writer is not implementer yet')",
  "def write_MSH(filename, vertices, faces):\n    raise NotImplementedError('MSH writer is not implemented yet')",
  "def write_MED(filename, vertices, faces):\n    raise NotImplementedError('MED writer is not implemented yet')",
  "def write_WRL(filename, vertices, faces):\n    raise NotImplementedError('VRML writer is not implemented yet')",
  "def write_PNL(filename, vertices, faces):\n    \"\"\"Write a mesh to a file using HAMS file format.\n\n    Took some inspiration from \"nemohmesh_to_pnl\" by Garett Barter\n    https://github.com/WISDEM/pyHAMS/blob/d10b51122e92849c63640b34e4fa9d413eb306fd/pyhams/pyhams.py#L11\n\n    This writer does not support symmetries.\n\n    Parameters\n    ----------\n    filename: str\n        name of the mesh file to be written on disk\n    vertices: ndarray\n        numpy array of the coordinates of the mesh's nodes\n    faces: ndarray\n        numpy array of the faces' nodes connectivities\n    \"\"\"\n    with open(filename, 'w') as f:\n        f.write('    --------------Hull Mesh File---------------\\n')\n        f.write('\\n')\n        f.write('    # Number of Panels, Nodes, X-Symmetry and Y-Symmetry\\n')\n        f.write(f'         {faces.shape[0]}         {vertices.shape[0]}         0         0\\n')\n        f.write('\\n')\n        f.write('    #Start Definition of Node Coordinates     ! node_number   x   y   z\\n')\n        for i, vertex in enumerate(vertices):\n            f.write(\"{:>5}{:>18.6f}{:>18.6f}{:>18.6f}\\n\".format(i+1, *vertex))\n        f.write('   #End Definition of Node Coordinates\\n')\n        f.write('\\n')\n        f.write('   #Start Definition of Node Relations   ! panel_number  number_of_vertices   Vertex1_ID   Vertex2_ID   Vertex3_ID   (Vertex4_ID)\\n')\n        for i, face in enumerate(faces):\n            face = face + 1\n            if face[2] == face[3]:  # Triangle\n                f.write(\"{:>5}{:>5}{:>10}{:>10}{:>10}\\n\".format(i+1, 3, *face[:3]))\n            else:\n                f.write(\"{:>5}{:>5}{:>10}{:>10}{:>10}{:>10}\\n\".format(i+1, 4, *face))\n        f.write('   #End Definition of Node Relations\\n')\n        f.write('\\n')\n        f.write('    --------------End Hull Mesh File---------------\\n')",
  "def problems_from_dataset(dataset: xr.Dataset,\n                          bodies: Union[FloatingBody, Sequence[FloatingBody]],\n                          ) -> List[LinearPotentialFlowProblem]:\n    \"\"\"Generate a list of problems from a test matrix.\n\n    Parameters\n    ----------\n    dataset : xarray Dataset\n        Test matrix containing the problems parameters.\n    bodies : FloatingBody or list of FloatingBody\n        The bodies on which the computations of the test matrix will be applied.\n        They should all have different names.\n\n    Returns\n    -------\n    list of LinearPotentialFlowProblem\n\n    Raises\n    ------\n    ValueError\n        if required fields are missing in the dataset\n    \"\"\"\n    if isinstance(bodies, FloatingBody):\n        bodies = [bodies]\n\n    # SANITY CHECKS\n    assert len(list(set(body.name for body in bodies))) == len(bodies), \\\n        \"All bodies should have different names.\"\n\n    # Warn user in case of key with unrecognized name (e.g. mispells)\n    keys_in_dataset = set(dataset.dims.keys())\n    accepted_keys = {'wave_direction', 'radiating_dof', 'influenced_dof',\n                     'body_name', 'omega', 'period', 'wavelength', 'wavenumber',\n                     'water_depth', 'rho', 'g'}\n    unrecognized_keys = keys_in_dataset.difference(accepted_keys)\n    if len(unrecognized_keys) > 0:\n        LOG.warning(f\"Unrecognized key(s) in dataset: {unrecognized_keys}\")\n\n    if (\"radiating_dof\" not in keys_in_dataset) and (\"wave_direction\" not in keys_in_dataset):\n        raise ValueError(\"Neither 'radiating_dof' nor 'wave_direction' has been provided in the dataset. \"\n                \"No linear potential flow problem can be inferred.\")\n\n    frequency_keys = keys_in_dataset & {'omega', 'period', 'wavelength', 'wavenumber'}\n    if len(frequency_keys) > 1:\n            raise ValueError(\"Setting problems requires at most one of the following: omega (angular frequency) OR period OR wavenumber OR wavelength.\\n\"\n                             \"Received {}\".format(frequency_keys))\n    # END SANITY CHECKS\n\n    dataset = _unsqueeze_dimensions(dataset)\n\n    if len(frequency_keys) == 0:\n        freq_type = \"omega\"\n        freq_range = [_default_parameters['omega']]\n    else:  # len(frequency_keys) == 1\n        freq_type = list(frequency_keys)[0]  # Get the only item\n        freq_range = dataset[freq_type].data\n\n    water_depth_range = dataset['water_depth'].data if 'water_depth' in dataset else [_default_parameters['water_depth']]\n    rho_range = dataset['rho'].data if 'rho' in dataset else [_default_parameters['rho']]\n    g_range = dataset['g'].data if 'g' in dataset else [_default_parameters['g']]\n\n    wave_direction_range = dataset['wave_direction'].data if 'wave_direction' in dataset else None\n    radiating_dofs = dataset['radiating_dof'].data.astype(object) if 'radiating_dof' in dataset else None\n    # astype(object) is meant to convert Numpy internal string type numpy.str_ to Python general string type.\n\n    if 'body_name' in dataset:\n        assert set(dataset['body_name'].data) <= {body.name for body in bodies}, \\\n            \"Some body named in the dataset was not given as argument to `problems_from_dataset`.\"\n        body_range = {body.name: body for body in bodies if body.name in dataset['body_name'].data}\n        # Only the bodies listed in the dataset have been kept\n    else:\n        body_range = {body.name: body for body in bodies}\n\n    problems = []\n    if wave_direction_range is not None:\n        for freq, wave_direction, water_depth, body_name, rho, g \\\n                in product(freq_range, wave_direction_range, water_depth_range, body_range, rho_range, g_range):\n            problems.append(\n                DiffractionProblem(body=body_range[body_name], **{freq_type: freq},\n                                   wave_direction=wave_direction, water_depth=water_depth, rho=rho, g=g)\n            )\n\n    if radiating_dofs is not None:\n        for freq, radiating_dof, water_depth, body_name, rho, g \\\n                in product(freq_range, radiating_dofs, water_depth_range, body_range, rho_range, g_range):\n            problems.append(\n                RadiationProblem(body=body_range[body_name], **{freq_type: freq},\n                                 radiating_dof=radiating_dof, water_depth=water_depth, rho=rho, g=g)\n            )\n\n    return sorted(problems)",
  "def _squeeze_dimensions(data_array, dimensions=None):\n    \"\"\"Remove dimensions if they are of size 1. The coordinates become scalar coordinates.\"\"\"\n    if dimensions is None:\n        dimensions = data_array.dims\n    for dim in dimensions:\n        if len(data_array[dim]) == 1:\n            data_array = data_array.squeeze(dim, drop=False)\n    return data_array",
  "def _unsqueeze_dimensions(data_array, dimensions=None):\n    \"\"\"Add scalar coordinates as dimensions of size 1.\"\"\"\n    if dimensions is None:\n        dimensions = list(data_array.coords.keys())\n    for dim in dimensions:\n        if len(data_array.coords[dim].values.shape) == 0:\n            data_array = xr.concat([data_array], dim=dim)\n    return data_array",
  "def _dataset_from_dataframe(df: pd.DataFrame,\n                            variables: Union[str, Sequence[str]],\n                            dimensions: Sequence[str],\n                            optional_dims: Sequence[str],\n                            ) -> Union[xr.DataArray, xr.Dataset]:\n    \"\"\"Transform a pandas.Dataframe into a xarray.Dataset.\n\n    Parameters\n    ----------\n    df: pandas.DataFrame\n        the input dataframe\n    variables: string or sequence of strings\n        the variables that will be stored in the output dataset.\n        If a single name is provided, a DataArray of this variable will be provided instead.\n    dimensions: sequence of strings\n        Names of dimensions the variables depends on.\n        They will always appear as dimension in the output dataset.\n    optional_dims: sequence of strings\n        Names of dimensions the variables depends on.\n        They will appears as dimension in the output dataset only if they have\n        more than one different values.\n    \"\"\"\n\n    for variable_name in variables:\n        df = df[df[variable_name].notnull()].dropna(axis='columns')  # Keep only records with non null values of all the variables\n    df = df.drop_duplicates(optional_dims + dimensions)\n    df = df.set_index(optional_dims + dimensions)\n\n    da = df.to_xarray()[variables]\n    da = _squeeze_dimensions(da, dimensions=optional_dims)\n    return da",
  "def hydrostatics_dataset(bodies: Sequence[FloatingBody]) -> xr.Dataset:\n    \"\"\"Create a dataset by looking for 'inertia_matrix' and 'hydrostatic_stiffness'\n    for each of the bodies in the list passed as argument.\n    \"\"\"\n    dataset = xr.Dataset()\n    for body_property in ['inertia_matrix', 'hydrostatic_stiffness']:\n        bodies_properties = {body.name: body.__getattribute__(body_property) for body in bodies if hasattr(body, body_property)}\n        if len(bodies_properties) > 0:\n            bodies_properties = xr.concat(bodies_properties.values(), pd.Index(bodies_properties.keys(), name='body_name'))\n            bodies_properties = _squeeze_dimensions(bodies_properties, dimensions=['body_name'])\n            dataset = xr.merge([dataset, {body_property: bodies_properties}])\n    return dataset",
  "def kochin_data_array(results: Sequence[LinearPotentialFlowResult],\n                      theta_range: Sequence[float],\n                      **kwargs,\n                      ) -> xr.Dataset:\n    \"\"\"Compute the Kochin function for a list of results and fills a dataset.\n\n    .. seealso::\n        :meth:`~capytaine.post_pro.kochin.compute_kochin`\n            The present function is just a wrapper around :code:`compute_kochin`.\n    \"\"\"\n    records = pd.DataFrame([\n        dict(**result.problem._asdict(), theta=theta, kochin=kochin)\n        for result in results\n        for theta, kochin in zip(theta_range.data,\n                                 compute_kochin(result, theta_range, **kwargs))\n    ])\n\n    kochin_data = {}\n\n    if 'wave_direction' in records.columns:\n        diffraction = _dataset_from_dataframe(\n            records[~records['wave_direction'].isnull()],\n            ['kochin'],\n            dimensions=['omega', 'wave_direction', 'theta'],\n            optional_dims=['g', 'rho', 'body_name', 'water_depth']\n        )\n        kochin_data['kochin_diffraction'] = diffraction['kochin']\n\n    if 'radiating_dof' in records.columns:\n        radiation = _dataset_from_dataframe(\n            records[~records['radiating_dof'].isnull()],\n            variables=['kochin'],\n            dimensions=['omega', 'radiating_dof', 'theta'],\n            optional_dims=['g', 'rho', 'body_name', 'water_depth']\n        )\n        kochin_data['kochin'] = radiation['kochin']\n\n    return kochin_data",
  "def collect_records(results):\n    records_list = []\n    warned_once_about_no_free_surface = False\n    for result in results:\n        if result.free_surface == np.infty:\n            if not warned_once_about_no_free_surface:\n                LOG.warning(\"Datasets currently only support cases with a free surface (free_surface=0.0).\\n\"\n                            \"Cases without a free surface (free_surface=infty) are ignored.\\n\"\n                            \"See also https://github.com/mancellin/capytaine/issues/88\")\n                warned_once_about_no_free_surface = True\n            else:\n                pass\n        else:\n            for record in result.records:\n                records_list.append(record)\n    return records_list",
  "def assemble_dataset(results,\n                     omega=True, wavenumber=True, wavelength=True, period=True,\n                     mesh=False, hydrostatics=True, attrs=None) -> xr.Dataset:\n    \"\"\"Transform a list of :class:`LinearPotentialFlowResult` into a :class:`xarray.Dataset`.\n\n    .. todo:: The :code:`mesh` option to store information on the mesh could be improved.\n              It could store the full mesh in the dataset to ensure the reproducibility of\n              the results.\n\n    Parameters\n    ----------\n    results: list of LinearPotentialFlowResult\n        The results that will be read.\n    omega: bool, optional\n        If True, the coordinate 'omega' will be added to the output dataset.\n    wavenumber: bool, optional\n        If True, the coordinate 'wavenumber' will be added to the output dataset.\n    wavelength: bool, optional\n        If True, the coordinate 'wavelength' will be added to the output dataset.\n    period: bool, optional\n        If True, the coordinate 'period' will be added to the output dataset.\n    mesh: bool, optional\n        If True, store some infos on the mesh in the output dataset.\n    hydrostatics: bool, optional\n        If True, store the hydrostatic data in the output dataset if they exist.\n    attrs: dict, optional\n        Attributes that should be added to the output dataset.\n    \"\"\"\n    dataset = xr.Dataset()\n\n    error_msg = 'The first argument of `assemble_dataset` must be either a list of LinearPotentialFlowResult or a bemio.io object'\n    if hasattr(results, '__iter__'):\n        try:\n            if 'capytaine' in results[0].__module__:\n                bemio_import = False\n            else:\n                raise TypeError(error_msg)\n        except:\n            raise TypeError(error_msg)\n\n    else:\n        try:\n            if 'bemio.io' in results.__module__:\n                bemio_import = True\n            else:\n                raise TypeError(error_msg)\n        except:\n            raise TypeError(error_msg)\n\n    if bemio_import:\n        records = dataframe_from_bemio(results, wavenumber, wavelength) # TODO add hydrostatics\n        all_dofs_in_order = {'Surge': None, 'Sway': None, 'Heave': None, 'Roll': None, 'Pitch': None, 'Yaw': None}\n        main_freq_type = \"omega\"\n\n    else:\n        records = pd.DataFrame(collect_records(results))\n        all_dofs_in_order = {k: None for r in results for k in r.body.dofs.keys()}\n        main_freq_type = Counter((res.provided_freq_type for res in results)).most_common(1)[0][0]\n\n    if attrs is None:\n        attrs = {}\n    attrs['creation_of_dataset'] = datetime.now().isoformat()\n    if len(records) == 0:\n        raise ValueError(\"No result passed to assemble_dataset.\")\n\n    inf_dof_cat = pd.CategoricalDtype(categories=all_dofs_in_order.keys())\n    records[\"influenced_dof\"] = records[\"influenced_dof\"].astype(inf_dof_cat)\n    rad_dof_cat = pd.CategoricalDtype(categories=all_dofs_in_order.keys())\n    if 'added_mass' in records.columns:\n        records[\"radiating_dof\"] = records[\"radiating_dof\"].astype(rad_dof_cat)\n\n    optional_dims = ['g', 'rho', 'body_name', 'water_depth']\n\n    # RADIATION RESULTS\n    if 'added_mass' in records.columns:\n        radiation_cases = _dataset_from_dataframe(\n            records,\n            variables=['added_mass', 'radiation_damping'],\n            dimensions=[main_freq_type, 'radiating_dof', 'influenced_dof'],\n            optional_dims=optional_dims)\n        radiation_cases.added_mass.attrs['long_name'] = 'Added mass'\n        radiation_cases.radiation_damping.attrs['long_name'] = 'Radiation damping'\n        radiation_cases.radiating_dof.attrs['long_name'] = 'Radiating DOF'\n        radiation_cases.influenced_dof.attrs['long_name'] = 'Influenced DOF'\n        dataset = xr.merge([dataset, radiation_cases])\n\n    # DIFFRACTION RESULTS\n    if 'diffraction_force' in records.columns:\n        diffraction_cases = _dataset_from_dataframe(\n            records,\n            variables=['diffraction_force', 'Froude_Krylov_force'],\n            dimensions=[main_freq_type, 'wave_direction', 'influenced_dof'],\n            optional_dims=optional_dims)\n        diffraction_cases.diffraction_force.attrs['long_name'] = 'Diffraction force'\n        diffraction_cases.Froude_Krylov_force.attrs['long_name'] = 'Froude Krylov force'\n        diffraction_cases.influenced_dof.attrs['long_name'] = 'Influenced DOF'\n        diffraction_cases.wave_direction.attrs['long_name'] = 'Wave direction'\n        diffraction_cases.wave_direction.attrs['units'] = 'rad'\n        dataset = xr.merge([dataset, diffraction_cases])\n\n    # OTHER FREQUENCIES TYPES\n    if omega and main_freq_type != \"omega\":\n        omega_ds = _dataset_from_dataframe(\n                records,\n                variables=['omega'],\n                dimensions=[main_freq_type],\n                optional_dims=['g', 'water_depth'] if main_freq_type in {'wavelength', 'wavenumber'} else []\n                )\n        dataset.coords['omega'] = omega_ds['omega']\n        dataset.omega.attrs['long_name'] = 'Angular frequency'\n        dataset.omega.attrs['units'] = 'rad/s'\n\n    if period and main_freq_type != \"period\":\n        period_ds = _dataset_from_dataframe(\n                records,\n                variables=['period'],\n                dimensions=[main_freq_type],\n                optional_dims=['g', 'water_depth'] if main_freq_type in {'wavelength', 'wavenumber'} else []\n                )\n        dataset.coords['period'] = period_ds['period']\n        dataset.period.attrs['long_name'] = 'Period'\n        dataset.period.attrs['units'] = 's'\n\n    if wavenumber and main_freq_type != \"wavenumber\":\n        wavenumber_ds = _dataset_from_dataframe(\n                records,\n                variables=['wavenumber'],\n                dimensions=[main_freq_type],\n                optional_dims=['g', 'water_depth'] if main_freq_type in {'period', 'omega'} else []\n                )\n        dataset.coords['wavenumber'] = wavenumber_ds['wavenumber']\n        dataset.wavenumber.attrs['long_name'] = 'Angular wavenumber'\n        dataset.wavenumber.attrs['units'] = 'rad/m'\n\n    if wavelength and main_freq_type != \"wavelength\":\n        wavelength_ds = _dataset_from_dataframe(\n                records,\n                variables=['wavelength'],\n                dimensions=[main_freq_type],\n                optional_dims=['g', 'water_depth'] if main_freq_type in {'period', 'omega'} else []\n                )\n        dataset.coords['wavelength'] = wavelength_ds['wavelength']\n        dataset.wavelength.attrs['long_name'] = 'Wave length'\n        dataset.wavelength.attrs['units'] = 'm'\n\n    if mesh:\n        if bemio_import:\n            LOG.warning('Bemio data does not include mesh data. mesh=True is ignored.')\n        else:\n            # TODO: Store full mesh...\n            bodies = list({result.body for result in results})  # Filter out duplicate bodies in the list of results\n            nb_faces = {body.name: body.mesh.nb_faces for body in bodies}\n\n            def name_or_str(c):\n                return c.name if hasattr(c, 'name') else str(c)\n            quad_methods = {body.name: name_or_str(body.mesh.quadrature_method) for body in bodies}\n\n            if len(nb_faces) > 1:\n                dataset.coords['nb_faces'] = ('body_name', [nb_faces[name] for name in dataset.coords['body_name'].data])\n                dataset.coords['quadrature_method'] = ('body_name', [quad_methods[name] for name in dataset.coords['body_name'].data])\n            else:\n                def the_only(d):\n                    \"\"\"Return the only element of a 1-element dictionary\"\"\"\n                    return next(iter(d.values()))\n                dataset.coords['nb_faces'] = the_only(nb_faces)\n                dataset.coords['quadrature_method'] = the_only(quad_methods)\n\n    # HYDROSTATICS\n    if hydrostatics:\n        if bemio_import:\n            LOG.warning('Bemio data import being used, hydrostatics=True is ignored.')\n        else:\n            bodies = list({result.body for result in results})\n            dataset = xr.merge([dataset, hydrostatics_dataset(bodies)])\n\n    dataset.attrs.update(attrs)\n    dataset.attrs['capytaine_version'] = __version__\n    return dataset",
  "def separate_complex_values(ds: xr.Dataset) -> xr.Dataset:\n    \"\"\"Return a new Dataset where complex-valued arrays of shape (...)\n    have been replaced by real-valued arrays of shape (2, ...).\n\n    .. seealso::\n        :func:`merge_complex_values`\n            The invert operation\n    \"\"\"\n    ds = ds.copy()\n    for variable in ds.data_vars:\n        if ds[variable].dtype == complex:\n            da = ds[variable]\n            new_da = xr.DataArray(np.asarray((np.real(da).data, np.imag(da).data)),\n                                  dims=('complex',) + da.dims)\n            ds[variable] = new_da\n            ds.coords['complex'] = ['re', 'im']\n    return ds",
  "def merge_complex_values(ds: xr.Dataset) -> xr.Dataset:\n    \"\"\"Return a new Dataset where real-valued arrays of shape (2, ...)\n    have been replaced by complex-valued arrays of shape (...).\n\n    .. seealso::\n        :func:`separate_complex_values`\n            The invert operation\n    \"\"\"\n    if 'complex' in ds.coords:\n        ds = ds.copy()\n        for variable in ds.data_vars:\n            if 'complex' in ds[variable].coords:\n                da = ds[variable]\n                new_dims = [d for d in da.dims if d != 'complex']\n                new_da = xr.DataArray(da.sel(complex='re').data + 1j*da.sel(complex='im').data, dims=new_dims)\n                ds[variable] = new_da\n        ds = ds.drop_vars('complex')\n    return ds",
  "def name_or_str(c):\n                return c.name if hasattr(c, 'name') else str(c)",
  "def the_only(d):\n                    \"\"\"Return the only element of a 1-element dictionary\"\"\"\n                    return next(iter(d.values()))",
  "def dataframe_from_bemio(bemio_obj, wavenumber, wavelength):\n    \"\"\"Transform a :class:`bemio.data_structures.bem.HydrodynamicData` into a\n        :class:`pandas.DataFrame`.\n\n        Parameters\n        ----------\n        bemio_obj: Bemio data_stuctures.bem.HydrodynamicData class\n            Loaded NEMOH, AQWA, or WAMIT data created using `bemio.io.nemoh.read`,\n            `bemio.io.aqwa.read`, or `bemio.io.wamit.read` functions, respectively.\n        wavenumber: bool\n            If True, the coordinate 'wavenumber' will be added to the output dataset.\n        wavelength: bool\n            If True, the coordinate 'wavelength' will be added to the output dataset.\n        \"\"\"\n\n\n    dofs = np.array(['Surge', 'Sway', 'Heave', 'Roll', 'Pitch', 'Yaw'])\n    for i in range(bemio_obj.body[0].num_bodies):\n        difr_dict = []\n        rad_dict = []\n\n        rho = bemio_obj.body[0].rho\n        g = bemio_obj.body[0].g\n\n        if bemio_obj.body[i].water_depth == 'infinite':\n            bemio_obj.body[i].water_depth = np.infty\n\n        if bemio_obj.body[i].bem_code == 'WAMIT': # WAMIT coefficients need to be dimensionalized\n            from_wamit = True\n\n        for omega_idx, omega in enumerate(np.sort(bemio_obj.body[i].w)):\n\n            # DiffractionProblem variable equivalents\n            for dir_idx, dir in enumerate(bemio_obj.body[i].wave_dir):\n                temp_dict = {}\n                temp_dict['body_name'] = bemio_obj.body[i].name\n                temp_dict['water_depth'] = bemio_obj.body[i].water_depth\n                temp_dict['omega'] = omega\n                temp_dict['period'] = 2*np.pi/omega\n                temp_dict['rho'] = rho\n                temp_dict['g'] = g\n                temp_dict['wave_direction'] = np.radians(dir)\n                temp_dict['influenced_dof'] = dofs\n                \n                if wavenumber or wavelength:\n                    if temp_dict['water_depth'] == np.infty or omega**2*temp_dict['water_depth']/temp_dict['g'] > 20:\n                        k = omega**2/temp_dict['g']\n                    else:\n                        k = newton(lambda x: x*np.tanh(x) - omega**2*temp_dict['water_depth']/temp_dict['g'], x0=1.0)/temp_dict['water_depth']\n                    \n                    if wavenumber:\n                        temp_dict['wavenumber'] = k\n\n                    if wavelength:\n                        if k == 0.0:\n                            temp_dict['wavelength'] = np.infty\n                        else:\n                            temp_dict['wavelength'] = 2*np.pi/k\n\n                Fexc = np.empty(shape=bemio_obj.body[i].ex.re[:, dir_idx, omega_idx].shape, dtype=np.complex128)\n                if from_wamit:\n                    Fexc.real = bemio_obj.body[i].ex.re[:, dir_idx, omega_idx] * rho * g\n                    Fexc.imag = bemio_obj.body[i].ex.im[:, dir_idx, omega_idx] * rho * g\n                else:\n                    Fexc.real = bemio_obj.body[i].ex.re[:, dir_idx, omega_idx]\n                    Fexc.imag = bemio_obj.body[i].ex.im[:, dir_idx, omega_idx]\n                temp_dict['diffraction_force'] = Fexc.flatten()\n            \n                try:\n                    Fexc_fk = np.empty(shape=bemio_obj.body[i].ex.fk.re[:, dir_idx, omega_idx].shape, dtype=np.complex128)\n                    if from_wamit:\n                        Fexc_fk.real = bemio_obj.body[i].ex.fk.re[:, dir_idx, omega_idx] * rho * g\n                        Fexc_fk.imag = bemio_obj.body[i].ex.fk.im[:, dir_idx, omega_idx] * rho * g\n                    else:\n                        Fexc_fk.real = bemio_obj.body[i].ex.fk.re[:, dir_idx, omega_idx]\n                        Fexc_fk.imag = bemio_obj.body[i].ex.fk.im[:, dir_idx, omega_idx]\n                    temp_dict['Froude_Krylov_force'] = Fexc_fk.flatten()\n\n                except AttributeError:\n                        # LOG.warning('\\tNo Froude-Krylov forces found for ' + bemio_obj.body[i].name + ' at ' + str(dir) + \\\n                        #       ' degrees (omega = ' + str(omega) + '), replacing with zeros.')\n                        temp_dict['Froude_Krylov_force'] = np.zeros((bemio_obj.body[i].ex.re[:, dir_idx, omega_idx].size,), dtype=np.complex128)\n\n                difr_dict.append(temp_dict)\n\n            # RadiationProblem + Hydrostatics variable equivalents\n            for radiating_dof_idx, radiating_dof in enumerate(dofs):\n                temp_dict = {}\n                temp_dict['body_name'] = bemio_obj.body[i].name\n                temp_dict['water_depth'] = bemio_obj.body[i].water_depth\n                temp_dict['omega'] = omega\n                temp_dict['rho'] = rho\n                temp_dict['g'] = g\n                temp_dict['influenced_dof'] = dofs\n                temp_dict['radiating_dof'] = radiating_dof\n                temp_dict['added_mass'] = bemio_obj.body[i].am.all[radiating_dof_idx, :, omega_idx].flatten()\n                temp_dict['radiation_damping'] = bemio_obj.body[i].rd.all[radiating_dof_idx, :, omega_idx].flatten()\n\n                if from_wamit:\n                    temp_dict['added_mass'] = temp_dict['added_mass'] * rho\n                    temp_dict['radiation_damping'] = temp_dict['radiation_damping'] * rho * omega\n\n                if wavenumber or wavelength:\n                    if temp_dict['water_depth'] == np.infty or omega**2*temp_dict['water_depth']/temp_dict['g'] > 20:\n                        k = omega**2/temp_dict['g']\n                    else:\n                        k = newton(lambda x: x*np.tanh(x) - omega**2*temp_dict['water_depth']/temp_dict['g'], x0=1.0)/temp_dict['water_depth']\n                    \n                    if wavenumber:\n                        temp_dict['wavenumber'] = k\n\n                    if wavelength:\n                        if k == 0.0:\n                            temp_dict['wavelength'] = np.infty\n                        else:\n                            temp_dict['wavelength'] = 2*np.pi/k\n\n                rad_dict.append(temp_dict)\n\n    df = pd.concat([\n        pd.DataFrame.from_dict(difr_dict).explode(['influenced_dof', 'diffraction_force', 'Froude_Krylov_force']),\n        pd.DataFrame.from_dict(rad_dict).explode(['influenced_dof', 'added_mass', 'radiation_damping'])\n        ])\n    df = df.astype({'added_mass': np.float64, 'radiation_damping': np.float64, 'diffraction_force': np.complex128, 'Froude_Krylov_force': np.complex128})\n\n    return df",
  "def cut_matrix(full_matrix, x_shapes, y_shapes, check=False):\n    \"\"\"Transform a numpy array into a block matrix of numpy arrays.\n\n    Parameters\n    ----------\n    full_matrix: numpy array\n        The matrix to split into blocks.\n    x_shapes: sequence of int\n        The columns at which to split the blocks.\n    y_shapes: sequence of int\n        The lines at which to split the blocks.\n    check: bool, optional\n        Check to dimensions and type of the matrix after creation (default: False).\n\n    Return\n    ------\n    BlockMatrix\n        The same matrix as the input one but in block form.\n    \"\"\"\n    new_block_matrix = []\n    for i, di in zip(accumulate([0] + x_shapes[:-1]), x_shapes):\n        line = []\n        for j, dj in zip(accumulate([0] + x_shapes[:-1]), y_shapes):\n            line.append(full_matrix[i:i+di, j:j+dj])\n        new_block_matrix.append(line)\n    return BlockMatrix(new_block_matrix, check=check)",
  "def random_block_matrix(x_shapes, y_shapes):\n    \"\"\"A random block matrix.\"\"\"\n    return cut_matrix(np.random.rand(sum(x_shapes), sum(y_shapes)), x_shapes, y_shapes)",
  "def full_like(A, value, dtype=np.float64):\n    \"\"\"A matrix of the same kind and shape as A but filled with a single value.\"\"\"\n    if isinstance(A, BlockMatrix):\n        new_matrix = []\n        for i in range(A._stored_nb_blocks[0]):\n            line = []\n            for j in range(A._stored_nb_blocks[1]):\n                line.append(full_like(A._stored_blocks[i, j], value, dtype=dtype))\n            new_matrix.append(line)\n        return A.__class__(new_matrix)\n    elif isinstance(A, LowRankMatrix):\n        return LowRankMatrix(np.ones((A.shape[0], 1)), np.full((1, A.shape[1]), value))\n    elif isinstance(A, np.ndarray):\n        return np.full_like(A, value, dtype=dtype)",
  "def zeros_like(A, dtype=np.float64):\n    \"\"\"A matrix of the same kind and shape as A but filled with zeros.\"\"\"\n    return full_like(A, 0.0, dtype=dtype)",
  "def ones_like(A, dtype=np.float64):\n    \"\"\"A matrix of the same kind and shape as A but filled with ones.\"\"\"\n    return full_like(A, 1.0, dtype=dtype)",
  "def identity_like(A, dtype=np.float64):\n    \"\"\"A identity matrix of the same kind and shape as A.\"\"\"\n    if isinstance(A, BlockMatrix):\n        I = []\n        for i in range(A._stored_nb_blocks[0]):\n            line = []\n            for j in range(A._stored_nb_blocks[1]):\n                if i == j:\n                    line.append(identity_like(A._stored_blocks[i, j], dtype=dtype))\n                else:\n                    line.append(zeros_like(A._stored_blocks[i, j], dtype=dtype))\n            I.append(line)\n        return A.__class__(I)\n    elif isinstance(A, np.ndarray):\n        return np.eye(A.shape[0], A.shape[1], dtype=dtype)",
  "class NoConvergenceOfACA(Exception):\n    pass",
  "class LowRankMatrix:\n    \"\"\"Matrix defined as the tensor product of two small matrices.\n\n    Parameters\n    ----------\n    left_matrix: numpy.array\n        Matrix of shape (nb_cols, rank).\n    right_matrix: numpy.array\n        Matrix of shape (rank, nb_rows).\n\n    Attributes\n    ----------\n    shape: Tuple[int, int]\n        The shape of the full matrix.\n    rank: int\n        The rank of the full matrix.\n    dtype: numpy.dtype\n        Type of data in the matrix.\n    \"\"\"\n\n    ndim = 2\n\n    ##############\n    #  Creation  #\n    ##############\n\n    def __init__(self, left_matrix, right_matrix):\n        self.left_matrix = left_matrix\n        self.right_matrix = right_matrix\n        self.shape = left_matrix.shape[0], right_matrix.shape[1]\n        assert left_matrix.shape[1] == right_matrix.shape[0], \"Sizes of the left and right matrices do not match.\"\n        self.rank = left_matrix.shape[1]  # == right_matrix.shape[0]\n        assert left_matrix.dtype == right_matrix.dtype, \"Left and right matrices should have the same type of data.\"\n        self.dtype = left_matrix.dtype\n\n    @classmethod\n    def from_full_matrix_with_SVD(cls, full_matrix, max_rank):\n        \"\"\"Create a low rank matrix from a full matrix using Singular Value Decomposition.\n\n        Parameters\n        ----------\n        full_matrix: numpy array\n            The matrix that will be approximated.\n        max_rank: int\n            Rank of the low-rank approximation.\n\n        Returns\n        -------\n        LowRankMatrix\n        \"\"\"\n        u, s, v = np.linalg.svd(full_matrix)\n        left_matrix = u[:, 0:max_rank] @ np.diag(s[0:max_rank])\n        right_matrix = v[0:max_rank, :]\n        return cls(left_matrix, right_matrix)\n\n    @classmethod\n    def from_full_matrix_with_ACA(cls, full_matrix, max_rank=None, tol=0.0):\n        \"\"\"Create a low rank matrix from a full matrix using Adaptive Cross Approximation.\n        The user should provide either the `max_rank` optional argument or the `tol` optional argument.\n\n        Parameters\n        ----------\n        full_matrix: numpy.array\n            The matrix that will be approximated.\n        max_rank: int, optional\n            The maximum rank allowed for the output low rank matrix.\n            The default value is half the size of the full matrix, that is no gain in storage space.\n        tol: float, optional\n            The tolerance on the relative error (default: 0).\n            If the Frobenius norm of the increment is lower than the tolerance, the iteration stops.\n            If the tolerance is set to 0, the resulting matrix will have the maximum rank defined by `max_rank`.\n\n        Returns\n        -------\n        LowRankMatrix\n        \"\"\"\n        def get_row(i):\n            return full_matrix[i, :]\n\n        def get_col(j):\n            return full_matrix[:, j]\n\n        return cls.from_rows_and_cols_functions_with_ACA(\n            get_row, get_col, full_matrix.shape[0], full_matrix.shape[1],\n            max_rank=max_rank, tol=tol, dtype=full_matrix.dtype\n        )\n\n    @classmethod\n    def from_function_with_ACA(cls, func, nb_rows, nb_cols, max_rank=None, tol=0.0, dtype=np.float64):\n        \"\"\"Create a low rank matrix from a function using Adaptive Cross Approximation.\n        The user should provide either the `max_rank` optional argument or the `tol` optional argument.\n\n        Parameters\n        ----------\n        func: Function\n            Function such that `func(i, j)` returns the value of the (i, j) entry of the full matrix.\n        nb_rows: int\n            Number of rows in the full matrix.\n        nb_cols: int\n            Number of cols in the full matrix.\n        max_rank: int, optional\n            The maximum rank allowed for the output low rank matrix.\n            The default value is half the size of the full matrix, that is no gain in storage space.\n        tol: float, optional\n            The tolerance on the relative error (default: 0).\n            If the Frobenius norm of the increment is lower than the tolerance, the iteration stops.\n            If the tolerance is set to 0, the resulting matrix will have the maximum rank defined by `max_rank`.\n        dtype: numpy.dtype, optional\n            Type of the data returned by the function.\n\n        Returns\n        -------\n        LowRankMatrix\n        \"\"\"\n        def get_row(i):\n            return np.asarray([func(i, j) for j in range(nb_cols)])\n\n        def get_col(j):\n            return np.asarray([func(i, j) for i in range(nb_rows)])\n\n        return cls.from_rows_and_cols_functions_with_ACA(\n            get_row, get_col, nb_rows, nb_cols,\n            max_rank=max_rank, tol=tol, dtype=dtype\n        )\n\n    @classmethod\n    def from_rows_and_cols_functions_with_ACA(cls, get_row_func, get_col_func, nb_rows, nb_cols, max_rank=None, tol=0.0, dtype=np.float64):\n        \"\"\"Create a low rank matrix from functions using Adaptive Cross Approximation.\n        The user should provide either the `max_rank` optional argument or the `tol` optional argument.\n\n        Parameters\n        ----------\n        get_row_func: Function\n            Function such that `get_row_func(i)` returns the `i`-th row of the full matrix.\n        get_col_func: Function\n            Function such that `get_col_func(j)` returns the `j`-th column of the full matrix.\n        nb_rows: int\n            Number of rows in the full matrix.\n        nb_cols: int\n            Number of columns in the full matrix.\n        max_rank: int, optional\n            The maximum rank allowed for the output low rank matrix.\n            The default value is half the size of the full matrix, that is no gain in storage space.\n        tol: float, optional\n            The tolerance on the relative error (default: 0).\n            If the Frobenius norm of the increment is lower than the tolerance, the iteration stops.\n            If the tolerance is set to 0, the resulting matrix will have the maximum rank defined by `max_rank`.\n        dtype: numpy.dtype, optional\n            The type of data in the low rank matrix (default: float64).\n\n        Returns\n        -------\n        LowRankMatrix\n        \"\"\"\n        # Just some wrapping and unwrapping to use the multi-ACA below.\n        def get_row(i):\n            return [get_row_func(i)]\n\n        def get_col(j):\n            return [get_col_func(j)]\n\n        return cls.from_rows_and_cols_functions_with_multi_ACA(\n            get_row, get_col, nb_rows, nb_cols,\n            nb_matrices=1, id_main=0,\n            max_rank=max_rank, tol=tol, dtype=dtype\n        )[0]\n\n    @classmethod\n    def from_rows_and_cols_functions_with_multi_ACA(cls, get_row, get_col, nb_rows, nb_cols,\n                                                    nb_matrices=1, id_main=0,\n                                                    max_rank=None, tol=0.0, dtype=np.float64):\n        \"\"\"Create several low rank matrices while running an Adaptive Cross Approximation.\n        The user should provide either the `max_rank` optional argument or the `tol` optional argument.\n\n        In Capytaine, the routines evaluating the influence matrices return the values of two matrices\n        (S and V) at once, because there is a lot of common computations in their evaluation.\n        The present function can be used to build the ACA of one of them while getting at the same time\n        an approximation of the other for free.\n\n        Freely adapted from the routine hmxACA.m from Gypsilab.\n\n        Parameters\n        ----------\n        get_row: Function\n            Function such that `get_row(i)` returns the `i`-th row of all the full matrices.\n        get_col: Function\n            Function such that `get_col(j)` returns the `j`-th column of all the full matrices.\n        nb_rows: int\n            Number of rows in all full matrices.\n        nb_cols: int\n            Number of columns in all full matrices.\n        nb_matrices: int, optional\n            The number of matrices approximated at the same time.\n        id_main: int, optional\n            The matrix used primarily in the ACA.\n        max_rank: int, optional\n            The maximum rank allowed for both output low rank matrices.\n            The default value is half the size of the full matrix, that is no gain in storage space.\n        tol: float, optional\n            The tolerance on the relative error (default: 0).\n            If the Frobenius norm of the increment is lower than the tolerance, the iteration stops.\n            If the tolerance is set to 0, the resulting matrix will have the maximum rank defined by `max_rank`.\n        dtype: numpy.dtype, optional\n            The type of data in both low rank matrices (default: float64).\n\n        Returns\n        -------\n        List[LowRankMatrix]\n        \"\"\"\n        if max_rank is None and tol <= 0.0:\n            LOG.warning(\"No stopping criterion for the Adaptive Cross Approximation.\"\n                        \"Please provide either max_rank or tol.\")\n\n        if max_rank is None:\n            max_rank = min(nb_rows, nb_cols)//2\n\n        # Initialize work matrices\n        left = np.zeros((nb_matrices, nb_rows, max_rank), dtype=dtype)\n        right = np.zeros((nb_matrices, max_rank, nb_cols), dtype=dtype)\n\n        squared_norm_of_low_rank_approximation = 0.0\n\n        # List of indices of unused entries in the full matrix\n        available_rows = list(range(nb_rows))\n        available_cols = list(range(nb_cols))\n\n        for l in range(max_rank):\n            # Pick a row\n            if l == 0:\n                relative_i = 0\n                # Could also have been chosen at random.\n            else:\n                relative_i = int(np.argmax(np.abs(left[id_main, available_rows, l-1])))\n                # The \"int\" is useless except for my type checker...\n\n            i = available_rows.pop(relative_i)\n            # relative_i is the index of the row in the list of remaining rows,\n            # e.g. if available_rows = [2, 7, 8] and relative_i = 2, the chosen\n            # row has index i = 8 in the full matrix.\n\n            # Add the chosen row to the approximation of all the matrices\n            one_row = get_row(i)\n            for id_mat in range(nb_matrices):\n                right[id_mat, l, :] = one_row[id_mat] - left[id_mat, i, :l] @ right[id_mat, :l, :]\n\n            # Pick a column\n            relative_j = int(np.argmax(np.abs(right[id_main, l, available_cols])))\n            j = available_cols.pop(relative_j)\n            # Similar to i above.\n\n            one_col = get_col(j)\n\n            # Add the column to the approximations of all matrices.\n            for id_mat in range(nb_matrices):\n                new_col = one_col[id_mat] - left[id_mat, :, :l] @ right[id_mat, :l, j]\n                pivot = new_col[i]\n                if abs(pivot) < 1e-12:\n                    pivot = 1e-12\n                left[id_mat, :, l] = new_col/pivot\n\n            # Update norm of the full matrix\n            squared_norm_of_increment = np.real(\n                    (np.conj(left[id_main, :, l]) @ left[id_main, :, l]) *\n                    (np.conj(right[id_main, l, :]) @ right[id_main, l, :])\n            )\n\n            crossed_terms = (\n                    (np.conj(left[id_main, :, l].T) @ left[id_main, :, :l]) @\n                    (np.conj(right[id_main, l, :]) @ right[id_main, :l, :].T)\n            )\n            squared_norm_of_low_rank_approximation += squared_norm_of_increment + 2*np.real(crossed_terms)\n\n            if squared_norm_of_increment <= tol**2*squared_norm_of_low_rank_approximation:\n                LOG.debug(f\"The ACA has found an approximation of rank {l}.\")\n\n                if l == 0:  # Edge case of the zero matrix, ...\n                    l = 1  # ... we actually return a \"rank 1\" LowRankMatrix with coefficients equal to zero.\n\n                return [LowRankMatrix(left[id_mat, :, :l], right[id_mat, :l, :]) for id_mat in range(nb_matrices)]\n\n        if tol > 0:\n            LOG.warning(f\"The ACA was unable to find a low rank approximation \"\n                        f\"of rank lower or equal to {max_rank} with tolerance {tol:.2e} (latest iteration: \"\n                        f\"{np.sqrt(squared_norm_of_increment):.2e}/{np.sqrt(squared_norm_of_low_rank_approximation):.2e}).\")\n            raise NoConvergenceOfACA()\n\n        return [LowRankMatrix(left[id_mat, :, :], right[id_mat, :, :]) for id_mat in range(nb_matrices)]\n\n    ####################\n    #  Representation  #\n    ####################\n\n    def full_matrix(self, dtype=None):\n        if dtype is not None:\n            return (self.left_matrix @ self.right_matrix).astype(dtype)\n        else:\n            return self.left_matrix @ self.right_matrix\n\n    def __array__(self, dtype=None):\n        return self.full_matrix(dtype=dtype)\n\n    @property\n    def stored_data_size(self):\n        return np.prod(self.left_matrix.shape) + np.prod(self.right_matrix.shape)\n\n    @property\n    def density(self):\n        return self.stored_data_size/np.prod(self.shape)\n\n    @property\n    def sparcity(self):\n        return 1 - self.density\n\n    ####################\n    #  Transformation  #\n    ####################\n\n    def recompress(self, tol=None, new_rank=None):\n        \"\"\"Recompress the matrix to a lower rank. Based on the routine hmxQRSVD.m from Gipsylab.\"\"\"\n        if new_rank is None:\n            new_rank = self.rank\n        QA, RA = np.linalg.qr(self.left_matrix)\n        QB, RB = np.linalg.qr(self.right_matrix.T)\n        U, S, V = np.linalg.svd(RA @ RB.T)\n        if tol is not None:\n            new_rank = np.count_nonzero(S/S[0] >= tol)\n        A = QA @ (U[:, :new_rank] @ np.diag(S[:new_rank]))\n        B = QB @ V[:, :new_rank]\n        return LowRankMatrix(A, B.T)\n\n    def __add__(self, other):\n        if isinstance(other, LowRankMatrix):\n            new_left = np.concatenate([self.left_matrix, other.left_matrix], axis=1)\n            new_right = np.concatenate([self.right_matrix, other.right_matrix], axis=0)\n            return LowRankMatrix(new_left, new_right).recompress(new_rank=min(self.rank, other.rank))\n        else:\n            return NotImplemented\n\n    def __neg__(self):\n        return LowRankMatrix(-self.left_matrix, self.right_matrix)\n\n    def __sub__(self, other):\n        return self + (-other)\n\n    def __truediv__(self, other):\n        from numbers import Number\n        if isinstance(other, Number):\n            return LowRankMatrix(self.left_matrix, self.right_matrix/other)\n        else:\n            return NotImplemented\n\n    def __matmul__(self, other):\n        if isinstance(other, np.ndarray) and len(other.shape) == 1:\n            return self._mul_with_vector(other)\n        else:\n            return NotImplemented\n\n    def _mul_with_vector(self, other):\n        return self.left_matrix @ (self.right_matrix @ other)\n\n    def __rmatmul__(self, other):\n        if isinstance(other, np.ndarray) and len(other.shape) == 1:\n            return self._rmul_with_vector(other)\n        else:\n            return NotImplemented\n\n    def _rmul_with_vector(self, other):\n        return (other @ self.left_matrix) @ self.right_matrix\n\n    def astype(self, dtype):\n        return LowRankMatrix(self.left_matrix.astype(dtype), self.right_matrix.astype(dtype))",
  "def __init__(self, left_matrix, right_matrix):\n        self.left_matrix = left_matrix\n        self.right_matrix = right_matrix\n        self.shape = left_matrix.shape[0], right_matrix.shape[1]\n        assert left_matrix.shape[1] == right_matrix.shape[0], \"Sizes of the left and right matrices do not match.\"\n        self.rank = left_matrix.shape[1]  # == right_matrix.shape[0]\n        assert left_matrix.dtype == right_matrix.dtype, \"Left and right matrices should have the same type of data.\"\n        self.dtype = left_matrix.dtype",
  "def from_full_matrix_with_SVD(cls, full_matrix, max_rank):\n        \"\"\"Create a low rank matrix from a full matrix using Singular Value Decomposition.\n\n        Parameters\n        ----------\n        full_matrix: numpy array\n            The matrix that will be approximated.\n        max_rank: int\n            Rank of the low-rank approximation.\n\n        Returns\n        -------\n        LowRankMatrix\n        \"\"\"\n        u, s, v = np.linalg.svd(full_matrix)\n        left_matrix = u[:, 0:max_rank] @ np.diag(s[0:max_rank])\n        right_matrix = v[0:max_rank, :]\n        return cls(left_matrix, right_matrix)",
  "def from_full_matrix_with_ACA(cls, full_matrix, max_rank=None, tol=0.0):\n        \"\"\"Create a low rank matrix from a full matrix using Adaptive Cross Approximation.\n        The user should provide either the `max_rank` optional argument or the `tol` optional argument.\n\n        Parameters\n        ----------\n        full_matrix: numpy.array\n            The matrix that will be approximated.\n        max_rank: int, optional\n            The maximum rank allowed for the output low rank matrix.\n            The default value is half the size of the full matrix, that is no gain in storage space.\n        tol: float, optional\n            The tolerance on the relative error (default: 0).\n            If the Frobenius norm of the increment is lower than the tolerance, the iteration stops.\n            If the tolerance is set to 0, the resulting matrix will have the maximum rank defined by `max_rank`.\n\n        Returns\n        -------\n        LowRankMatrix\n        \"\"\"\n        def get_row(i):\n            return full_matrix[i, :]\n\n        def get_col(j):\n            return full_matrix[:, j]\n\n        return cls.from_rows_and_cols_functions_with_ACA(\n            get_row, get_col, full_matrix.shape[0], full_matrix.shape[1],\n            max_rank=max_rank, tol=tol, dtype=full_matrix.dtype\n        )",
  "def from_function_with_ACA(cls, func, nb_rows, nb_cols, max_rank=None, tol=0.0, dtype=np.float64):\n        \"\"\"Create a low rank matrix from a function using Adaptive Cross Approximation.\n        The user should provide either the `max_rank` optional argument or the `tol` optional argument.\n\n        Parameters\n        ----------\n        func: Function\n            Function such that `func(i, j)` returns the value of the (i, j) entry of the full matrix.\n        nb_rows: int\n            Number of rows in the full matrix.\n        nb_cols: int\n            Number of cols in the full matrix.\n        max_rank: int, optional\n            The maximum rank allowed for the output low rank matrix.\n            The default value is half the size of the full matrix, that is no gain in storage space.\n        tol: float, optional\n            The tolerance on the relative error (default: 0).\n            If the Frobenius norm of the increment is lower than the tolerance, the iteration stops.\n            If the tolerance is set to 0, the resulting matrix will have the maximum rank defined by `max_rank`.\n        dtype: numpy.dtype, optional\n            Type of the data returned by the function.\n\n        Returns\n        -------\n        LowRankMatrix\n        \"\"\"\n        def get_row(i):\n            return np.asarray([func(i, j) for j in range(nb_cols)])\n\n        def get_col(j):\n            return np.asarray([func(i, j) for i in range(nb_rows)])\n\n        return cls.from_rows_and_cols_functions_with_ACA(\n            get_row, get_col, nb_rows, nb_cols,\n            max_rank=max_rank, tol=tol, dtype=dtype\n        )",
  "def from_rows_and_cols_functions_with_ACA(cls, get_row_func, get_col_func, nb_rows, nb_cols, max_rank=None, tol=0.0, dtype=np.float64):\n        \"\"\"Create a low rank matrix from functions using Adaptive Cross Approximation.\n        The user should provide either the `max_rank` optional argument or the `tol` optional argument.\n\n        Parameters\n        ----------\n        get_row_func: Function\n            Function such that `get_row_func(i)` returns the `i`-th row of the full matrix.\n        get_col_func: Function\n            Function such that `get_col_func(j)` returns the `j`-th column of the full matrix.\n        nb_rows: int\n            Number of rows in the full matrix.\n        nb_cols: int\n            Number of columns in the full matrix.\n        max_rank: int, optional\n            The maximum rank allowed for the output low rank matrix.\n            The default value is half the size of the full matrix, that is no gain in storage space.\n        tol: float, optional\n            The tolerance on the relative error (default: 0).\n            If the Frobenius norm of the increment is lower than the tolerance, the iteration stops.\n            If the tolerance is set to 0, the resulting matrix will have the maximum rank defined by `max_rank`.\n        dtype: numpy.dtype, optional\n            The type of data in the low rank matrix (default: float64).\n\n        Returns\n        -------\n        LowRankMatrix\n        \"\"\"\n        # Just some wrapping and unwrapping to use the multi-ACA below.\n        def get_row(i):\n            return [get_row_func(i)]\n\n        def get_col(j):\n            return [get_col_func(j)]\n\n        return cls.from_rows_and_cols_functions_with_multi_ACA(\n            get_row, get_col, nb_rows, nb_cols,\n            nb_matrices=1, id_main=0,\n            max_rank=max_rank, tol=tol, dtype=dtype\n        )[0]",
  "def from_rows_and_cols_functions_with_multi_ACA(cls, get_row, get_col, nb_rows, nb_cols,\n                                                    nb_matrices=1, id_main=0,\n                                                    max_rank=None, tol=0.0, dtype=np.float64):\n        \"\"\"Create several low rank matrices while running an Adaptive Cross Approximation.\n        The user should provide either the `max_rank` optional argument or the `tol` optional argument.\n\n        In Capytaine, the routines evaluating the influence matrices return the values of two matrices\n        (S and V) at once, because there is a lot of common computations in their evaluation.\n        The present function can be used to build the ACA of one of them while getting at the same time\n        an approximation of the other for free.\n\n        Freely adapted from the routine hmxACA.m from Gypsilab.\n\n        Parameters\n        ----------\n        get_row: Function\n            Function such that `get_row(i)` returns the `i`-th row of all the full matrices.\n        get_col: Function\n            Function such that `get_col(j)` returns the `j`-th column of all the full matrices.\n        nb_rows: int\n            Number of rows in all full matrices.\n        nb_cols: int\n            Number of columns in all full matrices.\n        nb_matrices: int, optional\n            The number of matrices approximated at the same time.\n        id_main: int, optional\n            The matrix used primarily in the ACA.\n        max_rank: int, optional\n            The maximum rank allowed for both output low rank matrices.\n            The default value is half the size of the full matrix, that is no gain in storage space.\n        tol: float, optional\n            The tolerance on the relative error (default: 0).\n            If the Frobenius norm of the increment is lower than the tolerance, the iteration stops.\n            If the tolerance is set to 0, the resulting matrix will have the maximum rank defined by `max_rank`.\n        dtype: numpy.dtype, optional\n            The type of data in both low rank matrices (default: float64).\n\n        Returns\n        -------\n        List[LowRankMatrix]\n        \"\"\"\n        if max_rank is None and tol <= 0.0:\n            LOG.warning(\"No stopping criterion for the Adaptive Cross Approximation.\"\n                        \"Please provide either max_rank or tol.\")\n\n        if max_rank is None:\n            max_rank = min(nb_rows, nb_cols)//2\n\n        # Initialize work matrices\n        left = np.zeros((nb_matrices, nb_rows, max_rank), dtype=dtype)\n        right = np.zeros((nb_matrices, max_rank, nb_cols), dtype=dtype)\n\n        squared_norm_of_low_rank_approximation = 0.0\n\n        # List of indices of unused entries in the full matrix\n        available_rows = list(range(nb_rows))\n        available_cols = list(range(nb_cols))\n\n        for l in range(max_rank):\n            # Pick a row\n            if l == 0:\n                relative_i = 0\n                # Could also have been chosen at random.\n            else:\n                relative_i = int(np.argmax(np.abs(left[id_main, available_rows, l-1])))\n                # The \"int\" is useless except for my type checker...\n\n            i = available_rows.pop(relative_i)\n            # relative_i is the index of the row in the list of remaining rows,\n            # e.g. if available_rows = [2, 7, 8] and relative_i = 2, the chosen\n            # row has index i = 8 in the full matrix.\n\n            # Add the chosen row to the approximation of all the matrices\n            one_row = get_row(i)\n            for id_mat in range(nb_matrices):\n                right[id_mat, l, :] = one_row[id_mat] - left[id_mat, i, :l] @ right[id_mat, :l, :]\n\n            # Pick a column\n            relative_j = int(np.argmax(np.abs(right[id_main, l, available_cols])))\n            j = available_cols.pop(relative_j)\n            # Similar to i above.\n\n            one_col = get_col(j)\n\n            # Add the column to the approximations of all matrices.\n            for id_mat in range(nb_matrices):\n                new_col = one_col[id_mat] - left[id_mat, :, :l] @ right[id_mat, :l, j]\n                pivot = new_col[i]\n                if abs(pivot) < 1e-12:\n                    pivot = 1e-12\n                left[id_mat, :, l] = new_col/pivot\n\n            # Update norm of the full matrix\n            squared_norm_of_increment = np.real(\n                    (np.conj(left[id_main, :, l]) @ left[id_main, :, l]) *\n                    (np.conj(right[id_main, l, :]) @ right[id_main, l, :])\n            )\n\n            crossed_terms = (\n                    (np.conj(left[id_main, :, l].T) @ left[id_main, :, :l]) @\n                    (np.conj(right[id_main, l, :]) @ right[id_main, :l, :].T)\n            )\n            squared_norm_of_low_rank_approximation += squared_norm_of_increment + 2*np.real(crossed_terms)\n\n            if squared_norm_of_increment <= tol**2*squared_norm_of_low_rank_approximation:\n                LOG.debug(f\"The ACA has found an approximation of rank {l}.\")\n\n                if l == 0:  # Edge case of the zero matrix, ...\n                    l = 1  # ... we actually return a \"rank 1\" LowRankMatrix with coefficients equal to zero.\n\n                return [LowRankMatrix(left[id_mat, :, :l], right[id_mat, :l, :]) for id_mat in range(nb_matrices)]\n\n        if tol > 0:\n            LOG.warning(f\"The ACA was unable to find a low rank approximation \"\n                        f\"of rank lower or equal to {max_rank} with tolerance {tol:.2e} (latest iteration: \"\n                        f\"{np.sqrt(squared_norm_of_increment):.2e}/{np.sqrt(squared_norm_of_low_rank_approximation):.2e}).\")\n            raise NoConvergenceOfACA()\n\n        return [LowRankMatrix(left[id_mat, :, :], right[id_mat, :, :]) for id_mat in range(nb_matrices)]",
  "def full_matrix(self, dtype=None):\n        if dtype is not None:\n            return (self.left_matrix @ self.right_matrix).astype(dtype)\n        else:\n            return self.left_matrix @ self.right_matrix",
  "def __array__(self, dtype=None):\n        return self.full_matrix(dtype=dtype)",
  "def stored_data_size(self):\n        return np.prod(self.left_matrix.shape) + np.prod(self.right_matrix.shape)",
  "def density(self):\n        return self.stored_data_size/np.prod(self.shape)",
  "def sparcity(self):\n        return 1 - self.density",
  "def recompress(self, tol=None, new_rank=None):\n        \"\"\"Recompress the matrix to a lower rank. Based on the routine hmxQRSVD.m from Gipsylab.\"\"\"\n        if new_rank is None:\n            new_rank = self.rank\n        QA, RA = np.linalg.qr(self.left_matrix)\n        QB, RB = np.linalg.qr(self.right_matrix.T)\n        U, S, V = np.linalg.svd(RA @ RB.T)\n        if tol is not None:\n            new_rank = np.count_nonzero(S/S[0] >= tol)\n        A = QA @ (U[:, :new_rank] @ np.diag(S[:new_rank]))\n        B = QB @ V[:, :new_rank]\n        return LowRankMatrix(A, B.T)",
  "def __add__(self, other):\n        if isinstance(other, LowRankMatrix):\n            new_left = np.concatenate([self.left_matrix, other.left_matrix], axis=1)\n            new_right = np.concatenate([self.right_matrix, other.right_matrix], axis=0)\n            return LowRankMatrix(new_left, new_right).recompress(new_rank=min(self.rank, other.rank))\n        else:\n            return NotImplemented",
  "def __neg__(self):\n        return LowRankMatrix(-self.left_matrix, self.right_matrix)",
  "def __sub__(self, other):\n        return self + (-other)",
  "def __truediv__(self, other):\n        from numbers import Number\n        if isinstance(other, Number):\n            return LowRankMatrix(self.left_matrix, self.right_matrix/other)\n        else:\n            return NotImplemented",
  "def __matmul__(self, other):\n        if isinstance(other, np.ndarray) and len(other.shape) == 1:\n            return self._mul_with_vector(other)\n        else:\n            return NotImplemented",
  "def _mul_with_vector(self, other):\n        return self.left_matrix @ (self.right_matrix @ other)",
  "def __rmatmul__(self, other):\n        if isinstance(other, np.ndarray) and len(other.shape) == 1:\n            return self._rmul_with_vector(other)\n        else:\n            return NotImplemented",
  "def _rmul_with_vector(self, other):\n        return (other @ self.left_matrix) @ self.right_matrix",
  "def astype(self, dtype):\n        return LowRankMatrix(self.left_matrix.astype(dtype), self.right_matrix.astype(dtype))",
  "def get_row(i):\n            return full_matrix[i, :]",
  "def get_col(j):\n            return full_matrix[:, j]",
  "def get_row(i):\n            return np.asarray([func(i, j) for j in range(nb_cols)])",
  "def get_col(j):\n            return np.asarray([func(i, j) for i in range(nb_rows)])",
  "def get_row(i):\n            return [get_row_func(i)]",
  "def get_col(j):\n            return [get_col_func(j)]",
  "class BlockToeplitzMatrix(BlockMatrix):\n    \"\"\"A (2D) block Toeplitz matrix, stored as a list of blocks.\n    All blocks should have the same shape.\n\n    Stored in the backend as a 1\u00d7(2N-1) array of arrays.\"\"\"\n\n    # INITIALIZATION\n\n    def _compute_shape(self) -> Tuple[int, int]:\n        # The full shape is found by multiplying the shape of the blocks. All of them have the same shape.\n        return (self._stored_block_shapes[0][0]*self.nb_blocks[0],\n                self._stored_block_shapes[1][0]*self.nb_blocks[1])\n\n    def _compute_nb_blocks(self) -> Tuple[int, int]:\n        \"\"\"Will be overridden by subclasses.\"\"\"\n        assert self._stored_nb_blocks[1] % 2 == 1, \"Expecting an odd number of blocks to build a Toeplitz matrix\"\n        n = (self._stored_nb_blocks[1]+1)//2\n        return n, n\n\n    def _check_dimensions_of_blocks(self) -> bool:\n        for block in self._stored_blocks[0, :]:\n            if not block.shape == self.block_shape:  # All blocks have same shape\n                return False\n        return True\n\n    # ACCESSING DATA\n\n    @property\n    def block_shapes(self) -> Tuple[List[int], List[int]]:\n        \"\"\"The shapes of the blocks composing the block matrix.\n        Actually, they should be all the same.\"\"\"\n        return ([self._stored_block_shapes[0][0]]*self.nb_blocks[0],\n                [self._stored_block_shapes[1][0]]*self.nb_blocks[1])\n\n    @property\n    def block_shape(self) -> Tuple[int, int]:\n        \"\"\"The shape of any of the blocks.\"\"\"\n        return self._stored_block_shapes[0][0], self._stored_block_shapes[1][0]\n\n    def _block_indices_of(self, k: int) -> Set[Tuple[int, int]]:\n        \"\"\"The block indices at which the stored block k can be found in the full matrix of size n.\n        Will be overridden by subclasses.\"\"\"\n        n = self.nb_blocks[0]\n\n        if k < n:\n            i, j = 0, k  # Upper triangle\n        elif n <= k < 2*n:\n            i, j = 2*n-1-k, 0  # Lower triangle\n        else:\n            raise AttributeError\n\n        indices = set()\n        while i < n and j < n:\n            indices.add((i, j))\n            i, j = i+1, j+1  # Going along the diagonal\n\n        return indices\n\n    @property\n    def all_blocks(self):\n        all_blocks = np.empty(self.nb_blocks, dtype=object)\n        for k in range(self._stored_nb_blocks[1]):\n            for i, j in self._block_indices_of(k):\n                all_blocks[i, j] = self._stored_blocks[0, k]\n        return all_blocks\n\n    def _positions_of(self, k: int, global_frame=(0, 0)) -> List[Tuple[int, int]]:\n        \"\"\"The positions in the full matrix at which the block k from the first line can also be found.\"\"\"\n        shape = self.block_shape\n        return sorted([(global_frame[0] + i*shape[0], global_frame[1] + j*shape[1])\n                       for i, j in self._block_indices_of(k)])\n\n    def _stored_block_positions(self, global_frame=(0, 0)) -> Iterable[List[Tuple[int, int]]]:\n        \"\"\"The position of each blocks in the matrix.\n\n        Example::\n\n            AABB\n            AABB  ->  list(matrix._stored_block_positions) = [[(0,0), (2, 2)], [(0, 2), (2, 0)]]\n            BBAA\n            BBAA\n        \"\"\"\n        return (self._positions_of(k, global_frame=global_frame) for k in range(self._stored_nb_blocks[1]))\n\n    # # TRANSFORMING DATA\n\n    @property\n    def circulant_super_matrix(self):\n        if not hasattr(self, '_circulant_super_matrix'):\n            self._circulant_super_matrix = BlockCirculantMatrix(\n                self._stored_blocks,\n                _stored_block_shapes=self._stored_block_shapes,\n                check=False)\n        return self._circulant_super_matrix\n\n    def matvec(self, other):\n        \"\"\"Matrix vector product.\n        Named as such to be used as scipy LinearOperator.\"\"\"\n        LOG.debug(f\"Product of {self} with vector of shape {other.shape}\")\n        A = self.circulant_super_matrix\n        b = np.concatenate([other, np.zeros(A.shape[1] - self.shape[1])])\n        return (A @ b)[:self.shape[0]]\n\n    def rmatvec(self, other):\n        \"\"\"Matrix vector product.\n        Named as such to be used as scipy LinearOperator.\"\"\"\n        LOG.debug(f\"Product of vector of shape {other.shape} with {self}\")\n        if other.ndim == 2 and other.shape[0] == 1:  # Actually a 1\u00d7N matrix\n            other = other[0, :]\n        A = self.circulant_super_matrix\n        b = np.concatenate([other, np.zeros(A.shape[0] - self.shape[0])])\n        return (A.rmatvec(b))[:self.shape[1]]",
  "class BlockSymmetricToeplitzMatrix(BlockToeplitzMatrix):\n    \"\"\"A (2D) block symmetric Toeplitz matrix, stored as a list of blocks.\n    All blocks should have the same shape.\n\n    Stored in the backend as a 1\u00d7N array of arrays.\"\"\"\n\n    def _compute_nb_blocks(self) -> Tuple[int, int]:\n        n = self._stored_nb_blocks[1]\n        return n, n\n\n    def _block_indices_of(self, k: int) -> Set[Tuple[int, int]]:\n        \"\"\"The block indices at which the stored block k can be found in the full matrix.\"\"\"\n        n = self.nb_blocks[0]\n        assert k < n\n\n        if k == 0:\n            return BlockToeplitzMatrix._block_indices_of(self, 0)\n        else:\n            return BlockToeplitzMatrix._block_indices_of(self, k).union(\n                BlockToeplitzMatrix._block_indices_of(self, 2*n-k-1))\n\n    @property\n    def circulant_super_matrix(self):\n        if not hasattr(self, '_circulant_super_matrix'):\n            self._circulant_super_matrix = EvenBlockSymmetricCirculantMatrix(\n                self._stored_blocks,\n                _stored_block_shapes=self._stored_block_shapes,\n                check=False)\n        return self._circulant_super_matrix",
  "class BlockCirculantMatrix(BlockToeplitzMatrix):\n    \"\"\"A (2D) block circulant matrix, stored as a list of blocks.\n    All blocks should have the same shape.\n\n    Stored in the backend as a 1\u00d7N array of arrays.\"\"\"\n\n    def _compute_nb_blocks(self) -> Tuple[int, int]:\n        n = self._stored_nb_blocks[1]\n        return n, n\n\n    def _block_indices_of(self, k: int) -> Set[Tuple[int, int]]:\n        \"\"\"The block indices at which the stored block k can be found in the full matrix.\"\"\"\n        n = self.nb_blocks[0]\n        assert k < n\n\n        if k == 0:\n            return BlockToeplitzMatrix._block_indices_of(self, 0)\n        else:\n            return BlockToeplitzMatrix._block_indices_of(self, k).union(\n                BlockToeplitzMatrix._block_indices_of(self, n+k-1))\n\n    # LINEAR SYSTEMS\n\n    def block_diagonalize(self):\n        \"\"\"Returns a vector of matrices\"\"\"\n        if not hasattr(self, 'block_diagonalization'):\n            if all(isinstance(matrix, BlockMatrix) for matrix in self._stored_blocks[0, :]):\n                self.block_diagonalization = BlockMatrix.fft_of_list(*self.all_blocks[:, 0])\n            else:\n                stacked_blocks = np.empty((self.nb_blocks[1],) + self.block_shape, dtype=self.dtype)\n                for i, block in enumerate(self.all_blocks[:, 0]):\n                    stacked_blocks[i] = block.full_matrix() if not isinstance(block, np.ndarray) else block\n                self.block_diagonalization =  np.fft.fft(stacked_blocks, axis=0)\n        return self.block_diagonalization\n\n    def matvec(self, other):\n        \"\"\"Matrix vector product.\n        Named as such to be used as scipy LinearOperator.\"\"\"\n        LOG.debug(f\"Product of {self} with vector of shape {other.shape}\")\n        fft_of_vector = np.fft.fft(np.reshape(other, (self.nb_blocks[0], self.block_shape[1], 1)), axis=0)\n        blocks_of_diagonalization = self.block_diagonalize()\n        try:  # Try to run it as vectorized numpy arrays.\n            fft_of_result = blocks_of_diagonalization @ fft_of_vector\n        # When the above fails, numpy 1.15 returns a TypeError, whereas numpy 1.16 returns a ValueError.\n        except (TypeError, ValueError):  # Or do the same thing with list comprehension.\n            fft_of_result = np.array([block @ vec for block, vec in zip(blocks_of_diagonalization, fft_of_vector)])\n        result = np.fft.ifft(fft_of_result, axis=0).reshape(self.shape[0])\n        if self.dtype == complex or other.dtype == complex:\n            return np.asarray(result)\n        else:\n            return np.asarray(np.real(result))\n\n    def rmatvec(self, other):\n        \"\"\"Matrix vector product.\n        Named as such to be used as scipy LinearOperator.\"\"\"\n        other = np.conjugate(other)\n        fft_of_vector = np.fft.ifft(np.reshape(other, (self.nb_blocks[0], 1, self.block_shape[0])), axis=0)\n        blocks_of_diagonalization = self.block_diagonalize()\n        try:  # Try to run it as vectorized numpy arrays.\n            fft_of_result = fft_of_vector @ blocks_of_diagonalization\n        # When the above fails, numpy 1.15 returns a TypeError, whereas numpy 1.16 returns a ValueError.\n        except (TypeError, ValueError):\n            # Instead we do the same thing with list comprehension.\n            fft_of_result = np.array(\n                [block.rmatvec(vec.flatten()) for block, vec in zip(blocks_of_diagonalization, fft_of_vector)]\n            )\n        result = np.fft.fft(fft_of_result, axis=0).reshape(self.shape[1])\n        if self.dtype == complex or other.dtype == complex:\n            return np.asarray(result)\n        else:\n            return np.asarray(np.real(result))",
  "class EvenBlockSymmetricCirculantMatrix(BlockCirculantMatrix, BlockSymmetricToeplitzMatrix):\n    \"\"\"A block symmetric circulant matrix, with an even number of blocks.\n\n    Examples::\n\n        ABCB\n        BABC\n        CBAB\n        BCBA\n\n        ABCDCB\n        BABCDB\n        CBABCD\n        DCBABC\n        CDCBAB\n        BCDCBA\n\n    Stored in the backend as a 1\u00d7(N/2+1) array of arrays.\"\"\"\n\n    def _compute_nb_blocks(self) -> Tuple[int, int]:\n        \"\"\"The number of blocks in the full matrix.\"\"\"\n        n = (self._stored_nb_blocks[1] - 1)*2\n        return n, n\n\n    def _block_indices_of(self, k: int) -> List[Tuple[int, int]]:\n        n = self.nb_blocks[0]\n        assert k < n/2 + 1\n        if k == 0:\n            return BlockToeplitzMatrix._block_indices_of(self, 0)\n        else:\n            return (BlockToeplitzMatrix._block_indices_of(self, k) |\n                BlockToeplitzMatrix._block_indices_of(self, n+k-1) |\n                BlockToeplitzMatrix._block_indices_of(self, n-k)   |\n                BlockToeplitzMatrix._block_indices_of(self, 2*n-k-1)\n                    )",
  "class OddBlockSymmetricCirculantMatrix(BlockCirculantMatrix, BlockSymmetricToeplitzMatrix):\n    \"\"\"A block symmetric circulant matrix, with an odd number of blocks.\n\n    Examples::\n\n        ABCCB\n        BABCC\n        CBABC\n        CCBAB\n        BCCBA\n\n        ABCDDCB\n        BABCDDB\n        CBABCDD\n        DCBABCD\n        DDCBABC\n        CDDCBAB\n        BCDDCBA\n\n    Stored in the backend as a 1\u00d7(N+1)/2 array of arrays.\"\"\"\n\n    def _compute_nb_blocks(self) -> Tuple[int, int]:\n        n = self._stored_nb_blocks[1]*2 - 1\n        return n, n\n\n    def _block_indices_of(self, k: int) -> List[Tuple[int, int]]:\n        n = self.nb_blocks[0]\n        assert k < (n+1)/2\n        if k == 0:\n            return BlockToeplitzMatrix._block_indices_of(self, 0)\n        else:\n            return (BlockToeplitzMatrix._block_indices_of(self, k) |\n                BlockToeplitzMatrix._block_indices_of(self, n+k-1) |\n                BlockToeplitzMatrix._block_indices_of(self, n-k)   |\n                BlockToeplitzMatrix._block_indices_of(self, 2*n-k-1)\n                    )",
  "def _compute_shape(self) -> Tuple[int, int]:\n        # The full shape is found by multiplying the shape of the blocks. All of them have the same shape.\n        return (self._stored_block_shapes[0][0]*self.nb_blocks[0],\n                self._stored_block_shapes[1][0]*self.nb_blocks[1])",
  "def _compute_nb_blocks(self) -> Tuple[int, int]:\n        \"\"\"Will be overridden by subclasses.\"\"\"\n        assert self._stored_nb_blocks[1] % 2 == 1, \"Expecting an odd number of blocks to build a Toeplitz matrix\"\n        n = (self._stored_nb_blocks[1]+1)//2\n        return n, n",
  "def _check_dimensions_of_blocks(self) -> bool:\n        for block in self._stored_blocks[0, :]:\n            if not block.shape == self.block_shape:  # All blocks have same shape\n                return False\n        return True",
  "def block_shapes(self) -> Tuple[List[int], List[int]]:\n        \"\"\"The shapes of the blocks composing the block matrix.\n        Actually, they should be all the same.\"\"\"\n        return ([self._stored_block_shapes[0][0]]*self.nb_blocks[0],\n                [self._stored_block_shapes[1][0]]*self.nb_blocks[1])",
  "def block_shape(self) -> Tuple[int, int]:\n        \"\"\"The shape of any of the blocks.\"\"\"\n        return self._stored_block_shapes[0][0], self._stored_block_shapes[1][0]",
  "def _block_indices_of(self, k: int) -> Set[Tuple[int, int]]:\n        \"\"\"The block indices at which the stored block k can be found in the full matrix of size n.\n        Will be overridden by subclasses.\"\"\"\n        n = self.nb_blocks[0]\n\n        if k < n:\n            i, j = 0, k  # Upper triangle\n        elif n <= k < 2*n:\n            i, j = 2*n-1-k, 0  # Lower triangle\n        else:\n            raise AttributeError\n\n        indices = set()\n        while i < n and j < n:\n            indices.add((i, j))\n            i, j = i+1, j+1  # Going along the diagonal\n\n        return indices",
  "def all_blocks(self):\n        all_blocks = np.empty(self.nb_blocks, dtype=object)\n        for k in range(self._stored_nb_blocks[1]):\n            for i, j in self._block_indices_of(k):\n                all_blocks[i, j] = self._stored_blocks[0, k]\n        return all_blocks",
  "def _positions_of(self, k: int, global_frame=(0, 0)) -> List[Tuple[int, int]]:\n        \"\"\"The positions in the full matrix at which the block k from the first line can also be found.\"\"\"\n        shape = self.block_shape\n        return sorted([(global_frame[0] + i*shape[0], global_frame[1] + j*shape[1])\n                       for i, j in self._block_indices_of(k)])",
  "def _stored_block_positions(self, global_frame=(0, 0)) -> Iterable[List[Tuple[int, int]]]:\n        \"\"\"The position of each blocks in the matrix.\n\n        Example::\n\n            AABB\n            AABB  ->  list(matrix._stored_block_positions) = [[(0,0), (2, 2)], [(0, 2), (2, 0)]]\n            BBAA\n            BBAA\n        \"\"\"\n        return (self._positions_of(k, global_frame=global_frame) for k in range(self._stored_nb_blocks[1]))",
  "def circulant_super_matrix(self):\n        if not hasattr(self, '_circulant_super_matrix'):\n            self._circulant_super_matrix = BlockCirculantMatrix(\n                self._stored_blocks,\n                _stored_block_shapes=self._stored_block_shapes,\n                check=False)\n        return self._circulant_super_matrix",
  "def matvec(self, other):\n        \"\"\"Matrix vector product.\n        Named as such to be used as scipy LinearOperator.\"\"\"\n        LOG.debug(f\"Product of {self} with vector of shape {other.shape}\")\n        A = self.circulant_super_matrix\n        b = np.concatenate([other, np.zeros(A.shape[1] - self.shape[1])])\n        return (A @ b)[:self.shape[0]]",
  "def rmatvec(self, other):\n        \"\"\"Matrix vector product.\n        Named as such to be used as scipy LinearOperator.\"\"\"\n        LOG.debug(f\"Product of vector of shape {other.shape} with {self}\")\n        if other.ndim == 2 and other.shape[0] == 1:  # Actually a 1\u00d7N matrix\n            other = other[0, :]\n        A = self.circulant_super_matrix\n        b = np.concatenate([other, np.zeros(A.shape[0] - self.shape[0])])\n        return (A.rmatvec(b))[:self.shape[1]]",
  "def _compute_nb_blocks(self) -> Tuple[int, int]:\n        n = self._stored_nb_blocks[1]\n        return n, n",
  "def _block_indices_of(self, k: int) -> Set[Tuple[int, int]]:\n        \"\"\"The block indices at which the stored block k can be found in the full matrix.\"\"\"\n        n = self.nb_blocks[0]\n        assert k < n\n\n        if k == 0:\n            return BlockToeplitzMatrix._block_indices_of(self, 0)\n        else:\n            return BlockToeplitzMatrix._block_indices_of(self, k).union(\n                BlockToeplitzMatrix._block_indices_of(self, 2*n-k-1))",
  "def circulant_super_matrix(self):\n        if not hasattr(self, '_circulant_super_matrix'):\n            self._circulant_super_matrix = EvenBlockSymmetricCirculantMatrix(\n                self._stored_blocks,\n                _stored_block_shapes=self._stored_block_shapes,\n                check=False)\n        return self._circulant_super_matrix",
  "def _compute_nb_blocks(self) -> Tuple[int, int]:\n        n = self._stored_nb_blocks[1]\n        return n, n",
  "def _block_indices_of(self, k: int) -> Set[Tuple[int, int]]:\n        \"\"\"The block indices at which the stored block k can be found in the full matrix.\"\"\"\n        n = self.nb_blocks[0]\n        assert k < n\n\n        if k == 0:\n            return BlockToeplitzMatrix._block_indices_of(self, 0)\n        else:\n            return BlockToeplitzMatrix._block_indices_of(self, k).union(\n                BlockToeplitzMatrix._block_indices_of(self, n+k-1))",
  "def block_diagonalize(self):\n        \"\"\"Returns a vector of matrices\"\"\"\n        if not hasattr(self, 'block_diagonalization'):\n            if all(isinstance(matrix, BlockMatrix) for matrix in self._stored_blocks[0, :]):\n                self.block_diagonalization = BlockMatrix.fft_of_list(*self.all_blocks[:, 0])\n            else:\n                stacked_blocks = np.empty((self.nb_blocks[1],) + self.block_shape, dtype=self.dtype)\n                for i, block in enumerate(self.all_blocks[:, 0]):\n                    stacked_blocks[i] = block.full_matrix() if not isinstance(block, np.ndarray) else block\n                self.block_diagonalization =  np.fft.fft(stacked_blocks, axis=0)\n        return self.block_diagonalization",
  "def matvec(self, other):\n        \"\"\"Matrix vector product.\n        Named as such to be used as scipy LinearOperator.\"\"\"\n        LOG.debug(f\"Product of {self} with vector of shape {other.shape}\")\n        fft_of_vector = np.fft.fft(np.reshape(other, (self.nb_blocks[0], self.block_shape[1], 1)), axis=0)\n        blocks_of_diagonalization = self.block_diagonalize()\n        try:  # Try to run it as vectorized numpy arrays.\n            fft_of_result = blocks_of_diagonalization @ fft_of_vector\n        # When the above fails, numpy 1.15 returns a TypeError, whereas numpy 1.16 returns a ValueError.\n        except (TypeError, ValueError):  # Or do the same thing with list comprehension.\n            fft_of_result = np.array([block @ vec for block, vec in zip(blocks_of_diagonalization, fft_of_vector)])\n        result = np.fft.ifft(fft_of_result, axis=0).reshape(self.shape[0])\n        if self.dtype == complex or other.dtype == complex:\n            return np.asarray(result)\n        else:\n            return np.asarray(np.real(result))",
  "def rmatvec(self, other):\n        \"\"\"Matrix vector product.\n        Named as such to be used as scipy LinearOperator.\"\"\"\n        other = np.conjugate(other)\n        fft_of_vector = np.fft.ifft(np.reshape(other, (self.nb_blocks[0], 1, self.block_shape[0])), axis=0)\n        blocks_of_diagonalization = self.block_diagonalize()\n        try:  # Try to run it as vectorized numpy arrays.\n            fft_of_result = fft_of_vector @ blocks_of_diagonalization\n        # When the above fails, numpy 1.15 returns a TypeError, whereas numpy 1.16 returns a ValueError.\n        except (TypeError, ValueError):\n            # Instead we do the same thing with list comprehension.\n            fft_of_result = np.array(\n                [block.rmatvec(vec.flatten()) for block, vec in zip(blocks_of_diagonalization, fft_of_vector)]\n            )\n        result = np.fft.fft(fft_of_result, axis=0).reshape(self.shape[1])\n        if self.dtype == complex or other.dtype == complex:\n            return np.asarray(result)\n        else:\n            return np.asarray(np.real(result))",
  "def _compute_nb_blocks(self) -> Tuple[int, int]:\n        \"\"\"The number of blocks in the full matrix.\"\"\"\n        n = (self._stored_nb_blocks[1] - 1)*2\n        return n, n",
  "def _block_indices_of(self, k: int) -> List[Tuple[int, int]]:\n        n = self.nb_blocks[0]\n        assert k < n/2 + 1\n        if k == 0:\n            return BlockToeplitzMatrix._block_indices_of(self, 0)\n        else:\n            return (BlockToeplitzMatrix._block_indices_of(self, k) |\n                BlockToeplitzMatrix._block_indices_of(self, n+k-1) |\n                BlockToeplitzMatrix._block_indices_of(self, n-k)   |\n                BlockToeplitzMatrix._block_indices_of(self, 2*n-k-1)\n                    )",
  "def _compute_nb_blocks(self) -> Tuple[int, int]:\n        n = self._stored_nb_blocks[1]*2 - 1\n        return n, n",
  "def _block_indices_of(self, k: int) -> List[Tuple[int, int]]:\n        n = self.nb_blocks[0]\n        assert k < (n+1)/2\n        if k == 0:\n            return BlockToeplitzMatrix._block_indices_of(self, 0)\n        else:\n            return (BlockToeplitzMatrix._block_indices_of(self, k) |\n                BlockToeplitzMatrix._block_indices_of(self, n+k-1) |\n                BlockToeplitzMatrix._block_indices_of(self, n-k)   |\n                BlockToeplitzMatrix._block_indices_of(self, 2*n-k-1)\n                    )",
  "def solve_directly(A, b):\n    assert isinstance(b, np.ndarray) and A.ndim == b.ndim+1 and A.shape[-2] == b.shape[-1]\n    if isinstance(A, BlockCirculantMatrix):\n        LOG.debug(\"\\tSolve linear system %s\", A)\n        blocks_of_diagonalization = A.block_diagonalize()\n        fft_of_rhs = np.fft.fft(np.reshape(b, (A.nb_blocks[0], A.block_shape[0])), axis=0)\n        try:  # Try to run it as vectorized numpy arrays.\n            fft_of_result = np.linalg.solve(blocks_of_diagonalization, fft_of_rhs)\n        except np.linalg.LinAlgError:  # Or do the same thing with list comprehension.\n            fft_of_result = np.array([solve_directly(block, vec) for block, vec in zip(blocks_of_diagonalization, fft_of_rhs)])\n        result = np.fft.ifft(fft_of_result, axis=0).reshape((A.shape[1],))\n        return result\n\n    elif isinstance(A, BlockSymmetricToeplitzMatrix):\n        if A.nb_blocks == (2, 2):\n            LOG.debug(\"\\tSolve linear system %s\", A)\n            A1, A2 = A._stored_blocks[0, :]\n            b1, b2 = b[:len(b)//2], b[len(b)//2:]\n            x_plus = solve_directly(A1 + A2, b1 + b2)\n            x_minus = solve_directly(A1 - A2, b1 - b2)\n            return np.concatenate([x_plus + x_minus, x_plus - x_minus])/2\n        else:\n            # Not implemented\n            LOG.debug(\"\\tSolve linear system %s\", A)\n            return solve_directly(A.full_matrix(), b)\n\n    elif isinstance(A, BlockMatrix):\n        LOG.debug(\"\\tSolve linear system %s\", A)\n        return solve_directly(A.full_matrix(), b)\n\n    elif isinstance(A, np.ndarray):\n        LOG.debug(f\"\\tSolve linear system (size: {A.shape}) with numpy direct solver.\")\n        return np.linalg.solve(A, b)\n\n    else:\n        raise ValueError(f\"Unrecognized type of matrix to solve: {A}\")",
  "class LUSolverWithCache:\n    \"\"\"Solve linear system with the LU decomposition.\n\n    The latest LU decomposition is kept in memory, if a system with the same matrix needs to be solved again, then the decomposition is reused.\n\n    Most of the complexity of this class comes from:\n    1. @lru_cache does not work because numpy arrays are not hashable. So a basic cache system has been recoded from scratch.\n    2. To be the default solver for the BasicMatrixEngine, the solver needs to support matrices for problems with one or two reflection symmetries.\n    Hence, a custom way to cache the LU decomposition of the matrices involved in the direct linear resolution of the symmetric problem.\n    \"\"\"\n    def __init__(self):\n        self.cached_matrix = None\n        self.cached_decomp = None\n\n    def solve(self, A, b):\n        return self.solve_with_decomp(self.cached_lu_decomp(A), b)\n\n    def lu_decomp(self, A):\n        \"\"\"Return the LU decomposition of A.\n        If A is BlockSymmetricToeplitzMatrix, then return a list of LU decompositions for each block of the block diagonalisation of the matrix.\n        \"\"\"\n        if isinstance(A, BlockSymmetricToeplitzMatrix) and A.nb_blocks == (2, 2):\n            A1, A2 = A._stored_blocks[0, :]\n            return [self.lu_decomp(A1 + A2), self.lu_decomp(A1 - A2)]\n        elif isinstance(A, np.ndarray):\n            return sl.lu_factor(A)\n        else:\n            raise NotImplementedError(\"Cached LU solver is only implemented for dense matrices and 2\u00d72 BlockSymmetricToeplitzMatrix.\")\n\n    def cached_lu_decomp(self, A):\n        if not(A is self.cached_matrix):\n            self.cached_matrix = A\n            LOG.debug(f\"Computing and caching LU decomposition\")\n            self.cached_decomp = self.lu_decomp(A)\n        else:\n            LOG.debug(f\"Using cached LU decomposition\")\n        return self.cached_decomp\n\n    def solve_with_decomp(self, decomp, b):\n        \"\"\"Solve the system using the precomputed LU decomposition.\n        TODO: find a better way to differentiate a LU decomposition (returned as tuple by sl.lu_factor)\n        and a set of LU decomposition (stored as a list in self.lu_decomp).\n        \"\"\"\n        if isinstance(decomp, list):  # The matrix was a BlockSymmetricToeplitzMatrix\n            b1, b2 = b[:len(b)//2], b[len(b)//2:]\n            x_plus = self.solve_with_decomp(decomp[0], b1 + b2)\n            x_minus = self.solve_with_decomp(decomp[1], b1 - b2)\n            return np.concatenate([x_plus + x_minus, x_plus - x_minus])/2\n        elif isinstance(decomp, tuple):  # The matrix was a np.ndarray\n            return sl.lu_solve(decomp, b)\n        else:\n            raise NotImplementedError(\"Cached LU solver is only implemented for dense matrices and 2\u00d72 BlockSymmetricToeplitzMatrix.\")",
  "class Counter:\n    def __init__(self):\n        self.nb_iter = 0\n\n    def __call__(self, *args, **kwargs):\n        self.nb_iter += 1",
  "def solve_gmres(A, b):\n    LOG.debug(f\"Solve with GMRES for {A}.\")\n\n    if LOG.isEnabledFor(logging.DEBUG):\n        counter = Counter()\n        x, info = ssl.gmres(A, b, atol=1e-6, callback=counter)\n        LOG.debug(f\"End of GMRES after {counter.nb_iter} iterations.\")\n\n    else:\n        x, info = ssl.gmres(A, b, atol=1e-6)\n\n    if info > 0:\n        raise RuntimeError(f\"No convergence of the GMRES after {info} iterations.\\n\"\n                            \"This can be due to overlapping panels or irregular frequencies.\\n\"\n                            \"In the latter case, using a direct solver can help (https://github.com/mancellin/capytaine/issues/30).\")\n\n    return x",
  "def gmres_no_fft(A, b):\n    LOG.debug(f\"Solve with GMRES for {A} without using FFT.\")\n\n    x, info = ssl.gmres(A.no_toeplitz() if isinstance(A, BlockMatrix) else A, b, atol=1e-6)\n\n    if info != 0:\n        LOG.warning(f\"No convergence of the GMRES. Error code: {info}\")\n\n    return x",
  "def __init__(self):\n        self.cached_matrix = None\n        self.cached_decomp = None",
  "def solve(self, A, b):\n        return self.solve_with_decomp(self.cached_lu_decomp(A), b)",
  "def lu_decomp(self, A):\n        \"\"\"Return the LU decomposition of A.\n        If A is BlockSymmetricToeplitzMatrix, then return a list of LU decompositions for each block of the block diagonalisation of the matrix.\n        \"\"\"\n        if isinstance(A, BlockSymmetricToeplitzMatrix) and A.nb_blocks == (2, 2):\n            A1, A2 = A._stored_blocks[0, :]\n            return [self.lu_decomp(A1 + A2), self.lu_decomp(A1 - A2)]\n        elif isinstance(A, np.ndarray):\n            return sl.lu_factor(A)\n        else:\n            raise NotImplementedError(\"Cached LU solver is only implemented for dense matrices and 2\u00d72 BlockSymmetricToeplitzMatrix.\")",
  "def cached_lu_decomp(self, A):\n        if not(A is self.cached_matrix):\n            self.cached_matrix = A\n            LOG.debug(f\"Computing and caching LU decomposition\")\n            self.cached_decomp = self.lu_decomp(A)\n        else:\n            LOG.debug(f\"Using cached LU decomposition\")\n        return self.cached_decomp",
  "def solve_with_decomp(self, decomp, b):\n        \"\"\"Solve the system using the precomputed LU decomposition.\n        TODO: find a better way to differentiate a LU decomposition (returned as tuple by sl.lu_factor)\n        and a set of LU decomposition (stored as a list in self.lu_decomp).\n        \"\"\"\n        if isinstance(decomp, list):  # The matrix was a BlockSymmetricToeplitzMatrix\n            b1, b2 = b[:len(b)//2], b[len(b)//2:]\n            x_plus = self.solve_with_decomp(decomp[0], b1 + b2)\n            x_minus = self.solve_with_decomp(decomp[1], b1 - b2)\n            return np.concatenate([x_plus + x_minus, x_plus - x_minus])/2\n        elif isinstance(decomp, tuple):  # The matrix was a np.ndarray\n            return sl.lu_solve(decomp, b)\n        else:\n            raise NotImplementedError(\"Cached LU solver is only implemented for dense matrices and 2\u00d72 BlockSymmetricToeplitzMatrix.\")",
  "def __init__(self):\n        self.nb_iter = 0",
  "def __call__(self, *args, **kwargs):\n        self.nb_iter += 1",
  "class BlockMatrix:\n    \"\"\"A (2D) matrix, stored as a set of submatrices (or blocks).\n\n    Parameters\n    ----------\n    blocks: list of list of matrices\n        The blocks of the block matrix.\n    check: bool, optional\n        Should the code perform sanity checks on the inputs? (default: True)\n\n    Attributes\n    ----------\n    shape: pair of ints\n        shape of the full matrix\n    nb_blocks: pair of ints\n        number of blocks in each directions.\n        Example::\n\n            AAAABB\n            AAAABB  ->  nb_blocks = (1, 2)\n            AAAABB\n    \"\"\"\n\n    ndim = 2  # Other dimensions have not been implemented.\n\n    def __init__(self, blocks, _stored_block_shapes=None, check=True):\n\n        self._stored_nb_blocks = (len(blocks), len(blocks[0]))\n\n        self._stored_blocks = np.empty(self._stored_nb_blocks, dtype=object)\n        for i in range(len(blocks)):\n            for j in range(len(blocks[i])):\n                self._stored_blocks[i, j] = blocks[i][j]\n\n        if _stored_block_shapes is None:\n            self._stored_block_shapes = ([block.shape[0] for block in self._stored_blocks[:, 0]],\n                                         [block.shape[1] for block in self._stored_blocks[0, :]])\n        else:\n            # To avoid going down the tree if it is already known.\n            self._stored_block_shapes = _stored_block_shapes\n\n        # Block shape of the full matrix\n        self.nb_blocks = self._compute_nb_blocks()\n\n        # Total shape of the full matrix\n        self.shape = self._compute_shape()\n\n        LOG.debug(f\"New block matrix: %s\", self)\n\n        if check:\n            assert self._check_dimensions_of_blocks()\n            assert self._check_dtype()\n\n    def _compute_shape(self):\n        # In a dedicated routine because it will be overloaded by subclasses.\n        return sum(self._stored_block_shapes[0]), sum(self._stored_block_shapes[1])\n\n    def _compute_nb_blocks(self):\n        return self._stored_nb_blocks\n\n    def _check_dimensions_of_blocks(self) -> bool:\n        \"\"\"Check that the dimensions of the blocks are consistent.\"\"\"\n        if not all(block.ndim == self.ndim for line in self._stored_blocks for block in line):\n            return False\n\n        for line in self.all_blocks:\n            block_height = line[0].shape[0]\n            for block in line[1:]:\n                if not block.shape[0] == block_height:  # Same height on a given line\n                    return False\n\n        for col in np.moveaxis(self.all_blocks, 1, 0):\n            block_width = col[0].shape[1]\n            for block in col[1:]:\n                if not block.shape[1] == block_width:  # Same width on a given column\n                    return False\n        return True\n\n    def _check_dtype(self) -> bool:\n        \"\"\"Check that the type of the blocks are consistent.\"\"\"\n        for line in self._stored_blocks:\n            for block in line:\n                if block.dtype != self.dtype:\n                    return False\n        return True\n\n    # ACCESSING DATA\n\n    @property\n    def dtype(self):\n        \"\"\"The type of data of all of the subblocks.\"\"\"\n        try:\n            return self._stored_blocks[0][0].dtype\n        except AttributeError:\n            return None\n\n    @property\n    def all_blocks(self) -> np.ndarray:\n        \"\"\"The matrix of matrices. For a full block matrix, all the blocks are stored in memory.\"\"\"\n        return self._stored_blocks\n\n    @property\n    def block_shapes(self) -> Tuple[List[int], List[int]]:\n        \"\"\"The shapes of the blocks composing the BlockMatrix.\n\n        Example::\n\n            AAAABB\n            AAAABB  ->  block_shapes = ([3], [4, 2])\n            AAAABB\n        \"\"\"\n        return self._stored_block_shapes\n\n    def _stored_block_positions(self, global_frame=(0, 0)) -> Iterable[List[Tuple[int, int]]]:\n        \"\"\"The position of each blocks in the matrix as a generator.\n        The list is used by subclasses where the same block may appear several times in different positions.\n\n        Parameters\n        ----------\n        global_frame: Tuple[int], optional\n            the coordinate of the top right corner. Default: 0, 0.\n\n        Example::\n\n            AAAABB\n            AAAABB  ->  list(matrix._stored_block_positions) = [[(0,0)], [(0, 4)], [(2, 0)], [(2, 4)]]\n            CCCCDD\n        \"\"\"\n        x_acc = accumulate([0] + self.block_shapes[0][:-1])\n        y_acc = accumulate([0] + self.block_shapes[1][:-1])\n        return ([(global_frame[0] + x, global_frame[1] + y)] for x, y in product(x_acc, y_acc))\n\n    def _put_in_full_matrix(self, full_matrix: np.ndarray, where=(0, 0)) -> None:\n        \"\"\"Copy the content of the block matrix in a matrix, which is modified in place.\"\"\"\n        all_blocks_in_flat_iterator = (block for line in self._stored_blocks for block in line)\n        positions_of_all_blocks = self._stored_block_positions(global_frame=where)\n        for block, positions_of_the_block in zip(all_blocks_in_flat_iterator, positions_of_all_blocks):\n            if isinstance(block, BlockMatrix):\n                position_of_first_appearance = positions_of_the_block[0]\n                frame_of_first_appearance = (slice(position_of_first_appearance[0], position_of_first_appearance[0]+block.shape[0]),\n                                             slice(position_of_first_appearance[1], position_of_first_appearance[1]+block.shape[1]))\n                block._put_in_full_matrix(full_matrix, where=position_of_first_appearance)\n\n                for position in positions_of_the_block[1:]:  # For the other appearances, only copy the first appearance\n                    block_frame = (slice(position[0], position[0]+block.shape[0]),\n                                   slice(position[1], position[1]+block.shape[1]))\n                    full_matrix[block_frame] = full_matrix[frame_of_first_appearance]\n\n            else:\n                full_block = block if isinstance(block, np.ndarray) else block.full_matrix()\n                for position in positions_of_the_block:\n                    block_frame = (slice(position[0], position[0]+block.shape[0]),\n                                   slice(position[1], position[1]+block.shape[1]))\n                    full_matrix[block_frame] = full_block\n\n    def full_matrix(self, dtype=None) -> np.ndarray:\n        \"\"\"Flatten the block structure and return a full matrix.\"\"\"\n        if dtype is None: dtype = self.dtype\n        full_matrix = np.empty(self.shape, dtype=dtype)\n        self._put_in_full_matrix(full_matrix)\n        return full_matrix\n\n    def __array__(self, dtype=None):\n        return self.full_matrix(dtype=dtype)\n\n    def no_toeplitz(self):\n        \"\"\"Recursively replace the block toeplitz matrices by usual block matrices.\n        WARNING: the block matrices may still contain several references to the same block.\"\"\"\n        blocks = [[block.no_toeplitz() if isinstance(block, BlockMatrix) else block for block in line] for line in self.all_blocks]\n        return BlockMatrix(blocks)\n\n    def __deepcopy__(self, memo):\n        from copy import deepcopy\n        blocks = [[deepcopy(block) for block in line] for line in self._stored_blocks]\n        # The call to deepcopy does not use the memo on purpose:\n        # the goal is to replace references to the same block by references to different copies of the block.\n        return self.__class__(blocks)\n\n    @property\n    def stored_data_size(self):\n        \"\"\"Return the number of entries actually stored in memory.\"\"\"\n        size = 0\n        for line in self._stored_blocks:\n            for block in line:\n                if isinstance(block, np.ndarray):\n                    size += np.prod(block.shape)\n                else:\n                    size += block.stored_data_size\n        return size\n\n    @property\n    def density(self):\n        return self.stored_data_size/np.prod(self.shape)\n\n    @property\n    def sparcity(self):\n        return 1 - self.density\n\n    def __hash__(self):\n        # Temporary\n        return id(self)\n\n    # TRANSFORMING DATA\n\n    def _apply_unary_op(self, op: Callable) -> 'BlockMatrix':\n        \"\"\"Helper function applying a function recursively on all submatrices.\"\"\"\n        LOG.debug(f\"Apply op {op.__name__} to {self}\")\n        result = [[op(block) for block in line] for line in self._stored_blocks]\n        return self.__class__(result, _stored_block_shapes=self._stored_block_shapes, check=False)\n\n    def _apply_binary_op(self, op: Callable, other: 'BlockMatrix') -> 'BlockMatrix':\n        \"\"\"Helper function applying a binary operator recursively on all submatrices.\"\"\"\n        if isinstance(other, self.__class__) and self.nb_blocks == other.nb_blocks:\n            LOG.debug(f\"Apply op {op.__name__} to {self} and {other}\")\n            result = [\n                [op(block, other_block) for block, other_block in zip(line, other_line)]\n                for line, other_line in zip(self._stored_blocks, other._stored_blocks)\n            ]\n            return self.__class__(result, _stored_block_shapes=self._stored_block_shapes, check=False)\n        else:\n            return NotImplemented\n\n    def __add__(self, other: 'BlockMatrix') -> 'BlockMatrix':\n        from operator import add\n        return self._apply_binary_op(add, other)\n\n    def __radd__(self, other: 'BlockMatrix') -> 'BlockMatrix':\n        return self + other\n\n    def __neg__(self) -> 'BlockMatrix':\n        from operator import neg\n        return self._apply_unary_op(neg)\n\n    def __sub__(self, other: 'BlockMatrix') -> 'BlockMatrix':\n        from operator import sub\n        return self._apply_binary_op(sub, other)\n\n    def __rsub__(self, other: 'BlockMatrix') -> 'BlockMatrix':\n        from operator import sub\n        return other._apply_binary_op(sub, self)\n\n    def __mul__(self, other: Union['BlockMatrix', Number]) -> 'BlockMatrix':\n        if isinstance(other, Number):\n            return self._apply_unary_op(lambda x: other*x)\n        else:\n            from operator import mul\n            return self._apply_binary_op(mul, other)\n\n    def __rmul__(self, other: Union['BlockMatrix', Number]) -> 'BlockMatrix':\n        return self * other\n\n    def __truediv__(self, other: Union['BlockMatrix', Number]) -> 'BlockMatrix':\n        from numbers import Number\n        if isinstance(other, Number):\n            return self._apply_unary_op(lambda x: x/other)\n        else:\n            from operator import truediv\n            return self._apply_binary_op(truediv, other)\n\n    def __rtruediv__(self, other: Union['BlockMatrix', Number]) -> 'BlockMatrix':\n        from numbers import Number\n        if isinstance(other, Number):\n            return self._apply_unary_op(lambda x: other/x)\n        else:\n            return self._apply_binary_op(lambda x, y: y/x, other)\n\n    def matvec(self, other):\n        \"\"\"Matrix vector product.\n        Named as such to be used as scipy LinearOperator.\"\"\"\n        LOG.debug(f\"Multiplication of {self} with a full vector of size {other.shape}.\")\n        result = np.zeros(self.shape[0], dtype=other.dtype)\n        line_heights = self.block_shapes[0]\n        line_positions = list(accumulate(chain([0], line_heights)))\n        col_widths = self.block_shapes[1]\n        col_positions = list(accumulate(chain([0], col_widths)))\n        for line, line_position, line_height in zip(self.all_blocks, line_positions, line_heights):\n            line_slice = slice(line_position, line_position+line_height)\n            for block, col_position, col_width in zip(line, col_positions, col_widths):\n                col_slice = slice(col_position, col_position+col_width)\n                result[line_slice] += block @ other[col_slice]\n        return result\n\n    def rmatvec(self, other):\n        \"\"\"Vector matrix product.\n        Named as such to be used as scipy LinearOperator.\"\"\"\n        LOG.debug(f\"Multiplication of a full vector of size {other.shape} with {self}.\")\n        result = np.zeros(self.shape[1], dtype=other.dtype)\n        line_heights = self.block_shapes[0]\n        line_positions = list(accumulate(chain([0], line_heights)))\n        col_widths = self.block_shapes[1]\n        col_positions = list(accumulate(chain([0], col_widths)))\n        for col, col_position, col_width in zip(self.all_blocks.T, col_positions, col_widths):\n            col_slice = slice(col_position, col_position+col_width)\n            for block, line_position, line_height in zip(col, line_positions, line_heights):\n                line_slice = slice(line_position, line_position+line_height)\n                if isinstance(block, BlockMatrix):\n                    result[col_slice] += block.rmatvec(other[line_slice])\n                else:\n                    result[col_slice] += other[line_slice] @ block\n        return result\n\n    def matmat(self, other):\n        \"\"\"Matrix-matrix product.\"\"\"\n        if isinstance(other, BlockMatrix) and self.block_shapes[1] == other.block_shapes[0]:\n            LOG.debug(f\"Multiplication of %s with %s\", self, other)\n            own_blocks = self.all_blocks\n            other_blocks = np.moveaxis(other.all_blocks, 1, 0)\n            new_matrix = []\n            for own_line in own_blocks:\n                new_line = []\n                for other_col in other_blocks:\n                    new_line.append(sum(own_block @ other_block for own_block, other_block in zip(own_line, other_col)))\n                new_matrix.append(new_line)\n            return BlockMatrix(new_matrix, check=False)\n\n        elif isinstance(other, np.ndarray) and self.shape[1] == other.shape[0]:\n            LOG.debug(f\"Multiplication of {self} with a full matrix of shape {other.shape}.\")\n            # Cut the matrix and recursively call itself to use the code above.\n            from capytaine.matrices.builders import cut_matrix\n            cut_other = cut_matrix(other, self.block_shapes[1], [other.shape[1]], check=False)\n            return (self @ cut_other).full_matrix()\n\n    def __matmul__(self, other: Union['BlockMatrix', np.ndarray]) -> Union['BlockMatrix', np.ndarray]:\n        if not (isinstance(other, BlockMatrix) or isinstance(other, np.ndarray)):\n            return NotImplemented\n        elif other.ndim == 2:  # Other is a matrix\n            if other.shape[1] == 1:  # Actually a column vector\n                return self.matvec(other.flatten())\n            else:\n                return self.matmat(other)\n        elif other.ndim == 1:  # Other is a vector\n            return self.matvec(other)\n        else:\n            return NotImplemented\n\n    def astype(self, dtype: np.dtype) -> 'BlockMatrix':\n        return self._apply_unary_op(lambda x: x.astype(dtype))\n\n    def fft_of_list(*block_matrices, check=True):\n        \"\"\"Compute the fft of a list of block matrices of the same type and shape.\n        The output is a list of block matrices of the same shape as the input ones.\n        The fft is computed element-wise, so the block structure does not cause any mathematical difficulty.\n        Returns an array of BlockMatrices.\n        \"\"\"\n        class_of_matrices = type(block_matrices[0])\n        nb_blocks = block_matrices[0]._stored_nb_blocks\n\n        LOG.debug(f\"FFT of {len(block_matrices)} {class_of_matrices.__name__} (stored blocks = {nb_blocks})\")\n\n        if check:\n            # Check the validity of the shapes of the matrices given as input\n            shape = block_matrices[0].shape\n            assert [nb_blocks == matrix._stored_nb_blocks for matrix in block_matrices[1:]]\n            assert [shape == matrix.shape for matrix in block_matrices[1:]]\n            assert [class_of_matrices == type(matrix) for matrix in block_matrices[1:]]\n\n        # Initialize a vector of block matrices without values in the blocks.\n        result = np.empty(len(block_matrices), dtype=object)\n        for i in range(len(block_matrices)):\n            result[i] = class_of_matrices(np.empty(nb_blocks, dtype=object),\n                                          _stored_block_shapes=block_matrices[0]._stored_block_shapes,\n                                          check=False)\n\n        for i_block, j_block in product(range(nb_blocks[0]), range(nb_blocks[1])):\n            list_of_i_j_blocks = [block_matrices[i_matrix]._stored_blocks[i_block, j_block]\n                                  for i_matrix in range(len(block_matrices))]\n\n            if any(isinstance(block, np.ndarray) or isinstance(block, LowRankMatrix) for block in list_of_i_j_blocks):\n                list_of_i_j_blocks = [block if isinstance(block, np.ndarray) else block.full_matrix() for block in list_of_i_j_blocks]\n                fft_of_blocks = np.fft.fft(list_of_i_j_blocks, axis=0)\n            else:\n                fft_of_blocks = BlockMatrix.fft_of_list(*list_of_i_j_blocks, check=False)\n\n            for matrix, computed_block in zip(result, fft_of_blocks):\n                matrix._stored_blocks[i_block, j_block] = computed_block\n\n        return result\n\n    # COMPARISON AND REDUCTION\n\n    def __eq__(self, other: 'BlockMatrix') -> 'BlockMatrix[bool]':\n        from operator import eq\n        return self._apply_binary_op(eq, other)\n\n    def __invert__(self) -> 'BlockMatrix':\n        \"\"\"Boolean not (~)\"\"\"\n        from operator import invert\n        return self._apply_unary_op(invert)\n\n    def __ne__(self, other: 'BlockMatrix') -> 'BlockMatrix[bool]':\n        return ~(self == other)\n\n    def all(self) -> bool:\n        for line in self._stored_blocks:\n            for block in line:\n                if not block.all():\n                    return False\n        return True\n\n    def any(self) -> bool:\n        for line in self._stored_blocks:\n            for block in line:\n                if block.any():\n                    return True\n        return False\n\n    def min(self) -> Number:\n        return min(block.min() for line in self._stored_blocks for block in line)\n\n    def max(self) -> Number:\n        return max(block.max() for line in self._stored_blocks for block in line)\n\n    # DISPLAYING DATA\n\n    @property\n    def str_shape(self):\n        blocks_str = []\n        for line in self.all_blocks:\n            for block in line:\n                if isinstance(block, BlockMatrix):\n                    blocks_str.append(block.str_shape)\n                elif isinstance(block, np.ndarray) or isinstance(block, LowRankMatrix):\n                    blocks_str.append(\"{}\u00d7{}\".format(*block.shape))\n                else:\n                    blocks_str.append(\"?\u00d7?\")\n\n        if len(set(blocks_str)) == 1:\n            return \"{}\u00d7{}\u00d7[\".format(*self.nb_blocks) + blocks_str[0] + \"]\"\n        else:\n            blocks_str = np.array(blocks_str).reshape(self.nb_blocks).tolist()\n            return str(blocks_str).replace(\"'\", \"\")\n\n    def __str__(self):\n        if not hasattr(self, '_str'):\n            args = [self.str_shape]\n            if self.dtype not in [np.float64, float]:\n                args.append(f\"dtype={self.dtype}\")\n            self._str = f\"{self.__class__.__name__}(\" + \", \".join(args) + \")\"\n        return self._str\n\n    def _repr_pretty_(self, p, cycle):\n        p.text(self.__str__())\n\n    display_color = cycle([f'C{i}' for i in range(10)])\n\n    def _patches(self,\n                 global_frame: Union[Tuple[int, int], np.ndarray]\n                 ):\n        \"\"\"Helper function for displaying the shape of the matrix.\n        Recursively returns a list of rectangles representing the sub-blocks of the matrix.\n\n        Uses BlockMatrix.display_color to assign color to the blocks.\n        By default, it cycles through matplotlib default colors.\n        But if display_color is redefined as a callable, it is called with the block as argument.\n\n        Parameters\n        ----------\n        global_frame: tuple of ints\n            coordinates of the origin in the top left corner.\n\n        Returns\n        -------\n        list of matplotlib.patches.Rectangle\n        \"\"\"\n        matplotlib_patches = import_optional_dependency(\"matplotlib.patches\", \"matplotlib\")\n        Rectangle = matplotlib_patches.Rectangle\n\n        all_blocks_in_flat_iterator = (block for line in self._stored_blocks for block in line)\n        positions_of_all_blocks = self._stored_block_positions(global_frame=global_frame)\n        patches = []\n        for block, positions_of_the_block in zip(all_blocks_in_flat_iterator, positions_of_all_blocks):\n            position_of_first_appearance = positions_of_the_block[0]\n            # Exchange coordinates: row index i -> y, column index j -> x\n            position_of_first_appearance = np.array((position_of_first_appearance[1], position_of_first_appearance[0]))\n\n            if isinstance(block, BlockMatrix):\n                patches_of_this_block = block._patches(np.array((position_of_first_appearance[1], position_of_first_appearance[0])))\n            elif isinstance(block, np.ndarray):\n\n                if isinstance(self.display_color, Iterator):\n                    color = next(self.display_color)\n                elif callable(self.display_color):\n                    color = self.display_color(block)\n                else:\n                    color = np.random.rand(3)\n\n                patches_of_this_block = [Rectangle(position_of_first_appearance,\n                                                   block.shape[1], block.shape[0],\n                                                   edgecolor='k', facecolor=color)]\n            elif isinstance(block, LowRankMatrix):\n\n                if isinstance(self.display_color, Iterator):\n                    color = next(self.display_color)\n                elif callable(self.display_color):\n                    color = self.display_color(block)\n                else:\n                    color = np.random.rand(3)\n\n                patches_of_this_block = [\n                    # Left block\n                    Rectangle(position_of_first_appearance,\n                              block.left_matrix.shape[1], block.left_matrix.shape[0],\n                              edgecolor='k', facecolor=color),\n                    # Top block\n                    Rectangle(position_of_first_appearance,\n                              block.right_matrix.shape[1], block.right_matrix.shape[0],\n                              edgecolor='k', facecolor=color),\n                    # Rest of the matrix\n                    Rectangle(position_of_first_appearance,\n                              block.right_matrix.shape[1], block.left_matrix.shape[0],\n                              facecolor=color, alpha=0.2),\n                ]\n            else:\n                raise NotImplementedError()\n\n            patches.extend(patches_of_this_block)\n\n            # For the other appearances, copy the patches of the first appearance\n            for block_position in positions_of_the_block[1:]:\n                block_position = np.array((block_position[1], block_position[0]))\n                for patch in patches_of_this_block:  # A block can be made of several patches.\n                    shift = block_position - position_of_first_appearance\n                    patch_position = np.array(patch.get_xy()) + shift\n                    patches.append(Rectangle(patch_position, patch.get_width(), patch.get_height(),\n                                             facecolor=patch.get_facecolor(), alpha=0.2))\n\n        return patches\n\n    def plot_shape(self):\n        \"\"\"Plot the structure of the matrix using matplotlib.\"\"\"\n        matplotlib = import_optional_dependency(\"matplotlib\")\n        plt = matplotlib.pyplot\n\n        plt.figure()\n        for patch in self._patches((0, 0)):\n            plt.gca().add_patch(patch)\n        plt.axis('equal')\n        plt.xlim(0, self.shape[1])\n        plt.ylim(0, self.shape[0])\n        plt.gca().invert_yaxis()",
  "def __init__(self, blocks, _stored_block_shapes=None, check=True):\n\n        self._stored_nb_blocks = (len(blocks), len(blocks[0]))\n\n        self._stored_blocks = np.empty(self._stored_nb_blocks, dtype=object)\n        for i in range(len(blocks)):\n            for j in range(len(blocks[i])):\n                self._stored_blocks[i, j] = blocks[i][j]\n\n        if _stored_block_shapes is None:\n            self._stored_block_shapes = ([block.shape[0] for block in self._stored_blocks[:, 0]],\n                                         [block.shape[1] for block in self._stored_blocks[0, :]])\n        else:\n            # To avoid going down the tree if it is already known.\n            self._stored_block_shapes = _stored_block_shapes\n\n        # Block shape of the full matrix\n        self.nb_blocks = self._compute_nb_blocks()\n\n        # Total shape of the full matrix\n        self.shape = self._compute_shape()\n\n        LOG.debug(f\"New block matrix: %s\", self)\n\n        if check:\n            assert self._check_dimensions_of_blocks()\n            assert self._check_dtype()",
  "def _compute_shape(self):\n        # In a dedicated routine because it will be overloaded by subclasses.\n        return sum(self._stored_block_shapes[0]), sum(self._stored_block_shapes[1])",
  "def _compute_nb_blocks(self):\n        return self._stored_nb_blocks",
  "def _check_dimensions_of_blocks(self) -> bool:\n        \"\"\"Check that the dimensions of the blocks are consistent.\"\"\"\n        if not all(block.ndim == self.ndim for line in self._stored_blocks for block in line):\n            return False\n\n        for line in self.all_blocks:\n            block_height = line[0].shape[0]\n            for block in line[1:]:\n                if not block.shape[0] == block_height:  # Same height on a given line\n                    return False\n\n        for col in np.moveaxis(self.all_blocks, 1, 0):\n            block_width = col[0].shape[1]\n            for block in col[1:]:\n                if not block.shape[1] == block_width:  # Same width on a given column\n                    return False\n        return True",
  "def _check_dtype(self) -> bool:\n        \"\"\"Check that the type of the blocks are consistent.\"\"\"\n        for line in self._stored_blocks:\n            for block in line:\n                if block.dtype != self.dtype:\n                    return False\n        return True",
  "def dtype(self):\n        \"\"\"The type of data of all of the subblocks.\"\"\"\n        try:\n            return self._stored_blocks[0][0].dtype\n        except AttributeError:\n            return None",
  "def all_blocks(self) -> np.ndarray:\n        \"\"\"The matrix of matrices. For a full block matrix, all the blocks are stored in memory.\"\"\"\n        return self._stored_blocks",
  "def block_shapes(self) -> Tuple[List[int], List[int]]:\n        \"\"\"The shapes of the blocks composing the BlockMatrix.\n\n        Example::\n\n            AAAABB\n            AAAABB  ->  block_shapes = ([3], [4, 2])\n            AAAABB\n        \"\"\"\n        return self._stored_block_shapes",
  "def _stored_block_positions(self, global_frame=(0, 0)) -> Iterable[List[Tuple[int, int]]]:\n        \"\"\"The position of each blocks in the matrix as a generator.\n        The list is used by subclasses where the same block may appear several times in different positions.\n\n        Parameters\n        ----------\n        global_frame: Tuple[int], optional\n            the coordinate of the top right corner. Default: 0, 0.\n\n        Example::\n\n            AAAABB\n            AAAABB  ->  list(matrix._stored_block_positions) = [[(0,0)], [(0, 4)], [(2, 0)], [(2, 4)]]\n            CCCCDD\n        \"\"\"\n        x_acc = accumulate([0] + self.block_shapes[0][:-1])\n        y_acc = accumulate([0] + self.block_shapes[1][:-1])\n        return ([(global_frame[0] + x, global_frame[1] + y)] for x, y in product(x_acc, y_acc))",
  "def _put_in_full_matrix(self, full_matrix: np.ndarray, where=(0, 0)) -> None:\n        \"\"\"Copy the content of the block matrix in a matrix, which is modified in place.\"\"\"\n        all_blocks_in_flat_iterator = (block for line in self._stored_blocks for block in line)\n        positions_of_all_blocks = self._stored_block_positions(global_frame=where)\n        for block, positions_of_the_block in zip(all_blocks_in_flat_iterator, positions_of_all_blocks):\n            if isinstance(block, BlockMatrix):\n                position_of_first_appearance = positions_of_the_block[0]\n                frame_of_first_appearance = (slice(position_of_first_appearance[0], position_of_first_appearance[0]+block.shape[0]),\n                                             slice(position_of_first_appearance[1], position_of_first_appearance[1]+block.shape[1]))\n                block._put_in_full_matrix(full_matrix, where=position_of_first_appearance)\n\n                for position in positions_of_the_block[1:]:  # For the other appearances, only copy the first appearance\n                    block_frame = (slice(position[0], position[0]+block.shape[0]),\n                                   slice(position[1], position[1]+block.shape[1]))\n                    full_matrix[block_frame] = full_matrix[frame_of_first_appearance]\n\n            else:\n                full_block = block if isinstance(block, np.ndarray) else block.full_matrix()\n                for position in positions_of_the_block:\n                    block_frame = (slice(position[0], position[0]+block.shape[0]),\n                                   slice(position[1], position[1]+block.shape[1]))\n                    full_matrix[block_frame] = full_block",
  "def full_matrix(self, dtype=None) -> np.ndarray:\n        \"\"\"Flatten the block structure and return a full matrix.\"\"\"\n        if dtype is None: dtype = self.dtype\n        full_matrix = np.empty(self.shape, dtype=dtype)\n        self._put_in_full_matrix(full_matrix)\n        return full_matrix",
  "def __array__(self, dtype=None):\n        return self.full_matrix(dtype=dtype)",
  "def no_toeplitz(self):\n        \"\"\"Recursively replace the block toeplitz matrices by usual block matrices.\n        WARNING: the block matrices may still contain several references to the same block.\"\"\"\n        blocks = [[block.no_toeplitz() if isinstance(block, BlockMatrix) else block for block in line] for line in self.all_blocks]\n        return BlockMatrix(blocks)",
  "def __deepcopy__(self, memo):\n        from copy import deepcopy\n        blocks = [[deepcopy(block) for block in line] for line in self._stored_blocks]\n        # The call to deepcopy does not use the memo on purpose:\n        # the goal is to replace references to the same block by references to different copies of the block.\n        return self.__class__(blocks)",
  "def stored_data_size(self):\n        \"\"\"Return the number of entries actually stored in memory.\"\"\"\n        size = 0\n        for line in self._stored_blocks:\n            for block in line:\n                if isinstance(block, np.ndarray):\n                    size += np.prod(block.shape)\n                else:\n                    size += block.stored_data_size\n        return size",
  "def density(self):\n        return self.stored_data_size/np.prod(self.shape)",
  "def sparcity(self):\n        return 1 - self.density",
  "def __hash__(self):\n        # Temporary\n        return id(self)",
  "def _apply_unary_op(self, op: Callable) -> 'BlockMatrix':\n        \"\"\"Helper function applying a function recursively on all submatrices.\"\"\"\n        LOG.debug(f\"Apply op {op.__name__} to {self}\")\n        result = [[op(block) for block in line] for line in self._stored_blocks]\n        return self.__class__(result, _stored_block_shapes=self._stored_block_shapes, check=False)",
  "def _apply_binary_op(self, op: Callable, other: 'BlockMatrix') -> 'BlockMatrix':\n        \"\"\"Helper function applying a binary operator recursively on all submatrices.\"\"\"\n        if isinstance(other, self.__class__) and self.nb_blocks == other.nb_blocks:\n            LOG.debug(f\"Apply op {op.__name__} to {self} and {other}\")\n            result = [\n                [op(block, other_block) for block, other_block in zip(line, other_line)]\n                for line, other_line in zip(self._stored_blocks, other._stored_blocks)\n            ]\n            return self.__class__(result, _stored_block_shapes=self._stored_block_shapes, check=False)\n        else:\n            return NotImplemented",
  "def __add__(self, other: 'BlockMatrix') -> 'BlockMatrix':\n        from operator import add\n        return self._apply_binary_op(add, other)",
  "def __radd__(self, other: 'BlockMatrix') -> 'BlockMatrix':\n        return self + other",
  "def __neg__(self) -> 'BlockMatrix':\n        from operator import neg\n        return self._apply_unary_op(neg)",
  "def __sub__(self, other: 'BlockMatrix') -> 'BlockMatrix':\n        from operator import sub\n        return self._apply_binary_op(sub, other)",
  "def __rsub__(self, other: 'BlockMatrix') -> 'BlockMatrix':\n        from operator import sub\n        return other._apply_binary_op(sub, self)",
  "def __mul__(self, other: Union['BlockMatrix', Number]) -> 'BlockMatrix':\n        if isinstance(other, Number):\n            return self._apply_unary_op(lambda x: other*x)\n        else:\n            from operator import mul\n            return self._apply_binary_op(mul, other)",
  "def __rmul__(self, other: Union['BlockMatrix', Number]) -> 'BlockMatrix':\n        return self * other",
  "def __truediv__(self, other: Union['BlockMatrix', Number]) -> 'BlockMatrix':\n        from numbers import Number\n        if isinstance(other, Number):\n            return self._apply_unary_op(lambda x: x/other)\n        else:\n            from operator import truediv\n            return self._apply_binary_op(truediv, other)",
  "def __rtruediv__(self, other: Union['BlockMatrix', Number]) -> 'BlockMatrix':\n        from numbers import Number\n        if isinstance(other, Number):\n            return self._apply_unary_op(lambda x: other/x)\n        else:\n            return self._apply_binary_op(lambda x, y: y/x, other)",
  "def matvec(self, other):\n        \"\"\"Matrix vector product.\n        Named as such to be used as scipy LinearOperator.\"\"\"\n        LOG.debug(f\"Multiplication of {self} with a full vector of size {other.shape}.\")\n        result = np.zeros(self.shape[0], dtype=other.dtype)\n        line_heights = self.block_shapes[0]\n        line_positions = list(accumulate(chain([0], line_heights)))\n        col_widths = self.block_shapes[1]\n        col_positions = list(accumulate(chain([0], col_widths)))\n        for line, line_position, line_height in zip(self.all_blocks, line_positions, line_heights):\n            line_slice = slice(line_position, line_position+line_height)\n            for block, col_position, col_width in zip(line, col_positions, col_widths):\n                col_slice = slice(col_position, col_position+col_width)\n                result[line_slice] += block @ other[col_slice]\n        return result",
  "def rmatvec(self, other):\n        \"\"\"Vector matrix product.\n        Named as such to be used as scipy LinearOperator.\"\"\"\n        LOG.debug(f\"Multiplication of a full vector of size {other.shape} with {self}.\")\n        result = np.zeros(self.shape[1], dtype=other.dtype)\n        line_heights = self.block_shapes[0]\n        line_positions = list(accumulate(chain([0], line_heights)))\n        col_widths = self.block_shapes[1]\n        col_positions = list(accumulate(chain([0], col_widths)))\n        for col, col_position, col_width in zip(self.all_blocks.T, col_positions, col_widths):\n            col_slice = slice(col_position, col_position+col_width)\n            for block, line_position, line_height in zip(col, line_positions, line_heights):\n                line_slice = slice(line_position, line_position+line_height)\n                if isinstance(block, BlockMatrix):\n                    result[col_slice] += block.rmatvec(other[line_slice])\n                else:\n                    result[col_slice] += other[line_slice] @ block\n        return result",
  "def matmat(self, other):\n        \"\"\"Matrix-matrix product.\"\"\"\n        if isinstance(other, BlockMatrix) and self.block_shapes[1] == other.block_shapes[0]:\n            LOG.debug(f\"Multiplication of %s with %s\", self, other)\n            own_blocks = self.all_blocks\n            other_blocks = np.moveaxis(other.all_blocks, 1, 0)\n            new_matrix = []\n            for own_line in own_blocks:\n                new_line = []\n                for other_col in other_blocks:\n                    new_line.append(sum(own_block @ other_block for own_block, other_block in zip(own_line, other_col)))\n                new_matrix.append(new_line)\n            return BlockMatrix(new_matrix, check=False)\n\n        elif isinstance(other, np.ndarray) and self.shape[1] == other.shape[0]:\n            LOG.debug(f\"Multiplication of {self} with a full matrix of shape {other.shape}.\")\n            # Cut the matrix and recursively call itself to use the code above.\n            from capytaine.matrices.builders import cut_matrix\n            cut_other = cut_matrix(other, self.block_shapes[1], [other.shape[1]], check=False)\n            return (self @ cut_other).full_matrix()",
  "def __matmul__(self, other: Union['BlockMatrix', np.ndarray]) -> Union['BlockMatrix', np.ndarray]:\n        if not (isinstance(other, BlockMatrix) or isinstance(other, np.ndarray)):\n            return NotImplemented\n        elif other.ndim == 2:  # Other is a matrix\n            if other.shape[1] == 1:  # Actually a column vector\n                return self.matvec(other.flatten())\n            else:\n                return self.matmat(other)\n        elif other.ndim == 1:  # Other is a vector\n            return self.matvec(other)\n        else:\n            return NotImplemented",
  "def astype(self, dtype: np.dtype) -> 'BlockMatrix':\n        return self._apply_unary_op(lambda x: x.astype(dtype))",
  "def fft_of_list(*block_matrices, check=True):\n        \"\"\"Compute the fft of a list of block matrices of the same type and shape.\n        The output is a list of block matrices of the same shape as the input ones.\n        The fft is computed element-wise, so the block structure does not cause any mathematical difficulty.\n        Returns an array of BlockMatrices.\n        \"\"\"\n        class_of_matrices = type(block_matrices[0])\n        nb_blocks = block_matrices[0]._stored_nb_blocks\n\n        LOG.debug(f\"FFT of {len(block_matrices)} {class_of_matrices.__name__} (stored blocks = {nb_blocks})\")\n\n        if check:\n            # Check the validity of the shapes of the matrices given as input\n            shape = block_matrices[0].shape\n            assert [nb_blocks == matrix._stored_nb_blocks for matrix in block_matrices[1:]]\n            assert [shape == matrix.shape for matrix in block_matrices[1:]]\n            assert [class_of_matrices == type(matrix) for matrix in block_matrices[1:]]\n\n        # Initialize a vector of block matrices without values in the blocks.\n        result = np.empty(len(block_matrices), dtype=object)\n        for i in range(len(block_matrices)):\n            result[i] = class_of_matrices(np.empty(nb_blocks, dtype=object),\n                                          _stored_block_shapes=block_matrices[0]._stored_block_shapes,\n                                          check=False)\n\n        for i_block, j_block in product(range(nb_blocks[0]), range(nb_blocks[1])):\n            list_of_i_j_blocks = [block_matrices[i_matrix]._stored_blocks[i_block, j_block]\n                                  for i_matrix in range(len(block_matrices))]\n\n            if any(isinstance(block, np.ndarray) or isinstance(block, LowRankMatrix) for block in list_of_i_j_blocks):\n                list_of_i_j_blocks = [block if isinstance(block, np.ndarray) else block.full_matrix() for block in list_of_i_j_blocks]\n                fft_of_blocks = np.fft.fft(list_of_i_j_blocks, axis=0)\n            else:\n                fft_of_blocks = BlockMatrix.fft_of_list(*list_of_i_j_blocks, check=False)\n\n            for matrix, computed_block in zip(result, fft_of_blocks):\n                matrix._stored_blocks[i_block, j_block] = computed_block\n\n        return result",
  "def __eq__(self, other: 'BlockMatrix') -> 'BlockMatrix[bool]':\n        from operator import eq\n        return self._apply_binary_op(eq, other)",
  "def __invert__(self) -> 'BlockMatrix':\n        \"\"\"Boolean not (~)\"\"\"\n        from operator import invert\n        return self._apply_unary_op(invert)",
  "def __ne__(self, other: 'BlockMatrix') -> 'BlockMatrix[bool]':\n        return ~(self == other)",
  "def all(self) -> bool:\n        for line in self._stored_blocks:\n            for block in line:\n                if not block.all():\n                    return False\n        return True",
  "def any(self) -> bool:\n        for line in self._stored_blocks:\n            for block in line:\n                if block.any():\n                    return True\n        return False",
  "def min(self) -> Number:\n        return min(block.min() for line in self._stored_blocks for block in line)",
  "def max(self) -> Number:\n        return max(block.max() for line in self._stored_blocks for block in line)",
  "def str_shape(self):\n        blocks_str = []\n        for line in self.all_blocks:\n            for block in line:\n                if isinstance(block, BlockMatrix):\n                    blocks_str.append(block.str_shape)\n                elif isinstance(block, np.ndarray) or isinstance(block, LowRankMatrix):\n                    blocks_str.append(\"{}\u00d7{}\".format(*block.shape))\n                else:\n                    blocks_str.append(\"?\u00d7?\")\n\n        if len(set(blocks_str)) == 1:\n            return \"{}\u00d7{}\u00d7[\".format(*self.nb_blocks) + blocks_str[0] + \"]\"\n        else:\n            blocks_str = np.array(blocks_str).reshape(self.nb_blocks).tolist()\n            return str(blocks_str).replace(\"'\", \"\")",
  "def __str__(self):\n        if not hasattr(self, '_str'):\n            args = [self.str_shape]\n            if self.dtype not in [np.float64, float]:\n                args.append(f\"dtype={self.dtype}\")\n            self._str = f\"{self.__class__.__name__}(\" + \", \".join(args) + \")\"\n        return self._str",
  "def _repr_pretty_(self, p, cycle):\n        p.text(self.__str__())",
  "def _patches(self,\n                 global_frame: Union[Tuple[int, int], np.ndarray]\n                 ):\n        \"\"\"Helper function for displaying the shape of the matrix.\n        Recursively returns a list of rectangles representing the sub-blocks of the matrix.\n\n        Uses BlockMatrix.display_color to assign color to the blocks.\n        By default, it cycles through matplotlib default colors.\n        But if display_color is redefined as a callable, it is called with the block as argument.\n\n        Parameters\n        ----------\n        global_frame: tuple of ints\n            coordinates of the origin in the top left corner.\n\n        Returns\n        -------\n        list of matplotlib.patches.Rectangle\n        \"\"\"\n        matplotlib_patches = import_optional_dependency(\"matplotlib.patches\", \"matplotlib\")\n        Rectangle = matplotlib_patches.Rectangle\n\n        all_blocks_in_flat_iterator = (block for line in self._stored_blocks for block in line)\n        positions_of_all_blocks = self._stored_block_positions(global_frame=global_frame)\n        patches = []\n        for block, positions_of_the_block in zip(all_blocks_in_flat_iterator, positions_of_all_blocks):\n            position_of_first_appearance = positions_of_the_block[0]\n            # Exchange coordinates: row index i -> y, column index j -> x\n            position_of_first_appearance = np.array((position_of_first_appearance[1], position_of_first_appearance[0]))\n\n            if isinstance(block, BlockMatrix):\n                patches_of_this_block = block._patches(np.array((position_of_first_appearance[1], position_of_first_appearance[0])))\n            elif isinstance(block, np.ndarray):\n\n                if isinstance(self.display_color, Iterator):\n                    color = next(self.display_color)\n                elif callable(self.display_color):\n                    color = self.display_color(block)\n                else:\n                    color = np.random.rand(3)\n\n                patches_of_this_block = [Rectangle(position_of_first_appearance,\n                                                   block.shape[1], block.shape[0],\n                                                   edgecolor='k', facecolor=color)]\n            elif isinstance(block, LowRankMatrix):\n\n                if isinstance(self.display_color, Iterator):\n                    color = next(self.display_color)\n                elif callable(self.display_color):\n                    color = self.display_color(block)\n                else:\n                    color = np.random.rand(3)\n\n                patches_of_this_block = [\n                    # Left block\n                    Rectangle(position_of_first_appearance,\n                              block.left_matrix.shape[1], block.left_matrix.shape[0],\n                              edgecolor='k', facecolor=color),\n                    # Top block\n                    Rectangle(position_of_first_appearance,\n                              block.right_matrix.shape[1], block.right_matrix.shape[0],\n                              edgecolor='k', facecolor=color),\n                    # Rest of the matrix\n                    Rectangle(position_of_first_appearance,\n                              block.right_matrix.shape[1], block.left_matrix.shape[0],\n                              facecolor=color, alpha=0.2),\n                ]\n            else:\n                raise NotImplementedError()\n\n            patches.extend(patches_of_this_block)\n\n            # For the other appearances, copy the patches of the first appearance\n            for block_position in positions_of_the_block[1:]:\n                block_position = np.array((block_position[1], block_position[0]))\n                for patch in patches_of_this_block:  # A block can be made of several patches.\n                    shift = block_position - position_of_first_appearance\n                    patch_position = np.array(patch.get_xy()) + shift\n                    patches.append(Rectangle(patch_position, patch.get_width(), patch.get_height(),\n                                             facecolor=patch.get_facecolor(), alpha=0.2))\n\n        return patches",
  "def plot_shape(self):\n        \"\"\"Plot the structure of the matrix using matplotlib.\"\"\"\n        matplotlib = import_optional_dependency(\"matplotlib\")\n        plt = matplotlib.pyplot\n\n        plt.figure()\n        for patch in self._patches((0, 0)):\n            plt.gca().add_patch(patch)\n        plt.axis('equal')\n        plt.xlim(0, self.shape[1])\n        plt.ylim(0, self.shape[0])\n        plt.gca().invert_yaxis()",
  "def _get_water_depth(free_surface, water_depth, sea_bottom, default_water_depth=np.infty):\n    if water_depth is None and sea_bottom is None:\n        return default_water_depth\n    elif water_depth is not None and sea_bottom is None:\n        if water_depth <= 0.0:\n            raise ValueError(f\"`water_depth` should be stricly positive. Received value: {water_depth}\")\n        return float(water_depth)\n    elif water_depth is None and sea_bottom is not None:\n        LOG.warning(\"To uniformize notations througouth Capytaine, setting `water_depth` is preferred to `sea_bottom` since version 2.0.\")\n        return float(free_surface - sea_bottom)\n    else:\n        raise ValueError(\"Cannot give both a `water_depth` and a `sea_bottom`.\")",
  "def exponential_decomposition(X, F, m):\n    \"\"\"Use Prony's method to approximate the sampled real function F=f(X) as a sum of m\n    exponential functions x \u2192 \u03a3 a_i exp(lamda_i x).\n\n    Parameters\n    ----------\n    X: 1D array\n        sampling points.\n    F: 1D array (same size as X)\n        values of the function to approximate at the points of x.\n    m: integer\n        number of exponential functions\n\n    Return\n    ------\n    a: 1D array (size m)\n        coefficients of the exponentials\n    lamda: 1D array (size m)\n        growth rate of the exponentials\n    \"\"\"\n    assert X.shape == F.shape\n\n    # Compute the coefficients of the polynomials of Prony's method\n    A = toeplitz(c=F[m-1:-1], r=F[:m][::-1])\n    P, *_ = np.linalg.lstsq(A, F[m:], rcond=None)\n\n    # Build and solve polynomial function\n    coeffs = np.ones(m+1)\n    # coeffs[:m] = -P[::-1]\n    for i in range(m):\n        coeffs[m-i-1] = -P[i]\n    roots = polynomial.polyroots(coeffs)\n\n    # Discard values where log is undefined\n    roots = roots[np.logical_or(np.imag(roots) != 0.0, np.real(roots) >= 0.0)]\n\n    # Deduce lamda and keep only interesting values\n    lamda = np.real(np.log(roots)/(X[1] - X[0]))\n    lamda = np.unique(lamda)\n    lamda = lamda[np.logical_and(-20.0 < lamda, lamda < 0.0)]\n\n    # Fit the values of 'a' on the curve\n    def f(x, *ar):\n        ar = np.asarray(ar)[:, np.newaxis]\n        la = lamda[:, np.newaxis]\n        return np.sum(ar * np.exp(la * x), axis=0)\n    a, *_ = curve_fit(f, X, F, p0=np.zeros(lamda.shape))\n\n    return a, lamda",
  "def error_exponential_decomposition(X, F, a, lamda):\n    \"\"\"Compare exponential decomposition defined by the coefficients a and lamda to the reference\n    values in F.\n\n    Parameters\n    ----------\n    X: 1D array\n        sampling points\n    F: 1D array (same size as X)\n        reference values\n    a: 1D array\n        coefficients of the exponentials\n    lamda: 1D array (same size as a)\n        growth rate of the exponentials\n\n    Returns\n    -------\n    error: float\n        mean square error of the decomposition\n    \"\"\"\n    a = np.asarray(a)[:, np.newaxis]\n    lamda = np.asarray(lamda)[:, np.newaxis]\n\n    def f(x):\n        return np.sum(a * np.exp(lamda*x), axis=0)\n\n    return np.square(f(X) - F).mean()",
  "def f(x, *ar):\n        ar = np.asarray(ar)[:, np.newaxis]\n        la = lamda[:, np.newaxis]\n        return np.sum(ar * np.exp(la * x), axis=0)",
  "def f(x):\n        return np.sum(a * np.exp(lamda*x), axis=0)",
  "def _normalize_points(points, keep_mesh=False):\n    if isinstance(points, (FloatingBody, FreeSurface)):\n        if keep_mesh:\n            return points.mesh, (points.mesh.nb_faces,)\n        else:\n            return points.mesh.faces_centers, (points.mesh.nb_faces,)\n\n    if isinstance(points, (Mesh, CollectionOfMeshes)):\n        if keep_mesh:\n            return points, (points.nb_faces,)\n        else:\n            return points.faces_centers, (points.nb_faces,)\n\n    points = np.asarray(points)\n\n    if points.ndim == 1:  # A single point has been provided\n        output_shape = (1,)\n        points = points.reshape((1, points.shape[0]))\n\n    elif points.ndim == 2:\n        output_shape = (points.shape[0],)\n\n    elif points.ndim > 2:\n        # `points` is expected to be the resuls of a meshgrid. Points has shape (d, nx, ny, ...)\n        output_shape = points.shape[1:]\n        points = points.reshape(points.shape[0], -1).transpose()\n        # points is now a (nx*ny*... , d) array\n\n    else:\n        raise ValueError(f\"Expected a list of points or a mesh, but got instead: {points}\")\n\n    return points, output_shape",
  "def _normalize_free_surface_points(points, keep_mesh=False):\n    if keep_mesh and isinstance(points, (FloatingBody, FreeSurface)):\n        return points.mesh, (points.mesh.nb_faces,)\n\n    if keep_mesh and isinstance(points, (Mesh, CollectionOfMeshes)):\n        return points, (points.nb_faces,)\n\n    points, output_shape = _normalize_points(points, keep_mesh)\n\n    if points.ndim == 2 and points.shape[1] == 2:  # Only x and y have been provided\n        points = np.concatenate([points, np.zeros((points.shape[0], 1))], axis=1)\n\n    return points, output_shape",
  "def delete_first_lru_cache(maxsize=1):\n    \"\"\"Behaves like functools.lru_cache(), but the oldest data in the cache is\n    deleted *before* computing a new one, in order to limit RAM usage when\n    stored objects are big.\"\"\"\n\n    def decorator(f):\n        cache = OrderedDict()\n\n        @wraps(f)\n        def decorated_f(*args, **kwargs):\n            # /!\\ cache only args\n\n            if args in cache:\n                # Get item in cache\n                return cache[args]\n\n            if len(cache) + 1 > maxsize:\n                # Drop oldest item in cache.\n                cache.popitem(last=False)\n\n            # Compute and store\n            result = f(*args, **kwargs)\n            cache[args] = result\n\n            return result\n\n        return decorated_f\n\n    return decorator",
  "def decorator(f):\n        cache = OrderedDict()\n\n        @wraps(f)\n        def decorated_f(*args, **kwargs):\n            # /!\\ cache only args\n\n            if args in cache:\n                # Get item in cache\n                return cache[args]\n\n            if len(cache) + 1 > maxsize:\n                # Drop oldest item in cache.\n                cache.popitem(last=False)\n\n            # Compute and store\n            result = f(*args, **kwargs)\n            cache[args] = result\n\n            return result\n\n        return decorated_f",
  "def decorated_f(*args, **kwargs):\n            # /!\\ cache only args\n\n            if args in cache:\n                # Get item in cache\n                return cache[args]\n\n            if len(cache) + 1 > maxsize:\n                # Drop oldest item in cache.\n                cache.popitem(last=False)\n\n            # Compute and store\n            result = f(*args, **kwargs)\n            cache[args] = result\n\n            return result",
  "def import_optional_dependency(module_name: str, package_name: str = None):\n    try:\n        module = importlib.import_module(module_name)\n    except ImportError:\n        if package_name is None:\n            package_name = module_name\n\n        message = (\n            f\"Missing optional dependency '{module_name}'. \"\n            f\"Use pip or conda to install {package_name}.\"\n        )\n        raise ImportError(message) from None\n\n    return module",
  "def silently_import_optional_dependency(module_name: str):\n    # Same as above, except it does not raise a exception when the module is not found.\n    # Instead, simply returns None.\n    try:\n        module = importlib.import_module(module_name)\n    except ImportError:\n        module = None\n    return module",
  "def airy_waves_potential(points, pb):\n    \"\"\"Compute the potential for Airy waves at a given point (or array of points).\n\n    Parameters\n    ----------\n    points: array of shape (3) or (N x 3)\n        coordinates of the points in which to evaluate the potential.\n    pb: DiffractionProblem\n        problem with the environmental conditions (g, rho, ...) of interest\n\n    Returns\n    -------\n    array of shape (1) or (N x 1)\n        The potential\n    \"\"\"\n    points, output_shape = _normalize_points(points)\n\n    x, y, z = points.T\n    k = pb.wavenumber\n    h = pb.water_depth\n    wbar = x * np.cos(pb.wave_direction) + y * np.sin(pb.wave_direction)\n\n    if 0 <= k*h < 20:\n        cih = np.cosh(k*(z+h))/np.cosh(k*h)\n        # sih = np.sinh(k*(z+h))/np.cosh(k*h)\n    else:\n        cih = np.exp(k*z)\n        # sih = np.exp(k*z)\n\n    phi = -1j*pb.g/pb.omega * cih * np.exp(1j * k * wbar)\n    return phi.reshape(output_shape)",
  "def airy_waves_velocity(points, pb):\n    \"\"\"Compute the fluid velocity for Airy waves at a given point (or array of points).\n\n    Parameters\n    ----------\n    points: array of shape (3) or (N x 3)\n        coordinates of the points in which to evaluate the potential.\n    pb: DiffractionProblem\n        problem with the environmental conditions (g, rho, ...) of interest\n\n    Returns\n    -------\n    array of shape (3) or (N x 3)\n        the velocity vectors\n    \"\"\"\n\n    points, output_shape = _normalize_points(points)\n\n    x, y, z = points.T\n    k = pb.wavenumber\n    h = pb.water_depth\n\n    wbar = x * np.cos(pb.wave_direction) + y * np.sin(pb.wave_direction)\n\n    if 0 <= k*h < 20:\n        cih = np.cosh(k*(z+h))/np.cosh(k*h)\n        sih = np.sinh(k*(z+h))/np.cosh(k*h)\n    else:\n        cih = np.exp(k*z)\n        sih = np.exp(k*z)\n\n    v = pb.g*k/pb.omega * \\\n        np.exp(1j * k * wbar) * \\\n        np.array([np.cos(pb.wave_direction) * cih, np.sin(pb.wave_direction) * cih, -1j * sih])\n\n    return v.T.reshape((*output_shape, 3))",
  "def airy_waves_pressure(points, pb):\n    return 1j * pb.omega * pb.rho * airy_waves_potential(points, pb)",
  "def froude_krylov_force(pb):\n    return pb.body.integrate_pressure(airy_waves_pressure(pb.body.mesh.faces_centers, pb))",
  "def airy_waves_free_surface_elevation(points, pb):\n    \"\"\"Compute the free surface elevation at points of the undisturbed Airy waves\n\n    Parameters\n    ----------\n    points: array of shape (3) or (N \u00d7 3) or (2) or (N \u00d7 2)\n        coordinates of the points in which to evaluate the potential.\n        If only two coordinates are passed, the last one is filled with zeros.\n    pb: DiffractionProblem\n        problem with the environmental conditions (g, rho, ...) of interest\n\n    Returns\n    -------\n    complex-valued array of shape (1,) or (N,)\n        the free surface elevations\n    \"\"\"\n    points, output_shape = _normalize_free_surface_points(points)\n    return 1j * pb.omega / pb.g * airy_waves_potential(points, pb).reshape(output_shape)",
  "class LinearPotentialFlowProblem:\n    \"\"\"General class of a potential flow problem.\n\n    At most one of the following parameter must be provided: omega, period, wavenumber or wavelength.\n    Internally only omega is stored, hence setting another parameter can lead to small rounding errors.\n\n    Parameters\n    ----------\n    body: FloatingBody, optional\n        The body interacting with the waves\n    free_surface: float, optional\n        The position of the free surface (accepted values: 0 and np.infty)\n    water_depth: float, optional\n        The depth of water in m (default: np.infty)\n    sea_bottom: float, optional\n        The position of the sea bottom (deprecated: please prefer setting water_depth)\n    omega: float, optional\n        The angular frequency of the waves in rad/s\n    period: float, optional\n        The period of the waves in s\n    wavenumber: float, optional\n        The angular wave number of the waves in rad/m\n    wavelength: float, optional\n        The wave length of the waves in m\n    rho: float, optional\n        The density of water in kg/m3 (default: 1000.0)\n    g: float, optional\n        The acceleration of gravity in m/s2 (default: 9.81)\n    boundary_condition: np.ndarray of shape (body.mesh.nb_faces,), optional\n        The Neumann boundary condition on the floating body\n    \"\"\"\n\n    def __init__(self, *,\n                 body=None,\n                 free_surface=_default_parameters['free_surface'],\n                 water_depth=None, sea_bottom=None,\n                 omega=None, period=None, wavenumber=None, wavelength=None,\n                 rho=_default_parameters['rho'],\n                 g=_default_parameters['g'],\n                 boundary_condition=None):\n\n        self.body = body\n        self.free_surface = float(free_surface)\n        self.rho = float(rho)\n        self.g = float(g)\n\n        self.boundary_condition = boundary_condition\n\n        self.water_depth = _get_water_depth(free_surface, water_depth, sea_bottom, default_water_depth=_default_parameters[\"water_depth\"])\n        self.omega, self.period, self.wavenumber, self.wavelength, self.provided_freq_type = \\\n                self._get_frequencies(omega, period, wavenumber, wavelength)\n\n        self._check_data()\n\n    def _get_frequencies(self, omega, period, wavenumber, wavelength):\n        frequency_data = dict(omega=omega, period=period, wavenumber=wavenumber, wavelength=wavelength)\n        nb_provided_frequency_data = 4 - list(frequency_data.values()).count(None)\n\n        if nb_provided_frequency_data > 1:\n            raise ValueError(\"Settings a problem requires at most one of the following: omega (angular frequency) OR period OR wavenumber OR wavelength.\\n\"\n                             \"Received {} of them: {}\".format(nb_provided_frequency_data, {k: v for k, v in frequency_data.items() if v is not None}))\n\n        if nb_provided_frequency_data == 0:\n            provided_freq_type = 'omega'\n            frequency_data = {'omega': _default_parameters['omega']}\n        else:\n            provided_freq_type = [k for k, v in frequency_data.items() if v is not None][0]\n\n        if frequency_data[provided_freq_type] in {0.0, np.infty}:\n            raise NotImplementedError(\"Zero and infinite frequencies are currently not supported.\")\n\n\n        if provided_freq_type in {'omega', 'period'}:\n            if provided_freq_type == 'omega':\n                omega = frequency_data['omega']\n                period = 2*np.pi/omega\n            else:  # provided_freq_type is 'period'\n                period = frequency_data['period']\n                omega = 2*np.pi/period\n\n            if self.water_depth == np.infty:\n                wavenumber = omega**2/self.g\n            else:\n                wavenumber = newton(lambda k: k*np.tanh(k*self.water_depth) - omega**2/self.g, x0=1.0)\n            wavelength = 2*np.pi/wavenumber\n\n        else:  # provided_freq_type is 'wavelength' or 'wavenumber'\n            if provided_freq_type == 'wavelength':\n                wavelength = frequency_data['wavelength']\n                wavenumber = 2*np.pi/wavelength\n            else:  # provided_freq_type is 'wavenumber'\n                wavenumber = frequency_data['wavenumber']\n                wavelength = 2*np.pi/wavenumber\n\n            omega = np.sqrt(self.g*wavenumber*np.tanh(wavenumber*self.water_depth))\n            period = 2*np.pi/omega\n\n        return omega, period, wavenumber, wavelength, provided_freq_type\n\n    def _check_data(self):\n        \"\"\"Sanity checks on the data.\"\"\"\n\n        if self.free_surface not in {0.0, np.infty}:\n            raise NotImplementedError(\n                f\"Free surface is {self.free_surface}. \"\n                \"Only z=0 and z=\u221e are accepted values for the free surface position.\"\n            )\n\n        if self.free_surface == np.infty and self.water_depth != np.infty:\n            raise NotImplementedError(\n                \"Problems with a sea bottom but no free surface have not been implemented.\"\n            )\n\n        if self.water_depth < 0.0:\n            raise ValueError(\"`water_depth` should be stricly positive.\")\n\n        if self.omega in {0, np.infty} and self.water_depth != np.infty:\n            raise NotImplementedError(\n                f\"omega={self.omega} is only implemented for infinite depth.\"\n            )\n\n        if self.body is not None:\n            if ((isinstance(self.body.mesh, CollectionOfMeshes) and len(self.body.mesh) == 0)\n                    or len(self.body.mesh.faces) == 0):\n                raise ValueError(f\"The mesh of the body {self.body.name} is empty.\")\n\n            if (any(self.body.mesh.faces_centers[:, 2] >= self.free_surface)\n                    or any(self.body.mesh.faces_centers[:, 2] <= -self.water_depth)):\n\n                LOG.warning(\n                    f\"The mesh of the body {self.body.name} is not inside the domain.\\n\"\n                    \"Check the position of the free_surface and the water_depth\\n\"\n                    \"or use body.keep_immersed_part() to clip the mesh.\"\n                )\n\n            if self.wavelength < self.body.minimal_computable_wavelength:\n                LOG.warning(f\"Mesh resolution for {self}:\\n\"\n                        f\"The resolution of the mesh '{self.body.mesh.name}' of the body '{self.body.name}' \"\n                        f\"might be insufficient for the wavelength \u03bb={self.wavelength:.2e}.\\n\"\n                        f\"This warning appears because the largest panel of this mesh has radius {self.body.mesh.faces_radiuses.max():.2e} > \u03bb/8.\"\n                        )\n\n        if self.boundary_condition is not None:\n            if len(self.boundary_condition.shape) != 1:\n                raise ValueError(\"Expected a 1-dimensional array as boundary_condition\")\n\n            if self.boundary_condition.shape[0] != self.body.mesh.nb_faces:\n                raise ValueError(\n                    f\"The shape of the boundary condition ({self.boundary_condition.shape})\"\n                    f\"does not match the number of faces of the mesh ({self.body.mesh.nb_faces}).\"\n                )\n\n    @property\n    def body_name(self):\n        return self.body.name if self.body is not None else 'None'\n\n    def _asdict(self):\n        return {\"body_name\": self.body_name,\n                \"water_depth\": self.water_depth,\n                \"omega\": self.omega,\n                \"period\": self.period,\n                \"wavelength\": self.wavelength,\n                \"wavenumber\": self.wavenumber,\n                \"rho\": self.rho,\n                \"g\": self.g}\n\n    @staticmethod\n    def _group_for_parallel_resolution(problems):\n        \"\"\"Given a list of problems, returns a list of groups of problems, such\n        that each group should be executed in the same process to benefit from\n        caching.\n        \"\"\"\n        problems_params = pd.DataFrame([pb._asdict() for pb in problems])\n        groups_of_indices = problems_params.groupby([\"body_name\", \"water_depth\", \"omega\", \"rho\", \"g\"]).groups.values()\n        groups_of_problems = [[problems[i] for i in grp] for grp in groups_of_indices]\n        return groups_of_problems\n\n    def __str__(self):\n        \"\"\"Do not display default values in str(problem).\"\"\"\n        parameters = [f\"body={self.body.__short_str__() if self.body is not None else None}\",\n                      f\"{self.provided_freq_type}={self.__getattribute__(self.provided_freq_type):.3f}\",\n                      f\"water_depth={self.water_depth}\"]\n        try:\n            parameters.extend(self._str_other_attributes())\n        except AttributeError:\n            pass\n\n        if not self.free_surface == _default_parameters['free_surface']:\n            parameters.append(f\"free_surface={self.free_surface}\")\n        if not self.g == _default_parameters['g']:\n            parameters.append(f\"g={self.g}\")\n        if not self.rho == _default_parameters['rho']:\n            parameters.append(f\"rho={self.rho}\")\n\n        return self.__class__.__name__ + \"(\" + ', '.join(parameters) + \")\"\n\n    def __repr__(self):\n        return self.__str__()\n\n    def _repr_pretty_(self, p, cycle):\n        p.text(self.__str__())\n\n    def __rich_repr__(self):\n        yield \"body\", self.body, None\n        yield self.provided_freq_type, self.__getattribute__(self.provided_freq_type)\n        yield \"water_depth\", self.water_depth, _default_parameters[\"water_depth\"]\n        try:\n            yield from self._specific_rich_repr()\n        except:\n            pass\n        yield \"g\", self.g, _default_parameters[\"g\"]\n        yield \"rho\", self.rho, _default_parameters[\"rho\"]\n\n    def _astuple(self):\n        return (self.body, self.free_surface, self.water_depth,\n                self.omega, self.period, self.wavenumber, self.wavelength,\n                self.rho, self.g)\n\n    def __eq__(self, other):\n        if isinstance(other, LinearPotentialFlowProblem):\n            return self._astuple() == other._astuple()\n        else:\n            return NotImplemented\n\n    def __lt__(self, other):\n        # Arbitrary order. Used for ordering of problems: problems with same body are grouped together.\n        if isinstance(other, LinearPotentialFlowProblem):\n            return self._astuple()[:9] < other._astuple()[:9]\n            # Not the whole tuple, because when using inheriting classes,\n            # \"radiating_dof\" cannot be compared with \"wave_direction\"\n        else:\n            return NotImplemented\n\n    @property\n    def depth(self):\n        return self.water_depth\n\n    @property\n    def influenced_dofs(self):\n        # TODO: let the user choose the influenced dofs\n        return self.body.dofs if self.body is not None else set()\n\n    def make_results_container(self):\n        return LinearPotentialFlowResult(self)",
  "class DiffractionProblem(LinearPotentialFlowProblem):\n    \"\"\"Particular LinearPotentialFlowProblem with boundary conditions\n    computed from an incoming Airy wave.\"\"\"\n\n    def __init__(self, *,\n                 body=None,\n                 free_surface=_default_parameters['free_surface'],\n                 water_depth=None, sea_bottom=None,\n                 omega=None, period=None, wavenumber=None, wavelength=None,\n                 rho=_default_parameters['rho'],\n                 g=_default_parameters['g'],\n                 wave_direction=_default_parameters['wave_direction']):\n\n        self.wave_direction = float(wave_direction)\n\n        super().__init__(body=body, free_surface=free_surface, water_depth=water_depth, sea_bottom=sea_bottom,\n                         omega=omega, period=period, wavenumber=wavenumber, wavelength=wavelength, rho=rho, g=g)\n\n        if not (-2*np.pi-1e-3 <= self.wave_direction <= 2*np.pi+1e-3):\n            LOG.warning(f\"The value {self.wave_direction} has been provided for the wave direction, and it does not look like an angle in radians. \"\n                         \"The wave direction in Capytaine is defined in radians and not in degrees, so the result might not be what you expect. \"\n                         \"If you were actually giving an angle in radians, use the modulo operator to give a value between -2\u03c0 and 2\u03c0 to disable this warning.\")\n\n        if self.body is not None:\n\n            self.boundary_condition = -(\n                    airy_waves_velocity(self.body.mesh.faces_centers, self)\n                    * self.body.mesh.faces_normals\n            ).sum(axis=1)\n\n            if len(self.body.dofs) == 0:\n                LOG.warning(f\"The body {self.body.name} used in diffraction problem has no dofs!\")\n\n    def _astuple(self):\n        return super()._astuple() + (self.wave_direction,)\n\n    def _asdict(self):\n        d = super()._asdict()\n        d[\"wave_direction\"] = self.wave_direction\n        return d\n\n    def _str_other_attributes(self):\n        return [f\"wave_direction={self.wave_direction:.3f}\"]\n\n    def _specific_rich_repr(self):\n        yield \"wave_direction\", self.wave_direction, _default_parameters[\"wave_direction\"]\n\n    def make_results_container(self, *args, **kwargs):\n        return DiffractionResult(self, *args, **kwargs)",
  "class RadiationProblem(LinearPotentialFlowProblem):\n    \"\"\"Particular LinearPotentialFlowProblem whose boundary conditions have\n    been computed from the degree of freedom of the body.\"\"\"\n\n    def __init__(self, *, body=None,\n                 free_surface=_default_parameters['free_surface'],\n                 water_depth=None, sea_bottom=None,\n                 omega=None, period=None, wavenumber=None, wavelength=None,\n                 rho=_default_parameters['rho'],\n                 g=_default_parameters['g'],\n                 radiating_dof=None):\n\n        self.radiating_dof = radiating_dof\n\n        super().__init__(body=body, free_surface=free_surface, water_depth=water_depth, sea_bottom=sea_bottom,\n                         omega=omega, period=period, wavenumber=wavenumber, wavelength=wavelength, rho=rho, g=g)\n\n        if self.body is not None:\n\n            if len(self.body.dofs) == 0:\n                raise ValueError(f\"Body {self.body.name} does not have any degrees of freedom.\")\n\n            if self.radiating_dof is None:\n                self.radiating_dof = next(iter(self.body.dofs))\n\n            if self.radiating_dof not in self.body.dofs:\n                LOG.error(f\"In {self}: the radiating degree of freedom {self.radiating_dof} is not one of\"\n                          f\"the degrees of freedom of the body.\\n\"\n                          f\"The dofs of the body are {list(self.body.dofs.keys())}\")\n                raise ValueError(\"Unrecognized degree of freedom name.\")\n\n            dof = self.body.dofs[self.radiating_dof]\n            self.boundary_condition = -1j*self.omega * np.sum(dof * self.body.mesh.faces_normals, axis=1)\n\n    def _astuple(self):\n        return super()._astuple() + (self.radiating_dof,)\n\n    def _asdict(self):\n        d = super()._asdict()\n        d[\"radiating_dof\"] = self.radiating_dof\n        return d\n\n    def _str_other_attributes(self):\n        return [f\"radiating_dof=\\'{self.radiating_dof}\\'\"]\n\n    def _specific_rich_repr(self):\n        yield \"radiating_dof\", self.radiating_dof\n\n    def make_results_container(self, *args, **kwargs):\n        return RadiationResult(self, *args, **kwargs)",
  "class LinearPotentialFlowResult:\n\n    def __init__(self, problem, forces=None, sources=None, potential=None, pressure=None):\n        self.problem = problem\n\n        self.sources = sources\n        self.potential = potential\n        self.pressure = pressure\n        self.fs_elevation = {}\n\n        # Copy data from problem\n        self.body               = self.problem.body\n        self.free_surface       = self.problem.free_surface\n        self.omega              = self.problem.omega\n        self.rho                = self.problem.rho\n        self.g                  = self.problem.g\n        self.boundary_condition = self.problem.boundary_condition\n        self.water_depth        = self.problem.water_depth\n        self.depth              = self.problem.water_depth\n        self.wavenumber         = self.problem.wavenumber\n        self.wavelength         = self.problem.wavelength\n        self.period             = self.problem.period\n        self.provided_freq_type = self.problem.provided_freq_type\n        self.body_name          = self.problem.body_name\n        self.influenced_dofs    = self.problem.influenced_dofs\n\n        if forces is not None:\n            for dof in self.influenced_dofs:\n                self.store_force(dof, forces[dof])\n\n    def store_force(self, dof, force):\n        pass  # Implemented in sub-classes\n\n    __str__ = LinearPotentialFlowProblem.__str__\n    __repr__ = LinearPotentialFlowProblem.__repr__\n    _repr_pretty_ = LinearPotentialFlowProblem._repr_pretty_\n    __rich_repr__ = LinearPotentialFlowProblem.__rich_repr__",
  "class DiffractionResult(LinearPotentialFlowResult):\n\n    def __init__(self, problem, *args, **kwargs):\n        self.forces = {}\n        super().__init__(problem, *args, **kwargs)\n        self.wave_direction = self.problem.wave_direction\n\n    _str_other_attributes = DiffractionProblem._str_other_attributes\n    _specific_rich_repr = DiffractionProblem._specific_rich_repr\n\n    def store_force(self, dof, force):\n        self.forces[dof] = force\n\n    @property\n    def records(self):\n        params = self.problem._asdict()\n        FK = froude_krylov_force(self.problem)\n        return [dict(**params,\n                     influenced_dof=dof,\n                     diffraction_force=self.forces[dof],\n                     Froude_Krylov_force=FK[dof])\n                for dof in self.influenced_dofs]",
  "class RadiationResult(LinearPotentialFlowResult):\n\n    def __init__(self, problem, *args, **kwargs):\n        self.added_masses = {}\n        self.radiation_dampings = {}\n        super().__init__(problem, *args, **kwargs)\n        self.radiating_dof = self.problem.radiating_dof\n\n    _str_other_attributes = RadiationProblem._str_other_attributes\n    _specific_rich_repr = RadiationProblem._specific_rich_repr\n\n    def store_force(self, dof, force):\n        self.added_masses[dof] = force.real/self.omega**2\n        self.radiation_dampings[dof] = force.imag/self.omega\n\n    @property\n    def records(self):\n        params = self.problem._asdict()\n        return [dict(params,\n                     influenced_dof=dof,\n                     added_mass=self.added_masses[dof],\n                     radiation_damping=self.radiation_dampings[dof])\n                for dof in self.influenced_dofs]",
  "def __init__(self, *,\n                 body=None,\n                 free_surface=_default_parameters['free_surface'],\n                 water_depth=None, sea_bottom=None,\n                 omega=None, period=None, wavenumber=None, wavelength=None,\n                 rho=_default_parameters['rho'],\n                 g=_default_parameters['g'],\n                 boundary_condition=None):\n\n        self.body = body\n        self.free_surface = float(free_surface)\n        self.rho = float(rho)\n        self.g = float(g)\n\n        self.boundary_condition = boundary_condition\n\n        self.water_depth = _get_water_depth(free_surface, water_depth, sea_bottom, default_water_depth=_default_parameters[\"water_depth\"])\n        self.omega, self.period, self.wavenumber, self.wavelength, self.provided_freq_type = \\\n                self._get_frequencies(omega, period, wavenumber, wavelength)\n\n        self._check_data()",
  "def _get_frequencies(self, omega, period, wavenumber, wavelength):\n        frequency_data = dict(omega=omega, period=period, wavenumber=wavenumber, wavelength=wavelength)\n        nb_provided_frequency_data = 4 - list(frequency_data.values()).count(None)\n\n        if nb_provided_frequency_data > 1:\n            raise ValueError(\"Settings a problem requires at most one of the following: omega (angular frequency) OR period OR wavenumber OR wavelength.\\n\"\n                             \"Received {} of them: {}\".format(nb_provided_frequency_data, {k: v for k, v in frequency_data.items() if v is not None}))\n\n        if nb_provided_frequency_data == 0:\n            provided_freq_type = 'omega'\n            frequency_data = {'omega': _default_parameters['omega']}\n        else:\n            provided_freq_type = [k for k, v in frequency_data.items() if v is not None][0]\n\n        if frequency_data[provided_freq_type] in {0.0, np.infty}:\n            raise NotImplementedError(\"Zero and infinite frequencies are currently not supported.\")\n\n\n        if provided_freq_type in {'omega', 'period'}:\n            if provided_freq_type == 'omega':\n                omega = frequency_data['omega']\n                period = 2*np.pi/omega\n            else:  # provided_freq_type is 'period'\n                period = frequency_data['period']\n                omega = 2*np.pi/period\n\n            if self.water_depth == np.infty:\n                wavenumber = omega**2/self.g\n            else:\n                wavenumber = newton(lambda k: k*np.tanh(k*self.water_depth) - omega**2/self.g, x0=1.0)\n            wavelength = 2*np.pi/wavenumber\n\n        else:  # provided_freq_type is 'wavelength' or 'wavenumber'\n            if provided_freq_type == 'wavelength':\n                wavelength = frequency_data['wavelength']\n                wavenumber = 2*np.pi/wavelength\n            else:  # provided_freq_type is 'wavenumber'\n                wavenumber = frequency_data['wavenumber']\n                wavelength = 2*np.pi/wavenumber\n\n            omega = np.sqrt(self.g*wavenumber*np.tanh(wavenumber*self.water_depth))\n            period = 2*np.pi/omega\n\n        return omega, period, wavenumber, wavelength, provided_freq_type",
  "def _check_data(self):\n        \"\"\"Sanity checks on the data.\"\"\"\n\n        if self.free_surface not in {0.0, np.infty}:\n            raise NotImplementedError(\n                f\"Free surface is {self.free_surface}. \"\n                \"Only z=0 and z=\u221e are accepted values for the free surface position.\"\n            )\n\n        if self.free_surface == np.infty and self.water_depth != np.infty:\n            raise NotImplementedError(\n                \"Problems with a sea bottom but no free surface have not been implemented.\"\n            )\n\n        if self.water_depth < 0.0:\n            raise ValueError(\"`water_depth` should be stricly positive.\")\n\n        if self.omega in {0, np.infty} and self.water_depth != np.infty:\n            raise NotImplementedError(\n                f\"omega={self.omega} is only implemented for infinite depth.\"\n            )\n\n        if self.body is not None:\n            if ((isinstance(self.body.mesh, CollectionOfMeshes) and len(self.body.mesh) == 0)\n                    or len(self.body.mesh.faces) == 0):\n                raise ValueError(f\"The mesh of the body {self.body.name} is empty.\")\n\n            if (any(self.body.mesh.faces_centers[:, 2] >= self.free_surface)\n                    or any(self.body.mesh.faces_centers[:, 2] <= -self.water_depth)):\n\n                LOG.warning(\n                    f\"The mesh of the body {self.body.name} is not inside the domain.\\n\"\n                    \"Check the position of the free_surface and the water_depth\\n\"\n                    \"or use body.keep_immersed_part() to clip the mesh.\"\n                )\n\n            if self.wavelength < self.body.minimal_computable_wavelength:\n                LOG.warning(f\"Mesh resolution for {self}:\\n\"\n                        f\"The resolution of the mesh '{self.body.mesh.name}' of the body '{self.body.name}' \"\n                        f\"might be insufficient for the wavelength \u03bb={self.wavelength:.2e}.\\n\"\n                        f\"This warning appears because the largest panel of this mesh has radius {self.body.mesh.faces_radiuses.max():.2e} > \u03bb/8.\"\n                        )\n\n        if self.boundary_condition is not None:\n            if len(self.boundary_condition.shape) != 1:\n                raise ValueError(\"Expected a 1-dimensional array as boundary_condition\")\n\n            if self.boundary_condition.shape[0] != self.body.mesh.nb_faces:\n                raise ValueError(\n                    f\"The shape of the boundary condition ({self.boundary_condition.shape})\"\n                    f\"does not match the number of faces of the mesh ({self.body.mesh.nb_faces}).\"\n                )",
  "def body_name(self):\n        return self.body.name if self.body is not None else 'None'",
  "def _asdict(self):\n        return {\"body_name\": self.body_name,\n                \"water_depth\": self.water_depth,\n                \"omega\": self.omega,\n                \"period\": self.period,\n                \"wavelength\": self.wavelength,\n                \"wavenumber\": self.wavenumber,\n                \"rho\": self.rho,\n                \"g\": self.g}",
  "def _group_for_parallel_resolution(problems):\n        \"\"\"Given a list of problems, returns a list of groups of problems, such\n        that each group should be executed in the same process to benefit from\n        caching.\n        \"\"\"\n        problems_params = pd.DataFrame([pb._asdict() for pb in problems])\n        groups_of_indices = problems_params.groupby([\"body_name\", \"water_depth\", \"omega\", \"rho\", \"g\"]).groups.values()\n        groups_of_problems = [[problems[i] for i in grp] for grp in groups_of_indices]\n        return groups_of_problems",
  "def __str__(self):\n        \"\"\"Do not display default values in str(problem).\"\"\"\n        parameters = [f\"body={self.body.__short_str__() if self.body is not None else None}\",\n                      f\"{self.provided_freq_type}={self.__getattribute__(self.provided_freq_type):.3f}\",\n                      f\"water_depth={self.water_depth}\"]\n        try:\n            parameters.extend(self._str_other_attributes())\n        except AttributeError:\n            pass\n\n        if not self.free_surface == _default_parameters['free_surface']:\n            parameters.append(f\"free_surface={self.free_surface}\")\n        if not self.g == _default_parameters['g']:\n            parameters.append(f\"g={self.g}\")\n        if not self.rho == _default_parameters['rho']:\n            parameters.append(f\"rho={self.rho}\")\n\n        return self.__class__.__name__ + \"(\" + ', '.join(parameters) + \")\"",
  "def __repr__(self):\n        return self.__str__()",
  "def _repr_pretty_(self, p, cycle):\n        p.text(self.__str__())",
  "def __rich_repr__(self):\n        yield \"body\", self.body, None\n        yield self.provided_freq_type, self.__getattribute__(self.provided_freq_type)\n        yield \"water_depth\", self.water_depth, _default_parameters[\"water_depth\"]\n        try:\n            yield from self._specific_rich_repr()\n        except:\n            pass\n        yield \"g\", self.g, _default_parameters[\"g\"]\n        yield \"rho\", self.rho, _default_parameters[\"rho\"]",
  "def _astuple(self):\n        return (self.body, self.free_surface, self.water_depth,\n                self.omega, self.period, self.wavenumber, self.wavelength,\n                self.rho, self.g)",
  "def __eq__(self, other):\n        if isinstance(other, LinearPotentialFlowProblem):\n            return self._astuple() == other._astuple()\n        else:\n            return NotImplemented",
  "def __lt__(self, other):\n        # Arbitrary order. Used for ordering of problems: problems with same body are grouped together.\n        if isinstance(other, LinearPotentialFlowProblem):\n            return self._astuple()[:9] < other._astuple()[:9]\n            # Not the whole tuple, because when using inheriting classes,\n            # \"radiating_dof\" cannot be compared with \"wave_direction\"\n        else:\n            return NotImplemented",
  "def depth(self):\n        return self.water_depth",
  "def influenced_dofs(self):\n        # TODO: let the user choose the influenced dofs\n        return self.body.dofs if self.body is not None else set()",
  "def make_results_container(self):\n        return LinearPotentialFlowResult(self)",
  "def __init__(self, *,\n                 body=None,\n                 free_surface=_default_parameters['free_surface'],\n                 water_depth=None, sea_bottom=None,\n                 omega=None, period=None, wavenumber=None, wavelength=None,\n                 rho=_default_parameters['rho'],\n                 g=_default_parameters['g'],\n                 wave_direction=_default_parameters['wave_direction']):\n\n        self.wave_direction = float(wave_direction)\n\n        super().__init__(body=body, free_surface=free_surface, water_depth=water_depth, sea_bottom=sea_bottom,\n                         omega=omega, period=period, wavenumber=wavenumber, wavelength=wavelength, rho=rho, g=g)\n\n        if not (-2*np.pi-1e-3 <= self.wave_direction <= 2*np.pi+1e-3):\n            LOG.warning(f\"The value {self.wave_direction} has been provided for the wave direction, and it does not look like an angle in radians. \"\n                         \"The wave direction in Capytaine is defined in radians and not in degrees, so the result might not be what you expect. \"\n                         \"If you were actually giving an angle in radians, use the modulo operator to give a value between -2\u03c0 and 2\u03c0 to disable this warning.\")\n\n        if self.body is not None:\n\n            self.boundary_condition = -(\n                    airy_waves_velocity(self.body.mesh.faces_centers, self)\n                    * self.body.mesh.faces_normals\n            ).sum(axis=1)\n\n            if len(self.body.dofs) == 0:\n                LOG.warning(f\"The body {self.body.name} used in diffraction problem has no dofs!\")",
  "def _astuple(self):\n        return super()._astuple() + (self.wave_direction,)",
  "def _asdict(self):\n        d = super()._asdict()\n        d[\"wave_direction\"] = self.wave_direction\n        return d",
  "def _str_other_attributes(self):\n        return [f\"wave_direction={self.wave_direction:.3f}\"]",
  "def _specific_rich_repr(self):\n        yield \"wave_direction\", self.wave_direction, _default_parameters[\"wave_direction\"]",
  "def make_results_container(self, *args, **kwargs):\n        return DiffractionResult(self, *args, **kwargs)",
  "def __init__(self, *, body=None,\n                 free_surface=_default_parameters['free_surface'],\n                 water_depth=None, sea_bottom=None,\n                 omega=None, period=None, wavenumber=None, wavelength=None,\n                 rho=_default_parameters['rho'],\n                 g=_default_parameters['g'],\n                 radiating_dof=None):\n\n        self.radiating_dof = radiating_dof\n\n        super().__init__(body=body, free_surface=free_surface, water_depth=water_depth, sea_bottom=sea_bottom,\n                         omega=omega, period=period, wavenumber=wavenumber, wavelength=wavelength, rho=rho, g=g)\n\n        if self.body is not None:\n\n            if len(self.body.dofs) == 0:\n                raise ValueError(f\"Body {self.body.name} does not have any degrees of freedom.\")\n\n            if self.radiating_dof is None:\n                self.radiating_dof = next(iter(self.body.dofs))\n\n            if self.radiating_dof not in self.body.dofs:\n                LOG.error(f\"In {self}: the radiating degree of freedom {self.radiating_dof} is not one of\"\n                          f\"the degrees of freedom of the body.\\n\"\n                          f\"The dofs of the body are {list(self.body.dofs.keys())}\")\n                raise ValueError(\"Unrecognized degree of freedom name.\")\n\n            dof = self.body.dofs[self.radiating_dof]\n            self.boundary_condition = -1j*self.omega * np.sum(dof * self.body.mesh.faces_normals, axis=1)",
  "def _astuple(self):\n        return super()._astuple() + (self.radiating_dof,)",
  "def _asdict(self):\n        d = super()._asdict()\n        d[\"radiating_dof\"] = self.radiating_dof\n        return d",
  "def _str_other_attributes(self):\n        return [f\"radiating_dof=\\'{self.radiating_dof}\\'\"]",
  "def _specific_rich_repr(self):\n        yield \"radiating_dof\", self.radiating_dof",
  "def make_results_container(self, *args, **kwargs):\n        return RadiationResult(self, *args, **kwargs)",
  "def __init__(self, problem, forces=None, sources=None, potential=None, pressure=None):\n        self.problem = problem\n\n        self.sources = sources\n        self.potential = potential\n        self.pressure = pressure\n        self.fs_elevation = {}\n\n        # Copy data from problem\n        self.body               = self.problem.body\n        self.free_surface       = self.problem.free_surface\n        self.omega              = self.problem.omega\n        self.rho                = self.problem.rho\n        self.g                  = self.problem.g\n        self.boundary_condition = self.problem.boundary_condition\n        self.water_depth        = self.problem.water_depth\n        self.depth              = self.problem.water_depth\n        self.wavenumber         = self.problem.wavenumber\n        self.wavelength         = self.problem.wavelength\n        self.period             = self.problem.period\n        self.provided_freq_type = self.problem.provided_freq_type\n        self.body_name          = self.problem.body_name\n        self.influenced_dofs    = self.problem.influenced_dofs\n\n        if forces is not None:\n            for dof in self.influenced_dofs:\n                self.store_force(dof, forces[dof])",
  "def store_force(self, dof, force):\n        pass",
  "def __init__(self, problem, *args, **kwargs):\n        self.forces = {}\n        super().__init__(problem, *args, **kwargs)\n        self.wave_direction = self.problem.wave_direction",
  "def store_force(self, dof, force):\n        self.forces[dof] = force",
  "def records(self):\n        params = self.problem._asdict()\n        FK = froude_krylov_force(self.problem)\n        return [dict(**params,\n                     influenced_dof=dof,\n                     diffraction_force=self.forces[dof],\n                     Froude_Krylov_force=FK[dof])\n                for dof in self.influenced_dofs]",
  "def __init__(self, problem, *args, **kwargs):\n        self.added_masses = {}\n        self.radiation_dampings = {}\n        super().__init__(problem, *args, **kwargs)\n        self.radiating_dof = self.problem.radiating_dof",
  "def store_force(self, dof, force):\n        self.added_masses[dof] = force.real/self.omega**2\n        self.radiation_dampings[dof] = force.imag/self.omega",
  "def records(self):\n        params = self.problem._asdict()\n        return [dict(params,\n                     influenced_dof=dof,\n                     added_mass=self.added_masses[dof],\n                     radiation_damping=self.radiation_dampings[dof])\n                for dof in self.influenced_dofs]",
  "class MatrixEngine(ABC):\n    \"\"\"Abstract method to build a matrix.\"\"\"\n\n    @abstractmethod\n    def build_matrices(self, mesh1, mesh2, free_surface, water_depth, wavenumber, green_function):\n        pass\n\n    def build_S_matrix(self, *args, **kwargs):\n        \"\"\"Similar to :code:`build_matrices`, but returning only :math:`S`\"\"\"\n        S, _ = self.build_matrices(*args, **kwargs)  # Could be optimized...\n        return S",
  "class BasicMatrixEngine(MatrixEngine):\n    \"\"\"\n    Simple engine that assemble a full matrix (except for one reflection symmetry).\n    Basically only calls :code:`green_function.evaluate`.\n\n    Parameters\n    ----------\n    linear_solver: str or function, optional\n        Setting of the numerical solver for linear problems Ax = b.\n        It can be set with the name of a preexisting solver\n        (available: \"direct\" and \"gmres\", the former is the default choice)\n        or by passing directly a solver function.\n    matrix_cache_size: int, optional\n        number of matrices to keep in cache\n    \"\"\"\n\n    available_linear_solvers = {'direct': linear_solvers.solve_directly,\n                                'lu_decomposition': linear_solvers.LUSolverWithCache().solve,\n                                'gmres': linear_solvers.solve_gmres,\n                                }\n\n    def __init__(self, *, linear_solver='lu_decomposition', matrix_cache_size=1):\n\n        if linear_solver in self.available_linear_solvers:\n            self.linear_solver = self.available_linear_solvers[linear_solver]\n        else:\n            self.linear_solver = linear_solver\n\n        if matrix_cache_size > 0:\n            self.build_matrices = delete_first_lru_cache(maxsize=matrix_cache_size)(self.build_matrices)\n\n        self.exportable_settings = {\n            'engine': 'BasicMatrixEngine',\n            'matrix_cache_size': matrix_cache_size,\n            'linear_solver': str(linear_solver),\n        }\n\n    def __str__(self):\n        params = f\"linear_solver=\\'{self.exportable_settings['linear_solver']}\\'\"\n        params += f\", matrix_cache_size={self.exportable_settings['matrix_cache_size']}\" if self.exportable_settings['matrix_cache_size'] != 1 else \"\"\n        return f\"BasicMatrixEngine({params})\"\n\n    def __repr__(self):\n        return self.__str__()\n\n    def _repr_pretty_(self, p, cycle):\n        p.text(self.__str__())\n\n    def build_matrices(self, mesh1, mesh2, free_surface, water_depth, wavenumber, green_function):\n        r\"\"\"Build the influence matrices between mesh1 and mesh2.\n\n        Parameters\n        ----------\n        mesh1: Mesh or CollectionOfMeshes\n            mesh of the receiving body (where the potential is measured)\n        mesh2: Mesh or CollectionOfMeshes\n            mesh of the source body (over which the source distribution is integrated)\n        free_surface: float\n            position of the free surface (default: :math:`z = 0`)\n        water_depth: float\n            position of the sea bottom (default: :math:`z = -\\infty`)\n        wavenumber: float\n            wavenumber (default: 1.0)\n        green_function: AbstractGreenFunction\n            object with an \"evaluate\" method that computes the Green function.\n\n        Returns\n        -------\n        tuple of matrix-like\n            the matrices :math:`S` and :math:`K`\n        \"\"\"\n\n        if (isinstance(mesh1, ReflectionSymmetricMesh)\n                and isinstance(mesh2, ReflectionSymmetricMesh)\n                and mesh1.plane == mesh2.plane):\n\n            S_a, V_a = self.build_matrices(\n                mesh1[0], mesh2[0], free_surface, water_depth, wavenumber,\n                green_function)\n            S_b, V_b = self.build_matrices(\n                mesh1[0], mesh2[1], free_surface, water_depth, wavenumber,\n                green_function)\n\n            return BlockSymmetricToeplitzMatrix([[S_a, S_b]]), BlockSymmetricToeplitzMatrix([[V_a, V_b]])\n\n        else:\n            return green_function.evaluate(\n                mesh1, mesh2, free_surface, water_depth, wavenumber,\n            )",
  "class HierarchicalToeplitzMatrixEngine(MatrixEngine):\n    \"\"\"An experimental matrix engine that build a hierarchical matrix with\n     some block-Toeplitz structure.\n\n    Parameters\n    ----------\n    ACA_distance: float, optional\n        Above this distance, the ACA is used to approximate the matrix with a low-rank block.\n    ACA_tol: float, optional\n        The tolerance of the ACA when building a low-rank matrix.\n    matrix_cache_size: int, optional\n        number of matrices to keep in cache\n    \"\"\"\n\n    def __init__(self, *, ACA_distance=8.0, ACA_tol=1e-2, matrix_cache_size=1):\n\n        if matrix_cache_size > 0:\n            self.build_matrices = delete_first_lru_cache(maxsize=matrix_cache_size)(self.build_matrices)\n\n        self.ACA_distance = ACA_distance\n        self.ACA_tol = ACA_tol\n\n        self.linear_solver = linear_solvers.solve_gmres\n\n        self.exportable_settings = {\n            'engine': 'HierarchicalToeplitzMatrixEngine',\n            'ACA_distance': ACA_distance,\n            'ACA_tol': ACA_tol,\n            'matrix_cache_size': matrix_cache_size,\n        }\n\n    def __str__(self):\n        params = f\"ACA_distance={self.ACA_distance}\"\n        params += f\", ACA_tol={self.ACA_tol}\"\n        params += f\", matrix_cache_size={self.exportable_settings['matrix_cache_size']}\" if self.exportable_settings['matrix_cache_size'] != 1 else \"\"\n        return f\"HierarchicalToeplitzMatrixEngine({params})\"\n\n    def _repr_pretty_(self, p, cycle):\n        p.text(self.__str__())\n\n\n    def build_matrices(self,\n                       mesh1, mesh2, free_surface, water_depth, wavenumber, green_function,\n                       _rec_depth=1):\n        \"\"\"Recursively builds a hierarchical matrix between mesh1 and mesh2.\n        \n        Same arguments as :func:`BasicMatrixEngine.build_matrices`.\n\n        :code:`_rec_depth` keeps track of the recursion depth only for pretty log printing.\n        \"\"\"\n\n        if logging.getLogger().isEnabledFor(logging.DEBUG):\n            log_entry = (\n                \"\\t\" * (_rec_depth+1) +\n                \"Build the S and K influence matrices between {mesh1} and {mesh2}\"\n                .format(mesh1=mesh1.name, mesh2=(mesh2.name if mesh2 is not mesh1 else 'itself'))\n            )\n        else:\n            log_entry = \"\"  # will not be used\n\n        # Distance between the meshes (for ACA).\n        distance = np.linalg.norm(mesh1.center_of_mass_of_nodes - mesh2.center_of_mass_of_nodes)\n\n        # I) SPARSE COMPUTATION\n        # I-i) BLOCK TOEPLITZ MATRIX\n\n        if (isinstance(mesh1, ReflectionSymmetricMesh)\n                and isinstance(mesh2, ReflectionSymmetricMesh)\n                and mesh1.plane == mesh2.plane):\n\n            LOG.debug(log_entry + \" using mirror symmetry.\")\n\n            S_a, V_a = self.build_matrices(\n                mesh1[0], mesh2[0], free_surface, water_depth, wavenumber, green_function,\n                _rec_depth=_rec_depth+1)\n            S_b, V_b = self.build_matrices(\n                mesh1[0], mesh2[1], free_surface, water_depth, wavenumber, green_function,\n                _rec_depth=_rec_depth+1)\n\n            return BlockSymmetricToeplitzMatrix([[S_a, S_b]]), BlockSymmetricToeplitzMatrix([[V_a, V_b]])\n\n        elif (isinstance(mesh1, TranslationalSymmetricMesh)\n              and isinstance(mesh2, TranslationalSymmetricMesh)\n              and np.allclose(mesh1.translation, mesh2.translation)\n              and mesh1.nb_submeshes == mesh2.nb_submeshes):\n\n            LOG.debug(log_entry + \" using translational symmetry.\")\n\n            S_list, V_list = [], []\n            for submesh in mesh2:\n                S, V = self.build_matrices(\n                    mesh1[0], submesh, free_surface, water_depth, wavenumber, green_function,\n                    _rec_depth=_rec_depth+1)\n                S_list.append(S)\n                V_list.append(V)\n            for submesh in mesh1[1:][::-1]:\n                S, V = self.build_matrices(\n                    submesh, mesh2[0], free_surface, water_depth, wavenumber, green_function,\n                    _rec_depth=_rec_depth+1)\n                S_list.append(S)\n                V_list.append(V)\n\n            return BlockToeplitzMatrix([S_list]), BlockToeplitzMatrix([V_list])\n\n        elif (isinstance(mesh1, AxialSymmetricMesh)\n              and isinstance(mesh2, AxialSymmetricMesh)\n              and mesh1.axis == mesh2.axis\n              and mesh1.nb_submeshes == mesh2.nb_submeshes):\n\n            LOG.debug(log_entry + \" using rotation symmetry.\")\n\n            S_line, V_line = [], []\n            for submesh in mesh2[:mesh2.nb_submeshes]:\n                S, V = self.build_matrices(\n                    mesh1[0], submesh, free_surface, water_depth, wavenumber, green_function,\n                    _rec_depth=_rec_depth+1)\n                S_line.append(S)\n                V_line.append(V)\n\n            return BlockCirculantMatrix([S_line]), BlockCirculantMatrix([V_line])\n\n        # I-ii) LOW-RANK MATRIX WITH ACA\n\n        elif distance > self.ACA_distance*mesh1.diameter_of_nodes or distance > self.ACA_distance*mesh2.diameter_of_nodes:\n\n            LOG.debug(log_entry + \" using ACA.\")\n\n            def get_row_func(i):\n                s, v = green_function.evaluate(\n                    mesh1.extract_one_face(i), mesh2,\n                    free_surface, water_depth, wavenumber\n                )\n                return s.flatten(), v.flatten()\n\n            def get_col_func(j):\n                s, v = green_function.evaluate(\n                    mesh1, mesh2.extract_one_face(j),\n                    free_surface, water_depth, wavenumber\n                )\n                return s.flatten(), v.flatten()\n\n            try:\n                return LowRankMatrix.from_rows_and_cols_functions_with_multi_ACA(\n                    get_row_func, get_col_func, mesh1.nb_faces, mesh2.nb_faces,\n                    nb_matrices=2, id_main=1,  # Approximate V and get an approximation of S at the same time\n                    tol=self.ACA_tol, dtype=np.complex128)\n            except NoConvergenceOfACA:\n                pass  # Continue with non sparse computation\n\n        # II) NON-SPARSE COMPUTATIONS\n        # II-i) BLOCK MATRIX\n\n        if (isinstance(mesh1, CollectionOfMeshes)\n              and isinstance(mesh2, CollectionOfMeshes)):\n\n            LOG.debug(log_entry + \" using block matrix structure.\")\n\n            S_matrix, V_matrix = [], []\n            for submesh1 in mesh1:\n                S_line, V_line = [], []\n                for submesh2 in mesh2:\n                    S, V = self.build_matrices(\n                        submesh1, submesh2, free_surface, water_depth, wavenumber, green_function,\n                        _rec_depth=_rec_depth+1)\n\n                    S_line.append(S)\n                    V_line.append(V)\n                S_matrix.append(S_line)\n                V_matrix.append(V_line)\n\n            return BlockMatrix(S_matrix), BlockMatrix(V_matrix)\n\n        # II-ii) PLAIN NUMPY ARRAY\n\n        else:\n            LOG.debug(log_entry)\n\n            S, V = green_function.evaluate(\n                mesh1, mesh2, free_surface, water_depth, wavenumber,\n            )\n            return S, V",
  "def build_matrices(self, mesh1, mesh2, free_surface, water_depth, wavenumber, green_function):\n        pass",
  "def build_S_matrix(self, *args, **kwargs):\n        \"\"\"Similar to :code:`build_matrices`, but returning only :math:`S`\"\"\"\n        S, _ = self.build_matrices(*args, **kwargs)  # Could be optimized...\n        return S",
  "def __init__(self, *, linear_solver='lu_decomposition', matrix_cache_size=1):\n\n        if linear_solver in self.available_linear_solvers:\n            self.linear_solver = self.available_linear_solvers[linear_solver]\n        else:\n            self.linear_solver = linear_solver\n\n        if matrix_cache_size > 0:\n            self.build_matrices = delete_first_lru_cache(maxsize=matrix_cache_size)(self.build_matrices)\n\n        self.exportable_settings = {\n            'engine': 'BasicMatrixEngine',\n            'matrix_cache_size': matrix_cache_size,\n            'linear_solver': str(linear_solver),\n        }",
  "def __str__(self):\n        params = f\"linear_solver=\\'{self.exportable_settings['linear_solver']}\\'\"\n        params += f\", matrix_cache_size={self.exportable_settings['matrix_cache_size']}\" if self.exportable_settings['matrix_cache_size'] != 1 else \"\"\n        return f\"BasicMatrixEngine({params})\"",
  "def __repr__(self):\n        return self.__str__()",
  "def _repr_pretty_(self, p, cycle):\n        p.text(self.__str__())",
  "def build_matrices(self, mesh1, mesh2, free_surface, water_depth, wavenumber, green_function):\n        r\"\"\"Build the influence matrices between mesh1 and mesh2.\n\n        Parameters\n        ----------\n        mesh1: Mesh or CollectionOfMeshes\n            mesh of the receiving body (where the potential is measured)\n        mesh2: Mesh or CollectionOfMeshes\n            mesh of the source body (over which the source distribution is integrated)\n        free_surface: float\n            position of the free surface (default: :math:`z = 0`)\n        water_depth: float\n            position of the sea bottom (default: :math:`z = -\\infty`)\n        wavenumber: float\n            wavenumber (default: 1.0)\n        green_function: AbstractGreenFunction\n            object with an \"evaluate\" method that computes the Green function.\n\n        Returns\n        -------\n        tuple of matrix-like\n            the matrices :math:`S` and :math:`K`\n        \"\"\"\n\n        if (isinstance(mesh1, ReflectionSymmetricMesh)\n                and isinstance(mesh2, ReflectionSymmetricMesh)\n                and mesh1.plane == mesh2.plane):\n\n            S_a, V_a = self.build_matrices(\n                mesh1[0], mesh2[0], free_surface, water_depth, wavenumber,\n                green_function)\n            S_b, V_b = self.build_matrices(\n                mesh1[0], mesh2[1], free_surface, water_depth, wavenumber,\n                green_function)\n\n            return BlockSymmetricToeplitzMatrix([[S_a, S_b]]), BlockSymmetricToeplitzMatrix([[V_a, V_b]])\n\n        else:\n            return green_function.evaluate(\n                mesh1, mesh2, free_surface, water_depth, wavenumber,\n            )",
  "def __init__(self, *, ACA_distance=8.0, ACA_tol=1e-2, matrix_cache_size=1):\n\n        if matrix_cache_size > 0:\n            self.build_matrices = delete_first_lru_cache(maxsize=matrix_cache_size)(self.build_matrices)\n\n        self.ACA_distance = ACA_distance\n        self.ACA_tol = ACA_tol\n\n        self.linear_solver = linear_solvers.solve_gmres\n\n        self.exportable_settings = {\n            'engine': 'HierarchicalToeplitzMatrixEngine',\n            'ACA_distance': ACA_distance,\n            'ACA_tol': ACA_tol,\n            'matrix_cache_size': matrix_cache_size,\n        }",
  "def __str__(self):\n        params = f\"ACA_distance={self.ACA_distance}\"\n        params += f\", ACA_tol={self.ACA_tol}\"\n        params += f\", matrix_cache_size={self.exportable_settings['matrix_cache_size']}\" if self.exportable_settings['matrix_cache_size'] != 1 else \"\"\n        return f\"HierarchicalToeplitzMatrixEngine({params})\"",
  "def _repr_pretty_(self, p, cycle):\n        p.text(self.__str__())",
  "def build_matrices(self,\n                       mesh1, mesh2, free_surface, water_depth, wavenumber, green_function,\n                       _rec_depth=1):\n        \"\"\"Recursively builds a hierarchical matrix between mesh1 and mesh2.\n        \n        Same arguments as :func:`BasicMatrixEngine.build_matrices`.\n\n        :code:`_rec_depth` keeps track of the recursion depth only for pretty log printing.\n        \"\"\"\n\n        if logging.getLogger().isEnabledFor(logging.DEBUG):\n            log_entry = (\n                \"\\t\" * (_rec_depth+1) +\n                \"Build the S and K influence matrices between {mesh1} and {mesh2}\"\n                .format(mesh1=mesh1.name, mesh2=(mesh2.name if mesh2 is not mesh1 else 'itself'))\n            )\n        else:\n            log_entry = \"\"  # will not be used\n\n        # Distance between the meshes (for ACA).\n        distance = np.linalg.norm(mesh1.center_of_mass_of_nodes - mesh2.center_of_mass_of_nodes)\n\n        # I) SPARSE COMPUTATION\n        # I-i) BLOCK TOEPLITZ MATRIX\n\n        if (isinstance(mesh1, ReflectionSymmetricMesh)\n                and isinstance(mesh2, ReflectionSymmetricMesh)\n                and mesh1.plane == mesh2.plane):\n\n            LOG.debug(log_entry + \" using mirror symmetry.\")\n\n            S_a, V_a = self.build_matrices(\n                mesh1[0], mesh2[0], free_surface, water_depth, wavenumber, green_function,\n                _rec_depth=_rec_depth+1)\n            S_b, V_b = self.build_matrices(\n                mesh1[0], mesh2[1], free_surface, water_depth, wavenumber, green_function,\n                _rec_depth=_rec_depth+1)\n\n            return BlockSymmetricToeplitzMatrix([[S_a, S_b]]), BlockSymmetricToeplitzMatrix([[V_a, V_b]])\n\n        elif (isinstance(mesh1, TranslationalSymmetricMesh)\n              and isinstance(mesh2, TranslationalSymmetricMesh)\n              and np.allclose(mesh1.translation, mesh2.translation)\n              and mesh1.nb_submeshes == mesh2.nb_submeshes):\n\n            LOG.debug(log_entry + \" using translational symmetry.\")\n\n            S_list, V_list = [], []\n            for submesh in mesh2:\n                S, V = self.build_matrices(\n                    mesh1[0], submesh, free_surface, water_depth, wavenumber, green_function,\n                    _rec_depth=_rec_depth+1)\n                S_list.append(S)\n                V_list.append(V)\n            for submesh in mesh1[1:][::-1]:\n                S, V = self.build_matrices(\n                    submesh, mesh2[0], free_surface, water_depth, wavenumber, green_function,\n                    _rec_depth=_rec_depth+1)\n                S_list.append(S)\n                V_list.append(V)\n\n            return BlockToeplitzMatrix([S_list]), BlockToeplitzMatrix([V_list])\n\n        elif (isinstance(mesh1, AxialSymmetricMesh)\n              and isinstance(mesh2, AxialSymmetricMesh)\n              and mesh1.axis == mesh2.axis\n              and mesh1.nb_submeshes == mesh2.nb_submeshes):\n\n            LOG.debug(log_entry + \" using rotation symmetry.\")\n\n            S_line, V_line = [], []\n            for submesh in mesh2[:mesh2.nb_submeshes]:\n                S, V = self.build_matrices(\n                    mesh1[0], submesh, free_surface, water_depth, wavenumber, green_function,\n                    _rec_depth=_rec_depth+1)\n                S_line.append(S)\n                V_line.append(V)\n\n            return BlockCirculantMatrix([S_line]), BlockCirculantMatrix([V_line])\n\n        # I-ii) LOW-RANK MATRIX WITH ACA\n\n        elif distance > self.ACA_distance*mesh1.diameter_of_nodes or distance > self.ACA_distance*mesh2.diameter_of_nodes:\n\n            LOG.debug(log_entry + \" using ACA.\")\n\n            def get_row_func(i):\n                s, v = green_function.evaluate(\n                    mesh1.extract_one_face(i), mesh2,\n                    free_surface, water_depth, wavenumber\n                )\n                return s.flatten(), v.flatten()\n\n            def get_col_func(j):\n                s, v = green_function.evaluate(\n                    mesh1, mesh2.extract_one_face(j),\n                    free_surface, water_depth, wavenumber\n                )\n                return s.flatten(), v.flatten()\n\n            try:\n                return LowRankMatrix.from_rows_and_cols_functions_with_multi_ACA(\n                    get_row_func, get_col_func, mesh1.nb_faces, mesh2.nb_faces,\n                    nb_matrices=2, id_main=1,  # Approximate V and get an approximation of S at the same time\n                    tol=self.ACA_tol, dtype=np.complex128)\n            except NoConvergenceOfACA:\n                pass  # Continue with non sparse computation\n\n        # II) NON-SPARSE COMPUTATIONS\n        # II-i) BLOCK MATRIX\n\n        if (isinstance(mesh1, CollectionOfMeshes)\n              and isinstance(mesh2, CollectionOfMeshes)):\n\n            LOG.debug(log_entry + \" using block matrix structure.\")\n\n            S_matrix, V_matrix = [], []\n            for submesh1 in mesh1:\n                S_line, V_line = [], []\n                for submesh2 in mesh2:\n                    S, V = self.build_matrices(\n                        submesh1, submesh2, free_surface, water_depth, wavenumber, green_function,\n                        _rec_depth=_rec_depth+1)\n\n                    S_line.append(S)\n                    V_line.append(V)\n                S_matrix.append(S_line)\n                V_matrix.append(V_line)\n\n            return BlockMatrix(S_matrix), BlockMatrix(V_matrix)\n\n        # II-ii) PLAIN NUMPY ARRAY\n\n        else:\n            LOG.debug(log_entry)\n\n            S, V = green_function.evaluate(\n                mesh1, mesh2, free_surface, water_depth, wavenumber,\n            )\n            return S, V",
  "def get_row_func(i):\n                s, v = green_function.evaluate(\n                    mesh1.extract_one_face(i), mesh2,\n                    free_surface, water_depth, wavenumber\n                )\n                return s.flatten(), v.flatten()",
  "def get_col_func(j):\n                s, v = green_function.evaluate(\n                    mesh1, mesh2.extract_one_face(j),\n                    free_surface, water_depth, wavenumber\n                )\n                return s.flatten(), v.flatten()",
  "class BEMSolver:\n    \"\"\"\n    Solver for linear potential flow problems.\n\n    Parameters\n    ----------\n    green_function: AbstractGreenFunction, optional\n        Object handling the computation of the Green function.\n        (default: :class:`~capytaine.green_function.delhommeau.Delhommeau`)\n    engine: MatrixEngine, optional\n        Object handling the building of matrices and the resolution of linear systems with these matrices.\n        (default: :class:`~capytaine.bem.engines.BasicMatrixEngine`)\n\n    Attributes\n    ----------\n    exportable_settings : dict\n        Settings of the solver that can be saved to reinit the same solver later.\n    \"\"\"\n\n    def __init__(self, *, green_function=None, engine=None):\n        self.green_function = Delhommeau() if green_function is None else green_function\n        self.engine = BasicMatrixEngine() if engine is None else engine\n\n        try:\n            self.exportable_settings = {\n                **self.green_function.exportable_settings,\n                **self.engine.exportable_settings\n            }\n        except AttributeError:\n            pass\n\n    def __str__(self):\n        return f\"BEMSolver(engine={self.engine}, green_function={self.green_function})\"\n\n    def __repr__(self):\n        return self.__str__()\n\n    def _repr_pretty_(self, p, cycle):\n        p.text(self.__str__())\n\n    @classmethod\n    def from_exported_settings(settings):\n        raise NotImplementedError\n\n    def solve(self, problem, keep_details=True):\n        \"\"\"Solve the linear potential flow problem.\n\n        Parameters\n        ----------\n        problem: LinearPotentialFlowProblem\n            the problem to be solved\n        keep_details: bool, optional\n            if True, store the sources and the potential on the floating body in the output object\n            (default: True)\n\n        Returns\n        -------\n        LinearPotentialFlowResult\n            an object storing the problem data and its results\n        \"\"\"\n        LOG.info(\"Solve %s.\", problem)\n\n        S, K = self.engine.build_matrices(\n            problem.body.mesh, problem.body.mesh,\n            problem.free_surface, problem.water_depth, problem.wavenumber,\n            self.green_function\n        )\n        sources = self.engine.linear_solver(K, problem.boundary_condition)\n        potential = S @ sources\n        pressure = 1j * problem.omega * problem.rho * potential\n\n        forces = problem.body.integrate_pressure(pressure)\n\n        if not keep_details:\n            result = problem.make_results_container(forces)\n        else:\n            result = problem.make_results_container(forces, sources, potential, pressure)\n\n        LOG.debug(\"Done!\")\n\n        return result\n\n    def solve_all(self, problems, *, n_jobs=1, **kwargs):\n        \"\"\"Solve several problems.\n        Optional keyword arguments are passed to `BEMSolver.solve`.\n\n        Parameters\n        ----------\n        problems: list of LinearPotentialFlowProblem\n            several problems to be solved\n        n_jobs: int, optional (default: 1)\n            the number of jobs to run in parallel using the optional dependency `joblib`\n            By defaults: do not use joblib and solve sequentially.\n\n        Returns\n        -------\n        list of LinearPotentialFlowResult\n            the solved problems\n        \"\"\"\n        if n_jobs == 1:  # force sequential resolution\n            return [self.solve(pb, **kwargs) for pb in sorted(problems)]\n        else:\n            joblib = silently_import_optional_dependency(\"joblib\")\n            if joblib is None:\n                raise ImportError(f\"Setting the `n_jobs` argument to {n_jobs} requires the missing optional dependency 'joblib'.\")\n            groups_of_problems = LinearPotentialFlowProblem._group_for_parallel_resolution(problems)\n            groups_of_results = joblib.Parallel(n_jobs=n_jobs)(joblib.delayed(self.solve_all)(grp, n_jobs=1, **kwargs) for grp in groups_of_problems)\n            results = [res for grp in groups_of_results for res in grp]  # flatten the nested list\n            return results\n\n    def fill_dataset(self, dataset, bodies, *, n_jobs=1, **kwargs):\n        \"\"\"Solve a set of problems defined by the coordinates of an xarray dataset.\n\n        Parameters\n        ----------\n        dataset : xarray Dataset\n            dataset containing the problems parameters: frequency, radiating_dof, water_depth, ...\n        bodies : FloatingBody or list of FloatingBody\n            The body or bodies involved in the problems\n            They should all have different names.\n        n_jobs: int, optional (default: 1)\n            the number of jobs to run in parallel using the optional dependency `joblib`\n            By defaults: do not use joblib and solve sequentially.\n\n        Returns\n        -------\n        xarray Dataset\n        \"\"\"\n        attrs = {'start_of_computation': datetime.now().isoformat(),\n                 **self.exportable_settings}\n        problems = problems_from_dataset(dataset, bodies)\n        if 'theta' in dataset.coords:\n            results = self.solve_all(problems, keep_details=True, n_jobs=n_jobs)\n            kochin = kochin_data_array(results, dataset.coords['theta'])\n            dataset = assemble_dataset(results, attrs=attrs, **kwargs)\n            dataset.update(kochin)\n        else:\n            results = self.solve_all(problems, keep_details=False, n_jobs=n_jobs)\n            dataset = assemble_dataset(results, attrs=attrs, **kwargs)\n        return dataset\n\n\n    def compute_potential(self, points, result):\n        \"\"\"Compute the value of the potential at given points for a previously solved potential flow problem.\n\n        Parameters\n        ----------\n        points: array of shape (3,) or (N, 3), or 3-ple of arrays returned by meshgrid, or cpt.Mesh or cpt.CollectionOfMeshes object\n            Coordinates of the point(s) at which the potential should be computed\n        results: LinearPotentialFlowResult\n            The return of the BEM solver\n\n        Returns\n        -------\n        complex-valued array of shape (1,) or (N,) or (nx, ny, nz) or (mesh.nb_faces,) depending of the kind of input\n            The value of the potential at the points\n\n        Raises\n        ------\n        Exception: if the :code:`LinearPotentialFlowResult` object given as input does not contain the source distribution.\n        \"\"\"\n        points, output_shape = _normalize_points(points, keep_mesh=True)\n        if result.sources is None:\n            raise Exception(f\"\"\"The values of the sources of {result} cannot been found.\n            They probably have not been stored by the solver because the option keep_details=True have not been set.\n            Please re-run the resolution with this option.\"\"\")\n\n        S, _ = self.green_function.evaluate(points, result.body.mesh, result.free_surface, result.water_depth, result.wavenumber)\n        potential = S @ result.sources  # Sum the contributions of all panels in the mesh\n        return potential.reshape(output_shape)\n\n\n    def compute_velocity(self, points, result):\n        \"\"\"Compute the value of the velocity vector at given points for a previously solved potential flow problem.\n\n        Parameters\n        ----------\n        points: array of shape (3,) or (N, 3), or 3-ple of arrays returned by meshgrid, or cpt.Mesh or cpt.CollectionOfMeshes object\n            Coordinates of the point(s) at which the velocity should be computed\n        results: LinearPotentialFlowResult\n            The return of the BEM solver\n\n        Returns\n        -------\n        complex-valued array of shape (3,) or (N,, 3) or (nx, ny, nz, 3) or (mesh.nb_faces, 3) depending of the kind of input\n            The value of the velocity at the points\n\n        Raises\n        ------\n        Exception: if the :code:`LinearPotentialFlowResult` object given as input does not contain the source distribution.\n        \"\"\"\n        points, output_shape = _normalize_points(points, keep_mesh=True)\n\n        if result.sources is None:\n            raise Exception(f\"\"\"The values of the sources of {result} cannot been found.\n            They probably have not been stored by the solver because the option keep_details=True have not been set.\n            Please re-run the resolution with this option.\"\"\")\n\n        _, gradG = self.green_function.evaluate(points, result.body.mesh, result.free_surface, result.water_depth, result.wavenumber,\n                                                early_dot_product=False)\n        velocities = np.einsum('ijk,j->ik', gradG, result.sources)  # Sum the contributions of all panels in the mesh\n        return velocities.reshape((*output_shape, 3))\n\n\n    def compute_pressure(self, points, result):\n        \"\"\"Compute the value of the pressure at given points for a previously solved potential flow problem.\n\n        Parameters\n        ----------\n        points: array of shape (3,) or (N, 3), or 3-ple of arrays returned by meshgrid, or cpt.Mesh or cpt.CollectionOfMeshes object\n            Coordinates of the point(s) at which the pressure should be computed\n        results: LinearPotentialFlowResult\n            The return of the BEM solver\n\n        Returns\n        -------\n        complex-valued array of shape (1,) or (N,) or (nx, ny, nz) or (mesh.nb_faces,) depending of the kind of input\n            The value of the pressure at the points\n\n        Raises\n        ------\n        Exception: if the :code:`LinearPotentialFlowResult` object given as input does not contain the source distribution.\n        \"\"\"\n        return 1j * result.omega * result.rho * self.compute_potential(points, results)\n\n\n    def compute_free_surface_elevation(self, points, result):\n        \"\"\"Compute the value of the free surface elevation at given points for a previously solved potential flow problem.\n\n        Parameters\n        ----------\n        points: array of shape (2,) or (N, 2), or 2-ple of arrays returned by meshgrid, or cpt.Mesh or cpt.CollectionOfMeshes object\n            Coordinates of the point(s) at which the free surface elevation should be computed\n        results: LinearPotentialFlowResult\n            The return of the BEM solver\n\n        Returns\n        -------\n        complex-valued array of shape (1,) or (N,) or (nx, ny, nz) or (mesh.nb_faces,) depending of the kind of input\n            The value of the free surface elevation at the points\n\n        Raises\n        ------\n        Exception: if the :code:`LinearPotentialFlowResult` object given as input does not contain the source distribution.\n        \"\"\"\n        points, output_shape = _normalize_free_surface_points(points, keep_mesh=True)\n\n        fs_elevation = 1j*result.omega/result.g * self.compute_potential(points, result)\n        return fs_elevation.reshape(output_shape)\n\n\n    ## Legacy\n\n    def get_potential_on_mesh(self, result, mesh, chunk_size=50):\n        \"\"\"Compute the potential on a mesh for the potential field of a previously solved problem.\n        Since the interaction matrix does not need to be computed in full to compute the matrix-vector product,\n        only a few lines are evaluated at a time to reduce the memory cost of the operation.\n\n        The newer method :code:`compute_potential` should be prefered in the future.\n\n        Parameters\n        ----------\n        result : LinearPotentialFlowResult\n            the return of the BEM solver\n        mesh : Mesh or CollectionOfMeshes\n            a mesh\n        chunk_size: int, optional\n            Number of lines to compute in the matrix.\n            (legacy, should be passed as an engine setting instead).\n\n        Returns\n        -------\n        array of shape (mesh.nb_faces,)\n            potential on the faces of the mesh\n\n        Raises\n        ------\n        Exception: if the :code:`Result` object given as input does not contain the source distribution.\n        \"\"\"\n        LOG.info(f\"Compute potential on {mesh.name} for {result}.\")\n\n        if result.sources is None:\n            raise Exception(f\"\"\"The values of the sources of {result} cannot been found.\n            They probably have not been stored by the solver because the option keep_details=True have not been set.\n            Please re-run the resolution with this option.\"\"\")\n\n        if chunk_size > mesh.nb_faces:\n            S = self.engine.build_S_matrix(\n                mesh,\n                result.body.mesh,\n                result.free_surface, result.water_depth, result.wavenumber,\n                self.green_function\n            )\n            phi = S @ result.sources\n\n        else:\n            phi = np.empty((mesh.nb_faces,), dtype=np.complex128)\n            for i in range(0, mesh.nb_faces, chunk_size):\n                faces_to_extract = list(range(i, min(i+chunk_size, mesh.nb_faces)))\n                S = self.engine.build_S_matrix(\n                    mesh.extract_faces(faces_to_extract),\n                    result.body.mesh,\n                    result.free_surface, result.water_depth, result.wavenumber,\n                    self.green_function\n                )\n                phi[i:i+chunk_size] = S @ result.sources\n\n        LOG.debug(f\"Done computing potential on {mesh.name} for {result}.\")\n\n        return phi\n\n    def get_free_surface_elevation(self, result, free_surface, keep_details=False):\n        \"\"\"Compute the elevation of the free surface on a mesh for a previously solved problem.\n\n        The newer method :code:`compute_free_surface_elevation` should be prefered in the future.\n\n        Parameters\n        ----------\n        result : LinearPotentialFlowResult\n            the return of the solver\n        free_surface : FreeSurface\n            a meshed free surface\n        keep_details : bool, optional\n            if True, keep the free surface elevation in the LinearPotentialFlowResult (default:False)\n\n        Returns\n        -------\n        array of shape (free_surface.nb_faces,)\n            the free surface elevation on each faces of the meshed free surface\n\n        Raises\n        ------\n        Exception: if the :code:`Result` object given as input does not contain the source distribution.\n        \"\"\"\n        fs_elevation = 1j*result.omega/result.g * self.get_potential_on_mesh(result, free_surface.mesh)\n        if keep_details:\n            result.fs_elevation[free_surface] = fs_elevation\n        return fs_elevation",
  "def __init__(self, *, green_function=None, engine=None):\n        self.green_function = Delhommeau() if green_function is None else green_function\n        self.engine = BasicMatrixEngine() if engine is None else engine\n\n        try:\n            self.exportable_settings = {\n                **self.green_function.exportable_settings,\n                **self.engine.exportable_settings\n            }\n        except AttributeError:\n            pass",
  "def __str__(self):\n        return f\"BEMSolver(engine={self.engine}, green_function={self.green_function})\"",
  "def __repr__(self):\n        return self.__str__()",
  "def _repr_pretty_(self, p, cycle):\n        p.text(self.__str__())",
  "def from_exported_settings(settings):\n        raise NotImplementedError",
  "def solve(self, problem, keep_details=True):\n        \"\"\"Solve the linear potential flow problem.\n\n        Parameters\n        ----------\n        problem: LinearPotentialFlowProblem\n            the problem to be solved\n        keep_details: bool, optional\n            if True, store the sources and the potential on the floating body in the output object\n            (default: True)\n\n        Returns\n        -------\n        LinearPotentialFlowResult\n            an object storing the problem data and its results\n        \"\"\"\n        LOG.info(\"Solve %s.\", problem)\n\n        S, K = self.engine.build_matrices(\n            problem.body.mesh, problem.body.mesh,\n            problem.free_surface, problem.water_depth, problem.wavenumber,\n            self.green_function\n        )\n        sources = self.engine.linear_solver(K, problem.boundary_condition)\n        potential = S @ sources\n        pressure = 1j * problem.omega * problem.rho * potential\n\n        forces = problem.body.integrate_pressure(pressure)\n\n        if not keep_details:\n            result = problem.make_results_container(forces)\n        else:\n            result = problem.make_results_container(forces, sources, potential, pressure)\n\n        LOG.debug(\"Done!\")\n\n        return result",
  "def solve_all(self, problems, *, n_jobs=1, **kwargs):\n        \"\"\"Solve several problems.\n        Optional keyword arguments are passed to `BEMSolver.solve`.\n\n        Parameters\n        ----------\n        problems: list of LinearPotentialFlowProblem\n            several problems to be solved\n        n_jobs: int, optional (default: 1)\n            the number of jobs to run in parallel using the optional dependency `joblib`\n            By defaults: do not use joblib and solve sequentially.\n\n        Returns\n        -------\n        list of LinearPotentialFlowResult\n            the solved problems\n        \"\"\"\n        if n_jobs == 1:  # force sequential resolution\n            return [self.solve(pb, **kwargs) for pb in sorted(problems)]\n        else:\n            joblib = silently_import_optional_dependency(\"joblib\")\n            if joblib is None:\n                raise ImportError(f\"Setting the `n_jobs` argument to {n_jobs} requires the missing optional dependency 'joblib'.\")\n            groups_of_problems = LinearPotentialFlowProblem._group_for_parallel_resolution(problems)\n            groups_of_results = joblib.Parallel(n_jobs=n_jobs)(joblib.delayed(self.solve_all)(grp, n_jobs=1, **kwargs) for grp in groups_of_problems)\n            results = [res for grp in groups_of_results for res in grp]  # flatten the nested list\n            return results",
  "def fill_dataset(self, dataset, bodies, *, n_jobs=1, **kwargs):\n        \"\"\"Solve a set of problems defined by the coordinates of an xarray dataset.\n\n        Parameters\n        ----------\n        dataset : xarray Dataset\n            dataset containing the problems parameters: frequency, radiating_dof, water_depth, ...\n        bodies : FloatingBody or list of FloatingBody\n            The body or bodies involved in the problems\n            They should all have different names.\n        n_jobs: int, optional (default: 1)\n            the number of jobs to run in parallel using the optional dependency `joblib`\n            By defaults: do not use joblib and solve sequentially.\n\n        Returns\n        -------\n        xarray Dataset\n        \"\"\"\n        attrs = {'start_of_computation': datetime.now().isoformat(),\n                 **self.exportable_settings}\n        problems = problems_from_dataset(dataset, bodies)\n        if 'theta' in dataset.coords:\n            results = self.solve_all(problems, keep_details=True, n_jobs=n_jobs)\n            kochin = kochin_data_array(results, dataset.coords['theta'])\n            dataset = assemble_dataset(results, attrs=attrs, **kwargs)\n            dataset.update(kochin)\n        else:\n            results = self.solve_all(problems, keep_details=False, n_jobs=n_jobs)\n            dataset = assemble_dataset(results, attrs=attrs, **kwargs)\n        return dataset",
  "def compute_potential(self, points, result):\n        \"\"\"Compute the value of the potential at given points for a previously solved potential flow problem.\n\n        Parameters\n        ----------\n        points: array of shape (3,) or (N, 3), or 3-ple of arrays returned by meshgrid, or cpt.Mesh or cpt.CollectionOfMeshes object\n            Coordinates of the point(s) at which the potential should be computed\n        results: LinearPotentialFlowResult\n            The return of the BEM solver\n\n        Returns\n        -------\n        complex-valued array of shape (1,) or (N,) or (nx, ny, nz) or (mesh.nb_faces,) depending of the kind of input\n            The value of the potential at the points\n\n        Raises\n        ------\n        Exception: if the :code:`LinearPotentialFlowResult` object given as input does not contain the source distribution.\n        \"\"\"\n        points, output_shape = _normalize_points(points, keep_mesh=True)\n        if result.sources is None:\n            raise Exception(f\"\"\"The values of the sources of {result} cannot been found.\n            They probably have not been stored by the solver because the option keep_details=True have not been set.\n            Please re-run the resolution with this option.\"\"\")\n\n        S, _ = self.green_function.evaluate(points, result.body.mesh, result.free_surface, result.water_depth, result.wavenumber)\n        potential = S @ result.sources  # Sum the contributions of all panels in the mesh\n        return potential.reshape(output_shape)",
  "def compute_velocity(self, points, result):\n        \"\"\"Compute the value of the velocity vector at given points for a previously solved potential flow problem.\n\n        Parameters\n        ----------\n        points: array of shape (3,) or (N, 3), or 3-ple of arrays returned by meshgrid, or cpt.Mesh or cpt.CollectionOfMeshes object\n            Coordinates of the point(s) at which the velocity should be computed\n        results: LinearPotentialFlowResult\n            The return of the BEM solver\n\n        Returns\n        -------\n        complex-valued array of shape (3,) or (N,, 3) or (nx, ny, nz, 3) or (mesh.nb_faces, 3) depending of the kind of input\n            The value of the velocity at the points\n\n        Raises\n        ------\n        Exception: if the :code:`LinearPotentialFlowResult` object given as input does not contain the source distribution.\n        \"\"\"\n        points, output_shape = _normalize_points(points, keep_mesh=True)\n\n        if result.sources is None:\n            raise Exception(f\"\"\"The values of the sources of {result} cannot been found.\n            They probably have not been stored by the solver because the option keep_details=True have not been set.\n            Please re-run the resolution with this option.\"\"\")\n\n        _, gradG = self.green_function.evaluate(points, result.body.mesh, result.free_surface, result.water_depth, result.wavenumber,\n                                                early_dot_product=False)\n        velocities = np.einsum('ijk,j->ik', gradG, result.sources)  # Sum the contributions of all panels in the mesh\n        return velocities.reshape((*output_shape, 3))",
  "def compute_pressure(self, points, result):\n        \"\"\"Compute the value of the pressure at given points for a previously solved potential flow problem.\n\n        Parameters\n        ----------\n        points: array of shape (3,) or (N, 3), or 3-ple of arrays returned by meshgrid, or cpt.Mesh or cpt.CollectionOfMeshes object\n            Coordinates of the point(s) at which the pressure should be computed\n        results: LinearPotentialFlowResult\n            The return of the BEM solver\n\n        Returns\n        -------\n        complex-valued array of shape (1,) or (N,) or (nx, ny, nz) or (mesh.nb_faces,) depending of the kind of input\n            The value of the pressure at the points\n\n        Raises\n        ------\n        Exception: if the :code:`LinearPotentialFlowResult` object given as input does not contain the source distribution.\n        \"\"\"\n        return 1j * result.omega * result.rho * self.compute_potential(points, results)",
  "def compute_free_surface_elevation(self, points, result):\n        \"\"\"Compute the value of the free surface elevation at given points for a previously solved potential flow problem.\n\n        Parameters\n        ----------\n        points: array of shape (2,) or (N, 2), or 2-ple of arrays returned by meshgrid, or cpt.Mesh or cpt.CollectionOfMeshes object\n            Coordinates of the point(s) at which the free surface elevation should be computed\n        results: LinearPotentialFlowResult\n            The return of the BEM solver\n\n        Returns\n        -------\n        complex-valued array of shape (1,) or (N,) or (nx, ny, nz) or (mesh.nb_faces,) depending of the kind of input\n            The value of the free surface elevation at the points\n\n        Raises\n        ------\n        Exception: if the :code:`LinearPotentialFlowResult` object given as input does not contain the source distribution.\n        \"\"\"\n        points, output_shape = _normalize_free_surface_points(points, keep_mesh=True)\n\n        fs_elevation = 1j*result.omega/result.g * self.compute_potential(points, result)\n        return fs_elevation.reshape(output_shape)",
  "def get_potential_on_mesh(self, result, mesh, chunk_size=50):\n        \"\"\"Compute the potential on a mesh for the potential field of a previously solved problem.\n        Since the interaction matrix does not need to be computed in full to compute the matrix-vector product,\n        only a few lines are evaluated at a time to reduce the memory cost of the operation.\n\n        The newer method :code:`compute_potential` should be prefered in the future.\n\n        Parameters\n        ----------\n        result : LinearPotentialFlowResult\n            the return of the BEM solver\n        mesh : Mesh or CollectionOfMeshes\n            a mesh\n        chunk_size: int, optional\n            Number of lines to compute in the matrix.\n            (legacy, should be passed as an engine setting instead).\n\n        Returns\n        -------\n        array of shape (mesh.nb_faces,)\n            potential on the faces of the mesh\n\n        Raises\n        ------\n        Exception: if the :code:`Result` object given as input does not contain the source distribution.\n        \"\"\"\n        LOG.info(f\"Compute potential on {mesh.name} for {result}.\")\n\n        if result.sources is None:\n            raise Exception(f\"\"\"The values of the sources of {result} cannot been found.\n            They probably have not been stored by the solver because the option keep_details=True have not been set.\n            Please re-run the resolution with this option.\"\"\")\n\n        if chunk_size > mesh.nb_faces:\n            S = self.engine.build_S_matrix(\n                mesh,\n                result.body.mesh,\n                result.free_surface, result.water_depth, result.wavenumber,\n                self.green_function\n            )\n            phi = S @ result.sources\n\n        else:\n            phi = np.empty((mesh.nb_faces,), dtype=np.complex128)\n            for i in range(0, mesh.nb_faces, chunk_size):\n                faces_to_extract = list(range(i, min(i+chunk_size, mesh.nb_faces)))\n                S = self.engine.build_S_matrix(\n                    mesh.extract_faces(faces_to_extract),\n                    result.body.mesh,\n                    result.free_surface, result.water_depth, result.wavenumber,\n                    self.green_function\n                )\n                phi[i:i+chunk_size] = S @ result.sources\n\n        LOG.debug(f\"Done computing potential on {mesh.name} for {result}.\")\n\n        return phi",
  "def get_free_surface_elevation(self, result, free_surface, keep_details=False):\n        \"\"\"Compute the elevation of the free surface on a mesh for a previously solved problem.\n\n        The newer method :code:`compute_free_surface_elevation` should be prefered in the future.\n\n        Parameters\n        ----------\n        result : LinearPotentialFlowResult\n            the return of the solver\n        free_surface : FreeSurface\n            a meshed free surface\n        keep_details : bool, optional\n            if True, keep the free surface elevation in the LinearPotentialFlowResult (default:False)\n\n        Returns\n        -------\n        array of shape (free_surface.nb_faces,)\n            the free surface elevation on each faces of the meshed free surface\n\n        Raises\n        ------\n        Exception: if the :code:`Result` object given as input does not contain the source distribution.\n        \"\"\"\n        fs_elevation = 1j*result.omega/result.g * self.get_potential_on_mesh(result, free_surface.mesh)\n        if keep_details:\n            result.fs_elevation[free_surface] = fs_elevation\n        return fs_elevation"
]